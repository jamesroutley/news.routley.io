<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://buttondown.email/jaffray/archive/the-case-of-a-curious-sql-query/">Original</a>
    <h1>The Case of a Curious SQL Query</h1>
    
    <div id="readability-page-1" class="page"><div>

                

                
                    
                        <p>Languages that suffer success often have to do so by selling out and adding features that go against some of the original purposes of their design.</p>
<p>SQL is a great example of a language built on very solid foundations: it comes from the idea that we should define an algebra for data retrieval, and then we can formally define how that algebra should behave, and then we can have a common tongue between humans who want to query databases and databases who want to execute CPU instructions.</p>
<p>This is the kind of thing that excites people who implement query engines and are conflict averse because it creates an authoritative source for how queries should behave. We&#39;ve formally defined what it means for this given operator to run, so if there&#39;s any dispute, we have the technology to figure out what the result &#34;should be.&#34;</p>
<p>This is particularly nice in the presence of optimizing such queries. For example, &#34;predicate pushdown&#34; is a well-known optimization which says that if I have a predicate on an inner join that only references columns from one of its arms, I can push it down into one of those arms and reduce the size of the data I have to join:</p>
<div><pre><span></span><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span> <span>AND</span> <span>abc.b</span> <span>=</span> <span>4</span>
</code></pre></div>

<p>can become</p>
<div><pre><span></span><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>(</span><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>WHERE</span> <span>abc.b</span> <span>=</span> <span>4</span><span>)</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span>
</code></pre></div>

<p>Which may or may not be hard to convince yourself of, but if you have formal definitions of what a join is laid out in front of you, it becomes very obvious that this is correct.</p>
<p>Things like this got messier when people actually started <em>using</em> these languages and started caring about properties (say, the order of a result set) of a relational query that the original model didn&#39;t have a notion of. Of course, that one is fairly easily resolved, but it is sometimes more difficult, and we get back into the realm of ambiguity.</p>
<p>Here is my favourite SQL query:</p>
<div><pre><span></span><code><span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span> <span>one_thousand</span> <span>INNER</span> <span>JOIN</span> <span>one_thousand</span> <span>ON</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
</code></pre></div>

<p>Where <code>one_thousand</code> is a single column table of the numbers 0, 1, ... 999.</p>
<p>What number do we expect to see from this query? To me, intuitively, it seems like we should be evaluating this predicate on each pair of rows from the two inputs, so we&#39;d expect to get about half of them, so we should see about ~500000 rows. And if we run this in DuckDB, that&#39;s indeed more or less what we see:</p>
<div><pre><span></span><code>D SELECT count(*) FROM one_thousand a INNER JOIN one_thousand b ON random() &lt; 0.5;
┌──────────────┐
│ count_star() │
├──────────────┤
│ 499910       │
└──────────────┘
D SELECT count(*) FROM one_thousand a INNER JOIN one_thousand b ON random() &lt; 0.5;
┌──────────────┐
│ count_star() │
├──────────────┤
│ 499739       │
└──────────────┘
D SELECT count(*) FROM one_thousand a INNER JOIN one_thousand b ON random() &lt; 0.5;
┌──────────────┐
│ count_star() │
├──────────────┤
│ 500054       │
└──────────────┘
D
</code></pre></div>

<p>I ran this query 1000 times, and the mean result was 500011.6034, hovering right around the expected mean of 500000.</p>
<p>We should expect these results to be binomially distributed, and if we plot a histogram of a bunch of runs, that&#39;s basically what we see:
 <img alt="image.png" src="https://buttondown.imgix.net/images/7ba49cec-d8b6-4e4f-b4c4-5767ac911358.png?w=960&amp;fit=max"/> </p>
<p>Let&#39;s try a different database, SQLite:</p>
<div><pre><span></span><code><span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>511000</span>
</code></pre></div>

<p>Looks pretty reasonable. Doing another 10000 trials, we get a mean of 499865.8, which seems like we&#39;re getting the same behaviour as DuckDB. Now let&#39;s plot those 10000 trials in a histogram:
 <img alt="image.png" src="https://buttondown.imgix.net/images/86611e07-5f63-4483-8dc2-2670abd32d4d.png?w=960&amp;fit=max"/> </p>
<p>Hm! Looks suspiciously different than DuckDB! Not only are the points much more sparse, the variance of the distribution is also much higher. What&#39;s going on here? SQLite plans are unfortunately not the most enlightening:</p>
<div><pre><span></span><code><span>sqlite</span><span>&gt;</span> <span>explain</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>addr</span>  <span>opcode</span>         <span>p1</span>    <span>p2</span>    <span>p3</span>    <span>p4</span>             <span>p5</span>  <span>comment</span>
<span>----  -------------  ----  ----  ----  -------------  --  -------------</span>
<span>0</span>     <span>Init</span>           <span>0</span>     <span>17</span>    <span>0</span>                    <span>0</span>   <span>Start</span> <span>at</span> <span>17</span>
<span>1</span>     <span>Null</span>           <span>0</span>     <span>1</span>     <span>1</span>                    <span>0</span>   <span>r[</span><span>1</span><span>..</span><span>1</span><span>]</span><span>=</span><span>NULL</span>
<span>2</span>     <span>OpenRead</span>       <span>0</span>     <span>2</span>     <span>0</span>     <span>0</span>              <span>0</span>   <span>root</span><span>=</span><span>2</span> <span>iDb</span><span>=</span><span>0</span><span>;</span> <span>one_thousand</span>
<span>3</span>     <span>OpenRead</span>       <span>1</span>     <span>2</span>     <span>0</span>     <span>0</span>              <span>0</span>   <span>root</span><span>=</span><span>2</span> <span>iDb</span><span>=</span><span>0</span><span>;</span> <span>one_thousand</span>
<span>4</span>     <span>Explain</span>        <span>4</span>     <span>0</span>     <span>0</span>     <span>SCAN</span> <span>a</span>         <span>0</span>
<span>5</span>     <span>Rewind</span>         <span>0</span>     <span>13</span>    <span>0</span>                    <span>0</span>
<span>6</span>       <span>Function</span>       <span>0</span>     <span>0</span>     <span>2</span>     <span>random(</span><span>0</span><span>)</span>      <span>0</span>   <span>r[</span><span>2</span><span>]</span><span>=</span><span>func()</span>
<span>7</span>       <span>Ge</span>             <span>3</span>     <span>12</span>    <span>2</span>                    <span>80</span>  <span>if</span> <span>r[</span><span>2</span><span>]</span><span>&gt;=</span><span>r[</span><span>3</span><span>]</span> <span>goto</span> <span>12</span>
<span>8</span>       <span>Explain</span>        <span>8</span>     <span>0</span>     <span>0</span>     <span>SCAN</span> <span>b</span>         <span>0</span>
<span>9</span>       <span>Rewind</span>         <span>1</span>     <span>13</span>    <span>0</span>                    <span>0</span>
<span>10</span>        <span>AggStep</span>        <span>0</span>     <span>0</span>     <span>1</span>     <span>count</span><span>(</span><span>0</span><span>)</span>       <span>0</span>   <span>accum</span><span>=</span><span>r[</span><span>1</span><span>]</span> <span>step(r[</span><span>0</span><span>])</span>
<span>11</span>      <span>Next</span>           <span>1</span>     <span>10</span>    <span>0</span>                    <span>1</span>
<span>12</span>    <span>Next</span>           <span>0</span>     <span>6</span>     <span>0</span>                    <span>1</span>
<span>13</span>    <span>AggFinal</span>       <span>1</span>     <span>0</span>     <span>0</span>     <span>count</span><span>(</span><span>0</span><span>)</span>       <span>0</span>   <span>accum</span><span>=</span><span>r[</span><span>1</span><span>]</span> <span>N</span><span>=</span><span>0</span>
<span>14</span>    <span>Copy</span>           <span>1</span>     <span>4</span>     <span>0</span>                    <span>0</span>   <span>r[</span><span>4</span><span>]</span><span>=</span><span>r[</span><span>1</span><span>]</span>
<span>15</span>    <span>ResultRow</span>      <span>4</span>     <span>1</span>     <span>0</span>                    <span>0</span>   <span>output</span><span>=</span><span>r[</span><span>4</span><span>]</span>
<span>16</span>    <span>Halt</span>           <span>0</span>     <span>0</span>     <span>0</span>                    <span>0</span>
<span>17</span>    <span>Transaction</span>    <span>0</span>     <span>0</span>     <span>1</span>     <span>0</span>              <span>1</span>   <span>usesStmtJournal</span><span>=</span><span>0</span>
<span>18</span>    <span>Real</span>           <span>0</span>     <span>3</span>     <span>0</span>     <span>0</span><span>.</span><span>5</span>            <span>0</span>   <span>r[</span><span>3</span><span>]</span><span>=</span><span>0</span><span>.</span><span>5</span>
<span>19</span>    <span>Goto</span>           <span>0</span>     <span>1</span>     <span>0</span>                    <span>0</span>
</code></pre></div>

<p>But what&#39;s going on here becomes a bit more obvious if we look at a few more sample results of our query:</p>
<div><pre><span></span><code><span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>481000</span>
<span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>486000</span>
<span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>503000</span>
<span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>518000</span>
</code></pre></div>

<p>They&#39;re notably all divisible by 1000. Recall the query transformation we opened with:</p>
<div><pre><span></span><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span> <span>AND</span> <span>abc.b</span> <span>=</span> <span>4</span>
<span>=&gt;</span>
<span>SELECT</span> <span>*</span> <span>FROM</span> <span>(</span><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>WHERE</span> <span>abc.b</span> <span>=</span> <span>4</span><span>)</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span>
</code></pre></div>

<p>This optimization is valid because <code>abc.b = 4</code> doesn&#39;t use any of the columns from <code>def</code>, so we can push it down and evaluate it directly on <code>abc</code>.</p>
<p>In this query:</p>
<div><pre><span></span><code><span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
</code></pre></div>

<p><code>random() &lt; 0.5</code> doesn&#39;t depend on any columns in <code>one_thousand</code>, and so SQLite concludes that it&#39;s valid to transform the query to something like:</p>
<div><pre><span></span><code><span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span>
  <span>(</span><span>SELECT</span> <span>*</span> <span>FROM</span> <span>one_thousand</span> <span>WHERE</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>)</span>
    <span>INNER</span> <span>JOIN</span>
  <span>one_thousand</span> 
    <span>ON</span> <span>true</span><span>;</span>
</code></pre></div>

<p>Which, since the right side of this join has exactly 1000 rows, will always result in a number divisible by 1000.</p>
<p>For a final example, let&#39;s look at CockroachDB (disclosure, disclosure, former employer, used to work on the query optimizer specifically).</p>
<p>Doing 10000 trials of our query in CockroachDB, we get a mean of 249889.969(!) and the following distribution:</p>
<p><img alt="image.png" src="https://buttondown.imgix.net/images/29467364-f2c6-4391-88a4-47e36519a80e.png?w=960&amp;fit=max"/></p>
<p>You might be able to guess what&#39;s going on here. Here&#39;s an <code>EXPLAIN</code> plan for the query:</p>
<div><pre><span></span><code>                              <span>info</span>
<span>----------------------------------------------------------------</span>
 <span>•</span> <span>group</span> <span>(scalar)</span>
 <span>└──</span> <span>•</span> <span>cross</span> <span>join</span>
     <span>├──</span> <span>•</span> <span>filter</span>
     <span>│</span>   <span>│</span> <span>filter:</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
     <span>│</span>   <span>└──</span> <span>•</span> <span>scan</span>
     <span>└──</span> <span>•</span> <span>filter</span>
         <span>│</span> <span>filter:</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
         <span>└──</span> <span>•</span> <span>scan</span>
</code></pre></div>

<p>Aha! CockroachDB notes that <code>random() &lt; 0.5</code> isn&#39;t bound by <em>either</em> side of the join, and so we can push it down into <em>both</em> sides. Now we are effectively performing this filter twice for each eventual output row, and thus we end up with about 25% of the rows in the final output. You&#39;ll also notice it&#39;s much spikier than the other ones, with more outliers, I suppose this is partly because this execution plan means we are more likely to output numbers which are more composite than ones which are closer to being prime.</p>
<p>As one last example, in some SQL dialects there is a &#34;set returning function&#34; called <code>generate_series</code> which can be used to construct tables like <code>one_thousand</code>. In CockroachDB:</p>
<div><pre><span></span><code><span>defaultdb</span><span>=&gt;</span> <span>select</span> <span>*</span> <span>from</span> <span>generate_series(</span><span>0</span><span>,</span> <span>999</span><span>);</span>
 <span>generate_series</span>
<span>-----------------</span>
               <span>0</span>
               <span>1</span>
               <span>2</span>
             <span>...</span>
             <span>999</span>
</code></pre></div>

<p>We can again construct our query but using this function instead of a reference to an explicit table:</p>
<div><pre><span></span><code><span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span>
  <span>generate_series(</span><span>0</span><span>,</span><span>999</span><span>)</span> <span>a</span>
    <span>inner</span> <span>join</span>
  <span>generate_series(</span><span>0</span><span>,</span><span>999</span><span>)</span>
    <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
</code></pre></div>

<p>Running 10000 trials of this, I got a mean of 241700.0.
Surprisingly far from the expected mean!
Plotting the histogram shows us why:</p>
<p><img alt="image.png" src="https://buttondown.imgix.net/images/fd729971-66b7-43b0-a427-9437fb039c36.png?w=960&amp;fit=max"/> </p>
<p>~75% of the results were 0, and ~25% were 1000000. To see why, we can look at the <code>EXPLAIN</code>:</p>
<div><pre><span></span><code>                 info
---------------------------------------
 • group (scalar)
 └── • cross join
     ├── • cross join
     │   ├── • project set
     │   │   └── • emptyrow
     │   └── • filter: random() &lt; 0.5
     │       └── • emptyrow
     └── • cross join
         ├── • project set
         │   └── • emptyrow
         └── • filter: random() &lt; 0.5
             └── • emptyrow
</code></pre></div>

<p>For reasons that aren&#39;t apparent in this simple example, these <code>generate_series</code> get planned as joins against some input. In this case, the input is just the unit row (zero columns, one row) named &#34;emptyrow&#34; in this plan.</p>
<p>For our query, the <code>random() &lt; 0.5</code> filter gets pushed down both sides <em>all the way</em> to this unit row. Where on any execution, it either gets filtered or not.
So if you win both coin flips, you get the full output, otherwise, you get nothing.</p>
<p>Is any of this a particularly important consideration? Eh. No. I dunno. Not really. I don&#39;t think you could make a particularly compelling claim that any of these behaviours are <em>wrong</em>. I don&#39;t know what, if anything, the SQL spec has to say on this, but I think the industry generally seems to agree that&#39;s more of a gentle suggestion than anything resembling a &#34;standard.&#34;</p>
<p>This kind of thing (impure functions in a declarative language, specifically) does cause <em>actual</em> problems in other contexts, which we will talk about someday, but for now I think it&#39;s a fun little mind bender that gives you some insight into the internals of these databases query engines without having to actually look at any code.</p>
                    
                

                
            </div></div>
  </body>
</html>
