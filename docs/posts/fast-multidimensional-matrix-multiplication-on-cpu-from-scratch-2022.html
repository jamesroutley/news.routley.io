<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://siboehm.com/articles/22/Fast-MMM-on-CPU">Original</a>
    <h1>Fast Multidimensional Matrix Multiplication on CPU from Scratch (2022)</h1>
    
    <div id="readability-page-1" class="page"><article>


<p>Numpy can multiply two 1024x1024 matrices on a 4-core Intel CPU in ~8ms.
This is incredibly fast, considering this boils down to 18 FLOPs / core / cycle, with a cycle taking a third of a nanosecond.
Numpy does this using a highly optimized BLAS implementation.<label for="1"></label><span><a href="https://netlib.org/blas/">BLAS</a> is short for Basic Linear Algebra Subprograms. These are libraries providing fast implementations of eg Matrix multiplications or dot-products. They are sometimes tailored to one specific (family of) CPUs, like <a href="https://en.wikipedia.org/wiki/Math_Kernel_Library">Intel’s MKL</a> or <a href="https://developer.apple.com/documentation/accelerate">Apple’s accelerate</a>. However, non-Vendor specific implementations like <a href="https://github.com/xianyi/OpenBLAS">OpenBLAS</a> are also available.</span>
How hard is it to recreate performance that’s roughly similar using plain C++?</p>

<h2 id="calculating-total-flops">Calculating total FLOPs</h2>
<p>For simplicity, let’s assume both matrices are square.
For each entry of our NxN result matrix, we have to perform a dot product between a row vector and a column vector, both of length N.</p>
<div><div><pre><code><span>def</span> <span>MMM</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>)</span>
    <span>C</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>A</span><span>.</span><span>n_rows</span><span>,</span> <span>B</span><span>.</span><span>n_columns</span><span>))</span>
    <span>for</span> <span>row</span> <span>in</span> <span>range</span><span>(</span><span>A</span><span>.</span><span>n_rows</span><span>):</span>
        <span>for</span> <span>col</span> <span>in</span> <span>range</span><span>(</span><span>B</span><span>.</span><span>n_columns</span><span>):</span>
            <span>for</span> <span>inner</span> <span>in</span> <span>range</span><span>(</span><span>A</span><span>.</span><span>n_inner</span><span>):</span>
                <span>C</span><span>[</span><span>row</span><span>,</span> <span>col</span><span>]</span> <span>=</span> <span>C</span><span>[</span><span>row</span><span>,</span> <span>col</span><span>]</span> <span>+</span> <span>A</span><span>[</span><span>row</span><span>,</span> <span>inner</span><span>]</span> <span>*</span> <span>B</span><span>[</span><span>inner</span><span>,</span> <span>col</span><span>]</span>
    <span>return</span> <span>C</span>
</code></pre></div></div>
<p>This results in N(=rows) * N(=columns) * N(=dot product) * 2(mul + add) = 2N³ FLOPs.</p>
<p><img src="https://siboehm.com/assets/img/MMM/Basic_MMM.png" alt="Basic MMM"/></p>
<h3 id="running-on-a-physical-machine">Running on a physical machine</h3>
<p>In Numpy, the code for our example looks like this:</p>
<div><div><pre><code><span>x</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>1024</span><span>,</span> <span>1024</span><span>).</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>float32</span><span>)</span>
<span>y</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>1024</span><span>,</span> <span>1024</span><span>).</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>float32</span><span>)</span>
<span>start</span> <span>=</span> <span>time</span><span>.</span><span>time_ns</span><span>()</span>
<span>z</span> <span>=</span> <span>np</span><span>.</span><span>dot</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
<span>end</span> <span>=</span> <span>time</span><span>.</span><span>time_ns</span><span>()</span> <span>-</span> <span>start</span>
</code></pre></div></div>
<p>When I run this on my dedicated server, equipped with an Intel i7-6700 (a quad-core Haswell CPU) it takes 8ms.</p>
<ul>
<li>Total FLOPs: 2 Billion.</li>
<li>Total memory (LOAD): 8MB using fp32.</li>
<li>Total cycles: 8ms x 3.4GHz = 27 Million</li>
</ul>
<p>That’s 18 FLOPS / core / cycle, or ~250GFLOP/s, on hardware released in 2015.
That’s a lot!</p>
<h3 id="how-can-a-single-core-do-18-flops-in-a-cycle">How can a single core do 18 FLOPs in a cycle?</h3>
<p>On my Haswell server, Numpy uses Intel’s MKL implementation of BLAS.
Particularly we care about how the <code>SGEMM</code> function is implemented, which is the function that is called for matrix multiplications.<label for="2"></label><span>SGEMM is short for single-precision general matrix multiply. <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">GEMM</a> performs this computation: <code>C = α*A*B + β*C</code>. A,B,C are matrices and α,β are scalars.</span>
Digging around the binary, there are multiple SGEMM implementations, each specific to one of Intel’s microarchitectures: eg <code>sgemm_kernel_HASWELL</code>, <code>sgemm_kernel_SANDYBRIDGE</code>, …
At runtime, the BLAS library will use the <a href="https://www.felixcloutier.com/x86/cpuid">cpuid</a> instruction to query the details of the processor and then call the suitable function.<label for="3"></label><span>This will increase the size of the BLAS binary considerably since it’s carrying around GEMM implementations for many architectures, even though we ever only need one.</span></p>
<p>Looking closely at the relevant <code>sgemm_kernel_HASWELL</code>, the speed comes from using vectorized<label for="4"></label><span>A vectorized / SIMD instruction performs the same instruction on all entries of the vector input at once:<img src="https://siboehm.com/assets/img/MMM/Scalar_vs_Vectorized.png" alt=""/></span> FMA<label for="5"></label><span>FMA stands for Fused Multiply Add. This means performing <code>A = A + B*C</code>, but using a single (fused) instruction. Sometimes this is also refered to as MAC (Multiply Accumulate).</span> instructions, in my particular case the <code>VFMADD</code> instruction.
It operates on three 256bit long <code>YMM</code> registers, calculating <code>(YMM1 * YMM2) + YMM3</code> and storing the result in <code>YMM3</code>.
That allows the CPU to perform 16 single-precision FLOPs in one instruction.
Checking <a href="https://agner.org/">Agner Fog</a>’s instruction tables and <a href="https://uops.info/">uops.info</a>, <code>VFMADD</code> has a throughput<label for="6"></label><span>Confusingly, this is the reciprocal throughput, meaning the average number of clock cycles per instruction, for a series of independent instructions of the same kind in the same thread.</span> of 0.5 cycles.
This means our theoretical upper limit should be 2 * <code>VFMADD</code> instructions per cycle, or 32 FLOPS / cycle.</p>
<p>At a latency<label for="7"></label><span>Latency here is the number of cycles between starting the instruction and having the result available to other instructions.</span> of 5 cycles, this means we need to find 10 * 16 FLOPs that we can schedule independently, since the result of previous instructions is only available 5 cycles after they were started.
This will be one consideration when writing optimized code: Grouping enough independent operations such that the CPU can schedule all of them at once, fully exploiting its instruction-level parallelism (ILP) capabilities.</p>
<p>So to conclude, Intel’s BLAS library achieves 18 FLOPs/core/cycle, where the theoretical upper bound is 32 FLOPs/core/cycle.
Pretty wild!
In reality, the implementation is even faster, since we’re also measuring some Python overhead and the time to start the OpenMP thread pool.</p>
<p>Note how even though the matrices aren’t that big, we’re strongly compute bound already.
Loading 8MB from RAM takes maybe 200μs, assuming a memory bandwidth of 40GB/s<label for="8"></label><span>For multi-threaded, SIMD memory accessing. Numbers from the excellent <a href="https://github.com/sirupsen/napkin-math">napkin math</a> project.</span>.
If the matrices get bigger, we become more compute-bound, since we’re performing 2n³ FLOPs for 2n² loads.</p>
<h2 id="trying-to-recreate-this-performance-from-scratch">Trying to recreate this performance from scratch</h2>
<p>To spoiler the outcome, we’ll end up with an implementation that performs 9 FLOPS / core / cycle, but only works for matrices of a specific size.
The goal of this post is not to write a competitive BLAS implementation, but to learn about common performance optimizations.<label for="9"></label><span>To compare, the <a href="https://github.com/xianyi/OpenBLAS/blob/develop/kernel/x86_64/sgemm_kernel_16x4_haswell.S">MMM implementation in OpenBlas</a> is ~7K LOC of handwritten assembly.</span></p>
<p>I’m running this on a quadcore Intel i7-6700 CPU @ 3.40GHz, on a dedicated server.
It has 32KiB per-core L1d cache, 256KiB of per-core L2 cache, and a shared 8MB L3 cache.<label for="10"></label><span>Visualized with <code>lstopo</code>: <img src="https://siboehm.com/assets/img/MMM/CPU-cache-layout.png" alt=""/></span></p>
<p>The compiler I’m using is clang v14.0.
Benchmarking is done through <a href="https://github.com/google/benchmark">Google Benchmark</a>.<label for="11"></label><span>For sanity, after each benchmark, I compare the result to PyTorch’s MMM implementation to make sure my implementation is correct.</span></p>
<table>
<thead>
<tr>
<th>implementation</th>
<th>time (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Naive Implementation (RCI)</td>
<td><code>4481</code></td>
</tr>
<tr>
<td>Naive Implementation (RCI) + compiler flags</td>
<td><code>1621</code></td>
</tr>
<tr>
<td>Naive Implementation (RCI) + flags + register accumulate</td>
<td><code>1512</code></td>
</tr>
<tr>
<td>Cache-aware loop reorder (RIC)</td>
<td><code>89</code></td>
</tr>
<tr>
<td>Loop reorder (RIC) + L1 tiling on I</td>
<td><code>70</code></td>
</tr>
<tr>
<td>Loop reorder (RIC) + L1 tiling on I + multithreading on R&amp;C</td>
<td><code>16</code></td>
</tr>
<tr>
<td>Numpy (MKL)</td>
<td><strong><code>8</code></strong></td>
</tr>
</tbody>
</table>
<h3 id="naive-implementation">Naive implementation</h3>
<p>Let’s start with a basic nested for-loop:</p>
<div><div><pre><code><span>template</span> <span>&lt;</span><span>int</span> <span>rows</span><span>,</span> <span>int</span> <span>columns</span><span>,</span> <span>int</span> <span>inners</span><span>&gt;</span>
<span>inline</span> <span>void</span> <span>matmulImplNaive</span><span>(</span><span>const</span> <span>float</span> <span>*</span><span>left</span><span>,</span> <span>const</span> <span>float</span> <span>*</span><span>right</span><span>,</span>
                            <span>float</span> <span>*</span><span>result</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>row</span> <span>=</span> <span>0</span><span>;</span> <span>row</span> <span>&lt;</span> <span>rows</span><span>;</span> <span>row</span><span>++</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>col</span> <span>=</span> <span>0</span><span>;</span> <span>col</span> <span>&lt;</span> <span>columns</span><span>;</span> <span>col</span><span>++</span><span>)</span> <span>{</span>
      <span>for</span> <span>(</span><span>int</span> <span>inner</span> <span>=</span> <span>0</span><span>;</span> <span>inner</span> <span>&lt;</span> <span>inners</span><span>;</span> <span>inner</span><span>++</span><span>)</span> <span>{</span>
        <span>result</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>]</span> <span>+=</span>
            <span>left</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>inner</span><span>]</span> <span>*</span> <span>right</span><span>[</span><span>inner</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>];</span>
<span>}</span> <span>}</span> <span>}</span> <span>}</span>
</code></pre></div></div>
<p>We hard-code the matrix dimensions, by templating them.
This makes it easier for the compiler to optimize, but makes the comparison against BLAS unfair since BLAS kernels have to work for all matrix sizes.
In practice, a good BLAS library will have multiple implementations of matrix multiplication implemented for different size ranges.
At runtime, based on the matrix dimensions, it’ll decide on which one to use.
However, fixed matrix dimensions commonly appear in practice, for example when JIT-compiling a neural network where the batch size is known.
Plus, the MKL implementation is so close to the theoretical maximum that it’ll serve as a good target.</p>
<p>Compiled with clang and default flags, this takes 4.4s.
The first, easy fix is to adjust the compiler flags.
First, we enable optimizations via <code>-O3</code>.
Then we tell the compiler to generate code that is specific to this microarchitecture (in our case Haswell) via <code>-march=native</code>.
This allows the compiler to output code that may not run on CPUs of a different microarchitecture, making it non-portable.
Lastly, we use <code>-ffast-math</code> allowing the compiler to do associative float math and promising it that there’ll be no NaNs / Infs in our program.<label for="12"></label><span>Which may or may not be reasonable! See this <a href="https://simonbyrne.github.io/notes/fastmath/">post</a> about the dangers of this innocent-sounding <code>fastmath</code> flag.</span>
Combined these flags bring down runtime to 1.6s.</p>
<p>Another straightforward improvement is to perform the inner dot-product in a register, and only write out the result once the dot-product is finished.</p>
<div><div><pre><code><span>template</span> <span>&lt;</span><span>int</span> <span>rows</span><span>,</span> <span>int</span> <span>columns</span><span>,</span> <span>int</span> <span>inners</span><span>&gt;</span>
<span>inline</span> <span>void</span> <span>matmulImplNaiveRegisterAcc</span><span>(</span><span>const</span> <span>float</span> <span>*</span><span>left</span><span>,</span> <span>const</span> <span>float</span> <span>*</span><span>right</span><span>,</span>
                                       <span>float</span> <span>*</span><span>result</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>row</span> <span>=</span> <span>0</span><span>;</span> <span>row</span> <span>&lt;</span> <span>rows</span><span>;</span> <span>row</span><span>++</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>col</span> <span>=</span> <span>0</span><span>;</span> <span>col</span> <span>&lt;</span> <span>columns</span><span>;</span> <span>col</span><span>++</span><span>)</span> <span>{</span>
      <span>float</span> <span>acc</span> <span>=</span> <span>0.0</span><span>;</span>
      <span>for</span> <span>(</span><span>int</span> <span>inner</span> <span>=</span> <span>0</span><span>;</span> <span>inner</span> <span>&lt;</span> <span>inners</span><span>;</span> <span>inner</span><span>++</span><span>)</span> <span>{</span>
        <span>acc</span> <span>+=</span> <span>left</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>inner</span><span>]</span> <span>*</span> <span>right</span><span>[</span><span>inner</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>];</span>
      <span>}</span>
      <span>result</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>]</span> <span>=</span> <span>acc</span><span>;</span>
<span>}</span> <span>}</span> <span>}</span>
</code></pre></div></div>
<p>This is a slight improvement, down to 1.5s.
I tried figuring out if it was legal for the compiler to optimize away the inner store, but couldn’t get a definite answer.
After inlining, the compiler could figure out where the pointers come from, and since they aren’t marked as volatile, get rid of some stores.
In any case, it’s better to write the register accumulation ourselves by hand, instead of relying on the compiler.</p>
<h3 id="cache-aware-implementation">Cache-aware implementation</h3>
<p>Multidimensional matrices are represented in memory using a strided representation.<label for="13"></label><span>For a more detailed explanation, see this <a href="https://ajcr.net/stride-guide-part-1/">blogpost</a>.</span>
In most programming languages you expect the matrix to be row-continuous, meaning that iterating through a single row by incrementing the column results in sequential memory access.</p>
<p><img src="https://siboehm.com/assets/img/MMM/stride_matrix_representation.png" alt="Strided matrices"/></p>
<p>This makes it clear why the inner, most important loop of our matrix multiplication is very cache unfriendly.
Normally, the processor loads data from memory using fixed-size cache lines, commonly 64 Byte large.
When iterating over the row of A, we incur a cache miss on the first entry.
The cache-line fetch by the processor will hold within it the next 15 floats as well, which is a good use of cache.</p>
<p>However, for matrix B, we walk down the rows, occurring a cache-miss at every step.
At 1024 rows * 64 Byte cache lines, that means we’ve loaded a total of 64KB from memory once we reach the bottom row.
On my specific CPU, the L1d cache is 32KB large, meaning later cache lines will kick earlier cache lines from the cache.
Once we’ve reached the final row, the first rows of A &amp; B are gone from the cache, and computing the next dot-product will start fetching all over again.</p>
<p><img src="https://siboehm.com/assets/img/MMM/cache-unaware-dot-product.png" alt="cache-unaware dot product"/></p>
<p>To fix this, we reorder the two inner-most loops:</p>
<div><div><pre><code><span>template</span> <span>&lt;</span><span>int</span> <span>rows</span><span>,</span> <span>int</span> <span>columns</span><span>,</span> <span>int</span> <span>inners</span><span>&gt;</span>
<span>inline</span> <span>void</span> <span>matmulImplLoopOrder</span><span>(</span><span>const</span> <span>float</span> <span>*</span><span>left</span><span>,</span> <span>const</span> <span>float</span> <span>*</span><span>right</span><span>,</span>
                                <span>float</span> <span>*</span><span>result</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>row</span> <span>=</span> <span>0</span><span>;</span> <span>row</span> <span>&lt;</span> <span>rows</span><span>;</span> <span>row</span><span>++</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>inner</span> <span>=</span> <span>0</span><span>;</span> <span>inner</span> <span>&lt;</span> <span>inners</span><span>;</span> <span>inner</span><span>++</span><span>)</span> <span>{</span>
      <span>for</span> <span>(</span><span>int</span> <span>col</span> <span>=</span> <span>0</span><span>;</span> <span>col</span> <span>&lt;</span> <span>columns</span><span>;</span> <span>col</span><span>++</span><span>)</span> <span>{</span>
        <span>result</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>]</span> <span>+=</span>
            <span>left</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>inner</span><span>]</span> <span>*</span> <span>right</span><span>[</span><span>inner</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>];</span>
<span>}</span> <span>}</span> <span>}</span> <span>}</span>
</code></pre></div></div>
<p>The improvement is quite spectacular, bringing runtime down to 89ms.
A 16x improvement!
Our inner loops now iterate through B &amp; C in a memory sequential manner.
The only time we do a large jump in memory access is when our middle loop finishes and we need to fetch the first row of B again.
Since we’re now only computing a partial result in the inner loop, we cannot perform the accumulation in a single register anymore.</p>
<p><img src="https://siboehm.com/assets/img/MMM/cache-aware-dot-prod-reorder-loops.png" alt="cache-aware dot product"/></p>
<p>Looking at the compiled output on <a href="https://godbolt.org/#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAZgQDbYB2AhgLbYgDkAjF%2BTXRMiAZVQtGIHgBYBQogFUAztgAKAD24AGfgCsp5eiyahUAUgBMAIUtXyKxqiIEh1ZpgDC6egFc2TA3cAGQImbAA5PwAjbFIDAAd0JWIXJi9ffwSklKEQsMi2GLiee2xHZyERIhZSInS/AJKHbCdUqpqiPIjo2IMlatr6zKaBztDuwt6eAEp7dB9SVE4uADd0AkwAajYWIgBZH3oASTZ4%2Blp6dF2AKk3SAmAkInJNmkubzcYaZ9f3olvSNglIciNMzAB2GxaACCm1eZE2EFCRDu6AA7pszABmAAimy02KsqIx2I8mx4Wgs0kJxNstmmmMhZhhcLhglIiORm1CYQ52LxBKxRJ5sUxWLJFKpNJFpDp1gZEKhsNZKvZnOEmwwGSYYoFNK1DTFEsp1KFmu8DTlVgVTJZKvtcMBwPoRDMAFYrKR0ZtbpLpJjrObte68XTcczlQ77V9XR6vRjfSaA8KmLyQz67g8nu6rDKM37k0HLW6cYSIw6IaW7XDK%2BXGVXoZWuLN6Nw3fwAlwdOR0NwPFbNkp5otsAGsXxyERtM3ZgBrEBurSGbjSfhsBdLzvd3tcfhKEBLqdd5vkOCwFDYdQtHwkChUCA1YBKVTGMoiJDozsTjCnBi7VIvmE9Dvp%2B07kD%2B8QML0wA8BYJQQVBpAAPI3iBaJbvwl4tNCpBPtwmFXqgVSEJ2/CCMIYgSJwMhyMIyhqJox7kPoJRGCYaBWoYBBRPukCzOg8QVEw%2B67nMCxLH0RAkYBb4fuh3ATkQgLLBOaKkCw8QKSerZcO25AYT23C4IRN4IuoAAcABsAC0ln%2BsAqCoOSFgAHQ8Ii/bWLYLz4MQCKWOO0z8EeOjTLMSDYCwOBxBALYrmuG76WBO57gek7TmF5Dzouy5cFiHbJfh6XHrMZ7IGg6C/owd7UAh1VoGxMHSFoS50C6sT7hAURgVEoQ1AAnlp4GVRwwhIUw9CDUxOA7CYkjTQQgKtCsQJgVhqA3ip/DImUYH0Nx6mkP1Xg4GBSkEOuvAnm8LBPgAagQ2Bokh8TMEN5GiOIkg0R99EaGBLGGMYpicftPHwPxgmpCJ1lIVimzWTQNAsP01k7EQSAIzsixIPySAo2iZT0Jse5lC0QluEwngWsM5DBOMBRFFkyRCUMjTkIkLOpF0jNTKU5RtKMbN9GTrSVKMPM9MU9hCzT7P9B0kuTMUsxDuJ1GTspQ1osYRAvUQRxMIIWlxbpBVMTuFk2XZmqNeS0guVojseZxmy%2BbeY4lJsXhVaKAUzMFGXhZF0XUHOiU6auSUW0V%2B6HkHuUWOb25FSFM7kCtpDJK40hAA%3D%3D">compiler explorer</a>, the loop reordering also enabled vectorization.<label for="14"></label><span>With the naive loop order the compiler was already using the <code>VFMADD</code> instruction, but only on a single fp32 at a time.</span>
The relevant parts of the assembly look like this:</p>
<div><div><pre><code><span>; In the loop setup, load a single fp32 from the current A row</span>
<span>; and broadcast it to all 8 entries of the ymm0 register</span>
<span>; vbroadcastss ymm0, dword ptr [rsi + 4*r8]</span>

<span>; In each instruction, load 8 entries from </span>
<span>; the current row of B into a ymm register</span>
<span>vmovups</span> <span>ymm1</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rbx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>96</span><span>]</span>
<span>vmovups</span> <span>ymm2</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rbx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>64</span><span>]</span>
<span>vmovups</span> <span>ymm3</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rbx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>32</span><span>]</span>
<span>vmovups</span> <span>ymm4</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rbx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span><span>]</span>
<span>; In each instruction, multipy the current entry of A (ymm0) times </span>
<span>; the entries of C (ymm1-4) and add partial results from C (memory load) </span>
<span>vfmadd213ps</span> <span>ymm1</span><span>,</span> <span>ymm0</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>96</span><span>]</span> <span>; ymm1 = (ymm0 * ymm1) + mem</span>
<span>vfmadd213ps</span> <span>ymm2</span><span>,</span> <span>ymm0</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>64</span><span>]</span> <span>; ymm2 = (ymm0 * ymm2) + mem</span>
<span>vfmadd213ps</span> <span>ymm3</span><span>,</span> <span>ymm0</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>32</span><span>]</span> <span>; ymm3 = (ymm0 * ymm3) + mem</span>
<span>vfmadd213ps</span> <span>ymm4</span><span>,</span> <span>ymm0</span><span>,</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span><span>]</span> <span>; ymm4 = (ymm0 * ymm4) + mem</span>
<span>; Store the partial results back to C&#39;s memory</span>
<span>vmovups</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>96</span><span>],</span> <span>ymm1</span>
<span>vmovups</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>64</span><span>],</span> <span>ymm2</span>
<span>vmovups</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span> <span>-</span> <span>32</span><span>],</span> <span>ymm3</span>
<span>vmovups</span> <span>ymmword</span> <span>ptr</span> <span>[</span><span>rcx</span> <span>+</span> <span>4</span><span>*</span><span>rbp</span><span>],</span> <span>ymm4</span>
</code></pre></div></div>
<h3 id="tiling">Tiling</h3>
<p>We just saw how reordering our loops made the caches happy and brought a lot of performance.
Next, we’ll cover a technique called tiling, sometimes also called cache blocking.</p>
<p>To motivate this: Assume this toy example of multiplying two 6x6 matrices and an L1d cache that fits 36 floats, is <a href="https://en.wikipedia.org/wiki/Cache_placement_policies">fully associative</a><label for="15"></label><span>This means every cache line from main memory can be placed at any location into the cache.</span> and has an omniscient<label for="16"></label><span>Though a FIFO replacement policy should suffice in our case.</span> cache replacement policy.
When we reach the end of our middle for-loop (over the I-dimension), our cache is full and the first two rows of B have already been evicted.
This means that upon starting the 2nd iteration of the outer for-loop and accessing B[I=0, C=0] we incur a cache miss.
Similarly, due to the FIFO nature of our hypothetical L1d cache, none of the rows of B we access during our middle for-loop will be cached once we need them.</p>
<p><img src="https://siboehm.com/assets/img/MMM/Basic_tiling_inner.png" alt="Cache Tiling"/></p>
<p>To solve this issue, we tile on the middle for-loop by introducing an additional outer loop with 2 iterations.
By splitting the middle-loop in 2 parts, we ensure that we get no more cache misses in the middle loop, in all but the
first iteration of the outer loop for each tile.</p>
<div><div><pre><code><span>def</span> <span>matmulImplTiling</span><span>(</span><span>left</span><span>,</span> <span>right</span><span>,</span> <span>result</span><span>)</span> <span>{</span>
  <span># iteration 1
</span>  <span>for</span> <span>row</span> <span>in</span> <span>range</span><span>(</span><span>6</span><span>):</span>
    <span>for</span> <span>inner</span> <span>in</span> <span>range</span><span>(</span><span>3</span><span>):</span>
      <span>for</span> <span>column</span> <span>in</span> <span>range</span><span>(</span><span>6</span><span>):</span>
        <span>result</span><span>[</span><span>row</span><span>,</span> <span>column</span><span>]</span> <span>+=</span> <span>left</span><span>[</span><span>row</span><span>,</span> <span>inner</span><span>]</span> <span>*</span> <span>right</span><span>[</span><span>inner</span><span>,</span> <span>column</span><span>]</span>

  <span># iteration 2
</span>  <span>for</span> <span>row</span> <span>in</span> <span>range</span><span>(</span><span>6</span><span>):</span>
    <span>for</span> <span>inner</span> <span>in</span> <span>range</span><span>(</span><span>3</span><span>,</span> <span>6</span><span>):</span>
      <span>for</span> <span>column</span> <span>in</span> <span>range</span><span>(</span><span>6</span><span>):</span>
        <span>result</span><span>[</span><span>row</span><span>,</span> <span>column</span><span>]</span> <span>+=</span> <span>left</span><span>[</span><span>row</span><span>,</span> <span>inner</span><span>]</span> <span>*</span> <span>right</span><span>[</span><span>inner</span><span>,</span> <span>column</span><span>]</span>
</code></pre></div></div>
<p>A visualization might help make this clear.
The block boxes are the tiles that we iterate over, the colored arrows denote our for-loop iterations.
Displayed are both the first and second iterations of our new outer loop.</p>
<p><img src="https://siboehm.com/assets/img/MMM/Tiling_on_inner.png" alt=""/></p>
<p>In Cpp, and implemented for arbitrary tile sizes, the code looks like this:</p>
<div><div><pre><code><span>template</span> <span>&lt;</span><span>int</span> <span>rows</span><span>,</span> <span>int</span> <span>columns</span><span>,</span> <span>int</span> <span>inners</span><span>,</span> <span>int</span> <span>tileSize</span><span>&gt;</span>
<span>inline</span> <span>void</span> <span>matmulImplTiling</span><span>(</span><span>const</span> <span>float</span> <span>*</span><span>left</span><span>,</span> <span>const</span> <span>float</span> <span>*</span><span>right</span><span>,</span>
                             <span>float</span> <span>*</span><span>result</span><span>)</span> <span>{</span>
  <span>for</span> <span>(</span><span>int</span> <span>innerTile</span> <span>=</span> <span>0</span><span>;</span> <span>innerTile</span> <span>&lt;</span> <span>inners</span><span>;</span> <span>innerTile</span> <span>+=</span> <span>tileSize</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>row</span> <span>=</span> <span>0</span><span>;</span> <span>row</span> <span>&lt;</span> <span>rows</span><span>;</span> <span>row</span><span>++</span><span>)</span> <span>{</span>
      <span>int</span> <span>innerTileEnd</span> <span>=</span> <span>std</span><span>::</span><span>min</span><span>(</span><span>inners</span><span>,</span> <span>innerTile</span> <span>+</span> <span>tileSize</span><span>);</span>
      <span>for</span> <span>(</span><span>int</span> <span>inner</span> <span>=</span> <span>innerTile</span><span>;</span> <span>inner</span> <span>&lt;</span> <span>innerTileEnd</span><span>;</span> <span>inner</span><span>++</span><span>)</span> <span>{</span>
        <span>for</span> <span>(</span><span>int</span> <span>column</span> <span>=</span> <span>0</span><span>;</span> <span>column</span> <span>&lt;</span> <span>columns</span><span>;</span> <span>column</span><span>++</span><span>)</span> <span>{</span>
          <span>result</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>column</span><span>]</span> <span>+=</span>
              <span>left</span><span>[</span><span>row</span> <span>*</span> <span>inners</span> <span>+</span> <span>inner</span><span>]</span> <span>*</span> <span>right</span><span>[</span><span>inner</span> <span>*</span> <span>columns</span> <span>+</span> <span>column</span><span>];</span>
<span>}</span> <span>}</span> <span>}</span> <span>}</span> <span>}</span>
</code></pre></div></div>
<p>In reality, picking the tile size is not that simple, since caches are seldomly fully associative.
Further, operating system context switching (either to other userspace processes, or to interrupt routines) may pollute the cache in ways we cannot predict.
In theory, the hot-set of our middle loop consists of:</p>
<ul>
<li>1024 sliced rows of A: 1024 * <code>TILESIZE</code> * 4B</li>
<li><code>TILESIZE</code> rows of B: <code>TILESIZE</code> * 1024 * 4B</li>
<li>1 row of C: 1024 * 4B</li>
</ul>
<p>At an L1d cache size (per core) of 32KB, the optimal tile size is ~3.5.
I grid searched through all reasonable values, and the optimal tile sizes ended up being significantly bigger.
At a tile size of 16, the runtime went to 70ms.
The optimal tile size will also be influenced by the loop overhead, and by how well the prefetcher can predict our memory accesses.
Searching through many different combinations to find one that fits the microarchitecture well is common practice.<label for="17"></label><span>For example, see the <a href="https://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software">Atlas</a> project.</span></p>
<h3 id="tiling-on-multiple-dimensions">Tiling on multiple dimensions</h3>
<p>Similar to our tiling on the inner dimension, we can also perform tiling on the rows, and eventually on the columns.
There are diminishing returns here for our small-sized matrices, but for larger matrices this makes sense.
Each new dimension that we tile on allows us to make the working set of our inner loops smaller while introducing extra overhead.</p>
<p><img src="https://siboehm.com/assets/img/MMM/full_tiling.png" alt=""/></p>
<h3 id="multithreaded-matrix-multiplication">Multithreaded matrix multiplication</h3>
<p>As the last step, we’ll enable multithreading by using OpenMP.
To pick a good strategy it’s important to consider the dependencies of each entry in the result matrix C.
We want to avoid having to do partial summing between threads, which would either require atomics or locking.<label for="18"></label><span>As a distant aside, the input dependencies show how recent deep learning Transformer models can partition the 2 linear layers that are part of their Transformer block across GPUs (Tensor parallelism) with only a single MPI communication step. <img src="https://siboehm.com/assets/img/MMM/Transformer-linear-layers.png" alt="Transformer linear layer partitioning"/> The first layer is column-partitioned on the weight matrix and produces column-partitioned output. The second layer is row-partitioned on the weight matrix, and its output is MPI.AllReduce’d (sum) into the final result.</span></p>
<p><img src="https://siboehm.com/assets/img/MMM/tiled_MMM_dependecies.png" alt=""/></p>
<p>One way to divide the work among threads without the need to perform communication is by partitioning the rows and columns.<label for="19"></label><span>Partitioning our toy 6x6 MMM between 4 threads: <img src="https://siboehm.com/assets/img/MMM/Thread_partitioning.png" alt="row threads partitioning"/>Each half of A &amp; B needs to be read by two threads, but each thread computes its chunk of the output matrix C independently.</span>
We split both the rows and columns into chunks of 4, giving us a total of 16 pieces of work, which we divide amongst 8 hyperthreads.</p>
<div><div><pre><code><span>template</span> <span>&lt;</span><span>int</span> <span>rows</span><span>,</span> <span>int</span> <span>columns</span><span>,</span> <span>int</span> <span>inners</span><span>,</span>
          <span>int</span> <span>tileSize</span> <span>=</span> <span>ROW_COL_PARALLEL_INNER_TILING_TILE_SIZE</span><span>&gt;</span>
<span>inline</span> <span>void</span> <span>matmulImplRowColParallelInnerTiling</span><span>(</span><span>const</span> <span>float</span> <span>*</span><span>left</span><span>,</span>
                                                <span>const</span> <span>float</span> <span>*</span><span>right</span><span>,</span>
                                                <span>float</span> <span>*</span><span>result</span><span>)</span> <span>{</span>
<span>#pragma omp parallel for shared(result, left, right) default(none) \
  collapse(2) num_threads(8)
</span>  <span>for</span> <span>(</span><span>int</span> <span>rowTile</span> <span>=</span> <span>0</span><span>;</span> <span>rowTile</span> <span>&lt;</span> <span>rows</span><span>;</span> <span>rowTile</span> <span>+=</span> <span>256</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>columnTile</span> <span>=</span> <span>0</span><span>;</span> <span>columnTile</span> <span>&lt;</span> <span>columns</span><span>;</span> <span>columnTile</span> <span>+=</span> <span>256</span><span>)</span> <span>{</span>
      <span>for</span> <span>(</span><span>int</span> <span>innerTile</span> <span>=</span> <span>0</span><span>;</span> <span>innerTile</span> <span>&lt;</span> <span>inners</span><span>;</span> <span>innerTile</span> <span>+=</span> <span>tileSize</span><span>)</span> <span>{</span>
        <span>for</span> <span>(</span><span>int</span> <span>row</span> <span>=</span> <span>rowTile</span><span>;</span> <span>row</span> <span>&lt;</span> <span>rowTile</span> <span>+</span> <span>256</span><span>;</span> <span>row</span><span>++</span><span>)</span> <span>{</span>
          <span>int</span> <span>innerTileEnd</span> <span>=</span> <span>std</span><span>::</span><span>min</span><span>(</span><span>inners</span><span>,</span> <span>innerTile</span> <span>+</span> <span>tileSize</span><span>);</span>
          <span>for</span> <span>(</span><span>int</span> <span>inner</span> <span>=</span> <span>innerTile</span><span>;</span> <span>inner</span> <span>&lt;</span> <span>innerTileEnd</span><span>;</span> <span>inner</span><span>++</span><span>)</span> <span>{</span>
            <span>for</span> <span>(</span><span>int</span> <span>col</span> <span>=</span> <span>columnTile</span><span>;</span> <span>col</span> <span>&lt;</span> <span>columnTile</span> <span>+</span> <span>256</span><span>;</span> <span>col</span><span>++</span><span>)</span> <span>{</span>
              <span>result</span><span>[</span><span>row</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>]</span> <span>+=</span>
                  <span>left</span><span>[</span><span>row</span> <span>*</span> <span>inners</span> <span>+</span> <span>inner</span><span>]</span> <span>*</span> <span>right</span><span>[</span><span>inner</span> <span>*</span> <span>columns</span> <span>+</span> <span>col</span><span>];</span>
<span>}</span> <span>}</span> <span>}</span> <span>}</span> <span>}</span> <span>}</span> <span>}</span>
</code></pre></div></div>
<p>The runtime of the final implementation is around 16ms.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Optimizing matrix multiplication is a fun exercise.
It touches upon loop reordering, cache-aware programming and proper work distribution during multithreading.
A BLAS implementation will probably also implement tiling for registers, and multi-dimensional tiling for all caches of the L1-L2-L3 hierarchy, among a few other optimizations that we didn’t cover here.</p>
<p>While writing this code it became apparent to me how easy it is to get lost while optimizing even a simple algorithm like matrix multiplication.
You really need to have a strong mental model of the workings of your CPU, and a well-oiled benchmarking &amp; testing setup to be able to iterate quickly.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>There are other algorithm’s for matrix multiplication that have an asymptotic runtime that’s faster than O(n^3), like the <a href="https://en.wikipedia.org/wiki/Strassen_algorithm">Coppersmith - Winograd algorithm</a> that runs in O(n^2.3755). However, these algorithms have large constant factors, making them slower for all commonly encountered matrix sizes. To my knowledge, no BLAS library uses them.</li>
<li>Just for comparison, a 2021 MacBook Pro with an M1 Pro chip runs the same Numpy code in 1ms, if you use Apple’s <code>accelerate</code> BLAS implementation. The M1 chips have <a href="https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f">undocumented matrix-matrix assembly instructions</a> that only Apple can compile for, which is where this speedup comes from (with OpenBLAS, the MBP takes ~8ms). Similar <a href="https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions">AMX</a> instructions (but documented) are included in Intel’s Sapphire Rapids microarchitecture.</li>
<li>I was led down this rabbit hole of optimization when I stumbled over <a href="https://marek.ai/matrix-multiplication-on-cpu.html">marek.ai/matrix-multiplication-on-cpu</a> a few weeks ago.</li>
<li><a href="https://github.com/flame/how-to-optimize-gemm">github.com/flame/how-to-optimize-gemm</a> goes much further and explains how to actually reach BLAS-like performance from scratch.</li>
</ul>
<hr/>
<p><img src="https://siboehm.com/assets/img/MMM/MMM_ad.jpeg" alt=""/></p>
</article></div>
  </body>
</html>
