<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://allthingsopen.org/articles/noisy-zfs-disks">Original</a>
    <h1>Why are my ZFS disks so noisy?</h1>
    
    <div id="readability-page-1" class="page"><div>
            
    
    
            
            <div>
                                        <div>
                        <p>September 26, 2024</p>
                                                    <p>15 min read</p>                    
                                            </div>
                    
                    <h2>Use these troubleshooting techniques to optimize the pool topology and block size of a ZFS disk drive.</h2>                    
                    <!-- image -->
                                        
                                                                            <figure>
                                <img src="https://allthingsopen.org/wp-content/uploads/2024/09/johnnys-cadillac-luke-fater-e1726521510430.jpg" alt="Black Cadillac car parked on grass"/>
                                                            <figcaption>&#34;One piece at a time&#34; is a 1976 Johnny Cash song about an autoworker who builds his very own Cadillac out of 25 years&#39; worth of individually purloined parts.

Image credit: <a href="https://www.atlasobscura.com/places/johnny-cash-s-one-piece-at-a-time-cadillac " target="_blank" rel="noopener">Luke Fater</a></figcaption>
                                                        </figure>
                                            
                    
<p>Earlier this year, a user at <a href="https://discourse.practicalzfs.com/" target="_blank" rel="noreferrer noopener">Practical ZFS</a> asked a deceptively simple question:</p>



<p><em>I have a Proxmox pool of three four-wide RAIDz1 vdevs. Whenever my VM is running, all twelve drives chatter audibly at least once per second, all day long. How come, and can I fix that?</em></p>



<p>There is, of course, a simple answer to this simple question: the VM in question is probably just trickling a constant stream of writes to disk, and those writes are getting sync’d once per second to the underlying metal.</p>



<p>Unfortunately, this simple answer doesn’t get to the heart of the question, and doesn’t offer any potential mitigations or solutions either. In order to <em>completely</em> answer the question–and offer some potential fixes–we have to dive down into some pretty deep fundamentals of how ZFS works, and why.</p>



<p>Answering this “simple” question required so many digressions, it felt a bit like asking the user to build <a href="https://www.atlasobscura.com/places/johnny-cash-s-one-piece-at-a-time-cadillac" target="_blank" rel="noreferrer noopener">Johnny Cash’s Cadillac</a> just so they could learn how to wash it!</p>



<h2>Awkward vdev width</h2>



<p>One of the first things that leaps to mind when looking at this user’s problem is their topology: They have twelve total drives, arranged in three RAIDz1 vdevs of four drives apiece. This isn’t an ideal width.</p>



<p>In order to understand why, we need to talk about how RAIDz works. In a nutshell, each <em>block</em>–the fundamental unit of ZFS storage data, whose size is defined by the <em>recordsize</em> or <em>volblocksize</em> properties as appropriate–is split into <em>n-p</em> pieces, where <em>n</em> is the total number of drives in the RAIDz vdev and <em>p </em>is the parity level.</p>



<p>Since block size must be an even power of 2, this means that <em>n-p</em> must also be an even power of 2, if you want the pieces of each block to divide up evenly amongst the drives in the vdev. Let’s see how that works out in practice:</p>



<ul>
<li>A 128KiB block is stored on a three-wide RAIDz1 vdev. 128KiB / (3-1) == 64KiB, so each drive gets 16 4KiB sectors written to it. The 128KiB block therefore occupies precisely 128KiB on disk (before parity), making this is an ideal width.<br/></li>



<li>A 128KiB block is stored on a four-wide RAIDz1 vdev. We can’t divide 128KiB evenly among three drives, so what we do instead is write the data in full stripes, followed by a partial-width stripe. This works out to ten stripes of three data sectors and one parity sector, followed by a narrow stripe with the remaining two sectors, and one parity sector for them. This adds up to an aggregate storage efficiency of 32 data sectors / 44 total sectors == 72.7%, not the 75% naively expected…but this is at the default filesystem blocksize of 128KiB, not Proxmox’s default volblocksize of 8KiB or 16KiB. That’s going to make things much worse.<br/></li>



<li>A 128KiB block is stored on an eight-wide RAIDz2 vdev. That means we”’ll need to write thirty-two data sectors–and since 32/6==5.3, we’ll have to write our block in five full-width stripes of six data sectors + two parity sectors, followed by a narrow stripe of two data sectors + two parity sectors. This comes out to the same 32 data sectors / 44 total sectors we saw for a four-wide Z1–again, only 72.7% rather than the 75% we naively expected.</li>
</ul>



<p>Now, none of this is the end of the world–OpenZFS founding developer Matt Ahrens famously <a href="https://www.delphix.com/blog/zfs-raidz-stripe-width-or-how-i-learned-stop-worrying-and-love-raidz" target="_blank" rel="noreferrer noopener">stated</a> that he learned to stop worrying and love RAIDz, and you should too. In particular, pools which mostly store compressible data are minimally affected by other-than-ideal vdev widths, because OpenZFS compression results in off-sized stripes anyway.</p>



<p>To be clear, I don’t disagree with Matt–the impact of off-size vdev widths is usually pretty minimal, and I rarely advise anyone to tear down a working pool just because they used an odd width.</p>



<p>With that said, if you’re building a new pool from scratch, the impact of offsize widths on both performance and efficiency <em>is</em> noticeable, so there’s no reason not to go ahead and build your pools optimally…and in this <em>particular</em> case, we aren’t just worried about performance and storage efficiency, we’re worried about noise.</p>



<p>We’re also working with very small blocksizes, which–as we’re about to see in the next section–amplifies all of these issues significantly.</p>



<h2>Awkward block size</h2>



<p>The next issue is a bit more subtle, and requires some knowledge of a ZFS-powered distribution to spot–the user is running Proxmox, a Debian-based “appliance” distribution which makes it easy to operate virtual machines (VMs) stored on top of OpenZFS.</p>



<p>Proxmox is a very opinionated distribution–it wants you to use zvols (block storage datasets) rather than filesystems (file storage datasets) for your virtual machines, and it defaults to very small <em>volblocksize</em>–8KiB for all but the most recent release, which bumps that up to 16KiB.</p>



<h3>volblocksize=8K is a bad idea</h3>



<p>Remember the math we did earlier, figuring out whether a vdev width was ideal or not? In Proxmox, using the default settings, there generally isn’t any ideal RAIDz vdev width–the blocks are too small to divvy up between them.</p>



<p>For anyone using Proxmox version 7 or earlier, the 8KiB volblocksize parameter means each block only has two sectors of data–and therefore, cannot be split evenly among more than two data drives.</p>



<p>Our noise-beleaguered questioner is running four-wide Z1, which means that each 8KiB block–two sectors of data–can only occupy three total sectors (two data and one parity). That’s only three drives wide–so OpenZFS stores each 8KiB block <em>on</em> only three drives, each of which writes a single sector only.</p>



<p>Since we are only writing to three disks per stripe, we’re only getting the 67% storage efficiency of a three-wide Z1, not the 75% storage efficiency we might naively expect from a four-wide Z1–or even the 72.7% we’d expect from that four-wide Z1 once we take extra stripes into account.</p>



<p>We’re also writing a single sector to each drive, with <em>ruinously</em> bad effect on both performance and drive noise–we’re maximizing the potential for fragmentation, which means maximal drive head seeks, minimal performance, and maximal drive noise.</p>



<h3>volblocksize=16K isn’t much better</h3>



<p>Beginning with Proxmox 8, the default volblocksize for new VMs is 16KiB, not 8KiB. This is still awfully narrow for many common use cases, but it’s at least not quite as punishing as the old 8KiB value.</p>



<p>As we saw in the previous section, we can’t split a block evenly into three pieces–so we’ll need one narrow stripe at the end of every block. Unfortunately, we’re only working with 16KiB at a time, not the 128KiB we looked at earlier–so the impact is more punishing.</p>



<p>Each 16KiB block must be split into four 4KiB sectors. Three of those sectors plus a parity sector go on one full-width stripe, then the remaining sector gets its own parity sector in a second, narrower stripe–leaving us with aggregate storage efficiency of 4 data sectors / 2 parity sectors == 67%.</p>



<p>The good news is, this does mean that on every block read or written, half of our drives get to do 8K I/O instead of 4K I/O. That will improve performance slightly, and <em>might</em> decrease noise a little…but probably not enough to notice, in either case.</p>



<h2>Minimizing noise and maximizing performance</h2>



<p>Now that we’ve built Johnny Cash’s Cadillac by hand, we can finally talk about washing it.</p>



<p>To recap, our user is unhappy with the amount of drive noise their Proxmox server is making. We know that the server has twelve drives, organized into three four-wide RAIDz1 vdevs. This is the user’s first Proxmox server, which means they’re almost certainly using default settings–meaning either volblocksize=8K, or volblocksize=16K.</p>



<p>As we’ve already learned, the combination of zvols, small volblocksize, and off-size RAIDz vdev widths means we’re writing more sectors than we really need to–potentially, a <em>lot</em> more sectors than naive napkin math might suggest.</p>



<p>Unfortunately, none of these poor configuration choices are easy to remedy–RAIDz vdevs cannot currently be reshaped, and volblocksize is immutable once set. So we’re looking at tearing down pretty much <em>everything</em> to hopefully resolve the problem.</p>



<h3>Pool topology improvements</h3>



<p>For pool topology, we can assume that our user was looking for that naive 75% storage efficiency that four-wide Z1 implies (but does not typically deliver). What we don’t know, yet, is how important that promised additional storage actually is.</p>



<p>If our user is really focused on that 75% storage efficiency, they’re pretty much out of luck–you don’t hit that number until you get to ten-wide RAIDz2, which is an ideal width offering a naive SE of 80%.</p>



<p>Our user does have twelve drives, so a ten-wide Z2 is at least possible–but our user would also be dropping from three vdevs to one, and would need a minimum volblocksize of 32KiB (eight 4KiB sectors) just to get full width writes at one sector per drive.</p>



<p>If the user is less focused on total capacity, it probably makes more sense to consider one of these topologies:</p>



<ul>
<li>Six 2-wide mirrors: six vdevs + full volblocksize writes to each disk + double IOPS for reads == maximum performance with minimum noise, at 50% SE and single redundancy</li>



<li>Four 3-wide Z1: four vdevs + half volblocksize written to each disk == very high performance with significantly reduced noise, at 67% SE and single redundancy</li>



<li>Three 4-wide Z2: three vdevs + half volblocksize written to each disk == high performance with significantly reduced noise, at 50% SE and dual redundancy</li>



<li>Two 6-wide Z2: two vdevs + quarter volblocksize written to each disk == moderate performance with possibly reduced noise, at 67% SE (maximum, see next section) and dual redundancy</li>
</ul>



<h3>Volblocksize improvements</h3>



<p>Generally speaking, the larger volblocksize is, the higher maximum throughput <em>may</em> be achieved–at the expense of increasing latency experienced per individual I/O operation. The maximum overall performance is achieved when volblocksize matches or <em>slightly</em> exceeds the most common random I/O operation size in the underlying workload.</p>



<p>This means that for a PostgreSQL DB using 8KiB pages, you want to use a volblocksize of either 8KiB or 16KiB. Although 8KiB is a direct match, 16KiB might be preferable–larger volblocksize offers higher compression potential, and larger I/O per individual disk (meaning higher per-disk performance). For our specific original complaint–noise–we <em>definitely</em> want to lean toward the higher volblocksize, because larger individual operations means fewer seeks, which in turn means less “platter chatter!”</p>



<p>However, most VMs aren’t dedicated PostgreSQL VMs. Even in the world of database engines–the most latency-sensitive applications–MySQL InnoDB defaults to 16KiB pages, and MSSQL generally defaults to 64KiB pages. Meanwhile, VMs used for bulk storage of “Linux ISOs” will mostly be moving entire multi-GiB files!</p>



<p>For a general-purpose VM, I recommend 64KiB volblocksize–this aims at a sweet spot between minimal latency and maximal throughput, with decent potential for compression.</p>



<h3>Putting it all together</h3>



<p>With volblocksize=16KiB on a pool consisting of three 4-wide Z1 vdevs, our user is experiencing a lot of unnecessary inefficiency as well as unwanted noise. Each write goes to only three of the four drives in each vdev, and consists of only a single sector per drive–absolutely maximizing fragmentation, which in turns minimizes performance and maximizes noise, all while screwing up the extra storage efficiency that our user was likely expecting to get.</p>



<p>Although we don’t know precisely what workload our user expects to see in their VM, we can assume a mixture of fileserving and desktop user interface stuff–in other words, “general-purpose use.” For this use case, we typically want each block to be around 64KiB.</p>



<p>If we rebuild the pool with four three-wide Z1 instead of three four-wide Z1, and change volblocksize from 16KiB to 64Kib, we will significantly increase the user’s storage efficiency and performance, as well as decreasing their experienced noise.</p>



<p>Let’s walk through the process of writing 64KiB to our original pool, and the same 64KiB to a redesigned pool using the same twelve drives:</p>



<ul>
<li>Our original pool uses 16KiB blocks and four-wide Z1 vdevs. Each block written is split into four two-sector wide writes (three data and one parity). It therefore requires four total blocks, and <strong>sixteen</strong> individual write operations, to commit 64KiB of data.</li>



<li>Our redesigned pool uses 64KiB blocks and three-wide Z1 vdevs. Each block written is split into three eight-sector wide writes (two data and one parity). It therefore requires only one total block, and <strong>three</strong> individual write operations, to commit the same 64KiB of data.</li>
</ul>



<p>When we compare these two setups, we can see that our user’s original pool requires a whopping <em>five times</em> more individual write operations for every 64KiB committed to disk than our revised setup does.</p>



<p>Obviously, this means our revised setup will enjoy much higher performance than the original–and thanks to the lessons learned in earlier sections, we also know that we’ll get improved on-disk storage efficiency to go along with it.</p>



<p>But more importantly–since the issue that caught our user’s attention in the first place was <em>noise</em>, not performance or capacity–20% of the total write operations issued means 20% of the opportunities to cause a drive’s head to seek, so we can expect our twelve-drive pool to be a <em>lot</em> less “chatty” while it works!</p>



<p>If that’s not good enough, a pool of six two-wide mirrors requires only <em>two</em> sixteen-sector-wide writes to commit the same 64KiB of data. That’s only 12.5% of the individual write operations the original setup needed!</p>



<h2>Visualization</h2>



<p>It may help to see a visual representation of how 64KiB of data will be written to several of the topologies and volblocksizes we’ve configured, from least performant (and noisiest) to most performant (and quietest).</p>



<p>We’ll show all twelve of our drives as they’re ordered in the pool topology, then we’ll show each sequential write operation necessary to commit 64KiB to that pool. Greyed-out areas of the graph don’t represent wasted space–they simply note that that drive was not used for that write operation.</p>



<figure><img fetchpriority="high" decoding="async" width="1024" height="119" src="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-8K-1024x119.png" alt="ZFS 64KiB write RAIDz visualization 4 wide Z1 8K output" srcset="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-8K-1024x119.png 1024w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-8K-300x35.png 300w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-8K-768x89.png 768w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-8K.png 1340w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>Image credits: Jim Salter, CC-BY-SA 4.0</figcaption></figure>



<p>If we’re working with Proxmox 7 or below, the default volblocksize is 8KiB. You simply can’t stretch an 8KiB block all the way across a four-wide RAIDz1 vdev, so we consistently write 3-wide stripes across groups of four drives.</p>



<p>This gives us terrible results for both performance and noise–each of our drives is forced to perform individual 4KiB (single-sector) operations on every vdev write.</p>



<p>We needed to issue 24 individual disk write operations, each only a single sector wide… which leaves us <em>hoping</em> that OpenZFS at least manages to order most of those single-sector operations contiguously both now and in the future.</p>



<figure><img decoding="async" width="1024" height="140" src="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-16K-1024x140.png" alt="ZFS 64KiB write RAIDz visualization 4 wide Z1 16K output" srcset="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-16K-1024x140.png 1024w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-16K-300x41.png 300w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-16K-768x105.png 768w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-4-wideZ1-16K.png 1340w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>Image credits: Jim Salter, CC-BY-SA 4.0</figcaption></figure>



<p>Beginning with Proxmox 8, the default volblocksize is 16KiB. This is generally an improvement, but leaves us with a bit of a mixed bag due to our off-sized vdevs.</p>



<p>In order to write 16KiB blocks, OpenZFS issues each vdev one full stripe write (one sector to each drive) followed by a partial stripe write. The first stripe contains three of the four data sectors we need to write, along with another sector of parity. The second stripe consists of the remaining one data sector we need to write, along with a parity sector for it.</p>



<p>This mish-mash allows your drives to perform 8KiB operations roughly half the time, and 4KiB operations the other half. It is an improvement, but we’re still looking at 18 individual disk write operations to commit our 64KiB of data to disk.</p>



<figure><img decoding="async" width="1024" height="203" src="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-3-wideZ1-1024x203.png" alt="ZFS 64KiB write RAIDz visualization 3 wide Z1 output" srcset="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-3-wideZ1-1024x203.png 1024w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-3-wideZ1-300x60.png 300w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-3-wideZ1-768x152.png 768w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-3-wideZ1.png 1340w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>Image credits: Jim Salter, CC-BY-SA 4.0</figcaption></figure>



<p>This time, we’re looking at ideal-width Z1 vdevs plus 64KiB volblocksize, with dramatic differences. We only need to write a single block, and that single block only lights up three total drives.</p>



<p>We committed the same 64KiB to disk, but gave the drives <em>six times</em> fewer opportunities to seek (remember: seeks both drop performance and irritate us with noise) when we did!</p>



<p>We also used the same total 24 4KiB sectors on-disk as the two four-wide Z1 vdevs did. Take this lesson to heart–just because a vdev is slightly wider doesn’t mean it’s necessarily going to offer greater storage efficiency.</p>



<figure><img decoding="async" width="1024" height="329" src="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-mirrors-1024x329.png" alt="ZFS 64KiB write RAIDz visualization mirrors output" srcset="https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-mirrors-1024x329.png 1024w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-mirrors-300x96.png 300w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-mirrors-768x246.png 768w, https://allthingsopen.org/wp-content/uploads/2024/09/zfs-64KiB-write-RAIDz-visualization-mirrors.png 1340w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>Image credits: Jim Salter, CC-BY-SA 4.0</figcaption></figure>



<p>Finally, we have my favorite topology for relatively small pools–mirror vdevs. We’re still using volblocksize=64K, so we’re still only issuing a single block write to our pool–but now we’re only issuing it to two disks, not three, saving us ⅓ the potential for platter chatter that our 3-wide RAIDz1 offered above.</p>



<p>Our mirror vdevs offer us significantly better performance than even the 3-wide RAIDz1 vdevs: we get half again the vdevs and double the read IOPS per vdev. Fault tolerance also increases <em>slightly</em>–each vdev still only survives a single disk failure, but there are fewer disks to fail per vdev.</p>



<p>If you want greater failure resistance, consider 4-wide or 6-wide RAIDz2 instead.</p>



<h2>An alternate approach</h2>



<p>Although we’ve successfully answered the question “<em>why</em> do my drives make noise?” we haven’t entirely answered the more important question, “and how do I stop hearing them.”</p>



<p>By properly configuring our pool topology and block size, we can drastically minimize the number of seeks issued to our drive heads, which in turn makes them far less “chattery.” But we should also ask the question, “what if we just can’t <em>hear</em> them?”</p>



<p>The amount of noise your mechanical drives make is largely influenced by the chassis you put them in. Ideally, you want a fairly heavy aluminum or steel case with rubber grommets for all the drive screws, and rubber bushings anywhere the drive might be expected to rest.</p>



<p>Dampening the vibrations transmitted to the chassis–and having a chassis with heavy panels, ideally <em>not</em> gamerriffic glass–will do wonders to make any noise your drives make much less obnoxious.</p>
<!-- /wp:paragraph -→
                                                                <!-- Author -->
                                                                        <div>
                        
                                                                                        
                                                        
                            
                            <div>
                                <h2>About the Author</h2>
                                <p>Jim Salter (@jrssnet) is an author, public speaker, mercenary sysadmin, and father of three—not necessarily in that order. </p>
                                <p><a href="https://allthingsopen.org/authors/jim-salter">Read Jim Salter&#39;s Full Bio</a>                                
                            </p></div>
                        </div>
                                                            

                    

                    <p>The opinions expressed on this website are those of each author, not of the author&#39;s employer or All Things Open/We Love Open Source.</p>


        </div>

        

    </div></div>
  </body>
</html>
