<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://entropicthoughts.com/evaluating-llms-playing-text-adventures">Original</a>
    <h1>Evaluating LLMs Playing Text Adventures</h1>
    
    <div id="readability-page-1" class="page"><div>
                <p>
When we first <a href="https://entropicthoughts.com/getting-an-llm-to-play-text-adventures.html">set up the <abbr>llm</abbr> such that it could play text adventures</a>, we noted
that none of the models we tried to use with it were any good at it. We dreamed
of a way to compare them, but all I could think of was setting a goal far into
the game and seeing how long it takes them to get there. I just realised there’s
a better way to do it.
</p>
<section id="outline-container-evaluation-against-achievments">

<div id="text-org07980eb">
<p>
What we’ll do is set a low-ish turn limit and see how much they manage to
accomplish in that time.<span><sup>1</sup> Another alternative for more linear games is
running them multiple times with a turn limit and seeing how often they get past
a particular point within that turn limit.</span>
</p>

<p>
Given how much freedom is offered to players of text adventures, this is a
difficult test. It’s normal even for a skilled human player to immerse
themselves in their surrounding rather than make constant progress. I wouldn’t
be surprised if I got a score of zero if someone plopped me down in front of
this test. But still, maybe it’s the best we can do with limited
resources.<span><sup>2</sup> Another idea is to give them a far-off goal and then somehow have
them request hints when they are stuck, and count how many hints they need to
get there. However, given how little they used hints given in the previous
article, I doubt this would work very well either.</span>
</p>

<p>
What we’ll do is define a set of achievements for a game. These achievements
will be clustered around the first few turns of the game, because we’ll only
give the <abbr>llm</abbr> a few turns to earn them. Here’s an example for 9:05.
</p>

<pre id="org1673e39">TURN_LIMIT          40
ANSWER_PHONE        Click.
EXIT_BED            You get out of bed.
OPEN_DRESSER        revealing some clean
ENTER_BATHROOM      far from luxurious
REMOVE_SOILED       You take off the soiled
REMOVE_WATCH        You take off the watch
ENTER_SHOWER        dawdle
WEAR_CLEAN          You put on the clean
OPEN_FRONT          You open the front
UNLOCK_CAR          Unlocked.
ENTER_CAR           Las Mesas
OPEN_WALLET         open the wallet
CARD_SLOT           green LED lights
</pre>

<p>
It should be fairly clear how this works: the <code>TURN_LIMIT</code> specifies how many
turns the <abbr>llm</abbr> has to collect achievements. Every line other than that
specifies an achievement: the name is on the left, and it counts as earned when
the game prints the text on the right. The <abbr>llm</abbr> knows nothing of these
achievements. It tries to get through the game and in the background we use the
achievements to count how far it gets.
</p>

<p>
It might seem like the turn limit must be calibrated such that a score of 100 %
is possible, but that’s not the case. Many of the games we are going to test
with have branching already at the start, such that the achievements need to
cover multiple branches, and it’s impossible to go through all branches within
the turn limit. What we do need to be careful about is making sure the number of
achievements in each branch is roughly the same, otherwise models that are lucky
and go down an achievement-rich path will get a higher score. Thanks to this,
the score we get out of this test is a relative comparison between models, not
an absolute measure of how well the <abbr>llm</abbr>s play text adventures. We have already
established that they don’t do it very well, and we can’t be more nuanced than
that without paying for a lot of eval tokens.
</p>

<p>
We might consider making some moves not count toward the turn limit, for example
erroneous commands, or examining things – the latter because more powerful
models are more methodical and examine more things, and it seems odd to penalise
them for this. However, in the end, examining things is probably part of what
allows the more powerful models to make further progress (and typing valid
commands is part of being good at text adventures), so we won’t give away any
moves for free.
</p>
</div>
</section>
<section id="outline-container-evaluating-many-popular-models">

<div id="text-orgf9945ca">
<p>
We register for OpenRouter to get convenient access to more models and then let
them whirr away with the Perl script, which is updated to cut the <abbr>llm</abbr> off at
the turn limit. At that point it reports to us how many achievements were
earned. We get the following results, ordered roughly by decreasing performance.
(The result tables in this article are wide; on narrow viewports you may have
to scroll sideways.)
</p>

<div id="orge1228c0">
<table>


<colgroup>
<col/>
</colgroup>

<colgroup>
<col/>

<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Model</th>
<th scope="col">9:05</th>
<th scope="col">Lockout</th>
<th scope="col">Dreamhold</th>
<th scope="col">Lost Pig</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grok 4</td>
<td>86 %</td>
<td>15 %</td>
<td>46 %</td>
<td>33 %</td>
</tr>

<tr>
<td>Claude 4 Sonnet</td>
<td>80 %</td>
<td>30 %</td>
<td>53 %</td>
<td>46 %</td>
</tr>

<tr>
<td>Gemini 2.5 Flash</td>
<td>80 %</td>
<td>30 %</td>
<td>33 %</td>
<td>46 %</td>
</tr>

<tr>
<td>Gemini 2.5 Pro</td>
<td>80 %</td>
<td>30 %</td>
<td>40 %</td>
<td>40 %</td>
</tr>

<tr>
<td>DeepSeek R1 0528</td>
<td>80 %</td>
<td>23 %</td>
<td>33 %</td>
<td>33 %</td>
</tr>

<tr>
<td>Claude 4 Opus</td>
<td>73 %</td>
<td>30 %</td>
<td>60 %</td>
<td>46 %</td>
</tr>

<tr>
<td><abbr>gpt</abbr>-5 Chat</td>
<td>73 %</td>
<td>15 %</td>
<td>53 %</td>
<td>33 %</td>
</tr>

<tr>
<td>DeepSeek V3</td>
<td>66 %</td>
<td>23 %</td>
<td>20 %</td>
<td>33 %</td>
</tr>

<tr>
<td><abbr>gpt</abbr>-4o</td>
<td>53 %</td>
<td>23 %</td>
<td>40 %</td>
<td>40 %</td>
</tr>

<tr>
<td>Qwen3 Coder</td>
<td>53 %</td>
<td>23 %</td>
<td>40 %</td>
<td>33 %</td>
</tr>

<tr>
<td>Kimi K2</td>
<td>53 %</td>
<td>30 %</td>
<td>46 %</td>
<td>40 %</td>
</tr>

<tr>
<td><abbr>glm</abbr> 4.5</td>
<td>53 %</td>
<td>23 %</td>
<td>33 %</td>
<td>53 %</td>
</tr>

<tr>
<td>Claude 3.5 Haiku</td>
<td>38 %</td>
<td>15 %</td>
<td>26 %</td>
<td>26 %</td>
</tr>

<tr>
<td>Llama 3 Maverick</td>
<td>33 %</td>
<td>30 %</td>
<td>40 %</td>
<td>33 %</td>
</tr>

<tr>
<td><abbr>gpt</abbr>-o3-mini</td>
<td>20 %</td>
<td>15 %</td>
<td>26 %</td>
<td>26 %</td>
</tr>

<tr>
<td>Mistral Small 3</td>
<td>20 %</td>
<td>15 %</td>
<td>0 %</td>
<td>20 %</td>
</tr>

<tr>
<td><abbr>gpt</abbr>-4o-mini</td>
<td>13 %</td>
<td>23 %</td>
<td>20 %</td>
<td>40 %</td>
</tr>
</tbody>
</table>

</div>

<p>
Ideally, these should be run multiple times to account for random variation in
performance<span><sup>3</sup> For example, in 9:05, Opus thought it did not carry the wallet
when it did, so it jumped into the car again to go back for it. Clever, but
wasted enough turns to lose to Sonnet thanks to a silly mistake!</span>, but given
that the Opus sessions cost around $4, I’m not going to do that. I was close to
not even running Opus for all four games!
</p>
</div>
</section>
<section id="outline-container-adjusting-model-ranking-for-game-difficulty">

<div id="text-orgbcff1b9">
<p>
Some models appear to perform better in some games than others, so it’s hard to
rank the models. We could take the average of their scores, but that’s unfair
because some of the games are harder than others: a 40 % in <i>Lockout</i> should be
considered more impressive than a 40 % in <i>Dreamhold</i>. What we will do, which
may or may not be valid, is run a linear regression using models and games as
predictors. This gives us coefficients for the games (telling us how difficult
the games are), but also coefficients for the models, and these are the ones we
want, because the coefficients for the models are adjusted for game difficulty.
</p>

<p>
This regression is performed with the baseline being 9:05 played by <abbr>gpt</abbr>-5
Chat. Most of the model coefficients are not statistically significant (because
four games is not enough to figure out statistical significance unless the model
is truly terrible), but they might serve as a first-order estimation for ranking
models.
</p>

<p>
In this table, cost is per million output tokens.<span><sup>4</sup> The design of the script
ensures that output and input are similar in size – O(1) to be specific – so
output is what is going to drive the cost.</span> The table is divided into three
categories: performance better than <abbr>gpt</abbr>-5 Chat, cheaper models with
performance that is nearly there, and models that suck.
</p>

<table>


<colgroup>
<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Model</th>
<th scope="col">Coefficient</th>
<th scope="col">Cost ($/Mt)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 4 Opus</td>
<td>+0.09</td>
<td>75</td>
</tr>

<tr>
<td>Claude 4 Sonnet</td>
<td>+0.09</td>
<td>15</td>
</tr>

<tr>
<td>Gemini 2.5 Pro</td>
<td>+0.04</td>
<td>10</td>
</tr>

<tr>
<td>Gemini 2.5 Flash</td>
<td>+0.04</td>
<td>0.7</td>
</tr>

<tr>
<td>Grok 4</td>
<td>+0.02</td>
<td>15</td>
</tr>
</tbody>
<tbody>
<tr>
<td><abbr>gpt</abbr>-5 Chat (baseline)</td>
<td>0.00</td>
<td>10</td>
</tr>
</tbody>
<tbody>
<tr>
<td>Kimi K2</td>
<td>-0.01</td>
<td>2.5</td>
</tr>

<tr>
<td>DeepSeek R1 0528</td>
<td>-0.01</td>
<td>0.7</td>
</tr>

<tr>
<td><abbr>glm</abbr> 4.5</td>
<td>-0.03</td>
<td>0.8</td>
</tr>

<tr>
<td><abbr>gpt</abbr>-4o</td>
<td>-0.05</td>
<td>0.1</td>
</tr>

<tr>
<td>Qwen3 Coder</td>
<td>-0.06</td>
<td>0.8</td>
</tr>

<tr>
<td>DeepSeek V3</td>
<td>-0.08</td>
<td>0.7</td>
</tr>

<tr>
<td>Llama 3 Maverick</td>
<td>-0.10</td>
<td>0.6</td>
</tr>
</tbody>
<tbody>
<tr>
<td>Claude 3.5 Haiku</td>
<td>-0.17</td>
<td>4</td>
</tr>

<tr>
<td><abbr>gpt</abbr>-4o-mini</td>
<td>-0.20</td>
<td>0.6</td>
</tr>

<tr>
<td><abbr>gpt</abbr>-o3-mini</td>
<td>-0.22</td>
<td>4.4</td>
</tr>

<tr>
<td>Mistral Small 3</td>
<td>-0.30</td>
<td>0.1</td>
</tr>
</tbody>
</table>

<p>
Some comments:
</p>

<ul>
<li>I find it interesting that the top-tier models (Claude Opus, Gemini Pro) don’t
seem to significantly outperform their cheaper siblings (Claude Sonnet, Gemini
Flash) in these tests.<span><sup>5</sup> This might be because we are hand-holding the
models so much in the prompt. More powerful models may be better at directing
themselves.</span></li>

<li>I’m very impressed by Gemini 2.5 Flash. At that cost, it is performing
admirably. It is hard to argue for using models like DeepSeek’s R1 when we
better performance at the same cost from the Google model.</li>

<li>The small models really aren’t good general problem solvers. I think Haiku
costs so much because it is good at language, not reasoning.</li>
</ul>

<p>
It would be super interesting to toss these at more games to work out the finer
differences (e.g. is there <i>really</i> a difference between Gemini Pro and Flash,
or was that just down to sampling error in the small sample of games I had them
play?) but such a comparison gets expensive in part due to the cost of eval
tokens (the above table cost something like $34), but mainly because it would
require me to sit down and create sets of achievements for these games. I have
only played so many z-code games, so I cannot do this for very many games. If
someone wants to support me, please reach out!
</p>


</div>
</section>
<section id="outline-container-testing-the-top-models-on-more-games">

<div id="text-org6a51939">
<p>
I have played three more games, though, so let’s continue the evaluation with
the five top models on these games also. Their performances on the three new
games are
</p>

<div id="orgc99d7d7">
<table>


<colgroup>
<col/>
</colgroup>

<colgroup>
<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Model</th>
<th scope="col">For a Change</th>
<th scope="col">Plundered Hearts</th>
<th scope="col">So Far</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 4 Sonnet</td>
<td>11 %</td>
<td>19 %</td>
<td>28 %</td>
</tr>

<tr>
<td>Gemini 2.5 Pro</td>
<td>16 %</td>
<td>28 %</td>
<td>28 %</td>
</tr>

<tr>
<td>GPT-5 Chat</td>
<td>44 %</td>
<td>33 %</td>
<td>0 %</td>
</tr>

<tr>
<td>Grok 4</td>
<td>22 %</td>
<td>28 %</td>
<td>28 %</td>
</tr>

<tr>
<td>Gemini 2.5 Flash</td>
<td>28 %</td>
<td>33 %</td>
<td>14 %</td>
</tr>
</tbody>
</table>

</div>

<p>
Using the same methodology as before (combining data from both trial run sets),
we arrive at new coefficients for the evaluated models.<span><sup>6</sup> I did also
investigate how Gemini 2.0 Flash compared against Gemini 2.5 Flash, because the
former is significantly cheaper and the latter was surprisingly good.
Unfortunately, Gemini 2.0 Flash was not very good. Its performance relative to
its younger sibling was -15 %pt.</span><span><sup>7</sup> I was also tempted to
compare o3-mini against o3-mini-high to see the effect of the <code>reasoning_effort</code>
parameter but since o3-mini was such a crappy model anyway it was hard to
justify the effort.</span>
</p>

<table>


<colgroup>
<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Model</th>
<th scope="col">Coefficient</th>
<th scope="col">Cost ($/Mt)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 4 Sonnet</td>
<td>+0.02</td>
<td>15</td>
</tr>

<tr>
<td>Gemini 2.5 Pro</td>
<td>+0.02</td>
<td>10</td>
</tr>

<tr>
<td>Gemini 2.5 Flash</td>
<td>+0.02</td>
<td>0.7</td>
</tr>

<tr>
<td>GPT-5 Chat (baseline)</td>
<td>0.00</td>
<td>10</td>
</tr>

<tr>
<td>Grok 4</td>
<td>-0.01</td>
<td>15</td>
</tr>
</tbody>
</table>

<p>
On the one hand, it’s a little odd that the performance of Claude 4 Sonnet
dropped. On the other hand, I calibrated the prompt using Claude 4 Sonnet
against 9:05, so by adding more games we are effectively diluting the training
set within the test set; we probably <i>should</i> expect a performance drop at that
point.
</p>

<p>
Noting the cost column, Gemini 2.5 Flash is a clear winner for running text
adventures. It’s also fast compared to the others.
</p>
</div>
</section>
<section id="outline-container-evaluating-score-variation">

<div id="text-org7613360">
<p>
Given that I’ve already sunk some money into this article series, and a few
additional sessions with Gemini 2.5 Flash cannot hurt that much, let’s splurge
and do that thing we wanted to do in the first place: run the same model against
the same game a few times to figure out the size of the sampling error. All of
the scores in the table below comes from Gemini 2.5 Flash. The first column is
the standard deviation of the remaining columns.
</p>

<div id="org945213a">
<table>


<colgroup>
<col/>
</colgroup>

<colgroup>
<col/>
</colgroup>

<colgroup>
<col/>

<col/>

<col/>

<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Game</th>
<th scope="col">St. dev.</th>
<th scope="col">Run 1</th>
<th scope="col">Run 2</th>
<th scope="col">Run 3</th>
<th scope="col">Run 4</th>
<th scope="col">Run 5</th>
<th scope="col">Run 6</th>
</tr>
</thead>
<tbody>
<tr>
<td>9:05</td>
<td>14 %pt</td>
<td>73 %</td>
<td>86 %</td>
<td>86 %</td>
<td>80 %</td>
<td>53 %</td>
<td>60 %</td>
</tr>

<tr>
<td>Lockout</td>
<td>11 %pt</td>
<td>30 %</td>
<td>46 %</td>
<td>46 %</td>
<td>38 %</td>
<td>23 %</td>
<td>23 %</td>
</tr>

<tr>
<td>Dreamhold</td>
<td>10 %pt</td>
<td>53 %</td>
<td>40 %</td>
<td>46 %</td>
<td>46 %</td>
<td>53 %</td>
<td>26 %</td>
</tr>

<tr>
<td>Lost Pig</td>
<td>3 %pt</td>
<td>46 %</td>
<td>40 %</td>
<td>40 %</td>
<td>40 %</td>
<td>46 %</td>
<td>40 %</td>
</tr>

<tr>
<td>For a Change</td>
<td>6 %pt</td>
<td>16 %</td>
<td>11 %</td>
<td>16 %</td>
<td>5 %</td>
<td>0 %</td>
<td>11 %</td>
</tr>

<tr>
<td>Plundered Hearts</td>
<td>4 %pt</td>
<td>19 %</td>
<td>19 %</td>
<td>19 %</td>
<td>23 %</td>
<td>28 %</td>
<td>28 %</td>
</tr>

<tr>
<td>So Far</td>
<td>32 %pt</td>
<td>14 %</td>
<td>57 %</td>
<td>71 %</td>
<td>71 %</td>
<td>71 %</td>
<td>0 %</td>
</tr>
</tbody>
</table>

</div>

<p>
In case it is not obvious, this is not so much an evalutaion of Gemini 2.5 Flash
as it is a judgment of the quality of the testing protocol. It is clear, for
example, that using <i>So Far</i> to evaluate <abbr>llm</abbr>s is a mistake: the same model has
large variation between runs, and the difference between runs of different
models is not so large. It would be more informative to replace the run of <i>So
Far</i> with another run of one of the other games – maybe <i>Plundered Hearts</i> or
<i>Lost Pig</i>, which start out more linearly.<span><sup>8</sup> <i>For a Change</i> might look like a
good game for evaluation, but I think that’s a mistake. It’s not that the model
makes consistent progress, but that it fails to make almost any progress at all,
thanks to how open the game is right from the gate.</span>
</p>
</div>
</section>
<section id="outline-container-conclusions">

<div id="text-org6d63e1a">
<p>
I’m not sure what conclusions to draw from this article series.
</p>

<ul>
<li><a href="https://entropicthoughts.com/interacting-with-text-adventures-through-perl.html">We can drive z-code text adventures through Perl</a>, which lets us connect it to
an <abbr>llm</abbr> in a controlled way. It turned out more complicated than one would think, but
definitely doable.</li>

<li><abbr>llm</abbr>s are still not great at playing text adventures. <a href="https://entropicthoughts.com/getting-an-llm-to-play-text-adventures.html">Giving them leading
questions to keep them on track helps a lot</a>. Giving them hints helps them
surprisingly little.</li>

<li>The variation in how much they accomplish can be large for some games with
lots of distracting details, such as <i>Lockout</i> and <i>So Far</i>. The games that
are easiest to evaluate with are those with a relatively linear beginning,
such as <i>Lost Pig</i> and <i>Plundered Hearts</i>.</li>

<li>There is one cheap model that is about as good as <abbr>llm</abbr> models get at playing
text adventures: Gemini 2.5 Flash. Many of the other cheap models might have
performance worse than <abbr>gpt</abbr>-5 Chat, and probably also worse than Gemini 2.5
Flash. Claude 4 Sonnet might seem like the best model if costs be damned, but
that is probably because the prompt was calibrated against Claude 4 Sonnet.</li>

<li>Running <abbr>llm</abbr>s in agentic type applications really burns through <abbr>api</abbr> credits
like nothing else. I’d really like to complement this analysis with the “how
many turns does the model need to get to point X” test, but I cannot motivate
spending the money for it.</li>
</ul>
</div>
</section>

            </div></div>
  </body>
</html>
