<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.corsix.org/content/galois-field-instructions-2021-cpus">Original</a>
    <h1>Galois field instructions on 2021 CPUs</h1>
    
    <div id="readability-page-1" class="page"><div>

<div>
<p data-sourcepos="1:1-1:661">CPUs like to operate on data types such as <code>uint8_t</code>, <code>uint16_t</code>, <code>uint32_t</code>, and <code>uint64_t</code>. These data types can be viewed as integers modulo 2<sup>8</sup>, 2<sup>16</sup>, 2<sup>32</sup>, and 2<sup>64</sup> respectively. Addition and multiplication can be defined on these integers modulo 2<sup>N</sup> in a way which is familiar to most people; for example, in <code>uint8_t</code> math, we have <code>199 + 201 == 144</code> and <code>16 * 20 == 64</code>. <code>uint8_t</code> math can be viewed as infinite-precision integer math combined with the reduction rule of <code>256 == 0</code>; for example in infinite-precision integer math we have <code>199 + 201 == 400 == 256 + 144</code> and <code>16 * 20 == 320 == 256 + 64</code>.</p>
<p data-sourcepos="3:1-3:769">Reduction rules of the form <code>2<sup>N</sup> == 0</code> (for <code>N &gt; 1</code>) are CPU-friendly, but not particularly math-friendly. On the other hand, reduction rules of the form <code>P == 0</code> for some prime number <code>P</code> are math-friendly, but not CPU-friendly. With infinite-precision integer math combined with the reduction rule of say <code>251 == 0</code>, we have <code>199 + 201 == 149</code> and <code>16 * 20 == 69</code>. As <code>251</code> is prime, this reduction rule is math-friendly. By that, I mean that for any <code>x</code> other than zero, there is some <code>y</code> such that <code>x * y == 1</code>. For example, taking <code>x</code> of 16, we have <code>16 * 204 == 1</code>. This property is not true for the <code>256 == 0</code> reduction rule; with that rule, there is no <code>y</code> such that <code>16 * y == 1</code>. Where it exists, this <code>y</code> can be called <code>inv(x)</code> or <code>1/x</code>.</p>
<p data-sourcepos="5:1-5:1726">If we want to keep the CPU-friendly <code>uint8_t</code>, <code>uint16_t</code>, <code>uint32_t</code>, and <code>uint64_t</code> data types, and also keep the math-friendly property of having <code>inv(x)</code> exist for all non-zero <code>x</code>, then we are actually in luck, albeit we have to use some exotic definitions of addition and multiplication. The exotic definitions come from <em>polynomial</em> math rather than integer math. A string of bits <code>b<sub>0</sub></code>, <code>b<sub>1</sub></code>, ..., <code>b<sub>n-1</sub></code> can be viewed as the polynomial <code>b<sub>0</sub> + b<sub>1</sub> * x<sup>1</sup> + ... + b<sub>n-1</sub> * x<sup>n-1</sup></code>, at which point addition or multiplication of two strings of bits can be done by converting both strings to polynomials, doing addition or multiplication of those two polynomials, and then converting the resultant polynomial back to a bit string. That final back-conversion step requires that every <code>b<sub>i</sub></code> in the resultant polynomial is either <code>0</code> or <code>1</code>, which we can ensure by applying the reduction rule <code>2 == 0</code> to every <code>b<sub>i</sub></code>. In the case of multiplication, the resultant bit string (or, equivalently, polynomial) is often going to be twice as long as its inputs. This should not come as a surprise, as the same thing happens with infinite-precision integer math. Continuing the trend of non-surprising things, just as reduction rules can be used to bring infinite-precision integer math down to fixed bit lengths, reduction rules can also be used to bring infinite-precision polynomial math down to fixed bit lengths. As yet another non-surprise, some polynomial reduction rules are math-friendly, and others are not. Some math-friendly polynomial reduction rules are:</p>
<ul>
<li><code>x<sup>8</sup> == x<sup>4</sup> + x<sup>3</sup> + x + 1</code></li>
<li><code>x<sup>16</sup> == x<sup>5</sup> + x<sup>3</sup> + x + 1</code></li>
<li><code>x<sup>32</sup> == x<sup>7</sup> + x<sup>3</sup> + x<sup>2</sup> + 1</code></li>
<li><code>x<sup>64</sup> == x<sup>4</sup> + x<sup>3</sup> + x + 1</code></li>
</ul>
<p data-sourcepos="13:1-13:263">(These reduction rules come from <a href="https://www.hpl.hp.com/techreports/98/HPL-98-135.pdf">Table of Low-Weight Binary Irreducible Polynomials</a>, and in some sense are the simplest reduction rules for each <code>x<sup>i</sup></code> on the left hand side of the <code>==</code>.)</p>
<p data-sourcepos="15:1-15:1140">By choosing one of these reduction rules, we can define exotic addition and multiplication for any of <code>uint8_t</code>, <code>uint16_t</code>, <code>uint32_t</code>, and <code>uint64_t</code>. For example, exotic addition or multiplication of two <code>uint8_t</code> values can be done by converting both values to polynomials, doing infinite-precision polynomial addition or multiplication, applying the reduction rule <code>x<sup>8</sup> == x<sup>4</sup> + x<sup>3</sup> + x + 1</code> to the polynomial (to get rid of <code>x<sup>8</sup></code>, <code>x<sup>9</sup></code>, and all other higher terms), and then converting the polynomial back to a string of 8 bits (remembering the <code>2 == 0</code> reduction applied to each <code>b<sub>i</sub></code> in the polynomial). It turns out that this exotic addition doesn&#39;t depend on the choice of polynomial reduction rule, and that the <code>2 == 0</code> reduction means that this exotic addition is actually just bitwise XOR of the original values. Unfortunately, exotic multiplication <em>does</em> depend on the choice of polynomial reduction rule, and for any math-friendly choice, exotic multiplication doesn&#39;t turn out to be the same as an everyday operation.</p>
<p data-sourcepos="17:1-17:688">With one of these polynomial reduction rules in hand, we can define exotic addition and multiplication for each of the <code>uint8_t</code>, <code>uint16_t</code>, <code>uint32_t</code>, and <code>uint64_t</code> data types. Furthermore, assuming that the polynomial reduction rule is math-friendly, this exotic multiplication has an <code>inv(x)</code> which exists for all non-zero <code>x</code>. In other words, we have what mathematicians call a <em>field</em>. As the number of elements (2<sup>8</sup>, 2<sup>16</sup>, 2<sup>32</sup>, and 2<sup>64</sup>, respectively) is finite in every case, they are furthermore called <em>Galois fields</em>. This is shortened to <em>GF</em>, giving GF(2<sup>8</sup>), GF(2<sup>16</sup>), GF(2<sup>32</sup>), and GF(2<sup>64</sup>).</p>
<p data-sourcepos="19:1-19:320">With that long introduction done, the question becomes: how can addition, multiplication, and <code>inv</code> of GF(2<sup>8</sup>) / GF(2<sup>16</sup>) / GF(2<sup>32</sup>) / GF(2<sup>64</sup>) values be done efficiently on contemporary CPUs? Addition is just XOR, so the interesting questions are around multiplication and <code>inv</code>.</p>
<p data-sourcepos="21:1-21:495">For GF(2<sup>8</sup>), <code>inv</code> could be implemented as a <code>uint8_t[256]</code> lookup table. Multiplication could be implemented as a <code>uint8_t[256][256]</code> lookup table. This latter table is starting to get a little large, but there&#39;s a time/space tradeoff possible: analogues to <code>log</code> and <code>exp</code> exist in GF(2<sup>N</sup>), and both of <code>log</code> and <code>exp</code> can be implemented as <code>uint8_t[256]</code> lookup tables. Multiplication of two values thus becomes two <code>log</code> lookups, an integer addition, and an <code>exp</code> lookup.</p>
<p data-sourcepos="23:1-23:142">For GF(2<sup>16</sup>), the same broad strokes apply, with each of <code>inv</code> and <code>log</code> and <code>exp</code> implementable as <code>uint16_t[65536]</code> lookup tables.</p>
<p data-sourcepos="25:1-25:374">For GF(2<sup>32</sup>) and GF(2<sup>64</sup>), the lookup tables would become too big, so alternative approaches are required. Recall that multiplication consists of two major substeps: infinite-precision polynomial multiplication, and then applying a reduction rule. For the infinite-precision polynomial multiplication part, some uncommon CPU instructions come to our aid:</p>
<ul data-sourcepos="27:3-31:0">
<li data-sourcepos="27:3-27:266">On x86 / amd64, <code>pclmulqdq</code> takes two 64-bit polynomials and returns the 128-bit polynomial product. As a fun extra twist, the inputs are actually 128-bit registers, and an immediate operand can be used to select between the low/high 64-bit halves of each input.</li>
<li data-sourcepos="28:3-28:229">On ARMv8-A / AArch64, <code>pmul</code> takes two 8-bit polynomials, computes the 16-bit polynomial product, and then returns the low 8 terms of that product. As a fun extra twist, this is done in SIMD fashion, either 8-wide or 16-wide.</li>
<li data-sourcepos="29:3-29:464">On ARMv8-A / AArch64, <code>pmull</code> takes two 8-bit polynomials or two 64-bit polynomials and returns the 16-bit or 128-bit polynomial product. The variant with a 16-bit result is done as 8-side SIMD. The variant with a 128-bit result is equivalent to <code>pclmulqdq</code> (selecting the low halves of each input). It is notable that <code>pmull</code> followed by <code>eor</code> is one of the few <a href="https://dougallj.github.io/applecpu/firestorm.html">fused instruction patterns on the Apple M1</a>.</li>
<li data-sourcepos="30:3-31:0">On ARMv8-A / AArch64, <code>pmull2</code> is like <code>pmull</code>, but selecting the high halves of each input. The variant with a 128-bit result is equivalent to <code>pclmulqdq</code> (selecting the high halves of each input).</li>
</ul>
<p data-sourcepos="32:1-32:81">The <code>pclmulqdq</code> instruction is available as the <code>_mm_clmulepi64_si128</code> intrinsic:</p>
<pre><code data-sourcepos="33:1-41:1">#include &lt;stdint.h&gt;
#include &lt;wmmintrin.h&gt;

__m128i poly_mul(uint64_t a, uint64_t b) {
    return _mm_clmulepi64_si128(_mm_cvtsi64_si128(a),
                                _mm_cvtsi64_si128(b), 0);
}
</code></pre>
<p data-sourcepos="43:1-43:468">To complete the GF(2<sup>64</sup>) multiplication, this infinite-precision polynomial multiplication needs to be followed by reduction back down to 64 bits using a suitable reduction rule. With the <code>x<sup>64</sup> == x<sup>4</sup> + x<sup>3</sup> + x + 1</code> rule, the right hand side of the rule can be represented as the bit string <code>(1 &lt;&lt; 4) + (1 &lt;&lt; 3) + (1 &lt;&lt; 1) + (1 &lt;&lt; 0)</code>, or <code>0x1b</code> for short. This rule can be applied in full by using <code>pclmulqdq</code> twice:</p>
<pre><code data-sourcepos="44:1-52:1">uint64_t poly_reduce(__m128i x, uint64_t r = 0x1b) {
    __m128i xr = _mm_cvtsi64_si128(r);
    __m128i x2 = _mm_clmulepi64_si128(x, xr, 1);
    x ^= x2;
    x ^= _mm_clmulepi64_si128(x2, xr, 1);
    return _mm_cvtsi128_si64(x);
}
</code></pre>
<p data-sourcepos="54:1-54:68">The two pieces combine to give multiplication in GF(2<sup>64</sup>):</p>
<pre><code data-sourcepos="55:1-59:1">uint64_t gf_mul(uint64_t a, uint64_t b, uint64_t r = 0x1b) {
    return poly_reduce(poly_mul(a, b), r);
}
</code></pre>
<p data-sourcepos="61:1-61:155">If performing a dot product rather than just one multiplication, then the reduction step can be performed just once rather than after every multiplication:</p>
<pre><code data-sourcepos="62:1-71:1">uint64_t gf_dot(const uint64_t* as, const uint64_t* bs,
                uint32_t n, uint64_t r = 0x1b) {
    __m128i x = _mm_setzero_si128();
    for (uint32_t i = 0; i &lt; n; ++i) {
        x ^= poly_mul(as[i], bs[i]);
    }
    return poly_reduce(x, r);
}
</code></pre>
<p data-sourcepos="73:1-73:111">Thanks to some neat mathematical properties, the <code>inv</code> operation can be implemented in terms of multiplication:</p>
<pre><code data-sourcepos="74:1-83:1">uint64_t gf_inv(uint64_t x, uint64_t r = 0x1b) {
    uint64_t y = x = gf_mul(x, x, r);
    for (int i = 2; i &lt; 64; ++i) {
      x = gf_mul(x, x, r);
      y = gf_mul(x, y, r);
    }
    return y;
}
</code></pre>
<p data-sourcepos="85:1-85:848">For GF(2<sup>32</sup>), things are actually slightly <em>harder</em>, as shifts by 32 bits are required in the reduction step rather than shifts by 64 bits, and <code>pclmulqdq</code> / <code>pmull2</code> can only do a &#34;free&#34; shift by 64 bits, so explicit shift/shuffle instructions are required. One interesting option is to use the x86 <code>crc32</code> instruction, as the reduction <code>x<sup>32</sup> == x<sup>28</sup> + x<sup>27</sup> + x<sup>26</sup> + x<sup>25</sup> + x<sup>23</sup> + x<sup>22</sup> + x<sup>20</sup> + x<sup>19</sup> + x<sup>18</sup> + x<sup>14</sup> + x<sup>13</sup> + x<sup>11</sup> + x<sup>10</sup> + x<sup>9</sup> + x<sup>8</sup> + x<sup>6</sup> + 1</code> is performed within this instruction, but this forces you into a particular reduction rule, and also subjects you to the other steps performed within <code>crc32</code>, namely some shifts and bit reflections.</p>
<p data-sourcepos="87:1-87:715">Going back down to GF(2<sup>8</sup>), very recent Intel chips have added three very relevant instructions under the name of GFNI (Galois Field New Instructions). The first of these is <code>gf2p8mulb</code>, which takes two <code>uint8_t</code> values, performs infinite-precision polynomial multiplication, and then does reduction using the <code>x<sup>8</sup> == x<sup>4</sup> + x<sup>3</sup> + x + 1</code> rule to return a <code>uint8_t</code> value. This is available at any SIMD width, i.e. 16 bytes at a time in <code>xmm</code> registers, 32 bytes at a time in <code>ymm</code> registers (AVX), or 64 bytes at a time in <code>zmm</code> registers (AVX512). The other two instructions are <code>gf2p8affineqb</code> and <code>gf2p8affineinvqb</code>, which both follow the same sequence of steps:</p>
<ol data-sourcepos="89:1-93:0">
<li data-sourcepos="89:1-89:38">Take some <code>uint8_t</code> value as input.</li>
<li data-sourcepos="90:1-90:238">Optionally perform the <code>inv</code> operation on the <code>uint8_t</code> (with reference to <code>x<sup>8</sup> == x<sup>4</sup> + x<sup>3</sup> + x + 1</code>). <code>gf2p8affine<b>inv</b>qb</code> performs this step, whereas <code>gf2p8affineqb</code> skips it.</li>
<li data-sourcepos="91:1-91:133">Form a new <code>uint8_t</code> where every bit of the output <code>uint8_t</code> is the horizontal XOR of any subset of bits from the input <code>uint8_t</code>.</li>
<li data-sourcepos="92:1-93:0">XOR the <code>uint8_t</code> with an eight-bit immediate.</li>
</ol>
<p data-sourcepos="94:1-94:678">Step 3 requires a <code>uint64_t</code> operand, as there are eight output bits, and each of those requires an eight-bit mask to specify the input subset. Both of <code>gf2p8affineqb</code> and <code>gf2p8affineinvqb</code> are available at any SIMD width, i.e. 16/32/64 bytes at a time. The second input to the instruction is another vector of the same width; it specifies the operands for step 3, but as this operand is <code>uint64_t</code> rather than <code>uint8_t</code>, the same operand is used for each group of eight bytes. If just the <code>inv</code> from step 2 is desired, then step 3 can be disabled by specifying that output bit <em>i</em> is formed from just input bit <em>i</em>, and step 4 can be disabled by specifying a constant of zero.</p>
<p data-sourcepos="96:1-96:335">(As an aside, step 3 on its own allows for all sorts of bit tricks. Amongst other things, the bits within an <code>uint8_t</code> can be permuted or <a href="https://wunkolo.github.io/post/2020/11/gf2p8affineqb-int8-shifting/">shifted</a> in any fashion, including <a href="https://wunkolo.github.io/post/2020/11/gf2p8affineqb-bit-reversal/">reversal</a> or rotation.)</p>
<p data-sourcepos="98:1-98:1256">One final case to consider is multiplication by a constant in GF(2<sup>8</sup>). That is, multiplying <code>a</code> with <code>c</code> for lots of different values of <code>a</code>, where both <code>a</code> and <code>c</code> are <code>uint8_t</code>. This extends to doing dot products where one of the vectors is constant, or doing matrix/vector multiplication where the matrix is constant. One option would be to replace <code>c</code> with a <code>uint8_t[256]</code> table, at which point multiplication against <code>c</code> is just a table lookup. A related observation is that <code>gf_mul(a, c) == gf_mul(a &amp; 0xf0, c) ^ gf_mul(a &amp; 0x0f, c)</code>, and both of the multiplications on the right only have 16 possible inputs for the <code>a &amp; 0xf0</code> and <code>a &amp; 0x0f</code> terms. This means that they can both be implemented with a <code>uint8_t[16]</code> table, bringing the memory requirement down from <code>uint8_t[256]</code> to <code>uint8_t[16+16]</code>. At this smaller size, the <code>uint8_t[16]</code> tables can exist in SIMD registers rather than in memory, and table lookup becomes a <code>pshufb</code> (amd64) or <code>tbl</code> (AArch64) instruction rather than a memory load. Even better, multiple such table lookups can be performed in parallel for any SIMD width. This is the approach used by Intel&#39;s <a href="https://github.com/intel/isa-l/tree/ad8dce15c6d3f0c7f3d1b486d9c649ed39223b45/erasure_code">ISA-L EC routines</a>.</p>
</div>
</div></div>
  </body>
</html>
