<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-932%3A&#43;Queues&#43;for&#43;Kafka">Original</a>
    <h1>KIP-932: Queues for Kafka</h1>
    
    <div id="readability-page-1" class="page"><div id="main-content">
                           
        <p><strong>Current state</strong>: <em>Under Discussion</em></p><p><strong>Discussion thread</strong>: <a href="https://lists.apache.org/thread/9wdxthfsbm5xf01y4xvq6qtlg0gq96lq" rel="nofollow">https://lists.apache.org/thread/9wdxthfsbm5xf01y4xvq6qtlg0gq96lq</a><em><br/></em></p><p><strong>JIRA</strong>: <em><a href="https://issues.apache.org/jira/browse/KAFKA-1" rel="nofollow">here</a> [Change the link from KAFKA-1 to your own ticket]<br/></em></p><p>Please keep the discussion on the mailing list rather than commenting on the wiki (wiki discussions get unwieldy fast).</p><p>Apache Kafka has achieved great success as a highly scalable event-streaming platform. The way that consumer groups assign partitions to members of the group gives a powerful combination of ordering and scalability, but it does introduce coupling between the number of consumers in a consumer group and the number of partitions. Users of Kafka often have to “over-partition” simply to ensure they can have sufficient parallel consumption to cope with peak loads.</p><p>There are plenty of situations in which consumers could cooperatively consume from a stream of events without needing to be assigned exclusive access to specific topic-partitions. This, together with per-message acknowledgement and delivery counts, enables a class of use-cases traditionally built around the concept of a queue. For example, a queue is perfect for a situation in which messages are independent work items that can be processed concurrently by a pool of applications, and individually retried or acknowledged as processing completes. This is much easier to achieve using a queue rather than a partitioned topic with a consumer group.</p><p>This KIP introduces the concept of a <strong>share group</strong> as a way of enabling cooperative consumption using Kafka topics. <span>It does not add the concept of a “queue” to Kafka per se</span>, but rather that introduces cooperative consumption to accommodate these queuing use-cases using regular Kafka topics. Share groups make this possible. You can think of a share group as roughly equivalent to a “durable shared subscription” in existing systems.</p><p>This is indeed Queues for Kafka - queues done in a Kafka way, with no maximum queue depth and the ability to reset to a specific time for point-in-time recovery.</p><p><strong>Share groups</strong> allow Kafka consumers to work together cooperatively consuming and processing the records from topics. They are an alternative to consumer groups for situations in which finer-grained sharing is required.</p><p>The fundamental differences between a share group and a consumer group are:</p><ul><li><p><span>The consumers in a share group cooperatively consume records without partition assignment</span></p></li><li><p>The number of consumers in a share group can exceed the number of partitions</p></li><li><p>Records are acknowledged on an individual basis, although the system is optimized to work in batches for improved efficiency</p></li><li><p>Delivery attempts to consumers in a share group are counted to enable automated handling of unprocessable records</p></li></ul><p>Share groups are a new kind of group, alongside the existing consumer groups.</p><p>All consumers in the same share group subscribed to the same topic cooperatively consume the records of that topic. If a topic is accessed by consumers in more than one share group, each share group cooperatively consumes from that topic independently of the other share groups.</p><p>Each consumer can dynamically set the list of topics it wants to subscribe to. In practice, all of the consumers in a share group will usually subscribe to the same topic or topics.</p><p>When a consumer in a share-group fetches records, it receives <strong>available</strong> records from any of the topic-partitions that match its subscriptions. Records are <strong>acquired</strong> for delivery to this consumer with a time-limited acquisition lock. While a record is acquired, it is not available for another consumer. By default, the lock duration is 30s, but it can also be controlled using a consumer configuration. The idea is that the lock is automatically released once the lock duration has elapsed, and then the record is available to be given to another consumer. The consumer which holds the lock can deal with it in the following ways:</p><ul><li><p>The consumer can <strong>acknowledge</strong> successful processing of the record</p></li><li><p>The consumer can <strong>release</strong> the record, which makes the record available for another delivery attempt</p></li><li><p>The consumer can <strong>reject</strong> the record, which indicates that the record is unprocessable and does not make the record available for another delivery attempt</p></li><li><p>The consumer can do nothing, in which case the lock is automatically released when the lock duration has elapsed</p></li></ul><p>The cluster limits the number of records acquired for consumers for each topic-partition in a share group. Once the limit is reached, fetching records will temporarily yield no further records until the number of acquired records reduces, as naturally happens when the locks time out. This limit is controlled by the broker <code>group.share.record.lock.partition.limit</code> configuration parameter. By limiting the duration of the acquisition lock and automatically releasing the locks, the broker ensures delivery progresses even in the presence of consumer failures.</p><h2 id="KIP932:QueuesforKafka-Concepts">Concepts</h2><p>There are some concepts being introduced to Kafka to support share groups.</p><p>A <strong>share-group coordinator</strong> is the broker which is the group coordinator for a share group. The responsibility for being share-group coordinator for the cluster’s share groups is distributed among the brokers, exactly as for consumer groups. The share-group coordinator has the following responsibilities:</p><ul><li><p>It maintains the list of share-group members.</p></li><li><p>It manages the topic-partition assignments for the share-group members. An initial, trivial implementation would be to give each member the list of all topic-partitions which matches its subscriptions and then use the pull-based protocol to fetch records from all partitions. A more sophisticated implementation could use <span>topic-partition load and lag metrics to distribute partitions among the consumers as a kind of autonomous, self-balancing partition assignment</span>, steering more consumers to busier partitions, for example. Alternatively, a push-based fetching scheme could be used.</p></li></ul><p>A <strong>share-partition</strong> is a topic-partition with a subscription in a share-group. For a topic-partition subscribed in more than one share group, each share group has its own share-partition.</p><p>A <strong>share-partition leader</strong> is a component of the broker which manages the share-group’s view of a topic-partition. It is co-located with the topic-partition leader, and the leadership of a share-partition follows the leadership of the topic-partition. The share-partition leader has the following responsibilities:</p><ul><li><p>It fetches the records from the replica manager from the local replica</p></li><li><p>It manages and persists the states of the in-flight records</p></li></ul><p>This KIP builds upon the new consumer group protocol in <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-848%3A+The+Next+Generation+of+the+Consumer+Rebalance+Protocol">KIP-848: The Next Generation of the Consumer Rebalance Protocol</a>.</p><p>A share group is a new type of group, adding <code>&#34;share&#34;</code>  to the existing group types of <code>&#34;generic&#34;</code>  and <code>&#34;consumer&#34;</code>.</p><p>Share group membership is controlled by the group coordinator. Consumers in a share group use the heartbeat mechanism to join, leave and confirm continued membership of the share group. Share-partition assignment is also piggybacked on the heartbeat mechanism. Share groups only support server-side assignors. This KIP introduces just one assignor, <code>org.apache.kafka.server.group.share.SimpleAssignor</code> , which assigns all partitions of all subscribed topics to all members.</p><p><span>In the future, a more sophisticated share group assignor could balance the number of consumers assigned to the partitions, and it may well revoke partitions from existing members in order to improve the balance. The simple assignor isn’t that smart.</span></p><p><span>For a share group, a rebalance is a much less significant event than for a consumer group because there’s no fencing. When a partition is assigned to a member of a share group, it’s telling the member that it should fetch records from that partition, which it may well be sharing with the other members of the share group. The members are not aware of each other, and there’s no synchronization barrier or fencing involved. The group coordinator, using the server-side assignor, is responsible for telling the members which partitions they are assigned and revoked. But the aim is to give every member useful work, rather than to keep the members&#39; assignments safely separated.</span></p><p><span>For a share group, the group coordinator does not need to persist the assignments, but it does need to persist the assignment epoch so that it doesn&#39;t move backwards if the group coordinator changes.</span></p><p><span>The reconciliation process for a share group is very simple because there is no fencing - the group coordinator revokes the partitions which are no longer in the target assignment of the member and assigns the new partitions to the member at the same time. There’s no need for the revocations to be acknowledged before new partitions are assigned. The member acknowledges changes to its assignment, but the group coordinator does not depend upon receiving the acknowledgement to proceed.</span></p><h3 id="KIP932:QueuesforKafka-Heartbeatandsession"><span>Heartbeat and session</span></h3><p>The member uses the ConsumerGroupHeartbeat API to establish a session with the group coordinator. The member is expected to heartbeat every <code>group.share.heartbeat.interval.ms</code> in order to keep its session opened. If it does not heartbeat at least once within the <code>group.share.session.timeout.ms</code>, the group coordinator will kick the member out from the group. The member is told the heartbeat interval in the response to the <code>ConsumerGroupHeartbeat</code> API.</p><p>If a member is kicked out of the group because it fails to heartbeat, because there’s intentionally no fencing, at the protocol level, the consumer does not lose the ability to fetch and acknowledge records. A failure to heartbeat is most likely because the consumer has died. If the consumer just failed to heartbeat due to a temporary pause, it could in theory continue to fetch and acknowledge records. When it finally sends a heartbeat and realises it’s been kicked out of the group, it should stop fetching records because its assignment has been revoked, and rejoin the group.</p><h3 id="KIP932:QueuesforKafka-Staticmembership">Static membership</h3><p><span>Share groups do not support static membership. Because the membership of a share group is more ephemeral, there’s less advantage to maintaining an assignment when a member has temporarily left but will rejoin within the session timeout.</span></p><p>Share groups do not have the ASSIGNING state because only server-side assignors are supported, and do not need the RECONCILING state because there’s no need for all members to converge before the group enters the STABLE state.</p><ul><li><strong>EMPTY</strong> - When a share group is created or the last member leaves the group, the share group is EMPTY.</li><li><strong>STABLE</strong> - When a share group has active members, the share group is STABLE.</li><li><strong>DEAD</strong> - When the share group remains EMPTY for a configured period, the group coordinator transitions it to DEAD to delete it.</li></ul><h2 id="KIP932:QueuesforKafka-In-flightrecords">In-flight records</h2><p>For each share-partition, the share group adds some state management for the records being consumed. The starting offset of records which are eligible for consumption is known as the <strong>share-partition start offset</strong> (SPSO), and the last offset of records which are eligible for consumption is known as the <strong>share-partition end offset</strong> (SPEO). The records between starting at the SPSO and up to the SPEO are known as the <strong>in-flight records</strong>. So, a share-partition is essentially managing the consumption of the in-flight records.</p><p>The SPEO is not necessarily always at the end of the topic-partition and it just advances freely as records are fetched beyond this point. The segment of the topic-partition between the SPSO and the SPEO is a sliding window that moves as records are consumed. The share-partition leader limits the distance between the SPSO and the SPEO. The upper bound is controlled by the broker configuration <code>group.share.record.lock.partition.limit</code>. Unlike existing queuing systems, there’s no “maximum queue depth”, but there is a limit to the number of in-flight records at any point in time.</p><p>The records in a share-partition are in one of four states:</p><div><table><colgroup><col/><col/></colgroup><tbody><tr><th colspan="1" rowspan="1"><p><strong>State</strong></p></th><th colspan="1" rowspan="1"><p><strong>Description</strong></p></th></tr><tr><td colspan="1" rowspan="1"><p><strong>Available</strong></p></td><td colspan="1" rowspan="1"><p>The record is available for a consumer</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>Acquired</strong></p></td><td colspan="1" rowspan="1"><p>The record has been acquired for a specific consumer, with a time-limited acquisition lock</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>Acknowledged</strong></p></td><td colspan="1" rowspan="1"><p>The record has been processed and acknowledged by a consumer</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>Archived</strong></p></td><td colspan="1" rowspan="1"><p>The record is not available for a consumer</p></td></tr></tbody></table></div><p>All records before the SPSO are in <strong>Archived</strong> state. All records after the SPEO are in <strong>Available</strong> state, but not yet being delivered to consumers.</p><p>The records also have a <strong>delivery count</strong> in order to prevent unprocessable records being endlessly delivered to consumers. If a record is repeatedly causing exceptions during its processing, it is likely that it is a “poison message”, perhaps with a formatting or semantic error. Every time that a record is acquired by a consumer in a share group, its delivery count increments by 1. The first time a record is acquired, its delivery count is 1.</p><p>The state transitions look like this:</p><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: text; gutter: false; theme: Default" data-theme="Default">+--------------+
|  Available   |&lt;------------------+
+--------------+                   |
       |                           |
       | acquired for consumer     | - if (delivery count &lt; group.share.delivery.attempt.limit)
       | (delivery count++)        |     - released by consumer
       |                           |     - acquisition lock elapsed
       V                           |
+--------------+                   |
|   Acquired   |-------------------+
+--------------+                   |
       |                           |
       |                           | - if (delivery count == group.share.delivery.attempt.limit)
       | accepted by consumer      |     - released by consumer
       |                           |     - acquisition lock elapsed
       V                           | OR
+--------------+                   | - rejected by consumer as unprocessable
| Acknowledged |                   |
+--------------+                   |
       |                           |
       |                           |
       | SPSO moves past record    |
       |                           |
       V                           |
+--------------+                   |
|   Archived   |&lt;------------------+
+--------------+</pre>
</div></div><p>The share-partition leader persists the states and delivery counts. These updates are not performed with exactly-once semantics, so the delivery count cannot be relied upon to be precise in all situations. It is intended as a way to protect against poison messages, rather than a precise count of the number of times a record is delivered to a consumer.</p><p>When records are fetched for a consumer, the share-partition leader starts at the SPSO and finds <strong>Available</strong> records. For each record it finds, it moves it into <strong>Acquired</strong> state, bumps its delivery count and adds it to a batch of acquired records to return to the consumer. The consumer then processes the records and acknowledges their consumption. The delivery attempt completes successfully and the records move into <strong>Acknowledged</strong> state.</p><p>Alternatively, if the consumer cannot process a record or its acquisition lock elapses, the delivery attempt completes unsuccessfully and the record’s next state depends on the delivery count. If the delivery count has reached the cluster’s share delivery attempt limit (5 by default), the record moves into <strong>Archived</strong> state and is not eligible for additional delivery attempts. If the delivery count has not reached the limit, the record moves back into <strong>Available</strong> state and can be delivered again.</p><p>This means that the delivery behavior is at-least-once.</p><h3 id="KIP932:QueuesforKafka-Ordering">Ordering</h3><p>Share groups focus primarily on sharing to allow consumers to be scaled independently of partitions. The records in a share-partition can be delivered out of order to a consumer, in particular when redeliveries occur.</p><p>For example, imagine two consumers in a share group consuming from a single-partition topic. The first consumer fetches records 100 to 109 inclusive and then crashes. At the same time, the second consumer fetches, processes and acknowledges records 110 to 119. When the second consumer fetches again, it gets records 100 to 109 with their delivery counts set to 2 because they are being redelivered. That’s exactly what you want, but the offsets do not necessarily increase monotonically in the same way as they do for a consumer group.</p><p>The records returned in a batch for particular share-partition are guaranteed to be in order of increasing offset. There are no guarantees about the ordering of offsets between different batches.</p><h3 id="KIP932:QueuesforKafka-ManagingtheSPSOandSPEO">Managing the SPSO and SPEO</h3><p>The consumer group concepts of seeking and position do not apply to share groups. The SPSO for each share-partition can be initialized for an empty share group and the SPEO naturally moves forwards as records are consumed.</p><p>When a topic subscription is added to a share group for the first time, the SPSO is initialized for each share-partition. By default, the SPSO for each share-partition is initialized to the latest offset for the corresponding topic-partitions.</p><p>Alternatively, there is an administrative action available using either <code>AdminClient.alterShareGroupOffsets</code> or the <code>kafka-share-groups.sh</code> tool to reset the <span>SPSO for an empty share group with no active members</span>. This can be used to “reset” a share group to the start of a topic, a particular timestamp or the end of a topic. It can also be used to initialize the share group to the start of a topic. Resetting the SPSO discards all of the in-flight record state and delivery counts.</p><p>For example, to start using a share group S1 to consume for the first time from the earliest offset of a topic T1, you could use:</p><p><em><span><span><code><span>$ kafka-share-groups.sh --bootstrap-server localhost:9092 --group S1 --topic T1 --reset-offsets --to-earliest --execute</span></code></span></span></em></p><p>If the number of partitions is increased for a topic with a subscription in a share group, the SPSO for the newly created share-partitions is initialized to 0 (which is of course both the earliest and latest offset for an empty topic-partition). This means there is no doubt about what happens when the number of partitions is increased.</p><p>If the SPSO is reset to an offset that has been tiered to remote storage (<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage">KIP-405: Kafka Tiered Storage</a>), there will be a performance impact just as for existing consumers fetching records from remote storage.</p><h3 id="KIP932:QueuesforKafka-In-flightrecordsexample">In-flight records example</h3><p>An example of a share-partition showing the states looks like this:</p><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+
|   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |  ...  | &lt;- offset
| Archv | Archv | Acqrd | Avail | Acqrd | Acked | Archv | Avail | Avail | Avail | Avail | &lt;- state
|       |       |   1   |   2   |   1   |       |       |       |       |       |       | &lt;- delivery count
+-------+-------+---^---+-------+-------+-------+-------+-------+-------+---^---+-------+
                    |                                                       |
                    +-- Share-partition start offset (SPSO)                 +-- Share-partition end offset (SPEO) </pre>
</div></div><p>The share group is currently managing the consumption of the in-flight records, which have offsets 2 to 8 inclusive.</p><ul><li><p>All records earlier than offset 2 are in <strong>Archived</strong> state and are not in-flight</p></li><li><p>Records 2 and 4 have been acquired for consumption by a consumer, and their delivery counts have been incremented to 1</p></li><li><p>Record 3 has previously been acquired twice for consumption by a consumer, but went back into <strong>Available</strong> state</p></li><li><p>Record 5 has been acknowledged</p></li><li><p>Record 6 has previously been acquired for consumption by a consumer, was rejected because it cannot be processed, and is in <strong>Archived</strong> state</p></li><li><p>Records 7 and 8 are available for consumption by a consumer</p></li><li><p>All records starting with offset 9 and later are in <strong>Available</strong> state</p></li></ul><p>The cluster records this information durably. In this example, the durable state contains the SPSO position, the non-zero delivery count for offset 3, the <strong>Acknowledged</strong> state of offset 5, and the <strong>Archived</strong> state of offset 6.</p><h3 id="KIP932:QueuesforKafka-Batching">Batching</h3><p>Cooperative consumption is inherently record-based, but the expectation is that batching is used to maximise performance. For example:</p><ul><li><p>When a consumer fetches records, the share-partition leader prefers to return complete record batches.</p></li><li><p>In the usual and optimal case, all of the records in a batch will be in <strong>Available</strong> state and can all be moved to <strong>Acquired</strong> state with the same acquisition lock time-out.</p></li><li><p>When the consumer has processed the fetched records, it can acknowledge delivery of all of the records as a single batch, transitioning them all into <strong>Acknowledged</strong> state.</p></li></ul><p>So, when a bunch of consumers are cooperatively consumed from a topic using a share group, the natural unit of sharing is the record batch. The processing loop is roughly:</p><ul><li><p>Fetch record batch</p></li><li><p>Process records</p></li><li><p>Acknowledge all records in batch</p></li></ul><p>In the situation where some records in a batch have been released or rejected separately, subsequent fetches of those records are more likely to have gaps.</p><h3 id="KIP932:QueuesforKafka-Fetchingandacknowledgingrecords">Fetching and acknowledging records</h3><p>Share groups introduce two new APIs in the Kafka protocol.</p><ul><li><code>ShareFetch</code>  for fetching records from share-partition leaders</li><li><code>ShareAcknowledge</code>  for acknowledging delivery with share-partition leaders</li></ul><p>The <code>ShareFetch</code> API works very much like incremental fetch using a concept called a <strong>share session</strong>. Each share session contains a set of topic-partitions which are managed in the share-partition leaders. The share-partition leader manages the fetching of records and the in-flight record state for its share-partitions. The consumer adds and removes topic-partitions from its share session using the <code>ShareFetch</code> API just like the <code>Fetch</code> API is used for incremental fetch. With the <code>Fetch</code> API, the consumer specifies the fetch offset. With the <code>ShareFetch</code> API, the consumer just fetches records and the share-partition leader decides which records to return.</p><p>In order to ensure no share-partitions are starved from records being fetched, the share-partition leader rotates the order of share-partitions for which it returns partition information. This ensures that it eventually returns data about all partitions for which data is available.</p><p>When a batch of records is first read from the log and added to the in-flight records for a share-partition, the broker does not know whether the set of records between the batch’s base offset and the last offset contains any gaps, as might occur for example as a result of log compaction. When the broker does not know which offsets correspond to records, the batch is considered an <strong>unmaterialized record batch</strong>. Rather than forcing the broker to iterate through all of the records in all cases, which might require decompressing every batch, the broker can send unmaterialized record batches to consumers. It initially assumes that all offsets between the base offset and the last offset correspond to records. When the consumer processes the batch, it may find gaps and it reports these using the <code>ShareAcknowledge</code> API. This means that the presence of unmaterialized record batches containing gaps might temporarily inflate the number of in-flight records, but this will be resolved by the consumer acknowledgements.</p><h2 id="KIP932:QueuesforKafka-Clientprogramminginterface">Client programming interface</h2><p>A new interface <code>KafkaShareConsumer</code> is introduced for consuming from share groups. It looks very similar to <code>KafkaConsumer</code> trimmed down to the methods that apply to share groups.</p><p>To join a share group, the client application instantiates a <code>KafkaShareConsumer</code> using the configuration parameter <code>group.id</code> to give the ID of the share group. Then, it uses <code>KafkaShareConsumer</code><code>.subscribe(Collection&lt;String&gt; topics)</code> to provide the list of topics that it wishes to consume from. The consumer is not allowed to assign partitions itself.</p><p>Each call to <code>KafkaShareConsumer</code><code>.poll(Duration)</code> fetches data from any of the topic-partitions for the topics to which it subscribed. It returns a set of in-flight records acquired for this consumer for the duration of the acquisition lock timeout. For efficiency, the consumer preferentially returns complete record sets with no gaps. The application then processes the records and acknowledges their delivery, either using explicit or implicit acknowledgement.</p><p>If the application calls the <code>KafkaShareConsumer</code><code>.acknowledge(ConsumerRecord, AcknowledgeType)</code> method for any record in the batch, it is using <u>explicit acknowledgement</u>. The calls to <code>KafkaShareConsumer</code><code>.acknowledge(ConsumerRecord, AcknowledgeType)</code> must be issued in the order in which the records appear in the <code>ConsumerRecords</code> object, which will be in order of increasing offset for each share-partition. In this case:</p><ul><li><p>The application calls <code>KafkaShareConsumer</code><code>.commitSync/Async()</code> which commits the acknowledgements to Kafka. If any records in the batch were not acknowledged, they remain acquired and will be presented to the application in response to a future poll.</p></li><li><p>The application calls <code>KafkaShareConsumer</code><code>.poll(Duration)</code> without committing first, which commits the acknowledgements to Kafka asynchronously. In this case, no exception is thrown by a failure to commit the acknowledgement. If any records in the batch were not acknowledged, they remain acquired and will be presented to the application in response to a future poll.</p></li><li><p>The application calls <code>KafkaShareConsumer</code><code>.close()</code> which attempts to commit any pending acknowledgements and releases any remaining acquired records.</p></li></ul><p>If the application does not call <code>KafkaShareConsumer</code><code>.acknowledge(ConsumerRecord, AcknowledgeType)</code> for any record in the batch, it is using <u>implicit acknowledgement</u>. In this case:</p><ul><li><p>The application calls <code>KafkaShareConsumer</code><code>.commitSync/Async()</code> which implicitly acknowledges all of the delivered records as processed successfully and commits the acknowledgements to Kafka.</p></li><li><p>The application calls <code>KafkaShareConsumer</code><code>.poll(Duration)</code> without committing, which also implicitly acknowledges all of the delivered records and commits the acknowledgements to Kafka asynchronously. In this case, no exception is thrown by a failure to commit the acknowledgements.</p></li><li><p>The application calls <code>KafkaShareConsumer</code><code>.close()</code> which releases any acquired records without acknowledgement.</p></li></ul><p>The <code>KafkaShareConsumer</code> guarantees that the records returned in the <code>ConsumerRecords</code> object for a specific share-partition are in order of increasing offset. For each share-partition, the share-partition leader guarantees that acknowledgements for the records in a batch are performed atomically. This makes error handling significantly more straightforward because there can be one error code per share-partition.</p><h3 id="KIP932:QueuesforKafka-Example-Acknowledgingabatchofrecords(implicitacknowledgement)">Example - Acknowledging a batch of records (implicit acknowledgement)</h3><p>In this example, a consumer using share group <code>&#34;myshare&#34;</code> subscribes to topic <code>&#34;foo&#34;</code>. It processes all of the records in the batch and then calls <code>KafkaShareConsumer</code><code>.commitSync()</code> which implicitly marks all of the records in the batch as successfully consumed and commits the acknowledgement synchronously with Kafka. Asynchronous commit would also be acceptable.</p><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">Properties props = new Properties();
props.setProperty(&#34;bootstrap.servers&#34;, &#34;localhost:9092&#34;);
props.setProperty(&#34;group.id&#34;, &#34;myshare&#34;);

KafkaShareConsumer&lt;String, String&gt; consumer = new KafkaShareConsumer&lt;&gt;(props, new StringDeserializer(), new StringDeserializer());
consumer.subscribe(Arrays.asList(&#34;foo&#34;));
while (true) { 
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));    // Returns a batch of acquired records
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        doProcessing(record);
    }
    consumer.commitSync();                                                              // Commit the acknowledgement of all the records in the batch
}</pre>
</div></div><p>Behind the scenes, the <code>KafkaShareConsumer</code> fetches records from the share-partition leader. The leader selects the records in <strong>Available</strong> state, and will return complete record batches (<a href="https://kafka.apache.org/documentation/#recordbatch" rel="nofollow">https://kafka.apache.org/documentation/#recordbatch</a>) if possible. It moves the records into <strong>Acquired</strong> state, increments the delivery count, starts the acquisition lock timeout, and returns them to the <code>KafkaShareConsumer</code> . Then the <code>KafkaShareConsumer</code> keeps a map of the state of the records it has fetched and returns a batch to the application.</p><p>When the application calls <code>KafkaShareConsumer</code><code>.commitSync()</code>, the <code>KafkaConsumer</code> updates the state map by marking all of the records in the batch as <strong>Acknowledged</strong> and it then commits the acknowledgements by sending the new state information to the share-partition leader. For each share-partition, the share-partition leader updates the record states atomically.</p><h3 id="KIP932:QueuesforKafka-Example-Per-recordacknowledgement(explicitacknowledgement)">Example - Per-record acknowledgement (explicit acknowledgement)</h3><p>In this example, the application uses the result of processing the records to acknowledge or reject the records in the batch.</p><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">Properties props = new Properties();
props.setProperty(&#34;bootstrap.servers&#34;, &#34;localhost:9092&#34;);
props.setProperty(&#34;group.id&#34;, &#34;myshare&#34;);

KafkaShareConsumer&lt;String, String&gt; consumer = new KafkaShareConsumer&lt;&gt;(props, new StringDeserializer(), new StringDeserializer());
consumer.subscribe(Arrays.asList(&#34;foo&#34;));
while (true) {
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));    // Returns a batch of acquired records
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        try {
            doProcessing(record);
            consumer.acknowledge(record, AcknowledgeType.ACCEPT);                       // Mark the record as processed successfully
        } catch (Exception e) {
            consumer.acknowledge(record, AcknowledgeType.REJECT);                       // Mark the record as unprocessable
        }
    }
    consumer.commitAsync();                                                             // Commit the acknowledgements of all the records in the batch
}</pre>
</div></div><p>In this example, each record processed is separately acknowledged using a call to the new <code>KafkaShareConsumer</code><code>.acknowledge(ConsumerRecord, AcknowledgeType)</code> method. The <code>AcknowledgeType</code> argument indicates whether the record was processed successfully or not. In this case, the bad records are rejected meaning that they’re not eligible for further delivery attempts. For a permanent error such as a deserialization error, this is appropriate. For a transient error which might not affect a subsequent processing attempt, the <code>AcknowledgeType.RELEASE</code> is more appropriate because the record remains eligible for further delivery attempts.</p><p>The calls to <code>KafkaShareConsumer</code><code>.acknowledge(ConsumerRecord, AcknowledgeType)</code> are simply updating the state map in the <code>KafkaConsumer</code>. It is only once <code>KafkaShareConsumer</code><code>.commitAsync()</code> is called that the acknowledgements are committed by sending the new state information to the share-partition leader.</p><h3 id="KIP932:QueuesforKafka-Example-Per-recordacknowledgement,endingprocessingofthebatchonanerror(explicitacknowledgement)">Example - Per-record acknowledgement, ending processing of the batch on an error (explicit acknowledgement)</h3><p>In this example, the application stops processing the batch when it encounters an exception.</p><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">Properties props = new Properties();
props.setProperty(&#34;bootstrap.servers&#34;, &#34;localhost:9092&#34;);
props.setProperty(&#34;group.id&#34;, &#34;myshare&#34;);

KafkaShareConsumer&lt;String, String&gt; consumer = new KafkaShareConsumer&lt;&gt;(props, new StringDeserializer(), new StringDeserializer());
consumer.subscribe(Arrays.asList(&#34;foo&#34;));
while (true) {
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));    // Returns a batch of acquired records
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        try {
            doProcessing(record);
            consumer.acknowledge(record, AcknowledgeType.ACCEPT);                       // Mark the record as processed successfully
        } catch (Exception e) {
            consumer.acknowledge(record, AcknowledgeType.REJECT);                       // Mark this record as unprocessable
            break;
        }
    }
    consumer.commitAsync();                                                             // Commit the acknowledgements of the acknowledged records only
}</pre>
</div></div><p>There are the following cases in this example:</p><ol><li><p>The batch contains no records, in which case the application just polls again. The call to <code>KafkaShareConsumer</code><code>.commitAsync()</code> just does nothing because the batch was empty.</p></li><li><p>All of the records in the batch are processed successfully. The calls to <code>KafkaShareConsumer</code><code>.acknowledge(ConsumerRecord, AcknowledgeType.ACCEPT)</code> marks all records in the batch as successfully processed.</p></li><li><p>One of the records encounters an exception. The call to <code>KafkaShareConsumer</code><code>.acknowledge(ConsumerRecord, AcknowledgeType.REJECT)</code> rejects that record. Earlier records in the batch have already been marked as successfully processed. The call to <code>KafkaShareConsumer</code><code>.commitAsync()</code> commits the acknowledgements, but the records after the failed record remain <strong>Acquired</strong> as part of the same delivery attempt and will be presented to the application in response to another poll.</p></li></ol><h2 id="KIP932:QueuesforKafka-Accesscontrol">Access control</h2><p>Share group access control is performed on the <code>GROUP</code> resource type, just the same as consumer groups, with the same rules for the actions checked. A share group is just a new kind of group.</p><ul><li><p>Operations which read information about a share group need permission to perform the <code>DESCRIBE</code> action on the named group resource</p></li><li><p>Operations which change information about a share group (such as consuming a record) need permission to perform the <code>READ</code> action on the named group resource</p></li></ul><p>The share-partition leader is responsible for recording the durable state for the share-partitions it leads. For each share-partition, we need to be able to recover:</p><ul><li><p>The Share-Partition Start Offset (SPSO)</p></li><li><p>The state of the in-flight records</p></li><li><p>The delivery counts of records whose delivery failed</p></li></ul><p>The delivery counts are only maintained approximately and the <strong>Acquired</strong> state is not persisted. This minimises the amount of share-partition state that has to be logged. The expectation is that most records will be fetched and acknowledged in batches with only one delivery attempt.</p><h3 id="KIP932:QueuesforKafka-Examples">Examples</h3><div><table><colgroup><col/><col/><col/></colgroup><tbody><tr><th colspan="1" rowspan="1"><p><strong>Operation</strong></p></th><th colspan="1" rowspan="1"><p><strong>State changes</strong></p></th><th colspan="1" rowspan="1"><p><strong>Cumulative state</strong></p></th></tr><tr><td colspan="1" rowspan="1"><p>Starting state of topic-partition with latest offset 100</p></td><td colspan="1" rowspan="1"><p>SPSO=100, SPEO=100</p></td><td colspan="1" rowspan="1"><p>SPSO=100, SPEO=100</p></td></tr><tr><td colspan="3" rowspan="1"><p><strong>In the batched case with successful processing, there’s a state change per batch to move the SPSO forwards</strong></p></td></tr><tr><td colspan="1" rowspan="1"><p>Fetch records 100-109</p></td><td colspan="1" rowspan="1"><p>SPEO=110, records 100-109 (acquired, delivery count 1)</p></td><td colspan="1" rowspan="1"><p>SPSO=100, SPEO=110, records 100-109 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Acknowledge 100-109</p></td><td colspan="1" rowspan="1"><p>SPSO=110</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=110</p></td></tr><tr><td colspan="3" rowspan="1"><p><strong>With a messier sequence of release and acknowledge, there’s a state change for each operation which can act on multiple records</strong></p></td></tr><tr><td colspan="1" rowspan="1"><p>Fetch records 110-119</p></td><td colspan="1" rowspan="1"><p>SPEO=120, records 110-119 (acquired, delivery count 1)</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=120, records 110-119 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Release 110</p></td><td colspan="1" rowspan="1"><p>record 110 (available, delivery count 1)</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=120, record 110 (available, delivery count 1), records 111-119 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Acknowledge 119</p></td><td colspan="1" rowspan="1"><p>record 110 (available, delivery count 1), records 111-118 acquired, record 119 acknowledged</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=120, record 110 (available, delivery count 1), records 111-118 (acquired, delivery count 1), record 119 acknowledged</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fetch records 110, 120</p></td><td colspan="1" rowspan="1"><p>SPEO=121, record 110 (acquired, delivery count 2), record 120 (acquired, delivery count 1)</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=121, record 110 (acquired, delivery count 2), records 111-118 (acquired, delivery count 1), record 119 acknowledged, record 120 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Lock timeout elapsed 111, 112</p></td><td colspan="1" rowspan="1"><p>records 111-112 (available, delivery count 1)</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=121, record 110 (acquired, delivery count 2), records 111-112 (available, delivery count 1), records 113-118 (acquired, delivery count 1), record 119 acknowledged, record 120 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Acknowledge 113-118</p></td><td colspan="1" rowspan="1"><p>records 113-118 acknowledged</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=121, record 110 (acquired, delivery count 2), records 111-112 (available, delivery count 1), records 113-119 acknowledged, record 120 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fetch records 111,112</p></td><td colspan="1" rowspan="1"><p>records 111-112 (acquired, delivery count 2)</p></td><td colspan="1" rowspan="1"><p>SPSO=110, SPEO=121, record 110-112 (acquired, delivery count 2), records 113-119 acknowledged, record 120 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Acknowledge 110</p></td><td colspan="1" rowspan="1"><p>SPSO=111</p></td><td colspan="1" rowspan="1"><p>SPSO=111, SPEO=121, record 111-112 (acquired, delivery count 2), records 113-119 acknowledged, record 120 (acquired, delivery count 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Acknowledge 111,112</p></td><td colspan="1" rowspan="1"><p>SPSO=120</p></td><td colspan="1" rowspan="1"><p>SPSO=120, SPEO=121, record 120 (acquired, delivery count 1)</p></td></tr></tbody></table></div><p>Further details to follow as the design progresses.</p><p>This KIP introduces extensive additions to the public interfaces.</p><h2 id="KIP932:QueuesforKafka-ClientAPIchanges">Client API changes</h2><p>This KIP introduces a new interface for consuming records from a share group.</p><div><table><colgroup><col/><col/></colgroup><tbody><tr><th>Method signature</th><th>Description</th></tr><tr><td><code>void acknowledge(ConsumerRecord record)</code> </td><td>Acknowledge successful delivery of a record returned on the last <code>poll(Duration)</code>. The acknowledgement is committed on the next <code>commitSync()</code>  or <code>commitAsync()</code>  call.</td></tr><tr><td><code>void acknowledge(ConsumerRecord record, AcknowledgeType type)</code> </td><td>Acknowledge delivery of a record returned on the last <code>poll(Duration)</code> indicating whether it was processed successfully. The acknowledgement is committed on the next <code>commitSync()</code>  or <code>commitAsync()</code>  call.</td></tr><tr><td><code>void close()</code> </td><td>Close the consumer, waiting for up to the default timeout of 30 seconds for any needed cleanup.</td></tr><tr><td><code>void close(Duration timeout)</code> </td><td>Tries to close the consumer cleanly within the specified timeout.</td></tr><tr><td><code>void commitAsync()</code> </td><td>Commits the acknowledgements for the records returned.</td></tr><tr><td><code>void commitSync()</code></td><td>Commits the acknowledgements for the records returned.</td></tr><tr><td><code>void commitSync(Duration timeout)</code></td><td>Commits the acknowledgements for the records returned.</td></tr><tr><td><code>Map&lt;MetricName, ? extends Metric&gt; metrics()</code> </td><td><span>Get the metrics kept by the consumer.</span></td></tr><tr><td><code>List&lt;PartitionInfo&gt; partitionsFor(String topic)</code> </td><td><span>Get metadata about the partitions for a given topic.</span></td></tr><tr><td><code>List&lt;PartitionInfo&gt; partitionsFor(String topic,  Duration timeout)</code> </td><td><span>Get metadata about the partitions for a given topic.</span></td></tr><tr><td><code>ConsumerRecords&lt;K,V&gt; poll(Duration timeout)</code> </td><td><span>Fetch data for the topics or partitions specified using the subscribe API.</span></td></tr><tr><td><code>void subscribe(Collection&lt;String&gt; topics)</code> </td><td><span>Subscribe to the given list of topics to get dynamically assigned partitions.</span></td></tr><tr><td><code>Set&lt;String&gt; subscription()</code> </td><td><span>Get the current subscription.</span></td></tr><tr><td><code>void unsubscribe()</code> </td><td><span>Unsubscribe from topics currently subscribed with <code>subscribe(Collection)</code> .</span></td></tr><tr><td><code>void wakeup()</code> </td><td><span>Wakeup the consumer.</span></td></tr></tbody></table></div><h3 id="KIP932:QueuesforKafka-AcknowledgeType">AcknowledgeType</h3><p>The new <code>org.apache.kafka.clients.consumer.AcknowledgeType</code>  enum distinguishes between the types of acknowledgement for a record consumer using a share group.</p><div><table><colgroup><col/><col/></colgroup><tbody><tr><th>Enum constant</th><th>Description</th></tr><tr><td><code>ACCEPT</code>  (0)</td><td>The record was consumed successfully</td></tr><tr><td><code>RELEASE</code>  (1)</td><td>The record was not consumed successfully. Release it for another delivery attempt.</td></tr><tr><td><code>REJECT</code>  (2)</td><td>The record was not consumed successfully. Reject it and do not release it for another delivery attempt.</td></tr></tbody></table></div><h3 id="KIP932:QueuesforKafka-AdminClient">AdminClient</h3><p>Add the following methods on the <code>AdminClient</code>  interface.</p><div><table><colgroup><col/><col/></colgroup><tbody><tr><th>Method signature</th><th>Description</th></tr><tr><td><code>AlterShareGroupOffsetsResult alterShareGroupOffsets(String groupId, Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets, AlterShareGroupOffsetsOptions options)</code> </td><td><span>Alter offset information for a share group.</span></td></tr><tr><td><code>DeleteShareGroupOffsetsResult deleteShareGroupOffsets(String groupId, Set&lt;TopicPartition&gt; partitions, DeleteShareGroupOffsetsOptions options)</code> </td><td><span>Delete offset information for a set of partitions in a share group.</span></td></tr><tr><td><code>DeleteShareGroupResult deleteShareGroups(Collection&lt;String&gt; groupIds, DeleteShareGroupOptions options)</code> </td><td><span>Delete share groups from the cluster.</span></td></tr><tr><td><code>DescribeShareGroupsResult describeShareGroups(Collection&lt;String&gt; groupIds, DescribeShareGroupsOptions options)</code> </td><td><span>Describe some share groups in the cluster.</span></td></tr><tr><td><code>ListShareGroupOffsetsResult listShareGroupOffsets(String groupId, ListShareGroupOffsetsOptions options)</code> </td><td><span>List the share group offsets available in the cluster.</span></td></tr><tr><td><code>ListShareGroupsResult listShareGroups(ListShareGroupsOptions options)</code> </td><td><span>List the share groups available in the cluster.</span></td></tr></tbody></table></div><p>The equivalence between the consumer group and share group interfaces is clear. There are some differences:</p><ul><li>Altering the offsets for a share group resets the Share Start Offset for topic-partitions in the share group (share-partitions)</li><li>The members of a share group are not assigned partitions</li><li>A share group has only three states - <code>EMPTY</code> , <code>STABLE</code> and <code>DEAD</code> </li></ul><p>A new tool is added for working with share groups called <code>kafka-share-groups.sh</code> . It has the following options:</p><div><table><colgroup><col/><col/></colgroup><tbody><tr><th colspan="1" rowspan="1"><p><strong>Option</strong></p></th><th colspan="1" rowspan="1"><p><strong>Description</strong></p></th></tr><tr><td colspan="1" rowspan="1"><p>--all-topics</p></td><td colspan="1" rowspan="1"><p>Consider all topics assigned to a group in the `reset-offsets` process.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--bootstrap-server &lt;String: server to connect to&gt;</p></td><td colspan="1" rowspan="1"><p>REQUIRED: The server(s) to connect to.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--command-config &lt;String: command config property file&gt;</p></td><td colspan="1" rowspan="1"><p>Property file containing configs to be passed to Admin Client.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--delete</p></td><td colspan="1" rowspan="1"><p>Pass in groups to delete topic partition offsets over the entire share group. For instance --group g1 --group g2</p></td></tr><tr><td colspan="1" rowspan="1"><p>--delete-offsets</p></td><td colspan="1" rowspan="1"><p>Delete offsets of share group. Supports one share group at the time, and multiple topics.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--describe</p></td><td colspan="1" rowspan="1"><p>Describe share group and list offset lag (number of records not yet processed) related to given group.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--dry-run</p></td><td colspan="1" rowspan="1"><p>Only show results without executing changes on share groups. Supported operations: reset-offsets.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--execute</p></td><td colspan="1" rowspan="1"><p>Execute operation. Supported operations: reset-offsets.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--group &lt;String: share group&gt;</p></td><td colspan="1" rowspan="1"><p>The share group we wish to act on.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--list</p></td><td colspan="1" rowspan="1"><p>List all share groups.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--members</p></td><td colspan="1" rowspan="1"><p>Describe members of the group. This option may be used with the &#39;--describe&#39; option only.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--offsets</p></td><td colspan="1" rowspan="1"><p>Describe the group and list all topic partitions in the group along with their offset lag. This is the default sub-action of and may be used with the &#39;--describe&#39; option only.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--reset-offsets</p></td><td colspan="1" rowspan="1"><p>Reset offsets of share group. Supports one share group at a time, and instances must be inactive.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--to-datetime &lt;String: datetime&gt;</p></td><td colspan="1" rowspan="1"><p>Reset offsets to offset from datetime. Format: &#39;YYYY-MM-DDTHH:mm:SS.sss&#39;.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--to-earliest</p></td><td colspan="1" rowspan="1"><p>Reset offsets to earliest offset.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--to-latest</p></td><td colspan="1" rowspan="1"><p>Reset offsets to latest offset.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--topic &lt;String: topic&gt;</p></td><td colspan="1" rowspan="1"><p>The topic whose share group information should be deleted or topic which should be included in the reset offset process.</p></td></tr><tr><td colspan="1" rowspan="1"><p>--version</p></td><td colspan="1" rowspan="1"><p>Display Kafka version.</p></td></tr></tbody></table></div><p>Here are some examples. </p><p>To display a list of all share groups:</p><p><em><span><span><code><span>$ kafka-share-groups.sh --bootstrap-server localhost:9092 --list</span></code></span></span></em></p><p>To delete the information for topic <code>T1</code>  from inactive share group <code>S1</code> , which essentially resets the consumption of this topic in the share group:</p><p><em><span><span><code><span>$ kafka-share-groups.sh --bootstrap-server localhost:9092 --group S1 --topic T1 --delete-offsets</span></code></span></span></em></p><p>To set the starting offset for consuming topic <code>T1</code>  in inactive share group <code>S1</code>  to a specific date and time:</p><p><em><span><span><code><span>$ kafka-share-groups.sh --bootstrap-server localhost:9092 --group S1 --topic T1 --reset-offsets --to-datetime </span><span>1999</span><span>-12-31T23:57:00.000 --execute</span></code></span></span></em></p><h2 id="KIP932:QueuesforKafka-Configuration">Configuration</h2><h3 id="KIP932:QueuesforKafka-Brokerconfiguration">Broker configuration</h3><div><table><colgroup><col/><col/><col/></colgroup><tbody><tr><th>Configuration</th><th>Description</th><th>Values</th></tr><tr><td><code>group.share.enable</code></td><td>Whether to enable share groups on the broker.</td><td>Default <code>false</code>  while the feature is being developed. Will become <code>true</code>  in a future release.</td></tr><tr><td><code>group.share.delivery.count.limit</code></td><td>The maximum number of delivery attempts for a record delivered to a share group.</td><td>Default 5, minimum 2, maximum 10</td></tr><tr><td><code>group.share.record.lock.duration.ms</code></td><td>Share-group record acquisition lock duration in milliseconds.</td><td>Default 30000 (30 seconds), minimum 1000 (1 second), maximum 60000 (60 seconds)</td></tr><tr><td><code>group.share.record.lock.duration.max.ms</code></td><td>Share-group record acquisition lock maximum duration in milliseconds.</td><td>Default 60000 (60 seconds), minimum 1000 (1 second), maximum 3600000 (1 hour)</td></tr><tr><td><code>group.share.record.lock.partition.limit</code></td><td>Share-group record lock limit per share-partition.</td><td>Default 200, minimum 100, maximum 10000</td></tr><tr><td><code>group.share.session.timeout.ms</code> </td><td><p>The timeout to detect client failures when using the group protocol.</p></td><td>Default 45000 (45 seconds)</td></tr><tr><td><code>group.share.min.session.timeout.ms</code> </td><td><p>The minimum session timeout.</p></td><td>Default 45000 (45 seconds)</td></tr><tr><td><code>group.share.max.session.timeout.ms</code> </td><td><p>The maximum session timeout.</p></td><td>Default 60000 (60 seconds)</td></tr><tr><td><code>group.share.heartbeat.interval.ms</code> </td><td><p>The heartbeat interval given to the members.</p></td><td>Default 5000 (5 seconds)</td></tr><tr><td><code>group.share.min.heartbeat.interval.ms</code> </td><td><p>The minimum heartbeat interval.</p></td><td>Default 5000 (5 seconds)</td></tr><tr><td><code>group.share.max.heartbeat.interval.ms</code> </td><td><p>The maximum heartbeat interval.</p></td><td>Default 15000 (15 seconds)</td></tr><tr><td><code>group.share.max.size</code> </td><td><p>The maximum number of consumers that a single share group can accommodate.</p></td><td>Default 200</td></tr><tr><td><code>group.share.assignors</code> </td><td><p>The server-side assignors as a list of full class names. In the initial delivery, only the first one in the list is used.</p></td><td>A list of class names. Default <code>&#34;org.apache.server.group.share.SimpleAssignor&#34;</code></td></tr><tr><td><code>max.share.session.cache.slots</code> </td><td><p>The maximum number of share sessions that the broker will maintain.</p></td><td>Default 1000</td></tr></tbody></table></div><h3 id="KIP932:QueuesforKafka-Groupconfiguration">Group configuration</h3><p>The following dynamic group configuration properties are added. These are properties for which it would be problematic to have consumers in the same share group using different behavior if the properties were specified in the consumer clients themselves.</p><div><table><colgroup><col/><col/><col/></colgroup><tbody><tr><th scope="col">Configuration</th><th scope="col">Description</th><th scope="col">Values</th></tr><tr><td><code>group.share.isolation.level</code> </td><td><p>Controls how to read records written transactionally. If set to <code>&#34;read_committed&#34;</code>, the share group will only deliver transactional records which have been committed. If set to <code>&#34;read_uncommitted&#34;</code>, the share group will return all messages, even transactional messages which have been aborted. Non-transactional records will be returned unconditionally in either mode.</p></td><td><p>Valid values <code>&#34;read_committed&#34;</code>  and <code>&#34;read_uncommitted&#34;</code> (default)</p></td></tr><tr><td><code>group.share.auto.offset.reset</code> </td><td><p>What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server:</p><ul><li><p><code>&#34;earliest&#34;</code> : automatically reset the offset to the earliest offset</p></li><li><p><code>&#34;latest&#34;</code> : automatically reset the offset to the latest offset</p></li></ul></td><td><p>Valid values <code>&#34;latest&#34;</code>  (default) and <code>&#34;earliest&#34;</code> </p></td></tr></tbody></table></div><h3 id="KIP932:QueuesforKafka-Consumerconfiguration">Consumer configuration</h3><p>The following new configuration properties are added for the consumer.</p><div><table><colgroup><col/><col/><col/></colgroup><tbody><tr><th>Configuration</th><th>Description</th><th>Values</th></tr><tr><td><code>record.lock.duration.ms</code></td><td>Record acquisition lock duration in milliseconds.</td><td>null, which uses the cluster configuration <code>share.record.lock.duration.ms</code>, minimum 1000, maximum limited by the cluster configuration <code>share.record.lock.duration.max.ms</code></td></tr></tbody></table></div><p>The existing consumer configurations apply for share groups with the following exceptions:</p><ul><li><code>auto.offset.reset</code> : this is handled by a dynamic group configuration <code>group.share.auto.offset.reset</code> </li><li><code>enable.auto.commit</code>  and <code>auto.commit.interval.ms</code> : share groups do not support auto-commit</li><li><code>isolation.level</code> : this is handled by a dynamic group configuration <code>group.share.isolation.level</code> </li><li><code>partition.assignment.strategy</code> : share groups do not support client-side partition assignors</li><li><code>interceptor.classes</code> : interceptors are not supported</li></ul><h2 id="KIP932:QueuesforKafka-Kafkaprotocolchanges">Kafka protocol changes</h2><p>This KIP introduces the following new APIs:</p><ul><li><code>ShareFetch</code> - for fetching records from share-partition leaders</li><li><code>ShareAcknowledge</code> - for acknowledging delivery of records with share-partition leaders</li><li>Additional APIs not yet documented for the AdminAPI enhancements which will follow the obvious existing precedents</li></ul><p>The ShareFetch API is used by share group consumers to fetch acquired records from share-partition leaders.</p><h4 id="KIP932:QueuesforKafka-Requestschema">Request schema</h4><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">{
  &#34;apiKey&#34;: NN,
  &#34;type&#34;: &#34;request&#34;,
  &#34;listeners&#34;: [&#34;zkBroker&#34;, &#34;broker&#34;],
  &#34;name&#34;: &#34;ShareFetchRequest&#34;,
  &#34;validVersions&#34;: &#34;0&#34;,
  &#34;flexibleVersions&#34;: &#34;0+&#34;,
  &#34;fields&#34;: [
    { &#34;name&#34;: &#34;GroupId&#34;, &#34;type&#34;: &#34;string&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;nullableVersions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;null&#34;, &#34;entityType&#34;: &#34;groupId&#34;,
      &#34;about&#34;: &#34;null if not provided or if it didn&#39;t change since the last fetch; the group identifier otherwise.&#34; },
    { &#34;name&#34;: &#34;MemberId&#34;, &#34;type&#34;: &#34;string&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;nullableVersions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;null&#34;,
      &#34;about&#34;: &#34;null if not provided or if it didn&#39;t change since the last fetch; the member id generated by the coordinator otherwise.&#34; },
    { &#34;name&#34;: &#34;AcquisitionTimeoutMs&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: -1,
      &#34;about&#34;: &#34;-1 if it didn&#39;t chance since the last fetch; the maximum time in milliseconds that the fetched records are acquired for the consumer.&#34; },
    { &#34;name&#34;: &#34;MaxWaitMs&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The maximum time in milliseconds to wait for the response.&#34; },
    { &#34;name&#34;: &#34;MinBytes&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The minimum bytes to accumulate in the response.&#34; },
    { &#34;name&#34;: &#34;MaxBytes&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;0x7fffffff&#34;, &#34;ignorable&#34;: true,
      &#34;about&#34;: &#34;The maximum bytes to fetch.  See KIP-74 for cases where this limit may not be honored.&#34; },
    { &#34;name&#34;: &#34;SessionId&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;0&#34;, &#34;ignorable&#34;: true,
      &#34;about&#34;: &#34;The share session ID.&#34; },
    { &#34;name&#34;: &#34;SessionEpoch&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;-1&#34;, &#34;ignorable&#34;: true,
      &#34;about&#34;: &#34;The share session epoch, which is used for ordering requests in a session.&#34; },
    { &#34;name&#34;: &#34;Topics&#34;, &#34;type&#34;: &#34;[]FetchTopic&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The topics to fetch.&#34;, &#34;fields&#34;: [
      { &#34;name&#34;: &#34;TopicId&#34;, &#34;type&#34;: &#34;uuid&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true, &#34;about&#34;: &#34;The unique topic ID&#34;},
      { &#34;name&#34;: &#34;Partitions&#34;, &#34;type&#34;: &#34;[]FetchPartition&#34;, &#34;versions&#34;: &#34;0+&#34;,
        &#34;about&#34;: &#34;The partitions to fetch.&#34;, &#34;fields&#34;: [
        { &#34;name&#34;: &#34;PartitionIndex&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;The partition index.&#34; },
        { &#34;name&#34;: &#34;CurrentLeaderEpoch&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;-1&#34;, &#34;ignorable&#34;: true,
          &#34;about&#34;: &#34;The current leader epoch of the partition.&#34; },
        { &#34;name&#34;: &#34;PartitionMaxBytes&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;The maximum bytes to fetch from this partition.  See KIP-74 for cases where this limit may not be honored.&#34; }
      ]}
    ]},
    { &#34;name&#34;: &#34;ForgottenTopicsData&#34;, &#34;type&#34;: &#34;[]ForgottenTopic&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: false,
      &#34;about&#34;: &#34;In an incremental fetch request, the partitions to remove.&#34;, &#34;fields&#34;: [
      { &#34;name&#34;: &#34;TopicId&#34;, &#34;type&#34;: &#34;uuid&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true, &#34;about&#34;: &#34;The unique topic ID&#34;},
      { &#34;name&#34;: &#34;Partitions&#34;, &#34;type&#34;: &#34;[]int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
        &#34;about&#34;: &#34;The partitions indexes to forget.&#34; }
    ]}
  ]
}</pre>
</div></div><h4 id="KIP932:QueuesforKafka-Responseschema">Response schema</h4><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">{
  &#34;apiKey&#34;: NN,
  &#34;type&#34;: &#34;response&#34;,
  &#34;name&#34;: &#34;ShareFetchResponse&#34;,
  &#34;validVersions&#34;: &#34;0&#34;,
  &#34;flexibleVersions&#34;: &#34;0+&#34;,
  &#34;fields&#34;: [
    { &#34;name&#34;: &#34;ThrottleTimeMs&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true,
      &#34;about&#34;: &#34;The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.&#34; },
    { &#34;name&#34;: &#34;ErrorCode&#34;, &#34;type&#34;: &#34;int16&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true,
      &#34;about&#34;: &#34;The top level response error code.&#34; },
    { &#34;name&#34;: &#34;SessionId&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;0&#34;, &#34;ignorable&#34;: false,
      &#34;about&#34;: &#34;The share session ID.&#34; },
    { &#34;name&#34;: &#34;Responses&#34;, &#34;type&#34;: &#34;[]ShareFetchableTopicResponse&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The response topics.&#34;, &#34;fields&#34;: [
      { &#34;name&#34;: &#34;TopicId&#34;, &#34;type&#34;: &#34;uuid&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true, &#34;about&#34;: &#34;The unique topic ID&#34;},
      { &#34;name&#34;: &#34;Partitions&#34;, &#34;type&#34;: &#34;[]PartitionData&#34;, &#34;versions&#34;: &#34;0+&#34;,
        &#34;about&#34;: &#34;The topic partitions.&#34;, &#34;fields&#34;: [
        { &#34;name&#34;: &#34;PartitionIndex&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;The partition index.&#34; },
        { &#34;name&#34;: &#34;ErrorCode&#34;, &#34;type&#34;: &#34;int16&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;The error code, or 0 if there was no fetch error.&#34; },
        { &#34;name&#34;: &#34;LastStableOffset&#34;, &#34;type&#34;: &#34;int64&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;-1&#34;, &#34;ignorable&#34;: true,
          &#34;about&#34;: &#34;The last stable offset (or LSO) of the partition. This is the last offset such that the state of all transactional records prior to this offset have been decided (ABORTED or COMMITTED)&#34; },
        { &#34;name&#34;: &#34;AbortedTransactions&#34;, &#34;type&#34;: &#34;[]AbortedTransaction&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;nullableVersions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true,
          &#34;about&#34;: &#34;The aborted transactions.&#34;,  &#34;fields&#34;: [
          { &#34;name&#34;: &#34;ProducerId&#34;, &#34;type&#34;: &#34;int64&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;entityType&#34;: &#34;producerId&#34;,
            &#34;about&#34;: &#34;The producer id associated with the aborted transaction.&#34; },
          { &#34;name&#34;: &#34;FirstOffset&#34;, &#34;type&#34;: &#34;int64&#34;, &#34;versions&#34;: &#34;0+&#34;,
            &#34;about&#34;: &#34;The first offset in the aborted transaction.&#34; }
        ]},
        { &#34;name&#34;: &#34;Records&#34;, &#34;type&#34;: &#34;records&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;nullableVersions&#34;: &#34;0+&#34;, &#34;about&#34;: &#34;The record data.&#34;},
        { &#34;name&#34;: &#34;AcquiredRecords&#34;, &#34;type&#34;: &#34;[]AcquiredRecords&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;about&#34;: &#34;The acquired records.&#34;, &#34;fields&#34;:  [
          {&#34;name&#34;: &#34;BaseOffset&#34;, &#34;type&#34;:  &#34;int64&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;about&#34;: &#34;The earliest offset in this batch of acquired records.&#34;},
          {&#34;name&#34;: &#34;LastOffset&#34;, &#34;type&#34;: &#34;int64&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;about&#34;: &#34;The last offset of this batch of acquired records.&#34;},
          {&#34;name&#34;: &#34;DeliveryCount&#34;, &#34;type&#34;: &#34;int16&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;about&#34;: &#34;The delivery count of this batch of acquired records.&#34;}
        ]}
      ]}
    ]}
  ]
}</pre>
</div></div><p>The ShareAcknowledge API is used by share group consumers to acknowledge delivery of records with share-partition leaders.</p><h4 id="KIP932:QueuesforKafka-Requestschema.1">Request schema</h4><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">{
  &#34;apiKey&#34;: NN,
  &#34;type&#34;: &#34;request&#34;,
  &#34;listeners&#34;: [&#34;zkBroker&#34;, &#34;broker&#34;],
  &#34;name&#34;: &#34;ShareAcknowledgeRequest&#34;,
  &#34;validVersions&#34;: &#34;0&#34;,
  &#34;flexibleVersions&#34;: &#34;0+&#34;,
  &#34;fields&#34;: [
    { &#34;name&#34;: &#34;SessionId&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The share session ID.&#34; },
    { &#34;name&#34;: &#34;SessionEpoch&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The share session epoch, which is used for ordering requests in a session.&#34; },
    { &#34;name&#34;: &#34;Topics&#34;, &#34;type&#34;: &#34;[]AcknowledgeTopic&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The topics containing records to acknowledge.&#34;, &#34;fields&#34;: [
      { &#34;name&#34;: &#34;TopicId&#34;, &#34;type&#34;: &#34;uuid&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;about&#34;: &#34;The unique topic ID&#34;},
      { &#34;name&#34;: &#34;Partitions&#34;, &#34;type&#34;: &#34;[]AcknowledgePartition&#34;, &#34;versions&#34;: &#34;0+&#34;,
        &#34;about&#34;: &#34;The partitions containing records to acknowledge.&#34;, &#34;fields&#34;: [
        { &#34;name&#34;: &#34;PartitionIndex&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;The partition index.&#34; },
        { &#34;name&#34;: &#34;AcknowledgementBatches&#34;, &#34;type&#34;: &#34;[]AcknowledgementBatch&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;Record batches to acknowledge.&#34;, &#34;fields&#34;: [
          { &#34;name&#34;: &#34;StartOffset&#34;, &#34;type&#34;: &#34;int64&#34;, &#34;versions&#34;: &#34;0+&#34;,
            &#34;about&#34;: &#34;Start offset of batch of records to acknowledge.&#34;},
          { &#34;name&#34;: &#34;LastOffset&#34;, &#34;type&#34;: &#34;int64&#34;, &#34;versions&#34;: &#34;0+&#34;,
            &#34;about&#34;: &#34;Last offset (inclusive) of batch of records to acknowledge.&#34;},
          { &#34;name&#34;: &#34;GapOffsets&#34;, &#34;type&#34;: &#34;[]int64&#34;, &#34;versions&#34;: &#34;0+&#34;,
            &#34;about&#34;: &#34;Array of offsets in this range which do not correspond to records.&#34;},
          { &#34;name&#34;: &#34;AcknowledgeType&#34;, &#34;type&#34;: &#34;string&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;accept&#34;,
            &#34;about&#34;: &#34;The type of acknowledgement, such as accept or release.&#34;}
        ]}
      ]}
    ]}
  ]
}</pre>
</div></div><h4 id="KIP932:QueuesforKafka-Responseschema.1">Response schema</h4><div data-hasbody="true" data-macro-name="code"><div>
<pre data-syntaxhighlighter-params="brush: java; gutter: false; theme: Default" data-theme="Default">{
  &#34;apiKey&#34;: NN,
  &#34;type&#34;: &#34;response&#34;,
  &#34;name&#34;: &#34;ShareAcknowledgeResponse&#34;,
  &#34;validVersions&#34;: &#34;0&#34;,
  &#34;flexibleVersions&#34;: &#34;0+&#34;,
  &#34;fields&#34;: [
    { &#34;name&#34;: &#34;ThrottleTimeMs&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true,
      &#34;about&#34;: &#34;The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.&#34; },
    { &#34;name&#34;: &#34;ErrorCode&#34;, &#34;type&#34;: &#34;int16&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true,
      &#34;about&#34;: &#34;The top level response error code.&#34; },
    { &#34;name&#34;: &#34;SessionId&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;default&#34;: &#34;0&#34;, &#34;ignorable&#34;: false,
      &#34;about&#34;: &#34;The share session ID.&#34; },
    { &#34;name&#34;: &#34;Responses&#34;, &#34;type&#34;: &#34;[]ShareAcknowledgeTopicResponse&#34;, &#34;versions&#34;: &#34;0+&#34;,
      &#34;about&#34;: &#34;The response topics.&#34;, &#34;fields&#34;: [
      { &#34;name&#34;: &#34;TopicId&#34;, &#34;type&#34;: &#34;uuid&#34;, &#34;versions&#34;: &#34;0+&#34;, &#34;ignorable&#34;: true, &#34;about&#34;: &#34;The unique topic ID&#34;},
      { &#34;name&#34;: &#34;Partitions&#34;, &#34;type&#34;: &#34;[]PartitionData&#34;, &#34;versions&#34;: &#34;0+&#34;,
        &#34;about&#34;: &#34;The topic partitions.&#34;, &#34;fields&#34;: [
        { &#34;name&#34;: &#34;PartitionIndex&#34;, &#34;type&#34;: &#34;int32&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;The partition index.&#34; },
        { &#34;name&#34;: &#34;ErrorCode&#34;, &#34;type&#34;: &#34;int16&#34;, &#34;versions&#34;: &#34;0+&#34;,
          &#34;about&#34;: &#34;The error code, or 0 if there was no error.&#34; }
      ]}
    ]}
  ]
}</pre>
</div></div><h2 id="KIP932:QueuesforKafka-Metrics">Metrics</h2><p>Further details to follow as the design progresses.</p><p>There are some obvious extensions to this idea which are not included in this KIP in order to keep the scope manageable.</p><p>This KIP introduces delivery counts and a maximum number of delivery attempts. An obvious future extension is the ability to copy records that failed to be delivered onto a dead-letter queue. This would of course give a way to handle poison messages without them permanently blocking processing.</p><p>A “browsing” consumer which does not modify the share group state or take acquisition locks could be supported which needs lesser permission ( <code>DESCRIBE</code> ) on the group than a proper consumer ( <code>READ</code> ). This is a little more complicated because it needs to have a position independent of the SPSO so that it can traverse along the queue.</p><p>The focus in this KIP is on sharing rather than ordering. The concept can be extended to give key-based ordering so that partial ordering and fine-grained sharing can be achieved at the same time.</p><p>Finally, this KIP does not include support for acknowledging delivery using transactions for exactly-once semantics. Conceptually, this is quite straightforward but would take changes to the API.</p><p>The changes in this KIP add to the capabilities of Kafka rather than changing existing behavior.</p><p>Detail to follow.</p><p>In this option, the regular <code>KafkaConsumer</code>  was used by consumers to consume records from a share group, using a configuration parameter <code>group.type</code>  to choose between using a share group or a consumer group. While this means that existing Kafka consumers can trivially make use of share groups, there are some obvious downsides:</p><ol><li>An application using <code>KafkaConsumer</code> with a consumer group could be switched to a share group with very different semantics with just a configuration change. There is almost no chance that the application would work correctly.</li><li>Libraries such as Kafka Connect which embed Kafka consumers while not work correctly with share groups without code changes beyond changing the configuration. As a result, there is a risk of breaking connectors due to misconfiguration using the <code>group.type</code><span>  configuration property.</span></li><li><span>More than half of the <code>KafkaConsumer</code>  methods do not make sense for share groups introducing a lot of unnecessary cruft.</span></li></ol><p>As a result, the KIP now proposes an entirely different class <code>KafkaShareConsumer</code>  which gives a very interface as <code>KafkaConsumer</code>  but eliminates the downsides listed above.</p>

                
        
    
        </div></div>
  </body>
</html>
