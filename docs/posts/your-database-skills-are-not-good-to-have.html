<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://renegadeotter.com/2023/11/12/your-database-skills-are-not-good-to-have.html">Original</a>
    <h1>Your database skills are not &#39;good to have&#39;</h1>
    
    <div id="readability-page-1" class="page"><div id="postBody">
    <h3 id="a-mysql-war-story">A MySQL war story</h3>

<p>It‚Äôs 2006, and the New York Magazine digital team set out to create a new search experience for its Fashion Week portal. It was 
one of
those projects where technical feasibility was not even discussed with the tech team - a common occurrence back then. Agile was
still new, let alone in publishing. It was just a vision, a real friggin‚Äô moonshot, and 10 to 12 weeks to develop 
the wireframed version of the product. There would be almost no time left for proper QA. Fashion Week does not start slowly but 
rather goes from zero to sixty in a blink.</p>

<p>The vision? Thousands of near-real-time fashion show images, each one with its sub-items categorized: ‚Äú2006‚Äù, ‚Äúbag‚Äù, ‚Äúred‚Äù, ‚Äú
leather‚Äù, and so on. A user will land on the search page and have 
the ability to ‚Äúdrill down‚Äù and narrow the results based on those properties. To make things <em>much</em> harder, all of these 
properties would come with exact counts.</p>

<p>The workflow was going to be intense. Photographers will courier their digital cartridges from downtown NYC to
our offices on Madison Avenue, where the images will be processed, tagged by interns, and then indexed every hour by our Perl
script, reading the tags from the embedded EXIF information. Failure to build the search product on our side would have collapsed
the entire ecosystem already in place, primed and ready to rumble.</p>



<p>‚ÄúOh! Just use the facets in Solr, dude‚Äù. Yeah, not so fast - <em>dude</em>. In 2006 that kind of technology didn‚Äôt even exist yet. I sat
through multiple enterprise search engine demos, and none of the products (which cost a LOT of money) could do a deep
faceted search. We already had an Autonomy license and my first try proved that‚Ä¶ it just couldn‚Äôt do it. It was supposed to be 
able to, but the counts were all wrong. Endeca (now owned by Oracle), 
<a href="https://www.digitalcommerce360.com/2006/03/31/endeca-unveils-the-endeca-information-access-platform-takes-aim/" target="_blank">came out of stealth</a>
when the design part of the project was already underway. Too new, too raw, too risky. The idea was just a little too 
ambitious for its time, especially for a tiny team in a non-tech company.</p>

<p>So here we were, a team of three, myself and two consultants, writing Perl for the indexing script, query-parsing logic, and
modeling the data - in MySQL 4. It was one of those projects where one single insurmountable technical risk would have sunk the
whole thing. I will cut the story short and spare you the excitement. We did it, and then we went out to celebrate at a karaoke
bar (where I got my very first work-stress-related severe hangover) ü§Æ</p>

<p>For someone who was in charge of the SQL model and queries, it was days and days of tuning those, timing every query and studying
the <code>EXPLAIN</code> output to see what else I could do to squeeze another 50ms out of the database. There were no free nights or
weekends. In the end, it was a combination of trial and error, digging deep into MySQL server settings, and crafting <code>GROUP BY</code>
queries that would make you nauseous. The MySQL query analyzer was fidgety back then, and sometimes re-arranging the fields in the
<code>SELECT</code> clause could change a query‚Äôs performance. Imagine if <code>SELECT field1, field2 FROM my_table</code> was faster than <code>SELECT 
field2, field1 FROM my_table</code>. Why would it do that? I have no idea to this day, and I don‚Äôt even want to know.</p>

<p>Unfortunately, I lost examples of this work, but the 
<a href="https://web.archive.org/web/20070519214259/http://nymag.com/fashion/fashionshows/2007/spring/main/newyork/womenrunway/yigalazrouel/" target="_blank">Way Back Machine</a>
has proof of our final product.</p>

<photo-element source="database-skills/fw-search-sm.jpg" aspect="original-size" link="database-skills/fw-search.png" credit="Early version of NYMAG Fashion Search (click to enlarge)" alt="NYMAG Fashion Week search">
</photo-element>

<p>The point here is - if you really know your database, you can do pretty crazy things with it, and with the modern generation of 
storage
technologies and beefier hardware, you don‚Äôt even need to push the limits - it should easily handle what I refer to as 
‚Äúcommon-scale‚Äù.</p>





<h3 id="the-fading-art-of-sql">The fading art of SQL</h3>

<p>In the past few years I have been noticing an unsettling trend - software engineers are eager to use exotic ‚Äúplanet-scale‚Äù
databases for pretty rudimentary problems, while at the same time not having a good grasp of the very powerful 
relational
database engine they are likely already using, let alone understanding the technology‚Äôs more advanced and useful capabilities. 
The SQL layer is buried so deep beneath libraries and too clever by a half ORMs that it all just becomes high-level 
code.</p>

<ul>
    <li>Why is it slow?</li>
    <li>No idea - <a href="https://medium.com/@liranbrimer/nice-to-meet-you-mondaydb-architecture-6d201b41e660" target="_blank">let&#39;s add Cassandra to it!</a></li>
</ul>

<p>Modern hardware certainly allows us to go way up from the CPU into the higher abstraction layers, while it wasn‚Äôt 
that uncommon in the past to convert certain functions to assembly code in order to squeeze every bit of performance out of the 
processor. Now compute and storage is cheaper - it‚Äôs true - but abusing this abundance has trained us
laziness and complacency. Suddenly, that Cloud bill is a wee too high, and heavens knows how 
much energy the world is burning by just running billions of auto-generated ‚ÄúSqueel‚Äù queries every second against mammoth 
database instances.</p>

<p>The morning of my first job interview in 2004, I was on a subway train memorizing 
the nine levels of database normalization. Or is it five levels? I don‚Äôt remember, and It doesn‚Äôt even matter - no one will ever
ask you this now in a software engineer interview.</p>

<p>Just skimming through the table of contents of your database of choice, say the now 
<a href="https://www.crunchydata.com/blog/when-did-postgres-become-cool" target="_blank">freshly in vogue Postgres</a>,
you will find an absolute treasure trove of features fit to handle everything but the most gruesome
planet-scale computer science problems. Petabyte-sized Postgres boxes, replicated, are effortlessly running now as you
are reading this.</p>

<p>The trick is to not expect your database or your ORM to read your mind. Speaking of‚Ä¶</p>

<h3 id="orms-are-not-magic">ORMs are not magic</h3>

<p>I was a new hire at an e-commerce outfit, and right off the bat I was thrown into fixing serious performance issues with the
company‚Äôs product catalog pages. Just a straight-forward, paginated grid of product images. How hard could it be? Believe it or
not - it be. The pages took over 10 seconds to load, sometimes longer, the database was struggling, and the solution was to ‚Äújust
cache it‚Äù. One last datapoint - this was not a high-traffic site. The pages were dead-slow even if there was no traffic at all. 
That‚Äôs a
rotten sign that something is seriously off.</p>

<p>After looking a bit closer, I realized that I hit the motherlode - all top three major database and coding mistakes in one.</p>

<h4 id="-mistake-1-there-is-no-index">‚ùå Mistake #1: There is no index</h4>

<p>The column that was hit in every single mission-critical query had no index. None. After adding the much-needed index in
production, you could practically hear MySQL exhaling in relief. Still, the performance was not quite there yet, so I
had to dig deeper, now in the code.</p>

<h4 id="-mistake-2-assuming-each-orm-call-is-free">‚ùå Mistake #2: Assuming each ORM call is free</h4>

<p>Activating the query logs locally and reloading a product listing page, I see‚Ä¶ 200, 300, 500 queries fired off just to load one
single page. What the shit? Turns out, this was the result of a classic ORM abuse of going through every record in a loop, to the 
effect of:</p>

<div><div><pre><code><span>for</span> <span>product_id</span> <span>in</span> <span>product_ids</span><span>:</span>
   <span>product</span> <span>=</span> <span>my_orm</span><span>.</span><span>products</span><span>.</span><span>get</span><span>(</span><span>id</span><span>=</span><span>product_id</span><span>)</span>
   <span>products</span><span>.</span><span>append</span><span>(</span><span>product</span><span>)</span>
</code></pre></div></div>

<p>The high number of queries was also due the fact that some of this logic was <em>nested</em>. The obvious solution is to keep the 
number of queries in each request to a minimum, using ORM capabilities to join and combine the data into one single blob. 
This is what relational databases do - <em>it‚Äôs in the name</em>.</p>

<p>What is happening above is that each separate query needs to travel to the database,
<a href="https://postgrespro.com/blog/pgsql/5969262#:~:text=Any%20query%20can%20be%20executed,rows%20that%20match%20your%20query" target="_blank">get parsed, transformed, analyzed, planned, executed</a>,
and then travel back to the caller. It is one of 
the most expensive operations you can do, and ORMs will happily do the worst possible thing for you in terms of 
performance. How does that ORM call translate to SQL? If it‚Äôs not what you think it should be, is it an ORM limitation or are you 
just not using the 
right library call? Is it a particular flavor of non-ANSI vendor SQL that your choice of ORM has a tough time with?
Do you ultimately need to drop into raw SQL for this call but not the others? And so on.</p>

<photo-element source="database-skills/55.png" aspect="original-size" alt="Don&#39;t do this"></photo-element>

<h4 id="-mistake-3-pulling-in-the-world">‚ùå Mistake #3: Pulling in the world</h4>

<p>To make matters worse, the amount of data here was relatively small, but there were dozens and dozens of columns. What do ORMs 
usually do by default in order to make your life ‚Äúeasier‚Äù? They send the whole thing, all the columns, clogging your network 
pipes with the data that you don‚Äôt even need. It is a form of <em>toxic technical debt</em>, where the speed of development will 
eventually start eating into performance.</p>

<p>I spent hours within the same project hacking the dark corners of the Dango admin, overriding default ORM queries to be less 
‚Äúeager‚Äù. This led 
to a much better office-facing experience.</p>

<h3 id="performance-is-a-feature">Performance IS a feature</h3>

<p>Serious, mission-critical systems have been running on classic and boring relational databases for decades, serving thousands of
requests per second. These systems have become more advanced, more capable, and more relevant. They are wonders of computer 
science, one can claim. You would think that an ancient database like Postgres (in development since 1982) is in some kind of
legacy maintenance mode at this point, but the opposite is true. In fact, the work has been only accelerating, with the scale
and 
<a href="https://www.se-radio.net/2019/04/se-radio-episode-362-simon-riggs-on-advanced-features-of-postgresql/" target="_blank">features becoming pretty impressive</a>.
What took multiple queries just a few years ago now takes a single one.</p>

<p>Why is this significant? It has been known for a long time, 
<a href="https://www.conductor.com/academy/page-speed-resources/faq/amazon-page-speed-study/" target="_blank">as discovered by Amazon</a>, 
that every additional 100ms of a user waiting for a page to load loses a business money. We also know now that from a user‚Äôs 
perspective, 
<a href="https://designingforperformance.com/performance-is-ux/#:~:text=A%20delay%20of%20less%20than,start%20to%20mentally%20context-switch" target="_blank">the maximum target response time for a web page is around 100 milliseconds</a>:</p>

<blockquote>
  <p>A delay of less than 100 milliseconds feels instant to a user, but a delay between 100 and 300 milliseconds is perceptible. 
A delay between 300 and 1,000 milliseconds makes the user feel like a machine is working, but if the delay is above 1,000 milliseconds, your user will likely start to mentally context-switch.</p>
</blockquote>

<p>The ‚Äújust add more CPU and RAM if it‚Äôs slow‚Äù approach may have worked for a while, but 
<a href="https://venturebeat.com/data-infrastructure/the-shift-to-the-cloud-could-be-costing-businesses-more-than-its-saving/" target="_blank">many are finding out the hard way</a>
that this kind of laziness is not sustainable in a frugal business environment where costs <em>matter</em>.</p>

<h3 id="database-anti-patterns">Database anti-patterns</h3>

<p>Knowing what <strong>not</strong> to do is as important as knowing what <strong>to</strong> do. Some of the below mistakes are all too common:</p>

<h4 id="-anti-pattern-1-using-exotic-databases-for-the-wrong-reasons">‚ùå Anti-pattern #1. Using exotic databases for the wrong reasons</h4>

<p>Technologies like DynamoDB are designed to handle scale at which Postgres and MySQL begin to fail. This is achieved by
denormalizing, duplicating the data aggressively, where the database is not doing much real-time data manipulation or joining.
Your data is now modeled after how it is queried, not after how it is <em>related</em>. Regular relational concepts disintegrate at this 
insane level of scale. Needless to say, if you are resorting to this kind of storage for ‚Äúcommon-scale‚Äù problems, you are already 
<a href="https://renegadeotter.com/2023/09/10/death-by-a-thousand-microservices.html" target="_blank">solving problems you don‚Äôt have</a>.</p>

<h4 id="-anti-pattern-2-caching-things-unnecessarily">‚ùå Anti-pattern #2. Caching things unnecessarily</h4>

<p>Caching is a necessary evil - <em>but it‚Äôs not always necessary</em>. There is an entire class of bugs and on-call issues that 
stem from stale cached data. Read-only database replicas are a classic architecture pattern that
is still very much not outdated, and it will buy you insane levels of performance before you have to worry about anything. It 
should 
not be a
surprise that mature relational databases already have query caching in place - it just
<a href="https://severalnines.com/blog/overview-caching-postgresql/" target="_blank">has to be tuned</a>
for your specific needs.</p>

<p>Cache invalidation is hard. It adds more complexity and states of
uncertainty to your system. It makes debugging more difficult. I received more emails from content teams than I care for
throughout my career that wondered ‚Äúwhy is the data not there, I updated it 30 minutes ago?!‚Äù</p>

<p><em>Caching should not act as a bandaid for bad architecture and non-performant code.</em></p>

<photo-element source="database-skills/two-problems.png" aspect="original-size" alt="Caching"></photo-element>

<h4 id="-anti-pattern-3-storing-everything-and-a-kitchen-sink">‚ùå Anti-pattern #3. Storing everything and a kitchen sink</h4>

<p>As much punishment as an industry-standard database can take, it‚Äôs probably not a good idea to not care at all about what‚Äôs going 
into 
it, treating it like a data landfill of sorts. Management, querying, backups, migrations - all becomes painful once the DB grows 
substantially. Even if that is of no concern as you are using a
managed cloud DB - the costs should be. An RDBMS is a sophisticated piece of technology, and storing data in it is <em>expensive</em>.</p>

<h3 id="figure-out-common-scale-first">Figure out common-scale first</h3>

<p>It is fairly easy to make a beefy Postgres or a MySQL database grind to a halt if you expect it to do magic without any extra 
work. <em>‚ÄúIt‚Äôs 
not web-scale, boss. Our 2 million records seem to be too much of a lift. We need DynamoDB, Kafka, and event
sourcing!‚Äù</em></p>

<p>A relational database is not some antiquated technology that only us tech fossils choose to be experts in, a thing that can be
waved off like an annoying insect. <em>‚ÄúHere we React and GraphQL all the things, old man‚Äù</em>. In legal speak, a modern RDBMS is
innocent until proven guilty, and the burden of proof should be extremely 
high - and almost entirely on you.</p>

<p>Finally, if I have to figure out ‚Äúwhy it‚Äôs slow‚Äù, my approximate runbook is:</p>

<ul>
  <li>Compile a list of unique queries, from logging, slow query log, etc.</li>
  <li>Look at the most frequent queries first</li>
  <li>Use <code>EXPLAIN</code> to check slow query plans for index usage</li>
  <li>Select only the data that needs to travel across the wire</li>
  <li>If an ORM is doing something silly without a workaround, pop the hood and get dirty with the raw SQL plumbing</li>
</ul>

<photo-element source="database-skills/merry-xmas.gif" aspect="landscape" alt="Merry Xmas!"></photo-element>

<p>Most importantly, study your database (and SQL). Learn it, love it, use it, abuse it. Spending a couple of days just leafing 
through that Postgres manual to see what it <em>can</em> do will probably make you a better engineer than spending more time on the next 
flavor-of-the-month JavaScript framework hotness. Again.</p>



<p><a href="https://renegadeotter.com/2023/07/26/i-am-not-your-cloud-person.html">I am not your Cloud person</a></p>

<h3 id="further-reading">Further reading</h3>

<p><a href="https://use-the-index-luke.com/" target="_blank">Use the index, Luke - SQL Indexing and Tuning e-Book</a></p>

<p><a href="https://wiki.postgresql.org/wiki/Don&#39;t_Do_This" target="_blank">Don‚Äôt do this - a Postgres WIKI</a></p>

</div></div>
  </body>
</html>
