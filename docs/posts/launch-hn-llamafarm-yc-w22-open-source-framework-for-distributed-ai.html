<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/llama-farm/llamafarm">Original</a>
    <h1>Launch HN: LlamaFarm (YC W22) ‚Äì Open-source framework for distributed AI</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<blockquote>
<p dir="auto">Build powerful AI locally, extend anywhere.</p>
</blockquote>
<p dir="auto"><a href="https://404wolf.com/llama-farm/llamafarm/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/1ee41f846b556b5ddea4bf3f6849a09bd2c2684c310cb381ffcc7e3233e1cc10/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6c6c616d612d6661726d2f6c6c616d616661726d" alt="License: Apache 2.0" data-canonical-src="https://img.shields.io/github/license/llama-farm/llamafarm"/></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/b1eed161475bfd46117275778eb1421141de0d18f3a35701fee741ffc95c2072/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d626c75652e737667" alt="Python 3.10+" data-canonical-src="https://img.shields.io/badge/python-3.10+-blue.svg"/></a>
<a href="https://go.dev/dl/" rel="nofollow"><img src="https://camo.githubusercontent.com/8bcb2b345d6a5f5a7f936a1f1be6eb4db05f78f1dda9692b34e666ca43dbe148/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f676f2d312e32342b2d3030414444382e737667" alt="Go 1.24+" data-canonical-src="https://img.shields.io/badge/go-1.24+-00ADD8.svg"/></a>
<a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/intro.md"><img src="https://camo.githubusercontent.com/75fbecd26307b4220874225d4b9ed7495ea54d2f038bc6734078ca67dca7a27a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d3443353142462e737667" alt="Docs" data-canonical-src="https://img.shields.io/badge/docs-latest-4C51BF.svg"/></a>
<a href="https://discord.gg/RrAUXTCVNF" rel="nofollow"><img src="https://camo.githubusercontent.com/1174f5e435d42a77ed3dbd641cd631ae6446b1e88d846f05a89032036382f4f8/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313339323839303432313737313839393032362e737667" alt="Discord" data-canonical-src="https://img.shields.io/discord/1392890421771899026.svg"/></a></p>
<p dir="auto">LlamaFarm is an open-source framework for building retrieval-augmented and agentic AI applications. It ships with opinionated defaults (Ollama for local models, Chroma for vector storage) while staying 100% extendable‚Äîswap in vLLM, remote OpenAI-compatible hosts, new parsers, or custom stores without rewriting your app.</p>
<ul dir="auto">
<li><strong>Local-first developer experience</strong> with a single CLI (<code>lf</code>) that manages projects, datasets, and chat sessions.</li>
<li><strong>Production-ready architecture</strong> that mirrors server endpoints and enforces schema-based configuration.</li>
<li><strong>Composable RAG pipelines</strong> you can tailor through YAML, not bespoke code.</li>
<li><strong>Extendable everything</strong>: runtimes, embedders, databases, extractors, and CLI tooling.</li>
</ul>
<p dir="auto"><strong>üì∫ Video demo (90 seconds):</strong> <a href="https://youtu.be/W7MHGyN0MdQ" rel="nofollow">https://youtu.be/W7MHGyN0MdQ</a></p>
<hr/>

<p dir="auto"><strong>Prerequisites:</strong></p>
<ul dir="auto">
<li><a href="https://www.docker.com/get-started/" rel="nofollow">Docker</a></li>
<li><a href="https://ollama.com/download" rel="nofollow">Ollama</a> <em>(local runtime; additional options coming soon)</em></li>
</ul>
<ol dir="auto">
<li>
<p dir="auto"><strong>Install the CLI</strong></p>
<p dir="auto">macOS / Linux</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash"><pre>curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh <span>|</span> bash</pre></div>
<p dir="auto">Windows (via winget)</p>
<div data-snippet-clipboard-copy-content="winget install LlamaFarm.CLI"><pre><code>winget install LlamaFarm.CLI
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Adjust Ollama context window</strong></p>
<ul dir="auto">
<li>Open the Ollama app, go to <strong>Settings ‚Üí Advanced</strong>, and set the context window to match production (e.g., 100K tokens).</li>
<li>Larger context windows improve RAG answers when long documents are ingested.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Create and run a project</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="lf init my-project            # Generates llamafarm.yaml using the server template
lf start                      # Spins up Docker services &amp; opens the dev chat UI"><pre>lf init my-project            <span><span>#</span> Generates llamafarm.yaml using the server template</span>
lf start                      <span><span>#</span> Spins up Docker services &amp; opens the dev chat UI</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Start an interactive project chat or send a one-off message</strong></p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Interactive project chat (auto-detects namespace/project from llamafarm.yaml)
lf chat

# One-off message
lf chat &#34;Hello, LlamaFarm!&#34;"><pre><span><span>#</span> Interactive project chat (auto-detects namespace/project from llamafarm.yaml)</span>
lf chat

<span><span>#</span> One-off message</span>
lf chat <span><span>&#34;</span>Hello, LlamaFarm!<span>&#34;</span></span></pre></div>
<p dir="auto">Need the full walkthrough with dataset ingestion and troubleshooting tips? Jump to the <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/quickstart/index.md">Quickstart guide</a>.</p>
<blockquote>
<p dir="auto">Prefer building from source? Clone the repo and follow the steps in <a href="#-development--testing">Development &amp; Testing</a>.</p>
</blockquote>
<p dir="auto"><strong>Run services manually (without Docker auto-start):</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/llama-farm/llamafarm.git
cd llamafarm

# Install Nx globally and bootstrap the workspace
npm install -g nx
nx init --useDotNxInstallation --interactive=false

# Option 1: start both server and RAG worker with one command
nx dev

# Option 2: start services in separate terminals
# Terminal 1
nx start rag
# Terminal 2
nx start server"><pre>git clone https://github.com/llama-farm/llamafarm.git
<span>cd</span> llamafarm

<span><span>#</span> Install Nx globally and bootstrap the workspace</span>
npm install -g nx
nx init --useDotNxInstallation --interactive=false

<span><span>#</span> Option 1: start both server and RAG worker with one command</span>
nx dev

<span><span>#</span> Option 2: start services in separate terminals</span>
<span><span>#</span> Terminal 1</span>
nx start rag
<span><span>#</span> Terminal 2</span>
nx start server</pre></div>
<p dir="auto">Open another terminal to run <code>lf</code> commands (installed or built from source). This is equivalent to what <code>lf start</code> orchestrates automatically.</p>
<hr/>

<ul dir="auto">
<li><strong>Own your stack</strong> ‚Äì Run small local models today and swap to hosted vLLM, Together, or custom APIs tomorrow by changing <code>llamafarm.yaml</code>.</li>
<li><strong>Battle-tested RAG</strong> ‚Äì Configure parsers, extractors, embedding strategies, and databases without touching orchestration code.</li>
<li><strong>Config over code</strong> ‚Äì Every project is defined by YAML schemas that are validated at runtime and easy to version control.</li>
<li><strong>Friendly CLI</strong> ‚Äì <code>lf</code> handles project bootstrapping, dataset lifecycle, RAG queries, and non-interactive chats.</li>
<li><strong>Built to extend</strong> ‚Äì Add a new provider or vector store by registering a backend and regenerating schema types.</li>
</ul>
<hr/>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Task</th>
<th>Command</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialize a project</td>
<td><code>lf init my-project</code></td>
<td>Creates <code>llamafarm.yaml</code> from server template.</td>
</tr>
<tr>
<td>Start dev stack + chat TUI</td>
<td><code>lf start</code></td>
<td>Spins up server, rag worker, monitors Ollama/vLLM.</td>
</tr>
<tr>
<td>Interactive project chat</td>
<td><code>lf chat</code></td>
<td>Opens TUI using project from <code>llamafarm.yaml</code>.</td>
</tr>
<tr>
<td>Send single prompt</td>
<td><code>lf chat &#34;Explain retrieval augmented generation&#34;</code></td>
<td>Uses RAG by default; add <code>--no-rag</code> for pure LLM.</td>
</tr>
<tr>
<td>Preview REST call</td>
<td><code>lf chat --curl &#34;What models are configured?&#34;</code></td>
<td>Prints sanitized <code>curl</code> command.</td>
</tr>
<tr>
<td>Create dataset</td>
<td><code>lf datasets create -s pdf_ingest -b main_db research-notes</code></td>
<td>Validates strategy/database against project config.</td>
</tr>
<tr>
<td>Upload files</td>
<td><code>lf datasets upload research-notes ./docs/*.pdf</code></td>
<td>Supports globs and directories.</td>
</tr>
<tr>
<td>Process dataset</td>
<td><code>lf datasets process research-notes</code></td>
<td>Streams heartbeat dots during long processing.</td>
</tr>
<tr>
<td>Semantic query</td>
<td><code>lf rag query --database main_db &#34;What did the 2024 FDA letters require?&#34;</code></td>
<td>Use <code>--filter</code>, <code>--include-metadata</code>, etc.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">See the <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/cli/index.md">CLI reference</a> for full command details and troubleshooting advice.</p>
<hr/>

<p dir="auto">LlamaFarm provides a comprehensive REST API (compatible with OpenAI&#39;s format) for integrating with your applications. The API runs at <code>http://localhost:8000</code>.</p>

<p dir="auto"><strong>Chat Completions</strong> (OpenAI-compatible)</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/chat/completions \
  -H &#34;Content-Type: application/json&#34; \
  -d &#39;{
    &#34;messages&#34;: [
      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What are the FDA requirements?&#34;}
    ],
    &#34;stream&#34;: false,
    &#34;rag_enabled&#34;: true,
    &#34;database&#34;: &#34;main_db&#34;
  }&#39;"><pre>curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/chat/completions \
  -H <span><span>&#34;</span>Content-Type: application/json<span>&#34;</span></span> \
  -d <span><span>&#39;</span>{</span>
<span>    &#34;messages&#34;: [</span>
<span>      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What are the FDA requirements?&#34;}</span>
<span>    ],</span>
<span>    &#34;stream&#34;: false,</span>
<span>    &#34;rag_enabled&#34;: true,</span>
<span>    &#34;database&#34;: &#34;main_db&#34;</span>
<span>  }<span>&#39;</span></span></pre></div>
<p dir="auto"><strong>RAG Query</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/rag/query \
  -H &#34;Content-Type: application/json&#34; \
  -d &#39;{
    &#34;query&#34;: &#34;clinical trial requirements&#34;,
    &#34;database&#34;: &#34;main_db&#34;,
    &#34;top_k&#34;: 5
  }&#39;"><pre>curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/rag/query \
  -H <span><span>&#34;</span>Content-Type: application/json<span>&#34;</span></span> \
  -d <span><span>&#39;</span>{</span>
<span>    &#34;query&#34;: &#34;clinical trial requirements&#34;,</span>
<span>    &#34;database&#34;: &#34;main_db&#34;,</span>
<span>    &#34;top_k&#34;: 5</span>
<span>  }<span>&#39;</span></span></pre></div>
<p dir="auto"><strong>Dataset Management</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Upload file
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/data \
  -F &#34;file=@document.pdf&#34;

# Process dataset
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/process"><pre><span><span>#</span> Upload file</span>
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/data \
  -F <span><span>&#34;</span>file=@document.pdf<span>&#34;</span></span>

<span><span>#</span> Process dataset</span>
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/process</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Finding Your Namespace and Project</h3><a id="user-content-finding-your-namespace-and-project" aria-label="Permalink: Finding Your Namespace and Project" href="#finding-your-namespace-and-project"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Check your <code>llamafarm.yaml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="name: my-project        # Your project name
namespace: my-org       # Your namespace"><pre><span>name</span>: <span>my-project        </span><span><span>#</span> Your project name</span>
<span>namespace</span>: <span>my-org       </span><span><span>#</span> Your namespace</span></pre></div>
<p dir="auto">Or inspect the file system: <code>~/.llamafarm/projects/{namespace}/{project}/</code></p>
<p dir="auto">See the complete <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/api/index.md">API Reference</a> for all endpoints, request/response formats, Python/TypeScript clients, and examples.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üóÇÔ∏è Configuration Snapshot</h2><a id="user-content-Ô∏è-configuration-snapshot" aria-label="Permalink: üóÇÔ∏è Configuration Snapshot" href="#Ô∏è-configuration-snapshot"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>llamafarm.yaml</code> is the source of truth for each project. The schema enforces required fields and documents every extension point.</p>
<div dir="auto" data-snippet-clipboard-copy-content="version: v1
name: fda-assistant
namespace: default

runtime:
  provider: openai                   # &#34;openai&#34; for any OpenAI-compatible host, &#34;ollama&#34; for local Ollama
  model: qwen2.5:7b
  base_url: http://localhost:8000/v1 # Point to vLLM, Together, etc.
  api_key: sk-local-placeholder
  instructor_mode: tools             # Optional: json, md_json, tools, etc.

prompts:
  - role: system
    content: &gt;-
      You are an FDA specialist. Answer using short paragraphs and cite document titles when available.

rag:
  databases:
    - name: main_db
      type: ChromaStore
      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: filtered_search
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text:latest
      retrieval_strategies:
        - name: filtered_search
          type: MetadataFilteredStrategy
          config:
            top_k: 5
  data_processing_strategies:
    - name: pdf_ingest
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1500
            chunk_overlap: 200
      extractors:
        - type: HeadingExtractor
        - type: ContentStatisticsExtractor

datasets:
  - name: research-notes
    data_processing_strategy: pdf_ingest
    database: main_db"><pre><span>version</span>: <span>v1</span>
<span>name</span>: <span>fda-assistant</span>
<span>namespace</span>: <span>default</span>

<span>runtime</span>:
  <span>provider</span>: <span>openai                   </span><span><span>#</span> &#34;openai&#34; for any OpenAI-compatible host, &#34;ollama&#34; for local Ollama</span>
  <span>model</span>: <span>qwen2.5:7b</span>
  <span>base_url</span>: <span>http://localhost:8000/v1 </span><span><span>#</span> Point to vLLM, Together, etc.</span>
  <span>api_key</span>: <span>sk-local-placeholder</span>
  <span>instructor_mode</span>: <span>tools             </span><span><span>#</span> Optional: json, md_json, tools, etc.</span>

<span>prompts</span>:
  - <span>role</span>: <span>system</span>
    <span>content</span>: <span>&gt;-</span>
<span>      You are an FDA specialist. Answer using short paragraphs and cite document titles when available.</span>
<span></span>
<span></span><span>rag</span>:
  <span>databases</span>:
    - <span>name</span>: <span>main_db</span>
      <span>type</span>: <span>ChromaStore</span>
      <span>default_embedding_strategy</span>: <span>default_embeddings</span>
      <span>default_retrieval_strategy</span>: <span>filtered_search</span>
      <span>embedding_strategies</span>:
        - <span>name</span>: <span>default_embeddings</span>
          <span>type</span>: <span>OllamaEmbedder</span>
          <span>config</span>:
            <span>model</span>: <span>nomic-embed-text:latest</span>
      <span>retrieval_strategies</span>:
        - <span>name</span>: <span>filtered_search</span>
          <span>type</span>: <span>MetadataFilteredStrategy</span>
          <span>config</span>:
            <span>top_k</span>: <span>5</span>
  <span>data_processing_strategies</span>:
    - <span>name</span>: <span>pdf_ingest</span>
      <span>parsers</span>:
        - <span>type</span>: <span>PDFParser_LlamaIndex</span>
          <span>config</span>:
            <span>chunk_size</span>: <span>1500</span>
            <span>chunk_overlap</span>: <span>200</span>
      <span>extractors</span>:
        - <span>type</span>: <span>HeadingExtractor</span>
        - <span>type</span>: <span>ContentStatisticsExtractor</span>

<span>datasets</span>:
  - <span>name</span>: <span>research-notes</span>
    <span>data_processing_strategy</span>: <span>pdf_ingest</span>
    <span>database</span>: <span>main_db</span></pre></div>
<p dir="auto">Configuration reference: <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/configuration/index.md">Configuration Guide</a> ‚Ä¢ <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/extending/index.md">Extending LlamaFarm</a></p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üß© Extensibility Highlights</h2><a id="user-content--extensibility-highlights" aria-label="Permalink: üß© Extensibility Highlights" href="#-extensibility-highlights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>Swap runtimes</strong> by pointing to any OpenAI-compatible endpoint (vLLM, Mistral, Anyscale). Update <code>runtime.provider</code>, <code>base_url</code>, and <code>api_key</code>; regenerate schema types if you add a new provider enum.</li>
<li><strong>Bring your own vector store</strong> by implementing a store backend, adding it to <code>rag/schema.yaml</code>, and updating the server service registry.</li>
<li><strong>Add parsers/extractors</strong> to support new file formats or metadata pipelines. Register implementations and extend the schema definitions.</li>
<li><strong>Extend the CLI</strong> with new Cobra commands under <code>cli/cmd</code>; the docs include guidance on adding dataset utilities or project tooling.</li>
</ul>
<p dir="auto">Check the <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/extending/index.md">Extending guide</a> for step-by-step instructions.</p>
<hr/>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Example</th>
<th>What it Shows</th>
<th>Location</th>
</tr>
</thead>
<tbody>
<tr>
<td>FDA Letters Assistant</td>
<td>Multi-document PDF ingestion, RAG queries, reference-style prompts</td>
<td><code>examples/fda_rag/</code> &amp; <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/examples/index.md#fda-letters-assistant">Docs</a></td>
</tr>
<tr>
<td>Raleigh UDO Planning Helper</td>
<td>Large ordinance ingestion, long-running processing tips, geospatial queries</td>
<td><code>examples/gov_rag/</code> &amp; <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/docs/website/docs/examples/index.md#raleigh-udo-planning-helper">Docs</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Run <code>lf datasets</code> and <code>lf rag query</code> commands from each example folder to reproduce the flows demonstrated in the docs.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üß™ Development &amp; Testing</h2><a id="user-content--development--testing" aria-label="Permalink: üß™ Development &amp; Testing" href="#-development--testing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Python server + RAG tests
cd server
uv sync
uv run --group test python -m pytest

# CLI tests
cd ../cli
go test ./...

# RAG tooling smoke tests
cd ../rag
uv sync
uv run python cli.py test

# Docs build (ensures navigation/link integrity)
cd ..
nx build docs"><pre><span><span>#</span> Python server + RAG tests</span>
<span>cd</span> server
uv sync
uv run --group <span>test</span> python -m pytest

<span><span>#</span> CLI tests</span>
<span>cd</span> ../cli
go <span>test</span> ./...

<span><span>#</span> RAG tooling smoke tests</span>
<span>cd</span> ../rag
uv sync
uv run python cli.py <span>test</span>

<span><span>#</span> Docs build (ensures navigation/link integrity)</span>
<span>cd</span> ..
nx build docs</pre></div>
<p dir="auto">Linting: <code>uv run ruff check --fix .</code> (Python), <code>go fmt ./...</code> and <code>go vet ./...</code> (Go).</p>
<hr/>

<ul dir="auto">
<li><a href="https://discord.gg/RrAUXTCVNF" rel="nofollow">Discord</a> ‚Äì chat with the team, share feedback, find collaborators.</li>
<li><a href="https://github.com/llama-farm/llamafarm/issues">GitHub Issues</a> ‚Äì bug reports and feature requests.</li>
<li><a href="https://github.com/llama-farm/llamafarm/discussions">Discussions</a> ‚Äì ideas, RFCs, roadmap proposals.</li>
<li><a href="https://404wolf.com/llama-farm/llamafarm/blob/main/CONTRIBUTING.md">Contributing Guide</a> ‚Äì code style, testing expectations, doc updates, schema regeneration steps.</li>
</ul>
<p dir="auto">Want to add a new provider, parser, or example? Start a discussion or open a draft PR‚Äîwe love extensions!</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üìÑ License &amp; Acknowledgments</h2><a id="user-content--license--acknowledgments" aria-label="Permalink: üìÑ License &amp; Acknowledgments" href="#-license--acknowledgments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Licensed under the <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/LICENSE">Apache 2.0 License</a>.</li>
<li>Built by the LlamaFarm community and inspired by the broader open-source AI ecosystem. See <a href="https://404wolf.com/llama-farm/llamafarm/blob/main/CREDITS.md">CREDITS</a> for detailed acknowledgments.</li>
</ul>
<hr/>
<p dir="auto">Build locally. Deploy anywhere. Own your AI.</p>
</article></div></div>
  </body>
</html>
