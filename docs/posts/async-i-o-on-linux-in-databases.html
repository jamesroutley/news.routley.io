<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.canoozie.net/async-i-o-on-linux-and-durability/">Original</a>
    <h1>Async I/O on Linux in databases</h1>
    
    <div id="readability-page-1" class="page"><section>
            <p>I&#39;ve been working on a complex multi-model database for a few weeks now, and recently I took time to simplify and test out an idea I had on a simple key-value database. I started with the basics: A hash table in memory, a simple append-only log for persistence and durability, and the classic fsync() call after every write to the log for durability.</p><p>It worked, but wasn&#39;t as fast as it could be.</p><p>In <a href="https://github.com/KevoDB/kevo?ref=blog.canoozie.net" rel="noreferrer">Kevo</a>, that&#39;s the approach I use, but in Klay (not public yet, but will be open sourced when ready), I&#39;m taking a different approach. What would a database look like if you treated the individual sectors on disk as unreliable, and how could you make it as fast as possible?</p><p>That&#39;s when I started reading about io_uring on Linux <a href="https://kernel.dk/io_uring.pdf?ref=blog.canoozie.net" rel="noreferrer">here (PDF)</a> and <a href="https://developers.mattermost.com/blog/hands-on-iouring-go/?ref=blog.canoozie.net" rel="noreferrer">here</a>.</p><h2 id="iouring-what">io_uring... what?</h2><p>You can read <a href="https://en.wikipedia.org/wiki/Io_uring?ref=blog.canoozie.net" rel="noreferrer">Wikipedia</a> as good as the next person, so let&#39;s skip ahead.</p><p>The promises seem to good to be true: truly async I/O for all types of operations, not just network sockets. No more thread pools to work around blocking disk I/O, no more complex state machines built around <code>epoll</code>... What&#39;s the catch?</p><p>Well, after doing some reading, the core insight behind io_uring clicked almost immediately. Traditional I/O interfaces force you to think synchronously--you make a system call, the kernel does work, you get a result. But modern storage hardware is inherently parallel. An NVMe SSD can handle thousands of operations simultaneously, and together, each with its own queue. The bottleneck isn&#39;t the hardware; it&#39;s the software abstraction.</p><p>io_uring exposes this parallelism through a pair of ring buffers shared between your application and the kernel. You submit operations to the submission queue (SQ) and collect results from the completion queue (CQ). Instead of one system call per operation, you can submit dozens of operations with a single <code>io_uring_submit</code> call.</p><p>My first io_uring experiment was simple: Replace my synchronous WAL writes with async ones. Instead of writing each log entry and waiting for completion, I would submit the write operation and continue processing. The results were dramatic--throughput increased by an order of magnitude almost immediately.</p><p>But then I started hitting consistency issues...</p><h2 id="durability">Durability</h2><p>The problem with naive async I/O in a database context at least, is that you lose the durability guarantee that makes databases useful. When a client receives a success response, their expectation is the data will survive a system crash. But with async I/O, by the time you send that response, the data might still be sitting in kernel buffers, not yet written to stable storage.</p><p>My initial solution was to track pending I/O operations and only return success after the corresponding completion arrived from io_uring. This worked, but it defeated the purpose--I was back to waiting for disk I/O before completing transactions.</p><p>Clearly, I need a better approach.</p><h2 id="rethinking-my-wal">Rethinking my WAL</h2><p>The traditional <a href="https://en.wikipedia.org/wiki/Write-ahead_logging?ref=blog.canoozie.net" rel="noreferrer">write-ahead log</a> (WAL) protocol is simple: log the change, force it to disk, then apply it. But what if we could separate the &#34;intent to change&#34; from the &#34;confirmation of the change&#34;? What if we could log our intentions quickly and asynchronously, then confirm completion separately?</p><p>That led me to a <a href="https://www.youtube.com/watch?v=tRgvaqpQPwE&amp;ref=blog.canoozie.net" rel="noreferrer">TigerBeetle talk</a> being given by Joran Dirk Greef. Turns out, TigerBeetle uses the same sort of approach. The more I learned about TigerBeetle, the more I had confidence in the approach. (Note: TigerBeetle doesn&#39;t externalize commits asynchronously though, see clarification in <a href="https://x.com/jorandirkgreef/status/1946200867163541708?ref=blog.canoozie.net" rel="noreferrer">this X post</a>.)</p><p>So I set out to experiment with a dual WAL design, in a simple in-memory key-value database, that used the dual WAL design:</p><ol><li><strong>Intent WAL</strong>: Records what operations I plan to perform</li><li><strong>Completion WAL</strong>: Records successful completion of these operations</li></ol><p>So the protocol ends up becoming:</p><ol><li>Write intent record (async)</li><li>Perform operation in memory</li><li>Write completion record (async)</li><li>Wait for the completion record to be written to the WAL</li><li>Return success to client</li></ol><p>During recovery, I only apply operations that have both intent and completion records. This ensures consistency while allowing much higher throughput.</p><p><strong>Update</strong>: <em>It&#39;s critical to note that while both WAL writes are submitted asynchronously, we must wait for the completion record to be durably written before responding to the client. This is tracked through io_uring&#39;s completion queue - we only send a success response after receiving confirmation that the completion record has been persisted to stable storage. Without this guarantee, we&#39;d violate durability expectations and risk data loss if the system crashes between sending the response and the actual disk write.</em></p><h2 id="building-the-dual-wal-system">Building the Dual WAL System</h2><p>Alright, I&#39;m also using Zig for this, since Klay is being written in Zig, I kept <a href="https://github.com/jeremytregunna/poro?ref=blog.canoozie.net" rel="noreferrer">Poro (GitHub)</a> using Zig as well to reduce the things I needed to keep in my head at once. In case it&#39;s not obvious, and it may not be, Poro is the experimental key-value database with the dual WAL system implemented as a demo.</p><p>Implementing this approach requires attention to several details. First, I need to separate io_uring instances--one for each WAL type. This prevents head-of-line blocking where completion writes might wait behind intent writes.</p><pre><code>pub const WAL = struct {
    intent_ring: io_uring,              // Dedicated ring for intent ops
    completion_ring: io_uring,          // Dedicated ring for completions
    intent_file_fd: std.posix.fd_t,
    completion_file_fd: std.posix.fd_t,
    intent_buffer: []u8,                // Circular buffer
    completion_buffer: []u8,            // Circular buffer
};</code></pre><p>The circular buffers are crucial for performance. Instead of writing individual entries to disk, I batch them into large buffers and flush only when they reach 75% capacity. This maximizes the benefits of io_uring&#39;s batching capabilities.</p><p>Each completion entry includes a checksum and references back to the corresponding intent entry:</p><pre><code>pub const CompletionEntry = struct {
    intent_offset: u32,  // Links back to intent entry
    timestamp: i64,      // Timestamp of completion record
    status: Status,      // Success, I/O error, or checksum error enum
    checksum: u32,       // CRC32 verification of key+value
};</code></pre><h3 id="the-recovery-process">The Recovery Process</h3><p>The recovery algorithm becomes more complex but much more robust:</p><ol><li>Read the entire intent log to see what operations were attempted</li><li>Read the entire completion log to see what operations completed successfully</li><li>Build a hash map linking intent entries to completion entries</li><li>Replay only the operations that have successful completion entries</li><li>Verify the checksums to ensure data integrity.</li></ol><p>This approach handles partial failures gracefully. If the system crashes between writing an intent record and its corresponding completion record, the operation is simply ignored during recovery--as if it never happened. In a networked database with replication, you can enhance this failure case by asking the cluster if any replica has the data, and if so, you can repair your version.</p><h3 id="addressing-latency-vs-batch-performance">Addressing latency vs batch performance</h3><p>The dual WAL design does introduce a latency cost for individual operations - instead of one synchronous write, we now have two writes that must complete before responding to the client. For single operations, this could theoretically double the latency.</p><p>However, the real performance win comes from batch processing. When multiple clients are writing concurrently, we can:</p><ul><li>Submit dozens of intent records in a single io_uring batch</li><li>Process all operations in memory while those writes are in flight</li><li>Submit all completion records as another batch</li><li>Wait for all completions together</li></ul><p>This batching transforms what would be 2N synchronous writes (for N operations) into just 2 io_uring submissions plus waiting for completions. The amortized cost per operation drops dramatically as batch size increases. In practice, under load, the system achieves a throughput improvement when it&#39;s processing many operations in parallel rather than serializing them one by one.</p><h3 id="performance-breakthrough">Performance Breakthrough</h3><p>The results exceeded by expectations. Benchmarks showed a 10x improvement in transaction throughput compared to my original synchronous implementation. More importantly, the system now scales with the number of CPU cores rather than being bottlenecked by disk I/O serialization.</p><p>io_uring&#39;s design aligns perfectly with the dual WAL approach. Each WAL can have its own ring buffer, preventing I/O contention. Operations can be batched and submitted together, reducing system call overhead. Finally, the completion queue provides precise information about which operations have finished, the result of that completion, enabling more sophisticated recovery logic than &#34;uhh error, throw this and everything after away.&#34;</p><h2 id="what-i-learned">What I learned</h2><p>Working through this implementation taught me several important lessons:</p><p><strong>Hardware parallelism matters</strong>: Modern storage devices can handle thousands of concurrent operations. Traditional I/O interfaces hide this parallelism behind synchronous abstractions.</p><p><strong>Batching is critical</strong>: The overhead of individual I/O operations is significant. Batching multiple operations together provides a major performance improvements.</p><p><strong>Consistency models are flexible</strong>: By separating intent from completion, we can maintain strong consistency guarantees while achieving much higher performance.</p><p><strong>Recovery can be sophisticated</strong>: More complex recovery algorithms enable simpler runtime protocols. So the effort invested in recovery logic pays dividends in operational performance.</p><h2 id="the-broader-impact">The Broader Impact</h2><p>This experiment changed how I think about database architecture. When I/O becomes cheap and parallel, many traditional design decisions need to be reconsidered. Buffer pool management, transaction scheduling, and concurrency control all benefit from rethinking around async I/O primitives.</p><p>Sometimes, the best optimizations come from questioning how we&#39;ve done things in the past. In this case, the assumption that I/O must be synchronous to have durable database storage, turned out to be wrong. The hardware was always parallel--we just needed software architectures that could take advantage of it.</p>
        </section></div>
  </body>
</html>
