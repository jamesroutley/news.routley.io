<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/AkiyoshiKitaoka/status/1642871991932944385">Original</a>
    <h1>Sugihara&#39;s Dog</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><span>I signed the </span><a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/" rel="">letter calling for a six-month pause in training AI models more powerful than GPT-4</a><span>. Not because I think six months is enough time to solve all problems (it isn’t), or because a magic recipe for AI alignment is near (nope), but because the letter pushes for “robust AI governance systems” that I support, and a six-month pause might open up space for cooperation, attention, and regulation. </span></p><p>Here are parts of the letter that I feel especially positive about: </p><ul><li><p>Standing up new/capable regulatory authorities; </p></li><li><p><span>Allowing time for AI controls to (attempt to) catch up with AI innovation</span></p><span>; </span></li><li><p>Implementing independent review, auditing, and certification; </p></li><li><p>Increasing funding for AI safety research (another way of helping controls catch up); </p></li><li><p>Working with policymakers instead of viewing government as the enemy, since government involvement in a technology as powerful as AI is inevitable. </p></li></ul><p>These would be steps toward shaping the industry’s future proactively and constructively. Rushing headlong past innovation milestones, like we did with the internet, won’t work—because moving fast and breaking things is insanely risky at the scale and speed of AI. </p><p>To engage a metaphor that may be useful: </p><ul><li><p>With the internet, we were running on fresh snow, and sometimes we slipped and fell. </p></li><li><p><span>With AI, we’re running on fresh snow that may hide a series of crevasses: thresholds that may appear subtle or almost unnoticeable but are vitally important and should be approached with care, caution, and appreciation for their magnitude. We can’t afford to rush across the snowfield without understanding the terrain. We don’t need to run the </span><a href="https://iditarod.com/" rel="">Iditarod</a><span> on a glacial field studded with ravines. </span></p></li></ul><p>A six-month pause would allow industry participants to better understand the current terrain and make sure they’re standing on stable ground before moving forward with more thoughtful intention. </p><p><strong>Cooperation, attention, regulation</strong></p><p>A six-month pause also might open the door for more cooperation, draw attention to the possibility of slowing down AI development at key moments, and set the stage for regulation. More on these:</p><ol><li><p><strong>Cooperation.</strong><span> If people who hold widely varying views on AI can agree on a six-month pause, it opens the door to future discussions and future agreements. We don’t know what forms those might take, because conditions always change, but setting a precedent for cooperation among different parties with different viewpoints is incredibly valuable. </span></p></li><li><p><strong>Attention.</strong><span> The more people hear about this debate and realize AI progress is not a foregone conclusion of a breakneck race in which they are powerless, the more likely it is that AI innovation will slow down at appropriate moments </span><em>in the future</em><span>. Simply having the debate opens up the possibility space (aka </span><a href="https://en.wikipedia.org/wiki/Overton_window" rel="">Overton Window</a><span>) for a time when slowing down might be even </span><em>more </em><span>important than it is now. I’ll come back to attention (essentially public relations) in a future post. </span></p></li><li><p><strong>Regulation.</strong><span> By mandating standards, regulation can curtail or even banish the worst actors from an industry. It doesn’t and can’t prevent all problems, but it can set baselines and boundaries for behavior. </span><a href="https://www.investopedia.com/terms/w/wildcat-banking.asp" rel="">Wildcat banks</a><span> aren’t a thing anymore because of financial regulation. I can trust that an aspirin won’t poison me because of pharmaceutical regulation. Is there still bad behavior and bad incentives? Yes. But are regulated systems more trustworthy overall? Also yes. </span></p></li></ol><p><strong>System dynamics of regulation</strong></p><p>Here’s a general system dynamics diagram for regulation: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg" width="800" height="519" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/dd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:519,&#34;width&#34;:800,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:85577,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/jpeg&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd323ba0-f3be-4112-85b0-cb51d1628e79_800x519.jpeg 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>For a step-by-step introduction to causal loop diagrams, read </span><a href="https://riskmusings.substack.com/p/a-system-dynamics-primer-drawing" rel="">my primer on drawing causal loops</a><span>. </span></p><p><span>To read this particular regulation diagram, first assess each pair of components in isolation (by convention, start these statements with the first component in each pair </span><em>increasing</em><span>). There are many pairs in the diagram, so I’ll list a few and then challenge you to complete the list yourself (if you want to see my full list, email me at riskmusings at substack and I’ll be happy to share it): </span></p><ul><li><p>If controls increase, the pace of innovation will likely decrease (an inverse relationship, denoted with a - sign).</p></li><li><p>If the pace of innovation increases, residual risk will likely increase (a direct relationship, denoted with a + sign).</p></li><li><p>If residual risk increases, incidents will likely increase (a direct relationship).</p></li><li><p>If incidents increase, the perception of risk (fear) will likely increase (a direct relationship). </p></li><li><p>If the perception of risk (fear) increases, support for regulation will likely increase (a direct relationship).</p></li></ul><p>That’s pretty much where we are now: the pace of AI innovation increased, which increased actual residual risk, which increased incidents (such as the rough initial Bing Chat rollout), which increased the perception of risk (fear), which has now increased support for regulation. If we continue to travel around the loop, increased support for regulation will eventually translate into actual regulation. </p><p><strong>It’s vital that regulation be well-crafted</strong></p><p><span>Here’s where it gets tricky: regulation can take different forms. There’s well-crafted regulation, and there’s poorly crafted regulation.</span></p><p><span> Both types of regulation increase controls, but poorly crafted regulation </span><em>also</em><span> has side effects that can undermine support for regulation and ultimately decrease regulators’ power. </span></p><p><span>One side effect is surface-level compliance, which occurs when regulated companies do the bare minimum to comply with poorly designed regulations.</span></p><p><span> The outcome is wasted time, money, and effort spent on unnecessary staff, unnecessary policies and procedures, unnecessary meetings, and unnecessary paperwork. This waste reduces support for regulation and, if perception sours enough, regulators can lose influence or funding. </span></p><p><span>In the worst cases, poorly crafted regulations aren’t just inefficient, </span><em>they</em><span> </span><em>don’t actually reduce the targeted risk at all. </em><span>When true failure of regulation becomes obvious as risk manifests, support for regulation understandably declines, and regulators may lose influence or funding (deservedly so). </span></p><p><em>The key is to minimize poorly crafted regulation and address the root causes of risks through well-crafted regulation.</em><span> </span></p><p><strong>What makes regulation well-crafted?</strong></p><p><span>In </span><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel="">an essay in October 2022</a><span>, I listed the following qualities of what I called “smart regulation”: </span></p><ul><li><p>It is targeted. </p></li><li><p>It engages the governed, listening to their concerns and gaining their trust as a governed party to ensure buy-in at a strategic level instead of tactical compliance in a check-the-box exercise. </p></li><li><p>It is not make-work and is reasonably time-efficient, allowing sufficient flexibility in implementation for different types of businesses and organizations. </p></li><li><p><span>Ideally, it makes required </span><em>something industry participants mostly wanted to do anyway but couldn’t</em><span> since doing so would have put them at a perceived or actual market disadvantage. In essence, regulation in this situation is a key for overcoming a prisoner’s dilemma.</span></p></li><li><p>It helps ensure safety for stakeholders that have insufficient leverage to negotiate with more powerful stakeholders on their own (consumer protection regulation is an example). </p></li><li><p>It is forward-looking, with consideration of how processes and systems are evolving and how the regulation might apply to their anticipated future states.  </p></li><li><p>It costs businesses and societies less than the risk it protects against. </p></li><li><p>Its second-order effects are manageable and do not undermine the intent of the regulation (e.g., by driving activity outside of the regulatory jurisdiction en masse without actually reducing risk in the system as a whole).</p></li></ul><p>I stand by this list and would add that, for AI regulation, it’s important to source regulators not just from the narrow field of machine learning or from a single philosophical tradition, but from a variety of backgrounds, such as: </p><ul><li><p>Machine learning and software engineering practitioners and researchers</p></li><li><p>Ethicists and lawyers</p></li><li><p>Operational risk experts who understand information security and change management, among other areas</p></li><li><p>Forecasting experts </p></li><li><p>Former regulators from other industries like energy, aerospace, environment, medicine, and finance, who understand regulatory processes and how to set them up in a streamlined but effective way</p></li><li><p>Former policymakers </p></li></ul><p><span>It’s also important to select regulators who won’t be lured away by the first AI company that will hire them. That means filtering for mission orientation and a mindset of service to all</span><em> </em><span>the humans who will be affected by AI developments, and compensating for a lack of equity shares with generous salaries, benefits, work-life balance, and appealing career paths.</span></p><p><strong>Summing it up</strong></p><p>In essence, to bring it back to the letter: </p><ul><li><p>Don’t let the perfect be the enemy of the good. </p></li><li><p>The letter isn’t perfect, but AI controls need to keep pace with AI innovation, so I signed. </p></li><li><p>It’s a decent start toward cooperation, brings attention to the problem, and puts regulation on the table. </p></li></ul><p>Although regulation brings its own challenges, it’s likely a leverage point to help controls catch up with innovation. That makes it worth doing—especially if it’s done right.</p></div></div></div>
  </body>
</html>
