<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://geohot.github.io//blog/jekyll/update/2025/12/29/five-years-of-tinygrad.html">Original</a>
    <h1>Five Years of Tinygrad</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>The first commit to <a href="https://github.com/tinygrad/tinygrad">tinygrad</a> was October 17, 2020. It’s been almost three years <a href="https://geohot.github.io/blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html">since we raised money</a>. The company is 6 people now. The codebase is 18,935 lines not including tests.</p>

<p>I have spent 5 years of my life working on 18,935 lines, and now many others have put years in as well. And there’s probably 5 more years to go. But this is the right process to compete with NVIDIA.</p>

<p>Only a fool begins by taping out a chip; it’s expensive and not the hard part. Once you have a fully sovereign software stack capable of training SOTA models, the chip is so easy. Note that AMD, Amazon, Tesla, and Groq have taped out fine chips, but only Google and NVIDIA chips have ever been seriously used for training. Because they have the software.</p>

<p>We are finally beginning to <a href="https://github.com/tinygrad/tinygrad/tree/master/extra/assembly/amd">tackle</a> LLVM removal, making tinygrad have 0 dependencies (except pure Python) to drive AMD GPUs. We have a frontend, we have a graph compiler, we have runtimes, and we have drivers. This is no longer a toy project, it outperforms PyTorch on many workloads. When this is finished and cleaned up, it’ll be about 20,000 lines. And that’s completely it.</p>

<hr/>


<p>I think a lot of how software is thought about is wrong. All codebases have workarounds for issues in other parts of the codebase. Sometimes you are lucky and these workarounds are clear, but many times they are so deep and structural that you’ll never see them reading the code line by line. I think this is so bad that 98% of lines of software are basically this in some way shape or form. tinygrad is following the Elon process for software.</p>

<blockquote>
  <p>Make the requirements less dumb. The best part is no part.</p>
</blockquote>

<p>Most requirements in software exist to maintain compatibility with other abstractions. Let’s look at an LLM server. The real requirement is that it provide an OpenAI compatible API for quickly running LLMs on GPUs. But when you look at these codebases and what they depend on, they are collectively millions and millions of lines. tinygrad is 1000x smaller.</p>

<p>It’s because each piece of code in that stack isn’t focused on the goal. It’s focused on the other pieces of code. I believe this is the same dysfunction that exists in organizations too.</p>

<hr/>


<p>The tiny corp is a company, but like how fancy chefs will deconstruct dishes, the tiny corp is a deconstructed company. We have almost nothing private, it’s a Discord and GitHub. To fund the operation, we have a <a href="https://tinygrad.org/#tinybox">computer sales division</a> that makes about $2M revenue a year.</p>

<p>We also <a href="https://x.com/__tinygrad__/status/1935364905949110532">have a contract with AMD</a> to get MI350X on MLPerf for Llama 405B training. This was negotiated mostly in public on Twitter.</p>

<p>People get hired by contributing to the repo. It’s a very self directed job, with one meeting a week and a goal of making tinygrad better.</p>

<p>Our mission is to <strong>commoditize the petaflop</strong>.</p>

  </div>
</article>

      </div>
    </div></div>
  </body>
</html>
