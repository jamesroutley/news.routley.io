<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/DachunKai/EvTexture">Original</a>
    <h1>Texture Enhancement for Video Super-Resolution</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Official Pytorch implementation for the &#34;EvTexture: Event-driven Texture Enhancement for Video Super-Resolution&#34; paper (ICML 2024).</p>
<p dir="auto">
    ğŸŒ <a href="https://dachunkai.github.io/evtexture.github.io/" rel="nofollow">Project</a> | ğŸ“ƒ <a href="https://arxiv.org/abs/2406.13457" rel="nofollow">Paper</a> | ğŸ–¼ï¸ <a href="https://docs.google.com/presentation/d/1nbDb39TFb374DzBwdz5v20kIREUA0nBH/edit?usp=sharing" rel="nofollow">Poster</a> <br/>
</p>
<p dir="auto"><strong>Authors</strong>: <a href="https://github.com/DachunKai/">Dachun Kai</a><sup><a href="mailto:dachunkai@mail.ustc.edu.cn">ğŸ“§ï¸</a></sup>, Jiayao Lu, <a href="https://scholar.google.com.hk/citations?user=LatWlFAAAAAJ&amp;hl=zh-CN&amp;oi=ao" rel="nofollow">Yueyi Zhang</a><sup><a href="mailto:zhyuey@ustc.edu.cn">ğŸ“§ï¸</a></sup>, <a href="https://scholar.google.com/citations?user=VRG3dw4AAAAJ&amp;hl=zh-CN" rel="nofollow">Xiaoyan Sun</a>, <em>University of Science and Technology of China</em></p>
<p dir="auto"><strong>Feel free to ask questions. If our work helps, please don&#39;t hesitate to give us a â­!</strong></p>

<ul>
<li> Release training code</li>
<li> 2024/06/28: Release details to prepare datasets</li>
<li> 2024/06/08: Publish docker image</li>
<li> 2024/06/08: Release pretrained models and test sets for quick testing</li>
<li> 2024/06/07: Video demos released</li>
<li> 2024/05/25: Initialize the repository</li>
<li> 2024/05/02: ğŸ‰ ğŸ‰ Our paper was accepted in ICML&#39;2024</li>
</ul>

<ol dir="auto">
<li><a href="#video-demos">Video Demos</a></li>
<li><a href="#code">Code</a></li>
<li><a href="#citation">Citation</a></li>
<li><a href="#contact">Contact</a></li>
<li><a href="#license-and-acknowledgement">License and Acknowledgement</a></li>
</ol>

<p dir="auto">A <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="7c0eb925ceaff36fe8c57599b280a491">$4\times$</math-renderer> upsampling results on the <a href="https://paperswithcode.com/sota/video-super-resolution-on-vid4-4x-upscaling" rel="nofollow">Vid4</a> and <a href="https://paperswithcode.com/dataset/reds" rel="nofollow">REDS4</a> test sets.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description Vid4_City.mp4">Vid4_City.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/66354783/337345732-fcf48952-ea48-491c-a4fb-002bb2d04ad3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDU3MzItZmNmNDg5NTItZWE0OC00OTFjLWE0ZmItMDAyYmIyZDA0YWQzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1OTA0YzY4YWVhZjA2YjY1MGU4MDcxZjVjYjI4YzBkMTFiNTFjNTFlZjQ0MGNjZWViNDEyYTg5MWZiN2IzNTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0._Kmy14vFh_vT1Clr4w6NVgijle_k8pS1wfrEhjYFqcs" data-canonical-src="https://private-user-images.githubusercontent.com/66354783/337345732-fcf48952-ea48-491c-a4fb-002bb2d04ad3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDU3MzItZmNmNDg5NTItZWE0OC00OTFjLWE0ZmItMDAyYmIyZDA0YWQzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1OTA0YzY4YWVhZjA2YjY1MGU4MDcxZjVjYjI4YzBkMTFiNTFjNTFlZjQ0MGNjZWViNDEyYTg5MWZiN2IzNTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0._Kmy14vFh_vT1Clr4w6NVgijle_k8pS1wfrEhjYFqcs" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description Vid4_Foliage.mp4">Vid4_Foliage.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/66354783/337345971-ea3dd475-ba8f-411f-883d-385a5fdf7ff6.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDU5NzEtZWEzZGQ0NzUtYmE4Zi00MTFmLTg4M2QtMzg1YTVmZGY3ZmY2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI5OTI1NDkzMzg1M2I5NWFmN2E1N2UwYTA2NGJkZmQ0MzkxNmYyZjQxM2E5YjJiNTAwMjFmODQ5NmNkODg1MTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.cDK5WbZ_CkS6aFliaaBe0Re2HCkw9D_zPz_HVlKCy0M" data-canonical-src="https://private-user-images.githubusercontent.com/66354783/337345971-ea3dd475-ba8f-411f-883d-385a5fdf7ff6.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDU5NzEtZWEzZGQ0NzUtYmE4Zi00MTFmLTg4M2QtMzg1YTVmZGY3ZmY2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI5OTI1NDkzMzg1M2I5NWFmN2E1N2UwYTA2NGJkZmQ0MzkxNmYyZjQxM2E5YjJiNTAwMjFmODQ5NmNkODg1MTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.cDK5WbZ_CkS6aFliaaBe0Re2HCkw9D_zPz_HVlKCy0M" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description REDS_000.mp4">REDS_000.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/66354783/337346085-e1e6b340-64b3-4d94-90ee-54f025f255fb.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDYwODUtZTFlNmIzNDAtNjRiMy00ZDk0LTkwZWUtNTRmMDI1ZjI1NWZiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg2YjU5MmRjOTc1YTBlMTJhMDdiN2FmY2YzMzZlYzJiMTQ0OTMxY2MxNDUxZTVlOTk0NjcwN2Q2Zjg4Y2YwZWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.r-x_TYi_winXtd1Uh3y5i8vE_niRo-gi-Nzs3LYYG1I" data-canonical-src="https://private-user-images.githubusercontent.com/66354783/337346085-e1e6b340-64b3-4d94-90ee-54f025f255fb.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDYwODUtZTFlNmIzNDAtNjRiMy00ZDk0LTkwZWUtNTRmMDI1ZjI1NWZiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg2YjU5MmRjOTc1YTBlMTJhMDdiN2FmY2YzMzZlYzJiMTQ0OTMxY2MxNDUxZTVlOTk0NjcwN2Q2Zjg4Y2YwZWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.r-x_TYi_winXtd1Uh3y5i8vE_niRo-gi-Nzs3LYYG1I" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description REDS_011.mp4">REDS_011.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/66354783/337346183-01880c40-147b-4c02-8789-ced0c1bff9c4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDYxODMtMDE4ODBjNDAtMTQ3Yi00YzAyLTg3ODktY2VkMGMxYmZmOWM0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg0MThiZGU2OGRiZWVhNWI0ZDE4ODk2MWZkN2ViMzgzMmEyOGYwMjcwNjg2MGNiYmM4NmJlY2YwNzM2OGY5Y2EmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.rJWhTGbB-PdSmsC1RfUgIwss1777zsZBHtLiaGYRvkE" data-canonical-src="https://private-user-images.githubusercontent.com/66354783/337346183-01880c40-147b-4c02-8789-ced0c1bff9c4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk2ODg1NjUsIm5iZiI6MTcxOTY4ODI2NSwicGF0aCI6Ii82NjM1NDc4My8zMzczNDYxODMtMDE4ODBjNDAtMTQ3Yi00YzAyLTg3ODktY2VkMGMxYmZmOWM0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjI5VDE5MTEwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg0MThiZGU2OGRiZWVhNWI0ZDE4ODk2MWZkN2ViMzgzMmEyOGYwMjcwNjg2MGNiYmM4NmJlY2YwNzM2OGY5Y2EmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.rJWhTGbB-PdSmsC1RfUgIwss1777zsZBHtLiaGYRvkE" controls="controls" muted="muted">

  </video>
</details>



<ul dir="auto">
<li>
<p dir="auto">Dependencies: <a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow">Miniconda</a>, <a href="https://developer.nvidia.com/cuda-11.1.1-download-archive" rel="nofollow">CUDA Toolkit 11.1.1</a>, <a href="https://download.pytorch.org/whl/cu111/torch-1.10.2%2Bcu111-cp37-cp37m-linux_x86_64.whl" rel="nofollow">torch 1.10.2+cu111</a>, and <a href="https://download.pytorch.org/whl/cu111/torchvision-0.11.3%2Bcu111-cp37-cp37m-linux_x86_64.whl" rel="nofollow">torchvision 0.11.3+cu111</a>.</p>
</li>
<li>
<p dir="auto">Run in Conda</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -y -n evtexture python=3.7
conda activate evtexture
pip install torch-1.10.2+cu111-cp37-cp37m-linux_x86_64.whl
pip install torchvision-0.11.3+cu111-cp37-cp37m-linux_x86_64.whl
git clone https://github.com/DachunKai/EvTexture.git
cd EvTexture &amp;&amp; pip install -r requirements.txt &amp;&amp; python setup.py develop"><pre>conda create -y -n evtexture python=3.7
conda activate evtexture
pip install torch-1.10.2+cu111-cp37-cp37m-linux_x86_64.whl
pip install torchvision-0.11.3+cu111-cp37-cp37m-linux_x86_64.whl
git clone https://github.com/DachunKai/EvTexture.git
<span>cd</span> EvTexture <span>&amp;&amp;</span> pip install -r requirements.txt <span>&amp;&amp;</span> python setup.py develop</pre></div>
</li>
<li>
<p dir="auto">Run in Docker ğŸ‘</p>
<p dir="auto">Note: before running the Docker image, make sure to install nvidia-docker by following the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">official instructions</a>.</p>
<p dir="auto">[Option 1] Directly pull the published Docker image we have provided from <a href="https://cr.console.aliyun.com/cn-hangzhou/instances" rel="nofollow">Alibaba Cloud</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pull registry.cn-hangzhou.aliyuncs.com/dachunkai/evtexture:latest"><pre>docker pull registry.cn-hangzhou.aliyuncs.com/dachunkai/evtexture:latest</pre></div>
<p dir="auto">[Option 2] We also provide a <a href="https://github.com/DachunKai/EvTexture/blob/main/docker/Dockerfile">Dockerfile</a> that you can use to build the image yourself.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd EvTexture &amp;&amp; docker build -t evtexture ./docker"><pre><span>cd</span> EvTexture <span>&amp;&amp;</span> docker build -t evtexture ./docker</pre></div>
<p dir="auto">The pulled or self-built Docker image containes a complete conda environment named <code>evtexture</code>. After running the image, you can mount your data and operate within this environment.</p>
<div dir="auto" data-snippet-clipboard-copy-content="source activate evtexture &amp;&amp; cd EvTexture &amp;&amp; python setup.py develop"><pre><span>source</span> activate evtexture <span>&amp;&amp;</span> <span>cd</span> EvTexture <span>&amp;&amp;</span> python setup.py develop</pre></div>
</li>
</ul>

<ol dir="auto">
<li>
<p dir="auto">Download the pretrained models from (<a href="https://github.com/DachunKai/EvTexture/releases">Releases</a> / <a href="https://1drv.ms/f/c/2d90e71fb9eb254f/EnMm8c2mP_FPv6lwt1jy01YB6bQhoPQ25vtzAhycYisERw?e=DiI2Ab" rel="nofollow">Onedrive</a> / <a href="https://drive.google.com/drive/folders/1oqOAZbroYW-yfyzIbLYPMJ2ZQmaaCXKy?usp=sharing" rel="nofollow">Google Drive</a> / <a href="https://pan.baidu.com/s/161bfWZGVH1UBCCka93ImqQ?pwd=n8hg" rel="nofollow">Baidu Cloud</a>(n8hg)) and place them to <code>experiments/pretrained_models/EvTexture/</code>. The network architecture code is in <a href="https://github.com/DachunKai/EvTexture/blob/main/basicsr/archs/evtexture_arch.py">evtexture_arch.py</a>.</p>
<ul dir="auto">
<li>
<em>EvTexture_REDS_BIx4.pth</em>: trained on REDS dataset with BI degradation for <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="7c0eb925ceaff36fe8c57599b280a491">$4\times$</math-renderer> SR scale.</li>
<li>
<em>EvTexture_Vimeo90K_BIx4.pth</em>: trained on Vimeo-90K dataset with BI degradation for <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="7c0eb925ceaff36fe8c57599b280a491">$4\times$</math-renderer> SR scale.</li>
</ul>
</li>
<li>
<p dir="auto">Download the preprocessed test sets (including events) for REDS4 and Vid4 from (<a href="https://github.com/DachunKai/EvTexture/releases">Releases</a> / <a href="https://1drv.ms/f/c/2d90e71fb9eb254f/EnMm8c2mP_FPv6lwt1jy01YB6bQhoPQ25vtzAhycYisERw?e=DiI2Ab" rel="nofollow">Onedrive</a> / <a href="https://drive.google.com/drive/folders/1oqOAZbroYW-yfyzIbLYPMJ2ZQmaaCXKy?usp=sharing" rel="nofollow">Google Drive</a> / <a href="https://pan.baidu.com/s/161bfWZGVH1UBCCka93ImqQ?pwd=n8hg" rel="nofollow">Baidu Cloud</a>(n8hg)), and place them to <code>datasets/</code>.</p>
<ul dir="auto">
<li>
<p dir="auto"><em>Vid4_h5</em>: HDF5 files containing preprocessed test datasets for Vid4.</p>
</li>
<li>
<p dir="auto"><em>REDS4_h5</em>: HDF5 files containing preprocessed test datasets for REDS4.</p>
</li>
</ul>
</li>
<li>
<p dir="auto">Run the following command:</p>
<ul dir="auto">
<li>Test on Vid4 for 4x VSR:
<div data-snippet-clipboard-copy-content="./scripts/dist_test.sh [num_gpus] options/test/EvTexture/test_EvTexture_Vid4_BIx4.yml"><pre lang="bash"><code>./scripts/dist_test.sh [num_gpus] options/test/EvTexture/test_EvTexture_Vid4_BIx4.yml
</code></pre></div>
</li>
<li>Test on REDS4 for 4x VSR:
<div data-snippet-clipboard-copy-content="./scripts/dist_test.sh [num_gpus] options/test/EvTexture/test_EvTexture_REDS4_BIx4.yml"><pre lang="bash"><code>./scripts/dist_test.sh [num_gpus] options/test/EvTexture/test_EvTexture_REDS4_BIx4.yml
</code></pre></div>
This will generate the inference results in <code>results/</code>. The output results on REDS4 and Vid4 can be downloaded from (<a href="https://github.com/DachunKai/EvTexture/releases">Releases</a> / <a href="https://1drv.ms/f/c/2d90e71fb9eb254f/EnMm8c2mP_FPv6lwt1jy01YB6bQhoPQ25vtzAhycYisERw?e=DiI2Ab" rel="nofollow">Onedrive</a> / <a href="https://drive.google.com/drive/folders/1oqOAZbroYW-yfyzIbLYPMJ2ZQmaaCXKy?usp=sharing" rel="nofollow">Google Drive</a> / <a href="https://pan.baidu.com/s/161bfWZGVH1UBCCka93ImqQ?pwd=n8hg" rel="nofollow">Baidu Cloud</a>(n8hg)).</li>
</ul>
</li>
</ol>

<ul dir="auto">
<li>
<p dir="auto">Both video and event data are required as input, as shown in the <a href="https://github.com/DachunKai/EvTexture/blob/main/basicsr/archs/evtexture_arch.py#L70">snippet</a>. We package each video and its event data into an <a href="https://docs.h5py.org/en/stable/quick.html#quick" rel="nofollow">HDF5</a> file.</p>
</li>
<li>
<p dir="auto">Example: The structure of <code>calendar.h5</code> file from the Vid4 dataset is shown below.</p>
<div data-snippet-clipboard-copy-content="calendar.h5
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ 000000 # frame, ndarray, [H, W, C]
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ voxels_f
â”‚   â”œâ”€â”€ 000000 # forward event voxel, ndarray, [Bins, H, W]
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ voxels_b
â”‚   â”œâ”€â”€ 000000 # backward event voxel, ndarray, [Bins, H, W]
â”‚   â”œâ”€â”€ ..."><pre lang="arduino"><code>calendar.h5
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ 000000 # frame, ndarray, [H, W, C]
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ voxels_f
â”‚   â”œâ”€â”€ 000000 # forward event voxel, ndarray, [Bins, H, W]
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ voxels_b
â”‚   â”œâ”€â”€ 000000 # backward event voxel, ndarray, [Bins, H, W]
â”‚   â”œâ”€â”€ ...
</code></pre></div>
</li>
<li>
<p dir="auto">To simulate and generate the event voxels, refer to the dataset preparation details in <a href="https://github.com/DachunKai/EvTexture/blob/main/datasets/DataPreparation.md">DataPreparation.md</a>.</p>
</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Inference on your own video</h3><a id="user-content-inference-on-your-own-video" aria-label="Permalink: Inference on your own video" href="#inference-on-your-own-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<blockquote>
<p dir="auto"><strong>â¤ï¸ Seeking Collaboration</strong>: For issues <a href="https://github.com/DachunKai/EvTexture/issues/6" data-hovercard-type="issue" data-hovercard-url="/DachunKai/EvTexture/issues/6/hovercard">#6</a> and <a href="https://github.com/DachunKai/EvTexture/issues/7" data-hovercard-type="issue" data-hovercard-url="/DachunKai/EvTexture/issues/7/hovercard">#7</a>, our method can indeed perform inference on videos without event data. The solution is to use an event camera simulator, such as <a href="https://github.com/uzh-rpg/rpg_vid2e">vid2e</a>, to generate event data from the video, and then input both the video data and the generated event data into our model. This part, however, may require extensive engineering work to package everything into a script, as detailed in <a href="https://github.com/DachunKai/EvTexture/blob/main/datasets/DataPreparation.md">DataPreparation.md</a>. We currently do not have enough time to undertake this task, so we are looking for collaborators to join us in this effort! ğŸ˜Š</p>
</blockquote>

<p dir="auto">If you find the code and pre-trained models useful for your research, please consider citing our paper. ğŸ˜ƒ</p>
<div data-snippet-clipboard-copy-content="@inproceedings{kai2024evtexture,
  title={Ev{T}exture: {E}vent-driven {T}exture {E}nhancement for {V}ideo {S}uper-{R}esolution},
  author={Kai, Dachun and Lu, Jiayao and Zhang, Yueyi and Sun, Xiaoyan},
  booktitle={International Conference on Machine Learning},
  year={2024},
  organization={PMLR}
}
"><pre><code>@inproceedings{kai2024evtexture,
  title={Ev{T}exture: {E}vent-driven {T}exture {E}nhancement for {V}ideo {S}uper-{R}esolution},
  author={Kai, Dachun and Lu, Jiayao and Zhang, Yueyi and Sun, Xiaoyan},
  booktitle={International Conference on Machine Learning},
  year={2024},
  organization={PMLR}
}

</code></pre></div>

<p dir="auto">If you meet any problems, please describe them in issues or contact:</p>
<ul dir="auto">
<li>Dachun Kai: <a href="mailto:dachunkai@mail.ustc.edu.cn">dachunkai@mail.ustc.edu.cn</a></li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">License and Acknowledgement</h2><a id="user-content-license-and-acknowledgement" aria-label="Permalink: License and Acknowledgement" href="#license-and-acknowledgement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This project is released under the Apache-2.0 license. Our work is built upon <a href="https://github.com/XPixelGroup/BasicSR">BasicSR</a>, which is an open source toolbox for image/video restoration tasks. Thanks to the inspirations and codes from <a href="https://github.com/princeton-vl/RAFT">RAFT</a> and <a href="https://github.com/TimoStoff/event_utils">event_utils</a>.</p>
</article></div></div>
  </body>
</html>
