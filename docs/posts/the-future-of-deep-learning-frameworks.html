<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://neel04.github.io/my-website/blog/pytorch_rant/">Original</a>
    <h1>The future of Deep Learning frameworks</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>Assumed audience:</strong> ML researchers who frequently work with <code>PyTorch</code>, but are interested in trying out <code>JAX</code> or have yet to be convinced.</p><hr/><p>Usually, people start these ‘critiques’ with a disclaimer that they are not trying to trash the framework, and talk about how it’s a tradeoff. However, this is assumed - I’m not going to waste your time with that.</p><p>Instead, I’ll focus on why PyTorch has been a net negative for all (if not most) scientific computing efforts, causing billions of dollars in lost productivity and thousands of wasted dev-hours.</p><p>This is not because its a <em>bad</em> framework per-se, but rather - it simply because it wasn’t designed for the use-cases it’s being employed in right now.</p><p>Ever since <a href="http://torch.ch/">LuaTorch</a>, PyTorch was supposed to be a “production ready, easy-to-use framework for quick prototyping”.</p><p>It wasn’t meant to be deployed onto huge, distributed clusters comprising of thousands of interconnected nodes and GPUs and scale <em>well</em> in a fault-tolerant and robust matter.</p><p>The guiding philosophy of <code>Torch</code> was never about scale - despite what their marketing may have you believe - but <em>flexibility</em>.</p><p>In response to the rising need of a scalable and performant framework, DeepMind developed <code>JAX</code> to meet a simple goal:</p><blockquote><p>“… Supporting state-of-the-art AI research [by] balancing rapid prototyping and quick iteration with the ability to deploy experiments at a scale …” - <a href="https://arc.net/l/quote/cumgnsor"><code>JAX</code> blogpost</a></p></blockquote><p>This post is about convincing you how important this idea/philosophy is not only for Deep Learning, but for all scientfic computing that needs to happen at scale.</p><p>I believe that all infrastructure built on <code>Torch</code> is just a huge pile of technical debt, that will haunt the field for a long, long time.</p><p>PyTorch’s philosophy has always, in some ways, been antithetical to that of Tensorflow’s.</p><p>Where <code>TF 1.x</code> tried to be a static but performant framework by making strong use of the <code>XLA</code> compiler, <code>PyTorch</code> instead focused on being dynamic, easily debuggable and pythonic.</p><p>Early on, the TF devs realized their mistakes when they came to realize how much the community hated the old <code>1.x</code> API, which was counter-intuitive and introduced anti-pythonic patterns that were difficult to grasp for beginners.</p><p>This prompted the core decision to use <code>Keras</code> as the main interface for TensorFlow and downplay the role of <a href="https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra"><code>XLA</code></a> compiler that was at TF’s core. The main focus was on cleaning up the frontend as much as possible.</p><p>This was a huge mistake.</p><p>Sure, the API did improve and worked well for some people - but only as long as your workloads were standard. Any deviations from the norm were punished by stacktrace dumps that were often literal pages of just garbled <code>XLA-HLO</code> that were a nightmare to debug unless you had a strong grasp on the internals of the framework/compiler - which you <strong>didn’t</strong> because <code>XLA</code> was a closed source, internal Google project at the time.</p><p>In short, it had every hallmark of a typical Google product.</p><p>Thus it comes as no surprise that people who switched over to PyTorch thought they had discovered literal heaven:</p><div><div><blockquote><p lang="en" dir="ltr">I&#39;ve been using PyTorch a few months now and I&#39;ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.</p>— Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/868178954032513024?ref_src=twsrc%5Etfw">May 26, 2017</a></blockquote></div></div><p>PyTorch stuck to its roots. Unlike TensorFlow’s static &amp; lazy approach, they took the bolder, more dynamic “eager” approach wherein all <code>torch.Tensor</code>s were evaluated immediately, leading to a much more cleaner abstraction than TensorFlow’s.</p><p>Clearly, they understood that complexity is the enemy of productivity. Instead of tacking on band-aids, they had pursued a fresh new path which paid off.</p><p>Unsurprisingly, almost serious research moved to PyTorch:</p><figure><p><img src="https://neel04.github.io/my-website/blog/pytorch_rant/image-1.png"/></p><figcaption>PyTorch vs. Tensorflow usage in research repos</figcaption></figure><p>But in 2021 <a href="https://arxiv.org/abs/2005.14165"><code>GPT-3</code></a> hit the scene and suddenly things started getting serious. All of a sudden, performance and scalability became the primary concern.</p><p><code>PyTorch</code> accomodated for this rising demand <em>decently</em> well, but because it wasn’t designed around this philosophy - slowly the debt starting catching up and the foundations started crumbling. It’s hard to reconcile flexibility with performance. Clearly, a tradeoff needed to be made.</p><p>Either they could give their biggest and richest users exactly what they wanted - a clean &amp; scalable ecosystem that prioritized performance - which would be a static-oriented <code>TF</code>-like design - or they could try to hold on to what made <code>Torch</code> so special in the first place - being dynamic and “eager” at the expense of performance, and somehow delegate those large-scale workloads to an entirely seperate technological stack.</p><p>So the devs, being the smart and rational engineers they are, choose an appropriate compromise which was . . . . to pursue both paths simultaneously.</p><p>They were unwilling to make any tradeoffs. They wanted their cake and were going to eat it too.</p><p>The new approach was ultimately a chaotic mishmash of competing features. You have on one hand, PyTorch’s committment to eventually use <em>some</em> compiler (likely <code>XLA</code>) as a performant and reliable default backend and on the other, to build up their own entire <a href="https://pytorch.org/docs/stable/torch.compiler.html"><code>torch.compile</code></a> stack that somehow meshes well with the eager, dynamic philosophy by giving users the freedom to invoke a compiler if need be.</p><p>This lack of real long-term strategy is a serious issue.</p><p>Take the <code>torch.compile</code> stack and the new <a href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md"><code>DTensor</code></a> API as an example. The documentation is transparent about the inspiration for this feature. It tries to
bring the sharding model of parallelization from <code>JAX</code> to <code>PyTorch</code>.</p><blockquote><p>… When using <code>DTensor</code> in a [distributed] fashion it might be … <strong>slower</strong> compared to existing solutions like DDP/FSDP. This is mainly because DDP/FSDP have a global view of the entire model … [and can thus] optimize for data parallel specifically … [whereas] DistributedTensor … can only optimize within individual tensor operations.</p></blockquote><blockquote><p>To improve efficiency of <code>DTensor</code>-based data parallel training, we are exploring a <strong>compiler-based</strong> solution on top of <code>DTensor</code>.</p></blockquote><p>Leaning on the compiler is diametrically opposed to torch’s dynamic philosophy - because at each step, you’re restricted by the constraints placed by the compiler which you <em>must</em> obey.</p><p><code>PyTorch</code> clearly doesn’t want to commit to a compiler-centric philosophy (like <code>JAX</code>) but I don’t see any good alternative solutions - and frankly, I doubt the devs do either.</p><p>Instead, what you end up getting getting is a fragmented suite of tools that’re barely usable without significant dev-hours sunk in just setting them up and coaxing them to work with each other.</p><p>It’s considerable friction for research teams who often spend more of their time babysitting the codebase and triangulating random errors rather than running more experiments.</p><p>I feel there is a stronger incentive internally on <em>marketing</em> and shipping ‘features’ rather than actually ensuring they integrate well into the ecosystem. It’s true that maintaining such a huge ecosystem will always have it’s problems, but the considering the case where devs shipped
a built-in implementation of <code>FSDP</code>, and it didn’t work at <em>all</em> with their own <code>torch.compile</code> stack for months, really goes to show where their priorities lie.</p><p>There is simply no excuse for two of your most core, critical features not working together at all. Users had to wait <a href="https://dev-discuss.pytorch.org/t/torch-compile-fsdp-dec-8th/1718">weeks</a> before it was officially patched and the bugs were ironed out to the point of it being in a ususable state where is stands now.</p><p>My point is that all these failures are systemic due to: a) bad organization and b) bad design decisions.</p><p>So what is the competition’s solution to this problem?</p><h2 id="compiler-driven-development">Compiler-driven development</h2><p><code>JAX</code> leverages TensorFlow’s formidable compiler stack, <a href="https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra"><code>XLA</code></a>. <code>XLA</code> is a pretty powerful compiler, but the beauty is that it’s all abstracted away for the end user.
For any function you have, as long as the function is <strong>pure</strong> (more on this later) you can use the simple <code>@jax.jit</code> decorator to JIT compile your function and make it available to <code>XLA</code>.</p><p>You can <code>jit</code> any JAX code - <code>XLA</code> handles the rest. This is what makes JAX such a great framework for scientific computing - its effectively an eDSL built entirely around <code>XLA</code>. The compiler handles and abstracts away a lot of the heavy lifting for us - verifying that the generated graph is correct,
<code>GSPMD</code> partitioner that handles the auto-parallelization w/ sharding in JAX, the graph optimizations, operator and kernel fusion, Latency hiding Scheduling, overlapping asynchronous comms, codegeneration to other backends such as <a href="https://openai.com/index/triton/"><code>triton</code></a> etc.
are all handled by <code>XLA</code> behind the scenes.</p><p>This is a powerful approach. As long as your code obeys some simple JAX restrictions, <code>XLA</code> does this automatically for you. For example, you don’t need <code>torch.distributed.barrier()</code> and other comms primitives when doing parallelization.
DDP support is as simple as:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span># Create a Sharding object to distribute a value across devices:</span>
</span></span><span><span>sharding = PositionalSharding(mesh_utils.create_device_mesh((8,)))   
</span></span><span><span>x = JAX.random.normal(JAX.random.key(0), (8192, 8192))
</span></span><span><span>y = JAX.device_put(x, sharding.reshape(4, 2))
</span></span></code></pre></div><p>which you can also visualize with the built in utilities:</p><div><pre tabindex="0"><code data-lang="py"><span><span>&gt;&gt;&gt; JAX.debug.visualize_array_sharding(z)
</span></span><span><span>
</span></span><span><span>+---------------------+
</span></span><span><span>|  TPU 0   |  TPU 1   |
</span></span><span><span>|----------|----------|
</span></span><span><span>|  TPU 2   |  TPU 3   |
</span></span><span><span>|----------|----------|
</span></span><span><span>|  TPU 6   |  TPU 7   |
</span></span><span><span>|----------|----------|
</span></span><span><span>|  TPU 4   |  TPU 5   |
</span></span><span><span>+---------------------+
</span></span></code></pre></div><p><code>XLA</code>’s approach is that computation follows sharding. Therefore, if the input array is sharded across some axis, <code>XLA</code> handles that automatically for any downstream computation. No other code changes needed.
No need to add communication collections or anything. <code>Pytorch</code> on the other hand requires a ton of boilerplate and modifications just to get a basic DDP setup working correctly.</p><p>This idea of “compiler driven development” is similar to how rust’s compiler works - helping you write better, cleaner code without worrying about a lot of mundane things.</p><p>You focus on the computation, the compiler does the rest.</p><p>I believe that comitting to a philosophy gives a framework a certain design skeleton and structure, that can simplify the code and create a smooth and wondeful experience for a developer.</p><p>Which is why I’m unhappy with the choice made by the <code>PyTorch</code> devs to integrate and rely on a compiler stack for the cool new features rather than keeping the core philosophy of <em>flexibility</em> and <em>freedom</em> alive.</p><p>For example, according to the official <a href="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/">roadmap</a> for <code>PyTorch</code> <code>2.x</code>, they clearly outline their long-term plans of fully integrating <code>XLA</code> with <code>Torch</code>:</p><blockquote><p>“PyTorch/<code>XLA</code> is set to migrate to the open source <code>OpenXLA</code> as its <strong>default</strong> downstream compiler”</p></blockquote><p>This is an awful idea. It’s like saying that shoehorning C++ code in the rust compiler, would somehow be a better experience than using rust itself.</p><p>Torch simply wasn’t <em>designed</em> around <code>XLA</code>, unlike <code>JAX</code>. The reason <code>JAX</code>’s’ ecosystem is so much more stable and well-integrated is precisely because they uphold it’s core values rather than working around them.</p><p>If, god forbid, <code>Pytorch</code> does end up going with the plan and commits to an <code>XLA</code> based compiler stack, then wouldn’t the ideal framework be the one that was <em>specifically</em> designed and built around it, as opposed to the one where it has just been crammed in with little thought and care?</p><p>And even <strong>if</strong> <code>Pytorch</code> ends up pursuing a ‘multi-backend’ approach, wherein you would be able choose whatever compiler backend you wish, wouldn’t that worsen the fragmentation problem and absolutely nuke the API, as it tries to respect the restrictions of every compiler stack?</p><p>This isn’t just baseless theorizing either — look at <code>Torch/XLA</code>. Anyone who’s ever dared to use it on TPUs suffers from a PTSD so severe that they’re eligible for benefits. The mere sight of “XLA” sends them into a state of cold sweat and nightmares to the caffeine-fuelled days spent debugging hundred-line <code>XLA</code> errors. The only path to recovery at such moments is to reassure the victim that they’ll always have their GPUs, and an <code>A100</code> may be placed beside them for emotional support.</p><h2 id="multi-backend-is-doomed">Multi-backend is doomed</h2><p><code>Pytorch</code>’s root problem is that it tries to do everything at once and fails miserably.</p><p>The “multi-backend” design decision makes this problem exponentially worse. In <em>theory</em>, it sounds like an amazing idea to be able to choose whichever stack you prefer - but in reality, its a tangled mess of obscure tracebacks and incompatibility issues.</p><p>It’s not that its <em>hard</em> to get these backends working. Rather, there are some constraints that these backends expect which are hard to mesh with the flexible and pythonic API of <code>PyTorch</code>.</p><p>There’s a tradeoff between keeping most of the API consistent vs. obeying the restrictions of the backends you leverage. As a result, the devs are seeking to rely more on codegen (say converting torch code to <code>triton</code> which then you can manually work with and leverage it’s compiler &amp; JIT stack)
as opposed to actually integrating/comitting to a single backend - which is arguably the worse option for <code>torch</code>.</p><p>Every decision <code>torch</code> takes somehow always feels like a compromise because it refuses to make meaningful tradeoffs. There’s no coherence, no overall strategy. It ends up feeling more like a mishmash of features that don’t mesh well together and ultimately cause a lot of frustration for the user.</p><p>There is no faster way to kill an ecosystem.</p><p>IMHO PyTorch should not follow the <code>JAX</code> “integrated compiler and backend” approach for a very simple reason: Jax was explcitly designed from the ground up to work <strong>with</strong> <code>XLA</code>. Not <strong>against</strong> it. That is why <code>TensorFlow</code> never really took off, and why it’s attempts at integrating <code>Keras</code> crashed and burned.</p><p>Your strategy simply cannot be to just replace the entire <code>PyTorch</code> frontend with <code>JAX</code>’s, because then you just have a shittier version of <code>JAX</code>! It’s virtually impossible to come up with a neat, functional API based on <code>XLA</code> that’s somehow better than <code>JAX</code>’s, and carries on <code>Torch</code>’s flexible nature.</p><p>I don’t blame the devs for trying new and different ideas - those are always welcome. However, if they want <code>PyTorch</code> to stand the test of time, more focus has to be put in shoring up the foundations than shipping shiny new features that immediately crumble outside ideal tutorial conditions.</p><h2 id="fragmentation--functional-programming">Fragmentation &amp; Functional Programming</h2><p><code>JAX</code> has a “functional” API. Earlier, I mentioned that <code>JAX</code> functions have to be pure (i.e they cannot have any global side effect. Just like mathematical functions, given the same data the function will always return the same output no matter the context of it’s execution.)</p><p>This design philosophy is what makes <code>JAX</code> stand out. Due to the functional roots of <code>JAX</code>, <code>JAX</code> functions are often composable and interoperate well with each other. It reduces the development complexity as functions are defined with specific signatures and a well-defined,
concrete task. If the types are respected, then the function is guranteed* to work out-of-the-box.</p><p>This is well suited to the kinds of workloads that one needs in scientific computing. In Deep Learning especially, since NNs are just a static functions, this functional paradigm makes writing even complex codebases easy.</p><p>For example, let’s look at the <code>optax</code> API from the <code>JAX</code> ecosystem.</p><p>Due to the functional approach, <code>optax</code> has what we call a “chain” that involves a bunch of functions sequentially applied on the gradients. So the fundamental building blocks are <code>GradientTransformation</code>s.</p><p>This makes it a really powerful but expressive API to work with.</p><div><pre tabindex="0"><code data-lang="py"><span><span>optimizer = optax.adam(1e-2)
</span></span></code></pre></div><p>If I wanted to clip the grads here for example, I could do it trivially:</p><div><pre tabindex="0"><code data-lang="py"><span><span>optimiser = optax.chain(
</span></span><span><span>    optax.clip_by_global_norm(1.0),
</span></span><span><span>    optax.adam(1e-2),
</span></span><span><span>)
</span></span></code></pre></div><p>If I wanted to do a simple operation such as take the <code>EMA</code> of my grads, in PyTorch that would’ve required setting up objects and then manually digging through the codebases to place methods appropriately.
But with <code>optax</code>,</p><div><pre tabindex="0"><code data-lang="py"><span><span>optimiser = optax.chain(
</span></span><span><span>    optax.clip_by_global_norm(1.0),
</span></span><span><span>    optax.adam(1e-2),
</span></span><span><span>    optax.ema(decay=0.999)
</span></span><span><span>)
</span></span></code></pre></div><p>A similar approach goes for combining optimizers, meta-learning approaches, gradient accumulation etc. It’s simply much more cleaner than <code>PyTorch</code>.</p><p>Another cool consequence of a functional design is <code>vmap</code>. This stands for ‘vectorized’ map which accurately describes what it does. You can <code>map</code> anything and as long as its a <code>vmap</code>, then <code>XLA</code> will automatically fuse and optimize it.</p><p>This means that when you write functions, you <strong>don’t think about the batch dimension!</strong> You just <code>vmap</code> all you code and this simplifies things.</p><p>For one, you need less <code>ein-*</code> ops. While <code>einops</code> are great and all - it’s simply more intuitive to grasp 2D/3D tensor manipulations, and are also much more readable IMO. Let’s take an extremely limited example operation, and compare the difference:</p><div><pre tabindex="0"><code data-lang="py"><span><span>arr: Array = jnp.ones((batch_size, seqlen, h_dim))
</span></span><span><span>
</span></span><span><span><span>def</span> <span>vanilla_func</span>(arr: Array) -&gt; Array:
</span></span><span><span>  <span>&#39;&#39;&#39;
</span></span></span><span><span><span>  We want to do a: &#39;(b s h, b s h) -&gt; (b s s, b s h) -&gt; b h s&#39; operation.
</span></span></span><span><span><span>  &#39;&#39;&#39;</span>
</span></span><span><span>  <span>return</span> ((arr @ arr.transpose(0, 2, 1)) @ arr).transpose(0, 2, 1)
</span></span><span><span>
</span></span><span><span>@jax.vmap
</span></span><span><span><span>def</span> <span>vmapped_func</span>(arr: Array) -&gt; Array:
</span></span><span><span>  <span>&#39;&#39;&#39;
</span></span></span><span><span><span>  We want to do a: &#39;(s h, s h) -&gt; (s s, s h) -&gt; h s&#39; operation.
</span></span></span><span><span><span>  &#39;&#39;&#39;</span>
</span></span><span><span>  <span>return</span> ((arr @ arr.T) @ arr).T
</span></span></code></pre></div><p>Even for this toy operation, you can immediately see how much more instantly readable the function is. Now imagine that with the more complex tensor manipulations, like the ones used in <code>MHSA</code>.</p><p>Subscribing to the functional paradigm means that it’s easier to write complex code that works <em>well</em>, since you only have to reason about individual components in isolation.
It’s both clean and performant because you can <code>@jax.jit</code> any pure function without worrying about anything else. It. Just. Works.</p><p>In the functional world, as long as you respect the purity constraints and have the right signature, you enjoy all the other benefits - such as composability.</p><p>With <code>torch</code> however, there is a non-trivial chance that whatever stack you use - say doing <code>FSDP + multi-node + torch.compile + ...</code>
something will <em>always</em> break due to the sheer complexity involved. Multiple things have to work correctly together, and if any component fails due to edge case, then you would be debugging till 3 a.m.</p><p>And because there would <strong>always</strong> be bugs that weren’t caught during development simply because it’s not possible to test each and every combination of the dozens of features <code>Pytorch</code> offers, It’s simply impossible to write code that works well without significant effort.</p><p>This has meant that the <code>torch</code> ecosystem has become very bloated and buggy - things don’t interoperate well together, so contributors often come up with new libraries and frameworks to solve specific issues (like say HuggingFace’s <code>accelerate</code> for distributed training)
which in turn aren’t designed to interface with other such “solutions” due to having no shared abstraction, so it quickly devolves into a mess of dependencies and <code>requirements.txt</code> and a training loop that looks like it was the scene of Guido Van Rossum’s murder.</p><p>I’d go on to say from my anecdotal experience about 70-80% of those GitHub issues or forum discussions are simply due to various libraries erorring out on each other.</p><p>Unfortunately, few solutions exist to fix it. This is very much an OOP as well as a design problem. I <em>think</em> having a fundamental, <code>PyTorch</code>~y object (like <code>JAX</code>’s <code>PyTree</code>) might’ve helped build a common base for abstraction, but I’m not an SWE so I’m not sure how much it’d have <em>really</em> helped.</p><p>Nor can you just adopt a functional programming paradigm, at which point you’d have converged to a worse version of <code>JAX</code> while breaking all backwards compatibility for every existing <code>torch</code> codebase.</p><p>The way I see it - <code>PyTorch</code> is well and truly fucked in this department.</p><h2 id="reproducibility">Reproducibility</h2><p>The “reproducility crisis” is an oft discussed problem in science, and AI/DL has been no exception.
Despite the existence of containerized environments and version control, researchers refuse to use them and journals/conferences place no requirements on them either.
Even upholding your pledge to open-source the code isn’t verified by academic institutions.</p><p>However, there are some design choices that nudge users to write code that facilitates reproduction, with minimal effort. This incentivizes users to put that little effort in
and gain masive advantages in return - such as being able to validate their older experiments at any point and ensure that randomness is not a factor in any their results.</p><p>I believe such oversights are usually because of laziness/carelessness than malicious intent. So such small decisions can ultimately add up to saving potentially dozens of dev-hours and a lot of cussing.</p><h3 id="seeding">Seeding</h3><p><code>torch</code>’s handling of something as simple as seeding is… not ideal. Typically, you’d have to do:</p><div><pre tabindex="0"><code data-lang="py"><span><span><span>import</span> <span>torch</span>
</span></span><span><span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
</span></span><span><span>
</span></span><span><span>np.random.seed(0) <span># if you&#39;re using numpy</span>
</span></span><span><span>torch.manual_seed(0)
</span></span><span><span>torch.cuda.manual_seed_all(args.seed)
</span></span><span><span>torch.use_deterministic_algorithms(<span>True</span>)
</span></span><span><span>torch.utils.deterministic.fill_uninitialized_memory(<span>True</span>)
</span></span></code></pre></div><p>Which let’s be honest, isn’t really the end of the world coming at barely half a dozen loc - But on the flipside, is easily forgettable/misconfigured especially in the heat of deadlines.</p><p>I’ve personally known researchers who set the seeds in the wrong file at the wrong place and they weren’t even used by <code>torch</code> at all - instead, were just silently ignored, thus invalidating all their experiments. (That researcher was me)</p><p><code>JAX</code> on the other hand forces you to create an explicit <code>key</code> which gets passed to any function that required randomness. This approach completely eliminates this problem as the RNG at all points is statically seeded. And because <code>jax</code> has its own version
of numpy (<code>jax.numpy</code>) you don’t need to remember to seed it seperately.</p><p>This is a small fry - but I feel such small QoL decisions can end up making the whole framework’s UX a whole lot better.</p><h3 id="portability">Portability</h3><p>One of the biggest banes of using torch codebases is the lack of portability - codebases written for running on CUDA/GPUs don’t really work well when run on non-Nvidia hardware like TPUs, NPUs, AMD GPUs etc.</p><p>Worse, it’s hard to port torch code written for 1x (one) node and port it to be multi-node. Multi-node often involves dozens of dev-hours and substantial code changes to manually integrate it in the correct places. Unsurprisingly, this quickly devolves into a horror story of errors, crashes and incorrect comms that leech performance.</p><p><code>JAX</code>’s compiler-centric approach however gives it a win in this department. <code>XLA</code> handles switching between device backends for us - and it already works well out-of-the-box on GPUs/TPUs/multi-node/multi-slice with minimal to no code changes. (Support for AMD GPUs is also coming, however anecdotally it’s not in a great place right now - which seems more reflective of AMD than <code>XLA</code>.)</p><p>One only needs to implement a device backend for <code>XLA</code> to use, and it automatically takes care of the intermediate steps of extracting computation from graph as specified in a framework (Jax/TF/PyTorch), produce an HLO, and then eventually emit a lower-level IR that hardware backends can then execute during runtime.</p><figure><p><img src="https://neel04.github.io/my-website/blog/pytorch_rant/jax_platforms.png"/></p><figcaption>Jax&#39;s hardware compatibility matrix, as of <i>Aug 2024</i></figcaption></figure><p>This way makes it easier for hardware vendors to support their devices, as well as make the transition between devices more easier.</p><p>I think this is an often overlooked, but important issue. Not everyone has access to the same hardware, so codebases that are portable across different types of hardware can be a small step towards making Deep Learning more accessible to beginners/intermediates as well as preventing a lot of frustration.</p><h3 id="auto-scaling">Auto Scaling</h3><p>This point ties in with the idea of portability. Codebases that can <em>autoscale</em> well on their own are massively helpful during reproduction. In an ideal case, this should happen automatically with minimal code changes, unfetterd by networking boundaries.</p><p>As we’ve come to expect, <code>JAX</code> does this well. When writing <code>JAX</code> code, you don’t need to specify communication primitives or do <code>torch.distributed.barrier()</code> everywhere - <code>XLA</code> automatically inserts that for us, taking the available hardware in account.</p><p>This philosophy means that whatever devices <code>JAX</code> can <em>detect</em> are automatically used, irrespective of networking, topology, configuration etc. You do not need to specify ranks or a central co-ordinator host.
<code>JAX</code> automagically synchronizes and stages all the computations for you, as well as apply optimization passes to maximize asynchronous execution of the kernels and minimize latency.</p><p>All a person has to do is specify the sharding of the tensors you want to distribute across, such as sharding the batch dimension of the input arrays and due to <code>XLA</code>’s “computation follows sharding” approach, it automatically figures out the rest. This is due to the <a href="https://arxiv.org/abs/2105.04663">GSPMD</a> partitioner in the XLA compiler stack.</p><p>This is really powerful, as experiments that have been validated at scale can be run by hobbyists easily to play around with them and potentially iterate on them - and vice-versa.</p><p>I feel this could help in discovery of forgotten ideas more easily, and encourage the field to be more ‘<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">bitter</a>’ - as ideas could be easily tested as a function at bigger scales with minimal effort, thus incentivizing such experiments.</p><h2 id="the-cons">The Cons</h2><p>I have been disproportionately covering only the problems plaguing torch so far. But it’s not all roses with <code>JAX</code> either. There are a few major concerns that I wish are given far more attention among <code>JAX/XLA</code> devs:</p><h3 id="governance-structure">Governance structure</h3><p>Currently, <code>XLA</code> is under TF governance, and while talk has been made of establishing a seperate organizing body to manage all affairs, similar to torch, not much concrete efforts have been made - atleast publicly.</p><p>Unfortunately, there isn’t a lot of trust in Google at the moment due to its reputation to axe unpopular products. Now, <code>JAX</code> is technically a DeepMind project and of core significance to Google’s entire AI push, but still I feel that having a seperate governing body would be of great long-term benefit for the ecosystem as a whole
by providing guidance to the development of the project.</p><p>This would give it a concrete structure, and decouple it with Google’s infamous bureaucracy - thus avoiding a whole host of problems in a single sweep.</p><p>I don’t think <code>JAX</code> necessary <em>needs</em> an official structure of this sort, but rather it’d be nice to have a gurantee that <code>JAX</code> development will take place for a long time regardless of Google upper-management’s decisions.</p><p>It would definitely help its case in adoption among companies and big labs as well, who are hesitant to spend resources incorporating tools that might stop being maintained at some point.</p><h3 id="open-source-transition-of-xla">Open source transition of <code>XLA</code></h3><p>For the longest time, <code>XLA</code> was a closed-source project. However, efforts have been made to open source it, and now <a href="https://openxla.org/"><code>OpenXLA</code></a> is at well outperforms the internal XLA build.</p><p>However, documentation about the internals of <code>XLA</code> is still sparse. Most of the resources are just live talks and the occasional paper, which are often out-of-date.</p><p>Having a publicly accessible roadmap of upcoming features would make it easier
for people to track progress and contribute to things they find particularly interesting.</p><p>I think it’d be nice to give practitioners a way to better gauge what <code>XLA</code> can and can’t do through <a href="http://blog.ezyang.com/2019/05/pytorch-internals/">Edward Yang styled</a> mini-blogposts that breakdown each stage of the <code>XLA</code> compiler stack and explain the nitty-gritty details.</p><p>I understand that this is resource intensive, which could be better directed elsewhere but I feel that people trust the tools more when they understand them, and there’s a positive spillover effect across the entire ecosystem that ends up benefitting everyone.</p><h3 id="coalescing-ecosystem">Coalescing ecosystem</h3><p>For various reasons outside the scope of this blog, I heartily dislike <code>flax</code>. It’s a bane on the <code>JAX</code> ecosystem. It has an unintuitive API, terse syntax and is absolutely hell for beginners transitioning from <code>PyTorch</code>.</p><p>Just use <code>equinox</code>.</p><p>There have been attempts to fix <code>flax</code>’s shortcomings from the dev team, namely by using <a href="https://flax.readthedocs.io/en/v0.8.3/experimental/nnx/index.html"><code>NNX</code></a> which is supposed to be a more “equinox-y” wrapper ontop of <code>flax</code>.</p><p>However, I think it’s ultimately a waste of time. If you want an <code>equinox</code>-styled API, then just use <code>equinox</code>. There isn’t a lot <code>flax</code> does especially better that’s hard to replicate with <code>equinox</code>. Plus, having little to no abstraction makes implementing things in <code>equinox</code> much easier and faster.</p><p>Right now, a lot of the <code>JAX</code> ecosystem is designed around <code>flax</code>. <code>equinox</code>, because it fundamentally interfaces with <code>PyTree</code>s is cross-compatible with all libraries, however you do have to do a little <code>eqx.partition</code>-ing and <code>filter</code>-ing, which can be a bit annoying.</p><p>I want to change the status quo. It should be the other way around - <code>equinox</code> should have first class support everywhere. Considering its popularity, I think this decision would objectively make thigns easier for the vast majority of serious <code>JAX</code> users and big codebases.</p><p>I know this is a controversial opinion simply because a lot of resources have been poured into <code>flax</code> - But this is classic <a href="https://en.wikipedia.org/wiki/Sunk_cost">sunk-cost fallacy</a>. <code>equinox</code> just does it better, in the way a <code>JAX</code> framework should always have been like. It may not be perfect, but its better than the alternatives by a mile.</p><figure><p><img src="https://neel04.github.io/my-website/blog/pytorch_rant/eqxvsflax.png"/></p><figcaption><code>equinox</code> vs. <code>flax</code>: as neatly summarized in the <a href="https://neel04.github.io/my-website/blog/pytorch_rant/url">equinox docs</a>.</figcaption></figure><p>It’s good to see that maintainers of the <code>JAX</code> ecosystem are realizing the popularity of <code>equinox</code> and adjusting accordingly - however, I’d love to see a bit more love officially from Google and the <code>flax</code> team as well.</p><p>If you want to try out <code>JAX</code> - it’s not even a question. Use <code>equinox</code>. You’ll thank me.</p><blockquote><p>“I’ve been using <code>equinox</code> for a few months now and I’ve never felt better. I have more energy. My skin is clearer. My eye sight has improved.”
– Me</p></blockquote><h3 id="sharp-edges">Sharp edges</h3><p>Due to some of the API design decisions and <code>XLA</code> restrictions, <code>JAX</code> has some “sharp edges” that you should be careful of. The well-written documentation explains this very concisely:</p><p><a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html">Common Gotchas in Jax.</a></p><p>So go give that a read atleast once before using <code>JAX</code>. It’ll save you a lot of time and energy (as RTFM-ing always does).</p><h2 id="conclusion">Conclusion</h2><p>This blogpost was to correct the often-parotted myth that <code>PyTorch</code> is simply the best for any real research workloads - especially on GPUs. That simply isn’t the case anymore.</p><p>Infact, I’d go as far as to argue that porting all <code>Torch</code> code to <code>JAX</code> would be <em>immensely</em> beneficial to the field as a whole. These are not minor features - having autoparallelization, reproducibility, a clean functional API etc. would be a godsend for a lot of research codebases.</p><p>So if you want to make this field a little bit better, consider rewriting your codebases in <code>JAX</code>.</p><p>Shoutout to <a href="https://kidger.site/">Patrick Kidger</a> as well for developing <code>equinox</code>. If you’re coming from <code>PyTorch</code>, I cannot recommend it enough!</p></div></div>
  </body>
</html>
