<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lm-sys/RouteLLM">Original</a>
    <h1>RouteLLM: A framework for serving and evaluating LLM routers</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">RouteLLM is a framework for serving and evaluating LLM routers.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/lm-sys/RouteLLM/blob/main/assets/router.png"><img src="https://github.com/lm-sys/RouteLLM/raw/main/assets/router.png" width="50%"/></a>
</p>
<p dir="auto">Our core features include:</p>
<ul dir="auto">
<li>Drop-in replacement for OpenAI&#39;s client (or launch an OpenAI-compatible server) to route simpler queries to cheaper models.</li>
<li>Trained routers are provided out of the box, which we have shown to <strong>reduce costs by up to 85%</strong> on widely-used benchmarks such as MT Bench while maintaining <strong>95% GPT-4 performance</strong>.</li>
<li>Easily extend the framework to include new routers and compare the performance of routers across multiple benchmarks.</li>
</ul>

<p dir="auto"><strong>From PyPI</strong></p>
<div data-snippet-clipboard-copy-content="pip install &#34;routellm[serve,eval]&#34;"><pre><code>pip install &#34;routellm[serve,eval]&#34;
</code></pre></div>
<p dir="auto"><strong>From source</strong></p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/lm-sys/RouteLLM.git
cd RouteLLM
pip install -e .[serve,eval]"><pre><code>git clone https://github.com/lm-sys/RouteLLM.git
cd RouteLLM
pip install -e .[serve,eval]
</code></pre></div>

<p dir="auto">Let&#39;s walkthrough replacing an existing OpenAI client to route queries between LLMs instead of using only a single model.</p>
<ol dir="auto">
<li>First, let&#39;s replace our OpenAI client by initializing the RouteLLM controller with the <code>mf</code> router. By default, RouteLLM will use the best-performing config:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="import os
from routellm.controller import Controller

os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;sk-XXXXXX&#34;
# Replace with your model provider, we use Anyscale&#39;s Mixtral here.
os.environ[&#34;ANYSCALE_API_KEY&#34;] = &#34;esecret_XXXXXX&#34;

client = Controller(
  routers=[&#34;mf&#34;],
  strong_model=&#34;gpt-4-1106-preview&#34;,
  weak_model=&#34;anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;,
)"><pre><span>import</span> <span>os</span>
<span>from</span> <span>routellm</span>.<span>controller</span> <span>import</span> <span>Controller</span>

<span>os</span>.<span>environ</span>[<span>&#34;OPENAI_API_KEY&#34;</span>] <span>=</span> <span>&#34;sk-XXXXXX&#34;</span>
<span># Replace with your model provider, we use Anyscale&#39;s Mixtral here.</span>
<span>os</span>.<span>environ</span>[<span>&#34;ANYSCALE_API_KEY&#34;</span>] <span>=</span> <span>&#34;esecret_XXXXXX&#34;</span>

<span>client</span> <span>=</span> <span>Controller</span>(
  <span>routers</span><span>=</span>[<span>&#34;mf&#34;</span>],
  <span>strong_model</span><span>=</span><span>&#34;gpt-4-1106-preview&#34;</span>,
  <span>weak_model</span><span>=</span><span>&#34;anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;</span>,
)</pre></div>
<p dir="auto">Above, we pick <code>gpt-4-1106-preview</code> as the strong model and <code>anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1</code> as the weak model, setting the API keys accordingly. You can route between different model pairs or providers by updating the model names as described in <a href="#model-support">Model Support</a>.</p>
<p dir="auto">Want to route to local models? Check out <a href="https://github.com/lm-sys/RouteLLM/blob/main/examples/routing_to_local_models.md">Routing to Local Models</a>.</p>
<ol start="2" dir="auto">
<li>Each routing request has a <em>cost threshold</em> that controls the tradeoff between cost and quality. We should calibrate this based on the types of queries we receive to maximize routing performance. As an example, let&#39;s calibrate our threshold for 50% GPT-4 calls using data from Chatbot Arena.</li>
</ol>
<div data-snippet-clipboard-copy-content="&gt; python -m routellm.calibrate_threshold --routers mf --strong-model-pct 0.5 --config config.example.yaml
For 50.0% strong model calls for mf, threshold = 0.11593"><pre><code>&gt; python -m routellm.calibrate_threshold --routers mf --strong-model-pct 0.5 --config config.example.yaml
For 50.0% strong model calls for mf, threshold = 0.11593
</code></pre></div>
<p dir="auto">This means that we want to use <code>0.11593</code> as our threshold so that approximately 50% of all queries (those that require GPT-4 the most) will be routed to it (see <a href="#threshold-calibration">Threshold Calibration</a> for details).</p>
<ol start="3" dir="auto">
<li>Now, let&#39;s update the <code>model</code> field when we generate completions to specify the router and threshold to use:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="response = client.chat.completions.create(
  # This tells RouteLLM to use the MF router with a cost threshold of 0.11593
  model=&#34;router-mf-0.11593&#34;,
  messages=[
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello!&#34;}
  ]
)"><pre><span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>completions</span>.<span>create</span>(
  <span># This tells RouteLLM to use the MF router with a cost threshold of 0.11593</span>
  <span>model</span><span>=</span><span>&#34;router-mf-0.11593&#34;</span>,
  <span>messages</span><span>=</span>[
    {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;Hello!&#34;</span>}
  ]
)</pre></div>
<p dir="auto">That&#39;s it! Now, requests with be routed between the strong and weak model depending on what is required, <strong>saving costs while maintaining a high quality of responses</strong>.</p>
<p dir="auto">Depending on your use case, you might want to consider using a different model pair, modifying the configuration, or calibrating the thresholds based on the types of queries you receive to improve performance.</p>

<p dir="auto">Instead of using the Python SDK, you can also launch an OpenAI-compatible server that will work with any existing OpenAI client, using similar steps:</p>
<div data-snippet-clipboard-copy-content="&gt; export OPENAI_API_KEY=sk-XXXXXX
&gt; export ANYSCALE_API_KEY=esecret_XXXXXX
&gt; python -m routellm.openai_server --routers mf --strong-model gpt-4-1106-preview --weak-model anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:6060 (Press CTRL+C to quit)"><pre><code>&gt; export OPENAI_API_KEY=sk-XXXXXX
&gt; export ANYSCALE_API_KEY=esecret_XXXXXX
&gt; python -m routellm.openai_server --routers mf --strong-model gpt-4-1106-preview --weak-model anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:6060 (Press CTRL+C to quit)
</code></pre></div>
<p dir="auto">Once the server is launched, you can start a local router chatbot to see how different messages are routed.</p>
<div data-snippet-clipboard-copy-content="python -m examples.router_chat --router mf --threshold 0.11593"><pre><code>python -m examples.router_chat --router mf --threshold 0.11593
</code></pre></div>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/lm-sys/RouteLLM/blob/main/assets/chat-interface.png"><img src="https://github.com/lm-sys/RouteLLM/raw/main/assets/chat-interface.png" width="50%"/></a>
</p>

<p dir="auto">In the above examples, GPT-4 and Mixtral 8x7B are used as the model pair, but you can modify this using the <code>strong-model</code> and <code>weak-model</code> arguments.</p>
<p dir="auto">We leverage <a href="https://github.com/BerriAI/litellm">LiteLLM</a> to support chat completions from a wide-range of open-source and closed models. In general, you need a setup an API key and point to the provider with the appropriate model name. Alternatively, you can also use <strong>any OpenAI-compatible endpoint</strong> by prefixing the model name with <code>openai/</code> and setting the <code>--base-url</code> and <code>--api-key</code> flags.</p>
<p dir="auto">Note that regardless of the model pair used, an <code>OPENAI_API_KEY</code> will currently still be required to generate embeddings for the <code>mf</code> and <code>sw_ranking</code> routers.</p>
<p dir="auto">Instructions for setting up your API keys for popular providers:</p>
<ul dir="auto">
<li>Local models with Ollama: see <a href="https://github.com/lm-sys/RouteLLM/blob/main/examples/routing_to_local_models.md">this guide</a></li>
<li><a href="https://litellm.vercel.app/docs/providers/anthropic#api-keys" rel="nofollow">Anthropic</a></li>
<li><a href="https://litellm.vercel.app/docs/providers/gemini#sample-usage" rel="nofollow">Gemini - Google AI Studio</a></li>
<li><a href="https://litellm.vercel.app/docs/providers/bedrock#required-environment-variables" rel="nofollow">Amazon Bedrock</a></li>
<li><a href="https://litellm.vercel.app/docs/providers/togetherai#api-keys" rel="nofollow">Together AI</a></li>
<li><a href="https://litellm.vercel.app/docs/providers/anyscale#api-key" rel="nofollow">Anyscale Endpoints</a></li>
</ul>
<p dir="auto">For other model providers, find instructions <a href="https://litellm.vercel.app/docs/providers" rel="nofollow">here</a> or raise an issue.</p>

<p dir="auto">Different LLMs vary widely in their costs and capabilities, which leads to a dilemma when deploying them: routing all queries to the most capable model leads to the highest-quality responses but can be very expensive, while routing queries to smaller models can save costs but may result in lower-quality responses.</p>
<p dir="auto"><em>LLM routing</em> offers a solution to this. We introduce a router that looks at queries and routes simpler queries to smaller, cheaper models, saving costs while maintaining quality. We focus on routing between 2 models: a stronger, more expensive model and a cheaper but weaker model. Each request is also associated with a <em>cost threshold</em> that determines the cost-quality tradeoff of that request - a higher cost threshold leads to lower cost but may lead to lower-quality responses.</p>

<p dir="auto">RouteLLM offers a lightweight OpenAI-compatible server for routing requests based on different routing strategies:</p>
<div data-snippet-clipboard-copy-content="python -m routellm.openai_server --routers mf --config config.example.yaml"><pre><code>python -m routellm.openai_server --routers mf --config config.example.yaml
</code></pre></div>
<ul dir="auto">
<li><code>--routers</code> specifies the list of routers available to the server. For instance, here, the server is started with one available router: <code>mf</code> (see below for the list of routers).</li>
<li><code>--config</code> specifies the path to the configuration file for the routers. If unspecified, the server will default to using our best-performing configuration (see <a href="#configuration">Configuration</a> for details).</li>
</ul>
<p dir="auto">For most use-cases, <strong>we recommend the <code>mf</code> router</strong> as we have evaluated it to be very strong and lightweight.</p>
<p dir="auto">When making a request to the server, clients specify the router and cost threshold to use for each request using the <code>model</code> field in the following format <code>router-[ROUTER NAME]-[THRESHOLD]</code>. For instance, using a <code>model</code> of <code>router-mf-0.5</code> specifies that the request should be routed using the <code>mf</code> router with a threshold of 0.5.</p>

<p dir="auto">The threshold used for routing controls the cost-quality tradeoff. The range of meaningful thresholds varies depending on the type of router and the queries you receive. Therefore, we recommend calibrating thresholds using a sample of your incoming queries, as well as the % of queries you&#39;d like to route to the stronger model.</p>
<p dir="auto">By default, we support calibrating thresholds based on the public <a href="https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k" rel="nofollow">Chatbot Arena dataset</a>. For example, to calibrate the threshold for the <code>mf</code> router such that 50% of calls are routed to the stronger model:</p>
<div data-snippet-clipboard-copy-content="&gt; python -m routellm.calibrate_threshold --task calibrate --routers mf --strong-model-pct 0.5 --config config.example.yaml
For 50.0% strong model calls for mf, threshold = 0.11593"><pre><code>&gt; python -m routellm.calibrate_threshold --task calibrate --routers mf --strong-model-pct 0.5 --config config.example.yaml
For 50.0% strong model calls for mf, threshold = 0.11593
</code></pre></div>
<p dir="auto">This means that the threshold should be set to 0.1881 for the <code>mf</code> router so that approximately 50% of calls are routed to the strong model i.e. using a <code>model</code> field of <code>router-mf-0.1159</code>.</p>
<p dir="auto">However, note that because we calibrate the thresholds based on an existing dataset, the % of calls routed to each model will differ based on the actual queries received. Therefore, we recommend calibrating on a dataset that closely resembles the types of queries you receive.</p>

<p dir="auto">RouteLLM also includes a evaluation framework to measure the performance of different routing strategies on benchmarks.</p>
<p dir="auto">To evaluate a router on a benchmark, you can use the following command:</p>
<div data-snippet-clipboard-copy-content="python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark gsm8k --config config.example.yaml "><pre><code>python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark gsm8k --config config.example.yaml 
</code></pre></div>
<ul dir="auto">
<li><code>--routers</code> specifies the list of routers to evaluate, for instance, <code>random</code> and <code>bert</code> in this case.</li>
<li><code>--benchmark</code> specifies the specific benchmark to evaluate the routers on. We currently support: <code>mmlu</code>, <code>gsm8k</code>, and <code>mt-bench</code>.</li>
</ul>
<p dir="auto">Evaluation results will be printed to the console. A plot of router performance will also be generated in the current directory (override the path using <code>--output</code>). To avoid recomputing results, the results for a router on a given benchmark is cached by default. This behavior can be overridden by using the <code>--overwrite-cache</code> flag, which takes in a list of routers to overwrite the cache for.</p>
<p dir="auto">The results for all our benchmarks have been cached. For MT Bench, we use the precomputed judgements for the desired model pair. For MMLU and GSM8K, we utilized <a href="https://github.com/sgl-project/sglang">SGLang</a> to compute the results for the desired model pair - the full code for this can be found in the benchmark directories if you would like to evaluate a different model pair.</p>
<p dir="auto">By default, GPT-4 and Mixtral are used as the model pair for evaluation. To modify the model pair used, set them using the <code>--strong-model</code> and <code>--weak-model</code> flags.</p>

<p dir="auto">Out of the box, RouteLLM supports 4 routers trained on the <code>gpt-4-1106-preview</code> and <code>mixtral-8x7b-instruct-v0.1</code> model pair.</p>
<p dir="auto">The full list of routers:</p>
<ol dir="auto">
<li><code>mf</code>: Uses a matrix factorization model trained on the preference data (recommended).</li>
<li><code>sw_ranking</code>: Uses a weighted Elo calculation for routing, where each vote is weighted according to how similar it is to the user&#39;s prompt.</li>
<li><code>bert</code>: Uses a BERT classifier trained on the preference data.</li>
<li><code>causal_llm</code>: Uses a LLM-based classifier tuned on the preference data.</li>
<li><code>random</code>: Randomly routes to either model.</li>
</ol>
<p dir="auto">While these routers have been trained on the <code>gpt-4-1106-preview</code> and <code>mixtral-8x7b-instruct-v0.1</code> model pair, we have found that these routers generalize well to other strong and weak model pairs as well. Therefore, you can replace the model pair used for routing without having to retrain these models!</p>
<p dir="auto">For the full details, refer to our <a href="https://arxiv.org/abs/2406.18665" rel="nofollow">paper</a>.</p>

<p dir="auto">The configuration for routers is specified in either the <code>config</code> argument for <code>Controller</code> or by passing in the path to a YAML file using the <code>--config</code> flag. It is a top-level mapping from router name to the keyword arguments used for router initialization.</p>
<p dir="auto">An example configuration is provided in the <code>config.example.yaml</code> file - it provides the configurations for routers that have trained on Arena data augmented using GPT-4 as a judge. The models and datasets used are all hosted on Hugging Face under the <a href="https://huggingface.co/routellm" rel="nofollow">RouteLLM</a> and <a href="https://huggingface.co/lmsys" rel="nofollow">LMSYS</a> organizations.</p>

<p dir="auto">We welcome contributions! Please feel free to open an issue or a pull request if you have any suggestions or improvements.</p>

<p dir="auto">To add a new router to RouteLLM, implement the abstract <code>Router</code> class in <code>routers.py</code> and add the new router to the <code>ROUTER_CLS</code> dictionary. Then, you can use immediately the new router in the server or evaluation framework.</p>
<p dir="auto">There is only a single method to implement: <code>calculate_strong_win_rate</code>, which takes in the user prompt and returns the win rate for the strong model conditioned on that given prompt - if this win rate is great than user-specified cost threshold, then the request is routed to the strong model. Otherwise, it is routed to the weak model.</p>

<p dir="auto">To add a new benchmark to RouteLLM, implement the abstract <code>Benchmark</code> class in <code>benchmarks.py</code> and update the <code>evaluate.py</code> module to properly initialize the new benchmark class. Ideally, the results for the benchmark should be precomputed to avoid having to regenerate the results for each evaluation run -- see the existing benchmarks for examples on how to do this.</p>

<p dir="auto">The code in this repository is based on the research from the <a href="https://arxiv.org/abs/2406.18665" rel="nofollow">paper</a>. Please cite if you find the repository helpful.</p>
<div data-snippet-clipboard-copy-content="@misc{ong2024routellmlearningroutellms,
      title={RouteLLM: Learning to Route LLMs with Preference Data},
      author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
      year={2024},
      eprint={2406.18665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.18665},
}"><pre><code>@misc{ong2024routellmlearningroutellms,
      title={RouteLLM: Learning to Route LLMs with Preference Data},
      author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
      year={2024},
      eprint={2406.18665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.18665},
}
</code></pre></div>
</article></div></div>
  </body>
</html>
