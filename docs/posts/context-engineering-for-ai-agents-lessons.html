<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">Original</a>
    <h1>Context Engineering for AI Agents: Lessons</h1>
    
    <div id="readability-page-1" class="page"><div><div spellcheck="false" data-slate-editor="true" data-slate-node="value" contenteditable="false" zindex="-1"><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">2025/7/18    -</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">-</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Yichao &#39;Peak&#39; Ji</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">At the very beginning of the </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://manus.im/app" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Manus</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> project, my team and I faced a key decision: should we train an end-to-end agentic model using open-source foundations, or build an agent on top of the </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://arxiv.org/abs/2301.00234" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">in-context learning</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> abilities of frontier models?</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Back in my first decade in NLP, we didn&#39;t have the luxury of that choice. In the distant days of </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://arxiv.org/abs/1810.04805" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">BERT</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> (yes, it&#39;s been seven years), models had to be fine-tuned—and evaluated—before they could transfer to a new task. That process often took weeks per iteration, even though the models were tiny compared to today&#39;s LLMs. For fast-moving applications, especially pre–PMF, such </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">slow feedback loops</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> are a deal-breaker. That was a bitter lesson from my last startup, where I trained models from scratch for </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://en.wikipedia.org/wiki/Open_information_extraction" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">open information extraction</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> and semantic search. Then came </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://arxiv.org/abs/2005.14165" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">GPT-3</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> and </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://arxiv.org/abs/2210.11416" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Flan-T5</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, and my in-house models became irrelevant overnight. Ironically, those same models marked the beginning of in-context learning—and a whole new path forward.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">That hard-earned lesson made the choice clear: </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Manus would bet on context engineering</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. This allows us to ship improvements in hours instead of weeks, and kept our product orthogonal to the underlying models: </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">If model progress is the rising tide, we want Manus to be the boat</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, not the pillar stuck to the seabed.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Still, context engineering turned out to be anything but straightforward. It&#39;s an experimental science—and we&#39;ve rebuilt our agent framework four times, each time after discovering a better way to shape context. We affectionately refer to this manual process of architecture searching, prompt fiddling, and empirical guesswork as &#34;</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Stochastic </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Graduate</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> Descent</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">&#34;. It&#39;s not elegant, but it works.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">This post shares the local optima we arrived at through our own &#34;SGD&#34;. If you&#39;re building your own AI agent, I hope these principles help you converge faster.</span></span></span></p></div><p><h3 data-slate-node="element" data-anchor="designaroundthekvcache" data-slug="designaroundthekvcache6"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Design Around the KV-Cache</span></span></span></h3></p><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">If I had to choose just one metric, I&#39;d argue that the </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">KV-cache hit rate</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> is the single most important metric for a production-stage AI agent. It directly affects both latency and cost. To understand why, let&#39;s look at how </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://arxiv.org/abs/2210.03629" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">a typical agent</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> operates:</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">After receiving a user input, the agent proceeds through a chain of tool uses to complete the task. In each iteration, the model selects an </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">action</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> from a predefined action space based on the current context. That action is then executed in the </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">environment</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> (e.g., Manus&#39;s virtual machine sandbox) to produce an </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">observation</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. The action and observation are appended to the context, forming the input for the next iteration. This loop continues until the task is complete.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">As you can imagine, the context grows with every step, while the output—usually a structured function call—remains relatively short. This makes the ratio between </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">prefilling</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> and </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">decoding</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> highly skewed in agents compared to chatbots. In Manus, for example, the average input-to-output token ratio is around </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">100:1</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Fortunately, contexts with identical prefixes can take advantage of </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://medium.com/@joaolages/kv-caching-explained-276520203249" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">KV-cache</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, which drastically reduces </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">time-to-first-token (TTFT)</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> and inference cost—whether you&#39;re using a self-hosted model or calling an inference API. And we&#39;re not talking about small savings: with Claude Sonnet, for instance, cached input tokens cost </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">0.30 USD/MTok</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, while uncached ones cost </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">3 USD/MTok</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">—a 10x difference.</span></span></span></p></div><div><div data-slate-node="element"><div><div data-slate-node="element" data-slate-void="true" data-slate-node-type="img"><p><span><img src="https://d1oupeiobkpcny.cloudfront.net/user_upload_by_module/markdown/310708716691272617/OhdKxGRSXCcuqOvz.png" alt="" id="q0J7syqeRdD0hh4oTEdnZZ"/></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div></div></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">From a context engineering perspective, improving KV-cache hit rate involves a few key practices:</span></span></span></p></div><div><p><span contenteditable="false"><span>1<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Keep your prompt prefix stable.</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> Due to the </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://en.wikipedia.org/wiki/Autoregressive_model" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">autoregressive</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> nature of LLMs, even a single-token difference can invalidate the cache from that token onward. A common mistake is including a timestamp—especially one precise to the second—at the beginning of the system prompt. Sure, it lets the model tell you the current time, but it also kills your cache hit rate.</span></span></span></span></p></div><div><p><span contenteditable="false"><span>2<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Make your context append-only.</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> Avoid modifying previous actions or observations. Ensure your serialization is deterministic. Many programming languages and libraries don&#39;t guarantee stable key ordering when serializing JSON objects, which can silently break the cache.</span></span></span></span></p></div><div><p><span contenteditable="false"><span>3<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Mark cache breakpoints explicitly when needed.</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> Some model providers or inference frameworks don&#39;t support automatic incremental prefix caching, and instead require manual insertion of cache breakpoints in the context. When assigning these, account for potential cache expiration and at minimum, ensure the breakpoint includes the end of the system prompt.</span></span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Additionally, if you&#39;re self-hosting models using frameworks like </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://github.com/vllm-project/vllm" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">vLLM</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, make sure </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://docs.vllm.ai/en/stable/design/v1/prefix_caching.html" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">prefix/prompt caching</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> is enabled, and that you&#39;re using techniques like </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">session IDs</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> to route requests consistently across distributed workers.</span></span></span></p></div><p><h3 data-slate-node="element" data-anchor="mask%2Cdon&#39;tremove" data-slug="mask%2Cdon&#39;tremove17"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Mask, Don&#39;t Remove</span></span></span></h3></p><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">As your agent takes on more capabilities, its action space naturally grows more complex—in plain terms, the </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">number of tools</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> explodes. The recent popularity of </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://modelcontextprotocol.io/introduction" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">MCP</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> only adds fuel to the fire. If you allow user-configurable tools, trust me: someone will inevitably plug hundreds of mysterious tools into your carefully curated action space. As a result, the model is more likely to select the wrong action or take an inefficient path. In short, your heavily armed agent gets dumber.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">A natural reaction is to design a dynamic action space—perhaps loading tools on demand using something </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">RAG</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">-like. We tried that in Manus too. But our experiments suggest a clear rule: unless absolutely necessary, </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">avoid dynamically adding or removing tools mid-iteration</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. There are two main reasons for this:</span></span></span></p></div><div><p><span contenteditable="false"><span>1<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">In most LLMs, tool definitions live near the front of the context after serialization, typically before or after the system prompt. So any change will invalidate the KV-cache for all subsequent actions and observations.</span></span></span></span></p></div><div><p><span contenteditable="false"><span>2<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">When previous actions and observations still refer to tools that are no longer defined in the current context, the model gets confused. Without </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://platform.openai.com/docs/guides/structured-outputs" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">constrained decoding</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, this often leads to </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">schema violations or hallucinated actions</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">.</span></span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">To solve this while still improving action selection, Manus uses a context-aware </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://en.wikipedia.org/wiki/Finite-state_machine" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">state machine</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> to manage tool availability. Rather than removing tools, it </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">masks the token logits</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> during decoding to prevent (or enforce) the selection of certain actions based on the current context.</span></span></span></p></div><div><div data-slate-node="element"><div><div data-slate-node="element" data-slate-void="true" data-slate-node-type="img"><p><span><img src="https://d1oupeiobkpcny.cloudfront.net/user_upload_by_module/markdown/310708716691272617/cWxINCvUfrmlbvfV.png" alt="" id="1AhN3gFLS0ozHluZjxAodh"/></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div></div></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">In practice, most model providers and inference frameworks support some form of </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">response prefill</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, which allows you to constrain the action space </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">without</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> modifying the tool definitions. There are generally three modes of function calling (we&#39;ll use the </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://github.com/NousResearch/Hermes-Function-Calling" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Hermes format</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> from NousResearch as an example):</span></span></span></p></div><div><p><span contenteditable="false"><span>•</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Auto</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> – The model may choose to call a function or not. Implemented by prefilling only the reply prefix: </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">&lt;|im_start|&gt;assistant</span></span></span></span></p></div><div><p><span contenteditable="false"><span>•</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Required</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> – The model must call a function, but the choice is unconstrained. Implemented by prefilling up to tool call token: </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">&lt;|im_start|&gt;assistant&lt;tool_call&gt;</span></span></span></span></p></div><div><p><span contenteditable="false"><span>•</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Specified</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> – The model must call a function </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">from a specific subset</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. Implemented by prefilling up to the beginning of the function name: </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">&lt;|im_start|&gt;assistant&lt;tool_call&gt;{&#34;name&#34;: “browser_</span></span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Using this, we constrain action selection by masking token logits directly. For example, when the user provides a new input, Manus must reply immediately instead of taking an action. We&#39;ve also deliberately designed action names with consistent prefixes—e.g., all browser-related tools start with </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">browser_</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, and command-line tools with </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">shell_</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. This allows us to easily enforce that the agent only chooses from a certain group of tools at a given state </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">without using stateful logits processors</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">These designs help ensure that the Manus agent loop remains stable—even under a model-driven architecture.</span></span></span></p></div><p><h3 data-slate-node="element" data-anchor="usethefilesystemascontext" data-slug="usethefilesystemascontext30"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Use the File System as Context</span></span></span></h3></p><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Modern frontier LLMs now offer context windows of 128K tokens or more. But in real-world agentic scenarios, that&#39;s often not enough, and sometimes even a liability. There are three common pain points:</span></span></span></p></div><div><p><span contenteditable="false"><span>1<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Observations can be huge</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, especially when agents interact with unstructured data like web pages or PDFs. It&#39;s easy to blow past the context limit.</span></span></span></span></p></div><div><p><span contenteditable="false"><span>2<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Model performance tends to degrade</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> beyond a certain context length, even if the window technically supports it.</span></span></span></span></p></div><div><p><span contenteditable="false"><span>3<!-- -->.</span></span><span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Long inputs are expensive</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">, even with prefix caching. You&#39;re still paying to transmit and prefill every token.</span></span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">To deal with this, many agent systems implement context truncation or compression strategies. But overly aggressive compression inevitably leads to information loss. The problem is fundamental: an agent, by nature, must predict the next action based on all prior state—and you </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">can&#39;t</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> reliably predict which observation might become critical ten steps later. From a logical standpoint, any irreversible compression carries risk.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">That&#39;s why we treat the </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">file system as the ultimate context</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> in Manus: unlimited in size, persistent by nature, and directly operable by the agent itself. The model learns to write to and read from files on demand—using the file system not just as storage, but as structured, externalized memory.</span></span></span></p></div><div><div data-slate-node="element"><div><div data-slate-node="element" data-slate-void="true" data-slate-node-type="img"><p><span><img src="https://d1oupeiobkpcny.cloudfront.net/user_upload_by_module/markdown/310708716691272617/sBITCOxGnHNUPHTD.png" alt="" id="FAEDMvC2PHHkVuajOaa4rh"/></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div></div></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Our compression strategies are always designed to be </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">restorable</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. For instance, the content of a web page can be dropped from the context as long as the URL is preserved, and a document&#39;s contents can be omitted if its path remains available in the sandbox. This allows Manus to shrink context length without permanently losing information.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">While developing this feature, I found myself imagining what it would take for a </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">State Space Model (SSM)</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> to work effectively in an agentic setting. Unlike Transformers, SSMs lack full attention and struggle with long-range backward dependencies. But if they could master file-based memory—externalizing long-term state instead of holding it in context—then their speed and efficiency might unlock a new class of agents. Agentic SSMs could be the real successors to </span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://arxiv.org/abs/1410.5401" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Neural Turing Machines</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">.</span></span></span></p></div><p><h3 data-slate-node="element" data-anchor="manipulateattentionthroughrecitation" data-slug="manipulateattentionthroughrecitation40"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Manipulate Attention Through Recitation</span></span></span></h3></p><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">If you&#39;ve worked with Manus, you&#39;ve probably noticed something curious: when handling complex tasks, it tends to create a </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">todo.md</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> file—and update it step-by-step as the task progresses, checking off completed items.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">That&#39;s not just cute behavior—it&#39;s a deliberate mechanism to </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">manipulate attention</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">.</span></span></span></p></div><div><div data-slate-node="element"><div><div data-slate-node="element" data-slate-void="true" data-slate-node-type="img"><p><span><img src="https://d1oupeiobkpcny.cloudfront.net/user_upload_by_module/markdown/310708716691272617/OYpTzfPZaBeeWFOx.png" alt="" id="ptf9bK7is0cJk9kodE3wBb"/></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div></div></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">A typical task in Manus requires around </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">50 tool calls</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> on average. That&#39;s a long loop—and since Manus relies on LLMs for decision-making, it&#39;s vulnerable to drifting off-topic or forgetting earlier goals, especially in long contexts or complicated tasks.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">By constantly rewriting the todo list, Manus is </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">reciting its objectives into the end of the context</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. This pushes the global plan into the model&#39;s recent attention span, avoiding &#34;</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">lost-in-the-middle</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">&#34; issues and reducing goal misalignment. In effect, it&#39;s using natural language to bias its own focus toward the task objective—without needing special architectural changes.</span></span></span></p></div><p><h3 data-slate-node="element" data-anchor="keepthewrongstuffin" data-slug="keepthewrongstuffin46"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Keep the Wrong Stuff In</span></span></span></h3></p><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Agents make mistakes. That&#39;s not a bug—it&#39;s reality. Language models hallucinate, environments return errors, external tools misbehave, and unexpected edge cases show up all the time. In multi-step tasks, failure is not the exception; it&#39;s part of the loop.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">And yet, a common impulse is to hide these errors: clean up the trace, retry the action, or reset the model&#39;s state and leave it to the magical &#34;</span></span></span><a data-slate-node="element" data-slate-inline="true" href="https://arxiv.org/abs/2405.00492" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">temperature</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">&#34;. That feels safer, more controlled. But it comes at a cost: </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Erasing failure removes evidence</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. And without evidence, the model can&#39;t adapt.</span></span></span></p></div><div><div data-slate-node="element"><div><div data-slate-node="element" data-slate-void="true" data-slate-node-type="img"><p><span><img src="https://d1oupeiobkpcny.cloudfront.net/user_upload_by_module/markdown/310708716691272617/dBjZlVbKJVhjgQuF.png" alt="" id="TkwQ2gnSQ7umRNzUacbBJX"/></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div></div></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">In our experience, one of the most effective ways to improve agent behavior is deceptively simple: </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">leave the wrong turns in the context</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. When the model sees a failed action—and the resulting observation or stack trace—it implicitly updates its internal beliefs. This shifts its prior away from similar actions, reducing the chance of repeating the same mistake.
In fact, we believe </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">error recovery</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> is one of the clearest indicators of true agentic behavior. Yet it&#39;s still underrepresented in most academic work and public benchmarks, which often focus on task success under ideal conditions.</span></span></span></p></div><p><h3 data-slate-node="element" data-anchor="don&#39;tgetfewshotted" data-slug="don&#39;tgetfewshotted51"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Don&#39;t Get Few-Shotted</span></span></span></h3></p><div><p><a data-slate-node="element" data-slate-inline="true" href="https://www.promptingguide.ai/techniques/fewshot" rel="noreferrer noopener" target="_blank"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Few-shot prompting</span></span></span></a><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> is a common technique for improving LLM outputs. But in agent systems, it can backfire in subtle ways.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Language models are excellent mimics; they </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">imitate the pattern of behavior</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true"> in the context. If your context is full of similar past action-observation pairs, the model will tend to follow that pattern, even when it&#39;s no longer optimal.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">This can be dangerous in tasks that involve repetitive decisions or actions. For example, when using Manus to help review a batch of 20 resumes, the agent often falls into a rhythm—repeating similar actions simply because that&#39;s what it sees in the context. This leads to drift, overgeneralization, or sometimes hallucination.</span></span></span></p></div><div><div data-slate-node="element"><div><div data-slate-node="element" data-slate-void="true" data-slate-node-type="img"><p><span><img src="https://d1oupeiobkpcny.cloudfront.net/user_upload_by_module/markdown/310708716691272617/IIyBBdwwuMDJUnUc.png" alt="" id="Vb0GNilfbfE4aTTESpiYSm"/></span></p><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="z" data-slate-length="0">﻿</span></span></span></p></div></div></div></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">The fix is to </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">increase diversity</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. Manus introduces small amounts of structured variation in actions and observations—different serialization templates, alternate phrasing, minor noise in order or formatting. This controlled randomness helps break the pattern and tweaks the model&#39;s attention.
In other words, </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">don&#39;t few-shot yourself into a rut</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. The more uniform your context, the more brittle your agent becomes.</span></span></span></p></div><p><h3 data-slate-node="element" data-anchor="conclusion" data-slug="conclusion57"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Conclusion</span></span></span></h3></p><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">Context engineering is still an emerging science—but for agent systems, it&#39;s already essential. Models may be getting stronger, faster, and cheaper, but no amount of raw capability replaces the need for memory, environment, and feedback. How you shape the context ultimately defines how your agent behaves: how fast it runs, how well it recovers, and how far it scales.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">At Manus, we&#39;ve learned these lessons through repeated rewrites, dead ends, and </span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">real-world testing across millions of users</span></span></span><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">. None of what we&#39;ve shared here is universal truth—but these are the patterns that worked for us. If they help you avoid even one painful iteration, then this post did its job.</span></span></span></p></div><div><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-string="true">The agentic future will be built one context at a time. Engineer them well.</span></span></span></p></div></div></div></div>
  </body>
</html>
