<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kuzudb.com/docusaurus/blog/llms-graphs-part-2/">Original</a>
    <h1>RAG Using Unstructured Data and Role of Knowledge Graphs</h1>
    
    <div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p><a href="https://kuzudb.com/docusaurus/blog/llms-graphs-part-1" target="_blank" rel="noopener noreferrer">In my previous post</a>,
I gave an overview of question answering (Q&amp;A) systems that use LLMs
over private enterprise data. I covered the architectures of these systems, the common tools
developers use to build these systems when the enterprise data used is structured,
i.e., data exists as records stored in some DBMS, relational or graph. I was referring to
these systems as <em>RAG systems using structured data</em>. In this post, I cover <em>RAG systems
that use unstructured data</em>, such as text files,
pdf documents, or internal html pages in an enterprise. I will refer to these as RAG-U systems
or sometimes simply as RAG-U (should have used the term RAG-S in the previous post!).</p><p>To remind readers, I decided to
write these two posts after doing a lot of reading in the space to understand the role of
knowledge graph (KGs) and graph DBMSs in LLM applications. My goals are (i) to overview the field to readers who want to get started
but are intimidated by the area; and (ii) point to several future work directions that I find
important.<sup id="fnref-1-c9223b"><a href="#fn-1-c9223b">1</a></sup></p><div><p><span><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>TL;DR: The key takeaways from this post are:</p><div><ul><li><strong>Two design decisions when preparing a RAG-U system are (i) &#34;What additional data&#34; to put in prompts; and (ii) &#34;How to store and fetch&#34; the additional data.</strong>: Explored options for types of additional data include chunks of texts, full documents, or automatically extracted triples from documents. There are different ways to store and fetch this additional data, such as use of vector indices. Many combinations of this design space are not yet explored.</li><li><strong>Standard RAG-U</strong>: A common design point, which I will call the standard RAG-U, is to add chunks of documents as additional data and store them in a vector index. I found some of the most technically deep and interesting future work directions in this space, e.g., extending vectors to matrices.</li><li><strong>An envisioned role for KGs in a RAG-U system is as a means to link chunks of text:</strong> If chunks can be linked to entities in an existing KG, then one can connect chunks to each other through the relationships in KG.
These connections can be exploited to retrieve more relevant chunks. This is a promising direction but
its potential benefits should be subjected to rigorously evaluation, e.g., as major SIGIR publications evaluate a new retrieval technique. It won&#39;t pick up through commercial blog posts.</li><li><strong>What if an enterprise does not have a KG?</strong> The hope of using KGs to do better retrieval in absence of a pre-existing KG raises the question and never ending quest of <em>automatic knowledge graph construction</em>. This is a very interesting topic and most recent research here uses LLMs for this purpose but: (i) LLMs seem behind in extracting quality knowledge graph facts; and (ii) it&#39;s not clear if use of LLMs for this purpose at scale is economically feasible.</li></ul></div></div><h2 id="rag-u-overview">RAG-U Overview<a href="#rag-u-overview" aria-label="Direct link to RAG-U Overview" title="Direct link to RAG-U Overview">​</a></h2><p>I will skip the overview of RAG systems, which I covered in <a href="https://kuzudb.com/docusaurus/blog/llms-graphs-part-1#a-note-on-the-term-rag" target="_blank" rel="noopener noreferrer">the previous post</a>.
The picture of RAG systems that use unstructured data looks as follows:</p><p><img loading="lazy" src="https://kuzudb.com/docusaurus/assets/images/rag-unstructured-overview-9cf06430dbd5bc1a010aa69dd7b98f6a.png" width="600"/></p><p>An enterprise has a corpus of unstructured data, i.e., some documents with text.
As a preprocessing step (omitted from the figure), the information in these documents are indexed and
stored in some storage system. The figure labels the 4 overall steps in a RAG-U system:</p><ol><li>A natural language query <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> comes into the RAG-U system.</li><li>Parts of the corpus of unstructured data that is expected to be helpful in answering <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
is fetched from some storage system.</li><li>The fetched data along with <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> given to an LLM.</li><li>LLM produces a natural language answer <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>A</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> for <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>.</li></ol><p>Any system built along these 4 high-level steps needs to make two design choices:</p><p><strong>Design Choice 1: What is the additional data?</strong> Among the posts, documentation, and demonstrations
I have read, I have seen three designs:</p><ul><li>Chunks of documents</li><li>Entire documents</li><li>Triples extracted from documents</li></ul><p><strong>Design Choice 2: How to store and fetch the additional data?</strong> Here, I have seen the following designs:</p><ul><li>Vector Index</li><li>Vector Index + Knowledge Graph (stored in a GDBMS)</li><li>GDBMS (for storing triples)</li></ul><p>Many combinations of these two choices are possible and can be tried. Each choice
can effectively be understood as a <em>retrieval heuristic</em> to fetch quality content that can
help LLMs answer <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> more accurately.
I will cover a few of the ones that I have seen but others are certainly possible and should be tried by people
developing RAG-U systems.</p><h2 id="standard-rag-u-chunks-of-documents-stored-in-a-vector-index">Standard RAG-U: Chunks of Documents Stored in a Vector Index<a href="#standard-rag-u-chunks-of-documents-stored-in-a-vector-index" aria-label="Direct link to Standard RAG-U: Chunks of Documents Stored in a Vector Index" title="Direct link to Standard RAG-U: Chunks of Documents Stored in a Vector Index">​</a></h2><p>Standard RAG-U is what you will read about in most places. Its design is as follows: (i) we split the text in the
documents into (possibly overlapping) &#34;chunks&#34;; (ii) we embed these chunks into vectors, i.e., high dimensional points, using
a text embedding model (many off-the-shelf open-source models exist from <a href="https://platform.openai.com/docs/guides/embeddings" target="_blank" rel="noopener noreferrer">OpenAI</a>, <a href="https://docs.cohere.com/reference/embed" target="_blank" rel="noopener noreferrer">Cohere</a>, and <a href="https://huggingface.co/blog/getting-started-with-embeddings" target="_blank" rel="noopener noreferrer">Hugging Face</a>);
and (iii) we store these vectors in a vector index. For example, see LangChain main documentation
on &#34;<a href="https://python.langchain.com/docs/use_cases/question_answering/" target="_blank" rel="noopener noreferrer">Q&amp;A with RAG</a>&#34; or LlamaIndex&#39;s
&#34;<a href="https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval.html" target="_blank" rel="noopener noreferrer">Building a RAG from Scratch</a>&#34; documentation.
The below figure shows the pre-processing and indexing steps of standard RAG-U:</p><p><img loading="lazy" src="https://kuzudb.com/docusaurus/assets/images/standard-rag-preprocessing-2ed0ed6a8e1e4583cc0036aa0e7a95f1.png" width="600"/></p><p><strong>First a note on vector indices:</strong> A vector index is one that indexes a
set of d-dimensional vectors and given a query vector w can answer several queries:
(i) <em>pure search</em>: does w exist in the index?; (ii) <em>k nearest neighbors</em>: return
the k vectors closest to w; or (iii) <em>range queries</em>: return vectors that are within
a radius r of w. There have been decades of work on this topic.
If d is very small, say 3 or 4, there are &#34;exact spatial indices&#34; like <a href="https://en.wikipedia.org/wiki/Quadtree" target="_blank" rel="noopener noreferrer">quad trees</a> (for 2D only), <a href="https://en.wikipedia.org/wiki/R-tree" target="_blank" rel="noopener noreferrer">r-trees</a>, or <a href="https://en.wikipedia.org/wiki/K-d_tree" target="_blank" rel="noopener noreferrer">k-d trees</a>.
These indices have good construction and query times when d is small but their performance degrades
fast when d increases and they quickly become impractical.
There have been some good work to index high-dimensional vectors as well.
<a href="https://dl.acm.org/doi/10.1007/s007780200060" target="_blank" rel="noopener noreferrer">SA-trees</a> by Navarro is the core
technique that underlies the nowadays popular indices, such as <a href="https://arxiv.org/abs/1603.09320" target="_blank" rel="noopener noreferrer">hierarchical navigable small-world graph (HNSW) indices</a>, which are extensions of <a href="https://www.sciencedirect.com/science/article/abs/pii/S0306437913001300" target="_blank" rel="noopener noreferrer">navigable small world (NSW)
indices</a>.
Navarro&#39;s SA-tree index returns exact results as well<sup id="fnref-2-c9223b"><a href="#fn-2-c9223b">2</a></sup> but does not have good query times.
In Navarro&#39;s experiments, even for
relatively small dimensions such as 10-20, sa-tree can scan 10-100% of all vectors in the index
for queries that need to return less than 1% of the vectors.
SNW and HSNW instead are not exact indices. They are called approximate indices but they are not
even approximate in the sense of having any approximation guarantees in their query results.
They are heuristic-based indices that can index very high-dimensional vector and are fast
both in their construction and their query times. Further, their query results are shown to be quite accurate empirically.
HNSW indices are nowadays used by vector databases like <a href="https://www.pinecone.io/learn/series/faiss/hnsw/" target="_blank" rel="noopener noreferrer">Pinecone</a>
and <a href="https://weaviate.io/developers/weaviate/concepts/vector-index" target="_blank" rel="noopener noreferrer">Weaviate</a> or search engine libraries such as <a href="https://lucene.apache.org/core/9_1_0/core/org/apache/lucene/util/hnsw/HnswGraph.html" target="_blank" rel="noopener noreferrer">Lucene</a> in their vector indices.
To understand these indices, I highly suggest first reading the Navarro paper
paper, which is the foundation. It&#39;s also a great example of a well-written database paper: one that
makes a very clear contribution and is explained in a very clean technical language.</p><p>Back to RAG-U. After the preprocessing step, the vector index that contains the chunks of documents is used
in a RAG-U system as follows:</p><p><img loading="lazy" src="https://kuzudb.com/docusaurus/assets/images/standard-rag-overview-1a978709d941cd0aa9c64f7135ee6b2c.png" width="600"/></p><p>The step are as follows: (i) The question <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> is first embedded into
the same d-dimensional vector space as the chunks were. Let&#39;s call this vector <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">v_{Q}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>Q</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>;
(ii) k nearest neighbors <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>w</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, ..., w_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">v_{Q}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>Q</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> are searched in the vector index (for some value of k) ; and (iii) the chunks
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>C</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_1, C_2, ..., C_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> that correspond to <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>w</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, ..., w_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> are retrieved (in the figure above, these
are the chunks in red boxes) and put into the LLM prompt along with <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. The
hope is that the chunks whose vector representation were close to <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">v_{Q}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>Q</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> contain
useful information for the LLM to answer <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. In practice there could be more steps to rank those <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span></span> chunks
and maybe select a fewer number of them to give to the LLM.</p><p>Overall, my reading on standard RAG-U was quite technically deep. That&#39;s not surprising since
the success of these pipelines depend on two core technical problems:</p><ol><li>How &#34;good&#34; are the embeddings that are inserted into the vector index, i.e., how well does it
capture the relatedness of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> to the chunks. This is a core problem in the neural IR.</li><li>How accurate is the vector index in finding top-k nearest neighbors to the vector embedding <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">v_{Q}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>Q</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>.
This is a core database problem.</li></ol><p><em>Important Future Work 1</em>: I believe we should be seeing more exciting work coming up in this space. One
topic is the use of matrices instead of vectors to embed chunks and questions.
This is done in the <a href="https://huggingface.co/colbert-ir/colbertv2.0" target="_blank" rel="noopener noreferrer">ColBERT-style</a> &#34;neural retrieval models&#34;
that are shown to work well on some Q&amp;A benchmarks. Indexing and retrieval of these matrices is an interesting
topic and I have even seen some off-the-shelf tools, e.g., the <a href="https://llamahub.ai/l/llama_packs-ragatouille_retriever?from=llama_packs" target="_blank" rel="noopener noreferrer">RAGatouille package of LlamaIndex</a>, that allows developers to replace the vectors in the
standard RAG-U figure above with matrices. Tons of good future work is possible in this space from improving the accuracy
and efficiency of the vector/matrix indices to the evaluation of RAG-U systems that use these vectors.</p><h2 id="first-envisioned-role-of-knowledge-graphs-in-rag-u-explicitly-linking-chunks">First Envisioned Role of Knowledge Graphs in RAG-U: Explicitly Linking Chunks<a href="#first-envisioned-role-of-knowledge-graphs-in-rag-u-explicitly-linking-chunks" aria-label="Direct link to First Envisioned Role of Knowledge Graphs in RAG-U: Explicitly Linking Chunks" title="Direct link to First Envisioned Role of Knowledge Graphs in RAG-U: Explicitly Linking Chunks">​</a></h2><p>One limitation of standard RAG-U is that the chunks are treated as isolated pieces of text. To address this problem,
several posts
that I read (<a href="https://medium.com/neo4j/implementing-advanced-retrieval-rag-strategies-with-neo4j-c968a002c513" target="_blank" rel="noopener noreferrer">1</a>, <a href="https://medium.com/neo4j/using-a-knowledge-graph-to-implement-a-devops-rag-application-b6ba24831b16" target="_blank" rel="noopener noreferrer">2</a>) envision linking these chunks to each other using a KG (or another form of graph). Compared to standard RAG-U,
the design choice for &#34;what additional data&#34; is still document chunks but &#34;how to fetch&#34; is different
and it is a mix of vector index + a KG stored in a GDBMS.
The preprocessing over standard RAG-U (see the preprocessing figure above)
would be enhanced with an additional step as follows:</p><p><img loading="lazy" src="https://kuzudb.com/docusaurus/assets/images/kg-enhanced-rag-preprocessing-0258a645794605b4b9469f2756c4a562.png" width="600"/></p><p>That is, using some entity extraction mechanism, the chunks would be linked to the entities that
they mention in the KG (assuming the KG contains these entities as nodes). You can think of this linking
as the adding new edges to the KG that relate entities to some chunkIDs that identify the chunks in the vector index.
After the preprocessing step, the standard RAG-U system would be enhanced as follows:</p><p><img loading="lazy" src="https://kuzudb.com/docusaurus/assets/images/kg-enhanced-rag-overview-57a9ca61a3e3739118a711237fa28740.png" width="600"/></p><p>Similar to standard RAG-U, we have a vector index and additionally a KG, say stored in a GDBMS.
As before <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> is embedded into a vector <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">v_Q</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>Q</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, whose k nearest neighbors
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>w</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">w_1, w_2, ..., w_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> and their corresponding chunks <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>C</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_1, C_2, ..., C_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
are found in the vector index.
Then, the system extracts additional chunks based on some graph
traversal heuristic. A simple heuristic is to traverse from the <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>C</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_1, C_2, ..., C_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> to all entities,
say {<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">e_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">e_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, ..., <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">e_m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>}, that are mentioned in <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>C</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_1, C_2, ..., C_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>...</span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>.
Then, we can optionally explore the neighborhood of these entities
to extract other entities, say {<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">e_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, ..., <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">e_m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">e_{m+1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>m</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, ..., <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">e_n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>}, where <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">e_{m+1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>m</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> to <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">e_n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
are the new entities extracted. Then, we further find other chunks that mention these entities. In the figure
above I&#39;m simulating this by having a third red box that was not retrieved in the standard RAG-U figure. Now through
another ranking, we can obtain another top-k chunks amongst this new set of chunks and put them into the prompt.</p><p>This vision is interesting and several prior papers also hint at similar related use of
KGs in Q&amp;A applications. The most interesting paper I read that&#39;s related to this approach was this <a href="https://aclanthology.org/P19-1598.pdf" target="_blank" rel="noopener noreferrer">ACL 2019 paper</a>. This paper pre-dates the current LLMs and is not about RAG. Instead, it
maps the entities mentioned in a question to entities in a KG, and then extracts the subgraph of relations between these
entities from the KG. Then, the relations and entities in this subgraph are used as possible
answers to the question (in some sense, this is also a form of RAG).
The paper&#39;s approach does not connect the chunks but connects the entities in the question using a KG.
Overall, I think the idea of linking chunks through the entities that they mention is promising
and I want to identify three important future work here that can push this approach forward.</p><p><strong>Important Future Work 2:</strong> This approach assumes that the enterprise already
has a knowledge graph. Although I am a strong believer that enterprises
should invest in the construction of clean enterprise-level KGs with
well defined and consistent vocabularies,
in practice many enterprises do not have readily-available KGs. Therefore the use of this style
of KG-enhanced RAG-U approaches rely on tools that can generate KGs. This is a never ending
quest in academic circles and I&#39;ll say more about this below (see &#34;Important Future Work 5&#34;).</p><p><em>Important Future Work 3:</em> The graph heuristic I described above to extract further chunks
is only one that can be explored amongst many others. For example, one can map the entities in the question
to nodes in the KG, find shortest paths between them, and retrieve the chunks that mention the nodes/entities
on these paths. These variants need to be systematically evaluated to optimize this approach.</p><p><em>Important Future Work 4:</em> Although variants of this approach, such as the ACL paper I mentioned have a lot of technical depth
and rigorous evaluations, this approach so far appears only in blog posts which don&#39;t present an in-depth study. This approach
needs to be subjected to a rigorous evaluation on Q&amp;A benchmarks.</p><p>In the <a href="https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html" target="_blank" rel="noopener noreferrer">LlamaIndex KnowledgeGraphIndex</a> package, there is one more usage of KGs in RAG-U applications.
In this approach, the answer to the &#34;what additional data&#34; question is &#34;triples extracted from the unstructured documents&#34;.
The answer to the &#34;how to fetch&#34; question is to do a retrieval of these triples from a GDBMS.
Here is the preprocessing step:</p><p><img loading="lazy" src="https://kuzudb.com/docusaurus/assets/images/triples-based-rag-preprocessing-a78585b0799b8bc8ffb947f170bff914.png" width="600"/></p><p>Using some triple extraction, the unstructured document is pre-processed to generate a KG. I will discuss
this step further but overall you can use either a triple extraction model, such as <a href="https://huggingface.co/Babelscape/rebel-large" target="_blank" rel="noopener noreferrer">REBEL</a> or another model, or an LLM directly. Both can be used off-the-shelf. Then, these triples, which form
a KG, are stored in a GDBMS and used in RAG in some form. I&#39;m giving one example approach below but others
are possible. The approach I will show
is implemented in the examples used in LlamaIndex&#39;s documentations using <a href="https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html" target="_blank" rel="noopener noreferrer">LlamaIndex KnowledgeGraphIndex</a>:</p><p><img loading="lazy" src="https://kuzudb.com/docusaurus/assets/images/triples-based-rag-overview-8034260dd33e3c205ab472f2f292eb2b.png" width="600"/></p><p>The triples are stored in a GDBMS. You can use a <a href="https://docs.llamaindex.ai/en/stable/community/integrations/graph_stores.html" target="_blank" rel="noopener noreferrer">LlamaIndex GraphStore</a> for this and Kùzu has an implementation; see the <a href="https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KuzuGraphDemo.html" target="_blank" rel="noopener noreferrer">KuzuGraphStore demo here</a>. The system extract entities using <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, using some
entity or keyword extractor. In the LlamaIndex demos, this is done by using an LLM. Specifically,
LLM is prompted with the following <a href="https://github.com/run-llama/llama_index/blob/ce82bd42329b56bca2a6a44e0f690ebedaf1f002/llama_index/prompts/default_prompts.py#L147" target="_blank" rel="noopener noreferrer">prompt</a>: <code>A question is provided below. Given the question, extract up to {max_keywords}
keywords from the text....</code> etc. These keywords are used
to extract triples from the GDBMS by using them in the query sent to the GDBMS as shown in the above figure.
Finally the returned triples are given to the LLM prompt as additional data to help answer <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>.</p><p>LlamaIndex offer other ways to extract these triples that are also readily available to use. For example,
you can embed these triples into vectors and use a vector index to fetch them. So although the final
additional data is still triples, how they&#39;re fetched is through a vector index and not a GDBMS. Other options
are also possible.</p><p>I want to make three points here. First, notice that extracting triples also
provides a means to link the text in the unstructured text, which was a
shortcoming I had highlighted in standard RAG-U. Second, by putting triples into prompts instead of chunks, you are also probably
saving the tokens you are using in your LLM applications. That&#39;s because triples are like summaries
of the statements in more verbose sentences in chunks.
Third, the success of such RAG applications depends on the quality of the triples
extracted in the pre-processing step, which is the next future work direction I want to highlight:</p><p><em>Important Future Work 5:</em> The success of RAG-U applications that use triples or the KG-enhanced standard
RAG-U applications depend on the availability of a technology that can automatically
extract knowledge graphs from unstructured documents.</p><p>This is a fascinating topic and is a never ending quest in research. Here is
an <a href="https://arxiv.org/pdf/2302.05019.pdf" target="_blank" rel="noopener noreferrer">extensive survey</a> with 358 citations that scared me so I decided to
skip it. But my point is that this has been a never ending research topic. The most recent
work I see here is on using LLMs for this task. I can recommend these two papers here: <a href="https://arxiv.org/abs/2208.11057" target="_blank" rel="noopener noreferrer">1</a> and <a href="https://arxiv.org/pdf/2308.10168.pdf" target="_blank" rel="noopener noreferrer">2</a>.
General conclusions so far are that LLMs are not yet competitive with specialized models
on extracting high quality triples. We&#39;ll see how far they will go. Surprisingly, a very important question
for which I could not find much to read on is this:</p><p><em>Important Future Work 6:</em> How economical would be the use of LLMs to extract KGs at scale (when they&#39;re competitive with
specialized methods)? </p><p>So could we ever dream of using LLMs to extract billions of triples from a large corpus of unstructured documents? Probably not
if you&#39;re using OpenAI APIs, as it would be painfully expensive, or even if you&#39;re running
your own model, as it would be excruciatingly slow. So I am a bit pessimistic here. My instinct
is that you might be able to generate KGs from unstructured documents using the slightly older
techniques like designing your own models or using extractor-based approaches like
<a href="http://deepdive.stanford.edu/" target="_blank" rel="noopener noreferrer">DeepDive</a><sup id="fnref-3-c9223b"><a href="#fn-3-c9223b">3</a></sup> and <a href="https://tomaarsen.github.io/SpanMarkerNER/" target="_blank" rel="noopener noreferrer">SpanMarker</a><sup id="fnref-4-c9223b"><a href="#fn-4-c9223b">4</a></sup>. I know it&#39;s not exciting to not use LLMs,
but you&#39;re likely to extract much higher quality triples and much more cheaply with specialized models.
So I don&#39;t know who really
thinks it could one day be a good idea to use LLMs to extract triples from large corpuses.</p><h2 id="agents-developing-rag-systems-that-use-both-structured--unstructured-data">Agents: Developing RAG Systems That Use Both Structured &amp; Unstructured Data<a href="#agents-developing-rag-systems-that-use-both-structured--unstructured-data" aria-label="Direct link to Agents: Developing RAG Systems That Use Both Structured &amp; Unstructured Data" title="Direct link to Agents: Developing RAG Systems That Use Both Structured &amp; Unstructured Data">​</a></h2><p>Let me now start wrapping up. In my last post and this one, I covered RAG systems using structured and unstructured data.
There are several tools you can use to develop RAG systems that retrieve data from both
structured records or one that conditionally retrieves from one or the other. <a href="https://python.langchain.com/docs/modules/agents/" target="_blank" rel="noopener noreferrer">LangChain Agents</a>
and <a href="https://docs.llamaindex.ai/en/stable/use_cases/agents.html" target="_blank" rel="noopener noreferrer">LlamaIndex Agents</a>
make it easy to develop such pipelines. You can for example instruct the &#34;Agent&#34; to take one of two actions
conditionally as follows:
&#34;if the question is about counting or aggregations retrieve records from the GDBMS by converting <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>N</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_{NL}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span><span><span><span><span><span><span></span><span><span><span>N</span><span>L</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> to a Cypher query;
otherwise follow the RAG-U pipeline to retrieve chunks from documents.&#34; These are essentially
tools to develop a control logic over your LLM applications. It reminds me of the good old days when
there was a crazy hype around MapReduce-like &#34;big data systems&#34; and several initial works, such as <a href="https://dl.acm.org/doi/abs/10.1145/1376616.1376726" target="_blank" rel="noopener noreferrer">Pig Latin</a>
and <a href="https://www.usenix.org/legacy/event/nsdi11/tech/full_papers/Murray.pdf" target="_blank" rel="noopener noreferrer">Ciel</a>, immediately were addressing how to develop libraries/languages
over these systems to implement advanced control flows. Agents, or the recent <a href="https://github.com/langchain-ai/langgraph" target="_blank" rel="noopener noreferrer">LangGraph</a>,
seem like initial answers to the question of &#34;how do you program advanced LLM applications?&#34;</p><h2 id="final-words">Final Words<a href="#final-words" aria-label="Direct link to Final Words" title="Direct link to Final Words">​</a></h2><p>I want to conclude with two final points. First, there are many other applications that can
use LLMs and KGs beyond Q&amp;A. I don&#39;t have space to cover
them here. Here is a <a href="https://arxiv.org/pdf/2306.08302.pdf" target="_blank" rel="noopener noreferrer">survey paper</a> that attempts to organize
the work in this space. The topics vary from how KGs can be used to train better LLMs to
how LLMs can be used to construct KGs to how one could embed both text and KG triples together as vectors
to better train LLMs.</p><p>Second, I listed 3 possible answers for the &#34;what additional data&#34;
design decision and 3 possible answers for &#34;how to fetch&#34; design decision. I further mentioned different
graph-based heuristics to extract chunks (or triples) once you can link the information in the unstructured
documents to each other through a KG. Many combinations of these design decisions and many other graph heuristics
are not yet explored. So there is quite a lot to explore in this space. The overall impression I was left
with was that we need more technically deep material in the space, which will only come through rigorous evaluations
of these RAG systems on standard benchmarks, as done in SIGIR or ACL publications.
I went through SIGIR 2023 publications and did not find work on a Q&amp;A system that uses LLMs + KGs
similar to the approaches I covered here. I hope to see such papers in 2024.</p></div></div>
  </body>
</html>
