<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals">Original</a>
    <h1>AGENTS.md outperforms skills in our agent evals</h1>
    
    <div id="readability-page-1" class="page"><article data-grid-cell=""><div data-customer-story="false"><p>We expected <a href="https://agentskills.io/home" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://agentskills.io/home" data-zone="null">skills</a> to be the solution for teaching coding agents framework-specific knowledge. After building evals focused on Next.js 16 APIs, we found something unexpected.</p><p>A compressed 8KB docs index embedded directly in <code>AGENTS.md</code> achieved a 100% pass rate, while skills maxed out at 79% even with explicit instructions telling the agent to use them. Without those instructions, skills performed no better than having no documentation at all.</p><p>Here&#39;s what we tried, what we learned, and how you can set this up for your own Next.js projects.</p><h2><a href="#the-problem-we-were-trying-to-solve" id="the-problem-we-were-trying-to-solve"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>The problem we were trying to solve</b></h2><p>AI coding agents rely on training data that becomes outdated. <a href="https://nextjs.org/blog/next-16" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://nextjs.org/blog/next-16" data-zone="null">Next.js 16 introduces</a> APIs like <code>&#39;use cache&#39;</code>, <code>connection()</code>, and <code>forbidden()</code> that aren&#39;t in current model training data. When agents don&#39;t know these APIs, they generate incorrect code or fall back to older patterns.</p><p>The reverse can also be true, where you&#39;re running an older Next.js version and the model suggests newer APIs that don&#39;t exist in your project yet. We wanted to fix this by giving agents access to version-matched documentation.</p><h2><a href="#two-approaches-for-teaching-agents-framework-knowledge" id="two-approaches-for-teaching-agents-framework-knowledge"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>Two approaches for teaching agents framework knowledge</b></h2><p>Before diving into results, a quick explanation of the two approaches we tested:</p><ul><li><p><b>Skills</b> are an <a href="https://agentskills.io/" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://agentskills.io/" data-zone="null">open standard</a> for packaging domain knowledge that coding agents can use. A skill bundles prompts, tools, and documentation that an agent can invoke on demand. The idea is that the agent recognizes when it needs framework-specific help, invokes the skill, and gets access to relevant docs.</p></li><li><p><a href="https://agents.md/" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://agents.md/" data-zone="null"><b><code>AGENTS.md</code></b></a> is a markdown file in your project root that provides persistent context to coding agents. Whatever you put in <code>AGENTS.md</code> is available to the agent on every turn, without the agent needing to decide to load it. Claude Code uses <code>CLAUDE.md</code> for the same purpose.</p></li></ul><p>We built a Next.js docs skill and an <code>AGENTS.md</code> docs index, then ran them through our eval suite to see which performed better.</p><h2><a href="#we-started-by-betting-on-skills" id="we-started-by-betting-on-skills"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>We started by betting on skills</b></h2><p>Skills seemed like the right abstraction. You package your framework docs into a skill, the agent invokes it when working on Next.js tasks, and you get correct code. Clean separation of concerns, minimal context overhead, and the agent only loads what it needs. There&#39;s even a growing directory of reusable skills at <a href="https://skills.sh/" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://skills.sh/" data-zone="null">skills.sh</a>.</p><p>We expected the agent to encounter a Next.js task, invoke the skill, read version-matched docs, and generate correct code.</p><p>Then we ran the evals.</p><h2><a href="#skills-weren&#39;t-being-triggered-reliably" id="skills-weren&#39;t-being-triggered-reliably"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>Skills weren&#39;t being triggered reliably</b></h2><p>In 56% of eval cases, the skill was never invoked. The agent had access to the documentation but didn&#39;t use it. Adding the skill produced no improvement over baseline:</p><div data-geist-scroller="" data-version="v1"><div data-geist-scroller-container="" data-overflow="x"><div><table><tbody><tr><td><p><b>Configuration</b></p></td><td><p><b>Pass Rate</b></p></td><td><p><b>vs Baseline</b></p></td></tr><tr><td><p>Baseline (no docs)</p></td><td><p>53%</p></td><td><p>—</p></td></tr><tr><td><p>Skill (default behavior)</p></td><td><p>53%</p></td><td><p>+0pp</p></td></tr></tbody></table></div></div></div><p>Zero improvement. The skill existed, the agent could use it, and the agent chose not to. On the detailed Build/Lint/Test breakdown, the skill actually performed worse than baseline on some metrics (58% vs 63% on tests), suggesting that an unused skill in the environment may introduce noise or distraction.</p><p>This isn&#39;t unique to our setup. Agents not reliably using available tools is a <a href="https://developers.openai.com/blog/eval-skills" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://developers.openai.com/blog/eval-skills" data-zone="null">known limitation</a> of current models.</p><h2><a href="#explicit-instructions-helped,-but-wording-was-fragile" id="explicit-instructions-helped,-but-wording-was-fragile"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>Explicit instructions helped, but wording was fragile</b></h2><p>We tried adding explicit instructions to <code>AGENTS.md</code> telling the agent to use the skill.</p><div><div data-geist-code-block=""><pre><code data-geist-code-block="true"><div data-geist-code-block-line="true" id="C469bb65d-L1"><p><span>Before writing code, first explore the project structure, </span></p></div><div data-geist-code-block-line="true" id="C469bb65d-L2"><p><span>then invoke the nextjs-doc skill for documentation.</span></p></div></code></pre></div><p>Example instruction added to AGENTS.md to trigger skill usage.</p></div><p>This improved the trigger rate to 95%+ and boosted the pass rate to 79%.</p><div data-geist-scroller="" data-version="v1"><div data-geist-scroller-container="" data-overflow="x"><div><table><tbody><tr><td><p><b>Configuration</b></p></td><td><p><b>Pass Rate</b></p></td><td><p><b>vs Baseline</b></p></td></tr><tr><td><p>Baseline (no docs)</p></td><td><p>53%</p></td><td><p>—</p></td></tr><tr><td><p>Skill (default behavior)</p></td><td><p>53%</p></td><td><p>+0pp</p></td></tr><tr><td><p>Skill with explicit instructions</p></td><td><p>79%</p></td><td><p>+26pp</p></td></tr></tbody></table></div></div></div><p>A solid improvement. But we discovered something unexpected about how the instruction wording affected agent behavior.</p><p><b>Different wordings produced dramatically different results:</b></p><div data-geist-scroller="" data-version="v1"><div data-geist-scroller-container="" data-overflow="x"><div><table><tbody><tr><td><p><b>Instruction</b></p></td><td><p><b>Behavior</b></p></td><td><p><b>Outcome</b></p></td></tr><tr><td><p>&#34;You MUST invoke the skill&#34;</p></td><td><p>Reads docs first, anchors on doc patterns</p></td><td><p>Misses project context</p></td></tr><tr><td><p>&#34;Explore project first, then invoke skill&#34;</p></td><td><p>Builds mental model first, uses docs as reference</p></td><td><p>Better results</p></td></tr></tbody></table></div></div></div><p>Same skill. Same docs. Different outcomes based on subtle wording changes.</p><p>In one eval (the <code>&#39;use cache&#39;</code> directive test), the &#34;invoke first&#34; approach wrote correct <code>page.tsx</code> but completely missed the required <code>next.config.ts</code> changes. The &#34;explore first&#34; approach got both.</p><p>This fragility concerned us. If small wording tweaks produce large behavioral swings, the approach feels brittle for production use.</p><h2><a href="#building-evals-we-could-trust" id="building-evals-we-could-trust"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>Building evals we could trust</b></h2><p>Before drawing conclusions, we needed evals we could trust. Our initial test suite had ambiguous prompts, tests that validated implementation details rather than observable behavior, and a focus on APIs already in model training data. We weren&#39;t measuring what we actually cared about.</p><p>We hardened the eval suite by removing test leakage, resolving contradictions, and shifting to behavior-based assertions. Most importantly, we added tests targeting Next.js 16 APIs that aren&#39;t in model training data.</p><p><b>APIs in our focused eval suite:</b></p><ul><li><p><code>connection()</code> for dynamic rendering</p></li><li><p><code>&#39;use cache&#39;</code> directive</p></li><li><p><code>cacheLife()</code> and <code>cacheTag()</code></p></li><li><p><code>forbidden()</code> and <code>unauthorized()</code></p></li><li><p><code>proxy.ts</code> for API proxying</p></li><li><p>Async <code>cookies()</code> and <code>headers()</code></p></li><li><p><code>after()</code>, <code>updateTag()</code>, <code>refresh()</code></p></li></ul><p>All the results that follow come from this hardened eval suite. Every configuration was judged against the same tests, with retries to rule out model variance.</p><h2><a href="#the-hunch-that-paid-off" id="the-hunch-that-paid-off"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>The hunch that paid off</b></h2><p>What if we removed the decision entirely? Instead of hoping agents would invoke a skill, we could embed a docs index directly in <code>AGENTS.md</code>. Not the full documentation, just an index that tells the agent where to find specific doc files that match your project&#39;s Next.js version. The agent can then read those files as needed, getting version-accurate information whether you&#39;re on the latest release or maintaining an older project.</p><p>We added a key instruction to the injected content.</p><div><div data-geist-code-block=""><pre><code data-geist-code-block="true"><div data-geist-code-block-line="true" id="C6d7cae5b-L1"><p><span>IMPORTANT: Prefer retrieval-led reasoning over pre-training-led reasoning </span></p></div></code></pre></div><p>Key instruction embedded in the docs index</p></div><p>This tells the agent to consult the docs rather than rely on potentially outdated training data.</p><h2><a href="#the-results-surprised-us" id="the-results-surprised-us"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>The results surprised us</b></h2><p>We ran the hardened eval suite across all four configurations:</p><div><p><img data-version="v1" alt="Eval results across all four configurations. AGENTS.md (third column) achieved 100% across Build, Lint, and Test" loading="lazy" width="900" height="494" decoding="async" data-nimg="1" srcset="/vc-ap-vercel-marketing/_next/image?url=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Fcontentful%2Fimage%2Fe5382hct74si%2F5klujg5rHUkECCKEGbllHN%2Fb6cf879ce5a9aa4b88e1c275e460e32f%2FCleanShot_2026-01-21_at_11.19.58_2x.png&amp;w=1080&amp;q=75 1x, /vc-ap-vercel-marketing/_next/image?url=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Fcontentful%2Fimage%2Fe5382hct74si%2F5klujg5rHUkECCKEGbllHN%2Fb6cf879ce5a9aa4b88e1c275e460e32f%2FCleanShot_2026-01-21_at_11.19.58_2x.png&amp;w=1920&amp;q=75 2x" src="https://vercel.com/vc-ap-vercel-marketing/_next/image?url=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Fcontentful%2Fimage%2Fe5382hct74si%2F5klujg5rHUkECCKEGbllHN%2Fb6cf879ce5a9aa4b88e1c275e460e32f%2FCleanShot_2026-01-21_at_11.19.58_2x.png&amp;w=1920&amp;q=75"/></p><figcaption>Eval results across all four configurations. AGENTS.md (third column) achieved 100% across Build, Lint, and Test</figcaption></div><p><b>Final pass rates:</b></p><div data-geist-scroller="" data-version="v1"><div data-geist-scroller-container="" data-overflow="x"><div><table><tbody><tr><td><p><b>Configuration</b></p></td><td><p><b>Pass Rate</b></p></td><td><p><b>vs Baseline</b></p></td></tr><tr><td><p>Baseline (no docs)</p></td><td><p>53%</p></td><td><p>—</p></td></tr><tr><td><p>Skill (default behavior)</p></td><td><p>53%</p></td><td><p>+0pp</p></td></tr><tr><td><p>Skill with explicit instructions</p></td><td><p>79%</p></td><td><p>+26pp</p></td></tr><tr><td><p><b><code>AGENTS.md</code></b></p><p><b> docs index</b></p></td><td><p><b>100%</b></p></td><td><p><b>+47pp</b></p></td></tr></tbody></table></div></div></div><p>On the detailed breakdown, <code>AGENTS.md</code> achieved perfect scores across Build, Lint, and Test.</p><div data-geist-scroller="" data-version="v1"><div data-geist-scroller-container="" data-overflow="x"><div><table><tbody><tr><td><p><b>Configuration</b></p></td><td><p><b>Build</b></p></td><td><p><b>Lint</b></p></td><td><p><b>Test</b></p></td></tr><tr><td><p>Baseline</p></td><td><p>84%</p></td><td><p>95%</p></td><td><p>63%</p></td></tr><tr><td><p>Skill (default behavior)</p></td><td><p>84%</p></td><td><p>89%</p></td><td><p>58%</p></td></tr><tr><td><p>Skill with explicit instructions</p></td><td><p>95%</p></td><td><p>100%</p></td><td><p>84%</p></td></tr><tr><td><p><b><code>AGENTS.md</code></b></p></td><td><p><b>100%</b></p></td><td><p><b>100%</b></p></td><td><p><b>100%</b></p></td></tr></tbody></table></div></div></div><p>This wasn&#39;t what we expected. The &#34;dumb&#34; approach (a static markdown file) outperformed the more sophisticated skill-based retrieval, even when we fine-tuned the skill triggers.</p><p><b>Why does passive context beat active retrieval?</b></p><p>Our working theory comes down to three factors.</p><ol><li><p><b>No decision point.</b> With <code>AGENTS.md</code>, there&#39;s no moment where the agent must decide &#34;should I look this up?&#34; The information is already present.</p></li><li><p><b>Consistent availability.</b> Skills load asynchronously and only when invoked. <code>AGENTS.md</code> content is in the system prompt for every turn.</p></li><li><p><b>No ordering issues.</b> Skills create sequencing decisions (read docs first vs. explore project first). Passive context avoids this entirely.</p></li></ol><h2><a href="#addressing-the-context-bloat-concern" id="addressing-the-context-bloat-concern"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>Addressing the context bloat concern</b></h2><p>Embedding docs in <code>AGENTS.md</code> risks bloating the context window. We addressed this with compression.</p><p>The initial docs injection was around 40KB. We compressed it down to 8KB (an 80% reduction) while maintaining the 100% pass rate. The compressed format uses a pipe-delimited structure that packs the docs index into minimal space:</p><div><div data-geist-code-block=""><pre><code data-geist-code-block="true"><div data-geist-code-block-line="true" id="C440a04c4-L1"><p><span>[Next.js Docs Index]|root: ./.next-docs</span></p></div><div data-geist-code-block-line="true" id="C440a04c4-L2"><p><span>|IMPORTANT: Prefer retrieval-led reasoning over pre-training-led reasoning</span></p></div><div data-geist-code-block-line="true" id="C440a04c4-L3"><p><span>|01-app/01-getting-started:{01-installation.mdx,02-project-structure.mdx,...}</span></p></div><div data-geist-code-block-line="true" id="C440a04c4-L4"><p><span>|01-app/02-building-your-application/01-routing:{01-defining-routes.mdx,...}</span></p></div></code></pre></div><p>Minified docs in AGENTS.md</p></div><p>The full index covers every section of the Next.js documentation:</p><div><p><img data-version="v1" alt="The full compressed docs index. Each line maps a directory path to the doc files it contains" loading="lazy" width="900" height="792" decoding="async" data-nimg="1" srcset="/vc-ap-vercel-marketing/_next/image?url=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Fcontentful%2Fimage%2Fe5382hct74si%2F7FuC0523TmDXUKhQNe8pAB%2F9060456f97c3863cb3b35a6404ca17a1%2Fimage_6_.png&amp;w=1080&amp;q=75 1x, /vc-ap-vercel-marketing/_next/image?url=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Fcontentful%2Fimage%2Fe5382hct74si%2F7FuC0523TmDXUKhQNe8pAB%2F9060456f97c3863cb3b35a6404ca17a1%2Fimage_6_.png&amp;w=1920&amp;q=75 2x" src="https://vercel.com/vc-ap-vercel-marketing/_next/image?url=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Fcontentful%2Fimage%2Fe5382hct74si%2F7FuC0523TmDXUKhQNe8pAB%2F9060456f97c3863cb3b35a6404ca17a1%2Fimage_6_.png&amp;w=1920&amp;q=75"/></p><figcaption>The full compressed docs index. Each line maps a directory path to the doc files it contains</figcaption></div><p>The agent knows where to find docs without having full content in context. When it needs specific information, it reads the relevant file from the <code>.next-docs/</code> directory.</p><h2><a href="#try-it-yourself" id="try-it-yourself"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>Try it yourself</b></h2><p>One command sets this up for your Next.js project:</p><p><code>npx @next/codemod@canary agents-md</code></p><p>This functionality is part of the official <a href="https://github.com/vercel/next.js/pull/88961" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://github.com/vercel/next.js/pull/88961" data-zone="null"><code>@next/codemod</code> package</a>.</p><p>This command does three things:</p><ol><li><p>Detects your Next.js version</p></li><li><p>Downloads matching documentation to <code>.next-docs/</code></p></li><li><p>Injects the compressed index into your <code>AGENTS.md</code></p></li></ol><p>If you&#39;re using an agent that respects <code>AGENTS.md</code> (like Cursor or other tools), the same approach works.</p><h2><a href="#what-this-means-for-framework-authors" id="what-this-means-for-framework-authors"><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M8.46968 1.46968C10.1433 -0.203925 12.8567 -0.203923 14.5303 1.46968C16.2039 3.14329 16.2039 5.85674 14.5303 7.53034L12.0303 10.0303L10.9697 8.96968L13.4697 6.46968C14.5575 5.38186 14.5575 3.61816 13.4697 2.53034C12.3819 1.44252 10.6182 1.44252 9.53034 2.53034L7.03034 5.03034L5.96968 3.96968L8.46968 1.46968ZM11.5303 5.53034L5.53034 11.5303L4.46968 10.4697L10.4697 4.46968L11.5303 5.53034ZM1.46968 14.5303C3.14329 16.2039 5.85673 16.204 7.53034 14.5303L10.0303 12.0303L8.96968 10.9697L6.46968 13.4697C5.38186 14.5575 3.61816 14.5575 2.53034 13.4697C1.44252 12.3819 1.44252 10.6182 2.53034 9.53034L5.03034 7.03034L3.96968 5.96968L1.46968 8.46968C-0.203923 10.1433 -0.203925 12.8567 1.46968 14.5303Z" fill="currentColor"></path></svg><span>Link to heading</span></a><b>What this means for framework authors</b></h2><p>Skills aren&#39;t useless. The <code>AGENTS.md</code> approach provides broad, horizontal improvements to how agents work with Next.js across all tasks. Skills work better for vertical, action-specific workflows that users explicitly trigger, like &#34;upgrade my Next.js version,&#34; &#34;migrate to the App Router,&#34; or <a href="https://x.com/huozhi/status/2015881140281004438" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://x.com/huozhi/status/2015881140281004438" data-zone="null">applying framework best practices</a>. The two approaches complement each other.</p><p>That said, for general framework knowledge, passive context currently outperforms on-demand retrieval. If you maintain a framework and want coding agents to generate correct code, consider providing an <code>AGENTS.md</code> snippet that users can add to their projects.</p><p><b>Practical recommendations:</b></p><ul><li><p><b>Don&#39;t wait for skills to improve.</b> The gap may close as models get better at tool use, but results matter now.</p></li><li><p><b>Compress aggressively.</b> You don&#39;t need full docs in context. An index pointing to retrievable files works just as well.</p></li><li><p><b>Test with evals.</b> Build evals targeting APIs not in training data. That&#39;s where doc access matters most.</p></li><li><p><b>Design for retrieval.</b> Structure your docs so agents can find and read specific files rather than needing everything upfront.</p></li></ul><p>The goal is to shift agents from pre-training-led reasoning to retrieval-led reasoning. <code>AGENTS.md</code> turns out to be the most reliable way to make that happen.</p><p><i>Research and evals by </i><a href="https://x.com/gao_jude" rel="noopener" target="_blank" data-track="true" data-track-click-name="article_link" data-track-click-value="https://x.com/gao_jude" data-zone="null"><i>Jude Gao</i></a><i>. CLI available at </i><i><code>npx @next/codemod@canary agents-md</code></i></p></div></article></div>
  </body>
</html>
