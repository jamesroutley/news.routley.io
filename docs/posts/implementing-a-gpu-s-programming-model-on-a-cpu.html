<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://litherum.blogspot.com/2023/10/implementing-gpus-programming-model-on.html">Original</a>
    <h1>Implementing a GPU&#39;s Programming Model on a CPU</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-1171869317934582790" itemprop="description articleBody">
<h2>SIMT</h2><p>The programming model of a GPU uses what has been coined &#34;single instruction multiple thread.&#34; The idea is that the programmer writes their program from the perspective of a single thread, using normal regular variables. So, for example, a programmer might write something like:</p><p><span>int x = threadID;</span></p><p><span>int y = 6;</span></p><p><span>int z = x + y;</span></p><p>Straightforward, right? Then, they ask the system to run this program a million times, in parallel, with different threadIDs.</p><p>The system *could* simply schedule a million threads to do this, but GPUs do better than this. Instead, the compiler will transparently rewrite the program to use vector registers and instructions in order to run multiple &#34;threads&#34; at the same time. So, imagine you have a vector register, where each item in the vector represents a scalar from a particular &#34;thread.&#34; In the above, program, x corresponds to a vector of [0, 1, 2, 3, etc.] and y corresponds to a vector of [6, 6, 6, 6, etc.]. Then, the operation x + y is simply a single vector add operation of both vectors. This way, performance can be dramatically improved, because these vector operations are usually significantly faster than if you had performed each scalar operation one-by-one.</p><p>(This is in contrast to SIMD, or &#34;single instruction multiple data,&#34; where the programmer explicitly uses vector types and operations in their program. The SIMD approach is suited for when you have a single program that has to process a lot of data, whereas SIMT is suited for when you have many programs and each one operates on its own data.)</p><p>SIMT gets complicated, though, when you have control flow. Imagine the program did something like:</p><p><span>if (threadID &lt; 4) {</span></p><p><span>    doSomethingObservable();</span></p><p><span>}</span></p><p>Here, the system has to behave as-if threads 0-3 executed the &#34;then&#34; block, but also behave as-if threads 4-n didn&#39;t execute it. And, of course, thread 0-3 want to take advantage of vector operations - you don&#39;t want to pessimize and run each thread serially. So, what do you do?</p><p>Well, the way that GPUs handle this is by using predicated instructions. There is a bitmask which indicates which &#34;threads&#34; are alive: within the above &#34;then&#34; block, that bitmask will have value 0xF. Then, all the vector instructions use this bitmask to determine which elements of the vector it should actually operate on. So, if the bitmask is 0xF, and you execute a vector add operation, the vector add operation is only going to perform the add on the 0th-3rd items in the vector. (Or, at least, it will behave &#34;as-if&#34; it only performed the operation on those items, from an observability perspective.) So, the way that control flow like this works is: all threads actually execute the &#34;then&#34; block, but all the operations in the block are predicated on a bitmask which specifies that only certain items in the vector operations should actually be performed. The &#34;if&#34; statement itself just modifies the bitmask.</p><h2>The Project</h2><p>AVX-512 is an optional instruction set on some (fairly rare) x86_64 machines. The exciting thing about AVX-512 is that it adds support for this predication bitmask thing. It has a bunch of vector registers (512 bits wide, named zmm0 - zmm31) and it also adds a set of predication bitmask registers (k0 - k7). The instructions that act upon the vector registers can be predicated on the value of one of those predication registers, to achieve the effect of SIMT.</p><p>It turns out I actually have a machine lying around in my home which supports AVX-512, so I thought I&#39;d give it a go, and actually implement a compiler that compiles a toy language, but performs the SIMT transformation to use the vector operations and predication registers. The purpose of this exercise isn&#39;t really to achieve incredible performance - there are lots of sophisticated compiler optimizations which I am not really interested in implementing - but instead the purpose is really just as a learning exercise. Hopefully, by implementing this transformation myself for a toy language, I can learn more about the kinds of things that real GPU compilers do.</p><p>The toy language is one I invented myself - it&#39;s very similar to C, with some syntax that&#39;s slightly easier to parse. Programs look like this:</p><p><span>function main(index: uint64): uint64 {</span></p><p><span>    variable accumulator: uint64 = 0;</span></p><p><span>    variable accumulatorPointer: pointer&lt;uint64&gt; = &amp;accumulator;</span></p><p><span>    for (variable i: uint64 = 0; i &lt; index; i = i + 1) {</span></p><p><span>        accumulator = *</span><span>accumulatorPointer </span><span>+ i;</span></p><p><span>    }</span></p><p><span>    return accumulator;</span></p><p><span>}</span></p><p>It&#39;s pretty straightforward. It doesn&#39;t have things like ++ or +=. It also doesn&#39;t have floating-point numbers (which is fine, because AVX-512 supports vector integer operations). It has pointers, for loops, continue &amp; break statements, early returns... the standard stuff.</p><h2>Tour</h2><p>Let&#39;s take a tour, and examine how each piece of a C-like language gets turned into AVX-512 SIMT. I implemented this so it can run real programs, and tested it somewhat-rigorously - enough to be fairly convinced that it&#39;s generally right and correct.</p><h3>Variables and Simple Math</h3><p>The most straightforward part of this system is variables and literal math. Consider:</p><p><span>variable accumulator: uint64;</span></p><p>This is a variable declaration. Each thread may store different values into the variable, so its storage needs to be a vector. No problem, right?</p><p>What about if the variable&#39;s type is a complex type? Consider:</p><p><span>struct Foo {</span></p><p><span>    x: uint64;</span></p><p><span>    y: uint64;</span></p><p><span>}</span></p><p><span>variable bar: Foo;</span></p><p>Here, we need to maintain the invariant that Foo.x has the same memory layout as any other uint64. This means that, rather than alternating x,y,x,y,x,y in memory, there instead has to be a vector for all the threads&#39; x value, followed by another vector for all the threads&#39; y values. This works recursively: if a struct has other structs inside it, the compiler will to through all the leaf-types in the tree, turn each leaf type into a vector, and then lay them out in memory end-to-end.</p><p>Simple math is even more straightforward. Literal numbers have the same value no matter which thread you&#39;re running, so they just turn into broadcast instructions. The program says &#34;3&#34; and the instruction that gets executed is &#34;broadcast 3 to every item in a vector&#34;. Easy peasy.</p><h3>L-values and R-values</h3><p>In a C-like language, every value is categorized as either an &#34;l-value&#34; or an &#34;r-value&#34;. An l-value is defined as having a location in memory, and r-values don&#39;t have a location in memory. The value produced by the expression &#34;2 + 3&#34; is an r-value, but the value produced by the expression &#34;*foo()&#34; is an l-value, because you dereferenced the pointer, so the thing the pointer points to is the location in memory of the resulting value. L-values can be assigned to; r-values cannot be assigned to. So, you can say things like &#34;foo = 3 + 4;&#34; (because &#34;foo&#34; refers to a variable, which has a memory location) but you can&#39;t say &#34;3 + 4 = foo;&#34;. That&#39;s why it&#39;s called &#34;l-value&#34; and &#34;r-value&#34; - l-values are legal on the left side of an assignment.</p><p>At runtime, every expression has to produce some value, which is consumed by its parent in the AST. E.g, in &#34;3 * 4 + 5&#34;, the &#34;3 * 4&#34; has to produce a &#34;12&#34; which the &#34;+&#34; will consume. The simplest way to handle l-values is to make them produce a pointer. This is so expressions like &#34;&amp;foo&#34; work - the &#34;foo&#34; is an lvalue and produces a pointer that points to the variable&#39;s storage, and the &amp; operator receives this pointer and produces that same pointer (unmodified!) as an r-value. The same thing happens in reverse for the unary * (&#34;dereference&#34;) operator: it accepts an r-value of pointer type, and produces an l-value - which is just the pointer it just received. This is how expressions like &#34;*&amp;*&amp;*&amp;*&amp;*&amp;foo = 7;&#34; work (which is totally legal and valid C!): the &#34;foo&#34; produces a pointer, which the &amp; operator accepts and passes through untouched to the &amp;, which takes it and passes it through untouched, all the way to the final *, which produces the same pointer as an lvalue, that points to the storage of foo.</p><p>The assignment operator knows that the thing on its left side must be an lvalue and therefore will always produce a pointer, so that&#39;s the storage that the assignment stores into. The right side can either be an l-value or an r-value; if it&#39;s an l-value, the assignment operator has to read from the thing it points to; otherwise, it&#39;s an r-value, and the assignment operator reads the value itself. This is generalized to every operation: it&#39;s legal to say &#34;foo + 3&#34;, so the + operator needs to determine which of its parameters are l-values, and will thus produce pointers instead of values, and it will need to react accordingly to read from the storage the pointers point to.</p><p>All this stuff means that, even for simple programs where the author didn&#39;t even spell the name &#34;pointer&#34; anywhere in the program, or even use the * or &amp; operators anywhere in the program, there will still be pointers internally used just by virtue of the fact that there will be l-values used in the program<span>. So, dealing with pointers is a core part of the language. They appear everywhere, whether the program author wants them to or not.</span></p><h3><span>Pointers</span></h3><p><span>If we now</span> think about what this means for SIMT, l-values produce pointers, but each thread has to get its own distinct pointer! That&#39;s because of programs like this:</p><p><span>variable x: pointer&lt;uint64&gt;;</span></p><p><span>if (...) {</span></p><p><span>    x = &amp;something;</span></p><p><span>} else {</span></p><p><span>    x = &amp;somethingElse;</span></p><p><span>}</span></p><p><span>*x = 4;</span></p><p>That *x expression is an l-value. It&#39;s not special - it&#39;s just like any other l-value. The assignment operator needs to handle the fact that, in SIMT, the lvalue that *x produces is a vector of pointers, where each pointer can potentially be distinct. Therefore, that assignment operator doesn&#39;t actually perform a single vector store; instead, it performs a &#34;scatter&#34; operation. There&#39;s a vector of pointers, and there&#39;s a vector of values to store to those pointers; the assignment operator might end up spraying those values all around memory. In AVX-512, there&#39;s an <a href="https://www.felixcloutier.com/x86/vpscatterdd:vpscatterdq:vpscatterqd:vpscatterqq">instruction</a> that does this scatter operation.</p><p>(Aside: That scatter operation in AVX-512 uses a predication mask register (of course), but the instruction has a side-effect of clearing that register. That kind of sucks from the programmer&#39;s point of view - the program has to save and restore the value of the register just because of a quirk of this instruction. But then, thinking about it more, I realized that the memory operation might cause a page fault, which has to be handled by the operating system. The operating system therefore needs to know which address triggered the page fault, so it knows which pages to load. The predication register holds this information - as each memory access completes, the corresponding bit in the predication register gets set to false. So the kernel can look at the register to determine the first predication bit that&#39;s high, which indicates which pointer in the vector caused the fault. So it makes sense why the operation will clear the register, but it is annoying to deal with from the programmer&#39;s perspective.)</p><p>And, of course, the operation can also say &#34;foo = *x;&#34; which means that there also has to be a <a href="https://www.felixcloutier.com/x86/vpgatherqd:vpgatherqq">gather operation</a>. Sure. Something like &#34;*x = *y;&#34; will end up doing both a gather and a scatter.</p><h3>Copying</h3><p>Consider a program like:</p><p><span>struct Foo {</span></p><p><span>    x: uint64;</span></p><p><span>    y: uint64;</span></p><p><span>}</span></p><p><span>someVariableOfFooType = aFunctionThatReturnsAFoo();</span></p><p>That initializer needs to set both fields inside the Foo. Naively, a compiler might be tempted to use a memcpy() to copy the contents - after all, the contents could be arbitrarily complex, with nested structs. However, that won&#39;t work for SIMT, because only some of the threads might be alive at this point in the program. Therefore, that assignment has to only copy the items of the vectors for the threads that are alive; it can&#39;t copy the whole vectors because that can clobber other entries in the destination vector which are supposed to persist.</p><p>So, all the stores to someVariableOfFooType need to be predicated using the predication registers - we can&#39;t naively use a memcpy(). This means that every assignment needs to actually perform n memory operations, where n is the number of leaf types in the struct being assigned - because those memory operations can be predicated correctly using the predication registers. We have to copy structs leaf-by-leaf. This means that the number of instructions to copy a type is proportional to the complexity of the type. Also, both the left side and the right side may be l-values, which means each leaf-copy could actually be a gather/scatter pair of instructions. So, depending on the complexity of the type and the context of the assignment, that single &#34;=&#34; operation might actually generate a huge amount of code.</p><h3>Pointers (Part 2)</h3><p>There&#39;s one other decision that needs to be made about pointers: Consider:</p><p><span>variable x: uint64;</span></p><p><span>... &amp;x ...</span></p><p>As I described above, the storage for the variable x is a vector (each thread owns one value in the vector). &amp;x produces a vector of pointers, sure. The question is: should all the pointer values point to the beginning of the x vector? Or should each pointer value point to its own slot inside the x vector? If they point to the beginning, that makes the &amp; operator itself really straightforward: it&#39;s just a broadcast instruction. But it also means that the scatter/gather operations get more complicated: they have to offset each pointer by a different amount in order to scatter/gather to the correct place. On the other hand, if each pointer points to its own slot inside x, that means the scatter/gather operations are already set up correctly, but the &amp; operation itself gets more complicated.</p><p>Both options will work, but I ended up making all the pointer point to the beginning of x. The reason for that is for programs like:</p><p><span>struct Foo {</span></p><p><span>    x: uint32;</span></p><p><span>    y: uint64;</span></p><p><span>}</span></p><p><span>variable x: </span><span>Foo</span><span>;</span></p><p><span>... &amp;x ...</span></p><p>If I picked the other option, and had the pointers point to their own slot inside x, it isn&#39;t clear which member of Foo they should be pointing inside of. I could have, like, found the first leaf, and made the pointers point into that, but what if the struct is empty... It&#39;s not very elegant.</p><p>Also, if I&#39;m assigning to x or something where I need to populate every field, because every copy operation has to copy leaf-by-leaf, I&#39;m going to have to be modifying the pointers to point to each field. If one of the fields is a uint32 and the next one is a uint64, I can&#39;t simply just add a constant amount to each pointer to get it to point to its slot in the next leaf. So, if I&#39;m going to be mucking about with individual pointer values for each leaf in a copy operation, I might as well have the original pointer point to the overall x vector rather than individual fields, because pointing to individual fields doesn&#39;t actually make anything simpler.</p><h3>Function Calls</h3><p>This language supports function pointers, which are callable. This means that you can write a program like this (taken from the test suite):</p><p><span>function helper1(): uint64 ...</span></p><p><span>function helper2(): uint64 ...</span></p><p><span>function main(index: uint64): uint64 {</span></p><p><span><span>    variable x: FunctionPointer&lt;uint64&gt;;</span></span></p><p><span>    if (index &lt; 3) {</span></p><p><span>        x = helper1;</span></p><p><span>    } else {</span></p><p><span>        x = helper2;</span></p><p><span>    }</span></p><p><span>    return x();</span></p><p><span>}</span></p><p>Here, that call to x() allows different threads to point to different functions. This is a problem for us, because all the &#34;threads&#34; that are running share the same instruction pointer. We can&#39;t actually have some threads call one function and other threads call another function. So, what we have to do instead is to set the predication bitmask to only the &#34;threads&#34; which call one function, then call that function, then set the predication bitmask to the remaining threads, then call the other function. Both functions get called, but the only &#34;threads&#34; that are alive during each call are only the ones that are supposed to actually be running the function.</p><p>This is tricky to get right, though, because anything could be in that function pointer vector. Maybe all the threads ended up with the same pointers! Or maybe each thread ended up with a different pointer! You *could* do the naive thing and do something like:</p><p><span>for i in 0 ..&lt; numThreads:</span></p><p><span>    predicationMask = originalPredicationMask &amp; (1 &lt;&lt; i)</span></p><p><span>    call function[i]</span></p><p>But this has really atrocious performance characteristics. This means that every call actually calls numThreads functions, one-by-one. But each one of those functions can have more function calls! The execution time will be proportional to numThreads ^ callDepth. Given that function calls are super common, this exponential runtime isn&#39;t acceptable.</p><p>Instead, what you have to do is gather up and deduplicate function pointers. You need to do something like this instead:</p><p><span>func generateMask(functionPointers, target):</span></p><p><span>    mask = 0;</span></p><p><span>    for i in 0 ..&lt; numThreads:</span></p><p><span>        if functionPointers[i] == target:</span></p><p><span>            mask |= 1 &lt;&lt; i;</span></p><p><span>    return mask;</span></p><p><span>for pointer in unique(functionPointers):</span></p><p><span>    predicationMask = originalPredicationMask &amp; generateMask(functionPointers, pointer)</span></p><p><span>    call pointer</span></p><p>I couldn&#39;t find an instruction in the Intel instruction set that did this. This is also a complicated enough algorithm that I didn&#39;t want to write this in assembly and have the compiler emit the instructions for it. So, instead, I wrote it in C++, and had the compiler emit code to call this function at runtime. Therefore, this routine can be considered a sort of &#34;runtime library&#34;: a function that automatically gets called when the code the author writes does a particular thing (in this case, &#34;does a particular thing&#34; means &#34;calls a function&#34;).</p><p>Doing it this way means that you don&#39;t get exponential runtime. Indeed, if your threads all have the same function pointer value, you get constant runtime. And if the threads diverge, the slowdown will be at most proportional to the number of threads. You&#39;ll never run a function where the predication bitmask is 0, which means there is a floor about how slow the worst case can be - it will never get worse than having each thread individually diverge from all the other threads.</p><h3>Control Flow</h3><p>As described above, control flow (meaning: if statements, for loops, breaks, continues, and returns) are implemented by changing the value of the predication bitmask register. The x86_64 instruction set has instructions that do this.</p><p>There are 2 ways to handle the predication registers. One way is to observe the fact that there are 8 predication registers, and to limit the language to only allow 8 (7? 6? 3?) levels of nested control flow. If you pick this approach, the code that you emit inside each if statement and for loop would use a different predication register. (Sibling if statements can use the same predication register, but nested ones have to use different predication registers.) </p><p>I elected to not add this restriction, but instead to save and restore the values of the predication register to the stack. This is slower, but it means that control flow can be nested without limit. So, all the instructions I emit are all predicated on the k1 register - I never use k2 - k7 (except - I use k2 to save/restore the value of k1 during scatter/gather operations because those clobber the value of whichever register you pass into it).</p><p>For an &#34;if&#34; statement, you actually need to save 2 predication masks:</p><ol><li>One that saves the predication mask that was incoming to the beginning of the &#34;if&#34; statement. You need to save this so that, after the &#34;if&#34; statement is totally completed, you can restore it back to what it was originally</li><li>If there&#39;s an &#34;else&#34; block, you also need to save the bitmask of the threads that should run the &#34;else&#34; block. You might think that you can compute this value at runtime instead of saving/loading it (it would be the inverse of the threads that ran the &#34;then&#34; block, and-ed with the set of incoming threads) but you actually can&#39;t do that because break and continue statements might actually need to modify this value. Consider if there&#39;s a break statement as a direct child of the &#34;then&#34; block - at the end of the &#34;then&#34; block, there will be no threads executing (because they all executed the &#34;break&#34; statement). If you then use the set of currently executing threads to try to determine which should execute the &#34;else&#34; block, you&#39;ll erroneously determine that all threads (even the ones which ran the &#34;then&#34; block!) should run the &#34;else&#34; block. Instead, you need to compute up-front the set of threads should be running the &#34;else&#34; block, save it, and re-load it when starting to execute the &#34;then&#34; block.</li></ol><p> For a &#34;for&#34; loop, you also need to save 2 predication masks:</p><ol><li>Again, you need to store the incoming predication mask, to restore it after the loop has totally completed</li><li>You also need to save and restore the set of threads which should execute the loop increment operation at the end of the loop. The purpose of saving and restoring this is so that break statements can modify it. Any thread that executes a break statement needs to remove itself from the set of threads which executes the loop increment. Any thread that executes a continue statement needs to remove itself from executing *until* the loop increment. Again, this is a place where you can&#39;t recompute the value at runtime because you don&#39;t know which threads will execute break or continue statements.</li></ol><p>If you set up &#34;if&#34; statements and &#34;for&#34; loops as above, then break and continue statements actually end up really quite simple. First, you can verify statically that no statement directly follows them - they should be the last statement in their block.</p><p>Then, what a break statement does is:</p><div><ol><li>Find the deepest loop it&#39;s inside of, and find all the &#34;if&#34; statements between that loop and the break statement</li><li>For each of the &#34;if&#34; statements:</li><ol><li>Emit code to remove all the currently running threads from both of the saved bitmasks associated with that &#34;if&#34; statement. Any thread that executes a break statement should not run an &#34;else&#34; block and should not come back to life after the &#34;if&#34; statement.</li></ol><li>Emit code to remove all the currently running threads from just the second bitmask associated with the loop. (This is the one that gets restored just before the loop increment operation). Any thread that executes a break statement should not execute the loop increment.</li></ol><p>A &#34;continue&#34; statement does the same thing except for the last step (those threads *should* execute the loop increment). And a &#34;return&#34; statement removes all the currently running threads from all bitmasks from every &#34;if&#34; statement and &#34;for&#34; loop it&#39;s inside of.</p></div><p>This is kind of interesting - it means an early return doesn&#39;t actually stop the function or perform a jmp or ret. The function still continues executing, albeit with a modified predication bitmask, because there might still be some threads &#34;alive.&#34; It also means that &#34;if&#34; statements don&#39;t actually need to have any jumps in them - in the general case, both the &#34;then&#34; block and the &#34;else&#34; block will be executed, so instead of jumps you can just modify the predication bitmasks - and emit straight-line code. (Of course, you&#39;ll want the &#34;then&#34; block and the &#34;else&#34; block to both jump to the end if they find that they start executing with an empty predication bitmask, but this isn&#39;t technically necessary - it&#39;s just an optimization.)</p><h3>Shared Variables</h3><p>When you&#39;re using the SIMT approach, one thing that becomes useful is the ability to interact with external memory. GPU threads don&#39;t really perform I/O as such, but instead just communicate with the outside world via reading/writing global memory. This is a bit of a problem for SIMT-generated code, because it will assume that the type of everything is vector type - one for each thread. But, when interacting with external memory, all &#34;threads&#34; see the same values - a shared int is just an int, not a vector of ints.</p><p>That means we now have a 3rd kind of value classification. Previously, we had l-values and r-values, but l-values can be further split into vector-l-values and scalar-l-values. A pointer type now needs to know statically whether it points to a vector-l-value or a scalar-l-value. (This information needs to be preserved as we pass it from l-value pointers through the &amp; and * operators.) In the language, this looks like &#34;pointer&lt;uint64 | shared&gt;&#34;.</p><p>It turns out that, beyond the classical type-checking analysis, it&#39;s actually pretty straightforward to deal with scalar-l-values. They are actually strictly simpler than vector-l-values.</p><p>In the language, you can declare something like:</p><p><span>variable&lt;shared&gt; x: uint64;</span></p><p><span>x = 4;</span></p><p>which means that it is shared among all the threads. If you then refer to x, that reference expression becomes a scalar-l-value, and produces a vector of pointers, all of which point to x&#39;s (shared) storage. The &#34;=&#34; in the &#34;x = 4;&#34; statement now has to be made aware that:</p><div><ol><li>If the left side is a vector-l-value, then the scatter operation needs to offset each pointer in the vector to point to the specific place inside the destination vectors that the memory operations should write to</li><li>But, if the left side is a scalar-l-value, then no such offset needs to occur. The pointers already point to the one single shared memory location. Everybody points to the right place already.</li></ol><p>(And, of course, same thing for the right side of the assignment, which can be either a vector-l-value, a scalar-l-value, or an r-value.)</p></div><h3>Comparisons and Booleans</h3><p>AVX-512 of course has <a href="https://www.felixcloutier.com/x86/vpcmpq:vpcmpuq">vector compare instructions</a>. The result of these vector comparisons *isn&#39;t* another vector. Instead, you specify one of the bitmask registers to receive the result of the comparison. This is useful if the comparison is the condition of an &#34;if&#34; statement, but it&#39;s also reasonable for a language to have a boolean type. If the boolean type is represented as a normal vector holding 0s and 1s, there&#39;s an elegant way to convert between the comparison and the boolean.</p><p>The comparison instructions look like:</p><p><span>vpcmpleq %zmm1,%zmm0,%k2{%k1}</span></p><p>If you were to speak this aloud, what you&#39;d say is &#34;do a vector packed compare for less-than-or-equal-to on the quadwords in zmm0 and zmm1, put the result in k2, and predicate the whole operation on the value of k1.&#34; Importantly, the operation itself is predicated, and the result can be put into a different predication register. This means that, after you execute this thing, you know which threads executed the instruction (because k1 is still there) but you also know the result of the comparison (because it&#39;s in k2).</p><p>So, what you can do is: use k1 to broadcast a constant 0 into a vector register, and then use k2 to broadcast a constant 1 into the same vector register. This will leave a 1 in all the spots where the test succeeded, and a 0 in all the remaining spots. Pretty cool!</p><p>If you want to go the other way, to convert from a boolean to a mask, you can just compare the boolean vector to a broadcasted 0, and compare for &#34;not equal.&#34; Pretty straightforward.</p><h3>Miscellanea</h3><p>I&#39;m using my own calling convention (and ABI) to pass values into and out of functions. It&#39;s for simplicity - the x64 calling convention is kind of complicated if you&#39;re using vector registers for everything. One of the most useful decisions I made was to formalize this calling convention by encoding it in a C++ class in the compiler. Rather than having various different parts of the compiler just assume they knew where parameters were stored, it was super useful to create a single source of truth about the layout of the stack at call frame boundaries. I ended up changing the layout a few different times, and having this single point of truth meant that such changes only required updating a single class, rather than a global change all over the compiler.</p><p>Inventing my own ABI also means that there will be a boundary, where the harness will have to call the generated code. At this boundary, there has to be a trampoline, where the contents of the stack gets rejiggered to set it up for the generated code to look in the right place for stuff. And, this trampoline can&#39;t be implemented in C++, because it has to do things like align the stack pointer register, which you can&#39;t do in C++. AVX-512 requires vectors to be loaded and stored at 64-byte alignment, but Windows only requires 16-byte stack alignments. So, in my own ABI I&#39;ve said &#34;stack frames are all aligned to 64-byte boundaries&#34; which means the trampoline has to enforce this before the entry point can be run. So the trampoline has to be written in assembly.</p><p>The scatter/gather operations (which are required for l-values to work) only operate on 32-bit and 64-bit values inside the AVX-512 registers. This means that the only types in the language can be 32-bit and 64-bit types. An AVX-512 vector, which is 512 bits = 64 bytes wide, can hold 8 64-bit values, or 16 32-bit values. However, the entire SIMT programming model requires you to pick up front how many &#34;threads&#34; will be executing at once. If some calculations in your program can calculate 8 values at a time, and some other calculations can calculate 16 values at a time, it doesn&#39;t matter - you have to pessimize and only use 8-at-a-time. So, if the language contains 64-bit types, then the max number of &#34;threads&#34; you can run at once is 8. If the language only contains 32-bit types (and you get rid of 64-bit type support, including 64-bit pointers), then you can run 16 &#34;threads&#34; at once. For me, I picked to include 64-bit types and do 8 &#34;threads&#34; at a time, because I didn&#39;t want to limit myself to the first 4GB of memory (the natural stack and heap are already farther than 4GB apart from each other in virtual address space, so I&#39;d have to, like, mess with Windows&#39;s VM subsystem to allocate my own stack/heap and put them close to each other, and yuck I&#39;ll just use 64-bit pointers thankyouverymuch).</p><h3>Conclusion</h3><p>And that&#39;s kind of it. I learned a lot along the way - there seem to be good reasons why, in many shading languages (which use this SIMT model),</p><div><ul><li>Support for 8-bit and 16-bit types is rare - the scatter/gather operations might not support them.</li><li>Support for 64-bit types is also rare - the smaller your types, the more parallelism you get, for a particular vector bit-width.</li><li>Memory loads and stores turn into scatter/gather operations instead.</li><ul><li>A sophisticated compiler could optimize this, and turn some of them into vector loads/stores instead.</li><li>This might be why explicit support for pointers is relatively rare in shading languages - no pointers means you can _always_ use vector load/store operations instead of scatter/gather operations (I think).</li></ul><li>You can&#39;t treat memory as a big byte array and memcpy() stuff around; instead you need to treat it logically and operate on well-typed fields, so the predication registers can do the right thing.</li><li>Shading languages usually don&#39;t have support for function pointers, because calling them ends up becoming a loop (with a complicated pointer coalescing phase, no less) in the presence of non-uniformity. Instead, it&#39;s easy for the language to just say &#34;You know what? All calls have to be direct. Them&#39;s the rules.&#34;</li><li>Pretty much every shading language has a concept of multiple address spaces. The need for them naturally arises when you have local variables which are stored in vectors, but you also need to interact with global memory, which every thread &#34;sees&#34; identically. Address spaces and SIMT are thoroughly intertwined.</li><li>I thought it was quite cool how AVX-512 complimented the existing (scalar) instruction set. E.g. all the math operations in the language use vector operations, but you still use the normal call/ret instructions. You use the same rsp/rbp registers to interact with the stack. The vector instructions can still use the SIB byte. The broadcast instruction broadcasts from a scalar register to a vector register. Given that AVX-512 came out of the Larrabee project, it strikes me as a very Intel-y way to build a GPU instruction set.</li></ul></div>

</div></div>
  </body>
</html>
