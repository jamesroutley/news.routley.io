<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.amplifypartners.com/blog-posts/arming-the-rebels-with-gpus-gradium-kyutai-and-audio-ai">Original</a>
    <h1>Audio is the one area small labs are winning</h1>
    
    <div id="readability-page-1" class="page"><div fs-richtext-element="rich-text"><blockquote>Disclosure: Amplify is an investor in Gradium.</blockquote><p>If AI research is Star Wars and OpenAI is the death star, then without a doubt the rebels are building audio models. The best models for voice – TTS, STS, STT, and the like – are <em>not</em> coming from the big labs. Instead, they’re built by their underfunded, understaffed, and underhyped siblings, a wave of incredible startups that is improbably crushing benchmarks with every model release. And if you believe that audio is the biggest future modality for AI – like many researchers do – this is one of the more interesting and underdiscussed topics in genAI today.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/698ded55b591ab70306aca31_20cb10aa.webp" loading="lazy" alt=""/></p></figure><p>One of these improbably cutting edge startups is <a href="https://gradium.ai/">Gradium</a>, born out of the open lab <a href="https://kyutai.org/">Kyutai</a>. </p><p>In summer 2024 on a stage in Paris, a Kyutai researcher (his name is Neil) demoed <a href="https://www.youtube.com/live/hm2IJSKcYvo?si=hROZHbpKekVV7e7N&amp;t=460">the first realtime audio conversation with AI</a>. This model (Moshi) could respond in real time, change its voice style and volume on request, and even recite an original poem in a French accent (research shows poems sound better this way). </p><p>You’ve probably seen audio AI demos before. You may not be particularly impressed. Didn’t OpenAI do this a few years ago? Well, not exactly:</p><ol role="list"><li>This was the <a href="https://moshi.chat/">first full-duplex conversational AI model</a>. Moshi could interrupt, be interrupted, backchannel (&#34;uh-huh&#34;, &#34;I see&#34;) and respond in around 160ms (faster than most human conversations).</li><li>This demo happened <em>before</em> OpenAI released Advanced Voice Mode, and a full year before xAI released a similar demo (with more latency).</li></ol><p>This would have been a groundbreaking release from a major lab, except it wasn’t from a major lab, it was from a team of 4 (four) researchers who built it completely from scratch (without a pre-trained base) in 6 months. The model is open source, and can even run on mobile. Oh, and the team was part of a non-profit with extremely limited funding. <a href="https://www.youtube.com/watch?v=Z7Bm_faK5Ic">How did they do it?</a></p><p>Based on extensive interviews with the Gradium team, this post is going to go in technical depth on an incredibly interesting niche of the increasingly top heavy AI world:</p><ul role="list"><li>A brief history of audio ML, and why it’s consistently overlooked</li><li>Dynamics of big labs and why small teams of researchers can outperform</li><li>Anatomy of training a voice AI model, and how it differs from text</li><li>Core Gradium / Kyutai research: full-duplex models, audio codecs, oh my!</li></ul><p>Let’s get to it.</p><h2><strong>A brief history of audio ML, and why it’s consistently overlooked</strong></h2><p>If you watch any science fiction movie — <a href="https://youtu.be/Wy4EfdnMZ5g?si=spQD0VynHgNpT4lI&amp;t=19">2001: A Space Odyssey</a>, <a href="https://youtu.be/dJTU48_yghs?si=1gITIq1nW_vHjTTI&amp;t=32">Her</a> and <a href="https://youtu.be/EfmVRQjoNcY?si=wGAUDGh13ovx-Zfs&amp;t=7">Iron Man</a> or incessantly invoked — the colloquial AI speaks in a distinctly natural, human-sounding voice. One simply needs to ask Siri what time it is (it took 5 seconds for me this morning) to realize how far away from this ideal our devices can be.</p><p>There’s an obvious question here: <em>how did we let it get this bad?</em> Why are we only now starting to see meaningful advances in audio AI, while text has been rapidly improving every single year since 2020?</p><p>This problem is actually foundational. For <em>years</em> audio has occupied the bottom tier of AI/ML’s informal coolness hierarchy. If you were around this scene pre-GPT, there was a clear ranking of what it was cool to work on. At the top was image classification via CNNs, which was for a while the most promising real world application of AI. Then came ML on tabular data, then text, and audio was somewhere all the way towards the bottom. For several reasons <em>audio just wasn’t sexy</em><sup>1</sup>.</p><p>There are practical reasons for this gap: training data for audio is genuinely scarce compared to text. You can scrape trillions of tokens from Wikipedia, Stack Overflow, books, and papers. High-quality conversational audio is harder to come by, and much of it isn’t particularly informative. A Stack Overflow answer (usually) teaches you something, but a typical phone conversation is mostly filler. And generating audio is much more complex than predicting text tokens, requiring real domain expertise to execute effectively.</p><p>But there’s also a cultural problem here. In the mid-2010s, when deep learning was taking off for images and starting to work for text, audio felt impossibly hard. Neural networks were doing cool things with photos. Maybe they’d eventually be okay at writing. Very, very few people conceived that one day, audio could have realtime conversations with proper turn-taking and expressiveness. Siri put a laughably bad voice assistant in everyone’s pocket…is it possible we slowly internalized defeat?</p><p>This was undoubtedly true at larger labs. When Neil (Kyutai co-founder and Gradium CEO) was hired at Google Brain in 2019, he was one of a very small group working on voice. Management considered voice to be a “solved problem.” Meanwhile, projects like Meta’s <a href="https://github.com/facebookresearch/seamless_communication">Seamless</a> and Google’s various speech initiatives shipped models, published papers, then languished. These repos haven’t been updated in years!</p><p>All of this created an opportunity. When you have a hard technical problem that’s been underfunded and underexplored, and yet has promise to be <em>the</em> core modality if things go right<sup>2</sup>, a few researchers who actually understand the domain can move incredibly fast. And they did.</p><h2><strong>Dynamics of big labs and why small teams of researchers can outperform</strong></h2><p>When Neil joined Facebook AI Research for his PhD in 2015 there was a clear social hierarchy among AI research:</p><ul role="list"><li>Research <strong>scientists</strong> were the “idea guys” — with prestigious academic backgrounds, working on theoretical problems, and rarely touching code<sup>3</sup>. </li><li>Research <strong>engineers</strong> implemented those ideas in code and with machines. They knew how to get theory into software and hardware. </li></ul><p>In quite a turn from SF culture today, the scientists almost universally had higher prestige and better compensation<sup>4</sup>. </p><p>Then deep learning happened, and the hierarchy completely inverted. Ideas became very cheap because Neural nets are universal approximators, and are essentially very dumb. A lot of research became “what can we throw Deep Learning at” and the hard problems were moving down the stack: training efficiently, managing distributed systems, etc. Now the engineers were in charge!</p><p>The researchers who thrived in this new climate — people like Noam Shazeer at Google — were actually both of these people. They could have the architectural insight <em>and</em> implement it themselves<sup>5</sup>. </p><blockquote><em>“The biggest scam in big companies is thinking that you can lead a research organization without doing research yourself, just by being an ‘idea guy’. The only way to understand what is possible, what is challenging, how we should allocate resources, is to understand every single detail to the deepest level.”</em></blockquote><p>The priority now was less how creative your idea is, and more what you can realize as a tangible outcome of an idea. And critically, this did not necessarily require massive compute budgets and teams. In a sense (perhaps a very weak sense) this was the AWS moment for startups…but for AI research. Not to mention that getting GPUs in the cloud was now a few clicks (if your clicks were fast enough).</p><p>This is the crux of why big labs <em>still</em> don’t dominate in audio like they do in text. Small groups of research engineers are able to completely outclass their larger, better staffed and funded competitors because they move fast, build their own ideas, and don’t have to deal with the incessant big lab politics that you are reading about every day on X. </p><p>Not only that, but as we’ll see, audio is a completely different beast than text. It is <em>not</em> just about scaling compute and data. There are a million little edges to creating elite audio models, from correct turn taking to backchanneling and managing latency, that require deep domain expertise. Great audio models are trained by great audio researchers, and throwing money at the problem will only get you mediocrity. </p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/698ded55b591ab70306aca28_9fa906cc.webp" loading="lazy" alt=""/></p></figure><p>All the Gradium cofounders (Neil + <a href="https://www.linkedin.com/in/alexandre-d%C3%A9fossez-b099ba7b/">Alex Défossez</a>, <a href="https://www.linkedin.com/in/olivier-teboul-800364103/">Olivier Teboul</a>, and <a href="https://www.linkedin.com/in/laurent-mazare-ab01a53/">Laurent Mazaré</a>) worked around some combination of these labs, absolutely cooking in relative obscurity in their underfunded audio divisions. It was a fun time in Paris. Alex was working on mathematical optimization but DJing on the side. They started building an AI-based synthesizer for fun.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/698ded55b591ab70306aca2b_92334c9c.webp" loading="lazy" alt=""/></p><figcaption><em>NOT Neil and Alex. </em></figcaption></figure><p>The first thing Neil did at Google Brain was work on audio compression, building the very first neural audio codec – <a href="https://arxiv.org/abs/2107.03312">SoundStream</a>. Better compression led Neil and Olivier to train the first model that could generate audio by predicting compressed tokens. After one week, they ran an experiment: “I passed three seconds of my voice to the model, and it kept talking in my voice.” (They had accidentally invented voice cloning). Every audio project at Google Brain started using this framework, which became the foundation for <a href="https://gemini.google/overview/gemini-live/">Gemini Live</a>.</p><p>Essentially, here was a small group of some of the best audio researchers on the planet all connected and working at big labs. It was only a matter of time…</p><p>Thus <a href="https://kyutai.org/">Kyutai</a> in 2023 was born and all of our characters united. It was the first and is the only open audio lab, named for the Japanese word for “sphere.” In fact their two major model releases also carry Japanese names:</p><ul role="list"><li><a href="https://kyutai.org/blog/2024-09-18-moshi-release">Moshi</a>, discussed earlier, the first realtime voice model (<a href="https://arxiv.org/abs/2410.00037">paper here</a>)</li><li><a href="https://kyutai.org/blog/2025-02-10-hibiki">Hibiki</a>, a simultaneous STS translation model in the speaker’s voice (<a href="https://arxiv.org/abs/2502.03382">paper here</a>)</li></ul><p>Kyutai is open because Neil and his cofounders believe in open research, and as competitive pressure between labs was intensifying, fewer and fewer papers were being published. With funding from Eric Schmidt and two French billionaires, they started cooking. </p><p>In addition to the above, Kyutai has released open source text-to-speech and speech-to-text models — the foundation for <a href="https://research.nvidia.com/labs/adlr/personaplex/">NVIDIA’s PersonaPlex</a> and <a href="https://github.com/QwenLM/Qwen3-TTS">Qwen3-TTS</a>. Their real-time speech-to-speech translation (you can check out the demo below) was running on-device many months before Apple’s. </p><p>All of this is nice, but mostly research as research. Kyutai models are fundamentally prototypes, and real apps need much more polished models. So part of the Kyutai team started Gradium to bridge that last mile between research and product, and <a href="http://google.com/search?q=gradium+funding&amp;rlz=1C5GCCM_en&amp;oq=gradium+fun&amp;gs_lcrp=EgZjaHJvbWUqBwgAEAAYgAQyBwgAEAAYgAQyBggBEEUYOTIKCAIQABiABBiiBDIHCAMQABjvBTIHCAQQABjvBTIKCAUQABiABBiiBDIKCAYQABiABBiiBNIBCDIwODlqMGo3qAIAsAIA&amp;sourceid=chrome&amp;ie=UTF-8">raised $70M to do it</a>. You can think of this as a sort of pipeline from fundamental Kyutai research into production-grade products via Gradium. And in a few short months, they built and shipped multi-lingual models that compete with the best in class. </p><h2><strong>Anatomy of training an audio model</strong></h2><p>When it comes to training audio is both like text and not like text. </p><p>To start with the similarities, most SOTA audio models use architectures that are pretty similar to text, e.g. they’re Transformer-based among other things. The nice thing about borrowing LLM architectures is you benefit from all of the advances in text over the past few years, RLHF techniques, distillation, and the hardware out there optimized for LLMs.</p><p>But unlike text that has the internet corpus, there is not a huge treasure trove of available high quality audio data. And what “audio data” even <em>means</em> is a moving target, because what exactly do you want to train on: labeled transcribed conversations? Translations of a single speaker’s voice? Conversations with multiple participants?  Peruse through the typical open datasets and test sets for audio AI (<a href="https://github.com/facebookresearch/voxpopuli">Voxpopuli</a>, <a href="https://arxiv.org/abs/2309.04662">MADLAD-400</a>, <a href="https://github.com/MicrosoftTranslator/NTREX">NTREX</a>) and you can get a sense of how much more disjointed this is than text.</p><p>Audio models are also very small compared to LLMs. Moshi, Kyutai’s foundation audio model, has 7B parameters and was trained on only 2.1T tokens. As a result they tend to <em>know</em> a lot less ground information than a typical LLM. </p><p>All-in-all Moshi was:</p><ul role="list"><li>Pretrained on 7M hours of audio with transcripts.</li><li>Post-trained on the Fisher dataset (2000 hours of phone conversations with separated channels).</li><li>Instruction finetuned on 20k+ hours of synthetic dialogue.</li></ul><p>One of the hardest parts of training these models, especially when it comes to reward functions in post-training, is the subjective nature of evaluations. This problem is <a href="https://www.amplifypartners.com/barrchives/how-suno-builds-ai-models-for-musicians">well documented</a> in the music generation space. Good conversations are completely subjective! Neil and co. completely gave up on quantitative measures and only trusted humans, doing tons of blind tests and just listening (they also supplement their efforts with freelancers).</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/698ded55b591ab70306aca25_bd56afbe.webp" loading="lazy" alt=""/></p></figure><h2><strong>Audio model architectures: speech-to-speech vs. full duplex</strong></h2><p>One of the hardest problems to solve in audio AI has been the <strong>turn taking problem</strong>. How do you effectively trade conversation with an unpredictable user?<sup> </sup>When is the user done talking, and when are they just thinking? How do you handle interruptions? Should the model ever interrupt the user, and if so when? (The model should just <a href="https://www.goodreads.com/book/show/4865.How_to_Win_Friends_Influence_People">read Dale Carnegie</a>, duh.) It turns out it’s really hard to nail this dynamic, and among audio researchers it’s thought of as one of the most challenging problems in the space.</p><p>Accordingly, you will see “speech-to-speech” thrown around a lot, but not all S2S models are created equal. </p><p>Today’s OpenAI models are <em>technically</em> speech-to-speech, but they are <strong>turn-based</strong>. They are trained to understand (predict) when the user is finished talking or asking their question, which sounds reasonable enough, but ends up creating weird dynamics. For example, if you (the user) are silent for a few seconds because you’re thinking about the right formulation, the model is going to talk even though you didn’t want it to. It also cannot interrupt you (even though sometimes it should), and until recent editions, it was impossible to interrupt the model itself. This is like talking to someone on a walkie talkie, fun but ultimately not quite the real thing. </p><p><strong>Full duplex</strong> models, on the other hand, are like being on the phone. It’s more like a real conversation, where the model interacts with you dynamically, you can both interrupt each other, and it’s more intelligent when it comes to interpreting your intent. These models are proficient at backchanneling (“aha, yes, I see, mhm”) which tends to make the conversation more lively and natural feeling.</p><p>You can see (hear?) this idea in action by talking to <a href="http://moshi.chat/">Moshi</a>, Kyutai’s realtime audio foundation model they released last year. It was the first realtime audio model on the planet, almost miraculously built by a non-profit team of 8 with a budget that was orders of magnitude smaller than the big labs. It’s a little rough around the edges, but the experience is pretty incredible.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/698ded55b591ab70306aca2e_11c4e818.webp" loading="lazy" alt=""/></p></figure><p>Kyutai <a href="https://arxiv.org/abs/2410.00037">pioneered this full duplex architecture</a>, and to build it required a few clever research ideas. </p><figure><a href="https://arxiv.org/abs/2410.00037" target="_blank"><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/698df0ffc52049c9c532202e_gradium1.webp" loading="lazy" alt=""/></p></a></figure><h3><strong>Multi-stream modeling</strong></h3><p>First, instead of modeling the conversation as one audio stream – the user’s – they model it with <strong>two</strong>, one for the user and one for the machine. When the machine isn’t <em>speaking</em>, it’s producing silence (or backchanneling) in the stream. This means that both can be active at the same time (or one active and one inactive), unlike turn based architectures. It’s an extremely simple architectural idea but it mostly solves turn taking, which is arguably the most challenging problems in dialogue / audio AI.  </p><p>The full duplex architecture ended up being useful for more than just basic realtime audio generation. Kyutai’s second model family <a href="https://kyutai.org/blog/2025-02-10-hibiki">Hibiki</a> uses the same idea to realtime translate audio into another language…using the speaker’s exact voice. It&#39;s <a href="https://kyutai.org/blog/2025-02-10-hibiki">one of my favorite demos</a> I’ve seen in a while.</p><p>Bidirectional audio, especially when you introduce multilinguality, is incredibly difficult. For example…where are you going to find data of the same person in the same voice saying the same thing in multiple different languages? Gradium’s approach here is called DSM (Delayed Streams Modeling) and though it’s beyond the scope of this post, you can <a href="https://arxiv.org/abs/2509.08753">read about it here</a>.</p><figure><a href="https://arxiv.org/abs/2509.08753" target="_blank"><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/698df163cde4353c94905d04_gradium2.webp" loading="lazy" alt=""/></p></a></figure><h3><strong>Audio codecs and Mimi</strong></h3><p>Second, which I mentioned earlier, is their SOTA codec called Mimi, based on earlier research from Neil at Google (<a href="https://arxiv.org/abs/2107.03312">SoundStream</a>). This one requires a bit of background, so bear with me.</p><p>Codec is short for encoder-decoder and it’s how you compress audio data into something a model can use; you can think of it like embeddings but for audio<sup>6</sup>. Codecs are in a sense modular, and there are specific ones developed for different types of audio like <a href="https://en.wikipedia.org/wiki/Comparison_of_audio_coding_formats">music</a> or <a href="https://en.wikipedia.org/wiki/Category:Speech_codecs">speech</a>. There are <em>tons</em> of these, and they implement very manual, bespoke rules that we know about the specific medium. Accordingly you can’t use a speech codec for music and vice versa. They also output completely different bitrates based on the target application. These are very bad qualities if you’re trying to train a model.</p><p>Neil’s idea while he was at Google Brain was instead to just train a model to do this. This work culminated in <a href="https://arxiv.org/abs/2107.03312">SoundStream</a>, a neural codec that can compress speech, music, <em>and</em> general audio at bitrates normally targeted by speech-only codecs. It worked pretty much just as well as domain-specific codecs with all of these ergonomic benefits, and was a big deal in the field at the time. </p><p>Another vein of research that Olivier and Neil worked on at Google, <a href="https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/">AudioLM</a>, introduced an even more novel idea for the model’s compression system. When it comes to audio there are two types of tokens researchers deal with:</p><ol role="list"><li><strong>Semantic tokens</strong> – these represent <em>the content</em> of what’s being said, the words and what they mean.</li><li><strong>Acoustic tokens</strong> – these represent <em>the acoustic style</em> of what’s being said, how the voice sounds, emotion, etc.</li></ol><p>Previous work in the space modeled these two tokens separately using a hierarchical approach, starting with the semantic (content) and then moving onto the acoustic (style). But prevailing open models for generating these semantic tokens are <strong>non-causal</strong> (like <a href="https://arxiv.org/abs/2110.13900">WavLM</a>) and so they absolutely do not work in real time. It needs future audio to compute the current embedding.</p><p>Kyutai’s approach – and how they solved the real time problem – is by <em>combining</em> these two types of tokens into a <em>single generation process</em>, thanks to some fancy footwork on summing vector quantizations. When Moshi generates speech it produces 8 tokens per timestep: token 1 is a semantic token, and 2-7 are acoustic tokens. The whole causal system processes audio as it arrives and is thus able to generate all tokens (semantic + acoustic) in real-time.</p><p>Reading Kyutai papers, you can’t help but get this frenetic sense of depth. Most of the AI stuff I read on Arxiv has 1-3 fundamental ideas that enable whatever new architecture the paper is about. But <strong>Kyutai papers have like 13</strong>. For the Moshi paper I’m skipping over innovations in generation (Temporal+Depth transformers), differing weights on token loss, their Inner Monologue method…it’s hard to keep up. </p><p>All of this fancy work results in latency (at least theoretically) of 160ms, which is lower than the typical 200-250ms you’d expect in a human conversation. Talking to Moshi you can see that in some senses it’s even <em>too</em> fast. </p><h2><strong>Why small teams win at audio</strong></h2><p>If you want to distill the story of Gradium (and other audio labs) and why startups continue to beat big labs at audio, it&#39;s this:</p><h3><strong>1) Audio models are much smaller and cheaper to train than text models </strong></h3><p>Moshi has 7B parameters and was trained on 2.1T tokens. Llama 3.1 has 405B parameters trained on 15T tokens—that’s orders of magnitude of difference in cost. You don’t need a thousand people or a massive compute cluster. You need a few exceptional people who understand the domain deeply.</p><h3><strong>2) Audio requires genuine domain expertise in a way text doesn’t</strong></h3><p>A text tokenizer is essentially a dictionary – you break words into subword tokens and you’re done. An audio codec like Mimi relies on deep understanding of how human hearing works, acoustic psychophysics, how to balance compression against perceptual quality. The bitter lesson is like…not so bitter here.</p><p>Similarly, if you’re training a multimodal model, you’re constantly compromising—more coding data means less audio data. Voice is always negotiating with text, video, and image teams for model capacity and training budget. At a focused lab, there&#39;s no compromise. Everything optimizes for audio.</p><h3><strong>3) Audio innovation is still about clever ideas efficiently executed, not just scale</strong></h3><p>The full duplex architecture that Kyutai pioneered is a simple idea (model both streams simultaneously) but it solved the turn-taking problem that had stumped researchers for years. The Mimi codec’s technique for combining semantic and acoustic tokens uses novel compression rather than brute force. The opportunity in audio AI is that “very few people just need to focus on the right questions”.</p><p>Gradium is taking all of this and bringing it to the people in real, production-ready models that you can actually use to build things. You can chat with their models <a href="https://gradium.ai/">on their site</a>, and look through <a href="https://gradium.ai/api_docs.html">their API docs here</a>. But most importantly...<strong>may the voice be with you</strong>.</p></div></div>
  </body>
</html>
