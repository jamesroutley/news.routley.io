<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cprimozic.net/blog/neural-network-experiments-and-visualizations/">Original</a>
    <h1>Exploring Neural Networks Visually in the Browser</h1>
    
    <div id="readability-page-1" class="page"><div><p><img width="673" height="463" alt="A 4-second loop of the neural network visualization application training a network, showing both the response of the network as a whole, a cost plot, as well as a visualization of neuron weights and an individual neuron&#39;s response plot" src="https://nn.ameo.dev/nn-viz-demo-animation.webp"/></p>
<p>While teaching myself the basics of neural networks, I was finding it hard to bridge the gap between the foundational theory and a practical &#34;feeling&#34; of how neural networks function at a fundamental level.  I learned how pieces like gradient descent and different activation functions worked, and I played with building and training some networks in a <a href="https://colab.research.google.com/">Google Colab</a> notebook.</p>
<p>Modern toolkits like Tensorflow handle the full pipeline from data preparation to training to testing and everything else you can think of - all behind extremely high-level, well-documented APIs.  The power of these tools is obvious.  Anyone can load, run, and play with state of the art deep learning architectures in GPU-accelerated Python notebooks instantly in the web browser.  Even implementations of bleeding-edge research papers are readily available on sites like <a href="https://huggingface.co/EleutherAI/gpt-j-6B">Hugging Face</a>.</p>
<h2 id="the-problem"><a href="#the-problem" aria-label="the problem permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Problem</h2>
<p>Despite the richness of the ecosystem and the incredible power of the available tools, I felt like I was missing a core piece of the puzzle in my understanding.</p>
<p>On one side, there are the very abstract concepts built on calculus and matrix multiplication which provide the underlying mechanism for how neural networks function.  On the other end, there are the extremely high-level software suites used to work with neural networks for practical and research purposes.  The idea of partial derivatives being used to compute gradients which optimize the neurons&#39; weights and biases made sense, but I couldn&#39;t get a clear picture of it in my head - especially how it scales up to thousands and millions of neurons and dozens of layers.</p>
<p>I come from a software background, and when I was learning how compilers and code generation worked one of my favorite tools was and still is <a href="https://godbolt.org/">Compiler Explorer</a> aka Godbolt.  It&#39;s a web application where you can type in any code you want in a variety of languages, choose a compiler and compilation options, and instantly view the disassembled output for a wide range of different hardware architectures.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/77267/compiler_explorer.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/8359c/compiler_explorer.avif 210w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/4c727/compiler_explorer.avif 420w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/4ab03/compiler_explorer.avif 840w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/c3c94/compiler_explorer.avif 1260w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/c095d/compiler_explorer.avif 1680w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/3baa7/compiler_explorer.avif 1916w" sizes="(max-width: 840px) 100vw, 840px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/aaa7a/compiler_explorer.png 210w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/2dc40/compiler_explorer.png 420w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/993bb/compiler_explorer.png 840w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/db723/compiler_explorer.png 1260w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/7bc17/compiler_explorer.png 1680w, https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/77267/compiler_explorer.png 1916w" sizes="(max-width: 840px) 100vw, 840px" type="image/png"/>
          <img src="https://cprimozic.b-cdn.net/static/82c9ae9f1b65da5e608a3a251d68bab1/993bb/compiler_explorer.png" alt="A screenshot of Godbolt showing line-by-line mappings of a Rust function into x86 assembly code" title="A screenshot of Godbolt showing line-by-line mappings of a Rust function into x86 assembly code" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>I find this tool to be unparalleled for learning about compiler code generation patterns and understanding what kinds of assembly gets output for different kinds of code input.  It&#39;s dynamic and responds instantly as soon as you poke it.  It&#39;s an environment for experimentation rather than a static knowledge resource.  Crucially, it provides a visual mapping between the two sides of the extremely complex transformation taking place under the hood.</p>
<p>This is what I wanted for neural networks: A constrained, simplified environment for building basic network topologies and experimenting live to see <i>visually</i> how different layer counts, sizes, activation functions, hyperparameters, etc. impact their functionality and performance.</p>
<h2 id="neural-network-sandbox"><a href="#neural-network-sandbox" aria-label="neural network sandbox permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neural Network Sandbox</h2>
<p>With this goal in mind, I created a browser-based tool for building, training, visualizing, and experimenting with neural networks.  Since it runs on the web, I&#39;ve embedded it directly in this post:</p>
<p>[hide]</p>
<p>What you see above is a fully-fledged neural network implementation running in your browser.  You can add, remove, and configure the layers to change the activation function, neuron count, and initialization parameters.  Hit one of the &#34;train&#34; buttons, and the network will start learning from examples to match one of the variety of selectable target functions.</p>
<p>There are also a few different built-in visualizations to provide insight into the progress of the network as it trains and inspect the internal state of the network - all the way down to individual neurons.</p>
<p>A standalone version of the sandbox is also available: <a href="https://nn.ameo.dev">https://nn.ameo.dev</a></p>
<p>The full source code is available on Github: <a href="https://github.com/ameobea/neural-network-from-scratch">https://github.com/ameobea/neural-network-from-scratch</a></p>
<h3 id="how-it-works"><a href="#how-it-works" aria-label="how it works permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How it Works</h3>
<p>The sandbox trains neural networks to model functions mapping vectors of 2 numbers from [0, 1] to a single output value from [0, 1].  Most neural networks you&#39;ll see in practice deal with vectors with thousands or more dimensions like images or <a href="https://cprimozic.net/blog/graph-embeddings-for-music-discovery/">graph embeddings</a>, but there is a good reason I chose to keep it this small.</p>
<p>By limiting the dimensionality of the input and output vectors to 2 and 1 respectively, the entire range of input data can be plotted as a 3D surface and visualized at once.</p>
<p>I refer to this as the &#34;response&#34; of the network, inspired by <a href="https://en.wikipedia.org/wiki/Frequency_response">frequency response</a> of digital filters which works with a very similar premise.</p>
<p>The 3D area plot shown by default shows a translucent view of the target function that the network is modelling.  As the network is trained, the network is periodically sampled with values throughout the entire valid input range, and the outputs are plotted alongside the target function.  If the network is learning successfully, its response plot will get closer and closer to the target as it sees more and more examples.</p>
<p>If you click the &#34;Layer Outputs Viz&#34; button, a secondary visualization of the network&#39;s internals is opened.  It shows the output value of all neurons in the network, and it updates live as the network is trained.  Additionally, clicking/tapping on any of the neurons will open a response plot <em>for that individual neuron</em>.  It allows you to see exactly what inputs will cause each neuron to &#34;fire&#34;, and how much of an effect it has on neurons in subsequent layers.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/9e344b807df3e4bb9f862da0e32f07f4/eee59/neuron-response-plot.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/9e344b807df3e4bb9f862da0e32f07f4/8359c/neuron-response-plot.avif 210w, https://cprimozic.b-cdn.net/static/9e344b807df3e4bb9f862da0e32f07f4/996b6/neuron-response-plot.avif 251w" sizes="(max-width: 251px) 100vw, 251px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/9e344b807df3e4bb9f862da0e32f07f4/aaa7a/neuron-response-plot.png 210w, https://cprimozic.b-cdn.net/static/9e344b807df3e4bb9f862da0e32f07f4/eee59/neuron-response-plot.png 251w" sizes="(max-width: 251px) 100vw, 251px" type="image/png"/>
          <img src="https://cprimozic.b-cdn.net/static/9e344b807df3e4bb9f862da0e32f07f4/eee59/neuron-response-plot.png" alt="A screenshot of the response plot for a neuron in one of the networks created using the neural network sandbox" title="A screenshot of the response plot for a neuron in one of the networks created using the neural network sandbox" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>Give it a try yourself!  Try using more or less hidden layers, pick a different target function, experiment with different activation functions, and try tweaking the learning rate.</p>
<h2 id="learnings--observations"><a href="#learnings--observations" aria-label="learnings  observations permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Learnings + Observations</h2>
<p>I&#39;ve personally spent a ton of time just playing with various topologies and parameters and seeing how the networks respond.  That was my whole reason behind building the sandbox after all!  Here&#39;s a collection of the most interesting things I&#39;ve observed.</p>
<h3 id="neuron-responses--feature-generation"><a href="#neuron-responses--feature-generation" aria-label="neuron responses  feature generation permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neuron Responses + Feature Generation</h3>
<p>Adding more layers gives networks the ability to do things that just adding more neurons to a single layer cannot.</p>
<p>This is especially apparent on more complex target functions.  For &#34;Fancy Sine Thing&#34;, a 2-layer network with sizes of 24 and 12 far outperformed a single layer with 128 neurons.  This makes some sense since the number of parameters in a network increases as the product of the count of neurons in adjacent layers.</p>
<p>Some additional clues as to why adding more layers can be so powerful can be found by looking at the response plots for individual neurons of different layers.  I created a network with 4 hidden layers where the number of neurons in each is half that of the one before it:</p>
<p>Click to open demo</p>
<p>After training the network for a few million examples, the network mostly settles on &#34;jobs&#34; for all of its neurons and the responses of neurons from different layers become very interesting.  The deeper you get in the network, the more complicated the response plots for the neurons get.</p>
<p>Neurons in the first hidden layer have responses that are limited by the dimensionality of the inputs and the simplicity of the activation function.  This is about as complex as it gets for hidden layer 1:</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/6ad59c7105ae690e0faba94189bad468/346e6/layer1.jpg" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/6ad59c7105ae690e0faba94189bad468/8359c/layer1.avif 210w, https://cprimozic.b-cdn.net/static/6ad59c7105ae690e0faba94189bad468/996b6/layer1.avif 251w" sizes="(max-width: 251px) 100vw, 251px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/6ad59c7105ae690e0faba94189bad468/0f426/layer1.jpg 210w, https://cprimozic.b-cdn.net/static/6ad59c7105ae690e0faba94189bad468/346e6/layer1.jpg 251w" sizes="(max-width: 251px) 100vw, 251px" type="image/jpeg"/>
          <img src="https://cprimozic.b-cdn.net/static/6ad59c7105ae690e0faba94189bad468/346e6/layer1.jpg" alt="The response of the first hidden layer of the network showing a linear-looking gradient" title="The response of the first hidden layer of the network showing a linear-looking gradient" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>The second layer gets a bit more interesting.  It pulls from multiple neurons in the first hidden layer which have their gradients oriented many different ways.  For example, this neuron only activates significantly within an &#34;island&#34;.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/7b115cbd2e9c9ccb42e859d50ecbf2d7/55324/layer2.jpg" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/7b115cbd2e9c9ccb42e859d50ecbf2d7/8359c/layer2.avif 210w, https://cprimozic.b-cdn.net/static/7b115cbd2e9c9ccb42e859d50ecbf2d7/23353/layer2.avif 248w" sizes="(max-width: 248px) 100vw, 248px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/7b115cbd2e9c9ccb42e859d50ecbf2d7/0f426/layer2.jpg 210w, https://cprimozic.b-cdn.net/static/7b115cbd2e9c9ccb42e859d50ecbf2d7/55324/layer2.jpg 248w" sizes="(max-width: 248px) 100vw, 248px" type="image/jpeg"/>
          <img src="https://cprimozic.b-cdn.net/static/7b115cbd2e9c9ccb42e859d50ecbf2d7/55324/layer2.jpg" alt="The response of the second hidden layer of the network showing an island-like region where it activates strongly and little activation activity outside of it" title="The response of the second hidden layer of the network showing an island-like region where it activates strongly and little activation activity outside of it" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>By the third layer, the neuron&#39;s response is significantly more complex with concave features and holes.  The transition zones between activated and and not activated are a lot sharper as well, making the output more binary.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/233247d5891f405b0190c99bfab70a49/c6128/layer3.jpg" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/233247d5891f405b0190c99bfab70a49/8359c/layer3.avif 210w, https://cprimozic.b-cdn.net/static/233247d5891f405b0190c99bfab70a49/99af4/layer3.avif 250w" sizes="(max-width: 250px) 100vw, 250px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/233247d5891f405b0190c99bfab70a49/0f426/layer3.jpg 210w, https://cprimozic.b-cdn.net/static/233247d5891f405b0190c99bfab70a49/c6128/layer3.jpg 250w" sizes="(max-width: 250px) 100vw, 250px" type="image/jpeg"/>
          <img src="https://cprimozic.b-cdn.net/static/233247d5891f405b0190c99bfab70a49/c6128/layer3.jpg" alt="The response of the third hidden layer of the network showing a more complicated response pattern with concave features and holes" title="The response of the third hidden layer of the network showing a more complicated response pattern with concave features and holes" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>In the fourth and final hidden layer, the response plot is more complex still and visually resembles parts of the response of the target function itself.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/4aea894d2dbc3afa569d81c02b15f059/55324/layer4.jpg" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/4aea894d2dbc3afa569d81c02b15f059/8359c/layer4.avif 210w, https://cprimozic.b-cdn.net/static/4aea894d2dbc3afa569d81c02b15f059/23353/layer4.avif 248w" sizes="(max-width: 248px) 100vw, 248px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/4aea894d2dbc3afa569d81c02b15f059/0f426/layer4.jpg 210w, https://cprimozic.b-cdn.net/static/4aea894d2dbc3afa569d81c02b15f059/55324/layer4.jpg 248w" sizes="(max-width: 248px) 100vw, 248px" type="image/jpeg"/>
          <img src="https://cprimozic.b-cdn.net/static/4aea894d2dbc3afa569d81c02b15f059/55324/layer4.jpg" alt="The response of the fourth hidden layer of the network showing a response plot that looks visually similar to part of the response of the target function" title="The response of the fourth hidden layer of the network showing a response plot that looks visually similar to part of the response of the target function" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>I find it fascinating to observe how the networks manage to create features for themselves out of extremely simple inputs and progressively refine them into more and more accurate representations of the target function.  I recommend trying out some different network structures and changing up the activation functions and seeing what the neuronal responses look like.</p>
<h3 id="network-topology--hyperparameters"><a href="#network-topology--hyperparameters" aria-label="network topology  hyperparameters permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Network Topology + Hyperparameters</h3>
<p>Slowly reducing the learning rate while training can help models reach a lower final error before converging</p>
<p>You can do this for yourself in the sandbox by dragging the &#34;learning rate&#34; slider down while the network is training a large batch of examples.</p>
<p>Another thing that sometimes works is increasing the learning rate for short periods of time to help break out of local minima - but this can just as easily have a negative effect.</p>
<p>Networks with more parameters (both wide and deep) seem to require more examples before converging.</p>
<p>This is partially due to the fact that lower training rates are needed to keep them stable during training, but it feels like more than that as well.  I saw some networks that still hadn&#39;t converged (loss was still decreasing) even after being trained with several million examples.</p>
<h3 id="relu-problems--limitations"><a href="#relu-problems--limitations" aria-label="relu problems  limitations permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ReLU Problems + Limitations</h3>
<p>Using the sandbox, you can directly visualize the <a href="https://arxiv.org/abs/1903.06733">Dying ReLU Problem</a>.  Since the ReLU activation function has a derivative of 0 when its output is &lt;= 0, the gradient will also be zero for these values which means the neurons can &#34;die&#34; and never output anything other than 0.</p>
<p>You can see this happen yourself using the sandbox.  Train 250k or so examples then click some of the neurons of hidden layer in the layers visualization.  Eventually, you should find one where the entire response plot should be gray - the neuron will only ever output 0 for all possible values in the input range.  This neuron is &#34;dead&#34; and will be stuck like that forever no matter how many examples are trained.</p>
<p>Click to open demo</p>
<p>One solution for the dying ReLU problem is to switch it for a different but very similar activation function called <a href="https://paperswithcode.com/method/leaky-relu">Leaky ReLU</a> which has a very small but non-zero gradient for zero and negative values.  Try swapping the activation functions for the two layers with &#34;leaky relu&#34; from the dropdown, reset the viz, and see what effect it has.</p>
<p>As the paper linked above notes, another method for alleviating the dying ReLU problem is altering the way that network parameters such as weights and biases are initialized.</p>
<p>The values that weights + biases are initialized to is critical for training performance and network stability.</p>
<p>Initializing weights or biases all to a constant value rarely seems to be the best option.  This is especially true for activation functions like ReLU which have gradients that behave badly at exactly zero due to the discontinuity at that point.  Initializing starting weights or biases to values that are too large can cause the training to diverge immediately.</p>
<h3 id="complex-activation-functions"><a href="#complex-activation-functions" aria-label="complex activation functions permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Complex Activation Functions</h3>
<p>A <a href="https://arxiv.org/abs/2108.12943v2">recent paper</a> from August 2021 introduced the rather exotic Growing Cosine Unit (GCU) activation function.  As its name suggests, it uses the equation <code>x * cos(x)</code> to provide nonlinearity.  Here&#39;s a plot of its output I made using Wolfram Alpha:</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/ff445e5d513d35e82ecc202f03236cf5/85129/gcu-plot.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/ff445e5d513d35e82ecc202f03236cf5/8359c/gcu-plot.avif 210w, https://cprimozic.b-cdn.net/static/ff445e5d513d35e82ecc202f03236cf5/15235/gcu-plot.avif 353w" sizes="(max-width: 353px) 100vw, 353px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/ff445e5d513d35e82ecc202f03236cf5/aaa7a/gcu-plot.png 210w, https://cprimozic.b-cdn.net/static/ff445e5d513d35e82ecc202f03236cf5/85129/gcu-plot.png 353w" sizes="(max-width: 353px) 100vw, 353px" type="image/png"/>
          <img src="https://cprimozic.b-cdn.net/static/ff445e5d513d35e82ecc202f03236cf5/85129/gcu-plot.png" alt="A screenshot of a plot of the output of the growing cosine unit (GCU) activation function from x=-5 to 5.  It shows a complex curve that switches from negative to positive 5 times within that range." title="A screenshot of a plot of the output of the growing cosine unit (GCU) activation function from x=-5 to 5.  It shows a complex curve that switches from negative to positive 5 times within that range." loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>As stated in the paper&#39;s abstract, &#34;It is shown that oscillatory activation functions allow neurons to switch classification (sign of output) within the interior of neuronal hyperplane positive and negative half-spaces allowing complex decisions with fewer neurons.&#34;</p>
<p>We can test that claim directly with the sandbox:</p>
<p>Click to open demo</p>
<p>The network in the above demo only has 2 layers of sizes 16 and 12.  The first layer uses the GCU activation function, and the second uses Leaky ReLU.  Despite its small layer sizes and low layer counts, it is still able to fit the target function pretty well and it only takes ~300k examples for it to mostly converge.</p>
<p>By examining the output of some of the GCU neurons in the first layer, it is clear that the response is much more complex than what you can get with a simple activation function like ReLU.  It can also be seen that the output does indeed switch between negative and positive multiple times, just as the abstract claimed.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/ce4106149432ff8da15390a4c59a9231/7e472/gcu-response.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/ce4106149432ff8da15390a4c59a9231/8359c/gcu-response.avif 210w, https://cprimozic.b-cdn.net/static/ce4106149432ff8da15390a4c59a9231/23353/gcu-response.avif 248w" sizes="(max-width: 248px) 100vw, 248px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/ce4106149432ff8da15390a4c59a9231/aaa7a/gcu-response.png 210w, https://cprimozic.b-cdn.net/static/ce4106149432ff8da15390a4c59a9231/7e472/gcu-response.png 248w" sizes="(max-width: 248px) 100vw, 248px" type="image/png"/>
          <img src="https://cprimozic.b-cdn.net/static/ce4106149432ff8da15390a4c59a9231/7e472/gcu-response.png" alt="A screenshot of a plot of the response of a single neuron with the growing cosine unit (GCU) activation function, demonstrating multiple transitions between positive and negative values" title="A screenshot of a plot of the response of a single neuron with the growing cosine unit (GCU) activation function, demonstrating multiple transitions between positive and negative values" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<p>By using a more complex activation functions like the GCU early on in networks&#39; layers, a greater amount and variety of internal features can be generated for the later layers to refine down and process further.  There is a price for this, though - the GCU is much more expensive to compute than the ReLU which is pretty much just a single multiplication.  This means that training and inference are slower.  There are some ways to improve this, though, which I&#39;ll detail later in this writeup.  Plus, the additional power that complex activation functions like GCU can provide provides means the networks that use them can be smaller, and smaller networks are inherently cheaper to train.</p>
<h3 id="other-observations"><a href="#other-observations" aria-label="other observations permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Other Observations</h3>
<p>Using ReLU and ReLU-like activation functions is by far the fastest for training.</p>
<p>This makes sense due to how incredibly trivial they are to compute - it&#39;s about as simple as it gets.  I was able to implement SIMD-accelerated versions of their activation functions as well as their derivatives for calculating gradients during backpropagation.</p>
<p>Models have trouble dealing with sharp transitions in multiple dimensions between different domains</p>
<p>Neural networks seems to require more &#34;resources&#34; (layer sizes/counts) to deal with these kinds of features in the functions they model.  They seem to be able to model smoother functions more easily; sharp discontinuity-like areas in the target function are hard for them to represent cleanly.  I&#39;d be willing to be that there&#39;s some research paper out there full of extremely dense math notation which proves this or something similar to it.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/fcfbd/multi_dimensional_domain_cutoff.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/8359c/multi_dimensional_domain_cutoff.avif 210w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/4c727/multi_dimensional_domain_cutoff.avif 420w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/4ab03/multi_dimensional_domain_cutoff.avif 840w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/c3c94/multi_dimensional_domain_cutoff.avif 1260w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/4e67d/multi_dimensional_domain_cutoff.avif 1506w" sizes="(max-width: 840px) 100vw, 840px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/aaa7a/multi_dimensional_domain_cutoff.png 210w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/2dc40/multi_dimensional_domain_cutoff.png 420w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/993bb/multi_dimensional_domain_cutoff.png 840w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/db723/multi_dimensional_domain_cutoff.png 1260w, https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/fcfbd/multi_dimensional_domain_cutoff.png 1506w" sizes="(max-width: 840px) 100vw, 840px" type="image/png"/>
          <img src="https://cprimozic.b-cdn.net/static/b2bf2a1c8facd28f47668fa3073bdf60/993bb/multi_dimensional_domain_cutoff.png" alt="A screenshot of the neural network sandbox showing how the network has difficulty dealing with sharp multidimensional transitions between different domains of a complex target function." title="A screenshot of the neural network sandbox showing how the network has difficulty dealing with sharp multidimensional transitions between different domains of a complex target function." loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<h2 id="technical-implementation--performance"><a href="#technical-implementation--performance" aria-label="technical implementation  performance permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Technical Implementation + Performance</h2>
<p>The neural network engine and much of the supporting visualizations and other UI features are built in Rust and compiled to WebAssembly.  The full source code for everything is <a href="https://github.com/Ameobea/neural-network-from-scratch/tree/main/engine/libnn">on Github</a>.  The visualizations are made using a combination of <a href="https://echarts.apache.org/en/index.html">ECharts</a> and hand-rolled canvas-based things.</p>
<h3 id="webassembly--wasm-simd"><a href="#webassembly--wasm-simd" aria-label="webassembly  wasm simd permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>WebAssembly + Wasm SIMD</h3>
<p>The best way to train neural networks is by using GPUs or other specialized hardware.  Even though it&#39;s possible to do this in the browser using WebGL compute shaders or <a href="https://web.dev/gpu/">WebGPU</a> in the future (this is how <a href="https://github.com/tensorflow/tfjs">TensorFlow.js</a> does it), you can still get great performance on the CPU - especially for the small networks used by the sandbox.</p>
<p>One of the biggest benefits of having the sandbox running in WebAssembly is it can be accelerated via <a href="https://v8.dev/features/simd">Wasm SIMD</a>.  I&#39;ve worked with this <a href="https://cprimozic.net/blog/speeding-up-webcola-with-webassembly/#wasm-simd--other-misc-optimizations">in the past</a> to accelerate various web-based applications and visualizations, and the performance it can provide even on relatively low-end mobile devices can be very impressive.</p>
<p>The core of neural network training is matrix multiplication, which is about as SIMD-friendly as it gets.  Multiple pieces of training + inference are implemented using hand-written Wasm SIMD</p>
<h3 id="approximations"><a href="#approximations" aria-label="approximations permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Approximations</h3>
<p>In addition to that, I used approximations based on the <a href="https://docs.rs/fastapprox"><code>fastapprox</code></a> library for several of the more complex activation functions.  Activation functions like Sigmoid, Hyperbolic Tangent, and GCU can be extremely expensive to compute in software, but luckily neural networks are very good at smoothing over the small imprecisions caused by the approximations while sometimes nearly 10x-ing the training rate.</p>
<h3 id="multi-threading"><a href="#multi-threading" aria-label="multi threading permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-Threading</h3>
<p>One of the most important things I did to make the sandbox more responsive and faster while training was to run the entire neural network on a separate thread from the UI via a web worker.  I used the excellent <a href="https://github.com/GoogleChromeLabs/comlink">Comlink</a> library to create easy-to-use bindings that wrap the underlying message passing interface between the UI thread and the neural network web worker.  This allows even the expensive 3D surface visualization to update decently smoothly while the model trains at full speed.</p>
<p><span>
      <a href="https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/2b9e4/web-worker-profile.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
          <source srcset="https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/8359c/web-worker-profile.avif 210w, https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/4c727/web-worker-profile.avif 420w, https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/0bb1a/web-worker-profile.avif 775w" sizes="(max-width: 775px) 100vw, 775px" type="image/avif"/>
          <source srcset="https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/aaa7a/web-worker-profile.png 210w, https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/2dc40/web-worker-profile.png 420w, https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/2b9e4/web-worker-profile.png 775w" sizes="(max-width: 775px) 100vw, 775px" type="image/png"/>
          <img src="https://cprimozic.b-cdn.net/static/df95dc591974f883a3a241f0bf089e70/2b9e4/web-worker-profile.png" alt="A screenshot of a CPU profile for the neural network sandbox showing the network training and visualizations running in separate threads" title="A screenshot of a CPU profile for the neural network sandbox showing the network training and visualizations running in separate threads" loading="lazy" decoding="async"/>
        </picture>
  </a>
    </span></p>
<h2 id="limitations"><a href="#limitations" aria-label="limitations permalink"><svg aria-hidden="true" height="20" version="1.1" viewBox="0 0 16 16" width="20" style="stroke:rgb(220, 220, 220)"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Limitations</h2>
<p>Although the sandbox is very useful for trying out a variety of different neural network topologies, there are some missing pieces and features it lacks:</p>
<ul>
<li>Examples are fed in one by one rather than in batches.  This is just a limitation of my implementation of the neural network&#39;s training; training efficiency and performance can often be greatly improved by combining the gradients of batches of examples during backpropagation.</li>
<li>Since the target functions are so simple, it&#39;s extremely easy for networks to overfit them.  For real applications, the inputs and outputs often have orders of magnitude more dimensions which is where bigger and deeper networks shine.</li>
<li>Additionally, since these networks are so simple, there are likely differences between how they work compared to huge networks with billions of parameters.  I&#39;d be interested to expand this neural network to support different kinds of input/output data types and shapes to see how the perform.</li>
<li>All layers in the sandbox&#39;s networks are densely connected.  Lots of modern networks use sparsely connected layers and other complex layers to help improve performance or enhance the networks&#39; capabilities.</li>
</ul>
<p>In addition, I would love to someday update the tool to support more features and network types.  In particular, I&#39;d personally be extremely interested to add support for basic RNNs.</p>

<ul>
<li><a href="https://www.cs.ryerson.ca/~aharley/vis/conv/">3D Visualization of a Convolutional Neural Network</a> - an awesome interactive 3D visualization that I&#39;ve spent a long time playing with myself.</li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> - an incredibly inspiring read that also includes lots of really useful graphics and examples to understand how RNNs work.</li>
</ul></div></div>
  </body>
</html>
