<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://soap.coffee/~lthms/posts/i-cannot-ssh-into-my-server-anymore.html">Original</a>
    <h1>I Cannot SSH into My Server Anymore (and That&#39;s Fine)</h1>
    
    <div id="readability-page-1" class="page"><article> 
<blockquote>
<p><em>I would like to thank Yann Régis-Gianas, Sylvain Ribstein and Paul Laforgue
for their feedback and careful review.</em></p>
</blockquote>
<p>To kick off 2026, I had clear objectives in mind: decommissioning <code>moana</code>, my
trusty $100+/month VPS, and setting up <code>tinkerbell</code>, its far less costly
successor.</p>
<p>On the one hand, I have been using <code>moana</code> to self-host a number of services,
and it was very handy to know that I had always a go-to place to experiment
with whatever caught my interest. On the other hand, $100/month is obviously a
lot of money, and looking back at how I used it in 2025, it was not
particularly well spent. It was time to downsize.</p>
<p>Now that <code>tinkerbell</code> is up and running, I cannot even SSH into it. In fact,
<em>nothing</em> can.</p>
<p>There is no need. To update one of the services it hosts, I push a new container
image to the appropriate registry with the correct tag. <code>tinkerbell</code> will fetch
and deploy it. All on its own.</p>
<p>In this article, I walk through the journey that led me to the smoke and
mirrors behind this magic trick: <a href="https://fedoraproject.org/coreos/" marked="">Fedora CoreOS <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a>, <a href="https://coreos.github.io/ignition/" marked="">Ignition <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> and <a href="https://docs.podman.io/en/latest/markdown/podman-quadlet.1.html" marked="">Podman
Quadlets <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> in the main roles, with <a href="https://developer.hashicorp.com/terraform" marked="">Terraform <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> as an essential supporting
character. This stack checks all the boxes I care about.</p>
<div><p><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>For interested readers, I have published <code>tinkerbell</code>’s <a href="https://github.com/lthms/tinkerbell" marked="">full setup <span><svg><use href="/~lthms/img/icons.svg#github"></use></svg></span></a> on
GitHub. This article reads as an experiment log, and if you are only
interested in the final result, you should definitely have a look.</p>
</div>
<h2>Container-Centric, Declarative, and Low-Maintenance</h2>
<p>Going into this, I knew I didn’t want to reproduce <code>moana</code>’s setup—it was fully
manual<label for="fn1"></label><span>
</span> and I no longer have the time or the motivation to fiddle with
the internals of a server. Instead, I wanted to embrace the principles my
DevOps colleagues had taught me over the past two years.</p>
<p>My initial idea was to start with this very website, since it was the only
service deployed on <code>moana</code> that I really wanted to keep. Since <a href="https://soap.coffee/~lthms/posts/DreamWebsite.html" marked="">I had written
a container image for this website</a>, I just had to look
for the most straightforward and future-proof way to deploy it in production™—
something I could later extend to deploy more cool projects, if I ever wanted
to<label for="fn2"></label><span>
</span>.</p>
<p><a href="https://docs.docker.com/compose/" marked="">Docker Compose <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> alone wasn’t a good fit. I like compose files, but one needs
to provision and manage a VM to host them. Ansible can provision VMs, but that
road comes with its own set of struggles. Writing good playbooks has always
felt surprisingly difficult to me. In particular, a good playbook is supposed
to handle two very different scenarios—provisioning a brand new machine, and
updating a pre-existing deployment—and I have found it particularly challenging
to ensure that both paths reliably produce the same result.</p>
<p><a href="https://kubernetes.io/" marked="">Kubernetes <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> was <em>very</em> appealing on paper. I have seen engineers turn compose
files into <a href="https://helm.sh/docs/topics/charts/" marked="">Helm charts <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> and be done with it. If I could do the same thing,
wouldn’t that be bliss? Unfortunately, Kubernetes is a notoriously complex
stack, resulting from compromises made to address challenges I simply don’t
face. Managed clusters could make things easier, but they aren’t cheap. That
would defeat the initial motivation behind retiring <code>moana</code>.</p>
<p><a href="https://fedoraproject.org/coreos/" marked="">CoreOS <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a>, being an operating system specifically built to <em>run containers</em>,
obviously stood out. That said, I had very little intuition on how it could
work in practice. So I started digging. I learned about Ignition first. Its
purpose is to provision a VM exactly once, at first boot. If you need to change
something afterwards, you throw away your VM and create a new one. This may
seem counter-intuitive, but since it eliminates the main reason I was looking
for an alternative to Ansible, I was hooked<label for="fn3"></label><span>
</span>.</p>
<p>I found out how to use systemd unit files to start containers via <code>podman</code> CLI
commands. That was way too cumbersome, so I pushed on for a way to orchestrate
containers <em>à la</em> Docker Compose. That’s when I discovered Podman Quadlets and
<a href="https://docs.podman.io/en/stable/markdown/podman-auto-update.1.html" marked="">auto-updates <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a>.</p>
<p>With that, everything clicked. I knew what I wanted to do, and I was very
excited about it.</p>
<h2>Assembling <code>tinkerbell</code></h2>
<p>For more than a year now, my website has been <a href="https://soap.coffee/~lthms/posts/DreamWebsite.html" marked="">served from RAM by a standalone,
static binary built in OCaml</a>, with TLS termination handled by Nginx and
<code>certbot</code>’s certificates renewal performed by yours truly<label for="fn4"></label><span>
</span>. I didn’t
have any reason to fundamentally change this architecture. I was simply looking
for a way to automate their deployment.</p>
<h3>Container-Centric, …</h3>
<p>The logical thing to do was to have <code>tinkerbell</code> run two containers:</p>
<ul>
<li><strong>The reverse proxy:</strong> I had been firmly on Team Nginx for years now, but
when I heard <a href="https://caddyserver.com/" marked="">Caddy <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> could “<em>automatically obtain</em> and <em>renew</em> TLS
certificates,” I was sold on giving it a try.</li>
<li><strong>The website itself:</strong> Static binaries can be wrapped inside a container
with close to zero overhead using the <a href="https://hub.docker.com/_/scratch" marked=""><code>scratch</code> <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a> base image, so I
did just that. I published it to a free-plan, public registry hosted on Vultr
that I created for the occasion<label for="fn5"></label><span>
</span>.</li>
</ul>
<figure><img src="https://mermaid.ink/img/pako:eNo9UMtOwzAQ_BVrTyCllZs4JPEBiYZjuQASEnUPTrxJLBK7ch1BafPvuC_2tDM7M7vaA9RWIXBoevtdd9J5snoVhoR6Wpe9RuM3ZDZ7PPKcEs5YciTLtYBSKrUnd--rt3sBm4t-N1atk9uOeG2-0FXY95fB8hZwJOX6A6ud9nj1oFEQQeu0Au7diBEM6AZ5gnA4SQT4DgcUwENrcPRO9gKEmYJtK82ntcPN6ezYdsAb2e8CGrdKenzWMtw0_LMuLERX2tF44AnLziHAD_ATIM3mMY3zlCZskRUPRQR74DGL58WCXrk8S6cIfs9b6TxlWZ4EcsFYkdOgR6W9dS-Xj9bWNLqF6Q97DWuS?type=png"/><figcaption><p>Nothing beats a straightforward architecture</p></figcaption></figure>
<p>Nothing fancy or unexpected here, which made it a good target for a first
deployment. It was time to open Neovim to write some YAML.</p>
<h3>Declarative, …</h3>
<p>At this point, the architecture was clear. The next step was to turn it into
something a machine could execute. To that end, I needed two things: first an
Ignition configuration, then a CoreOS VM to run it.</p>
<h4>The Proof of Concept</h4>
<p>Ignition configurations (<code>.ign</code>) are JSON files primarily intended to be
consumed by machines. They are produced from YAML files using a tool called
<a href="https://coreos.github.io/butane/" marked="">Butane <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a>. For instance, here is the first Butane configuration file I ended up
writing. It provisions a CoreOS VM by creating a new user (<code>lthms</code>), along with
a <code>.ssh/authorized_keys</code> file allowing me to SSH into the VM<label for="fn6"></label><span>
</span>.</p>
<pre><code><span>variant:</span> <span>fcos</span>
<span>version:</span> <span>1.5</span><span>.0</span>
<span>passwd:</span>
  <span>users:</span>
    <span>-</span> <span>name:</span> <span>lthms</span>
      <span>ssh_authorized_keys:</span>
        <span>-</span> <span>ssh-ed25519</span> <span>AAAAC3NzaC1lZDI1NTE5AAAAIKajIx3VWRjhqIrza4ZnVnnI1g2q6NfMfMOcnSciP1Ws</span> <span>lthms@vanellope</span>
</code></pre>
<p>What’s important to keep in mind is that Ignition runs exactly once, at first
boot. Then it is never used again. This single fact has far-reaching
consequences, and is the reason why any meaningful change implies replacing the
machine, not modifying it.</p>
<p>Before going any further, I wanted to understand how the actual deployment was
going to work. I generated the Ignition configuration file.</p>
<pre><code>butane main.bu &gt; main.ign
</code></pre>
<p>Then, I decided to investigate how to define the Vultr VM in Terraform. The
resulting configuration is twofold. First, we need to configure Terraform to be
able to interact with the Vultr API, using the <a href="https://registry.terraform.io/providers/vultr/vultr/latest/docs" marked="">Vultr provider <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a>. Second, I
needed to <a href="https://registry.terraform.io/providers/vultr/vultr/latest/docs/resources/instance" marked="">create the VM <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a><label for="fn7"></label><span>
</span> and feed it the Ignition
configuration.</p>
<pre><code>resource &#34;vultr_instance&#34; &#34;tinkerbell&#34; {
  region = &#34;cdg&#34;
  plan = &#34;vc2-1c-1gb&#34;
  os_id = &#34;391&#34;

  label = &#34;tinkerbell&#34;
  hostname = &#34;tinkerbell&#34;

  user_data = file(&#34;main.ign&#34;)
}
</code></pre>
<p>And that was it. I invoked <code>terraform apply</code>, waited for a little while, then
SSHed into the newly created VM with my <code>lthms</code> user. Sure enough, the
<code>tinkerbell</code> VM was now listed in the Vultr web interface. I explored for a
little while, then called <code>terraform destroy</code> and rejoiced when everything
worked as expected.</p>
<h4>The MVP</h4>
<p>At this point, I was basically done with Terraform, and I just needed to write
the Butane configuration that would bring my containers to life. As I mentioned
earlier, the first approach I tried was to define a systemd service responsible
for invoking <code>podman</code>.</p>
<pre><code><span>systemd:</span>
  <span>units:</span>
    <span>-</span> <span>name:</span> <span>soap.coffee.service</span>
      <span>enabled:</span> <span>true</span>
      <span>contents:</span> <span>|
        [Unit]
        Description=Web Service
        After=network-online.target
        Wants=network-online.target
</span>
        [<span>Service</span>]
        <span>ExecStart=/usr/bin/podman</span> <span>run</span> <span>\</span>
          <span>--name</span> <span>soap.coffee</span> <span>\</span>
          <span>-p</span> <span>8901</span><span>:8901</span> <span>\</span>
          <span>--restart=always</span> <span>\</span>
          <span>ams.vultrcr.com/lthms/www/soap.coffee:latest</span>
        <span>ExecStop=/usr/bin/podman</span> <span>stop</span> <span>soap.coffee</span>

        [<span>Install</span>]
        <span>WantedBy=multi-user.target</span>
</code></pre>
<p>Adding this entry in my Butane configuration and redeploying <code>tinkerbell</code> got
me exactly what I wanted. My website was up and running. For the sake of
getting something working first, I added the necessary configuration for Caddy
(the container and the provisioning of its configuration file), redeployed
<code>tinkerbell</code> again, only to realize I also needed to create a network so that
the two containers could talk together. After half an hour or so, I got
everything working, but was left with a sour taste in my mouth.</p>
<p>This would simply not do. I wasn’t defining anything, I was writing a shell
script in the most cumbersome way possible.</p>
<p>Then, I remembered my initial train of thought and started to search for a way
to have Docker Compose work on CoreOS<label for="fn8"></label><span>
</span>. That is when I discovered
Quadlet, whose <a href="https://github.com/containers/quadlet" marked="">initial repository does a good job justifying its
existence <span><svg><use href="/~lthms/img/icons.svg#github"></use></svg></span></a><label for="fn9"></label><span>
</span>. In particular,</p>
<blockquote>
<p>With quadlet, you describe how to run a container in a format that is very
similar to regular systemd config files. From these actual systemd
configurations are automatically generated (using <a href="https://github.com/containers/quadlet" marked="">systemd generators <span><svg><use href="/~lthms/img/icons.svg#github"></use></svg></span></a>).</p>
</blockquote>
<p>To give a concrete example, here is the <code>.container</code> file I wrote for my
website server.</p>
<pre><code><span>[Container]</span>
<span>ContainerName</span>=soap.c<span>off</span>ee
<span>Image</span>=ams.vultrcr.com/lthms/www/soap.c<span>off</span>ee:live

<span>[Service]</span>
<span>Restart</span>=always

<span>[Install]</span>
<span>WantedBy</span>=multi-user.target
</code></pre>
<p>I wasn’t wasting my time teaching systemd how to start containers anymore. I
was now declaring what should exist, so that systemd—repurposed for the
occasion as a container orchestrator—could take care of the rest.</p>
<div><p><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p>If your containers are basically ignored by systemd, be smarter than me. Do
not try to blindly change your <code>.container</code> files and redeploy your VM in a
very painful and frustrating loop. Simply ask systemd for the generator logs.</p>
<pre><code>sudo journalctl -b | grep -i quadlet 
</code></pre>
</div>
<p>I excitedly turned <code>caddy.service</code> into <code>caddy.container</code>, redeployed
<code>tinkerbell</code>, ran into the exact same issue I had encountered before and
discovered the easiest way for two Quadlet-defined containers to talk to each
other was to introduce a <a href="https://docs.podman.io/en/latest/_static/api.html?version=latest#tag/pods" marked=""><em>pod</em> <span><svg><use href="/~lthms/img/icons.svg#external-link"></use></svg></span></a>. Unlike Docker Compose which uses DNS
over a bridge network, a pod shares the network namespace, allowing containers
to communicate over <em>localhost</em>.</p>
<p>To define a pod, one needs to create a <code>.pod</code> file, and to reference it in
their <code>.container</code> files using the <code>PodName=</code> configuration option. A “few”
redeployments later, I got everything working again, and I was ready to call it
a day.</p>
<p>And with that, <code>tinkerbell</code> was basically ready.</p>
<div><p><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749 0 0 1 5 0h6c.199 0 .389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749 0 0 1-.22.53l-4.25 4.25A.749.749 0 0 1 11 16H5a.749.749 0 0 1-.53-.22L.22 11.53A.749.749 0 0 1 0 11V5c0-.199.079-.389.22-.53Zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5ZM8 4a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 4Zm0 8a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Caution</p><p>I’ve later learned that restarting a container that is part of a pod will
have the (to me, unexpected) side-effect to restart all the other containers
of that pod.</p>
</div>
<h3>And Low-Maintenance</h3>
<p>Now, the end of the previous section might have given you pause.</p>
<p>Even a static website like this one isn’t completely “stateless.” Not only does
Caddy require a configuration file to do anything meaningful, but it is also a
stateful application as it manages TLS certificates over time. Besides, I <em>do</em>
publish technical write-ups from time to time<label for="fn10"></label><span>
</span>.</p>
<p>Was I really at peace with having to destroy and redeploy <code>tinkerbell</code> every
time I need to change anything on my website?</p>
<p>On the one hand, <em>yes</em>. I believe I could live with that. I modify my website
only a handful of times even in good months, I think my audience could survive
with a minute of downtime before being allowed to read my latest pieces. It may
be an unpopular opinion, but considering my actual use case, it <em>was</em> good
enough. Even the fact that I do not store the TLS certificates obtained by
Caddy anywhere persistent should not be an issue. I mean, Let’s Encrypt has
fairly generous weekly issuance limits per domain <label for="fn11"></label><span>
</span>. I should be fine.</p>
<p>On the other hand, the setup was starting to grow on me, and I have <em>other</em> use
cases in mind that could be a good fit for it. So I started researching again,
this time to understand how a deployment philosophy so focused on immutability
was managing what seemed to be conflicting requirements.</p>
<p>I went down other rabbit holes, looking for answers. The discovery that stood
out the most to me—to the point where it became the hook of this article—was
Podman auto-updates.</p>
<p>To deploy a new version of a containerized application, you pull the new image
and restart the container. When you commit to this pattern, why should you be
the one performing this action? Instead, your VM can regularly check registries
for new images, and <em>update the required containers when necessary</em>.</p>
<p>In practice, Podman made this approach trivial to put in place. I just needed
to label my containers with <code>io.containers.autoupdate</code> set to <code>registry</code>,
enable the <code>podman-auto-update</code> timer<label for="fn12"></label><span>
</span>, and that was it. Now, every
time I update the tag <code>www/soap.coffee:live</code> to point to a newer version of my
image, my website is updated within the hour.</p>
<p>And that is when the final piece clicked. At this point, publishing an image
becomes the only deployment step. I didn’t need SSH anymore.</p>
<h2>The Road Ahead</h2>
<p><code>tinkerbell</code> has been running for a few days now, and I am quite pleased with
the system I have put in place. In retrospect, none of this is particularly
novel. It feels more like I am converging toward a set of practices the
industry has been gravitating toward for years.</p>
<figure><img src="https://soap.coffee/~lthms/img/iac-meme.jpg"/><figcaption><p>A man looking at the “CoreOS &amp; Quadlets” butterfly and wondering whether he’s looking at Infrastructure as Code. I’m not entirely sure of the answer.</p></figcaption></figure>
<p>The journey is far from being over, though. <code>tinkerbell</code> is up and running, and
it served you this HTML page just fine, but the moment I put SSH out of the
picture, it became a black box. Aside from some hardware metrics kindly
provided by the Vultr dashboard, I have no real visibility into what’s going on
inside. That is fine for now, but it is not a place I want to stay in forever.
I plan to spend a few more weekends building an observability stack<label for="fn13"></label><span>
</span>.
That will come in handy when things go wrong—as they inevitably do. I would
rather have the means to understand failures than guess my way around them.</p>
<p>Did I ever mention I am an enthusiastic Opentelemetry convert?</p>


</article></div>
  </body>
</html>
