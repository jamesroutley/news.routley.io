<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tembo.io/blog/tembo-data-warehouse">Original</a>
    <h1>We built our customer data warehouse all on Postgres</h1>
    
    <div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p><img loading="lazy" alt="tembo-dwh-stack" src="https://danluu.com/assets/images/dw_social-5b64307fab94a93d8f40fedf1ce29418.png" title="tembo-dwh-stacl" width="1092" height="500"/></p><p>At Tembo (like every as-a-service provider), we wanted to have a customer data warehouse to track and understand customer usage and behavior. We wanted to quickly answer questions like &#34;How many Postgres instances have we deployed?&#34;, &#34;Who is our most active customer?&#34; and &#34;How many signups do we have by time?&#34;. In order to do this, we needed to bring data from several sources into a single location and keep it up-to-date so we could build the dashboards.</p><p><img loading="lazy" alt="tembo-dashboard" src="https://danluu.com/assets/images/tembo_metrics-81a84fd44df51f1b585df45d7dfecf3c.png" title="tembo-dashboard" width="2572" height="554"/></p><p>Typically, this process requires several orchestration tools and technologies and the end result is a highly complex data ecosystem. However, we built our customer data warehouse completely on Postgres by using foreign data wrappers and other Postgres extensions, enhancing efficiency and simplifying the process. We released all the tools we&#39;ve built as open source projects which you can host on your own using our <a href="https://github.com/tembo-io/tembo/tree/main/tembo-operator" target="_blank" rel="noopener noreferrer">Kubernetes Operator</a>. We also made it straightforward for anybody to build such a data warehouse on <a href="https://cloud.tembo.io/" target="_blank" rel="noopener noreferrer">Tembo Cloud</a> by using the <a href="https://github.com/tembo-io/tembo/blob/main/tembo-operator/src/stacks/templates/data_warehouse.yaml/" target="_blank" rel="noopener noreferrer">Tembo Data Warehouse</a> stack.</p><h2 id="loading-data-from-several-sources">Loading data from several sources<a href="#loading-data-from-several-sources" aria-label="Direct link to Loading data from several sources" title="Direct link to Loading data from several sources">​</a></h2><p><img loading="lazy" alt="task" src="https://danluu.com/assets/images/task-297f518bef76679cf410069587e6eb4a.png" title="task" width="1060" height="577"/></p><p>To build our data warehouse at Tembo, we first needed to pull our operational data into Postgres from several external sources, namely:</p><ul><li><a href="https://www.postgresql.org/" target="_blank" rel="noopener noreferrer">Postgres</a> - We run a dedicated Postgres cluster (called &#34;control-plane&#34;) to store all metadata for our customer&#39;s Tembo instances. This database contains information like cpu, memory and storage specs of instances, when the instances were created, their names, the organization it belongs to etc. We also run a message queue in Postgres using <a href="https://github.com/tembo-io/pgmq" target="_blank" rel="noopener noreferrer">pgmq</a>, which contains an archive of historical events from the system.</li><li><a href="https://prometheus.io/" target="_blank" rel="noopener noreferrer">Prometheus</a> - Prometheus stores our usage metrics such as cpu and memory usage which are exported from across our infrastructure including Postgres and Kubernetes.</li><li><a href="https://clerk.com/" target="_blank" rel="noopener noreferrer">Clerk.dev</a> - We partner with Clerk to provide authentication and authorization for all our customers, as well as management of our users&#39; organizations. So, Clerk has all our user metadata and organization information.</li></ul><p>The first task was to bring all this data into a single place so that we could join the data together, analyze it, and present it in our dashboards.</p><h2 id="why-all-in-postgres">Why all-in Postgres?<a href="#why-all-in-postgres" aria-label="Direct link to Why all-in Postgres?" title="Direct link to Why all-in Postgres?">​</a></h2><p>Moving data from different sources into a single place is a common task for data engineers. There are lots of tools in the ecosystem to help with this task. Many organizations bring in external tools, and vendors to handle this complexity. Tools like <a href="https://airflow.apache.org/" target="_blank" rel="noopener noreferrer">Airflow</a>, <a href="https://github.com/dbt-labs/dbt-core" target="_blank" rel="noopener noreferrer">dbt</a>, <a href="https://www.fivetran.com/blog/modern-data-warehouse" target="_blank" rel="noopener noreferrer">Fivetran</a>, <a href="https://dagster.io/" target="_blank" rel="noopener noreferrer">Dagster</a> are very popular, and outstanding projects, but using them comes at a huge cost.</p><p>Every time we bring in a new technology into the ecosystem, it becomes a piece of software that needs to be learned, mastered and maintained. This becomes a huge cost in the form of cognitive overhead for the team in addition to the time and resources it takes to set up, manage and maintain it. The system also gets expensive and much harder to debug due to the sheer number of tools involved. Many software systems today consist of far too many tools and technologies for any human to keep in their head. Some engineers at Uber spoke briefly about this <a href="https://youtu.be/zQ5e3B5I-U0?t=81" target="_blank" rel="noopener noreferrer">recently</a>.</p><p>So, rather than bring in new tools, we decided to use Postgres extensions to do the work for us. As a developer, extensions feel natural, like installing and importing a module or package from your favorite repository, which is much lighter and easier to manage than a completely new tool.</p><h2 id="connecting-postgres-to-sources">Connecting Postgres to sources<a href="#connecting-postgres-to-sources" aria-label="Direct link to Connecting Postgres to sources" title="Direct link to Connecting Postgres to sources">​</a></h2><p>Now, how do we get the data from all these sources into Postgres? We already knew we could use <a href="https://www.postgresql.org/docs/current/postgres-fdw.html" target="_blank" rel="noopener noreferrer">postgres_fdw</a> to connect our data warehouse and control-plane Postgres instances. But, we were not certain how we could do the same for our data in Prometheus and Clerk. Prometheus had <a href="https://tembo.io/blog/monitoring-with-prometheus-fdw" target="_blank" rel="noopener noreferrer">several projects already available</a>, but none of them were a good fit for our use-case. Clerk.dev did not have any existing extensions for Postgres, so we decided to <a href="https://tembo.io/blog/clerk-fdw" target="_blank" rel="noopener noreferrer">build our own</a>.</p><h3 id="an-intro-to-foreign-data-wrappers">An intro to Foreign Data Wrappers<a href="#an-intro-to-foreign-data-wrappers" aria-label="Direct link to An intro to Foreign Data Wrappers" title="Direct link to An intro to Foreign Data Wrappers">​</a></h3><p>Foreign data wrappers (FDW) are a class of Postgres extensions which provide you with a simple interface that connects Postgres to another data source. If you&#39;ve worked with Kafka, this is similar to &#39;Connectors&#39;. There are many different foreign data wrappers available, and you can even write your own. Additionally, the <a href="https://github.com/supabase/wrappers" target="_blank" rel="noopener noreferrer">Wrappers</a> framework makes it very easy to develop FDWs in Rust.</p><p>So, we built two new FDWs using the Wrappers framework to connect to those sources; <a href="https://github.com/tembo-io/clerk_fdw" target="_blank" rel="noopener noreferrer">clerk_fdw</a> and <a href="https://github.com/tembo-io/prometheus_fdw" target="_blank" rel="noopener noreferrer">prometheus_fdw</a>.</p><p>Working with FDWs is a fairly consistent experience. We&#39;ll walk through the process for how we set up <code>clerk_fdw</code>, but it is similar for <code>prometheus_fdw</code> and <code>postgres_fdw</code>.</p><h3 id="setting-up-clerk_fdw">Setting up clerk_fdw<a href="#setting-up-clerk_fdw" aria-label="Direct link to Setting up clerk_fdw" title="Direct link to Setting up clerk_fdw">​</a></h3><p>First - run the create extension command. For developers, this might feel a lot like importing a module, and I think that is a good analogy. We also need to create the foreign data wrapper object itself.</p><div><div><pre tabindex="0"><code><span><span>CREATE</span><span> EXTENSION clerk_fdw</span><span>;</span><span></span><br/></span><span><span></span><br/></span><span><span></span><span>CREATE</span><span> </span><span>FOREIGN</span><span> </span><span>DATA</span><span> WRAPPER clerk_wrapper</span><br/></span><span><span>  </span><span>handler</span><span> clerk_fdw_handler</span><br/></span><span><span>  validator clerk_fdw_validator</span><span>;</span><br/></span></code></pre></div></div><p>Next, we create a server object. This is where we configure the connection to the source data system. In the case of Clerk.dev, we need to provide our API key. The server object also needs to know which FDW to use, so we direct it to the clerk_wrapper we created above.</p><div><div><pre tabindex="0"><code><span><span>CREATE</span><span> SERVER clerk_server</span><br/></span><span><span>  </span><span>foreign</span><span> </span><span>data</span><span> wrapper clerk_wrapper</span><br/></span><span><span>  options </span><span>(</span><span></span><br/></span><span><span>    api_key </span><span>&#39;&lt;clerk secret Key&gt;&#39;</span><span>)</span><span>;</span><br/></span></code></pre></div></div><p>Finally, we create a foreign table. This is where we tell Postgres how to map the data from Clerk into a table.</p><div><div><pre tabindex="0"><code><span><span>CREATE</span><span> </span><span>FOREIGN</span><span> </span><span>TABLE</span><span> clerk_users </span><span>(</span><span></span><br/></span><span><span>  user_id </span><span>text</span><span>,</span><span></span><br/></span><span><span>  first_name </span><span>text</span><span>,</span><span></span><br/></span><span><span>  last_name </span><span>text</span><span>,</span><span></span><br/></span><span><span>  email </span><span>text</span><span>,</span><span></span><br/></span><span><span>  gender </span><span>text</span><span>,</span><span></span><br/></span><span><span>  created_at </span><span>bigint</span><span>,</span><span></span><br/></span><span><span>  updated_at </span><span>bigint</span><span>,</span><span></span><br/></span><span><span>  last_sign_in_at </span><span>bigint</span><span>,</span><span></span><br/></span><span><span>  phone_numbers </span><span>bigint</span><span>,</span><span></span><br/></span><span><span>  username </span><span>text</span><span></span><br/></span><span><span>  </span><span>)</span><span></span><br/></span><span><span>  server clerk_server</span><br/></span><span><span>  options </span><span>(</span><span></span><br/></span><span><span>      object </span><span>&#39;users&#39;</span><span></span><br/></span><span><span>  </span><span>)</span><span>;</span><br/></span></code></pre></div></div><p>We did similar processes to map our metrics data from prometheus and instance information from Postgres.</p><p>We can view all the foreign tables we created in our database in <code>psql</code> with the <code>\dE</code> command, or by executing the following statement:</p><div><div><pre tabindex="0"><code><span><span>SELECT</span><span> </span><span>*</span><span></span><br/></span><span><span></span><span>FROM</span><span> information_schema</span><span>.</span><span>foreign_tables</span><br/></span></code></pre></div></div><div><div><pre tabindex="0"><code><span><span> foreign_table_catalog | foreign_table_schema |       foreign_table_name       | foreign_server_catalog | foreign_server_name</span><br/></span><span><span>-----------------------+----------------------+--------------------------------+------------------------+----------------------</span><br/></span><span><span> postgres              | public               | pgmq_saas_queue_archive        | postgres               | control_plane_server</span><br/></span><span><span> postgres              | public               | clerk_users                    | postgres               | clerk_server</span><br/></span><span><span> postgres              | public               | clerk_organizations            | postgres               | clerk_server</span><br/></span><span><span> postgres              | public               | clerk_organization_memberships | postgres               | clerk_server</span><br/></span><span><span> postgres              | public               | metrics                        | postgres               | prometheus_server</span><br/></span><span><span> postgres              | f                    | instance_state                 | postgres               | control_plane_server</span><br/></span><span><span> postgres              | f                    | instances                      | postgres               | control_plane_server</span><br/></span><span><span> postgres              | f                    | organizations                  | postgres               | control_plane_server</span><br/></span></code></pre></div></div><h2 id="scheduling-updates-with-pg_cron">Scheduling Updates with pg_cron<a href="#scheduling-updates-with-pg_cron" aria-label="Direct link to Scheduling Updates with pg_cron" title="Direct link to Scheduling Updates with pg_cron">​</a></h2><p>New users are signing up for Tembo and creating new instances every day, so we need make sure that the data in our data warehouse stays up to date. To do that, we use a popular job scheduling extension <a href="https://github.com/citusdata/pg_cron" target="_blank" rel="noopener noreferrer">pg_cron</a>. If you&#39;re familiar with the unix utility &#34;cron&#34;, then pg_cron is exactly like that but all within Postgres.</p><p>We create a function to refresh our data sources, then we tell pg_cron to call that function on a schedule. Working with pg_cron is very intuitive: we simply provide a name for the job, a schedule, and the command to execute. This is a lot like creating a job to execute some code you wrote using Apache Airflow or Dagster.</p><p>For example, we can simply create a function to delete and re-populate our cluster metadata from the control-plane, then schedule it to run every 5 minutes. Note that this is just an example but can be easily modified to UPSERT only newer or updated clusters.</p><div><div><pre tabindex="0"><code><span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>FUNCTION</span><span> </span><span>public</span><span>.</span><span>refresh_clusters</span><span>(</span><span>)</span><span></span><br/></span><span><span> </span><span>RETURNS</span><span> void</span><br/></span><span><span> </span><span>LANGUAGE</span><span> plpgsql</span><br/></span><span><span></span><span>AS</span><span> $</span><span>function</span><span>$</span><br/></span><span><span></span><span>BEGIN</span><span></span><br/></span><span><span></span><br/></span><span><span>RAISE NOTICE </span><span>&#39;Refreshing clusters from instance_vw to ingested_clusters&#39;</span><span>;</span><span></span><br/></span><span><span></span><br/></span><span><span></span><span>delete</span><span> </span><span>from</span><span> ingested_clusters</span><span>;</span><span></span><br/></span><span><span></span><span>insert</span><span> </span><span>into</span><span> ingested_clusters</span><br/></span><span><span>  </span><span>(</span><span>select</span><span></span><br/></span><span><span>    </span><span>now</span><span>(</span><span>)</span><span>,</span><span></span><br/></span><span><span>    organization_id</span><span>,</span><span></span><br/></span><span><span>    instance_id</span><span>,</span><span></span><br/></span><span><span>    instance_name</span><span>,</span><span></span><br/></span><span><span>    entity_type</span><span>,</span><span></span><br/></span><span><span>    created_at</span><span>,</span><span></span><br/></span><span><span>    desired_spec</span><span>,</span><span></span><br/></span><span><span>    state</span><span>,</span><span></span><br/></span><span><span>    environment</span><br/></span><span><span>  </span><span>from</span><span> instance_vw</span><br/></span><span><span>  </span><span>)</span><span>;</span><span></span><br/></span><span><span></span><span>END</span><span>;</span><span></span><br/></span><span><span>$</span><span>function</span><span>$</span><br/></span></code></pre></div></div><div><div><pre tabindex="0"><code><span><span>SELECT</span><span> cron</span><span>.</span><span>schedule</span><span>(</span><span>&#39;update-clusters&#39;</span><span>,</span><span> </span><span>&#39;5 minutes&#39;</span><span>,</span><span> </span><span>&#39;CALL refresh_clusters()&#39;</span><span>)</span><span>;</span><br/></span></code></pre></div></div><h2 id="partitioning-with-pg_partman-for-performance-and-easy-expiry">Partitioning with pg_partman for performance and easy expiry<a href="#partitioning-with-pg_partman-for-performance-and-easy-expiry" aria-label="Direct link to Partitioning with pg_partman for performance and easy expiry" title="Direct link to Partitioning with pg_partman for performance and easy expiry">​</a></h2><p>Partitioning is a <a href="https://www.postgresql.org/docs/current/ddl-partitioning.html" target="_blank" rel="noopener noreferrer">native feature</a> in Postgres, and it is the logical splitting of one table into smaller physical tables. Some, but not all of the tables in our data warehouse have grown quite large. Largest being our metrics, and this presents two problems that must be addressed; performance and storage. The majority of our dashboard queries aggregate data over time, and most commonly on a daily interval. So, we can partition our tables by day, and only query the partitions we need to answer our questions. This provides a substantial improvement to the performance of those queries which makes our dashboards very snappy.</p><p>Partitioning in Postgres is a batteries-not-included experience, which means you need to handle the creating, updating, and deleting of partitions yourself. That is, unless you use the <a href="https://github.com/pgpartman/pg_partman" target="_blank" rel="noopener noreferrer">pg_partman</a> extension.</p><p>Our stakeholders do not require visualization for the entirety of our metric data, in fact they are typically only concerned with a 30 days at most. So, we can automatically drop partitions that are older than 30 days by setting up a retention policy and reclaiming that storage. Dropping a partition is much faster than deleting rows from a table and also skips having to deal with bloat. As we&#39;ll see in a moment, it is trivial to configure partitioning on Postgres if you use pg_partman (which is my personal favorite Postgres extension).</p><p>Creating a partitioned table just like creating a regular table but you have to specify a partition column. One caveat, is that we must create an index on the column that we want to partition by. In our case, we want a new partition for every day, so we partition by the <code>time</code> column, then create an index there as well. We use partitioning in other places at Tembo as well, and we wrote a bit about those use-cases earlier this year in <a href="https://tembo.io/blog/table-version-history" target="_blank" rel="noopener noreferrer">another blog</a>.</p><div><div><pre tabindex="0"><code><span><span>CREATE</span><span> </span><span>TABLE</span><span> </span><span>public</span><span>.</span><span>metric_values </span><span>(</span><span></span><br/></span><span><span>    id int8 </span><span>NOT</span><span> </span><span>NULL</span><span>,</span><span></span><br/></span><span><span>    </span><span>&#34;time&#34;</span><span> int8 </span><span>NULL</span><span>,</span><span></span><br/></span><span><span>    </span><span>value</span><span> float8 </span><span>NOT</span><span> </span><span>NULL</span><span>,</span><span></span><br/></span><span><span>    </span><span>CONSTRAINT</span><span> metric_values_unique </span><span>UNIQUE</span><span> </span><span>(</span><span>id</span><span>,</span><span> </span><span>&#34;time&#34;</span><span>)</span><span></span><br/></span><span><span></span><span>)</span><span></span><br/></span><span><span></span><span>PARTITION</span><span> </span><span>BY</span><span> RANGE </span><span>(</span><span>&#34;time&#34;</span><span>)</span><span>;</span><span></span><br/></span><span><span></span><span>CREATE</span><span> </span><span>INDEX</span><span> metric_values_time_idx </span><span>ON</span><span> ONLY </span><span>public</span><span>.</span><span>metric_values </span><span>USING</span><span> </span><span>btree</span><span> </span><span>(</span><span>&#34;time&#34;</span><span> </span><span>DESC</span><span>)</span><span>;</span><br/></span></code></pre></div></div><p>Next, we set up pg_partman. We pass our partitioned table into <code>create_parent()</code>.</p><div><div><pre tabindex="0"><code><span><span>SELECT</span><span> create_parent</span><span>(</span><span>&#39;public.metric_values&#39;</span><span>,</span><span> </span><span>&#39;time&#39;</span><span>,</span><span> </span><span>&#39;native&#39;</span><span>,</span><span> </span><span>&#39;daily&#39;</span><span>)</span><span>;</span><br/></span></code></pre></div></div><h2 id="wrapping-up">Wrapping up<a href="#wrapping-up" aria-label="Direct link to Wrapping up" title="Direct link to Wrapping up">​</a></h2><p>Tembo&#39;s data warehouse was made possible by the hard work from <a href="https://github.com/Jayko001" target="_blank" rel="noopener noreferrer">Jay Kothari</a> and <a href="https://github.com/sjmiller609" target="_blank" rel="noopener noreferrer">Steven Miller</a>, who paved the path for building it all on Postgres. They used foreign data wrappers to connect Postgres to external sources, pg_cron as a scheduler to keep out data up-to-date, and pg_partman to improve performance and automate our retention policy. You can use any visualization tool to create dashboards as most tools have Postgres support. We picked <a href="https://preset.io/" target="_blank" rel="noopener noreferrer">Preset</a>. We were able to build a data warehouse that is easy to maintain, and easy to reason about, and quick to onboard new engineers. In the end, our stakeholders get the dashboards they need to make business decisions.</p><p><img loading="lazy" alt="tembo-dw-stack" src="https://danluu.com/assets/images/fin-6142c6f47b071535830157cdc82fe701.png" title="final" width="1010" height="632"/></p><p>The Tembo Datawarehouse Stack is <a href="https://github.com/tembo-io/tembo-stacks/blob/main/tembo-operator/src/stacks/templates/data_warehouse.yaml" target="_blank" rel="noopener noreferrer">open source</a>, and is available to deploy with a single click on <a href="https://cloud.tembo.io" target="_blank" rel="noopener noreferrer">Tembo Cloud</a>.</p><p>Join the conversation about the Tembo Data Warehouse Stack in our <a href="https://join.slack.com/t/tembocommunity/shared_invite/zt-293gc1k0k-3K8z~eKW1SEIfrqEI~5_yw." target="_blank" rel="noopener noreferrer">Slack Community</a></p></div></div>
  </body>
</html>
