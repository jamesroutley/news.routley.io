<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting">Original</a>
    <h1>Gaussian Splatting – A$AP Rocky &#34;Helicopter&#34; music video</h1>
    
    <div id="readability-page-1" class="page"><div data-framer-name="Content"><div data-framer-name="Content"><div data-framer-name="Content" id="1migl9j" data-framer-component-type="RichTextContainer"><p>Believe it or not, A$AP Rocky is a huge fan of radiance fields.</p><p>Yesterday, when A$AP Rocky released the music video for <em>Helicopter</em>, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. What’s easier to miss, unless you know what you’re looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.</p><p>I spoke with <!--$--><a href="https://evercoast.com/" target="_blank" rel="noopener">Evercoast</a><!--/$-->, the team responsible for capturing the performances, as well as Chris Rutledge, the project’s CG Supervisor at <!--$--><a href="https://vimeo.com/grinmachine" target="_blank" rel="noopener">Grin Machine</a><!--/$-->, and Wilfred Driscoll of WildCapture and <!--$--><a href="https://fitsu.ai/" target="_blank" rel="noopener">Fitsū.ai</a><!--/$-->, to understand how <em>Helicopter</em> came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.</p><p>The decision to shoot <em>Helicopter</em> volumetrically wasn’t driven by technology for technology’s sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.</p><p>Chris told me he’d been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply weren’t possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a “someday” workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.</p><div data-width="fill"><article role="presentation"><img decoding="async" src="https://i.ytimg.com/vi_webp/g1-46Nu3HxQ/sddefault.webp"/></article></div><p>The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.</p><p>Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoast’s system. It’s all real performance, preserved spatially.</p><p>This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for <em>Shittin’ Me</em> featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.</p><div data-width="fill"><article role="presentation"><img decoding="async" src="https://i.ytimg.com/vi_webp/Oh8b_P7_AuA/sddefault.webp"/></article></div><p>The primary shoot for <em>Helicopter</em> took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.</p><p>Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.</p><p>Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.</p><p>That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOY’s OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.</p><p>One of the more powerful aspects of the workflow was Evercoast’s ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoast’s web player before downloading massive PLY sequences for Houdini.</p><p>In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. It’s a workflow that more closely resembles simulation than traditional filming.</p><p>Chris also discovered that Octane’s Houdini integration had matured, and that Octane’s early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional “3D video” look was a major reason the final aesthetic lands the way it does.</p><p>The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCapture’s internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdini’s simulation toolset to handle rigid body, soft body, and more physically grounded interactions.</p><p>One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldn’t be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You aren’t limited by the camera’s composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply can’t.</p><p>In other words, radiance field technology isn’t replacing reality. It’s preserving everything.</p></div></div></div></div>
  </body>
</html>
