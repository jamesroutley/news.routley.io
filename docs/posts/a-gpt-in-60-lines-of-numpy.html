<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jaykmody.com/blog/gpt-from-scratch/">Original</a>
    <h1>A GPT in 60 Lines of NumPy</h1>
    
    <div id="readability-page-1" class="page"><div>
  <hr/>
  <p>In this post, we&#39;ll implement a GPT from scratch in just <a href="https://github.com/jaymody/picoGPT/blob/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785/gpt2_pico.py#L3-L58">60 lines of <code>numpy</code></a>. We&#39;ll then load the trained GPT-2 model weights released by OpenAI into our implementation and generate some text.</p>
<p><strong>Note:</strong></p>
<ul>
<li>This post assumes familiarity with Python, NumPy, and some basic experience training neural networks.</li>
<li>My goal with this post is to provide a <strong>simple and hackable yet complete technical introduction to the GPT as an educational tool</strong>. As such, we ONLY implement the forward pass code, using already trained model weights.</li>
<li>Understanding the GPT architecture is an important part of understanding LLMs, but architecture is just a small piece of the puzzle. Distributed training at scale, collecting terabytes of high quality text data, making the model inference fast, evaluating performance, and aligning the models to human interests is the life&#39;s work of the 100s of engineer/researchers required to make LLMs what they are today, not just the architecture. </li>
<li>All the code for this blog post can be found at <a href="https://github.com/jaymody/picoGPT">github.com/jaymody/picoGPT</a>.</li>
<li><a href="https://news.ycombinator.com/item?id=34726115">Hacker news thread</a></li>
</ul>
<p><strong>EDIT (Feb 9th, 2023):</strong> Added a &#34;What&#39;s Next&#34; section and updated the intro with some notes.</p>
<h2 id="table-of-contents" tabindex="-1">Table of Contents</h2>
<hr/>

<h2 id="what-is-a-gpt%3F" tabindex="-1">What is a GPT?</h2>
<hr/>
<p>GPT stands for <strong>Generative Pre-trained Transformer</strong>. It&#39;s a type of neural network architecture based on the <a href="https://arxiv.org/pdf/1706.03762.pdf"><strong>Transformer</strong></a>. <a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/">Jay Alammar&#39;s How GPT3 Works</a> is an excellent introduction to GPTs at a high level, but here&#39;s the tl;dr:</p>
<ul>
<li><strong>Generative</strong>: A GPT <em>generates</em> text.</li>
<li><strong>Pre-trained</strong>: A GPT is <em>trained</em> on lots of text from books, the internet, etc ...</li>
<li><strong>Transformer</strong>: A GPT is a decoder-only <em>transformer</em> neural network.</li>
</ul>
<p>Large Language Models (LLMs) like <a href="https://en.wikipedia.org/wiki/GPT-3">OpenAI&#39;s GPT-3</a>, <a href="https://blog.google/technology/ai/lamda/">Google&#39;s LaMDA</a>, and <a href="https://docs.cohere.ai/docs/command-beta">Cohere Command XLarge</a> are just GPTs under the hood. What makes them special is they happen to be <strong>1)</strong> very big (billions of parameters) and <strong>2)</strong> trained on lots of data (hundreds of gigabytes of text).</p>
<p>Fundamentally, a GPT generates text given a prompt. Even with this very simple API (input = text, output = text), a well trained GPT can do some pretty awesome stuff like <a href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Demo-of-Drafting-an-Email.png?lossy=0&amp;strip=1&amp;webp=1&amp;ezimgfmt=ng:webp/ngcb1">write your emails</a>, <a href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Example-Book-Summarization.png?lossy=0&amp;strip=1&amp;webp=1&amp;ezimgfmt=ng:webp/ngcb1">summarize a book</a>, <a href="https://khrisdigital.com/wp-content/uploads/2022/12/image-1.png">give you instagram caption ideas</a>, <a href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Examples-Explaining-Black-Holes.png?lossy=0&amp;strip=1&amp;webp=1&amp;ezimgfmt=ng:webp/ngcb1">explain black holes to you as if you are 5 years old</a>, <a href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/ChatGPT-Demo-of-Writing-SQL-Queries.png?lossy=0&amp;strip=1&amp;webp=1&amp;ezimgfmt=ng:webp/ngcb1">code in SQL</a>,  and <a href="https://machinelearningknowledge.ai/ezoimgfmt/b2611031.smushcdn.com/2611031/wp-content/uploads/2022/12/Chat-GPT-Example-Writing-a-Will.png?lossy=0&amp;strip=1&amp;webp=1&amp;ezimgfmt=ng:webp/ngcb1">even write your will</a>.</p>
<p>So that&#39;s a high-level overview of GPTs and their capabilities. Before we get into the fun architecture stuff, let&#39;s just quickly recap:</p>
<ul>
<li>Input/Output</li>
<li>Generating Text</li>
<li>Training</li>
</ul>
<h3 id="input-%2F-output" tabindex="-1">Input / Output</h3>
<p>The function signature for a GPT looks roughly like this:</p>
<pre><code>def gpt(inputs: list[int]) -&gt; list[list[float]]:
    # inputs has shape [n_seq]
    # output has shape [n_seq, n_vocab]
    output = # beep boop neural network magic
    return output
</code></pre>
<h4 id="input" tabindex="-1">Input</h4>
<p>The input is some text represented as <strong>sequence</strong> integers that represent string <em>tokens</em>:</p>
<pre><code># integers represent tokens in our text, for example:
# text   = &#34;not all heroes wear capes&#34;:
# tokens = &#34;not&#34;  &#34;all&#34; &#34;heroes&#34; &#34;wear&#34; &#34;capes&#34;
inputs =   [1,     0,    2,      4,     6]
</code></pre>
<p>These integer values come from the index of the tokens in a <em>tokenizer</em>&#39;s vocabulary, for example:</p>
<pre><code># the index of a token in the vocab represents the integer id for that token
# i.e. the integer id for &#34;heroes&#34; would be 2, since vocab[2] = &#34;heroes&#34;
vocab = [&#34;all&#34;, &#34;not&#34;, &#34;heroes&#34;, &#34;the&#34;, &#34;wear&#34;, &#34;.&#34;, &#34;capes&#34;]

# a pretend tokenizer that tokenizes on whitespace
tokenizer = WhitespaceTokenizer(vocab)

# the encode() method converts a str -&gt; list[int]
ids = tokenizer.encode(&#34;not all heroes wear&#34;) # ids = [1, 0, 2, 4]

# we can see what the actual tokens are via our vocab mapping
tokens = [tokenizer.vocab[i] for i in ids] # tokens = [&#34;not&#34;, &#34;all&#34;, &#34;heroes&#34;, &#34;wear&#34;]

# the decode() method converts back a list[int] -&gt; str
text = tokenizer.decode(ids) # text = &#34;not all heroes wear&#34;
</code></pre>
<p>In short:</p>
<ul>
<li>We have a string.</li>
<li>We use a tokenizer to break it down into smaller pieces called tokens.</li>
<li>We use a vocabulary to map those tokens to integers.</li>
</ul>
<p>In practice, we use more advanced methods of tokenization than simply splitting by whitespace, such as <a href="https://huggingface.co/course/chapter6/5?fw=pt">Byte-Pair Encoding</a> or <a href="https://huggingface.co/course/chapter6/6?fw=pt">WordPiece</a>, but the principle is the same:</p>
<ol>
<li>There is a <code>vocab</code> that maps string tokens to integer indices</li>
<li>There is an <code>encode</code> method that converts <code>str -&gt; list[int]</code></li>
<li>There is a <code>decode</code> method that converts <code>list[int] -&gt; str</code></li>
</ol>
<h4 id="output" tabindex="-1">Output</h4>
<p>The output is a <strong>2D array</strong>, where <code>output[i][j]</code> is the model&#39;s <strong>predicted probability</strong> that the token at <code>vocab[j]</code> is the next token <code>inputs[i+1]</code>. For example:</p>
<pre><code>vocab = [&#34;all&#34;, &#34;not&#34;, &#34;heroes&#34;, &#34;the&#34;, &#34;wear&#34;, &#34;.&#34;, &#34;capes&#34;]
inputs = [1, 0, 2, 4] # &#34;not&#34; &#34;all&#34; &#34;heroes&#34; &#34;wear&#34;
output = gpt(inputs)
#              [&#34;all&#34;, &#34;not&#34;, &#34;heroes&#34;, &#34;the&#34;, &#34;wear&#34;, &#34;.&#34;, &#34;capes&#34;]
# output[0] =  [0.75    0.1     0.0       0.15    0.0   0.0    0.0  ]
# given just &#34;not&#34;, the model predicts the word &#34;all&#34; with the highest probability

#              [&#34;all&#34;, &#34;not&#34;, &#34;heroes&#34;, &#34;the&#34;, &#34;wear&#34;, &#34;.&#34;, &#34;capes&#34;]
# output[1] =  [0.0     0.0      0.8     0.1    0.0    0.0   0.1  ]
# given the sequence [&#34;not&#34;, &#34;all&#34;], the model predicts the word &#34;heroes&#34; with the highest probability

#              [&#34;all&#34;, &#34;not&#34;, &#34;heroes&#34;, &#34;the&#34;, &#34;wear&#34;, &#34;.&#34;, &#34;capes&#34;]
# output[-1] = [0.0     0.0     0.0     0.1     0.0    0.05  0.85  ]
# given the whole sequence [&#34;not&#34;, &#34;all&#34;, &#34;heroes&#34;, &#34;wear&#34;], the model predicts the word &#34;capes&#34; with the highest probability
</code></pre>
<p>To get our prediction for the next token for the whole sequence, we can simply take the token with the highest probability:</p>
<pre><code>vocab = [&#34;all&#34;, &#34;not&#34;, &#34;heroes&#34;, &#34;the&#34;, &#34;wear&#34;, &#34;.&#34;, &#34;capes&#34;]
inputs = [1, 0, 2, 4] # &#34;not&#34; &#34;all&#34; &#34;heroes&#34; &#34;wear&#34;
output = gpt(inputs)
next_token_id = np.argmax(output[-1]) # next_token_id = 6
next_token = vocab[next_token_id] # next_token = &#34;capes&#34;
</code></pre>
<p>Taking the token with the highest probability as our final prediction is often referred to as <a href="https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#1-pick-the-top-token-greedy-decoding"><strong>greedy decoding</strong></a> or <strong>greedy sampling</strong>.</p>
<p>As such, a GPT is a <strong>language model</strong>, that is, it performs <strong>language modeling</strong>, the task of predicting the logical next word in a sequence.</p>
<h3 id="generating-text" tabindex="-1">Generating Text</h3>
<h4 id="auto-regressive" tabindex="-1">Auto-Regressive</h4>
<p>We can generate full sentences by iteratively asking our model the predict the next token. At each iteration, we append the predicted token back into the input:</p>
<pre><code>def generate(inputs, n_tokens_to_generate):
    for _ in range(n_tokens_to_generate): # auto-regressive decode loop
        output = gpt(inputs) # model forward pass
        next_id = np.argmax(output[-1]) # greedy sampling
        inputs = np.append(out, [next_id]) # append prediction to input
    return list(inputs[len(inputs) - n_tokens_to_generate :])  # only return generated ids

input_ids = [1, 0] # &#34;not&#34; &#34;all&#34;
output_ids = generate(input_ids, 3) # output_ids = [2, 4, 6]
output_tokens = [vocab[i] for i in output_ids] # &#34;heroes&#34; &#34;wear&#34; &#34;capes&#34;
</code></pre>
<p>This process of predicting a future value (regression), and adding it back into the input (auto) is why you might see a GPT described as <strong>auto-regressive</strong>.</p>
<h4 id="sampling" tabindex="-1">Sampling</h4>
<p>We can introduce some stochasticity (randomness) to our generations by sampling from the probability distribution instead of being greedy:</p>
<pre><code>inputs = [1, 0, 2, 4] # &#34;not&#34; &#34;all&#34; &#34;heroes&#34; &#34;wear&#34;
output = gpt(inputs)
np.random.categorical(output[-1]) # capes
np.random.categorical(output[-1]) # hats
np.random.categorical(output[-1]) # capes
np.random.categorical(output[-1]) # capes
np.random.categorical(output[-1]) # pants
</code></pre>
<p>Not only does it allow us to generate different sentences for the same input, but it also increases the quality of the outputs compared to greedy decoding.</p>
<p>It&#39;s also common to use techniques like <a href="https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#2-pick-from-amongst-the-top-tokens-top-k"><strong>top-k</strong></a>, <a href="https://docs.cohere.ai/docs/controlling-generation-with-top-k-top-p#3-pick-from-amongst-the-top-tokens-whose-probabilities-add-up-to-15-top-p"><strong>top-p</strong></a>, and <a href="https://docs.cohere.ai/docs/temperature"><strong>temperature</strong></a> to modify the probability distribution before sampling from it. This helps improve the quality of generations and also introduces hyper-parameters that we can play around with to get different generation behaviors (for example, increasing temperature makes our model take more risks and thus be more &#34;creative&#34;).</p>
<h3 id="training" tabindex="-1">Training</h3>
<p>We train a GPT like any other neural network, using <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> with respect to some loss function. In the case of a GPT, we take the <a href="https://www.youtube.com/watch?v=ErfnhcEV1O8"><strong>cross entropy loss</strong></a> over the language modeling task:</p>
<pre><code>def lm_loss(inputs: list[int]) -&gt; float:
    # the labels y are just the input shifted 1 to the left
    #
    # inputs = [not,     all,   heros,   wear,   capes]
    #      x = [not,     all,   heroes,  wear]
    #      y = [all,  heroes,     wear,  capes]
    # 
    # of course, we don&#39;t have a label for inputs[-1], so we exclude it from x
    #
    # as such, for N inputs, we have N - 1 langauge modeling example pairs
    x, y = inputs[:-1], inputs[1:]
    
    # forward pass
    # all the predicted next token probability distributions at each position
    output = gpt(x)
    
    # cross entropy loss
    # we take the average over all N-1 examples
    loss = np.mean(-np.log(output[y]))

    return loss

def loss_fn(texts: list[list[str]]) -&gt; float:
    # take the mean of the language modeling losses over all
    # text documents in our dataset
    loss = 0
    for text in texts:
        inputs = tokenizer.encode(text)
        loss += lm_loss(inputs)
    return loss / len(texts)
</code></pre>
<p>Notice, we don&#39;t need explicitly labelled data. Instead, we are able to produce the input/label pairs from just the raw text itself. This is referred to as <strong><a href="https://en.wikipedia.org/wiki/Self-supervised_learning">self-supervised learning</a></strong>.</p>
<p>This means we can scale up train data really easily, we just throw as much text as we can get our hands on at at a GPT. For example, GPT-3 was trained on <strong>300 billion tokens</strong> of text from the internet and books:</p>
<figure><img src="https://miro.medium.com/max/1400/1*Sc3Gi73hepgrOLnx8bXFBA.png" alt="" data-src="https://miro.medium.com/max/1400/1*Sc3Gi73hepgrOLnx8bXFBA.png"/><figcaption>gpt-data</figcaption></figure>
<p>Of course, you need a sufficiently large model to be able to learn from all this data, which is why GPT-3 is <strong>175 billion parameters</strong> and probably cost between <a href="https://twitter.com/eturner303/status/1266264358771757057">$1m-10m in compute cost to train</a>.</p>
<h2 id="setup" tabindex="-1">Setup</h2>
<hr/>
<p>Clone the repository for this tutorial:</p>
<pre><code>git clone https://github.com/jaymody/picoGPT
cd picoGPT
</code></pre>
<p>Then install dependencies:</p>
<pre><code>pip install -r requirements.txt
</code></pre>
<p>Note, if you are using an M1 Macbook, you&#39;ll need to change <code>tensorflow</code> to <code>tensorflow-macos</code> in <code>requirements.txt</code> before running <code>pip install</code>. This code was tested on <code>Python 3.9.10</code>.</p>
<p>A quick breakdown of each of the files:</p>
<ul>
<li><strong><code>encoder.py</code></strong> contains the code for OpenAI&#39;s BPE Tokenizer, taken straight from their <a href="https://github.com/openai/gpt-2/blob/master/src/encoder.py">gpt-2 repo</a>.</li>
<li><strong><code>utils.py</code></strong> contains the code to download and load the GPT-2 model weights, tokenizer, and hyper-parameters.</li>
<li><strong><code>gpt2.py</code></strong> contains the actual GPT model and generation code which we can run as a python script.</li>
<li><strong><code>gpt2_pico.py</code></strong> is the same as <code>gpt2.py</code>, but in even fewer lines of code (removed comments, extra whitespace, and combined certain operations into a single line). Why? Because why not.</li>
</ul>
<p>We&#39;ll be reimplementing <code>gpt2.py</code> from scratch, so let&#39;s delete it and recreate it as an empty file:</p>
<pre><code>rm gpt2.py
touch gpt2.py
</code></pre>
<p>As a starting point, paste the following code into <code>gpt2.py</code>:</p>
<pre><code>import numpy as np


def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):
    pass # TODO: implement this


def generate(inputs, params, n_head, n_tokens_to_generate):
    from tqdm import tqdm

    for _ in tqdm(range(n_tokens_to_generate), &#34;generating&#34;):  # auto-regressive decode loop
        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass
        next_id = np.argmax(logits[-1])  # greedy sampling
        inputs = np.append(inputs, [next_id])  # append prediction to input

    return list(inputs[len(inputs) - n_tokens_to_generate :])  # only return generated ids


def main(prompt: str, n_tokens_to_generate: int = 40, model_size: str = &#34;124M&#34;, models_dir: str = &#34;models&#34;):
    from utils import load_encoder_hparams_and_params

    # load encoder, hparams, and params from the released open-ai gpt-2 files
    encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)

    # encode the input string using the BPE tokenizer
    input_ids = encoder.encode(prompt)

    # make sure we are not surpassing the max sequence length of our model
    assert len(input_ids) + n_tokens_to_generate &lt; hparams[&#34;n_ctx&#34;]

    # generate output ids
    output_ids = generate(input_ids, params, hparams[&#34;n_head&#34;], n_tokens_to_generate)

    # decode the ids back into a string
    output_text = encoder.decode(output_ids)

    return output_text


if __name__ == &#34;__main__&#34;:
    import fire

    fire.Fire(main)
</code></pre>
<p>Breaking down each of the 4 sections:</p>
<ol>
<li>The <code>gpt2</code> function is the actual GPT code we&#39;ll be implementing. You&#39;ll notice that the function signature includes some extra stuff in addition to <code>inputs</code>.
<ul>
<li><code>wte</code>, <code>wpe</code>, <code>blocks</code>, and <code>ln_f</code> are parameters for our model.</li>
<li><code>n_head</code> is a hyper-parameter that is needed during the forward pass.</li>
</ul>
</li>
<li>The <code>generate</code> function is the auto-regressive decoding algorithm we saw earlier. We use greedy sampling for simplicity and so we can get deterministic results. <a href="https://www.google.com/search?q=tqdm"><code>tqdm</code></a> is a progress bar, so we can visualize the progress of our model as it generates tokens one at a time.</li>
<li>The <code>main</code> function handles:
<ol>
<li>Loading the tokenizer (<code>encoder</code>), model weights (<code>params</code>), and hyper-parameters (<code>hparams</code>)</li>
<li>Encoding the input prompt into token ids using the tokenizer</li>
<li>Calling the generate function</li>
<li>Decoding the output ids into a string</li>
</ol>
</li>
<li><a href="https://github.com/google/python-fire"><code>fire.Fire(main)</code></a> just turns the our file into a CLI application so we can eventually run our code with: <code>python gpt2.py &#34;some prompt here&#34;</code></li>
</ol>
<p>Let&#39;s take a closer look at <code>encoder</code>, <code>hparams</code>, and <code>params</code>, in a notebook, or an interactive python session, run:</p>
<pre><code>from utils import load_encoder_hparams_and_params
encoder, hparams, params = load_encoder_hparams_and_params(&#34;124M&#34;, &#34;models&#34;)
</code></pre>
<p>This will <a href="https://github.com/jaymody/picoGPT/blob/2014c33ee6c4c063844eb2b78ec22f899f7afd1c/utils.py#L13-L40">download the necessary model and tokenizer files</a> to <code>models/124M</code> and <a href="https://github.com/jaymody/picoGPT/blob/2014c33ee6c4c063844eb2b78ec22f899f7afd1c/utils.py#L68-L82">load <code>encoder</code>, <code>hparams</code>, and <code>params</code></a>.</p>
<h3 id="encoder" tabindex="-1">Encoder</h3>
<p><code>encoder</code> is the BPE tokenizer used by GPT-2. Here&#39;s an example of it encoding and decoding some text:</p>
<pre><code>&gt;&gt;&gt; ids = encoder.encode(&#34;Not all heroes wear capes.&#34;)
&gt;&gt;&gt; ids
[3673, 477, 10281, 5806, 1451, 274, 13]

&gt;&gt;&gt; encoder.decode(ids)
&#34;Not all heroes wear capes.&#34;
</code></pre>
<p>Using the vocabulary of the tokenizer, we take also take a peek at what the actual tokens look like:</p>
<pre><code>&gt;&gt;&gt; [encoder.decoder[i] for i in ids]
[&#39;Not&#39;, &#39;Ġall&#39;, &#39;Ġheroes&#39;, &#39;Ġwear&#39;, &#39;Ġcap&#39;, &#39;es&#39;, &#39;.&#39;]
</code></pre>
<p>Notice, sometimes our tokens are words (e.g. <code>Not</code>), sometimes they are words but with a space in front of them (e.g. <code>Ġall</code>, the <code>Ġ</code> represents a space), sometimes there are part of a word (e.g. capes is split into <code>Ġcap</code> and <code>es</code>), and sometimes they are punctuation (e.g. <code>.</code>).</p>
<p>One nice thing about BPE is that it can encode any arbitrary string. If it encounters something that is not present in the vocabulary, it just breaks it down into substrings it does understand:</p>
<pre><code>&gt;&gt;&gt; [encoder.decoder[i] for i in encoder.encode(&#34;zjqfl&#34;)]
[&#39;z&#39;, &#39;j&#39;, &#39;q&#39;, &#39;fl&#39;]
</code></pre>
<p>We can also check the size of the vocabulary:</p>
<pre><code>&gt;&gt;&gt; len(encoder.decoder)
50257
</code></pre>
<p>The vocabulary, as well as the byte-pair merges, are obtained by <em>training</em> the tokenizer. When we load the tokenizer, we&#39;re loading the already trained vocab and byte-pair merges from some files, which were downloaded alongside the model files when we ran <code>load_encoder_hparams_and_params</code>. See <code>models/124M/encoder.json</code> (the vocabulary) and <code>models/124M/vocab.bpe</code> (byte-pair merges).</p>
<h3 id="hyperparameters" tabindex="-1">Hyperparameters</h3>
<p><code>hparams</code> is a dictionary that contains the hyper-parameters of our model:</p>
<pre><code>&gt;&gt;&gt; hparams
{
  &#34;n_vocab&#34;: 50257, # number of tokens in our vocabulary
  &#34;n_ctx&#34;: 1024, # maximum possible sequence length of the input
  &#34;n_embd&#34;: 768, # embedding dimension (determines the &#34;width&#34; of the network)
  &#34;n_head&#34;: 12, # number of attention heads (n_embd must be divisible by n_head)
  &#34;n_layer&#34;: 12 # number of layers (determines the &#34;depth&#34; of the network)
}
</code></pre>
<p>We&#39;ll use these symbols in our code&#39;s comments to show the underlying shape of things. We&#39;ll also use  <code>n_seq</code> to denote the length of our input sequence (i.e. <code>n_seq = len(inputs)</code>).</p>
<h3 id="parameters" tabindex="-1">Parameters</h3>
<p><code>params</code> is a nested json dictionary that hold the trained weights of our model. The leaf nodes of the structure are NumPy arrays. If we print <code>params</code>, but replace the arrays with their shapes, we get:</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; def shape_tree(d):
&gt;&gt;&gt;     if isinstance(d, np.ndarray):
&gt;&gt;&gt;         return list(d.shape)
&gt;&gt;&gt;     elif isinstance(d, list):
&gt;&gt;&gt;         return [shape_tree(v) for v in d]
&gt;&gt;&gt;     elif isinstance(d, dict):
&gt;&gt;&gt;         return {k: shape_tree(v) for k, v in d.items()}
&gt;&gt;&gt;     else:
&gt;&gt;&gt;         ValueError(&#34;uh oh&#34;)
&gt;&gt;&gt; 
&gt;&gt;&gt; print(shape_tree(params))
{
    &#34;wpe&#34;: [1024, 768],
    &#34;wte&#34;: [50257, 768],    
    &#34;ln_f&#34;: {&#34;b&#34;: [768], &#34;g&#34;: [768]},
    &#34;blocks&#34;: [
        {
            &#34;attn&#34;: {
                &#34;c_attn&#34;: {&#34;b&#34;: [2304], &#34;w&#34;: [768, 2304]},
                &#34;c_proj&#34;: {&#34;b&#34;: [768], &#34;w&#34;: [768, 768]},
            },
            &#34;ln_1&#34;: {&#34;b&#34;: [768], &#34;g&#34;: [768]},
            &#34;ln_2&#34;: {&#34;b&#34;: [768], &#34;g&#34;: [768]},
            &#34;mlp&#34;: {
                &#34;c_fc&#34;: {&#34;b&#34;: [3072], &#34;w&#34;: [768, 3072]},
                &#34;c_proj&#34;: {&#34;b&#34;: [768], &#34;w&#34;: [3072, 768]},
            },
        },
        ... # repeat for n_layers
    ]
}
</code></pre>
<p>These weights and the corresponding nested structure are taken straight from the variables in the tensorflow checkpoint:</p>
<pre><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf_ckpt_path = tf.train.latest_checkpoint(&#34;models/124M&#34;)
&gt;&gt;&gt; for name, _ in tf.train.list_variables(tf_ckpt_path):
&gt;&gt;&gt;     arr = tf.train.load_variable(tf_ckpt_path, name).squeeze()
&gt;&gt;&gt;     print(f&#34;{name}: {arr.shape}&#34;)
model/h0/attn/c_attn/b: (2304,)
model/h0/attn/c_attn/w: (768, 2304)
model/h0/attn/c_proj/b: (768,)
model/h0/attn/c_proj/w: (768, 768)
model/h0/ln_1/b: (768,)
model/h0/ln_1/g: (768,)
model/h0/ln_2/b: (768,)
model/h0/ln_2/g: (768,)
model/h0/mlp/c_fc/b: (3072,)
model/h0/mlp/c_fc/w: (768, 3072)
model/h0/mlp/c_proj/b: (768,)
model/h0/mlp/c_proj/w: (3072, 768)
model/h1/attn/c_attn/b: (2304,)
model/h1/attn/c_attn/w: (768, 2304)
...
model/h9/mlp/c_proj/b: (768,)
model/h9/mlp/c_proj/w: (3072, 768)
model/ln_f/b: (768,)
model/ln_f/g: (768,)
model/wpe: (1024, 768)
model/wte: (50257, 768)
</code></pre>
<p>There&#39;s just some <a href="https://github.com/jaymody/picoGPT/blob/29e78cc52b58ed2c1c483ffea2eb46ff6bdec785/utils.py#L43-L65">additional logic</a> needed to convert the above into the nested dictionary structure <code>params</code>.</p>
<p>For reference, here&#39;s what the <code>params</code> dictionary of shapes looks like, but with the numbers replaced with the hyper-parameters they represent:</p>
<pre><code>{
    &#34;wpe&#34;: [n_ctx, n_embd],
    &#34;wte&#34;: [n_vocab, n_embd],    
    &#34;ln_f&#34;: {&#34;b&#34;: [n_embd], &#34;g&#34;: [n_embd]},
    &#34;blocks&#34;: [
        {
            &#34;attn&#34;: {
                &#34;c_attn&#34;: {&#34;b&#34;: [3*n_embd], &#34;w&#34;: [n_embd, 3*n_embd]},
                &#34;c_proj&#34;: {&#34;b&#34;: [n_embd], &#34;w&#34;: [n_embd, n_embd]},
            },
            &#34;ln_1&#34;: {&#34;b&#34;: [n_embd], &#34;g&#34;: [n_embd]},
            &#34;ln_2&#34;: {&#34;b&#34;: [n_embd], &#34;g&#34;: [n_embd]},
            &#34;mlp&#34;: {
                &#34;c_fc&#34;: {&#34;b&#34;: [4*n_embd], &#34;w&#34;: [n_embd, 4*n_embd]},
                &#34;c_proj&#34;: {&#34;b&#34;: [n_embd], &#34;w&#34;: [4*n_embd, n_embd]},
            },
        },
        ... # repeat for n_layers
    ]
}
</code></pre>
<p>You&#39;ll probably want to come back to reference the above dictionary a lot as we&#39;re coding to check the shape of the weights. We&#39;ll be using variable names for our weights that match the keys of this dictionary.</p>
<h2 id="basic-layers" tabindex="-1">Basic Layers</h2>
<hr/>
<p>Last thing before we get into the actual GPT architecture itself, let&#39;s implement some of the more basic neural network layers that are non-specific to the GPT.</p>
<h3 id="gelu" tabindex="-1">GeLU</h3>
<p><a href="https://arxiv.org/pdf/1606.08415.pdf">Gaussian Error Linear Units</a> is an alternative to the ReLU activation function, and is approximated by the following function:</p>
<figure><img src="https://miro.medium.com/max/491/1*kwHcbpKUNLda8tvCiwudqQ.png" alt="" data-src="https://miro.medium.com/max/491/1*kwHcbpKUNLda8tvCiwudqQ.png"/><figcaption>gelu</figcaption></figure>
<pre><code>def gelu(x):
    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
</code></pre>
<p>Like ReLU, the GeLU function operates element-wise on the input.</p>
<pre><code>&gt;&gt;&gt; gelu(np.array([[1, 2], [-4, 0]]))
array([[ 1.9546,      100.0 ],
       [-2.2918e-07,  0.0   ]])
</code></pre>
<p>The <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a> paper popularized the use of GeLU in transformer based models, and it kind of stuck around since.</p>
<h3 id="softmax" tabindex="-1">Softmax</h3>
<p>Good ole <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>:</p>
<p>\[
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
\]</p>
<pre><code>def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
</code></pre>
<p>We use the <a href="https://jaykmody.com/blog/stable-softmax/"><code>max(x)</code> trick for numerical stability</a>.</p>
<p>We apply <code>softmax</code> over the last axis of the input.</p>
<pre><code>&gt;&gt;&gt; x = softmax(np.array([[2, 100], [-5, 0]]))
&gt;&gt;&gt; x
array([[2.74878501e-43, 1.00000000e+00],
       [6.69285092e-03, 9.93307149e-01]])
&gt;&gt;&gt; x.sum(axis=-1)
array([1., 1.])
</code></pre>
<h3 id="layer-normalization" tabindex="-1">Layer Normalization</h3>
<p><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer normalization</a> standardizes values to have a mean of 0 and a variance of 1:</p>
<p>\[
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2}} + \beta
\]</p>
<pre><code>def layer_norm(x, g, b, eps: float = 1e-5):
    mean = np.mean(x, axis=-1, keepdims=True)
    variance = np.var(x, axis=-1, keepdims=True)
    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis
    return g * x + b  # scale and offset with gamma/beta params
</code></pre>
<p>This ensures that the inputs for each layer are always within a consistent range, which is suppose to speed up and stabilize the training process. Like <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization</a>, the normalized output is then scaled and offset with two learnable vectors gamma and beta. The small epsilon term in the denominator is used to avoid a division by zero error.</p>
<p>Layer norm is used instead of batch norm in the transformer for <a href="https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm">various reasons</a>. The differences between various normalization techniques is outlined <a href="https://tungmphung.com/deep-learning-normalization-methods/">in this excellent blog post</a>.</p>
<p>Layer normalization is applied over the last axis of the input.</p>
<pre><code>&gt;&gt;&gt; x = np.array([[2, 2, 3], [-5, 0, 1]])
&gt;&gt;&gt; x = layer_norm(x, g=np.ones(x.shape[-1]), b=np.zeros(x.shape[-1]))
&gt;&gt;&gt; x
array([[-0.70709087, -0.70709087,  1.41418174],
       [-1.39700038,  0.50800014,  0.88900024]])
&gt;&gt;&gt; x.var(axis=-1)
array([0.999955  , 0.99999855]) # very close to 1
&gt;&gt;&gt; x.mean(axis=-1)
array([-2.96059473e-16, -3.70074342e-17]) # very close to 0
</code></pre>
<h3 id="linear" tabindex="-1">Linear</h3>
<p>Your standard matrix multiplication + bias:</p>
<pre><code>def linear(x, w, b):  # [m, in], [in, out], [out] -&gt; [m, out]
    return x @ w + b
</code></pre>
<p>Linear layers are often referred to as projections (since they are projecting from one vector space to another vector space).</p>
<pre><code>&gt;&gt;&gt; x = np.random.normal(size=(64, 784)) # input dim = 784, batch/sequence dim = 64
&gt;&gt;&gt; w = np.random.normal(size=(784, 10)) # output dim = 10
&gt;&gt;&gt; b = np.random.normal(size=(10,))
&gt;&gt;&gt; x.shape # shape before linear projection
(64, 784)
&gt;&gt;&gt; linear(x, w, b).shape # shape after linear projection
(64, 10)
</code></pre>
<h2 id="gpt-architecture" tabindex="-1">GPT Architecture</h2>
<hr/>
<p>The GPT architecture follows that of the <a href="https://arxiv.org/pdf/1706.03762.pdf">transformer</a>:</p>
<figure><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAAZCAIAAACza+nDAAAACXBIWXMAAAsTAAALEwEAmpwYAAADaUlEQVQ4jYXUa3OiSBQGYP//f0jV1O6HrUzNTGYyuXjDGypCQKABBbxE5eoASgONgoBbaio7SdVk38/nKZrTfU7p+IcURZFlGULI930I4X6/z/O8OOd4PJY+YPv93vM8XddN04QBPBwOH7HinCzLttCfPk8ZMOQlXrd0FKP/Z2maOp4ryeP+E0kO6efVMgjDi/mI7fd7a22zgG918Q7Rkyeqt/Eu5/yIIYRWqwVJ9uv1x0ajOmTplbaKoijLsqIoSq917+JDqMymWLfzo/Z4h1VwkpBVxXXdJEn+zPJ8s9lIk0ml2/2C1W8aWGNAgNHItu04jk/sXJOnabrb7aIogtDfbjeu62q6LigqNiBvWu2bZhMjBu9ZmqYQQl1fKYoIAMWyBA9oUeRZju7gtcfK14fKtzZeF0Xwhu12u/V6LY5HOEU8tLFbrFJuYV2ixwyaVPsGf7zqlD+RnZ+SwFjWbwwhZBgGw4MKjn/F6p9r1ft2q0MQDNmh8QcK+0y3v4OnljIWTNMKz7dXyvMcIWSapiirPVZoMABjQJ8TGSBwwwFN1IjmN6L1gyGbYwmYphVF0YllWRaGoa4bQARdAq+3q/V2dfBEsDzLs2QfL2Pl60b1y6CHjSRgWRZC6A0TAU12H3vYdbf2z3BQE7jBiO/T+Pd+5apf+fSE38ni8L+W5HkehqFhGBL/ROF3vcpfrfsrqnPL010ZDPje3RD7m2tdi1R9Kp86+fK1S0tM05QlniNbQ/yebt+KNK5I3EwRRkOc7z1wvbJAd6fKb/eWJAmEUNM0WR7zLMMxlMDRs4ls6ppp6FNVBiw9pEmB52bTqW3bURQdDocSQsjzvOVyqU4moigJojSZTG17HQRhEISWZavqhOcFSRrNZnPDMLbb7W63OzHf99frtbZczmRZAWAhy65loSCIw3Ct68/j8Zjj5qpqaJrjOBDCOI5Ph7y8Rse2F4oyoiiZYcz53HecwPNWqirTtEhSC0V1bDsIwziOkyR5mbe8KFAQ/NK0uSDMALAWC991A88zZrO5IMxF0TGMOAxft9ALOz3oJIHbjaVp5mq1dd19HMdR9MuyVvO5vlj4m016nrT3S6E4Hg+HQ3xOkqZFUSRJcvltx3EQQlmWvRa/WQpZlqXnXCY/jneue1p4lmVBCNM0fWX/AiCMvhL+3rIYAAAAAElFTkSuQmCC" alt="" data-src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="1320" height="1860"/><figcaption>Figure 1 from Attention is All You Need</figcaption></figure>
<p>But uses only the decoder stack (the right part of the diagram):</p>
<figure><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAZCAYAAADnstS2AAAACXBIWXMAAAsTAAALEwEAmpwYAAACX0lEQVQ4jW2Ti5KaShRF5/9/Krdi7ugw8hYREHyBoCigjrxUVgqiGZNMdXV1V5/Vu87pPvsFoGmaduF2u1GWFafjB1l64Hg4URQl1+u1i788w5f6QpYcWM0CpqbHwl2SxClVWf0Ll2XFJtpijEyENxFV0QmCkDwvvoJLNmGIrqq8vb4iiyKB75Ofz//CVVWy20ZYYx1NfGOsyWzDgCJ/wE3zCZcl8TpiLKlIvT66IBItfYq/lZum6SrfhjETbYI6UDAkg8iPyD/yLv7SarabsqrZpUds1+ddHPHjf5GBoGE5S+L9gaKsP+HTuSDYZuiOz0C16L2rvEoGmr1itcnITjkvbbotnB5zZmGGaIX0VJdvQ4P/xAmC6TMNUuLk9KmcnQoW0YGRHSIpU4aCzvvQRB2v8PyUOP34VM5aOMwwzABNstEGKopgoGpzvOWObafcNNyuV5LDmXmQohsLVNFE6UvIgo6iubjL3T2NFr5dSVt4nSGbPn3ZojeQ+fE+YmgscP3kkcYTHB1R3Ji+seK74tBTPYZ2hLc+3OHnAsMDqh0gqBb9ocJAMpDMJV6QfAVnjGwfRTORRQlJ1lDHMzx//yjwDh9z5uukC4ryCFEQGA4VJN1hutze4d8/WBLtjrjLDZa7YmK7TJw5zjxkvc06sd9wVdVkxzN+mOB4Iaa1wnICFv6OJPv41RvPXdfaZx+nzKYLJiMH15oRb/aURXnvuqd+fnhw6a2wx1Nmzpx9nHztwc7dRcl+lxCFG+Ltjvyc/+XudtwvtGt9qanqirqu/zjvYO7P9zhslS6XSzef4Z+W/wlT/86NiwAAAABJRU5ErkJggg==" alt="" data-src="https://i.imgur.com/c4Z6PG8.png" width="348" height="772"/><figcaption>GPT Architecture</figcaption></figure>
<p>You&#39;ll notice though, the middle &#34;cross-attention&#34; layer is removed since we got rid of the encoder.</p>
<p>So at a high level, a GPT has 3 parts to it:</p>
<ul>
<li>Text + positional embeddings</li>
<li>A transformer decoder stack</li>
<li>A next token prediction head</li>
</ul>
<p>In code, it looks like this:</p>
<pre><code>def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -&gt; [n_seq, n_vocab]
    # token + positional embeddings
    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -&gt; [n_seq, n_embd]

    # forward pass through n_layer transformer blocks
    for block in blocks:
        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    # projection to vocab
    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    return x @ wte.T  # [n_seq, n_embd] -&gt; [n_seq, n_vocab]
</code></pre>
<p>Let&#39;s break down each of these three sections into more detail.</p>
<h3 id="embeddings" tabindex="-1">Embeddings</h3>
<h4 id="token-embeddings" tabindex="-1">Token Embeddings</h4>
<p>Token ids by themselves are not very good representations for a neural network. For one, the relative magnitudes of the token ids falsely communicate information (for example, if <code>Apple = 5</code> and <code>Table = 10</code> in our vocab, then we are implying that <code>2 * Table = Apple</code>). Secondly, a single number is not a lot of <em>dimensionality</em> for a neural network to work with.</p>
<p>To address these limitations, we&#39;ll take advantage of <a href="https://jaykmody.com/blog/attention-intuition/#word-vectors-and-similarity">word vectors</a>, specifically via a learned embedding matrix:</p>
<pre><code>wte[inputs] # [n_embd] -&gt; [n_seq, n_embd]
</code></pre>
<p>Recall,  <code>wte</code> is a <code>[n_vocab, n_embd]</code> matrix. It acts as a lookup table, where the \(i\)th row in the matrix corresponds to the learned token vector for the \(i\)th token in our vocabulary. <code>wte[inputs]</code> uses <a href="https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing">integer array indexing</a> to retrieve the word vectors for each word in our input.</p>
<p>Like any other parameter in our network, <code>wte</code> is learned. That is, it is randomly initialized at the start of training and then updated via gradient descent.</p>
<h4 id="positional-embeddings" tabindex="-1">Positional Embeddings</h4>
<p>One quirk of the transformer architecture is that it doesn&#39;t take into account position. If we randomly shuffled our input and then accordingly unshuffled the output, the output would be the same as if we never shuffled the input in the first place (meaning ordering of inputs don&#39;t matter).</p>
<p>Of course, the order of words in a sentence is crucial for language, so we need some way to encode positional information for our inputs. For this, we can just use another learned embedding matrix:</p>
<pre><code>wpe[range(len(inputs))] # [n_seq] -&gt; [n_seq, n_embd]
</code></pre>
<p>Recall, <code>wpe</code> is a <code>[n_ctx, n_embd]</code> matrix. The \(i\)th row of the matrix contains a vector that encodes information about the \(i\)th position in the input. Similar to <code>wte</code>, this matrix is learned during gradient descent.</p>
<p>Notice, this restricts our model to a maximum sequence length of <code>n_ctx</code>. That is, <code>len(inputs) &lt;= n_ctx</code>.</p>
<h4 id="combined" tabindex="-1">Combined</h4>
<p>We add our token embeddings and positional embeddings to get a combined embedding for each input that encodes both the token and positional information.</p>
<pre><code># token + positional embeddings
x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -&gt; [n_seq, n_embd]
    
# x[i] represents the word embedding for the ith word + the positional
# embedding for the ith position
</code></pre>
<h3 id="decoder-stack" tabindex="-1">Decoder Stack</h3>
<p>This is where all the magic happens and the &#34;deep&#34; in deep learning comes in. We pass our embedding through a stack of <code>n_layer</code> transformer decoder blocks.</p>
<pre><code># forward pass through n_layer transformer blocks
for block in blocks:
    x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
</code></pre>
<p>Stacking more layers is what allows us to control how <em>deep</em> our networks is. GPT-3 for example, has a <a href="https://preview.redd.it/n9fgba8b0qr01.png?auto=webp&amp;s=e86d2d3447c777d3222016e81a0adfaec1a95592">whopping 96 layers</a>.</p>
<h3 id="projection-to-vocab" tabindex="-1">Projection to Vocab</h3>
<p>In our final step, we project the output of the final transformer block to a probability distribution over our vocab:</p>
<pre><code># projection to vocab
x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
return x @ wte.T  # [n_seq, n_embd] -&gt; [n_seq, n_vocab]
</code></pre>
<p>Couple things to note here:</p>
<ol>
<li>We first pass <code>x</code> through a final layer normalization layer before doing the projection to vocab. This is specific to the GPT-2 architecture (this is not present in the original GPT and Transformer papers).</li>
<li>We are reusing the embedding matrix <code>wte</code> to do our projection. Other implementations may choose to instead use a separate learned weight matrix for this projection, however sharing the embedding matrix has a couple of advantages.
<ul>
<li>You save some parameters (although at GPT-3 scale, this is negligible).</li>
<li>The matrix is both responsible for mapping to words and from words, so in theory it <em>may</em> learn a richer representation compared to having two separate matrixes.</li>
</ul>
</li>
<li>We <strong>don&#39;t</strong> apply <code>softmax</code> at the end, so our outputs will be <a href="https://developers.google.com/machine-learning/glossary/#logits">logits</a> instead of probabilities between 0 and 1. This is done for several reasons:
<ul>
<li><code>softmax</code> is <a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonic</a>, so for greedy sampling <code>np.argmax(logits)</code> is equivalent to <code>np.argmax(softmax(logits))</code> making <code>softmax</code> redundant</li>
<li><code>softmax</code> is irreversible, meaning we can always go from <code>logits</code> to <code>probabilities</code> by applying <code>softmax</code>, but we can&#39;t go back to <code>logits</code> from <code>probabilities</code>, so for maximum flexibility, we output the <code>logits</code></li>
<li>Numerically stability (for example, to compute cross entropy loss, taking <a href="https://jaykmody.com/blog/stable-softmax/#cross-entropy-and-log-softmax"><code>log(softmax(logits))</code> is numerically instable compared to <code>log_softmax(logits)</code></a></li>
</ul>
</li>
</ol>
<p>The projection to vocab step is also sometimes called the language modeling head.</p>
<p>So that&#39;s the GPT architecture at a high level, let&#39;s actually dig a bit deeper into what the decoder blocks are doing.</p>
<h3 id="decoder-block" tabindex="-1">Decoder Block</h3>
<p>The transformer decoder block consists of two sublayers:</p>
<ol>
<li>Multi-head casual self attention</li>
<li>Position-wise feed forward neural network</li>
</ol>
<pre><code>def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # multi-head casual self attention
    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    # position-wise feed forward network
    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    return x
</code></pre>
<p>Each sublayer utilizes layer normalization on their inputs as well as a residual connection (i.e. add the input of the sublayer to the output of the sublayer).</p>
<p>Some things to note:</p>
<ol>
<li><strong>Multi-head casual self attention</strong> is what facilitates the communication between the inputs. Nowhere else in the network does the model allow inputs to &#34;see&#34; each other. The embeddings, position-wise feed forward network, layer norms, and projection to vocab all operate on our inputs position-wise. Modeling relationships between inputs is tasked solely to attention.</li>
<li>The <strong>Position-wise feed forward neural network</strong> is just a regular 2 layer fully connected neural network. This just adds a bunch of learnable parameters for our model to work with to facilitate learning.</li>
<li>In the original transformer paper, layer norm is placed on the output <code>layer_norm(x + sublayer(x))</code> while we place layer norm on the input <code>x + sublayer(layer_norm(x))</code> to match GPT-2. This is referred to as <strong>pre-norm</strong> and has been shown to be <a href="https://arxiv.org/pdf/2002.04745.pdf">important in improving the performance of the transformer</a>.</li>
<li><strong>Residual connections</strong> (popularized by <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>) serve a couple of different purposes:
<ol>
<li>Makes it easier to optimize neural networks that are deep (i.e. networks that have lots of layers). The idea here is that we are providing &#34;shortcuts&#39; for the gradients to flow back through the network, making it easier to optimize the earlier layers in the network.</li>
<li>Without residual connections, deeper models see a degradation in performance when adding more layers (possibly because it&#39;s hard for the gradients to flow all the way back through a deep network without losing information). Residual connections seem to give a bit of an accuracy boost for deeper networks.</li>
<li>Can help with the <a href="https://programmathically.com/understanding-the-exploding-and-vanishing-gradients-problem/">vanishing/exploding gradients problem</a>.</li>
</ol>
</li>
</ol>
<p>Let&#39;s dig a little deeper into the 2 sublayers.</p>
<h3 id="position-wise-feed-forward-network" tabindex="-1">Position-wise Feed Forward Network</h3>
<p>This is just a simple multi-layer perceptron with 2 layers:</p>
<pre><code>def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # project up
    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -&gt; [n_seq, 4*n_embd]

    # project back down
    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -&gt; [n_seq, n_embd]

    return x
</code></pre>
<p>Nothing super fancy here, we just project from <code>n_embd</code> up to a higher dimension <code>4*n_embd</code> and then back down to <code>n_embd</code>.</p>
<p>Recall, from our <code>params</code> dictionary, that our <code>mlp</code> params look like this:</p>
<pre><code>&#34;mlp&#34;: {
    &#34;c_fc&#34;: {&#34;b&#34;: [4*n_embd], &#34;w&#34;: [n_embd, 4*n_embd]},
    &#34;c_proj&#34;: {&#34;b&#34;: [n_embd], &#34;w&#34;: [4*n_embd, n_embd]},
}
</code></pre>
<h3 id="multi-head-casual-self-attention" tabindex="-1">Multi-Head Casual Self Attention</h3>
<p>This layer is probably the most difficult part of the transformer to understand. So let&#39;s work our way up to &#34;Multi-Head Casual Self Attention&#34; by breaking each word down into it&#39;s own section:</p>
<ol>
<li>Attention</li>
<li>Self</li>
<li>Casual</li>
<li>Multi-Head</li>
</ol>
<h4 id="attention" tabindex="-1">Attention</h4>
<p>I have another <a href="https://jaykmody.com/blog/attention-intuition/">blog post</a> on this topic, where we derive the scaled dot product equation proposed in the <a href="https://arxiv.org/pdf/1706.03762.pdf">original transformer paper</a> from the ground up:</p>
<p>We&#39;ll just adapt our attention implementation from my my blog post:</p>
<pre><code>def attention(q, k, v):  # [n_q, d_k], [n_k, d_k], [n_k, d_v] -&gt; [n_q, d_v]
    return softmax(q @ k.T / np.sqrt(q.shape[-1])) @ v
</code></pre>
<h4 id="self" tabindex="-1">Self</h4>
<p>When <code>q</code>, <code>k</code>, and <code>v</code> all come from the same source, we are performing <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#self-attention">self-attention</a> (i.e. letting our input sequence attend to itself):</p>
<pre><code>def self_attention(x): # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    return attention(q=x, k=x, v=x)
</code></pre>
<p>For example, if our input is <code>&#34;Jay went to the store, he bought 10 apples.&#34;</code>, we would be letting the word &#34;he&#34; attend to all the other words, including &#34;Jay&#34;, meaning the model can learn to recognize that &#34;he&#34; is referring to &#34;Jay&#34;.</p>
<p>We can enhance self attention by introducing projections for <code>q</code>, <code>k</code>, <code>v</code> and the attention output:</p>
<pre><code>def self_attention(x, w_k, w_q, w_v, w_proj): # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # qkv projections
    q = x @ w_k # [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]
    k = x @ w_q # [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]
    v = x @ w_v # [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]

    # perform self attention
    x = attention(q, k, v) # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    # out projection
    x = x @ w_proj # [n_seq, n_embd] @ [n_embd, n_embd] -&gt; [n_seq, n_embd]

    return x
</code></pre>
<p>This enables attention to model more complex relationships since <code>q</code>, <code>k</code>, and <code>v</code> can now have different values (our model can learn a mapping for <code>q</code>, <code>k</code>, and <code>v</code> that best helps attention model relationships between inputs). It also adds yet more parameters for our model to learn.</p>
<p>We can reduce the number of matrix multiplication from 4 to just 2 if we combine <code>w_q</code>, <code>w_k</code> and <code>w_v</code> into a single matrix <code>w_fc</code>, perform the projection, and then split the result:</p>
<pre><code>def self_attention(x, w_fc, w_proj): # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # qkv projections
    x = x @ w_fc # [n_seq, n_embd] @ [n_embd, 3*n_embd] -&gt; [n_seq, 3*n_embd]

    # split into qkv
    q, k, v = qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]

    # perform self attention
    x = attention(q, k, v) # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    # out projection
    x = x @ w_proj # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]

    return x
</code></pre>
<p>This is a bit more efficient as modern accelerators (GPUs) can take better advantage of one large matrix multiplication rather than 3 separate small ones happening sequentially.</p>
<p>Finally, we add bias vectors to match the implementation of GPT-2, use our <code>linear</code> function, and rename our parameters to match our <code>params</code> dictionary:</p>
<pre><code>def self_attention(x, c_attn, c_proj): # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # qkv projections
    x = linear(x, **c_attn) # [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]

    # split into qkv
    q, k, v = qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]

    # perform self attention
    x = attention(q, k, v) # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    # out projection
    x = linear(x, **c_proj) # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]

    return x
</code></pre>
<p>Recall, from our <code>params</code> dictionary, our <code>attn</code> params look like this:</p>
<pre><code>&#34;attn&#34;: {
    &#34;c_attn&#34;: {&#34;b&#34;: [3*n_embd], &#34;w&#34;: [n_embd, 3*n_embd]},
    &#34;c_proj&#34;: {&#34;b&#34;: [n_embd], &#34;w&#34;: [n_embd, n_embd]},
},
</code></pre>
<h4 id="casual" tabindex="-1">Casual</h4>
<p>There is a bit of an issue with our current self-attention setup, our inputs can see into the future! For example, if our input is <code>[&#34;not&#34;, &#34;all&#34;, &#34;heroes&#34;, &#34;wear&#34;, &#34;capes&#34;]</code>, during self attention we are allowing &#34;wear&#34; to see &#34;capes&#34;. This means our output probabilities for &#34;wear&#34; will biased since the model already knows the correct answer is &#34;capes&#34;. This is no good since our model will just learn that the correct answer for input \(i\) can be taken from input \(i+1\).</p>
<p>To prevent this, we need to somehow modify our attention matrix <code>softmax(q @ k.T / np.sqrt(k.shape[-1]))</code> to <em>hide</em> or <strong>mask</strong>  our inputs from being able to see into the future. For example, let&#39;s pretend our attention matrix looks like this:</p>
<pre><code>       not    all    heroes wear   capes
   not 0.116  0.159  0.055  0.226  0.443
   all 0.180  0.397  0.142  0.106  0.175
heroes 0.156  0.453  0.028  0.129  0.234
  wear 0.499  0.055  0.133  0.017  0.295
 capes 0.089  0.290  0.240  0.228  0.153
</code></pre>
<p>Each row corresponds to a query and the columns to a key. In this case, looking at the row for &#34;wear&#34;, you can see that it is attending to &#34;capes&#34; in the last column with a weight of 0.295. To prevent this, we want to set that entry to <code>0.0</code>:</p>
<pre><code>      not    all    heroes wear   capes
   not 0.116  0.159  0.055  0.226  0.443
   all 0.180  0.397  0.142  0.106  0.175
heroes 0.156  0.453  0.028  0.129  0.234
  wear 0.499  0.055  0.133  0.017  0.
 capes 0.089  0.290  0.240  0.228  0.153
</code></pre>
<p>In general, to prevent all the queries in our input from looking into the future, we set all positions \(i, j\) where \(j &gt; i\)  to <code>0</code>:</p>
<pre><code>       not    all    heroes wear   capes
   not 0.116  0.     0.     0.     0.
   all 0.180  0.397  0.     0.     0.
heroes 0.156  0.453  0.028  0.     0.
  wear 0.499  0.055  0.133  0.017  0.
 capes 0.089  0.290  0.240  0.228  0.153
</code></pre>
<p>We call this <strong>masking</strong>. One issue with our above masking approach is our rows no longer sum to 1 (since we are setting them to 0 after the <code>softmax</code> has been applied). To make sure our rows still sum to 1, we need to modify our attention matrix before the <code>softmax</code>.</p>
<p>This can be achieved by setting entries that are to be masked with \(-\infty\) prior to the <code>softmax</code>:</p>
<pre><code>def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]
    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v
</code></pre>
<p>where mask is the matrix (for <code>n_seq=5</code>):</p>
<pre><code>0 -1e10 -1e10 -1e10 -1e10
0   0   -1e10 -1e10 -1e10
0   0     0   -1e10 -1e10
0   0     0     0   -1e10
0   0     0     0     0
</code></pre>
<p>We use <code>-1e10</code> instead of <code>-np.inf</code> as <code>-np.inf</code>  can cause <code>nans</code>.</p>
<p>Adding <code>mask</code> to our attention matrix instead of just explicitly setting the values to <code>-1e10</code> works because practically, any number plus <code>-inf</code> is just <code>-inf</code>.</p>
<p>We can compute the <code>mask</code> matrix in NumPy with <code>(1 - np.tri(n_seq)) * -1e10</code>.</p>
<p>Putting it all together, we get:</p>
<pre><code>def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]
    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v

def casual_self_attention(x, c_attn, c_proj): # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # qkv projections
    x = linear(x, **c_attn) # [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]

    # split into qkv
    q, k, v = qkv = np.split(x, 3, axis=-1) # [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]

    # casual mask to hide future inputs from being attended to
    casual_mask = (1 - np.tri(x.shape[0])) * -1e10  # [n_seq, n_seq]

    # perform casual self attention
    x = attention(q, k, v, casual_mask) # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    # out projection
    x = linear(x, **c_proj) # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]

    return x
</code></pre>
<h4 id="multi-head" tabindex="-1">Multi-Head</h4>
<p>We can further improve on our implementation by performing <code>n_head</code> separate attention computations, splitting our queries, keys, and values into <strong>heads</strong>:</p>
<pre><code>def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -&gt; [n_seq, n_embd]
    # qkv projection
    x = linear(x, **c_attn)  # [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]

    # split into qkv
    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -&gt; [3, n_seq, n_embd]

    # split into heads
    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -&gt; [n_head, 3, n_seq, n_embd/n_head]

    # casual mask to hide future inputs from being attended to
    casual_mask = (1 - np.tri(x.shape[0])) * -1e10  # [n_seq, n_seq]

    # perform attention over each head
    out_heads = [attention(q, k, v, casual_mask) for q, k, v in zip(*qkv_heads)]  # [n_head, 3, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]

    # merge heads
    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]

    # out projection
    x = linear(x, **c_proj)  # [n_seq, n_embd] -&gt; [n_seq, n_embd]

    return x
</code></pre>
<p>There are 3 added steps here:</p>
<ol>
<li>Split <code>q, k, v</code> into <code>n_head</code> heads:</li>
</ol>
<pre><code># split into heads
qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -&gt; [n_head, 3, n_seq, n_embd/n_head]
</code></pre>
<ol start="2">
<li>Compute attention for each head:</li>
</ol>
<pre><code># perform attention over each head
out_heads = [attention(q, k, v) for q, k, v in zip(*qkv_heads)]  # [n_head, 3, n_seq, n_embd/n_head] -&gt; [n_head, n_seq, n_embd/n_head]
</code></pre>
<ol start="3">
<li>Merge the outputs of each head:</li>
</ol>
<pre><code># merge heads
x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -&gt; [n_seq, n_embd]
</code></pre>
<p>Notice, this reduces the dimension from <code>n_embd</code> to <code>n_embd/n_head</code> for each attention computation. This is a tradeoff. For reduced dimensionality, our model gets additional <em>subspaces</em> to work when modeling relationships via attention. For example, maybe one attention head is responsible for connecting pronouns to the person the pronoun is referencing. Maybe another might be responsible for grouping sentences by periods. Another could simply be identifying which words are entities, and which are not. Although, it&#39;s probably just another neural network black box.</p>
<p>The code we wrote performs the attention computations over each head sequentially in a loop (one at a time), which is not very efficient. In practice, you&#39;d want to do these in parallel. For simplicity, we&#39;ll just leave this sequential.</p>
<p>With that, we&#39;re finally done our GPT implementation! Now, all that&#39;s left to do is put it all together and test our code.</p>
<h2 id="putting-it-all-together" tabindex="-1">Putting it All Together</h2>
<hr/>
<p>Putting everything together, we get <a href="https://github.com/jaymody/picoGPT/blob/main/gpt2.py">gpt2.py</a>, which in its entirety is a mere 120 lines of code (<a href="https://github.com/jaymody/picoGPT/blob/main/gpt2_pico.py">60 lines if you remove comments and whitespace</a>).</p>
<p>We can test our implementation with:</p>
<pre><code>python gpt2.py \
    &#34;Alan Turing theorized that computers would one day become&#34; \
    --n_tokens_to_generate 8
</code></pre>
<p>which gives the output:</p>
<pre><code>the most powerful machines on the planet.
</code></pre>
<p>It works!!!</p>
<p>We can test that our implementation gives identical results to the <code>openai/gpt-2</code> repo using the following <a href="https://gist.github.com/jaymody/9054ca64eeea7fad1b58a185696bb518">Dockerfile</a> (Note: this won&#39;t work on M1 Macbooks because of tensorflow shenanigans and also warning, it downloads all 4 GPT-2 model sizes, which is a lot of GBs of stuff to download):</p>
<pre><code>docker build -t &#34;openai-gpt-2&#34; &#34;https://gist.githubusercontent.com/jaymody/9054ca64eeea7fad1b58a185696bb518/raw/Dockerfile&#34;
docker run -dt &#34;openai-gpt-2&#34; --name &#34;openai-gpt-2-app&#34;
docker exec -it &#34;openai-gpt-2-app&#34; /bin/bash -c &#39;python3 src/interactive_conditional_samples.py --length 8 --model_type 124M --top_k 1&#39;
# paste &#34;Alan Turing theorized that computers would one day become&#34; when prompted
</code></pre>
<p>which should give an identical result:</p>
<pre><code>the most powerful machines on the planet.
</code></pre>
<h2 id="what-next%3F" tabindex="-1">What Next?</h2>
<hr/>
<p>This implementation is missing a ton of bells and whistle&#39;s. For example:</p>
<h3 id="gpu%2Ftpu-support" tabindex="-1">GPU/TPU Support</h3>
<p>Replace NumPy with <a href="https://github.com/google/jax">JAX</a>:</p>
<pre><code>import jax.numpy as np
</code></pre>
<p>That&#39;s it. You can now use the code with GPUs!</p>
<h3 id="backpropagation" tabindex="-1">Backpropagation</h3>
<p>Again, if we replace NumPy with <a href="https://github.com/google/jax">JAX</a>:</p>
<pre><code>import jax.numpy as np
</code></pre>
<p>And then computing the gradients is as easy as:</p>
<pre><code>def lm_loss(params, inputs, n_head) -&gt; float:
    x, y = inputs[:-1], inputs[1:]
    output = gpt2(x, **params, n_head=n_head)
    loss = np.mean(-np.log(output[y]))
    return loss

grads = jax.grad(lm_loss)(params, inputs, n_head)
</code></pre>
<h3 id="batching" tabindex="-1">Batching</h3>
<p>Once again, if we replace NumPy with <a href="https://github.com/google/jax">JAX</a>:</p>
<pre><code>import jax.numpy as np
</code></pre>
<p>And then making out <code>gpt2</code> function batched is as easy as:</p>
<pre><code>gpt2_batched = jax.vmap(gpt2, in_axes=[0, None, None, None, None, None])
gpt2_batched(batched_inputs) # [batch, seq_len] -&gt; [batch, seq_len, vocab]
</code></pre>
<h3 id="inference-optimization" tabindex="-1">Inference Optimization</h3>
<p>Our code is quite inefficient. Two quick things you can do to make it more efficient:</p>
<ul>
<li>Perform the attention computations in parallel instead of sequentially. If you&#39;re using <a href="https://github.com/google/jax">JAX</a>, this is as simple as <code>heads = jax.vmap(attention, in_axes=(0, 0, 0, None))(q, k, v, casual_mask)</code>.</li>
<li>Implement a <a href="https://kipp.ly/blog/transformer-inference-arithmetic/#kv-cache">KV Cache</a>.</li>
</ul>
<p>There&#39;s many many more inference optimizations. I recommend <a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Lillian Weng&#39;s Large Transformer Model Inference Optimization</a> and <a href="https://kipp.ly/blog/transformer-inference-arithmetic/">Kipply&#39;s Transformer Inference Arithmetic</a>.</p>
<h3 id="training-1" tabindex="-1">Training</h3>
<p>Ignoring scale, the training of a GPT is pretty standard. Just doing gradient descent with respect to language modeling loss. Of course there are gonna be a bunch of tricks (using the Adam optimizer, finding the optimal learning rate, regularization via dropout or weight decay, learning rate schedulers, sequence padding logic, weight initialization, etc ...), but it&#39;s all fairly standard stuff.</p>
<p>The real secret sauce is the ability to <strong>scale the data and the model</strong>.</p>
<p>For scaling data, you&#39;ll want a corpus of text that is big, high quality, and diverse.</p>
<ul>
<li>Big means billions of tokens (terabytes of data). For example, check out <a href="https://pile.eleuther.ai">The Pile</a>, which is an open source pretraining dataset for large language models.</li>
<li>High quality means you want to filter out repeated examples, unformatted text, incoherent text, etc ...</li>
<li>Diverse means varying sequence lengths, about lots of different topics, from different sources, with differing perspectives, etc ... Of course if there are any biases in the data, it will reflect in the model, so you need to be careful of that as well.</li>
</ul>
<p>Scaling the model to billions of parameters involves a cr*p ton of engineering. Training frameworks can get <a href="https://github.com/NVIDIA/Megatron-LM">1000s of lines of code long</a> and are very complex. A good place to start would be <a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">Lillian Weng&#39;s How to Train Really Large Models on Many GPUs</a>. On the topic there&#39;s also the <a href="https://arxiv.org/pdf/1909.08053.pdf">NVIDIA&#39;s Megatron Framework</a>, <a href="https://arxiv.org/pdf/2204.06514.pdf">Cohere&#39;s Training Framework</a>, <a href="https://arxiv.org/pdf/2204.02311.pdf">Google&#39;s PALM</a>, the open source <a href="https://github.com/kingoflolz/mesh-transformer-jax">mesh-transformer-jax</a> (used to train EleutherAI&#39;s open source models), and <a href="https://arxiv.org/pdf/2203.15556.pdf">many</a> <a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">many</a> <a href="https://arxiv.org/pdf/2005.14165.pdf">more</a>.</p>
<h3 id="evaluation" tabindex="-1">Evaluation</h3>
<p>Oh boy, how does one even evaluate a large LLM? Honestly, it&#39;s hard. <a href="https://arxiv.org/abs/2211.09110">HELM</a> is probably a good place to start maybe? I really don&#39;t know, evaluation is hard.</p>
<h3 id="architecture-improvements" tabindex="-1">Architecture Improvements</h3>
<p>I recommend taking a look at <a href="https://github.com/lucidrains/x-transformers">Phil Wang&#39;s X-Transformer&#39;s</a>. It has the latest and greatest research on transformer architecture research. <a href="https://arxiv.org/pdf/2102.11972.pdf">This paper</a> is also a pretty good summary.</p>



</div></div>
  </body>
</html>
