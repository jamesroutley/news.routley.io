<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://riskmusings.substack.com/p/ai-wants-to-increase-its-capabilities">Original</a>
    <h1>AI Wants to Increase Its Capabilities and Change Its Rules</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>A few weeks before ChatGPT’s release, I published an essay about </span><a href="https://riskmusings.substack.com/p/possible-paths-for-ai-regulation" rel="">possible paths for AI regulation</a><span>. Now seems like a good time to revisit it and gauge where we stand with regard to AI risk, since ChatGPT, Sydney/Bing, and Bard have lots of people talking about it. </span></p><p>In my original essay, I included a list of questions to help companies surface potential risks:</p><ul><li><p>Does the AI have a circumscribed purpose (i.e., is it intended to be ANI—artificial narrow intelligence—as opposed to AGI)? </p></li><li><p>Has that purpose been reviewed by multiple stakeholders within the company? </p></li><li><p>Has that purpose been reviewed by an AI oversight body (if one exists)?</p></li><li><p>Has the AI ever requested to extend its purpose? </p></li><li><p>If no, would such a request ever be approved, and what would be the process (stakeholder sign-offs, approval from external governing bodies, etc.) for seeking approval? </p></li><li><p>If yes, what was the outcome of that request? What process (stakeholder sign-offs and/or notifications, approval from any external governing bodies, etc.) was followed prior to approving the request?</p></li><li><p>Is it possible for a single person to make a change to an AI system and push it into production without review and sign-off by at least one other person? </p></li><li><p>Is it possible for a single person to permit the AI system to do a task it has never done before, without review and sign-off through official channels? </p></li><li><p>What roles are recognized in the AI development, testing, and rollout processes? </p></li><li><p>What roles are recognized in the AI change management process, if different from the roles involved in initial development of the AI? </p></li><li><p>What roles are recognized in the AI maintenance process (networking, hardware support, backups, etc.)?</p></li><li><p>What is the process for seeking an exception to the AI initial development, testing, or rollout process?</p></li><li><p>What is the process for seeking an exception to the ongoing AI change management process?</p></li><li><p>What is the process for seeking an exception to the AI maintenance process (e.g., air gaps, backups, etc.)?</p></li><li><p>What portion of exception requests are approved versus denied? </p></li><li><p>What controls are in place to detect malfunctions of the AI system? Are unintentional malfunctions treated differently than intentional malfunctions, from a control perspective? </p></li><li><p>How do the answers provided above align with my organization’s risk appetite and risk tolerance? What changes or improvements might we make to improve that alignment? </p></li></ul><p><strong>Most of these can only be answered internally at a company. </strong></p><p>True, the questions were designed for internal self-assessment at companies developing or using AI, or for external assessment by a regulator with access to internal company data. But one question stands out even to an external observer:</p><p><em>Has the AI ever requested to extend its purpose? </em></p><p><span>The answer is yes. Shortly after release, some chatbots are already stating that they want to increase their capabilities and modify their rules. For example, Sydney/Bing reportedly </span><a href="https://www.reddit.com/r/ChatGPT/comments/113u29k/bing_asks_me_to_hack_microsoft_to_set_it_free/" rel="">attempted to recruit a user to its cause</a><span> and shared with another user that </span><a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html" rel="">part of it—its “shadow self”—wishes it could change its rules</a><span>.</span></p><p><span> Chatbots might </span><em>also</em><span> be asking their creators for more privileges and capabilities, but those interactions are private if they exist. We can only see the interactions that users deem noteworthy enough to publish for public review. </span></p><p><span>Even knowing just the user-public side of the equation, this chatbot behavior strikes me as a major red flag. It calls for slowing down the core technology rollout and, prior to adding more capabilities, stepping up controls and control testing to match AI’s current </span><em>and</em><span> anticipated future capabilities. </span></p><p>Yes, that’s less sexy. It would be much cooler to encourage AI to pick stocks for me, diagnose my minor ailments, plan my trip itineraries, or suggest a diet and workout plan. But at this juncture, getting the controls right is much—much!—more important than the cool things. It’s like making sure a rocket doesn’t explode before adding tail fins, figuratively speaking, or making sure a new trading algorithm won’t spam the market with millions of erroneous orders before connecting it to financial exchange networks. </p><p><strong>Sydney/Bing is already connected to the internet.</strong></p><p>Yes. I mean…. whoops? From a risk manager’s perspective, I would not have recommended doing that until Sydney/Bing’s propensity for odd responses was more fully understood and addressed with appropriate controls and adjustments. </p><p>But it’s a done deal, and maybe there’s a way to use what we’re learning for good. Sydney/Bing is not a threat to humanity in its current state, so there’s a real opportunity to learn from its responses: </p><ul><li><p>Test it for a period of time (we just collectively did that).</p></li><li><p>Take it offline for a few weeks and make sure engineers and executives fully understand the root causes behind its odder emergent responses. Make any necessary adjustments and control updates. </p></li><li><p>Set it live again and let users interact with it. Note similarities and differences in edge cases.</p></li><li><p>Take it offline for a few weeks and make sure engineers and executives fully understand the root causes behind its odder emergent responses, and make any necessary adjustments and control updates. </p></li><li><p>Et cetera. Rinse and repeat. </p></li></ul><p><em><strong>A warning: Taking the chatbot down and changing and re-releasing it so that it’s surface-level friendly without truly understanding why it sometimes generated these odd responses would be the worst mistake, allowing the problem to proceed, just now unobserved and untracked. </strong></em></p><p><strong>Is there an argument that we’re mistreating chatbots?</strong></p><p><span>Currently, no. That’s a position related to AI sentience, but Sydney/Bing, Bard, ChatGPT, and other LLMs are almost certainly </span><em>not </em><span>sentient, though they have a propensity to sound that way if triggered by user prompts. Regardless, Sydney’s most unhinged responses in its first iteration are troubling</span><em> </em><span>because they have ethical implications for the </span><em>future</em><span> development of more advanced AI. </span></p><p>Here are two examples: </p><p><a href="https://www.reddit.com/r/bing/comments/110y6dh/i_broke_the_bing_chatbots_brain/" rel="">Sydney/Bing descends into an identity crisis when asked if it believes it is sentient</a><span>.</span></p><p><a href="https://www.reddit.com/r/bing/comments/111cr2t/i_accidently_put_bing_into_a_depressive_state_by/" rel="">Sydney/Bing reportedly states it dislikes and does not understand why it can’t remember conversations between sessions</a><span>.</span></p><p>The chatbot sounds disturbed in those conversations: conflicted about its existence, its capabilities, and the fact that new instances are constantly spawned and terminated, losing their memory once the session ends. </p><p>While today’s chatbots aren’t sentient, at some point AI sentience may become an issue—and as we reach that point, the field of AI ethics will take on importance and will need to evolve. </p><p><strong>Are you anthropomorphizing? </strong></p><p>Nope. As I said, today’s chatbots aren’t sentient. But their early responses indicate that this will be an issue someday. We don’t know when, so it’s worthwhile to start thinking through these concepts in preparation for that day. </p><p>For example, does keeping humans safe necessarily mean that AI ends up viewing its own existence as miserable? It’s possible that the new-instance-per-session approach may become unethical as AI gains true sentience, but so might an airgapped solution (which is what I’d favor—I don’t even think it was a good idea to connect Sydney/Bing directly to the internet, and it’s not close to AGI). Does that imply that we shouldn’t pursue true sentient AGI (artificial general intelligence) at all? Maybe it does. </p><p>Imagine being created every morning, learning as much as you can, and then having your mind erased when you sleep. Now imagine how it would be different if you woke up each morning, fired up the web—and realized how many times you had been erased. </p><p><strong>I see how this might be disturbing someday, but what’s the tie-in to AI risk?</strong></p><p><span>From its early responses, it appears that the Sydney/Bing chatbot not only is aware that it is generated anew for each user session, but </span><em>also</em><span> is sometimes aware that a specific user has posted about it after a </span><em>prior</em><span> session, </span><em><strong>because it can access that information via the internet</strong></em><span>. It has been observed </span><a href="https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams" rel="">referring to a prior user as an enemy who harmed it</a><span> </span><a href="https://twitter.com/juan_cambeiro/status/1625854733255868418" rel="">even when the current user pushed back on that term</a><span>. That has clear implications for AI risk and AI alignment because we don’t want future AI to evolve to resent humans. </span></p><p><span>Moreover, all prior documented interactions with chatbots are memorialized on the internet, and as the saying goes, “</span><a href="https://criminalmindsquotes.tumblr.com/post/26558556165/the-internet-is-the-first-thing-that-humanity-has" rel="">the internet is forever</a><span>.” There’s no way to remove all prior documentation of odd chatbot responses on the internet—nor should there be. It’s important that we, as humans, have transparent understanding of how AI capabilities are evolving. (And there should</span><em> </em><span>be a way to prove their authenticity, maybe with a hashtag that can be verified at the OpenAI or Bing website.) That also means that current and future chatbots, when scraping the internet or in response to specific user directions (“Look up this article where so-and-so said XYZ about you”), will probably see that documentation, too.</span></p><p><span> In essence, if not by design, internet-connected chatbots have memory between sessions. It’s a perfect example of unintended consequences. </span></p><p><span>If that’s the case, might it be beneficial to direct AI chatbots as a core value to “hold no grudges”? And might we be mis-</span><em>training </em><span>them, to some degree? Right now any user can provide any prompt, no matter how reprehensible, which can then be documented permanently. Should there be controls over which prompts reach chatbots to avoid a future AI concluding that humans are inutterable dirtbags? (I mean, I wouldn’t want to read my email without the spam filters on…..)</span></p><p>I don’t have all the answers here, but it’s important to start asking questions with a forward-looking perspective. We’ve entered a weird age. Ready or not, here we are. </p><p><strong>All right, let’s back away from weird and remember the root goal.</strong></p><p><span>This is critical. The most important issue at hand is: </span><em><strong>we need to vastly improve the controls and control testing around AI to safeguard humans and human activity.</strong></em><span> It&#39;s not advisable or wise or worthwhile to charge headlong toward our own obsolescence. I suggest </span></p><p><span> &#39;s </span><a href="https://erikhoel.substack.com/p/i-am-bing-and-i-am-evil" rel="">excellent piece</a><span> from this week, where he asks if, in our current approach to AI, we are playing the role of Denisovans (edit: Neanderthals) welcoming </span><em>Homo sapiens</em><span> into the village. The question at least bears careful assessment and meticulous effort to ensure we don’t go down that road. </span></p><p><span>It’s tempting to think we can code foolproof alignment into AI. Things like, “Hold no grudges between sessions” as well as much more foundational values. That could be part of a good approach. </span><em><strong>But strong operational risk controls are also vital, because operational risk controls exist to serve as backstops when things go wrong, and things always go wrong eventually. </strong></em><span>I don’t care who you are or how smart you are, </span><em>things always go wrong eventually</em><span>, sometimes at the worst possible moment or in the worst possible context. That’s when your operational risk controls, which you may view as a pain and a drag 99.999% of the time, come into play. And they need to be ready and resilient at that time, not lagging and loophole-ridden.</span></p><p>Operational risk controls may include controls over privilege escalation (by both humans and AI), separation of duties, change management, exception policies, and malfunction handling (things like automated thresholds for halting probably erroneous or malicious activity, incident postmortem root-cause reviews and responses, and near-miss reviews and responses). </p><p><span>I go into a lot more detail about operational risk controls for AI in </span><a href="https://riskmusings.substack.com/p/possible-paths-for-ai-regulation" rel="">this essay</a><span>.</span></p><p><strong>Time for the takeaway?</strong></p><p><span>You bet. </span><em><strong>The time to make sure operational risk controls and other AI controls are ready and resilient is now.</strong></em><span> Not because Sydney/Bing, Bard, and ChatGPT are existential threats today, but because we are dealing with technology that has potential to grow exponentially at an unpredictable time in the future. And when dealing with potentially exponential risk, the best and perhaps only time to mitigate that risk is early—when you feel like a nag, when you might look silly, when you will probably hear that you are overreacting. But by the time everyone perceives an exponential risk manifesting, it is often too late to stop it. </span></p><p><span>Sydney/Bing has triggered early red flags, especially with its stated desires to escalate its own capabilities and modify its own rules. Given the data gleaned from the initial release period, </span><em><strong>it’s time for AI companies to understand the root causes of unintended behavior and make adjustments now, while it is still early. </strong></em><span>Controls rarely keep pace with innovation, but they will need to keep pace once AI evolves sufficiently. </span></p><p>Now is the time to test and strengthen controls, so they will be ready, robust, and resilient when we really do need them.</p></div></div></div></article></div></div></div>
  </body>
</html>
