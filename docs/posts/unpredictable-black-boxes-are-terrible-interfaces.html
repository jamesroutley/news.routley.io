<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://magrawala.substack.com/p/unpredictable-black-boxes-are-terrible">Original</a>
    <h1>Unpredictable Black Boxes Are Terrible Interfaces</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>I recently decided I should update the profile picture on my </span><a href="http://graphics.stanford.edu/~maneesh" rel="nofollow ugc noopener">webpage</a><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png" width="728" height="245" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png&#34;,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:490,&#34;width&#34;:1456,&#34;resizeWidth&#34;:728,&#34;bytes&#34;:852991,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87dddf6a-17f3-48c6-a81f-74d46f8264a8_2316x780.png 1456w" sizes="100vw"/></picture></div></a><figcaption>Yes, I too have  fallen into the academic trap of using a terribly out-of-date picture on my webpage.</figcaption></figure></div><p><span>As a Computer Science Professor, I figured the easiest way to produce a high-quality photo would be to generate it using </span><a href="https://openai.com/product/dall-e-2" rel="nofollow ugc noopener">DALL-E2</a><span>. So I wrote a simple prompt, </span><em>“Picture of a Professor named Maneesh Agrawala”</em><span> and DALL-E2 made an image that is … well … stunning. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png" width="1456" height="785" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:785,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:3813165,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4819598-eb39-4b97-89af-6900bb6f0eee_3802x2050.png 1456w" sizes="100vw"/></picture></div></a><figcaption><span>From the text prompt “</span><em>Picture of a Professor named Maneesh Agrawala</em><span>” a black box generative AI model (i.e., DALL-E2) can produce an amazingly coherent photo.</span></figcaption></figure></div><p><span>From the text prompt alone it generated a person who looks to be of Indian origin, dressed him in “professorly” attire and placed him in an academic conference room. At a lower level, the objects, the lighting, the shading and the shadows are coherent and appear to form a single unified image. I won’t quibble about the artifacts — the fingers don’t really look right, one temple of the glasses seems to be missing and of course I was hoping to look a bit cooler and younger.  But overall, it is absolutely amazing that a generative AI model can produce such high-quality images, as quickly as we can think of prompt text. </span><em><strong>This is a new capability that we have never had before in human history.</strong></em><span> </span></p><p>And it is not just images. Modern generative AI models are black boxes that take a natural language prompt as input and transmute it into surprisingly high-quality text (GPT-4, ChatGPT), images (DALL-E2, Stable Diffusion, Midjourney), video (Make-A-Video), 3D models (DreamFusion) and even program code (Copilot, Codex).</p><p><span>So let’s use DALL-E2 to make another picture. This time I’d like to see what Stanford’s main quad would look like if it appeared in the style of the film, </span><a href="https://en.wikipedia.org/wiki/Blade_Runner" rel="nofollow ugc noopener">Blade Runner</a><span>. When I think of Stanford’s main quad I think about the facade of Memorial Church and palm trees. When I think I of Blade Runner, I think of neon signs, crowded night markets, rain, and food stalls. I start with a simple prompt, “</span><em>stanford memorial church with neon signage in the style of bladerunner</em><span>”.  </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png" width="1456" height="625" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:625,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:4253820,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee66a73a-2365-4b70-b430-879d57cbc270_3326x1428.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Using DALL-E2 with different prompts to try and find a combination that would generate an image of Stanford’s main quad in the style of Blade Runner. None of DALL-E2’s image really look like the picture I had in mind.</figcaption></figure></div><p><span>At this first iteration the resulting images don’t really show the Stanford quad with its palm trees. So I first add “</span><em>and main quad</em><span>”  to the prompt for iteration 2 and after inspecting those results I add “</span><em>with palm trees</em><span>” for iteration 3. The resulting images look more like the Stanford quad, but don’t really look like the rainy nighttime scenes of Blade Runner. So I cyclically revise the prompt, inspect the DALL-E2 generated images and then update the prompt, to try and find a combination of prompt words that produce something like the image I have in mind.  At iteration 21, after several hours of somewhat randomly trying different prompt words, I decide to stop. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png" width="1456" height="950" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/f12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:950,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:3833834,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff12719ab-364c-4cbb-bf65-ea0c14ba707c_2896x1890.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>After 21 iterations and  hours of trying different prompts, the DALL-E2 image still doesn’t really match my sense of Stanford’s main quad in the style of Blade Runner.</figcaption></figure></div><p><span>The resulting image isn’t really what I had in mind. Even worse, it is unclear to me how to change the prompt to move the image towards the image I want. </span><em>This is frustrating.</em></p><p><span>In fact, finding effective prompts is so difficult that there are websites and forums dedicated to collecting and sharing prompts (e.g. </span><a href="https://prompthero.com/" rel="nofollow ugc noopener">PromptHero</a><span>, </span><a href="https://arthub.ai/" rel="nofollow ugc noopener">Arthub.ai</a><span>, </span><a href="https://www.reddit.com/r/StableDiffusion/" rel="nofollow ugc noopener">Reddit/StableDiffusion</a><span>). There are also marketplaces for buying and selling prompts (e.g. </span><a href="https://promptbase.com/" rel="nofollow ugc noopener">PromptBase</a><span>). And there is a cottage industry of research papers on </span><em>prompt engineering</em><span>. </span></p><p><span>To understand why writing effective prompts is hard, I think it is instructive to remember an anecdote from </span><a href="http://jnd.org" rel="nofollow ugc noopener">Don Norman’s</a><span> classic book, </span><a href="https://jnd.org/the-design-of-everyday-things-revised-and-expanded-edition/" rel="nofollow ugc noopener">The Design of Everyday Things</a><span>. The story is about a two-compartment refrigerator he owned, but found extremely difficult to set the temperature for properly. The temperature controls  looked something like this:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png" width="1456" height="634" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:634,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:255103,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3046653a-e771-4007-998e-db6610dbfdd6_3484x1516.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Temperature controls for Don Norman’s two compartment refrigerator.</figcaption></figure></div><p><span>Separate controls for the freezer and fresh food compartments suggest that each one has its own independent cooling unit. But this </span><em>conceptual model</em><span> is wrong. Norman explains that there is only one cooling unit; the freezer controls sets the cooling unit’s temperature while the fresh food control sets a valve that directs the cooling to the two compartments. The true </span><em>system model</em><span> couples the controls in complicated way. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png" width="728" height="216" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png&#34;,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:432,&#34;width&#34;:1456,&#34;resizeWidth&#34;:728,&#34;bytes&#34;:438462,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64000c93-d5ac-4e6f-a170-f15da452aa67_5600x1660.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Users build a conceptual model (left)  in which each control independently sets the temperature for the corresponding compartment. But in the true system model (right) the controls interact in a complicated way. </figcaption></figure></div><p><span>With an incorrect conceptual model users cannot predict how the input controls produce the output temperature values. Instead they have to resort to an iterative, trial-and-error process of (i) setting the controls, (ii) waiting 24 hours for the temperature to stabilize and (iii) checking the resulting temperature. If the stabilized temperature is still not right they must going back to step (i) and try again. </span><em>This is frustrating.</em><span>  </span></p><p>For me there are two main takeaways from this anecdote.</p><ol><li><p><em><strong>Well designed interfaces let users build a conceptual model that can predict how the input controls affect the output.</strong></em></p></li><li><p><em><strong>When a conceptual model is not predictive, users are forced into using trial-and-error.</strong></em></p></li></ol><p>The job of an interface designer is to develop an interface that lets users build a predictive conceptual model.</p><p>Generative AI black boxes are terrible interfaces because they do not provide users with a predictive conceptual model. It is unclear how the AI converts an input natural language prompt into the output result. Even the designers of the AI usually can’t explain how this conversion occurs in a way that would allow users to build a predictive conceptual model. </p><p><span>I went back to DALL-E2 to see if I could get it to produce an even better picture of me, using the following prompt, “</span><em>Picture of a cool, young Computer Science Professor named Maneesh Agrawala”. </em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png" width="1456" height="841" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/c8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:841,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:2831211,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8f171f9-ce92-4f0b-971b-4de300b9e454_2868x1656.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>DALL-E2 generates a cooler and younger picture of me. </figcaption></figure></div><p>But I have no idea how the prompt affects the picture. Does the word “cool” produce the sports coat and T-shirt combination, or do they come from the word “young”? How does the term “Computer Science” affect the result? Does the word “picture” imply the creation of a realistic photograph rather than an illustration? Without a predictive conceptual model I cannot answer these questions. My only recourse is trial-and-error to find the prompt that generates the image I want. </p><p>One goal of AI is to build models that are indistinguishable from humans. You might argue that natural language is what we use to work with other humans and obviously humans are good interfaces. I disagree. Humans are also terrible interfaces for many generative tasks. And humans are terrible for exactly the same reasons that AI black boxes are terrible. As users we often lack a conceptual model that can precisely predict how another human will convert a natural language prompt into output content.  </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png" width="1456" height="722" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:722,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:2604173,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F982f8f2c-8fec-4826-9dff-63bcbde31799_3304x1638.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Collaborating with another human is better than working with generative AI  in part because conversation allows us to establish common ground, build shared semantics and engage in repair strategies when something is ambiguous.</figcaption></figure></div><p><span>Yet, our conceptual models of humans are often better (more predictive) than our conceptual models of AI black boxes, for two main reasons. First, our conceptual model of the way a human collaborator will respond to a prompt is likely based on the way we ourselves would respond to the request. We have strong prior for the conceptual model as we assume that a human collaborator will act similarly to the way we would act. Second, as psycholinguists like </span><a href="https://en.wikipedia.org/wiki/Herbert_H._Clark" rel="nofollow ugc noopener">Herb Clark</a><span> have </span><a href="https://www.amazon.com/Psychology-Language-Psycholinguistics-Herbert-Clark/dp/0155728156/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1379657960&amp;sr=1-2" rel="nofollow ugc noopener">pointed out</a><span>, we can converse with a human collaborator to establish common ground and build shared semantics. We can use repair strategies to fix ambiguities and misunderstandings that arise in natural language conversations. Common ground, shared semantics and repair strategies are fundamental for collaboration between humans. </span></p><p>Yet, despite these advantages, working with another human to generate high-quality content often requires multiple iterations. And the most effective collaborations often involve weeks, months, or even years of conversation to build the requisite common ground. </p><p>As I said, humans are terrible interfaces. But they are better than AI black boxes. </p><p>With AI, our conceptual models are either non-existent or worse, they are based on the prior we have for human collaborators. We assume the AI will generate what a human collaborator might generate given the prompt. Unfortunately, this type of anthropomorphization is difficult to avoid. Claims that an AI model “understands” natural language or that it has “reasoning” capabilities, reinforce that the idea that the model somehow understands and reasons the way a human understands and reasons. Yet, it is almost certainly the case that AI does not understand or reason about anything the way a human does. </p><p><span>So how can we make better generative AI tools? One way might be to support  conversational interactions. Text generation tools like </span><a href="https://openai.com/blog/chatgpt" rel="nofollow ugc noopener">ChatGPT</a><span> are already starting to do this. Such tools support conversational turn-taking and can treat earlier exchanges as context for future exchanges. The context lets both the AI and the user refer to concepts mentioned earlier in the conversation and thereby enables a kind of shared common ground. But it is unclear how much common sense knowledge such systems contain and the grounding of semantic concepts seems rather shallow. For users it is unclear what ChatGPT knows and what it doesn’t know, so conversations can require multiple turns just to establish basic shared facts. Moreover, the conversational interaction with a user doesn’t update the AI model so the AI cannot learn new concepts from the user. Adding common sense, grounding and symbolic reasoning to these models remains a major thrust of ongoing AI research.</span></p><p><span>Natural language is often ambiguous. In conversations, people use repair strategies to reduce such ambiguity and ensure that they are talking about the same thing. Researchers have started to build such repair mechanisms into text-to-image AI systems.  For example, </span><a href="https://prompt-to-prompt.github.io/" rel="nofollow ugc noopener">Prompt-to-Prompt image editing [Hertz 2022]</a><span>  is a technique that lets users generate an image from an initial text prompt and then refine the prompt to produce a new image but with only a minimal set of changes required to reflect the edited prompt. An initial prompt of “a cake with decorations” might be refined to “a cake with </span><em>jelly bean</em><span> decorations” and the initial image would be updated accordingly. Such refinement is a form of repair. </span></p><p><span>Another way to reduce the ambiguity of natural language is to let users add constraints as conditioning on the generation process. </span><a href="https://phillipi.github.io/pix2pix/" rel="nofollow ugc noopener">Image-to-image translation [Isola 2016]</a><span> showed how to apply this approach in the context of image synthesis. It converts one type of input image (e.g. a label map, an edge image, etc.) into another type of image (e.g., photograph, a map, etc.) by learning a generative adversarial network (GAN) conditioned on the input image type.  The input image imposes spatially localized constraints on the composition of the output image.  Such input images are effective controls, because it is much easier for users to specify precise spatial composition using imagery rather than spatially ambiguous natural language. Recently, we and many other groups have applied this approach in the context of text-to-image AI models.</span></p><p><span>Conversational interactions can also go beyond natural language. In the context of text-to-image AI models,  researchers have started to develop methods that establish common ground.  </span><a href="https://textual-inversion.github.io/" rel="nofollow ugc noopener">Textual Inversion [Gal 2022]</a><span> and </span><a href="https://dreambooth.github.io/" rel="nofollow ugc noopener">DreamBooth [Ruiz 2022]</a><span> let users provide a few example images of an object and the AI model learns to associate a text token with it (both methods fine-tune a diffusion model using the example images). When users put the learned token in a new prompt, the system includes the corresponding object into the image. Thus the user and the system build a kind of shared grounding for the object. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png" width="1456" height="618" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:618,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:919054,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff157aa8-0c1d-4cd2-bde2-f2919ff8ba43_3712x1576.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><span>Neurosymbolic generative AI tools might generate code that must be executed to generate the content.  The code could then serve as intermediate language that both the AI and a human developer might understand in the same way facilitating the conversation between them.  (Code and chair model from </span><a href="https://rkjones4.github.io/shapeAssembly.html" rel="nofollow ugc noopener">Jones et al. [2020]</a><span>)</span></figcaption></figure></div><p><a href="https://drive.google.com/file/d/1g_VpZ80MzkcqZBqj81kBc2VXYKiyzhbO/view?usp=share_link" rel="nofollow ugc noopener">Neurosymbolic methods</a><span> may provide another path to a conversational interface with AI models. Consider a generative AI model that instead of directly outputting content, outputs a program which must be executed to produce the content. The advantage of this approach is that the output program is something that both humans and the AI model might be able to understand in the same way. It may be possible to formalize the semantics of the programming language in ways that allows for shared understanding between the a human developer and the AI. Even without formal semantics, the human developer might be able to inspect the code and check that the code is doing “the right thing”.  And when the code fails, the developer might be able to suggest fixes to the AI in the programming language itself  rather than relying on natural language input alone. This approach is essentially about shifting the language for communicating with the AI from human natural language to something closer to a programming language. </span></p><p>Generative AI models are amazing and yet they are terrible interfaces. When users cannot predict how input controls affect outputs they have to resort to trial-and-error, which is frustrating.  This is a major issue when using generative AI for creating new content and it will remain an issue as long as the mapping between the input controls and outputs is unclear. But we can improve AI interfaces by enabling conversational interactions that can let users establish common ground/shared semantics with the AI, and that provide repair mechanisms when such shared semantics are missing.  </p><p><em><span>This post is a revised and updated version of a talk I gave at the </span><a href="https://hai.stanford.edu/events/2022-hai-fall-conference-ai-loop-humans-charge" rel="nofollow ugc noopener">HAI 2022 Fall Conference on AI in the Loop: Humans in Charge</a><span>. Thanks to Michael Bernstein, Jean-Peïc Chou, Kayvon Fatahalian, James Landay, Jeongyeon Kim, Jingyi Li, Sean Liu, Jiaju Ma, Jacob Ritchie, Daniel Ritchie, Ben Shneiderman, Lvmin Zhang and Sharon Zhang for providing feedback on the ideas presented here.</span></em><span> </span></p><p><em><a href="http://graphics.stanford.edu/~maneesh" rel="nofollow ugc noopener">Maneesh Agrawala</a><span> (</span><a href="https://twitter.com/magrawala" rel="nofollow ugc noopener">@magrawala,</a><span> </span><a href="https://fediscience.org/@magrawala" rel="nofollow ugc noopener">@magrawala@fediscience.org</a><span>) is a cool, young Computer Science Professor and Director of the </span><a href="http://brown.stanford.edu" rel="nofollow ugc noopener">Brown Institute for Media Innovation</a><span> at Stanford University. He is on sabbatical at </span><a href="https://corp.roblox.com/research/" rel="nofollow ugc noopener">Roblox</a><span>.</span></em><span> </span></p></div></div></div></article></div></div></div>
  </body>
</html>
