<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/memgraph/odin">Original</a>
    <h1>Show HN: Odin – the integration of LLMs with Obsidian note taking</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>A look at episode #5: <a href="https://youtu.be/q8SA3rM6ckI?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener noreffer ">The spelled-out intro to language modeling: Building makemore Part 4: Becoming a Backprop Ninja</a> from <a href="https://karpathy.ai/" target="_blank" rel="noopener noreffer ">Andrej Karpathy</a> amazing tutorial series.</p>

<p>
  <iframe src="https://www.youtube.com/embed/q8SA3rM6ckI" allowfullscreen="" title="YouTube Video"></iframe>
</p>

<p>We go back to the previous N-gram character-level MLP model from session #4 and dive into a hands-on manual backpropagation session.</p>
<h2 id="computing-gradients-by-hand">Computing Gradients by Hand</h2>
<p>This lesson is a bit of a different format. It’s a lot more exercise centric and less of a type-along lecture. The tasks feel a bit repetitives after a while but it gives a good sense of what micrograd/PyTorch are doing in the background for us.</p>
<p>Andrej intentionnaly cut the code into very minimal atomic operation to make the job of differentiating easier. And later refactor some of the many operations into single higher level blocks (e.g. <code>cross_entropy</code> goes from a block of 8 gradients to a one-liner).</p>
<h2 id="sticking-point">Sticking point</h2>
<p>To me the least intuitive line is around converting the minibatch entries into vector space embeddings <code>C[Xb]</code> and it leads to nested loops code that sticks out from the rest of the code.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># &lt;???&gt; this is the least intuitive line to me, the `C[Xb]`</span>
</span></span><span><span><span># with C.shape = [27, 10], Xb.shape = [32, 3], and C[Xb].shape = [32, 3, 10]</span>
</span></span><span><span><span># component: emb = C[Xb]</span>
</span></span><span><span><span>dC</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>C</span><span>)</span>
</span></span><span><span><span>for</span> <span>k</span> <span>in</span> <span>range</span><span>(</span><span>Xb</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
</span></span><span><span>    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>Xb</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]):</span>
</span></span><span><span>        <span>ix</span> <span>=</span> <span>Xb</span><span>[</span><span>k</span><span>,</span> <span>j</span><span>]</span>
</span></span><span><span>        <span>dC</span><span>[</span><span>ix</span><span>]</span> <span>+=</span> <span>demb</span><span>[</span><span>k</span><span>,</span> <span>j</span><span>]</span>
</span></span><span><span><span>cmp</span><span>(</span><span>&#39;C&#39;</span><span>,</span> <span>dC</span><span>,</span> <span>C</span><span>)</span>
</span></span></code></pre></div><p>I postulate that reformulating <code>C[Xb]</code> as a matrix dot product instead <code>F.one_hot(Xb).float() @ C</code> leads to a more elegant solution.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># Hypothesis: I think it would be easier to follow if `C[Xb]` was rewritten as</span>
</span></span><span><span><span># matrix dot product instead C[Xb] = F.one_hot(Xb).float() @ C</span>
</span></span><span><span><span># with emb.shape = [32, 3, 10], F.one_hot(Xb).float() = [32, 3, 27], C = [27, 10]</span>
</span></span><span><span><span># so instead we get:</span>
</span></span><span><span><span># component: emb = F.one_hot(Xb).float() @ C</span>
</span></span><span><span><span>dC</span> <span>=</span> <span>torch</span><span>.</span><span>tensordot</span><span>(</span><span>F</span><span>.</span><span>one_hot</span><span>(</span><span>Xb</span><span>,</span> <span>num_classes</span><span>=</span><span>vocab_size</span><span>)</span><span>.</span><span>float</span><span>(),</span> <span>demb</span><span>,</span> <span>dims</span><span>=</span><span>([</span><span>0</span><span>,</span> <span>1</span><span>],</span> <span>[</span><span>0</span><span>,</span> <span>1</span><span>]))</span>
</span></span><span><span><span>cmp</span><span>(</span><span>&#39;C&#39;</span><span>,</span> <span>dC</span><span>,</span> <span>C</span><span>)</span>
</span></span></code></pre></div><p>Computing the gradient become a 3D tensor dot product and we avoid manual loops totally.</p>
<h2 id="the-code">The code</h2>
<p>Here’s my take on the tutorial with additional notes. You can get the code on <a href="https://github.com/peluche/makemore" target="_blank" rel="noopener noreffer ">GitHub</a> or bellow.</p>


</div></div>
  </body>
</html>
