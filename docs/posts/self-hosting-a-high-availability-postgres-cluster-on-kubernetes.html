<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ryan-schachte.com/blog/ha_postgres_zolando/">Original</a>
    <h1>Self-hosting a high-availability Postgres cluster on Kubernetes</h1>
    
    <div id="readability-page-1" class="page"><div> <div> <p><span>Self-hosting a high-availability Postgres cluster on Kubernetes</span> <span>Deploying an HA Postgres cluster using Zalando w/ K8s.</span></p> </div> <p>Large cloud providers <em>do</em> indeed go down and without carefully architecting your systems to handle the scenario of <em>when</em> they go down, not <em>if</em> they go down, you need to have automated <a href="https://www.cloudflare.com/learning/performance/what-is-server-failover/">service failover</a> integrated into your system design process to ensure maximal uptime of your most critical applications. 2023 was a fun year for incidents, let’s have a look at just a few I cherry-picked below.</p>
<ul>
<li><a href="https://status.cloud.google.com/incidents/dS9ps52MUnxQfyDGPfkY">Google Cloud Paris Data Center Fire (April 25, 2023)</a></li>
<li><a href="https://aws.amazon.com/message/061323/">AWS us-east-1 Outage (June 13, 2023)</a></li>
<li><a href="https://www.datacenterdynamics.com/en/news/data-center-power-surge-and-cooling-failure-brings-down-microsoft-azure-services/">Microsoft Azure South East Asia Outage (February 9, 2023)</a></li>
<li><a href="https://newsletter.pragmaticengineer.com/p/three-cloud-providers-three-outages">Microsoft Azure West Europe Outage (July 5, 2023)</a></li>
<li><a href="https://www.networkworld.com/article/957256/microsoft-blames-aussie-data-center-outage-on-staff-strength-failed-automation.html">Microsoft Azure Australia Outage (August 30, 2023)</a></li>
</ul>
<blockquote>
<p>If you’re not convinced or feel like you want to study the topic of high-availability more, you should check out <a href="https://www.cockroachlabs.com/blog/brief-history-high-availability/">this fantastic article</a> from Cockroach Labs (creators of CockroachDB).</p>
</blockquote>
<p>As I’ve invested more in standardizing hosting my applications on Kubernetes, I started to think about shifting my database into Kubernetes as well to centralize <em>how</em> I manage all my services in one place. While this topic doesn’t come without heavy debate, I think we’ve evolved to a point where supporting this hosting paradigm is pretty streamlined.</p>
<p>By the end of this article we will have:</p>
<ul>
<li>Setup a highly-available and fault tolerant Postgres cluster from scratch</li>
<li>Host a UI to manage the clusters</li>
<li>Create a database and test disaster recovery by killing nodes</li>
</ul>
<blockquote>
<p>If you see me issue the command <code>k</code>, it’s a custom alias mapped in my <code>~/.zshrc</code> that maps to <code>kubectl</code>. I’ll often just type <code>k</code> for brevity.</p>
</blockquote>
<h2 id="zalando-postgres-operator">Zalando Postgres operator</h2>
<p><a href="https://github.com/zalando/postgres-operator">Zalando</a> is a Postgres <a href="https://www.redhat.com/en/blog/introducing-operators-putting-operational-knowledge-into-software">operator</a> that facilitates the deployment of a highly available (HA) Postgres cluster. Zalando uses <a href="https://patroni.readthedocs.io/en/latest/">Patroni</a> under the hood, which is essentially a template for scaling HA Postgres clusters.</p>
<p>Key features of the Zalando Postgres Operator include:</p>
<ul>
<li>Rolling updates on Postgres cluster changes, including quick minor version updates.</li>
<li>Live volume resize without pod restarts (for certain storage types like AWS EBS and PVC).</li>
<li>Database connection pooling with PGBouncer</li>
</ul>
<p>Let’s start by deploying the Zalando operator to our cluster followed by the Postgres cluster itself. We will be heavily referencing the <a href="https://postgres-operator.readthedocs.io/en/latest/">documentation</a> moving forward.</p>
<p>To keep things raw, we will use the manifests directly with <code>kubectl</code>.</p>
<figure data-rehype-pretty-code-figure=""><figcaption id="root-fig-34bid7j1"> PostgresOperatorDeploy.sh <!-- <span class="custom-figcaption__controls__lang">{lang}</span> -->  </figcaption> <pre tabindex="0" data-language="bash" data-theme=""><code data-line-numbers="" data-language="bash" data-theme="" data-line-numbers-max-digits="1"><span data-line=""><span># First, clone the repository and change to the directory</span></span>
<span data-line=""><span>git</span><span> clone</span><span> https://github.com/zalando/postgres-operator.git</span></span>
<span data-line=""><span>cd</span><span> postgres-operator</span></span>
<span data-line=""> </span>
<span data-line=""><span># apply the manifests in the following order</span></span>
<span data-line=""><span>kubectl</span><span> create</span><span> -f</span><span> manifests/configmap.yaml</span><span>  # configuration</span></span>
<span data-line=""><span>kubectl</span><span> create</span><span> -f</span><span> manifests/operator-service-account-rbac.yaml</span><span>  # identity and permissions</span></span>
<span data-line=""><span>kubectl</span><span> create</span><span> -f</span><span> manifests/postgres-operator.yaml</span><span>  # deployment</span></span>
<span data-line=""><span>kubectl</span><span> create</span><span> -f</span><span> manifests/api-service.yaml</span><span>  # operator API to be used by UI</span></span></code></pre></figure>
<p>I recommend reading each one of these manifests thoroughly to ensure compatibility with your cluster.</p>
<p>You can validate everything is running with <code>kubectl get pod -l name=postgres-operator</code>.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span>NAME</span><span>                                READY</span><span>   STATUS</span><span>    RESTARTS</span><span>   AGE</span></span>
<span data-line=""><span>postgres-operator-77f6d658c-mnlj6</span><span>   1</span><span>/1</span><span>     Running</span><span>   0</span><span>          37</span><span>s</span></span></code></pre></figure>
<h3 id="zalando-management-ui-optional">Zalando management UI (optional)</h3>
<p>Zalando provides a UI out of the box which is incredibly easy to deploy.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span># using kubectl</span></span>
<span data-line=""><span>kubectl</span><span> apply</span><span> -f</span><span> ui/manifests/</span></span>
<span data-line=""> </span>
<span data-line=""><span># or using Helmcharts</span></span>
<span data-line=""><span># helm install postgres-operator-ui ./charts/postgres-operator-ui</span></span></code></pre></figure>
<p>You can validate the deployment is running by grepping for the pods.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span>❯</span><span> kubectl</span><span> get</span><span> pods</span><span> |</span><span> grep</span><span> -i</span><span> postgres-operator-ui</span></span>
<span data-line=""><span>postgres-operator-ui-7bf9676b84-824j6</span><span>   1</span><span>/1</span><span>     Running</span><span>   0</span><span>          28</span><span>m</span></span></code></pre></figure>
<p>Since I don’t plan on exposing this, we can quickly crack it open with using <code>kubectl port-forward</code>.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span>kubectl</span><span> port-forward</span><span> deploy/postgres-operator-ui</span><span> 9090</span><span>:8081</span></span></code></pre></figure>
<p>From here, we should be able to access the UI via <code>http://localhost:9090</code>.</p>

<p>Moving forward, I will <em>not</em> be using the UI, but it’s available if you prefer it.</p>
<h2 id="postgres-cluster-deployment">Postgres cluster deployment</h2>
<p>We will still be leveraging <a href="https://github.com/zalando/postgres-operator">Zalando</a> for deploying the Postgres cluster as well. You can view more options in the <a href="https://github.com/zalando/postgres-operator/blob/master/manifests/complete-postgres-manifest.yaml">complete manifest example YML</a>or the <a href="https://opensource.zalando.com/postgres-operator/docs/reference/cluster_manifest.html">parameter overview here</a>.</p>
<p>I have 3 nodes in my cluster and I want 2/3 to actually run the operator (node2 and node3). Before configuring the affinity to do this, we need to label our data plane nodes. The label here is arbitrary.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span>kubectl</span><span> label</span><span> nodes</span><span> node2</span><span> pg-replication=</span><span>true</span></span>
<span data-line=""><span>kubectl</span><span> label</span><span> nodes</span><span> node3</span><span> pg-replication=</span><span>true</span></span></code></pre></figure>
<p>In my case, I’m just going to schedule on my <code>worker</code> nodes. using <code>node-role.kubernetes.io/worker=worker</code>.</p>
<p>Now we should be able to apply the <code>postgres</code> resource to our cluster.</p>
<figure data-rehype-pretty-code-figure=""><figcaption id="root-fig-f11ca6i5"> zalando/Deployment.yml <!-- <span class="custom-figcaption__controls__lang">{lang}</span> -->  </figcaption> <pre tabindex="0" data-language="yaml" data-theme=""><code data-line-numbers="" data-language="yaml" data-theme="" data-line-numbers-max-digits="2"><span data-line=""><span>apiVersion</span><span>:</span><span> &#34;acid.zalan.do/v1&#34;</span></span>
<span data-line=""><span>kind</span><span>:</span><span> postgresql</span></span>
<span data-line=""><span>metadata</span><span>:</span></span>
<span data-line=""><span>  name</span><span>:</span><span> pg-cluster</span></span>
<span data-line=""><span>spec</span><span>:</span></span>
<span data-line=""><span>  teamId</span><span>:</span><span> &#34;homelab&#34;</span></span>
<span data-line=""><span>  volume</span><span>:</span></span>
<span data-line=""><span>    size</span><span>:</span><span> 20Gi</span></span>
<span data-line=""><span>    storageClass</span><span>:</span><span> local-path</span></span>
<span data-line=""><span>  enableMasterLoadBalancer</span><span>:</span><span> true</span></span>
<span data-line=""><span>  numberOfInstances</span><span>:</span><span> 2</span></span>
<span data-line=""><span>  users</span><span>:</span></span>
<span data-line=""><span>    schachte</span><span>:</span></span>
<span data-line=""><span>    -</span><span> superuser</span></span>
<span data-line=""><span>    -</span><span> createdb</span></span>
<span data-line=""><span>  postgresql</span><span>:</span></span>
<span data-line=""><span>    version</span><span>:</span><span> &#34;15&#34;</span></span>
<span data-line=""><span>  nodeAffinity</span><span>:</span></span>
<span data-line=""><span>    requiredDuringSchedulingIgnoredDuringExecution</span><span>:</span></span>
<span data-line=""><span>      nodeSelectorTerms</span><span>:</span></span>
<span data-line=""><span>        -</span><span> matchExpressions</span><span>:</span></span>
<span data-line=""><span>            -</span><span> key</span><span>:</span><span> node-role.kubernetes.io/worker</span></span>
<span data-line=""><span>              operator</span><span>:</span><span> In</span></span>
<span data-line=""><span>              values</span><span>:</span></span>
<span data-line=""><span>                -</span><span> worker</span></span></code></pre></figure>
<blockquote>
<p>Note: I have <a href="https://longhorn.io/"><code>Longhorn</code></a> installed on my cluster, so my <code>storageClass</code> may be different than yours. Feel free to omit the <code>storageClass</code> key or validate what class to use via <code>kubectl get storageClass</code>.</p>
</blockquote>
<p>There are a couple important things to point out that are subtle.</p>
<ul>
<li><code>numberOfInstances</code> total number of instances for a given cluster. The operator parameters <code>max_instances</code> and <code>min_instances</code> may also adjust this number. Required field.</li>
<li><code>enableMasterLoadBalancer</code> boolean flag to override the operator defaults (set by the <code>enable_master_load_balancer</code> parameter) to define whether to enable the load balancer pointing to the Postgres primary. Optional.</li>
<li><code>size</code> will be the amount allocated <em>for each</em> instance. In this case, we will allocate a total of 40Gi of storage.</li>
</ul>
<p>In this case we have 2 running clusters that will sync with one another. The clusters will be running on 2 separate nodes due to the <code>nodeAffinity</code> selector.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span># view allocated storage</span></span>
<span data-line=""><span>❯</span><span> k</span><span> get</span><span> pv</span></span>
<span data-line=""> </span>
<span data-line=""><span>NAME</span><span>                                       CAPACITY</span><span>   ACCESS</span><span> MODES</span><span>   RECLAIM</span><span> POLICY</span><span>   STATUS</span><span>   CLAIM</span><span>                         STORAGECLASS</span><span>   REASON</span><span>   AGE</span></span>
<span data-line=""><span>pvc-913d0700-ae39-4be5-96e6-afddf86ae64a</span><span>   20</span><span>Gi</span><span>       RWO</span><span>            Delete</span><span>           Bound</span><span>    default/pgdata-pg-cluster-0</span><span>   local-path</span><span>              8</span><span>m3s</span></span>
<span data-line=""><span>pvc-efec9b6a-24d1-46ff-9039-749d6e72841c</span><span>   20</span><span>Gi</span><span>       RWO</span><span>            Delete</span><span>           Bound</span><span>    default/pgdata-pg-cluster-1</span><span>   local-path</span><span>              7</span><span>m56s</span></span></code></pre></figure>
<p>Great, each node has a 20Gi replica of the DB in case one of the nodes dies, the data consumers shouldn’t notice any degradation.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span># view running K8s services</span></span>
<span data-line=""><span>&gt;</span><span> k get svc</span></span>
<span data-line=""> </span>
<span data-line=""><span># omitting other services for brevity</span></span>
<span data-line=""><span>...</span></span>
<span data-line=""><span>pg-cluster</span><span>             LoadBalancer</span><span>   10.43</span><span>.112.7</span><span>     192.168</span><span>.70.10,192.168.70.20,192.168.70.30</span><span>   5432</span><span>:32461/TCP</span><span>   10</span><span>m</span></span></code></pre></figure>
<p>Alas the benefit of the <code>enableMasterLoadBalancer</code> parameter. Since I’m running on a bare-metal server, this param allocates me the node private IPs as the externally accessible addresses instead of just the internal clusterIP.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span>&gt;</span><span> k get pods</span></span>
<span data-line=""> </span>
<span data-line=""><span># omitting other pods for brevity</span></span>
<span data-line=""><span>postgres-operator-77f6d658c-mnlj6</span><span>       1</span><span>/1</span><span>     Running</span><span>   0</span><span>          78</span><span>m</span></span>
<span data-line=""><span>postgres-operator-ui-7bf9676b84-824j6</span><span>   1</span><span>/1</span><span>     Running</span><span>   0</span><span>          76</span><span>m</span></span>
<span data-line=""><span>pg-cluster-1</span><span>                            1</span><span>/1</span><span>     Running</span><span>   0</span><span>          6</span><span>m38s</span></span>
<span data-line=""><span>pg-cluster-0</span><span>                            1</span><span>/1</span><span>     Running</span><span>   0</span><span>          6</span><span>m19s</span></span></code></pre></figure>
<p>In total we have:</p>
<ul>
<li>Postgres operator for managing the cluster</li>
<li>Postgres operator UI</li>
<li>Cluster 0 running on node 2</li>
<li>Cluster 1 running on node 3</li>
</ul>
<h2 id="connecting-to-the-database">Connecting to the database</h2>
<p>Let’s connect to the DB. You can use whatever client you like, in my case I’ll use <code>PGAdmin</code>, but <code>psql</code> or something similar would suffice.</p>

<p>Typically you would have a load balancer IP that balances across your nodes running Postgres, but we’ll validate with just a couple nodes independently and validate the data is replicated.</p>
<p>The <code>password</code> is auto generated and placed in a K8s secret. View it with:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span># find the secret for the user you specified</span></span>
<span data-line=""><span>k</span><span> get</span><span> secrets</span></span>
<span data-line=""> </span>
<span data-line=""><span># export the secret to stdout and base64 decode it so we can use within our DB client</span></span>
<span data-line=""><span>kubectl</span><span> get</span><span> secret</span><span> schachte.pg-cluster.credentials.postgresql.acid.zalan.do</span><span> -o</span><span> jsonpath=</span><span>&#34;{.data.password}&#34;</span><span> |</span><span> base64</span><span> --decode</span></span></code></pre></figure>
<p>I’m going to create a <code>blog</code> database and add a <code>comments</code> table with dummy data as our starting point.</p>
<figure data-rehype-pretty-code-figure=""><figcaption id="root-fig-b67h2abg"> dummyCommentsFixtures.sql <!-- <span class="custom-figcaption__controls__lang">{lang}</span> -->  </figcaption> <pre tabindex="0" data-language="sql" data-theme=""><code data-line-numbers="" data-language="sql" data-theme="" data-line-numbers-max-digits="2"><span data-line=""><span>CREATE</span><span> TABLE</span><span> comments</span><span> (</span></span>
<span data-line=""><span>  id </span><span>SERIAL</span><span> PRIMARY KEY</span><span>, </span></span>
<span data-line=""><span>  email </span><span>VARCHAR</span><span>(</span><span>255</span><span>) </span><span>NOT NULL</span><span>, </span></span>
<span data-line=""><span>  date</span><span> TIMESTAMP</span><span> NOT NULL</span><span>, </span></span>
<span data-line=""><span>  name</span><span> VARCHAR</span><span>(</span><span>255</span><span>) </span><span>NOT NULL</span><span>, </span></span>
<span data-line=""><span>  comment </span><span>TEXT</span><span> NOT NULL</span></span>
<span data-line=""><span>);</span></span>
<span data-line=""> </span>
<span data-line=""><span>-- Insert 20 dummy records</span></span>
<span data-line=""><span>INSERT INTO</span><span> comments (email, </span><span>date</span><span>, </span><span>name</span><span>, comment) </span></span>
<span data-line=""><span>SELECT</span><span> </span></span>
<span data-line=""><span>  md5(</span></span>
<span data-line=""><span>    random</span><span>()</span><span>:: </span><span>text</span></span>
<span data-line=""><span>  ):: </span><span>varchar</span><span>(</span><span>255</span><span>) </span><span>AS</span><span> email, </span></span>
<span data-line=""><span>  TIMESTAMP</span><span> &#39;2024-01-01&#39;</span><span> +</span><span> (</span></span>
<span data-line=""><span>    random</span><span>()</span><span> *</span><span> (</span></span>
<span data-line=""><span>      TIMESTAMP</span><span> &#39;2024-12-31&#39;</span><span> -</span><span> TIMESTAMP</span><span> &#39;2024-01-01&#39;</span></span>
<span data-line=""><span>    )</span></span>
<span data-line=""><span>  ) </span><span>AS</span><span> date</span><span>, </span></span>
<span data-line=""><span>  md5(</span></span>
<span data-line=""><span>    random</span><span>()</span><span>:: </span><span>text</span></span>
<span data-line=""><span>  ):: </span><span>varchar</span><span>(</span><span>255</span><span>) </span><span>AS</span><span> name</span><span>, </span></span>
<span data-line=""><span>  md5(</span></span>
<span data-line=""><span>    random</span><span>()</span><span>:: </span><span>text</span></span>
<span data-line=""><span>  ):: </span><span>varchar</span><span>(</span><span>255</span><span>) </span><span>AS</span><span> comment </span></span>
<span data-line=""><span>FROM</span><span> </span></span>
<span data-line=""><span>  generate_series</span><span>(</span><span>1</span><span>, </span><span>20</span><span>);</span></span></code></pre></figure>
<p>This will push 20 dummy records into the <code>comments</code> table.</p>
<h2 id="failover-test">Failover test</h2>
<p>We won’t deploy any crazy chaos testing automation, but we will simulate node failure to ensure failover is working without degradation. In my case, I have 2 nodes:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="bash" data-theme=""><code data-language="bash" data-theme=""><span data-line=""><span>❯</span><span> k</span><span> describe</span><span> pod/pg-cluster-1</span><span> |</span><span> grep</span><span> -i</span><span> &#34;Node:&#34;</span></span>
<span data-line=""><span>Node:</span><span>             node2/192.168.70.20</span></span>
<span data-line=""> </span>
<span data-line=""><span>❯</span><span> k</span><span> describe</span><span> pod/pg-cluster-0</span><span> |</span><span> grep</span><span> -i</span><span> &#34;Node:&#34;</span></span>
<span data-line=""><span>Node:</span><span>             node3/192.168.70.30</span></span></code></pre></figure>
<p>If we delete <code>pg-cluster-1</code> using <code>k delete pod/pg-cluster-1</code>, we should still be able to read &amp; write to the DB. Once the node is deleted, it will be automatically scheduled to recreate itself based on the instance count specified in the Zalando deployment yaml.</p>
<h2 id="takeaways">Takeaways</h2>
<p>I wouldn’t deploy this directly to production, but I think it gets a very good baseline setup. I would say next important steps would be:</p>
<ul>
<li>metrics with Prometheus</li>
<li>alerting via ntfy</li>
<li>better load balancing across nodes</li>
</ul> </div></div>
  </body>
</html>
