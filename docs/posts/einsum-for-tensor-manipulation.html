<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/einsum-for-tensor-manipulation/">Original</a>
    <h1>Einsum for Tensor Manipulation</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p><em>In the ethereal dance of the cosmos, where the arcane whispers intertwine with the silent echoes of unseen dimensions, the Ioun Stone of Mastery emerges as a beacon of unparalleled prowess. This luminescent orb, orbiting its bearer’s head, is a testament to the mastery of both magical and mathematical realms, offering a bridge between the manipulation of arcane energies and the intricate ballet of tensor mathematics. As the stone orbits, it casts a subtle glow, its presence a constant reminder of the dual dominion it grants over the spellbinding complexities of magic and the abstract elegance of multidimensional calculations, making the wielder a maestro of both mystical incantations and the unseen algebra of the universe.</em></p>
<figure><a href="https://swe-to-mle.pages.dev/posts/einsum-for-tensor-manipulation/ioun-stone.png" title="ioun-stone" data-thumbnail="/posts/einsum-for-tensor-manipulation/ioun-stone.png" data-sub-html="&lt;h2&gt;Ioun Stone of Mastery&lt;/h2&gt;&lt;p&gt;ioun-stone&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="/posts/einsum-for-tensor-manipulation/ioun-stone.png" data-srcset="/posts/einsum-for-tensor-manipulation/ioun-stone.png, /posts/einsum-for-tensor-manipulation/ioun-stone.png 1.5x, /posts/einsum-for-tensor-manipulation/ioun-stone.png 2x" data-sizes="auto" alt="/posts/einsum-for-tensor-manipulation/ioun-stone.png" width="1024" height="1024"/>
    </a><figcaption>Ioun Stone of Mastery</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>Study how the Ioun Stone powers work. Understand how Einsum operates over tensors.</p>
<h2 id="einsum-uses">Einsum Uses</h2>
<p>Einsum (and einops in general) is a great tool for manipulating tensors. In ML it is often used to implement matrix multiplication or dot products. The simplest case would look like:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>x</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>4</span><span>,</span> <span>5</span><span>))</span>
</span></span><span><span><span>y</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>5</span><span>,</span> <span>3</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span># the torch way</span>
</span></span><span><span><span>res</span> <span>=</span> <span>x</span> <span>@</span> <span>y</span>
</span></span><span><span>
</span></span><span><span><span># einsum way</span>
</span></span><span><span><span>res</span> <span>=</span> <span>einsum</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>&#39;a b, b c -&gt; a c&#39;</span><span>)</span>
</span></span></code></pre></div><p>Right now it looks like a verbose way of doing the same thing, but it sometimes presents the following advantages:</p>
<ul>
<li>documenting the tensor dimensions for ease of reading</li>
<li>implicit reordering of dimensions</li>
</ul>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>query</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>100</span><span>,</span> <span>20</span><span>,</span> <span>32</span><span>))</span>
</span></span><span><span><span>key</span>   <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>100</span><span>,</span> <span>20</span><span>,</span> <span>32</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span># the torch way</span>
</span></span><span><span><span>keyT</span> <span>=</span> <span>key</span><span>.</span><span>permute</span><span>((</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>))</span>
</span></span><span><span><span>res</span> <span>=</span> <span>query</span> <span>@</span> <span>keyT</span>
</span></span><span><span>
</span></span><span><span><span># einsum way</span>
</span></span><span><span><span>res2</span> <span>=</span> <span>dumbsum</span><span>(</span><span>query</span><span>,</span> <span>key</span><span>,</span> <span>&#39;batch seq_q d_model, batch seq_k d_model -&gt; batch seq_q seq_k&#39;</span><span>)</span>
</span></span></code></pre></div><h2 id="einsum-the-iterative-way">Einsum the Iterative way</h2>
<p>Conceptually it’s possible to think of einsum as bunch of nested loops:</p>
<ul>
<li>the first set of nested loops is used to index into the inputs and output.</li>
<li>the second set of nested loops for summing all the left over dimensions that are getting reduced.</li>
</ul>
<p>It could be written by hand as:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>result</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>((</span><span>10</span><span>,</span> <span>20</span><span>,</span> <span>20</span><span>))</span>
</span></span><span><span><span>for</span> <span>batch</span> <span>in</span> <span>range</span><span>(</span><span>10</span><span>):</span>
</span></span><span><span>  <span>for</span> <span>seq_q</span> <span>in</span> <span>range</span><span>(</span><span>20</span><span>):</span>
</span></span><span><span>    <span>for</span> <span>seq_k</span> <span>in</span> <span>range</span><span>(</span><span>20</span><span>):</span>
</span></span><span><span>      <span>tot</span> <span>=</span> <span>0</span>
</span></span><span><span>      <span>for</span> <span>d_model</span> <span>in</span> <span>range</span><span>(</span><span>32</span><span>):</span>
</span></span><span><span>        <span>tot</span> <span>+=</span> <span>query</span><span>[</span><span>batch</span><span>,</span> <span>seq_q</span><span>,</span> <span>d_model</span><span>]</span> <span>*</span> <span>key</span><span>[</span><span>batch</span><span>,</span> <span>seq_k</span><span>,</span> <span>d_model</span><span>]</span>
</span></span><span><span>      <span>result</span><span>[</span><span>batch</span><span>,</span> <span>seq_q</span><span>,</span> <span>seq_k</span><span>]</span> <span>=</span> <span>tot</span>
</span></span></code></pre></div><p>One way to generate these nested loops is to use recursion:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>dumbsum</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>shapes</span><span>):</span>
</span></span><span><span>  <span>&#39;&#39;&#39;
</span></span></span><span><span><span>  dumb implem for my own intuition building sake, with absolutely no value for real life use.
</span></span></span><span><span><span>  not vectorized, and do not handle splitting / merging / creating extra dim.
</span></span></span><span><span><span>  
</span></span></span><span><span><span>  the main idea is to:
</span></span></span><span><span><span>  1- generate nested loops for indexing for each dim in the output
</span></span></span><span><span><span>  2- generate nexted loops for summing everything else
</span></span></span><span><span><span>  e.g. &#39;a b c d e, a c e -&gt; a d b&#39;
</span></span></span><span><span><span>  for a in range(x.shape[0]):
</span></span></span><span><span><span>    for d in range(x.shape[3]):
</span></span></span><span><span><span>      for b in range(x.shape[1]):
</span></span></span><span><span><span>        tot = 0
</span></span></span><span><span><span>        for c in range(x.shape[2]):
</span></span></span><span><span><span>          for e in range(x.shape[4]):
</span></span></span><span><span><span>            tot += x[a, b, c, d, e] * y[a, c, e]
</span></span></span><span><span><span>        res[a, d, b] = tot
</span></span></span><span><span><span>
</span></span></span><span><span><span>  in practice I initialize res to a tensor of zero, and update it in place instead of accumulating in a tot
</span></span></span><span><span><span>  res[a, d, b] += x[a, b, c, d, e] * y[a, c, e]
</span></span></span><span><span><span>  &#39;&#39;&#39;</span>
</span></span><span><span>  <span>def</span> <span>split_shape</span><span>(</span><span>shape</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>[</span><span>x</span> <span>for</span> <span>x</span> <span>in</span> <span>shape</span><span>.</span><span>split</span><span>(</span><span>&#39; &#39;</span><span>)</span> <span>if</span> <span>x</span><span>]</span>
</span></span><span><span>  <span>def</span> <span>parse</span><span>(</span><span>shapes</span><span>):</span>
</span></span><span><span>    <span>assert</span> <span>shapes</span><span>.</span><span>count</span><span>(</span><span>&#39;,&#39;</span><span>)</span> <span>==</span> <span>1</span>
</span></span><span><span>    <span>assert</span> <span>shapes</span><span>.</span><span>count</span><span>(</span><span>&#39;-&gt;&#39;</span><span>)</span> <span>==</span> <span>1</span>
</span></span><span><span>    <span>shapes</span><span>,</span> <span>res_shape</span> <span>=</span> <span>shapes</span><span>.</span><span>split</span><span>(</span><span>&#39;-&gt;&#39;</span><span>)</span>
</span></span><span><span>    <span>x_shape</span><span>,</span> <span>y_shape</span> <span>=</span> <span>shapes</span><span>.</span><span>split</span><span>(</span><span>&#39;,&#39;</span><span>)</span>
</span></span><span><span>    <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span> <span>=</span> <span>(</span><span>split_shape</span><span>(</span><span>s</span><span>)</span> <span>for</span> <span>s</span> <span>in</span> <span>(</span><span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>))</span>
</span></span><span><span>    <span>sum_shape</span> <span>=</span> <span>list</span><span>(</span><span>set</span><span>(</span><span>x_shape</span> <span>+</span> <span>y_shape</span><span>)</span> <span>-</span> <span>set</span><span>(</span><span>res_shape</span><span>))</span>
</span></span><span><span>    <span>assert</span> <span>set</span><span>(</span><span>res_shape</span><span>)</span><span>.</span><span>issubset</span><span>(</span><span>set</span><span>(</span><span>x_shape</span> <span>+</span> <span>y_shape</span><span>))</span>
</span></span><span><span>    <span>return</span> <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>,</span> <span>sum_shape</span>
</span></span><span><span>  <span>def</span> <span>build_dim_lookup</span><span>(</span><span>t</span><span>,</span> <span>t_shape</span><span>,</span> <span>lookup</span><span>=</span><span>None</span><span>):</span>
</span></span><span><span>    <span>if</span> <span>not</span> <span>lookup</span><span>:</span> <span>lookup</span> <span>=</span> <span>{}</span>
</span></span><span><span>    <span>dims</span> <span>=</span> <span>t</span><span>.</span><span>shape</span>
</span></span><span><span>    <span>for</span> <span>dim</span><span>,</span> <span>letter</span> <span>in</span> <span>zip</span><span>(</span><span>dims</span><span>,</span> <span>t_shape</span><span>):</span>
</span></span><span><span>      <span>assert</span> <span>lookup</span><span>.</span><span>get</span><span>(</span><span>letter</span><span>,</span> <span>dim</span><span>)</span> <span>==</span> <span>dim</span>
</span></span><span><span>      <span>lookup</span><span>[</span><span>letter</span><span>]</span> <span>=</span> <span>dim</span>
</span></span><span><span>    <span>return</span> <span>lookup</span>
</span></span><span><span>  <span>def</span> <span>iterate</span><span>(</span><span>shape</span><span>,</span> <span>sum_shape</span><span>,</span> <span>fn</span><span>,</span> <span>lookup</span><span>,</span> <span>indexes</span><span>):</span>
</span></span><span><span>    <span>if</span> <span>not</span> <span>shape</span><span>:</span>
</span></span><span><span>      <span>iterate_sum</span><span>(</span><span>sum_shape</span><span>[:],</span> <span>fn</span><span>,</span> <span>lookup</span><span>,</span> <span>indexes</span><span>)</span>
</span></span><span><span>      <span>return</span>
</span></span><span><span>    <span>dim</span> <span>=</span> <span>shape</span><span>.</span><span>pop</span><span>(</span><span>-</span><span>1</span><span>)</span>
</span></span><span><span>    <span># print(f&#39;iterate over → {dim}&#39;)</span>
</span></span><span><span>    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>lookup</span><span>[</span><span>dim</span><span>]):</span>
</span></span><span><span>      <span>indexes</span><span>[</span><span>dim</span><span>]</span> <span>=</span> <span>i</span>
</span></span><span><span>      <span>iterate</span><span>(</span><span>shape</span><span>[:],</span> <span>sum_shape</span><span>,</span> <span>fn</span><span>,</span> <span>lookup</span><span>,</span> <span>indexes</span><span>)</span>
</span></span><span><span>  <span>def</span> <span>iterate_sum</span><span>(</span><span>sum_shape</span><span>,</span> <span>fn</span><span>,</span> <span>lookup</span><span>,</span> <span>indexes</span><span>):</span>
</span></span><span><span>    <span>if</span> <span>not</span> <span>sum_shape</span><span>:</span>
</span></span><span><span>      <span>fn</span><span>(</span><span>indexes</span><span>)</span>
</span></span><span><span>      <span>return</span>
</span></span><span><span>    <span>dim</span> <span>=</span> <span>sum_shape</span><span>.</span><span>pop</span><span>(</span><span>-</span><span>1</span><span>)</span>
</span></span><span><span>    <span># print(f&#39;sum over → {dim}&#39;)</span>
</span></span><span><span>    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>lookup</span><span>[</span><span>dim</span><span>]):</span>
</span></span><span><span>      <span>indexes</span><span>[</span><span>dim</span><span>]</span> <span>=</span> <span>i</span>
</span></span><span><span>      <span>iterate_sum</span><span>(</span><span>sum_shape</span><span>[:],</span> <span>fn</span><span>,</span> <span>lookup</span><span>,</span> <span>indexes</span><span>)</span>
</span></span><span><span>  <span>def</span> <span>ind</span><span>(</span><span>t_shape</span><span>,</span> <span>indexes</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>(</span><span>indexes</span><span>[</span><span>dim</span><span>]</span> <span>for</span> <span>dim</span> <span>in</span> <span>t_shape</span><span>)</span>
</span></span><span><span>  <span>def</span> <span>close_sum</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>res</span><span>,</span> <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>):</span>
</span></span><span><span>    <span>def</span> <span>fn</span><span>(</span><span>indexes</span><span>):</span>
</span></span><span><span>      <span># print(f&#39;res[{tuple(ind(res_shape, indexes))}] += x[{tuple(ind(x_shape, indexes))}] * y[{tuple(ind(y_shape, indexes))}]&#39;)</span>
</span></span><span><span>      <span>res</span><span>[</span><span>*</span><span>ind</span><span>(</span><span>res_shape</span><span>,</span> <span>indexes</span><span>)]</span> <span>+=</span> <span>x</span><span>[</span><span>*</span><span>ind</span><span>(</span><span>x_shape</span><span>,</span> <span>indexes</span><span>)]</span> <span>*</span> <span>y</span><span>[</span><span>*</span><span>ind</span><span>(</span><span>y_shape</span><span>,</span> <span>indexes</span><span>)]</span>
</span></span><span><span>    <span>return</span> <span>fn</span>
</span></span><span><span>
</span></span><span><span>  <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>,</span> <span>sum_shape</span> <span>=</span> <span>parse</span><span>(</span><span>shapes</span><span>)</span>
</span></span><span><span>  <span>assert</span> <span>len</span><span>(</span><span>x_shape</span><span>)</span> <span>==</span> <span>x</span><span>.</span><span>dim</span><span>()</span>
</span></span><span><span>  <span>assert</span> <span>len</span><span>(</span><span>y_shape</span><span>)</span> <span>==</span> <span>y</span><span>.</span><span>dim</span><span>()</span>
</span></span><span><span>  <span>lookup</span> <span>=</span> <span>build_dim_lookup</span><span>(</span><span>x</span><span>,</span> <span>x_shape</span><span>)</span>
</span></span><span><span>  <span>lookup</span> <span>=</span> <span>build_dim_lookup</span><span>(</span><span>y</span><span>,</span> <span>y_shape</span><span>,</span> <span>lookup</span><span>=</span><span>lookup</span><span>)</span>
</span></span><span><span>  <span>res</span> <span>=</span> <span>t</span><span>.</span><span>zeros</span><span>(</span><span>tuple</span><span>(</span><span>lookup</span><span>[</span><span>s</span><span>]</span> <span>for</span> <span>s</span> <span>in</span> <span>res_shape</span><span>))</span>
</span></span><span><span>  <span>fn</span> <span>=</span> <span>close_sum</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>res</span><span>,</span> <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>)</span>
</span></span><span><span>  <span>iterate</span><span>(</span><span>res_shape</span><span>[:],</span> <span>sum_shape</span><span>[:],</span> <span>fn</span><span>,</span> <span>lookup</span><span>,</span> <span>{})</span>
</span></span><span><span>  <span>return</span> <span>res</span>
</span></span></code></pre></div><h2 id="einsum-vectorized">Einsum Vectorized</h2>
<p>The loop version is great for intuition building, but it is extremely slow. Another way to implement einsum is to compose vectorized torch operations.</p>
<p>By hand it would look something like:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>query</span> <span>=</span> <span>query</span><span>[</span><span>...</span><span>,</span> <span>None</span><span>]</span> <span># add a seq_k dimension</span>
</span></span><span><span><span>key</span> <span>=</span> <span>key</span><span>[</span><span>...</span><span>,</span> <span>None</span><span>]</span>     <span># add a seq_q dimension</span>
</span></span><span><span><span>query</span> <span>=</span> <span>query</span><span>.</span><span>permute</span><span>((</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>))</span> <span># align the dimensions as: batch, seq_q, seq_k, d_model</span>
</span></span><span><span><span>key</span> <span>=</span> <span>key</span><span>.</span><span>permute</span><span>((</span><span>0</span><span>,</span> <span>3</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>))</span>     <span># align the dimensions as: batch, seq_q, seq_k, d_model </span>
</span></span><span><span><span>product</span> <span>=</span> <span>query</span> <span>*</span> <span>key</span> <span># multiply element wise using implicit broadcasting</span>
</span></span><span><span><span>result</span> <span>=</span> <span>product</span><span>.</span><span>sum</span><span>((</span><span>3</span><span>))</span> <span># reduce the extra dimension out</span>
</span></span></code></pre></div><p>Which in code could look a little something like:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>dumbsum_vectorized</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>shapes</span><span>):</span>
</span></span><span><span>  <span>&#39;&#39;&#39;
</span></span></span><span><span><span>  vectorize it, still do not handle splitting / merging / creating extra dim.
</span></span></span><span><span><span>  my vectorized also does not handle repeated dim (e.g. &#39;a a b, a a c -&gt; a a&#39;).
</span></span></span><span><span><span>  
</span></span></span><span><span><span>  the main idea is to:
</span></span></span><span><span><span>  1- align the dimensions of x and y, completing the holes with fake `1` dimensions
</span></span></span><span><span><span>  2- multiply x and y
</span></span></span><span><span><span>  3- sum out the extra dims
</span></span></span><span><span><span>  e.g. &#39;a c d e, a c e -&gt; a d b&#39;
</span></span></span><span><span><span>  # align dims
</span></span></span><span><span><span>  x = reshape(&#39;a c d e -&gt; a 1 c d e&#39;)
</span></span></span><span><span><span>  y = reshape(&#39;a c e   -&gt; a 1 c 1 e&#39;)
</span></span></span><span><span><span>  # order dims
</span></span></span><span><span><span>  x = reshape(&#39;a 1 c d e -&gt; a d 1 c e&#39;)
</span></span></span><span><span><span>  y = reshape(&#39;a 1 c 1 e -&gt; a 1 1 c e&#39;)
</span></span></span><span><span><span>  # mult and sum
</span></span></span><span><span><span>  res = x * y
</span></span></span><span><span><span>  res = res.sum((3, 4))
</span></span></span><span><span><span>  &#39;&#39;&#39;</span>
</span></span><span><span>  <span>def</span> <span>split_shape</span><span>(</span><span>shape</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>[</span><span>x</span> <span>for</span> <span>x</span> <span>in</span> <span>shape</span><span>.</span><span>split</span><span>(</span><span>&#39; &#39;</span><span>)</span> <span>if</span> <span>x</span><span>]</span>
</span></span><span><span>  <span>def</span> <span>parse</span><span>(</span><span>shapes</span><span>):</span>
</span></span><span><span>    <span>assert</span> <span>shapes</span><span>.</span><span>count</span><span>(</span><span>&#39;,&#39;</span><span>)</span> <span>==</span> <span>1</span>
</span></span><span><span>    <span>assert</span> <span>shapes</span><span>.</span><span>count</span><span>(</span><span>&#39;-&gt;&#39;</span><span>)</span> <span>==</span> <span>1</span>
</span></span><span><span>    <span>shapes</span><span>,</span> <span>res_shape</span> <span>=</span> <span>shapes</span><span>.</span><span>split</span><span>(</span><span>&#39;-&gt;&#39;</span><span>)</span>
</span></span><span><span>    <span>x_shape</span><span>,</span> <span>y_shape</span> <span>=</span> <span>shapes</span><span>.</span><span>split</span><span>(</span><span>&#39;,&#39;</span><span>)</span>
</span></span><span><span>    <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span> <span>=</span> <span>(</span><span>split_shape</span><span>(</span><span>s</span><span>)</span> <span>for</span> <span>s</span> <span>in</span> <span>(</span><span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>))</span>
</span></span><span><span>    <span>sum_shape</span> <span>=</span> <span>list</span><span>(</span><span>set</span><span>(</span><span>x_shape</span> <span>+</span> <span>y_shape</span><span>)</span> <span>-</span> <span>set</span><span>(</span><span>res_shape</span><span>))</span>
</span></span><span><span>    <span>assert</span> <span>set</span><span>(</span><span>res_shape</span><span>)</span><span>.</span><span>issubset</span><span>(</span><span>set</span><span>(</span><span>x_shape</span> <span>+</span> <span>y_shape</span><span>))</span>
</span></span><span><span>    <span>return</span> <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>,</span> <span>sum_shape</span>
</span></span><span><span>  <span>def</span> <span>build_dim_pos_lookup</span><span>(</span><span>t_shape</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>{</span><span>letter</span><span>:</span> <span>dim</span> <span>for</span> <span>dim</span><span>,</span> <span>letter</span> <span>in</span> <span>enumerate</span><span>(</span><span>t_shape</span><span>)}</span>
</span></span><span><span>  <span>def</span> <span>expand</span><span>(</span><span>t</span><span>,</span> <span>t_shape</span><span>,</span> <span>merged</span><span>):</span>
</span></span><span><span>    <span>lookup</span> <span>=</span> <span>build_dim_pos_lookup</span><span>(</span><span>t_shape</span><span>)</span>
</span></span><span><span>    <span>ind</span> <span>=</span> <span>len</span><span>(</span><span>lookup</span><span>)</span>
</span></span><span><span>    <span>for</span> <span>dim</span> <span>in</span> <span>merged</span><span>:</span>
</span></span><span><span>      <span>if</span> <span>dim</span> <span>not</span> <span>in</span> <span>lookup</span><span>:</span>
</span></span><span><span>        <span>t</span> <span>=</span> <span>t</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span>
</span></span><span><span>        <span>lookup</span><span>[</span><span>dim</span><span>]</span> <span>=</span> <span>ind</span>
</span></span><span><span>        <span>ind</span> <span>+=</span> <span>1</span>
</span></span><span><span>    <span>return</span> <span>t</span><span>,</span> <span>lookup</span>
</span></span><span><span>  <span>def</span> <span>align</span><span>(</span><span>t</span><span>,</span> <span>lookup</span><span>,</span> <span>res_lookup</span><span>):</span>
</span></span><span><span>    <span># rely on dict being ordered (python &gt;= 3.7)</span>
</span></span><span><span>    <span>permuted_dims</span> <span>=</span> <span>tuple</span><span>(</span><span>lookup</span><span>[</span><span>dim</span><span>]</span> <span>for</span> <span>dim</span> <span>in</span> <span>res_lookup</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>t</span><span>.</span><span>permute</span><span>(</span><span>permuted_dims</span><span>)</span>
</span></span><span><span>  <span>def</span> <span>dims_to_sum</span><span>(</span><span>res_shape</span><span>,</span> <span>res_lookup</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>tuple</span><span>(</span><span>range</span><span>(</span><span>len</span><span>(</span><span>res_shape</span><span>),</span> <span>len</span><span>(</span><span>res_lookup</span><span>)))</span>
</span></span><span><span>
</span></span><span><span>  <span>x_shape</span><span>,</span> <span>y_shape</span><span>,</span> <span>res_shape</span><span>,</span> <span>sum_shape</span> <span>=</span> <span>parse</span><span>(</span><span>shapes</span><span>)</span>
</span></span><span><span>  <span>assert</span> <span>len</span><span>(</span><span>x_shape</span><span>)</span> <span>==</span> <span>x</span><span>.</span><span>dim</span><span>()</span>
</span></span><span><span>  <span>assert</span> <span>len</span><span>(</span><span>y_shape</span><span>)</span> <span>==</span> <span>y</span><span>.</span><span>dim</span><span>()</span>
</span></span><span><span>  <span>merged</span> <span>=</span> <span>set</span><span>(</span><span>x_shape</span> <span>+</span> <span>y_shape</span><span>)</span>
</span></span><span><span>  <span>x</span><span>,</span> <span>x_lookup</span> <span>=</span> <span>expand</span><span>(</span><span>x</span><span>,</span> <span>x_shape</span><span>,</span> <span>merged</span><span>)</span>
</span></span><span><span>  <span>y</span><span>,</span> <span>y_lookup</span> <span>=</span> <span>expand</span><span>(</span><span>y</span><span>,</span> <span>y_shape</span><span>,</span> <span>merged</span><span>)</span>
</span></span><span><span>  <span>_</span><span>,</span> <span>res_lookup</span> <span>=</span> <span>expand</span><span>(</span><span>t</span><span>.</span><span>zeros</span><span>((</span><span>0</span><span>)),</span> <span>res_shape</span><span>,</span> <span>merged</span><span>)</span>
</span></span><span><span>  <span>x</span> <span>=</span> <span>align</span><span>(</span><span>x</span><span>,</span> <span>x_lookup</span><span>,</span> <span>res_lookup</span><span>)</span>
</span></span><span><span>  <span>y</span> <span>=</span> <span>align</span><span>(</span><span>y</span><span>,</span> <span>y_lookup</span><span>,</span> <span>res_lookup</span><span>)</span>
</span></span><span><span>  <span>res</span> <span>=</span> <span>x</span> <span>*</span> <span>y</span>
</span></span><span><span>  <span>dims</span> <span>=</span> <span>dims_to_sum</span><span>(</span><span>res_shape</span><span>,</span> <span>res_lookup</span><span>)</span>
</span></span><span><span>  <span>if</span> <span>dims</span><span>:</span> <span>res</span> <span>=</span> <span>res</span><span>.</span><span>sum</span><span>(</span><span>dims</span><span>)</span>
</span></span><span><span>  <span>return</span> <span>res</span>
</span></span></code></pre></div><h2 id="compare-both">Compare both</h2>
<h3 id="correctness">Correctness</h3>
<p>We can verify that both versions are producing the same results as the original einsum:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>torch</span><span>,</span> <span>einops</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>einops_test</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>pattern</span><span>):</span>
</span></span><span><span>  <span>a</span> <span>=</span> <span>dumbsum</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>pattern</span><span>)</span>
</span></span><span><span>  <span>b</span> <span>=</span> <span>dumbsum_vectorized</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>pattern</span><span>)</span>
</span></span><span><span>  <span>c</span> <span>=</span> <span>einops</span><span>.</span><span>einsum</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>pattern</span><span>)</span>
</span></span><span><span>  <span>assert</span> <span>a</span><span>.</span><span>allclose</span><span>(</span><span>c</span><span>)</span>
</span></span><span><span>  <span>assert</span> <span>b</span><span>.</span><span>allclose</span><span>(</span><span>c</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>x</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>10</span><span>,</span> <span>5</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>))</span>
</span></span><span><span><span>y</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>3</span><span>,</span> <span>10</span><span>,</span> <span>5</span><span>,</span> <span>7</span><span>))</span>
</span></span><span><span><span>einops_test</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>&#39;a b c d, d a b e -&gt; b e c&#39;</span><span>)</span>
</span></span><span><span><span>einops_test</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>&#39;a b c d, d a b e -&gt; a b c d e&#39;</span><span>)</span>
</span></span><span><span><span>einops_test</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>&#39;a b c d, d a b e -&gt; e d c b a&#39;</span><span>)</span>
</span></span><span><span><span>einops_test</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>&#39;a b c d, d a b e -&gt; a&#39;</span><span>)</span>
</span></span><span><span><span>einops_test</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>&#39;a b c d, d a b e -&gt;&#39;</span><span>)</span>
</span></span><span><span><span>einops_test</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>&#39;a b c d, d a b e -&gt; a e&#39;</span><span>)</span>
</span></span></code></pre></div><h3 id="speed">Speed</h3>
<p>Timing the iterative version:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>%%</span><span>time</span>
</span></span><span><span><span>query</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>100</span><span>,</span> <span>20</span><span>,</span> <span>32</span><span>))</span>
</span></span><span><span><span>key</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>100</span><span>,</span> <span>20</span><span>,</span> <span>32</span><span>))</span>
</span></span><span><span><span>_</span> <span>=</span> <span>dumbsum</span><span>(</span><span>query</span><span>,</span> <span>key</span><span>,</span> <span>&#39;batch seq_q d_model, batch seq_k d_model -&gt; batch seq_q seq_k&#39;</span><span>)</span>
</span></span></code></pre></div><pre tabindex="0"><code>CPU times: total: 9.58 s
Wall time: 31.3 s
</code></pre><p>Against the vectorized version:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>%%</span><span>time</span>
</span></span><span><span><span>query</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>100</span><span>,</span> <span>20</span><span>,</span> <span>32</span><span>))</span>
</span></span><span><span><span>key</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>((</span><span>100</span><span>,</span> <span>20</span><span>,</span> <span>32</span><span>))</span>
</span></span><span><span><span>_</span> <span>=</span> <span>dumbsum_vectorized</span><span>(</span><span>query</span><span>,</span> <span>key</span><span>,</span> <span>&#39;batch seq_q d_model, batch seq_k d_model -&gt; batch seq_q seq_k&#39;</span><span>)</span>
</span></span></code></pre></div><pre tabindex="0"><code>CPU times: total: 0 ns
Wall time: 975 µs
</code></pre><p>Demonstrates the significant speedup brought by using vectorized code.</p>
<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/ml-misc/blob/master/einsum-intuition.ipynb" target="_blank" rel="noopener noreffer ">https://github.com/peluche/ml-misc/blob/master/einsum-intuition.ipynb</a></p>
</div></div>
  </body>
</html>
