<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://seanpedersen.github.io/posts/structure-of-neural-latent-space">Original</a>
    <h1>The Structure of Neural Embeddings</h1>
    
    <div id="readability-page-1" class="page"><div><p>A small collection of insights on the structure of embeddings (latent spaces) produced by deep neural networks.</p>
<p><strong>Manifold Hypothesis</strong>: High-dimensional data sampled from natural (real-world) processes lies in a low-dimensional manifold.</p>
<ul>
<li><a href="https://arxiv.org/abs/2208.11665">https://arxiv.org/abs/2208.11665</a></li>
</ul>
<p><strong>Hierarchical Organization</strong>: Features organize hierarchically across layers - earlier layers capture low-level (small context) features while deeper layers represent increasingly abstract (large context) concepts.</p>
<ul>
<li><a href="https://colah.github.io/posts/2015-01-Visualizing-Representations/">https://colah.github.io/posts/2015-01-Visualizing-Representations/</a></li>
</ul>
<p><strong>Linear Hypothesis</strong>: Neural networks represent features as linear directions in their activation space.</p>
<ul>
<li><a href="https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/</a></li>
<li><a href="https://www.lesswrong.com/posts/tojtPCCRpKLSHBdpn/the-strong-feature-hypothesis-could-be-wrong">https://www.lesswrong.com/posts/tojtPCCRpKLSHBdpn/the-strong-feature-hypothesis-could-be-wrong</a></li>
</ul>
<p><strong>Superposition Hypothesis</strong>: Neural nets represent more “independent” features than a layer has neurons (dimensions) by representing features as a linear combination of neurons.</p>
<ul>
<li><a href="https://transformer-circuits.pub/2022/toy_model/index.html">https://transformer-circuits.pub/2022/toy_model/index.html</a></li>
</ul>
<p><strong>Universality Hypothesis</strong>: Circuits reappear across different models for the same data.</p>
<ul>
<li><a href="https://www.lesswrong.com/posts/5CApLZiHGkt37nRQ2/an-111-the-circuits-hypotheses-for-deep-learning">https://www.lesswrong.com/posts/5CApLZiHGkt37nRQ2/an-111-the-circuits-hypotheses-for-deep-learning</a></li>
</ul>
<p><strong>Adversarial Vulnerability</strong>: Small changes in input space can cause large shifts in embeddings and therefore also in predictions made from them, suggesting the learned manifolds have irregular geometric properties.</p>
<ul>
<li><a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a></li>
</ul>
<p><strong>Neural Collapse</strong>: After extensive training, class features in the final layer cluster tightly around their means, with the network&#39;s classification weights aligning with these mean directions. Within-class variation becomes minimal compared to between-class differences, effectively creating distinct, well-separated clusters for each class.</p>
<ul>
<li><a href="https://arxiv.org/abs/2008.08186">https://arxiv.org/abs/2008.08186</a></li>
</ul>
</div></div>
  </body>
</html>
