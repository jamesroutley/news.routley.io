<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/policy-gradient/GRPO-Zero">Original</a>
    <h1>Implementing DeepSeek R1&#39;s GRPO algorithm from scratch</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">GRPO training with minimal dependencies. We implement almost everything from scratch and only depend on <code>tokenizers</code> for tokenization and <code>pytorch</code> for training.</p>
<ul dir="auto">
<li>No <code>transformers</code> and <code>vLLM</code> dependencies!</li>
<li>The default config is set to run on a single A40 GPU (48GB VRAM) for a few hours to get good results. (A40 costs <code>$0.44</code> per hour if you rent it from RunPod.)</li>
<li>We support several improvements over the original GRPO algorithm from the <a href="https://arxiv.org/abs/2503.14476" rel="nofollow">DAPO project</a>, including:
<ul dir="auto">
<li><strong>Token-level policy gradient loss</strong>: every token is equally weighted in the policy gradient loss.</li>
<li><strong>Removing KL Divergence</strong>: the KL divergence is not used in the policy gradient loss. This reduces GPU memory usage as we no longer need the reference policy network.</li>
<li><strong>Overlong episode filtering</strong>: skips unfinished episodes that exceed context length limits. This stabilizes training. Though we disabled it by default to observe model learning under limited context length. Set <code>skip_unfinished_episodes</code> to <code>true</code> to enable it.</li>
</ul>
</li>
</ul>

<p dir="auto">Group Relative Policy Optimization (GRPO) is an algorithm proposed by Deepseek for training large language models with reinforcement learning. The idea is simple: for each question, we randomly sample multiple answers. The advantage of an answer is then defined as the normalized reward. This gets rid of the value estimation network. In particular, we implement the following algorithm:</p>
<ol dir="auto">
<li>For each training step, randomly sample <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$N$</math-renderer> questions <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$q_1, q_2, \cdots, q_N$</math-renderer>.</li>
<li>For each question <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$q_i$</math-renderer>, sample <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$M$</math-renderer> answers <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$a_{i,1}, a_{i,2}, \cdots, a_{i,M}$</math-renderer>.</li>
<li>Compute the reward <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$r_{i,j}$</math-renderer> for each answer <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$a_{i,j}$</math-renderer>.</li>
<li>Compute the mean and std of the rewards for each question <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$q_i$</math-renderer>.</li>
</ol>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$$
\begin{aligned}
\mu_i &amp;\leftarrow \text{mean}(r_{i,1}, r_{i,2}, \cdots, r_{i,M}) \\
\sigma_i &amp;\leftarrow \text{std}(r_{i,1}, r_{i,2}, \cdots, r_{i,M})
\end{aligned}
$$</math-renderer></p>
<ol start="5" dir="auto">
<li>For each token <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$t$</math-renderer> in the answer <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$a_{i,j}$</math-renderer>, compute the advantage as</li>
</ol>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$$A_{i,j}[t] \leftarrow \frac{r_{i,j} - \mu_i}{\sigma_i}$$</math-renderer></p>
<ol start="6" dir="auto">
<li>Compute policy gradient using PPO surrogate objective. For simplicity, we will only do one policy update per iteration, in which the gradient of the PPO objective is equivalent to following vanilla policy gradient estimation (per token).</li>
</ol>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$$
\nabla_\theta \log \pi_\theta(a_{i,j}[t]) \cdot A_{i,j}[t]
$$</math-renderer></p>
<ol start="7" dir="auto">
<li>Update the policy network <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="8295aa76bdd41bae827292fe3fdbec38">$\pi(\theta)$</math-renderer> using the gradient. Go back to step 1.</li>
</ol>

<p dir="auto">We are going to train the Qwen2.5 models on the <a href="https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4" rel="nofollow">CountDown task</a>. Given a list of 3 or 4 numbers and a target number, the model needs to generate a mathematical expression using simple arithmetic operations (+, -, *, /) that evaluates to the target number. For example:</p>
<div data-snippet-clipboard-copy-content="Question: Given 1 2 3 4 and a target number 11. Show an expression that evaluates to 11.
Answer: 1 + (2 * 3) + 4"><pre><code>Question: Given 1 2 3 4 and a target number 11. Show an expression that evaluates to 11.
Answer: 1 + (2 * 3) + 4
</code></pre></div>

<p dir="auto">To solve the CountDown task, we will use the GRPO algorithm to train the model to generate the chain of thought reasoning before generating the final expression. Specifically, the model is trained to follow the format:</p>
<div data-snippet-clipboard-copy-content="&lt;think&gt;Model step by step reasoning&lt;/think&gt;
&lt;answer&gt;Final answer&lt;/answer&gt;"><pre><code>&lt;think&gt;Model step by step reasoning&lt;/think&gt;
&lt;answer&gt;Final answer&lt;/answer&gt;
</code></pre></div>
<p dir="auto">The reward is the sum of two components:</p>
<ol dir="auto">
<li><strong>Format Reward</strong>: The model earns a reward of <code>0.1</code> when it correctly follows the specified format with thinking and answer tags, and <code>0</code> otherwise.</li>
<li><strong>Answer Reward</strong>: The model receives a reward of <code>1</code> if its final answer uses each provided number exactly once and correctly evaluates to the target value, otherwise it receives <code>0</code>.</li>
</ol>

<p dir="auto">We use the <code>Qwen2.5-3B-Instruct</code> model for training. To train the model, run the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# initialize the environment
pip install uv
uv sync

# install git-lfs
apt update; apt install git-lfs -y; git lfs install

# download the dataset
git clone https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4

# download the pretrained model
git clone https://huggingface.co/Qwen/Qwen2.5-3B-Instruct

# train the model
uv run train.py"><pre><span><span>#</span> initialize the environment</span>
pip install uv
uv sync

<span><span>#</span> install git-lfs</span>
apt update<span>;</span> apt install git-lfs -y<span>;</span> git lfs install

<span><span>#</span> download the dataset</span>
git clone https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4

<span><span>#</span> download the pretrained model</span>
git clone https://huggingface.co/Qwen/Qwen2.5-3B-Instruct

<span><span>#</span> train the model</span>
uv run train.py</pre></div>

<p dir="auto">This project builds upon the work of several outstanding projects:</p>
<ul dir="auto">
<li><a href="https://arxiv.org/abs/2402.03300" rel="nofollow">DeepSeekMath</a> for pioneering the GRPO algorithm.</li>
<li><a href="https://arxiv.org/abs/2503.14476" rel="nofollow">DAPO</a> for their enhancements to the original GRPO algorithm.</li>
<li><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a> for their implementation of GRPO and creation of the <a href="https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4" rel="nofollow">CountDown-Tasks-3to4</a> dataset.</li>
<li><a href="https://github.com/McGill-NLP/nano-aha-moment/tree/main">nano-aha-moment</a> for their clear implementation and tutorial on the GRPO algorithm.</li>
<li><a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct" rel="nofollow">Qwen2.5</a> for developing the high-quality pretrained model used in this project.</li>
</ul>
</article></div></div>
  </body>
</html>
