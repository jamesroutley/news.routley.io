<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/punica-ai/punica">Original</a>
    <h1>Punica: Serving multiple LoRA finetuned LLM as one</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="https://arxiv.org/abs/2310.18547" rel="nofollow">(paper)</a></p>
<h2 tabindex="-1" id="user-content-overview" dir="auto"><a href="#overview">Overview<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto"><a href="https://arxiv.org/abs/2106.09685" rel="nofollow">Low rank adapation</a> (LoRA) is a parameter efficient way to add new knowledge to a pretrained LLM. Although the pretrained LLM takes 100s of GB storage, a LoRA finetuned model only adds 1% storage and memory overhead. Punica enables running multiple LoRA finetuned models at the cost of running one.</p>
<p dir="auto">How?</p>
<p dir="auto">Assuming <code>W</code> of shape <code>[H1, H2]</code> is the weight of the pretrained model, LoRA adds two small matrices <code>A</code> of shape <code>[H1, r]</code> and <code>B</code> of <code>[r, H2]</code>. Running a input <code>x</code> on the finetuned model would be <code>y := x @ (W + A@B)</code>, which is the same as <code>y := x@W + x@A@B</code>.</p>
<p dir="auto">When there are <code>n</code> LoRA models, there will be <code>A1</code>, <code>B1</code>, <code>A2</code>, <code>B2</code>, ..., <code>An</code>, <code>Bn</code>.  Given a input batch <code>X := (x1,x2,...,xn)</code> that maps to each LoRA model, the output is <code>Y := X@W + (x1@A1@B1, x2@A2@B2, ..., xn@An@Bn)</code>. The left-hand-side computes the input batch on the pretrained model. It is quite efficient. The latency is almost the same as when there&#39;s only one input, thanks to the strong <a href="https://le.qun.ch/en/blog/2023/05/13/transformer-batching/" rel="nofollow">batching effect</a>.</p>
<p dir="auto">We figured out an efficient way to compute the right-hand-side (the LoRA addon). We encapsulate this operation in a CUDA kernel, called Segmented Gather Matrix-Vector multiplication (SGMV), as illustrated below.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/punica-ai/punica/blob/master/assets/sgmv.png"><img src="https://github.com/punica-ai/punica/raw/master/assets/sgmv.png" alt="SGMV"/></a></p>
<p dir="auto">In the following microbenchmark figure, we can observe the strong batching effect of the pretrained model. Naive implementation of LoRA is slow, as depicted in the orange line. LoRA implemented via SGMV is effificent and preserves the strong batching effect.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/punica-ai/punica/blob/master/assets/backbone-vs-sgmv.png"><img src="https://github.com/punica-ai/punica/raw/master/assets/backbone-vs-sgmv.png" alt="SGMV is fast and maintains strong batching effect"/></a></p>
<p dir="auto">The following figure shows the text generation throughput comparison between Punica and other systems, including <a href="https://github.com/huggingface/transformers/">HuggingFace Transformers</a>, <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, <a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a>, <a href="https://github.com/vllm-project/vllm">vLLM</a>. The benchmark considers different settings of LoRA model popularity. <em>Distinct</em> means that each request is for a different LoRA model. <em>Identical</em> means that all requests are for the same LoRA model. <em>Uniform</em> and <em>Skewed</em> are in between. <strong>Punica achieves 12x throughput compared to state-of-the-art systems.</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/punica-ai/punica/blob/master/assets/textgen.png"><img src="https://github.com/punica-ai/punica/raw/master/assets/textgen.png" alt="Punica achieves 12x throughput compared to state-of-the-art systems"/></a></p>
<p dir="auto">Read our paper to understand more: <a href="https://arxiv.org/abs/2310.18547" rel="nofollow">Punica: Multi-Tenant LoRA Serving</a>.</p>
<h2 tabindex="-1" id="user-content-citation" dir="auto"><a href="#citation">Citation<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{punica,
    title={Punica: Multi-Tenant LoRA Serving},
    author={Lequn Chen and Zihao Ye and Yongji Wu and Danyang Zhuo and Luis Ceze and Arvind Krishnamurthy},
    year={2023},
    eprint={2310.18547},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}"><pre><span>@misc</span>{<span>punica</span>,
    <span>title</span>=<span><span>{</span>Punica: Multi-Tenant LoRA Serving<span>}</span></span>,
    <span>author</span>=<span><span>{</span>Lequn Chen and Zihao Ye and Yongji Wu and Danyang Zhuo and Luis Ceze and Arvind Krishnamurthy<span>}</span></span>,
    <span>year</span>=<span><span>{</span>2023<span>}</span></span>,
    <span>eprint</span>=<span><span>{</span>2310.18547<span>}</span></span>,
    <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
    <span>primaryClass</span>=<span><span>{</span>cs.DC<span>}</span></span>
}</pre></div>
</article>
          </div></div>
  </body>
</html>
