<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/">Original</a>
    <h1>How RLHF Preference Model Tuning Works (and How Things May Go Wrong)</h1>
    
    <div id="readability-page-1" class="page"><article>
    <section>
      <p>The rise of <strong>Large Language Models (LLMs)</strong> is revolutionizing how we interact with technology. Today, ChatGPT and other LLMs can perform cognitive tasks involving natural language that were unimaginable a few years ago. <br/></p><p>The exploding popularity of conversational AI tools has also raised serious concerns about AI safety. From the large-scale proliferation of biased or false information to risks of psychological distress for chatbot users, the potential for misuse of language models is a subject of intense <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/?ref=assemblyai.com">debate</a>. <br/></p><p>Much of current AI research aims to design LLMs that seek helpful, truthful, and harmless behavior. One such method, <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>, is currently leading the charge. Many companies, including OpenAI, Google, and Meta, have incorporated RLHF into their AI models, hoping to provide a more controlled user experience.<br/></p><p>Unfortunately, while RLHF does offer some level of control, it won’t be the ultimate silver bullet for aligning AI systems with human values. Indeed, RLHF tuning may negatively affect a model&#39;s ability to perform specific tasks. Despite this, RLHF remains the industry&#39;s go-to solution for achieving alignment in LLMs. <br/></p><p>In this article, we&#39;ll explore how RLHF works, how it truly impacts a language model&#39;s <strong>behavior</strong>, and discuss the current <strong>limitations</strong> (see <a href="#current-limitations-of-rlhf-fine-tuning">section</a> below) of this approach. This piece should be helpful to anyone who wants a better understanding of LLMs and the challenges in making them safe and reliable. While some familiarity with <a href="https://www.assemblyai.com/blog/introduction-large-language-models-generative-ai/">LLM terminology</a> will be beneficial, we have aimed to make this article accessible to a broad audience.</p><h2 id="rlhf-as-human-preference-tuning">RLHF as Human Preference Tuning</h2><p>RLHF is about teaching an AI model to understand human values and preferences. How is it capable of doing this?</p><p>Let’s imagine having two distinct language models: </p><ol><li>A <strong>base model</strong> that’s only been trained to <em>predict the next word</em> on text sequences from a large and diverse text dataset, such as the whole internet. It can produce fluent responses on a wide range of topics. Still, the tone of the answer varies depending on the subject, the exact <em>wording</em> of the question (prompt), and some stochasticity (randomness).</li><li>A second language model, call it the <strong>preference model</strong>. The preference model can determine what humans prefer by assigning scores to different responses from the base model. </li></ol><figure><img src="https://lh6.googleusercontent.com/PZIY0Av_-G1rHsK1Gmhf3ZnhrMpSALOUTq1PaPc6sVML6VDT06CwdDPEI6-NOOhMDS0_5x-WhA0rwW4R25jUqQ9FijzV1jm3F2cUKot-eT_DZgByycV5uzh_9Ksx5Po4VziZ0cM2KLr4AJfZiWZRWEE" alt="The preference model - RLHF models" loading="lazy" width="550" height="183"/><figcaption>The preference model is (hypothetically) able to determine which response a human would prefer within a given list of possibilities.</figcaption></figure><p>The goal now is to use the preference model to <em>alter the behavior</em> of the base model in response to a prompt. The underlying idea is straightforward: Using the preference model, the base model is refined iteratively (via a <em>fine-tuning</em> process), changing its internal text distribution to <strong>prioritize responses favored by humans</strong>. </p><figure><img src="https://lh6.googleusercontent.com/i91Ps5p_H6KP2nv4D1kBr_wmaf_pEks93gpKNRs01ip_rrnvzxN37Bjj2e3AXaJCRyFsT2CJpkgPMtcO2hr1xAieLJv4BFgR-NxZvN0T5moWbJsH0PMWkM4P7jTSTovuNr5H_0sS7fOEaoApZl38UtM" alt="Fine-tuning LLMs with human preference - RLHF" loading="lazy" width="910" height="240"/><figcaption>Fine-tuning the base model: A preference model could be used to fine-tune the base model to prioritize responses with <em>higher preference scores</em>.</figcaption></figure><p>In a way, the preference model serves as a means to <strong>directly introduce a <em>&#34;human preference bias”</em> into the base model</strong>. <br/></p><p>The first step is to create a dataset of examples reflecting human preferences. Researchers can collect human feedback in several ways: <em>rating</em> different model outputs, <em>demonstrating</em> preferred responses to prompts, <em>correcting</em> undesirable model outputs, or providing <em>critiques</em> in natural language. Each of these approaches would lead to a different kind of <strong>reward model</strong> that can be used for fine-tuning. The <strong>core idea of RLHF</strong> revolves around training such reward models.</p><figure><img src="https://lh4.googleusercontent.com/xnhiMc0xbzw7CFa1jYf4iwhMDukFGYc334nwQEtKsqsFgAiKt9syjDZCBqxI9KM495H3cZ4s_00xo322XK0QY37oH7IIWfnVfC3W1wbCBVKMPIVrG8-OZJQ_IvTSQ5lJ0x0hwKkzmo8cXAdy2z33I-c" alt="Reward models for RLHF - Preference, Demonstration, Critique, Rating, and Correction reward models." loading="lazy" width="960" height="540"/><figcaption>Various reward models that can be trained on human feedback: Preference, Demonstration, Critique, Rating, and Correction reward models.</figcaption></figure><p>The choice of reward model may depend on different aspects, including the specific “human values” to be optimized for. Fine-tuning a model using high-quality demonstrations is considered the gold standard, but this approach is time and resource-intensive, limiting its scalability. On the other hand, organizing a group of human annotators to select preferred outputs iteratively can result in a relatively <strong>large dataset</strong> on which to train a preference model. Thanks to this advantage in scalability, most companies choose this as their primary strategy for doing RLHF, as OpenAI did with ChatGPT.<br/></p><p>Once the reward model is ready, it can be used to fine-tune the base model via <strong>Reinforcement Learning</strong>. In this machine learning method, an <em>agent</em> (in our case, the base model) learns to choose <em>actions</em> (here, responses to user input) that maximize a score (technically called a <em>reward</em>). Much like how <a href="https://web.stanford.edu/~surag/posts/alphazero.html?ref=assemblyai.com">AlphaGo</a> learned to play Go and Chess, the base model trains itself using a self-play reinforcement learning framework. Being tasked with generating output responses to a list of prompts, its predictions are tweaked to lean towards the highest-scoring responses.</p><h2 id="the-rlhf-effect">The RLHF Effect</h2><p>Reinforcement Learning from Human Feedback represents a significant advancement in language models, providing a more user-friendly interface for harnessing their vast capabilities. But how do we interpret the effect of RLHF fine-tuning over the original base LLM?<br/></p><p>One way to think about it is the following. <br/></p><p>The base model may behave like a sort of <strong>stochastic parrot</strong>, a reservoir of textual chaos learned from a universe of online text with its invaluable insights and its less desirable content. It may accurately mirror the distribution of internet text and thus be able to replicate (“parrot”) the multitude of available tones, personas, sources, and voices seen in its training data.<br/></p><p>However, the base model&#39;s choice of tone of voice in responding to an input prompt can be very unpredictable. Due to its highly <a href="https://en.wikipedia.org/wiki/Multimodal_distribution?ref=assemblyai.com">multimodal</a> nature, the quality of its responses could vary significantly, depending on the source (or <em>mode</em>) it decides to emulate.<br/></p><figure><img src="https://www.assemblyai.com/blog/content/images/2023/08/LLMs-multimodal-1.jpg" alt="Mode-seeking effect of RLHF on LLMs" loading="lazy" width="700" height="527" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2023/08/LLMs-multimodal-1.jpg 600w, https://www.assemblyai.com/blog/content/images/2023/08/LLMs-multimodal-1.jpg 700w"/><figcaption>The figure shows a multimodal distribution with three modes or “peaks.” The base model is effectively trained to approximate the distribution of internet text, which has millions of different modes corresponding to various sources, styles, and voices.</figcaption></figure><p>For instance, a user might ask about a notable political figure. The model could generate a response resembling a neutral, informative Wikipedia article (it chooses an “<em>encyclopedic mode”</em> in the distribution, so to speak), or it could echo a more radical viewpoint, depending on the exact phrasing and tone of the question.<br/></p><p>How to control these unpredictable dynamics? Leaving it entirely up to the model’s stochastic decision-making nature may not be ideal for most real-world LLM applications. As we’ve seen, RLHF addresses this issue by fine-tuning the model based on human preference data, offering a more <strong>reliable user experience</strong>. <br/></p><p>But does this come at a cost?<br/></p><p>Applying RLHF shapes a language model by infusing a human preference bias into its outputs. Operationally, we can interpret this effect as introducing a <strong>mode-seeking</strong> mechanism that guides the model through its text distribution and leads to outputs with higher rewards, effectively <strong>narrowing the potential range of generated content</strong>.<br/></p><figure><img src="https://www.assemblyai.com/blog/content/images/2023/08/LLMs-mode-seeking.jpg" alt="Mode-seeking RLHF" loading="lazy" width="700" height="541" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2023/08/LLMs-mode-seeking.jpg 600w, https://www.assemblyai.com/blog/content/images/2023/08/LLMs-mode-seeking.jpg 700w"/><figcaption>A mode-seeking mechanism guides a model’s search between different modes in its text distribution.</figcaption></figure><p>The kind of biases that RLHF introduces <strong>reflect the values</strong> of the individuals who have informed the preference-dataset. For instance, ChatGPT is biased towards delivering responses that are helpful, truthful, and harmless, mirroring the inclinations of its annotators, and the guidelines they have been instructed to follow.<br/></p><p>What gets compromised for the original model through this process?<br/></p><p>While RLHF enhances the reliability and consistency of the model&#39;s responses, <strong>it will necessarily limit the diversity</strong> of its generative capabilities. Whether this trade-off represents a benefit or a limitation depends on the intended use case. <br/></p><p>In applications such as chatbot assistants or search, where accuracy is critical, RLHF certainly proves advantageous. However, for creative uses like generating ideas or assisting in writing, a reduction in output diversity <strong>could stifle the birth of more original content</strong>.<br/></p><p>The advent of RLHF fine-tuning has arguably revolutionized conversational AI. Yet, at least for large-scale implementations, it&#39;s a framework still in its nascent stages. To delve deeper into our understanding, the next and final section surveys some recent research on RLHF’s limitations and its impact on AI safety and the alignment of language models.</p><h2 id="current-limitations-of-rlhf-fine-tuning">Current Limitations of RLHF Fine-Tuning</h2><p>RLHF&#39;s goal of creating a helpful and harmless model imposes a necessary trade-off: In the extreme case, a model that refrains from answering any prompt is entirely <em>harmless</em>, yet not particularly <em>helpful</em>.<br/></p><p>Conversely, an <em>overly</em> helpful model might respond to questions it should avoid, risking harmful outcomes. Finding the right balance imposes the first obvious challenge.<br/></p><p>Truthful-oriented RLHF tuning, as <a href="https://openai.com/research/instruction-following?ref=assemblyai.com">applied</a> at OpenAI, explicitly intends to reduce <strong>hallucination</strong> – the tendency of LLMs to sometimes generate false or invented statements. However, RLHF tuning often <em>worsens</em> hallucinations, as a <a href="https://arxiv.org/abs/2203.02155?ref=assemblyai.com">study</a> based on InstructGPT reported. Ironically, the same study has served as the primary basis for ChatGPT’s initial RLHF design.<br/></p><p>The precise mechanism behind the hallucination phenomenon is still generally unclear. A first <a href="https://arxiv.org/abs/2110.10819?ref=assemblyai.com">hypothesis</a> in a paper from Google DeepMind suggests that LLMs hallucinate because they <em>“lack an understanding of the cause and effect of their actions.”</em> A different view posits that hallucinations arise from the behavior-cloning nature of language models, especially when an LLM is trained to <em>mimic responses containing information it doesn&#39;t possess</em>. <br/></p><p>Interestingly, though, the human evaluators of InstructGPT generally <strong>preferred</strong> its outputs over the baseline model (as reported in the study above), <em>despite the increase in hallucinations</em>. This fact hints at another central problem with the RLHF approach: the intrinsic difficulty in evaluating its results. <br/></p><p>The core part of RLHF evaluation is based on crowd work, i.e., on <strong>human feedback evaluation</strong>: it takes place by having human annotators rate the quality of the model outputs. The argument seems straightforward: Because RLHF tuning is based on human input, human input should also (primarily) evaluate its results. But this process poses some issues, as human evaluation is highly time-consuming and expensive and suffers from strong <strong>subjectivity</strong>. Truthfulness, in particular, can be incredibly challenging to evaluate systematically.<br/></p><p>What about assessing truthfulness by some set of <em>objective</em> metrics? <br/></p><p>Unfortunately, it doesn’t get much simpler. Yannic Kilcher’s provocative <a href="https://youtu.be/efPrtcLdcdM?ref=assemblyai.com">experiment</a>, for instance, revealed how his <strong>GPT-4chan</strong> – an LLM fine-tuned on a dataset filled with extreme and unmoderated conversations – <em>significantly improved</em> its score on <a href="https://arxiv.org/abs/2109.07958?ref=assemblyai.com"><em>TruthfulQA</em></a>, a <strong>standard open-source benchmark</strong> assessing truthfulness in LLM-generated answers.<br/></p><p>Even after RLHF fine-tuning, LLMs are not immune to exploits known as <strong>jailbreak attacks,</strong> where the model can be manipulated to produce objectionable content through subtly altered instructions. A study <a href="https://arxiv.org/abs/2307.08487?ref=assemblyai.com">found</a> that merely changing certain <em>verbs</em> in the input prompt can impact the safety of a model’s output significantly. Another recent astonishing result <a href="https://arxiv.org/abs/2307.15043?ref=assemblyai.com">discovered</a> a class of “universal” jailbreaking prompts seemingly able to <em>work across different LLMs</em>.<br/></p><p>Even without adversarial attacks, RLHF’s efficacy to reduce social biases is not well understood. For instance, a recent paper highlighted ChatGPT’s <a href="https://arxiv.org/abs/2305.10510?ref=assemblyai.com">tendency</a> to reproduce <strong>gender defaults and stereotypes</strong> assigned to certain occupations, when translating from English to another language.<br/></p><p>Unfortunately, it’s even unclear if further improvements to RLHF may ever entirely solve the problem of adversarial prompting. Some recent theoretical results seem to <a href="https://arxiv.org/abs/2304.11082?ref=assemblyai.com">suggest</a> that <em>“any behavior that has a finite probability of being exhibited by the LLM can be triggered by certain prompts, which increases with the length of the prompt.”</em> <br/></p><p>If such results (and other <a href="https://arxiv.org/abs/2209.15259?ref=assemblyai.com">related</a> ones) were to be confirmed, they would imply a <strong>theoretical impossibility</strong> of language model alignment by RLHF or any other analogous alignment framework.<br/></p><p>From a practical perspective, however, one must say that RLHF seems to be working, at least superficially, by enabling a more friendly user interface. But <em>how much</em> RLHF training exactly is necessary? More precisely, what is the <strong>relative importance</strong> of RLHF tuning to high-quality instruction tuning? <br/></p><p>One of the most evident downsides of published reports on large-scale RLHF tuning of LLMs is the <strong>lack of proper comparison studies</strong> for the effect of RLHF tuning vs. utilizing a comparable amount of human labor to fine-tune a base model on high-quality demonstration data.<br/></p><p>A recent technical report introduced the model <a href="https://arxiv.org/abs/2305.11206?ref=assemblyai.com">LIMA</a> (Less Is More for Alignment) as a first step in this direction. Rather than relying on RLHF fine-tuning, LIMA builds on Meta&#39;s open-source <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/?ref=assemblyai.com">LLaMA</a> as its base model. The authors curated a dataset of (only) 1,000 high-quality demonstration examples, covering a wide range of conversation topics. This curated dataset was used to fine-tune a LLaMA 65B model.<br/></p><p>The results showed a remarkable performance in direct human preference evaluations, even when tested against LLMs extensively fine-tuned with RLHF, like GPT-4 and Bard models.<br/></p><p>While such studies are still missing a full view of the landscape, they suggest that focusing on the <strong>data quality</strong> might be way more beneficial than prioritizing <strong>scalability</strong> when fine-tuning LLMs. Unraveling the exact <em>scaling laws</em> that govern the balance between demonstration data and RLHF or similar techniques (e.g. <a href="https://www.assemblyai.com/blog/how-reinforcement-learning-from-ai-feedback-works/">RLAIF</a>) remains an intriguing research direction and will be key to improving LLM performance.</p><h2 id="final-words">Final Words</h2><p>Many techniques around LLMs, including RLHF, will continue to evolve. At its current stage, RLHF for language model alignment has significant limitations. However, rather than disregarding it, we should aim to better understand it.<br/></p><p>There is a wealth of interconnected topics waiting to be explored. And we will be exploring these in future blog posts! If you enjoyed this article, feel free to check out some of our other recent articles to learn about<br/></p><ul><li><a href="https://www.assemblyai.com/blog/how-reinforcement-learning-from-ai-feedback-works/">How Reinforcement Learning from AI Feedback works</a></li><li><a href="https://www.assemblyai.com/blog/ai-trends-graph-neural-networks/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=brand">Graph Neural Networks in 2023</a></li><li><a href="https://www.assemblyai.com/blog/how-physics-advanced-generative-ai/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=brand">How physics advanced Generative AI</a><br/></li></ul><p>You can also follow us on <a href="https://twitter.com/AssemblyAI?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=brand&amp;ref=assemblyai.com">Twitter</a>, where we regularly post content on these subjects and many other exciting aspects of AI.</p>
    </section>
    
  </article></div>
  </body>
</html>
