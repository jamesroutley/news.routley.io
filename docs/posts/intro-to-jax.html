<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://isakfalk.com/notes/intro_to_jax.html">Original</a>
    <h1>Intro to Jax</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
    <header>
      
    </header>
    <nav id="table-of-contents" role="doc-toc">
      <h2>Table of Contents</h2>
      
    </nav>
    <div id="outline-container-imports">
      <h2 id="imports"><a href="#imports">Imports</a></h2>
      <div id="text-imports">
        <p>Some of the libraries we will use throughout this post are imported
        below.</p>
        <div>
          <pre><span>import</span> time

<span>import</span> numpy <span>as</span> np
<span>import</span> matplotlib.pyplot <span>as</span> plt
<span>import</span> matplotlib <span>as</span> mpl
<span>import</span> seaborn <span>as</span> sns
</pre>
        </div>
      </div>
    </div>
    <div id="outline-container-introduction">
      <h2 id="introduction"><a href="#introduction">Introduction</a></h2>
      <div id="text-introduction">
        <p>The <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html">Jax
        Quickstart</a> tutorial states</p>
        <blockquote>
          <p>JAX is NumPy on the CPU, GPU, and TPU, with great automatic
          differentiation for high-performance machine learning research.</p>
        </blockquote>
        <p>What does this mean? And how does this differ from other deep
        learning libraries such as torch and tensorflow?</p>
        <p>As is standard we will import some jax libraries and functions</p>
        <div>
          <pre><span>import</span> jax
<span>from</span> jax <span>import</span> jit, grad, vmap
<span>from</span> jax <span>import</span> random
<span>import</span> jax.numpy <span>as</span> jnp
<span>import</span> jax.scipy <span>as</span> jscp
</pre>
        </div>
      </div>
    </div>
    <div id="outline-container-backend-xla">
      <h2 id="backend-xla"><a href="#backend-xla">Backend: XLA</a></h2>
      <div id="text-backend-xla">
        <p>Jax is basically a compiler for turning python code and vector
        operations using the <a href="https://www.tensorflow.org/xla/architecture">XLA compiler</a> to
        machine instructions for different computer architectures. The standard
        computer architecture we use is the GPU, but there are others, for
        example</p>
        <ul>
          <li>CPUs</li>
          <li>
            <a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit">TPUs</a>
          </li>
          <li>
            <a href="https://www.graphcore.ai/products/ipu">IPUs</a>
          </li>
        </ul>
        <p>or other specially created hardware which accelerates operations or
        make them more efficient in some way. The <strong>point is that python
        is slow and XLA makes this very fast using techniques such as fusing
        operations and removing redundant code and operations</strong>.
        Personally, this feels like a pretty future-proof way of decoupling how
        we specify what we want using e.g. python+jax vs how it is made to run
        on hardware, here using XLA. It reminds me of how LSP has solved the
        decoupling problem for code editing for editors<sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup>. There
        seem to be even more specialized hardware being created for e.g.
        inference of LLMs (<a href="https://www.positron.ai/">like this</a>
        which is one of several LLM inference hardware companies I saw at
        <a href="https://isakfalk.com/notes/notes/NeurIPS2023retrospect.html">NeurIPS 2023</a>) so who
        knows what funky architectures will become available in the future.</p>
      </div>
    </div>
    <div id="outline-container-what-is-jax">
      <h2 id="what-is-jax"><a href="#what-is-jax">What is jax?</a></h2>
      <p>Jax is a reimplementation of the older linear algebra and science
        stack for python including <kbd>numpy</kbd> and <kbd>scipy</kbd>, with
        a just-in-time compiler and ways to perform automatic differentiation.
        To really hammer this home, jax has reimplemented a subset of both of
        these packages which seem pretty feature-complete. The current state of
        this API can be found in <a href="https://jax.readthedocs.io/en/latest/jax.html">the docs</a>.</p>
    </div>
    <div id="outline-container-jax-primitives-jit-grad-vmap">
      <h2 id="jax-primitives-jit-grad-vmap"><a href="#jax-primitives-jit-grad-vmap">Jax primitives: <kbd>jit</kbd>,
      <kbd>grad</kbd>, <kbd>vmap</kbd></a></h2>
      <p>There are 3 functions which are integral to almost any jax
        program.</p>
      <div id="outline-container-jit">
        <h3 id="jit"><a href="#jit"><kbd>jit</kbd></a></h3>
        <div id="text-jit">
          <p>The <kbd>jit</kbd> function takes a large subset of python
          together with jax functions and compile it down to XLA-kernels which
          are very fast. Below I&#39;ve done a very quick benchmark of how
          <kbd>jit</kbd> speeds up matrix-matrix multiplication.</p>
          <div>
            <pre><span>def</span> <span>jax_matmul</span>(A, B):
    A @ B

<span>jit_jax_matmul</span> <span>=</span> jit(jax_matmul)

<span>import</span> timeit
<span>n</span>, <span>p</span>, <span>k</span> <span>=</span> 10<span>**</span>4, 10<span>**</span>4, 10<span>**</span>4
<span>A</span> <span>=</span> jnp.ones((n, p))
<span>B</span> <span>=</span> jnp.ones((p, k))
jit_jax_matmul(A, B) 
<span>print</span>(f<span>&#34;jax: </span>{timeit.timeit(<span>lambda</span>: jax_matmul(A, B), number<span>=</span>10)}<span>&#34;</span>)
<span>print</span>(f<span>&#34;jax (JIT): </span>{timeit.timeit(<span>lambda</span>: jit_jax_matmul(A, B), number<span>=</span>10)}<span>&#34;</span>)
</pre>
          </div>
          <pre>jax: 0.37372643800335936
jax (JIT): 0.0003170749987475574
</pre>
          <p>which is about double the speed. The gains are much greater when
          we jit things which does not have an already efficient implementation
          (such as a matmul). Additionally, this allows us to speed things up
          which cannot be done without considerable vectorization effort in
          numpy or may be outright impossible.</p>
        </div>
      </div>
      <div id="outline-container-grad">
        <h3 id="grad"><a href="#grad"><kbd>grad</kbd></a></h3>
        <div id="text-grad">
          <p>The <kbd>grad</kbd> function takes as input a function \(f\)
          mapping to \(\mathbb{R}\) and spits out the gradient of that function
          \(\nabla f\). This can be a very natural way of working with
          gradients if you are used to the math.</p>
          <div>
            <pre><span>def</span> <span>sum_of_squares</span>(x):
    <span>return</span> jnp.<span>sum</span>(x<span>**</span>2)

<span>sum_of_squares_dx</span> <span>=</span> grad(sum_of_squares)
</pre>
          </div>
          <p>The function <kbd>sum_of_squares_dx</kbd> is the mathematical
          gradient of <kbd>sum_of_squares</kbd>. The randomness is handled
          explicitly by splitting the state (key), read about it <a href="https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html">
          here</a>.</p>
          <div>
            <pre><span>key</span> <span>=</span> jax.random.PRNGKey(0)
<span>key</span>, <span>subkey</span> <span>=</span> jax.random.split(key)
<span>in_x</span> <span>=</span> jax.random.normal(key, (3, 3))
<span>dx</span> <span>=</span> sum_of_squares_dx(in_x)
<span>print</span>(dx)
<span>print</span>(dx.shape)
</pre>
          </div>
          <pre>[[-5.2211165   0.06770565  2.1726665 ]
 [-2.960598    3.0806496   2.125032  ]
 [ 1.0834967   0.0340456   0.544537  ]]
(3, 3)
</pre>
        </div>
      </div>
      <div id="outline-container-vmap">
        <h3 id="vmap"><a href="#vmap"><kbd>vmap</kbd></a></h3>
        <div id="text-vmap">
          <p>The function <kbd>vmap</kbd> allows you to lift a function to a
          batched function, <strong>without having to go through
          vectorization</strong>. For example, if we wanted to batch the
          <kbd>sum_of_squares</kbd> function we can do this by simply applying
          <kbd>vmap</kbd></p>
          <div>
            <pre><span>batched_sum_of_squares</span> <span>=</span> vmap(sum_of_squares)
<span>x</span> <span>=</span> jax.random.normal(key, (5, 3, 3))
<span>print</span>(batched_sum_of_squares(x))
<span>print</span>(batched_sum_of_squares(x).shape)
</pre>
          </div>
          <pre>[ 7.109205   7.1214614 21.167786   6.137778   4.915494 ]
(5,)
</pre>
          <p>This is pretty powerful: often it&#39;s easy to specify the function
          for a sample \(x\) but harder to vectorize. For a standard neural
          network it may be pretty simple, but imagine something like LLMs,
          GANs or working with inputs which are not points, e.g. sets.
          Additionally, we can use the <kbd>in_axes</kbd> argument to batch in
          according to different input arguments and ignore others.</p>
          <div>
            <pre><span>def</span> <span>multi_matmul</span>(A, B, C):
    <span>return</span> A @ B @ C


<span>vmap_multi_matmul</span> <span>=</span> vmap(multi_matmul, in_axes<span>=</span>(0, <span>None</span>, 0))

<span>l</span>, <span>n</span>, <span>p</span>, <span>d</span>, <span>m</span> <span>=</span> 3, 5, 7, 9, 11
<span>A</span> <span>=</span> jnp.ones((l, n, p))
<span>B</span> <span>=</span> jnp.ones((p, d))
<span>C</span> <span>=</span> jnp.ones((l, d, m))

<span>print</span>(vmap_multi_matmul(A, B, C).shape) 
</pre>
          </div>
          <pre>(3, 5, 11)
</pre>
        </div>
      </div>
      <div id="outline-container-composition">
        <h3 id="composition"><a href="#composition">Composition</a></h3>
        <div id="text-composition">
          <p>You can compose all of these functions as you see fit</p>
          <div>
            <pre><span>jit_batched_sum_of_squares_dx</span> <span>=</span> jit(vmap(grad(sum_of_squares)))
<span>print</span>(jit_batched_sum_of_squares_dx(x).shape)
</pre>
          </div>
          <pre>(5, 3, 3)
</pre>
          <p>This allows for utlizing the autodiff framework fully.</p>
        </div>
      </div>
    </div>
    <div id="outline-container-building-a-neural-network-from-scratch">
      <h2 id="building-a-neural-network-from-scratch"><a href="#building-a-neural-network-from-scratch">Building a neural network from
      scratch</a></h2>
      <div id="text-building-a-neural-network-from-scratch">
        <p>We&#39;ll build an MLP using nothing but jax. We will train this on
        MNIST. To load the data I&#39;m using the <a href="https://pypi.org/project/jax-dataloader/">jax-dataloader</a>
        library.</p>
        <div>
          <pre><span>import</span> jax_dataloader <span>as</span> jdl
<span>from</span> torchvision.datasets <span>import</span> MNIST

<span>pt_ds</span> <span>=</span> MNIST(<span>&#34;/tmp/mnist&#34;</span>, download<span>=</span><span>True</span>, transform<span>=</span><span>lambda</span> x: np.array(x, np.float32), train<span>=</span><span>True</span>)
<span>train_dataloader</span> <span>=</span> jdl.DataLoader(pt_ds, backend<span>=</span><span>&#34;pytorch&#34;</span>, batch_size<span>=</span>128, shuffle<span>=</span><span>True</span>)
<span>pt_ds</span> <span>=</span> MNIST(<span>&#34;/tmp/mnist&#34;</span>, download<span>=</span><span>True</span>, transform<span>=</span><span>lambda</span> x: np.array(x, np.float32), train<span>=</span><span>False</span>)
<span>test_dataloader</span> <span>=</span> jdl.DataLoader(pt_ds, backend<span>=</span><span>&#34;pytorch&#34;</span>, batch_size<span>=</span>128, shuffle<span>=</span><span>True</span>)
</pre>
        </div>
        <p>The jax library have some helpful functions for building neural
        networks. Here we create parameters and define a prediction function
        which given a <a href="https://jax.readthedocs.io/en/latest/pytrees.html">pytree</a> of
        parameters and an input outputs the predicted logits. Pytrees is a
        great thing about jax where it allow us to intuitively and effectively
        use not only raw arrays but also tree-like structures of by composing
        lists, tuples and dictionaries with each other and arrays as leaves and
        map over these as if they were arrays.</p>
      </div>
      <div id="outline-container-creating-the-neural-network">
        <h3 id="creating-the-neural-network"><a href="#creating-the-neural-network">Creating the neural network</a></h3>
        <div id="text-creating-the-neural-network">
          <div>
            <pre><span>from</span> jax.nn <span>import</span> relu
<span>from</span> jax.nn.initializers <span>import</span> glorot_normal
<span>from</span> jax.scipy.special <span>import</span> logsumexp

<span>def</span> <span>create_mlp_weights</span>(num_layers: <span>int</span>, in_dim: <span>int</span>, out_dim: <span>int</span>, hidden_dim: <span>int</span>, key):
    
    <span>def</span> <span>create_layer_weights</span>(in_dim, out_dim, key):
        <span>return</span> {
            <span>&#34;W&#34;</span>: glorot_normal()(key, (in_dim, out_dim)),
            <span>&#34;b&#34;</span>: np.zeros(out_dim)
        }
    <span>params</span> <span>=</span> []
    <span>key</span>, <span>subkey</span> <span>=</span> jax.random.split(key)
    
    params.append(create_layer_weights(in_dim, hidden_dim, subkey))
    <span>for</span> _ <span>in</span> <span>range</span>(1, num_layers):
        <span>key</span>, <span>subkey</span> <span>=</span> jax.random.split(key)
        params.append(create_layer_weights(hidden_dim, hidden_dim, key))
    <span>key</span>, <span>subkey</span> <span>=</span> jax.random.split(key)
    params.append(create_layer_weights(hidden_dim, out_dim, subkey))
    <span>return</span> params

<span>def</span> <span>predict</span>(params, x):
    <span>for</span> layer <span>in</span> params[:<span>-</span>1]:
        <span>x</span> <span>=</span> relu(x @ layer[<span>&#34;W&#34;</span>] <span>+</span> layer[<span>&#34;b&#34;</span>])
    <span>logits</span> <span>=</span> x @ params[<span>-</span>1][<span>&#34;W&#34;</span>] <span>+</span> params[<span>-</span>1][<span>&#34;b&#34;</span>]
    <span>return</span> logits <span>-</span> logsumexp(logits)
</pre>
          </div>
          <p>Let&#39;s pick some reasonable defaults. We see that all shapes are
          correct and we have batched the <kbd>predict</kbd> function.</p>
          <div>
            <pre><span>num_layers</span> <span>=</span> 3
<span>in_dim</span> <span>=</span> 28 <span>*</span> 28
<span>out_dim</span> <span>=</span> 10
<span>hidden_dim</span> <span>=</span> 128
<span>key</span> <span>=</span> jax.random.PRNGKey(2023)
<span>params</span> <span>=</span> create_mlp_weights(num_layers, in_dim, out_dim, hidden_dim, key)
<span>print</span>(predict(params, jnp.ones(28 <span>*</span> 28)))

<span>batched_predict</span> <span>=</span> vmap(predict, in_axes<span>=</span>(<span>None</span>, 0))
<span>print</span>(batched_predict(params, jnp.ones((4, 28 <span>*</span> 28))).shape)
<span>print</span>(<span>len</span>(params))
<span>print</span>(<span>type</span>(params[0][<span>&#34;W&#34;</span>]))
</pre>
          </div>
          <pre>[-3.3419425 -1.4851335 -2.5466485 -3.1445212 -1.8924606 -2.5047162
 -2.622343  -2.6072748 -1.5674857 -3.5270252]
(4, 10)
4
&lt;class &#39;jaxlib.xla_extension.ArrayImpl&#39;&gt;
</pre>
        </div>
      </div>
      <div id="outline-container-training">
        <h3 id="training"><a href="#training">Training</a></h3>
        <div id="text-training">
          <p>Now we write the helper functions to train this network. In
          particular we use the <a href="https://jax.readthedocs.io/en/latest/pytrees.html">pytree</a>
          functionality of jax to update the parameters which is a pytree since
          it&#39;s a list of dictionaries of arrays.</p>
          <div>
            <pre><span>import</span> jax.tree_util <span>as</span> tree_util

<span>def</span> <span>one_hot</span>(x, k, dtype<span>=</span>jnp.float32):
  <span>&#34;&#34;&#34;Create a one-hot encoding of x of size k.&#34;&#34;&#34;</span>
  <span>return</span> jnp.array(x[:, <span>None</span>] <span>==</span> jnp.arange(k), dtype)

<span>@jit</span>
<span>def</span> <span>accuracy</span>(params, images, targets):
  <span>target_class</span> <span>=</span> jnp.argmax(targets, axis<span>=</span>1)
  <span>predicted_class</span> <span>=</span> jnp.argmax(batched_predict(params, images), axis<span>=</span>1)
  <span>return</span> jnp.mean(predicted_class <span>==</span> target_class)

<span>def</span> <span>loss</span>(params, images, targets):
  <span>preds</span> <span>=</span> batched_predict(params, images)
  <span>return</span> <span>-</span>jnp.mean(preds <span>*</span> targets)

<span>@jit</span>
<span>def</span> <span>update</span>(params, x, y, step_size):
  <span>grads</span> <span>=</span> grad(loss)(params, x, y)
  <span>return</span> tree_util.tree_map(<span>lambda</span> w, g: w <span>-</span> step_size <span>*</span> g, params, grads)

<span>EPOCHS</span> <span>=</span> 10
<span>STEP_SIZE</span> <span>=</span> 10 <span>**</span> <span>-</span>2
<span>train_acc</span> <span>=</span> []
<span>train_loss</span> <span>=</span> []
<span>test_acc</span> <span>=</span> []
<span>test_loss</span> <span>=</span> []

<span>for</span> epoch <span>in</span> <span>range</span>(EPOCHS):
  <span>print</span>(<span>&#39;Epoch&#39;</span>, epoch)
  <span>for</span> image, output <span>in</span> train_dataloader:
    <span>image</span>, <span>output</span> <span>=</span> jnp.array(image).reshape(<span>-</span>1, 28 <span>*</span> 28), one_hot(jnp.array(output), 10)
    train_acc.append(accuracy(params, image, output).item())
    train_loss.append(loss(params, image, output).item())
    <span>params</span> <span>=</span> update(params, image, output, STEP_SIZE)
  <span>print</span>(f<span>&#39;Train accuracy: </span>{np.mean(train_acc):.3f}<span>&#39;</span>)
  <span>print</span>(f<span>&#39;Train loss: </span>{np.mean(train_loss):.3f}<span>&#39;</span>)
  <span>_test_acc</span> <span>=</span> []
  <span>_test_loss</span> <span>=</span> []
  <span>for</span> image, output <span>in</span> test_dataloader:
    <span>image</span>, <span>output</span> <span>=</span> jnp.array(image).reshape(<span>-</span>1, 28 <span>*</span> 28), one_hot(jnp.array(output), 10)
    _test_acc.append(accuracy(params, image, output).item())
    _test_loss.append(loss(params, image, output).item())
  test_acc.append(_test_acc)
  test_loss.append(_test_loss)
  <span>print</span>(f<span>&#39;Test accuracy: </span>{np.mean(test_acc):.3f}<span>&#39;</span>)
  <span>print</span>(f<span>&#39;Test loss: </span>{np.mean(test_loss):.3f}<span>&#39;</span>)
</pre>
          </div>
          <pre id="orgbf9cf84">Epoch 0
Train accuracy: 0.788
Train loss: 0.213
Test accuracy: 0.856
Test loss: 0.073
Epoch 1
Train accuracy: 0.832
Train loss: 0.135
Test accuracy: 0.872
Test loss: 0.062
Epoch 2
Train accuracy: 0.856
Train loss: 0.103
Test accuracy: 0.882
Test loss: 0.055
Epoch 3
Train accuracy: 0.872
Train loss: 0.085
Test accuracy: 0.889
Test loss: 0.051
Epoch 4
Train accuracy: 0.883
Train loss: 0.074
Test accuracy: 0.894
Test loss: 0.048
Epoch 5
Train accuracy: 0.892
Train loss: 0.065
Test accuracy: 0.898
Test loss: 0.045
Epoch 6
Train accuracy: 0.899
Train loss: 0.059
Test accuracy: 0.902
Test loss: 0.043
Epoch 7
Train accuracy: 0.905
Train loss: 0.054
Test accuracy: 0.904
Test loss: 0.042
Epoch 8
Train accuracy: 0.910
Train loss: 0.050
Test accuracy: 0.907
Test loss: 0.040
Epoch 9
Train accuracy: 0.914
Train loss: 0.046
Test accuracy: 0.909
Test loss: 0.039
</pre>
          <p>Finally we plot the learning curves</p>
          <div>
            <pre>sns.set_theme(<span>&#34;notebook&#34;</span>)
sns.set_style(<span>&#34;ticks&#34;</span>)

<span>iterations_per_epoch</span> <span>=</span> <span>len</span>(train_dataloader)

<span>fig</span>, <span>ax</span> <span>=</span> plt.subplots(2, 1)
ax[0].plot(np.array(train_loss), label<span>=</span><span>&#34;train_loss&#34;</span>)
ax[0].plot((np.arange(<span>len</span>(test_loss)) <span>+</span> 1) <span>*</span> iterations_per_epoch, np.array(test_loss).mean(<span>-</span>1), label<span>=</span><span>&#34;test_loss&#34;</span>)
ax[0].set_ylim([0.0, 0.1])
ax[0].legend()

ax[1].plot(np.array(train_acc), label<span>=</span><span>&#34;train_acc&#34;</span>)
ax[1].plot((np.arange(<span>len</span>(test_acc)) <span>+</span> 1) <span>*</span> iterations_per_epoch, np.array(test_acc).mean(<span>-</span>1), label<span>=</span><span>&#34;test_acc&#34;</span>)
ax[1].set_ylim([0.8, 1.0])
ax[1].legend()

plt.tight_layout()
</pre>
          </div>
          <figure id="orgb58db4d">
            <img src="https://isakfalk.com/assets/images/intro_to_jax/learning_curves.webp" alt="Loss and accuracy learning curves on train and test set of an MLP on mnist, with the curves doing well" width="800" loading="lazy"/>
            <figcaption>
              <span>Figure 1:</span> Both test and train
              loss goes down and accuracy goes up as we train for longer
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
    
  </div></div>
  </body>
</html>
