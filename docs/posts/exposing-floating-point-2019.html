<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ciechanow.ski/exposing-floating-point/">Original</a>
    <h1>Exposing Floating Point (2019)</h1>
    
    <div id="readability-page-1" class="page"><div><p>Despite everyday use, floating point numbers are often understood in a hand-wavy manner and their behavior <a href="https://stackoverflow.com/q/588004/558816">raises many eyebrows</a>.
Over the course of this article I’d like to show that things aren’t actually that complicated.</p>
<p>This blog post is a companion to my recently launched website – <a href="https://float.exposed">float.exposed</a>. Other than exploiting the absurdity of present day <a href="https://en.wikipedia.org/wiki/List_of_Internet_top-level_domains#ICANN-era_generic_top-level_domains">list of top level domains</a>, it’s intended to be a handy tool for inspecting floating point numbers. While I encourage you to play with it, the purpose of many of its elements may be exotic at first. By the time we’ve finished, however, all of them will hopefully become familiar.</p>
<p>On a technical note, by floating point I’m referring to the ubiquitous <a href="https://en.m.wikipedia.org/wiki/IEEE_754">IEEE 754</a> binary floating point format. Types <code>half</code>, <code>float</code>, and <code>double</code> are understood to be binary16, binary32, and binary64 respectively. There were <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic#History">other formats</a> back in the day, but whatever device you’re reading this on is <a href="https://stackoverflow.com/q/2234468/558816">pretty much guaranteed</a> to use IEEE 754.</p>
<p>With the formalities out of the way, let’s start at the shallow end of the pool.</p>

<p>We’ll begin with the very basics of writing numeric values. The initial steps may seem trivial, but starting from the first principles will help us build a working model of floating point numbers.</p>
<h2 id="decimal-numbers">Decimal Numbers<a href="https://ciechanow.ski/exposing-floating-point/#decimal-numbers" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>Consider the number 327.849. Digits to the left of the decimal point represent increasing powers of ten, while digits to the right of the decimal point represent decreasing powers of ten:</p>
<p><span>
<p>3</p>
<p>10<sup>2</sup></p>
</span>
<span>
<p>2</p>
<p>10<sup>1</sup></p>
</span>
<span>
<p>7</p>
<p>10<sup>0</sup></p>
</span>
<span>
<p>.</p>

</span>
<span>
<p>8</p>
<p>10<sup>−1</sup></p>
</span>
<span>
<p>4</p>
<p>10<sup>−2</sup></p>
</span>
<span>
<p>9</p>
<p>10<sup>−3</sup></p>
</span>
</p>
<p>Even though this notation is very natural, it has a few disadvantages:</p>
<ul>
<li>small numbers like 0.000000000653 require skimming over many zeros before they start “showing” actually useful digits</li>
<li>it’s hard to estimate the magnitude of large numbers like 7298345251 at a glance</li>
<li>at some point the distant digits of a number become increasingly less significant and could often be dropped, yet for big numbers we don’t save any space by replacing them with zeros, e.g. 7298000000</li>
</ul>
<p>By “small” and “big” numbers I’m referring to their <em>magnitude</em> so −4205 is understood to be bigger than 0.03 even though it’s to the left of it on the real number line.</p>
<p><a href="https://en.wikipedia.org/wiki/Scientific_notation">Scientific notation</a> solves all these problems. It shifts the decimal point to right after the first non-zero digit and sets the exponent accordingly:</p>

<p>Scientific notation has three major components: the sign (+), the significand (3.27849), and the exponent (2). For positive values the “+” sign is often omitted, but we’ll keep it around for the sake of verbosity. Note that the “10” simply shows that we’re dealing with base-10 system. The aforementioned disadvantages disappear:</p>
<ul>
<li>the 0-heavy small number is presented as 6.53×10<sup>−10</sup> with all the pesky zeros removed</li>
<li>just by looking at the first digit and the exponent of 7.298345251×10<sup>9</sup> we know that number is roughly 7 billion</li>
<li>we can drop the unwanted distant digits from the tail to get 7.298×10<sup>9</sup></li>
</ul>
<p>Continuing with the protagonist of this section, if we’re only interested in 4 most significant digits we can round the number using one of the <a href="https://en.wikipedia.org/wiki/Rounding">many rounding rules</a>:</p>

<p>The number of digits shown describes the precision we’re dealing with. A number with 8 digits of precision could be printed as:</p>

<h2 id="binary-numbers">Binary Numbers<a href="https://ciechanow.ski/exposing-floating-point/#binary-numbers" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>With the familiar base-10 out of the way, let’s look at the binary numbers. The rules of the game are exactly the same, it’s just that the base is 2 and not 10. Digits to the left of the binary point represent increasing powers of two, while digits to the right of the binary point represent decreasing powers of two:</p>
<p><span>
<p>1</p>
<p>2<sup>3</sup></p>
</span>
<span>
<p>0</p>
<p>2<sup>2</sup></p>
</span>
<span>
<p>0</p>
<p>2<sup>1</sup></p>
</span>
<span>
<p>1</p>
<p>2<sup>0</sup></p>
</span>
<span>
<p>.</p>

</span>
<span>
<p>0</p>
<p>2<sup>−1</sup></p>
</span>
<span>
<p>1</p>
<p>2<sup>−2</sup></p>
</span>
<span>
<p>0</p>
<p>2<sup>−3</sup></p>
</span>
<span>
<p>1</p>
<p>2<sup>−4</sup></p>
</span>
</p>
<p>When ambiguous I’ll use <sub>2</sub> to mean the number is in base-2. As such, 1000<sub>2</sub> is not a thousand, but 2<sup>3</sup> i.e. eight. To get the decimal value of the discussed 1001.0101<sub>2</sub> we simply sum up the powers of two that have the bit set: 8 + 1 + 0.25 + 0.0625, ending up with the value of 9.3125.</p>
<p>Binary numbers can use scientific notation as well. Since we’re shifting the binary point by three places, the exponent ends up having the value of 3:</p>

<p><a id="implicit_bit_not_needed"></a>
Similarly to scientific notation in base-10, we also moved the binary point to right after the first non-zero digit of the original representation. However, since the only non-zero digit in base-2 system is 1, <em>every</em> non-zero binary number in scientific notation starts with a 1.</p>
<p>We can round the number to a shorter form:</p>

<p>Or show that we’re more accurate by storing 11 binary digits:</p>

<p>If you’ve grasped everything that we’ve discussed so far then congratulations – you understand how floating point numbers work.</p>

<p>Floating points numbers are just numbers in base-2 scientific notation with the following two restrictions:</p>
<ul>
<li>limited number of digits in the significand</li>
<li>limited range of the exponent – it can’t be greater than some maximum limit and also can’t be less than some minimum limit</li>
</ul>
<p>That’s (almost) all there is to them.</p>
<p>Different floating point types have different number of significand digits and allowed exponent range. For example, a <code>float</code> has 24 binary digits (i.e. bits) of significand and the exponent range of [−126, +127], where “[” and “]” denote inclusivity of the range (e.g. +127 is valid, but +128 is not). Here’s a number with a decimal value of −616134.5625 that can fit in a <code>float</code>:</p>
<div>
<p><span>−</span><span>1.00101100110110001101001</span>×2<span><sup>19</sup></span></p>
</div>
<p>Unfortunately, the number of bits of significand in a <code>float</code> is limited, so some real values may not be perfectly representable in the floating point form. A decimal number 0.2 has the following base-2 representation:</p>

<p>The <span>overline</span> (technically known as <a href="https://en.wikipedia.org/wiki/Vinculum_(symbol)">vinculum</a>) indicates a forever repeating value. The 25<sup>th</sup> and later significant digits of the perfect base-2 scientific representation of that number won’t fit in a <code>float</code> and have to be accounted for by rounding the remaining bits. The full significand:</p>
<div>
<p>
1.10011001100110011001100<span><span>1</span><span>1</span><span>0</span><span>0</span></span>
</p>
</div>
<p>Will be rounded to:</p>
<div>
<p>
1.10011001100110011001101
</p>
</div>
<p>After multiplication by the exponent the resulting number has a <em>different</em> decimal value than the perfect 0.2:</p>
<div>
<p>
0.20000000298023223876953125
</p>
</div>
<p>If we tried rounding the full significand down:</p>
<div>
<p>
1.10011001100110011001100
</p>
</div>
<p>The resulting number would be equal to:</p>
<div>
<p>
0.199999988079071044921875​
</p>
</div>
<p>No matter what we do, the limited number of bits in the significand prevents us from getting the correct result. This explains why some decimal numbers don’t have their exact floating point representation.</p>
<p>Similarly, since the value of the exponent is limited, many huge and many tiny numbers won’t fit in a <code>float</code>: neither 2<sup>200</sup> nor 2<sup>−300</sup> can be represented since they don’t fall into the allowed exponent range of [−126, +127].</p>

<p>Knowing the number of bits in the significand and the allowed range of the exponent we can start encoding floating point numbers into their binary representation. We’ll use the number −2343.53125 which has the following representation in base-2 scientific notation:</p>
<div>
<p><span>−</span><span>1.0010010011110001</span>×2<span><sup>11</sup></span></p>
</div>
<h2 id="the-sign">The Sign<a href="https://ciechanow.ski/exposing-floating-point/#the-sign" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>The sign is easy – we just need 1 bit to express whether the number is positive or negative. IEEE 754 uses the value of <code>0</code> for the former and <code>1</code> for the latter. Since the discussed number is negative we’ll use one:</p>
<p><span>1</span>
</p>
<h2 id="the-significand">The Significand<a href="https://ciechanow.ski/exposing-floating-point/#the-significand" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>For the significand of a <code>float</code> we need 24 bits. However, per what <a href="#implicit_bit_not_needed">we’ve already discussed</a>, the first digit of the significand in base-2 is always 1, so the format cleverly skips it to save a bit. We just have to remember it’s there when doing calculations. We copy the remaining 23 digits verbatim while filling in the missing bits at the end with 0s:</p>
<p><span>00100100111100010000000</span>
</p>
<p>The leading “1” we skipped is often referred to as an “implicit bit”.</p>
<h2 id="the-exponent">The Exponent<a href="https://ciechanow.ski/exposing-floating-point/#the-exponent" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>Since the exponent range of [−126, +127] allows 254 possible values, we’ll need 8 bits to store it. To avoid special handling of negative exponent values we’ll add a fixed <em>bias</em> to make sure no encoded exponent is negative.</p>
<p>To obtain a <em>biased</em> exponent we’ll use the bias value of 127. While 126 would work for regular range of exponents, using 127 will let us reserve a biased value of 0 for <a href="#special-values">special purposes</a>. Biasing is just a matter of shifting all values to the right:</p>


<div>
    <p><img src="https://ciechanow.ski/images/float_bias.svg" alt="The bias in a float" width="460" height="180"/> 
    </p><p><span>
            <p>The bias in a <code>float</code></p>
        </span>
</p></div>

<p>For the discussed number we have to shift its exponent of 11 by 127 to get 138, or 10001010<sub>2</sub> and that’s what we will encode as the exponent:</p>
<p><span>10001010</span>
</p>
<h2 id="putting-it-all-together">Putting it All Together<a href="https://ciechanow.ski/exposing-floating-point/#putting-it-all-together" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>To conform with the standard we’ll put the sign bit first, then the exponent bits, and finally, the significand bits. While seemingly arbitrary, the order is part of the standard’s <a href="#raw-integer-value">ingenuity</a>. By sticking all the pieces together a <code>float</code> is born:</p>
<p><span>1</span><span>10001010</span><span>00100100111100010000000</span>
</p>
<p>The entire encoding occupies 32 bits. To verify we did things correctly we can fire up LLDB and let the hacky <a href="https://en.wikipedia.org/wiki/Type_punning">type punning</a> do its work:</p>
<div><pre tabindex="0"><code data-lang="plain">(lldb) p -2343.53125f
(float) $0 = -2343.53125

(lldb) p/t *(uint32_t *)&amp;$0
(uint32_t) $1 = 0b11000101000100100111100010000000
</code></pre></div><p>While neither C nor C++ standards <em>technically</em> require a <code>float</code> or a <code>double</code> to be represented using IEEE 754 format, the rest of this article will sensibly assume so.</p>
<p>The same procedure of encoding a number in base-2 scientific notation can be repeated for almost any number, however, some of them require special handling.</p>

<p>The <code>float</code> exponent range allows 254 different values and with a bias of 127 we’re left with two yet unused biased exponent values: 0 and 255. Both are employed for very useful purposes.</p>
<h2 id="a-map-of-floats">A Map of Floats<a href="https://ciechanow.ski/exposing-floating-point/#a-map-of-floats" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>A dry description doesn’t really paint a picture, so let’s present all the special values visually. In the following plot every dot represents a unique positive <code>float</code>:</p>


<div>
    <p><img src="https://ciechanow.ski/images/float_special_values.svg" alt="All the special values" id="float_map" width="560" height="400"/> 
    </p><p><span>
            <p>All the special values</p>
        </span>
</p></div>

<p><span id="float_show_bw">If you have trouble seeing color you can <a href="#" onclick="show_bw_map();return false;">switch to the alternative version</a>.</span>
<span id="float_show_color">If you don’t have trouble seeing color you can <a href="#" onclick="show_color_map();return false;">switch to the color version</a>.</span>
Notice the necessary truncation of a large part of exponents and of a gigantic part of significand values. At your current viewing size you’d have to scroll through roughly <span id="widths_worth"></span> window widths to see all the values of the significand.</p>
<p>We’ve already discussed all the unmarked dots — the normal floats. It’s time to dive into the remaining values.</p>
<h2 id="zero">Zero<a href="https://ciechanow.ski/exposing-floating-point/#zero" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>A <code>float</code> number with biased exponent value of 0 <em>and</em> all zeros in significand is interpreted as positive or negative 0. The arbitrary value of sign (shown as <code>_</code>) decides which 0 we’re dealing with:</p>
<p><span>_</span><span>00000000</span><span>00000000000000000000000</span>
</p>
<p>Yes, the floating point standard specifies both +0.0 and −0.0. This concept is actually useful because it tells us from which “direction” the 0 was approached as a result of storing value too small to be represented in a <code>float</code>. For instance <code>-10e-30f / 10e30f</code> won’t fit in a <code>float</code>, however, it will produce the value of <code>-0.0</code>.</p>
<p>When working with zeros note that <code>0.0 == -0.0</code> is true even though the two zeros have different encoding. Additionally, <code>-0.0 + 0.0</code> is equal to <code>0.0</code>, so by default the compiler can’t optimize <code>a + 0.0</code> into just <code>a</code>, however, you can <a href="https://stackoverflow.com/a/22135559/558816">set flags</a> to relax the strict conformance.</p>
<h2 id="infinity">Infinity<a href="https://ciechanow.ski/exposing-floating-point/#infinity" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>A <code>float</code> number with maximum biased exponent value <em>and</em> all zeros in significand is interpreted as positive or negative infinity depending on the value of the sign bit:</p>
<p><span>_</span><span>11111111</span><span>00000000000000000000000</span>
</p>
<p>Infinity arises as a result of rounding a value that’s too large to fit in the type (assuming default rounding mode). In case of a <code>float</code>, any number in base-2 scientific notation with exponent greater than 127 will become infinity. You can also use macro <code>INFINITY</code> directly.</p>
<p>The positive and negative zeros become useful again since dividing a positive value by +0.0 will produce a positive infinity, while dividing it by −0.0 will produce a negative infinity.</p>
<p>Operations involving <em>finite</em> numbers and infinities are actually well defined and follow common sense property of keeping infinities infinite:</p>
<ul>
<li>any finite value added to or subtracted from ±infinity ends up as ±infinity</li>
<li>any finite positive value multiplied by ±infinity ends up as ±infinity, while any finite negative value multiplied by ±infinity flips its sign to ∓infinity</li>
<li>division by a finite non-zero value works similarly to multiplication (think of division as multiplication by an inverse)</li>
<li>square root of a +infinity is +infinity</li>
<li>any finite value divided by ±infinity will become ±0.0 depending on the signs of the operands</li>
</ul>
<p>In other words, infinities are so big that any shifting or scaling won’t affect their infinite magnitude, only their sign may flip. However, some operations throw a wrench into that simple rule.</p>
<h2 id="nans">NaNs<a href="https://ciechanow.ski/exposing-floating-point/#nans" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>A <code>float</code> number with maximum biased exponent value <em>and</em> non-zero significand is interpreted as NaN – Not a Number:</p>
<p><span>_</span><span>11111111</span><span>    at least one 1     </span>
</p>
<p>The easiest way to obtain NaN directly is by using <code>NAN</code> macro. In practice though, NaN arises in the following set of operations:</p>
<ul>
<li>±0.0 multiplied by ±infinity</li>
<li>−infinity added to +infinity</li>
<li>±0.0 divided by ±0.0</li>
<li>±infinity divided by ±infinity</li>
<li>square root of a negative number (−0.0 is fine though!)</li>
</ul>
<p>If the floating point variable is uninitialized, it’s also somewhat likely to contain NaNs. By default the result of any operation involving NaNs will result in a NaN as well. That’s <em>one</em> of the reasons why compiler can’t optimize seemingly simple cases like <code>a + (b - b)</code> into just <code>a</code>. If <code>b</code> is NaN the result of the entire operation <em>has to</em> be NaN too.</p>
<p>NaNs are not equal to anything, even to themselves. If you were to look at your compiler’s implementation of <code>isnan</code> function you’d see something like <code>return x != x;</code>.</p>
<p>It’s worth pointing out how many different NaN values there are – a <code>float</code> can store 2<sup>23</sup>−1 (over 8 million) different NaNs, while a <code>double</code> fits 2<sup>52</sup>−1 (over 4.5 quadrillion) different NaNs. It may seem wasteful, but the standard specifically made the pool large for, quote, “uninitialized variables and arithmetic-like enhancements”. You can read about one of those uses in <a href="https://anniecherkaev.com/about/">Annie Cherkaev</a>’s very interesting <a href="https://anniecherkaev.com/the-secret-life-of-nan">“the secret life of NaN”</a>. Her article also discusses the concepts of quiet and signaling NaNs.</p>
<h2 id="maximum--minimum">Maximum &amp; Minimum<a href="https://ciechanow.ski/exposing-floating-point/#maximum--minimum" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>The exponent range limit puts some constraints on the minimum and the maximum value that can be represented with a <code>float</code>. The maximum value of that type is 2<sup>128</sup> − 2<sup>104</sup> (3.40282347×10<sup>38</sup>). The biased exponent is one short of maximum value and the significand is all lit up:</p>
<p><span>0</span><span>11111110</span><span>11111111111111111111111</span>
</p>
<p>The smallest <em>normal</em> <code>float</code> is 2<sup>−126</sup> (roughly 1.17549435×10<sup>−38</sup>). Its biased exponent is set to 1 and the significand is cleared out:</p>
<p><span>0</span><span>00000001</span><span>00000000000000000000000</span>
</p>
<p>In C the minimum and maximum values can be accessed with <code>FLT_MIN</code> and <code>FLT_MAX</code> macros respectively. While <code>FLT_MIN</code> is the smallest normal value, it’s not <em>the</em> smallest value a <code>float</code> can store. We can squeeze things down even more.</p>
<h2 id="subnormals">Subnormals<a href="https://ciechanow.ski/exposing-floating-point/#subnormals" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>When discussing base-2 scientific notation we assumed the numbers were normalized, i.e. the first digit of the significand was 1:</p>
<div>
<p><span>+</span><span>1.00101100110110001101001</span>×2<span><sup>19</sup></span></p>
</div>
<p>The range of subnormals (also known as denormals) relaxes that requirement. When the biased exponent is set to 0, the exponent is interpreted as −126 (<em>not</em> −127 despite the bias), and the leading digit is assumed to be 0:</p>
<div>
<p><span>+</span><span>0.00000000000110001101001</span>×2<span><sup>−126</sup></span></p>
</div>
<p>The encoding doesn’t change, when performing calculations we just have to remember that this time the implicit bit is 0 and not 1:</p>
<p><span>0</span><span>00000000</span><span>00000000000110001101001</span>
</p>
<p>While subnormals let us store smaller values than the minimum normal value, it comes at the cost of precision. As the significand decreases we effectively have fewer bits to work with, which is more apparent after normalization:</p>

<p>The classic example for the need for subnormals is based on simple arithmetic. If two floating point values are equal to each other:</p>

<p>Then by simply rearranging the terms it follows that their difference should be equal to 0:</p>

<p>Without subnormal values that simple assumption would not be true! Consider <code>x</code> set to a valid normal <code>float</code> number:</p>
<div>
<p><span>+</span><span>1.01100001111101010000101</span>×2<span><sup>−124</sup></span></p>
</div>
<p>And <code>y</code> as:</p>
<div>
<p><span>+</span><span>1.01100000011001011100001</span>×2<span><sup>−124</sup></span></p>
</div>
<p>The numbers are distinct (observe the last few bits of significand). Their difference is:</p>
<div>
<p><span>+</span><span>1.1000111101001</span>×2<span><sup>−132</sup></span></p>
</div>
<p>Which is outside of the normal range of a <code>float</code> (notice the exponent value smaller than −126). If it wasn’t for subnormals the difference after rounding would be equal to 0, thus implying the equality of not equal numbers.</p>
<p>On a historical note, subnormals were very controversial part of the IEEE 754 standardization process, you can read about it more in <a href="https://people.eecs.berkeley.edu/~wkahan/ieee754status/754story.html">“An Interview with the Old Man of Floating-Point”</a>.</p>

<p>Due to the fixed number of bits in the significand floating point numbers can’t store arbitrarily precise values. Moreover, the exponential part causes the distribution of values in a <code>float</code> to be uneven. In the picture below each tick on the horizontal axis represents a unique float value:</p>


<div>
    <p><img src="https://ciechanow.ski/images/float_space.svg" alt="Chunky float values" width="680" height="160"/> 
    </p><p><span>
            <p>Chunky <code>float</code> values</p>
        </span>
</p></div>

<p>Notice how the powers of 2 are special – they define the transition points for the change of “chunkiness”. The distance between representable <code>float</code> values in between neighboring powers of two (i.e. between 2<sup>n</sup> and 2<sup>n + 1</sup>) are constant and we can jump between them by changing the significand by 1 bit.</p>
<p>The larger the exponent the “larger” the 1 bit of significand is. For example, the number 0.5 has the exponent value of −1 (since 2<sup>−1</sup> is 0.5) and 1 bit of its significand jumps by 2<sup>−24</sup>. For the number 1.0 the step is equal to 2<sup>−23</sup>. The width of the jump at 1.0 has a dedicated name – <a href="https://en.wikipedia.org/wiki/Machine_epsilon">machine epsilon</a>. For a <code>float</code> it can be accessed via <code>FLT_EPSILON</code> macro.</p>
<p>Starting at 2<sup>23</sup> (decimal value of 8388608) increasing significand by 1 increases the decimal value of float by 1.0. As such, 2<sup>24</sup> (16777216 in base-10) is the limit of the range of integers that can be stored in a <code>float</code> without omitting <em>any</em> of them. The next float has the value of 16777218, the value of 16777217 can’t be represented in a <code>float</code>:</p>


<div>
    <p><img src="https://ciechanow.ski/images/float_space_integers.svg" alt="The end of the gapless region" width="460" height="170"/> 
    </p><p><span>
            <p>The end of the gapless region</p>
        </span>
</p></div>

<p>Note that the type can handle <em>some</em> larger integers as well, however, 2<sup>24</sup> defines the end of the gapless region.</p>

<p>With a fixed exponent increasing the significand by 1 bit jumps between equidistant float values, however, the format has more tricks up its sleeve. Consider 2097151.875 stored in a <code>float</code>:</p>
<p><span>0</span><span>10010011</span><span>11111111111111111111111</span>
</p>
<p>Ignoring the division into three parts for a second, we can think of the number as a string of 32 bits. Let’s try interpreting them as a 32-bit unsigned integer:</p>
<p><span>01001001111111111111111111111111</span>
</p>
<p>As a quick experiment, let’s add one to the value…</p>
<p><span>01001010000000000000000000000000</span>
</p>
<p>…and put the bits verbatim back into the <code>float</code> format:</p>
<p><span>0</span><span>10010100</span><span>00000000000000000000000</span>
</p>
<p>We’ve just obtained the value of 2097152.0, which is the next representable <code>float</code> – the type can’t store <em>any</em> other values between this and the previous one.</p>
<p>Notice how adding one overflowed the significand and added one to the exponent value. This is the beauty of putting the exponent part <em>before</em> the significand. It lets us easily obtain the next/previous representable float (away/towards zero) by simply increasing/decreasing its raw integer value.</p>
<p>Incrementing the integer representation of the maximum <code>float</code> value by one? You get infinity. Decrementing the integer form of the minimum <code>float</code>? You enter the world of subnormals. Decrease it for the smallest subnormal? You get zero. Things fall into place just perfectly. The two caveats with this trick is that it won’t jump from +0.0 to −0.0 and vice versa, moreover, infinities will “increment” to NaNs, and the last NaN will increment to zero.</p>

<p>So far we’ve focused our discussion on a <code>float</code>, but its popular bigger cousin <code>double</code> and the less common <code>half</code> are also worth looking at.</p>
<h2 id="double">Double<a href="https://ciechanow.ski/exposing-floating-point/#double" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>In base-2 scientific notation a <code>double</code> has 53 digits of significand and exponent range of [−1022, +1023] resulting in an encoding with 11 bits dedicated to exponent and 52 bits to significand to form a 64-bit encoding:</p>
<p><span>1</span><span>01111110100</span><span>1011000101101101100100111101101110100010001101101000</span>
</p>
<h2 id="half">Half<a href="https://ciechanow.ski/exposing-floating-point/#half" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>Half-float is used relatively often in computer graphics. In base-2 scientific notation a <code>half</code> has 11 digits of significand and exponent range of [−14, +15] resulting in an encoding with 5 bits dedicated to exponent and 10 bits to significand creating a 16-bit type:</p>
<p><span>0</span><span>10110</span><span>1101010001</span>
</p>
<p><code>half</code> is really compact, but also has very small range of representable values. Additionally, given only 5 bits of the exponent, almost 1/32 of the possible <code>half</code> values are dedicated to NaNs.</p>
<h2 id="larger-types">Larger Types<a href="https://ciechanow.ski/exposing-floating-point/#larger-types" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>IEEE 754 specifies <a href="https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format">128-bit floating point format</a>, however, native hardware support is <a href="https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format#Hardware_support">very limited</a>. Some compilers will <a href="https://godbolt.org/z/ATAFss">let you use it</a> when <code>__float128</code> type is used, but the operations are usually done in software.</p>
<p>The standard also suggests equations for obtaining the number of exponent and significand bits in higher precision formats (e.g. <a href="https://en.wikipedia.org/wiki/Octuple-precision_floating-point_format">256-bit</a>), but I think it’s fair to say those are rather impractical.</p>
<h2 id="same-behavior">Same Behavior<a href="https://ciechanow.ski/exposing-floating-point/#same-behavior" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>While all IEEE 754 types have different lengths, they all behave the same way:</p>
<ul>
<li>±0.0 always has all the bits of the exponent and the significand set to zero</li>
<li>±infinity has all ones in the exponent and all zeros in the significand</li>
<li>NaNs have all ones in the exponent and a non-zero significand</li>
<li>the encoded exponent of subnormals is 0</li>
</ul>
<p>The only difference between the types is in how many bits they dedicate to the exponent and to the significand.</p>

<p>While in practice many floating point calculations are performed using the same type throughout, a type change is often unavoidable. For example, JavaScript’s <code>Number</code> is just a <code>double</code>, however, WebGL deals with <code>float</code> values. Conversions to a larger and a smaller type behave differently.</p>
<h2 id="conversion-to-a-larger-type">Conversion to a Larger Type<a href="https://ciechanow.ski/exposing-floating-point/#conversion-to-a-larger-type" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>Since a <code>double</code> has more bits of the significand and of the exponent than a <code>float</code> and so does a <code>float</code> compared to a <code>half</code> we can be sure that converting a floating-point value to a higher precision type will maintain the exact stored value.</p>
<p>Let’s see how this pans out for a <code>half</code> value of 234.125. Its binary representation is:</p>
<p><span>0</span>      <span>10110</span><span>1101010001</span>                                          
</p>
<p>The same number stored in a <code>float</code> has the following representation:</p>
<p><span>0</span>   <span>10000110</span><span>11010100010000000000000</span>                             
</p>
<p>And in a <code>double</code>:</p>
<p><span>0</span><span>10000000110</span><span>1101010001000000000000000000000000000000000000000000</span>
</p>
<p>Note that the new significand bits in a larger format are filled with zeros, which simply follows from scientific notation. The new exponent bits are filled with 0s when the highest bit is 1, and with 1s when the highest bit is 0 (you can see it by changing type e.g. for <a href="https://float.exposed/0x2f40">0.11328125</a>) – a result of unbiasing the value with original bias then biasing again with the new bias.</p>
<h2 id="conversion-to-a-smaller-type">Conversion to a Smaller Type<a href="https://ciechanow.ski/exposing-floating-point/#conversion-to-a-smaller-type" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>The following should be fairly unsurprising, but it’s worth going through an example. Consider a <code>double</code> value of <a href="https://float.exposed/0xc111454028400000">−282960.039306640625</a>:</p>
<p><span>1</span><span>10000010001</span><span>0001010001010100000000101000010000000000000000000000</span>
</p>
<p>When converting to a <code>float</code> we have to account for the significand bits that don’t fit, which is by default done using <a href="https://en.wikipedia.org/wiki/Rounding#Round_half_to_even">rounding-to-nearest-even</a> method. As such, the same number stored in a <code>float</code> has the following representation:</p>
<p><span>1</span>   <span>10010001</span><span>00010100010101000000001</span>                             
</p>
<p>The decimal value of this float is <a href="https://float.exposed/0xc88a2a01">−282960.03125</a>, i.e. a different number than the one stored in a <code>double</code>. Converting to a <code>half</code> produces:</p>
<p><span>1</span>      <span>11111</span><span>0000000000</span>                                          
</p>
<p>What happened here? The exponent value of 18 that fits perfectly fine in a <code>float</code> is too large for the maximum exponent of 15 that a <code>half</code> can handle and the resulting value is −infinity.</p>
<p>Converting from a higher to a lower precision floating point type will maintain the exact value if the significand bits that don’t fit in the smaller type are 0s <em>and</em> the exponent value can be represented in the smaller type. If we were to convert the previously examined <code>234.125</code> from a <code>double</code> to a <code>float</code> or to a <code>half</code> it would keep its exact value in all three types.</p>
<h3 id="a-sidenote-on-rounding">A Sidenote on Rounding<a href="https://ciechanow.ski/exposing-floating-point/#a-sidenote-on-rounding" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h3>
<p>While <a href="https://en.wikipedia.org/wiki/Rounding#Round_half_up">round-half-up</a> (“If the fraction is .5 – round up”) is the common rounding rule used in everyday life, it’s actually quite flawed. Consider the results of the following made up survey:</p>
<ul>
<li>725 responders said their favorite color is <span>red</span></li>
<li>275 responders said their favorite color is <span>green</span></li>
</ul>
<p>The distribution of votes is 72.5% and 27.5% respectively. If we wanted to round the percentages to integer values and were to use round-half-up we’d end up with the following outcome: 73% and 28%. To everyone’s dissatisfaction we just made the survey results add up to 101%.</p>
<p>Round-to-nearest-even solves this problem by, unsurprisingly, rounding to nearest even value. 72.5% becomes 72%, 27.5% becomes 28%. The expected sum of 100% is restored.</p>
<h2 id="conversion-of-special-values">Conversion of Special Values<a href="https://ciechanow.ski/exposing-floating-point/#conversion-of-special-values" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>Neither NaNs nor infinities follow the usual conventions. Their special rule is very straightforward: NaNs remain NaNs and infinities remain infinities in all the type conversions.</p>

<p>Working with floating point numbers often requires printing their value so that it can be restored accurately — every bit should maintain its exact value. When it comes to <code>printf</code>-style formatting characters, <code>%f</code> and <code>%e</code> are commonly used. Sadly, they often fail to maintain enough precision:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="objc"><span>float</span> <span>f0</span> <span>=</span> <span>3.0080111026763916015f</span><span>;</span>
<span>float</span> <span>f1</span> <span>=</span> <span>3.0080118179321289062f</span><span>;</span>

<span>printf</span><span>(</span><span>&#34;%f</span><span>\n</span><span>&#34;</span><span>,</span> <span>f0</span><span>);</span>
<span>printf</span><span>(</span><span>&#34;%f</span><span>\n</span><span>&#34;</span><span>,</span> <span>f1</span><span>);</span>
<span>printf</span><span>(</span><span>&#34;%e</span><span>\n</span><span>&#34;</span><span>,</span> <span>f0</span><span>);</span>
<span>printf</span><span>(</span><span>&#34;%e</span><span>\n</span><span>&#34;</span><span>,</span> <span>f1</span><span>);</span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>Produces:</p>
<div><pre tabindex="0"><code data-lang="plain">3.008011
3.008011
3.008011e+00
3.008011e+00
</code></pre></div><p>However, those two floating point numbers are <em>not</em> the same and store different values. <code>f0</code> is:</p>
<p><span>0</span><span>10000000</span><span>10000001000001101000001</span>
</p>
<p>And <code>f1</code> differs from <code>f0</code> by 3:</p>
<p><span>0</span><span>10000000</span><span>10000001000001101000100</span>
</p>
<p>The usual solution to this problem is to specify the precision manually to the maximum number of digits. We can use <code>FLT_DECIMAL_DIG</code> macro (value of 9) for this purpose:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="objc"><span>float</span> <span>f0</span> <span>=</span> <span>3.0080111026763916015f</span><span>;</span>
<span>float</span> <span>f1</span> <span>=</span> <span>3.0080118179321289062f</span><span>;</span>

<span>printf</span><span>(</span><span>&#34;%.*e</span><span>\n</span><span>&#34;</span><span>,</span> <span>FLT_DECIMAL_DIG</span><span>,</span> <span>f0</span><span>);</span>
<span>printf</span><span>(</span><span>&#34;%.*e</span><span>\n</span><span>&#34;</span><span>,</span> <span>FLT_DECIMAL_DIG</span><span>,</span> <span>f1</span><span>);</span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>Yields:</p>
<div><pre tabindex="0"><code data-lang="plain">3.008011102e+00
3.008011817e+00
</code></pre></div><p>Unfortunately, it will print the long form even for simple values, e.g. <code>3.0f</code> will be printed as <code>3.000000000e+00</code>. It seems that <a href="https://stackoverflow.com/a/19897395/558816">there is no way</a> to configure the printing of floating point values to automatically maintain exact number of <em>decimal</em> digits needed to accurately represent the value.</p>
<h2 id="hexadecimal-form">Hexadecimal Form<a href="https://ciechanow.ski/exposing-floating-point/#hexadecimal-form" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"/></a> </h2>
<p>Luckily, hexadecimal form comes to the rescue. It uses <code>%a</code> specifier and prints the shortest, exact representation of floating point number in a hexadecimal form:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="objc"><span>float</span> <span>f0</span> <span>=</span> <span>3.0080111026763916015f</span><span>;</span>
<span>float</span> <span>f1</span> <span>=</span> <span>3.0080118179321289062f</span><span>;</span>

<span>printf</span><span>(</span><span>&#34;%a</span><span>\n</span><span>&#34;</span><span>,</span> <span>f0</span><span>);</span>
<span>printf</span><span>(</span><span>&#34;%a</span><span>\n</span><span>&#34;</span><span>,</span> <span>f1</span><span>);</span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>Produces:</p>
<div><pre tabindex="0"><code data-lang="plain">0x1.810682p+1
0x1.810688p+1
</code></pre></div><p>The hexadecimal constant can be used verbatim in code or as an input to <code>scanf</code>\<code>strtof</code> on any reasonable compiler and platform. To verify the results we can fire up LLDB one more time:</p>
<div><pre tabindex="0"><code data-lang="plain">(lldb) p 0x1.810682p+1f
(float) $0 = 3.0080111

(lldb) p 0x1.810688p+1f
(float) $1 = 3.00801182

(lldb) p/t *(uint32_t *)&amp;$0
(uint32_t) $2 = 0b01000000010000001000001101000001

(lldb) p/t *(uint32_t *)&amp;$1
(uint32_t) $3 = 0b01000000010000001000001101000100
</code></pre></div><p>The hexadecimal form is exact and concise – each set of four bits of the significand is converted to the corresponding hex digit. Using our example values: <code>1000</code> becomes <code>8</code>, <code>0001</code> becomes <code>1</code> and so on. An unbiased exponent just follows the letter <code>p</code>. You can find more details about the <code>%a</code> specifier in <a href="https://www.exploringbinary.com/hexadecimal-floating-point-constants/">“Hexadecimal Floating-Point Constants”</a>.</p>
<p>Nine digits may be enough to <em>maintain</em> the exact value, but it’s nowhere near the number of digits required to show the floating point number in its <em>full</em> decimal glory.</p>

<p>While not every decimal number can be represented using floating point numbers (the infamous 0.1), <em>every</em> floating point number has its own exact decimal representation. The following example is done on a <code>half</code> since it’s much more compact, but the method is equivalent for a <code>float</code> and a <code>double</code>.</p>
<p>Let’s consider the value of 3.142578125 stored in a <code>half</code>:</p>
<p><span>0</span><span>10000</span><span>1001001001</span>
</p>
<p>The equivalent value in scientific base-2 notation is:</p>

<p>Firstly, we can convert the significand part to an integer by multiplying it by 1:</p>

<p>Which we can cleverly expand:</p>
<div>
<p><span>1.1001001001</span>×2<sup>10</sup>×2<sup>−10</sup></p>
</div>
<p>To obtain an integer times a power of two:</p>

<p>Then we can combine the fractional part with the exponent part:</p>

<p>And in decimal form:</p>

<p>We can get rid of the power of two by multiplying it by a cleverly written value of 1 yet another time:</p>

<p>We can pair every 2 with every 5 to obtain:</p>

<p>Putting back all the pieces together we end up with a product of two integers and a shift of a decimal place encoded in the power of 10:</p>
<div>
<p>10<sup>−9</sup>×5<sup>9</sup>×1609 = 3.142578125</p>
</div>
<p>Coincidentally, the trick of multiplying by 5<sup>−n</sup>×5<sup>n</sup> also explains why negative powers of 2 are just powers of 5 with a shifted decimal place (e.g. 1/4 is 25/100, and 1/16 is 625/10000).</p>
<p>Even though the exact decimal representation always exists, it’s often cumbersome to use – some small numbers that can be stored in a <code>double</code> have <a href="https://float.exposed/0x000fffffffffffff">over 760 significant digits</a> of decimal representation!</p>

<p>My article is just a drop in the sea of resources about floating point numbers. Perhaps the most thorough technical write-ups on floating point numbers is <a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">“What Every Computer Scientist Should Know About Floating-Point Arithmetic”</a>. While very comprehensive, I find it difficult to get through. Almost five years have passed since I first <a href="https://ciechanow.ski/exploring-gpgpu-on-ios/#reasons">mentioned it</a> on this blog and, frankly, I’ve still limited my engagement to mostly skimming through it.</p>
<p>One of the most fascinating resources out there is <a href="https://twitter.com/BruceDawson0xB">Bruce Dawson</a>’s amazing <a href="https://randomascii.wordpress.com/category/floating-point/">series of posts</a>. Bruce dives into a ton of details about the format and its behavior. I consider many of his articles a must-read for any programmer who deals with floating point numbers on a regular basis, but if you only have time for one I’d go with <a href="https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/">“Comparing Floating Point Numbers, 2012 Edition”</a>.</p>
<p><a href="https://www.exploringbinary.com/">Exploring Binary</a> contains many detailed <a href="https://www.exploringbinary.com/tag/floating-point/">articles on floating point format</a>. As a delightful example, it <a href="https://www.exploringbinary.com/maximum-number-of-decimal-digits-in-binary-floating-point-numbers/">demonstrates</a> that the maximum number of significant digits in the decimal representation of a <code>float</code> is 112, while a <code>double</code> requires up to 767 digits.</p>
<p>For a different look on floating point numbers I recommend <a href="https://twitter.com/fabynou">Fabien Sanglard</a>’s <a href="http://fabiensanglard.net/floating_point_visually_explained/">“Floating Point Visually Explained”</a> – it shows an interesting concept of the exponent interpreted as a sliding window and the significand as an offset into that window.</p>

<p>Even though we’re done, I encourage you to go on. Any of the mentioned resources should let you discover something more in the vast space of floating point numbers.</p>
<p>The more I learn about IEEE 754 the more enchanted I feel. <a href="https://en.wikipedia.org/wiki/William_Kahan">William Kahan</a> with the aid of Jerome Coonen and Harold Stone created something truly beautiful and ever-lasting.</p>
<p>I genuinely hope this trip through the details of floating point numbers made them a bit less mysterious and showed you some of that beauty.</p>

    </div></div>
  </body>
</html>
