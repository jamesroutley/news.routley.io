<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.instantdb.com/essays/pg_upgrade">Original</a>
    <h1>A major Postgres upgrade with zero downtime</h1>
    
    <div id="readability-page-1" class="page"><div><p>
We’re Instant, a modern Firebase. <a href="https://www.instantdb.com/tutorial">Try out the demo</a>, you can spin up a database and make queries within a minute — no login required.
</p>

<p>Right before Christmas we discovered that our Aurora Postgres instance needed a major version upgrade. We found a great essay by the <a href="https://eng.lyft.com/postgres-aurora-db-major-version-upgrade-with-minimal-downtime-4e26178f07a0">Lyft team</a>, showing how they ran their upgrade with about 7 minutes of downtime.</p>
<p>We started with Lyft’s checklist but made some changes, particularly with how we switched masters. <strong>In our process we got to 0 seconds of downtime.</strong></p>
<p>Doing a major version upgrade is stressful, and reading other’s reports definitely helped us along the way. So we wanted to write an experience report of our own, in the hopes that it’s as useful to you as reading others were for us.</p>
<p>In this write-up we’ll share the path we took — from false starts, to gotchas, to the steps that ultimately worked. Fair warning, our system runs at a modest scale. We have less than a terabyte of data, we read about 1.8 million tuples per second, and write about 500 tuples per second as of this writing. If you run at a much higher scale, this may be less relevant to you.</p>
<p>With all that said, let’s get into the story!</p>

<p>Let’s start with a brief outline of our system:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/the_system.png" alt=""/></p>
<p>Browsers connect to sync servers. Sync servers keep track of active queries. Sync servers also listen to Postgres’ write-ahead log; they take transactions, find affected queries, and send novelty back to browsers. <sup id="marked-fnref-1"><a href="#marked-fn-1">[1]</a></sup> Crucially, all Instant databases are hosted under one Aurora Postgres instance. <sup id="marked-fnref-2"><a href="#marked-fn-2">[2]</a></sup></p>
<h2 id="trouble-erupts">Trouble Erupts</h2>
<p>After our open source launch in August <sup id="marked-fnref-3"><a href="#marked-fn-3">[3]</a></sup>, we experienced about a 100x increase in throughput. For the first 2 months, whenever we saw perf issues they usually lived in our Client SDK or the Sync Server. When we hit a new high in December though, our Aurora Postgres instance started to spike in CPU and stumble.</p>
<p>To give us breathing room, we kept upgrading the size of the machine, until we reached db.r6g.16xlarge. <sup id="marked-fnref-4"><a href="#marked-fn-4">[4]</a></sup> We had to do something about the queries we were writing.</p>
<h2 id="sometimes-new-is-better-than-old">Sometimes, new is better than old</h2>
<p>We started to reproduce slow queries locally and began to optimize them. Within the first hour we noticed something strange: one teammate constantly reported faster query results then the rest of us.</p>
<p>Turns out this teammate was running Postgres 16, while most of us (and our production instance) were running Postgres 13.</p>
<p>We did some more backtesting and realized that Postgres 16 improved many of the egregious queries by 30% or more. Not bad. There came our first learning: sometimes, just upgrading Postgres is a great way to improve perf. <sup id="marked-fnref-5"><a href="#marked-fn-5">[5]</a></sup></p>
<p>So we thought, let’s upgrade to Postgres 16. Now how do we go about it?</p>

<p>We were a team of 4 and we were in a crunch. If we could find a quick option we’d have been happy to take it. Here’s what we tried:</p>

<h2 id="1-in-place-upgradesbut-they-take-15-minutes">1) In-Place Upgrades...but they take 15 minutes</h2>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/in_place.png" alt=""/></p>
<p>The easiest choice would have been to run an in-place upgrade. Put the database in maintenance mode, upgrade major versions, then turn it back on again. In RDS console you can do this with a <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_UpgradeDBInstance.PostgreSQL.MajorVersion.html#USER_UpgradeDBInstance.Upgrading.Manual:~:text=the%20RDS%20API.-,Console,-To%20upgrade%20the">few button clicks</a>.</p>
<p>The big problem is the downtime. Your DB is in maintenance mode for the entirety of the upgrade. The Lyft team said an in-place upgrade would have caused them a <a href="https://eng.lyft.com/postgres-aurora-db-major-version-upgrade-with-minimal-downtime-4e26178f07a0#4831">30 minute</a> outage.</p>
<p>We wanted to test this for ourselves though, in a case smaller database upgraded more quickly. So we cloned our production database and tested an in-place upgrade. Even with our smaller size, it took about 15 minutes for the clone to come back online.</p>
<p>Crunch or not, a 15-minute outage was off the table for us. Since launch we had folks sign up across the U.S, Europe and Asia; traffic ebbed and flowed, but there wasn’t a period where 15 minutes of downtime felt tolerable.</p>

<h2 id="2-blue-green-deploymentsbut-you-cant-have-active-replication-slots">2) Blue-Green Deployments...but you can’t have active replication slots</h2>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/blue_green.png" alt=""/></p>
<p>Well, Aurora Postgres also has <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/blue-green-deployments-overview.html">blue-green deployments</a>. AWS spins up an upgraded replica for you, and you can switch masters with a button click. They promise about a minute of downtime.</p>
<p>With such little operational effort, a minute of downtime sounded like a great option for us.</p>
<p>So we cloned our DB and tested a blue-green deployment. Yup, the connection came back in a minute! It looked like we were done. Until we tried a full rehearsal.</p>
<p>We spun up a complete staging environment, this time with active sync servers and connected clients. Now the blue-green deployment would go on for 30 minutes, and then break with a configuration error:</p>
<blockquote>
<p>Creation of blue/green deployment failed due to incompatible parameter settings. See <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/blue-green-deployments-creating.html#blue-green-deployments-creating-preparing-postgres">link</a> to help resolve the issues, then delete and recreate the blue/green deployment.</p>
</blockquote>
<p>The next few hours was frustrating: we would change a setting, start again, wait 30 minutes, and invariably end up with the same error.</p>
<p>Once we exhausted the suggestions from in error message, we began a process of elimination: when did the upgrade work, and what change made it fail? Eliminating the sync servers revealed the issue: active replication slots.</p>
<p>Remember how our sync servers listen to Postgres’ write-ahead log? To do this, we opened <a href="https://www.postgresql.org/docs/current/logicaldecoding-explanation.html#LOGICALDECODING-REPLICATION-SLOTS">replication slots</a>. We couldn’t create a blue-green deployment when the master DB had active replication slots. The AWS docs did not mention this. <sup id="marked-fnref-6"><a href="#marked-fn-6">[6]</a></sup></p>
<p>At least this experience highlighted a learning: <em>always</em> run a rehearsal that’s as close to production as possible, you never know what you’ll find.</p>
<p>In order to stop using replication slots we’d have to disconnect our sync servers. But then we would lose reactivity, potentially for 30 minutes. Apps would appear broken if we queries were out of sync that long; blue-green deployments were off the table too.</p>

<p>When the managed options don’t work, it’s time to go manual. We knew that a manual upgrade would have to involve three steps:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/going_manual.png" alt=""/></p>
<p>First, we would stand up a new replica running Postgres 16 — Let’s call this machine &#34;16&#34;. Once 16 was running, we could get our sync servers to subscribe to 16. The remaining step would be to switch writes &#34;all in one go&#34; (what this meant TBD) to 16. When that was done, migration done.</p>
<p>Now to figure out the steps</p>

<p>The first problem was to create our replica running Postgres 16.</p>
<h2 id="a-clone-upgrade-replicate-led-tolost-data">a) Clone-Upgrade-Replicate led to...lost data</h2>
<p>Lyft had a great <a href="https://eng.lyft.com/postgres-aurora-db-major-version-upgrade-with-minimal-downtime-4e26178f07a0#a7df">series of steps</a> to create a replica, so we tried to follow it. There were three stages:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/clone_upgrade_replicate.png" alt=""/></p>
<p>First, we clone our database, then we upgrade our clone, and then we start replication. By the end, our clone would have become a replica running Postgres 16.</p>
<p>Steps 1 (clone) &amp; 2 (upgrade) worked great. The trouble started with step 3 (replicate).</p>
<h3 id="lost-pg-functions">Lost PG functions</h3>
<p>When we turned on replication, we saw this error:</p>
<pre><code><span>:ERROR: function is_jsonb_valid_timestamp(jsonb) does not exist at character 1</span>
</code></pre><p>That’s weird. We <em>did</em> have a custom Postgres function called <code>is_jsonb_valid_timestamp</code>. And the function existed on both machines; if we logged in with PSQL, we could write queries:</p>
<pre><code><span>select</span><span> is_jsonb_valid_timestamp</span><span>(</span><span>&#39;1724344362000&#39;</span><span>::jsonb</span><span>)</span><span>;</span>
</code></pre><pre><code><span> is_jsonb_valid_timestamp</span>
<span>--------------------------</span>
<span> t</span>
</code></pre><p>We thought maybe there was an error with our WAL level, or maybe some input worked in 13, but stopped working in 16.</p>

<h3 id="search-paths">Search paths</h3>
<p>So we went down a rabbit hole investigating and searching in <a href="https://www.postgresql.org/message-id/flat/D2B9F2A20670C84685EF7D183F2949E2373D64%40gigant.nidsa.net#8132cc2fa455dd1f1bb02c63cdd04678">PG’s mailing list.</a> Finally, we discovered the problem was <a href="https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH">search paths</a>. <sup id="marked-fnref-7"><a href="#marked-fn-7">[7]</a></sup></p>
<pre><code><span>show</span><span> search_path</span><span>;</span>
</code></pre><pre><code><span>   search_path</span>
<span>-----------------</span>
<span> &#34;$user&#34;, public</span>
</code></pre><p>Postgres stores custom functions in a <a href="https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PUBLIC">schema</a>. When you write a function in your query, PG uses a <code>search_path</code> to decide which schema to look into. During replication, Postgres was having trouble finding our function. To get around this issue, we <a href="https://github.com/instantdb/instant/pull/593">wrote a PR</a> to add the <code>public</code> prefix explicitly in all our function definitions:</p>
<pre><code><span></span>
<span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> is_jsonb_valid_timestamp</span><span>(</span><span>value</span><span> jsonb</span><span>)</span><span></span>
<span></span><span></span>
<span></span><span>create</span><span> </span><span>or</span><span> </span><span>replace</span><span> </span><span>function</span><span> </span><span>public</span><span>.</span><span>is_jsonb_valid_timestamp</span><span>(</span><span>value</span><span> jsonb</span><span>)</span>
</code></pre><p>Note to us: make sure to use <code>public</code> in all our function definitions. <sup id="marked-fnref-8"><a href="#marked-fn-8">[8]</a></sup></p>
<p>With PG functions working, 3) replicate ran smoothly! Or so we thought.</p>

<h3 id="missing-data">Missing data</h3>
<p>For all intents and purposes, our new clone looked like a functioning replica. But we wanted to absolutely make sure that we didn’t lose any data.</p>
<p>Thankfully, we had a special <code>transactions</code> table — it’s an immutable table we use internally <sup id="marked-fnref-9"><a href="#marked-fn-9">[9]</a></sup>:</p>
<pre><code><span>instant=&gt; \d transactions;</span>
<!-- -->
<span>   Column   |            Type             | -- ... </span>
<span>------------+-----------------------------+</span>
<span> id         | bigint                      |</span>
<span> app_id     | uuid                        |</span>
<span> created_at | timestamp without time zone |</span>
</code></pre><p>Since we never modify rows, we could also use the <code>transactions</code> table for quick sanity checks — was there any data lost in the table? Here’s the query we ran to do that:</p>
<pre><code><span></span>
<span></span><span>select</span><span> </span><span>max</span><span>(</span><span>id</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span>;</span><span></span>
<span></span><span>select</span><span> </span><span>count</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span><span></span>
<!-- -->
<span></span><span></span>
<span></span><span></span>
<span></span><span>select</span><span> </span><span>COUNT</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span>
</code></pre><p>To our surprise...we found 13 missing transactions! That definitely stumped us. We weren’t quite sure where the data loss came from <sup id="marked-fnref-10"><a href="#marked-fn-10">[10]</a></sup></p>
<h2 id="b-create-replicateworked-great">b) Create, Replicate...worked great!</h2>
<p>So we went back to the drawing board. One problem with our replica checklist was that it had about 13 steps in it. If we could remove the number of steps, perhaps we could kill whatever caused this data loss.</p>
<p>So we cooked up an alternate approach:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/create_replicate.png" alt=""/></p>
<p>Instead of creating, cloning, and then upgrading, we would start with a fresh database running Postgres 16, and replicate from scratch. Lyft chose to clone their DB, because they had over 30TB of data and could leverage <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Clone.html#Aurora.Clone.Overview">Aurora Cloning</a>. But we had less than a terabyte of data; starting replication from scratch wasn’t a big a deal for us. <sup id="marked-fnref-11"><a href="#marked-fn-11">[11]</a></sup></p>
<p>So we created a checklist and ended up with 7 steps:</p>

<div>

<h3>Checklist: Create an upgraded Replica</h3>

<div> 

<ol>
<li><p><strong>16: Create a new Postgres Aurora Database on Postgres 16.</strong></p>
<p> Make sure to set <code>wal_level = logical</code></p>
</li>
<li><p><strong>13: Extract the schema</strong></p>
<pre><code><span>pg_dump </span><span>${DATABASE_URL}</span><span> --schema-only -f dump.schema.sql</span>
</code></pre></li>
<li><p><strong>16: Import the schema into 16</strong></p>
<pre><code><span>psql </span><span>${NEW_DATABASE_URL}</span><span> -f dump.schema.sql</span>
</code></pre></li>
<li><p><strong>13: Create a publication</strong></p>
<pre><code><span>create</span><span> publication pub_all_table </span><span>for</span><span> </span><span>all</span><span> </span><span>tables</span><span>;</span>
</code></pre></li>
<li><p><strong>16: Create a subscription with copy_data = true</strong></p>
<pre><code><span>create</span><span> subscription pub_from_scratch </span>
<span>connection </span><span>&#39;host=host_here dbname=name_here port=5432 user=user_here password=password_here&#39;</span><span></span>
<span>publication pub_from_scratch</span>
<span></span><span>with</span><span> </span><span>(</span><span> </span>
<span>  copy_data </span><span>=</span><span> </span><span>true</span><span>,</span><span> create_slot </span><span>=</span><span> </span><span>true</span><span>,</span><span> enabled </span><span>=</span><span> </span><span>true</span><span>,</span><span> </span>
<span>  </span><span>connect</span><span> </span><span>=</span><span> </span><span>true</span><span>,</span><span> </span>
<span>  slot_name </span><span>=</span><span> </span><span>&#39;pub_from_scratch&#39;</span><span></span>
<span></span><span>)</span><span>;</span>
</code></pre></li>
<li><p><strong>Confirm that there’s no data loss</strong></p>
<pre><code><span> </span><span></span>
<span> </span><span>select</span><span> </span><span>max</span><span>(</span><span>id</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span>;</span><span></span>
<span> </span><span>select</span><span> </span><span>count</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span><span></span>
<!-- -->
<span> </span><span></span>
<span> </span><span></span>
<span> </span><span>select</span><span> </span><span>count</span><span>(</span><span>*</span><span>)</span><span> </span><span>from</span><span> </span><span>transactions</span><span> </span><span>where</span><span> id </span><span>&lt;</span><span> :max</span><span>-</span><span>id</span><span>;</span>
</code></pre></li>
<li><p><strong>16: Run vaccum analyze</strong></p>
<pre><code><span> vacuum </span><span>(</span><span>verbose</span><span>,</span><span> </span><span>analyze</span><span>,</span><span> </span><span>full</span><span>)</span><span>;</span>
</code></pre></li>
</ol>
</div>

</div>


<p>We ran step 6 with bated breath...and it all turned out well! <sup id="marked-fnref-12"><a href="#marked-fn-12">[12]</a></sup> Now we had a replica running Postgres 16.</p>

<p>Next step, to switch subscriptions. Let’s remind ourselves what we’re looking to do:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/switch_subs.png" alt=""/></p>
<p>We’d need to get our sync servers to create replication slots in 16, rather than 13.</p>
<p>To do this, we added a <code>next-database-url</code> variable to our sync servers. During startup, if <code>next-database-url</code> was set, sync servers would subscribe from from there:</p>
<pre><code><span></span>
<span></span><span></span>
<span></span><span>(</span><span>defn</span><span> start</span>
<span>  </span><span>(</span><span>[</span><span>process-id</span><span>]</span><span></span>
<span>    </span><span></span>
<span>    </span><span>(</span><span>wal/start-worker</span><span> </span><span>{</span><span>:conn-config</span><span></span>
<span>                      </span><span>(</span><span>or</span><span> </span><span>(</span><span>config/get-next-aurora-config</span><span>)</span><span></span>
<span>                          </span><span></span>
<span>                          </span><span></span>
<span>                          </span><span></span>
<span>                          </span><span></span>
<span>                          </span><span>(</span><span>config/get-aurora-config</span><span>)</span><span>)</span><span>}</span><span>)</span><span></span>
<span>    </span><span></span>
<span>    </span><span>)</span><span>)</span>
</code></pre><p>Once we deployed this change, sync servers replicated from 16. Phew, this was at least one step in the story that didn’t feel nerve-wracking!</p>

<p>Now to worry about writes:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/switch_writes.png" alt=""/></p>
<p>Ultimately, we needed to click some button and trigger a switch. To make the switch work, we’d need to follow two rules:</p>
<ol>
<li><p><strong>16 must be caught up</strong></p>
<p> If there are <em>any</em> writes in 13 that haven’t replicated to 16 yet, we can’t turn on writes to 16. Otherwise transactions would come in the wrong order</p>
</li>
<li><p><strong>Once caught up, all new writes must go to 16</strong></p>
<p> If <em>any</em> write accidentally goes to 13, we could lose data.</p>
</li>
</ol>
<p>So, how could we follow these rules?</p>
<h2 id="we-could-stop-the-worldbut-thats-downtime">We could stop the world...but that’s downtime</h2>
<p>The simplest way to switch writes would have been to stop the world:</p>
<ol>
<li>Turn off all writes.</li>
<li>Wait for 16 to catch up</li>
<li>Enable writes again — this time they all go to 16</li>
</ol>
<p>If we went with the ‘stop the world approach’, we’d have about the same kind of downtime as blue-green deployments: a minute or so.</p>
<p>We were okay with a minute of downtime. But we had already spent a day setting up our manual method, could we do better?</p>
<p>Since we were switching manually we had finer control over our connections. We realized that with just a little bit more work...we could have no downtime at all!</p>

<h2 id="or-we-could-write-an-algorithm-with-zero-downtime">Or we could write an algorithm with zero downtime!</h2>
<p>Our co-author Daniel shared an algorithm he used at his previous startup:</p>
<p><img src="https://www.instantdb.com/posts/pg_upgrade/no_downtime.png" alt=""/></p>
<p>First, we pause all new transactions. Then, we wait for active transactions to complete and for 16 to catch up. Finally we unpause all transactions and have them go to 16. If we did this right, we could switch major versions without any downtime at all!</p>
<h3 id="the-benefits-of-being-small">The benefits of being small</h3>
<p>Sounds good in theory, but it can be hard to pull off. Unless of course you run at a modest scale.</p>
<p>Our switching algorithm hinges on being able to control all active connections. If you have tons of machines, how could you control all active connections?</p>
<p>Well, since our throughput was still modest, we could temporarily scale our sync servers down to just one giant machine. Clojure and java came handy here too. We had threads and the JVM is efficient, so we could take full advantage of the <a href="https://instances.vantage.sh/aws/ec2/m6a.16xlarge?region=us-east-1&amp;os=linux&amp;cost_duration=monthly&amp;reserved_term=Standard.noUpfront">m6a.16xlarge</a> sync server we moved to for the switch.</p>
<h3 id="writing-out-a-failover-function">Writing out a failover function</h3>
<p>So we went forward and translated our zero-downtime algorithm into code. Here’s how it looked:</p>
<pre><code><span>(</span><span>defn</span><span> do-failover-to-new-db </span><span>[</span><span>]</span><span></span>
<span>  </span><span>(</span><span>let</span><span> </span><span>[</span><span>prev-pool aurora/-conn-pool</span>
<span>        next-pool </span><span>(</span><span>start-new-pool</span><span> next-config</span><span>)</span><span></span>
<span>        next-pool-promise </span><span>(</span><span>promise</span><span>)</span><span>]</span><span></span>
<!-- -->
<span>    </span><span></span>
<span>    </span><span>(</span><span>alter-var-root</span><span> </span><span>#</span><span>&#39;aurora/conn-pool </span><span>(</span><span>fn</span><span> </span><span>[</span><span>_</span><span>]</span><span> </span><span>(</span><span>fn</span><span> </span><span>[</span><span>]</span><span> </span><span>@</span><span>next-pool-promise</span><span>)</span><span>)</span><span>)</span><span></span>
<!-- -->
<span>    </span><span></span>
<span>    </span><span>(</span><span>Thread/sleep</span><span> </span><span>2500</span><span>)</span><span></span>
<span>    </span><span></span>
<span>    </span><span>(</span><span>sql/cancel-in-progress</span><span> sql/default-statement-tracker</span><span>)</span><span></span>
<!-- -->
<span>    </span><span></span>
<span>    </span><span>(</span><span>let</span><span> </span><span>[</span><span>tx </span><span>(</span><span>transaction-model/create!</span><span> aurora/-conn-pool</span>
<span>                                        </span><span>{</span><span>:app-id</span><span> </span><span>(</span><span>config/instant-config-app-id</span><span>)</span><span>}</span><span>)</span><span>]</span><span></span>
<span>      </span><span>(</span><span>loop</span><span> </span><span>[</span><span>i </span><span>0</span><span>]</span><span></span>
<span>        </span><span>(</span><span>if-let</span><span> </span><span>[</span><span>row </span><span>(</span><span>sql/select-one</span><span> next-pool</span>
<span>                                      </span><span>[</span><span>&#34;select * from transactions where app_id = ?::uuid and id = ?::bigint&#34;</span><span></span>
<span>                                      </span><span>(</span><span>config/instant-config-app-id</span><span>)</span><span> </span><span>(</span><span>:id</span><span> tx</span><span>)</span><span>]</span><span>)</span><span>]</span><span></span>
<span>          </span><span>(</span><span>println</span><span> </span><span>&#34;we are caught up!&#34;</span><span>)</span><span></span>
<span>          </span><span></span>
<span>          </span><span>(</span><span>do</span><span> </span><span>(</span><span>Thread/sleep</span><span> </span><span>50</span><span>)</span><span></span>
<span>              </span><span>(</span><span>recur</span><span> inc i</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span></span>
<!-- -->
<!-- -->
<span>    </span><span></span>
<span>    </span><span>(</span><span>deliver</span><span> next-pool-promise next-pool</span><span>)</span><span></span>
<span>    </span><span>(</span><span>alter-var-root</span><span> </span><span>#</span><span>&#39;aurora/-conn-pool </span><span>(</span><span>fn</span><span> </span><span>[</span><span>_</span><span>]</span><span> next-pool</span><span>)</span><span>)</span><span>)</span><span>)</span>
</code></pre><p>We spun up staging again, ran our failover function...buut transactions failed again. We were getting unique constraint violations on our transactions table.</p>
<h3 id="dont-forget-sequences">Don’t forget sequences</h3>
<p>This time the fix was easy to catch: sequences. Postgres does not <a href="https://www.postgresql.org/docs/current/logical-replication-restrictions.html">replicate sequence</a> data. This meant that when a new <code>transaction</code> row was created, we were using ids that already existed.</p>
<p>To fix it, we incremented our sequences in the failover function:</p>
<pre><code><span>-</span><span>           (println &#34;we are caught up!&#34;)</span>
<span></span><span>+</span><span>           (sql/execute! next-pool</span>
<span></span><span>+</span><span>                         [&#34;select setval(&#39;transactions_id_seq&#39;, ?::bigint, true)&#34;</span>
<span></span><span>+</span><span>                         (+ (:id row) 1000)])</span>
</code></pre><p>This time we ran the failover function...and it worked great!</p>
<p>If you’re curious, here’s how the actual failover <a href="https://github.com/instantdb/instant/blob/main/server/src/instant/jdbc/failover.clj#L25-L87">function</a> looked for production.</p>
<h3 id="running-in-prod">Running in Prod</h3>
<p>Now that we had a good practice run, we got ourselves ready, had our sparkling waters in hand, and began to ran our steps in production. </p>
<p>After about a 3.5 second pause <sup id="marked-fnref-13"><a href="#marked-fn-13">[13]</a></sup>, the failover function completed smoothly! We had a new Postgres instance serving requests, and best of all, nobody noticed. <sup id="marked-fnref-14"><a href="#marked-fn-14">[14]</a></sup></p>
<h3 id="future-improvements">Future Improvements</h3>
<p>Our <code>do-failover-to-new-db</code> worked at our scale, but will probably fail us in a few months. There are two improvements we plan to make:</p>
<ol>
<li>We paused <em>both</em> writes and reads. But technically we don’t need to pause reads. Daniel pushed <a href="https://github.com/instantdb/instant/pull/743">up a PR</a> to be explicit about read-only connections. In the future we can skip pausing them.</li>
<li>In December we were able to scale down to one big machine. We’re approaching the limits to one big machine today. <sup id="marked-fnref-15"><a href="#marked-fn-15">[15]</a></sup> We’re going to try to evolve this into a kind of <code>two-phase-commit</code>, where each machine reports their stage, and a coordinator progresses when all machines hit the same stage.</li>
</ol>

<p>Aand that’s our story of how did our major version upgrade. We wanted to finish up with a summary of learnings, in the hopes that’s easier for you to get back to this essay when you’re considering an upgrade. Here’s what we wish we knew when we started:</p>
<ol>
<li>Sometimes, newer Postgres versions improve perf. Make sure to check this if you face perf issues.</li>
<li>If you need to upgrade<ol>
<li>Pick a buddy if you can, it’s a lot more fun (and less nerve-racking) to do this with a partner.</li>
<li>Before you do anything in production, do a full rehearsal. Use a staging environment that mimics production as closely as possible</li>
<li>If you are okay with 15 minutes of downtime, do an <a href="#in-place-upgrade">in-place upgrade</a></li>
<li>If you don’t have active replication slots and are okay with a minute of downtime, try a <a href="#blue-green-deployment">blue-green deployment</a></li>
<li>When you need to do a manual upgrade:<ol>
<li>If you can, skip cloning and create a replica from scratch. There are only <a href="#replica-checklist">7 steps</a></li>
<li>If you wrote custom pg functions, make sure to check your <a href="#search-paths">search_path</a></li>
<li>Do some sanity checks to make sure you don’t <a href="#missing-data">lose data</a></li>
<li>If you can get writes down to one machine, try our <a href="#zero-downtime-algo">algorithm for zero downtime</a></li>
</ol>
</li>
</ol>
</li>
</ol>
<p>Hopefully, this was a fun read for you :)</p>
<p><em>Thanks to Nikita Prokopov, Joe Averbukh, Martin Raison, Irakli Safareli, Ian Sinnott for reviewing drafts of this essay</em></p>
<p><a id="marked-fn-1" href="#marked-fnref-1">[1]</a>  Our sync strategy was inspired by Figma’s LiveGraph and Asana’s Luna. The LiveGraph team wrote a <a href="https://www.figma.com/blog/livegraph-real-time-data-fetching-at-figma/">great essay</a> that explains the sync strategy. You can read our original <a href="https://www.instantdb.com/essays/next_firebase">design essay</a> to learn more about Instant</p>
<p><a id="marked-fn-2" href="#marked-fnref-2">[2]</a>  You may be wondering: how do we host multiple &#34;Instant databases&#34;, under one &#34;Aurora database&#34;? The short answer is that we wrote a query engine on top of Postgres. This lets us create a multi-tenant system., where we can &#34;spin up&#34; dbs on demand. I hope to share more about this in a separate essay.</p>
<p><a id="marked-fn-3" href="#marked-fnref-3">[3]</a>  All of the code (including this blog) is open sourced <a href="https://github.com/instantdb/instant">here</a>.</p>
<p><a id="marked-fn-4" href="#marked-fnref-4">[4]</a>  <a href="https://instances.vantage.sh/aws/rds/db.r6g.16xlarge">db.r6g.16xlarge</a> would cost us north of 6K per month. That was out of the question for the kind of traffic we were handling.</p>
<p><a id="marked-fn-5" href="#marked-fnref-5">[5]</a>  In case you were wondering, we also looked to optimize the queries. After we upgraded (took about a day and a half), we added a partial index that improved perf another 50% or so.</p>
<p><a id="marked-fn-6" href="#marked-fnref-6">[6]</a>  We did see a note about replication in <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/blue-green-deployments-switching.html#blue-green-deployments-switching-guardrails">&#34;Switchover Guardrails&#34;</a>, but this note is about the second step: after 1) creating a green deployment, we 2) run the switch.</p>
<p><a id="marked-fn-7" href="#marked-fnref-7">[7]</a>  The key to discovering this issue was our co-author Daniel’s sleuthing. He planned test upgrades locally: going from 13 → 14 → 15 → 16, to see where things broke. When Daniel tried 13 → 14, it failed. To sanity check things, he then tried a migration from 13 → 13…and that failed too! From there we knew something had to be up with our process.</p>
<p><a id="marked-fn-8" href="#marked-fnref-8">[8]</a>  An alternative would have been to enhance the dump file with the search path. We like the idea of being more explicit in our definitions though; especially if we can find a good linter.</p>
<p><a id="marked-fn-9" href="#marked-fnref-9">[9]</a>  Why do we have it? We use the transaction’s id column for record-keeping inside sync servers.</p>
<p><a id="marked-fn-10" href="#marked-fnref-10">[10]</a>  If you are curious, you can look at a slice of the checklist we used <a href="https://gist.github.com/stopachka/f05d3682223e206ed6465cafe3ec9f2a">here</a>. If you have a hunch where the data loss could have come from, let us know</p>
<p><a id="marked-fn-11" href="#marked-fnref-11">[11]</a>  Though even with 30TB, it would only take a week to transfer at a modest 50 mb/second.</p>
<p><a id="marked-fn-12" href="#marked-fnref-12">[12]</a>  You may be wondering — sure, the transactions table was okay, but what if there was data loss in other tables? We wrote a <a href="https://github.com/instantdb/instant/blob/main/server/src/instant/jdbc/failover.clj#L258">more involved script</a> to check for every table too. We really wanted to make sure there was no data loss.</p>
<p><a id="marked-fn-13" href="#marked-fnref-13">[13]</a>  About 2.5 seconds to let active queries complete, and about 1 second for the replica to catch up</p>
<p><a id="marked-fn-14" href="#marked-fnref-14">[14]</a>  You may be wondering, how did we run the function? Where’s the feature flag? That’s one more Clojure win: we could SSH into production, and execute this function in our REPL!</p>
<p><a id="marked-fn-15" href="#marked-fnref-15">[15]</a>  The big bottleneck is all the active connections — it slows down the sync engine too much. If we improve perf, perhaps we can get to one big machine again!</p>
</div></div>
  </body>
</html>
