<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kenschutte.com/gzip-knn-paper2/">Original</a>
    <h1>Gzip beats BERT? Part 2: dataset issues, improved speed, and results</h1>
    
    <div id="readability-page-1" class="page"><div><h3>&#34;Gzip beats BERT?&#34; Part 2: dataset issues, improved speed, and results</h3><p><i>Published 2023 / 07 / 28</i></p><div id="topimg"><p><img src="https://kenschutte.com/gzip-knn-paper2/banner.png"/></p><p>
Image: Four DALL-E results for &#34;gzip as a
super intelligence&#34; (No, I am not a &#34;prompt engineer&#34;).
</p></div><div><p><b>Summary</b>: <i>Update to my <a href="https://kenschutte.com/gzip-knn-paper/">last post</a> analyzing the gzip+knn paper. Some of the datasets have heavily contaminated train/test tests. I show some ways to significantly speed-up the <code>gzip</code> method. I present more results of my <span>&#34;fair&#34;</span> reimplemented version.</i></p></div><div><div><p>Last week I wrote
<a href="https://kenschutte.com/gzip-knn-paper/">my analysis</a>
of the code
for the <a href="https://aclanthology.org/2023.findings-acl.426/">paper</a>
<i><span>&#34;Low-Resource&#34;</span> Text Classification: A Parameter-Free Classification Method with Compressors</i>.
The paper proposed a text classification method using
<code>gzip</code> + <code>kNN</code> and gained some attention on twitter by showing this <span>&#34;simple&#34;</span>
method beat many benchmarks, including language models like BERT.</p><p>It turns out that the classification method used
in their code <i>looked at the test label</i> as
part of the decision method and thus led
to an unfair comparison to the baseline results.
You can read the discussion with the author
on this <a href="https://github.com/bazingagin/npc_gzip/issues/3">github issue</a>.
They confirmed that this was not a bug, but a way
to compute the <i>maximum possible</i> accuracy for a stochastic
classifier (choosing one of the top-2 nearest neighbors). I claim that this too unfairly inflates their
method’s results against the baselines.
Also, it’s unnecessary: you can use
a stochastic classifier and present
the <i>average</i> (and possibly more stats like min, max, std, etc.) over repeated trials.</p><p>In this post I describe another major problem that
someone discovered: some of the datasets have heavily
contaminated train/test sets. I also show
some ways to greatly speed up the <code>gzip</code>-based method,
and I present more complete results using
what I consider a <span>&#34;fair&#34;</span> accuracy calculation.</p></div></div><h3>Contents</h3><ul><li><a href="#dataset-issues">1. Dataset issues</a></li><ul><li><a href="#details">1.1. Details</a></li><li><a href="#ambiguous-data-points">1.2. Ambiguous data points</a></li></ul><li><a href="#speed-improvements">2. Speed Improvements</a></li><ul><li><a href="#repeated-work">2.1. Repeated work</a></li><li><a href="#gzip-/-zlib-tricks">2.2. gzip / zlib tricks</a></li><li><a href="#multiprocessing">2.3. Multiprocessing</a></li></ul><li><a href="#results">3. Results</a></li><ul><li><a href="#notes">3.1. Notes</a></li></ul><li><a href="#conclusions">4. Conclusions</a></li></ul><div><div><h3 id="dataset-issues">1. Dataset issues</h3><div><div><p>At the end of my previous post, I included a brief
to-do note: <i><span>&#34;Why is filipino so high (1.0 in one case)?&#34;</span></i>,
referring to one dataset (<code>DengueFilipino</code>) having suspiciously high accuracy.
My attempt to run the official code
gave 100% accuracy. The paper reported 99.8% (Table 5).</p><p>Before I got around to looking at that, someone
else discovered and reported the problem (see the
<a href="https://github.com/bazingagin/npc_gzip/issues/13">github issue</a>) - the <code>filipino</code> dataset has <i>exactly equal train and test tests</i>! Other datasets also have
significant train/test contamination.
There are also duplicate data points.</p></div></div><div><div><h3 id="details">1.1. Details</h3><div><p>
Here are the statistics I computed for all the datasets: </p><table id="statstable"><tbody><tr><th></th><th colspan="3">train set</th><th colspan="3">test set</th><th colspan="2">train-test intersection</th></tr><tr><th>name</th><th>count</th><th>unique</th><th>%dup</th><th>count</th><th>unique</th><th>%dup</th><th>count</th><th>% of test</th></tr><tr><td>AG_NEWS</td><td>120000</td><td>120000</td><td></td><td>7600</td><td>7600</td><td></td><td>0</td><td></td></tr><tr><td>DBpedia</td><td>560000</td><td>560000</td><td></td><td>70000</td><td>70000</td><td></td><td>0</td><td></td></tr><tr><td>YahooAnswers</td><td>1400000</td><td>1400000</td><td></td><td>60000</td><td>60000</td><td></td><td>0</td><td></td></tr><tr><td>20News</td><td>11314</td><td>11314</td><td></td><td>7532</td><td>7532</td><td></td><td>0</td><td></td></tr><tr><td>ohsumed</td><td>3357</td><td>3357</td><td></td><td>4043</td><td>4043</td><td></td><td>0</td><td></td></tr><tr><td>R8</td><td>5485</td><td>5427</td><td>1.1%</td><td>2189</td><td>2176</td><td>0.6%</td><td>4</td><td>0.2%</td></tr><tr><td>R52</td><td>6532</td><td>6454</td><td>1.2%</td><td>2568</td><td>2553</td><td>0.6%</td><td>6</td><td>0.2%</td></tr><tr><td>KinyarwandaNews</td><td>17014</td><td>9199</td><td>45.9%</td><td>4254</td><td>2702</td><td>36.5%</td><td>643</td><td>23.8%</td></tr><tr><td>KirundiNews</td><td>3689</td><td>1791</td><td>51.5%</td><td>923</td><td>698</td><td>24.4%</td><td>631</td><td>90.4%</td></tr><tr><td>DengueFilipino</td><td>4015</td><td>3951</td><td>1.6%</td><td>4015</td><td>3951</td><td>1.6%</td><td>3951</td><td>100.0%</td></tr><tr><td>SwahiliNews</td><td>22207</td><td>22207</td><td></td><td>7338</td><td>7338</td><td></td><td>34</td><td>0.5%</td></tr><tr><td>SogouNews</td><td>450000</td><td>450000</td><td></td><td>60000</td><td>60000</td><td></td><td>0</td><td></td></tr></tbody></table><p>The <a href="https://github.com/bazingagin/npc_gzip/">paper’s repo</a> does minimal processing on the datasets. It turns out that <i>these problems exist in the source Huggingface datasets</i>. The two worst ones can be checked quickly using only Huggingface’s <code>datasets.load_dataset</code>:</p><div><pre><span></span><span>&gt;&gt;&gt;</span> <span>from</span> <span>datasets</span> <span>import</span> <span>load_dataset</span>
<span>&gt;&gt;&gt;</span> <span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>&gt;&gt;&gt;</span> <span># train == test</span>
<span>&gt;&gt;&gt;</span> <span>ds</span> <span>=</span> <span>load_dataset</span><span>(</span><span>&#34;dengue_filipino&#34;</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>print</span><span>(</span><span>list</span><span>(</span><span>ds</span><span>[</span><span>&#39;train&#39;</span><span>])</span> <span>==</span> <span>list</span><span>(</span><span>ds</span><span>[</span><span>&#39;test&#39;</span><span>]))</span>
<span>True</span>

<span>&gt;&gt;&gt;</span> <span># 90% overlap:</span>
<span>&gt;&gt;&gt;</span> <span>ds2</span> <span>=</span> <span>load_dataset</span><span>(</span><span>&#39;kinnews_kirnews&#39;</span><span>,</span> <span>&#39;kirnews_cleaned&#39;</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>keys</span> <span>=</span> <span>sorted</span><span>(</span><span>ds2</span><span>[</span><span>&#39;test&#39;</span><span>][</span><span>0</span><span>]</span><span>.</span><span>keys</span><span>())</span>
<span>&gt;&gt;&gt;</span> <span>set_train</span> <span>=</span> <span>set</span><span>([</span><span>tuple</span><span>([</span><span>item</span><span>[</span><span>key</span><span>]</span> <span>for</span> <span>key</span> <span>in</span> <span>keys</span><span>])</span> <span>for</span> <span>item</span> <span>in</span> <span>ds2</span><span>[</span><span>&#39;train&#39;</span><span>]])</span>
<span>&gt;&gt;&gt;</span> <span>set_test</span>  <span>=</span> <span>set</span><span>([</span><span>tuple</span><span>([</span><span>item</span><span>[</span><span>key</span><span>]</span> <span>for</span> <span>key</span> <span>in</span> <span>keys</span><span>])</span> <span>for</span> <span>item</span> <span>in</span> <span>ds2</span><span>[</span><span>&#39;test&#39;</span><span>]])</span>
<span>&gt;&gt;&gt;</span> <span>print</span><span>(</span><span>np</span><span>.</span><span>mean</span><span>([</span><span>a</span> <span>in</span> <span>set_train</span> <span>for</span> <span>a</span> <span>in</span> <span>set_test</span><span>]))</span>
<span>0.9040114613180515</span>  
</pre></div></div></div><div><h3 id="ambiguous-data-points">1.2. Ambiguous data points</h3><div><div><p>Of course, having test points that
are in your training data
will artificially raise the
accuracy for all classifiers. But,
it is particularly beneficial
for <code>kNN</code> - a point’s nearest neighbor will be
itself and we should get everything perfectly correct
(as opposed to neural network
classifiers that typically have some
amount of training error).</p><p>So, how did the <code>DengueFilipino</code> tests
have below 100% accuracy for <code>kNN</code>?
As I mentioned, my re-running
of the official code gave 100%, so I’m not
sure how they got the 99.8% in Table 5.
My <code>kNN(k=1)</code> classifier got 99.9%
accuracy (5 errors out of 4015), so let’s
look at the five test points it got wrong:</p></div><table id="tab_errors"><tbody><tr><th></th><th colspan="2">test point</th><th colspan="2">closest train point</th></tr><tr><th>index</th><th>text</th><th>label</th><th>text</th><th>label</th></tr><tr><td>1269</td><td>&#34;Broken heart syndrome ?&#34;</td><td>4</td><td>&#34;Broken heart syndrome ?&#34;</td><td>2</td></tr><tr><td>2363</td><td>&#34;Monday sickness ?&#34;</td><td>4</td><td>&#34;Monday sickness ?&#34;</td><td>0</td></tr><tr><td>2441</td><td>&#34;Monday sickness ?&#34;</td><td>4</td><td>&#34;Monday sickness ?&#34;</td><td>0</td></tr><tr><td>2677</td><td>&#34;Monday sickness ?&#34;</td><td>2</td><td>&#34;Monday sickness ?&#34;</td><td>0</td></tr><tr><td>3440</td><td>&#34;playsafe, u ain&#39;t sick ?&#34;</td><td>4</td><td>&#34;playsafe, u ain&#39;t sick ?&#34;</td><td>2</td></tr></tbody></table><div><p>So, the dataset has <i>repeated texts with different
ground-truth labels</i>.
Note, this is different from the <i>dup</i> (duplicates)
column in the above table, which was counting
duplicates of the <code>(data,label)</code> tuples. In the <code>kNN</code>, these
duplicates have equal distances, and in these cases,
it happened to select the wrong ones.</p><p>Someone with knowledge of the
dataset will have to determine
if this is a preparation error or an actual
desired property.
If it’s the latter, perhaps a different
scoring criterion could be used (of course,
followed by all compared methods).</p></div></div></div></div></div><div><h3 id="speed-improvements">2. Speed Improvements</h3><div><div><p>In trying to recreate the baseline results,
you will quickly learn that the <code>gzip + kNN</code>
method is <i>slow</i> for datasets
with a large training set.</p><p>The costly computation is the distance
matrix containing <code>num_train × num_test</code> elements.</p><p>In the paper’s codebase, there is a
<code>calc_dis</code> method <a href="https://github.com/bazingagin/npc_gzip/blob/c18e2206af844abaf934c00f2e61b8e8f4bf2925/experiments.py#L34">here</a> and
if we remove the abstractions and
remove some of the unused options,
we see it doing the following:</p></div><div><pre><span></span><span>#test_data:[str]</span>
<span>#train_data:[str]</span>

<span>#clen: compute the compressed-length of data:</span>
<span>clen</span> <span>=</span> <span>lambda</span> <span>data</span> <span>:</span> <span>len</span><span>(</span><span>gzip</span><span>.</span><span>compress</span><span>(</span><span>data</span><span>))</span>        

<span># NCD: Normalized Compression Distance</span>
<span># distance function</span>
<span>NCD</span> <span>=</span> <span>lambda</span> <span>c1</span><span>,</span> <span>c2</span><span>,</span> <span>c12</span> <span>:</span> <span>(</span><span>c12</span> <span>-</span> <span>min</span><span>(</span><span>c1</span><span>,</span><span>c2</span><span>))</span> <span>/</span> <span>max</span><span>(</span><span>c1</span><span>,</span> <span>c2</span><span>)</span>
        
<span>for</span> <span>i</span><span>,</span><span>t1</span> <span>in</span> <span>enumerate</span><span>(</span><span>test_data</span><span>):</span>
    <span>l1</span> <span>=</span> <span>clen</span><span>(</span><span>t1</span><span>.</span><span>encode</span><span>(</span><span>&#39;utf8&#39;</span><span>))</span>
    <span>for</span> <span>j</span><span>,</span><span>t2</span> <span>in</span> <span>enumerate</span><span>(</span><span>train_data</span><span>):</span>
        <span>l2</span> <span>=</span> <span>clen</span><span>(</span><span>t2</span><span>.</span><span>encode</span><span>(</span><span>&#39;utf8&#39;</span><span>))</span>
	<span>l12</span> <span>=</span> <span>clen</span><span>(</span> <span>(</span><span>t1</span> <span>+</span> <span>&#39; &#39;</span> <span>+</span> <span>t2</span><span>)</span><span>.</span><span>encode</span><span>(</span><span>&#39;utf8&#39;</span><span>)</span> <span>)</span>
	<span>D</span><span>[</span><span>i</span><span>,</span><span>j</span><span>]</span> <span>=</span> <span>NCD</span><span>(</span><span>l1</span><span>,</span><span>l2</span><span>,</span><span>l12</span><span>)</span> <span># fill distance matrix</span>
        
</pre></div></div><div><div><h3 id="repeated-work">2.1. Repeated work</h3><div><div><p>We can quickly see a few things are being unnecessarily repeated,</p><ul><li><p>There are a few repeated <code>str.encode()</code>.
We can start by converting everything to bytes.</p></li><li><p>The computation of <code>l2</code> is in the inner loop, but does
not depend on the test data (<code>t1</code>).
We should pre-compute <code>l2</code> for the whole training
set only once.</p></li></ul><p>So, now we have something like,</p></div><div><pre><span></span><span>train_data</span> <span>=</span> <span>[</span><span>a</span><span>.</span><span>encode</span><span>(</span><span>&#39;utf8&#39;</span><span>)</span> <span>for</span> <span>a</span> <span>in</span> <span>train_data</span><span>]</span>    
<span>test_data</span>  <span>=</span> <span>[</span><span>a</span><span>.</span><span>encode</span><span>(</span><span>&#39;utf8&#39;</span><span>)</span> <span>for</span> <span>a</span> <span>in</span> <span>test_data</span><span>]</span>

<span>train_lengths</span> <span>=</span> <span>[</span><span>clen</span><span>(</span><span>t2</span><span>)</span> <span>for</span> <span>t2</span> <span>in</span> <span>train_data</span><span>]</span>
    
<span>for</span> <span>i</span><span>,</span><span>t1</span> <span>in</span> <span>enumerate</span><span>(</span><span>test_data</span><span>):</span>
    <span>l1</span> <span>=</span> <span>clen</span><span>(</span><span>t1</span><span>)</span>
    <span>for</span> <span>j</span><span>,</span><span>t2</span> <span>in</span> <span>enumerate</span><span>(</span><span>train_data</span><span>):</span>
        <span>l2</span> <span>=</span> <span>train_lengths</span><span>[</span><span>j</span><span>]</span>
	<span>l12</span> <span>=</span> <span>clen</span><span>(</span> <span>(</span><span>t1</span> <span>+</span> <span>b</span><span>&#39; &#39;</span> <span>+</span> <span>t2</span><span>)</span> <span>)</span>
	<span>D</span><span>[</span><span>i</span><span>,</span><span>j</span><span>]</span> <span>=</span> <span>NCD</span><span>(</span><span>l1</span><span>,</span><span>l2</span><span>,</span><span>l12</span><span>)</span>
</pre></div><p>I don’t have the exact speed-up numbers on this step right
now, but I think it is pretty significant.
For example, <code>YahooAnswers</code> dataset
has 1.4 million training samples and 60 thousand test samples.
So, in the original code, for each of the 60 thousand test
points, it was recomputing the <code>clen()</code> for each of the 1.4 million
training documents <i>every time</i>.</p></div></div><div><h3 id="gzip-/-zlib-tricks">2.2. gzip / zlib tricks</h3><div><div><p>Another case of repeated computation: we compress <code>t1</code>
(the test point), but
then in the inner-loop, we compress it again (as
the start of the larger string <code>t1 + &#34; &#34; + t2</code>).
Can we remove this rendancy? Yes we can!</p><p>First, the relationship between a few related terms:
<b><a href="https://en.wikipedia.org/wiki/Gzip">GZIP</a></b>
is a <i>file format</i> for holding compressed data.
The data is compressed with the <b><a href="https://en.wikipedia.org/wiki/Deflate">DEFLATE</a></b> algorithm.
<b><a href="https://www.zlib.net/">zlib</a></b>
is a library that implements <i>DEFLATE</i>
and has it’s own lower-level format for
compressed data.</p><p>So, <i>gzip</i> is simply a small
wrapper around <i>zlib</i>.
You can see this clearly in Python’s source code
for the <code>gzip.compress(x)</code> function [<a href="https://github.com/python/cpython/blob/6138ecdeb80d3a62d5cef27b08669495bccbe19b/Lib/gzip.py#L576">here</a>]:
it simply returns <code>header + zlib.compress(x) + footer</code>.
[<i>So maybe instead of the headline <span>&#34;gzip beats BERT?&#34;</span> it should be <span>&#34;zlib beats BERT?&#34;</span> or <span>&#34;DEFLATE beats BERT?&#34;</span></i>]</p><p>We want to compress a string (test point <code>t1</code>) with <code>zlib</code>, then continue that compression with different training points (<code>t2</code>). Luckily Python’s
<a href="https://docs.python.org/3/library/zlib.html"><code>zlib</code> module</a> provides the exact interface we need: a <code>compressobj</code> that stores the state of the <code>zlib</code> compressor and a <code>copy</code> method to copy its state.
The <a href="https://docs.python.org/3/library/zlib.html#zlib.Compress.copy">Python docs for <code>copy()</code></a>
even mention our use-case: <code>Compress.copy(): Returns a copy of the compression object. This can be used to efficiently compress a set of data that share a common initial prefix.</code></p><p>I implemented this in a class called <a href="https://github.com/kts/gzip-knn/blob/main/gziplength.py"><code>GzipLengthCalc</code></a>. Using this, the main loop is now,</p></div><div><pre><span></span><span>for</span> <span>i</span><span>,</span><span>t1</span> <span>in</span> <span>enumerate</span><span>(</span><span>test_data</span><span>):</span>

    <span>#starts compressing &#39;t1 + b&#34; &#34;&#39;        </span>
    <span>g</span> <span>=</span> <span>GzipLengthCalc</span><span>(</span><span>t1</span><span>)</span>
    <span>l1</span> <span>=</span> <span>g</span><span>.</span><span>length1</span>

    <span>for</span> <span>j</span><span>,</span><span>t2</span> <span>in</span> <span>enumerate</span><span>(</span><span>train_data</span><span>):</span>
        <span>l2</span> <span>=</span> <span>precomputed_lengths</span><span>[</span><span>j</span><span>]</span>
        <span># continues from stored zlib state to</span>
        <span># compress t2:</span>
        <span>l12</span> <span>=</span> <span>g</span><span>.</span><span>length2</span><span>(</span><span>t2</span><span>)</span>
        <span>D</span><span>[</span><span>i</span><span>,</span><span>j</span><span>]</span> <span>=</span> <span>NCD</span><span>(</span><span>l1</span><span>,</span> <span>l2</span><span>,</span> <span>l12</span><span>)</span>
</pre></div><div><p>Keep in mind that the inner-loop is run billions of times for some of the datasets, so removing anything there can be a huge speed up.</p><p>Any more room for improvement?</p><ul><li><p>I tried removing more of the <code>NCD</code> calculation from the inner loop. The loops can compute the matrix of distances, and we compute the <code>NCD</code> outside of the loops using vectorized numpy, <code>D = (C12 - np.minimum(C1,C2)) / np.maximum(C1,C2)</code>. This had a small speed improvement, but at the memory cost of allocated 3 matrices instead of 1, so I didn’t use that.</p></li><li><p>Is it possible that precomputed <code>clen(t2)</code> can speed up the computation of <code>clen(t1 + b&#39; &#39; + t2)</code>? Probably not. <code>zlib</code> works sequentially. <i>Perhaps</i>, with some real <code>zlib</code> wizardry: there are internal parameters like a <span>&#34;block size&#34;</span> such that the past bytes a certain distance before the current point no longer matter. Perhaps this could be exploited in the case of large texts.</p></li><li><p>If more significant gains were wanted, I’d suggest computing this double-loop in C.</p></li></ul></div></div></div><div><h3 id="multiprocessing">2.3. Multiprocessing</h3><div><div><p>The original code had some support for multiprocessing.
I expanded this using <a href="https://docs.python.org/3/library/concurrent.futures.html">concurrent.futures</a> and made it flexible to parallelize to the number of workers using <code>os.cpu_count()</code>.</p><p>For example, if you run a machine with 36 vCPUs, you can get 36 Python
processes all running at 100% as shown below.</p><p>So, who cares about the <a href="https://wiki.python.org/moin/GlobalInterpreterLock">GIL</a>? Well, as-is, the
multiprocessing has quite a bit of overhead in passing around large amounts of data to the
child processes. This could be improved.</p></div><p><a href="https://kenschutte.com/gzip-knn-paper2/top.png"><img width="500" src="https://kenschutte.com/gzip-knn-paper2/top_tn.png"/></a></p></div></div></div></div><div><h3 id="results">3. Results</h3><div><div><p>With these speed improvements, I could
run my implementation across all the datasets.
I present results below along with the
comparable results from the paper, Table 3 and Table 5.</p><p>The rows in my results table are:</p><ul><li><p><i>paper</i>: Copied value from the paper (Table 3 and Table 5).</p></li><li><p><i>rerun</i>: My run of the (<a href="https://github.com/bazingagin/npc_gzip">code</a>) provided by the authors. The blanks in this row mean it was too slow, and I didn’t finish it.</p></li><li><p><i>top2</i>: My implementation, with <i>top-2</i> accuracy. <code>kNN</code> that is marked correct if either of the top-2 nearest neighbors are correct.</p></li><li><p><i>knn1</i>: My implementation, standard <code>kNN</code> with <code>k=1</code>, i.e. just taking the label of the single nearest neighbor.</p></li></ul></div><p><a href="https://kenschutte.com/gzip-knn-paper2/table3b.jpg"><img src="https://kenschutte.com/gzip-knn-paper2/table3b.jpg"/></a></p><table><tbody><tr><th></th><th>AGNews</th><th>DBpedia</th><th>YahooAn.</th><th>20News</th><th>Ohsumed</th><th>R8</th><th>R52</th></tr><tr><th>paper</th><td>0.937</td><td>0.970</td><td>0.638</td><td>0.685</td><td>0.521</td><td>0.954</td><td>0.896</td></tr><tr><th>rerun</th><td></td><td></td><td></td><td>0.685</td><td></td><td></td><td></td></tr><tr><th>top2</th><td>0.937</td><td>0.970</td><td>0.622</td><td>0.685</td><td>0.481</td><td>0.952</td><td>0.889</td></tr><tr><th>knn1</th><td>0.876</td><td>0.942</td><td>0.485</td><td>0.607</td><td>0.365</td><td>0.913</td><td>0.852</td></tr></tbody></table><p><a href="https://kenschutte.com/gzip-knn-paper2/table5b.jpg"><img src="https://kenschutte.com/gzip-knn-paper2/table5b.jpg"/></a></p><table><tbody><tr><th></th><th>KinyarwandaNews</th><th>KirundiNews</th><th>DengueFilipino</th><th>SwahiliNews</th><th>SogouNews</th></tr><tr><th>paper</th><td>0.891</td><td>0.905</td><td>0.998</td><td>0.927</td><td>0.975</td></tr><tr><th>rerun</th><td>0.891</td><td>0.906</td><td>1.000</td><td>0.927</td><td></td></tr><tr><th>top2</th><td>0.891</td><td>0.906</td><td>1.000</td><td>0.927</td><td>0.973</td></tr><tr><th>knn1</th><td>0.835</td><td>0.858</td><td>0.999</td><td>0.850</td><td>0.951</td></tr></tbody></table></div><div><div><h3 id="notes">3.1. Notes</h3><div><div><p>The following 3 rows should all have the same value: <i>paper</i>, <i>rerun</i>, and <i>top2</i>. As in my previous blog post, that confirms that the
paper used a non-comparable <i>top-2</i> accuracy metric and I was
able to recreate it. There are a few cases where these 3 are not the same:</p><ul><li><p>The three datasets: <i>Ohsumed</i>, <i>R8</i>, and <i>R52</i> are the three datasets that are not automatically prepared by the <code>npc_gzip</code> repo [<a href="https://github.com/bazingagin/npc_gzip/issues/17">see issue</a>]. It is very possible that I am using a slightly different dataset than the original paper.</p></li><li><p><code>YahooAnswers</code> dataset. Not sure why <code>paper = 0.638</code> and <code>top2 = 0.622</code>.</p></li><li><p><code>DengueFilipino</code> is very close. This was the one where training test equals test set.</p></li></ul><p>In the table images, I wrote the <code>knn1</code> scores below the table. When using these as the <code>gzip</code> results, we generally don’t see improvement compared to baseline methods.</p><p>For now, I only have the <code>k=1</code> results here. <a href="https://kenschutte.com/gzip-knn-paper/">My last post</a> showed a handfull of different <code>k</code> values and methods for breaking ties. In those results, there was no clear winner across datasets. I will try generate these numbers and add to the results above.</p></div></div></div></div></div><div><h3 id="conclusions">4. Conclusions</h3><div><div><p>Between the accuracy calculation and
contaminated datasets, I believe that
many of the key results (and thus also the conclusions)
in the paper are not valid.</p><p>The paper touts <code>kNN + gzip</code> as computationally
simpler than language-model-based methods,
but beware that it is <i>slow</i> for the datasets
with large amount of training samples.</p><p>Nevertheless, using ideas from text compression
for text classification tasks is an interesting idea
and may lead to other interesting research.
For example, see <a href="https://github.com/cyrilou242/ftcc">this work</a> using <code>zstd</code> dictionaries per class to do something with the same concept, but much more efficiently.</p></div></div></div></div></div></div>
  </body>
</html>
