<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://davidlattimore.github.io/posts/2024/02/04/speeding-up-the-rust-edit-build-run-cycle.html">Original</a>
    <h1>Speeding up the Rust edit-build-run cycle</h1>
    
    <div id="readability-page-1" class="page"><div>
    <article>
      <p>
  
  David Lattimore - <time datetime="2024-02-04">2024-02-04</time>
  
  
</p>
      <p>There are two main aspects to compile times that matter to developers. Cold build times, when
building from scratch and warm build times when you’ve already built and you’re rebuilding following
an edit. This article focuses on warm build times, which for rapid iteration during development is
what generally matters most.</p>

<p>We start with some tips for speeding up your Rust development cycle, then talk about work that I’m
doing in this space to make it even faster.</p>

<p>For projects with minimal dependencies, you might find that your development cycle is already fast
enough. In that case, great. This article will therefore use a crate with some heavyweight
dependencies such that warm compilation times are a problem.</p>

<h2 id="benchmark-setup">Benchmark setup</h2>

<ul>
  <li>The benchmarked crate, benchmarking scripts can be found in <a href="https://github.com/davidlattimore/warm-build-benchmark">this
repository</a>.</li>
  <li>All benchmarks were run on my few-year-old System 76 laptop. It’s got an i7-10510U CPU, 42GB of
RAM and is running Pop-OS (a Linux distribution).</li>
  <li>A couple of warmup builds are done first.</li>
  <li>Before each build, a trivial edit is made. This is similar to adding or removing a print
statement, which is what we’d like to emulate.</li>
</ul>

<p>To start, we benchmark the <code>cargo run</code> time for a default configuration debug build using the
default system linker (GNU ld). This gives us a time of 20.202 s ± 0.256 s which is slow enough to
be fairly annoying.</p>

<h2 id="use-a-faster-linker-like-mold">Use a faster linker like Mold</h2>

<p>Switching to a faster linker like <code>mold</code> (or <code>sold</code> for Mac) can make a very big difference. lld is
also pretty fast - see the timings for it later.</p>

<p>With mold version 2.3.3 installed, we add the following to our <code>.cargo/config.toml</code>:</p>

<div><div><pre><code><span>[target.x86_64-unknown-linux-gnu]</span>
<span>linker</span> <span>=</span> <span>&#34;/usr/bin/clang-15&#34;</span>
<span>rustflags</span> <span>=</span> <span>[</span><span>&#34;-C&#34;</span><span>,</span> <span>&#34;link-arg=--ld-path</span><span>=</span><span>/usr/local/bin/mold</span><span>&#34;]</span><span>
</span></code></pre></div></div>

<p>That reduces our <code>cargo run</code> time to 7.539 s ± 1.691 s.</p>

<h2 id="avoid-linking-debug-info">Avoid linking debug info</h2>

<p>Debug information tends to be large and linking it slows down linking quite considerably. If you’re
like many developers and you generally use println for debugging and rarely or never use an actual
debugger, then this is wasted time.</p>

<p>There are two ways you can do avoid linking debug information:</p>

<ul>
  <li>Skip compiling the debug information in the first place by setting <code>debug=0</code> in your profile.</li>
  <li>Skip linking it, even it it was compiled by setting <code>strip=&#34;debuginfo&#34;</code> in your profile.</li>
</ul>

<p>Using just the second option, should mean in theory that if you need debug information, that you can
just need to relink. Unfortunately Cargo currently rebuilds everything from scratch when this option
is changed. Also, as far as I know, this option won’t help on Mac, since rustc uses an external
<code>strip</code> command to remove the debug information after linking is complete, presumably because the
Mac linker doesn’t support the <code>--strip-debug</code> flag.</p>

<p>One limitation of setting <code>debug=0</code> is that it doesn’t affect compilation of the Rust standard
library because that comes precompiled with debug info. That means that with just <code>debug=0</code> set,
we’ll still be linking a bit of debug info.</p>

<div><div><pre><code><span>[profile.dev]</span>
<span>debug</span> <span>=</span> <span>0</span>
<span>strip</span> <span>=</span> <span>&#34;debuginfo&#34;</span>
</code></pre></div></div>

<p>With just <code>strip=&#34;debuginfo&#34;</code> our time comes down to 1.507 s ± 0.301 s.</p>

<p>With just <code>debug=0</code> we’re slightly slower at 1.611 s ± 0.329 s</p>

<p>With both we get 1.576 s ± 0.355 s.</p>

<p>All three are pretty similar and well within 1 standard deviation of each other, so they’re
effectively the same.</p>

<p>If developing on Linux, I’d probably set both, since setting <code>debug=0</code> also improves the cold build
times (77 seconds down from 107). If developing on Mac, probably just set <code>debug=0</code>. If developing
on Windows, it’s probably a good idea to try both and measure the warm build time to see what
combination has the best effect.</p>

<p>One downside of no debug info is that your backtraces will only have function names, with no line
numbers. Personally, I don’t mind this and despite having <code>debug=0</code> in my profile for a long time,
didn’t even notice until matthieum (on Reddit) pointed it out. Note that you’ll still get the line
number where the panic occurred, just not line numbers for the functions that called it. I find this
to be an acceptable trade-off for faster warm builds.</p>

<p>If you do need debug info, setting <code>split-debuginfo=&#34;unpacked&#34;</code> isn’t quite as fast as no debug
info, but it’s a lot faster than actually linking the debug info. Thanks VorpalWay on Reddit for
suggesting this option. Note however that this is already the default on Mac and it isn’t supported
on Windows, so you’ll probably only see a difference when you set this on Linux. Whatever you do,
it’s a good idea to measure the effect of your settings change on your warm build times.</p>

<p>You might be wondering about the effect of setting <code>strip=&#34;symbols&#34;</code> which strips not just debug
information, but also symbol tables. This can potentially speed up warm builds a little more, but
has the significant downside that you won’t be able to get backtraces at all when you set
<code>RUST_BACKTRACE=1</code>, so I wouldn’t recommend it for development builds.</p>

<h2 id="building-a-non-relocatable-executable">Building a non-relocatable executable</h2>

<p>Rust by default compiles relocatable executables. This means that each time the binary gets run, the
operating system loads it at a random address. This is called ASLR (address space layout
randomisation) and is an important mitigation against security vulnerabilities. However it’s
generally not something we need during development and it turns out that we can save a little more
time by turning it off. This step should be skipped if you’re building or using shared objects (e.g.
.so files on Linux).</p>

<p>I found it easier to do this if we also statically link our binary with musl libc. The switch to
musl and to static linking didn’t have any significant effect on the benchmark time.</p>

<p>You can install the musl toolchain with:</p>

<div><div><pre><code>rustup target add x86_64-unknown-linux-musl
</code></pre></div></div>

<p>Then add the following to your <code>.cargo/config.toml</code>.</p>

<div><div><pre><code><span>[build]</span>
<span>target</span> <span>=</span> <span>&#34;x86_64-unknown-linux-musl&#34;</span>

<span>[target.x86_64-unknown-linux-musl]</span>
<span>linker</span> <span>=</span> <span>&#34;/usr/bin/clang-15&#34;</span>
<span>rustflags</span> <span>=</span> <span>[</span> <span>&#34;-C&#34;</span><span>,</span> <span>&#34;relocation-model</span><span>=</span><span>static</span><span>&#34;, &#34;</span><span>-C</span><span>&#34;, &#34;</span><span>link-arg=--ld-path</span><span>=</span><span>mold</span><span>&#34; ]</span><span>
</span></code></pre></div></div>

<p>This reduces our <code>cargo run</code> time to 1.156 s ± 0.039 s.</p>

<p>If you want to confirm that the executable is no longer relocatable, you can run <code>file</code> command on
it. It should say <code>ELF 64-bit LSB executable</code> instead of <code>ELF 64-bit LSB pie executable</code>. The “pie”
stands for Position Independent Executable.</p>

<h2 id="alternatives-to-mold">Alternatives to Mold</h2>

<p>lld is also a pretty fast linker. It’s not quite as fast as mold, but it’s pretty fast. Switching to
lld increases our <code>cargo run</code> time to 1.602 s ± 0.011 s, which is still pretty good.</p>

<h2 id="summary-of-improvements">Summary of improvements</h2>

<p>At this point we’ve reduced the warm build (and run) time from 20 seconds to 1.2 seconds, a 16x
improvement! The main takeaways are:</p>

<ul>
  <li>Use the fastest linker you can. 20 s -&gt; 7.5 s</li>
  <li>Don’t link debug info. 7.5 s -&gt; 1.6 s</li>
  <li>Build a non-position-independent executable. 1.6 s -&gt; 1.2 s</li>
</ul>

<p>I’d suggest you experiment with what configuration works best for your project and platform. You can
also play with other configuration options. For example, if you’re happy to abort if you get a panic
(at least during development), you might set <code>panic=&#34;abort&#34;</code>, which from my measurements gives
another small reduction in warm build time. The key is to measure. Here’s a one-liner to help with
that:</p>

<div><div><pre><code>hyperfine <span>--warmup</span> 2 <span>--prepare</span> <span>&#39;touch src/main.rs&#39;</span> <span>&#39;cargo build&#39;</span>
</code></pre></div></div>

<p>If you don’t have hyperfine, see the <a href="https://github.com/sharkdp/hyperfine">hyperfine</a> repository
for installation instructions.</p>

<h2 id="diagnosing-unexpected-rebuilds-of-dependencies">Diagnosing unexpected rebuilds of dependencies</h2>

<p>If you’re frequently seeing cargo rebuild your dependencies when you’ve only changed your crate, it
can take ages and really sap your productivity. It’s worth taking some time to figure out why. The
main tool to help diagnose this is building with the <code>-v</code> flag. e.g.</p>



<p>You’re looking for lines like the following:</p>

<div><div><pre><code>Dirty rayon v1.8.1: the rustflags changed
Dirty regex-automata v0.4.5: the config settings changed
</code></pre></div></div>

<p>It’s possible that you’ll see crates being recompiled and Cargo doesn’t give you a reason. One
common reason in this case is that the features for that crate have changed. This typically happens
when you’re using a cargo workspace. Different crates in your workspace might request different
features from your dependencies. When building the whole workspace with <code>cargo build</code>, the union of
those features is used, however if you then request to just build a single crate, e.g. <code>cargo build
-p foo</code>, only the features needed by the <code>foo</code> crate will be built, which means the dependency needs
to be rebuilt. One way that some people solve this is to create a <code>workspace-hack</code> package that
depends on all your dependencies with the union of all their features then have all your packages
depend on <code>workspace-hack</code>. <a href="https://crates.io/crates/cargo-hakari">cargo-hakari</a> is a tool to help
automate this. Other options are to just not use workspaces or never use the <code>-p</code> flag.</p>

<h2 id="investigating-remaining-time">Investigating remaining time</h2>

<p>For many people, 1.2 s might be fast enough. But what if our project has an order of magnitude more
dependencies, or if we’re running on a slower computer. Perhaps even 1.2 seconds isn’t fast enough
for some.</p>

<p>Now that we’ve gotten it as fast as we can, it’s time to look at what’s taking time.</p>

<p>We can look at the times when each process gets started using <code>strace</code>. e.g.:</p>

<div><div><pre><code>./random-edit src/main.rs<span>;</span> strace <span>-tt</span> <span>-f</span> <span>--trace</span><span>=</span>execve <span>-o</span> strace.out cargo run
</code></pre></div></div>

<p>This increases the build time reported by cargo from 1.07s to about 1.33s, but that’s still close
enough that it should give us a pretty good idea of how long things are taking. Looking at the
resulting <code>strace.out</code>, we can note a few things:</p>

<ul>
  <li>The time from executing <code>~/.cargo/bin/cargo</code> (the rustup wrapper) to executing the actual cargo
binary is 37 ms. <a href="https://github.com/rust-lang/rustup/issues/2626">Rustup issue</a> for improving
this.</li>
  <li>The time from executing <code>cargo</code> to executing <code>rustc</code> is 230 ms. I wonder if there’s scope for
caching whatever expensive computations cargo is doing here. Update: epage has been <a href="https://github.com/rust-lang/cargo/pull/13399">looking into
this</a>.</li>
  <li>The time from when <code>rustc</code> starts until it finishes is 1100 ms. Not surprisingly this is where we
spend the majority of our time.</li>
  <li>There’s 19 ms between when <code>rustc</code> exits and when <code>cargo</code> executes our binary. Looks like this is
a few different things. Writing a fingerprint file, deleting the old binary and putting the new
binary in its place then freeing memory.</li>
  <li>The time to actually execute the binary we built is 3 ms.</li>
</ul>

<p>Lets look a little more closely at what rustc is doing. Using a nightly version of rustc, we can
build with the <code>-Ztime-passes</code> flag.</p>

<div><div><pre><code>./random-edit src/main.rs<span>;</span> cargo rustc <span>--</span> <span>-Ztime-passes</span>
</code></pre></div></div>

<p>The most interesting lines are the following:</p>

<div><div><pre><code>time:   0.411; rss:  138MB -&gt;  285MB ( +147MB)	codegen_crate
time:   0.454; rss:  145MB -&gt;  146MB (   +1MB)	link
time:   0.939; rss:   26MB -&gt;   75MB (  +48MB)	total
</code></pre></div></div>

<p>So of the 939 ms total, we’re spending about 454 ms linking, 411 ms on codegen and 74 ms on other
stuff.</p>

<h2 id="possible-future-changes">Possible future changes</h2>

<h3 id="changing-the-defaults">Changing the defaults</h3>

<p>We managed a 16x speedup in warm <code>cargo run</code> time relative to the defaults. This is great, but new
users won’t necessarily know to do these things and will just be left with the impression that rust
projects are slower to build than what’s actually necessary.</p>

<p>There has been talk about bundling lld with rust and using it by default. That would go a long way.</p>

<p>I also wonder what might be done about the debug information. In an earlier version of this article,
I suggested that maybe the default should be no debug info, however given that this makes backtraces
not have line numbers, I’m now not so sure. Probably the <code>split-debuginfo=&#34;unpacked&#34;</code> should become
the default on Linux like it is on Mac.</p>

<h3 id="incremental-linking">Incremental linking</h3>

<p>What if we didn’t need to redo linking every time we made a change? i.e. if adding a print statement
to a function could just result in that one function being updated in our binary. This could
potentially give us an even faster dev cycle, especially for projects with even more dependencies or
where we still need debug information.</p>

<p>The downside of incremental linking is that the resulting binary probably wouldn’t be bit-for-bit
identical to what we’d get if we linked from scratch. But personally I don’t see that as a problem
when I’m iterating on code making small edits.</p>

<p>Implementing linkers is hard, complex work and adding incremental linking to the mix makes it even
more challenging. The Mold author <a href="https://www.reddit.com/r/cpp/comments/kxvw5c/mold_a_modern_linker/">has
said</a> that incremental linking
is too hard and has too many downsides. But my experience is that Rust makes hard, complex things
easier to implement, so let’s build an incremental linker in Rust!</p>

<p>There’s somewhat of a tradition that linkers end with the letters “ld” and it’s intended to
eventually be an incremental linker, so that suggests it ends in “ild”. A quick search for words
ending in “ild” yields “wild” as an interesting name. Let’s go with that.</p>

<p>Over the last couple of months, I’ve <a href="https://github.com/davidlattimore/wild">made a start on it</a>.
There’s still heaps to do, including actually making it incremental, but it can now link itself.</p>

<p>The performance of the wild linker can’t yet be properly compared with other linkers, since I
haven’t yet implemented support for eh_frames, which are required for unwinding to work properly.
They’re the next thing that I intend to implement.</p>

<p>My plan for this year is to work full time on improving Rust warm build times, starting by writing
an incremental linker. I’ll be living off savings, the goodwill of my partner and github sponsors.
How long I can do that for will depend on how much sponsorship I can get. So if you think your
company might benefit from that, perhaps you can convince them to sponsor me? Huge thanks to my
existing sponsors, especially Embark studios.</p>

<h2 id="discussion-threads">Discussion threads</h2>

<p><a href="https://www.reddit.com/r/rust/comments/1ajdm1v/speeding_up_rust_editbuildrun_cycle/">Reddit</a></p>

    </article>
  </div></div>
  </body>
</html>
