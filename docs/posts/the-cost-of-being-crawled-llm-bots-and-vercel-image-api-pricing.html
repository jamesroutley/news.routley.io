<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://metacast.app/blog/engineering/postmortem-llm-bots-image-optimization">Original</a>
    <h1>The Cost of Being Crawled: LLM Bots and Vercel Image API Pricing</h1>
    
    <div id="readability-page-1" class="page"><article><p>A misconfiguration that might have cost us $7,000</p><p><img alt="LLM bots + Next.js image optimization = recipe for bankruptcy (post-mortem)" loading="lazy" width="1200" height="630" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=3840&amp;q=75 3840w" src="https://metacast.app/_next/image?url=%2Fimages%2Fblog%2Fengineering%2Fpostmortem-llm-bots-image-optimization%2Fcover-postmortem-llm-bots-image-optimization.png&amp;w=3840&amp;q=75"/></p><div><div><div><p><img alt="Author&#39;s picture" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" srcset="/_next/image?url=%2Fimages%2Fpeople%2Filya-avatar.jpg&amp;w=128&amp;q=75 1x, /_next/image?url=%2Fimages%2Fpeople%2Filya-avatar.jpg&amp;w=256&amp;q=75 2x" src="https://metacast.app/_next/image?url=%2Fimages%2Fpeople%2Filya-avatar.jpg&amp;w=256&amp;q=75"/></p><p>Ilya Bezdelev</p></div><p>Published on <time datetime="2025-02-10T13:33:07.000Z">February	10, 2025</time></p></div></div><hr/><section><div><h2 id="tldr"><a href="#tldr">TL;DR</a></h2>
<p>On Friday, Feb 7, 2025 we had an incident with our Next.js web app hosted on Vercel that could&#39;ve cost us $7,000 if we didn&#39;t notice it in time.</p>
<p>We had a spike in LLM bot traffic coming from Amazonbot, Claudebot, Meta and an unknown bot. Together they sent 66.5k requests to our site within a single day. Bots scraped thousands of images that used Vercel&#39;s Image Optimization API, which cost us $5 per 1k images.</p>
<p>The misconfiguration on our side combined with the aggressive bot traffic created an economically risky situation for our tiny bootstrapped startup.</p>
<h2 id="context"><a href="#context">Context</a></h2>
<p>Metacast is a podcast tech startup. Our main product is a <a href="https://metacast.app">podcast app</a> for iOS and Android.</p>
<p>For every podcast episode on the platform, our web app has a web page. Our platform has ~1.4M episodes, which means we have 1.4M web pages that are discoverable by crawlers. These pages are generated server-side at request time, then cached.</p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/lex-podcast-page-metacast.png" alt="Lex Fridman Podcast"/></p>
<h2 id="how-we-discovered-the-problem"><a href="#how-we-discovered-the-problem">How we discovered the problem</a></h2>
<h3 id="step-1-a-cost-spike"><a href="#step-1-a-cost-spike">Step 1: A cost spike</a></h3>
<p>First, we received a cost alert from Vercel saying that we&#39;ve hit 50% of the budget for resources metered by usage.</p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-cost-alert.png" alt="Vercel cost alert"/></p>
<h3 id="step-2-image-optimization-api-usage-spike"><a href="#step-2-image-optimization-api-usage-spike">Step 2: Image Optimization API usage spike</a></h3>
<p>We looked into it and saw that it&#39;s driven by the Image Optimization API, which peaked on Feb 7.</p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-image-optimization-usage-summary.jpg" alt="Image Optimization Usage"/></p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-image-optimization-chart.jpg" alt="Image Optimization Usage Chart"/></p>
<p>Every page in the podcast directory has an image of a podcast cover (source image dimensions are 3000x3000px). With Image Optimization, podcast covers were reduced to 1/10th of the size, then cached. Image Optimization made the web app really snappy. It worked like a charm, except it turned out to be very expensive.</p>
<p>Vercel <a href="https://vercel.com/docs/image-optimization/limits-and-pricing">charges</a> $5 for every 1,000 images optimized. With thousands of requests coming our way, we were accumulating cost at the rate of $5 per each 1k image requests. In the worst case scenario, if all 1.4M images were crawled we&#39;d hypothetically be looking at a $7k bill from Vercel.</p>
<h3 id="step-3-tens-of-thousands-of-requests-from-llm-bots"><a href="#step-3-tens-of-thousands-of-requests-from-llm-bots">Step 3: Tens of thousands of requests from LLM bots</a></h3>
<p>We looked at the user agents of requests in the Firewall in Vercel and saw Amazonbot, ClaudeBot, meta_externalagent and an unknown bot disguising itself as a browser.</p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-user-agent-stats.png" alt="User Agents"/></p>
<p>We can&#39;t say definitively which bots were downloading images, because we are on the Pro plan on Vercel and no longer have access to logs from Friday. We only know that it was bot traffic.</p>
<h2 id="mitigation"><a href="#mitigation">Mitigation</a></h2>
<h3 id="step-1-stop-the-bleeding"><a href="#step-1-stop-the-bleeding">Step 1: Stop the bleeding</a></h3>
<p>Both of us used to work at AWS where we internalized the golden rule of incident recovery - <strong>stop the bleeding first, do a long-term fix later</strong>.</p>
<p>We configured firewall rules in Vercel to block bots from Amazon, Anthropic, OpenAI and Meta. To be fair, OpenAI didn&#39;t crawl our site, but we blocked it as a preventative measure.</p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-firewall-rules.jpg" alt="Firewall rules"/></p>
<h3 id="step-2-disable-image-optimization"><a href="#step-2-disable-image-optimization">Step 2: Disable Image Optimization</a></h3>
<p>First, we disabled image optimization by adding an <code>unoptimized</code> property to podcast images in Next.js. Our reasoning was that users accessing the pages will get the latest version of the page with unoptimized images.</p>
<p>We didn&#39;t consider that:</p>
<ul>
<li>Bots had already crawled thousands of pages and would crawl the <em>optimized</em> images using the URLs they extracted from the &#34;old&#34; HTML.</li>
<li>Our site enabled image optimization for all external hosts.</li>
</ul>
<p>The latter is the most embarrassing part of the story. We missed an obvious exploit in the web app.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="javascript" data-theme="github-dark-dimmed github-light"><code data-language="javascript" data-theme="github-dark-dimmed github-light"><span data-line=""><span>const</span><span> nextConfig</span><span> =</span><span> {</span></span>
<span data-line=""><span>  images: {</span></span>
<span data-line=""><span>    remotePatterns: [</span></span>
<span data-line=""><span>      {</span></span>
<span data-line=""><span>        protocol: </span><span>&#39;https&#39;</span><span>,</span></span>
<span data-line=""><span>        hostname: </span><span>&#39;**&#39;</span><span>,</span></span>
<span data-line=""><span>      },</span></span>
<span data-line=""><span>      {</span></span>
<span data-line=""><span>        protocol: </span><span>&#39;http&#39;</span><span>,</span></span>
<span data-line=""><span>        hostname: </span><span>&#39;**&#39;</span><span>,</span></span>
<span data-line=""><span>      },</span></span>
<span data-line=""><span>    ],</span></span>
<span data-line=""><span>  },</span></span>
<span data-line=""><span>  ...</span></span></code></pre></figure>
<p>To explain why we did this in the first place, we need to add some important context about podcasting.</p>
<p>We do not own the podcast content displayed on our site. Similar to other podcast apps like Apple and Spotify, we ingest podcast information from RSS feeds and display it in our directory. The cover images are hosted on specialized podcast hosting platforms like <a href="https://transistor.fm/">Transistor</a>, <a href="https://www.buzzsprout.com/">Buzzsprout</a>, and others. But podcasts could be hosted anywhere from a WordPress website to an S3 bucket. It is impractical to allowlist all possible hosts.</p>
<p>Optimizing an image meant that Next.js downloaded the image from one of those hosts to Vercel first, optimized it, then served to the users. If we wanted to make our site snappy, we had to either build and maintain an image optimization pipeline ourselves or use the built-in capability. As a scrappy startup for whom a web app was at best secondary, we chose the faster route without thinking much about it.</p>
<p>In retrospect, we should&#39;ve researched how it works. We&#39;re lucky no one started using our site as an image optimization API.</p>
<p>To mitigate the problem entirely, we disabled image optimization for any external URLs. Now, image optimization is only enabled for images hosted on our own domain. Podcast covers load noticeably slower. We&#39;ll need to do something about it eventually.</p>
<p>But this is not all.</p>
<h3 id="step-3-robotstxt"><a href="#step-3-robotstxt">Step 3: robots.txt</a></h3>
<p>Of course, we knew about <code>robots.txt</code>, a file that tells crawlers whether they&#39;re allowed to crawl the site or not.</p>
<p>Since both of us were new to managing a large-scale content site (our background is in backends, APIs, and web apps behind auth), we didn&#39;t even think about LLM bots. It&#39;s just not something that was on our radar. So, our <code>robots.txt</code> was a simple allow-all except for a few paths that we disallowed.</p>
<p>Our first reaction was to disable all bot traffic except Google. But when we understood that the root cause of the problem lied in the misconfigured image optimization, we decided to keep our site open to all LLM and search engine bots. Serving the text content doesn&#39;t cost us much, but we may benefit from being shown as a source of data in LLMs, which would be similar to being shown on a search engine results page (SERP).</p>
<p>We generate <code>robots.txt</code> programmatically using <a href="https://nextjs.org/docs/app/api-reference/file-conventions/metadata/robots">robots.ts</a> in Next.js. We researched the bots and added their user agents to our code. If we ever need to disable any of the bots, we can do so very quickly now. While we were at it, we disabled some paths for SEO bots like Semrush and MJ12Bot.</p>
<p>Note that <code>robots.txt</code> only works if bots respect it. It&#39;s honor-based system and there are still bad bots out there that ignore it and/or attempt to disguise themselves as users.</p>
<h4 id="user-agents-of-llm-bots"><a href="#user-agents-of-llm-bots">User agents of LLM bots</a></h4>
<table>
<thead>
<tr>
<th>User Agent</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Amazonbot</code></td>
<td><a href="https://developer.amazon.com/amazonbot">Amazon</a></td>
</tr>
<tr>
<td><code>CCBot</code></td>
<td><a href="https://commoncrawl.org/faq">Common Crawl</a></td>
</tr>
<tr>
<td><code>ClaudeBot</code></td>
<td><a href="https://privacy.anthropic.com/en/articles/10023637-does-anthropic-crawl-data-from-the-web-and-how-can-site-owners-block-the-crawler">Anthropic</a></td>
</tr>
<tr>
<td><code>GPTBot</code></td>
<td><a href="https://platform.openai.com/docs/bots/">OpenAI</a></td>
</tr>
<tr>
<td><code>Meta-ExternalAgent</code></td>
<td><a href="https://developers.facebook.com/docs/sharing/webmasters/web-crawlers/">Meta</a></td>
</tr>
<tr>
<td><code>PerplexityBot</code></td>
<td><a href="https://docs.perplexity.ai/guides/bots">Perplexity</a></td>
</tr>
</tbody>
</table>
<h4 id="user-agents-of-search-engine-crawlers"><a href="#user-agents-of-search-engine-crawlers">User agents of search engine crawlers</a></h4>
<table>
<thead>
<tr>
<th>User Agent</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Applebot</code></td>
<td><a href="https://support.apple.com/en-us/119829">Apple</a></td>
</tr>
<tr>
<td><code>Baiduspider</code></td>
<td><a href="https://www.baidu.com/search/robots_english.html">Baidu</a></td>
</tr>
<tr>
<td><code>Bingbot</code></td>
<td><a href="https://www.bing.com/webmasters/help/which-crawlers-does-bing-use-8c184ec0">Bing</a></td>
</tr>
<tr>
<td><code>ChatGPT-User</code> &amp; <code>OAI-SearchBot</code></td>
<td><a href="https://platform.openai.com/docs/bots/">OpenAI</a></td>
</tr>
<tr>
<td><code>DuckDuckBot</code></td>
<td><a href="https://duckduckgo.com/duckduckgo-help-pages/results/duckduckbot/">DuckDuckGo</a></td>
</tr>
<tr>
<td><code>Googlebot</code></td>
<td><a href="https://developers.google.com/search/docs/crawling-indexing/googlebot">Google</a></td>
</tr>
<tr>
<td><code>ImageSift</code></td>
<td><a href="https://imagesift.com/about">ImageSift by Hive</a></td>
</tr>
<tr>
<td><code>Perplexity‑User</code></td>
<td><a href="https://docs.perplexity.ai/guides/bots">Perplexity</a></td>
</tr>
<tr>
<td><code>YandexBot</code></td>
<td><a href="https://www.yandex.com/support/webmaster/robot-workings/user-agent.html">Yandex</a></td>
</tr>
</tbody>
</table>
<h4 id="user-agents-of-seo-bots"><a href="#user-agents-of-seo-bots">User agents of SEO bots</a></h4>
<table>
<thead>
<tr>
<th>User Agent</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>AhrefsBot</code></td>
<td><a href="https://ahrefs.com/robot">Ahrefs</a></td>
</tr>
<tr>
<td><code>DataForSeoBot</code></td>
<td><a href="https://dataforseo.com/dataforseo-bot">DataForSeoBot</a></td>
</tr>
<tr>
<td><code>DotBot</code></td>
<td><a href="https://moz.com/help/moz-procedures/crawlers/dotbot">DotBot</a></td>
</tr>
<tr>
<td><code>MJ12bot</code></td>
<td><a href="https://mj12bot.com/">MS12Bot</a></td>
</tr>
<tr>
<td><code>SemrushBot</code></td>
<td><a href="https://www.semrush.com/bot/">Semrush</a></td>
</tr>
</tbody>
</table>
<h2 id="how-do-we-prevent-this-in-the-future"><a href="#how-do-we-prevent-this-in-the-future">How do we prevent this in the future?</a></h2>
<p>We will start with the one thing we&#39;ve done well.</p>
<h3 id="continue-with-a-sensitive-spend-limit"><a href="#continue-with-a-sensitive-spend-limit">Continue with a sensitive spend limit</a></h3>
<p>We had a very sensitive spend limit alert. We knew we should not be spending much on Vercel, so we set it very low. When it triggered, we knew something was off.</p>
<p>This may be the most important lesson to all startups and big enterprises alike - always set spend limits for your infrastructure, or the bill may ruin you. You can probably negotiate with Vercel, AWS, GCP, etc. and they&#39;ll reduce or forgive your bill. But it&#39;s best to not put yourself in a situation where you have to ask for a favor.</p>
<h3 id="mindset-for-scale"><a href="#mindset-for-scale">Mindset for scale</a></h3>
<p>We&#39;ve learned a ton and have (hopefully) attuned ourselves to:</p>
<ul>
<li><strong>The scale we&#39;re operating at</strong> – we&#39;re serving millions of pages and need to be prepared for user traffic at that scale. The bots gave us a taste for what it would&#39;ve been like had our app gone viral.</li>
<li><strong>The scale of web crawlers, both good and bad</strong> – we need to be prepared to be &#34;anthropized&#34;, &#34;openAIed&#34;, &#34;amazoned&#34;, or &#34;semrushed.&#34; It&#39;s the new <a href="https://en.wikipedia.org/wiki/Slashdot_effect">slasdot effect</a> but without the benefit of immediate gratification.</li>
</ul>
<h3 id="ready-for-defense"><a href="#ready-for-defense">Ready for defense</a></h3>
<p>We&#39;ve now better understood the options we have for firewalling ourselves from bots if we have to do so in the future. We can use Vercel firewall as the first line of defense or add a more advanced WAF from Cloudflare if things get dire.</p>
<p>See this post from Cloudflare: <a href="https://blog.cloudflare.com/declaring-your-aindependence-block-ai-bots-scrapers-and-crawlers-with-a-single-click/">Declare your AIndependence: block AI bots, scrapers and crawlers with a single click</a></p>

<p>When we discovered the rate at which bots were crawling our site, we <a href="https://www.linkedin.com/posts/ilyabezdelev_our-site-is-getting-totally-slammed-by-activity-7293596095778074624-MD_I">posted about it</a> on LinkedIn. We were just sharing what&#39;s going on in real time, but boy did it hit the nerve. Almost 400k impressions, 2.4k likes, 270+ comments, 120+ reposts.</p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/linkedin-post.jpg" alt="LinkedIn post stats"/></p>
<p>We&#39;ve gone through all comments on the post and responded to most of them.</p>
<p>Lots of folks offered solutions like <a href="https://blog.cloudflare.com/declaring-your-aindependence-block-ai-bots-scrapers-and-crawlers-with-a-single-click/">CloudFlare</a>, using middleware, rate limiting, etc. Some offered to feed junk back to LLM bots.</p>
<p>We learned about <a href="https://arstechnica.com/tech-policy/2025/01/ai-haters-build-tarpits-to-trap-and-trick-ai-scrapers-that-ignore-robots-txt/">tarpit</a> tools like <a href="https://iocaine.madhouse-project.org/">iocaine</a> and <a href="https://zadzmo.org/code/nepenthes/">Nepenthes</a>.</p>
<blockquote>
<p><em>You could lure them into a honeypot?</em></p>
</blockquote>
<p>People rightfully pointed out that you can get ruined by infinite scalability of cloud resources.</p>
<blockquote>
<p><em>that&#39;s my biggest concern about cloud providers. You make a small mistake (everyone does) and the costs can skyrocket overnight.</em></p>
</blockquote>
<p>We learned that some people aren&#39;t aware of the LLM bot crawling activity or the scale of it. They thanked us for raising awareness.</p>
<blockquote>
<p><em>WOW - thanks for alerting us.</em></p>
</blockquote>
<p>Some people had been surprised by bots just like we were.</p>
<blockquote>
<p><em>Same here. At first I was super excited to get so many new subscriptions. We did reCaptcha and Cloudflare. Things have quieted down. Thanks for posting. I thought we were the only ones</em></p>
</blockquote>
<p>Some aren&#39;t surprised at all and see it as a problem.</p>
<blockquote>
<p><em>Very recognizable (unfortunately). These (predominantly AI) bots started noticeably hitting our platform back in May/June 2024. Lots of time &amp; efforts wasted to keep our bills in check. We also found out that not all of them respect Robots.txt, so indeed a WAF is needed as well. I can(not) imagine how painful this must/will be for smaller businesses...</em></p>
</blockquote>
<p>Some people blamed us for not being prepared and called us out on calling out AI companies. Others defended us. Virality is a double-edged sword.</p>
<p>A large portion of the comments were claiming that data scraping is unethical, illegal, etc. People were outraged. It wasn&#39;t our intention, but our post brought the issue to the zeitgeist of that day.</p>
<h2 id="parting-thoughts"><a href="#parting-thoughts">Parting thoughts</a></h2>
<p><strong>There&#39;s a part of me that is glad that this happened.</strong></p>
<p>We got a taste of operating a web app at scale before reaching scale. It was easy to block bots, but had it been caused by user traffic, we&#39;d have to swallow the cost or downgrade the experience. Bots were the canaries in a coalmine.</p>
<p><strong>Any technology has negative externalities.</strong></p>
<p>Some are obvious, some aren&#39;t. Of all the things that were happening, I was worried that we&#39;d get penalized by podcast hosters whose endpoints we were hitting at the same rate as bots requested images from our site.</p>
<p><strong>Operating at scale on the internet is a game of defense</strong></p>
<p>We can rant about bots as much as we want, but that <em>is</em> the reality we operate in. So we better acknowledge it and deal with it.</p>
<p>P.S. We&#39;ll be discussing this topic on the next episode of the <a href="https://metacast.app/podcasts/7d7e381e-907c-5b22-aefc-1fc8311d2a71">Metacast: Behind the scenes</a> podcast. Follow us wherever you listen to podcasts to hear the story with more nuance.</p>
<h2 id="upd-vercel-changed-their-image-optimization-pricing"><a href="#upd-vercel-changed-their-image-optimization-pricing">UPD: Vercel changed their image optimization pricing</a></h2>
<p>On Feb 18, 2025, just a few days after we published this blog post, Vercel <a href="https://vercel.com/changelog/faster-transformations-and-reduced-pricing-for-image-optimization">changed</a> their image optimization pricing. With the new pricing we&#39;d not have faced a huge bill.</p>
<p><img src="https://metacast.app/images/blog/engineering/postmortem-llm-bots-image-optimization/vercel-new-pricing.webp" alt="New Image Optimization pricing on Vercel"/></p>
<p>However, this wouldn&#39;t address the problem that we need to optimize images hosted outside of our domain. We ended up implementing our own image optimization.</p></div></section></article></div>
  </body>
</html>
