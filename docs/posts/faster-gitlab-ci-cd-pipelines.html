<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.nimbleways.com/let-s-make-faster-gitlab-ci-cd-pipelines/">Original</a>
    <h1>Faster Gitlab CI/CD pipelines</h1>
    
    <div id="readability-page-1" class="page"><section>
                <div>
                    <h2 id="introduction">Introduction</h2><p>CI/CD (Continuous Integration/Continuous Delivery) is one of the most useful techniques to reduce friction between dev and ops teams and improve software delivery time. However not doing it right can have an opposite effect on the delivery process and sometimes on the team productivity and happiness.</p><p>In this article, we will create a GitLab CI/CD pipeline for a <a href="https://github.com/rahuldkjain/github-profile-readme-generator">React project</a> and walk through the optimization process exploring the most common problems, their solutions, and the trade-offs we can make to have faster pipelines and happier development teams. </p><h2 id="the-job-anatomy">The job Anatomy</h2><p>Before diving deeper into pipeline optimization, let&#39;s take a closer look on how Gitlab jobs work and what are their execution phases.</p><p>When a job is created,  it goes into the pending state, until a runner is available. Once the runner is available, the job gets picked.</p><p>Then the runner starts by preparing the execution environment: in the docker executor case, it pulls the image specified in <code>gitlab-ci.yml</code> and creates a container from the same image. </p><p>By the time the container is up, the runner clones the git repo into it and runs our script from <code>.gitlab-ci.yml</code> against our code.</p><p>In case we&#39;ve specified artifacts or caches in our CI files, the job carries two more tasks out, for pulling and(/or) pushing some files as caches or artifacts. The files are even stored in an object storage service like S3 or MinIO or in the container filesystem itself.  </p><figure><img src="https://blog.nimbleways.com/content/images/2021/08/Article-CI_CD.png"/><figcaption>Gitlab job executing phases</figcaption></figure><h2 id="the-pipeline">The Pipeline</h2><p>In Gitlab CI/CD, a pipeline is simply a collection of jobs. For our case we will work on the most basic pipeline for a node project.</p><p>Our pipeline will install our node dependencies, test the code, build it and package it as a docker image.</p><figure><img src="https://blog.nimbleways.com/content/images/2021/08/Article-CI_CD-Page-2.png"/><figcaption>Gitlab CI/CD pipeline</figcaption></figure><p>An example <code>gitlab-ci.yml</code> file will look something like this:</p><pre><code>stages:
  - install_deps
  - test
  - build
  - package

default:
  image: node:lts
  before_script:
    - yarn install

Install Dependencies:
  stage: install_deps
  script:
    - yarn install
  when: manual

Test:
  stage: test
  script:
    - yarn test

Build:
  stage: build
  script:
    - yarn build
  artifacts:
    paths:
      - public

Docker Build:
  before_script:
    # to skip default before_script
    - docker info
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY

  stage: package
  image: docker:stable
  services:
    - docker:dind
  script:
    - docker build -t ${CI_REGISTRY_IMAGE}/gprg:latest .
    - docker push ${CI_REGISTRY_IMAGE}/gprg:latest
</code></pre><h2 id="caching">Caching</h2><h4 id="the-first-steps">The first steps</h4><p>Caching is one of the most useful techniques we can use to speed-up gitlab CI jobs. The Gitlab documentation devoted an <a href="https://docs.gitlab.com/ee/ci/caching/">entire page</a> to caching with a good amount of details about all the caching features and their use-cases.</p><p>In the previous <code>gitlab-ci.yaml</code> file we can see that we need to install yarn dependencies aka <code>node_modules</code> each time we need to execute a yarn command. This operation is redundant and so costly in time. For this reason it should be done at most once per pipeline execution. </p><p>To do so we&#39;ll need to cache the <code>node_modules</code> folder and preferably use <code>yarn.lock</code> file as a cache key. This way each time <code>yarn.lock</code> changes, the cache gets invalidated and rebuilt from scratch.</p><p>We will add the cache key to all yarn-related jobs. The new version will look something like this (The unchanged parts are omitted for brevity): </p><figure><pre><code>...
Install Dependencies:
  stage: install_deps
  cache:
    key:
      files:
        - yarn.lock
    paths:
      - node_modules
  script:
    - yarn install

Test:
  stage: test
  cache:
    key:
      files:
        - yarn.lock
    paths:
      - node_modules
  script:
    - yarn test

Build:
  stage: build
  cache:
    key:
      files:
        - yarn.lock
    paths:
      - node_modules
  script:
    - yarn build
  artifacts:
    paths:
      - public
...</code></pre><figcaption>gitlab-ci.yaml with cache</figcaption></figure><p>In our case the pipeline with cached <code>node_modules</code> are faster by more than 2 mins than the one without. The gain can be more important if you are self-hosting Gitlab on a machine with slow internet speed. </p><h4 id="avoiding-useless-work-using-cache-policy">Avoiding useless work using cache policy</h4><p>By default, each time a job with cache tries to pull the cache specified in its definition in <code>gitlab-ci.yaml</code>, then it executes the commands in the <code>script</code> and finally, it pushes the new changes to the files under <code>cache:paths</code> to the cache storage server again. We can change this behavior by changing the cache policy. </p><p>There are 3 cache policies:</p><ul><li><code>push-pull</code> (default policy): the jobs pull the cache at the beginning and push it at end of the job execution.</li><li><code>pull</code> the jobs pull the cache at the beginning but do not push the changes again.</li><li><code>push</code>: the jobs do not pull the cache before the job execution, however they do push it at end.</li></ul><p>In our example pipeline, only the <code>install Dependencies</code> job needs to update the cache, unlike the <code>Test</code> and <code>Build</code> which do not and only need the cache to be pulled.</p><p>Having that the resulting pipeline will look like this:</p><figure><pre><code>...
Install Dependencies:
  stage: install_deps
  cache:
    key:
      files:
        - yarn.lock
    paths:
      - node_modules
    policy: pull-push
  script:
    - yarn install

Test:
  stage: test
  cache:
    key:
      files:
        - yarn.lock
    paths:
      - node_modules
    policy: pull
  script:
    - yarn test

Build:
  stage: build
  cache:
    key:
      files:
        - yarn.lock
    paths:
      - node_modules
    policy: pull
  script:
    - yarn build
  artifacts:
    paths:
      - public
...</code></pre><figcaption>cache policies</figcaption></figure><p>Changing the cache policy to pull for <code>Test</code> and <code>Build</code> decreased the pipeline time by more than 30s in our case. And in some cases having the right cache policies for your jobs can make a huge difference especially if your caches have bigger sizes.</p><h4 id="faster-caching-by-controlling-compression">Faster caching by controlling compression</h4><p>Before getting pushed or pulled, the caches (and artifacts) are compressed using the zip algorithm. And starting from version 13.6, we can use fastzip to zip/unzip our caches and artifacts. Even better we can have 5 compression levels to choose from according to the speed/compression ratio we want to achieve (slowest, slow, default, fast, and fastest)</p><p>This feature is deployed under a <a href="https://docs.gitlab.com/runner/configuration/feature-flags.html">feature flag</a> and should be enabled by setting the <code>FF_USE_FASTZIP</code> to <code>true</code>. </p><figure><pre><code>variables:
  FF_USE_FASTZIP: &#34;true&#34;
  # These can be specified per job or per pipeline
  ARTIFACT_COMPRESSION_LEVEL: &#34;fast&#34;
  CACHE_COMPRESSION_LEVEL: &#34;fast&#34;</code></pre><figcaption>Fastzip</figcaption></figure><p>More speed! In our case, the pipeline time drop by almost another 30s using <code>fast</code> compression level since the compression takes less time while the cache size does not increase drastically. </p><p>Remember it is always about trade-offs: We could use <code>fastest</code> instead but the size will be huge, and we will lose the time we economized from compression time, due to more upload/download time.</p><p>So if you are hosting your own runners, it would be better to experience with the different compression levels and choose the one that suits you the best. The choice will depend on how much storage you can afford, your network IO speed, and your CPU power (for compression). </p><p>For example, if you have a lot of storage and good network speed, but your machines have slow CPUs <code>fastest</code> can be a good choice, whereas if you have fast CPUs and less storage (or the network between your runners and your cache server is slow) <code>slow</code> or <code>slower</code> will give better results.</p><h4 id="caching-more-things-using-multiple-caches">Caching more things using multiple caches</h4><p>Until Gitlab 13.10, we could only have one cache per job. This pushed us to shove many folders and files into the same cache, which can be considered bad practice because it breaks the atomicity of caches and make the cache size very big. </p><figure><img src="https://blog.nimbleways.com/content/images/2021/08/Article-CI_CD-Page-4.png"/><figcaption>One cache vs Multiple caches </figcaption></figure><p>Now we can have up to 4 caches per job, that we can use to cache other useful things:</p><ol><li>yarn cache: According to the <a href="https://classic.yarnpkg.com/en/docs/install-ci/#gitlab">yarn docs</a> &#34;<em>Yarn stores every package in a global cache in your user directory on the file system</em>&#34;. </li><li>Test cache(jest): In our example project we are using jest to execute our test. Jest uses cache to speed up its execution. </li><li>Build cache:  When running <code>yarn build</code> the build tool creates a build cache and stores it somewhere in the project directory (often under <code>.cache</code> or <code>.node_modules/.cache</code>). This cache can speed up the build drastically.</li></ol><p>Finally, our jobs will have multiple caches each:</p><figure><pre><code>Install Dependencies:
  stage: install_deps
  cache:
    - key:
        files:
          - yarn.lock
      paths:
        - node_modules
      policy: pull-push
    - key: yarn-$CI_JOB_IMAGE
      paths:
        - .yarn
      policy: pull-push

  script:
    - yarn install --cache-folder .yarn

Test:
  stage: test
  script:
    - yarn test --cacheDirectory &#34;.jestcache&#34;
  cache:
    - key:
        files:
          - yarn.lock
      paths:
        - node_modules
      policy: pull
    - key: test-$CI_JOB_IMAGE
      paths:
        - .jestcache
      policy: pull-push

Build:
  stage: build
  cache:
    - key:
        files:
          - yarn.lock
      paths:
        - node_modules
      policy: pull
    - key: build-$CI_JOB_IMAGE
      paths:
        - .cache
        - public
      policy: pull-push
  script:
    - yarn build
  artifacts:
    paths:
      - public</code></pre><figcaption>Multiple caches</figcaption></figure><h4 id="one-last-refactor">One last refactor</h4><p>Even though our <code>gitlab-ci.yaml</code> works as expected, it is not the best when it comes to readability and style. To fix that we are going to use some YAML-Fu notably <a href="https://docs.gitlab.com/ee/ci/yaml/#anchors">yaml anchors</a>.</p><figure><pre><code># Caches
.node_modules-cache: &amp;node_modules-cache
  key:
    files:
      - yarn.lock
  paths:
    - node_modules
  policy: pull

.yarn-cache: &amp;yarn-cache
  key: yarn-$CI_JOB_IMAGE
  paths:
    - .yarn

.test-cache: &amp;test-cache
  key: test-$CI_JOB_IMAGE
  paths:
    - .jestcache
  policy: pull-push

.build-cache: &amp;build-cache
  key: build-$CI_JOB_IMAGE
  paths:
      - .cache
      - public
  policy: pull-push

# Jobs
Install Dependencies:
  stage: install_deps
  script:
    - yarn install --cache-folder .yarn
  cache:
    - &lt;&lt;: *node_modules-cache
      policy: pull-push # We override the policy
    - &lt;&lt;: *yarn-cache # This is not required if your internet speed is good or if you use a local npm registry. 

Test:
  stage: test
  script:
    - yarn test --cacheDirectory &#34;.jestcache&#34;
  cache:
    - &lt;&lt;: *node_modules-cache
    - &lt;&lt;: *test-cache

Build:
  stage: build
  script:
    - yarn build
  cache:
    - &lt;&lt;: *node_modules-cache
    - &lt;&lt;: *build-cache
  artifacts:
    paths:
      - public</code></pre><figcaption>Refactoring using anchors</figcaption></figure><p>Again, we gained another 30s. And again, it is because we are working on a small project with smaller test suites and faster builds. On bigger projects caching build and test caches in Gitlab, can decrease the pipeline execution time by minutes.  </p><h4 id="another-type-of-cache-docker-cache">Another type of cache: Docker cache</h4><p>In our pipeline, we had a job where we build and push a container image to Gitlab container registry. This job is taking more than a minute when it most of the time it shouldn&#39;t.</p><p>Each time the <code>Docker Build</code> job runs it builds the image from scratch, which can take a lot of time depending on our build steps. To avoid that, we pull the latest image from our registry (Gitlab registry) and we cache from it in the current build:</p><pre><code>...
Docker Build:
  before_script:
    # to skip default before_script
    - docker info
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  stage: package
  cache: {}
  image: docker:stable
  services:
    - docker:dind
  script:
    - docker pull ${CI_REGISTRY_IMAGE}/gprg:latest || true
    - docker build
      --cache-from ${CI_REGISTRY_IMAGE}/gprg:latest
      -t ${CI_REGISTRY_IMAGE}/gprg:prod
      -t ${CI_REGISTRY_IMAGE}/gprg:latest .
    - docker push ${CI_REGISTRY_IMAGE}/gprg</code></pre><p>This is only useful if it takes less to pull an image from your container registry than to build it from scratch. An example of this situation is when you are hosting your own Gitlab registry and your runners in the same network.</p><p>In our case, it didn&#39;t make a big difference, since it took almost the same time to pull the Nginx from the docker hub than from Gitlab container registry.  </p><h2 id="controlling-pipelines-jobs-execution-flow">Controlling Pipelines/jobs execution flow</h2><h4 id="pipelines-should-not-always-run">Pipelines should not always run</h4><p>Until now, having the <code>gitlab-ci.yaml</code> committed into a Gitlab repository, Gitlab CI/CD will create a pipeline each time we make a change to our entire git repository. It depends on the project requirements and organization but often this is not the desired behavior since it creates many unwanted pipelines.</p><p>To specify when to run our pipelines we use <code>workflow:rules</code></p><figure><pre><code>....
workflow:
  rules:
    - if: &#39;$CI_MERGE_REQUEST_IID&#39; # Run pipelines on Merge Requests
    - if: &#39;$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH&#39; # Run pipelines on main branch
...</code></pre><figcaption>wokflow:rules</figcaption></figure><p>Now that we have our workflow defined, let&#39;s create a branch and push it to Gitlab.</p><pre><code>git checkout -b &#34;workflow-rules&#34;
git push --set-upstream origin workflow-rules</code></pre><p>Nothing will happen, and no pipeline will be created until we create a merge request.</p><h4 id="jobs-should-not-always-run-too">Jobs should not always run too</h4><p>Like pipelines, we can control when a job is created and when it is not. To do so we either use <code>only:</code> and <code>except</code> or we use <code>rules</code>. </p><p><code>rules</code> is recommended by Gitlab, since they have plans to deprecate <code>only</code> and <code>except</code>. </p><p>Let&#39;s suppose that in our example, we would like to run <code>Build</code> and <code>Docker Build</code> on the main branch only and <code>Test</code> on merge request.</p><figure><img src="https://blog.nimbleways.com/content/images/2021/08/Article-CI_CD-Copy-of-Page-2-1-.png"/></figure><p>Our <code>gitlab-ci.yaml</code> will look like this:</p><pre><code>....
Test:
  stage: test
  rules:
    - if: &#39;$CI_MERGE_REQUEST_IID&#39; # Run job on Merge Requests
...

Build:
  stage: build
  rules:
    - if: &#39;$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH&#39; # Run job on main branch
...

Docker Build:
  stage: package
  rules:
    - if: &#39;$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH&#39; # Run job on main branch</code></pre><p>Now when we create a merge request the created pipeline will only contain <code>Install Dependencies</code> and <code>Test</code>. In the other hand once the Merge Request is merged, the pipeline will contain <code>Install Dependencies</code>, <code>Build</code> and <code>Docker Build</code>.</p><h4 id="dag-pipelines">DAG Pipelines</h4><p>DAG (Directed Acyclic Graph) pipelines are one of the most useful Gitlab CI/CD features, especially for big multi-tier projects, and/or mono repos.  </p><p>By default jobs in one stage are not executed until all the jobs in the previous stage succeed. However, this is not always the way things are in real projects. Enters DAG Pipelines. </p><p>Using the <code>needs</code> keyword we can explicitly specify the relationships between our jobs and more specifically the jobs on which we want each job to depend.</p><p>To demonstrate the use of <code>needs</code> we will create an additional <code>Build</code> that won&#39;t depend on <code>tests</code>, unlike the <code>Build</code> we already had.</p><figure><img src="https://blog.nimbleways.com/content/images/2021/08/Article-CI_CD-Copy-of-Page-3.png"/><figcaption>DAG pipeline</figcaption></figure><p>To implement the previous pipeline, we should add a <code>needs</code> keyword to <code>Build B</code></p><figure><pre><code>...
Build B:
  needs: [&#34;Install Dependencies&#34;]
  stage: build
  rules:
    - if: &#39;$CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH&#39; # Run pipelines on main branch
  script:
    - yarn build
  cache:
    - &lt;&lt;: *node_modules-cache
    - &lt;&lt;: *build-cache
...</code></pre><figcaption>needs</figcaption></figure><p>When the pipeline runs, the <code>Build B</code> won&#39;t wait for <code>Test</code> to finish, it will start as soon as <code>Install Dependencies</code> finishes.</p><p>If you still don&#39;t get how DAG pipelines are useful, here is another example where you should use them.</p><figure><img src="https://blog.nimbleways.com/content/images/2021/08/Article-CI_CD-Copy-of-Page-2-2-.png"/><figcaption>Ordinary vs DAG pipelines (Monorepo)</figcaption></figure><h4 id="optional-needs">Optional Needs </h4><p>Optional needs are a very important feature introduced in Gitlab v13.10 to complement <code>needs</code>. </p><p>To understand how optional needs work, let&#39;s suppose we want to achieve the following:</p><ul><li>Tests Jobs run on merge requests only. The other jobs run always.</li><li><code>Build A</code> depends on <code>Test A</code> and <code>Build B</code> depends on <code>Test B</code>.</li></ul><figure><img src="https://blog.nimbleways.com/content/images/2021/08/Article-CI_CD-Copy-of-Page-3-2-.png"/><figcaption>Optional Needs</figcaption></figure><p>The obvious solution will be the following:</p><pre><code>Test A:
  stage: test
  rules:
    - if: &#39;$CI_MERGE_REQUEST_IID&#39; 
Test B:
  rules:
    - if: &#39;$CI_MERGE_REQUEST_IID&#39;
Build A:
  needs: [&#34;Test A&#34;]
Build B:
  needs: [&#34;Test B&#34;]</code></pre><p>This solution will work only in case of merge requests. Otherwise, the Test jobs won&#39;t be created and the pipeline will fail since Build Jobs depend on them. </p><p>Another solution is not using DAG at all. This way the pipeline won&#39;t fail but in the merge request, Build jobs won&#39;t start until both Test jobs finish. </p><p>The better solution is to use optional needs to specify the dependency between Build and Test jobs without imposing it.</p><pre><code>Test A:
  stage: test
  rules:
    - if: &#39;$CI_MERGE_REQUEST_IID&#39; 
Test B:
  rules:
    - if: &#39;$CI_MERGE_REQUEST_IID&#39;
Build A:
  needs: 
    - job: &#34;Test A&#34;
      optional: true
Build B:
  needs: 
    - job: &#34;Test B&#34;
      optional: true</code></pre><h2 id="miscellaneous">Miscellaneous</h2><h4 id="always-use-small-prebuilt-local-images">Always use small prebuilt local images</h4><p>If you are using docker or Kubernetes executor always choose small images under the <code>image:</code> tag. Having smaller images the runner takes less time preparing the environment especially if the image changes a lot and needs to be pulled from elsewhere.</p><p>Another thing you should consider if you are hosting Gitlab in your own servers is to store the images used in CI in a local container registry (You can create a repo for all your images and use Gitlab CI to update the images once in a while). This way pulling them will be faster, and more secure.</p><figure><pre><code>default:
  image: node:lts-alpine</code></pre><figcaption>use alpine images when you can</figcaption></figure><p>This is so important! A change this easy dropped the pipeline execution time by 1.5 minutes.</p><h4 id="design-your-pipelines-well">Design your pipelines well</h4><p>Take your time to think about the features you want to have in your pipeline, and what are the ways available to implement them. Most of the time, there are many ways to do things, and you should pick the best and the most performant and scalable.</p><p>Another thing you should keep in mind when designing CI/CD pipelines is debuggability (if that&#39;s a word), which means when something does not work properly you should be able to investigate the problem easily.</p><p>The last thing I want to mention is avoiding delaying feedback. The pipeline should fail when it should fail! If there is something wrong it should be detected earlier in the pipeline.</p><h4 id="write-better-dockerfiles">Write better Dockerfiles</h4><p>If you are using Docker to package your application, try to make your images as small as possible, and try to make the most out of the Docker&#39;s cache. Our friends at Padok have discussed this in their excellent Gitlab CI/CD <a href="https://www.padok.fr/en/blog/gitlab-ci-optimization">article</a>.</p><p>Again! Always use small base images!</p><h4 id="don-t-use-docker">Don&#39;t use Docker</h4><p>This may seem a bit weird, but it is one of the most relevant points to consider when trying to optimize your CI/CD pipelines.</p><p>In order to build images using Docker in Gitlab using the docker or Kubernetes executor. you should spin up two containers and wait for the docker daemon to be ready, then start building your images. These operations take a lot of time from the initialization part.</p><p>To avoid all of this you can use alternative image builders like Kaniko or Buildah.</p><p>In our example using buildah instead of Docker, brought us more than 30s </p><pre><code>Docker Build:
  before_script:
    # to skip default before_script
    - buildah info
    - buildah login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  stage: package
  image: quay.io/buildah/stable:latest
  script:
    - buildah build
      -t ${CI_REGISTRY_IMAGE}/gprg:prod
      -t ${CI_REGISTRY_IMAGE}/gprg:latest .
    - buildah push ${CI_REGISTRY_IMAGE}/gprg</code></pre><h4 id="interruptible-pipelines">Interruptible pipelines</h4><p>When you a pipeline is running on a specific branch and you run another one, most of the time you will want to stop/interrupt the one already running.</p><p>To do so you should label all its job or at least the one you want as <a href="https://docs.gitlab.com/ee/ci/yaml/index.html#interruptible">interruptible</a>. This is useful to limit the number of jobs running at the same time.</p><pre><code>Install Dependencies:
  interruptible: true
  ...
  
Test:
  interruptible: true
  ...
  
Build:
  interruptible: false
  ...</code></pre><p>A new pipeline on a branch will cancel the previous one if it running <code>Install Dependencies</code> or <code>Test</code> job. Once <code>Build</code> starts running the old pipeline can&#39;t be interrupted.</p><h2 id="conclusion">Conclusion</h2><p>Although applying all these small tips and tricks one at a time, made our pipeline much faster: the pipeline time went from 08:14 to 02:55 (around 280% faster), there is still room for improvement. </p><p>In fact, Gitlab CI/CD offers more features like self-hosted runners and parallel jobs that we can be leveraged to improve the performance even further. </p><p>Thank you for reading! Happy learning! </p>
                </div>
            </section></div>
  </body>
</html>
