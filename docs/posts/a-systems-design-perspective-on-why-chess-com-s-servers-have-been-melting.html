<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ntietz.com/blog/chess-com-servers-melting-why/">Original</a>
    <h1>A systems design perspective on why chess.com&#39;s servers have been melting</h1>
    
    <div id="readability-page-1" class="page"><article>
    

    <p><strong>Monday, February 13, 2023</strong></p>

    <p>January 2023 was a rough month if you wanted to play chess on the most popular chess website, <a href="https://chess.com">chess.com</a>.
Their service has been experiencing an unprecedented amount downtime because of a huge influx of users.
There have been days where it&#39;s all but unusable.
It&#39;s frustrating as a user!
It&#39;s also surely frustrating for the business behind the site.</p>
<p>Chess has reached an all-time peak in popularity.
In January 2023, <a href="https://trends.google.com/trends/explore?date=today%205-y&amp;geo=US&amp;q=chess">Google search traffic</a> exceeded the boom from the release of The Queen&#39;s Gambit.
There&#39;s a huge influx of new or returning players, and they flock to the site with the obvious domain.
Chess.com&#39;s app has hit <strong>#1 most downloaded free game</strong> on the iOS app store.</p>
<p>Part of doing good systems design is planning for capacity.
A general rule of thumb is that you should design a system for up to a certain amount of growth.
Beyond some point, architectural requirements will be dramatically different.
Planning for capacity does not mean planning for <em>infinite</em> capacity, but what may <em>realistically</em> happen.</p>
<p>Why not plan for universal adoption from the very beginning?
Why not create something which can scale infinitely?
Because <strong>it&#39;s usually too expensive</strong>.
Making something that&#39;s infinitely scalable means that you need to have (effectively) infinite capacity, and servers have to be paid for somehow.</p>
<p>Some things can easily and cheaply be scaled up to the max.
Static sites are pretty easy on that front.
You can put a CDN like Cloudflare or Fastly in front of them and you get a lot of scale for very little money, and they can absorb big spikes in traffic.
But it&#39;s not free, and it&#39;s not as cheap as it seems.</p>
<p>This blog is hosted on a small VPS without a CDN.
So far, the traffic hasn&#39;t required a CDN to serve: the little VPS chugs along just fine.
I could put a CDN in front of it, and it would be free or cheap to get gigantic capacity.
I&#39;ve held off on doing it, because there&#39;s cost from complexity.
By adding in another component like a CDN, I would have to worry about caching and propagation time.
I would have to worry about deployment and configuration.</p>
<p>There&#39;s value in simplicity.
Scaling usually adds complexity.</p>
<p>Adding complexity early on leaves a lot on the table.
Instead of adding features that users could benefit from, you have this intangible benefit: the ability to ✨scale✨.
In an ideal world, users never even <em>notice</em> the work you put in to scaling, because things work as they expect.
Users really only notice when scaling <em>isn&#39;t</em> happening.</p>
<p>So if the current growth wasn&#39;t planned for already, why can&#39;t they just scale up <em>now</em>?
We can&#39;t say for sure, because we don&#39;t know the details of their systems.
But we can gather some information:</p>
<ul>
<li>According to a <a href="https://showcase.withgoogle.com/chess">Google Cloud showcase</a>, chess.com uses GCP. So <strong>they use some cloud services</strong>.</li>
<li>They also <strong>use a lot of on-prem hardware</strong>, according to their <a href="https://chesscom.rippling-ats.com/job/402726/sysops-site-reliability-engineer-sre">SRE job description</a>.</li>
<li>They <strong>use MySQL as a primary database</strong>, based on their job descriptions.</li>
<li>They <strong>use a NoSQL store as another database</strong>, also based on their job descriptions.</li>
</ul>
<p>They have a <a href="https://www.chess.com/blog/CHESScom/chess-is-booming-and-our-servers-are-struggling">blog post</a> out about why their servers are struggling, and they explicitly mention that they have hardware shipments arriving soon with &#34;the most powerful possible live chess and database servers&#34;, so presumably a lot of what powers their live play is still their on-prem hardware.</p>
<p>But they also say that they have other bottlenecks.
This is the whack-a-mole aspect of scaling systems.
You measure the system and find one bottleneck, and you generally cannot find the <em>next</em> bottleneck until that one is resolved.</p>
<p>They&#39;ve identified a number of bottlenecks already, and one of their actions in particular gives some reasonable information about what the database looked like before: they&#39;re working on separating out the database with <strong>users and gameplay</strong>.
From their description, it sounds like <strong>all</strong> of the data for chess.com was in one big MySQL database.
With beefy hardware, this can last a long time, but eventually it hits a breaking point.
We found the breaking point!</p>
<p>Why wasn&#39;t it addressed earlier?
Two primary reasons, I think:</p>
<ul>
<li>It was likely known that it <em>will</em> be an issue later, but again, scale is <em>expensive</em>.
Choosing to break apart the database now would be a very expensive project, delaying major new features, when that scale doesn&#39;t seem likely!
And on top of that, they&#39;re in the midst of integrating in systems from acquiring the Play Magnus Group, so they&#39;re not exactly short of work to do.</li>
<li>Load testing is <em>hard</em>, so capacity planning is hard.
It&#39;s tough to create a load that&#39;s a good facsimile of real production data, so it&#39;s likely that the test will not give an exact understanding of the load you can handle.
(That&#39;s why you aim for load test results that are better than you need by a wide margin.)
So it&#39;s possible they didn&#39;t know exactly <em>when</em> they would hit the breaking point, and what would break when they did.</li>
</ul>
<p>All the things that they&#39;re doing to respond to this influx of users are labor intensive and expensive, in terms of time now, real money, and perhaps most importantly in terms of future maintenance costs.
It&#39;s going to be <em>harder</em> to maintain chess.com now that their database is sharded and tables are split out across separate databases.
It&#39;s very <em>easy</em> to spin up a local stack for development when you have fewer things to spin up!</p>
<p>All that to get to the point:
From a systems design perspective, <strong>a system is well-designed if it meets the requirements, but doesn&#39;t dramatically exceed them</strong>.
One part is about doing what it&#39;s supposed to; the other part is about doing so <em>efficiently</em>.
If they&#39;d been able to handle this massive boom in users, well beyond what any reasonable model would have projected, then they would have produced a design that was in all likelihood very wasteful.</p>
<p>Major hugs to all the folks at chess.com who are dealing with these outages.
I know you&#39;re doing your best.
Hang in there.</p>
<hr/>



  </article><p>
    If this post was enjoyable or useful for you, <strong>please share it!</strong>
    If you have comments, questions, or feedback, please email <a href="mailto:~ntietz/public-inbox@lists.sr.ht">my public inbox</a> or <a href="mailto:me@ntietz.com">my personal email</a>.
    To get new posts, please use my <a href="https://ntietz.com/atom.xml">RSS feed</a>.
  </p></div>
  </body>
</html>
