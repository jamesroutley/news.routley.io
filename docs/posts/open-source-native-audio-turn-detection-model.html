<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/pipecat-ai/smart-turn">Original</a>
    <h1>Show HN: Open-source, native audio turn detection model</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This is an open source, community-driven, native audio turn detection model.</p>
<p dir="auto">HuggingFace page: <a href="https://huggingface.co/pipecat-ai/smart-turn" rel="nofollow">pipecat-ai/smart-turn</a></p>
<p dir="auto">Turn detection is one of the most important functions of a conversational voice AI technology stack. Turn detection means deciding when a voice agent should respond to human speech.</p>
<p dir="auto">Most voice agents today use <em>voice activity detection (VAD)</em> as the basis for turn detection. VAD segments audio into &#34;speech&#34; and &#34;non-speech&#34; segments. VAD can&#39;t take into account the actual linguistic or acoustic content of the speech. Humans do turn detection based on grammar, tone and pace of speech, and various other complex audio and semantic cues. We want to build a model that matches human expectations more closely than the VAD-based approach can.</p>
<p dir="auto">This is a truly open model (BSD 2-clause license). Anyone can use, fork, and contribute to this project. This model started its life as a work in progress component of the <a href="https://pipecat.ai" rel="nofollow">Pipecat</a> ecosystem. Pipecat is an open source, vendor neutral framework for building voice and multimodal realtime AI agents.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Current state of the model</h2><a id="user-content-current-state-of-the-model" aria-label="Permalink: Current state of the model" href="#current-state-of-the-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This is an initial proof-of-concept model. It handles a small number of common non-completion scenarios. It supports only English. The training data set is relatively small.</p>
<p dir="auto">We have experimented with a number of different architectures and approaches to training data, and are releasing this version of the model now because we are confident that performance can be rapidly improved.</p>
<p dir="auto">We invite you to try it, and to contribute to model development and experimentation.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Run the proof-of-concept model checkpoint</h2><a id="user-content-run-the-proof-of-concept-model-checkpoint" aria-label="Permalink: Run the proof-of-concept model checkpoint" href="#run-the-proof-of-concept-model-checkpoint"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Set up the environment.</p>
<div data-snippet-clipboard-copy-content="python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt"><pre><code>python3.12 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
</code></pre></div>
<p dir="auto">Run a command-line utility that streams audio from the system microphone, detects segment start/stop using VAD, and sends each segment to the model for a phrase endpoint prediction.</p>
<div data-snippet-clipboard-copy-content="# 
# It will take about 30 seconds to start up the first time.
#

# &#34;Vocabulary&#34; is limited. Try:
#
#   - &#34;I can&#39;t seem to, um ...&#34;
#   - &#34;I can&#39;t seem to, um, find the return label.&#34;

python record_and_predict.py"><pre><code># 
# It will take about 30 seconds to start up the first time.
#

# &#34;Vocabulary&#34; is limited. Try:
#
#   - &#34;I can&#39;t seem to, um ...&#34;
#   - &#34;I can&#39;t seem to, um, find the return label.&#34;

python record_and_predict.py
</code></pre></div>

<p dir="auto">The current version of this model is based on Meta AI&#39;s Wav2Vec2-BERT backbone. More on model architecture below.</p>
<p dir="auto">The high-level goal of this project is to build a state-of-the-art turn detection model that is:</p>
<ul dir="auto">
<li>Anyone can use,</li>
<li>Is easy to deploy in production,</li>
<li>Is easy to fine-tune for specific application needs.</li>
</ul>
<p dir="auto">Current limitations:</p>
<ul dir="auto">
<li>English only</li>
<li>Relatively slow inference
<ul dir="auto">
<li>~150ms on GPU</li>
<li>~1500ms on CPU</li>
</ul>
</li>
<li>Training data focused primarily on pause filler words at the end of a segment.</li>
</ul>
<p dir="auto">Medium-term goals:</p>
<ul dir="auto">
<li>Support for a wide range of languages</li>
<li>Inference time &lt;50ms on GPU and &lt;500ms on CPU</li>
<li>Much wider range of speech nuances captured in training data</li>
<li>A completely synthetic training data pipeline</li>
<li>Text conditioning of the model, to support &#34;modes&#34; like credit card, telephone number, and address entry.</li>
</ul>

<p dir="auto">Wav2Vec2-BERT is a speech encoder model developed as part of Meta AI&#39;s Seamless-M4T project. It is a 580M parameter base model that can leverage both acoustic and linguistic information. The base model is trained on 4.5M hours of unlabeled audio data covering more than 143 languages.</p>
<p dir="auto">To use Wav2Vec2-BERT, you generally add additional layers to the base model and then train/fine-tune on an application-specific dataset.</p>
<p dir="auto">We are currently using a simple, two-layer classification head, conveniently packaged in the Hugging Face Transformers library as <code>Wav2Vec2BertForSequenceClassification</code>.</p>
<p dir="auto">We have experimented with a variety of architectures, including the widely-used predecessor of Wav2Vec2-BERT, Wav2Vec2, and more complex approaches to classification. Some of us who are working on the model think that the simple classification head will work well as we scale the data set to include more complexity. Some of us have the opposite intuition. Time will tell! Experimenting with additions to the model architecture is an excellent learning project if you are just getting into ML engineering. See &#34;Things to do&#34; below.</p>

<ul dir="auto">
<li><a href="https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/" rel="nofollow">Meta AI Seamless paper</a></li>
<li><a href="https://github.com/facebookresearch/seamless_communication?tab=readme-ov-file#w2v-bert-20-speech-encoder">W2v-BERT 2.0 speech encoder README</a></li>
<li><a href="https://huggingface.co/docs/transformers/v4.49.0/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification" rel="nofollow">Wav2Vec2BertForSequenceClassification HuggingFace docs</a></li>
</ul>

<p dir="auto"><code>predict.py</code> shows how to pass an audio sample through the model for classification. A small convenience function in <code>inference.py</code> wraps the audio preprocessing and PyTorch inference code.</p>
<div data-snippet-clipboard-copy-content="# defined in inference.py
def predict_endpoint(audio_array):
    &#34;&#34;&#34;
    Predict whether an audio segment is complete (turn ended) or incomplete.

    Args:
        audio_array: Numpy array containing audio samples at 16kHz

    Returns:
        Dictionary containing prediction results:
        - prediction: 1 for complete, 0 for incomplete
        - probability: Probability of completion class
    &#34;&#34;&#34;

    # Process audio
    inputs = processor(
        audio_array,
        sampling_rate=16000,
        padding=&#34;max_length&#34;,
        truncation=True,
        max_length=800,  # Maximum length as specified in training
        return_attention_mask=True,
        return_tensors=&#34;pt&#34;,
    )

    # Move inputs to device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Run inference
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

        # Get probabilities using softmax
        probabilities = torch.nn.functional.softmax(logits, dim=1)
        completion_prob = probabilities[0, 1].item()  # Probability of class 1 (Complete)

        # Make prediction (1 for Complete, 0 for Incomplete)
        prediction = 1 if completion_prob &gt; 0.5 else 0

    return {
        &#34;prediction&#34;: prediction,
        &#34;probability&#34;: completion_prob,
    }"><pre><code># defined in inference.py
def predict_endpoint(audio_array):
    &#34;&#34;&#34;
    Predict whether an audio segment is complete (turn ended) or incomplete.

    Args:
        audio_array: Numpy array containing audio samples at 16kHz

    Returns:
        Dictionary containing prediction results:
        - prediction: 1 for complete, 0 for incomplete
        - probability: Probability of completion class
    &#34;&#34;&#34;

    # Process audio
    inputs = processor(
        audio_array,
        sampling_rate=16000,
        padding=&#34;max_length&#34;,
        truncation=True,
        max_length=800,  # Maximum length as specified in training
        return_attention_mask=True,
        return_tensors=&#34;pt&#34;,
    )

    # Move inputs to device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Run inference
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

        # Get probabilities using softmax
        probabilities = torch.nn.functional.softmax(logits, dim=1)
        completion_prob = probabilities[0, 1].item()  # Probability of class 1 (Complete)

        # Make prediction (1 for Complete, 0 for Incomplete)
        prediction = 1 if completion_prob &gt; 0.5 else 0

    return {
        &#34;prediction&#34;: prediction,
        &#34;probability&#34;: completion_prob,
    }
</code></pre></div>

<p dir="auto">All training code is defined in <code>train.py</code>.</p>
<p dir="auto">You can run training locally or using <a href="https://modal.com" rel="nofollow">Modal</a>. Training runs are logged to <a href="https://www.wandb.ai" rel="nofollow">Weights &amp; Biases</a> unless you disable logging.</p>
<div data-snippet-clipboard-copy-content="# To run a training job on Modal, upload training data to a Modal volume,
# set up the Modal environment, then run:
modal run --detach train.py"><pre><code># To run a training job on Modal, upload training data to a Modal volume,
# set up the Modal environment, then run:
modal run --detach train.py
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Collecting and contributing data</h3><a id="user-content-collecting-and-contributing-data" aria-label="Permalink: Collecting and contributing data" href="#collecting-and-contributing-data"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Currently, there are two datasets used for training and evaluation:</p>
<ul dir="auto">
<li>datasets/human_5_all -- segmented speech recorded from human interactions</li>
<li>datasets/rime_2 -- synthetic speech generated using <a href="https://rime.ai/" rel="nofollow">Rime</a></li>
</ul>
<p dir="auto">Four splits are created <a href="https://github.com/pipecat-ai/smart-turn/blob/a9e49f18da2d70dde94477be05405638db9dd8bc/train.py#L188">when these two datasets are loaded</a>.</p>
<ul dir="auto">
<li>The train, validate, and test sets are a mix of synthetic and human data</li>
<li>The human eval set contains only human data</li>
</ul>
<div data-snippet-clipboard-copy-content="  7 -- TRAIN --
  8   Total samples: 5,694
  9   Positive samples (Complete): 2,733 (48.00%)
 10   Negative samples (Incomplete): 2,961 (52.00%)
 11 
 12 -- VALIDATION --
 13   Total samples: 712
 14   Positive samples (Complete): 352 (49.44%)
 15   Negative samples (Incomplete): 360 (50.56%)
 16 
 17 -- TEST --
 18   Total samples: 712
 19   Positive samples (Complete): 339 (47.61%)
 20   Negative samples (Incomplete): 373 (52.39%)
 21 
 22 -- HUMAN_EVAL --
 23   Total samples: 773
 24   Positive samples (Complete): 372 (48.12%)
 25   Negative samples (Incomplete): 401 (51.88%)"><pre><code>  7 -- TRAIN --
  8   Total samples: 5,694
  9   Positive samples (Complete): 2,733 (48.00%)
 10   Negative samples (Incomplete): 2,961 (52.00%)
 11 
 12 -- VALIDATION --
 13   Total samples: 712
 14   Positive samples (Complete): 352 (49.44%)
 15   Negative samples (Incomplete): 360 (50.56%)
 16 
 17 -- TEST --
 18   Total samples: 712
 19   Positive samples (Complete): 339 (47.61%)
 20   Negative samples (Incomplete): 373 (52.39%)
 21 
 22 -- HUMAN_EVAL --
 23   Total samples: 773
 24   Positive samples (Complete): 372 (48.12%)
 25   Negative samples (Incomplete): 401 (51.88%)
</code></pre></div>
<p dir="auto">Our goal for an initial version of this model was to overfit on a non-trivial amount of data, plus exceed a non-quantitative vibes threshold when experimenting interactively. The next step is to broaden the amount of data and move away from overfitting towards more generalization.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pipecat-ai/smart-turn/blob/main/docs/static/confusion_matrix_test_1360_b0d85b27b14cc7bd6a0d.png"><img src="https://github.com/pipecat-ai/smart-turn/raw/main/docs/static/confusion_matrix_test_1360_b0d85b27b14cc7bd6a0d.png" alt="Confusion matrix for test set"/></a></p>
<p dir="auto">[ more notes on data coming soon ]</p>


<p dir="auto">The base Wav2Vec2-BERT model is trained on a large amount of multi-lingual data. Supporting additional languages will require either collecting and cleaning or synthetically generating data for each language.</p>

<p dir="auto">The current checkpoint was trained on a dataset of approximately 8,000 samples. These samples mostly focus on filler words that are typical indications of a pause without utterance completion in English-language speech.</p>
<p dir="auto">Two data sets are used in training: around 4,000 samples collected from human speakers, and around 4,000 synthetic data samples generated using <a href="https://rime.ai/" rel="nofollow">Rime</a>.</p>
<p dir="auto">The biggest short-term data need is to collect, categorize, and clean human data samples that represent a broader range of speech patterns:</p>
<ul dir="auto">
<li>inflection and pacing that indicates a &#34;thinking&#34; pause rather than a completed speech segment</li>
<li>grammatical structures that typically occur in unfinished speech segments (but not in finished segments)</li>
<li>more individual speakers represented</li>
<li>more regions and accents represented</li>
</ul>
<p dir="auto">The synthetic data samples in the <code>datasets/rime_2</code> dataset only improve model performance by a small margin, right now. But one possible goal for this project is to work towards a completely synthetic data generation pipeline. The potential advantages of such a pipeline include the ability to support more languages more easily, a better flywheel for building more accurate versions of the model, and the ability to rapidly customize the model for specific use cases.</p>
<p dir="auto">If you have expertise in steering speech models so that they output specific patterns (or if you want to experiment and learn), please consider contributing synthetic data.</p>

<p dir="auto">The current model architecture is relatively simple, because the base Wav2Vec2-BERT model is already quite powerful.</p>
<p dir="auto">However, it would be interesting to experiment with other approaches to classification added on top of the Wav2Vec2-BERT model. This might be particularly useful if we want to move away from binary classification towards an approach that is more customized for this turn detection task.</p>
<p dir="auto">For example, it would be great to provide the model with additional context to condition the inference. A use case for this would be for the model to &#34;know&#34; that the user is currently reciting a credit card number, or a phone number, or an email address.</p>
<p dir="auto">Adding additional context to the model is an open-ended research challenge. Some simpler todo list items include:</p>
<ul dir="auto">
<li>Experimenting with freezing different numbers of layers during training.</li>
<li>Hyperparameter tuning.</li>
<li>Trying different sizes for the classification head or moderately different classification head designs and loss functions.</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Supporting training on more platforms</h3><a id="user-content-supporting-training-on-more-platforms" aria-label="Permalink: Supporting training on more platforms" href="#supporting-training-on-more-platforms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We trained early versions of this model on Google Colab. We should support Colab as a training platform, again! It would be great to have quickstarts for training on a wide variety of platforms.</p>
<p dir="auto">We should alsoport the training code to Apple&#39;s MLX platform. A lot of us have MacBooks!</p>

<p dir="auto">This model will likely perform well in quantized versions. Quantized versions should run significantly faster than the current float32 weights.</p>
<p dir="auto">The PyTorch inference code is not particularly optimized. We should be able to hand-write inference code that runs substantially faster on both GPU and CPU, for this model architecture.</p>
<p dir="auto">It would be nice to port inference code to Apple&#39;s MLX platform. This would be particular useful for local development and debugging, as well as potentially open up the possibility of running this model locally on iOS devices (in combination with quantization).</p>

<ul dir="auto">
<li><a href="https://github.com/marcus-daily">Marcus</a></li>
<li><a href="https://github.com/ebb351">Eli</a></li>
<li><a href="https://github.com/markbackman">Mark</a></li>
<li><a href="https://github.com/kwindla">Kwindla</a></li>
</ul>
</article></div></div>
  </body>
</html>
