<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/">Original</a>
    <h1>ChatGPT for Robotics</h1>
    
    <div id="readability-page-1" class="page"><div>
									
<p>We extended the capabilities of ChatGPT to robotics, and controlled multiple platforms such as robot arms, drones, and home assistant robots intuitively with language.</p>



<figure><img decoding="async" loading="lazy" width="600" height="338" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/chatgpt_robotics_gif.gif" alt="main gif of multiple robots"/></figure>







<p>Have you ever wanted to tell a robot what to do using your own words, like you would to a human? Wouldnâ€™t it be amazing to just tell your home assistant robot: â€œ<em>Please warm up my lunch</em>â€œ, and have it find the microwave by itself? Even though language is the most intuitive way for us to express our intentions, we still rely heavily on hand-written code to control robots. Our team has been exploring how we can change this reality and make natural human-robot interactions possible using <a href="https://openai.com/" target="_blank" rel="noreferrer noopener">OpenAI</a>â€˜s new AI language model, <a href="https://openai.com/blog/chatgpt/" target="_blank" rel="noreferrer noopener">ChatGPT</a>.</p>



<p>ChatGPT is a language model trained on a massive corpus of text and human interactions, allowing it to generate coherent and grammatically correct responses to a wide range of prompts and questions. Our goal with this research is to see if ChatGPT can think beyond text, and reason about the physical world to help with robotics tasks. We want to help people interact with robots more easily, without needing to learn complex programming languages or details about robotic systems. The key challenge here is teaching ChatGPT how to solve problems considering the laws of physics, the context of the operating environment, and how the robotâ€™s physical actions can change the state of the world. </p>



<p>It turns out that ChatGPT can do a lot by itself, but it still needs some help. Our <a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf" target="_blank" rel="noreferrer noopener">technical paper</a> describes a series of design principles that can be used to guide language models towards solving robotics tasks. These include, and are not limited to, special prompting structures, high-level APIs, and human feedback via text. We believe that our work is just the start of a shift in how we develop robotics systems, and we hope to inspire other researchers to jump into this exciting field. Continue reading for more technical details on our methods and ideas.</p>



<figure><p>
<iframe loading="lazy" title="ChatGPT for Robotics" width="500" height="281" src="https://www.youtube-nocookie.com/embed/NYd0QcZcS6Q?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<h3 id="heading-challenges-in-robotics-today-and-how-chatgpt-can-help">Challenges in robotics today, and how ChatGPT can help</h3>



<p>Current robotics pipelines begin with an engineer or technical user that needs to translate the taskâ€™s requirements into code for the system. The engineer sits <em>in the loop</em>, meaning that they need to write new code and specifications to correct the robotâ€™s behavior. Overall, this process is slow (user needs to write low-level code), expensive (requires highly skilled users with deep knowledge of robotics), and inefficient (requires multiple interactions to get things working properly).</p>



<figure><img decoding="async" loading="lazy" width="2362" height="714" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main.jpg" alt="robotics today versus with chatgpt" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main.jpg 2362w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main-300x91.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main-1024x310.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main-768x232.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main-1536x464.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main-2048x619.jpg 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/main-240x73.jpg 240w" sizes="(max-width: 2362px) 100vw, 2362px"/></figure>



<p>ChatGPT unlocks a new robotics paradigm, and allows a (potentially non-technical) user to sit <em>on the loop</em>, providing high-level feedback to the large language model (LLM) while monitoring the robotâ€™s performance. By following our set of design principles, ChatGPT can generate code for robotics scenarios. Without any fine-tuning we leverage the LLMâ€™s knowledge to control different robots form factors for a variety of tasks. In our work we show multiple examples of ChatGPT solving robotics puzzles, along with complex robot deployments in the manipulation, aerial, and navigation domains.</p>



<h3 id="heading-robotics-with-chatgpt-design-principles">Robotics with ChatGPT: design principles</h3>



<p>Prompting LLMs is a highly empirical science. Through trial and error, we built a methodology and a set of design principles for writing prompts for robotics tasks:</p>



<figure><img decoding="async" loading="lazy" width="2021" height="450" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/steps.png" alt="new pipeline with chatgpt" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/steps.png 2021w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/steps-300x67.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/steps-1024x228.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/steps-768x171.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/steps-1536x342.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/steps-240x53.png 240w" sizes="(max-width: 2021px) 100vw, 2021px"/></figure>



<ol>
<li>First, we define a set of high-level robot APIs or function library. This library can be specific to a particular robot, and should map to existing low-level implementations from the robotâ€™s control stack or a perception library. Itâ€™s very important to use descriptive names for the high-level APIs so ChatGPT can reason about their behaviors;</li>



<li>Next, we write a text prompt for ChatGPT which describes the task goal while also explicitly stating which functions from the high-level library are available. The prompt can also contain information about task constraints,</li>



<li>The user stays on the loop to evaluate ChatGPTâ€™s code output, either through direct inspection or using a  simulator. If needed, the user uses natural language to provide feedback to ChatGPT on the answerâ€™s quality and safety.</li>



<li>When the user is happy with the solution, the final code can be deployed onto the robot.</li>
</ol>



<h3 id="heading-enough-theory-what-exactly-can-chatgpt-do">Enough theoryâ€¦ What exactly can ChatGPT do?</h3>



<p>Letâ€™s take a look at a few examplesâ€¦ You can find even more case studies <a href="https://github.com/microsoft/PromptCraft-Robotics" target="_blank" rel="noreferrer noopener">in our code repository</a>. </p>



<h4 id="heading-zero-shot-task-planning">Zero-shot task planning</h4>



<p>We gave ChatGPT access to functions that control a real drone, and it proved to be an extremely intuitive language-based interface between the non-technical user and the robot. ChatGPT asked clarification questions when the userâ€™s instructions were ambiguous, and wrote complex code structures for the drone such as a zig-zag pattern to visually inspect shelves. It even figured out how to take a selfie! ðŸ“· ðŸ˜Ž</p>



<figure><p>
<iframe loading="lazy" title="ChatGPT + Real Drone" width="500" height="281" src="https://www.youtube-nocookie.com/embed/i5wZJFb4dyA?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>We also used ChatGPT in a simulated industrial inspection scenario with the <a href="https://github.com/microsoft/AirSim" target="_blank" rel="noreferrer noopener">Microsoft AirSim simulator</a>. The model was able to effectively parse the userâ€™s high-level intent and geometrical cues to control the drone accurately.</p>



<figure><p>
<iframe loading="lazy" title="ChatGPT + AirSim" width="500" height="281" src="https://www.youtube-nocookie.com/embed/38lA3U2J43w?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<h4 id="heading-user-on-the-loop-when-a-conversation-is-needed-for-a-complex-tasks">User on the loop: when a conversation is needed for a complex tasks</h4>



<p>Next, we used ChatGPT in a manipulation scenario with a robot arm. We used conversational feedback to teach the model how to compose the originally provided APIs into more complex high-level functions: that ChatGPT coded by itself. Using a curriculum-based strategy, the model was able to chain these learned skills together logically to perform operations such as stacking blocks. </p>



<p>In addition, the model displayed a fascinating example of bridging the textual and physical domains when tasked with building the Microsoft logo out of wooden blocks. Not only was it able to recall the logo from its internal knowledge base, it was able to â€˜drawâ€™ the logo (as SVG code), and then use the skills learned above to figure out which existing robot actions can compose its physical form.</p>



<figure><img decoding="async" loading="lazy" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Screenshot-2023-02-20-105158.png" alt="Excerpt from ChatGPT conversation where it recalls the Microsoft logo from its knowledge base and draws it using SVG code." width="558" height="423" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Screenshot-2023-02-20-105158.png 1115w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Screenshot-2023-02-20-105158-300x227.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Screenshot-2023-02-20-105158-1024x776.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Screenshot-2023-02-20-105158-768x582.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Screenshot-2023-02-20-105158-80x60.png 80w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/Screenshot-2023-02-20-105158-238x180.png 238w" sizes="(max-width: 558px) 100vw, 558px"/></figure>



<figure><p>
<iframe loading="lazy" title="ChatGPT + Manipulation" width="500" height="281" src="https://www.youtube-nocookie.com/embed/wLOChUtdqoA?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>Next, we tasked ChatGPT to write an algorithm for a drone to reach a goal in space while not crashing into obstacles. We told the model that this drone has a forward facing distance sensor, and ChatGPT coded most of the key building blocks for the algorithm right away. This task required some conversation with the human, and we were impressed by ChatGPTâ€™s ability to make localized code improvements using only language feedback.</p>



<figure><p>
<iframe loading="lazy" title="ChatGPT - Aerial Obstacle Avoidance" width="500" height="281" src="https://www.youtube-nocookie.com/embed/Vn6NapLlHPE?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<h4 id="heading-perception-action-loops-robots-that-sense-the-world-before-they-act">Perception-action loops: robots that sense the world before they act</h4>



<p>The ability to sense the world (perception) before doing something (action) is fundamental to any robotics system. Therefore, we decided to test ChatGPTâ€™s understanding of this concept and asked it to explore an environment until finding a user-specified object. We gave the model access to functions such as object detection and object distance APIs, and verified that the code it generated successfully implemented a perception-action loop.  </p>



<p>In experimental character, we ran additional experiments to evaluate if ChatGPT is able to decide where the robot should go based on sensor feedback in real time (as opposed to having ChatGPT generate a code loop that makes these decisions). Interestingly, we verified that we can feed a textual description of the camera image at each step into the chat, and the model was able to figure out how to control the robot until it reaches a particular object.  </p>



<figure><p>
<iframe loading="lazy" title="ChatGPT + Embodied AI" width="500" height="281" src="https://www.youtube-nocookie.com/embed/p0fDH9zZm_c?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<h3 id="heading-promptcraft-a-collaborative-open-sourced-tool-for-llmrobotics-research">PromptCraft, a collaborative open-sourced tool for LLM+Robotics research</h3>



<p>Good prompt engineering is crucial for the success of LLMs such as ChatGPT for robotics tasks. Unfortunately, prompting is an empirical science, and there is a lack of comprehensive and accessible resources with good (and bad) examples to help researchers and enthusiasts in the field. To address this gap, we introduce <a href="https://github.com/microsoft/PromptCraft-Robotics" target="_blank" rel="noreferrer noopener">PromptCraft</a>, a collaborative open-source platform where anyone can share examples of prompting strategies for different robotics categories. We release all of the prompts and conversations used in this study. We invite the readers to contribute with more! </p>



<p>Besides prompt design, we hope to also include multiple robotics simulators and interfaces to allow users to test their ChatGPT-generated algorithms. As a start, we also release an AirSim environment with ChatGPT integration that anyone can use to get started with these ideas. We welcome contributions of new simulators and interfaces as well.</p>



<figure><img decoding="async" loading="lazy" width="1695" height="1005" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/AirSimScreenshot.jpg" alt="Screenshot of the ChatGPT - AirSim interface" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/02/AirSimScreenshot.jpg 1695w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/AirSimScreenshot-300x178.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/AirSimScreenshot-1024x607.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/AirSimScreenshot-768x455.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/AirSimScreenshot-1536x911.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/02/AirSimScreenshot-240x142.jpg 240w" sizes="(max-width: 1695px) 100vw, 1695px"/><figcaption>The ChatGPT-AirSim interface</figcaption></figure>



<figure><p>
<iframe loading="lazy" title="ChatGPT AirSim Interface" width="500" height="281" src="https://www.youtube-nocookie.com/embed/iE5tZ6_ZYE8?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<h3 id="heading-bringing-robotics-out-of-labs-and-into-the-world">Bringing robotics out of labs, and into the world</h3>



<p>We are excited to release these technologies with the aim of bringing robotics to the reach of a wider audience. We believe that language-based robotics control will be fundamental to bring robotics out of science labs, and into the hands of everyday users. </p>



<p>That said, we do emphasize that the outputs from ChatGPT are not meant to be deployed directly on robots without careful analysis. We encourage users to harness the power of simulations in order to evaluate these algorithms before potential real life deployments, and to always take the necessary safety precautions. Our work represents only a small fraction of what is possible within the intersection of large language models operating in the robotics space, and we hope to inspire much of the work to come.</p>



<p><em>This work is being undertaken by members of the </em><a href="https://www.microsoft.com/en-us/ai/autonomous-systems" target="_blank" rel="noreferrer noopener"><em>Microsoft Autonomous Systems and Robotics Research Group</em></a>. <em>The researchers included in this project are: <a href="https://www.microsoft.com/en-us/research/people/savempra/" target="_blank" rel="noreferrer noopener"><em>Sai Vemprala</em></a></em>, <a href="http://rogeriobonatti.com/" target="_blank" rel="noreferrer noopener"><em>Rogerio Bonatti</em></a>, <a href="https://scholar.google.com.br/citations?user=8cEgwaEAAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="noreferrer noopener"><em>Arthur Bucker</em></a><em>, and </em><a href="https://www.microsoft.com/en-us/research/people/akapoor/" target="_blank" rel="noreferrer noopener"><em>Ashish Kapoor</em></a><em>. </em></p>
								</div></div>
  </body>
</html>
