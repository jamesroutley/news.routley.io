<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/leandromoreira/cdn-up-and-running">Original</a>
    <h1>Writing a Mini-CDN to Learn Nginx/Prometheus/Grafana/Lua</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">The objective of this repo is to build a body of knowledge on how CDNs work by coding one from &#34;scratch&#34;. The CDN we&#39;re going to design uses: nginx, lua, docker, docker-compose, Prometheus, grafana, and wrk.</p>
<p dir="auto">We&#39;ll start creating a single backend service and expand from there to a multi-node, latency simulated, observable, and testable CDN. In each section, there are discussions regarding the challenges and trade-offs of building/managing/operating a CDN.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/4.0.1_metrics.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/4.0.1_metrics.webp" alt="grafana screenshot" title="grafana screenshot"/></a></p>
<h2 dir="auto"><a id="user-content-what-is-a-cdn" aria-hidden="true" href="#what-is-a-cdn"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is a CDN?</h2>
<p dir="auto">A Content Delivery Network is a set of computers, spatially distributed in order to provide high availability and <strong>better performance</strong> for systems that have their <strong>work cached</strong> on this network.</p>
<h2 dir="auto"><a id="user-content-why-do-you-need-a-cdn" aria-hidden="true" href="#why-do-you-need-a-cdn"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why do you need a CDN?</h2>
<p dir="auto">A CDN helps to improve:</p>
<ul dir="auto">
<li>loading times (smoother streaming, instant page to buy, quick friends feed, etc)</li>
<li>accommodate traffic spikes (black friday, popular streaming release, breaking news, etc)</li>
<li>decrease costs (traffic offloading)</li>
<li>scalability for millions</li>
</ul>
<h2 dir="auto"><a id="user-content-how-does-a-cdn-work" aria-hidden="true" href="#how-does-a-cdn-work"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How does a CDN work?</h2>
<p dir="auto">CDNs are able to make services faster by placing the content (media files, pages, games, javascript, a json response, etc) closer to the users.</p>
<p dir="auto">When a user wants to consume a service, the CDN routing system will deliver the &#34;best&#34; node where the content is likely <strong>already cached and closer to the client</strong>. Don&#39;t worry about the loose use of the word best in here. I hope that throughout the reading, the understanding of what is the best node will be elucidated.</p>
<h2 dir="auto"><a id="user-content-the-cdn-stack" aria-hidden="true" href="#the-cdn-stack"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The CDN stack</h2>
<p dir="auto">The CDN we&#39;ll build relies on:</p>
<ul dir="auto">
<li><a href="https://www.linux.org/" rel="nofollow"><code>Linux/GNU/Kernel</code></a> - a kernel / operating system with outstanding networking capabilities as well as IO excellence.</li>
<li><a href="http://nginx.org/" rel="nofollow"><code>Nginx</code></a> - an excellent web server that can be used as a reverse proxy providing caching capability.</li>
<li><a href="https://luajit.org/" rel="nofollow"><code>Lua(jit)</code></a> - a simple powerful language to add features into nginx.</li>
<li><a href="https://prometheus.io/" rel="nofollow"><code>Prometheus</code></a> - A system with a dimensional data model, flexible query language, efficient time series database.</li>
<li><a href="https://github.com/grafana/grafana"><code>Grafana</code></a> - An open source analytics &amp; monitoring tool that plugs with many sources, including prometheus.</li>
<li><a href="https://www.docker.com/" rel="nofollow"><code>Containers</code></a> - technology to package, deploy, and isolate applications, we&#39;ll use docker and docker compose.</li>
</ul>

<p dir="auto">Origin is the system where the content is created - or at least it&#39;s the source to the CDN. The sample service we&#39;re going to build will be a straightforward JSON API. The backend service could be returning an image, video, javascript, HTML page, game, or anything you want to deliver to your clients.</p>
<p dir="auto">We&#39;ll use Nginx and Lua to design the backend service. It&#39;s a great excuse to introduce Nginx and Lua since we&#39;re going to use them a lot here.</p>
<blockquote>
<p dir="auto"><strong>Heads up: the backend service could be written in any language you like.</strong></p>
</blockquote>
<h2 dir="auto"><a id="user-content-nginx---quick-introduction" aria-hidden="true" href="#nginx---quick-introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Nginx - quick introduction</h2>
<p dir="auto">Nginx is a web server that will follow its <a href="http://nginx.org/en/docs/beginners_guide.html#conf_structure" rel="nofollow">configuration</a>. The config file uses <a href="http://nginx.org/en/docs/dirindex.html" rel="nofollow">directives</a> as the dominant factor. A directive is a simple construction to set properties in nginx. There are two types of directives: <strong>simple and block (context)</strong>.</p>
<p dir="auto">A <strong>simple directive</strong> is formed by its name followed by parameters ending with a semicolon.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Syntax: &lt;name&gt; &lt;parameters&gt;;
# Example
add_header X-Header AnyValue;"><pre><span># Syntax: &lt;name&gt; &lt;parameters&gt;;</span>
<span># Example</span>
<span>add_header</span> X-Header AnyValue;</pre></div>
<p dir="auto">The <strong>block directive</strong> follows the same pattern, but instead of a semicolon, it ends surrounded by curly braces. A block directive can also have directives within it. This block is also known as context.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Syntax: &lt;name&gt; &lt;parameters&gt; &lt;block&gt;
location / {
  add_header X-Header AnyValue;
}"><pre><span># Syntax: &lt;name&gt; &lt;parameters&gt; &lt;block&gt;</span>
<span>location</span> <span>/ </span>{
  <span>add_header</span> X-Header AnyValue;
}</pre></div>
<p dir="auto">Nginx uses workers (processes) to handle the requests. The <a href="https://www.aosabook.org/en/nginx.html" rel="nofollow">nginx architecture</a> plays a crucial role in its performance.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/simplified_workers_nginx_architecture.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/simplified_workers_nginx_architecture.webp" alt="simplified workers nginx architecture" title="simplified workers nginx architecture"/></a></p>
<blockquote>
<p dir="auto"><strong>Heads up: Although a single accept queue serving multiple workers is common, there are other models to <a href="https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/" rel="nofollow">load balance the incoming requests</a>.</strong></p>
</blockquote>
<h2 dir="auto"><a id="user-content-backend-service-conf" aria-hidden="true" href="#backend-service-conf"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Backend service conf</h2>
<p dir="auto">Let&#39;s walk through the backend JSON API nginx configuration. I think it&#39;ll be much easier if we see it in action.</p>
<div dir="auto" data-snippet-clipboard-copy-content="events {
  worker_connections 1024;
}
error_log stderr;

http {
  access_log /dev/stdout;

  server {
    listen 8080;

    location / {
      content_by_lua_block {
        ngx.header[&#39;Content-Type&#39;] = &#39;application/json&#39;
        ngx.say(&#39;{&#34;service&#34;: &#34;api&#34;, &#34;value&#34;: 42}&#39;)
      }
    }
  }
}"><pre><span>events</span> {
  <span>worker_connections</span> <span>1024</span>;
}
<span>error_log</span> stderr;

<span>http</span> {
  <span>access_log</span> /dev/stdout;

  <span>server</span> {
    <span>listen</span> <span>8080</span>;

    <span>location</span> <span>/ </span>{
      <span>content_by_lua_block</span> {
        ngx.header[&#39;Content-Type&#39;] = &#39;<span>application/json</span><span>&#39;</span>
<span>        ngx.say(&#39;</span>{<span>&#34;service&#34;</span>: <span>&#34;api&#34;</span>, <span>&#34;value&#34;</span>: 42}<span>&#39;)</span>
<span>      }</span>
<span>    }</span>
<span>  }</span>
<span>}</span></pre></div>
<p dir="auto">Were you able to understand what this config is doing? In any case, let&#39;s break it down by making comments on each directive.</p>
<p dir="auto">The <a href="http://nginx.org/en/docs/ngx_core_module.html#events" rel="nofollow"><code>events</code></a> provides context for <a href="http://nginx.org/en/docs/events.html" rel="nofollow">connection processing configurations</a>, and the <a href="http://nginx.org/en/docs/ngx_core_module.html#worker_connections" rel="nofollow"><code>worker_connections</code></a> defines the maximum number of simultaneous connections that can be opened by a worker process.</p>
<div dir="auto" data-snippet-clipboard-copy-content="events {
  worker_connections 1024;
}"><pre><span>events</span> {
  <span>worker_connections</span> <span>1024</span>;
}</pre></div>
<p dir="auto">The <a href="http://nginx.org/en/docs/ngx_core_module.html#error_log" rel="nofollow"><code>error_log</code></a> configures logging for error. Here we just send all the errors to the stdout (error)</p>

<p dir="auto">The <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#http" rel="nofollow"><code>http</code></a> provides a root context to set up all the http/s servers.</p>

<p dir="auto">The <a href="http://nginx.org/en/docs/http/ngx_http_log_module.html#access_log" rel="nofollow"><code>access_log</code></a> configures the path (and optionally format, etc) for the access logging.</p>

<p dir="auto">The <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#server" rel="nofollow"><code>server</code></a> sets the root configuration for a server, aka where we&#39;re going to setup specific behavior to the server. You can have multiple <code>server</code> blocks per <code>http</code> context.</p>

<p dir="auto">Within the <code>server</code> we can set the <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#listen" rel="nofollow"><code>listen</code></a> directive controlling the address and/or the port on which the <a href="http://nginx.org/en/docs/http/request_processing.html" rel="nofollow">server will accept requests</a>.</p>

<p dir="auto">In the server configuration, we can specify a route by using the <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#location" rel="nofollow"><code>location</code></a> directive. This will be used to provide specific configuration for that matching request path.</p>

<p dir="auto">Within this location (by the way, <code>/</code> will handle all the requests) we&#39;ll use Lua to create the response. There&#39;s a directive called <a href="https://github.com/openresty/lua-nginx-module#content_by_lua_block"><code>content_by_lua_block</code></a> which provides a context where the Lua code will run.</p>

<p dir="auto">Finally, we&#39;ll use Lua and the basic <a href="https://github.com/openresty/lua-nginx-module#nginx-api-for-lua">Nginx Lua API</a> to set the desired behavior.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- ngx.header sets the current response header that is to be sent.
ngx.header[&#39;Content-Type&#39;] = &#39;application/json&#39;
-- ngx.say will write the response body
ngx.say(&#39;{&#34;service&#34;: &#34;api&#34;, &#34;value&#34;: 42}&#39;)"><pre><span><span>--</span> ngx.header sets the current response header that is to be sent.</span>
ngx.<span>header</span>[<span><span>&#39;</span>Content-Type<span>&#39;</span></span>] <span>=</span> <span><span>&#39;</span>application/json<span>&#39;</span></span>
<span><span>--</span> ngx.say will write the response body</span>
ngx.<span>say</span>(<span><span>&#39;</span>{&#34;service&#34;: &#34;api&#34;, &#34;value&#34;: 42}<span>&#39;</span></span>)</pre></div>
<p dir="auto">Notice that most of the directives contain their scope. For instance, the <code>location</code> is only applicable within the <code>location</code> (recursively) and <code>server</code> context.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/nginx_directive_restriction.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/nginx_directive_restriction.webp" alt="directive restriction" title="directive restriction"/></a></p>
<blockquote>
<p dir="auto"><strong>Heads up: we won&#39;t comment on each directive we add from now on, we&#39;ll only describe the most relevant for the section.</strong></p>
</blockquote>
<h2 dir="auto"><a id="user-content-cdn-100-demo-time" aria-hidden="true" href="#cdn-100-demo-time"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CDN 1.0.0 Demo time</h2>
<p dir="auto">Let&#39;s see what we did.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git checkout 1.0.0 # going back to specific configuration
docker-compose run --rm --service-ports backend # run the containers exposing the service
http http://localhost:8080/path/to/my/content.ext # consuming the service, I used httpie but you can use curl or anything you like

# you should see the json response :)"><pre>git checkout 1.0.0 <span><span>#</span> going back to specific configuration</span>
docker-compose run --rm --service-ports backend <span><span>#</span> run the containers exposing the service</span>
http http://localhost:8080/path/to/my/content.ext <span><span>#</span> consuming the service, I used httpie but you can use curl or anything you like</span>

<span><span>#</span> you should see the json response :)</span></pre></div>
<h2 dir="auto"><a id="user-content-adding-caching-capabilities" aria-hidden="true" href="#adding-caching-capabilities"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding caching capabilities</h2>
<p dir="auto">For the backend service to be cacheable we need to set up the caching policy. We&#39;ll use the HTTP header <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control" rel="nofollow">Cache-Control</a> to setup what caching behavior we want.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- we want the content to be cached by 10 seconds OR the provided max_age (ex: /path/to/service?max_age=40 for 40 seconds)
ngx.header[&#39;Cache-Control&#39;] = &#39;public, max-age=&#39; .. (ngx.var.arg_max_age or 10)"><pre><span><span>--</span> we want the content to be cached by 10 seconds OR the provided max_age (ex: /path/to/service?max_age=40 for 40 seconds)</span>
ngx.<span>header</span>[<span><span>&#39;</span>Cache-Control<span>&#39;</span></span>] <span>=</span> <span><span>&#39;</span>public, max-age=<span>&#39;</span></span> <span>..</span> (ngx.<span>var</span>.<span>arg_max_age</span> <span>or</span> <span>10</span>)</pre></div>
<p dir="auto">And, if you want, make sure to check the returned response header <code>Cache-Control</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git checkout 1.0.1 # going back to specific configuration
docker-compose run --rm --service-ports backend
http &#34;http://localhost:8080/path/to/my/content.ext?max_age=30&#34;"><pre>git checkout 1.0.1 <span><span>#</span> going back to specific configuration</span>
docker-compose run --rm --service-ports backend
http <span><span>&#34;</span>http://localhost:8080/path/to/my/content.ext?max_age=30<span>&#34;</span></span></pre></div>
<h2 dir="auto"><a id="user-content-adding-metrics" aria-hidden="true" href="#adding-metrics"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding metrics</h2>
<p dir="auto">Checking the logging is fine for debugging. But once we&#39;re reaching more traffic, it&#39;ll be nearly impossible to understand how the service is operating. To tackle this case, we&#39;re going to use <a href="https://github.com/vozlt/nginx-module-vts">VTS</a>, an nginx module which adds metrics measurements.</p>
<div dir="auto" data-snippet-clipboard-copy-content="vhost_traffic_status_zone shared:vhost_traffic_status:12m;
vhost_traffic_status_filter_by_set_key $status status::*;
vhost_traffic_status_histogram_buckets 0.005 0.01 0.05 0.1 0.5 1 5 10; # buckets are in seconds"><pre><span>vhost_traffic_status_zone</span> shared:vhost_traffic_status:12m;
<span>vhost_traffic_status_filter_by_set_key</span> <span>$status</span> status::*;
<span>vhost_traffic_status_histogram_buckets</span> 0.005 <span>0.01</span> <span>0.05</span> <span>0.1</span> <span>0.5</span> <span>1</span> <span>5</span> <span>10</span>; <span># buckets are in seconds</span></pre></div>
<p dir="auto">The <a href="https://github.com/vozlt/nginx-module-vts#vhost_traffic_status_zone"><code>vhost_traffic_status_zone</code></a> sets a memory space required for the metrics. The  <a href="https://github.com/vozlt/nginx-module-vts#vhost_traffic_status_filter_by_set_key"><code>vhost_traffic_status_filter_by_set_key</code></a> groups metrics by a given variable (for instance, we decided to group metrics by <code>status</code>) and finally, the <a href="https://github.com/vozlt/nginx-module-vts#vhost_traffic_status_histogram_buckets"><code>vhost_traffic_status_histogram_buckets</code></a> provides a way to bucketize the metrics in seconds. We decided to create buckets varying from <code>0.005</code> to <code>10</code> seconds, because they will help us to create percentiles (<code>p99</code>, <code>p50</code>, etc).</p>
<div dir="auto" data-snippet-clipboard-copy-content="location /status {
  vhost_traffic_status_display;
  vhost_traffic_status_display_format html;
}"><pre><span>location</span> <span>/status </span>{
  vhost_traffic_status_display;
  <span>vhost_traffic_status_display_format</span> html;
}</pre></div>
<p dir="auto">We also must expose the metrics in a location. We will use the <code>/status</code> to do it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git checkout 1.1.0
docker-compose run --rm --service-ports backend
# if you go to http://localhost:8080/status/format/html you&#39;ll see information about the server 8080
# notice that VTS also provides other formats such as status/format/prometheus, which will be pretty helpful for us in near future"><pre>git checkout 1.1.0
docker-compose run --rm --service-ports backend
<span><span>#</span> if you go to http://localhost:8080/status/format/html you&#39;ll see information about the server 8080</span>
<span><span>#</span> notice that VTS also provides other formats such as status/format/prometheus, which will be pretty helpful for us in near future</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/metrics_status.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/metrics_status.webp" alt="nginx vts status page" title="nginx vts status page"/></a></p>
<p dir="auto">With metrics, we can run (load) tests and see if the configuration changes we made are resulting in a better performance or not.</p>
<blockquote>
<p dir="auto"><strong>Heads up</strong>: You can <a href="https://github.com/leandromoreira/cdn-up-and-running/commit/105f54a27d1b58b88659789ae024d70c89d4a478">group the metrics under a custom namespace</a>. This is useful when you have a single location that behaves differently depending on the context.</p>
</blockquote>
<h2 dir="auto"><a id="user-content-refactoring-the-nginx-conf" aria-hidden="true" href="#refactoring-the-nginx-conf"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Refactoring the nginx conf</h2>
<p dir="auto">As the configuration becomes bigger, it also gets harder to comprehend. Nginx offers a neat directive called <a href="http://nginx.org/en/docs/ngx_core_module.html#include" rel="nofollow"><code>include</code></a> which allows us to create partial config files and include them into the root configuration file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-    location /status {
-      vhost_traffic_status_display;
-      vhost_traffic_status_display_format html;
-    }
+    include basic_vts_location.conf;
"><pre><span><span>-</span>    location /status {</span>
<span><span>-</span>      vhost_traffic_status_display;</span>
<span><span>-</span>      vhost_traffic_status_display_format html;</span>
<span><span>-</span>    }</span>
<span><span>+</span>    include basic_vts_location.conf;</span>
</pre></div>
<p dir="auto">We can extract location, group configurations per similarities, or anything that makes sense to a file. We can do <a href="https://github.com/openresty/lua-nginx-module#lua_package_path">a similar thing for the Lua code</a> as well.</p>
<div dir="auto" data-snippet-clipboard-copy-content="       content_by_lua_block {
-        ngx.header[&#39;Content-Type&#39;] = &#39;application/json&#39;
-        ngx.header[&#39;Cache-Control&#39;] = &#39;public, max-age=&#39; .. (ngx.var.arg_max_age or 10)
-
-        ngx.say(&#39;{&#34;service&#34;: &#34;api&#34;, &#34;value&#34;: 42, &#34;request&#34;: &#34;&#39; .. ngx.var.uri .. &#39;&#34;}&#39;)
+        local backend = require &#34;backend&#34;
+        backend.generate_content()
       }"><pre>       content_by_lua_block {
<span><span>-</span>        ngx.header[&#39;Content-Type&#39;] = &#39;application/json&#39;</span>
<span><span>-</span>        ngx.header[&#39;Cache-Control&#39;] = &#39;public, max-age=&#39; .. (ngx.var.arg_max_age or 10)</span>
<span><span>-</span></span>
<span><span>-</span>        ngx.say(&#39;{&#34;service&#34;: &#34;api&#34;, &#34;value&#34;: 42, &#34;request&#34;: &#34;&#39; .. ngx.var.uri .. &#39;&#34;}&#39;)</span>
<span><span>+</span>        local backend = require &#34;backend&#34;</span>
<span><span>+</span>        backend.generate_content()</span>
       }</pre></div>
<p dir="auto">All these modifications were made to improve readability, but it also promotes reuse.</p>

<h2 dir="auto"><a id="user-content-proxying" aria-hidden="true" href="#proxying"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Proxying</h2>
<p dir="auto">What we did so far has nothing to do with the CDN. Now it&#39;s time to start building the CDN. For that, we&#39;ll create another node with nginx, just adding a few new directives to connect the <code>edge</code> (CDN) node with the <code>backend</code> node.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/edge_backend.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/edge_backend.webp" alt="backend edge architecture" title="backend edge architecture"/></a></p>
<p dir="auto">There&#39;s really nothing fancy here, it&#39;s just an <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream" rel="nofollow"><code>upstream</code></a> block with a server pointing to our <code>backend</code> endpoint. In the location, we do not provide the content, but instead we point to the upstream, using the <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass" rel="nofollow"><code>proxy_pass</code></a>, we just created.</p>
<div dir="auto" data-snippet-clipboard-copy-content="upstream backend {
  server backend:8080;
  keepalive 10;  # connection pool for reuse
}

server {
  listen 8080;

  location / {
    proxy_pass http://backend;
    add_header X-Cache-Status $upstream_cache_status;
  }
}"><pre><span>upstream</span> <span>backend </span>{
  <span>server</span> backend:8080;
  <span>keepalive</span> 10;  <span># connection pool for reuse</span>
}

<span>server</span> {
  <span>listen</span> <span>8080</span>;

  <span>location</span> <span>/ </span>{
    <span>proxy_pass</span> http://backend;
    <span>add_header</span> X-Cache-Status <span>$upstream_cache_status</span>;
  }
}</pre></div>
<p dir="auto">We also added a new header (X-Cache-Status) to indicate whether the <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#variables" rel="nofollow">cache was used or not</a>.</p>
<ul dir="auto">
<li><strong>HIT</strong>: when the content is in the CDN, the <code>X-Cache-Status</code> should return a hit.</li>
<li><strong>MISS</strong>: when the content isn&#39;t in the CDN, the <code>X-Cache-Status</code> should return a miss.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="git checkout 2.0.0
docker-compose up
# we still can fetch the content from the backend
http &#34;http://localhost:8080/path/to/my/content.ext&#34;
# but we really want to access the content through the edge (CDN)
http &#34;http://localhost:8081/path/to/my/content.ext&#34;"><pre>git checkout 2.0.0
docker-compose up
<span><span>#</span> we still can fetch the content from the backend</span>
http <span><span>&#34;</span>http://localhost:8080/path/to/my/content.ext<span>&#34;</span></span>
<span><span>#</span> but we really want to access the content through the edge (CDN)</span>
http <span><span>&#34;</span>http://localhost:8081/path/to/my/content.ext<span>&#34;</span></span></pre></div>
<h2 dir="auto"><a id="user-content-caching" aria-hidden="true" href="#caching"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Caching</h2>
<p dir="auto">When we try to fetch content, the <code>X-Cache-Status</code> header is absent. It seems that the edge node is always invariably requesting the backend. This is not the way a CDN should work, right?</p>
<div data-snippet-clipboard-copy-content="backend_1     | 172.22.0.4 - - [05/Jan/2022:17:24:48 +0000] &#34;GET /path/to/my/content.ext HTTP/1.0&#34; 200 70 &#34;-&#34; &#34;HTTPie/2.6.0&#34;
edge_1        | 172.22.0.1 - - [05/Jan/2022:17:24:48 +0000] &#34;GET /path/to/my/content.ext HTTP/1.1&#34; 200 70 &#34;-&#34; &#34;HTTPie/2.6.0&#34;"><pre lang="log"><code>backend_1     | 172.22.0.4 - - [05/Jan/2022:17:24:48 +0000] &#34;GET /path/to/my/content.ext HTTP/1.0&#34; 200 70 &#34;-&#34; &#34;HTTPie/2.6.0&#34;
edge_1        | 172.22.0.1 - - [05/Jan/2022:17:24:48 +0000] &#34;GET /path/to/my/content.ext HTTP/1.1&#34; 200 70 &#34;-&#34; &#34;HTTPie/2.6.0&#34;
</code></pre></div>
<p dir="auto">The edge is just proxying the clients to the backend. What are we missing? Is there any reason to use a &#34;simple&#34; proxy at all? Well, it does, maybe you want to provide throttling, authentication, authorization, tls termination, or a gateway for multiple services, but that&#39;s not what we want.</p>
<p dir="auto">We need to create a cache area on nginx through the directive <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_path" rel="nofollow"><code>proxy_cache_path</code></a>. It&#39;s setting up the path where the cached content will reside, the shared memory <code>key_zone</code>, and policies such as <code>inactive</code>, <code>max_size</code>, among others, to control how we want the cache to behave.</p>
<div dir="auto" data-snippet-clipboard-copy-content="proxy_cache_path /cache/ levels=2:2 keys_zone=zone_1:10m max_size=10m inactive=10m use_temp_path=off;"><pre><span>proxy_cache_path</span> /cache/ levels=2:2 keys_zone=zone_1:10m max_size=10m inactive=10m use_temp_path=off;</pre></div>
<p dir="auto">Once we&#39;ve configured a proper cache, we must also set up the <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache" rel="nofollow"><code>proxy_cache</code></a> pointing to the right zone (via <code>proxy_cache_path keys_zone=&lt;name&gt;:size</code>), and the <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass" rel="nofollow"><code>proxy_pass</code></a> linking to the upstream we&#39;ve created.</p>
<div dir="auto" data-snippet-clipboard-copy-content="location / {
    # ...
    proxy_pass http://backend;
    proxy_cache zone_1;
}"><pre><span>location</span> <span>/ </span>{
    <span># ...</span>
    <span>proxy_pass</span> http://backend;
    <span>proxy_cache</span> zone_1;
}</pre></div>
<p dir="auto">There is another important aspect of caching which is managed by the directive <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_key" rel="nofollow"><code>proxy_cache_key</code></a>.
When a client requests content from nginx, it will (highly simplified):</p>
<ul dir="auto">
<li>Receive the request (let&#39;s say: <code>GET /path/to/something.txt</code>)</li>
<li>Apply a hash md5 function over the cache key value (let&#39;s assume that the cache key is the <code>uri</code>)
<ul dir="auto">
<li>md5(&#34;/path/to/something.txt&#34;) =&gt; <code>b3c4c5e7dc10b13dc2e3f852e52afcf3</code>
<ul dir="auto">
<li>you can check that on your terminarl <code>echo -n &#34;/path/to/something.txt&#34; | md5</code></li>
</ul>
</li>
</ul>
</li>
<li>It checks whether the content (hash <code>b3c4..</code>) is cached or not</li>
<li>If it&#39;s cached, it just returns the object otherwise it fetches the content from the backend
<ul dir="auto">
<li>It also saves locally (in memory and on disk) to avoid future requests</li>
</ul>
</li>
</ul>
<p dir="auto">Let&#39;s create a variable called <code>cache_key</code> using the lua directive <a href="https://github.com/openresty/lua-nginx-module#set_by_lua"><code>set_by_lua_block</code></a>. It will, for each incoming request, fill the <code>cache_key</code> with the <code>uri</code> <strong>value</strong>. Beyond that, we also need to update the <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_key" rel="nofollow"><code>proxy_cache_key</code></a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="location / {
    set_by_lua_block $cache_key {
      return ngx.var.uri
    }
    # ...
    proxy_cache_key $cache_key;
}"><pre><span>location</span> <span>/ </span>{
    <span>set_by_lua_block</span> <span>$cache_key</span> {
      <span>return</span> ngx.var.uri
    }
    # ...
    proxy_cache_key <span>$cache_key</span>;
}</pre></div>
<blockquote>
<p dir="auto"><strong>Heads up</strong>: Using <code>uri</code> as cache key will make the following two requests <a href="http://example.com/path/to/content.ext" rel="nofollow">http://example.com/path/to/content.ext</a> and <a href="http://example.edu/path/to/content.ext" rel="nofollow">http://example.edu/path/to/content.ext</a> (if they&#39;re using the same cache proxy) as if they were a single object. If you do not provide a cache key, nginx will use a reasonable <strong>default value</strong> <code>$scheme$proxy_host$request_uri</code>.</p>
</blockquote>
<p dir="auto">Now we can see the caching properly working.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git checkout 2.1.0
docker-compose up
http &#34;http://localhost:8081/path/to/my/content.ext&#34;
# the second request must get the content from the CDN without leaving to the backend
http &#34;http://localhost:8081/path/to/my/content.ext&#34;"><pre>git checkout 2.1.0
docker-compose up
http <span><span>&#34;</span>http://localhost:8081/path/to/my/content.ext<span>&#34;</span></span>
<span><span>#</span> the second request must get the content from the CDN without leaving to the backend</span>
http <span><span>&#34;</span>http://localhost:8081/path/to/my/content.ext<span>&#34;</span></span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/cache_hit.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/cache_hit.webp" alt="cache hit header" title="cache hit header"/></a></p>
<h2 dir="auto"><a id="user-content-monitoring-tools" aria-hidden="true" href="#monitoring-tools"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Monitoring Tools</h2>
<p dir="auto">Checking the cache effectiveness by looking at the command line isn&#39;t efficient. It&#39;s better if we use a tool for that. <strong>Prometheus</strong> will be used to scrape metrics on all servers, and <strong>Grafana</strong> will show graphics based on the metrics collected by the prometheus.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/metrics_architecture.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/metrics_architecture.webp" alt="instrumentalization architecture" title="instrumentalization architecture"/></a></p>
<p dir="auto">Prometheus configuration will look like this.</p>
<div dir="auto" data-snippet-clipboard-copy-content="global:
  scrape_interval:     10s # each 10s prometheus will scrape targets
  evaluation_interval: 10s
  scrape_timeout: 2s

  external_labels:
      monitor: &#39;CDN&#39;

scrape_configs:
  - job_name: &#39;prometheus&#39;
    metrics_path: &#39;/status/format/prometheus&#39;
    static_configs:
      - targets: [&#39;edge:8080&#39;, &#39;backend:8080&#39;] # the server list to be scrapped by the scrap_path"><pre><span>global</span>:
  <span>scrape_interval</span>:     <span>10s</span> <span><span>#</span> each 10s prometheus will scrape targets</span>
  <span>evaluation_interval</span>: <span>10s</span>
  <span>scrape_timeout</span>: <span>2s</span>

  <span>external_labels</span>:
      <span>monitor</span>: <span><span>&#39;</span>CDN<span>&#39;</span></span>

<span>scrape_configs</span>:
  - <span>job_name</span>: <span><span>&#39;</span>prometheus<span>&#39;</span></span>
    <span>metrics_path</span>: <span><span>&#39;</span>/status/format/prometheus<span>&#39;</span></span>
    <span>static_configs</span>:
      - <span>targets</span>: <span>[&#39;edge:8080&#39;, &#39;backend:8080&#39;] </span><span><span>#</span> the server list to be scrapped by the scrap_path</span></pre></div>
<p dir="auto">Now, we need to add a prometheus source for Grafana.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/add_source.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/add_source.webp" alt="grafana source" title="grafana source"/></a></p>
<p dir="auto">And set the proper prometheus server.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/set_source.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/set_source.webp" alt="grafana source set" title="grafana source set"/></a></p>
<h2 dir="auto"><a id="user-content-simulated-work-latency" aria-hidden="true" href="#simulated-work-latency"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Simulated Work (latency)</h2>
<p dir="auto">The backend server is artificially creating responses. We&#39;ll add simulated latency using lua. The idea is to make it closer to real-world situations. We&#39;re going to model the latency using <a href="https://www.mathsisfun.com/data/percentiles.html" rel="nofollow">percentiles</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="percentile_config={
    {p=50, min=1, max=20,}, {p=90, min=21, max=50,}, {p=95, min=51, max=150,}, {p=99, min=151, max=500,},
}"><pre>percentile_config<span>=</span>{
    {p<span>=</span><span>50</span>, min<span>=</span><span>1</span>, max<span>=</span><span>20</span>,}, {p<span>=</span><span>90</span>, min<span>=</span><span>21</span>, max<span>=</span><span>50</span>,}, {p<span>=</span><span>95</span>, min<span>=</span><span>51</span>, max<span>=</span><span>150</span>,}, {p<span>=</span><span>99</span>, min<span>=</span><span>151</span>, max<span>=</span><span>500</span>,},
}</pre></div>
<p dir="auto">We randomly pick a number from 1 to 100, and then we apply another random using the respective <code>percentile profile</code> ranging from the min to the max. Finally, we <a href="https://github.com/openresty/lua-nginx-module#ngxsleep"><code>sleep</code></a> that duration.</p>
<div dir="auto" data-snippet-clipboard-copy-content="local current_percentage = random(1, 100) -- decide with percentile this request will be
-- let&#39;s assume we picked 94
-- therefore we&#39;ll use the percentile_config with p90
local sleep_duration = random(p90.min, p90.max)
sleep(sleep_seconds)"><pre><span>local</span> current_percentage <span>=</span> <span>random</span>(<span>1</span>, <span>100</span>) <span><span>--</span> decide with percentile this request will be</span>
<span><span>--</span> let&#39;s assume we picked 94</span>
<span><span>--</span> therefore we&#39;ll use the percentile_config with p90</span>
<span>local</span> sleep_duration <span>=</span> <span>random</span>(p90.<span>min</span>, p90.<span>max</span>)
<span>sleep</span>(sleep_seconds)</pre></div>
<p dir="auto">This model lets us freely try to emulate closer to <a href="https://research.google/pubs/pub40801/" rel="nofollow">real-world observed latencies</a>.</p>
<h2 dir="auto"><a id="user-content-load-testing" aria-hidden="true" href="#load-testing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Load Testing</h2>
<p dir="auto">We&#39;ll run some load testing to learn more about the solution we&#39;re building. Wrk is an HTTP benchmarking tool that you can dynamically configure using lua. We pick a random number from 1 to 100 and request that item.</p>
<div dir="auto" data-snippet-clipboard-copy-content="request = function()
  local item = &#34;item_&#34; .. random(1, 100)

  return wrk.format(nil, &#34;/&#34; .. item .. &#34;.ext&#34;)
end"><pre>request <span>=</span> <span>function</span>()
  <span>local</span> item <span>=</span> <span><span>&#34;</span>item_<span>&#34;</span></span> <span>..</span> <span>random</span>(<span>1</span>, <span>100</span>)

  <span>return</span> wrk.<span>format</span>(<span>nil</span>, <span><span>&#34;</span>/<span>&#34;</span></span> <span>..</span> item <span>..</span> <span><span>&#34;</span>.ext<span>&#34;</span></span>)
<span>end</span></pre></div>
<p dir="auto">The command line will run the tests for 10 minutes (600s), using two threads, and 10 connections.</p>
<div dir="auto" data-snippet-clipboard-copy-content="wrk -c10 -t2 -d600s -s ./src/load_tests.lua --latency http://localhost:8081"><pre>wrk -c10 -t2 -d600s -s ./src/load_tests.lua --latency http://localhost:8081</pre></div>
<p dir="auto">Of course, you can run this on your machine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker-compose up

# run the tests
./load_test.sh

# go check on grafana, how the system is behaving
http://localhost:9091"><pre>docker-compose up

<span><span>#</span> run the tests</span>
./load_test.sh

<span><span>#</span> go check on grafana, how the system is behaving</span>
http://localhost:9091</pre></div>
<p dir="auto">The <code>wrk</code> output was as shown bellow. There were <strong>37k</strong> requests with <strong>674</strong> failing requests in total.</p>
<div dir="auto" data-snippet-clipboard-copy-content="Running 10m test @ http://localhost:8081
  2 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   218.31ms  236.55ms   1.99s    84.32%
    Req/Sec    35.14     29.02   202.00     79.15%
  Latency Distribution
     50%  162.73ms
     75%  350.33ms
     90%  519.56ms
     99%    1.02s
  37689 requests in 10.00m, 15.50MB read
  Non-2xx or 3xx responses: 674
Requests/sec:     62.80
Transfer/sec:     26.44KB"><pre>Running 10m <span>test</span> @ http://localhost:8081
  2 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   218.31ms  236.55ms   1.99s    84.32%
    Req/Sec    35.14     29.02   202.00     79.15%
  Latency Distribution
     50%  162.73ms
     75%  350.33ms
     90%  519.56ms
     99%    1.02s
  37689 requests <span>in</span> 10.00m, 15.50MB <span>read</span>
  Non-2xx or 3xx responses: 674
Requests/sec:     62.80
Transfer/sec:     26.44KB</pre></div>
<p dir="auto">Grafana showed that in a given instant, <strong>68</strong> requests were responded by the <code>edge</code>. From these requests, <strong>16</strong> went through the <code>backend</code>. The <a href="https://www.cloudflare.com/learning/cdn/what-is-a-cache-hit-ratio/" rel="nofollow">cache efficiency</a> was <strong>76%</strong>, 1% of the request&#39;s latency was longer than <strong>3.6s</strong>, 5% observed more than <strong>786ms</strong>, and the median was around <strong>73ms</strong>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/2.2.0_metrics.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/2.2.0_metrics.webp" alt="grafana result for 2.2.0" title="grafana result for 2.2.0"/></a></p>
<h2 dir="auto"><a id="user-content-learning-by-testing---lets-change-the-cache-ttl-max-age" aria-hidden="true" href="#learning-by-testing---lets-change-the-cache-ttl-max-age"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Learning by testing - let&#39;s change the cache ttl (max age)</h2>
<p dir="auto">This project should engage you to experiment, change parameters values, run load testing, and check the results. I think this loop can be a great to learn. Let&#39;s try to see what happens when we change the cache behavior.</p>
<h3 dir="auto"><a id="user-content-1s" aria-hidden="true" href="#1s"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1s</h3>
<p dir="auto">Using 1s for cache validity.</p>
<div dir="auto" data-snippet-clipboard-copy-content="request = function()
  local item = &#34;item_&#34; .. random(1, 100)

  return wrk.format(nil, &#34;/&#34; .. item .. &#34;.ext?max_age=1&#34;)
end"><pre>request <span>=</span> <span>function</span>()
  <span>local</span> item <span>=</span> <span><span>&#34;</span>item_<span>&#34;</span></span> <span>..</span> <span>random</span>(<span>1</span>, <span>100</span>)

  <span>return</span> wrk.<span>format</span>(<span>nil</span>, <span><span>&#34;</span>/<span>&#34;</span></span> <span>..</span> item <span>..</span> <span><span>&#34;</span>.ext?max_age=1<span>&#34;</span></span>)
<span>end</span></pre></div>
<p dir="auto">Run the tests, and the result is: only 16k requests with 773 errors.</p>
<div data-snippet-clipboard-copy-content="Running 10m test @ http://localhost:8081
  2 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   378.72ms  254.21ms   1.46s    68.40%
    Req/Sec    15.11      9.98    90.00     74.18%
  Latency Distribution
     50%  396.15ms
     75%  507.22ms
     90%  664.18ms
     99%    1.05s
  16643 requests in 10.00m, 6.83MB read
  Non-2xx or 3xx responses: 773
Requests/sec:     27.74
Transfer/sec:     11.66KB"><pre><code>Running 10m test @ http://localhost:8081
  2 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   378.72ms  254.21ms   1.46s    68.40%
    Req/Sec    15.11      9.98    90.00     74.18%
  Latency Distribution
     50%  396.15ms
     75%  507.22ms
     90%  664.18ms
     99%    1.05s
  16643 requests in 10.00m, 6.83MB read
  Non-2xx or 3xx responses: 773
Requests/sec:     27.74
Transfer/sec:     11.66KB
</code></pre></div>
<p dir="auto">We also noticed that the cache hit went down significantly <code>(23%)</code>, and many more requests leaked to the backend.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/2.2.1_metrics_1s.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/2.2.1_metrics_1s.webp" alt="grafana result for 2.2.1 1 second" title="grafana result for 2.2.1 1 second"/></a></p>
<h3 dir="auto"><a id="user-content-60s" aria-hidden="true" href="#60s"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>60s</h3>
<p dir="auto">What if instead we increase the caching expire to a complete minute?!</p>
<div dir="auto" data-snippet-clipboard-copy-content="request = function()
  local item = &#34;item_&#34; .. random(1, 100)

  return wrk.format(nil, &#34;/&#34; .. item .. &#34;.ext?max_age=60&#34;)
end"><pre>request <span>=</span> <span>function</span>()
  <span>local</span> item <span>=</span> <span><span>&#34;</span>item_<span>&#34;</span></span> <span>..</span> <span>random</span>(<span>1</span>, <span>100</span>)

  <span>return</span> wrk.<span>format</span>(<span>nil</span>, <span><span>&#34;</span>/<span>&#34;</span></span> <span>..</span> item <span>..</span> <span><span>&#34;</span>.ext?max_age=60<span>&#34;</span></span>)
<span>end</span></pre></div>
<p dir="auto">Run the tests, and the result now is: 45k requests with 551 errors.</p>
<div dir="auto" data-snippet-clipboard-copy-content="Running 10m test @ http://localhost:8081
  2 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   196.27ms  223.43ms   1.79s    84.74%
    Req/Sec    42.31     34.80   242.00     78.01%
  Latency Distribution
     50%   79.67ms
     75%  321.06ms
     90%  494.41ms
     99%    1.01s
  45695 requests in 10.00m, 18.79MB read
  Non-2xx or 3xx responses: 551
Requests/sec:     76.15
Transfer/sec:     32.06KB"><pre>Running 10m <span>test</span> @ http://localhost:8081
  2 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   196.27ms  223.43ms   1.79s    84.74%
    Req/Sec    42.31     34.80   242.00     78.01%
  Latency Distribution
     50%   79.67ms
     75%  321.06ms
     90%  494.41ms
     99%    1.01s
  45695 requests <span>in</span> 10.00m, 18.79MB <span>read</span>
  Non-2xx or 3xx responses: 551
Requests/sec:     76.15
Transfer/sec:     32.06KB</pre></div>
<p dir="auto">We see a much better <strong>cache efficiency (80% vs 23%)</strong> and <strong>throughput (45k vs 16k requests)</strong>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/2.2.1_metrics_60s.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/2.2.1_metrics_60s.webp" alt="grafana result for 2.2.1 60 seconds" title="grafana result for 2.2.1 60 seconds"/></a></p>
<blockquote>
<p dir="auto"><strong>Heads up</strong>: caching for longer helps improve performance but at the cost of stale content.</p>
</blockquote>
<h2 dir="auto"><a id="user-content-fine-tunning---cache-lock-stale-timeout-network" aria-hidden="true" href="#fine-tunning---cache-lock-stale-timeout-network"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fine tunning - cache lock, stale, timeout, network</h2>
<p dir="auto">Using default configurations for Nginx, linux, and others will be sufficient for many small workloads. But when you&#39;re goal is more ambitious, you will inevitably need to fine-tune the CDN for your need.</p>
<p dir="auto">The process of fine-tuning a web server is gigantic. It goes from managing how <a href="https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/" rel="nofollow"><code>nginx/Linux process sockets</code></a>, to <a href="https://github.com/leandromoreira/linux-network-performance-parameters"><code>linux network queuing</code></a>, how <a href="https://serverfault.com/questions/796665/what-are-the-performance-implications-for-millions-of-files-in-a-modern-file-sys" rel="nofollow"><code>io</code></a> affects performance, among other aspects. There is a lot of symbiosis between the <a href="https://nginx.org/en/docs/http/ngx_http_core_module.html#sendfile" rel="nofollow">application and OS</a> with direct implications to the performance, for instance <a href="https://docs.kernel.org/networking/tls-offload.html" rel="nofollow">saving user land switch context with ktls</a>.</p>
<p dir="auto">You&#39;ll be reading a lot of man pages, mostly tweaking timeouts and buffers. The test loop can help you build confidence in your ideas, let&#39;s see.</p>
<ul dir="auto">
<li>You have a hypothesis or have observed something weird and want to test a parameter value
<ul dir="auto">
<li>stick to a single set of related parameters each time</li>
</ul>
</li>
<li>Set the new value</li>
<li>Run the tests</li>
<li>Check results against the same server with the old parameter</li>
</ul>
<blockquote>
<p dir="auto"><strong>Heads up</strong>: doing tests locally is fine for learning, but most of the time you&#39;ll only trust your production results. Be prepared to do a partial deployment, compare old system/config to newer test parameters.</p>
</blockquote>
<p dir="auto">Did you notice that the errors were all related to timeout? It seems that the <code>backend</code> is taking longer to respond than what the <code>edge</code> is willing to wait.</p>
<div data-snippet-clipboard-copy-content="edge_1        | 2021/12/29 11:52:45 [error] 8#8: *3 upstream timed out (110: Operation timed out) while reading response header from upstream, client: 172.25.0.1, server: , request: &#34;GET /item_34.ext HTTP/1.1&#34;, upstream: &#34;http://172.25.0.3:8080/item_34.ext&#34;, host: &#34;localhost:8081&#34;"><pre lang="log"><code>edge_1        | 2021/12/29 11:52:45 [error] 8#8: *3 upstream timed out (110: Operation timed out) while reading response header from upstream, client: 172.25.0.1, server: , request: &#34;GET /item_34.ext HTTP/1.1&#34;, upstream: &#34;http://172.25.0.3:8080/item_34.ext&#34;, host: &#34;localhost:8081&#34;
</code></pre></div>
<p dir="auto">To solve this problem we can try to increase the proxy timeouts. We&#39;re also using a neat directive <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_use_stale" rel="nofollow"><code>proxy_cache_use_stale</code></a> that serves <code>stale content</code> when nginx is dealing with <code>errors, timeout, or even updating the cache</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="proxy_cache_lock_timeout 2s;
proxy_read_timeout 2s;
proxy_send_timeout 2s;
proxy_cache_use_stale error timeout updating;"><pre><span>proxy_cache_lock_timeout</span> 2s;
<span>proxy_read_timeout</span> <span>2s</span>;
<span>proxy_send_timeout</span> <span>2s</span>;
<span>proxy_cache_use_stale</span><span> error</span> timeout updating;</pre></div>
<p dir="auto">While we were reading about proxy caching, something catch our attention. There&#39;s a directive called <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_lock" rel="nofollow"><code>proxy_cache_lock</code></a> that collapses multiple user requests for the same content into a single request going <code>upstream</code> to fetch the content at a time. This is very often known as <a href="https://cloud.google.com/cdn/docs/caching#request-coalescing" rel="nofollow">coalescing</a>.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/cache_lock.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/cache_lock.webp" alt="caching lock" title="caching lock"/></a></p>
<p dir="auto">Running the tests we observed that we decrease the timeout errors but we also got less throughput. Why? Maybe it&#39;s because of lock contention. The big benefit of this feature it&#39;s to avoid the <a href="https://alexpareto.com/2020/06/15/thundering-herds.html" rel="nofollow">thundering herd</a> in the backend. Traffic went down from <strong>6k to 3k</strong> and requests from <strong>16 to 8</strong>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/3.0.0_metrics.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/3.0.0_metrics.webp" alt="grafana result for test 3.0.0" title="grafana result for test 3.0.0"/></a></p>
<h2 dir="auto"><a id="user-content-from-normal-to-long-tail-distribution" aria-hidden="true" href="#from-normal-to-long-tail-distribution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>From normal to long tail distribution</h2>
<p dir="auto">We&#39;ve been running load testing assuming a <a href="https://en.wikipedia.org/wiki/Normal_distribution" rel="nofollow">normal distribution</a> but that&#39;s far from reality. What we might see in production is <a href="https://en.wikipedia.org/wiki/Long_tail" rel="nofollow">most of the requests will be towards a few items</a>. To closer simulate that, we&#39;ll tweak our code to randomly pick a number from 1 to 100 and then decide if it&#39;s a popular item or not.</p>
<div dir="auto" data-snippet-clipboard-copy-content="local popular_percentage = 96 -- 96% of users are requesting top 5 content
local popular_items_quantity = 5 -- top content quantity
local max_total_items = 200 -- total items clientes are requesting

request = function()
  local is_popular = random(1, 100) &lt;= popular_percentage
  local item = &#34;&#34;

  if is_popular then -- if it&#39;s popular let&#39;s pick one of the top content
    item = &#34;item-&#34; .. random(1, popular_items_quantity)
  else -- otherwise let&#39;s pick any resting items
    item = &#34;item-&#34; .. random(popular_items_quantity + 1, popular_items_quantity + max_total_items)
  end

  return wrk.format(nil, &#34;/path/&#34; .. item .. &#34;.ext&#34;)
end"><pre><span>local</span> popular_percentage <span>=</span> <span>96</span> <span><span>--</span> 96% of users are requesting top 5 content</span>
<span>local</span> popular_items_quantity <span>=</span> <span>5</span> <span><span>--</span> top content quantity</span>
<span>local</span> max_total_items <span>=</span> <span>200</span> <span><span>--</span> total items clientes are requesting</span>

request <span>=</span> <span>function</span>()
  <span>local</span> is_popular <span>=</span> <span>random</span>(<span>1</span>, <span>100</span>) <span>&lt;=</span> popular_percentage
  <span>local</span> item <span>=</span> <span><span>&#34;</span><span>&#34;</span></span>

  <span>if</span> is_popular <span>then</span> <span><span>--</span> if it&#39;s popular let&#39;s pick one of the top content</span>
    item <span>=</span> <span><span>&#34;</span>item-<span>&#34;</span></span> <span>..</span> <span>random</span>(<span>1</span>, popular_items_quantity)
  <span>else</span> <span><span>--</span> otherwise let&#39;s pick any resting items</span>
    item <span>=</span> <span><span>&#34;</span>item-<span>&#34;</span></span> <span>..</span> <span>random</span>(popular_items_quantity <span>+</span> <span>1</span>, popular_items_quantity <span>+</span> max_total_items)
  <span>end</span>

  <span>return</span> wrk.<span>format</span>(<span>nil</span>, <span><span>&#34;</span>/path/<span>&#34;</span></span> <span>..</span> item <span>..</span> <span><span>&#34;</span>.ext<span>&#34;</span></span>)
<span>end</span></pre></div>
<blockquote>
<p dir="auto"><strong>Heads-up</strong>: we could model the long tail using <a href="https://firstmonday.org/ojs/index.php/fm/article/view/1832/1716" rel="nofollow">a formula</a>, but for the purpose of this repo, this extrapolation might be good enough.</p>
</blockquote>
<p dir="auto">Now, let&#39;s test again with <code>proxy_cache_lock</code> <code>off</code> and <code>on</code>.</p>
<h3 dir="auto"><a id="user-content-long-tail-proxy_cache_lock-off" aria-hidden="true" href="#long-tail-proxy_cache_lock-off"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Long tail <code>proxy_cache_lock</code> off</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/3.1.0_metrics.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/3.1.0_metrics.webp" alt="grafana result for test 3.1.0" title="grafana result for test 3.1.0"/></a></p>
<h3 dir="auto"><a id="user-content-long-tail-proxy_cache_lock-on" aria-hidden="true" href="#long-tail-proxy_cache_lock-on"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Long tail <code>proxy_cache_lock</code> on</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/3.1.1_metrics.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/3.1.1_metrics.webp" alt="grafana result for test 3.1.1" title="grafana result for test 3.1.1"/></a></p>
<p dir="auto">It&#39;s pretty close, even though the <code>lock off</code> is still better marginally. This feature might go to production to show if it&#39;s worthy or not.</p>
<blockquote>
<p dir="auto"><strong>Heads up</strong>: the <code>proxy_cache_lock_timeout</code> is dangerous but necessary, if the configured time has passed, all the requests will go to the backend.</p>
</blockquote>
<h2 dir="auto"><a id="user-content-routing-challenges" aria-hidden="true" href="#routing-challenges"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Routing challenges</h2>
<p dir="auto">We&#39;ve been testing a single edge but in reality, there will be hundreds of nodes. Having more edge nodes is necessary for scalability, resilience and also to provide closer to user responses. Introducing multiple nodes also introduces another challenge, clients need somehow to figure out which node to fetch the content.</p>
<p dir="auto">There are many ways to overcome this complication, and we&#39;ll try to explore some of them.</p>
<h3 dir="auto"><a id="user-content-load-balancing" aria-hidden="true" href="#load-balancing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Load balancing</h3>
<p dir="auto">A load balancer will spread the client&#39;s requests among all the edges.</p>
<h4 dir="auto"><a id="user-content-round-robin" aria-hidden="true" href="#round-robin"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Round-robin</h4>
<p dir="auto">Round-robin is a balancing policy that takes an ordered list of edges and goes serving requests picking a server each time and wrapping around when the server list ends.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# on nginx, if we do not specify anything the default policy is weighted round-robin
# http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream
upstream backend {
  server edge:8080;
  server edge1:8080;
  server edge2:8080;
}

server {
  listen 8080;

  location / {
    proxy_pass http://backend;
    add_header X-Edge LoadBaalancer;
  }
}"><pre><span># on nginx, if we do not specify anything the default policy is weighted round-robin</span>
<span># http://nginx.org/en/docs/http/ngx_http_upstream_module.html#upstream</span>
<span>upstream</span> <span>backend </span>{
  <span>server</span> edge:8080;
  <span>server</span> edge1:8080;
  <span>server</span> edge2:8080;
}

<span>server</span> {
  <span>listen</span> <span>8080</span>;

  <span>location</span> <span>/ </span>{
    <span>proxy_pass</span> http://backend;
    <span>add_header</span> X-Edge LoadBaalancer;
  }
}</pre></div>
<p dir="auto">What&#39;s good about <code>round-robin</code>? The requests are shared almost equally to all servers. There might be slower servers or responses which may enqueue lots of requests. There is the <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#least_conn" rel="nofollow"><code>least_conn</code></a> that also considers many connections.</p>
<p dir="auto">What&#39;s not good about it? It&#39;s not caching-aware, meaning multiple clients will face higher latencies because they&#39;re asking uncached servers.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# demo time
git checkout 4.0.0
docker-compose up
./load_test.sh"><pre><span><span>#</span> demo time</span>
git checkout 4.0.0
docker-compose up
./load_test.sh</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/4.0.0_metrics.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/4.0.0_metrics.webp" alt="round-robin grafana" title="round-robin grafana"/></a></p>
<blockquote>
<p dir="auto"><strong>Heads up</strong>: the load balancer itself here plays a single point of failure role. <a href="https://www.youtube.com/watch?v=bxhYNfFeVF4" rel="nofollow">Facebook has a great talk explaining</a> how they created a load balancer that is resilient, maintainable, and scalable.</p>
</blockquote>
<h4 dir="auto"><a id="user-content-consistent-hashing" aria-hidden="true" href="#consistent-hashing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Consistent Hashing</h4>
<p dir="auto">Knowing that caching awareness is important for a CDN, it&#39;s hard to use round-robin as it is. There is a balancing method known as <a href="https://en.wikipedia.org/wiki/Consistent_hashing" rel="nofollow"><code>consistent hashing</code></a> which tries to solve this problem by choosing a signal (the <code>uri</code> for instance) and mapping it to a hash table, consistently sending all the requests to the same server.</p>
<p dir="auto">There is a directive for that on nginx as well, it&#39;s called <a href="http://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash" rel="nofollow"><code>hash</code></a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="upstream backend {
  hash $request_uri consistent;
  server edge:8080;
  server edge1:8080;
  server edge2:8080;
}

server {
  listen 8080;

  location / {
    proxy_pass http://backend;
    add_header X-Edge LoadBaalancer;
  }
}"><pre><span>upstream</span> <span>backend </span>{
  <span>hash</span> <span>$request_uri</span> consistent;
  <span>server</span> edge:8080;
  <span>server</span> edge1:8080;
  <span>server</span> edge2:8080;
}

<span>server</span> {
  <span>listen</span> <span>8080</span>;

  <span>location</span> <span>/ </span>{
    <span>proxy_pass</span> http://backend;
    <span>add_header</span> X-Edge LoadBaalancer;
  }
}</pre></div>
<p dir="auto">What&#39;s good about <code>consistent hashing</code>? It enforces a policy that will increase the chances of a cache hit.</p>
<p dir="auto">What&#39;s not good about it? Imagine a single content (video, game) is peaking and now we have a problem of a small number of servers to respond to most of the clients.</p>
<blockquote>
<p dir="auto"><strong>Heads up</strong> <a href="https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed" rel="nofollow">Consistent Hashing With Bounded Load</a> born to solve this problem.</p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="# demo time
git checkout 4.0.1
docker-compose up
./load_test.sh"><pre><span><span>#</span> demo time</span>
git checkout 4.0.1
docker-compose up
./load_test.sh</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/leandromoreira/cdn-up-and-running/blob/main/img/4.0.1_metrics.webp"><img src="https://github.com/leandromoreira/cdn-up-and-running/raw/main/img/4.0.1_metrics.webp" alt="consistent hashing grafana" title="consistent hashing grafana"/></a></p>
<blockquote>
<p dir="auto"><strong>Heads up</strong> Initially I used a lua library because I thought the consistent hashing was only available for comercial nginx.</p>
</blockquote>
<h4 dir="auto"><a id="user-content-load-balancer-bottleneck" aria-hidden="true" href="#load-balancer-bottleneck"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Load balancer bottleneck</h4>
<p dir="auto">There are at least two problems (beyond it being a <a href="https://en.wikipedia.org/wiki/Single_point_of_failure" rel="nofollow">SPoF</a>) with a load balancer:</p>
<ul dir="auto">
<li>Network egress - the input/output bandwidth capacity of the load balancer must be at least sum of all its servers.
<ul dir="auto">
<li>one could use <a href="https://www.loadbalancer.org/blog/yahoos-l3-direct-server-return-an-alternative-to-lvs-tun-explored/" rel="nofollow">DSR</a> or <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/307" rel="nofollow">307</a>.</li>
</ul>
</li>
<li>Distributed edges - there might be nodes geographically sparsed that impose a hard time for a load balancer.</li>
</ul>
<h3 dir="auto"><a id="user-content-network-reachability" aria-hidden="true" href="#network-reachability"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Network reachability</h3>
<p dir="auto">Many of the problems we saw on the load balancer section are about network reachability. Here we&#39;re going to discuss some of the ways we can tackle that, and each one with their ups and downs.</p>
<h4 dir="auto"><a id="user-content-api" aria-hidden="true" href="#api"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>API</h4>
<p dir="auto">We could introduce an <code>API (cdn routing)</code>, all clients will only know where to find a content (<code>a specific edge node</code>) after asking for this API. Clients might need to deal with failover.</p>
<blockquote>
<p dir="auto"><strong>Heads up</strong> solving on the software side, one could mix the best of all worlds: start balacing using <code>consistent hashing</code> and then when a given content becames popular uses <a href="https://brooker.co.za/blog/2012/01/17/two-random.html" rel="nofollow">a better natural distribution</a></p>
</blockquote>
<h4 dir="auto"><a id="user-content-dns" aria-hidden="true" href="#dns"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DNS</h4>
<p dir="auto">We could use DNS for that. It looks pretty similar to the API but we&#39;re going to rely on dns caching ttl for that. Failover on this case is even harder.</p>
<h4 dir="auto"><a id="user-content-anycast" aria-hidden="true" href="#anycast"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Anycast</h4>
<p dir="auto">We could also use a single <a href="https://en.wikipedia.org/wiki/Anycast" rel="nofollow">Domain/IP, announcing the IP</a> in all places we have nodes, leave the <a href="https://www.youtube.com/watch?v=O6tCoD5c_U0" rel="nofollow">network routing protocols</a> to find the closest node for a given user.</p>
<h2 dir="auto"><a id="user-content-miscellaneous" aria-hidden="true" href="#miscellaneous"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Miscellaneous</h2>
<p dir="auto">We didn&#39;t talk about lots of important aspects of a CDN such as:</p>
<ul dir="auto">
<li><a href="https://www.peeringdb.com/" rel="nofollow">Peering</a> - CDNs will host their nodes/content on ISPs, public peering places and private places.</li>
<li>Security - CDNs suffer a lot of attacks, DDoS, <a href="https://youst.in/posts/cache-poisoning-at-scale/" rel="nofollow">caching poisoning</a>, and others.</li>
<li><a href="https://netflixtechblog.com/netflix-and-fill-c43a32b490c0" rel="nofollow">Caching strategies</a> - in some cases, instead of pulling the content from the backend, the backend pushes the content to the edge.</li>
<li><a href="https://en.wikipedia.org/wiki/Multitenancy" rel="nofollow">Tenants</a>/Isolation - CDNs will host multiple clients on the same nodes, isolation is a must.
<ul dir="auto">
<li>metrics, caching area, configurations (caching policies, backend), and etc.</li>
</ul>
</li>
<li>Tokens - CDNs offer some form of <a href="https://en.wikipedia.org/wiki/JSON_Web_Token" rel="nofollow">token protection</a> for content from unauthorized clients.</li>
<li><a href="https://youtu.be/1TIzPL4878Q?t=782" rel="nofollow">Health check (fault detection)</a> - stating whether a node is functional or not.</li>
<li>HTTP Headers - very often (i.e. <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS" rel="nofollow">CORS</a>) a client wants to add some headers (sometimes dynamically)</li>
<li><a href="https://github.com/leev/ngx_http_geoip2_module#example-usage">Geoblocking</a> - to save money or enforce contractual restrictions, your CDN will employ some policy regarding the locality of users.</li>
<li>Purging - the ability to <a href="https://docs.nginx.com/nginx/admin-guide/content-cache/content-caching/#purging-content-from-the-cache" rel="nofollow">purge content from the cache</a>.</li>
<li><a href="https://github.com/leandromoreira/nginx-lua-redis-rate-measuring#use-case-distributed-throttling">Throttling</a> - limit the number of concurrent requests.</li>
<li><a href="https://leandromoreira.com/2020/04/19/building-an-edge-computing-platform/" rel="nofollow">Edge computing</a> - ability to run code as a filter for the content hosted.</li>
<li>and so on...</li>
</ul>
<h2 dir="auto"><a id="user-content-conclusion" aria-hidden="true" href="#conclusion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Conclusion</h2>
<p dir="auto">I hope you learned a little bit about how a CDN works. It&#39;s a complex endeavor, highly dependent on how close your nodes are to the clients and how well you can distribute the load, taking caching into consideration, to accommodate spikes and low traffics likewise.</p>
</article>
          </div></div>
  </body>
</html>
