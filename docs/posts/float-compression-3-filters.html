<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aras-p.info/blog/2023/02/01/Float-Compression-3-Filters/">Original</a>
    <h1>Float Compression 3: Filters</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
      <div>

<article>
  <header>
    
    
  </header>
  <section>
    <p><em>Introduction and index of this series <a href="https://aras-p.info/blog/2023/01/29/Float-Compression-0-Intro/">is here</a></em>.</p>
<p>In the <a href="https://aras-p.info/blog/2023/01/29/Float-Compression-1-Generic/">previous</a> <a href="https://aras-p.info/blog/2023/01/31/Float-Compression-2-Oodleflate/">parts</a> we saw
that using generic data compression libraries, we can get our 94.5MB data down to 33.8MB (zstd level 7) or 29.6MB (oodle kraken level 2)
size, if we’re not willing to spend more than one second compressing it.</p>
<p>That’s not bad, but is there something else we can do? Turns out, there is, and in fact it’s quite simple. Enter data filtering.</p>
<h4 id="prediction--filtering">Prediction / filtering</h4>
<p>We saw filtering in the past (<a href="https://aras-p.info/blog/2021/08/27/EXR-Filtering-and-ZFP/">EXR</a> and <a href="https://aras-p.info/blog/2016/09/01/SPIR-V-Compression/">SPIR-V</a>), and the idea
is simple: losslessly <em>transform</em> the data so that it is <em>more compressible</em>. Filtering alone does nothing to reduce the data size,
but (hopefully!) it decreases data <em>randomness</em>. So the process is: filter the data, then compress that. Decompression is reverse: decompress, then
un-filter it.</p>
<p>Here’s some simple filters that I’ve tried (there are many, <em>many</em> other filters possible, I did not try them all!).</p>
<h4 id="reorder-floats-array-of-structures-style">Reorder floats array-of-structures style</h4>
<p>Recall that in our data, we know that water simulation has four floats per “element” (height, velocity x, velocity y, pollution);
snow simulation similarly has four floats per element; and other data is either four or three floats per element. Instead of having
the data like that (“array of structures” style), we can try to reorder it into “structure of arrays” style. For water simulation,
that would be all heights first, then all x velocities, then all y velocities, etc.</p>
<p>So this:</p>
<p>Completely unoptimized code to do that could look like this (and our data is floats, i.e. 4 bytes, so you’d call these templates with a 4-byte
type e.g. <code>uint32_t</code>):</p>
<pre><code>// channels: how many items per data element
// dataElems: how many data elements
template&lt;typename T&gt;
static void Split(const T* src, T* dst, size_t channels, size_t dataElems)
{
	for (size_t ich = 0; ich &lt; channels; ++ich)
	{
		const T* ptr = src + ich;
		for (size_t ip = 0; ip &lt; dataElems; ++ip)
		{
			*dst = *ptr;
			ptr += channels;
			dst += 1;
		}
	}
}
template&lt;typename T&gt;
static void UnSplit(const T* src, T* dst, size_t channels, size_t dataElems)
{
	for (size_t ich = 0; ich &lt; channels; ++ich)
	{
		T* ptr = dst + ich;
		for (size_t ip = 0; ip &lt; dataElems; ++ip)
		{
			*ptr = *src;
			src += 1;
			ptr += channels;
		}
	}
}
</code></pre>
<p>Does that help? The results are interesting <em>(click for an interactive chart)</em>: </p>
<ul>
<li>It does help LZ4 to achieve a bit higher compression ratios.</li>
<li>Makes zstd compress <em>faster</em>, and helps the ratio at lower levels, but <em>hurts</em> the ratio at higher levels.</li>
<li>Hurts oodle kraken compression.</li>
<li>Hurts the decompression performance quite a bit (for lz4 and kraken, slashes it in half). In all cases the data still decompresses
under 0.1 seconds, so acceptable for my case, but the extra pass over memory is not free.</li>
</ul>
<p>Ok, so this one’s a bit “meh”, but hey, now that the data is grouped together (all heights, then all velocities, …), we could try to
exploit the fact that <em>maybe</em> neighboring elements are similar to each other?</p>
<h4 id="reorder-floats--xor">Reorder floats + XOR</h4>
<p>In the simulation data example, it’s probably expected that usually the height, or velocity, or snow coverage, does not vary “randomly” over
the terrain surface. Or in an image, you might have a color gradient that varies smoothly.</p>
<p>“But can’t data compressors already compress that really well?!”</p>
<p>Yes and no. Usually generic data compressors can’t. Most of them are very much oriented at finding <em>repeated</em> sequences of <em>bytes</em>. So if you have
a very smoothly varying surface height or image pixel color, e.g. a sequence of bytes <code>10, 11, 12, 13, 14, 15</code>, well that is not compressible
at all! There are <em>no</em> repeating byte sequences.</p>
<p>But, if you transform the sequence using some sort of “difference” between neighboring elements, then repeated byte sequences might start
appearing. At first I tried XOR’ing the neighboring elements together (interpreting each float as an <code>uint32_t</code>), since at some
point I saw that trick being mentioned in some “time series database” writeups
(e.g. <a href="https://joe.schafer.dev/gorilla-time-series-database/#time-series-value-compression">Gorilla</a>).</p>
<p>A completely unoptimized code to do that:</p>
<pre><code>template&lt;typename T&gt;
static void EncodeDeltaXor(T* data, size_t dataElems)
{
	T prev = 0;
	for (size_t i = 0; i &lt; dataElems; ++i)
	{
		T v = *data;
		*data = v ^ prev;
		prev = v;
		++data;
	}
}
template&lt;typename T&gt;
static void DecodeDeltaXor(T* data, size_t dataElems)
{
	T prev = 0;
	for (size_t i = 0; i &lt; dataElems; ++i)
	{
		T v = *data;
		v = prev ^ v;
		*data = v;
		prev = v;
		++data;
	}
}
</code></pre>
<p>And that gives (faint dashed line: raw compression, thin line: previous attempt (split floats), thick line: split floats + XOR): </p>
<ul>
<li>Compression ratio is <em>way</em> better for zstd and lz4 (for kraken, only at lower levels).</li>
<li>zstd pretty much reaches kraken compression levels! The lines almost overlap in the graph.</li>
<li>Decompression speed takes a bit of a hit, as expected. I might need to do something about it later.</li>
</ul>
<p>So far we got from 33.8MB (zstd) / 29.6MB (kraken) at beginning of the post down to 28MB (zstd, kraken), while
still compressing in under 1 second. Nice, we’re getting somewhere.</p>
<h4 id="reorder-floats--delta">Reorder floats + Delta</h4>
<p>The “xor neighboring floats” trick from Gorilla database was in the context of then extracting the non-zero sequences of bits from the result and storing that
in less space than four bytes. I’m not doing any of that, so how about this: instead of XOR, do a difference (“delta”) between the neighboring elements?</p>
<pre><code>template&lt;typename T&gt;
static void EncodeDeltaDif(T* data, size_t dataElems)
{
	T prev = 0;
	for (size_t i = 0; i &lt; dataElems; ++i)
	{
		T v = *data;
		*data = v - prev;
		prev = v;
		++data;
	}
}
template&lt;typename T&gt;
static void DecodeDeltaDif(T* data, size_t dataElems)
{
	T prev = 0;
	for (size_t i = 0; i &lt; dataElems; ++i)
	{
		T v = *data;
		v = prev + v;
		*data = v;
		prev = v;
		++data;
	}
}
</code></pre>
<p>And that gives (faint dashed line: raw compression, thin line: previous attempt (split floats + XOR), thick line: split floats + delta): </p>
<p>Now that’s quite an improvement! All three compressors tested get their compression ratio lifted up. Good! Let’s keep on going.</p>
<h4 id="reorder-bytes">Reorder bytes</h4>
<p>Hey, how about instead of splitting each data point into 4-byte-wide “streams”, we split into 1-byte-wide ones? After all, general compression
libraries are oriented at finding <em>byte</em> sequences that would be repeating. Exactly the same <code>Split</code> and <code>UnSplit</code> functions as above, just with <code>uint8_t</code>
type.</p>
<p>Faint dashed line: raw compression, thin line: previous attempt (split floats + Delta), thick line: split bytes: </p>
<ul>
<li>kraken results are almost the same as with “split floats and delta”. Curious!</li>
<li>zstd ratio (and compression speed) is improved a bit.</li>
<li>lz4 ratio is improved <em>a lot</em> (it’s beating original unfiltered kraken at this point!).</li>
</ul>
<p>I’ll declare this a small win, and let’s continue.</p>
<h4 id="reorder-bytes--delta">Reorder bytes + Delta</h4>
<p>Split by bytes as previous, and delta-encode that. Faint dashed line: raw compression, thin line: previous attempt (split bytes), thick line: split bytes + delta: </p>
<p>Holy <del>macaroni</del> <em>grated potato dumplings</em>!</p>
<ul>
<li>Another compression ratio increase. Both zstd and kraken get our data to 23MB in about one second (whereas it was 33.8MB and 29.6MB at the start of the post).</li>
<li>zstd actually slightly <em>surpasses</em> kraken at compression ratios in the area (“under 1 sec”) that I care about. 😮</li>
<li>lz4 is not too shabby either, being well ahead of unfiltered kraken.</li>
<li>Downside: decompression is slightly longer than 0.1 seconds now. Not “terrible”, but I’d want to look into whether all this reordering and delta could be sped up.</li>
</ul>
<h3 id="conclusion-and-whats-next">Conclusion and what’s next</h3>
<p>There’s <em>lots</em> of other data filtering approaches and ideas I could have tried, but for now I’m gonna call <strong>“reorder bytes and delta” a pretty good win</strong>;
it’s extremely simple to implement and gives a <em>massive</em> compression ratio improvement on my data set.</p>
<p>I did actually try a couple other filtering approaches. Split data by <em>bits</em> (using <a href="https://github.com/kiyo-masui/bitshuffle">bitshuffle</a> library)
was producing worse ratios than splitting by bytes. Rotating each float left by one bit, to make the mantissa &amp; exponent aligned on byte boundaties, was
also not an impressive result. Oh well!</p>
<p>Next up, I’ll look at an open source library that does not advertise itself as a general data compressor, but I’m gonna try it anyway :) Until then!</p>

  </section>
  
</article>

    </div>
    
    </div>
  </div></div>
  </body>
</html>
