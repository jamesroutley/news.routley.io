<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eagledot.xyz/hachi.md.html">Original</a>
    <h1>Hachi: An Image Search Engine</h1>
    
    <div id="readability-page-1" class="page"><article>
<h2>Hachi: An (Image) Search engine</h2>
<blockquote>
<p>Only the dead have seen the end of war ..   George Santayana
<br/></p>
</blockquote>
<p>For quite some time now, i have been working on and off on a fully self-hosted <a href="https://github.com/eagledot/hachi">search engine</a>, in hope to make it easier to search across Personal data in an <code>end to end</code> manner. Even as individuals, we are hoarding and generating more and more data with no end in sight. Such &#34;personal&#34; data is being stored from local hard-disks to corporate controlled cloud-centers which makes it distributed in nature. So for following discussion, &#34;Personal&#34; meaning would be flexible enough to accommodate resources on a remote server and/or on different devices, as long the user could prove authentication and/or authorization to that data.
Current implementation supports only &#34;images&#34;, but eventual goal is also to support other modalities like <code>video</code>, <code>text</code> and <code>audio</code>, some code would be shared, while some new code would be required to better extract <code>Features</code> for each modality.</p>
<p>Such distributed nature of data and potential capabilities of current self-hosted Machine learning models to extract semantic information, only to be queried through a single interface seemed enticing enough for me start this experiment in the first place. Following post at times may seem in-coherent, as i try to articulate my thoughts on the journey of development, challenges faced and future ideas. I hope to treat this as a personal essay with multiple themes, anecdotes and even random thoughts aiming to provide a higher level view of the journey and philosophy so far in more concrete terms.</p>
<h2>Motivation:</h2>
<p>As Humans we tend to remember different attributes/parts of an entity/information at different times, and most of search engines&#39; interfaces refuse to accomodate that. User generally end up with an unidirectional flow of information, with no recourse of providing feedback to improve upon the on-going query. Even most advanced interfaces fail to handle the stochastic nature of queries and humans&#39; pre-disposition towards partial information to keep moving, it should be default for search-engines to present best-effort suggestions for queries even if they couldn&#39;t be <em>fully</em> resolved. </p>
<p>I also note that, it is not always easy to model the imperfect information like handelling a mis-spelling, which itself could be mis-spelled in many ways. It would require a conscious effort to put in a better search interface, as most digital machines make it easy to model when &#34;something&#34; is &#34;correct&#34; or when something is &#34;incorrect&#34;. Conveying &#34;Why&#34; something is incorrect takes a lot more code, effort and time, hence indicating that economic realities are more to blame for such cases than bad intentions!</p>
<p>It also presents an opportunity to analyze the capabilities of a <em>good</em> interface, as personal data would make it very easy to notice its limitations, which couldn&#39;t be observed through seemingly complete interfaces exposed by many e-commerce companies. </p>
<p>Inspired by above stated ideas, My try has been to expose multiple (if not all) attributes for a resource directly to user and then letting user recursively refine query to get to desired result. Implementation is still far from complete, but this theme has served me well to set a basic roadmap for the project. Other themes such as self-hosting, hostile behaviour towards users in terms of privacy-invading features, limited or no options to refine a search by google, github etc has contributed to evolution of this experiment. Distributed queries being served by a cluster of (refurbished) smart-phones or single-board-computers remains a lofty goal of this experiment too! </p>
<p>Despite all the good intentions and ideas, any search interface should pass that threshold of being fast enough to not end up as another impractical experiment. Efforts were involved from the beginning to embrace the inevitable complexity such projects come to include despite many refactorings. 
Below is a minimal video to help visualize the current state and capabilities of the project.</p>
<video id="video-player" width="100%" controls="" preload="metadata" poster="/assets/hachi_demo.png">
        <source id="video-source" src="/assets/hachi_demo.mp4" type="video/mp4"/>
        Your browser does not support the video tag.
</video>

<h2>Broader Ideas:</h2>
<ul>
<li>
<p>Minimalism:
Minimalism in terms of number of external dependencies required for this project to be bootstraped, could explain a lot about downstream choices and evolution of the project to its current form. This has  of any existing (source) code if possible or writing it from scratch which itself would require reading of a lot of existing code before i could port it to extend the project in a pure source sense.
If it would be practical to reuse some code from existing capable projects/databases, i would have done so but most of such projects are designed to be de-coupled from application code for good reasons, as they are supposed to offer much more guarantees and stay robust even under heavy load.
Being an (embedded) part of personal application we can choose to do away with such guarantees and yet expose much more information by tightly integrating ML models pipeline. In the end, application would handle much more complex indexing and inferencing pipelines, which would require a lot more code apart from search and storage interface generally expose!</p>
</li>
<li>
<p>Experimentation:
Thinking more about in terms of augmenting the existing information, rather than to duplicate it, while fusing traditional (deterministic) attributes with semantic(ML) attributes. I think this is an interesting problem and which have not been fully utilized/explored for personal applications. Most of traditional databases were written to only handle &#34;text&#34; modality, but current ML models allow us to query semantic information too, which opens up a new space to experiment in.
I treat semantic information as necessary and independent, but not the only signal useful to implement great search interfaces.</p>
</li>
<li>
<p>Hackability:
For this project i wanted it be very easy for someone to start modifying it according to their needs, and this mostly co-relates with the first point about minimalism, lesser the number of dependencies, lesser is the amount of configuration required to bootstrap the developing environment. Both Python and Nim are stable, cross-platform languages and are easier to extend just using a C compiler. Nim source code it easy to compile and/or cross-compile to on almost all platforms. There are already python bridges for many languages, so all such languages are fair game to extend the codebase in any desired way!</p>
</li>
</ul>
<p>Even though above ideas may seem worthy to follow on, there is always an on-going fight to prevent dilution of agreed upon principals. Counter-intuitively i think there is some kind of <code>activation-enery</code> (<a href="https://en.wikipedia.org/wiki/Activation_energy">https://en.wikipedia.org/wiki/Activation_energy</a>) requirement for each project, past that it actually is much easier to extend, modify, optimize the codebase somewhat like paying a debt to live debt free:)     </p>
<p>There are already very capable projects like <code>Sqlite</code>, <code>Lucene</code> offering full-text search capabilities, but they implement their own storage backends which require all data to be transformed to the compatible format which leads to duplication of data . This is something i wanted to avoid, as we would be continuously transforming every newer data and this would become computationally expensive when such data wouldn&#39;t even reside on same physical machine/node. If we could get away with fast-enough queries through a much simpler index/database, that seems like something worthy to pursue further.</p>
<p>It leads to a design comprising a meta-data indexing engine, coupled with vector-search engines for semantic search. We never intend to duplicate the original-data and don&#39;t care where it actually resides, once indexing is done. As i think search is more about reaching to a desired file/resource before that resource could be used! Pin-pointing that resource location quickly is the major motivation by incorporating the user intentions and context recursively!</p>
<p>(C)Python is used as the major language for backend and Nim (and C) is used to speed up the bottleneck portions of the codebase where-ever warranted. 
Writing from scratch allows me to update the api as i fit to handle a bottleneck portion of the pipeline (querying or indexing), without asking or waiting for a change in some upstream dependency.
Nim itself is a language with relatively smaller community, so i am getting a bit comfortable porting code from other languages to my projects with only standard library and even experimenting with my own data-structures based on (protected) reference semantics than default value semantics that Nim use!</p>

<p>Its a minimal module/database to handle (store and query) meta-data being extracted from resources(images) and has been written in Nim. Currently it is single-threaded, column-oriented database using Json as data-exchange mechanism between python and Nim. In future idea is to shift to leveraging multiple threads for workloads/size greater than a threshold, to better use the current hardware capabilities. It is possible to generate an <code>auxilary</code> index to speed up queries for a column/attribute on demand, which internally would use cache-friendly and hierichal data-structures to achieve so for most of scenarios! </p>
<p>I have also experimented with multi-versioning storage design as <strong>Lmdb</strong>, to protect the original information created by code itself from user revisions. But current implementation instead favours creation of a dedicated field/attribute for user to modify/update.
For example  during face clustering process, backend will assign an unique Id for each new <code>cluster</code>, to which user may want to change to a more descriptive name, this leads to presence of attributes like <code>personML</code> and <code>person</code> in the final schema. By default, any attribute/information generated through during indexing pipeline is supposed to be immutable to be easily reset to genesis state.</p>
<p>There are also Simd opportunities inside the &#34;querying&#34; code, but since its design is being guided by overall needs for the product itself, i hope to add those architecture specific optimizations only after system-design becomes stable enough for most of the features supposed to be supported! </p>

<p>Being able to group same person(s) with a high probability, as another attribute to search for or mix with other attributes, would be a very quality addition to any search interface. Current DL models for some-time now have been able to distinguish faces with a high accuracy. But being able to distinguish real-life faces still requires a conformance to the pipeline such models would have been trained with.</p>
<p>It almost always overwhelming to decide on a particular Implementation to build upon, while accommodating various factors like <code>latency</code>, <code>accuracy</code> , <code>hardware requirements</code>, and most of such intensive pro-bono work would never even be visible to the end-user. For me atleast this goes much further, as i would be implementing each such model using an independent ML framework, which would require me to understand also all the pre-processing and post-processing code, to be  faithfully ported to Nim.</p>
<p>Current pipeline uses <code>retina-face</code> model to predict faces and landmarks in one go which helps producing stable facial-landmarks and speeding up the pipeline. (As predicting facial-landmarks would be much cheaper from internal features than through a dedicated model, and it also helps stabilizing  the training of the model).
Though it could make sense to argue about a model&#39;s ability to internalize learning <em>correlated</em> features without adding an explicit loss, but in practice it is always (very) beneficial to use multiple losses explicitly.
Interestingly, <code>residual</code> connection in <code>ResNets</code> was an important innovation making it possible to train much deeper networks at that time, even though it would be just mimicing an <code>identity</code> function.
<label for="residual-0">⊕</label>

<span>
  <img src="http://tinylogger.com/max/assets/residual_0.png" alt="Residual component"/>
  Residual block, see <a href="https://en.wikipedia.org/wiki/Residual_neural_network">https://en.wikipedia.org/wiki/Residual_neural_network</a>
</span>
Explicit multiple losses decrease the chances of over-fitting by large. There could be other auxiliary objectives that are used during training only by means of an smaller auxiliary network and then not used/required during inference, just like training wheels :)</p>
<p>In my experience, dataset being used for training and choice of the objective function are two very major factors influencing the performance of your model on real-life (bit out-of-distribution datasets). I find it a good practice to always visually debug some of the random samples to get a &#34;feel&#34; for the dataset!</p>
<p>Even after having a good pipeline to generate &#34;embeddings&#34; for a face, <code>clustering</code> remains a very challenging problem, due to various reasons.
Like with almost all clustering algorithms, we start out with no prior information about of the underlying (number) distribution of the data (faces). (as this is what we would be trying to estimate). As we keep encountering the newer information, possible <code>updates</code> through <code>back-filling</code> are required for the underlying index, which somewhat resembles of an auto-regressive operation and hence the error-accumulation rate is relatively high. We would also need to wait for some &#34;initial&#34; amount of data/information to be collected, to estimate initial stable centroids. This difficulty is further compounded by the choices for various thresholds like face-detection, some measure for blurness in the detected face, and a dependence on order of information being encountered.</p>
<p><label for="face-models-info">⊕</label>

<span>
As indicated, choosing same model to predict landmarks and face-bounding boxes, helps reduce the impedance mismatch that occurs when output of one model is being fed through another model. We would need to a dedicate model for facial-features though as earlier features may not be dense enough to distinguish among individual faces!
</span></p>
<p>Currently Implementation works by collecting some minimal amount of information before <code>Cluster</code> creation process could begin. 
Each Cluster is a union of a set of main/master embeddings and a set of follower/slave embeddings. Selection of main embeddings is a crucial part to maintain the stability of a cluster even when new information would be encountered. Initial filtering of unfeasible (master) embeddings is done through some static criterias, for example we strive to filter any of <code>blurred</code> faces, face-profile is estimated through facial-landmarks, stable forward-facing profiles make face-alignment easier further in the pipeline. Such (static) criterias definitely help to reduce the number of invalid candidates, but may not be enough for many real-life datasets. A further minimal module comparing the <code>hog-features</code> with a set of pre-saved hog-features is introduced to help invalidate faces with <code>sunglasses</code> and some false positives not caught by earlier criterias!</p>
<p><label for="hog-compare">⊕</label>

<span>
  <img src="http://tinylogger.com/max/assets/hog_compare.png" height="120" alt="hog comparison"/>
  Hog features are finally compared at pixel level, after applying normalization!
</span>
After experimenting with other approaches like SIFT-features, i found it easier to compare hog-features generated from <em>aligned</em> faces/eyes. Alignment part of the pipeline is crucial to generate rich embeddings, even minor deviation from reference landmarks end up producing bad-embeddings rendering the pipeline useless. All feasible candidates/embeddings are then compared sequentially to create final clusters conditioned on some threshold. Note for now this is not exhaustive and hence order in which information is being encountered would have some effect on final clusters! Remaining follower ids are also then assigned (sequentially) to one of the clusters or to a special cluster like <code>no-categorical-info</code>, when not able to being fit into any of the clusters.
Note that a lot of empirical data comes into effect as multiple decisions would be required while choosing many thresholds and may require multiple runs .</p>
<p><img alt="alt ML codebase sample" src="http://tinylogger.com/max/assets/face_rec_1.png"/></p>
<p>Since face-recognition is very subjective and i myself have to compare other features to make sure that indeed the <em>correct</em> person(s) have been grouped together by the pipeline. But with a latency of around 25 ms, it seems to do very good on a held out dataset of persons with close up faces, (Zen-Z) selfies and sunglasses occluded eyes. Personal photos are much easier to classify/recognize compared to such a dataset!  </p>
<p>For any practical ML integrated product, We would need to have a very performant concurrent pipeline to keep feeding the model while being constantly aware of any data-distribution impedance mismatch, to reach anywhere near the &#39;accuracy&#39; and <code>speed</code> promised in a research paper. This touches upon the issue of having good understanding of software engineering basics, while being aware of possible regressions resulting from a newer functionality like ML.</p>
<h2>Indexing:</h2>
<p>Indexing pipeline begins with desired data location as its input to recursively scanning <code>directories</code> to collect <code>raw-data</code> in batches.
Multiple meta attributes such as <code>exif-data</code>, <code>size</code>, <code>mount location</code>, <code>name</code> are extracted to be later queried through the Meta-indexing engine. Focus has been on designing a good schema to accomodate future use-cases, but since we would be collecting only meta-information without ever modifying the original or duplicating the original data, it remains relatively easier to shift to a newer version/schema even through automatic means.</p>
<div><pre><span></span><code><span>def</span> <span>preprocess_kernel</span><span>(</span>
    <span>image</span><span>:</span><span>Tensor</span><span>[</span><span>uint8</span><span>],</span>
    <span>new_shape</span><span>:</span><span>tuple</span><span>[</span><span>int</span><span>,</span><span>int</span><span>],</span> 
    <span>rgb_to_bgr</span><span>:</span><span>bool</span> <span>=</span> <span>True</span><span>,</span> 
    <span>normalize</span><span>:</span><span>bool</span> <span>=</span> <span>True</span><span>):</span>
    <span># Preprocess kernel, may fuse resize, color_conversion and normalization into one function!</span>

    <span># Pseudo-code!</span>

    <span>result</span> <span>=</span> <span>newEmptyTensor</span><span>[</span><span>uint8</span><span>](</span><span>new_shape</span><span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>new_height</span><span>:</span>
        <span>for</span> <span>j</span> <span>in</span> <span>new_width</span><span>:</span>
            <span>inp_h</span><span>,</span> <span>inp_w</span> <span>=</span> <span>get_corresponding_pixel</span><span>(</span><span>image</span><span>,</span> <span>i</span><span>,</span> <span>j</span><span>)</span>
            <span>for</span> <span>k</span> <span>in</span> <span>0.</span><span>.&lt;</span><span>3</span><span>:</span>
                <span>if</span> <span>rgb_to_bgr</span><span>:</span>
                    <span>result</span><span>[</span><span>i</span><span>,</span><span>j</span> <span>,</span> <span>3</span><span>-</span><span>k</span><span>-</span><span>1</span><span>]</span> <span>=</span> <span>image</span><span>[</span><span>inp_h</span><span>,</span> <span>inp_w</span><span>,</span> <span>k</span><span>]</span>
                    <span># normalize based on mean and deviation used for training dataset further...</span>
                <span>else</span><span>:</span>
                    <span>result</span><span>[</span> <span>i</span><span>,</span><span>j</span><span>,</span><span>k</span><span>]</span> <span>=</span> <span>image</span><span>[</span><span>inp_h</span><span>,</span> <span>inp_w</span><span>,</span> <span>k</span><span>]</span>
</code></pre></div>

<p>Each <code>resource</code> could be assumed to go through a flow like this:</p>
<div><pre><span></span><code><span>resource_location</span> <span>=</span> <span>&#34;file://xyz.jpg&#34;</span>
<span># OR</span>
<span>resource_location</span> <span>=</span> <span>&#34;remoteProtocol://xyz.jpg&#34;</span>

<span>raw_data</span> <span>=</span> <span>download_raw_data</span><span>(</span><span>resource_location</span><span>)</span>

<span>embeddings</span> <span>=</span> <span>ML_model</span><span>(</span> <span>preprocess</span><span>(</span><span>raw_data</span><span>))</span>
<span>exif_data</span> <span>=</span> <span>extract_exif_data</span><span>(</span><span>raw_data</span><span>)</span>
<span>preview</span> <span>=</span> <span>generate_preview</span><span>(</span><span>raw_data</span><span>)</span>
<span>write_preview</span><span>(</span><span>preview</span><span>)</span>
<span>....</span>
</code></pre></div>

<h2>Vector Index:</h2>
<p>It is another minimal module to store vector-embeddings as <strong>shards</strong> on the disk. Necessary meta-data is stashed along with that shard, to make it self-contained, which in future will help in distributed/parallel retrieval. For now each shard is just a numpy (float32) Tensor, and comparison routine is a <code>np.dot</code> operator, which itself use the <code>blas/openblas</code> library to speed up this operation! Each shard is loaded from the Disk during a <em>query</em>, and <code>top-k</code> candidates are collected to be fused together with other deterministic meta-attributes. Loading from Disk do add some latency, but it allows me to regulate RAM usage through <code>shard-size</code> hyper-parameter, to allow running this on different platforms  with diverse specifications including single-board computers. <code>Shard-size</code> could be kept relatively high for higher RAM systems to speed up shard querying.</p>
<p><code>Matmul</code> is one of the most optimized algorithms which run at almost 90% of theoretical capacity on most of intel/amd Cpus when leveraging <code>Blas</code> like libraries. So every further optimization from here-on would involve some kind of information loss. There is a whole literature now to speed up this comparison/retrieval process through <code>quantization</code> and/or <code>nearest neighbour</code> indices like HNSW. Fast SSDs are also leveraged to run such comparisons at very high speed for upto billion vectors on just a single node in near real time!</p>
<p>But such all techniques involve compression of information (which itself is best-effort being the result of modeling a large amount of biased data) through out-of-band mechanisms, for example creating centroids/clusters is just based on the vector values and taking some mean without a way to pass back the information to the model which produced those vectors in the first place. This way is quick and you would get great speed-ups, and there is an active debate among vector-database vendors across various metrics and implementations. In my experience only visual results on a <em>personal</em> data would be a good metric a user should test for. Product-quantization is something i would be implementing if were to choose one, as i think coupled with <code>top-k</code>, it should work reasonably well to include (subjectively) correct results (high recall!) .</p>
<p>Another worthy and very effective solution i think is to instead <em>train</em> a linear layer to <em>finetune</em> the original model depending upon the task. ML Features/embeddings from a big enough model, could assumed to have a <em>knowledge</em> about diverse topics, but for example, a user may be trying to distinguish between different plants. A linear layer could easily be trained with just few thousand samples, to achieve so with much higher accuracy than original models, and even with half the size/dimension of original embeddings. Intuitively it could be thought that we freed the information channels to just focus on plants, decreasing the entropy model earlier had to deal with. Any such layer could be trained even without any framework, as it would just be one <code>backward</code> operation to implement.
OpenAI has a nice cookbook if a reader would want to explore this further!
<label for="embed-playbook">⊕</label>

<span>
  <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb</a></span></p>

<p>An interesting thing sharding allows is to use any available <code>hardware</code> to speed up retrieval. Since we need just <code>comparison</code> routine and corresponding shard(s) to return top-k candidates, it de-couples it from any of application code. A new smartphone could be detected, and some <code>shards</code> could be transferred during initial set-up, optimal percentage/number of shards could be easily calculated by running same <code>comparsion</code> operation on new device. Like running a <code>2048 x 2048</code> , inner-product op and comparing latency with <code>master/main</code> device, would tell us the capacity of the new device and so that number of shards would be transferred to speed up retrieval process!</p>
<p>There are performance gains to be have in the current implementation, would like to atleast start using <code>float16</code> data-type, but its a bit tricky on intel cpus with no compiler support for this type. Printing of CPU capabilities <em>do</em> show the presence of float16 hardware support on my system ! 
ARM(v8 64) seems to offer native float16/floatH types, there seems to be difference in that type either supported natively by compiler or as an intrinsics/assembly code. I have not been able to get expected speed up for now! Such code is still being experimented upon in the limited time i have.</p>
<h2>Backend:</h2>
<p>Backend is written in python, which exposes a pure API server, to let the client/frontend to make API calls to. Starting with very naive code to just return all the meta-data for a <code>directory</code> to current pagination support it have gone through many revisions and design iterations and now i have much clearer idea about how to architect/wrap a big enough piece of functionality. I wanted the app to be end to end, but this also put extra pressure on app to be responsive enough for all user events. Current indexing code is capable of providing rich details such as directory currently being scanned, estimated time (eta) and allows robust <em>Cancellation</em> of an ongoing task/threads. 
It has not been easy to model such communication b/w concurrent tasks and touches upon much discussed <code>structured-concurrency</code> debate i.e how to run multiple tasks asynchronously, while being able to robustly cancel them at any point in time, all while being able to collect all errors cleanly!<br/></p>
<p>From C days, i have been a user of (Posix) <code>threads</code> type implementations, since major OSes provide those minimal but stable APIs, it helps me during context switching to different languages. Both C and Nim expose that, Python itself let the OS manage threads without its own runtime implementation, but bypassing the GIL when makes sense is something user have to do to fully utilize the resources! Also this kind of code requires user to handle a lot of code as to communicate b/w threads but atleast i (think) understand the basic ideas to prevent deadlocking if occurs and iron out initial bugs. As you run such threads deeper and deeper inside application stack , it keeps getting harder to communicate information back to the client. But when it starts working, it is really cool to have a central interface to see all the stuff backend is doing and predict very good ETA !</p>
<p><code>Flask</code> was initially used to easily map <code>functions</code> to a particular route/url to wrap up initial implementation, current implementation now just uses <code>werkzeug</code> (main engine behind flask) directly, hence doing away with a lot of unrequired dependencies like a template engines that Flask ships with. Even though this would not effect the end user in any visible way, this has been a very nice quality-of-life improvement like stuff for me as a developer. Since werkzeug is pure python, it can now be shipped/bundled directly as source code. Also each request is now handled by an available thread (from a pool) by reading <code>http environment</code> from a shared queue following conventional model. By default for multi-threaded option, werkzeug would create a new fresh thread for handling that request. This does away with lots of OS/system calls for each new request and latency now seems more consistent and predictive. I have also  stumbled upon a pattern to actually make it easier to <code>mount</code> multiple <code>apps</code>  cleanly given i never liked and even understood the <code>blueprint</code> that flask offers to make it easier to distribute the logic of your app to other modules too.
Since WSGI protocol just expect a callable python object, it should be much easier to develop independent <em>apps</em> without having any knowledge where it would be called/used. It also makes it quite fun to actually write/expose python code to handle client inputs. </p>
<div><pre><span></span><code><span>class</span> <span>SimpleApp</span><span>():</span>
    <span>&#34;&#34;&#34;Each instance could be used a WSGI compatible callable&#34;&#34;&#34;</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>allow_local_cors</span><span>:</span><span>bool</span> <span>=</span> <span>False</span><span>):</span>
        <span>self</span><span>.</span><span>initialized</span> <span>=</span> <span>False</span>
        <span>self</span><span>.</span><span>http_methods</span> <span>=</span> <span>[</span><span>&#34;GET&#34;</span><span>,</span> <span>&#34;POST&#34;</span><span>,</span> <span>&#34;PUT&#34;</span><span>,</span> <span>&#34;DELETE&#34;</span><span>,</span> <span>&#34;OPTIONS&#34;</span><span>]</span> 
        <span>self</span><span>.</span><span>url_map</span> <span>=</span> <span>None</span> <span># we will lazily initialize it!</span>
        <span>self</span><span>.</span><span>extension_prefix</span> <span>=</span> <span>&#34;ext&#34;</span> <span># as apps would be registered/</span>
        <span>self</span><span>.</span><span>registered_extensions</span><span>:</span><span>dict</span><span>[</span><span>str</span><span>,</span> <span>SimpleApp</span><span>]</span> <span>=</span> <span>{}</span>

        <span>....</span>

    <span>def</span> <span>add_url_rule</span><span>(</span><span>self</span>
                     <span>rule</span><span>:</span><span>str</span><span>,</span> 
                     <span>view_function</span><span>:</span><span>Callable</span><span>,</span> <span># corresponding view.</span>
                     <span>endpoint</span><span>:</span><span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>,</span> <span># set to view_function</span>
                     <span>methods</span><span>:</span><span>list</span><span>[</span><span>str</span><span>]</span><span>=</span> <span>[</span><span>&#34;GET&#34;</span><span>]):</span>

        <span>...</span> <span># some validation code.</span>

        <span>self</span><span>.</span><span>endpoint_2_uri</span><span>[</span><span>endpoint</span><span>]</span> <span>=</span> <span>(</span><span>Rule</span>
            <span>(</span><span>rule</span><span>,</span> <span>endpoint</span> <span>=</span> <span>endpoint</span><span>),</span> <span>methods</span>
            <span>)</span>
        <span>self</span><span>.</span><span>endpoint_2_viewFunction</span><span>[</span><span>endpoint</span><span>]</span>  <span>=</span> <span>view_function</span>
        <span>self</span><span>.</span><span>initialized</span> <span>=</span> <span>False</span>

    <span>def</span> <span>register</span><span>(</span><span>self</span><span>,</span> <span>app</span><span>:</span><span>SimpleApp</span><span>,</span> <span>name</span><span>:</span><span>str</span><span>):</span>
        <span>&#34;&#34;&#34;</span>
<span>        Here we register another such `app`.</span>
<span>        It would be mounted at `/ext/&lt;name&gt;` , so all requests to /ext/&lt;name&gt;/&lt;route&gt;, would be forwarded to this `app` .</span>
<span>        &#34;&#34;&#34;</span>

        <span>...</span> <span># some validation code.</span>
        <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>name</span><span>]</span> <span>=</span> <span>app</span>
        <span>print</span><span>(</span><span>&#34;Extension registered at: </span><span>{}</span><span>/</span><span>{}</span><span>&#34;</span><span>.</span><span>format</span><span>(</span><span>self</span><span>.</span><span>extension_prefix</span><span>,</span> <span>name</span><span>))</span>


    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>environ</span><span>,</span> <span>start_response</span><span>)</span> <span>-&gt;</span> <span>Iterable</span><span>[</span><span>bytes</span><span>]:</span>
        <span># This is called </span>
        <span>if</span> <span>not</span> <span>(</span><span>self</span><span>.</span><span>initialized</span><span>):</span>
            <span>print</span><span>(</span><span>&#34;[Initializing]: Parent&#34;</span><span>)</span>
            <span>self</span><span>.</span><span>initialize</span><span>()</span>

        <span>for</span> <span>ext</span> <span>in</span> <span>self</span><span>.</span><span>registered_extensions</span><span>:</span>
            <span>if</span> <span>not</span> <span>(</span><span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>ext</span><span>]</span><span>.</span><span>initialized</span><span>):</span>
                <span>print</span><span>(</span><span>&#34;[Initializing]: </span><span>{}</span><span>&#34;</span><span>.</span><span>format</span><span>(</span><span>ext</span><span>))</span>
                <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>ext</span><span>]</span><span>.</span><span>initialize</span><span>()</span>

        <span># If a call to such an extension.. we modify the environment a bit.</span>
        <span>active_app</span> <span>=</span> <span>self</span>
        <span>extension_name</span> <span>=</span> <span>None</span>
        <span>temp_path</span> <span>=</span> <span>environ</span><span>[</span><span>&#39;PATH_INFO&#39;</span><span>]</span>
        <span>temp_split</span> <span>=</span> <span>temp_path</span><span>.</span><span>split</span><span>(</span><span>&#34;/&#34;</span><span>)</span>
        <span>if</span> <span>temp_split</span><span>[</span><span>1</span><span>]</span> <span>==</span> <span>self</span><span>.</span><span>extension_prefix</span><span>:</span>

            <span>extension_name</span> <span>=</span> <span>temp_split</span><span>[</span><span>2</span><span>]</span>
            <span>assert</span> <span>extension_name</span> <span>in</span> <span>self</span><span>.</span><span>registered_extensions</span><span>,</span> 
            <span>extension_path</span> <span>=</span> <span>temp_path</span><span>.</span><span>replace</span><span>(</span><span>&#34;/</span><span>{}</span><span>/</span><span>{}</span><span>&#34;</span><span>.</span><span>format</span><span>(</span><span>self</span><span>.</span><span>extension_prefix</span><span>,</span> <span>extension_name</span><span>),</span> <span>&#34;&#34;</span><span>)</span>


            <span>environ</span><span>[</span><span>&#39;PATH_INFO&#39;</span><span>]</span> <span>=</span> <span>extension_path</span>
            <span>environ</span><span>[</span><span>&#39;REQUEST_URI&#39;</span><span>]</span> <span>=</span> <span>extension_path</span>
            <span>environ</span><span>[</span><span>&#39;RAW_URI&#39;</span><span>]</span> <span>=</span> <span>extension_path</span>

            <span>active_app</span> <span>=</span> <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>extension_name</span><span>]</span>

    <span>## -----------------------------------------------</span>
    <span># NOTE: only werkzeug specific code is here!</span>
    <span># ---------------------------------------------</span>
    <span>request</span> <span>=</span> <span>Request</span><span>(</span><span>environ</span> <span>=</span> <span>environ</span><span>)</span> <span># minimal wrapping code!</span>
    <span>urls</span> <span>=</span> <span>active_app</span><span>.</span><span>url_map</span><span>.</span><span>bind_to_environ</span><span>(</span><span>environ</span><span>)</span>
    <span>endpoint</span><span>,</span> <span>args</span> <span>=</span> <span>urls</span><span>.</span><span>match</span><span>()</span>

    <span># view function can choose to return iterable[bytes] are the result of view function or call , or further wrap it to be as expected by werkzeug!</span>
    <span>iterable_bytes</span> <span>=</span> <span>active_app</span><span>.</span><span>endpoint_2_viewFunction</span><span>[</span><span>endpoint</span><span>](</span><span>request</span><span>,</span> <span>**</span><span>args</span><span>)</span> 
    <span>return</span> <span>iterable_bytes</span>  <span># as WSGI protocol expects!</span>
    <span># ---------------------------------------------------------</span>
</code></pre></div>

<p>Note that, any existing Python object, can be made to accept <code>client</code> requests on demand by adding very minimal code and could be done for selective functionality. For example, during setup of a new android device, i may have to ask user to choose one of the existing <code>devices</code>, this kind of interactive input can be modeled easily now, as i just add a new routine in the Corresponding class to accept requests on a route such as <code>/ext/android/beginSetup</code>, once i get that, all the existing logic already written could be used to finish setup. It is as easy as <code>parent_app.register(app = thisApp, name = &#34;android&#34;)</code> to start routing corresponding requests to this app!</p>
<h2>ML:</h2>
<p>Machine learning is being powered by a framework written completely in Nim, most of work was done on that framework before i even stared working on this project. This has allowed me to wrap CLIP and Face-Recognition Pipeline along with the application while only depending on OneDNN for some routines. OneDNN (mkldnn) (<a href="https://github.com/uxlfoundation/oneDNN">https://github.com/uxlfoundation/oneDNN</a>) is one of the libraries to speed up various Deep learning operations with great documentation. </p>
<p>Ported models run faster  on intel/Amd Cpus than pytorch counterparts, owing to fusion of operations like Batch Normalization and Convolution, and high re-use of pre-allocated memory (similar to in-place operations). Current <code>torch.compile</code> like engine would end up making some of those optimizations after analyzing the graph, but for at-least 2.0 version it is not supported on Windows for me to compare against!</p>
<p>It took a lot of effort during one-two years i was working on it to be complete enough for me to start porting Deep-learning models using it. Also OneDNN shifted to V3 during that time, and only some code was updated to newer API and this has left the project in a unstable state with no visible stable APIs for users to work with. For each model i have to manually analyze the locations/requirements for fusion of operations, port quite a lot of pre-processing and post-processing code to make it end to end. These reasons contributed to a lot of technical debt, which i have not found the resources to tackle yet. Without removing that debt it never made sense to open-source it, besides there are now projects like GGML, and tiny-grad to serve inference only needs with minimal resources! </p>
<p><img alt="alt ML codebase sample" src="http://tinylogger.com/assets/ml_1.png"/></p>
<p><img alt="alt ML codebase sample" src="http://tinylogger.com/assets/ml_2.png"/></p>
<p>Porting of each model is quite an involved task, as you have to read enough papers to understand ideas about model if want to later fine-tune that model too. You may want to find first find or create a simpler implementation in pytorch to make it easier to port to a new language. All experimentation could be done in pytorch/python, for example i experimented with alternate quantized attention layers for CLIP model, and it indeed had a better performance for eval datasets mentioned in CLIP paper. Tangentially it was really cool to read through Open-AI implementations and papers, papers were written in an approachable manner to let the read indulge in hypothesis, codebases were clean with minimal dependencies. Its really a shame what that company/organisation chose to become under the guise of &#34;user-safety&#34; effectively clipping the (open) ethos of this field, but at same time i am grateful for all the researchers&#39; work in this current DL/ML era and seeing the evolution of this field in such an open manner!   </p>
<p>I would like to work on the project though atleast enough to tackle that debt and open-source it in state for users to extend upon, if found useful.
Even though i am using OneDNN for some routines, i think it is better to have a common and easier to extend codebase to allow more experimentation and aggressive optimizations , but this itself is a huge-task and now with multiple GPU architectures its just something that couldn&#39;t be tackled without a lot of time and money. Even in this age where H100 is the baseline for benchmarks in testing, i find it worthwhile to work on a minimal DL Compiler to just tackle ARM/Intel/Risc Cpus to start taking advantage of these cheaper machines. Being able to pin-point a tennis ball in a 3D space remains the dream !</p>
<h2>Frontend / App:</h2>
<p>Current front-end is completely written in Html, Js(Ts) and (tailwind) css as multi page webapp. Earlier frontend was written in Svelte, but lack of internal documentation and too much &#34;magic&#34; became too &#34;viral&#34; for me to handle. For me, abstractions and APIs exposed by Browsers are more than enough to maintain required precision during DOM updates. Care is taken to use batch updates, prevent redundant rendering, judicial usage of resources to prevent unrequired pressure through pagination, even for a local backend server. It has passed our litmus test for search over 180 Gb of indexed Pexels dataset on a (minimal) remote server. My friend <a href="https://github.com/akshaymalik1995">Akshay</a> helped a lot in frontend development, testing various datasets and offering detailed bug reports which helped uncover a lot of edge cases during development of the project. There would always be room for improvements on the UX/UI side, but we have found it is much easier to extend and improve frontend with a stable backend! </p>
<p><label for="pexel-link">⊕</label>

<span>
  Pexels dataset: <a href="https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf">https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf</a></span></p>

<p>Apart from webapp, there is also a Windows App, which under the hood uses the <code>webview</code> to render the frontend. All native Windows APIs remain available to use from the Nim code, which puts it into a hybrid category. It is not ideal, but atleast it doesn&#39;t require me to ship a full web-browser, which i think is waste of compute resources, but at the same time leaves me wondering how current GUI development became so resource intensive for a single developer to manage while offering little benefits! I have been looking into forks of earlier GTK versions for linux to keep the complexity/learning contained, but that also seems nothing less than an adventure!  </p>
<h2>Tools/libraries:</h2>
<ul>
<li>
<p>Nimpy (<a href="https://github.com/yglukhov/nimpy">https://github.com/yglukhov/nimpy</a>) : A minimal python-Nim bridge to make it easier to write extensions in Nim to be called from python and to use python modules in Nim. Unlike many such bridges which includes a lot of boiler-plate code, there are no complex classes/interfaces to be included in the extension. It targets necessary features like marshaling of native python types to and from Nim, targets the minimal Python API to not depend on python versions, finding underlying python.dll at runtime.</p>
</li>
<li>
<p>Stb Image (<a href="https://github.com/nothings/stb">https://github.com/nothings/stb</a>): A big fan of such single header libraries, this one implements encoders for most of image formats in pure C. Its very easy to modify it pass pointer to the raw-data and writing raw-data to a pre-allocated memory saving costly memory copying particularly visible for 4k photos! It helps remove dependency on OpenCV for image reading ! Nim made it very easy to just compile this along with other Nim code.</p>
</li>
<li>
<p>LibWebp (<a href="https://github.com/webmproject/libwebp">https://github.com/webmproject/libwebp</a>): Allows decoding and encoding for webp formats,  Though documentation is a bit sparse on some internal API usage, lot of examples are included in the repository to read. I managed to use <code>argb</code> field directly to pass <code>argb</code> format data to do away with transformation logic and some (memory) allocations. It follows callback passing convention to implement custom behaviour like a progress bar and to write encoded data to a user provided buffer. Written completely in C and very easy to compile and read, it is being used for writing image previews, helping remove dependency on OpenCV.</p>
</li>
<li>
<p>Zig-cc (<a href="https://ziglang.org">https://ziglang.org</a>): Using <code>zig/clang</code> as a C compiler, allowed me to easily cross-compile a lot of Nim code for Linux, targeting <code>2.27 libc</code>. Making it easier to set a LibC target has proved very useful to bypass that <code>libC</code> mismatching stuff!
Really cool work by Zig community to tackle a lot of such technical debt to make software development much easier !</p>
</li>
</ul>
<p>As mentioned earlier i try to use a lot of existing open-source code if i can, even it would be for reading/understanding purposes only. It still blows my mind even after many years, to just read/understand some complex implementation and modify it for personal use-case for <em>Free</em>.
For example even though <code>OpenCV</code> is a big/complex dependency, its still has a very readable codebase and i read code from it a few times during this project to understand differences b/w my port and OpenCV one.</p>
<p>Being able to integrate multiple languages has its own challenges too, as it would require us to understand boundaries, internal details, assumptions that each runtime would want developer to respect. It gets complex to reproduce and understand bugs while running multiple multi-threaded runtimes as  debugging gets more difficult. Debugging is one of things i would like to get better at, i have very limited knowledge of GDB as of now, which is expected to be table stakes for debugging in such environments. I have had some nasty bugs , but being able to compile all required pieces made it a bit easier to debug even with print-style debugging :)</p>
<h2>Current State:</h2>
<p>A lot of functionality is working, than not and having tested over 500k images i could be a bit satisfied about internals&#39; performance and robustness. I would like to say that it can easily handle 10 millions of images/resources, and there is nothing to suggest that it won&#39;t, but it is different from using a production database to extrapolate the performance confidently. Despite writing from (almost) scratch in a number of languages, both indexing and inferencing pipeline are more expressive, robust and faster than many similar images search apps, but benchmarking for such complex apps could be subjective and more so when you mix in semantic search.</p>
<p>There are still some hardcoded constants and also intentionally some low performing components, like using ViT B/32 variant of CLIP model, which are acting as placeholders, and would be replaced easily with better counterparts in the future. </p>
<p>It has been tested on Windows 10/11 and on Fedora 42/43 with an assumption of <code>x64</code> architecture. Compiled extensions are also packaged to quickly test the application, but users are free to compile code as they see fit. Linux shared objects target <code>LibC 2.27</code>, so should work on most of recent distributions out of the box. Except some ML code there is main requirement of any/a C compiler to further extend the codebase by the user. Most of testing is done on my Laptop with  <code>i5-8300H</code> processor and 8 GB memory. I don&#39;t have a MacOS to test on, ML code would need to be modified to target ARM architecture, except that very minor modifications should be needed if any. It is quite possible for initial users to encounter minor bugs, due to its limited run in diverse dev environments, but installation and usage on Cloud servers during testing has been quite smooth.</p>
<p>Below is a video showcasing workflow to index data from multiple MTP/Android devices. (Still a WIP). </p>
<video id="video-player" width="100%" controls="" preload="auto" poster="/assets/extension_demo.png">
        <source id="video-source" src="/assets/extension_demo.mp4" type="video/mp4"/>
        Your browser does not support the video tag.
</video>

<h2>Random Thoughts/Philosophy:</h2>
<p>I think it gets easier with time to grok larger codebases to isolate/find the functionality/implementation reader would be interested in. Most of mature codebases are organized to help navigating the source-tree anyway, and have detailed documentation. Being able to have enough patience to make yourself comfortable is a necessary part of growing as a developer, as initial documentation/codebase would always seem alien and big enough to trigger that flight reaction! </p>
<p>Batching and Caching are two generic strategies that could be applied to speed up most of bottleneck portions. Both strategies lead to better/denser utilization of CPUs by (trying to) minimise the costly load/store instructions during a hot loop. Batching for example could do it by allocating necessary memory up-front for a batch and de-allocating all at once when no longer required, reducing the number of costly system-calls. Caching may involve designing or using a (hardware)cache friendly data-structure, when it is possible to do so.</p>
<p>Each optimization would involve assumptions and each subsequent optimization would become harder and harder to implement, may preventing the clean refactoring of code when future functionalities may need to be accommodated. It itself is a kind of rabbit-hole, and user should know when to stop as there would always be something <em>else</em> to be optimized! </p>
<p>With (coding) tools involving AI/LLMs it is easier than ever to get a piece of desired functionality, as a developer i understand it is another useful tool in a long-history of improvements, that most of developers would come to use in their workflow. Current LLMs have undeniable ability to handle complex instructions, explain non-trivial code and that so for various mixed modalities! It has been a bit <em>unreasonable</em> to end up with such abilities with just next token prediction as primary objective, even for a researcher working in this field.
My usage for such tools is only through a (free) search engine(s), Although for now there has been <em>no</em> scenario in such tools have helped me, that i wouldn&#39;t have got to using traditional means. But i can admit such tools/engines are really effective in helping us to get unstuck in a variety of situations, arguably helping us to <em>learn</em> faster. <code>Functions/routines</code> are nice and enough abstractions to provide enough context to such engines, to get the required <em>help</em>, without ever needing <code>review/edit/rewrite</code> cycle.</p>
<p>Despite the dominant usage of LLMs there exist equally interesting smaller models/architectures representing the huge potential that this field of deep-learning holds. Neural-networks allow us to (good enough)model any arbitrary function/flow using an iterative framework from a few thousand samples representing the function space, effectively equipping us with a very power statistical tool 
<label for="ssl-0">⊕</label>

<span>
Self-supervised learning don&#39;t even need explicit outputs, how cool is that..
See <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/">https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/</a> this work for more information.
</span>
to introduce a new independent signal to reduce the entropy of the problem in many domains. I am a fan of smaller personalized models&#39; potential to tackle everyday problems, and myself uses cheap off-the-self cameras coupled with a DL model to detect those Damn Monkeys, and for local voice-synthesis.
<label for="monkey-trivia">⊕</label>

<span>
  Monkey Capturing was even on the manifesto of one of the candidates at city-level elections!
</span>
In country like India, where even (traditional) Automation is limited to products of very few big companies, I can&#39;t help smiling whenever i point remote at my &#34;AI&#34; controlled AC :) </p>
<p>Living in a two-tier town in northern India with very minimal fixed-costs has allowed me to work on this for quite a long time without any savings or continuous financial freedom. But i cannot be a hypocrite about it, as it was a conscious decision to learn, explore and implement some of the ideas i had for some time. In return, this has allowed me to stay in touch with friends, played a lot of outdoor games, and help me in reflecting on the things i would want to spend more time in future.</p>
<p>Timely financial grants during the last one and half year from <a href="https://samagata.org/">Samagata foundation</a> and <a href="https://Fossunited.org">FossUnited</a> has allowed me to complete a bulk of work to point, where i am satisfied with the current state of the project, for which i will always be grateful.</p>
<p>I would very much like to continue on this or adjacent projects, as there are still a lot of ideas and code pending, to make it a very stable everyday engine for users to use . But for that i will have to figure out a way to sustain this , without ever compromising the Core features/functionality in any way, As those were some of reasons i started working on it in the first place! Extensions to allow indexing remote storage like Google Drive or Android devices smoothly from the app itself seems like a good direction in that regard for now!</p>
</article></div>
  </body>
</html>
