<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html">Original</a>
    <h1>Parsing protobuf at 2&#43;GB/s: how I learned to love tail calls in C (2021)</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>An exciting feature <a href="https://reviews.llvm.org/D99517">just landed in the main branch of the Clang
compiler</a>. Using the <code>[[clang::musttail]]</code> or
<code>__attribute__((musttail))</code> statement attributes, you can now get guaranteed
tail calls in C, C++, and Objective-C.</p>



<p>While tail calls are usually associated with a functional programming style, I
am interested in them purely for performance reasons.  It turns out that in
some cases we can use tail calls to get better code out of the compiler than
would otherwise be possible—at least given current compiler
technology—without dropping to assembly.</p>

<p>Applying this technique to protobuf parsing has yielded amazing results: <a href="https://github.com/protocolbuffers/upb/pull/310">we
have managed to demonstrate protobuf parsing at over
2GB/s</a>, more than double the
previous state of the art.  There are multiple techniques that contributed to
this speedup, so “tail calls == 2x speedup” is the wrong message to take away.
But tail calls are a key part of what made that speedup possible.</p>

<p>In this blog entry I will describe why tail calls are such a powerful
technique, how we applied them to protobuf parsing, and how this technique
generalizes to interpreters. I think it’s likely that all of the major language
interpreters written in C (Python, Ruby, PHP, Lua, etc.) could get significant
performance benefits by adopting this technique.  The main downside is
portability: currently <code>musttail</code> is a nonstandard compiler extension, and
while I hope it catches on it will be a while before it spreads widely enough
that your system’s C compiler is likely to support it.  That said, at build
time you can compromise some efficiency for portability if you detect that
<code>musttail</code> is not available.</p>



<p>A tail call is any function call that is in tail position, the final action to
be performed before a function returns.  When <em>tail call optimization</em> occurs,
the compiler emits a <code>jmp</code> instruction for the tail call instead of <code>call</code>.
This skips over the bookkeeping that would normally allow the callee <code>g()</code> to
return back to the caller <code>f()</code>, like creating a new stack frame or pushing the
return address.  Instead <code>f()</code> jumps directly to <code>g()</code> as if it were part of
the same function, and <code>g()</code> returns directly to whatever function called
<code>f()</code>. This optimization is safe because <code>f()</code>’s stack frame is no longer
needed once the tail call has begun, since it is no longer possible to access
any of <code>f()</code>’s local variables.</p>

<p>While this may seem like a run-of-the-mill optimization, it has two very
important properties unlock new possibilities in the kinds of algorithms we can
write.  First, it reduces the stack memory from from ++O(n)++ to ++O(1)++ when
making ++n++ consecutive tail calls, which is important because stack memory is
limited and stack overflow will crash your program.  This means that certain
algorithms are not actually safe to write unless this optimization is
performed.  Secondly, <code>jmp</code> eliminates the performance overhead of <code>call</code>, such
that a function call can be just as efficient as any other branch. These two
properties enable us to use tail calls as an efficient alternative to
normal iterative control structures like <code>for</code> or <code>while</code>.</p>

<p>This is by no means a new idea, indeed it goes back to at least 1977 when Guy
Steele wrote <a href="http://dspace.mit.edu/handle/1721.1/5753">an entire paper</a>
arguing that procedure calls make for cleaner designs than <code>GOTO</code>, and that
tail call optimization can make them just as fast. This was one of the <a href="https://en.wikipedia.org/wiki/History_of_the_Scheme_programming_language#The_Lambda_Papers">“Lambda
Papers”</a>
written between 1975 and 1980 that developed many of the ideas underlying Lisp
and Scheme.</p>

<p>Tail call optimization is not even new to Clang: like GCC and many other
compilers, Clang was already capable of optimizing tail calls. In fact, the
<code>musttail</code> attribute in our first example above did not change the output of
the compiler at all: Clang would already have optimized the tail call under
<code>-O2</code>.</p>

<p>What is new is the <em>guarantee</em>. While compilers will often optimize tail calls
successfully, this is best-effort, not something you can rely on. In
particular, the optimization will most likely not happen in non-optimized
builds:</p>



<p>Here the tail call was compiled to an actual <code>call</code>, so we are back to ++O(n)++
stack space. This is why we need <code>musttail</code>: unless we can get a guarantee from
the compiler that our tail calls will <em>always</em> be optimized, in all build
modes, it isn’t safe to write algorithms that use tail calls for iteration.  It
would be a pretty severe limitation to have code that only works when
optimizations are enabled.</p>



<p>Compilers are incredible pieces of technology, but they are not perfect.  Mike
Pall, author of LuaJIT, decided to write LuaJIT 2.x’s interpreter in assembly
rather than C, and he cites this decision as a major factor that explains <a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">why
LuaJIT’s interpreter is so
fast</a>.
He later went into more detail about <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">why C compilers struggle with interpreter
main loops</a>.  His two
most central points are:</p>

<ul>
  <li>The larger a function is, and the more complex and connected its control
flow, the harder it is for the compiler’s register allocator to keep the most
important data in registers.</li>
  <li>When fast paths and slow paths are intermixed in the same function, the
presence of the slow paths compromises the code quality of the fast paths.</li>
</ul>

<p>These observations closely mirror our experiences optimizing protobuf parsing.
The good news is that tail calls can help solve both of these problems.</p>

<p>It may seem odd to compare interpreter loops to protobuf parsers, but the
nature of the protobuf wire format makes them more similar than you might
expect.  The protobuf wire format is a series of tag/value pairs, where the tag
contains a field number and wire type.  This tag acts similarly to an
interpreter opcode: it tells us what operation we need to perform to parse this
field’s data.  Like interpreter opcodes, protobuf field numbers can come in any
order, so we have to be prepared to dispatch to any part of the code at any
time.</p>

<p>The natural way to write such a parser is to have a <code>while</code> loop surrounding a
<code>switch</code> statement, and indeed this has been the state of the art in protobuf
parsing for basically as long as protobufs have existed. For example, <a href="https://github.com/protocolbuffers/protobuf/blob/f763a2a86084371fd0da95f3eeb879c2ff26b06d/src/google/protobuf/descriptor.pb.cc#L2175-L2227">here is
some parsing code from the current C++ version of protobuf</a>.  If we represent the
control flow graphically, we get something like this:</p>

<center>
<img src="https://docs.google.com/drawings/d/e/2PACX-1vRKuMNm6Tw_Sn2xJxBDQMHT0u0osp9DUA0Ldr-MhMVp63GPinzYB9JT0qRY9HTymmsesomq3aZe7QEs/pub?w=740"/>
</center>

<p>But this is incomplete, because at almost every stage there are things that
can go wrong.  The wire type could be wrong, or we could see some corrupt
data, or we could just hit the end of the current buffer.  So the full
control flow graph looks more like this.</p>

<center>
<img src="https://docs.google.com/drawings/d/e/2PACX-1vTGgQuAThUGv9ejI_pjujfKRM8rgo7c5b8lP7uveSkJTkJMbZDrtDbJzmRA4HOoDozwjj9WWPlDu8JX/pub?w=740"/>
</center>

<p>We want to stay on the fast paths (in blue) as much as possible, but when we
hit a hard case we have to execute some fallback code to handle it.  These
fallback paths are usually bigger and more complicated than the fast paths,
touch more data, and often even make out-of-line calls to other functions to
handle the more complex cases.</p>

<p>Theoretically, this control flow graph paired with a profile should give the
compiler all of the information it needs to generate the most optimal code.  In
practice, when a function is this big and connected, we often find ourselves
fighting the compiler.  It spills an important variable when we want it to keep
it in a register.  It hoists stack frame manipulation that we want to shrink
wrap around a fallback function invocation.  It merges identical code paths
that we wanted to keep separate for branch prediction reasons.  The experience
can end up feeling like trying to play the piano while wearing mittens.</p>



<p>The analysis above is mainly just a rehash of of Mike’s <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">observations about
interpreter main
loops</a>.  But instead of
dropping to assembly, as Mike did with LuaJIT 2.x, we found that a tail call
oriented design could give us the control we needed to get nearly optimal code
from C.  I worked on this together with my colleague Gerben Stavenga, who came
up with much of the design.  Our approach is similar to the design of the
<a href="https://github.com/wasm3/wasm3">wasm3 WebAssembly interpreter</a> which describes
this pattern as a <a href="https://github.com/wasm3/wasm3/blob/main/docs/Interpreter.md#m3-massey-meta-machine">“meta
machine”</a>.</p>

<p>The code for our 2+GB/s protobuf parser was submitted to
<a href="https://github.com/protocolbuffers/upb">upb</a>, a small protobuf library written
in C, in <a href="https://github.com/protocolbuffers/upb/pull/310">pull/310</a>.  While it
is fully working and passing all protobuf conformance tests, it is not rolled
out anywhere yet, and the design has not been implemented in the C++ version of
protobuf.  But now that <code>musttail</code> is available in Clang (and <a href="https://github.com/protocolbuffers/upb/pull/390">upb has been
updated to use it</a>), one of
the biggest barriers to fully productionizing the fast parser has been removed.</p>

<p>Our design does away with a single big parse function and instead gives each
operation its own small function.  Each function tail calls the next operation
in sequence.  For example here is a function to parse a single fixed-width
field.  (This code is simplified from the actual code in upb; there are many
details of our design that I am leaving out of this article, but will hopefully
cover in future articles).</p>

<div><div><pre><code><span>#include</span> <span>&lt;stdint.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
</span>
<span>typedef</span> <span>void</span> <span>*</span><span>upb_msg</span><span>;</span>
<span>struct</span> <span>upb_decstate</span><span>;</span>
<span>typedef</span> <span>struct</span> <span>upb_decstate</span> <span>upb_decstate</span><span>;</span>

<span>// The standard set of arguments passed to each parsing function.</span>
<span>// Thanks to x86-64 calling conventions, these will be passed in registers.</span>
<span>#define UPB_PARSE_PARAMS                                          \
  upb_decstate *d, const char *ptr, upb_msg *msg, intptr_t table, \
      uint64_t hasbits, uint64_t data
#define UPB_PARSE_ARGS d, ptr, msg, table, hasbits, data
</span>
<span>#define UNLIKELY(x) __builtin_expect(x, 0)
#define MUSTTAIL __attribute__((musttail))
</span>
<span>const</span> <span>char</span> <span>*</span><span>fallback</span><span>(</span><span>UPB_PARSE_PARAMS</span><span>);</span>
<span>const</span> <span>char</span> <span>*</span><span>dispatch</span><span>(</span><span>UPB_PARSE_PARAMS</span><span>);</span>

<span>// Code to parse a 4-byte fixed field that uses a 1-byte tag (field 1-15).</span>
<span>const</span> <span>char</span> <span>*</span><span>upb_pf32_1bt</span><span>(</span><span>UPB_PARSE_PARAMS</span><span>)</span> <span>{</span>
  <span>// Decode &#34;data&#34;, which contains information about this field.</span>
  <span>uint8_t</span> <span>hasbit_index</span> <span>=</span> <span>data</span> <span>&gt;&gt;</span> <span>24</span><span>;</span>
  <span>size_t</span> <span>ofs</span> <span>=</span> <span>data</span> <span>&gt;&gt;</span> <span>48</span><span>;</span>

  <span>if</span> <span>(</span><span>UNLIKELY</span><span>(</span><span>data</span> <span>&amp;</span> <span>0xff</span><span>))</span> <span>{</span>
    <span>// Wire type mismatch (the dispatch function xor&#39;s the expected wire type</span>
    <span>// with the actual wire type, so data &amp; 0xff == 0 indicates a match).</span>
    <span>MUSTTAIL</span> <span>return</span> <span>fallback</span><span>(</span><span>UPB_PARSE_ARGS</span><span>);</span>
  <span>}</span>

  <span>ptr</span> <span>+=</span> <span>1</span><span>;</span>  <span>// Advance past tag.</span>

  <span>// Store data to message.</span>
  <span>hasbits</span> <span>|=</span> <span>1ull</span> <span>&lt;&lt;</span> <span>hasbit_index</span><span>;</span>
  <span>memcpy</span><span>((</span><span>char</span><span>*</span><span>)</span><span>msg</span> <span>+</span> <span>ofs</span><span>,</span> <span>ptr</span><span>,</span> <span>4</span><span>);</span>

  <span>ptr</span> <span>+=</span> <span>4</span><span>;</span>  <span>// Advance past data.</span>

  <span>// Call dispatch function, which will read the next tag and branch to the</span>
  <span>// correct field parser function.</span>
  <span>MUSTTAIL</span> <span>return</span> <span>dispatch</span><span>(</span><span>UPB_PARSE_ARGS</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>For a function this small and simple, Clang gives us code that is
basically impossible to beat.</p>

<pre><code>upb_pf32_1bt:                           # @upb_pf32_1bt
        mov     rax, r9
        shr     rax, 24
        bts     r8, rax
        test    r9b, r9b
        jne     .LBB0_1
        mov     r10, r9
        shr     r10, 48
        mov     eax, dword ptr [rsi + 1]
        mov     dword ptr [rdx + r10], eax
        add     rsi, 5
        jmp     dispatch                        # TAILCALL
.LBB0_1:
        jmp     fallback                        # TAILCALL
</code></pre>

<p>Note that there is no prologue or epilogue, no register spills, indeed there is
no usage of the stack whatsoever.  The only exits are <code>jmp</code>s from the two tail
calls, but no code is required to forward the parameters, because the arguments
are already sitting in the correct registers.  Pretty much the only improvement
we could hope for is to get a conditional jump for the tail call, <code>jne
fallback</code>, instead of <code>jne</code> followed by <code>jmp</code>.</p>

<p>If you were looking at a disassembly of this code without symbol information,
you would have no reason to know that this was an entire function. It could
just as easily be a basic block from a larger function.  And that, in essence,
is exactly what we are doing.  We are taking an interpreter loop that is
conceptually a big complicated function and programming it block by block,
transferring control flow from one to the next via tail calls.  We have full
control of the register allocation at every block boundary (well, for six
registers at least), and by using the same set of parameters for each function,
we eliminate the need to shuffle any values around from one call to the next.
As long as the function is simple enough to not spill those six registers,
we’ve achieved our goal of keeping our most important state in registers
throughout all of the fast paths.</p>

<p>We can optimize every instruction sequence independently, and crucially, the
compiler will treat each sequence as independent too because they are in
separate functions (we can prevent inlining with <code>noinline</code> if necessary).
This solves the problem we described earlier where the code from fallback paths
would degrade the code quality for fast paths. If we put the slow paths in
entirely separate functions from the fast paths, we can be guaranteed that
the fast paths will not suffer.  The nice assembly sequence we see above is
effectively frozen, unaffected by any changes we make to other parts of the
parser.</p>

<p>If we apply this pattern to <a href="https://www.reddit.com/r/programming/comments/badl2/luajit_2_beta_3_is_out_support_both_x32_x64/c0lrus0/">Mike’s example from
LuaJIT</a>,
we can more or less <a href="https://godbolt.org/z/K8Mo6hcGa">match his hand-written assembly with only minor code
quality defects</a>:</p>

<div><div><pre><code>#define PARAMS unsigned RA, void *table, unsigned inst, \
               int *op_p, double *consts, double *regs
#define ARGS RA, table, inst, op_p, consts, regs
typedef void (*op_func)(PARAMS);
void fallback(PARAMS);

#define UNLIKELY(x) __builtin_expect(x, 0)
#define MUSTTAIL __attribute__((musttail))

void ADDVN(PARAMS) {
    op_func *op_table = table;
    unsigned RC = inst &amp; 0xff;
    unsigned RB = (inst &gt;&gt; 8) &amp; 0xff;
    unsigned type;
    memcpy(&amp;type, (char*)&amp;regs[RB] + 4, 4);
    if (UNLIKELY(type &gt; -13)) {
        return fallback(ARGS);
    }
    regs[RA] += consts[RC];
    inst = *op_p++;
    unsigned op = inst &amp; 0xff;
    RA = (inst &gt;&gt; 8) &amp; 0xff;
    inst &gt;&gt;= 16;
    MUSTTAIL return op_table[op](ARGS);
}
</code></pre></div></div>

<p>The resulting assembly is:</p>

<pre><code>ADDVN:                                  # @ADDVN
        movzx   eax, dh
        cmp     dword ptr [r9 + 8*rax + 4], -12
        jae     .LBB0_1
        movzx   eax, dl
        movsd   xmm0, qword ptr [r8 + 8*rax]    # xmm0 = mem[0],zero
        mov     eax, edi
        addsd   xmm0, qword ptr [r9 + 8*rax]
        movsd   qword ptr [r9 + 8*rax], xmm0
        mov     edx, dword ptr [rcx]
        add     rcx, 4
        movzx   eax, dl
        movzx   edi, dh
        shr     edx, 16
        mov     rax, qword ptr [rsi + 8*rax]
        jmp     rax                             # TAILCALL
.LBB0_1:
        jmp     fallback
</code></pre>

<p>The only opportunity for improvement I see here, aside from the <code>jne fallback</code>
issue mentioned before, is that for some reason the compiler doesn’t want to
generate <code>jmp qword ptr [rsi + 8*rax]</code>. Instead it prefers to load into <code>rax</code>
and then follow with <code>jmp rax</code>.  These are minor code generation issues that
could hopefully be fixed in Clang without too much work.</p>



<p>One of the biggest caveats with this approach is that these beautiful assembly
sequences get catastrophically pessimized if any non tail calls are present.
Any non tail call forces a stack frame to be created, and a lot of data spills
to the stack.</p>

<div><div><pre><code><span>#define PARAMS unsigned RA, void *table, unsigned inst, \
               int *op_p, double *consts, double *regs
#define ARGS RA, table, inst, op_p, consts, regs
</span><span>typedef</span> <span>void</span> <span>(</span><span>*</span><span>op_func</span><span>)(</span><span>PARAMS</span><span>);</span>
<span>void</span> <span>fallback</span><span>(</span><span>PARAMS</span><span>);</span>

<span>#define UNLIKELY(x) __builtin_expect(x, 0)
#define MUSTTAIL __attribute__((musttail))
</span>
<span>void</span> <span>ADDVN</span><span>(</span><span>PARAMS</span><span>)</span> <span>{</span>
    <span>op_func</span> <span>*</span><span>op_table</span> <span>=</span> <span>table</span><span>;</span>
    <span>unsigned</span> <span>RC</span> <span>=</span> <span>inst</span> <span>&amp;</span> <span>0xff</span><span>;</span>
    <span>unsigned</span> <span>RB</span> <span>=</span> <span>(</span><span>inst</span> <span>&gt;&gt;</span> <span>8</span><span>)</span> <span>&amp;</span> <span>0xff</span><span>;</span>
    <span>unsigned</span> <span>type</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>type</span><span>,</span> <span>(</span><span>char</span><span>*</span><span>)</span><span>&amp;</span><span>regs</span><span>[</span><span>RB</span><span>]</span> <span>+</span> <span>4</span><span>,</span> <span>4</span><span>);</span>
    <span>if</span> <span>(</span><span>UNLIKELY</span><span>(</span><span>type</span> <span>&gt;</span> <span>-</span><span>13</span><span>))</span> <span>{</span>
        <span>// When we leave off &#34;return&#34;, things get real bad.</span>
        <span>fallback</span><span>(</span><span>ARGS</span><span>);</span>
    <span>}</span>
    <span>regs</span><span>[</span><span>RA</span><span>]</span> <span>+=</span> <span>consts</span><span>[</span><span>RC</span><span>];</span>
    <span>inst</span> <span>=</span> <span>*</span><span>op_p</span><span>++</span><span>;</span>
    <span>unsigned</span> <span>op</span> <span>=</span> <span>inst</span> <span>&amp;</span> <span>0xff</span><span>;</span>
    <span>RA</span> <span>=</span> <span>(</span><span>inst</span> <span>&gt;&gt;</span> <span>8</span><span>)</span> <span>&amp;</span> <span>0xff</span><span>;</span>
    <span>inst</span> <span>&gt;&gt;=</span> <span>16</span><span>;</span>
    <span>MUSTTAIL</span> <span>return</span> <span>op_table</span><span>[</span><span>op</span><span>](</span><span>ARGS</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>This leads to the very unfortunate:</p>

<pre><code>ADDVN:                                  # @ADDVN
        push    rbp
        push    r15
        push    r14
        push    r13
        push    r12
        push    rbx
        push    rax
        mov     r15, r9
        mov     r14, r8
        mov     rbx, rcx
        mov     r12, rsi
        mov     ebp, edi
        movzx   eax, dh
        cmp     dword ptr [r9 + 8*rax + 4], -12
        jae     .LBB0_1
.LBB0_2:
        movzx   eax, dl
        movsd   xmm0, qword ptr [r14 + 8*rax]   # xmm0 = mem[0],zero
        mov     eax, ebp
        addsd   xmm0, qword ptr [r15 + 8*rax]
        movsd   qword ptr [r15 + 8*rax], xmm0
        mov     edx, dword ptr [rbx]
        add     rbx, 4
        movzx   eax, dl
        movzx   edi, dh
        shr     edx, 16
        mov     rax, qword ptr [r12 + 8*rax]
        mov     rsi, r12
        mov     rcx, rbx
        mov     r8, r14
        mov     r9, r15
        add     rsp, 8
        pop     rbx
        pop     r12
        pop     r13
        pop     r14
        pop     r15
        pop     rbp
        jmp     rax                             # TAILCALL
.LBB0_1:
        mov     edi, ebp
        mov     rsi, r12
        mov     r13d, edx
        mov     rcx, rbx
        mov     r8, r14
        mov     r9, r15
        call    fallback
        mov     edx, r13d
        jmp     .LBB0_2
</code></pre>

<p>To avoid this, we tried to follow a discipline of only calling other functions
via inlining or tail calls.  This can get annoying if an operation has multiple
points at which an unusual case can occur that is not an error.  For example,
when we are parsing protobufs, the fast and common case is that varints are
only one byte long, but longer varints are not an error.  Handling the unusual
case inline can compromise the quality of the fast path if the fallback code is
too complicated.  But tail calling to a fallback function gives no way of
easily resuming the operation once the unusual case is handled, so the fallback
function must be capable of pushing forward and completing the operation.  This
leads to code duplication and complexity.</p>

<p>Ideally this issue could be solved by adding
<a href="https://clang.llvm.org/docs/AttributeReference.html#preserve-most"><code>__attribute__((preserve_most))</code></a>
to the fallback functions and then calling them normally, without tail calls.
The <code>preserve_most</code> attribute makes the callee responsible for preserving
nearly all registers, which moves the cost of the register spills to the
fallback functions where we want it.  We experimented some with this attribute
but ran into some mysterious problems that we were not able to get to the
bottom of.  It may have been an error on our part; revisiting this is future
work.  <em>[<strong>Update: 2023-03-14:</strong> it turns out this was a bug in Clang, which was
<a href="https://reviews.llvm.org/D141020">fixed two months ago</a>]</em></p>

<p>The other major limitation is that <code>musttail</code> is not portable.  I very much
hope that the attribute will catch on, spreading to GCC, Visual C++, and other
popular compilers, and even get standardized someday.  But that day is far off,
so what to do in the meantime?</p>

<p>When <code>musttail</code> is not available, we need to perform at least one true
<code>return</code>, without a tail call, for every conceptual loop iteration.  We have
not yet implemented this fallback in upb, but I expect it will involve a macro
that either tail calls to dispatch or just returns, based on the availability
of musttail.</p>

  </div>

</article>

      </div>
    </div></div>
  </body>
</html>
