<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://andre.arko.net/2025/08/18/in-memory-filesystems-in-rust/">Original</a>
    <h1>In-Memory Filesystems in Rust</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>18 Aug 2025</p><p>I’ve been working on a CLI tool recently, and one of the things it does is manage files on disk. I have written a lot of file management tests for <a href="https://bundler.io">Bundler</a>, and the two biggest reasons that the Bundler test suite is slow are <code>exec</code> and <code>fstat</code>. Knowing that, I thought I would try to get out ahead of the slow file stat problem by using an in-memory filesystem for testing.</p><p>A collaborator mentioned being happy with the Go package named <a href="https://github.com/spf13/afero">Afero</a> for this purpose, and so I set off to look for a Rust equivalent to Afero. Conceptually, I was hoping to be able to replace <code>std::fs::</code> with <code>mycli::fs::</code> and swap out the backend in tests for something that’s completely in-memory so I don’t have to spend time waiting for syscalls.</p><p>Unfortunately, based on my searching, not only is there nothing like Afero, but simply asking about it gets you <a href="https://users.rust-lang.org/t/virtual-filesystems-for-rust/117173">a lecture about how such things aren’t necessary in Rust</a>. Somewhat frustrated, I continued searching and eventually found a few options to try.</p><p>First, I discovered the <a href="https://crates.io/crates/vfs">vfs</a> crate, whose documentation seems pretty promising. It’s possible to swap out vfs backends to get a real filesystem, an in-memory filesystem, a new filesystem scoped into a directory, or files embedded in an executable. It’s actively maintained, and seems to have a decent number of current users. Unfortunately, as I got further along, it became apparent that the vfs crate isn’t actually a viable alternative to interacting directly with the filesystem.</p><p>The vfs crate doesn’t have any support for symlinks, so resolving symlinks means going back to <code>std::fs</code> after all, and having to write special-cased symlink resolution code that doesn’t run if the filesystem is vfs. The real killer, though, was that vfs doesn’t contain any support for the concept of file permissions. Because of that, it’s impossible to write executables, which is core functionality for my tool.</p><p>It turns out the intended primary use case of the crate is to store files inside Rust binaries but still have an API sort of like the filesystem API to interact with them. Unfortunately, that information is hidden away in a comment on a random GitHub issue, rather than included in the project readme. At that point, I realized I was probably not going to be able to use vfs and also build the tool I wanted to build.</p><p>Next, I looked at <a href="https://crates.io/crates/rsfs">rsfs</a>, which is a little bit older, and seems sort of unmaintained, but explicitly says that it aims to reproduce the functionality of <code>std::fs</code>, while adding the ability to run the filesystem in memory if desired. Unfortunately, the design of the rsfs crate means that every function that talks to the filesystem now has to be parameterized against the <code>rsfs::FS</code> type, and that makes the type signatures of every function suddenly much worse.</p><p>After doing an experimental port of my initial codebase to rsfs, the gnarly type situation drove me to actually test to see what the advantages were of using rsfs to run tests fully in-memory. If the advantages were big enough, I would suck up the types and deal, but I didn’t want to make all my types worse if there wasn’t actually a payoff.</p><p>That’s when things got weird.</p><p>The first weirdness was trying an initial benchmark comparing the <code>vfs</code> implementation to a <code>std::fs</code> implementation. My naive “just run <code>cargo test</code> to benchmark” strategy seemingly paid off, with <code>vfs</code> taking around 850ms and <code>std::fs</code> taking around 1200ms. That seems pretty significant, right?</p><p>Giving away the conclusion in advance, I am sadly forced to admit that it was not actually significant. While that high level speedup from using <code>vfs</code> never went away, I was eventually forced to conclude it was a difference in linker cache, or an artifact of <code>cargo test</code> running many executables, or something else entirely.</p><p>Trying to hunt down exactly what was faster about using <code>vfs</code>, I started to pick apart the test executables generated by cargo, and I found something even more confusing. Using <code>hyperfine</code>, I was able to benchmark a single <code>cargo test</code> executable that ran through most of the filesystem calls in my code.</p><p>With <code>vfs</code> providing an in-memory filesystem, the tests benchmarked as taking about 45ms. With <code>rsfs</code> providing an in-memory filesystem, the tests benchmarked as taking… about 45ms. With <code>rsfs</code> providing the regular filesystem, the tests benchmarked as taking… also about 45ms.</p><p>Starting to feel even more confused, I set up additional tests using <code>std::fs</code> running against a ramdisk, and got a high accuracy benchmark result of… 45ms. At that point, I figured I needed to include the completely regular <code>std::fs</code> pointed directly at my regular SSD. That also took 45ms.</p><p>At this point, I can only assume that modern SSDs (and macOS filesystem cache) work so effectively together that there is effectively zero performance to be gained by making the file-related syscalls virtual? That doesn’t really mesh with my understanding of how expensive syscalls are vs function calls into a fake in-memory filesystem, but all my benchmarks seem to disagree.</p><p>If you have examples of performance differences from using an in-memory filesystem in Rust, please let me know the details!</p><p>In the meantime, it seems like modern SSDs (and modern OS filesystem caches) are so fast that it doesn’t even matter. Eat trash, be free, test directly against the filesystem. Why not.</p></div></div>
  </body>
</html>
