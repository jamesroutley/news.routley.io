<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.hopsworks.ai/dictionary/two-tower-embedding-model">Original</a>
    <h1>Two-Tower Embedding Model</h1>
    
    <div id="readability-page-1" class="page"><div fs-codehighlight-theme="base16/gruvbox-light-hard" fs-codehighlight-element="code"><p>The two-tower (or twin-tower) embedding model is a model training method for connecting <a href="https://www.hopsworks.ai/dictionary/embedding">embeddings</a> in two different modalities by placing both modalities in the same vector space. For example, a two-tower model could generate embeddings of both images and text in the same vector space. Personalized recommendation systems often use items and user-histories as the two different modalities. The modalities need to be “grounded”. For example, image and text can be “grounded” by creating <a href="https://www.hopsworks.ai/dictionary/training-data">training data</a> where a caption matches an image. Two-tower models are able to map embeddings from different modalities into the same space by ensuring both modalities have the same dimension “d”. For example, if the item embedding is of length 100, then the query embedding dimension should be 100.</p><h2><strong>Personalized recommendations for Products - Linking Two Modalities</strong></h2><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/618399cd49d125734c8dec95/650ff449db8fd48fc72908ed_82HZeHGX7BzcfkMhQDQUb8LkQiHuarnBLsdiZunevY8CVy5gXIqNJBxxCWuxr5tKWSIIq9KI1fJEwn-AroBQeYdoKONFSpDskzUhP1PYWlgKk595ggWZztCo3vNWc-7P0R5CMM8tH-TyTArMNGkU21U.png" alt=""/></p></figure><p>The two-tower model for personalized recommendations combines two items and “user history and context”. That is, given user history and context, can we generate hundreds of candidate items from a corpus of millions or billions of items? Given, those hundreds of candidates, can personalize the ranking of the candidates to the user’s history and context (e.g., items that are trending)?</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/618399cd49d125734c8dec95/650ff4494e3ade3475940067_CuQBOuJcsYuSM_BG451ZcDdnQ38wrZwMPIJmIFW-iTZXSP7nmy96WbgSDWuKgSKMw0sQDINsrkyHi4ONPEMhJDx9i_ilYrmgxkUzyQehq_7sPCYe1NXzV5HFlNQ-LaPqlFY0qof1J0pC5hl87x2Rl18.png" alt=""/></p></figure><p>We can create training data for our personalized recommender system that combines the two modalities by presenting items in response to the user query. The user may click on some item, purchase another item, place another item in a shopping cart, and not click on another item. We collect training samples as the combination of the item, a score for the user&#39;s action (0=not clicked, 5=purchased, 1=clicked), and the user’s query, user history, and context. </p><h2><strong>What is a two-tower embedding model architecture?</strong></h2><p>Most two-tower architectures are used for personalized search/recommendations, where you have queries and items as the two modalities. In this case, the two-tower embedding model architecture is a deep learning model architecture that consists of a query tower and an item tower. The query tower encodes search query and user profile to query embeddings, and the item tower encodes the item, store, and location features to item embeddings. </p><p>The probability of a user query resulting in an item being clicked or placed in a shopping cart is computed using a distance measure (such as the dot product, cosine similarity, Euclidean distance, or Hadamard product) between the embeddings from two towers. The query and item tower models are trained jointly on the history of user queries and item interactions.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/618399cd49d125734c8dec95/650ff449f5f5d64b824dd29a_TNwn71KcgOpzMhLg40_V-TNoIA4Yq6ry_3qWMs4iA4XIfEdQnYZ8a18OTAyy1HFeL8VMUqH00QczRWkRTM7eM-p4MELqx0Ef3WqD-Vmj3O7DOO4CBgNwQoQV3RsDPxHoLwQK__ZKVdRlKYtpW_yPUb0.png" alt=""/></p></figure><h2><strong>Personalized Recommendations/Search with Hopsworks</strong></h2><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/618399cd49d125734c8dec95/650ff44930921266b77e5d42_G4UGADPkvjYjDpmRKeHdiEKw4CYcQtz7n7XTRyYFgmJVXbtzF9TergrJnjY97QFIyB3NrT8ylLRIg-ZS8elM8KAE7OEnUZgClZW-YEP4m0KnQbHH0GJiJHiD6l3cB8M6IpQIn7ms-sJ4MV8KPaBuG7w.png" alt=""/></p></figure><p>You can use the <a href="https://www.hopsworks.ai/the-ml-platform-for-batch-and-real-time-data">Hopsworks platform</a> to manage the collection and usage of feature data when building two-tower models. Hopsworks includes a <a href="https://www.hopsworks.ai/dictionary/feature-store">feature store</a>, <a href="https://www.hopsworks.ai/dictionary/model-registry">model registry</a>, and <a href="https://www.hopsworks.ai/dictionary/vector-database">vector database</a>, providing both the online services needed to collect and manage training data, and the online infrastructure for candidate retrieval (VectorDB) and personalized ranking (feature store).</p><h2>What other types of applications use the twin-tower model architecture?</h2><p>Bytedance used <a href="https://www.anyscale.com/blog/how-bytedance-scales-offline-inference-with-multi-modal-llms-to-200TB-data">test/images with ALBERT and Vision transformer twin-tower model architecture</a>. Another example is the TextGNN Architecture that uses the two tower structure for decoupled generation of query/keyword embeddings</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/618399cd49d125734c8dec95/6502d8c345f17ecfa6434cb4_wT6DgXpM19AxrDPDZ3QV_sOST6Wgj2SUGf5Q-nuohNQzo4dD3arvPkv-8PbUnAAx4df603zJCn307-9H34qlTRJH9y0WfGvzwqHhrvczrwiiEouKlWp0zbWT5LfSU9JrI2SQD7d1o9SeZ51dzsFTRQ.png" alt=""/></p><figcaption>Image from TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search by Zhu et al - <a href="https://arxiv.org/abs/2101.06323">https://arxiv.org/abs/2101.06323</a> </figcaption></figure><h3><strong>Can two-tower embedding models be extended to more modalities?</strong></h3><p>A two-tower embedding model connects vectors in only two different modalities. Research is ongoing in generalizing to 3 modalities or more. Multi-modal models trained as N-tower embedding models could potentially connect vectors from N different modalities.</p></div></div>
  </body>
</html>
