<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171">Original</a>
    <h1>MD RAID or DRBD can be broken from userspace when using O_DIRECT</h1>
    
    <div id="readability-page-1" class="page"><div id="comments">


<!-- This auto-sizes the comments and positions the collapse/expand links 
     to the right. -->
<table>
<tbody><tr>
<td>
<div id="c0">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c0">Description</a>
        </span>

        <span>
          <span><span>Stanislav German-Evtushenko</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2015-05-29 09:56:10 UTC
        </span>

      </p>




<pre>Created <span><a href="https://bugzilla.kernel.org/attachment.cgi?id=178311" name="attach_178311" title="drbd_oos_test.c">attachment 178311</a> <a href="https://bugzilla.kernel.org/attachment.cgi?id=178311&amp;action=edit" title="drbd_oos_test.c">[details]</a></span>
drbd_oos_test.c

Hello,

MD RAID, DRBD and may be other software raid-like block devices can become inconsistent (silently) if program in userspace is doing something wrong.


*** How to reproduce ***

1. Prepare

gcc -pthread drbd_oos_test.c
dd if=/dev/zero of=/tmp/mdadm1 bs=1M count=100
dd if=/dev/zero of=/tmp/mdadm2 bs=1M count=100
losetup /dev/loop1 /tmp/mdadm1
losetup /dev/loop2 /tmp/mdadm2
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/loop{1,2}

2. Write data with O_DIRECT

./a.out /dev/md0

3. Check consistency with vbindiff

vbindiff /tmp/mdadm{1,2}      #press enter multiple times to skip metadata



*** Variant: EXT3 or EXT4 on top of md0 ***

The step 2 can be extended by creating file system:

mkfs.ext3 /dev/md0
mkdir /tmp/ext3
mount /dev/md0 /tmp/ext3
./a.out /tmp/ext3/testfile1
vbindiff /tmp/mdadm{1,2}      #press enter multiple times to skip metadata


In both cases data on /tmp/mdadm1 and /tmp/mdadm2 will differ. We get the same result when we use DRBD instead of MD RAID.

Best regards,
Stanislav</pre>
    </div>

    <div id="c1">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c1">Comment 1</a>
        </span>

        <span>
          <span><span>Phil Turmel</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2017-06-30 14:40:20 UTC
        </span>

      </p>




<pre>I&#39;m not convinced this is a meaningful testcase.  Any userspace application that modifies a data buffer in one thread while another thread is writing that buffer to disk is certain to not get predicable data back when reading it later.  Whether this situation results in a mismatch among raid mirrors is not terribly meaningful.</pre>
    </div>

    <div id="c2">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c2">Comment 2</a>
        </span>

        <span>
          <span><span>Wolfgang Bumiller</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2017-06-30 17:30:13 UTC
        </span>

      </p>




<pre>This is not at all about the contents of the data. It is expected that garbage is written to the disks, but each disk making up the raid will contain different garbage, which means the disks are out of sync, iow. the raid is &#34;broken&#34;. This in turn means the user space can &#34;break&#34; the raid.
The problem is that with O_DIRECT the the user space pointer is passed to the block drivers for the underlying layers making up the raid, and they all read from it independently. Any user who can run a program where they can use O_DIRECT on a file on a raid can break the raid.

It is expected that garbage is written to the disk, but the whole point of a raid is that each disk should contain the *same* garbage. Keep the garbage consistent... or something.</pre>
    </div>

    <div id="c3">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c3">Comment 3</a>
        </span>

        <span>
          <span><span>John Brooks</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2017-06-30 18:43:49 UTC
        </span>

      </p>




<pre>If any data, garbage or otherwise, is written to the RAID, should not the array be consistent afterwards? Any action by a userspace program (short of bypassing the RAID and directly writing to the constituent block devices) that results in the array becoming out of sync sounds like a bug to me.</pre>
    </div>

    <div id="c4">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c4">Comment 4</a>
        </span>

        <span>
          <span><span>Phil Turmel</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2017-06-30 18:52:47 UTC
        </span>

      </p>




<pre>I&#39;d be nice for it to be consistent, but giving up the performance of zero-copy operations to avoid what can only be garbage doesn&#39;t seem like a great tradeoff to me.  And it is long-known behaviour thanks to direct access by the kernel on mirrored swap devices.</pre>
    </div>

    <div id="c5">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c5">Comment 5</a>
        </span>

        <span>
          <span><span>Wolfgang Bumiller</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2017-10-30 15:22:00 UTC
        </span>

      </p>




<pre>Since this comes up every once in a while I thought I&#39;d also share a &#34;legitimate&#34; case where this can happen.
Legitimate in the sense that the data being written is legitimately also being modified (keep reading), and _somewhat_ common because the setup _seems_ to make sense (initially):

Take a virtual machine, give it a disk - put the image on a software raid and tell qemu to disable caching (iow. use O_DIRECT, because the guest already does caching anyway).
Run linux in the VM, add part of the/a disk on the raid as swap, and cause the guest to start swapping a lot.

What *seems* to be happening is this: kernel decides to swap out part of some memory. At the same time the process it belongs to exits and the kernel marks the pages as unused - the swap write is still in flight. The kernel now knows that this area is unused and thus there is no reason to ever re-read it from the swap device. Someone else needs memory, the kernel gives &#39;em the affected pages. The swap write is still in flight. The new process starts using the memory, at this point we really don&#39;t care what kind of garbage data ends up being written to the disk, simply because we won&#39;t ever need it.
The swap writes finish. Now the raid is degraded.

The lesson: if you use software raid you kinda need to know the possible pitfalls you can run into...</pre>
    </div>

    <div id="c6">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c6">Comment 6</a>
        </span>

        <span>
          <span><span>bcs</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2018-03-26 10:25:06 UTC
        </span>

      </p>




<pre>Created <span><a href="https://bugzilla.kernel.org/attachment.cgi?id=274945&amp;action=diff" name="attach_274945" title="drbd copy of write bio">attachment 274945</a> <a href="https://bugzilla.kernel.org/attachment.cgi?id=274945&amp;action=edit" title="drbd copy of write bio">[details]</a></span>
drbd copy of write bio

We can confirm the testcase and the problem.

Therefore we developed a solution.

With this patch we solved the problem without impact of performance.

Feel free to participate on the solution without data corruption.</pre>
    </div>

    <div id="c7">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c7">Comment 7</a>
        </span>

        <span>
          <span><span>Melroy</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2023-03-25 23:28:25 UTC
        </span>

      </p>




<pre>Is this patch already delivered upstream or anything!? Or is this still not solved by default?</pre>
    </div>

    <div id="c8">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c8">Comment 8</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2023-05-07 10:54:36 UTC
        </span>

      </p>




<pre>hello, i&#39;d also be interested what&#39;s the status of this bug!?

i&#39; really curious why this exists for so long and getting so few notice

i bet there are a LOT people out there using virtual machines on top of mdraid and if this is broken, this should be either fixed or at least this should be known more widely

also see:
<a title="NEW - MD RAID or DRBD can be broken from userspace when using O_DIRECT" href="https://bugzilla.kernel.org/show_bug.cgi?id=99171">https://bugzilla.kernel.org/show_bug.cgi?id=99171</a></pre>
    </div>

    

    <div id="c10">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c10">Comment 10</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2023-05-08 20:53:51 UTC
        </span>

      </p>




<pre>there is no mention of O_DIRECT on that page

anyhow :

<a href="https://lkml.org/lkml/2007/1/10/235">https://lkml.org/lkml/2007/1/10/235</a>

&#34;So O_DIRECT not only is a total disaster from a design standpoint (just 
look at all the crap it results in)&#34;


<a href="https://lkml.org/lkml/2007/1/11/121">https://lkml.org/lkml/2007/1/11/121</a>

&#34;Yes. O_DIRECT is really fundamentally broken. There&#39;s just no way to fix 
it sanely. Except by teaching people not to use it, and making the normal 
paths fast enough &#34;


mhhh, apparently this still seems to be true !?</pre>
    </div>

    

    <div id="c12">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c12">Comment 12</a>
        </span>

        <span>
          <span><span>Stanislav German-Evtushenko</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2024-10-12 13:34:11 UTC
        </span>

      </p>




<pre>Roland Kletzing,

The test case from the description (<a title="NEW - MD RAID or DRBD can be broken from userspace when using O_DIRECT" href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c0">https://bugzilla.kernel.org/show_bug.cgi?id=99171#c0</a>) is still reproducible on Ubuntu 24.04, 6.8.0-45-generic. The original reason I started investigating this issue was that virtual machines running on top of DRBD with cache=none were sometimes hanging during live migration. I found out that this was caused by inconsistencies in underlying DRBD storage caused by VMs with cache=none writing to their swaps. I can&#39;t think of other (than swapping) examples of something modifying buffers while in flight. The artificial example was crafted to reproduce the case reliably as running a VM and waiting until swapping inside this VM causes the issue may take long.</pre>
    </div>

    <div id="c13">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c13">Comment 13</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2024-10-12 17:10:48 UTC
        </span>

      </p>




<pre>@stanislav, i can reproduce the problem with your tool, vbindiff showing difference. 

but consistency check via &#34;echo check &gt;/sys/block/md0/md/sync_action&#34; does NOT show those raid inconsistencies. 

any clue, why ?</pre>
    </div>

    <div id="c14">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c14">Comment 14</a>
        </span>

        <span>
          <span><span>Stanislav German-Evtushenko</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2024-10-15 01:04:33 UTC
        </span>

      </p>




<pre>You need to make sure you have permissions to /dev/md0 when running ./a.out /dev/md0 (it won&#39;t tell you if you don&#39;t).

<span>&gt; but consistency check via &#34;echo check &gt;/sys/block/md0/md/sync_action&#34; does
&gt; NOT show those raid inconsistencies.</span>

It shows them in my experiments:

echo check | sudo tee /sys/block/md0/md/sync_action
cat /sys/block/md0/md/mismatch_cnt
640</pre>
    </div>

    <div id="c15">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c15">Comment 15</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2024-10-15 20:23:34 UTC
        </span>

      </p>




<pre><span>&gt;but consistency check via &#34;echo check &gt;/sys/block/md0/md/sync_action&#34; does NOT
&gt;&gt;show those raid inconsistencies. </span>

i did wrong, i can see it now. 

i have tested a little bit further , and apparently with drbd_oos_test.c  from above, it even seems to be possible to degrade mdraid array from within a virtual machine.

this is what i did:

- place virtual disk on mdraid and add that disk to virtual machine
- ext4 format inside virtual machine
- mount inside virtual machine
- make the mountpoint writeable for a non-root user
- run drbd_oos_test on that mount as non-root user

even with this, you can make the raid going degraded.

that means, whoever is running a virtual machine with &#34;cache=none&#34; (=o_direct, which is default for proxmox but not probably not for other hypervisors - libvirt is using writeback for example),  imposes the risk that any malicious user inside a VM can degrade the raid array OUTSIDE the vm.

that means, any customer can provide sleepness nights to a hoster&#39;s sysadmin/storage admin.  

with this, i would consider the test case pretty valid, even if it&#39;s not the correct way to submit/handle data via o_direct. 

sorry, but i would really consider raid which can be broken from inside a VM (from userspace and via non-root user) fundamentally broken.</pre>
    </div>

    <div id="c16">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c16">Comment 16</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2024-10-16 21:03:49 UTC
        </span>

      </p>




<pre>btw - btrfs suffers from the same issue.  so you may expand the ticket subject to include btrfs @stanislav.

in this mail there is another btrfs specific testing tool for doing &#34;O_DIRECT write the wrong way&#34; <a href="https://lore.kernel.org/linux-btrfs/cf8a733f-2c9d-7ffe-e865-4c13d99dfb60@libero.it/">https://lore.kernel.org/linux-btrfs/cf8a733f-2c9d-7ffe-e865-4c13d99dfb60@libero.it/</a>

with that i i can (like above with mdraid) corrupt the btrfs raid on the host from inside a VM. as ordinary non-root user

$ ./a.out test.dat
main:  data = 0x72a57eee6000
write_thread pid = 12488
read_thread pid = 12489
update_thread pid = 12490
read_thread:  data = 0x72a57eee2000
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error
ERROR: read thread; e = 5 - Input/output error

pve-host:

# btrfs device stats -c /btrfs/
[/dev/sdf1].write_io_errs    0
[/dev/sdf1].read_io_errs     0
[/dev/sdf1].flush_io_errs    0
[/dev/sdf1].corruption_errs  2340
[/dev/sdf1].generation_errs  0
[/dev/sdh1].write_io_errs    0
[/dev/sdh1].read_io_errs     0
[/dev/sdh1].flush_io_errs    0
[/dev/sdh1].corruption_errs  2343
[/dev/sdh1].generation_errs  0


what&#39;s a little bit more problematic for me this time is, that proxmox is adding btrfs support to their proxmox virtual environment product (experimental status) , but they take hidden/invisible/intransparent countermeasure to  avoid O_DIRECT on btrfs (i.e. cache=none) 

<a href="https://forum.proxmox.com/threads/important-information-on-btrfs-getting-lost-in-wiki-wrong-vm-disk-defaults-with-btrfs-storage.143413/">https://forum.proxmox.com/threads/important-information-on-btrfs-getting-lost-in-wiki-wrong-vm-disk-defaults-with-btrfs-storage.143413/</a>
<a href="https://forum.proxmox.com/threads/virtual-disk-default-no-cache-settings-weirdness.143430/">https://forum.proxmox.com/threads/virtual-disk-default-no-cache-settings-weirdness.143430/</a>
<a href="https://bugzilla.proxmox.com/show_bug.cgi?id=5320">https://bugzilla.proxmox.com/show_bug.cgi?id=5320</a>

i just found out by chance, that this quirk to avoid O_DIRECT does only seem to apply when you freshly start a VM on btrfs storage - but not if you live migrate a running VM from another storage to btrfs storage. So the quirk is incomplete.

so any malicious user in a vm still can break the mirror and introduce inconsistency into the hosts raid.   

the problem with O_DIRECT is at least documented in the proxmox wiki - but with the same argument (problems with O_DIRECT), proxmox team don&#39;t like to support mdraid below virtual machines - which is a little bit inconsequent, imho - especially when taking into consideration, that mdraid is technology which exists since at least 1997. i.e. mdraid is at least 10 years older.

at least it&#39;s good to know that there are quirks applied (as btrfs in proxmox is it&#39;s own storage class, where proxmox creates btrfs subvolumes/images for each virtual disk), i.e. the hypervisor &#34;knows&#34; that you configured a VM with virtual disk on btrfs. 

for mdraid, you would need to use storage class/type &#34;dir&#34; on top of any filesystem/volume manager - for which proper detection and adding quirk would be difficult.

anyhow - the whole O_DIRECT stuff looks like one big mess to me. 

imho, for every storage technology/driver supporting it, imho we would perhaps be better off and more secure, to have it disabled per default on kernel/driver level and need to be forcefully enabled as boot-time or module param - for those people &#34;who know what they are doing and why the do need that (including all the issues&#34;.</pre>
    </div>

    <div id="c17">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c17">Comment 17</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2024-10-16 21:19:42 UTC
        </span>

      </p>




<pre><span>&gt;imho, for every storage technology/driver supporting it, imho we would perhaps
&gt;be better off and more secure, to have it disabled per default on
&gt;kernel/driver level and need to be forcefully enabled as boot-time or module
&gt;param - for those people &#34;who know what they are doing and why the do need
&gt;that (including all the issues&#34;.</span>

i&#39;m don&#39;t mean a global switch here but a switch for every filesystem driver / volume manager where unresolved O_DIRECT issues exist.</pre>
    </div>

    <div id="c18">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c18">Comment 18</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-02-14 09:38:43 UTC
        </span>

      </p>




<pre>yesterday i have tested proxmox with zfs 2.3 and was not able to break the mirror with drbd_oos_test.c

so the question is what zfs does right / does better then mdraid/drbd/btrfs regarding O_DIRECT, as we can see apparently it&#39;s possible to do right.</pre>
    </div>

    <div id="c19">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c19">Comment 19</a>
        </span>

        <span>
          <span>bat.men
</span>
        </span>

        <span>
        </span>

        <span>
          2025-02-18 17:20:34 UTC
        </span>

      </p>




<pre>(In reply to Roland Kletzing from <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c18">comment #18</a>)
<span>&gt; yesterday i have tested proxmox with zfs 2.3 and was not able to break the
&gt; mirror with drbd_oos_test.c
&gt; 
&gt; so the question is what zfs does right / does better then mdraid/drbd/btrfs
&gt; regarding O_DIRECT, as we can see apparently it&#39;s possible to do right.</span>

Hello :)

I am currently trying to solve or work around this kind of problem on proxmox 8.3.3 with btrfs.

From this message <a href="https://lore.kernel.org/linux-btrfs/09c963e3-7b70-e817-d634-3fa54d571f91@inwind.it/">https://lore.kernel.org/linux-btrfs/09c963e3-7b70-e817-d634-3fa54d571f91@inwind.it/</a>
I found a link to what the zfs people did in connection with the O_DIRECT bug - maybe that answers your question <a href="https://github.com/zfsonlinux/zfs/issues/224">https://github.com/zfsonlinux/zfs/issues/224</a> .

p.s. This is my first post here on bugzilla.kernel.org, I hope I did not violate any rules of behavior. If I did, I am grateful for correction.</pre>
    </div>

    <div id="c20">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c20">Comment 20</a>
        </span>

        <span>
          <span>bat.men
</span>
        </span>

        <span>
        </span>

        <span>
          2025-02-19 16:09:04 UTC
        </span>

      </p>




<pre>(In reply to bat.men from <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c19">comment #19</a>)
<span>&gt; (In reply to Roland Kletzing from <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c18">comment #18</a>)
&gt; &gt; yesterday i have tested proxmox with zfs 2.3 and was not able to break the
&gt; &gt; mirror with drbd_oos_test.c
&gt; &gt; 
&gt; &gt; so the question is what zfs does right / does better then mdraid/drbd/btrfs
&gt; &gt; regarding O_DIRECT, as we can see apparently it&#39;s possible to do right.
&gt; 
&gt; Hello :)
&gt; 
&gt; I am currently trying to solve or work around this kind of problem on
&gt; proxmox 8.3.3 with btrfs.
&gt; 
&gt; From this message
&gt; <a href="https://lore.kernel.org/linux-btrfs/09c963e3-7b70-e817-d634">https://lore.kernel.org/linux-btrfs/09c963e3-7b70-e817-d634</a>-
&gt; <a href="mailto:3fa54d571f91@inwind.it">3fa54d571f91@inwind.it</a>/
&gt; I found a link to what the zfs people did in connection with the O_DIRECT
&gt; bug - maybe that answers your question
&gt; <a href="https://github.com/zfsonlinux/zfs/issues/224">https://github.com/zfsonlinux/zfs/issues/224</a> .
&gt; 
&gt; p.s. This is my first post here on bugzilla.kernel.org, I hope I did not
&gt; violate any rules of behavior. If I did, I am grateful for correction.</span>

I guess i made a old reference to zfs issue 224, the O_DIRECT fix coming with zfs 2.3 Roland Kletzing maybe mentioned above, can be found here - i guess, sorry my mistake ;-) <a href="https://github.com/openzfs/zfs/pull/10018">https://github.com/openzfs/zfs/pull/10018</a></pre>
    </div>

    <div id="c21">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c21">Comment 21</a>
        </span>

        <span>
          <span><span>apassi</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-07-31 09:23:07 UTC
        </span>

      </p>




<pre>If the definition of the cache=writeback helps on virtuals mount, does it solve the issue made as root for the non-virtual host, if it assumed to be ok to lose the latest changes on cache?</pre>
    </div>

    <div id="c22">

      




<pre>(In reply to apassi from <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c21">comment #21</a>)
<span>&gt; If the definition of the cache=writeback helps on virtuals mount, does it
&gt; solve the issue made as root for the non-virtual host, if it assumed to be
&gt; ok to lose the latest changes on cache?</span>

Why should you avoid live migration or accept data corruption?
It would be better to fix the errors in the drivers instead of accepting further problems.</pre>
    </div>

    <div id="c23">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c23">Comment 23</a>
        </span>

        <span>
          <span><span>apassi</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-07-31 12:22:16 UTC
        </span>

      </p>




<pre>(In reply to bcs from <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c22">comment #22</a>)
<span>&gt; (In reply to apassi from <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c21">comment #21</a>)
&gt; &gt; If the definition of the cache=writeback helps on virtuals mount, does it
&gt; &gt; solve the issue made as root for the non-virtual host, if it assumed to be
&gt; &gt; ok to lose the latest changes on cache?
&gt; 
&gt; Why should you avoid live migration or accept data corruption?
&gt; It would be better to fix the errors in the drivers instead of accepting
&gt; further problems.</span>
+1 for the driver fix, i will wait that.

Until that, i would like to have some fix to use the mdraid safely. What i have understand, the zfs if currently only FS, which has made the fix for the o_direct issue, but i dont want to use the zfs, because what i have learn, it would probably require enterprise ssd:s, which costs lots of money.</pre>
    </div>

    <div id="c24">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c24">Comment 24</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-11 08:25:40 UTC
        </span>

      </p>




<pre>lvm raid is also affected from this problem

# lvs -o +raid_sync_action,raid_mismatch_count
  LV      VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert SyncAction Mismatches
  raidlv1 raidvg rwi-aor-m- 4.99g                                    63.81            check            2176</pre>
    </div>

    

    <div id="c26">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c26">Comment 26</a>
        </span>

        <span>
          <span><span>Roland Kletzing</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-15 22:50:49 UTC
        </span>

      </p>




<pre>i played around with bcachefs today and tested if i could break the raid via o_direct there, and did not succeed

this is what i did:

- installed proxmox pve9
- added pve-test repo
- installed latest 6.17 kernel
- installed latest bcachefs dkms + tools
- created mirrored bcachefs on 2 hdd partitions
- created datastore type &#34;dir&#34; on top of that
- put vm into that datastore with cache=none
- ran &#34;break-raid-odirect&#34; (see last post) inside vm
- shutdown vm
- pulled one of the disks, mounted bcachefs degraded on /bcachefs1
- md5summed vm files on that
- put back the disk and pulled the other one, again mounted bcachefs in degraded state on /bcachefs2
- md5summed the same files (i.e. the mirrored copy)

results:

 root@pve-hpmini-gen8:/bcachefs1/images/101# md5sum *
 8aadc84095dd250d7b2edd03de861366  vm-101-disk-0.qcow2
 
 root@pve-hpmini-gen8:/bcachefs2/images/101# md5sum *
 8aadc84095dd250d7b2edd03de861366  vm-101-disk-0.qcow2

so, apparently no inconsistencies here.</pre>
    </div>

    <div id="c27">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c27">Comment 27</a>
        </span>

        <span>
          <span><span>Kent Overstreet</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-16 13:48:45 UTC
        </span>

      </p>




<pre>Your test case is mixing O_DIRECT and buffered IO - loopback by default                                                                                                                                                                                                                                                           
uses buffered IO.                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                  
bcachefs has real locking between buffered IO and operations that bypass the                                                                                                                                                                                                                                                           
pagecache - the rest of the kernel was rather late to that party and                                                                                                                                                                                                                                                             
never added it for O_DIRECT io.</pre>
    </div>

    <div id="c28">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c28">Comment 28</a>
        </span>

        <span>
          <span><span>Qu Wenruo</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-16 20:29:20 UTC
        </span>

      </p>




<pre>It&#39;s not just mixing buffered IO and direct IO. For pure direct IO it&#39;s the same.

Just check the test case generic/761, which does direct IO write and modifies the buffer halfway.


If a fs with data checksum hits such case without proper protection, it will result data checksum mismatch, and future read will all fail.

Considering how bad the situation is, btrfs falls back to buffered IO so that the fs has the full control of the page cache if the inode has data checksum requirement.
Which has a very obvious performance drop, 1/5 ~ 1/10 performance on modern NVMEs.

But on the other hand, if the inode doesn&#39;t require checksum, and RAID chunks are involved, it will be the same problem as dm-raids, different data on different copies.

The root cause is that direct IO buffer is out of fs&#39; control, it can not be locked to prevent modification halfway.

IIRC HCH has proposed some ideas to do the proper locking, but not sure if that has any progress.</pre>
    </div>

    <div id="c29">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c29">Comment 29</a>
        </span>

        <span>
          <span><span>Kent Overstreet</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-16 21:19:14 UTC
        </span>

      </p>




<pre><span>&gt; It&#39;s not just mixing buffered IO and direct IO. For pure direct IO it&#39;s the
&gt; same.</span>

You&#39;re right, I didn&#39;t look closely enough the first time.

So bcachefs is actually only immune to this issue when data checksumming is enabled; if data checksumming is not enabled, we use the userspace buffer, same as any other filesystem.

If you&#39;re explicitly turning data checksumming off, you&#39;re probably more interested in performance than integrity.

If data checksumming is on, we just bounce.

And the thing is, bouncing is not that expensive in this case because doing the checksum requires touching that memory and pulling it into cache anyways.

Locking the userspace buffer sounds fairly nutso, changing page table mappings on each IO is going to be way more expensive.</pre>
    </div>

    <div id="c30">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c30">Comment 30</a>
        </span>

        <span>
          <span><span>Qu Wenruo</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-16 21:43:39 UTC
        </span>

      </p>




<pre>Yep, I think bcachefs and btrfs are on the same boat here.

So the only next feasible step is, to understand why page cache is so slow for the fallback-to-buffered case.

To be honest, modern hardware can handle memory copy in the tens of GiB/s range, the memory copy overhead should not be that huge, thus it must be something else wrong.

I think the memcopy itself is not the root cause of performance drop, but all the extra memory allocation and page cache dropping etc.


Maybe we can explore some middle ground solution, e.g. copy the direct IO buffer into a dedicated buffer, not page cache, then use that buffer to do proper zero-copy direct IO.

Hopes that can still get a reasonable performance other than the SATA speed level performance from page cache.</pre>
    </div>

    <div id="c31">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c31">Comment 31</a>
        </span>

        <span>
          <span><span>Kent Overstreet</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-16 21:48:10 UTC
        </span>

      </p>




<pre>No, btrfs and bcachefs are not in the same boat.

Falling back to buffered IO is not remotely the same as just bouncing the IO, buffered IO is significantly more costly, and has different semantics. You don&#39;t want to be mixing buffered and direct IO.

btrfs is also quite a bit looser than bcachefs, and will happily do O_DIRECT checksummed IO to the userspace buffer, meaning userspace can scribble over the buffer and cause checksum errors. bcachefs does not allow that.</pre>
    </div>

    <div id="c32">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c32">Comment 32</a>
        </span>

        <span>
          <span><span>Qu Wenruo</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-16 21:51:47 UTC
        </span>

      </p>




<pre>That O_DIRECT read is another bug that I have sent patches to fix, but the performance drop is too huge as it&#39;s going to do the same buffered fallback.

I&#39;ll check how the bouncing part works so that maybe btrfs can get rid of the buffered IO performance drop.</pre>
    </div>

    <div id="c33">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c33">Comment 33</a>
        </span>

        <span>
          <span><span>Qu Wenruo</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-16 22:59:43 UTC
        </span>

      </p>




<pre>Mind to share how bcachefs handles direct IO when there is data checksum requirement?

For the read part, it&#39;s the same bio_iov_iter_get_pages(), and I can not find any extra handling regarding data checksum during read. Thus it looks like what btrfs is doing. (AKA, user modifying the direct IO buffer can lead to csum mismatch)

For the write part it&#39;s the same, I failed to find any special handling related to checksums nor extra page copying to avoid the contents being modified halfway.

Or is the special handling in the data checksum part?</pre>
    </div>

    <div id="c34">

      <p><span>
          <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c34">Comment 34</a>
        </span>

        <span>
          <span><span>Kent Overstreet</span>
</span>
        </span>

        <span>
        </span>

        <span>
          2025-10-17 00:39:13 UTC
        </span>

      </p>




<pre>On Thu, Oct 16, 2025 at 10:59:43PM +0000, <a href="mailto:bugzilla-daemon@kernel.org">bugzilla-daemon@kernel.org</a> wrote:
<span>&gt; <a title="NEW - MD RAID or DRBD can be broken from userspace when using O_DIRECT" href="https://bugzilla.kernel.org/show_bug.cgi?id=99171">https://bugzilla.kernel.org/show_bug.cgi?id=99171</a>
&gt; 
&gt; --- <a href="https://bugzilla.kernel.org/show_bug.cgi?id=99171#c33">Comment #33</a> from Qu Wenruo (<a href="mailto:wqu@suse.com">wqu@suse.com</a>) ---
&gt; Mind to share how bcachefs handles direct IO when there is data checksum
&gt; requirement?
&gt; 
&gt; For the read part, it&#39;s the same bio_iov_iter_get_pages(), and I can not find
&gt; any extra handling regarding data checksum during read. Thus it looks like
&gt; what
&gt; btrfs is doing. (AKA, user modifying the direct IO buffer can lead to csum
&gt; mismatch)</span>

On read, we initially do read to the userspace buffer, but if we get a
checksum error we switch to bouncing.

<span>&gt; For the write part it&#39;s the same, I failed to find any special handling
&gt; related
&gt; to checksums nor extra page copying to avoid the contents being modified
&gt; halfway.
&gt; 
&gt; Or is the special handling in the data checksum part?</span>

You&#39;re looking in data/write.c, bch2_write_extent() - that&#39;s where
bouncing happens.

We do it for writes from the page cache too, because those can be
mapped into userspace as well...</pre>
    </div>

    


  

</td>
<td>
</td>
</tr></tbody></table>
  </div></div>
  </body>
</html>
