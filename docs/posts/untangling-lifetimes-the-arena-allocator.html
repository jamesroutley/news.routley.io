<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator">Original</a>
    <h1>Untangling Lifetimes: The Arena Allocator</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>In every instance when I’ve said that I prefer to write my software in C, the response is—normally—raised eyebrows. Several dominant memes in the programming world make my position unpopular, and thus uncommon to find. I regularly hear, “why would you write new code in an unsafe systems language?”, “performance isn’t everything!”, and perhaps the most common, </span><strong>“why subject yourself to the requirement of manually managing memory?”</strong><span>.</span></p><p><span>The perception that manual memory management is difficult to do, difficult to do </span><em>correctly</em><span>, and thus inherently bug-prone and unstable is common. Through my years in a university computer science program, this way of thinking was peddled repeatedly. Learning how to manage memory in C was an endeavor to “learn how things work”, strictly for academic purposes. The idea that anyone would ever </span><em>actually</em><span> do manual memory management in C </span><em>these days</em><span> was just unthinkable—I mean, after all, it’s </span><em>current year</em><span>, and everyone knows that once it’s </span><em>current year</em><span>, an arbitrary set of ideas that I don’t like become intrinsically false.</span></p><p><span>Education around memory management—as it was presented to me—was purely a historical, academic endeavor. How did the Linux kernel originally do memory management? Let’s do an assignment on that subject so you can see how </span><em>gross</em><span> it is! Eww, look at that </span><code>malloc</code><span>! Weird! Oh, don’t forget to </span><code>free</code><span> it! But don’t worry, kiddo; in next class, you can return to your “safe” and “managed” padded-room languages where bugs and instabilities are “impossible” (or so they claim).</span></p><p>After you’ve been exposed to the dark underworld of “manual memory management in C”, your professor will rescue you, and introduce automatic reference counting, RAII, and/or garbage collectors as the “solutions”.</p><p><span>As you may be able to tell from my tone, I think this way of thinking is nonsensical. It’s not that I think the “normal” methods of memory management in C are fine—in fact, quite the opposite—but instead it’s that I don’t think more complex compilers nor language features are required in order to find a dramatically better alternative. And, in keeping with a core principle of mine—maintaining the possibility of self-reliance through simple, rewritable systems, instead of increasingly complex tools—I think better (yet simpler) alternatives are </span><em>the only path forward</em><span>.</span></p><p><span>In this post, I’ll be presenting an alternative to the traditional strategy of manual memory management that I’ve had success with: the </span><em>arena allocator</em><span>. But first, let’s analyze “manual memory management in C” as it is normally presented—the classic </span><code>malloc</code><span> and </span><code>free</code><span> interface—and its consequences.</span></p><p><code>malloc</code><span>—which is short for “</span><strong>m</strong><span>emory </span><strong>alloc</strong><span>ate”—is an API to which you pass some number of bytes that you need for a dynamic allocation, and returns to you a pointer to a block of memory that supports that many validly-accessible bytes.</span></p><p><span>The symmetric counterpart to </span><code>malloc</code><span>, </span><code>free</code><span>, just expects a pointer that you got from </span><code>malloc</code><span>, and it guarantees that whatever block of memory that pointer points to will be made available for subsequent calls to </span><code>malloc</code><span>.</span></p><p><span>The </span><code>malloc</code><span> and </span><code>free</code><span> interface was built to support usage code that wants to dynamically allocate blocks of memory of arbitrarily-different </span><em>sizes</em><span>, with each allocation having an arbitrarily-different </span><em>lifetime</em><span>. There is no restriction on either of those two factors, meaning the following usages are all valid:</span></p><pre><code>void *p1 = malloc(512);
void *p2 = malloc(64);
void *p3 = malloc(1024*1024);
void *p4 = malloc(7);
free(p1);
free(p2);
free(p3);
free(p4);</code></pre><pre><code>void *p1 = malloc(512);
void *p2 = malloc(64);
void *p3 = malloc(1024*1024);
void *p4 = malloc(7);
free(p4);
free(p3);
free(p2);
free(p1);</code></pre><pre><code>void *p1 = malloc(512);
void *p2 = malloc(64);
void *p3 = malloc(1024*1024);
void *p4 = malloc(7);
// (no freeing whatsoever)</code></pre><pre><code>void *p1 = malloc(512);
free(p1);
void *p2 = malloc(64); // part of p1 used here?
void *p3 = malloc(1024*1024);
free(p2);
free(p3);
void *p4 = malloc(7); // part of p1/p2 used here?
free(p4);</code></pre><p><code>malloc</code><span> and </span><code>free</code><span>—as an </span><em>interface</em><span>—attempt to enforce </span><em>very little</em><span> on the calling code. They form an </span><em>entirely</em><span> </span><em>generic memory allocator</em><span>.</span></p><p><span>The primary—and quite understandable—criticism people have of </span><code>malloc</code><span> and </span><code>free</code><span>, or what they call “manual memory management in C”, is that using it for granular allocations with varying lifetimes across several layers in a codebase can easily lead to a rat’s nest of complexity. In these rat’s nests, it’s easy to accidentally </span><code>free</code><span> the same pointer twice, to access memory in a block that has already been </span><code>free</code><span>’d, to forget to </span><code>free</code><span> a pointer altogether (causing a leak), or to force your program to suffer computationally because of a need to </span><code>free</code><span> each small allocation in, for instance, a complex data structure with many nontrivial links between nodes.</span></p><p><span>The worst of these mistakes can lead to serious security and reliability issues. Imagine that memory is allocated with </span><code>malloc</code><span>, then freed </span><em>once</em><span>, then mistakenly freed again. An allocation may occur between the first and second call to </span><code>free</code><span>, which possibly reuses the already-released (at that point in time) memory. Because the usage rules of </span><code>malloc</code><span> and </span><code>free</code><span> have been broken, the allocator’s </span><em>implementation</em><span> and </span><em>user</em><span> disagree on an important detail—whether or not the allocation reusing portions of the first is allocated or not.</span></p><p>As I’ll present in this post, these rat’s nests can be avoided. But why, then, are they seemingly so common in C codebases, and why is there a dominant perception that they are unavoidable without more complex compiler and language features?</p><p><code>malloc</code><span> and </span><code>free</code><span> enforce </span><em>very little</em><span> on their usage code, so there is a </span><em>large space of possibilities</em><span> in how </span><code>malloc</code><span> and </span><code>free</code><span> are used (as the number of constraints increases, the number of solutions decreases). Many of those possibilities are the ever-common rat’s nests. The natural path for many C codebases is simply that of least (initial) resistance, which is to assume </span><code>malloc</code><span> and </span><code>free</code><span> as a suitable memory allocation interface (which is not necessarily unreasonable, given a lack of data), and so they will adopt it as a pattern. In the case of </span><code>malloc</code><span> and </span><code>free</code><span>, that means adopting a large space of possibilities—including the subset of those possibilities which include misuse, or explosions of complexity (“rat’s nests”).</span></p><p><span>There is a philosophy of abstraction in the programming world that believes in providing a certain desirable (for one reason or another) interface </span><em>irrespective</em><span> of the implications that interface has on its implementation. It is this philosophy that also claims that an interface can remain stable even with the implementation of the interface wildly changing. For some reason, it took several years before I became aware that the reality is precisely the opposite—an </span><em>interface</em><span> and its </span><em>implementation</em><span> are intrinsically related in subtle ways.</span></p><p><span>There is, of course, some degree to which an interface may remain stable with modifications to its implementation, but when the </span><em>nature of the implementation</em><span> must change, the </span><em>interface</em><span> must also fundamentally change, at least to avoid introducing unnecessary distortions to a problem (for instance, an interface being simply malformed for a given implementation, result in performance issues, bugs, underpowered APIs, and so on). The interface certainly hides </span><em>some details</em><span>, but it also </span><em>explicitly does not</em><span> hide others—in fact, an interface itself is </span><em>defined by</em><span> exposed guarantees or constraints, which both the user of the interface and the implementation must agree upon. There is certainly a more precise way to formally demonstrate this, but for now, I’ll leave it at that.</span></p><p><code>malloc</code><span> and </span><code>free</code><span> serve as a useful example of how an interface’s definition is closely related with both its usage patterns and implementation. The fine-grained control over an individual allocation’s size and lifetime—to allow for arbitrarily overlapping lifetimes with arbitrarily different sizes—result in very few constraints on the usage code. This causes, firstly, complications in the implementation of the allocator. While that causes performance problems (and thus has caused a popular meme that “dynamic allocation in a hot loop is bad”), the worst of the issues arise in the patterns relating to usage of a </span><code>malloc</code><span> and </span><code>free</code><span> style interface.</span></p><p>Iterating all of these patterns would be impossible, but I’ve gathered a few for this post to help illustrate the issue.</p><p><span>You’ll notice that—in all conversations about manual memory management in C—the </span><em>common case</em><span> of memory allocation is never discussed, because it mostly stays in the background, is trivial to use, and more-or-less works correctly and invisibly: the stack.</span></p><p><span>Sometimes, however, the stack is not an option—I’ll get more into that later. But when </span><code>malloc</code><span> and </span><code>free</code><span> (or equivalent) have been adopted as the default memory allocation pattern in a codebase, they are the first choice whenever the stack stops being an option. This is a common pattern in several C codebases in the wild, which—knowingly or not—have been corrupted by object-oriented thinking, even if their writers do not explicitly think that is the style of thinking they are using. When an individual allocation doesn’t work on the stack, for whatever reason, the author of the codebase in question is often taught that the only other option is to use “heap allocation”, which to most people means “use </span><code>malloc</code><span>”. So, instead of using the </span><em>stack allocator</em><span>, they will switch to the </span><em>extremely generic heap allocator, </em><span>often being unaware that these are not the only two choices.</span></p><p><span>As such, these codebases will have de facto objects, and these objects will generally have some initialization mechanism, and some deinitialization mechanism. If the codebase adopts a rule that suggests these objects may be allocated and deallocated in the same way an individual </span><code>malloc</code><span> allocation may be allocated and deallocated (which is the natural path of least resistance, which requires the fewest new possibly-bad assumptions), then the object’s interface will </span><em>wrap </em><span>the </span><code>malloc</code><span> and </span><code>free</code><span> interface, and follow the same rules pertaining to lifetimes:</span></p><pre><code>struct MyObject
{
  int x; // imagine some heavy weight stuff here!
};

MyObject *MyObjectAlloc(void)
{
  MyObject *obj = malloc(sizeof(MyObject));
  // other important initialization/mallocs happen here!
  return obj;
}

void MyObjectRelease(MyObject *object)
{
  // other important deinit/frees happen here!
  free(object);
}</code></pre><p><span>When the above style of interface becomes a </span><em>rule</em><span> within a codebase, it is frequently built </span><em>by default</em><span> without accounting for a number of actual constraints which may have otherwise simplified the problem:</span></p><ul><li><p><span>When does usage code actually need a </span><code>MyObject</code><span>?</span></p></li><li><p>How many of them does it need?</p></li><li><p><span>Is a </span><code>MyObject</code><span> only required given the presence of another object?</span></p></li><li><p>How easily can you predict how many are needed?</p></li><li><p>If multiple, are they freed all at once, or one at a time? In what order?</p></li><li><p><span>How important is it that a </span><code>MyObject</code><span> is released at all?</span></p></li><li><p><span>Is it important to be able to keep track of all </span><code>MyObject</code><span> allocations independently from other allocations?</span></p></li></ul><p><span>When such questions are ignored, the above pattern of wrapping a totally-generic allocation/deallocation interface becomes </span><em>common parlance</em><span> in a codebase, which leads to a proliferation of code that must assume full responsibility for finely-managing the lifetime of any individual object that doesn’t fit stack allocation (either because it has dynamic runtime requirements, </span><code>alloca</code><span> is not an option, it’s too large, or it doesn’t have lifetime requirements that fit the stack). This leads to this pattern being used even for </span><em>very granular “objects”</em><span>, which </span><em>explodes</em><span> the number of actual dynamic allocations and deallocations, and makes it far more likely that a programming mistake occurs. Pairing a single </span><code>malloc</code><span> with a single </span><code>free</code><span> is easy—pairing 1,000 </span><code>malloc</code><span>s with 1,000 </span><code>free</code><span>s—especially when many of those individual </span><code>malloc</code><span>s have dependencies on others—is dramatically more difficult to write </span><em>once</em><span>, and especially more difficult to </span><em>maintain overtime</em><span>.</span></p><p><span>If the problem of managing 1,000 (or 10,000, or 100,000) various lifetimes wasn’t enough of a problem for you, now consider that these lifetimes often have complex </span><em>dependencies</em><span> on one another. Eventually, there is a </span><em>graph</em><span> of lifetimes, each node (lifetime) in which relies on certain assumptions about some number of other lifetimes. “Object A”, within its own lifetime, will refer to “Object B”—care, then, must be taken to ensure that, for instance, “Object B” is not freed before it is accessed through “Object A”.</span></p><p><span>If there is no organizing principle around managing these relationships and their corresponding lifetimes, through a number of subtle mistakes (that often do not arise immediately) a codebase quickly turns into a sludge, where important work is constantly deferred behind bugfixing or maintenance work—or, worse, where important work occurs </span><em>before</em><span> bugfixing and maintenance work, and thus bugs and maintenance issues accumulate over time.</span></p><p><span>What a memory leak </span><em>literally is</em><span> on a modern computer is very often glossed over in programming education. It is very frequently perceived as a scary no-no—if your program has a leak, it’s a bad program, and you’re a bad programmer, and you will go to programmer hell!</span></p><p><span>With this perception, programmers will often carefully </span><code>free</code><span> allocations in their program </span><em>to the point of religiosity</em><span>, even when doing so is </span><em>strictly worse</em><span> than never writing a single line of cleanup code.</span></p><p><span>To clear this up, I’ll first explain—literally—what happens when you allocate something with </span><code>malloc</code><span> on a computer these days, and then subsequently what happens when you fail to call </span><code>free</code><span> with a pointer returned to you from </span><code>malloc</code><span>.</span></p><p><span>When you first call </span><code>malloc</code><span>, you’re ultimately just calling a function that was implemented by whoever wrote the implementation of the C runtime library that you’re using. The person who wrote that code had a task—implement a dynamic memory allocator, given the constraints in the C specification. So, their job is to return you a pointer to a block of memory that’s at least as large as what you asked for, and then to also be able to release that memory (make it available for re-allocation in a later </span><code>malloc</code><span>) in the </span><code>free</code><span> implementation.</span></p><p><span>On a modern machine like the computer or phone you’re reading this on, at some point the allocator will ask the </span><em>operating system</em><span> for memory dynamically. Part of the utility of the C runtime library is, at the end of the day, simply abstracting over operating-system-specific code. To do this, it will request that the operating system maps new pages into the relevant virtual address space (by calling an operating system API, like </span><code>VirtualAlloc</code><span> on Windows). The pointer that </span><code>malloc</code><span> returns to you is not </span><em>literally an address</em><span> of any physical memory—it’s instead an address in your own </span><em>virtual address space</em><span>. The operating system, then, manages a mapping data structure, called a “page table”, which maps </span><em>virtual</em><span> addresses into </span><em>physical</em><span> addresses.</span></p><p><span>Being called by </span><code>VirtualAlloc</code><span> (or similar), the operating system will make adjustments to that mapping data structure, which changes the mapping between sections of a virtual address space and physical pages in memory. After that is complete, the operating system—whenever it chooses to schedule your program’s thread—can then prepare the memory management unit (MMU) accordingly, so that whenever your thread asks about an address, it can map to the appropriate physical address.</span></p><p><span>The main point being, whenever a process crashes (hits a hardware-level exception, like a page fault) or normally exits, the operating system continues running (or, at least, it had better), and has a whole understanding of what pages in your program’s virtual address space were mapped to physical memory addresses. So the </span><em>operating system</em><span> is able—and really, required, in the presence of code that cannot be trusted to never crash—to “release” the physical pages that your process originally asked for.</span></p><p><span>So, tying that all together, what happens when you never call </span><code>free</code><span> with a pointer that was returned to you by </span><code>malloc</code><span>? The first obvious point is that—of course—the </span><code>malloc</code><span>/</span><code>free</code><span> allocator fails to ever see the pointer again, and so it assumes it is still “allocated” by the usage code, which means that memory can never be reused again for another </span><code>malloc</code><span> allocation. </span><em>That</em><span> is what is called a “memory leak”. On the program exiting or crashing, no resources are truly “leaked”.</span></p><p><span>Now, to clarify, a leak may actually be a problem. For instance, if you’re building a program that runs “forever” to interact with the user through a graphical user interface, and on every frame, you call </span><code>malloc</code><span> several times and never </span><code>free</code><span> the memory you allocate, your program will leak some number of bytes </span><em>per frame</em><span>. Considering that—at least in dynamic scenarios—your program will be chugging through a frame around 60, 120, 144 times per second, that leak will likely add up fairly quickly. It’s possible that such a leak would prohibit normal usage of the program—for instance, after 30 minutes, an allocation failure occurs, or the operating system spends far too much time paging in memory from disk to allow your program’s memory usage to continuously grow.</span></p><p><span>On the other hand, however, a leak is very often </span><em>not a problem</em><span>—for example, when allocating memory that needs to be allocated for the duration of the program, or if you expect the program to only boot up and perform a task, then to close.</span></p><p><span>In other words, you may treat the operating system as “the ultimate garbage collector”—</span><code>free</code><span>ing memory when it is unnecessary will simply waste both your and the user’s time, and lead to code complexity and bugs that would otherwise not exist. Unfortunately, many popular programming education resources teach that cleanup code is </span><em>always necessary</em><span>. This is false.</span></p><p><span>The </span><code>malloc</code><span> and </span><code>free</code><span> interface is symmetric—for each call to </span><code>malloc</code><span> that returns a pointer to memory that must be freed, there will be one call to </span><code>free</code><span>. So for instance, if memory for an entity in a game world is </span><code>malloc</code><span>’d when the entity is spawned, then it will be </span><code>free</code><span>’d when the entity is killed or deleted.</span></p><p><span>A problem arises when the manner in which memory is acquired does not match the manner in which it is ultimately released. For example, if I load a level in my game engine’s level editor, and for each entity that is spawned some number of </span><code>malloc</code><span>s occur, and for each entity I spawn while editing the level some more </span><code>malloc</code><span>s occur, then I want to unload that level and load a different one, then there is no “free all of the memory I allocated for this level” button. Each </span><code>malloc</code><span> must receive its symmetric </span><code>free</code><span>, and so I must iterate all of the entities and </span><code>free</code><span> each result that was returned to me from </span><code>malloc</code><span>.</span></p><p><span>There is no “</span><code>malloc</code><span> checker” in C compilers, and there is no obvious element of incorrectness that comes from forgetting to </span><code>free</code><span> something in such a codepath (unlike the element of incorrectness you’d quickly find on forgetting to </span><em>allocate</em><span> something), and so it’s quite easy to allow the “free everything” path to become out-of-sync with the “allocate one thing” path. This leads to both bugs and leaks—and, of course, it contributes to the sum-total of code required to implement something.</span></p><p><span>The above patterns are surely not exhaustive, but they hopefully provide a decent picture of how the </span><em>nature of an interface</em><span> like </span><code>malloc</code><span> and </span><code>free</code><span> can spiral into an out-of-control mess, where bugs and leaks regularly arise. This mess will ultimately result in a decline in software quality, an increase in iteration time, and thus a decline in one’s ability to meaningfully work on a project (other than purely maintaining its inertia—for instance, by fixing a leak reported by a customer).</span></p><p><span>One attempted solution to this problem found in the modern programming world is to introduce compiler and language features to </span><em>automatically generate</em><span> “inconvenient code”—in this case, that being the code responsible for correctly calling initialization and deinitialization code (which may include allocations and deallocations).</span></p><p><span>This is done in the C++ world through RAII—whenever an object goes out of scope, either by being initially allocated on the stack, explicitly allocated (in C++, through </span><code>new</code><span> and </span><code>delete</code><span>), or by being within another object that is going out of scope, some code will automatically be called, which is responsible for cleanup (the </span><em>destructor</em><span>). When an object’s lifetime </span><em>starts</em><span>, then some other code will automatically be called, which is responsible for initialization (the </span><em>constructor</em><span>). </span></p><p><span>The constructor and destructor of an object mark the beginning and ending (respectively) of that object’s </span><em>lifetime</em><span>. The fact that this has the same overhead that </span><code>malloc</code><span> and </span><code>free</code><span> had is not relevant—it is purely trying to automate the generation of some code by assuming initialization and deinitialization are coupled with an object’s </span><em>lifetime</em><span>. This, of course, does not eliminate all possible bugs (misuse is still possible, and often not checkable in a language like C++)—so, this idea is often paired in newer languages with heavier (and more complex) compile-time checking features, which attempt to both automate this code generation, and prohibit misuse.</span></p><p><span>Another attempted solution is </span><em>garbage collection</em><span>, which is a large enforcement structure that tracks everything and interrupts productive work in order to perform its function (much like a government agency, except in this case, the garbage collector is ostensibly doing something approximating useful work—although both function by stealing valuable resources involuntarily). A garbage collector will periodically interrupt a running program—which is running normally—to explore the set of individual </span><code>malloc</code><span>-style allocations (objects) and find which of them are still being referenced somewhere in live data structures, thus detecting the termination of some allocation </span><em>lifetimes</em><span>, and being able to release those allocations. In many cases, garbage collectors do actually perform their function, although nevertheless it’s still possible to produce de facto leaks, by mistakenly holding an unnecessary reference to an object, which prohibits the garbage collector from releasing it.</span></p><p><span>The above solution attempts see the problem as fundamentally an </span><em>automation</em><span> or </span><em>checking</em><span> problem. It isn’t simply that memory management was being approached in an entirely wrong way—instead, the </span><code>malloc</code><span> and </span><code>free</code><span> rat’s nest is simply a part of the memory management problem’s intrinsic nature, and so </span><em>tooling</em><span> must simply aid the programmer in making fewer mistakes.</span></p><p><span>This view follows quite naturally from another aspect of modern programming thinking (and education), which claims many problems are </span><em>gross</em><span> and </span><em>complex</em><span>, and thus we need </span><em>abstraction</em><span> to make them </span><em>appear simpler</em><span>. It’s not, advocates of this philosophy claim, that the problems themselves should be simplified—they are, on the other hand, </span><em>intrinsically complex</em><span>, and it is the job of tooling to make them appear less complex.</span></p><p><span>I don’t agree with this view for a number of reasons. Firstly, complexity hidden by an interface (be it an API or a tool’s controls) does not simply disappear—it can be detected through performance problems, subtle bugs, and a lack of composability. Secondly, there is not a single </span><em>user</em><span> and a single </span><em>producer</em><span> in computing—the ecosystem is a complex graph of interdependent problems. There are “leaf nodes” in this graph, where produced software has no dependents—this would be, for instance, a game that never has any of its code reused. But the </span><em>vast majority</em><span> of the graph is producing software that must be composed with other software in unpredictable ways, and that production occurs by composing other </span><em>dependency</em><span> software in unpredictable ways. So </span><em>complexity</em><span> introduced at any point in this graph does not simply disappear—instead, it </span><em>compounds</em><span>. It is no coincidence that modern software—after many decades of cruft accumulation and software composition—seems to be slower, buggier, less reliable, and more frustrating to use than it should be.</span></p><p><span>Nevertheless, this view is dominant in the programming sphere at large, and as such is responsible for a host of solutions that don’t mind </span><em>adding</em><span> complexity to the problem.</span></p><p><span>My approach, on the other hand, is this: instead of </span><em>assuming</em><span> that </span><code>malloc</code><span> and </span><code>free</code><span> were the correct low-level operations, we can </span><em>change the memory allocation interface</em><span>—tweaking what the user and implementation agree on—to simplify the problem and eliminate many of the problems found in the traditional </span><code>malloc</code><span> and </span><code>free</code><span> style of memory management.</span></p><p><span>What, exactly, does that approach look like? To begin understanding it, let’s take a look at another style of memory management in C that </span><em>does not</em><span> have the same problems that </span><code>malloc</code><span> and </span><code>free</code><span> do: the stack.</span></p><p><span>As I mentioned earlier, the primary focus of criticism on memory management in C is on the </span><code>malloc</code><span> and </span><code>free</code><span> interface, and </span><em>not</em><span> the stack. That is for a good reason—using the stack correctly is remarkably simple. Misusing it is, of course, still possible. But after a new C programmer learns a few simple rules, it’s not particularly difficult to avoid almost all mistakes.</span></p><p><span>With stack allocation, the idea is simple: multiple allocation lifetimes—all using a single block of memory—may be in-flight at a single time, but the </span><em>end</em><span> of a lifetime may </span><em>never cross</em><span> the beginning of another lifetime. This means that several </span><em>nested</em><span> allocation lifetimes may exist, but it is not an entirely arbitrary timeline of overlapping allocation lifetimes (as in the case of </span><code>malloc</code><span> and </span><code>free</code><span>).</span></p><p>This rule can be clearly visualized by looking at virtually any CPU profiler, many of which make use of what’s called a “flame graph”:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png" width="786" height="209" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/d6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:209,&#34;width&#34;:786,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:4239,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6b30c2a-bcdc-428c-a200-800614adb51e_786x209.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>Each block in the above picture corresponds with one of these lifetimes. Once a lifetime has been entered, its </span><em>parent lifetime</em><span> cannot end until </span><em>it first ends</em><span>.</span></p><p><span>When a new lifetime opens—delimited by the </span><code>{</code><span> symbol in C’s syntax—that signifies a new “scope” on the stack. When a lifetime closes—delimited by the </span><code>}</code><span> symbol—the lifetime began by the corresponding </span><code>{</code><span> symbol is terminated. Any variables declared within those two symbols “belong” to that lifetime.</span></p><p><span>The syntax makes the rule of a lifetime’s </span><em>end</em><span> not crossing another lifetime’s </span><em>beginning</em><span> quite clear. The following is a valid case of multiple lifetimes:</span></p><pre><code>// lifetime A
{
  int x = 0; // an integer is allocated in this scope
  int y = 0; // another integer is allocated in this scope

  // lifetime B
  {
    int z = 0; // an integer is allocated in this scope
  } // end of lifetime B

} // end of lifetime A</code></pre><p>But the following idea is not valid (and cannot even be expressed within the grammar):</p><pre><code>// lifetime A
{
  int x = 0; // an integer is allocated in this scope
  int y = 0; // another integer is allocated in this scope

  // lifetime B
  {
    int z = 0; // an integer is allocated in this scope

} // end of lifetime A
  
  } // end of lifetime B</code></pre><p><span>The reasons why this cannot be expressed in C’s grammar are clear when considering the compiler tasked with parsing the above text. Because whitespace is insignificant in C, the first </span><code>}</code><span> encountered will be identified as closing </span><code>lifetime B</code><span>, and so it can not be interpreted as ending </span><code>lifetime A</code><span>.</span></p><p><span>The inability of the grammar to express that concept is ultimately irrelevant, though (you can imagine a language’s syntax that </span><em>does</em><span> make such a concept expressible)—what’s more interesting is the implications that this rule has on the “stack allocator”.</span></p><p><span>First what’s notable is that the concept of a </span><em>lifetime</em><span> has become detached from individual allocations, and is now delimited </span><em>independently</em><span> from allocations. When an allocation occurs (a variable is declared on the stack), its lifetime is chosen by virtue of which scope it is placed within. This is unlike </span><code>malloc</code><span> and </span><code>free</code><span>, which offers per-individual-allocation lifetime control. This allows an individual lifetime to be used to group </span><em>many </em><span>allocations into a single common case. Note that, additionally, this fits the “asymmetric allocation and deallocation” pattern I mentioned earlier—allocations can occur within a scope in sporadic ways, but all end together simultaneously (quite similar to the aforementioned “unloading a level in my game’s level editor” example).</span></p><p>Secondly, because of the rule that lifetimes within the stack may only exist entirely within other lifetimes, implementing a stack allocator is trivial. All you need is a single block of memory and a single integer:</p><pre><code>U8 *stack_memory = ...;
U64 stack_alloc_pos = ...;

// allocating 64 bytes on the stack:
void *ptr = stack_memory + stack_alloc_pos; // `ptr` points to 64 bytes
stack_alloc_pos += 64;

// popping 64 bytes from the stack:
stack_alloc_pos -= 64;</code></pre><p><span>To create a “sub-lifetime”, the “allocation position” (</span><code>stack_alloc_pos</code><span>) needs to simply be remembered </span><em>before</em><span> the sub-lifetime begins, and then it must be restored when the sub-lifetime ends:</span></p><pre><code>U8 *stack_memory = ...;
U64 stack_alloc_pos = ...;

U64 restore_stack_alloc_pos = stack_alloc_pos;
{
  // in here, increment stack_alloc_pos as needed!
}
stack_alloc_pos = restore_stack_alloc_pos;</code></pre><p><span>You can also imagine “freeing” </span><em>everything</em><span> in the entire stack block, just by setting </span><code>stack_alloc_pos</code><span> to </span><code>0</code><span>.</span></p><p><span>The above is expressed in C syntax, but if you dig into what stack allocation </span><em>actually means</em><span> at the assembly level, you’ll find that C’s stack is implemented much like this. Notice how computationally trivial it is to perform both allocations and deallocations on the stack.</span></p><p><span>That is all well and good, and it’s great that the stack is so simple, and that it’s trivial to use it. But the stack is not an option in many cases—that is, after all, why </span><code>malloc</code><span> and </span><code>free</code><span> are often used as “the alternative”.</span></p><p><em>Why is the stack not an option in some cases, though</em><span>?</span></p><p>Let’s form an example where the stack would simply not work. Let’s start with the simplest example:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png" width="386" height="209" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:209,&#34;width&#34;:386,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:3036,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7645f757-da4b-47b6-b700-fa051c3fd0fb_386x209.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>In the above image, I’m </span><em>beginning </em><span>a lifetime at the red </span><code>{</code><span> character, and </span><em>ending</em><span> that lifetime at the red </span><code>}</code><span> character. This clearly breaks the rules of the stack, because the lifetime I am attempting to form does not fit within the </span><em>parent lifetimes</em><span> I am attempting to begin or end the lifetime within.</span></p><p><span>Note that </span><em>this lifetime can still be expressed</em><span> in this timeline! I would simply need to begin the lifetime </span><em>several ancestor scopes higher</em><span>, and end the lifetime at that same level. But, doing that is often </span><em>still</em><span> not an option, because of </span><em>composability</em><span>.</span></p><p><span>In the below picture, the top red rectangle delimits </span><em>one layer</em><span> of code, and the bottom delimits </span><em>another layer</em><span> of code. Imagine that the top layer is some high-level application code, and the bottom layer is a helper library for parsing a file format.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png" width="702" height="194" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/ac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:194,&#34;width&#34;:702,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:5725,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fac7969bb-7204-46da-91b1-1edc93cd52b0_702x194.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>Memory allocation is almost always coupled with the particular details of a task, and this is especially true in parsing, so in order for the </span><em>application code </em><span>(top) to call into the </span><em>parsing code</em><span> (bottom), the </span><em>application code</em><span> would need to do all of the memory allocation, somehow without doing any of the work involved in usefully using or filling that memory.</span></p><p><span>While that may be in principle feasible (even though in many practical scenarios, it’s </span><em>not</em><span> feasible), we can still form a case that is </span><em>not</em><span> theoretically feasible at all, by introducing another overlapping lifetime:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png" width="702" height="194" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:194,&#34;width&#34;:702,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:4708,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9bb24b7d-6cc2-4bd8-b694-858cbb910a82_702x194.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>In this case, it’s impossible to keep the same order of events, while keeping both lifetimes in the same stack. In cases like this, the stack stops being an option.</p><p><span>But now, consider this: can you tackle the same lifetime problems as above, but </span><em>with the ability to construct as many independent stacks as you need</em><span>? The answer being “yes, of course”—now, solving each problem is trivial. Some lifetimes must simply belong to different stacks than others.</span></p><p>This is the approach of the arena allocator: take the absurdly simple linear allocator, which offers lightning fast allocation and deallocation, eliminating per-allocation freeing requirements, first being proved out by the stack, and make that a formal allocator concept—the “arena”. Usage code can make as many arenas as necessary, and choose them at will for specific allocations.</p><p>An arena allocator’s fundamental API, then, may look like this:</p><pre><code>// create or destroy a &#39;stack&#39; - an &#34;arena&#34;
Arena *ArenaAlloc(void);
void ArenaRelease(Arena *arena);

// push some bytes onto the &#39;stack&#39; - the way to allocate
void *ArenaPush(Arena *arena, U64 size);
void *ArenaPushZero(Arena *arena, U64 size);

// some macro helpers that I&#39;ve found nice:
#define PushArray(arena, type, count) (type *)ArenaPush((arena), sizeof(type)*(count))
#define PushArrayZero(arena, type, count) (type *)ArenaPushZero((arena), sizeof(type)*(count))
#define PushStruct(arena, type) PushArray((arena), (type), 1)
#define PushStructZero(arena, type) PushArrayZero((arena), (type), 1)

// pop some bytes off the &#39;stack&#39; - the way to free
void ArenaPop(Arena *arena, U64 size);

// get the # of bytes currently allocated.
U64 ArenaGetPos(Arena *arena);

// also some useful popping helpers:
void ArenaSetPosBack(Arena *arena, U64 pos);
void ArenaClear(Arena *arena);</code></pre><p><span>Notice that, </span><em>at the limit</em><span>, this allocator becomes equivalent to </span><code>malloc</code><span> and </span><code>free</code><span>. To see this, consider that each </span><code>malloc</code><span> and </span><code>free</code><span> can simply be identified as beginning and ending their own little “stack”, where it’s only used for a single allocation. Using an arena in such a way—while it works—is not where this style of allocator will make an obvious difference.</span></p><p><span>The key point, I’ve found, is that </span><em>in virtually every case</em><span>, programs do not operate at such a limit. You don’t form a new scope for each new variable you declare on the stack—similarly, you don’t need an arena for each new allocation with a stack-breaking lifetime requirement. In almost every case, </span><em>a large number of allocations</em><span> can be bucketed into the </span><em>same arena</em><span>. And in those cases, once your arenas are set up accordingly, the requirement to </span><em>deallocate any allocation</em><span> disappears (other than the deallocation of the arena in its entirety, if required). Once you’ve performed an allocation, you’ve chosen an arena, and by virtue of that, the allocation’s corresponding memory will be made available again in accordance with the </span><em>arena’s</em><span> overarching lifetime.</span></p><p><span>By getting </span><em>just a bit more organized</em><span> about which arena we choose for an allocation, we’ve freed ourselves from the burden of </span><code>free</code><span>ing all of our dynamic allocations. We’ve also made it much easier to, for instance, track memory usage in our application, or bucket all allocations for a particular purpose, which may be useful for debugging or performance—we now have a fairly obvious path to determine which arena a given allocation is </span><em>within</em><span>, and to free </span><em>all</em><span> allocations in any arena we choose, irrespective of </span><em>who</em><span> pushed </span><em>what</em><span> onto the arena, </span><em>when</em><span>, and </span><em>in what order</em><span>.</span></p><p>A very high level description of an arena is “a handle to which allocations are bound”. When an allocation occurs, it is “bound” to an “arena handle”. This makes it easily expressible to, for instance, clear all allocations “bound” to an “arena handle”.</p><p>One useful property of arenas is that they gracefully propagate through several layers in a codebase. This occurs through the parameterization of codepaths with the arena they use to perform allocations.</p><pre><code>// the function is *asking the user* where to allocate
ComplexStructure *MakeComplexStructure(Arena *arena);</code></pre><p>It is trivial, then, to identify which functions are performing allocations. And because, in an API like the above, the arena is a required parameter, the caller must choose an arena, and thus determine the lifetime of any persistent allocations.</p><p><span>For instance, imagine that there is a file format which encodes a tree structure (like JSON, or </span><a href="https://dion.systems/metadesk" rel="">Metadesk</a><span>). A parser for that file format will produce a complex tree structure, perhaps with pointers linking various tree nodes together. With arenas, that parsing API can be a function that takes </span><code>(Arena*, String8)</code><span> → </span><code>(Node)</code><span> (where a single </span><code>Node</code><span> contains pointers to, for instance, its children nodes). The </span><em>usage code</em><span> passes an arena where it’d like the results allocated, instead of—for instance—having to carefully free the resultant structure at a later time. By choosing an arena, the freeing work is complete.</span></p><p><span>This simplifies </span><em>all codepaths</em><span> in this system. The parsing code becomes simpler, because it does not have to have any cleanup code whatsoever.</span><em> </em><span>The calling code becomes simpler, because it does not have to manage the lifetime of the parsed tree independently. And finally, the allocator code itself remains nearly trivial, and lightning fast.</span></p><p><span>Contrast this with a </span><code>malloc</code><span>-style interface propagated throughout the system. Because of the allocator’s </span><em>assumption</em><span> of arbitrary lifetimes and arbitrary sizes, the allocator code itself becomes more complex (it is a generic heap-style allocator); the calling code becomes more complex (it must manage the lifetime of the parsed structure); and the parser code becomes more complex (it must provide a careful freeing routine). So, this is not merely a </span><em>choice</em><span>—the arena solution is simpler and faster by </span><em>every important metric</em><span>.</span></p><p><span>As I’ve mentioned, a key concept behind the arena is </span><em>grouping lifetimes together</em><span>. But that, sometimes, is not as simple as it seems.</span></p><p><span>For instance, imagine I’ve allocated storage for 1,000 entities in my game’s level editor on an arena. But now, I’d like to go and </span><em>remove one</em><span> out of the middle. If each entity were </span><code>malloc</code><span>’d, then all I would need to do is </span><code>free</code><span> the allocation for the entity in the middle. But if that storage is directly on an arena, how might that work?</span></p><p><span>Recall what “freeing memory” </span><em>literally means</em><span>—it means informing whatever allocator you allocated memory from that </span><em>the memory you allocated is now available</em><span> for future allocations, and you don’t plan on using anymore (at least, before allocating something new).</span></p><p><span>This desired behavior—of reusing memory that has been released—is still possible with an arena. To demonstrate the concept, I’ll provide a simple </span><em>growable pool allocator</em><span> implementation </span><em>that composes with an arena</em><span>.</span></p><p><span>A pool allocator only offers allocations of a fixed size, but with each allocation having an arbitrarily-different lifetime from all other allocations. So, it’s much simpler to implement than a fully generic </span><code>malloc</code><span>-style allocator, which makes it useful to demonstrate my point of “compositions with arenas”. That being said, keep in mind that </span><code>malloc</code><span>-style allocators can </span><em>still</em><span> be implemented as a composition with an arena—I’ve implemented a number of more sophisticated allocators as compositions with arenas, including </span><code>malloc</code><span>-style allocators, a quad-tree allocator, and others.</span></p><p>But in any case, here is what a simple pool allocator will look like:</p><pre><code>struct Entity
{
  Entity *next;
  Vec2F32 position;
  Vec2F32 velocity;
  // some more stuff in here
};

struct GameState
{
  Arena *permanent_arena;
  Entity *first_free_entity;
};

Entity *EntityAlloc(GameState *game_state)
{
  // first, try to grab the top of the free list...
  Entity *result = game_state-&gt;first_free_entity;
  if(result != 0)
  {
    game_state-&gt;first_free_entity = game_state-&gt;first_free_entity-&gt;next;
    MemoryZeroStruct(result);
  }

  // if the free list was empty, push a new entity onto the arena
  else
  {
    result = PushStructZero(game_state-&gt;permanent_arena, Entity);
  }

  return result;
}

void EntityRelease(GameState *game_state, Entity *entity)
{
  // releasing -&gt; push onto free list. next allocation
  // will take the top of the free list, not push onto
  // the arena.
  entity-&gt;next = game_state-&gt;first_free_entity;
  game_state-&gt;first_free_entity = entity;
}</code></pre><p>Arenas are an extremely versatile building block. To aid in furthering understanding of just how versatile they are, I’ve gathered a number of scenarios—perhaps non-obvious to some readers—in which an arena clicks perfectly into place.</p><p><span>In games and graphical applications, programs are organized at a top-level by a loop. This loop performs the same operations repeatedly, in order to communicate to the user, and receive information from the user. On each iteration of this loop, a </span><em>frame</em><span> is produced.</span></p><p><span>It’s very common to have per-frame concepts in this scenario. For instance, you might be building a user interface every frame, or producing a batch of drawing commands. In such cases, it’s useful to have a lifetime that exists for the duration of a single frame. This can easily be implemented with an arena. The arena itself would be allocated permanently, but at the beginning of each frame, it has its allocation counter reset to 0 (e.g. via the </span><code>ArenaClear</code><span> API I described above). So, if in application code you’d like to dynamically allocate a few complicated strings for the purposes of rendering, you can simply use the frame arena, and know that the memory will be released at an appropriate time.</span></p><p><span>It’s also very common to refer to the </span><em>previous frame’s</em><span> state on any given frame. This can be done with a simple extension to the “frame arena” idea—instead of having a single frame arena, have </span><em>two</em><span>, and switch between them each frame (in effect, “double-buffering” the frame arena). This allows safe references to the prior frame’s state.</span></p><p>You may now see that the stack, as it is normally understood in C, is almost like a specialized per-thread arena. It’s very common to use the stack as a sort of “scratch arena”, by “pushing” (declaring) a temporary buffer in a local scope, and using it to do some useful work. While that is ultimately true, there are a few limitations of the stack that make it underpowered.</p><p><span>First of all, the stack is coupled with the </span><em>call stack</em><span>—so it’s not possible to, for example, push something onto the stack, and return back to a function’s caller, while keeping that allocation there (without making some brittle assumptions that inevitably break).</span></p><p><span>Secondly, the stack cannot compose with any code you’ve written that uses arenas. So if you’d like to, for instance, call a function with a signature like </span><code>Node *ParseString(Arena *arena, String8 string)</code><span>, there isn’t really a way to tell the function to use the </span><em>stack</em><span>—it is written to use </span><code>arena</code><span>.</span></p><p><span>This is where </span><em>per-thread scratch arenas</em><span> become useful. These are simply thread-local arenas, which can be retrieved at any time. They can then be used with the “temporary sub-lifetime” trick I introduced earlier, with an additional API layered over the core arena API:</span></p><pre><code>struct ArenaTemp
{
  Arena *arena;
  U64 pos;
};

ArenaTemp ArenaTempBegin(Arena *arena); // grabs arena&#39;s position
void ArenaTempEnd(ArenaTemp temp);      // restores arena&#39;s position</code></pre><p><span>The API to retrieve a scratch arena, then, can </span><em>almost </em><span>be the following:</span></p><pre><code>ArenaTemp GetScratch(void); // grabs a thread-local scratch arena
#define ReleaseScratch(t) ArenaTempEnd(t)</code></pre><p>Although the above is not quite sufficient. The reason why is quite subtle, but let me explain with an example:</p><pre><code>void *FunctionA(Arena *arena)
{
  ArenaTemp scratch = GetScratch();
  void *result = PushArrayZero(arena, U8, 1024);
  // fill result...
  ReleaseScratch(scratch);
  return result;
}

void FunctionB(void)
{
  ArenaTemp scratch = GetScratch();
  void *result = FunctionA(scratch.arena);
  // use result
  ReleaseScratch(scratch);
}</code></pre><p><span>Notice that, in the above case, if there is only a single per-thread scratch arena, both </span><code>FunctionA</code><span> </span><em>and</em><span> </span><code>FunctionB</code><span> grab the </span><em>same scratch arena</em><span>. The difference is that, in </span><code>FunctionA</code><span>, it is treating </span><code>scratch.arena</code><span> </span><em>differently</em><span> than </span><code>arena</code><span>. </span><code>scratch.arena</code><span> is an arena it’s using for temporary work that it expects to disappear before returning—</span><code>arena</code><span> is where it is allocating </span><em>persistent results</em><span> for the caller.</span></p><p><span>But, because </span><code>scratch.arena</code><span> </span><em>is </em><code>arena</code><span>, the result memory is actually freed before returning to the caller.</span></p><p><span>So, another rule must be adopted. When </span><code>GetScratch</code><span> is called, it must take any arenas being used for </span><em>persistent</em><span> allocations, to ensure that it returns a different arena, to avoid mixing </span><em>persistent</em><span> allocations with </span><em>scratch</em><span> allocations. The API, then, becomes the following:</span></p><pre><code>ArenaTemp GetScratch(Arena **conflicts, U64 conflict_count); // grabs a thread-local scratch arena
#define ReleaseScratch(t) ArenaTempEnd(t)</code></pre><p><span>If only a single “persistent” arena is present at any point in any codepath (e.g. a caller never passes in two arenas), then you will not need more than two scratch arenas. Those two scratch arenas can be used for arbitrarily-deep call stacks, because each frame in any call stack will </span><em>alternate</em><span> between using a single arena for persistent allocations, and the other for scratch allocations.</span></p><p><span>This issue is, as I said, very subtle, and it does require an extra rule that makes scratch arenas </span><em>slightly more complex</em><span> to use correctly. But, the day-to-day habit of using scratch arenas is still not particularly difficult to follow: “If you want a scratch arena, and an arena is already in scope for persistent allocations, then pass it into </span><code>GetScratch</code><span>. If you </span><em>introduce</em><span> an arena parameterization into an already-written codepath, then find all instances of </span><code>GetScratch</code><span> and update them accordingly”.</span></p><p><span>As I’ll explain in the </span><em>Implementation Notes</em><span>, an arena implementation can have a strategy for </span><em>growing</em><span> and </span><em>shrinking</em><span> (while not relocating existing allocations). This makes them useful for storing collections of information where the size is not known upfront.</span></p><p><span>Depending on the exact growth strategy, this either makes arenas a suitable </span><em>replacement </em><span>for dynamic arrays, </span><em>or</em><span> a perfect implementation of one. Because the specifics of this style of arena usage are tightly coupled with implementation details, I’ll just mention this for now, and hopefully the </span><em>Implementation Notes</em><span> will illuminate the parts I’m glossing over at the moment.</span></p><p>I’ve already introduced the basic mechanism for implementing an arena, when I described how the stack works. An arena works in precisely the same way (although you can play with the details—for example, auto-aligning allocations).</p><p><span>There is one aspect I’ve not yet covered, however, which is </span><em>what happens when the arena runs out of storage</em><span>. There are a number of strategies one might employ in tackling this scenario, depending on their particular case.</span></p><p>One strategy is to simply pre-allocate a single fixed-size block of memory for an arena, and abort when storage runs out, because the project has strict memory usage requirements (if it’s, for example, on an embedded device).</p><p><span>The more common case, though, is that you’re writing code for, say, a modern consumer desktop computer, or a modern game console. In this case, it’s much easier to have a strategy for </span><em>growing the arena’s storage</em><span> when it runs out.</span></p><p><span>One strategy is to spread the arena across a variably-sized linked list of large blocks. If there is not enough room for a new allocation on an arena, your implementation falls back to asking the operating system for a new block, and then begins allocating on the new block. When the arena’s allocation position is pushed far back enough, it can return to the previous block. This notably, eliminates the guarantee of </span><em>memory contiguity</em><span> within the arena, which makes the arena unusable for implementing a dynamic array, because a dynamic array can be trivially accessed with a single offset into a single block. In this case, to access an arbitrary offset in the storage, you first would need to scan the chain of blocks. This is likely not prohibitively expensive, and the exact performance characteristics can be tweaked by changing the block size—but it does mean using it is more nuanced than just a single block of memory.</span></p><p><span>Another fancier strategy is to take advantage of modern MMUs and 64-bit CPUs (which are virtually ubiquitous these days). On modern PCs, for instance, it’s likely that you have 48 bits (256 terabytes!) of virtual address space at your disposal. This means, functionally, that you can still have arenas </span><em>both</em><span> grow dynamically, </span><em>and </em><span>work with just a single block (and thus maintain the guarantee of memory contiguity). Instead of committing </span><em>physical storage</em><span> for the entire block upfront (e.g. by using </span><code>malloc</code><span> for the storage), the implementation simply </span><em>reserves</em><span> </span><em>the</em><span> </span><em>address range</em><span> in your virtual address space (e.g. by using </span><code>VirtualAlloc</code><span> on Windows). Then, when the arena reaches a new page in the </span><em>virtual address space range </em><span>that has not been backed by physical memory, it will request more physical memory from the operating system. In this case, you still must decide on an upper-bound for your arena storage, but because of the power of exponentiation, this upper-bound can be ridiculously large (say, 64 gigabytes). So, in short, reserve a massive upper-bound of contiguous virtual address space, then commit physical pages as needed.</span></p><p>Finally, it’s easy to compose all of the above strategies, or make them all available in various scenarios, through the same API. So, don’t assume they’re mutually exclusive!</p><p><span>Before choosing any strategy, carefully consider which platforms you’re writing for, and the real constraints on your solution. For example, my understanding is that the Nintendo Switch has 38 bits of virtual address space. The magic growing arena can still </span><em>work</em><span> here, but you’ll have a tighter constraint on address space.</span></p><p>Learning how to work with arenas entirely revolutionized my experience with writing code in C. I almost never think about memory management, these days—it is not particularly more cumbersome than writing in a garbage-collected scripting language. Unlike such a language, however, I know where my memory is coming from, and when it’ll be “released”, and what that even means.</p><p><span>Making memory management dramatically easier didn’t take a gigantic, complex compiler, it didn’t take a garbage collector, and it didn’t require sacrificing contact with the lower level. It was, actually, precisely the opposite—it came through </span><em>harmonizing</em><span> the lower level requirements </span><em>with</em><span> the higher level requirements, and considering the problem from first principles. It is for this reason that I strongly dislike the average modern programmer’s conception of “high-level vs. low-level”, and how the “lower level details” are considered gross, inconvenient, and annoying. The lower level details are nothing to shy away from—they’re a necessary part of the reality of the problem, and by giving them their due time, both high-level code and low-level code can benefit.</span></p><p>I hope this post provided a useful expansion to your programming toolbox. Good luck programming!</p><p><em><strong>Note: </strong><span>The application template repository in my </span><a href="https://git.rfleury.com" rel="">Code Depot</a><span>—available only to paid subscribers—contains an example arena implementation following the strategies mentioned in this post.</span></em></p><p>If you enjoyed this post, please consider subscribing. Thanks for reading.</p><p>-Ryan</p></div></div></div></article></div></div></div>
  </body>
</html>
