<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simedw.com/2026/01/31/ear-pronunication-via-ctc/">Original</a>
    <h1>Show HN: I trained a 9M speech model to fix my Mandarin tones</h1>
    
    <div id="readability-page-1" class="page"><div>
          <p><span>TL;DR:</span> Mandarin pronunciation has been hard for me, so I took ~300 hours of transcribed speech and trained a small CTC model to <em>grade</em> my pronunciation. You can try it <a href="https://simedw.com/projects/ear">here</a>.</p>
<p>In my previous post about <a href="https://simedw.com/2025/12/15/langseed/">Langseed</a>, I introduced a platform for defining words using only vocabulary I had already mastered. My vocabulary has grown since then, but unfortunately, people still struggle to understand what I&#39;m saying.</p>
<p>Part of the problem is tones. They&#39;re fairly foreign to me, and I&#39;m bad at hearing my own mistakes, which is deeply frustrating when you don’t have a teacher.</p>
<h2 id="first-attempt-pitch-visualisation">First attempt: pitch visualisation</h2>
<p>My initial plan was to build a pitch visualiser: split incoming audio into small chunks, run an FFT, extract the dominant pitch over time, and map it using an energy-based heuristic, loosely inspired by Praat. </p>
<p>But this approach quickly became brittle. There were endless special cases: background noise, coarticulation, speaker variation, voicing transitions, and so on.</p>
<p>And if there’s one thing we’ve learned over the last decade, it’s the <em>bitter lesson</em>: when you have enough data and compute, learned representations usually beat carefully hand-tuned systems.</p>
<p>So instead, I decided to build a <strong>deep learning–based Computer-Assisted Pronunciation Training (CAPT)</strong> system that could run entirely on-device. There are already commercial APIs that do this, but hey, where’s the fun in that?</p>
<video controls="" width="100%">
  <source src="videos/ear-lite.mp4" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

<h2 id="architecture">Architecture</h2>
<p>I treated this as a specialised Automatic Speech Recognition (ASR) task. Instead of just transcribing text, the model needs to be pedantic about <em>how</em> something was said.</p>
<p>I settled on a <strong>Conformer encoder</strong> trained with <strong>CTC (Connectionist Temporal Classification)</strong> loss.</p>
<h2 id="why-conformer">Why Conformer?</h2>
<p>Speech is weird: you need to catch both local and global patterns:</p>
<ol>
<li>
<p><strong>Local interactions</strong></p>
</li>
<li>
<p><strong>Global interactions</strong></p>
</li>
</ol>
<p>Conformers combine both: convolution for local detail, attention for global structure.</p>
<h2 id="why-ctc">Why CTC?</h2>
<p>Most modern ASR models (e.g. Whisper) are sequence-to-sequence: they turn audio into the <em>most likely text</em>. The downside is they&#39;ll happily auto-correct you.</p>
<p>That’s a feature for transcription, but it’s a bug for language learning. If my tone is wrong, I don’t want the model to guess what I <em>meant</em>. I want it to tell me what I actually said.</p>
<p>CTC works differently. It outputs a probability distribution for every frame of audio (roughly every 40 ms). To handle alignment, it introduces a special <code>&lt;blank&gt;</code> token.</p>
<p>If the audio is &#34;hello&#34;, the raw output might look like:</p>
<div><pre><span></span><code>h h h &lt;blank&gt; e e &lt;blank&gt; l l l l &lt;blank&gt; l l o o o
</code></pre></div>

<p>Collapsing repeats and removing blanks gives <code>hello</code>. This forces the model has to deal with what I actually said, frame by frame.</p>
<h2 id="forced-alignment-knowing-when-you-said-it">Forced alignment: knowing <em>when</em> you said it</h2>
<p>CTC tells us <em>what</em> was said, but not exactly <em>when</em>.</p>
<p>For a 3-second clip, the model might output a matrix with ~150 time steps (columns), each containing probabilities over all tokens (rows). Most of that matrix is just <code>&lt;blank&gt;</code>.</p>
<p>If the user reads &#34;Nǐ hǎo&#34; (ni3, hao3), we expect two regions of high probability: one for <code>ni3</code>, one for <code>hao3</code>.</p>
<p>We need to find a single, optimal path through this matrix that:</p>
<ul>
<li>Starts at the beginning</li>
<li>Ends at the end</li>
<li>Passes through <code>ni3</code> → <code>hao3</code> in order</li>
<li>Maximises total probability</li>
</ul>
<video controls="" width="100%" autoplay="" muted="" playsinline="">
  <source src="videos/viterbi_animation.mp4" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

<p>This is exactly what the <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi algorithm</a> computes, using dynamic programming. </p>
<h2 id="tokenisation-pinyin-tone-as-first-class-tokens">Tokenisation: Pinyin + tone as first-class tokens</h2>
<p>Most Mandarin ASR systems output Hanzi. That hides pronunciation errors, because the writing system encodes meaning rather than pronunciation.</p>
<p>Instead, I created a token for every <strong>Pinyin syllable + tone</strong>:</p>
<ul>
<li><code>zhong1</code> is one token  </li>
<li><code>zhong4</code> is a completely different token</li>
</ul>
<p>If I say the wrong tone, the model explicitly predicts the wrong token ID.</p>
<p>I also normalised the neutral tone by forcing it to be tone 5 (<code>ma5</code>). This resulted in a vocabulary of <strong>1,254 tokens</strong>, plus <code>&lt;unk&gt;</code> and <code>&lt;blank&gt;</code>.</p>
<h3 id="training">Training</h3>
<p>I combined the <strong>AISHELL-1</strong> and <strong>Primewords</strong> datasets (~300 hours total), augmented by SpecAugment (time/frequency masking). On 4× NVIDIA GeForce RTX 4090s, training took about 8 hours. 
Instead of obsessing over loss, I mostly focused on these metrics: </p>
<ol>
<li>TER (Token Error Rate): overall accuracy.</li>
<li>Tone Accuracy: accuracy over tones 1-5.</li>
<li>Confusion Groups: errors between difficult initial pairs like zh/ch/sh vs z/c/s.</li>
</ol>
<p><img alt="Validation TER score during training" src="https://simedw.com/2026/01/31/ear-pronunication-via-ctc/images/ear_val_ter.png"/>
<img alt="Validation Tone confusion matrix during training" src="https://simedw.com/2026/01/31/ear-pronunication-via-ctc/images/ear_val_tone_confuse_matrix.png"/></p>
<h2 id="honey-i-shrank-the-model">Honey, I shrank the model</h2>
<p>I started with a &#34;medium&#34; model (~75M parameters). It worked well, but I wanted something that could run in a browser or on a phone without killing the battery.</p>
<p>So I kept shrinking it, and I was honestly surprised by how little accuracy I lost:</p>
<table>
<thead>
<tr>
<th># Parameters</th>
<th>TER</th>
<th>Tone accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>75M</td>
<td>4.83%</td>
<td>98.47%</td>
</tr>
<tr>
<td>35M</td>
<td>5.16%</td>
<td>98.36%</td>
</tr>
<tr>
<td>9M</td>
<td>5.27%</td>
<td>98.29%</td>
</tr>
</tbody>
</table>
<p>The 9M-parameter model was barely worse. This strongly suggests the task is <strong>data-bound, not compute-bound</strong>.</p>
<p>The FP32 model was ~37 MB. After INT8 quantisation, it shrank to ~11 MB with a negligible accuracy drop (+0.0003 TER). Small enough to load instantly via <code>onnxruntime-web</code>.</p>
<h2 id="alignment-bug-silence-ruins-everything">Alignment bug: silence ruins everything</h2>
<p>To highlight mistakes, we need forced alignment. But I hit a nasty bug with <strong>leading silence</strong>.</p>
<p>I recorded myself saying &#34;我喜欢…&#34; and paused for a second before speaking.
The model confidently told me my first syllable was wrong.
Confidence score: 0.0.</p>
<p>Why?</p>
<p>The alignment assigned the silent frames to <code>wo3</code>. When I averaged probabilities over that span, the overwhelming <code>&lt;blank&gt;</code> probability completely drowned out <code>wo3</code>.</p>
<h3 id="the-fix">The fix</h3>
<p>I decoupled <strong>UI spans</strong> (what gets highlighted) from <strong>scoring frames</strong> (what contributes to confidence).</p>
<p>We simply ignore frames where the model is confident it’s seeing silence:</p>
<div><pre><span></span><code><span>def</span><span> </span><span>_filter_nonblank_frames</span><span>(</span><span>span_logp</span><span>:</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>blank_id</span><span>:</span> <span>int</span> <span>=</span> <span>0</span><span>,</span> <span>thr</span><span>:</span> <span>float</span> <span>=</span> <span>0.7</span><span>):</span>
<span>    </span><span>&#34;&#34;&#34;</span>
<span>    Only keep frames where the probability of &lt;blank&gt; is below a threshold.</span>
<span>    If we filter everything (total silence), we fall back to scoring the whole span.</span>
<span>    &#34;&#34;&#34;</span>
    <span>p_blank</span> <span>=</span> <span>span_logp</span><span>[:,</span> <span>blank_id</span><span>]</span><span>.</span><span>exp</span><span>()</span>
    <span>keep</span> <span>=</span> <span>p_blank</span> <span>&lt;</span> <span>thr</span>
    <span>if</span> <span>keep</span><span>.</span><span>any</span><span>():</span>
        <span>return</span> <span>span_logp</span><span>[</span><span>keep</span><span>]</span>
    <span>return</span> <span>span_logp</span>  <span># Fallback</span>
</code></pre></div>

<p>This single change moved my confidence score for the first syllable from <strong>0.0 → 0.99</strong>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I can already feel my pronunciation improving while beta testing this. It’s strict and unforgiving, exactly what I needed.</p>
<p>Native speakers, interestingly, complained that they had to over-enunciate to get marked correct. That’s likely a domain-shift issue: AISHELL is mostly read speech, while casual speech is faster and more slurred. Kids do poorly too: their pitch is higher, and they&#39;re basically absent from the training data. Adding conversational datasets like Common Voice feels like the obvious next step.</p>
<p>You can try the live demo <a href="https://simedw.com/projects/ear">here</a>. It runs entirely in your browser. The download is ~13MB, still smaller than most websites today. </p>

        </div></div>
  </body>
</html>
