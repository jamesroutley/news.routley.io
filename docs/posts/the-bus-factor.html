<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mclare.blog/posts/the-bus-factor">Original</a>
    <h1>The Bus Factor</h1>
    
    <div id="readability-page-1" class="page"><div id="observablehq-main">

<blockquote>
<p>From <a href="https://en.wikipedia.org/wiki/Bus_factor" target="_blank" rel="noopener noreferrer">Wikipedia</a>:
The bus factor is a measurement of the risk resulting from information and capabilities not being shared among team members, derived from the phrase &#34;in case they get hit by a bus&#34;. It is also known as the bus problem, truck factor, bus/truck number or circus factor.</p>
</blockquote>
<h2 id="motivation" tabindex="-1"><a href="#motivation">Motivation</a></h2>
<p>Every company I&#39;ve worked at (structural and software) has at some point raised the issue of the &#34;bus factor&#34; in managing project development.</p>
<p>As a structural engineer, this was immensely difficult to estimate because our deliverables were so spread across employees, and documentation was scarce. The only time it became evident was after someone had quit, and there was an urgent RFI (request for information) six months later on someone&#39;s calculation package (though often the official calculation package would have the EOR&#39;s name, rather than the design engineers directly responsible for the calculations). Following such incidents, there would be promises of better documentation, which would invariably fall by the wayside as all team members migrated to new projects without any debriefing. I&#39;ve seen 100% turnover in design engineers on long term projects, so this is one heck of an antipattern.</p>
<p>As a software engineer, there are a lot of parallels in the industry, but by the nature of the work, the deliverables of shipped code <em>are</em> one way to measure the bus factor. At least that&#39;s what a number of researchers have examined, including <a href="https://ieeexplore.ieee.org/abstract/document/7503718" target="_blank" rel="noopener noreferrer">this paper</a>, which has quite a number of citations (156 according to google scholar!) since it was first published in 2016 (with a <a href="https://peerj.com/preprints/1233v1.pdf" target="_blank" rel="noopener noreferrer">preprint</a> made available in 2015). <a href="https://www.scannedinavian.com/the-github-plugin-my-coworkers-asked-me-not-to-write.html" target="_blank" rel="noopener noreferrer">Shae</a> sent me the paper, and once we discovered the <a href="http://aserg.labsoft.dcc.ufmg.br/truckfactor/" target="_blank" rel="noopener noreferrer">original data</a> <em>and</em> the <a href="https://github.com/aserg-ufmg/Truck-Factor" target="_blank" rel="noopener noreferrer">source code</a> was readily available, it was the perfect candidate for a weekend project to at least get an idea of interesting open source metrics.</p>
<h2 id="methods" tabindex="-1"><a href="#methods">Methods</a></h2>
<p>The first step was to see if the source code would build at all. The algorithm itself hasn&#39;t been changed since 2018, but I rarely build anything with Java. Fortunately, the README instructions worked, for the most part. There are <code>docker</code> commands displayed in the same instructions as the local build, but I opted to just do a local build.</p>
<p>Some of the commands were finicky about calling directories. In particular, the <code>commit_log_script.sh</code> needs to be run while inside the <code>scripts</code> directory, as otherwise you&#39;ll get this error:</p>
<pre data-language="sh"><code>awk: can<span>&#39;t open file /Users/mclare/workspaces/Truck-Factor/gittruckfactor/log.awk
 source line number 1 source file /Users/mclare/workspaces/Truck-Factor/gittruckfactor/log.awk
</span></code></pre>
<p>Then, to run the program itself:</p>
<pre data-language="sh"><code>mvn package
<span>cd</span> Truck-Factor/gittruckfactor
java -jar ./target/gittruckfactor-1.0.jar ~/workspaces/numpy ~/workspaces/numpy
</code></pre>
<p>The output (if successful) will look something like this:</p>
<pre data-language="sh"><code>TF = 9 (coverage = 48.74%)
TF authors (Developer;Files;Percentage):
Charles Harris;206;15.28
Bas van Beek;204;15.13
Sebastian Berg;142;10.53
Sayed Adel;138;10.24
Travis Oliphant;116;8.61
Rohit Goswami;87;6.45
Mateusz Soko≈Ç;82;6.08
David Cournapeau;75;5.56
Matti Picus;70;5.19
</code></pre>
<p>After trying this for a single package, I wanted to run this against <em>all</em> the repos in the original paper, which involved  <a href="https://www.gnu.org/software/parallel/" target="_blank" rel="noopener noreferrer"><code>parallel</code></a> and some other command line tools.</p>
<p>I figured out all the repos from the paper using the accompanying website, which loads interactive D3 graphics based on downloaded CSVs. All I needed to do was extract the first column of the loaded CSV, which I saved in a file (<code>repo_list.txt</code>) for later use.</p>
<h4 id="cloning-repos" tabindex="-1"><a href="#cloning-repos">Cloning Repos</a></h4>
<pre data-language="sh"><code>parallel -j 8 git <span>clone</span> ::: $(<span>cat</span> ../meta/repo_list.txt)
</code></pre>
<p>These repos are about a 64 gb download!</p>
<p>Cloning the repos was resource intensive. Despite passing a job count of 8, the cloning/building process ate almost all of my CPU resources immediately.</p>
<p><img src="https://mclare.blog/_file/posts/the-bus-factor/htop_output.f569bf87.png" alt="HTOP Output"/></p>
<h4 id="running-the-analysis" tabindex="-1"><a href="#running-the-analysis">Running the analysis</a></h4>
<pre data-language="sh"><code><span>ls</span> ../cloned_repos | xargs -I {} <span>echo</span> <span>&#34;java -jar ./target/gittruckfactor-1.0.jar /Users/mclare/Truck-Factor/cloned_repos/{} {} &gt; results_linguist/{}.txt&#34;</span> | parallel -j 8
</code></pre>
<p>I recommend running shell scripts with `echo` initially to double check that the output is what you expect to run.</p>
<p>Running this took only about 4 minutes, thanks to the parallelization (one repo, <code>platform_framework_base</code> was the lagging repo by about 2 minutes).</p>
<h2 id="initial-research-questions" tabindex="-1"><a href="#initial-research-questions">Initial Research Questions</a></h2>
<p>In the interest of finishing this in the 2 day time frame, the initial research questions for this exploration were as follows:</p>
<ul>
<li>How does running the analysis with <code>linguist</code> change the results?</li>
<li>What do we expect to see from the results for these packages after eight years?</li>
</ul>
<h2 id="results" tabindex="-1"><a href="#results">Results</a></h2>
<p>When running the analysis with <code>linguist</code>, I did see some significant changes in terms of code coverage, and the truck factor itself. Overall, I expected <code>linguist</code> to reduce the bus factor, since it should remove non-critical files, such as documentation. However, in a few cases, the <code>linguist</code> run analysis didn&#39;t return <em>any</em> bus factor. In other cases, the bus factor actually <em>increased</em>. This was surprising to me, and merits further analysis of what <code>linguist</code> is doing, and which files it&#39;s removing in those repos.</p>
<p>I expected that the bus factor would <em>decrease</em> in many of the repos that were tracked. The past decade has shown how difficult it is to be an open source maintainer, especially given the current economic climate. The most surprising bus factor change was in the Linux source repo. The original paper had a bus factor of 57, which reduced to 12 in my initial analysis, and 8 in the <code>linguist</code> analysis. About 30% saw no change at all (mostly those with a bus factor of 1), 15% increased by 1 (though those were TF 1-3 to begin with), and 20% decreased by 1 or more.</p>
<p>You can explore the results yourself by selecting two different data sets in the dropdowns.</p>







<p>The following plot shows a grouped bar chart of the three different data sets, which also shows that there haven&#39;t been significant improvements overall for reducing the number of low bus factor projects</p>

<h2 id="further-work" tabindex="-1"><a href="#further-work">Further Work</a></h2>
<p>This is really just an initial exploration into how tools like this can be used to assess the health of open source projects.</p>
<p>One missing aspect that jumped out to me is that this only looks at <strong>authorship</strong> as a metric, rather than <strong>review</strong>. One would hope that for significant changes, the code review process would result in shared knowledge, but I don&#39;t know if this is captured, or can be easily captured from git logs.</p>
<p>Other research questions for another day:</p>
<ul>
<li>Can we replicate the results of the initial paper? (Not easily, the date of github pulls is not indicated in the supporting data or original research paper)</li>
<li>What can be gained from setting developer aliases, as indicated in the <a href="https://github.com/aserg-ufmg/Truck-Factor" target="_blank" rel="noopener noreferrer">repo optional settings</a>? Will this potentially increase the bus factor by merging contributions from the same person?</li>
<li>Can we replicate the other <a href="http://aserg.labsoft.dcc.ufmg.br/truckfactor/authorship.html" target="_blank" rel="noopener noreferrer">graphs</a> from the source data utilizing other data from the github API like number of developers?</li>
<li>Can we better understand the degree of authorship (DOA) metric? There seem to be some magic numbers that <em>might</em> indicate some kind of linear regression was used. This appears to possibly come from other papers.</li>
</ul>
<h2 id="other-notes" tabindex="-1"><a href="#other-notes">Other notes</a></h2>
<ul>
<li>A different perspective of this experiment can be found on <a href="https://www.scannedinavian.com/the-github-plugin-my-coworkers-asked-me-not-to-write.html" target="_blank" rel="noopener noreferrer">Shae&#39;s blog</a></li>
<li>I used <a href="https://observablehq.com/plot/" target="_blank" rel="noopener noreferrer">Observable Plot</a> to generate the graphs. Ironically, D3 is one of those repos with an unchanging bus factor of 1!</li>
</ul>
</div></div>
  </body>
</html>
