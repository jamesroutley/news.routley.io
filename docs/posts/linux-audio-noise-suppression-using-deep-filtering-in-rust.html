<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Rikorose/DeepFilterNet">Original</a>
    <h1>Linux Audio Noise suppression using deep filtering in Rust</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">A Low Complexity Speech Enhancement Framework for Full-Band Audio (48kHz) using on Deep Filtering.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/16517898/225623209-a54fea75-ca00-404c-a394-c91d2d1146d2.svg"><img src="https://user-images.githubusercontent.com/16517898/225623209-a54fea75-ca00-404c-a394-c91d2d1146d2.svg" alt="deepfilternet3"/></a></p>
<h3 tabindex="-1" dir="auto"><a id="user-content-demo" aria-hidden="true" href="#demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Demo</h3>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description DeepFilterNet-Demo-new.mp4">DeepFilterNet-Demo-new.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/16517898/240178609-79679fd7-de73-4c22-948c-891927c7d2ca.mp4" data-canonical-src="https://user-images.githubusercontent.com/16517898/240178609-79679fd7-de73-4c22-948c-891927c7d2ca.mp4" controls="controls" muted="muted">

  </video>
</details>

<h3 tabindex="-1" dir="auto"><a id="user-content-news" aria-hidden="true" href="#news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>News</h3>
<ul dir="auto">
<li>
<p dir="auto">New DeepFilterNet Demo: <em>DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement</em></p>
<ul dir="auto">
<li>Paper: <a href="https://arxiv.org/abs/2305.08227" rel="nofollow">https://arxiv.org/abs/2305.08227</a></li>
<li>Video: <a href="https://youtu.be/EO7n96YwnyE" rel="nofollow">https://youtu.be/EO7n96YwnyE</a></li>
</ul>
</li>
<li>
<p dir="auto">New Multi-Frame Filtering Paper: <em>Deep Multi-Frame Filtering for Hearing Aids</em></p>
<ul dir="auto">
<li>Paper: <a href="https://arxiv.org/abs/2305.08225" rel="nofollow">https://arxiv.org/abs/2305.08225</a></li>
</ul>
</li>
<li>
<p dir="auto">Real-time version and a LADSPA plugin</p>
<ul dir="auto">
<li><a href="#deep-filter">Pre-compiled binary</a>, no python dependencies. Usage: <code>deep-filter audio-file.wav</code></li>
<li><a href="https://theleo.zone/Rikorose/DeepFilterNet/blob/main/ladspa">LADSPA plugin</a> with pipewire filter-chain integration for real-time noise reduction on your mic.</li>
</ul>
</li>
<li>
<p dir="auto">DeepFilterNet2 Paper: <em>DeepFilterNet2: Towards Real-Time Speech Enhancement on Embedded Devices for Full-Band Audio</em></p>
<ul dir="auto">
<li>Paper: <a href="https://arxiv.org/abs/2205.05474" rel="nofollow">https://arxiv.org/abs/2205.05474</a></li>
<li>Samples: <a href="https://rikorose.github.io/DeepFilterNet2-Samples/" rel="nofollow">https://rikorose.github.io/DeepFilterNet2-Samples/</a></li>
<li>Demo: <a href="https://huggingface.co/spaces/hshr/DeepFilterNet2" rel="nofollow">https://huggingface.co/spaces/hshr/DeepFilterNet2</a></li>
</ul>
</li>
<li>
<p dir="auto">Original DeepFilterNet Paper: <em>DeepFilterNet: A Low Complexity Speech Enhancement Framework for Full-Band Audio based on Deep Filtering</em></p>
<ul dir="auto">
<li>Paper: <a href="https://arxiv.org/abs/2110.05588" rel="nofollow">https://arxiv.org/abs/2110.05588</a></li>
<li>Samples: <a href="https://rikorose.github.io/DeepFilterNet-Samples/" rel="nofollow">https://rikorose.github.io/DeepFilterNet-Samples/</a></li>
<li>Demo: <a href="https://huggingface.co/spaces/hshr/DeepFilterNet" rel="nofollow">https://huggingface.co/spaces/hshr/DeepFilterNet</a></li>
<li>Video Lecture: <a href="https://youtu.be/it90gBqkY6k" rel="nofollow">https://youtu.be/it90gBqkY6k</a></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-deep-filter" aria-hidden="true" href="#deep-filter"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>deep-filter</h3>
<p dir="auto">Download a pre-compiled deep-filter binary from the <a href="https://github.com/Rikorose/DeepFilterNet/releases/">release page</a>.
You can use <code>deep-filter</code> to suppress noise in noisy .wav audio files. Currently, only wav files with a sampling rate of 48kHz are supported.</p>
<div dir="auto" data-snippet-clipboard-copy-content="USAGE:
    deep-filter [OPTIONS] [FILES]...

ARGS:
    &lt;FILES&gt;...

OPTIONS:
    -D, --compensate-delay
            Compensate delay of STFT and model lookahead
    -h, --help
            Print help information
    -m, --model &lt;MODEL&gt;
            Path to model tar.gz. Defaults to DeepFilterNet2.
    -o, --out-dir &lt;OUT_DIR&gt;
            [default: out]
    --pf
            Enable postfilter
    -v, --verbose
            Logging verbosity
    -V, --version
            Print version information"><pre>USAGE:
    deep-filter [OPTIONS] [FILES]...

ARGS:
    <span>&lt;</span>FILES<span>&gt;</span>...

OPTIONS:
    -D, --compensate-delay
            Compensate delay of STFT and model lookahead
    -h, --help
            Print <span>help</span> information
    -m, --model <span>&lt;</span>MODEL<span>&gt;</span>
            Path to model tar.gz. Defaults to DeepFilterNet2.
    -o, --out-dir <span>&lt;</span>OUT_DIR<span>&gt;</span>
            [default: out]
    --pf
            Enable postfilter
    -v, --verbose
            Logging verbosity
    -V, --version
            Print version information</pre></div>
<p dir="auto">If you want to use the pytorch backend e.g. for GPU processing, see further below for the Python usage.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-deepfilternet-framework" aria-hidden="true" href="#deepfilternet-framework"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DeepFilterNet Framework</h3>
<p dir="auto">This framework supports Linux, MacOS and Windows. Training is only tested under Linux. The framework is structured as follows:</p>
<ul dir="auto">
<li><code>libDF</code> contains Rust code used for data loading and augmentation.</li>
<li><code>DeepFilterNet</code> contains DeepFilterNet code training, evaluation and visualization as well as pretrained model weights.</li>
<li><code>pyDF</code> contains a Python wrapper of libDF STFT/ISTFT processing loop.</li>
<li><code>pyDF-data</code> contains a Python wrapper of libDF dataset functionality and provides a pytorch data loader.</li>
<li><code>ladspa</code> contains a LADSPA plugin for real-time noise suppression.</li>
<li><code>models</code> contains pretrained for usage in DeepFilterNet (Python) or libDF/deep-filter (Rust)</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-deepfilternet-python-pypi" aria-hidden="true" href="#deepfilternet-python-pypi"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>DeepFilterNet Python: PyPI</h3>
<p dir="auto">Install the DeepFilterNet Python wheel via pip:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install cpu/cuda pytorch (&gt;=1.9) dependency from pytorch.org, e.g.:
pip install torch torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html
# Install DeepFilterNet
pip install deepfilternet
# Or install DeepFilterNet including data loading functionality for training (Linux only)
pip install deepfilternet[train]"><pre><span><span>#</span> Install cpu/cuda pytorch (&gt;=1.9) dependency from pytorch.org, e.g.:</span>
pip install torch torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html
<span><span>#</span> Install DeepFilterNet</span>
pip install deepfilternet
<span><span>#</span> Or install DeepFilterNet including data loading functionality for training (Linux only)</span>
pip install deepfilternet[train]</pre></div>
<p dir="auto">To enhance noisy audio files using DeepFilterNet run</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Specify an output directory with --output-dir [OUTPUT_DIR]
deepFilter path/to/noisy_audio.wav"><pre><span><span>#</span> Specify an output directory with --output-dir [OUTPUT_DIR]</span>
deepFilter path/to/noisy_audio.wav</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-manual-installation" aria-hidden="true" href="#manual-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Manual Installation</h3>
<p dir="auto">Install cargo via <a href="https://rustup.rs/" rel="nofollow">rustup</a>. Usage of a <code>conda</code> or <code>virtualenv</code> recommended.</p>
<p dir="auto">Installation of python dependencies and libDF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd path/to/DeepFilterNet/  # cd into repository
# Recommended: Install or activate a python env
# Mandatory: Install cpu/cuda pytorch (&gt;=1.8) dependency from pytorch.org, e.g.:
pip install torch torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html
# Install build dependencies used to compile libdf and DeepFilterNet python wheels
pip install maturin poetry
# Build and install libdf python package required for enhance.py
maturin develop --release -m pyDF/Cargo.toml
# Optional: Install libdfdata python package with dataset and dataloading functionality for training
# Required build dependency: HDF5 headers (e.g. ubuntu: libhdf5-dev)
maturin develop --release -m pyDF-data/Cargo.toml
# If you have troubles with hdf5 you may try to build and link hdf5 statically:
maturin develop --release --features hdf5-static -m pyDF-data/Cargo.toml
# Install remaining DeepFilterNet python dependencies
cd DeepFilterNet
poetry install -E train -E eval # Note: This globally installs DeepFilterNet in your environment
# Alternatively for developement: Install only dependencies and work with the repository version
poetry install -E train -E eval --no-root
# You may need to set the python path
export PYTHONPATH=$PWD"><pre><span>cd</span> path/to/DeepFilterNet/  <span><span>#</span> cd into repository</span>
<span><span>#</span> Recommended: Install or activate a python env</span>
<span><span>#</span> Mandatory: Install cpu/cuda pytorch (&gt;=1.8) dependency from pytorch.org, e.g.:</span>
pip install torch torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html
<span><span>#</span> Install build dependencies used to compile libdf and DeepFilterNet python wheels</span>
pip install maturin poetry
<span><span>#</span> Build and install libdf python package required for enhance.py</span>
maturin develop --release -m pyDF/Cargo.toml
<span><span>#</span> Optional: Install libdfdata python package with dataset and dataloading functionality for training</span>
<span><span>#</span> Required build dependency: HDF5 headers (e.g. ubuntu: libhdf5-dev)</span>
maturin develop --release -m pyDF-data/Cargo.toml
<span><span>#</span> If you have troubles with hdf5 you may try to build and link hdf5 statically:</span>
maturin develop --release --features hdf5-static -m pyDF-data/Cargo.toml
<span><span>#</span> Install remaining DeepFilterNet python dependencies</span>
<span>cd</span> DeepFilterNet
poetry install -E train -E <span>eval</span> <span><span>#</span> Note: This globally installs DeepFilterNet in your environment</span>
<span><span>#</span> Alternatively for developement: Install only dependencies and work with the repository version</span>
poetry install -E train -E <span>eval</span> --no-root
<span><span>#</span> You may need to set the python path</span>
<span>export</span> PYTHONPATH=<span>$PWD</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-use-deepfilternet-from-command-line" aria-hidden="true" href="#use-deepfilternet-from-command-line"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Use DeepFilterNet from command line</h3>
<p dir="auto">To enhance noisy audio files using DeepFilterNet run</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ python DeepFilterNet/df/enhance.py --help
usage: enhance.py [-h] [--model-base-dir MODEL_BASE_DIR] [--pf] [--output-dir OUTPUT_DIR] [--log-level LOG_LEVEL] [--compensate-delay]
                  noisy_audio_files [noisy_audio_files ...]

positional arguments:
  noisy_audio_files     List of noise files to mix with the clean speech file.

optional arguments:
  -h, --help            show this help message and exit
  --model-base-dir MODEL_BASE_DIR, -m MODEL_BASE_DIR
                        Model directory containing checkpoints and config.
                        To load a pretrained model, you may just provide the model name, e.g. `DeepFilterNet`.
                        By default, the pretrained DeepFilterNet2 model is loaded.
  --pf                  Post-filter that slightly over-attenuates very noisy sections.
  --output-dir OUTPUT_DIR, -o OUTPUT_DIR
                        Directory in which the enhanced audio files will be stored.
  --log-level LOG_LEVEL
                        Logger verbosity. Can be one of (debug, info, error, none)
  --compensate-delay, -D
                        Add some paddig to compensate the delay introduced by the real-time STFT/ISTFT implementation.

# Enhance audio with original DeepFilterNet
python DeepFilterNet/df/enhance.py -m DeepFilterNet path/to/noisy_audio.wav

# Enhance audio with DeepFilterNet2
python DeepFilterNet/df/enhance.py -m DeepFilterNet2 path/to/noisy_audio.wav"><pre>$ python DeepFilterNet/df/enhance.py --help
usage: enhance.py [-h] [--model-base-dir MODEL_BASE_DIR] [--pf] [--output-dir OUTPUT_DIR] [--log-level LOG_LEVEL] [--compensate-delay]
                  noisy_audio_files [noisy_audio_files ...]

positional arguments:
  noisy_audio_files     List of noise files to mix with the clean speech file.

optional arguments:
  -h, --help            show this <span>help</span> message and <span>exit</span>
  --model-base-dir MODEL_BASE_DIR, -m MODEL_BASE_DIR
                        Model directory containing checkpoints and config.
                        To load a pretrained model, you may just provide the model name, e.g. <span><span>`</span>DeepFilterNet<span>`</span></span>.
                        By default, the pretrained DeepFilterNet2 model is loaded.
  --pf                  Post-filter that slightly over-attenuates very noisy sections.
  --output-dir OUTPUT_DIR, -o OUTPUT_DIR
                        Directory <span>in</span> which the enhanced audio files will be stored.
  --log-level LOG_LEVEL
                        Logger verbosity. Can be one of (debug, info, error, none)
  --compensate-delay, -D
                        Add some paddig to compensate the delay introduced by the real-time STFT/ISTFT implementation.

<span><span>#</span> Enhance audio with original DeepFilterNet</span>
python DeepFilterNet/df/enhance.py -m DeepFilterNet path/to/noisy_audio.wav

<span><span>#</span> Enhance audio with DeepFilterNet2</span>
python DeepFilterNet/df/enhance.py -m DeepFilterNet2 path/to/noisy_audio.wav</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-use-deepfilternet-within-your-python-script" aria-hidden="true" href="#use-deepfilternet-within-your-python-script"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Use DeepFilterNet within your Python script</h3>
<div dir="auto" data-snippet-clipboard-copy-content="from df import enhance, init_df

model, df_state, _ = init_df()  # Load default model
enhanced_audio = enhance(model, df_state, noisy_audio)"><pre><span>from</span> <span>df</span> <span>import</span> <span>enhance</span>, <span>init_df</span>

<span>model</span>, <span>df_state</span>, <span>_</span> <span>=</span> <span>init_df</span>()  <span># Load default model</span>
<span>enhanced_audio</span> <span>=</span> <span>enhance</span>(<span>model</span>, <span>df_state</span>, <span>noisy_audio</span>)</pre></div>
<p dir="auto">See <a href="https://github.com/Rikorose/DeepFilterNet/blob/main/scripts/external_usage.py">here</a> for a full example.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-training" aria-hidden="true" href="#training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training</h3>
<p dir="auto">The entry point is <code>DeepFilterNet/df/train.py</code>. It expects a data directory containing HDF5 dataset
as well as a dataset configuration json file.</p>
<p dir="auto">So, you first need to create your datasets in HDF5 format. Each dataset typically only
holds training, validation, or test set of noise, speech or RIRs.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install additional dependencies for dataset creation
pip install h5py librosa soundfile
# Go to DeepFilterNet python package
cd path/to/DeepFilterNet/DeepFilterNet
# Prepare text file (e.g. called training_set.txt) containing paths to .wav files
#
# usage: prepare_data.py [-h] [--num_workers NUM_WORKERS] [--max_freq MAX_FREQ] [--sr SR] [--dtype DTYPE]
#                        [--codec CODEC] [--mono] [--compression COMPRESSION]
#                        type audio_files hdf5_db
#
# where:
#   type: One of `speech`, `noise`, `rir`
#   audio_files: Text file containing paths to audio files to include in the dataset
#   hdf5_db: Output HDF5 dataset.
python df/scripts/prepare_data.py --sr 48000 speech training_set.txt TRAIN_SET_SPEECH.hdf5"><pre><span># Install additional dependencies for dataset creation</span>
<span>pip</span> <span>install</span> <span>h5py</span> <span>librosa</span> <span>soundfile</span>
<span># Go to DeepFilterNet python package</span>
<span>cd</span> <span>path</span><span>/</span><span>to</span><span>/</span><span>DeepFilterNet</span><span>/</span><span>DeepFilterNet</span>
<span># Prepare text file (e.g. called training_set.txt) containing paths to .wav files</span>
<span>#</span>
<span># usage: prepare_data.py [-h] [--num_workers NUM_WORKERS] [--max_freq MAX_FREQ] [--sr SR] [--dtype DTYPE]</span>
<span>#                        [--codec CODEC] [--mono] [--compression COMPRESSION]</span>
<span>#                        type audio_files hdf5_db</span>
<span>#</span>
<span># where:</span>
<span>#   type: One of `speech`, `noise`, `rir`</span>
<span>#   audio_files: Text file containing paths to audio files to include in the dataset</span>
<span>#   hdf5_db: Output HDF5 dataset.</span>
<span>python</span> <span>df</span><span>/</span><span>scripts</span><span>/</span><span>prepare_data</span>.<span>py</span> <span>-</span><span>-</span><span>sr</span> <span>48000</span> <span>speech</span> <span>training_set</span>.<span>txt</span> <span>TRAIN_SET_SPEECH</span>.<span>hdf5</span></pre></div>
<p dir="auto">All datasets should be made available in one dataset folder for the train script.</p>
<p dir="auto">The dataset configuration file should contain 3 entries: &#34;train&#34;, &#34;valid&#34;, &#34;test&#34;. Each of those
contains a list of datasets (e.g. a speech, noise and a RIR dataset). You can use multiple speech
or noise dataset. Optionally, a sampling factor may be specified that can be used to over/under-sample
the dataset. Say, you have a specific dataset with transient noises and want to increase the amount
of non-stationary noises by oversampling. In most cases you want to set this factor to 1.</p>
<details>
  <summary>Dataset config example:</summary>
<p dir="auto"><code>dataset.cfg</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;train&#34;: [
    [
      &#34;TRAIN_SET_SPEECH.hdf5&#34;,
      1.0
    ],
    [
      &#34;TRAIN_SET_NOISE.hdf5&#34;,
      1.0
    ],
    [
      &#34;TRAIN_SET_RIR.hdf5&#34;,
      1.0
    ]
  ],
  &#34;valid&#34;: [
    [
      &#34;VALID_SET_SPEECH.hdf5&#34;,
      1.0
    ],
    [
      &#34;VALID_SET_NOISE.hdf5&#34;,
      1.0
    ],
    [
      &#34;VALID_SET_RIR.hdf5&#34;,
      1.0
    ]
  ],
  &#34;test&#34;: [
    [
      &#34;TEST_SET_SPEECH.hdf5&#34;,
      1.0
    ],
    [
      &#34;TEST_SET_NOISE.hdf5&#34;,
      1.0
    ],
    [
      &#34;TEST_SET_RIR.hdf5&#34;,
      1.0
    ]
  ]
}"><pre>{
  <span>&#34;train&#34;</span>: [
    [
      <span><span>&#34;</span>TRAIN_SET_SPEECH.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ],
    [
      <span><span>&#34;</span>TRAIN_SET_NOISE.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ],
    [
      <span><span>&#34;</span>TRAIN_SET_RIR.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ]
  ],
  <span>&#34;valid&#34;</span>: [
    [
      <span><span>&#34;</span>VALID_SET_SPEECH.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ],
    [
      <span><span>&#34;</span>VALID_SET_NOISE.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ],
    [
      <span><span>&#34;</span>VALID_SET_RIR.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ]
  ],
  <span>&#34;test&#34;</span>: [
    [
      <span><span>&#34;</span>TEST_SET_SPEECH.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ],
    [
      <span><span>&#34;</span>TEST_SET_NOISE.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ],
    [
      <span><span>&#34;</span>TEST_SET_RIR.hdf5<span>&#34;</span></span>,
      <span>1.0</span>
    ]
  ]
}</pre></div>

</details>
<p dir="auto">Finally, start the training script. The training script may create a model <code>base_dir</code> if not
existing used for logging, some audio samples, model checkpoints, and config. If no config file is
found, it will create a default config. See
<a href="https://github.com/Rikorose/DeepFilterNet/blob/main/DeepFilterNet/pretrained_models/DeepFilterNet/config.ini">DeepFilterNet/pretrained_models/DeepFilterNet</a>
for a config file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# usage: train.py [-h] [--debug] data_config_file data_dir base_dir
python df/train.py path/to/dataset.cfg path/to/data_dir/ path/to/base_dir/"><pre><span># usage: train.py [-h] [--debug] data_config_file data_dir base_dir</span>
<span>python</span> <span>df</span><span>/</span><span>train</span>.<span>py</span> <span>path</span><span>/</span><span>to</span><span>/</span><span>dataset</span>.<span>cfg</span> <span>path</span><span>/</span><span>to</span><span>/</span><span>data_dir</span><span>/</span> <span>path</span><span>/</span><span>to</span><span>/</span><span>base_dir</span><span>/</span></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-citation-guide" aria-hidden="true" href="#citation-guide"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation Guide</h2>
<p dir="auto">If you use this framework, please cite: <em>DeepFilterNet: A Low Complexity Speech Enhancement Framework for Full-Band Audio based on Deep Filtering</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{schroeter2022deepfilternet,
  title={{DeepFilterNet}: A Low Complexity Speech Enhancement Framework for Full-Band Audio based on Deep Filtering}, 
  author = {Schröter, Hendrik and Escalante-B., Alberto N. and Rosenkranz, Tobias and Maier, Andreas},
  booktitle={ICASSP 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2022},
  organization={IEEE}
}"><pre><span>@inproceedings</span>{<span>schroeter2022deepfilternet</span>,
  <span>title</span>=<span><span>{</span>{DeepFilterNet}: A Low Complexity Speech Enhancement Framework for Full-Band Audio based on Deep Filtering<span>}</span></span>, 
  <span>author</span> = <span><span>{</span>Schröter, Hendrik and Escalante-B., Alberto N. and Rosenkranz, Tobias and Maier, Andreas<span>}</span></span>,
  <span>booktitle</span>=<span><span>{</span>ICASSP 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2022<span>}</span></span>,
  <span>organization</span>=<span><span>{</span>IEEE<span>}</span></span>
}</pre></div>
<p dir="auto">If you use the DeepFilterNet2 model, please cite: <em>DeepFilterNet2: Towards Real-Time Speech Enhancement on Embedded Devices for Full-Band Audio</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{schroeter2022deepfilternet2,
  title = {{DeepFilterNet2}: Towards Real-Time Speech Enhancement on Embedded Devices for Full-Band Audio},
  author = {Schröter, Hendrik and Escalante-B., Alberto N. and Rosenkranz, Tobias and Maier, Andreas},
  booktitle={17th International Workshop on Acoustic Signal Enhancement (IWAENC 2022)},
  year = {2022},
}
"><pre><span>@inproceedings</span>{<span>schroeter2022deepfilternet2</span>,
  <span>title</span> = <span><span>{</span>{DeepFilterNet2}: Towards Real-Time Speech Enhancement on Embedded Devices for Full-Band Audio<span>}</span></span>,
  <span>author</span> = <span><span>{</span>Schröter, Hendrik and Escalante-B., Alberto N. and Rosenkranz, Tobias and Maier, Andreas<span>}</span></span>,
  <span>booktitle</span>=<span><span>{</span>17th International Workshop on Acoustic Signal Enhancement (IWAENC 2022)<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2022<span>}</span></span>,
}
</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">DeepFilterNet is free and open source! All code in this repository is dual-licensed under either:</p>
<ul dir="auto">
<li>MIT License (<a href="https://theleo.zone/Rikorose/DeepFilterNet/blob/main/LICENSE-MIT">LICENSE-MIT</a> or <a href="http://opensource.org/licenses/MIT" rel="nofollow">http://opensource.org/licenses/MIT</a>)</li>
<li>Apache License, Version 2.0 (<a href="https://theleo.zone/Rikorose/DeepFilterNet/blob/main/LICENSE-APACHE">LICENSE-APACHE</a> or <a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">http://www.apache.org/licenses/LICENSE-2.0</a>)</li>
</ul>
<p dir="auto">at your option. This means you can select the license you prefer!</p>
<p dir="auto">Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.</p>
</article>
          </div></div>
  </body>
</html>
