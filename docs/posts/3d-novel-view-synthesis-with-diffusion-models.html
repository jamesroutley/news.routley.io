<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://3d-diffusion.github.io/">Original</a>
    <h1>3D Novel View Synthesis with Diffusion Models</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        <p>
We present 3DiM (pronounced &#34;three-dim&#34;), a diffusion model for 3D novel view synthesis from as few as a single image.
The core of 3DiM is an image-to-image diffusion model -- 3DiM takes a single reference view and a relative pose as input, and generates a novel view via diffusion.
3DiM can then generate a full 3D consistent scene following our novel <i>stochastic conditioning</i> sampler. The output frames of the scene are generated autoregressively. During the reverse diffusion process of each individual frame, we select a random conditioning frame from the set of previous frames at each denoising step. We demonstrate that stochastic conditioning yields much more 3D consistent results compared to the na√Øve sampling process which only conditions on a single previous frame.
We compare 3DiMs to prior work on the SRN ShapeNet dataset, demonstrating that 3DiM&#39;s generated videos from a single view achieve much higher fidelity while being approximately 3D consistent. We also introduce a new evaluation methodology, <i>3D consistency scoring</i>, to <i>measure</i> the 3D consistency of a generated object by training a neural field on the model&#39;s output views.
3DiMs are geometry free, do not rely on hyper-networks or test-time optimization for novel view synthesis, and allow a single model to easily scale to a large number of scenes.
        </p>
        </div>
    </div></div>
  </body>
</html>
