<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://seohong.me/blog/q-learning-is-not-yet-scalable/">Original</a>
    <h1>Q-learning is not yet scalable</h1>
    
    <div id="readability-page-1" class="page"><div>
        <div>
            <h2>Does RL scale?</h2>
            <p>
                Over the past few years,
                we&#39;ve seen that next-token prediction scales, denoising diffusion scales, contrastive learning scales,
                and so on, all the way to the point where we can train models with billions of parameters
                with a <i>scalable</i> objective that can eat up as much data as we can throw at it.
                Then, what about reinforcement learning (RL)?
                <b>Does RL also <i>scale</i> like all the other objectives?</b>
            </p>
            <p>
                Apparently, it does.
                In 2016, RL achieved superhuman-level performance in games like Go and Chess.
                Now, RL is solving complex reasoning tasks in math and coding with large language models (LLMs).
                This is great. However, there is one important caveat:
                most of the current real-world successes of RL have been achieved with <b>on-policy RL</b> algorithms
                (<i>e.g.</i>, REINFORCE, PPO, GRPO, etc.),
                which <i>always</i> require fresh, newly sampled rollouts from the current policy,
                and cannot reuse previous data
                (<i>note: while PPO-like methods can technically reuse data to some (limited) degree, I&#39;ll classify them as on-policy RL,
                as in <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">OpenAI&#39;s documentation</a></i>).
                This is not a problem in <i>some</i> settings like board games and LLMs,
                where we can cheaply generate as many rollouts as we want.
                However, it is a significant limitation in <i>most</i> real-world problems.
                For example, in robotics, it takes <a href="https://x.com/KyleStachowicz/status/1885359401546162638">more than several months</a> in the real world to generate
                the amount of samples used to post-train a language model with RL,
                not to mention that a human must be present 24/7 next to the robot to reset it during the entire training time!
            </p>
        </div>
        <div>
            <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/on_off.png"/>
                </p>
                <p><span>
                    On-policy RL can only use fresh data collected by the current policy \(\pi\).
                    Off-policy RL can use <i>any</i> data \(\mathcal{D}\).
                </span>
            </p></div>
        </div>
        <div>
            <p>
                This is where <b>off-policy RL</b> comes to the rescue.
                In principle, off-policy RL algorithms can use <i>any</i> data, regardless of when and how it was collected.
                Hence, they generally lead to much better sample efficiency, by reusing data many times.
                For example, off-policy RL can train <a href="https://sites.google.com/berkeley.edu/walk-in-the-park">a dog robot to walk in 20 minutes from scratch in the real world</a>.
                <b>Q-learning</b> is the most widely used off-policy RL algorithm.
                It minimizes the following temporal difference (TD) loss:
                <span>
                    $$\begin{aligned}
                    \mathbb{E}_{(s, a, r, s&#39;) \sim \mathcal{D}} \bigg[ \Big( Q_\theta(s, a) - \big(r + \gamma \max_{a&#39;} Q_{\bar \theta}(s&#39;, a&#39;) \big) \Big)^2 \bigg],
                    \end{aligned}$$
                </span>
                where \(\bar \theta\) is the parameter of the target network.
                Most practical (model-free) off-policy RL algorithms are based on some variants of the TD loss above.
                So, to apply RL to many real-world problems,
                the question becomes: <b>does Q-learning (TD learning) scale?</b>
                If the answer is yes, this would lead to at least an equivalent level of impact as the successes of AlphaGo and LLMs,
                enabling RL to solve far more diverse and complex real-world tasks very efficiently,
                in robotics, computer-using agents, and so on.
            </p>
            <h2>Q-learning is not yet scalable</h2>
            <p>
                Unfortunately, my current belief is that the answer is <b>not yet</b>.
                I believe current Q-learning algorithms are not readily scalable, at least to <i>long-horizon</i> problems that require more than (say) 100 semantic decision steps.
            </p>
            <p>
                Let me clarify. My definition of scalability here is the ability to solve <i>more challenging, longer-horizon</i> problems
                with more data (of sufficient coverage), compute, and time.
                This notion is different from the ability to solve merely a <i>larger number</i> of (but not necessarily harder) tasks with a single model,
                which many excellent <a href="https://sites.google.com/view/scaling-offlinerl/home">prior</a> <a href="https://sites.google.com/view/perceiver-actor-critic">scaling</a> <a href="https://arxiv.org/abs/2505.23150">studies</a> have shown to be possible.
                You can think of the former as the &#34;depth&#34; axis and the latter as the &#34;width&#34; axis.
                The depth axis is more important and harder to push, because it requires developing more advanced decision-making capabilities.
            </p>
            <p>
                I claim that Q-learning, in its current form, is <i>not</i> highly scalable along the depth axis.
                In other words, I believe we still need <i>algorithmic breakthroughs</i> to scale up Q-learning (and off-policy RL) to complex, long-horizon problems.
                Below, I&#39;ll explain two main reasons why I think so:
                one is anecdotal, and the other is based on our <a href="https://arxiv.org/abs/2506.04168">recent scaling study</a>.
            </p>
        </div>
        <div>
            <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/logos.png"/>
                </p>
                <p><span>
                    Both AlphaGo and DeepSeek are based on <i>on-policy</i> RL and do not use TD learning.
                </span>
            </p></div>
        </div>
        <div>
            <p>
                Anecdotal evidence first.
                As mentioned earlier, most real-world successes of RL are based on on-policy RL algorithms.
                AlphaGo, AlphaZero, and MuZero are based on model-based RL and Monte Carlo tree search, and do not use TD learning on board games
                (see 15p of the <a href="https://arxiv.org/abs/1911.08265">MuZero</a> paper).
                OpenAI Five achieves superhuman performance in Dota 2 with PPO
                (see footnote 6 of the <a href="https://arxiv.org/abs/1912.06680">OpenAI Five</a> paper).
                RL for LLMs is currently dominated by variants of on-policy policy gradient methods, such as PPO and GRPO.
                Let me ask: do we know of any <i>real-world</i> successes of off-policy RL (1-step TD learning, in particular) on a similar scale to AlphaGo or LLMs?
                If you do, please let me know and I&#39;ll happily update this post.
            </p>
            <p>
                Of course, I&#39;m not making this claim based only on anecdotal evidence.
                As said before, I&#39;ll show concrete experiments to empirically prove this point later in this post.
                Also, please don&#39;t get me wrong, I&#39;m still highly optimistic about off-policy RL and Q-learning (as an RL researcher who mainly works in off-policy RL!).
                I just think that we are not there yet, and <b>the purpose of this post is to call for research in RL algorithms, rather than to discourage it!</b>
            </p>
            <h2>What&#39;s the problem?</h2>
            <p>
                Then, what fundamentally makes Q-learning not readily scalable to complex, long-horizon problems, unlike other objectives?
                Here is my answer:
                <span>
                    $$\begin{aligned}
                    \definecolor{myblue}{RGB}{89, 139, 231}
                    \mathbb{E}_{(s, a, r, s&#39;) \sim \mathcal{D}} \bigg[ \Big( Q_\theta(s, a) - \underbrace{\big(r + \gamma \max_{a&#39;} Q_{\bar \theta}(s&#39;, a&#39;) \big)}_{{\color{myblue}\texttt{Biased }} (\textit{i.e., }\neq Q^*(s, a))} \Big)^2 \bigg]
                    \end{aligned}$$
                </span>
                Q-learning struggles to scale because <b>the prediction targets are biased, and these biases <i>accumulate</i> over the horizon.</b>
                The presence of <b>bias accumulation</b> is a fundamental limitation that is <i>unique</i> to Q-learning (TD learning).
                For example, there are no biases in prediction targets in other scalable objectives
                (<i>e.g.</i>, next-token prediction, denoising diffusion, contrastive learning, etc.)
                or at least these biases do not accumulate over the horizon (<i>e.g.</i>, BYOL, DINO, etc.).
            </p>
        </div>
        <div>
            <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/toy_accumulation_simple.png"/>
                </p>
                <p><span>
                    Biases accumulate over the horizon.
                </span>
            </p></div>
        </div>
        <div>
            <p>
                As the problem becomes more complex and the horizon gets longer, the biases in bootstrapped targets accumulate more and more severely,
                to the point where we cannot easily mitigate them with more data and larger models.
                I believe this is the main reason why we almost never use larger discount factors (\(\gamma &gt; 0.999\)) in practice,
                and why it is challenging to scale up Q-learning.
                Note that policy gradient methods suffer much less from this issue.
                This is because <a href="https://arxiv.org/abs/1506.02438">GAE</a> or similar on-policy value estimation techniques
                can deal with longer horizons relatively more easily (though at the expense of higher variance), without strict 1-step recursions.
            </p>
            <h2>Empirical scaling study</h2>
            <p>
                In <a href="https://arxiv.org/abs/2506.04168">our recent paper</a>, we empirically verified the above claim via diverse, controlled scaling studies.
            </p>
            <p>
                We wanted to see whether current off-policy RL methods can solve highly challenging tasks by just scaling up data and compute.
                To do this, we first prepared highly complex, previously unsolved tasks in <a href="https://seohong.me/projects/ogbench/">OGBench</a>.
                Here are some videos:
            </p>
        </div>
        <div>
            
            
            <div>
                
                <p><span>
                    <span>humanoidmaze</span><br/>
                </span>
                <span>
                    <span>humanoidmaze</span><br/>
                </span>
            </p></div>
        </div>
        <div>
            <p>
                These tasks are really difficult.
                To solve them, the agent must learn complex goal-reaching behaviors from unstructured, random (play-style) demonstrations.
                At test time, the agent must perform precise manipulation, combinatorial puzzle-solving, or long-horizon navigation,
                over 1,000 environment steps.
            </p>
            <p>
                We then collected <i>near-infinite</i> data on these environments, to the degree that overfitting is virtually impossible.
                We also removed as many confounding factors as possible.
                For example, we focused on offline RL to abstract away exploration.
                We ensured that the datasets had sufficient coverage, and that all the tasks were solvable from the given datasets.
                We directly provided the agent with the ground-truth state observations to reduce the burden of representation learning.
            </p>
            <p>
                Hence, a &#34;scalable&#34; RL algorithm must really be able to solve these tasks, given sufficient data and compute.
                If Q-learning does not scale <b>even in this controlled setting with near-infinite data</b>,
                there is little hope that it will scale in more realistic settings,
                where we have limited data, noisy observations, and so on.
            </p>
        </div>
        <div>
            <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/teaser1.png"/>
                </p>
                <p><span>
                    Standard offline RL methods struggle to scale on complex tasks, even with \(1000\times\) more data.
                </span>
            </p></div>
        </div>
        <div>
            <p>
                So, how did the existing algorithms work?
                The results were a bit disappointing.
                None of the standard, widely used offline RL algorithms (flow BC, IQL, CRL, and SAC+BC) were able to solve all of these tasks,
                even with 1B-sized datasets, which are \(1000 \times\) larger than typical datasets used in offline RL.
                More importantly, their performance often plateaued far below the optimal performance.
                In other words, they didn&#39;t scale well on these complex, long-horizon tasks.
            </p>
            <p>
                You might ask:
                Are you really sure these tasks are solvable? Did you try larger models?
                Did you train them for longer? Did you try different hyperparameters? And so on.
                In the paper, we tried our best to address as many questions as possible with a number of ablations and controlled experiments,
                showing that <b>none</b> of these fixes worked...
                <b>except for one:</b>
            </p>
            <h2>Horizon reduction makes RL scalable</h2>
            <p>
                Recall that my claim earlier was that the <b>horizon</b> (and bias accumulation thereof) is the main obstacle to scaling up off-policy RL.
                To verify this,
                we tried diverse <i>horizon reduction</i> techniques (<i>e.g.</i>, n-step returns, hierarchical RL, etc.) that reduce the number of biased TD backups.
            </p>
        </div>
        <div>
            <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/teaser2.png"/>
                </p>
                <p><span>
                    Horizon reduction was the only technique we found that substantially improved scaling.
                </span>
            </p></div>
        </div>
        <div>
            <p>
                The results were promising!
                Even simple tricks like n-step returns
                significantly improved scalability and even <i>asymptotic performance</i>
                (so it is <i>not</i> just a &#34;trick&#34; that merely makes training faster!).
                Full-fledged hierarchical methods worked even better.
                More importantly, horizon reduction is the <b>only</b> technique that worked across the board in our experiments.
                This suggests that simply scaling up data and compute is <i>not</i> enough to address the curse of horizon.
                In other words, we need <i>better algorithms</i> that directly address this fundamental horizon problem.
            </p>
            <h2>Call for research: find a <i>scalable</i> off-policy RL objective</h2>
            <p>
                We saw that horizon reduction unlocks the scalability of Q-learning.
                So are we done? Can we now just scale up Q-learning?
                I&#39;d say this is only the beginning.
                While it is great to know the cause and have some solutions,
                most of the current horizon reduction techniques (n-step returns, hierarchical RL, etc.) only <i>mitigate</i> the issue by a constant factor,
                and do not fundamentally solve the problem.
                I think <b>we&#39;re currently missing an off-policy RL algorithm that scales to arbitrarily complex, long-horizon problems</b>
                (or perhaps we may already have a solution, but just haven&#39;t stress-tested it enough yet!).
                I believe finding such a scalable off-policy RL algorithm is <b>the most important missing piece in machine learning today</b>.
                This will enable solving <i>much</i> more diverse real-world problems,
                including robotics, language models, agents, and basically any data-driven decision-making tasks.
            </p>
            <p>
                I&#39;ll conclude this post with my thoughts about potential solutions to scalable off-policy RL.
            </p>
            <ul>
                <li>
                    Can we find a simple, scalable way to extend beyond two-level hierarchies to deal with horizons of arbitrary lengths?
                    Such a solution should be able to naturally form a recursive hierarchical structure,
                    while being <i>simple enough</i> to be scalable.
                    One great example of this (though in a different field) is chain-of-thought in LLMs.
                </li>
                <li>
                    Another completely different approach (which I intentionally didn&#39;t mention so far for simplicity)
                    is <i>model-based RL</i>.
                    We know that model learning is scalable, because it&#39;s just supervised learning.
                    We also know that on-policy RL is scalable.
                    So why don&#39;t we combine the two, where we first learn a model and run on-policy RL within the model?
                    Would model-based RL indeed <i>scale</i> better than TD-based Q-learning?
                </li>
                <li>
                    Or is there a way to just completely avoid TD learning?
                    Among the methods that I know of,
                    one such example is <a href="https://www.tongzhouwang.info/quasimetric_rl/">quasimetric RL</a>,
                    which is essentially based on the LP formulation of RL.
                    Perhaps this sort of &#34;exotic&#34; RL methods, or MC-based methods like <a href="https://ben-eysenbach.github.io/contrastive_rl/">contrastive RL</a>, might eventually scale better than TD-based approaches?
                </li>
            </ul>
            <p>
                Our setup above can be a great starting point for testing these ideas.
                We have already designed a set of highly challenging robotic tasks, made the datasets, and verified that they are solvable.
                One can even make the tasks arbitrarily difficult (<i>e.g.</i>, by adding more cubes)
                and further stress-test the scalability of algorithms in a controlled way.
                We also put our efforts into making the code as clean as possible.
                Check out <a href="https://github.com/seohongpark/horizon-reduction">our code</a>!
            </p>
            <p>
                Feel free to let me know via email/Twitter/X or reach out to me at conferences if you have any questions, comments, or feedback.
                I hope that at some point, I can write another post about off-policy RL with a more positive title in the near future!
            </p>
            <h3>Acknowledgments</h3>
            <p>
                I would like to thank
                <a href="https://kvfrans.com/">Kevin Frans</a>,
                <a href="https://hongsukchoi.github.io/">Hongsuk Choi</a>,
                <a href="https://ben-eysenbach.github.io/">Ben Eysenbach</a>,
                <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a>,
                and <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                for their helpful feedback on this post.
                This post is partly based on our recent work, <a href="https://arxiv.org/abs/2506.04168">Horizon Reduction Makes RL Scalable</a>.
                The views in this post are my own, and do not necessarily reflect those of my coauthors.
            </p>
        </div>
    </div></div>
  </body>
</html>
