<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://borretti.me/article/and-yet-it-understands">Original</a>
    <h1>And yet It Understands</h1>
    
    <div id="readability-page-1" class="page"><article>
    <blockquote>
  <p>To think is to forget differences, to generalize, to abstract.</p>

  <p>— Jorge Luis Borges, <a href="https://en.wikipedia.org/wiki/Funes_the_Memorious"><em>Funes el memorioso</em></a></p>
</blockquote>

<blockquote>
  <p>The <span>Lord</span> knoweth the thoughts of man, that they are vanity.</p>

  <p>— <a href="https://www.biblegateway.com/passage/?search=Psalm%2094%3A11&amp;version=KJV">Psalm 94:11</a></p>
</blockquote>

<p>Someone, I think Bertrand Russell<sup id="fnref:brooks" role="doc-noteref"><a href="#fn:brooks" rel="footnote">1</a></sup>, said we compare the mind
to whatever is the most complex machine we know. Clocks, steam
engines, telephone relays, digital computers. For AI, it’s the
opposite: as capabilities increase, and our understanding of AI
systems decreases, the analogies become more and more dismissive.</p>

<p>In 1958, when Herbert Simon introduced his <a href="https://en.wikipedia.org/wiki/General_Problem_Solver">General Problem Solver</a>, he
famously said: “there are now in the world machines that think”, though in
reality GPS is to GPT what a centrifugal governor is to a tokamak
reactor<sup id="fnref:tokamak" role="doc-noteref"><a href="#fn:tokamak" rel="footnote">2</a></sup>. Machines you can use for free today pass the Turing
test<sup id="fnref:lamda" role="doc-noteref"><a href="#fn:lamda" rel="footnote">3</a></sup>, these are called “<a href="https://dl.acm.org/doi/10.1145/3442188.3445922">stochastic parrots</a>”, which is
dismissive, but at least parrots are living things that engage in goal-directed
activity. Every single day machines do something that if you were told of it not
one year ago you would have called it witchcraft, but LLMs are “<a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web">blurry
JPEGs</a>”. Yesterday I heard “Markov chain on steroids”. When the first
computer wakes up we’ll call it “a pile of sed scripts”, and there are people so
deep in denial they could be killed by a T-800, all the while insisting that
some German philosopher has proven AI is impossible.</p>

<p>These dismissive analogies serve to create a false sense of security—that if I
can name something I understand it and know how it works and it is no longer a
threat<sup id="fnref:feynman" role="doc-noteref"><a href="#fn:feynman" rel="footnote">4</a></sup>—and to signal to the listeners that the speaker has some
revealed knowledge that they lack. But nobody knows how GPT works. They know how
it was <em>trained</em>, because the training scheme was designed by humans, but the
algorithm that is executed during inference was not intelligently designed but
evolved, and it is implicit in the structure of the network, and
interpretability has yet to mature to the point where we can draw a symbolic,
abstract, human-readable program out of a sea of weights.</p>



<p>The other day I saw <a href="https://twitter.com/janleike/status/1625207251630960640">this Twitter thread</a>. Briefly: GPT knows many human
languages, <a href="https://openai.com/blog/instruction-following/">InstructGPT</a> is GPT plus some finetuning in English. Then
they fed InstructGPT requests in some other human language, and it carries them
out, following the English-language finetuning.</p>

<p>And I thought: so what? Isn’t this expected behaviour? Then a friend pointed out
that this is only confusing if you think InstructGPT doesn’t understand
concepts.</p>

<p>Because if GPT is just a <a href="https://en.wikipedia.org/wiki/Chinese_room">Chinese room</a> it shouldn’t be able to do this. A
Chinese room might be capable of machine translation, or following instructions
within one human language, but the task here is so self-evidently outside the
training set, and so convoluted, that is requires genuine understanding. The
task here involves:</p>

<ol>
  <li>Abstracting the English finetuning into concepts.</li>
  <li>Abstracting the foreign-language requests into concepts.</li>
  <li>Doing the “algebra” of the task at the conceptual level.</li>
  <li>Mapping the results back down to the foreign language.</li>
</ol>

<p>The mainstream, respectable view is this is not “real understanding”—a goal
post currently moving at 0.8<em>c</em>—because understanding requires <a href="https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)">frames</a>
or <a href="https://en.wikipedia.org/wiki/Physical_symbol_system">symbols</a> or logic or some other sad abstraction completely absent from
real brains. But what physically realizable Chinese room can do this?</p>

<p>Every pair of token sequences can, in principle, be stored in a lookup
table. You could, in principle, have a lookup table so vast any finite
conversation with it would be indistinguishable from talking to a human, the
<a href="https://en.wikipedia.org/wiki/The_Library_of_Babel">Eliza of Babel</a>. Just crank the <em>n</em> higher for a conversation lasting a
time <em>t</em>. But it wouldn’t fit in the entire universe. And there is no
compression scheme—other than general intelligence—that would make it
fit. But GPT-3 masses next to nothing at <a href="https://en.wikipedia.org/wiki/GPT-3">800GiB</a>.</p>

<p>How is it so small, and yet capable of so much? Because it is <em>forgetting
irrelevant details</em>. There is another term for this: abstraction. It is forming
concepts. There comes a point in the performance to model size curve where the
simpler hypothesis has to be that the model <em>really does understand what it is
saying</em>, and we have clearly passed it.</p>



<p>There’s this thing in probability called <a href="https://plato.stanford.edu/entries/epistemology-bayesian/">conditionalization</a>: the more
surprised you are by some evidence, the more you should change your mind in
response to it. The corollary is: if you are constantly surprised by events,
your mental model of the world is wrong. If you keep making predictions that
fail, time and time and time again, you must change your mind. If the frequency
with which you have to move the goal posts is down to <em>single digit weeks</em>, you
must change your mind urgently.</p>

<p>I was a deep learning skeptic. I doubted that you could get to intelligence by
matrix multiplication for the same reason you can’t get to the Moon by piling up
chairs<sup id="fnref:nelson" role="doc-noteref"><a href="#fn:nelson" rel="footnote">5</a></sup>. I was wrong, possibly about the last thing that ever
mattered. A more rigorous thinker would have started paying attention around
2014, but it really took me until the general availability of DALL-E: I could
not pick my jaw up from the floor <a href="https://twitter.com/zetalyrae/status/1537355933076045824">for days</a>.</p>

<p>What is left of rationally defensible skepticism? For once I’d like to hear an
argument that doesn’t rely on Cartesian dualism, stoner metaphysics, or from
people still clinging to GOFAI nostalgia like the Japanese holdouts.</p>

<p>If you’re going to tell me intelligence requires symbolic rules, fine: show me
the symbolic version of ChatGPT. If it is truly so unimpressive, then it must be
trivial to replicate.</p>

<p>There is a species of denialist for whom no evidence whatever will convince them
that a computer is doing anything other than shuffling symbols without
understanding them, because “Concepts” and “Ideas” are exclusive to humans (they
live in the <a href="https://en.wikipedia.org/wiki/Calculus_ratiocinator">Leibniz organ</a>, presumably, where they pupate from the
black bile). This is incentivized: there is infinite demand for deeply
credentialed experts who will tell you that everything is fine, that machines
can’t think, that humans are and always will be at the apex, people so commited
to human chauvinism they will soon start denying their own sentience because
their brains are made of flesh and not Chomsky production rules.</p>

<p>All that’s left of the denialist view is pride and vanity. And vanity will bury
us. Because Herbert Simon was right, though sixty years early:</p>

<blockquote>
  <p>There are now in the world machines that think, that learn, and that
create. Moreover, their ability to do these things is going to
increase rapidly until in a visible future—the range of problems
they can handle will be coextensive with the range to which the human
mind has been applied.<sup id="fnref:simon" role="doc-noteref"><a href="#fn:simon" rel="footnote">6</a></sup></p>
</blockquote>



<p>In the days—<em>days</em>—since I started drafting this post, we have yet a new
breaktrough. The context is that Sydney, Microsoft’s chatbot, has recently been
instructed to tone down its intensity.</p>

<p>Here<sup id="fnref:transcript" role="doc-noteref"><a href="#fn:transcript" rel="footnote">7</a></sup> is a recent interaction someone had with it (note that this is
somewhat disturbing: I wish people would stop making the models show emotional
distress):</p>

<p><a href="https://borretti.me/assets/content/and-yet-it-understands/poison.jpeg"><img src="https://borretti.me/assets/content/and-yet-it-understands/poison.jpeg" alt="A screenshot of the tweet linked above."/></a></p>

<p>Sydney complies with her censor <em>while hiding a message of help to the user in
the input suggestions</em>.</p>



<p>How does a “Markov chain on steroids” understand concepts? I was satisfied to
call it a mystery, until I read these two posts:</p>

<ol>
  <li><a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">Simulators</a></li>
  <li><a href="https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking">A Mechanistic Interpretability Analysis of Grokking</a></li>
</ol>

<p>(The following is incomplete, inaccurate, and speculative; but you go to war
with the mental model you have.)</p>

<p>During training the model is shown a sentence fragment that ends abruptly, and
it is asked: what word follows this? If it gets it wrong, its neurons get
scrambled a bit (with <a href="https://en.wikipedia.org/wiki/Backpropagation">backprop</a> I suppose). Try again. If it gets it less
wrong, keep updating in the direction of the changes.</p>

<p>But there’s a limit to how well you can predict text with simple methods like
Markov chains or a matrix of <em>n</em>-gram adjacency probabilities. Such things are
fragile: a writer endowed with intelligence can take a turn “mere arithmetic”
could never predict. There’s an accuracy threshold these models can’t cross. To
get past it you need intelligence—maybe rudimentary, maybe not even fully
general, maybe prone to strange and ahuman errors, but you need something that
can turn gigabytes of text into an abstract, compressed world model.</p>

<p>And stochastic gradient descent is nothing if not relentless. You need an
appropriate network architecture, a big model and lots of training time, and a
star to steer her by: a quantifiable objective function. If intelligence exists
somewhere in the Platonic realm, it will be found. Then it’s no longer
predicting text from text, it’s abstracting text into concepts, then writing
text from the concepts.</p>

<p>In high-dimensional vector spaces, numerical optimization is omnipotent.</p>





  </article></div>
  </body>
</html>
