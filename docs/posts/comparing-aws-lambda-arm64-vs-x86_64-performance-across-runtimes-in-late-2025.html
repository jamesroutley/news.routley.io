<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://chrisebert.net/comparing-aws-lambda-arm64-vs-x86_64-performance-across-multiple-runtimes-in-late-2025/">Original</a>
    <h1>Comparing AWS Lambda ARM64 vs. x86_64 Performance Across Runtimes in Late 2025</h1>
    
    <div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        

            <p>See how AWS Lambda arm64 stacks up against x86_64 in real-world benchmarks across CPU, memory, and I/O workloads using Node.js, Python, and Rust.</p>

        <div>
        <section>

            <ul>
                <li>
                    <a href="https://chrisebert.net/author/chris/" aria-label="Read more of Chris Ebert">
                        <img src="https://chrisebert.net/content/images/size/w100/2024/01/chris_ebert.jpg" alt="Chris Ebert"/>
                    </a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2025-11-24">24 Nov 2025</time>
                        <span><span>•</span> 14 min read</span>
                </p>
            </div>

        </section>
        </div>

            <figure>
                <img srcset="/content/images/size/w300/2025/11/Generated-Image-November-21--2025---10_34PM-1.jpeg 300w,
                            /content/images/size/w600/2025/11/Generated-Image-November-21--2025---10_34PM-1.jpeg 600w,
                            /content/images/size/w1000/2025/11/Generated-Image-November-21--2025---10_34PM-1.jpeg 1000w,
                            /content/images/size/w2000/2025/11/Generated-Image-November-21--2025---10_34PM-1.jpeg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://chrisebert.net/content/images/size/w2000/2025/11/Generated-Image-November-21--2025---10_34PM-1.jpeg" alt="Illustration labeled “AWS Lambda Benchmarking” showing a blue x86_64 processor on the left and an orange arm64 processor on the right, connected by an arrow, with Node.js, Python, and Rust lo"/>
                    <figcaption><span>Benchmarking AWS Lambda in late 2025</span></figcaption>
            </figure>

    </header>

    <section>
        <p><strong>Update: </strong>This article was initially published on November 24th. All the takeaways from the original article still hold, but it turns out it&#39;s possible to make Rust even faster on Arm! After publishing this benchmark, <a href="https://github.com/khawajashams?ref=chrisebert.net">Khawaja Shams</a></p><p>If you know me at all, you know I’m a big proponent of serverless services when they’re used for the right workloads. I helped launch a new product that increased usage by over 30x so far this year. Choosing serverless was a big part of why that launch went as smoothly as it did. Given how heavily I use Lambda, I have a vested interest in how its performance and architecture evolve.</p><p>Initially, Amazon Web Services (AWS) Lambda only supported x86_64-based compute. In 2021, AWS added support for arm64-based Graviton processors, which were advertised as offering equal or better performance at a lower price point and with a smaller environmental footprint.</p><p>Back in October 2023, AWS published a blog post titled <a href="https://aws.amazon.com/blogs/apn/comparing-aws-lambda-arm-vs-x86-performance-cost-and-analysis-2/?ref=chrisebert.net">&#34;Comparing AWS Lambda Arm vs. x86 Performance, Cost, and Analysis.&#34;</a> This post was a great reference at the time, but nearly two years later, I haven’t seen many follow-up benchmarks either on the official AWS blog or from the community. I’ve been wondering how things look in 2025 if you apply a similar methodology, which led me to build a more modern, generic benchmark of my own.</p><p>Going into this, I expected arm64 to be the most performant architecture and Rust to be the most performant runtime, but I wanted actual data to support my assumptions. So, I built a benchmark that runs Lambda functions on both x86_64 and arm64 architectures across CPU-intensive, memory-intensive, and light workloads, using the actively supported AWS runtimes for Node.js, Rust, and Python. While you should always benchmark and evaluate the performance of your real-world production workloads, generic benchmarks are always interesting for investigating general performance trends.</p><p>In this post, I’ll first highlight the high-level findings. Then I’ll walk through the benchmark design (workloads, runtimes, and configurations), and finally, I’ll dig into the detailed results. Unlike the AWS benchmark mentioned earlier, this project is fully open source and available on GitHub. You’re welcome to replicate my results, extend the tests, or adapt them to your own workloads. You can find the code in the <a href="https://github.com/cebert/aws-lambda-performance-benchmarks?ref=chrisebert.net">aws-lambda-performance-benchmarks</a> repository. The results of a recent benchmark run are also published to that repo.</p><blockquote><strong>Note:</strong> This benchmark includes the officially supported Rust runtime (announced GA on November 14, 2025) and the Python 3.14 runtime (announced GA on November 18, 2025). I’ll talk more about those under <strong>Runtimes</strong> below.</blockquote><h2 id="tldr-the-winners">TLDR: The Winners</h2><p>If you don’t have time to read the entire post, here are the key takeaways. I ran the benchmark several times in the <code>us-east-2</code> (Ohio) region, and observed similar results across benchmark runs. The results shared in this post come from my most recent run, which tested 42 Lambda functions (7 runtimes × 2 architectures × 3 workloads). After collecting samples, I removed outliers using basic statistical techniques and calculated mean, median, and P50/P90/P95/P99 percentiles.</p><ul><li><strong>Performance champion:</strong> Rust on arm64 is the most performant and cost-efficient combination overall. There are a few instances where x86_64 Rust slightly beats out arm64 by a thin margin, but with a 20% cost discount, arm still wins at efficiency.</li><li><strong>Python:</strong> Python 3.11 on arm64 slightly outperformed the newer Python runtimes in my tests (which honestly surprised me, but turns out to match other public benchmarks).</li><li><strong>Node.js:</strong> Node.js 22 on arm64 was consistently faster than Node.js 20 on x86_64. There&#39;s essentially a “free” ~15-20% speedup<strong> </strong>just by switching architectures!</li><li><strong>Cost:</strong> Across the board, arm64 delivered roughly 30–40% lower compute costs with equal or better performance than x86_64. Unless you&#39;re using a library that isn&#39;t compatible with arm64 or have a unique workload, arm is a good default architecture choice for Lambda.</li></ul><h2 id="benchmark-methodology">Benchmark Methodology</h2><p>My goal was to create an updated benchmark similar to the <a href="https://aws.amazon.com/blogs/apn/comparing-aws-lambda-arm-vs-x86-performance-cost-and-analysis-2/?ref=chrisebert.net">2023 AWS Lambda benchmark blog post</a>. Unfortunately, I was unable to find the original code used for that benchmark, so I created a new, similar benchmark from scratch.</p><p>The existing AWS benchmark used three types of workloads, which I also adopted:</p><ul><li><strong>Light</strong>: A workload that is lightweight but realistic.</li><li><strong>CPU-intensive</strong>: A workload that stresses compute utilization.</li><li><strong>Memory-intensive</strong>: A workload that stresses memory utilization.</li></ul><p>The original benchmark included both single-threaded and multi-threaded CPU and memory-intensive tests. I chose not to include multi-threaded tests in my benchmark for two reasons. First, building a robust benchmark harness is time-intensive. It took a good amount of my free time to make the single-threaded version of this benchmark before adding even more tests to run and analyze. Second, most of my personal use cases for Lambda aren’t multi-threaded, and the multi-threaded results weren’t particularly interesting to me.</p><p>It’s worth reminding readers that AWS <a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html?ref=chrisebert.net">Lambda allocates CPU power in proportion to the amount of memory configured for a function</a>. This means that to get <strong>1 full vCPU of compute, you must allocate 1,769 MB of memory to a Lambda</strong>. For my single-threaded workloads, you’d expect diminishing returns if a Lambda is configured with more memory than this, since a single-threaded Lambda workload cannot use more than one vCPU in these tests. However, for the memory-intensive workload, additional performance can be achieved at an extra cost by allocating more memory.</p><p>The benchmark includes detailed documentation on its workloads in the <a href="https://github.com/cebert/aws-lambda-performance-benchmarks/blob/main/docs/benchmark-design.md?ref=chrisebert.net">Benchmark Design page</a> on GitHub. Here’s a summary of<strong> </strong>what each workload actually tests:</p><h3 id="workloads">Workloads</h3><ul><li><strong>Light</strong>: A realistic workload that uses DynamoDB batch write (5 items) followed by batch read (5 items). This test includes AWS SDK overhead, serialization/deserialization, and network I/O latency with minimal compute. Because this test accesses DynamoDB, it introduces some performance variability due to network latency and underlying database performance, but it remains a realistic Lambda scenario.</li><li><strong>CPU-intensive</strong>: Performs 500,000 iterations of SHA-256 cryptographic hashing in a tight loop. This is a pure compute workload with no AWS SDK dependencies, designed to stress CPU performance and measure single-threaded execution speed.</li><li><strong>Memory-intensive</strong>: Allocates and sorts a 100 MB array using native 64-bit types, stressing memory bandwidth and CPU together. I chose 100 MB to ensure lower-memory Lambda configurations could complete successfully. I initially tried allocating larger arrays, but this quickly became too much for Python on the lower-power configurations. The Rust and Node.js runtimes were capable of allocating larger arrays even at the smallest 128 MB Lambda memory allocation.</li></ul><p>A best effort was made to consistently implement these workloads across runtimes. There are always some variances across languages, but the implementations are very similar.</p><h3 id="runtimes">Runtimes</h3><p>The <a href="https://aws.amazon.com/blogs/apn/comparing-aws-lambda-arm-vs-x86-performance-cost-and-analysis-2/?ref=chrisebert.net">original 2023 blog post</a> tested similar workloads across Node.js, Ruby, and Python. The runtime versions tested at that time are no longer supported by AWS. Since 2023, Rust has grown in popularity, and Ruby usage in Lambda has remained relatively niche compared to Node.js and Python. I chose to replace Ruby with Rust, which AWS officially supported <a href="https://aws.amazon.com/about-aws/whats-new/2025/11/aws-lambda-rust/?ref=chrisebert.net">this month</a>. I had also never created a Rust Lambda before and wanted to try it out.</p><h4 id="benchmarked-aws-runtimes">Benchmarked AWS Runtimes</h4><p>As of November 2025, the benchmark tested all actively supported AWS runtimes for Node.js, Python, and Rust:</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Lambda Runtime</th>
<th>Release Date</th>
</tr>
</thead>
<tbody>
<tr>
<td>Node.js 20</td>
<td>November 15, 2023</td>
</tr>
<tr>
<td>Node.js 22</td>
<td>November 22, 2024</td>
</tr>
<tr>
<td>Python 3.11</td>
<td>July 27, 2023</td>
</tr>
<tr>
<td>Python 3.12</td>
<td>December 14, 2023</td>
</tr>
<tr>
<td>Python 3.13</td>
<td>November 13, 2024</td>
</tr>
<tr>
<td>Python 3.14</td>
<td>November 18, 2025</td>
</tr>
<tr>
<td>Rust</td>
<td>November 14, 2025</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<p>You can always view the latest runtimes supported by AWS on the <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html?ref=chrisebert.net">Lambda runtimes</a> page.</p><h3 id="memory-configurations">Memory Configurations</h3><p>I ran each workload across all runtimes included in the benchmark using the following memory configurations on x86_64 and arm64 architectures.</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Workload Type</th>
<th>Memory Configurations (MB)</th>
<th>Total Configs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Light</td>
<td>128, 256, 512, 1024, 1769, 2048</td>
<td>6</td>
</tr>
<tr>
<td>CPU-intensive</td>
<td>128, 256, 512, 1024, 1769, 2048</td>
<td>6</td>
</tr>
<tr>
<td>Memory-intensive</td>
<td>128, 256, 512, 1024, 1769, 2048, 4096, 8192, 10240</td>
<td>9</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h4 id="sampling-runs-and-cold-vs-warm-starts">Sampling, Runs, and Cold vs. Warm Starts</h4><p>For each combination of runtime, architecture, workload, and memory configuration, I ran multiple test runs and captured cold start and warm start metrics separately. I was more interested in warm start sampling, but also wanted to collect enough cold start samples so we could compare cold starts across runtimes. At the time I completed these tests, the 3.14 Python Lambda runtime was just released. Since this runtime is newer, there may be a higher cold-start penalty, since most AWS customers haven&#39;t had a chance to update their Lambdas to target it yet, which makes the Lambda infrastructure less likely to get a cache hit when loading the runtime for a cold Python 3.14 Lambda. If we ran these tests a few weeks from now, the cold start times for the Python 3.14 runtime could improve.</p><p>At a high level:</p><ul><li><strong>Data collected</strong>: <a href="https://github.com/cebert/aws-lambda-performance-benchmarks/blob/main/docs/metrics-collection-implementation.md?ref=chrisebert.net">The Metrics Collection page</a> provides complete details on how metrics are collected using CloudWatch log trailing, without adding any additional overhead to the tests. Init duration, duration, billed duration, and max memory used metrics for each Lambda invocation are collected and stored in DynamoDB. AWS provides helpful documentation on what these metrics mean on the <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html?ref=chrisebert.net">Understanding the Lambda execution environment lifecycle</a> page.</li><li><strong>Sample size</strong>: For each configuration, the benchmark performs 125 cold invocations and 500 warm invocations (625 total invocations per config) across 294 unique test configurations, resulting in 183,750 Lambda invocations per test run. I ran multiple benchmark runs in <code>us-east-2</code> and saw results that were relatively similar across time of day.</li><li><strong>Cold start tests</strong>: Cold starts were triggered by toggling the Lambda&#39;s memory configuration, which invalidates all warm instances without requiring redeployment. For this benchmark, I collected 125 cold samples for each configuration. I borrowed this cold start testing technique from <a href="https://github.com/astuyve/cold-start-benchmarker?ref=chrisebert.net">AJ Stuyvenberg&#39;s blog posts</a>. AJ&#39;s approach dramatically speeds up testing by eliminating the need to wait for natural cold starts. The <a href="https://github.com/alexcasalboni/aws-lambda-power-tuning?ref=chrisebert.net">AWS Lambda Power Tuning Tool</a> takes a similar approach when used to benchmark an individual Lambda.</li><li><strong>Warm start tests: </strong>For each configuration, 500 warm Lambda invocations were collected for this benchmark.</li><li><strong>Statistics</strong>: After collecting raw samples, statistical outliers (min and max</li></ul><h2 id="results-overview">Results Overview</h2><p>The primary comparisons in this post focus on <strong>warm invocations</strong>, since that&#39;s what most production traffic looks like. Here are some key findings from the data:</p><ul><li><strong>arm64 wins on cost in every scenario</strong></li><li><strong>Rust is dramatically faster than interpreted runtimes</strong> <ul><li>8x faster than Node.js, 2x faster than Python</li><li>Cold starts favor ARM64 with 13-24% faster initialization</li><li>x86 Rust slightly outperformed arm64 Rust for CPU-intensive work at high memory </li></ul></li><li><strong>Python 3.11 is the fastest Python</strong><ul><li>9-15% faster than 3.12, 3.13, and 3.14</li></ul></li><li><strong>Node.js 22 beats Node.js 20</strong><ul><li> Node.js 22 beat out Node.js 20 by 8-11% across the board</li></ul></li></ul><h2 id="cpu-intensive-workload-results">CPU-Intensive Workload Results</h2><p>The CPU-intensive workload (500,000 SHA-256 iterations) provides a clean view of raw compute performance without I/O overhead.</p><h3 id="warm-start-performance"><strong>Warm Start Performance</strong></h3><figure><img src="https://chrisebert.net/content/images/2025/11/memory-scaling-warm-1.png" alt="" loading="lazy" width="2000" height="1135" srcset="https://chrisebert.net/content/images/size/w600/2025/11/memory-scaling-warm-1.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/memory-scaling-warm-1.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/memory-scaling-warm-1.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/memory-scaling-warm-1.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>CPU-intensive warm start duration by memory configuration across all runtimes and architectures</em></i></figcaption></figure><table>
<thead>
<tr>
<th>Runtime</th>
<th>arm64 @2048MB</th>
<th>x86 @2048MB</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Rust</strong></td>
<td>163ms</td>
<td>147ms</td>
</tr>
<tr>
<td>Python 3.11</td>
<td>263ms</td>
<td>341ms</td>
</tr>
<tr>
<td>Python 3.14</td>
<td>287ms</td>
<td>358ms</td>
</tr>
<tr>
<td>Node.js 22</td>
<td>1,260ms</td>
<td>1,384ms</td>
</tr>
<tr>
<td>Node.js 20</td>
<td>1,377ms</td>
<td>1,549ms</td>
</tr>
</tbody>
</table>
<p>Rust is <strong>8x faster than Node.js</strong> and nearly <strong>twice as fast as Python</strong> for this compute-heavy workload.</p><p>In the original publication of these benchmark results, x86 Rust marginally outperformed arm64 by 10% at higher memory allocations. However, this was due to the SHA-256 implementation not leveraging arm&#39;s NEON SIMD instructions. After enabling assembly-optimized hashing (sha2 crate&#39;s asm feature), arm64 Rust is now 4-5x faster than x86 across all memory configurations (see the chart at the end of this post for a performance comparison before and after this change was applied).</p><h3 id="python-version-comparison"> Python Version Comparison</h3><figure><img src="https://chrisebert.net/content/images/2025/11/python-comparison-warm.png" alt="" loading="lazy" width="2000" height="1135" srcset="https://chrisebert.net/content/images/size/w600/2025/11/python-comparison-warm.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/python-comparison-warm.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/python-comparison-warm.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/python-comparison-warm.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Python version comparison for CPU-intensive warm starts</em></i></figcaption></figure><p>Python 3.11 consistently outperformed newer versions across all memory configurations. It was 9-15% faster than Python 3.12, 3.13, and 3.14. This surprised me, but matched other Python benchmarks I found online. Python 3.11 had notable performance improvements over previous versions, and subsequent releases have had negligible single-threaded improvements.</p><h3 id="nodejs-version-comparison">Node.js Version Comparison</h3><figure><img src="https://chrisebert.net/content/images/2025/11/nodejs-comparison-warm.png" alt="" loading="lazy" width="2000" height="1135" srcset="https://chrisebert.net/content/images/size/w600/2025/11/nodejs-comparison-warm.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/nodejs-comparison-warm.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/nodejs-comparison-warm.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/nodejs-comparison-warm.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Node.js 20 vs 22 comparison for CPU-intensive warm starts</em></i></figcaption></figure><p>Node.js 22 showed consistent improvements over Node.js 20, with <strong>execution times 8-11% faster </strong>across memory configurations. Combined with the benefits of the arm64 architecture, upgrading from Node.js 20 on x86 to Node.js 22 on arm64 delivers approximately <strong>18% performance improvement </strong>at no additional cost.</p><h3 id="rust-comparison">Rust Comparison</h3><p>Rust performed consistently well regardless of the amount of memory allocated to it. It is a highly efficient runtime for CPU-intensive workloads.</p><figure><img src="https://chrisebert.net/content/images/2025/11/runtime-family-p99-warm.png" alt="" loading="lazy" width="2000" height="1134" srcset="https://chrisebert.net/content/images/size/w600/2025/11/runtime-family-p99-warm.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/runtime-family-p99-warm.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/runtime-family-p99-warm.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/runtime-family-p99-warm.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>P99 duration by runtime family for CPU-intensive warm starts</em></i></figcaption></figure><figure><img src="https://chrisebert.net/content/images/2025/11/nodejs-rust-comparison-warm.png" alt="" loading="lazy" width="2000" height="1135" srcset="https://chrisebert.net/content/images/size/w600/2025/11/nodejs-rust-comparison-warm.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/nodejs-rust-comparison-warm.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/nodejs-rust-comparison-warm.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/nodejs-rust-comparison-warm.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Rust vs Node.js 22 comparison for CPU-intensive warm starts</em></i></figcaption></figure><h3 id="cost-analysis">Cost Analysis</h3><figure><img src="https://chrisebert.net/content/images/2025/11/cost-savings-warm-3.png" alt="" loading="lazy" width="2000" height="1061" srcset="https://chrisebert.net/content/images/size/w600/2025/11/cost-savings-warm-3.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/cost-savings-warm-3.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/cost-savings-warm-3.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/cost-savings-warm-3.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Cost savings (%) when using arm64 vs x86 for CPU-intensive workloads</em></i></figcaption></figure><p>Arm64 delivered <strong>7-38% cost savings</strong> for CPU-intensive workloads across all runtimes. Even when x86 is slightly faster, arm64&#39;s 20% lower price per GB-second wins on total cost. </p><h2 id="memory-intensive-workload-results">Memory-Intensive Workload Results</h2><p>The memory-intensive workload allocates and sorts a 100MB array, stressing both memory bandwidth and CPU.</p><h3 id="warm-start-performance-1">Warm Start Performance</h3><figure><img src="https://chrisebert.net/content/images/2025/11/memory-scaling-warm-2.png" alt="" loading="lazy" width="2000" height="1135" srcset="https://chrisebert.net/content/images/size/w600/2025/11/memory-scaling-warm-2.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/memory-scaling-warm-2.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/memory-scaling-warm-2.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/memory-scaling-warm-2.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Memory-intensive warm start duration by memory configuration</em></i></figcaption></figure><p>This workload showed interesting patterns:</p><table>
<thead>
<tr>
<th>Runtime</th>
<th>arm64 @10240MB</th>
<th>x86 @10240MB</th>
<th>arm64 Advantage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Rust</strong></td>
<td>706ms</td>
<td>811ms</td>
<td>+13%</td>
</tr>
<tr>
<td>Node.js 20</td>
<td>1,900ms</td>
<td>2,623ms</td>
<td>+28%</td>
</tr>
<tr>
<td>Node.js 22</td>
<td>1,894ms</td>
<td>2,597ms</td>
<td>+27%</td>
</tr>
<tr>
<td>Python 3.11</td>
<td>9,178ms</td>
<td>12,717ms</td>
<td>+28%</td>
</tr>
</tbody>
</table>
<p>Arm64&#39;s advantage grows with memory allocation in these charts.<strong> </strong>At the maximum 10GB Lambda configuration, arm64 was 27-28% faster than x86 for Node.js workloads.</p><p>Rust again dominates, completing the memory-intensive workload 2.7x faster than Node.js and 13x faster than Python.</p><figure><img src="https://chrisebert.net/content/images/2025/11/python-comparison-warm-2.png" alt="" loading="lazy" width="2000" height="1136" srcset="https://chrisebert.net/content/images/size/w600/2025/11/python-comparison-warm-2.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/python-comparison-warm-2.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/python-comparison-warm-2.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/python-comparison-warm-2.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Python version comparison for memory-intensive workloads</em></i></figcaption></figure><h3 id="cost-analysis-1"></h3><figure><img src="https://chrisebert.net/content/images/2025/11/cost-savings-warm-4.png" alt="" loading="lazy" width="2000" height="1061" srcset="https://chrisebert.net/content/images/size/w600/2025/11/cost-savings-warm-4.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/cost-savings-warm-4.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/cost-savings-warm-4.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/cost-savings-warm-4.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Cost savings (%) when using arm64 vs x86 for memory-intensive workloads</em></i></figcaption></figure><p>Arm64 delivered significant cost savings for memory-intensive workloads, up to 42%, at higher memory configurations where arm64&#39;s performance advantage is most clear. Node.js and Rust showed the most consistent savings (23-42%), while Python&#39;s cost efficiency varied due to the performance anomalies noted above.<br/></p><h2 id="light-workload-results">Light Workload Results</h2><p>The light workload (DynamoDB batch read/write) represents a realistic Lambda scenario in which I/O latency dominates execution time. I&#39;ll keep this section brief since the results tell a simple story.</p><figure><img src="https://chrisebert.net/content/images/2025/11/memory-scaling-warm-3.png" alt="" loading="lazy" width="2000" height="1134" srcset="https://chrisebert.net/content/images/size/w600/2025/11/memory-scaling-warm-3.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/memory-scaling-warm-3.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/memory-scaling-warm-3.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/memory-scaling-warm-3.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Light workload warm start duration by memory configuration</em></i></figcaption></figure><p>For I/O-bound workloads, the runtime differences largely disappear. When network latency is the bottleneck, whether you&#39;re running Rust or Python matters far less. All runtimes completed the light workload in 15-80ms at 512MB and above. The main takeaway for light workloads: <strong>optimize for cost, not raw performance.</strong> </p><h2 id="cold-start-analysis">Cold Start Analysis</h2><p>Cold starts are often a critical concern for Lambda users, especially for latency-sensitive applications.</p><p>Here&#39;s a breakdown of the init duration by runtime we observed during these benchmarks:<br/></p><table>
<thead>
<tr>
<th>Runtime</th>
<th>arm64 Init (avg)</th>
<th>x86 Init (avg)</th>
<th>arm64 Advantage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Rust</strong></td>
<td><strong>16ms</strong></td>
<td>21ms</td>
<td>+24%</td>
</tr>
<tr>
<td>Python 3.11</td>
<td>79ms</td>
<td>94ms</td>
<td>+16%</td>
</tr>
<tr>
<td>Python 3.12</td>
<td>89ms</td>
<td>107ms</td>
<td>+17%</td>
</tr>
<tr>
<td>Python 3.13</td>
<td>100ms</td>
<td>122ms</td>
<td>+18%</td>
</tr>
<tr>
<td>Python 3.14</td>
<td>124ms</td>
<td>143ms</td>
<td>+13%</td>
</tr>
<tr>
<td>Node.js 20</td>
<td>134ms</td>
<td>155ms</td>
<td>+13%</td>
</tr>
<tr>
<td>Node.js 22</td>
<td>129ms</td>
<td>150ms</td>
<td>+14%</td>
</tr>
</tbody>
</table>
<p><strong>Rust cold starts are 5-8x faster than interpreted runtimes. </strong>At 16ms on arm64, Rust initialization is nearly imperceptible. This makes Rust an excellent choice for latency-sensitive applications where cold starts matter.</p><p>Arm64 consistently showed 13-24% faster cold start initialization across all runtimes. This is a meaningful improvement that compounds with cold start frequency.</p><h3 id="cost-efficiency-deep-dive"><strong>Cost Efficiency Deep Dive</strong></h3><figure><img src="https://chrisebert.net/content/images/2025/11/cost-vs-performance-warm.png" alt="" loading="lazy" width="2000" height="1327" srcset="https://chrisebert.net/content/images/size/w600/2025/11/cost-vs-performance-warm.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/cost-vs-performance-warm.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/cost-vs-performance-warm.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/cost-vs-performance-warm.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em>Cost vs performance scatter plot for CPU-intensive workloads</em></i></figcaption></figure><p>The cost analysis reveals why arm64 is such a great default:</p><ul><li>arm64 is 20% cheaper per GB-second than x86</li><li>arm64 performance matches or exceeds x86 in most cases</li></ul><p>The combined cost and performance efficiency of arm64 make it a strong default architecture for Lambda, unless a library you are using in a Lambda isn&#39;t compatible with arm. Even when x86 shows a slight performance edge (like Rust CPU-intensive at high memory), arm64&#39;s price advantage typically results in better cost efficiency.</p><h3 id="performance-consistency-p99-latency">Performance Consistency (P99 Latency)</h3><figure><img src="https://chrisebert.net/content/images/2025/11/p99-scaling-warm.png" alt="" loading="lazy" width="2000" height="1135" srcset="https://chrisebert.net/content/images/size/w600/2025/11/p99-scaling-warm.png 600w, https://chrisebert.net/content/images/size/w1000/2025/11/p99-scaling-warm.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/11/p99-scaling-warm.png 1600w, https://chrisebert.net/content/images/size/w2400/2025/11/p99-scaling-warm.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption><i><em> P99 latency scaling for CPU-intensive workloads</em></i></figcaption></figure><h2 id="conclusions">Conclusions</h2><p><strong>The verdict is clear: arm64 should be your default targeted CPU architecture for Lambda.</strong> After multiple benchmark runs analyzing 183,750 Lambda invocations across 294 configurations, the data consistently points in one direction. Unless you have a specific library compatibility issue, arm64 wins.</p><h3 id="why-arm64-wins">Why ARM64 Wins</h3><table>
<thead>
<tr>
<th>Benefit</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Performance</strong></td>
<td>Equal or better in 90%+ of scenarios</td>
</tr>
<tr>
<td><strong>Cold starts</strong></td>
<td>13-24% faster initialization</td>
</tr>
<tr>
<td><strong>Cost</strong></td>
<td>25-40% lower per invocation</td>
</tr>
</tbody>
</table>
<p>The rare exceptions (like Rust being CPU-intensive at high memory usage) don&#39;t outweigh these consistent benefits.</p><h3 id="runtime-selection-guide"><strong>Runtime Selection Guide</strong></h3><p>You should never rely on generic benchmarks and instead, evaluate Lambdas for your specific workloads. However, if we were to use these generic benchmarks as a guide, this is what they would suggest:</p><table>
<thead>
<tr>
<th>Your Priority</th>
<th>Best Choice</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum performance</td>
<td>Rust on arm64</td>
</tr>
<tr>
<td>Minimal cold starts</td>
<td>Rust on arm64 (16ms init)</td>
</tr>
<tr>
<td>Python workloads</td>
<td>Python 3.11 on arm64</td>
</tr>
<tr>
<td>Node.js workloads</td>
<td>Node.js 22 on arm64</td>
</tr>
<tr>
<td>I/O-bound workloads</td>
<td>Any runtime—optimize for cost</td>
</tr>
</tbody>
</table>
<h3 id="writing-benchmarks-is-hard">Writing Benchmarks Is Hard</h3><p>This benchmark is relatively simple, with only three workloads to test. However, because each workload runs across multiple architectures and runtimes, even a ‘simple’ benchmark can become time-consuming to build, deploy, and collect results. I’m sure this benchmark isn’t perfect, but it still reveals some useful trends. Going through this process also gave me tremendous respect for the people who regularly build and contribute to benchmarking tools.</p><h3 id="reproduce-these-results">Reproduce These Results</h3><h2 id="rust-on-arm-december-update">Rust on Arm (December update)</h2><figure><img src="https://chrisebert.net/content/images/2025/12/rust-sse-optimization-comparison.png" alt="" loading="lazy" width="2000" height="852" srcset="https://chrisebert.net/content/images/size/w600/2025/12/rust-sse-optimization-comparison.png 600w, https://chrisebert.net/content/images/size/w1000/2025/12/rust-sse-optimization-comparison.png 1000w, https://chrisebert.net/content/images/size/w1600/2025/12/rust-sse-optimization-comparison.png 1600w, https://chrisebert.net/content/images/2025/12/rust-sse-optimization-comparison.png 2086w" sizes="(min-width: 720px) 720px"/><figcaption><span>Rust CPU-Intensive SSE performance Improvement comparison</span></figcaption></figure>
    </section>

        

</article>
</div></div>
  </body>
</html>
