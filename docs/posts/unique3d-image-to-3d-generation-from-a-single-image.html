<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/AiuniAI/Unique3D">Original</a>
    <h1>Unique3D: Image-to-3D Generation from a Single Image</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><strong><a href="https://github.com/AiuniAI/Unique3D/blob/main/README_zh.md">中文版本</a></strong></p>
<p dir="auto"><strong><a href="https://github.com/AiuniAI/Unique3D/blob/main/README_jp.md">日本語版</a></strong></p>

<p dir="auto">Official implementation of Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image.</p>
<p dir="auto"><a href="https://scholar.google.com/citations?user=VTU0gysAAAAJ&amp;hl=zh-CN&amp;oi=ao" rel="nofollow">Kailu Wu</a>, <a href="https://liuff19.github.io/" rel="nofollow">Fangfu Liu</a>, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, <a href="https://duanyueqi.github.io/" rel="nofollow">Yueqi Duan</a>, <a href="https://group.iiis.tsinghua.edu.cn/~maks/" rel="nofollow">Kaisheng Ma</a></p>

<ul dir="auto">
<li>Demo inference speed: Gradio Demo &gt; Huggingface Demo &gt; Huggingface Demo2 &gt; Online Demo</li>
</ul>
<p dir="auto"><strong>If the Gradio Demo unfortunately hangs or is very crowded, you can use the Online Demo <a href="https://www.aiuni.ai/" rel="nofollow">aiuni.ai</a>, which is free to try (get the registration invitation code Join Discord: <a href="https://discord.gg/aiuni" rel="nofollow">https://discord.gg/aiuni</a>). However, the Online Demo is slightly different from the Gradio Demo, in that the inference speed is slower, and the generation results is less stable, but the quality of the material is better.</strong></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/AiuniAI/Unique3D/blob/main/assets/teaser_safe.jpg"><img src="https://github.com/AiuniAI/Unique3D/raw/main/assets/teaser_safe.jpg"/></a>
</p>
<p dir="auto">High-fidelity and diverse textured meshes generated by Unique3D from single-view wild images in 30 seconds.</p>

<p dir="auto">The repo is still being under construction, thanks for your patience.</p>
<ul>
<li> Upload weights.</li>
<li> Local gradio demo.</li>
<li> Detailed tutorial.</li>
<li> Huggingface demo.</li>
<li> Detailed local demo.</li>
<li> Comfyui support.</li>
<li> Windows support.</li>
<li> Docker support.</li>
<li> More stable reconstruction with normal.</li>
<li> Training code release.</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Preparation for inference</h2><a id="user-content-preparation-for-inference" aria-label="Permalink: Preparation for inference" href="#preparation-for-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto">Adapted for Ubuntu 22.04.4 LTS and CUDA 12.1.</p>
<div data-snippet-clipboard-copy-content="conda create -n unique3d python=3.11
conda activate unique3d

pip install ninja
pip install diffusers==0.27.2

pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.3.1/index.html

pip install -r requirements.txt"><pre lang="angular2html"><code>conda create -n unique3d python=3.11
conda activate unique3d

pip install ninja
pip install diffusers==0.27.2

pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.3.1/index.html

pip install -r requirements.txt
</code></pre></div>
<p dir="auto"><a href="https://github.com/oak-barry">oak-barry</a> provide another setup script for torch210+cu121 at <a href="https://github.com/oak-barry/Unique3D">here</a>.</p>

<ul dir="auto">
<li>Thank you very much <code>jtydhr88</code> for the windows installation method! See <a href="https://github.com/AiuniAI/Unique3D/issues/15" data-hovercard-type="issue" data-hovercard-url="/AiuniAI/Unique3D/issues/15/hovercard">issues/15</a>.</li>
</ul>
<p dir="auto">According to <a href="https://github.com/AiuniAI/Unique3D/issues/15" data-hovercard-type="issue" data-hovercard-url="/AiuniAI/Unique3D/issues/15/hovercard">issues/15</a>, implemented a bat script to run the commands, so you can:</p>
<ol dir="auto">
<li>Might still require Visual Studio Build Tools, you can find it from <a href="https://visualstudio.microsoft.com/downloads/?q=build+tools" rel="nofollow">Visual Studio Build Tools</a>.</li>
<li>Create conda env and activate it
<ol dir="auto">
<li><code>conda create -n unique3d-py311 python=3.11</code></li>
<li><code>conda activate unique3d-py311</code></li>
</ol>
</li>
<li>download <a href="https://huggingface.co/madbuda/triton-windows-builds/resolve/main/triton-2.1.0-cp311-cp311-win_amd64.whl" rel="nofollow">triton whl</a> for py311, and put it into this project.</li>
<li>run <strong>install_windows_win_py311_cu121.bat</strong></li>
<li>answer y while asking you uninstall onnxruntime and onnxruntime-gpu</li>
<li>create the output folder <strong>tmp\gradio</strong> under the driver root, such as F:\tmp\gradio for me.</li>
<li>python app/gradio_local.py --port 7860</li>
</ol>
<p dir="auto">More details prefer to <a href="https://github.com/AiuniAI/Unique3D/issues/15" data-hovercard-type="issue" data-hovercard-url="/AiuniAI/Unique3D/issues/15/hovercard">issues/15</a>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Interactive inference: run your local gradio demo.</h3><a id="user-content-interactive-inference-run-your-local-gradio-demo" aria-label="Permalink: Interactive inference: run your local gradio demo." href="#interactive-inference-run-your-local-gradio-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol dir="auto">
<li>Download the weights from <a href="https://huggingface.co/spaces/Wuvin/Unique3D/tree/main/ckpt" rel="nofollow">huggingface spaces</a> or <a href="https://cloud.tsinghua.edu.cn/d/319762ec478d46c8bdf7/" rel="nofollow">Tsinghua Cloud Drive</a>, and extract it to <code>ckpt/*</code>.</li>
</ol>
<div data-snippet-clipboard-copy-content="Unique3D
    ├──ckpt
        ├── controlnet-tile/
        ├── image2normal/
        ├── img2mvimg/
        ├── realesrgan-x4.onnx
        └── v1-inference.yaml"><pre><code>Unique3D
    ├──ckpt
        ├── controlnet-tile/
        ├── image2normal/
        ├── img2mvimg/
        ├── realesrgan-x4.onnx
        └── v1-inference.yaml
</code></pre></div>
<ol start="2" dir="auto">
<li>Run the interactive inference locally.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python app/gradio_local.py --port 7860"><pre>python app/gradio_local.py --port 7860</pre></div>

<p dir="auto">Thanks for the <a href="https://github.com/jtydhr88/ComfyUI-Unique3D">ComfyUI-Unique3D</a> implementation from <a href="https://github.com/jtydhr88">jtydhr88</a>!</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Tips to get better results</h2><a id="user-content-tips-to-get-better-results" aria-label="Permalink: Tips to get better results" href="#tips-to-get-better-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol dir="auto">
<li>Unique3D is sensitive to the facing direction of input images. Due to the distribution of the training data, orthographic front-facing images with a rest pose always lead to good reconstructions.</li>
<li>Images with occlusions will cause worse reconstructions, since four views cannot cover the complete object. Images with fewer occlusions lead to better results.</li>
<li>Pass an image with as high a resolution as possible to the input when resolution is a factor.</li>
</ol>

<p dir="auto">We have intensively borrowed code from the following repositories. Many thanks to the authors for sharing their code.</p>
<ul dir="auto">
<li><a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a></li>
<li><a href="https://github.com/xxlong0/Wonder3D">Wonder3d</a></li>
<li><a href="https://github.com/SUDO-AI-3D/zero123plus">Zero123Plus</a></li>
<li><a href="https://github.com/Profactor/continuous-remeshing">Continues Remeshing</a></li>
<li><a href="https://github.com/YertleTurtleGit/depth-from-normals">Depth from Normals</a></li>
</ul>

<p dir="auto">Our mission is to create a 4D generative model with 3D concepts. This is just our first step, and the road ahead is still long, but we are confident. We warmly invite you to join the discussion and explore potential collaborations in any capacity. <span><strong>If you&#39;re interested in connecting or partnering with us, please don&#39;t hesitate to reach out via email (<a href="mailto:wkl22@mails.tsinghua.edu.cn">wkl22@mails.tsinghua.edu.cn</a>)</strong></span>.</p>
<ul dir="auto">
<li>Follow us on twitter for the latest updates: <a href="https://x.com/aiuni_ai" rel="nofollow">https://x.com/aiuni_ai</a></li>
<li>Join AIGC 3D/4D generation community on discord: <a href="https://discord.gg/aiuni" rel="nofollow">https://discord.gg/aiuni</a></li>
<li>Research collaboration, please contact: <a href="mailto:ai@aiuni.ai">ai@aiuni.ai</a></li>
</ul>

<p dir="auto">If you found Unique3D helpful, please cite our report:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{wu2024unique3d,
      title={Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image}, 
      author={Kailu Wu and Fangfu Liu and Zhihan Cai and Runjie Yan and Hanyang Wang and Yating Hu and Yueqi Duan and Kaisheng Ma},
      year={2024},
      eprint={2405.20343},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>wu2024unique3d</span>,
      <span>title</span>=<span><span>{</span>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Kailu Wu and Fangfu Liu and Zhihan Cai and Runjie Yan and Hanyang Wang and Yating Hu and Yueqi Duan and Kaisheng Ma<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2405.20343<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
