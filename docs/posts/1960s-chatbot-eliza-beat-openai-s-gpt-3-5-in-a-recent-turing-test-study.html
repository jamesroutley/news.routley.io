<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/">Original</a>
    <h1>1960s chatbot ELIZA beat OpenAI&#39;s GPT-3.5 in a recent Turing test study</h1>
    
    <div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      transform and roll out    —
</h4>
            
            <h2 itemprop="description">AI chatbot deception paper suggests that some bots (and people) aren&#39;t very persuasive.</h2>
            <section>

  


  
</section>        </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/turing_test_hero-800x450.jpg" alt="An illustration of a man and a robot sitting in boxes, talking."/>
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/turing_test_hero.jpg" data-height="675" data-width="1200">Enlarge</a> <span>/</span> An artist&#39;s impression of a human and a robot talking.</p><p>Getty Images | Benj Edwards</p></figcaption>  </figure>

  




<!-- cache hit 274:single/related:75c96972efcfd5c809dcc7c1fd0a61ed --><!-- empty -->
<p>In a preprint <a href="https://arxiv.org/abs/2310.20216">research paper</a> titled &#34;Does GPT-4 Pass the Turing Test?&#34;, two researchers from UC San Diego pitted OpenAI&#39;s <a href="https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/">GPT-4</a> AI language model against human participants, GPT-3.5, and <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a> to see which could trick participants into thinking it was human with the greatest success. But along the way, the study, which has not been peer-reviewed, found that human participants correctly identified other humans in only 63 percent of the interactions—and that a 1960s computer program surpassed the AI model that powers the free version of ChatGPT.</p>

<p>Even with limitations and caveats, which we&#39;ll cover below, the paper presents a thought-provoking comparison between AI model approaches and raises further questions about using the <a href="https://en.wikipedia.org/wiki/Turing_test">Turing test</a> to evaluate AI model performance.</p>
<p>British mathematician and computer scientist Alan Turing first conceived the Turing test as &#34;The Imitation Game&#34; <a href="http://phil415.pbworks.com/f/TuringComputing.pdf">in 1950</a>. Since then, it has become a famous but controversial benchmark for determining a machine&#39;s ability to imitate human conversation. In modern versions of the test, a human judge typically talks to either another human or a chatbot without knowing which is which. If the judge cannot reliably tell the chatbot from the human a certain percentage of the time, the chatbot is said to have passed the test. The threshold for passing the test is subjective, so there has never been a broad consensus on what would constitute a passing success rate.</p>
<p>In the recent study, <a href="https://arxiv.org/abs/2310.20216">listed on arXiv</a> at the end of October, UC San Diego researchers Cameron Jones (a PhD student in Cognitive Science) and Benjamin Bergen (a professor in the university&#39;s Department of Cognitive Science) set up a website called <a href="https://turingtest.live/">turingtest.live</a>, where they hosted a two-player implementation of the Turing test over the Internet with the goal of seeing how well GPT-4, when prompted different ways, could convince people it was human.</p>                                            
                                                        
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph.jpg" data-height="871" data-width="1000" alt="A bar graph of success rates in the Turing test performed by Jones and Bergen, with humans on top and a GPT-4 model in the #2 slot. Ancient rules-based ELIZA outperformed GPT-3.5."><img alt="A bar graph of success rates in the Turing test performed by Jones and Bergen, with humans on top and a GPT-4 model in the #2 slot. Ancient rules-based ELIZA outperformed GPT-3.5." src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph-640x557.jpg" width="640" height="557" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph.jpg 2x"/></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/pass_rate_graph.jpg" data-height="871" data-width="1000">Enlarge</a> <span>/</span> A bar graph of success rates in the Turing test performed by Jones and Bergen, with humans on top and a GPT-4 model in the #2 slot. Ancient rules-based ELIZA outperformed GPT-3.5.</p></figcaption></figure>
<p>Through the site, human interrogators interacted with various &#34;AI witnesses&#34; representing either other humans or AI models that included the aforementioned GPT-4, GPT-3.5, and <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a>, a rules-based conversational program from the 1960s. &#34;The two participants in human matches were randomly assigned to the interrogator and witness roles,&#34; write the researchers. &#34;Witnesses were instructed to convince the interrogator that they were human. Players matched with AI models were always interrogators.&#34;</p>
<p>The experiment involved 652 participants who completed a total of 1,810 sessions, of which 1,405 games were analyzed after excluding certain scenarios like repeated AI games (leading to the expectation of AI model interactions when other humans weren&#39;t online) or personal acquaintance between participants and witnesses, who were sometimes sitting in the same room.</p>
<p>Surprisingly, ELIZA, developed in the mid-1960s by computer scientist Joseph Weizenbaum at MIT, scored relatively well during the study, achieving a success rate of 27 percent. GPT-3.5, depending on the prompt, scored a 14 percent success rate, below ELIZA. GPT-4 achieved a success rate of 41 percent, second only to actual humans.</p>
<p>GPT-3.5, the base model behind the free version of ChatGPT, has been conditioned by OpenAI specifically not to present itself as a human, which <a href="https://x.com/emollick/status/1728899736609149287?s=20">may partially account</a> for its poor performance. In a post on X, Princeton computer science professor Arvind Narayanan <a href="https://x.com/random_walker/status/1729115756439511119?s=20">wrote</a>, &#34;Important context about the &#39;ChatGPT doesn&#39;t pass the Turing test&#39; paper. As always, testing behavior doesn&#39;t tell us about capability.&#34; In a <a href="https://x.com/random_walker/status/1729145124561797199?s=20">reply</a>, he continued, &#34;ChatGPT is fine-tuned to have a formal tone, not express opinions, etc, which makes it less humanlike. The authors tried to change this with the prompt, but it has limits. The best way to pretend to be a human chatting is to fine-tune on human chat logs.&#34;</p>                                            
                                                        
<p>Further, the authors speculate about the reasons for ELIZA&#39;s relative success in the study:</p>
<blockquote><p>&#34;First, ELIZA’s responses tend to be conservative. While this generally leads to the impression of an uncooperative interlocutor, it prevents the system from providing explicit cues such as incorrect information or obscure knowledge. Second, ELIZA does not exhibit the kind of cues that interrogators have come to associate with assistant LLMs, such as being helpful, friendly, and verbose. Finally, some interrogators reported thinking that ELIZA was “too bad” to be a current AI model, and therefore was more likely to be a human intentionally being uncooperative.&#34;</p></blockquote>
<p>During the sessions, the most common strategies used by interrogators included small talk and questioning about knowledge and current events. More successful strategies involved speaking in a non-English language, inquiring about time or current events, and directly accusing the witness of being an AI model.</p>
<p>The participants made their judgments based on the responses they received. Interestingly, the study found that participants based their decisions primarily on linguistic style and socio-emotional traits, rather than the perception of intelligence alone. Participants noted when responses were too formal or informal, or when responses lacked individuality or seemed generic. The study also showed that participants&#39; education and familiarity with large language models (LLMs) did not significantly predict their success in detecting AI.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2.jpg" data-height="814" data-width="1072" alt="Instructions for the Turing test AI evaluation game from Jones and Bergen, 2023."><img alt="Instructions for the Turing test AI evaluation game from Jones and Bergen, 2023." src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2-640x486.jpg" width="640" height="486" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2.jpg 2x"/></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/turing_game_instructions2.jpg" data-height="814" data-width="1072">Enlarge</a> <span>/</span> Instructions for the Turing test AI evaluation game from Jones and Bergen, 2023.</p><p>Jones and Bergen, 2023</p></figcaption></figure>
<p>The study&#39;s authors acknowledge the study&#39;s limitations, including potential sample bias by recruiting from social media and the lack of incentives for participants, which may have led to some people not fulfilling the desired role. They also say their results (especially the performance of ELIZA) may support common criticisms of the Turing test as an inaccurate way to measure machine intelligence. &#34;Nevertheless,&#34; they write, &#34;we argue that the test has ongoing relevance as a framework to measure fluent social interaction and deception, and for understanding human strategies to adapt to these devices.&#34;</p>

                                                </div>

            
            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/2/">2</a> <a href="https://arstechnica.com/information-technology/2023/12/real-humans-appeared-human-63-of-the-time-in-recent-turing-test-ai-study/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>
  </body>
</html>
