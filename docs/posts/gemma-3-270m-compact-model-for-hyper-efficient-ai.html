<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/">Original</a>
    <h1>Gemma 3 270M: Compact model for hyper-efficient AI</h1>
    
    <div id="readability-page-1" class="page"><div>

    
      <section>
        
      </section>
    

    <section>
      
    </section>

    <section>
      
    </section>

    <section>

      <section>
      
        
          
        
          
        
          
        
          
        
          
        
          
        

      
      </section>
      
    </section>

    
    <section>
      <div>
          

<div>
    <p data-block-key="8637c">The last few months have been an exciting time for the Gemma family of open models. We introduced <a href="https://blog.google/technology/developers/gemma-3/">Gemma 3</a> and <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">Gemma 3 QAT</a>, delivering state-of-the-art performance for single cloud and desktop accelerators. Then, we announced the full release of <a href="https://developers.googleblog.com/en/introducing-gemma-3n/">Gemma 3n</a>, a mobile-first architecture bringing powerful, real-time multimodal AI directly to edge devices. Our goal has been to provide useful tools for developers to build with AI, and we continue to be <a href="https://www.youtube.com/watch?v=Fx6IuEggeac">amazed</a> by the vibrant <a href="https://deepmind.google/models/gemma/gemmaverse/">Gemmaverse</a> you are helping create, celebrating together as downloads surpassed 200 million last week.</p><p data-block-key="6eq2f">Today, we&#39;re adding a new, highly specialized tool to the Gemma 3 toolkit: <a href="https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune">Gemma 3 270M</a>, a compact, 270-million parameter model designed from the ground up for task-specific fine-tuning with strong instruction-following and text structuring capabilities already trained in.</p>
</div>   


    
    <div>
        <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3-270M_Chart01_RD3-V01.original.jpg" alt="Gemma 3 270M"/></p><p>
                        Gemma 3 270M brings strong instruction-following capabilities to a small-footprint model. As shown by the IFEval benchmark (which tests a model&#39;s ability to follow verifiable instructions), it establishes a new level of performance for its size, making sophisticated AI capabilities more accessible for on-device and research applications.
                    </p>
                
            
        </div>
    </div>
  <div>
    <h2 data-block-key="v6pc7" id="core-capabilities-of-gemma-3-270m">Core capabilities of Gemma 3 270M</h2><ul><li data-block-key="4ki9m"><b>Compact and capable architecture:</b> Our new model has a total of 270 million parameters: 170 million embedding parameters due to a large vocabulary size and 100 million for our transformer blocks. Thanks to the large vocabulary of 256k tokens, the model can handle specific and rare tokens, making it a strong base model to be further fine-tuned in specific domains and languages.</li></ul><ul><li data-block-key="394ff"><b>Extreme energy efficiency:</b> A key advantage of Gemma 3 270M is its low power consumption. Internal tests on a Pixel 9 Pro SoC show the INT4-quantized model used just 0.75% of the battery for 25 conversations, making it our most power-efficient Gemma model.</li></ul><ul><li data-block-key="5c37m"><b>Instruction following:</b> An instruction-tuned model is released alongside a pre-trained checkpoint. While this model is not designed for complex conversational use cases, it’s a strong model that follows general instructions right out of the box.</li></ul><ul><li data-block-key="fonft"><b>Production-ready quantization:</b> <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">Quantization-Aware Trained</a> (QAT) <a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d">checkpoints are available</a>, enabling you to run the models at INT4 precision with minimal performance degradation, which is essential for deploying on resource-constrained devices.</li></ul><p data-block-key="cr9ib">In engineering, success is defined by efficiency, not just raw power. You wouldn&#39;t use a sledgehammer to hang a picture frame. The same principle applies to building with AI.</p><p data-block-key="2mh3o">Gemma 3 270M embodies this &#34;right tool for the job&#34; philosophy. It&#39;s a high-quality foundation model that follows instructions well out of the box, and its true power is unlocked through fine-tuning. Once specialized, it can execute tasks like text classification and data extraction with remarkable accuracy, speed, and cost-effectiveness. By starting with a compact, capable model, you can build production systems that are lean, fast, and dramatically cheaper to operate.</p><h2 data-block-key="o52yv" id="a-real-world-blueprint-for-success"><b><br/></b>A real-world blueprint for success</h2><p data-block-key="9ef55">The power of this approach has already delivered incredible results in the real world. A perfect example is <a href="https://deepmind.google/models/gemma/gemmaverse/adaptiveml/">the work done by Adaptive ML with SK Telecom.</a> Facing the challenge of nuanced, multilingual content moderation, they chose to specialize. Instead of using a massive, general-purpose model, Adaptive ML fine-tuned a Gemma 3 4B model. The results were stunning: the specialized Gemma model not only met but exceeded the performance of much larger proprietary models on its specific task.</p><p data-block-key="8htle">Gemma 3 270M is designed to let developers take this approach even further, unlocking even greater efficiency for well-defined tasks. It&#39;s the perfect starting point for creating a fleet of small, specialized models, each an expert at its own task.</p><p data-block-key="fah9p">But this power of specialization isn&#39;t just for enterprise tasks; it also enables powerful creative applications. For example, check out <a href="https://huggingface.co/spaces/webml-community/bedtime-story-generator">this Bedtime Story Generator web app</a>:</p>
</div>  <div>
    
    
        
            <p>Gemma 3 270M used to power a Bedtime Story Generator web app using Transformers.js. The model’s size and performance make it suitable for offline, web-based, creative tasks. (Credit: Joshua (@xenovacom on X) from the Hugging Face team)</p>
        
    
</div>  <div>
    <h2 data-block-key="0mwz8" id="when-to-choose-gemma-3-270m">When to choose Gemma 3 270M</h2><p data-block-key="88i9k">Gemma 3 270M inherits the advanced architecture and robust pre-training of the Gemma 3 collection, providing a solid foundation for your custom applications.</p><p data-block-key="5p4a2">Here’s when it’s the perfect choice:</p><ul><li data-block-key="dp1oc"><b>You have a high-volume, well-defined task.</b> Ideal for functions like sentiment analysis, entity extraction, query routing, unstructured to structured text processing, creative writing, and compliance checks.</li></ul><ul><li data-block-key="fb2n2"><b>You need to make every millisecond and micro-cent count.</b> Drastically reduce, or eliminate, your inference costs in production and deliver faster responses to your users. A fine-tuned 270M model can run on lightweight, inexpensive infrastructure or directly on-device.</li></ul><ul><li data-block-key="7oe6a"><b>You need to iterate and deploy quickly.</b> The small size of Gemma 3 270M allows for rapid fine-tuning experiments, helping you find the perfect configuration for your use case in hours, not days.</li></ul><ul><li data-block-key="e03e8"><b>You need to ensure user privacy.</b> Because the model can run entirely on-device, you can build applications that handle sensitive information without ever sending data to the cloud.</li></ul><ul><li data-block-key="1jppn"><b>You want a fleet of specialized task models.</b> Build and deploy multiple custom models, each expertly trained for a different task, without breaking your budget.</li></ul><h2 data-block-key="12vtd" id="get-started-with-fine-tuning"><b><br/></b>Get started with fine-tuning</h2><p data-block-key="1jvma">We want to make it as easy as possible to turn Gemma 3 270M into your own custom solution. It’s built on the same architecture as the rest of the Gemma 3 models, with recipes and tools to get you started quickly. You can find our guide on <a href="https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune">full fine-tuning</a> using Gemma 3 270M as part of the Gemma docs.</p><ul><li data-block-key="1448a"><b>Download the model:</b> Get the Gemma 3 270M models from <a href="https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d">Hugging Face</a>, <a href="https://ollama.com/library/gemma3">Ollama</a>, <a href="https://www.kaggle.com/models/google/gemma-3">Kaggle</a>, <a href="https://lmstudio.ai/models/google/gemma-3-270m">LM Studio</a>, or <a href="https://hub.docker.com/r/ai/gemma3">Docker</a>. We are releasing both pretrained and instruction tuned models.</li></ul><ul><li data-block-key="aqpc"><b>Try the model:</b> Try the models on <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3">Vertex AI</a> or with popular inference tools like <a href="https://huggingface.co/collections/ggml-org/gemma-3-270m-689e0105d56462786413d7fc">llama.cpp</a> <a href="https://www.kaggle.com/models/google/gemma-3/gemmaCpp">Gemma.cpp</a>, <a href="https://huggingface.co/litert-community/gemma-3-270m-it">LiteRT</a>, <a href="https://www.kaggle.com/models/keras/gemma3">Keras</a>, and <a href="https://huggingface.co/collections/mlx-community/gemma-3-270m-689e1de307ccaeec5ba22ec9">MLX</a>.</li></ul><ul><li data-block-key="1ahr6"><b>Start fine-tuning:</b> Use your favorite tools, including <a href="http://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune">Hugging Face</a>, <a href="https://docs.unsloth.ai/basics/gemma-3-how-to-run-and-fine-tune">UnSloth</a>, and <a href="https://gemma-llm.readthedocs.io/en/latest/colab_finetuning.html">JAX.</a></li></ul><ul><li data-block-key="4vece"><b>Deploy your solution:</b> Once fine-tuned, you can deploy your specialized model anywhere, from <a href="http://localhost:8080/">your own local environment</a> to <a href="https://cloud.google.com/run/docs/run-gemma-on-cloud-run">Google Cloud Run</a>.</li></ul><p data-block-key="1fc66">The Gemmaverse is built on the idea that innovation comes in all sizes. With Gemma 3 270M, we’re empowering developers to build smarter, faster, and more efficient AI solutions. We can’t wait to see the specialized models you create.</p>
</div> 
      </div>
    </section>
    

    <section>
      
      
    </section>

    
    
    
  </div></div>
  </body>
</html>
