<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus">Original</a>
    <h1>Localllm lets you develop gen AI apps on local CPUs</h1>
    
    <div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p><span>In today&#39;s fast-paced AI landscape, developers face numerous challenges when it comes to building applications that use large language models (LLMs). In particular, the scarcity of GPUs, which are traditionally required for running LLMs, poses a significant hurdle. In this post, we introduce you to a novel solution that allows developers to harness the power of LLMs locally on CPU and memory, right within </span><a href="https://cloud.google.com/workstations"><span>Cloud Workstations</span></a><span>, Google Cloud’s fully managed development environment. The models we use in this walkthrough are located on </span><a href="https://huggingface.co/" rel="noopener" target="_blank"><span>Hugging Face</span></a><span> and are specifically in a repo from “The Bloke” and are compatible with the quantization method used to allow them to run on CPUs or low power GPUs. This innovative approach not only eliminates the need for GPUs but also opens up a world of possibilities for seamless and efficient application development. By using a combination of “quantized models,” Cloud Workstations, a new open-source tool named </span><a href="https://github.com/googlecloudplatform/localllm" rel="noopener" target="_blank"><span>localllm</span></a><span>, and generally available resources, you can develop AI-based applications on a well-equipped development workstation, leveraging existing processes and workflows.</span></p>
<h3><strong>Quantized models + Cloud Workstations == Productivity</strong></h3>
<p><span>Quantized models are AI models that have been optimized to run on local devices with limited computational resources. These models are designed to be more efficient in terms of memory usage and processing power, allowing them to run smoothly on devices such as smartphones, laptops, and other edge devices. In this case, we are running them on Cloud Workstations with ample available resources. Here are some great examples of why leveraging quantized models in your development loop may unblock your efforts:</span></p>
<ul>
<li>
<p><strong>Improved performance:</strong><span> Quantized models are optimized to perform computations using lower-precision data types such as 8-bit integers, instead of standard 32-bit floating-point numbers. This reduction in precision allows for faster computations and improved performance on devices with limited resources.</span></p>
</li>
<li>
<p><strong>Reduced memory footprint: </strong><span>Quantization techniques help reduce the memory requirements of AI models. By representing weights and activations with fewer bits, the overall size of the model is reduced, making it easier to fit on devices with limited storage capacity. </span></p>
</li>
<li>
<p><strong>Faster inference:</strong><span> Quantized models can perform computations more quickly due to their reduced precision and smaller model size. This enables faster inference times, allowing AI applications to run more smoothly and responsively on local devices.</span></p>
</li>
</ul>
<p><span>Combining quantized models with Cloud Workstations allows you to take advantage of the flexibility, scalability and cost effectiveness of Cloud Workstations. Moreover, the traditional approach of relying on remote servers or cloud-based GPU instances for LLM-based application development can introduce latency, security concerns, and dependency on third-party services. A solution that lets you leverage LLMs locally, within your Cloud Workstations, without compromising performance, security, or control over your data, can have a lot of benefits.</span></p>
<h2><strong>Introducing </strong><strong>localllm</strong></h2>
<p><span>Today, we’re introducing  </span><code>localllm</code><span>, a set of tools and libraries that provides easy access to quantized models from HuggingFace through a command-line utility. </span><code>localllm</code> <span>can be </span><span>a game-changer for developers seeking to leverage LLMs without the constraints of GPU availability. This repository provides a comprehensive framework and tools to run LLMs locally on CPU and memory, right within the Google Cloud Workstation, using this method (though you can also run LLM models on your local machine or anywhere with sufficient CPU). By eliminating the dependency on GPUs, you can unlock the full potential of LLMs for your application development needs.</span></p>
<h3><strong>Key features and benefits</strong></h3>
<p><strong>GPU-free LLM execution</strong><span>: </span><code>localllm</code><span> lets you execute LLMs on CPU and memory, removing the need for scarce GPU resources, so you can integrate LLMs into your application development workflows, without compromising performance or productivity.</span></p>
<p><strong>Enhanced productivity</strong><span>: With </span><code>localllm</code><span>, you use LLMs directly within the Google Cloud ecosystem. This integration streamlines the development process, reducing the complexities associated with remote server setups or reliance on external services. Now, you can focus on building innovative applications without managing GPUs.</span></p>
<p><strong>Cost efficiency</strong><span>: By leveraging </span><code>localllm</code><span>, you can significantly reduce infrastructure costs associated with GPU provisioning. The ability to run LLMs on CPU and memory within the Google Cloud environment lets you optimize resource utilization, resulting in cost savings and improved return on investment.</span></p>
<p><strong>Improved data security</strong><span>: Running LLMs locally on CPU and memory helps keep sensitive data within your control. With </span><code>localllm</code><span>, you can mitigate the risks associated with data transfer and third-party access, enhancing data security and privacy.</span></p>
<p><strong>Seamless integration with Google Cloud services</strong><span>: </span><code>localllm</code><span> integrates with various Google Cloud services, including data storage, machine learning APIs, or other Google Cloud services, so you can leverage the full potential of the Google Cloud ecosystem. </span></p>
<h3><strong>Getting started with </strong><strong>localllm</strong></h3>
<p><span>To get started with the </span><code>localllm</code><span>, visit the GitHub repository at </span><a href="https://github.com/googlecloudplatform/localllm" rel="noopener" target="_blank"><span>https://github.com/googlecloudplatform/localllm</span></a><span>. The repository provides detailed documentation, code samples, and step-by-step instructions to set up and utilize LLMs locally on CPU and memory within the Google Cloud environment. You can explore the repository, contribute to its development, and leverage its capabilities to enhance your application development workflows. </span></p>
<p><span>Once you’ve cloned the repo locally, the following simple steps will run localllm with a quantized model of your choice from the HuggingFace repo “The Bloke,” then execute an initial sample prompt query. For example we are using Llama.</span></p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="# Install the tools
pip3 install openai
pip3 install ./llm-tool/.

# Download and run a model
llm run TheBloke/Llama-2-13B-Ensemble-v5-GGUF 8000

# Try out a query
./querylocal.py"><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><strong>Creating a </strong><strong>localllm</strong><strong>-enabled Cloud Workstation</strong></h3>
<p><span>To get started with </span><code>localllm</code><span> and Cloud Workstations, you&#39;ll need a </span><a href="https://cloud.google.com/docs/get-started"><span>Google Cloud Project</span></a><span> and to install the </span><a href="https://cloud.google.com/sdk/docs/install"><span>gcloud CLI</span></a><span>. First, build a Cloud Workstations container that includes localllm, then use that as the basis for our developer workstation (which also comes pre-equipped with VS Code).</span></p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="gcloud config set project $PROJECT_ID

# Enable needed services
gcloud services enable \
  cloudbuild.googleapis.com \
  workstations.googleapis.com \
  container.googleapis.com \
  containeranalysis.googleapis.com \
  containerscanning.googleapis.com \
  artifactregistry.googleapis.com

# Create AR Docker repository
gcloud artifacts repositories create localllm \
  --location=us-central1 \
  --repository-format=docker"><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Next, submit a build of the Dockerfile, which also pushes the image to Artifact Registry.</p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="gcloud builds submit ."><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>The published image is named</p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="us-central1-docker.pkg.dev/$PROJECT_ID/localllm/localllm."><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p><span>The next step is to </span><a href="https://cloud.google.com/workstations/docs/create-workstation"><span>create and launch a workstation</span></a><span> using our custom image. We suggest using a machine type of e2-standard-32 (32 vCPU, 16 core and 128 GB memory), an admittedly beefy machine.</span></p>
<p><span>The following example uses </span><span>gcloud</span><span> to configure a cluster, configuration and workstation using our custom base image with </span><code>llm</code><span> installed. Replace </span><span>$</span><code>CLUSTER</code><span> with your desired cluster name, and the command below will create a new one (which takes ~20 minutes).</span></p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="gcloud workstations clusters create $CLUSTER \
  --region=us-central1"><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>The next steps create the workstation, and starts it up. These steps will take ~10 minutes to run.</p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="# Create workstation configuration
gcloud workstations configs create localllm-workstation \
  --region=us-central1 \
  --cluster=$CLUSTER \
  --machine-type=e2-standard-32 \
  --container-custom-image=us-central1-docker.pkg.dev/$PROJECT_ID/localllm/localllm

# Create the workstation
gcloud workstations create localllm-workstation \
  --cluster=$CLUSTER \
  --config=localllm-workstation \
  --region=us-central1

# Grant access to the default Cloud Workstation Service Account
gcloud artifacts repositories add-iam-policy-binding \
  localllm \
  --location=us-central1 \
  --member=serviceAccount:service-$PROJECT_NUM@gcp-sa-workstationsvm.iam.gserviceaccount.com \
  --role=roles/artifactregistry.reader

# Start the workstation
gcloud workstations start localllm-workstation \
  --cluster=$CLUSTER \
  --config=localllm-workstation \
  --region=us-central1"><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>You can connect to the workstation using ssh (shown below), or <a href="https://cloud.google.com/workstations/docs/create-workstation#launch_a_workstation">interactively</a> in the browser.</p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="gcloud workstations ssh localllm-workstation \
  --cluster=$CLUSTER \
  --config=localllm-workstation \
  --region=us-central1"><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p><span>After serving a model (via the </span><code>llm</code><span> run</span><span> command with the port of your choice), you can interact with the model by visiting the live OpenAPI documentation page. You can apply this process to any model listed in the Bloke’s repo on HuggingFace Lllama was used in this scenario as an example. First, get the hostname of the workstation using:</span></p></span></section><section><section><div jscontroller="vkZm2d" jsaction="rcuQ6b:npT2md" data-text-to-copy="gcloud workstations describe localllm-workstation \
  --cluster=$CLUSTER \
  --config=localllm-workstation \
  --region=us-central1"><div jscontroller="M2lJBf" jsaction="rcuQ6b:npT2md" data-enable-content-editable-input-style="false" data-enable-line-wrapping="" data-read-only=""><div data-loadingmessage="Loading..." jscontroller="GFartf" jsaction="animationend:kWijWc;dyRcpb:dyRcpb" data-active="true" jsname="aZ2wEe"><p>Loading...</p><div jsname="Hxlbvc"><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div><div><div></div><div></div><div></div></div></div></div></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p><span>Then, in the browser, visit </span><span>https://$PORT-$HOSTNAME/docs</span><span>.</span></p>
<h3><strong>Conclusion</strong></h3>
<p><code>localllm</code><span> combined with Cloud Workstations revolutionizes AI-driven application development by letting you use LLMs locally on CPU and memory within the Google Cloud environment. By eliminating the need for GPUs, you can overcome the challenges posed by GPU scarcity and unlock the full potential of LLMs. With enhanced productivity, cost efficiency, and improved data security, localllm lets you build innovative applications with ease. Embrace the power of local LLMs and explore the possibilities within the Google Cloud ecosystem with </span><code>localllm</code><span> and Cloud Workstations today!</span></p></span></section><section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/application-development" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/application-development" track-metadata-module="tag list" track-metadata-module_headline="posted in">Application Development</a></li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li><li><a href="https://cloud.google.com/blog/topics/developers-practitioners" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/topics/developers-practitioners" track-metadata-module="tag list" track-metadata-module_headline="posted in">Developers &amp; Practitioners</a></li></ul></section></section></div></div>
  </body>
</html>
