<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://utcc.utoronto.ca/~cks/space/blog/linux/LoadAverageWhereFrom">Original</a>
    <h1>Where Linux&#39;s load average comes from in the kernel</h1>
    
    <div id="readability-page-1" class="page"><div><h2>Where Linux&#39;s load average comes from in the kernel</h2>

	<p><small>April 18, 2022</small></p>
</div><div><p>Suppose, not hypothetically, that you have a machine that periodically
has its load average briefly soar to relatively absurd levels for
no obvious reason; the machine is normally at, say, 0.5 load average
but briefly spikes to 10 or 15 a number of times a day. You would like
to know why this happens. Capturing &#39;top&#39; output once a minute doesn&#39;t
show anything revealing, and since these spikes are unpredictable it&#39;s
difficult to watch top continuously to possibly catch one in action.
A starting point is to understand how the Linux kernel puts the load
average together.</p>

<p>(This is on a multi-user login machine with a lot of people logged
in, so one obvious general hypothesis is that there is some per-user
background daemon or process that periodically wakes up and sometimes
they&#39;re all synchronized together, creating a brief load spike as they
all try to get scheduled and run at once.)</p>

<p>The core calculations are in <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/sched/loadavg.c">kernel/sched/loadavg.c</a>.
As lots of things will tell you, the load average is &#34;an exponentially
decaying average&#34; of a series of instantaneous samples. These samples
are taken at intervals of <code>LOAD_FREQ</code> (currently set to 5 seconds
in <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/linux/sched/loadavg.h">include/linux/sched/loadavg.h</a>).
To simplify a complicated implementation, the samples are the sum
of the per-CPU counts of the number of running tasks and uninterruptible
tasks (nr_running and nr_uninterruptible). The every five
second load average calculation doesn&#39;t compute these two counts
on the fly; instead they&#39;re maintained by the general kernel
scheduling code and then sampled. This means that if we somehow hook into
this periodic sampling with things like eBPF, we can&#39;t see the exact tasks
and programs involved in creating the load average; more or less the best
we could do would be to see the total numbers at each sample.</p>

<p>(This would already tell us more information than is generally
exposed by &#39;top&#39; and the load average; if nothing else, it might
tell us how fast the spike happens, how high it reaches, and whether
it&#39;s running tasks or tasks waiting on IO.)</p>

<p>When a task adds to or reduces the number of uninterruptible tasks
is somewhat accessible. A task exits this state in ttwu_do_activate()
in <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/sched/core.c">kernel/sched/core.c</a>
under conditions that are probably accessible to eBPF. A task
increases the number of uninterruptible tasks in the depths of
__schedule(), depths which don&#39;t seem amenable to hooking by
eBPF; however I think you might be able to conditionally hook
deactivate_task() to record this.</p>

<p>As you might expect, tasks become runnable or stop being runnable all
somewhat all over the place, and the code that tracks and implements
this is distributed around a number of kernel functions, some of
them inlined ones from headers (plus, the kernel has more than one
scheduler). It&#39;s not clear to me if there&#39;s any readily accessible eBPF
tracepoints or kernel functions that could be used to hook into when a
particular task becomes runnable or stops being runnable. There does
seem to be a scheduler tracepoint for when this number changes, but I&#39;m
not certain if you can extract the task information from the tracepoints
(and I think sometimes a number of tasks can become runnable all at
once).</p>

<p>The current instant value of nr_running is exposed in /proc/loadavg,
as covered in <a href="https://man7.org/linux/man-pages/man5/proc.5.html">proc(5)</a>, and as a result
often makes it into metrics systems. I don&#39;t think nr_uninterruptible
is exposed anywhere that&#39;s readily accessible. However, cgroups v1
does report it through <a href="https://www.kernel.org/doc/Documentation/accounting/">the general taskstats accounting interface</a>, sort of per
<a href="https://www.kernel.org/doc/Documentation/accounting/cgroupstats.rst">cgroupstats.rst</a>, with
the disclaimer that I&#39;m not sure how this interacts with systemd&#39;s
cgroups stunts. The kernel&#39;s <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/accounting/getdelays.c">tools/accounting/getdelays.c</a>
does seem to work system-wide, if you run it as eg &#39;getdelays -C
/sys/fs/cgroup/cpu,cpuacct&#39;, but you need to build the right version
of it; the version in the kernel&#39;s current source tree may not
compile on older kernels.</p>

<p>Having gone through all of this, what I&#39;ve learned is that tracing
this area with eBPF is probably too much work, but we could probably
get a fair with dumping basic process information every few seconds,
since the load average is only updated every five seconds or so and
what matters is the current state of things close to that time.
Sadly I don&#39;t think the kernel offers a way to get a bulk dump of
current processes and their states, say via netlink; instead I think
you have to go through /proc yourself (or let ps do it with an
appropriate output format).</p>
</div></div>
  </body>
</html>
