<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.briankitano.com/llama-from-scratch/">Original</a>
    <h1>Llama from Scratch (or how to implement a paper without crying)</h1>
    
    <div id="readability-page-1" class="page"><div>
    







<p>
    <i>
        <time datetime="2023-08-09">
            09 Aug, 2023
        </time>
    </i>
</p>

<p>I want to provide some tips from my experience implementing a paper. I&#39;m going to cover implementing a dramatically scaled-down version of <a href="https://arxiv.org/pdf/2302.13971.pdf">Llama</a> for training <a href="https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt">TinyShakespeare</a>. This post is heavily inspired by Karpathy&#39;s <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Makemore series</a>, which I highly recommend.</p>
<p>A preview of what we&#39;re going to end up with:</p>
<div><pre><span></span><span>print</span><span>(</span><span>generate</span><span>(</span><span>llama</span><span>,</span> <span>MASTER_CONFIG</span><span>,</span> <span>500</span><span>)[</span><span>0</span><span>])</span>
</pre></div>

<div><pre><span></span>Evend her break of thou thire xoing dieble had side, did foesors exenatedH in siffied up,
    No, none,
    And you ling as thought depond.

    MENENIUS:
    Tell officien:
    To pesiding be
    Best wanty and to spiege,
    To uncine shee patss again,
    I will hen: then they
    Moieth:
    I my cast in letch:
    For bereful, give toan I may

    LINT OF AUMERLE:
    Out, or me but thee here sir,
    Why first with canse pring;
    Now!

    Gide me couuse
    The haster:
    And suilt harming,
    Then as pereise with and go.

    FROMNIUS:
    I well? speak and wieke ac
</pre></div>

<p>I&#39;ll be skipping over some of the more obvious steps, like setting up a virtual environment and installing dependencies.</p>
<p>Github <a href="https://github.com/bkitano/llama-from-scratch">here</a>.</p>

<h2 id="always-work-iteratively-start-small-stay-certain-and-build-up">Always work iteratively: start small, stay certain, and build up.</h2>
<p>My approach for implementing papers is:</p>
<ol>
<li>Make all of the helper functions required to test your model quantitatively (data splits, training, plotting the loss).</li>
<li>Before you even look at the paper, pick a small, simple, and fast model that you&#39;ve done in the past. Then make a helper function to evaluate the model qualitatively.</li>
<li>Start by picking apart different components of the paper, and then implementing them one-by-one, training and evaluating as you go.</li>
</ol>
<h2 id="make-sure-your-layers-do-what-you-think">Make sure your layers do what you think.</h2>
<ol>
<li>Use <code>.shape</code> religiously. <code>assert</code> and <code>plt.imshow</code> are your friends.</li>
<li>Work out the results without matrix multiplication first, and then use the <code>torch</code> functions to make it efficient after.</li>
<li>Have a test to see that your layer is right. For example, the RoPE embeddings have a specific property that you can test for. For the Transformer, you can test that the attention is working by looking at the attention map.</li>
<li>Test your layers on various batch, sequence, and embedding sizes. Even if it works for one size, it might not work for others, which will cause problems at inference time.</li>
</ol>

<h2 id="about-llama">About Llama</h2>
<p>Llama is a transformer-based model for language modeling. Meta AI <a href="https://github.com/facebookresearch/llama">open-sourced</a> Llama this summer, and it&#39;s gained a lot of attention (pun intended). When you&#39;re reading the introduction, they clearly indicate their goal: make a model that&#39;s cheaper for running inference, rather than optimizing training costs.</p>
<p>At this point, we&#39;ll just load our libraries and get started.</p>
<div><pre><span></span><span>import</span> <span>torch</span>
<span>from</span> <span>torch</span> <span>import</span> <span>nn</span>
<span>from</span> <span>torch.nn</span> <span>import</span> <span>functional</span> <span>as</span> <span>F</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>matplotlib</span> <span>import</span> <span>pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>time</span>
<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
</pre></div>

<h2 id="setting-up-our-dataset">Setting up our dataset</h2>
<p>While in Llama they train on 1.4T tokens, our dataset TinyShakespeare, the collection of all of Shakespeare&#39;s works, is about 1M characters.</p>
<div><pre><span></span><span>lines</span> <span>=</span> <span>open</span><span>(</span><span>&#39;./input.txt&#39;</span><span>,</span> <span>&#39;r&#39;</span><span>)</span><span>.</span><span>read</span><span>()</span>

<span>vocab</span> <span>=</span> <span>sorted</span><span>(</span><span>list</span><span>(</span><span>set</span><span>(</span><span>lines</span><span>)))</span>
<span>itos</span> <span>=</span> <span>{</span><span>i</span><span>:</span><span>ch</span> <span>for</span> <span>i</span><span>,</span> <span>ch</span> <span>in</span> <span>enumerate</span><span>(</span><span>vocab</span><span>)}</span>
<span>stoi</span> <span>=</span> <span>{</span><span>ch</span><span>:</span><span>i</span> <span>for</span> <span>i</span><span>,</span> <span>ch</span> <span>in</span> <span>enumerate</span><span>(</span><span>vocab</span><span>)}</span>

<span>print</span><span>(</span><span>lines</span><span>[:</span><span>30</span><span>])</span>
</pre></div>

<div><pre><span></span>First Citizen:
Before we proce
</pre></div>

<p>They use the <a href="https://github.com/google/sentencepiece">SentencePiece</a> byte-pair encoding tokenizer, but we&#39;re going to just use a simple character-level tokenizer.</p>
<div><pre><span></span><span># simple tokenization by characters</span>
<span>def</span> <span>encode</span><span>(</span><span>s</span><span>):</span>
    <span>return</span> <span>[</span><span>stoi</span><span>[</span><span>ch</span><span>]</span> <span>for</span> <span>ch</span> <span>in</span> <span>s</span><span>]</span>

<span>def</span> <span>decode</span><span>(</span><span>l</span><span>):</span>
    <span>return</span> <span>&#39;&#39;</span><span>.</span><span>join</span><span>([</span><span>itos</span><span>[</span><span>i</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>l</span><span>])</span>

<span>print</span><span>(</span><span>&#39;vocab size:&#39;</span><span>,</span> <span>len</span><span>(</span><span>vocab</span><span>))</span>
<span>decode</span><span>(</span><span>encode</span><span>(</span><span>&#34;hello&#34;</span><span>))</span>
</pre></div>



<p>Since our dataset is small enough, we don&#39;t need to worry about how we store it in memory etc.</p>
<p>First tip: I&#39;m creating a <code>config</code> object that stores some basic model params. It makes our code way more readable and removes constants and magic numbers from the code. I&#39;m not going to use types, as  I want to keep things flexible for now and be able to add more parameters later on.</p>
<div><pre><span></span><span>MASTER_CONFIG</span> <span>=</span> <span>{</span>
    <span>&#34;vocab_size&#34;</span><span>:</span> <span>len</span><span>(</span><span>vocab</span><span>),</span>
<span>}</span>
</pre></div>

<div><pre><span></span><span>dataset</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>encode</span><span>(</span><span>lines</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>int8</span><span>)</span>
<span>dataset</span><span>.</span><span>shape</span>
</pre></div>



<p>Let&#39;s create a method to generate our training data and labels for batches. We&#39;ll use the same method for validation and test data. Note that I like to test my functions in the same block that I define them, just to make sure they work as expected before moving on.</p>
<div><pre><span></span><span>def</span> <span>get_batches</span><span>(</span><span>data</span><span>,</span> <span>split</span><span>,</span> <span>batch_size</span><span>,</span> <span>context_window</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>):</span>
    <span>train</span> <span>=</span> <span>data</span><span>[:</span><span>int</span><span>(</span><span>.8</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>))]</span>
    <span>val</span> <span>=</span> <span>data</span><span>[</span><span>int</span><span>(</span><span>.8</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)):</span> <span>int</span><span>(</span><span>.9</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>))]</span>
    <span>test</span> <span>=</span> <span>data</span><span>[</span><span>int</span><span>(</span><span>.9</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)):]</span>

    <span>batch_data</span> <span>=</span> <span>train</span>
    <span>if</span> <span>split</span> <span>==</span> <span>&#39;val&#39;</span><span>:</span>
        <span>batch_data</span> <span>=</span> <span>val</span>

    <span>if</span> <span>split</span> <span>==</span> <span>&#39;test&#39;</span><span>:</span>
        <span>batch_data</span> <span>=</span> <span>test</span>

    <span># pick random starting points</span>
    <span>ix</span> <span>=</span> <span>torch</span><span>.</span><span>randint</span><span>(</span><span>0</span><span>,</span> <span>batch_data</span><span>.</span><span>size</span><span>(</span><span>0</span><span>)</span> <span>-</span> <span>context_window</span> <span>-</span> <span>1</span><span>,</span> <span>(</span><span>batch_size</span><span>,))</span>
    <span>x</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>([</span><span>batch_data</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>context_window</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>ix</span><span>])</span><span>.</span><span>long</span><span>()</span>
    <span>y</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>([</span><span>batch_data</span><span>[</span><span>i</span><span>+</span><span>1</span><span>:</span><span>i</span><span>+</span><span>context_window</span><span>+</span><span>1</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>ix</span><span>])</span><span>.</span><span>long</span><span>()</span>
    <span>return</span> <span>x</span><span>,</span> <span>y</span>

<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>32</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>16</span>
<span>})</span>

<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>[(</span><span>decode</span><span>(</span><span>xs</span><span>[</span><span>i</span><span>]</span><span>.</span><span>tolist</span><span>()),</span> <span>decode</span><span>(</span><span>ys</span><span>[</span><span>i</span><span>]</span><span>.</span><span>tolist</span><span>()))</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span><span>xs</span><span>))]</span>
</pre></div>

<div><pre><span></span><span>[</span><span>(</span><span>&#39;t us sup betimes&#39;</span><span>,</span><span> </span><span>&#39; us sup betimes,&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;IO:</span><span>\n</span><span>Right.</span><span>\n\n</span><span>ROME&#39;</span><span>,</span><span> </span><span>&#39;O:</span><span>\n</span><span>Right.</span><span>\n\n</span><span>ROMEO&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;Nurse:</span><span>\n</span><span>O, she sa&#39;</span><span>,</span><span> </span><span>&#39;urse:</span><span>\n</span><span>O, she say&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#34; seem&#39;d to know,&#34;</span><span>,</span><span> </span><span>&#34;seem&#39;d to know,</span><span>\n</span><span>&#34;</span><span>),</span>
<span> </span><span>(</span><span>&#39; let her brother&#39;</span><span>,</span><span> </span><span>&#39;let her brother &#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;ood,</span><span>\n</span><span>Even with s&#39;</span><span>,</span><span> </span><span>&#39;od,</span><span>\n</span><span>Even with su&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;en</span><span>\n</span><span>Whisper the s&#39;</span><span>,</span><span> </span><span>&#39;n</span><span>\n</span><span>Whisper the sp&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;, fly! for all y&#39;</span><span>,</span><span> </span><span>&#39; fly! for all yo&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#34; Saint Peter&#39;s C&#34;</span><span>,</span><span> </span><span>&#34;Saint Peter&#39;s Ch&#34;</span><span>),</span>
<span> </span><span>(</span><span>&#39;, but that</span><span>\n</span><span>Which&#39;</span><span>,</span><span> </span><span>&#39; but that</span><span>\n</span><span>Which &#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;uld as willingly&#39;</span><span>,</span><span> </span><span>&#39;ld as willingly &#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;y brother,</span><span>\n</span><span>Is pr&#39;</span><span>,</span><span> </span><span>&#39; brother,</span><span>\n</span><span>Is pri&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39; you ready your &#39;</span><span>,</span><span> </span><span>&#39;you ready your s&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;rth the audience&#39;</span><span>,</span><span> </span><span>&#39;th the audience &#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;</span><span>\n\n</span><span>QUEEN ELIZABET&#39;</span><span>,</span><span> </span><span>&#39;</span><span>\n</span><span>QUEEN ELIZABETH&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;ection,</span><span>\n</span><span>which ca&#39;</span><span>,</span><span> </span><span>&#39;ction,</span><span>\n</span><span>which can&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;is wisdom hastes&#39;</span><span>,</span><span> </span><span>&#39;s wisdom hastes &#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39; and quinces in &#39;</span><span>,</span><span> </span><span>&#39;and quinces in t&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;nt death.</span><span>\n\n</span><span>SICIN&#39;</span><span>,</span><span> </span><span>&#39;t death.</span><span>\n\n</span><span>SICINI&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#34;y she&#39;s mad.</span><span>\n\n</span><span>BR&#34;</span><span>,</span><span> </span><span>&#34; she&#39;s mad.</span><span>\n\n</span><span>BRU&#34;</span><span>),</span>
<span> </span><span>(</span><span>&#39;eware of him;</span><span>\n</span><span>Si&#39;</span><span>,</span><span> </span><span>&#39;ware of him;</span><span>\n</span><span>Sin&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;s</span><span>\n</span><span>And make pursu&#39;</span><span>,</span><span> </span><span>&#39;</span><span>\n</span><span>And make pursui&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;r and be slain; &#39;</span><span>,</span><span> </span><span>&#39; and be slain; n&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39; I, with grief a&#39;</span><span>,</span><span> </span><span>&#39;I, with grief an&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;?</span><span>\n\n</span><span>Second Keeper&#39;</span><span>,</span><span> </span><span>&#39;</span><span>\n\n</span><span>Second Keeper:&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;</span><span>\n</span><span>Now, Thomas Mow&#39;</span><span>,</span><span> </span><span>&#39;Now, Thomas Mowb&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;or this once, ye&#39;</span><span>,</span><span> </span><span>&#39;r this once, yea&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#34;l &#39;tis just.</span><span>\n\n</span><span>LU&#34;</span><span>,</span><span> </span><span>&#34; &#39;tis just.</span><span>\n\n</span><span>LUC&#34;</span><span>),</span>
<span> </span><span>(</span><span>&#39;es like a lamb. &#39;</span><span>,</span><span> </span><span>&#39;s like a lamb. Y&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;t night, I warra&#39;</span><span>,</span><span> </span><span>&#39; night, I warran&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;y tears would wa&#39;</span><span>,</span><span> </span><span>&#39; tears would was&#39;</span><span>),</span>
<span> </span><span>(</span><span>&#39;</span><span>\n\n</span><span>ANGELO:</span><span>\n</span><span>Well, &#39;</span><span>,</span><span> </span><span>&#39;</span><span>\n</span><span>ANGELO:</span><span>\n</span><span>Well, l&#39;</span><span>)</span><span>]</span>
</pre></div>

<p>What&#39;s interesting about implementing papers is that there are two aspects to the model <em>working</em>: compilation (do your tensors all match up from layer to layer), and training (does the loss go down). Figuring out how to ensure that each of your components is working is key to developing your model in a predictable, engineering-minded way.</p>
<p>That&#39;s why we&#39;re also going to define the method for how we&#39;re going to evaluate the model. We want to do this before we even define the model, because we want to be able to use it to evaluate the model as we&#39;re training it.</p>
<div><pre><span></span><span>@torch</span><span>.</span><span>no_grad</span><span>()</span>  <span># don&#39;t compute gradients for this function</span>
<span>def</span> <span>evaluate_loss</span><span>(</span><span>model</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>):</span>
    <span>out</span> <span>=</span> <span>{}</span>
    <span>model</span><span>.</span><span>eval</span><span>()</span>
    <span>for</span> <span>split</span> <span>in</span> <span>[</span><span>&#34;train&#34;</span><span>,</span> <span>&#34;val&#34;</span><span>]:</span>
        <span>losses</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>10</span><span>):</span>
            <span>xb</span><span>,</span> <span>yb</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>split</span><span>,</span> <span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>
            <span>_</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xb</span><span>,</span> <span>yb</span><span>)</span>
            <span>losses</span><span>.</span><span>append</span><span>(</span><span>loss</span><span>.</span><span>item</span><span>())</span>
        <span>out</span><span>[</span><span>split</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span><span>losses</span><span>)</span>
    <span>model</span><span>.</span><span>train</span><span>()</span>
    <span>return</span> <span>out</span>
</pre></div>

<h2 id="setting-up-a-working-base-model">Setting up a working base model</h2>
<p>Here&#39;s a basic feed-forward neural network with embeddings. It&#39;s the base model we&#39;re going to start with, and then swap out parts of it as we go along until we eventually end up with the model as described in Llama.</p>
<div><pre><span></span><span>class</span> <span>SimpleBrokenModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        <span>a</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>
        <span>logits</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>a</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>128</span><span>,</span>
<span>})</span>
<span>model</span> <span>=</span> <span>SimpleBrokenModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
</pre></div>



<p>It&#39;s at this point that we have to start worrying about the shape of our tensors and making indices match. Check out this line of our model definition:</p>
<div><pre><span></span><span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
</pre></div>

<p>We have to reshape the <code>logits</code> and <code>targets</code> tensors so that their dimensions match when we compare. We do this with the <code>view</code> method. The <code>-1</code> argument means &#34;infer this dimension from the others&#34;. So, in this case, we&#39;re saying &#34;reshape <code>logits</code> and <code>targets</code> to have the same number of rows, and however many columns are needed to make that happen&#34;. This is a common pattern when you&#39;re working with batches of data.</p>
<p>Alright, let&#39;s train our <code>SimpleBrokenModel</code> to make sure gradients flow. After we confirm that, we can swap out parts of it to match Llama, train again, and track our progress. It&#39;s at this point that I start keeping a <em>log</em> of my training runs, so that I can easily just go back to a previous run in the event that I mess something up.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;epochs&#39;</span><span>:</span> <span>1000</span><span>,</span>
    <span>&#39;log_interval&#39;</span><span>:</span> <span>10</span> 
<span>})</span>
<span>model</span> <span>=</span> <span>SimpleBrokenModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>

<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span>
    <span>model</span><span>.</span><span>parameters</span><span>(),</span> 
<span>)</span>

<span>def</span> <span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>,</span> <span>scheduler</span><span>=</span><span>None</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>,</span> <span>print_logs</span><span>=</span><span>False</span><span>):</span>
    <span>losses</span> <span>=</span> <span>[]</span>
    <span>start_time</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>
    <span>for</span> <span>epoch</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;epochs&#39;</span><span>]):</span>
        <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>

        <span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>
        <span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>targets</span><span>=</span><span>ys</span><span>)</span>
        <span>loss</span><span>.</span><span>backward</span><span>()</span>
        <span>optimizer</span><span>.</span><span>step</span><span>()</span>

        <span>if</span> <span>scheduler</span><span>:</span>
            <span>scheduler</span><span>.</span><span>step</span><span>()</span>

        <span>if</span> <span>epoch</span> <span>%</span> <span>config</span><span>[</span><span>&#39;log_interval&#39;</span><span>]</span> <span>==</span> <span>0</span><span>:</span>
            <span>batch_time</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span> <span>-</span> <span>start_time</span>
            <span>x</span> <span>=</span> <span>evaluate_loss</span><span>(</span><span>model</span><span>)</span>
            <span>losses</span> <span>+=</span> <span>[</span><span>x</span><span>]</span>
            <span>if</span> <span>print_logs</span><span>:</span>
                <span>print</span><span>(</span><span>f</span><span>&#34;Epoch </span><span>{</span><span>epoch</span><span>}</span><span> | val loss </span><span>{</span><span>x</span><span>[</span><span>&#39;val&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span> | Time </span><span>{</span><span>batch_time</span><span>:</span><span>.3f</span><span>}</span><span> | ETA in seconds </span><span>{</span><span>batch_time</span><span> </span><span>*</span><span> </span><span>(</span><span>config</span><span>[</span><span>&#39;epochs&#39;</span><span>]</span><span> </span><span>-</span><span> </span><span>epoch</span><span>)</span><span>/</span><span>config</span><span>[</span><span>&#39;log_interval&#39;</span><span>]</span><span> </span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>)</span>
            <span>start_time</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>

            <span>if</span> <span>scheduler</span><span>:</span>
                <span>print</span><span>(</span><span>&#34;lr: &#34;</span><span>,</span> <span>scheduler</span><span>.</span><span>get_lr</span><span>())</span>

    <span>print</span><span>(</span><span>&#34;validation loss: &#34;</span><span>,</span> <span>losses</span><span>[</span><span>-</span><span>1</span><span>][</span><span>&#39;val&#39;</span><span>])</span>
    <span>return</span> <span>pd</span><span>.</span><span>DataFrame</span><span>(</span><span>losses</span><span>)</span><span>.</span><span>plot</span><span>()</span>

<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>

<div><pre><span></span>model params: 33217
validation loss:  3.9457355260849
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_22_2.png?raw=true"/></p>
<p>Notice how we get a training curve that goes down, but barely by anything. How do we know it&#39;s barely training? We have to use first principles. The cross-entropy loss before training is 4.17, and after 1000 epochs is 3.93. How can we make sense of it intuitively?</p>
<p>Cross-entropy in this context is referring to how likely we are to pick the wrong word. So here,</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></msubsup><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><mi>log</mi><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></p>
<p>where $q(x_i)$ is the probability of picking the right word, as estimated by the model. If $q(x_i)$ is close to 1, then $\log q$ is close to 0; similarly, if $q$ is small, then $\log q$ is a large negative number, so $-\log q$ will be a large positive number. Now to build the intuition: to start, $-\log q = 4.17$, so $q = 0.015$, or around $\frac{1}{64.715}$. Recall that the vocabulary size $|V| = 65$, so what we&#39;re basically saying here is that the model is as good at choosing the next letter as randomly picking from our vocabulary. After training, $-\log q = 3.93$, so we&#39;re now basically choosing between 50 letters. This is a very small improvement, so something is probably wrong.</p>
<p>To get an intuition for how the loss relates to the model&#39;s performance, think about the model choosing among $\tilde V$ tokens; when $\tilde V$ is small, the model is more likely to guess right. In addition, we know $\max \tilde V = V$, which can help us understand if our model is learning at all.</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mover><mi>V</mi><mo stretchy="false">~</mo></mover><mo>=</mo><mi>exp</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></math></p>
<p>Let&#39;s try to debug what&#39;s going on. Notice that in our model we&#39;re using a softmax layer on our logits, which is a function that takes a vector of numbers and squashes them into a probability distribution. But for using the built in <code>F.cross_entropy</code> function, we need to pass in the <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html">unnormalized logits directly</a>. So let&#39;s remove that from our model and try again.</p>
<div><pre><span></span><span>class</span> <span>SimpleModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>SimpleModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>

<div><pre><span></span>model params: 33217
validation loss:  2.5113263607025145
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_24_2.png?raw=true"/></p>
<p>Great, now our loss is $2.54$, so we&#39;re choosing from $12.67$ characters. That&#39;s way better than the 65 we started with. Let&#39;s add a generate method to our model so we visually see the results of our model.</p>
<div><pre><span></span><span>def</span> <span>generate</span><span>(</span><span>model</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>,</span> <span>max_new_tokens</span><span>=</span><span>30</span><span>):</span>
    <span>idx</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>5</span><span>,</span> <span>1</span><span>)</span><span>.</span><span>long</span><span>()</span>
    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>max_new_tokens</span><span>):</span>
        <span># call the model</span>
        <span>logits</span> <span>=</span> <span>model</span><span>(</span><span>idx</span><span>[:,</span> <span>-</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>]:])</span>
        <span>last_time_step_logits</span> <span>=</span> <span>logits</span><span>[</span>
            <span>:,</span> <span>-</span><span>1</span><span>,</span> <span>:</span>
        <span>]</span>  <span># all the batches (1), last time step, all the logits</span>
        <span>p</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>last_time_step_logits</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>  <span># softmax to get probabilities</span>
        <span>idx_next</span> <span>=</span> <span>torch</span><span>.</span><span>multinomial</span><span>(</span>
            <span>p</span><span>,</span> <span>num_samples</span><span>=</span><span>1</span>
        <span>)</span>  <span># sample from the distribution to get the next token</span>
        <span>idx</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>idx</span><span>,</span> <span>idx_next</span><span>],</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>  <span># append to the sequence</span>
    <span>return</span> <span>[</span><span>decode</span><span>(</span><span>x</span><span>)</span> <span>for</span> <span>x</span> <span>in</span> <span>idx</span><span>.</span><span>tolist</span><span>()]</span>

<span>generate</span><span>(</span><span>model</span><span>)</span>
</pre></div>

<div><pre><span></span>[&#39;\nWI in\nThed grtend\nA yod ys wit&#39;,
 &#39;\nY aroticunutser\nE oy mendomed &#39;,
 &#34;\n\nRIf t fan f ses, k be wn&#39;d mo&#34;,
 &#39;\nRu hiseedst den t wat onderyou&#39;,
 &#34;\nARaceps hond wr f\nI&#39; fu kn be &#34;]
</pre></div>

<p>It&#39;s not half bad, but also not half good. But now we have a working model that is training to a validation loss. So here we&#39;ll iterate on our model to make it closer to Llama.</p>
<h2 id="llama-specifics">Llama specifics</h2>
<p>Llama describes three architectural modifications to the original Transformer:</p>
<ol>
<li>RMSNorm for pre-normalization</li>
<li>Rotary embeddings</li>
<li>SwiGLU activation function</li>
</ol>
<p>We&#39;re going to add each one, one at a time to our base model, and iterate.</p>
<h3 id="rmsnorm">RMSNorm</h3>
<p>In Vaswani 2017, the original transformer uses BatchNormalization. In Llama, the authors use RMSNorm, which is where you scale the bector by the variance without centering it. In addition, while Vaswani applies normalization to the output of the attention layer (post-normalization), Llama applies it to the inputs before (pre-normalization).</p>
<div><pre><span></span><span>class</span> <span>RMSNorm</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>layer_shape</span><span>,</span> <span>eps</span><span>=</span><span>1e-8</span><span>,</span> <span>bias</span><span>=</span><span>False</span><span>):</span>
        <span>super</span><span>(</span><span>RMSNorm</span><span>,</span> <span>self</span><span>)</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>register_parameter</span><span>(</span><span>&#34;scale&#34;</span><span>,</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>layer_shape</span><span>)))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
<span>        </span><span>&#34;&#34;&#34;</span>
<span>        assumes shape is (batch, seq_len, d_model)</span>
<span>        &#34;&#34;&#34;</span>
        <span># frob norm is not the same as RMS. RMS = 1/sqrt(N) * frob norm</span>
        <span>ff_rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>*</span> <span>x</span><span>[</span><span>0</span><span>]</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span>
        <span>raw</span> <span>=</span> <span>x</span> <span>/</span> <span>ff_rms</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span>
        <span>return</span> <span>self</span><span>.</span><span>scale</span><span>[:</span><span>x</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>],</span> <span>:]</span><span>.</span><span>unsqueeze</span><span>(</span><span>0</span><span>)</span> <span>*</span> <span>raw</span>

<span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>5</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>11</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>13</span><span>,</span>
<span>}</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>m</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>g</span> <span>=</span> <span>m</span><span>(</span><span>batch</span><span>)</span>
<span>print</span><span>(</span><span>g</span><span>.</span><span>shape</span><span>)</span>
</pre></div>



<p>We want to test to ensure that the RMSNorm is doing what we think it should. We can do this the old-fashioned way: row-wise comparisons. The RMSNorm has the property where the norm of the layer will be the square root of the number of elements in the layer, so we can check that for every layer.</p>
<div><pre><span></span><span>rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>batch</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>*</span> <span>(</span><span>batch</span><span>[</span><span>0</span><span>]</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span><span>)</span>

<span># scaled_batch.var(dim=(1,2))</span>
<span>assert</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>)</span> <span>==</span> <span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>**</span> <span>2</span> <span>)</span><span>.</span><span>sum</span><span>()</span> <span>**</span> <span>.5</span>
<span>rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>)</span> <span>*</span> <span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span><span>.</span><span>float</span><span>()</span> <span>/</span> <span>rms</span><span>),</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>5</span> <span>**</span> <span>.5</span><span>))</span>
<span>ff_rms</span> <span>=</span> <span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>batch</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>*</span> <span>batch</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>:]</span><span>.</span><span>numel</span><span>()</span> <span>**</span> <span>-</span><span>.5</span>

<span># RMS for sure</span>
<span>ffx</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>batch</span><span>)</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>batch</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>ffx</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>batch</span><span>[</span><span>i</span><span>]</span> <span>/</span> <span>ff_rms</span><span>[</span><span>i</span><span>]</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>torch</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>ffx</span><span>,</span> <span>dim</span><span>=</span><span>(</span><span>1</span><span>,</span><span>2</span><span>))</span> <span>**</span> <span>2</span><span>,</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>143</span><span>)</span><span>.</span><span>float</span><span>())</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>ffx</span><span>,</span> <span>g</span><span>)</span>
</pre></div>

<p>Alright, so that&#39;s RMSNorm, and it seems like it&#39;s working. Again, let&#39;s test it out.</p>
<div><pre><span></span><span>class</span> <span>SimpleModel_RMS</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>SimpleModel_RMS</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>

<div><pre><span></span>model params: 35265
validation loss:  2.4792869329452514
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_34_2.png?raw=true"/></p>
<p>So RMSNorm works, and it got our loss down by a small amount.</p>
<h3 id="rotary-embeddings">Rotary Embeddings</h3>
<p><a href="https://arxiv.org/pdf/2104.09864.pdf">RoPE</a> is a kind of positional encoding for transformers. In Attention is All You Need, the authors propose two kinds of positional encodings, learned and fixed. In RoPE, the authors propose embedding the position of a token in a sequence by rotating the embedding, with a different rotation at each position.</p>
<div><pre><span></span><span>def</span> <span>get_rotary_matrix</span><span>(</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>):</span>
        <span>R</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>((</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>embedding_dim</span><span>),</span> <span>requires_grad</span><span>=</span><span>False</span><span>)</span>
        <span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>context_window</span><span>):</span>
            <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embedding_dim</span><span>//</span><span>2</span><span>):</span>
                <span>theta</span> <span>=</span> <span>10000.</span> <span>**</span> <span>(</span><span>-</span><span>2.</span><span>*</span><span>(</span><span>i</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>embedding_dim</span><span>)</span>
                <span>m_theta</span> <span>=</span> <span>position</span> <span>*</span> <span>theta</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>-</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
        <span>return</span> <span>R</span>
</pre></div>

<div><pre><span></span><span>K</span> <span>=</span> <span>3</span>
<span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>10</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>32</span><span>,</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>K</span><span>**</span><span>2</span><span>,</span>
<span>}</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>1</span><span>,</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>fig</span><span>,</span> <span>ax</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>K</span><span>,</span> <span>K</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>K</span> <span>*</span> <span>3</span><span>,</span> <span>K</span> <span>*</span> <span>4</span><span>))</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>K</span><span>):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>K</span><span>):</span>
        <span>ax</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span><span>.</span><span>imshow</span><span>(</span><span>R</span><span>[</span><span>i</span> <span>*</span> <span>K</span> <span>+</span> <span>j</span><span>,</span> <span>:,</span> <span>:]</span><span>.</span><span>detach</span><span>()</span><span>.</span><span>numpy</span><span>())</span>
        <span>ax</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>&#39;rotation at </span><span>{</span><span>i</span><span> </span><span>*</span><span> </span><span>K</span><span> </span><span>+</span><span> </span><span>j</span><span>}</span><span>&#39;</span><span>)</span>
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_38_0.png?raw=true"/></p>
<p>Let&#39;s make sure these work. They should exhibit the quality that
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msubsup><mi>q</mi><mi>m</mi><mi>T</mi></msubsup><msub><mi>k</mi><mi>n</mi></msub><mo>=</mo><mo stretchy="false">(</mo><msubsup><mi>R</mi><mrow><mi>Θ</mi><mo>,</mo><mi>m</mi></mrow><mi>d</mi></msubsup><msub><mi>W</mi><mi>q</mi></msub><msub><mi>x</mi><mi>m</mi></msub><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo stretchy="false">(</mo><msubsup><mi>R</mi><mrow><mi>Θ</mi><mo>,</mo><mi>n</mi></mrow><mi>d</mi></msubsup><msub><mi>W</mi><mi>k</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><msub><mi>W</mi><mi>q</mi></msub><msubsup><mi>R</mi><mrow><mi>Θ</mi><mo>,</mo><mi>n</mi><mo>−</mo><mi>m</mi></mrow><mi>d</mi></msubsup><msub><mi>W</mi><mi>k</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>.</mo></mrow></math></p>
<div><pre><span></span><span>config</span> <span>=</span> <span>{</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>128</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>16</span><span>,</span>
<span>}</span>

<span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>x</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
<span>y</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>

<span>m</span> <span>=</span> <span>3</span>
<span>n</span> <span>=</span> <span>13</span>

<span>x_m</span> <span>=</span> <span>R</span><span>[</span><span>m</span><span>,:,:]</span> <span>@</span> <span>x</span>
<span>x_n</span> <span>=</span> <span>R</span><span>[</span><span>n</span><span>,:,:]</span> <span>@</span> <span>y</span>

<span>assert</span> <span>torch</span><span>.</span><span>isclose</span><span>(</span><span>x_m</span> <span>@</span> <span>x_n</span><span>,</span> <span>x</span> <span>@</span> <span>R</span><span>[</span><span>n</span><span>-</span><span>m</span><span>,:,:]</span> <span>@</span> <span>y</span><span>)</span>
</pre></div>

<p>So the RoPE rotations work as expected.</p>
<div><pre><span></span><span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>10</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>512</span><span>,</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>16</span><span>,</span>
<span>}</span>

<span>class</span> <span>RoPEAttention</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>w_q</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_k</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_v</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>

        <span>self</span><span>.</span><span>multihead</span> <span>=</span> <span>nn</span><span>.</span><span>MultiheadAttention</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;n_heads&#39;</span><span>],</span> <span>dropout</span><span>=</span><span>0.1</span><span>,</span> <span>batch_first</span><span>=</span><span>True</span><span>)</span>
        <span>self</span><span>.</span><span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>

    <span>def</span> <span>get_rotary_matrix</span><span>(</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>):</span>
        <span>R</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>((</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>embedding_dim</span><span>),</span> <span>requires_grad</span><span>=</span><span>False</span><span>)</span>
        <span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>context_window</span><span>):</span>
            <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embedding_dim</span><span>//</span><span>2</span><span>):</span>
                <span>theta</span> <span>=</span> <span>10000.</span> <span>**</span> <span>(</span><span>-</span><span>2.</span><span>*</span><span>(</span><span>i</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>embedding_dim</span><span>)</span>
                <span>m_theta</span> <span>=</span> <span>position</span> <span>*</span> <span>theta</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>-</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
        <span>return</span> <span>R</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>False</span><span>):</span>
        <span>b</span><span>,</span><span>m</span><span>,</span><span>d</span> <span>=</span> <span>x</span><span>.</span><span>shape</span>

        <span>q</span> <span>=</span> <span>self</span><span>.</span><span>w_q</span><span>(</span><span>x</span><span>)</span>
        <span>k</span> <span>=</span> <span>self</span><span>.</span><span>w_k</span><span>(</span><span>x</span><span>)</span>
        <span>v</span> <span>=</span> <span>self</span><span>.</span><span>w_v</span><span>(</span><span>x</span><span>)</span>

        <span>q_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
        <span>k_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>k</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
        <span>v_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>v</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>

        <span>activations</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>self</span><span>.</span><span>multihead</span><span>(</span>
            <span>q_out</span><span>,</span><span>k_out</span><span>,</span><span>v_out</span><span>,</span> 
        <span>)</span>

        <span>if</span> <span>return_attn_weights</span><span>:</span>
            <span>return</span> <span>activations</span><span>,</span> <span>attn_weights</span>
        <span>return</span> <span>activations</span>

<span>layer</span> <span>=</span> <span>RoPEAttention</span><span>(</span><span>config</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>
</pre></div>

<blockquote>
<p>Tip here: know the difference between tensor dimensions at train time vs tensor dimensions at inference time.</p>
</blockquote>
<p>Although at train time, you can expect your tensor dimensions to match your model parameters closely, eg <code>batch.shape = (config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;])</code>, at inference time, you may have to deal with a single example, eg <code>batch.shape = (1, 1, config[&#39;d_model&#39;])</code>. For this reason, you need to make sure that when you&#39;re indexing in the <code>forward</code> pass, you&#39;re indexing using shapes derived from the input, not necessarily the model parameters.</p>
<p>Let&#39;s make sure it does what we think it does. For this layer, we&#39;re going to want to test three things:</p>
<ol>
<li>that it rotates embeddings the way we think it does</li>
<li>that the attention mask used for causal attention is working properly.</li>
</ol>
<div><pre><span></span><span>x</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>

<span>q</span> <span>=</span> <span>layer</span><span>.</span><span>w_q</span><span>(</span><span>x</span><span>)</span>
<span>k</span> <span>=</span> <span>layer</span><span>.</span><span>w_k</span><span>(</span><span>x</span><span>)</span>
<span>v</span> <span>=</span> <span>layer</span><span>.</span><span>w_v</span><span>(</span><span>x</span><span>)</span>

<span>q_rotated</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>x</span><span>)</span>
<span>k_rotated</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>x</span><span>)</span>
<span>v_rotated</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>x</span><span>)</span>

<span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>]):</span>
    <span>q_rotated</span><span>[:,</span><span>position</span><span>,:]</span> <span>=</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>q</span><span>[:,</span><span>position</span><span>,:],</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>position</span><span>,:,:])</span>
    <span>k_rotated</span><span>[:,</span><span>position</span><span>,:]</span> <span>=</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>k</span><span>[:,</span><span>position</span><span>,:],</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>position</span><span>,:,:])</span>
    <span>v_rotated</span><span>[:,</span><span>position</span><span>,:]</span> <span>=</span> <span>torch</span><span>.</span><span>matmul</span><span>(</span><span>v</span><span>[:,</span><span>position</span><span>,:],</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>position</span><span>,:,:])</span>

<span>q_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>layer</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
<span>k_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>k</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>layer</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
<span>v_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>v</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>layer</span><span>.</span><span>R</span><span>))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>

<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)[</span><span>0</span><span>],</span> <span>q</span><span>[:,</span><span>0</span><span>,:])</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)[</span><span>0</span><span>]</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>0</span><span>],</span> <span>q</span><span>[:,</span><span>0</span><span>,:]</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>0</span><span>])</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q_rotated</span><span>,</span> <span>q_out</span><span>)</span>
</pre></div>

<div><pre><span></span><span>config</span> <span>=</span> <span>{</span>
    <span>&#39;batch_size&#39;</span><span>:</span> <span>1</span><span>,</span>
    <span>&#39;d_model&#39;</span><span>:</span> <span>2</span><span>,</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>2</span><span>,</span>
    <span>&#39;context_window&#39;</span><span>:</span> <span>3</span><span>,</span>
<span>}</span>

<span>layer</span> <span>=</span> <span>RoPEAttention</span><span>(</span><span>config</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>

<span>m</span> <span>=</span> <span>0</span>
<span>x_q</span> <span>=</span> <span>batch</span><span>[</span><span>0</span><span>,</span> <span>m</span><span>]</span>
<span>q</span> <span>=</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>m</span><span>,:,:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>(</span><span>x_q</span><span>)</span>

<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>layer</span><span>.</span><span>w_q</span><span>(</span><span>x_q</span><span>),</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span><span>,</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>m</span><span>,</span> <span>:,</span> <span>:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>

<span>n</span> <span>=</span> <span>2</span>
<span>x_k</span> <span>=</span> <span>batch</span><span>[</span><span>0</span><span>,</span> <span>n</span><span>]</span>
<span>k</span> <span>=</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>,:,:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>(</span><span>x_k</span><span>)</span>

<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>layer</span><span>.</span><span>w_k</span><span>(</span><span>x_k</span><span>),</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span> <span>@</span> <span>x_k</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>k</span><span>,</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>,</span> <span>:,</span> <span>:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span> <span>@</span> <span>x_k</span><span>)</span>

<span>assert</span> <span>q</span><span>.</span><span>T</span> <span>@</span> <span>k</span> <span>==</span> <span>q</span> <span>@</span> <span>k</span> <span># transpose is redundant</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span> <span>@</span> <span>k</span><span>,</span> <span>x_k</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>,</span> <span>:,</span> <span>:]</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>m</span><span>,</span> <span>:,</span> <span>:]</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>
<span>assert</span> <span>torch</span><span>.</span><span>allclose</span><span>(</span><span>q</span> <span>@</span> <span>k</span><span>,</span> <span>x_k</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>w_k</span><span>.</span><span>weight</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>R</span><span>[</span><span>n</span><span>-</span><span>m</span><span>,</span> <span>:,</span> <span>:]</span><span>.</span><span>T</span> <span>@</span> <span>layer</span><span>.</span><span>w_q</span><span>.</span><span>weight</span> <span>@</span> <span>x_q</span><span>)</span>
</pre></div>

<div><pre><span></span><span>/</span><span>var</span><span>/</span><span>folders</span><span>/</span><span>w4</span><span>/</span><span>2j887</span><span>mvs097bkhhjpgfzjlyr0000gn</span><span>/</span><span>T</span><span>/</span><span>ipykernel_17478</span><span>/</span><span>2550954139.</span><span>py</span><span>:</span><span>26</span><span>:</span><span> </span><span>UserWarning</span><span>:</span><span> </span><span>The</span><span> </span><span>use</span><span> </span><span>of</span><span> </span><span>`</span><span>x</span><span>.</span><span>T</span><span>`</span><span> </span><span>on</span><span> </span><span>tensors</span><span> </span><span>of</span><span> </span><span>dimension</span><span> </span><span>other</span><span> </span><span>than</span><span> </span><span>2</span><span> </span><span>to</span><span> </span><span>reverse</span><span> </span><span>their</span><span> </span><span>shape</span><span> </span><span>is</span><span> </span><span>deprecated</span><span> </span><span>and</span><span> </span><span>it</span><span> </span><span>will</span><span> </span><span>throw</span><span> </span><span>an</span><span> </span><span>error</span><span> </span><span>in</span><span> </span><span>a</span><span> </span><span>future</span><span> </span><span>release</span><span>.</span><span> </span><span>Consider</span><span> </span><span>`</span><span>x</span><span>.</span><span>mT</span><span>`</span><span> </span><span>to</span><span> </span><span>transpose</span><span> </span><span>batches</span><span> </span><span>of</span><span> </span><span>matrices</span><span> </span><span>or</span><span> </span><span>`</span><span>x</span><span>.</span><span>permute</span><span>(</span><span>*</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>x</span><span>.</span><span>ndim</span><span> </span><span>-</span><span> </span><span>1</span><span>,</span><span> </span><span>-</span><span>1</span><span>,</span><span> </span><span>-</span><span>1</span><span>))</span><span>`</span><span> </span><span>to</span><span> </span><span>reverse</span><span> </span><span>the</span><span> </span><span>dimensions</span><span> </span><span>of</span><span> </span><span>a</span><span> </span><span>tensor</span><span>.</span><span> </span><span>(</span><span>Triggered</span><span> </span><span>internally</span><span> </span><span>at</span><span> </span><span>/</span><span>Users</span><span>/</span><span>runner</span><span>/</span><span>work</span><span>/</span><span>pytorch</span><span>/</span><span>pytorch</span><span>/</span><span>pytorch</span><span>/</span><span>aten</span><span>/</span><span>src</span><span>/</span><span>ATen</span><span>/</span><span>native</span><span>/</span><span>TensorShape</span><span>.</span><span>cpp</span><span>:</span><span>3575.</span><span>)</span>
<span>  </span><span>assert</span><span> </span><span>q</span><span>.</span><span>T</span><span> </span><span>@</span><span> </span><span>k</span><span> </span><span>==</span><span> </span><span>q</span><span> </span><span>@</span><span> </span><span>k</span><span> </span><span># transpose is redundant</span>
</pre></div>

<p>Now let&#39;s inspect the attention weights. Since this is causal, we would expect that due to masking, the upper triangular of the attention should be 0.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;n_heads&#39;</span><span>:</span> <span>8</span><span>,</span>
<span>})</span>
<span>layer</span> <span>=</span> <span>RoPEAttention</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>attn_weights</span><span>[</span><span>0</span><span>]</span><span>.</span><span>detach</span><span>()</span><span>.</span><span>numpy</span><span>(),</span> <span>interpolation</span><span>=</span><span>&#39;nearest&#39;</span><span>)</span>
<span>plt</span><span>.</span><span>colorbar</span><span>()</span>
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_48_1.png?raw=true"/></p>
<p>This is not good; it means that information is leaking across the attention. We need to ensure the causal mask is working.</p>
<div><pre><span></span><span>class</span> <span>RoPEAttention_wMask</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>w_q</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_k</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>w_v</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>bias</span><span>=</span><span>False</span><span>)</span>

        <span>self</span><span>.</span><span>multihead</span> <span>=</span> <span>nn</span><span>.</span><span>MultiheadAttention</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;n_heads&#39;</span><span>],</span> <span>dropout</span><span>=</span><span>0.1</span><span>,</span> <span>batch_first</span><span>=</span><span>True</span><span>)</span>
        <span>self</span><span>.</span><span>R</span> <span>=</span> <span>get_rotary_matrix</span><span>(</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>

    <span>def</span> <span>get_rotary_matrix</span><span>(</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>):</span>
        <span>R</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>((</span><span>context_window</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>embedding_dim</span><span>),</span> <span>requires_grad</span><span>=</span><span>False</span><span>)</span>
        <span>for</span> <span>position</span> <span>in</span> <span>range</span><span>(</span><span>context_window</span><span>):</span>
            <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embedding_dim</span><span>//</span><span>2</span><span>):</span>
                <span>theta</span> <span>=</span> <span>10000.</span> <span>**</span> <span>(</span><span>-</span><span>2.</span><span>*</span><span>(</span><span>i</span> <span>-</span> <span>1</span><span>)</span> <span>/</span> <span>embedding_dim</span><span>)</span>
                <span>m_theta</span> <span>=</span> <span>position</span> <span>*</span> <span>theta</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>-</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>m_theta</span><span>)</span>
                <span>R</span><span>[</span><span>position</span><span>,</span> <span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>,</span><span>2</span><span>*</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>m_theta</span><span>)</span>
        <span>return</span> <span>R</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>False</span><span>):</span>
        <span>b</span><span>,</span><span>m</span><span>,</span><span>d</span> <span>=</span> <span>x</span><span>.</span><span>shape</span>

        <span>q</span> <span>=</span> <span>self</span><span>.</span><span>w_q</span><span>(</span><span>x</span><span>)</span>
        <span>k</span> <span>=</span> <span>self</span><span>.</span><span>w_k</span><span>(</span><span>x</span><span>)</span>
        <span>v</span> <span>=</span> <span>self</span><span>.</span><span>w_v</span><span>(</span><span>x</span><span>)</span>

        <span>q_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>q</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>[:</span><span>m</span><span>,</span> <span>...</span><span>]))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
        <span>k_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>k</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>[:</span><span>m</span><span>,</span> <span>...</span><span>]))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>
        <span>v_out</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>bmm</span><span>(</span><span>v</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>),</span> <span>self</span><span>.</span><span>R</span><span>[:</span><span>m</span><span>,</span> <span>...</span><span>]))</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span>

        <span>activations</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>self</span><span>.</span><span>multihead</span><span>(</span>
            <span>q_out</span><span>,</span><span>k_out</span><span>,</span><span>v_out</span><span>,</span> 
            <span>attn_mask</span><span>=</span><span>nn</span><span>.</span><span>Transformer</span><span>.</span><span>generate_square_subsequent_mask</span><span>(</span><span>m</span><span>),</span>
            <span>is_causal</span><span>=</span><span>True</span>
        <span>)</span>

        <span>if</span> <span>return_attn_weights</span><span>:</span>
            <span>return</span> <span>activations</span><span>,</span> <span>attn_weights</span>
        <span>return</span> <span>activations</span>

<span>layer</span> <span>=</span> <span>RoPEAttention</span><span>(</span><span>config</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>config</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>
</pre></div>

<div><pre><span></span><span>layer</span> <span>=</span> <span>RoPEAttention_wMask</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>batch</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>((</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
<span>output</span><span>,</span> <span>attn_weights</span> <span>=</span> <span>layer</span><span>(</span><span>batch</span><span>,</span> <span>return_attn_weights</span><span>=</span><span>True</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>attn_weights</span><span>[</span><span>0</span><span>]</span><span>.</span><span>detach</span><span>()</span><span>.</span><span>numpy</span><span>())</span>
<span>plt</span><span>.</span><span>colorbar</span><span>()</span>
</pre></div>

<div><pre><span></span>&lt;matplotlib.colorbar.Colorbar at 0x16c2c7b50&gt;
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_51_1.png?raw=true"/></p>
<p>Alright, let&#39;s run it and see what happens.</p>
<div><pre><span></span><span>class</span> <span>RopeModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        <span>self</span><span>.</span><span>rope_attention</span> <span>=</span> <span>RoPEAttention_wMask</span><span>(</span><span>config</span><span>)</span>

        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
        <span>)</span>

        <span>self</span><span>.</span><span>last_linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>])</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>

        <span># one block of attention</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>rope_attention</span><span>(</span><span>x</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>last_linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>RopeModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>

<div><pre><span></span>model params: 150465
validation loss:  2.1157416343688964
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_53_2.png?raw=true"/></p>
<p>It looks like we can drive our loss down even lower. Let&#39;s do that by updating master config.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#34;epochs&#34;</span><span>:</span> <span>5000</span><span>,</span>
    <span>&#34;log_interval&#34;</span><span>:</span> <span>10</span><span>,</span>
<span>})</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>

<div><pre><span></span>validation loss:  1.9027801871299743
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_55_2.png?raw=true"/></p>
<h3 id="swiglu">SwiGLU</h3>
<p>As it says in the paper, &#34;We replace the ReLU non-linearity by the SwiGLU activation function...we use a dimension of $\frac{2}{3} 4d$ isntead of $4d$ as in PaLM.&#34; SwiGLU is defined as:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>SwiGLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mtext>Swish</mtext><mi>β</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mi>W</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>⊗</mo><mo stretchy="false">(</mo><mi>x</mi><mi>V</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></math></p>
<p>where $\otimes$ is a component-wise product. The Swish function is defined as:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mtext>Swish</mtext><mi>β</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi>σ</mi><mo stretchy="false">(</mo><mi>β</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></math></p>
<p>where $\beta$ is a learnable parameter.</p>
<div><pre><span></span><span>class</span> <span>SwiGLU</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
<span>    </span><span>&#34;&#34;&#34;</span>
<span>    Swish-Gated Linear Unit</span>
<span>    https://arxiv.org/pdf/2002.05202v1.pdf</span>
<span>    &#34;&#34;&#34;</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>size</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>linear_gate</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>size</span><span>,</span> <span>size</span><span>)</span>
        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>size</span><span>,</span> <span>size</span><span>)</span>
        <span>self</span><span>.</span><span>beta</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>1</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>

        <span>self</span><span>.</span><span>beta</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>1</span><span>))</span>
        <span>self</span><span>.</span><span>register_parameter</span><span>(</span><span>&#34;beta&#34;</span><span>,</span> <span>self</span><span>.</span><span>beta</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span> 
        <span>swish_gate</span> <span>=</span> <span>self</span><span>.</span><span>linear_gate</span><span>(</span><span>x</span><span>)</span> <span>*</span> <span>torch</span><span>.</span><span>sigmoid</span><span>(</span><span>self</span><span>.</span><span>beta</span> <span>*</span> <span>self</span><span>.</span><span>linear_gate</span><span>(</span><span>x</span><span>))</span>
        <span>out</span> <span>=</span> <span>swish_gate</span> <span>*</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>
        <span>return</span> <span>out</span>
</pre></div>

<div><pre><span></span><span>class</span> <span>RopeModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>
        <span>self</span><span>.</span><span>rope_attention</span> <span>=</span> <span>RoPEAttention_wMask</span><span>(</span><span>config</span><span>)</span>

        <span>self</span><span>.</span><span>linear</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>SwiGLU</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
        <span>)</span>

        <span>self</span><span>.</span><span>last_linear</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>])</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embedding</span><span>(</span><span>idx</span><span>)</span>

        <span># one block of attention</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>rope_attention</span><span>(</span><span>x</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>linear</span><span>(</span><span>x</span><span>)</span>

        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>last_linear</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

        <span>else</span><span>:</span>
            <span>return</span> <span>logits</span>

<span>model</span> <span>=</span> <span>RopeModel</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;train&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>model</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>model</span><span>,</span> <span>optimizer</span><span>)</span>
</pre></div>

<div><pre><span></span>model params: 183490
validation loss:  1.8666490197181702
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_58_2.png?raw=true"/></p>
<p>Now, let&#39;s add multiple layers of RopeAttention by creating blocks.</p>
<div><pre><span></span><span># add RMSNorm and residual conncection</span>
<span>class</span> <span>LlamaBlock</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>

        <span>self</span><span>.</span><span>rms</span> <span>=</span> <span>RMSNorm</span><span>((</span><span>config</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]))</span>

        <span>self</span><span>.</span><span>attention</span> <span>=</span> <span>RoPEAttention_wMask</span><span>(</span><span>config</span><span>)</span>
        <span>self</span><span>.</span><span>feedforward</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>SwiGLU</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
        <span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>attention</span><span>(</span><span>x</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>rms</span><span>(</span><span>x</span><span>)</span> <span># rms pre-normalization</span>
        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>feedforward</span><span>(</span><span>x</span><span>)</span>
        <span>return</span> <span>x</span>
</pre></div>

<div><pre><span></span><span>block</span> <span>=</span> <span>LlamaBlock</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>block</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;d_model&#39;</span><span>]));</span>
</pre></div>

<div><pre><span></span><span>from</span> <span>collections</span> <span>import</span> <span>OrderedDict</span>

<span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;n_layers&#39;</span><span>:</span> <span>4</span><span>,</span>
<span>})</span>
<span>class</span> <span>Llama</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>config</span><span>):</span>
        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>config</span> <span>=</span> <span>config</span>
        <span>self</span><span>.</span><span>embeddings</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>])</span>
        <span>self</span><span>.</span><span>llama_blocks</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>OrderedDict</span><span>([(</span><span>f</span><span>&#34;llama_</span><span>{</span><span>i</span><span>}</span><span>&#34;</span><span>,</span> <span>LlamaBlock</span><span>(</span><span>config</span><span>))</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>config</span><span>[</span><span>&#39;n_layers&#39;</span><span>])])</span>
        <span>)</span>

        <span>self</span><span>.</span><span>ffn</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>SwiGLU</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>]),</span>
            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>config</span><span>[</span><span>&#39;d_model&#39;</span><span>],</span> <span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span>
        <span>)</span>

        <span>print</span><span>(</span><span>&#34;model params:&#34;</span><span>,</span> <span>sum</span><span>([</span><span>m</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>()]))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>,</span> <span>targets</span><span>=</span><span>None</span><span>):</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>embeddings</span><span>(</span><span>idx</span><span>)</span>
        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>llama_blocks</span><span>(</span><span>x</span><span>)</span>
        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>ffn</span><span>(</span><span>x</span><span>)</span>

        <span>if</span> <span>targets</span> <span>is</span> <span>None</span><span>:</span>
            <span>return</span> <span>logits</span>

        <span>else</span><span>:</span>
            <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>logits</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>self</span><span>.</span><span>config</span><span>[</span><span>&#39;vocab_size&#39;</span><span>]),</span> <span>targets</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>))</span>
            <span>return</span> <span>logits</span><span>,</span> <span>loss</span>

<span>llama</span> <span>=</span> <span>Llama</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>llama</span><span>.</span><span>parameters</span><span>())</span>
<span>train</span><span>(</span><span>llama</span><span>,</span> <span>optimizer</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>)</span>
</pre></div>

<div><pre><span></span>model params: 733382
validation loss:  1.616115140914917
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_62_2.png?raw=true"/></p>
<p>It looks like we can drive the loss down even more, and although we&#39;re overfitting a little, I think we can still do better. Let&#39;s train longer.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#39;epochs&#39;</span><span>:</span> <span>10000</span><span>,</span>
<span>})</span>
<span>train</span><span>(</span><span>llama</span><span>,</span> <span>optimizer</span><span>,</span> <span>scheduler</span><span>=</span><span>None</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>)</span>
</pre></div>

<div><pre><span></span>validation loss:  0.9024032115936279
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_64_2.png?raw=true"/></p>
<p>It seems we can go even lower, still without serious overfitting. Either there is a leak, or it&#39;s actually doing well. The loss here is 1.08, which is equivalent to choosing between 2.9 tokens randomly.</p>
<div><pre><span></span><span>train</span><span>(</span><span>llama</span><span>,</span> <span>optimizer</span><span>,</span> <span>config</span><span>=</span><span>MASTER_CONFIG</span><span>)</span>
</pre></div>

<div><pre><span></span>validation loss:  0.746810007095337
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_66_2.png?raw=true"/></p>
<div><pre><span></span><span>print</span><span>(</span><span>generate</span><span>(</span><span>llama</span><span>,</span> <span>MASTER_CONFIG</span><span>,</span> <span>500</span><span>)[</span><span>0</span><span>])</span>
</pre></div>

<div><pre><span></span>Evend her break of thou thire xoing dieble had side, did foesors exenatedH in siffied up,
    No, none,
    And you ling as thought depond.

    MENENIUS:
    Tell officien:
    To pesiding be
    Best wanty and to spiege,
    To uncine shee patss again,
    I will hen: then they
    Moieth:
    I my cast in letch:
    For bereful, give toan I may

    LINT OF AUMERLE:
    Out, or me but thee here sir,
    Why first with canse pring;
    Now!

    Gide me couuse
    The haster:
    And suilt harming,
    Then as pereise with and go.

    FROMNIUS:
    I well? speak and wieke ac
</pre></div>

<p>At this point, we&#39;ve hit the bottom with our training. Let&#39;s test on the test set.</p>
<div><pre><span></span><span>xs</span><span>,</span> <span>ys</span> <span>=</span> <span>get_batches</span><span>(</span><span>dataset</span><span>,</span> <span>&#39;test&#39;</span><span>,</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;batch_size&#39;</span><span>],</span> <span>MASTER_CONFIG</span><span>[</span><span>&#39;context_window&#39;</span><span>])</span>

<span>logits</span><span>,</span> <span>loss</span> <span>=</span> <span>llama</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>

<span>print</span><span>(</span><span>loss</span><span>)</span>
</pre></div>

<div><pre><span></span>tensor(0.8304, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>


<h2 id="check-for-gradient-flows">Check for Gradient Flows</h2>
<p>Let&#39;s inspect the gradients, we want to see how they&#39;re flowing. If there are too many gradients where the value is close to 0, that&#39;s a problem.</p>
<div><pre><span></span><span># print the percentage that are near 0</span>
<span>def</span> <span>show_grads</span><span>(</span><span>model</span><span>,</span> <span>tol</span><span>=</span><span>1e-2</span><span>):</span>
    <span>return</span> <span>sorted</span><span>([(</span><span>name</span><span>,</span> <span>100.0</span> <span>*</span> <span>float</span><span>(</span><span>torch</span><span>.</span><span>sum</span><span>(</span><span>torch</span><span>.</span><span>abs</span><span>(</span><span>param</span><span>)</span> <span>&lt;=</span> <span>tol</span><span>))</span> <span>/</span> <span>float</span><span>(</span><span>param</span><span>.</span><span>nelement</span><span>()))</span> <span>for</span> <span>name</span><span>,</span> <span>param</span> <span>in</span> <span>model</span><span>.</span><span>named_parameters</span><span>()</span> <span>if</span> <span>param</span><span>.</span><span>requires_grad</span><span>],</span> <span>key</span><span>=</span><span>lambda</span> <span>t</span><span>:</span> <span>t</span><span>[</span><span>1</span><span>],</span> <span>reverse</span><span>=</span><span>True</span><span>)</span>

<span>show_grads</span><span>(</span><span>llama</span><span>)</span>
</pre></div>

<div><pre><span></span>[(&#39;llama_blocks.llama_0.attention.multihead.in_proj_bias&#39;, 36.71875),
 (&#39;llama_blocks.llama_3.attention.multihead.in_proj_bias&#39;, 35.9375),
 (&#39;llama_blocks.llama_1.attention.multihead.in_proj_bias&#39;, 33.59375),
 (&#39;llama_blocks.llama_2.attention.multihead.in_proj_bias&#39;, 33.333333333333336),
 (&#39;llama_blocks.llama_0.attention.multihead.in_proj_weight&#39;,
  21.840413411458332),
 (&#39;llama_blocks.llama_0.attention.w_q.weight&#39;, 14.892578125),
 (&#39;llama_blocks.llama_0.attention.multihead.out_proj.weight&#39;, 13.4765625),
 (&#39;llama_blocks.llama_0.attention.w_k.weight&#39;, 12.9638671875),
 (&#39;llama_blocks.llama_0.attention.w_v.weight&#39;, 11.8896484375),
 (&#39;llama_blocks.llama_1.attention.w_v.weight&#39;, 11.285400390625),
 (&#39;llama_blocks.llama_3.attention.multihead.out_proj.weight&#39;, 11.12060546875),
 (&#39;llama_blocks.llama_2.attention.w_v.weight&#39;, 10.68115234375),
 (&#39;llama_blocks.llama_0.feedforward.0.weight&#39;, 10.4248046875),
 (&#39;llama_blocks.llama_2.attention.multihead.out_proj.weight&#39;, 10.36376953125),
 (&#39;llama_blocks.llama_1.attention.multihead.out_proj.weight&#39;, 10.2783203125),
 (&#39;llama_blocks.llama_3.attention.multihead.out_proj.bias&#39;, 10.15625),
 (&#39;llama_blocks.llama_3.attention.w_v.weight&#39;, 9.991455078125),
 (&#39;llama_blocks.llama_0.feedforward.1.linear.weight&#39;, 9.9609375),
 (&#39;llama_blocks.llama_1.attention.multihead.in_proj_weight&#39;,
  9.908040364583334),
 (&#39;llama_blocks.llama_2.attention.multihead.in_proj_weight&#39;, 9.75341796875),
 (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.weight&#39;, 9.66796875),
 (&#39;llama_blocks.llama_1.attention.multihead.out_proj.bias&#39;, 9.375),
 (&#39;llama_blocks.llama_3.attention.multihead.in_proj_weight&#39;, 9.16748046875),
 (&#39;llama_blocks.llama_1.feedforward.0.weight&#39;, 8.77685546875),
 (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.bias&#39;, 8.59375),
 (&#39;llama_blocks.llama_2.feedforward.1.linear.bias&#39;, 8.59375),
 (&#39;llama_blocks.llama_2.feedforward.0.weight&#39;, 8.4228515625),
 (&#39;llama_blocks.llama_1.feedforward.1.linear.weight&#39;, 7.720947265625),
 (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.weight&#39;, 7.501220703125),
 (&#39;llama_blocks.llama_3.feedforward.0.weight&#39;, 7.440185546875),
 (&#39;llama_blocks.llama_2.feedforward.1.linear.weight&#39;, 7.31201171875),
 (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.weight&#39;, 7.196044921875),
 (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.bias&#39;, 7.03125),
 (&#39;llama_blocks.llama_2.attention.multihead.out_proj.bias&#39;, 7.03125),
 (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.bias&#39;, 7.03125),
 (&#39;llama_blocks.llama_2.attention.w_k.weight&#39;, 6.94580078125),
 (&#39;llama_blocks.llama_3.feedforward.1.linear.weight&#39;, 6.927490234375),
 (&#39;llama_blocks.llama_1.attention.w_k.weight&#39;, 6.82373046875),
 (&#39;llama_blocks.llama_2.attention.w_q.weight&#39;, 6.82373046875),
 (&#39;llama_blocks.llama_3.attention.w_k.weight&#39;, 6.585693359375),
 (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.weight&#39;, 6.494140625),
 (&#39;llama_blocks.llama_1.attention.w_q.weight&#39;, 6.396484375),
 (&#39;llama_blocks.llama_1.feedforward.1.linear.bias&#39;, 6.25),
 (&#39;llama_blocks.llama_3.feedforward.1.linear.bias&#39;, 6.25),
 (&#39;llama_blocks.llama_3.attention.w_q.weight&#39;, 6.2255859375),
 (&#39;ffn.0.weight&#39;, 5.633544921875),
 (&#39;llama_blocks.llama_0.feedforward.1.linear.bias&#39;, 5.46875),
 (&#39;ffn.1.linear_gate.bias&#39;, 5.46875),
 (&#39;ffn.1.linear_gate.weight&#39;, 5.2978515625),
 (&#39;ffn.1.linear.weight&#39;, 5.26123046875),
 (&#39;ffn.2.weight&#39;, 4.447115384615385),
 (&#39;ffn.1.linear.bias&#39;, 3.90625),
 (&#39;llama_blocks.llama_0.feedforward.0.bias&#39;, 3.125),
 (&#39;ffn.2.bias&#39;, 3.076923076923077),
 (&#39;llama_blocks.llama_3.feedforward.0.bias&#39;, 2.34375),
 (&#39;ffn.0.bias&#39;, 2.34375),
 (&#39;llama_blocks.llama_0.attention.multihead.out_proj.bias&#39;, 1.5625),
 (&#39;llama_blocks.llama_2.feedforward.0.bias&#39;, 1.5625),
 (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.bias&#39;, 1.5625),
 (&#39;llama_blocks.llama_1.feedforward.0.bias&#39;, 0.78125),
 (&#39;embeddings.weight&#39;, 0.7572115384615384),
 (&#39;llama_blocks.llama_0.rms.scale&#39;, 0.146484375),
 (&#39;llama_blocks.llama_0.feedforward.1.beta&#39;, 0.0),
 (&#39;llama_blocks.llama_1.rms.scale&#39;, 0.0),
 (&#39;llama_blocks.llama_1.feedforward.1.beta&#39;, 0.0),
 (&#39;llama_blocks.llama_2.rms.scale&#39;, 0.0),
 (&#39;llama_blocks.llama_2.feedforward.1.beta&#39;, 0.0),
 (&#39;llama_blocks.llama_3.rms.scale&#39;, 0.0),
 (&#39;llama_blocks.llama_3.feedforward.1.beta&#39;, 0.0),
 (&#39;ffn.1.beta&#39;, 0.0)]
</pre></div>

<p>Here, for all of our parameter gradients, the vast majority are non-zero, which is great. If we start to see this number peak higher, then our gradients would not be flowing.</p>
<h2 id="experiment-with-hyperparams-aka-change-the-oven-settings">Experiment with hyperparams, aka &#34;change the oven settings&#34;</h2>
<p>In the original Llama paper, the authors use Cosine Annealing learning schedule. We didn&#39;t do that here, because I experimented and saw that it was worse.</p>
<div><pre><span></span><span>MASTER_CONFIG</span><span>.</span><span>update</span><span>({</span>
    <span>&#34;epochs&#34;</span><span>:</span> <span>1000</span>
<span>})</span>
<span>llama_with_cosine</span> <span>=</span> <span>Llama</span><span>(</span><span>MASTER_CONFIG</span><span>)</span>
<span>llama_optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span>
    <span>llama</span><span>.</span><span>parameters</span><span>(),</span> 
    <span>betas</span><span>=</span><span>(</span><span>.9</span><span>,</span> <span>.95</span><span>),</span> 
    <span>weight_decay</span><span>=</span><span>.1</span><span>,</span> 
    <span>eps</span><span>=</span><span>1e-9</span><span>,</span> 
    <span>lr</span><span>=</span><span>1e-3</span>
<span>)</span>
<span>scheduler</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>lr_scheduler</span><span>.</span><span>CosineAnnealingLR</span><span>(</span><span>llama_optimizer</span><span>,</span> <span>300</span><span>,</span> <span>eta_min</span><span>=</span><span>1e-5</span><span>)</span>
<span>train</span><span>(</span><span>llama_with_cosine</span><span>,</span> <span>llama_optimizer</span><span>,</span> <span>scheduler</span><span>=</span><span>scheduler</span><span>)</span>
</pre></div>

<div><pre><span></span><span>model</span><span> </span><span>params</span><span>:</span><span> </span><span>733382</span>


<span>/</span><span>Users</span><span>/</span><span>bkitano</span><span>/</span><span>Desktop</span><span>/</span><span>projects</span><span>/</span><span>llama</span><span>/</span><span>.</span><span>llama</span><span>/</span><span>lib</span><span>/</span><span>python3</span><span>.1</span><span>1</span><span>/</span><span>site</span><span>-</span><span>packages</span><span>/</span><span>torch</span><span>/</span><span>optim</span><span>/</span><span>lr_scheduler</span><span>.</span><span>py</span><span>:</span><span>814</span><span>:</span><span> </span><span>UserWarning</span><span>:</span><span> </span><span>To</span><span> </span><span>get</span><span> </span><span>the</span><span> </span><span>last</span><span> </span><span>learning</span><span> </span><span>rate</span><span> </span><span>computed</span><span> </span><span>by</span><span> </span><span>the</span><span> </span><span>scheduler</span><span>,</span><span> </span><span>please</span><span> </span><span>use</span><span> </span><span>`</span><span>get_last_lr</span><span>()</span><span>`</span><span>.</span>
<span>  </span><span>warnings</span><span>.</span><span>warn</span><span>(</span><span>&#34;To get the last learning rate computed by the scheduler, &#34;</span>


<span>lr</span><span>:</span><span>  </span><span>[</span><span>0.0</span><span>009999457184159408</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>0.0</span><span>009961510274583004</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>0.0</span><span>009869757772816292</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>0.0</span><span>009725204933511963</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>0.0</span><span>009529435502760634</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>0.0</span><span>009284594366176498</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>0.0</span><span>008993364049014041</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>7.1</span><span>28036241775617</span><span>e</span><span>-</span><span>05</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>4.8</span><span>72936226262451</span><span>e</span><span>-</span><span>05</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>3.1</span><span>17756953567661</span><span>e</span><span>-</span><span>05</span><span>]</span>
<span>lr</span><span>:</span><span>  </span><span>[</span><span>1.8</span><span>816750064937722</span><span>e</span><span>-</span><span>05</span><span>]</span>
<span>validation</span><span> </span><span>loss</span><span>:</span><span>  </span><span>4.1</span><span>79551410675049</span>
</pre></div>

<p><img alt="png" src="https://github.com/bkitano/llama-from-scratch/blob/main/llama_files/llama_76_4.png?raw=true"/></p>
<div><pre><span></span><span>show_grads</span><span>(</span><span>llama_with_cosine</span><span>,</span> <span>1e-5</span><span>)</span>
</pre></div>

<div><pre><span></span>[(&#39;llama_blocks.llama_0.attention.multihead.in_proj_bias&#39;, 100.0),
 (&#39;llama_blocks.llama_0.attention.multihead.out_proj.bias&#39;, 100.0),
 (&#39;llama_blocks.llama_1.attention.multihead.in_proj_bias&#39;, 100.0),
 (&#39;llama_blocks.llama_1.attention.multihead.out_proj.bias&#39;, 100.0),
 (&#39;llama_blocks.llama_2.attention.multihead.in_proj_bias&#39;, 100.0),
 (&#39;llama_blocks.llama_2.attention.multihead.out_proj.bias&#39;, 100.0),
 (&#39;llama_blocks.llama_3.attention.multihead.in_proj_bias&#39;, 100.0),
 (&#39;llama_blocks.llama_3.attention.multihead.out_proj.bias&#39;, 100.0),
 (&#39;llama_blocks.llama_1.feedforward.1.linear.bias&#39;, 0.78125),
 (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.weight&#39;, 0.030517578125),
 (&#39;llama_blocks.llama_3.attention.w_q.weight&#39;, 0.030517578125),
 (&#39;llama_blocks.llama_0.attention.multihead.out_proj.weight&#39;, 0.0244140625),
 (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.weight&#39;, 0.0244140625),
 (&#39;llama_blocks.llama_0.feedforward.1.linear.weight&#39;, 0.0244140625),
 (&#39;llama_blocks.llama_1.attention.w_v.weight&#39;, 0.0244140625),
 (&#39;llama_blocks.llama_1.feedforward.1.linear.weight&#39;, 0.0244140625),
 (&#39;llama_blocks.llama_2.attention.w_q.weight&#39;, 0.0244140625),
 (&#39;llama_blocks.llama_3.attention.w_v.weight&#39;, 0.0244140625),
 (&#39;llama_blocks.llama_3.attention.multihead.out_proj.weight&#39;, 0.0244140625),
 (&#39;ffn.2.weight&#39;, 0.02403846153846154),
 (&#39;llama_blocks.llama_0.attention.w_v.weight&#39;, 0.018310546875),
 (&#39;llama_blocks.llama_0.feedforward.0.weight&#39;, 0.018310546875),
 (&#39;llama_blocks.llama_1.feedforward.0.weight&#39;, 0.018310546875),
 (&#39;llama_blocks.llama_3.attention.w_k.weight&#39;, 0.018310546875),
 (&#39;llama_blocks.llama_3.feedforward.0.weight&#39;, 0.018310546875),
 (&#39;llama_blocks.llama_0.attention.multihead.in_proj_weight&#39;,
  0.016276041666666668),
 (&#39;llama_blocks.llama_1.attention.multihead.out_proj.weight&#39;, 0.01220703125),
 (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.weight&#39;, 0.01220703125),
 (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.weight&#39;, 0.01220703125),
 (&#39;llama_blocks.llama_3.feedforward.1.linear.weight&#39;, 0.01220703125),
 (&#39;llama_blocks.llama_0.attention.w_q.weight&#39;, 0.006103515625),
 (&#39;llama_blocks.llama_0.attention.w_k.weight&#39;, 0.006103515625),
 (&#39;llama_blocks.llama_1.attention.w_q.weight&#39;, 0.006103515625),
 (&#39;llama_blocks.llama_1.attention.multihead.in_proj_weight&#39;, 0.006103515625),
 (&#39;llama_blocks.llama_2.attention.multihead.in_proj_weight&#39;, 0.006103515625),
 (&#39;llama_blocks.llama_2.attention.multihead.out_proj.weight&#39;, 0.006103515625),
 (&#39;llama_blocks.llama_2.feedforward.1.linear.weight&#39;, 0.006103515625),
 (&#39;llama_blocks.llama_3.attention.multihead.in_proj_weight&#39;,
  0.004069010416666667),
 (&#39;embeddings.weight&#39;, 0.0),
 (&#39;llama_blocks.llama_0.rms.scale&#39;, 0.0),
 (&#39;llama_blocks.llama_0.feedforward.0.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_0.feedforward.1.beta&#39;, 0.0),
 (&#39;llama_blocks.llama_0.feedforward.1.linear_gate.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_0.feedforward.1.linear.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_1.rms.scale&#39;, 0.0),
 (&#39;llama_blocks.llama_1.attention.w_k.weight&#39;, 0.0),
 (&#39;llama_blocks.llama_1.feedforward.0.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_1.feedforward.1.beta&#39;, 0.0),
 (&#39;llama_blocks.llama_1.feedforward.1.linear_gate.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_2.rms.scale&#39;, 0.0),
 (&#39;llama_blocks.llama_2.attention.w_k.weight&#39;, 0.0),
 (&#39;llama_blocks.llama_2.attention.w_v.weight&#39;, 0.0),
 (&#39;llama_blocks.llama_2.feedforward.0.weight&#39;, 0.0),
 (&#39;llama_blocks.llama_2.feedforward.0.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_2.feedforward.1.beta&#39;, 0.0),
 (&#39;llama_blocks.llama_2.feedforward.1.linear_gate.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_2.feedforward.1.linear.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_3.rms.scale&#39;, 0.0),
 (&#39;llama_blocks.llama_3.feedforward.0.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_3.feedforward.1.beta&#39;, 0.0),
 (&#39;llama_blocks.llama_3.feedforward.1.linear_gate.bias&#39;, 0.0),
 (&#39;llama_blocks.llama_3.feedforward.1.linear.bias&#39;, 0.0),
 (&#39;ffn.0.weight&#39;, 0.0),
 (&#39;ffn.0.bias&#39;, 0.0),
 (&#39;ffn.1.beta&#39;, 0.0),
 (&#39;ffn.1.linear_gate.weight&#39;, 0.0),
 (&#39;ffn.1.linear_gate.bias&#39;, 0.0),
 (&#39;ffn.1.linear.weight&#39;, 0.0),
 (&#39;ffn.1.linear.bias&#39;, 0.0),
 (&#39;ffn.2.bias&#39;, 0.0)]
</pre></div>

<p>Even at an extremely low tolerance, the attention biases are not getting any signal. I&#39;m not sure why the learning schedule from the paper doesn&#39;t work, but the lesson here is simple: start simple.</p>



    

    
    



    



  </div></div>
  </body>
</html>
