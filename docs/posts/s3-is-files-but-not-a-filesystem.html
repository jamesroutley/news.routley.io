<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://calpaterson.com/s3.html">Original</a>
    <h1>S3 is files, but not a filesystem</h1>
    
    <div id="readability-page-1" class="page"><div>
        <article>
            
            <p><time datetime="2024-03-05T00:00:00Z">March 2024</time></p>
            <p id="article-description">&#34;Deep&#34; modules, mismatched
            interfaces - and why SAP is so painful</p>
            <figure>
                <picture><source srcset="/images/photo/cal-misc.avif" type="image/avif"/> <source srcset="/images/photo/cal-misc.webp" type="image/webp"/> <img src="https://calpaterson.com/images/photo/cal-misc.jpeg" alt="a box labelled: CAL&#39;S MISC" height="450" width="800"/></picture>
                <figcaption>
                    My very own &#34;object store&#34;
                </figcaption>
            </figure>
            <p>Amazon S3 is the original cloud technology: it came out in 2006.
            &#34;Objects&#34; were popular at the time and S3 was labelled an &#34;object store&#34;,
            but everyone really knows that S3 is for files. S3 is a cloud filesystem,
            not an object-whatever.</p>
            <p>I think the idea that S3 is <em>really</em> &#34;Amazon Cloud Filesystem&#34; is
            a bit of a load bearing fiction. It&#39;s sort of true: S3 can store files.
            It&#39;s also a very useful belief in getting people to adopt S3, a
            fundamentally good technology, which otherwise they might not. But it&#39;s
            false: S3 is not a filesystem and can&#39;t stand in for one.</p>
            <h2>What filesystems are about, and module &#34;depth&#34;</h2>
            <p>The unix file API is pretty straightforward. There are just five basic
            functions. They don&#39;t take many arguments.</p>
            <p>Here are (the Python versions of) these five basic functions:</p>
            <div>
                <pre><span></span><span># open a file</span>
<span>open</span><span>(</span><span>filepath</span><span>)</span> <span># returns a `file`</span>

<span># read from that file (moving the position forward)</span>
<span>file</span><span>.</span><span>read</span><span>(</span><span>size</span><span>=</span><span>100</span><span>)</span> <span># returns 100 bytes</span>

<span># write to that file (moving the position forward)</span>
<span>file</span><span>.</span><span>write</span><span>(</span><span>&#34;hello, world&#34;</span><span>)</span>

<span># move the position to byte 94</span>
<span>file</span><span>.</span><span>seek</span><span>(</span><span>94</span><span>)</span>

<span># close the file</span>
<span>file</span><span>.</span><span>close</span><span>()</span>
</pre>
            </div>
            <p>Well, perhaps I should add an asterisk: I am simplifying a bit. There
            are loads more calls than that. But still, those five calls are the
            irreducible nub of the file API. They&#39;re all you need to read and write
            files.</p>
            <p>Those five functions handle a lot of concerns:</p>
            <ul>
                <li>buffering</li>
                <li>the page cache</li>
                <li>fragmentation</li>
                <li>permissions</li>
                <li>IO scheduling</li>
                <li>and whatever else</li>
            </ul>
            <p>Even though the file API <em>handles</em> all those concerns, it doesn&#39;t
            <em>expose</em> them to you. A narrow interface handling a large number of
            concerns - that makes the unix file API a &#34;deep&#34; module.</p>
            <figure>
                <img src="https://calpaterson.com/assets/wide-vs-narrow.svg" alt="diagram of a deep vs a shallow diagram"/>
            </figure>
            <p>Deep modules are great because you can benefit from their features -
            like wear-levelling on SD cards - but without bearing the psychic toll of
            thinking about any of it as you save a jpeg to your phone. Happy days.</p>
            <p>But if the file API is &#34;deep&#34;, what sorts of things are &#34;shallow&#34;?</p>
            <p>A shallow module would have a relatively large API surface in proportion
            to what it&#39;s handling for you. One hint these days that a module is shallow
            is that the interface to it is YAML. YAML <em>appears</em> to be a mere
            markup language but in practice is a reuseable syntax onto which almost any
            semantics can be plonked.</p>
            <p>Often YAML works as the &#34;Programming language of DevOps&#34; and programming
            languages provide about the widest interface possible. Examine your YAML
            micro-language closely. Does it offer a looping construct? If so, it&#39;s
            likely Turing complete.</p>
            <p>But sometimes it is hard to package something up nicely with a bow on
            top. SQL ORMs are inherently a leaky abstraction. You can&#39;t use them
            without some understanding of SQL. So being shallow isn&#39;t inherently a
            criticism. Sometimes a shallow module is the best that can be done. But all
            else equal, deeper is better.</p>
            <h2>What S3 is about (it is deep too)</h2>
            <p>The unix file API was in place by the early 1970s. The interface has
            been retained and the guts have been re-implemented many times for
            compatibility reasons.</p>
            <p>But Amazon S3 <em>does not</em> reimplement the unix filesystem API.</p>
            <p>It has a wholly different arrangement and the primitives are only partly
            compatible. Here&#39;s a brief description of the calls that are analogous to
            the above five basic unix calls:</p>
            <div>
                <pre><span></span><span># Read (part) of an object</span>
<span>GetObject</span><span>(</span><span>Bucket</span><span>,</span> <span>Key</span><span>,</span> <span>Range</span><span>=</span><span>None</span><span>)</span> <span># contents is the HTTP body</span>

<span># Write an (entire) object</span>
<span>PutObject</span><span>(</span><span>Bucket</span><span>,</span> <span>Key</span><span>)</span> <span># send contents as HTTP body</span>

<span># er, that&#39;s it!</span>
</pre>
            </div>
            <p>Two functions versus five. That&#39;s right, the S3 API is simpler than the
            unix file API. There is one additional concept (&#34;buckets&#34;) but I think when
            you net it out, S3&#39;s interface-to-functionality ratio is even better than
            the unix file API.</p>
            <p>But something is missing. While you can partially read an object using
            the <code>Range</code> argument to <code>GetObject</code>, <strong>you
            can&#39;t overwrite partially</strong>. Overwrites have to be the whole
            file.</p>
            <p>That sounds minor but actually scopes S3 to a subset of the old usecases
            for files.</p>
            <h2>Filesystem software, especially databases, can&#39;t be ported to Amazon
            S3</h2>
            <p>Databases of all kinds need a place to put their data. Generally, that
            place has ended up being various files on the filesystem. Postgres
            maintains two or three files per table, plus loads of others for
            bookkeeping. SQLite famously stores everything in a single file. MySQL,
            MongoDB, Elasticsearch - whatever - they all store data in files.</p>
            <p>Crucially, these databases overwhelmingly <em>rely</em> on the ability
            to do partial overwrites. They store data in <a href="https://calpaterson.com/how-a-sql-database-works.html">&#34;pages&#34;</a> (eg 4 or 8 kilobytes long) in
            &#34;heap&#34; files where writes are done page by page. There might be thousands
            of pages in a single file. Pages are overwritten as necessary to store
            whatever data is required. That means partial overwrites are absolutely
            essential.</p>
            <figure>
                <img src="https://calpaterson.com/assets/heap.svg" alt="diagram of a database heap file"/>
                <figcaption>
                    A heap file is full of pages (and empty slots). Pages are
                    overwritten individually as necessary.
                </figcaption>
            </figure>
            <p>Some software projects start with a dream of storing their data in a
            &#39;simple&#39; way by combining two well tested technologies: Amazon S3 and
            SQLite (or DuckDB). After all, what could be simpler and more
            straightforward? Sadly, they go together like oil and water.</p>
            <p>When your SQLite database is kept in S3, each write suddenly becomes a
            total overwrite of the entire database. While S3 can do big writes fast,
            even it isn&#39;t fast enough to make that strategy work for any but the
            smallest datasets. And you&#39;re jettisoning all the transactional integrity
            that the database authors have painstakingly implemented: rewriting the
            database file each time throws out all that stuff. On S3, the last write
            wins.</p>
            <h2>What S3 is good at and what it is bad at</h2>
            <p>The joy of S3 is that bandwidth (&#34;speed&#34;) for reads and writes is
            extremely, extremely high. It&#39;s not hard to find examples online of people
            who have written to or read from S3 at over 10 gigabytes per second. In
            fact I once saturated a financial client&#39;s office network with a set of S3
            writes.</p>
            <p>But the lack of partial overwrites isn&#39;t the only problem. There are a
            few more.</p>
            <p>S3 has no rename or move operation. Renaming is <code>CopyObject</code>
            and then <code>DeleteObject</code>. <code>CopyObject</code> takes linear
            time to the size of the file(s). This comes up fairly often when someone
            has written a lot of files to the wrong place - moving the files back is
            very slow.</p>
            <p>And listing files is slow. While the joy of Amazon S3 is that you can
            read and write at extremely, extremely, high bandwidths, listing out what
            is there is much much slower. Slower than a slow local filesystem.</p>
            <p>But S3 is much lower maintenance than a filesystem. You just name the
            bucket, name the key and the cloud elves will sort out everything else.
            This is worth a lot as setting backups, replicating offsite, provisioning
            (which, remember is for IO ops as well as capacity) is pure drudgework.</p>
            <h2>Module depth is even more important across organisations</h2>
            <p>In retrospect it is not a surprise that S3 was the first popular cloud
            API. If deep APIs are helpful in containing the complexity between
            different modules with a single system (like your computer) they are even
            more helpful in containing the complexity of an interaction between two
            different businesses, where the costs of interacting are so much
            higher.</p>
            <p>Consider a converse example. Traditionally when one business wants to
            get it&#39;s computers working with those of another they call it
            &#34;integration&#34;. It is a byword for suffering. Imagine you are tasked with
            integrating some Big Entreprise software horror into your organisation.
            Something like SAP. Is SAP a deep module? No. The tragedy of SAP is that
            almost your entire organisation has to understand it. Then you have to
            reconcile it with everything you&#39;re doing. At all times. SAP integration
            projects are consequently expensive, massive <a href="https://www.zdnet.com/article/millercoors-sues-hcl-tech-for-100-million-over-failure-to-implement-erp-project/">
            and</a> <a href="https://www.henricodolfing.com/2020/01/project-failure-case-study-leaseplan-sap.html">
            regularly</a> <a href="https://www.henricodolfing.com/2020/05/case-study-lidl-sap-debacle.html">fail</a>.</p>
            <p>There isn&#39;t much less complexity in S3 than there is in a SAP
            installation. Amazon named it the &#34;Simple Storage Service&#34; but the amount
            of complexity in S3 is <a href="https://brooker.co.za/blog/2023/03/23/economics.html">pretty
            frightening</a>. Queueing theory, IO contention, sharding, the list of
            problems just goes on and on - in addition too all the stuff I listed above
            that filesystems deal with. (And can you believe they do it all
            on-prem?)</p>
            <p>The &#34;simple&#34; in S3 is a misnomer. S3 is not actually simple. It&#39;s
            deep.</p>
            <hr/>
            <h2>Contact/etc</h2>
            
            <hr/>
            <h2>Other notes</h2>
            <p>I don&#39;t mean to suggest in any way via this article that S3 is not
            <a href="https://calpaterson.com/amazon-premium.html">overpriced for what it is</a>. To rephrase a
            famous joke about hedge funds, it often seems like The Cloud is a revenue
            model masquerading as a service model.</p>
            <p>The concept of deep vs shallow modules comes from John Ousterhout&#39;s
            <a href="https://www.amazon.co.uk/Philosophy-Software-Design-2nd/dp/173210221X">excellent
            book</a>. The book is effectly a list of ideas on software design. Some are
            real hits with me, others not, but well worth reading overall. Praise for
            making it succinct.</p>
            <p>A few databases are explicitly designed from the start to us the S3 API
            for storage. <a href="https://event.cwi.nl/lsde/papers/p215-dageville-snowflake.pdf">Snowflake
            was.</a> So it&#39;s possible - but not transparently. But snowflake is one of
            the few I&#39;m aware of (and they made this decision very early, at least by
            2016). If you know of others - let me know by email.</p>
            <p>It isn&#39;t just databases that struggle on S3. Many file formats assume
            that you&#39;ll be able to seek around cheaply and are less performant on S3
            than on disk. Zipfiles are a key example.</p>
            <h3>Other stuff about S3 that is a matter for regret</h3>
            <p>I genuinely like S3 so did not want to create the wrong impression by
            including a laundry list of complaints in the middle of the post but anyway
            here are the other major problems I didn&#39;t mention above:</p>
            <ol>
                <li>
                    <p>The S3 API is <em>only</em> available as XML. JSON was around in
                    2006 but XML was still dominant and so it&#39;s probably not a surprise
                    that Amazon picked XML originally. It is a surprise that Amazon
                    never released a JSON version though - particularly when they made
                    the switch from SOAP to REST, which would have been a good
                    time.</p>
                </li>
                <li>
                    <p>It&#39;s also a matter for regret that Amazon <a href="http://doc.s3.amazonaws.com/2006-03-01/AmazonS3.xsd">gave up</a>
                    on maintaining the XSD schema as this is one of the key benefits of
                    XML for APIs. The canonical documentation is just a website
                    now.</p>
                </li>
                <li>
                    <p>Criminally, Amazon - like many cloud service providers - have
                    never produced any kind of local test environment. In Python, the
                    more diligent test with the <a href="https://github.com/getmoto/moto">moto</a> library. moto is
                    maintained by volunteers which is weird given that it&#39;s a testing
                    tool for a commercial offering.</p>
                </li>
                <li>
                    <p>Amazon S3 <em>does</em> support checksums. For whatever reason
                    they are not turned on by default. Amazon makes many claims about
                    durability. I haven&#39;t heard of people having problems but equally:
                    I&#39;ve never seen these claims tested. I am at least a bit curious
                    about these claims.</p>
                </li>
                <li>
                    <p>For years Amazon S3 held one other trap for the unwary: eventual
                    consistency. If you read a file, then overwrote it, you might read
                    it back and find it hadn&#39;t changed yet. Particularly because it
                    only happened sometimes, for short periods of time, this caused all
                    manner of chaos. Other implementors of S3 didn&#39;t copy this property
                    and a few years ago <a href="https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/">
                    Amazon fixed it in their implementation</a>.</p>
                </li>
            </ol>
        </article>
    </div></div>
  </body>
</html>
