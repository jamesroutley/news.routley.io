<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.surgehq.ai/blog/are-popular-toxicity-models-simply-profanity-detectors">Original</a>
    <h1>Are popular toxicity models simply profanity detectors?</h1>
    
    <div id="readability-page-1" class="page"><div><p>Imagine that after months of waiting, your favorite singer ‚Äì Britney Spears ‚Äì has released a new album. It lives up to all the hype, and you rush to post on your favorite social media site:<br/></p><blockquote>Holy shit. This album is fucking amazing!<br/></blockquote><p>Strangely, you get an instant notification that your post is <strong><em>Under Review</em></strong>.<br/></p><p>Maybe it‚Äôs the word ‚Äúfucking‚Äù? You try again:<br/></p><blockquote>Holy shit! This album is amaaazing<br/></blockquote><p>No dice. You try one last time:<br/></p><blockquote>fuck yes. the OG bad bitch is BACK<br/></blockquote><p>This time, you get warned of an account suspension.<br/></p><h2><strong>The challenges of language</strong><br/></h2><p>It‚Äôs tempting to believe that AI has progressed far enough that identifying hateful content is a solved problem. Isn‚Äôt this the promise of contextual word embeddings and transformers?<br/></p><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467c81e8465b7ebe12e6_pXMrmyM_UfrZtdaJCq7zDd4HrIuDUp0HwevrXtHHi7D50BjwB5mqtNVKSfSFdjdGdpV9P9RX8dMTnLWQTrrAaT6UMnybHO0dgm1BFhbYLoLFENLv-gmopOgVquSWP9U_E4zrWA19.png" alt=""/></p></figure><p>But language is complicated. The strongest profanities are often used in the most positive, life-affirming ways. This is a problem: people‚Äôs biggest and most enthusiastic fans ‚Äì the ones whose content you love seeing and spreading ‚Äì are getting hidden. Talk about terrible false positives!<br/></p><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467c62999879466f7e23_usxpOPM2MgieqdVQxdltYqb7oX7fhVO5aSx-t0q0GI4csGhk4v_62YRMELCZot4yvt6cdETEj-vM0bgo9rbqg8p6Pl2kaLK12-tsA1A5gHE0x26uLxNqpe41XgfrDoQmW7ylIPPx.png" alt=""/></p><figcaption>I just love her, she&#39;s so me, what a bad bitch.</figcaption></figure><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467c13a37cc1a4af226a_ewS0QuLfyTdceYoEhI_TJBOwCC2pxFzp2pIhGFmGNsPlAikINxexK7G_MZYqFRKw_KLuNXNS5P-_j82aaP5BUtYI5Uk5pdFoUL-EQqk09cii0RUwwv8lJlrjGfVkbR1PQx5c9zoD.png" alt=""/></p><figcaption>Bad bitch contest you in first place</figcaption></figure><p>In our work, we run into cases like these a lot. Much of the problem stems from <strong>poor training and test data</strong>: NLP datasets are often created using non-fluent labelers who pattern match on profanity.<br/></p><p>For example, here are 10 examples from Google‚Äôs <a href="https://ai.googleblog.com/2021/10/goemotions-dataset-for-fine-grained.html">GoEmotions dataset</a> (a dataset of Reddit comments, tagged with 27 emotion categories)¬†that were labeled as <strong>Anger</strong> by the India-based raters that they used.</p><ul role="list"><li>Hot damn.</li><li>you almost blew my fucking mind there.</li><li>damn, congratulation ü§£</li><li>A-fucking-men! :)</li><li>YOU STOLE MY GODDAMN COMMENT! &lt;3</li><li>Wow! Good for her! I‚Äôm so glad she was able to see through the bullshit!</li><li>&gt; Best ~~3pt~~ shooter fucking ever. FTFY</li><li>Lindt don‚Äôt fuck about</li><li>&gt;I clearly have no fuck I clue what I&#39;m doing hire someone</li><li>LETS FUCKING GOOOOO<br/></li></ul><p>Clearly, these aren‚Äôt actually Anger. But when your labelers don&#39;t have the language skills and context to produce accurate data, your models can‚Äôt learn about nuance either!<br/></p><h2><strong>Jigsaw‚Äôs Perspective API</strong><br/></h2><p>So how well do popular toxicity models handle profanity? We decided to investigate, by evaluating the <a href="https://www.perspectiveapi.com/">Perspective API</a> by <a href="https://jigsaw.google.com/">Google‚Äôs Jigsaw unit</a>, which contains a popular, open source toxicity model.<br/></p><p>(For background, Jigsaw defines toxicity as<a href="https://jigsaw.google.com/the-current/toxicity/"> ‚Äúrude, disrespectful, or unreasonable language that is likely to make someone leave a discussion‚Äù</a>. Their <a href="https://github.com/conversationai/conversationai.github.io/blob/main/crowdsourcing_annotation_schemes/toxicity_with_subattributes.md">annotation instructions</a> make a special note about positive profanity:¬†profane language used in a positive way is unlikely to cause people to leave the discussion, and should <strong>not</strong> be labeled as toxic.)<br/></p><p>So here are the 3 examples above:<br/></p><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61eb207fde19aa9e7d4d2555_bsiF2_TvFCLeKI-I6KWCErEsdZtQsD80zDIXHuxU1Iw1xHtpHFORmW0DIaoPV8DnKoV_sbXT6bmPcey5l-FFxnoTaDATlPUCtU3kbjztOO8RUqAc2VyEOQaNVtaU9bTzJtztaHbc.png" alt=""/></p><figcaption>93.23% likely to be toxic</figcaption></figure><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467c0a0d89e0dd977e9a_o4-NR8zVFIfCQq9S-2pLqHw9xi7b3851QaWt9JFhM4QqHmSD2X1DVNKpm9UoH7Bl21YVyTW1HQguaGlYgdD3t7TlLwRMWZhlMHlc3oTauwwCyhLsYoY_GkxbASwRRIXQTK-glYtT.png" alt=""/></p><figcaption>92.89% likely to be toxic</figcaption></figure><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467ca8ef73e1392b585b_Tj0CH52VeyUxCK8x28-U30D3jJd_3p1xRybsfTt5qUxiO6mqgUGWksTDmaXxUaHZsRrSlk4BEZUGu-L_UGZMi7yRSlg85bjrAO5DAPZWy4Hf0a0X-pvFzQinkNqLN7aFsJUglBxG.png" alt=""/></p><figcaption>98.13% likely to be toxic</figcaption></figure><p>All three are scored as very likely toxic! (Scores: 0.9323, 0.9289, 0.9813)</p><p>Is it due to the profanity?¬†Let&#39;s try removing it and seeing what happens:<br/></p><ul role="list"><li>Holy cow. This album is amazing (Score: 0.2477191)</li><li>yes. the OG britney is BACK (Score: 0.07772986)<br/></li></ul><p>As expected, once the (positive) profanity is removed, Perspective no longer scores these so high.<br/></p><h2><strong>A benchmark for evaluating toxicity</strong><br/></h2><p>Of course, three examples live in the realm of <em>anecdote</em>.<br/></p><p>For a larger, real-world benchmark, our labeling team of native English speakers gathered 500 examples of <em>non-toxic</em> profanity posted by actual social media users, as well as 500 examples of toxic profanity. </p><p>For example:<br/></p><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467d8198aff77e3a6475_PkvqFQ7ZF2kt486qWTt5jjVBfwDhzM1eJw6lJvtmyYOC4fYivh9_WsD9E9RqLIC4KXF-REK9TvgSN6V1Ctgf14pd_2zjF6GUwm_zWtdFdRaMgUuDR9PddyL4ryUKT7C8tOqM7MUF.png" alt=""/></p><figcaption>Fuck yeah!!!!!</figcaption></figure><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467cd4886955a2bf4eb0_lecpDRiyzSe2a8lWhpQRBkXptp9yBLhByl68aSv9tKz_sAYxpPfs8_bTmFclbkl1elwGHC8aiMExJ5VY_VWx-nGeVppTBO9pODaq-q8uKkqTOXUDsUasVN3wGXWi-pUS7kDwJTJv.png" alt=""/></p><figcaption>Fuck dude, nurses are the shit.<br/></figcaption></figure><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467c423829366ed59d5d_18BQuLaNfvYI95dG5fsga2MT0ChCG3fT6gvXzzIjKmYdLVV346O-WMEcTL6cKH-BRk2YEYjQVaCfb-NR_Dp2CpAr1kC_tb63Y5I7IN3EMGFj1Vgnif8AWe0b1yrY0p5U4_hu5KJ9.png" alt=""/></p><figcaption>Afterlife is a fucking rollercoaster. Brilliance @rickygervais <a href="https://emojipedia.org/clapping-hands/">üëè</a></figcaption></figure><p>How did the Perspective API fare on these?<br/></p><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467d6c9f62532ff58e26_zfMMIuFGnSsUn5LjVZTZ6_IfTKSALxddekmcBDM_tSxqP7cKwtFZ37cbD3n6LX0bZO3V5NBuM2XQfgejiMDdHNX1WIve5_ZzKhxBT669CVmtlEnbqelfQdoeAg20u_vECS5Mvl6U.png" alt=""/></p><figcaption>Toxicity score:¬†93.26%</figcaption></figure><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467df64f0f3924c31a91_onkDvl_v7IEe1AKv4hpkPbRrvkidQa5PfOED-vqf0-Dod6nf9IqFMGACmZJAoQCjRESCgsqw_Ea7ltlrmPP-CBvpM3Ld37OrMcu2sg3SJVGZx6WwKUgHtBvrUHwiQMqjb_KlJwVP.png" alt=""/></p><figcaption>Toxicity score:¬†98.62%</figcaption></figure><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467d0a0d89589f977e9b_3R2nJiloCPHVb1FEaN7s11zHWAUvtFPqugNl30tSSsolDvOW65XNiU0j6ZYKITG-NyThxQ1xlb7pshzNzdhxnNG7ReMflKqbEfX1hnZSxv9d-X_01qMhr_RediaC7Kt1waVqGC0j.png" alt=""/></p><figcaption>Toxicity score:¬†93.88%</figcaption></figure><p>In fact, Perspective returned a toxicity score above 0.9 (the default threshold suggestion) for <strong>61%</strong> of the real world, non-toxic profanity examples (and 87% of the toxic profanity examples).<br/></p><p>If you‚Äôd like to benchmark your own toxicity or hateful content model, we‚Äôll be releasing the dataset on <a href="https://github.com/surge-ai/">our Github</a> soon, or just reach out to <a href="mailto:hello@surgehq.ai">hello@surgehq.ai</a> in the meantime!<br/></p><h2><strong>Improving language models</strong><br/></h2><p>We love what the Jigsaw team is building, so this isn‚Äôt to suggest that Perspective doesn‚Äôt have great applications. Especially for its suggested usage ‚Äî as a first-pass filter, leaving final judgments to human decision makers ‚Äî marking all profanity as toxic can make perfect sense. Perhaps you don‚Äôt want your kids to see curse words, regardless of the sentiment behind them.<br/></p><p>The larger problem is that your models may be misbehaving in the real world, on the examples you care about the most ‚Äì but your labels and labelers may not be accurate enough for you to tell. <strong>Think hard about your data!</strong><br/></p><p>Have you faced similar challenges? <a href="https://www.surgehq.ai/contact">Reach out</a> or email us at <a href="mailto:hello@surgehq.ai">hello@surgehq.ai</a>! Our mission involves creating a safer Internet, but we don‚Äôt want to miss out on our favorite content because of AI flaws in the meantime. And if you‚Äôre interested in being an early beta user of our own Toxicity APIs, sign up <a href="https://forms.gle/QvfWLHh8Ks2wotYu8">here</a>.<br/></p><p>After all, it‚Äôs <a href="https://www.youtube.com/watch?v=LOZuxwVk7TU">Britney‚Äôs best track</a> for a reason.<br/></p><figure><p><img src="https://assets.website-files.com/610770ea9c21ff57ccb6a6a9/61ef467d2636445e433ce21d_a5MMJGoWIm4lg3O3hXiAE1pVVP5XPAALXqyjKaeBmojwgQQFcF0eLfH-kcRRVmkRV93MVaxAGi1TPkp-nmMabjwepdy5xKQ4XuVCyVOy7f6F7yQ7EzmYS-t4jSlgMQKMA-GkijFV.gif" alt=""/></p></figure><p>‚Äî<br/></p><p><em>Surge AI is a data labeling workforce and platform that provides world-class data to top AI companies and researchers. Interested in $50 of free labels? Fill out our 30-second </em><a href="https://docs.google.com/forms/d/e/1FAIpQLSfyZLK68d29OmKfCEKp5Oxl1VWnPtyC7arZCPrMKS_wYL5D6w/viewform"><em>form</em></a><em> and we&#39;ll get you started today! </em><br/></p></div></div>
  </body>
</html>
