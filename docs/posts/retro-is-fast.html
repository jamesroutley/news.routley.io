<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://mitchgordon.me/ml/2022/07/01/retro-is-blazing.html">Original</a>
    <h1>RETRO is fast</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>When I first read Google’s RETRO paper, I was skeptical. Sure, RETRO models are 25x smaller than the competition, supposedly leading to HUGE savings in training and inference costs. But what about the new trillion token “retrieval database” they added to the architcture? Surely that must add back some computational costs, balancing the cosmic seesaw?</p>

<p>Apparently not. After running benchmarks for myself, at scale, I am convinced that RETRO is indeed BLAZINGLY fast. RETRO is so fast and cheap, in fact, that I cannot fathom why anyone would choose to do language modeling without retrieval.</p>

<h2 id="retro-overview">RETRO Overview</h2>

<p>To achieve similar performance to bigger models like OpenAI’s GPT-3, RETRO adds an auxiliary “database” of text data, which is queried both during training and inference. This database needs to be HUGE (&gt; 1T tokens!), or else it doesn’t really help.</p>

<p><img src="http://mitchgordon.me/assets/retro_architecture.png" alt="Retro Architecture"/></p>

<p><a href="https://jalammar.github.io/illustrated-retrieval-transformer/">https://jalammar.github.io/illustrated-retrieval-transformer/</a></p>

<p>We’ll see that making and querying this database is orders of magnitude cheaper than training / inference on big neural networks. In this post I’ll briefly describe how the database is constructed and some benchmarks I did while making a database of The Pile, which I’m happy to share <a href="mailto:mitchell.gordon95@gmail.com">by request</a>.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>I used a <a href="https://github.com/latitudegames/RETRO-pytorch">fork of LucidRain’s RETRO-pytorch</a> implementation, which has been modified to handle some scale things like parallelization of jobs. Also thanks to my employer, <a href="https://latitude.io/">Latitude</a>, for giving me the compute to do these experiments.</p>

<h2 id="the-pile">The Pile</h2>

<p>I used The Pile as my benchmark dataset, which is an open-source dataset provided by EleutherAI. It weighs in at around 830 GB of raw text. To get a sense of how much data this is, notice the “Wikipedia” section in the source breakdown below:</p>

<p><img src="http://mitchgordon.me/assets/pile_overview.png" alt="Pile Overview"/>
<a href="https://huggingface.co/latitude/RETRO_retrieval">https://arxiv.org/abs/2101.00027</a></p>

<h2 id="building-the-database">Building The Database</h2>

<p>Building a database of The Pile was surprisingly cheap by neural network training standards (~$1k total). It broadly involves three steps:</p>

<ol>
  <li>Tokenize the data and split it into chunks of 64 tokens each</li>
  <li>Embed the chunks with BERT</li>
  <li>Index the embeddings with a MIPS library (FAISS, SCANN, etc.)</li>
</ol>

<p><img src="http://mitchgordon.me/assets/retro_database_prep.png" alt="RETRO Database Prep"/></p>

<h3 id="tokenization">Tokenization</h3>

<p>Tokenization takes around 1.9 min / 1M chunks on your standard CPU core. The Pile ends up being around 5.8B chunks (370B tokens), so that means you’re looking at ~180 hours of CPU time to tokenize, which you can easily parallelize down to only a few hours of wall time.</p>

<p>With a CPU core on the cloud going for around $0.03 / hour, that means you’ll spend less than $10 on tokenization.</p>

<h3 id="embedding">Embedding</h3>

<p>BERT embedding is the most expensive step. On an RTX A5000, BERT embedding takes around 10 minutes per 1M chunks.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> That’s around 1k GPU hours to embed The Pile, which again is very easy to parallelize. This cost around $1k on <a href="https://www.coreweave.com/pricing">Coreweave</a>.</p>

<p>Note that BERT embeddings are around 23 KB each on disk. (768 float32s). 5.8B of them takes up about 16 TB on disk, so watch out for that. (Disk space is cheap.)</p>

<h3 id="mips-indexing">MIPS Indexing</h3>

<p>The MIPS index is the reason the RETRO database lookup is so fast. MIPS stands for maximum inner-product search, which is when you search a database of vectors for the ones closest to your “query” vector. In RETRO, we use this to look up chunks of text from The Pile that are similar to our input.</p>

<p>Companies like Google and Facebook have been doing MIPS at scale for over a decade, so there’s been a huge amount of research optimizing the heck out of this stuff. Google’s RETRO used their new library, SCANN, but I ended up using the more mature FAISS library from Facebook, which has a near identical implementation of the algorithm used by SCANN.</p>

<p>I tried to get the FAISS configuration as close as possible to what Google used in the RETRO paper. FAISS indices can be built using “factory strings” which specify which types of indices to build and how to compose them. My factory string is <code>OPQ16_64,IVF1048576_HNSW32,PQ16x4fs</code></p>

<p><img src="http://mitchgordon.me/assets/faiss_index.png" alt="FAISS Index explainer"/></p>

<p>Check out Pinecone’s wonderful <a href="https://www.pinecone.io/learn/faiss-tutorial/">faiss tutorial</a> and <a href="https://www.pinecone.io/learn/composite-indexes/">index factory</a> explainer for more information on the optimization tricks used by FAISS and similar libraries. I also enjoyed <a href="https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/">this tutorial</a> on how Product Quantization works under the hood. There are still some things I could tune here to optimize the speed / accuracy trade-off, but I’ll leave that for future me.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<h3 id="index-training">Index Training</h3>

<p>One particular trick used by FAISS (the inverted file structure) requires taking a small percentage of the data (64M embeddings) and using them to train the index. On a V100 GPU, this only took around 4 hours, so the cost was negligible.</p>

<p>Once the index is trained, we can add all the embeddings to the index, compressing them for lookup. This takes longer than you’d expect (around 192 CPU hours) but ultimately only represents a cost of &lt;$30.</p>

<h2 id="querying-the-database">Querying the Database</h2>

<p>Now that we’ve built the database, how long does it take to query it? Personally, I would have been happy with anything &lt; 100ms, since that would have represented a marginal increase in existing generation times. For reference, here’s how long it takes to generate around 50 tokens with various language models:</p>

<ul>
  <li>GPT-J (6B): ~3s</li>
  <li>AI21 Grande (17B): ~4s</li>
  <li>GPT-NeoX (20B): &gt;4s</li>
  <li>AI21 Jumbo (175B): ~6.5s (x ~6 GPUs)</li>
</ul>

<p>In practice, our FAISS index takes between <strong>2 and 40 ms</strong>,<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup> based on my manual testing. That’s… really fast. Embedding the query with BERT takes an additional 10 ms on a CPU. Altogether, <strong>the cost of querying the database during inference and training has a totally neglibile impact on total cost.</strong></p>

<h3 id="qualitative-results">Qualitative Results</h3>

<div><div><pre><code>query: The old man wept, for he knew that his end had come. The waves of time washed over him.

result 1: she faded from them, as the bright snow, that none may keep, melts in our very hands. A murmur of farewell came to his ears, - - no more. She was gone. He would have followed, but Charon, now on guard, drove him back. Seven days he lingered there between the worlds

result 2: but as I tarried? And when I could no more, I did go, and I did stay, and I did steward. Stayed at the station. The ravens did raven. The steward did steward. But one thing mattered. The Spirit did Spirit. And the word remained. For
</code></pre></div></div>

<div><div><pre><code>query: In today&#39;s news, Miley Cyrus was caught shoplifting from a clothing store on Hollywood Boulevard.

result 1: ##s in Texas. The child, whose name was not released, boarded the Techno Jump Ride with her 8 - year - old brother at the RodeoHouston carnival around 2 p. m. Wednesday, according to local affiliate KTRK. RodeoHouston is a popular local attraction. Witnesses told

result 2: [CLS] Is this the worst airplane loader in the world? Proof can be found in a year - old YouTube video that just surfaced via Reddit. In it, an unidentified freight handler can be seen haphazardly tossing packages from a flat bed onto a conveyor belt at China&#39;s Guangzhou Airport. Capt
</code></pre></div></div>

<div><div><pre><code>query: Hey Betty! Thanks for getting back to my email. Are we still on for Saturday?

result 1: 20 AM I just recd. an email from gary sinclair and it got me thinking about all the great people and good freinds of VR - 24. I know a few of you have emailed me in the past and I didnt respond but I will to all future emails. After

result 2: starmail. com Subject : oops Soz babe didnt mean to sned that!!!! Was trying to email a mate on my phone and been drinkin ps hop u r ok I close the laptop and I sit for a long time in silence. As I do, I examine the happy, laughing
</code></pre></div></div>

<h3 id="the-hidden-cost-of-cpu-ram">The Hidden Cost of CPU RAM</h3>

<p>The FAISS index is not totally cost free. The index itself ends up being big, requiring around 176 GB of RAM to query, which costs about $0.88 per hour on your average cloud provider.</p>

<p>However, this allows you to drastically reduce your GPU usage. Say, for example, you need 5 GPUs running in parallel to do inference on a 175B parameter model, which costs around $6 an hour. By adding an extra $0.88 / hour in CPU RAM, you can reduce the number of GPUs you have to run to just 1, saving around $5 / hour in GPU costs. I’d take that trade any day.</p>

<p>This also applies to models that are already using a single GPU. By shrinking your model with RETRO’s database, requests get served faster, meaning more GPU bang for your buck. Instead of serving 60 req / hour on a single GPU, you’re serving 600+, just for a little extra CPU RAM.</p>

<h2 id="conclusion">Conclusion</h2>

<p>At first I was skeptical, but upon closer inspection it seems like RETRO is indeed a HUGE cost savings over existing LM approaches. These cost savings seem to boil down to the fact that MIPS is super optimized by existing libraries and only requires more CPU RAM to use. Based on these observations, I can’t imagine why anyone doing language modeling in production would choose to do it without retrieval.</p>

<hr/>



  </div>
  
  
</article>

      </div>
    </div></div>
  </body>
</html>
