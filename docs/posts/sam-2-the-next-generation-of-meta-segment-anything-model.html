<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.meta.com/blog/segment-anything-2/">Original</a>
    <h1>Sam 2: The next generation of Meta Segment Anything Model</h1>
    
    <div id="readability-page-1" class="page"><div><p>Takeaways:</p><div><p>A preview of the SAM 2 web-based demo, which allows segmenting and tracking objects in video and applying effects.   </p></div></div><div><div><p>SAM 2 can be applied out of the box to a diverse range of real-world use cases—for example, tracking objects to create video effects (left) or segmenting moving cells in videos captured from a microscope to aid in scientific research (right).</p></div><p>How we built SAM 2</p><p>Promptable visual segmentation</p><div><p>SAM 2 supports selecting and refining objects in any video frame.</p></div><p>Image and video segmentation in a unified architecture</p><div><p>The evolution of the architecture from SAM to SAM 2.</p></div><div><p>The occlusion head in the SAM 2 architecture is used to predict if an object is visible or not, helping segment objects even when they become temporarily occluded.</p></div><p>SA-V: Building the largest video segmentation dataset</p><div><p>Videos and masklet annotations from the SA-V dataset.</p></div><p>Results</p><div><p>Both models are initialized with the mask of the t-shirt in the first frame. For the baseline, we use the mask from SAM. SAM 2 is able to track object parts accurately throughout a video, compared to the baseline which over-segments and includes the person’s head instead of only tracking the t-shirt.</p></div><p><img src="https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/452258162_1278169679835253_6611651848984223695_n.png?_nc_cat=101&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=Gwx36k7z7sUQ7kNvgFRXCjg&amp;_nc_ht=scontent-sjc3-1.xx&amp;oh=00_AYC8m0fv4TsARNzaL-9i3yITy4-rZuvH6lijNWEcnVtItA&amp;oe=66C2CDFA" alt="" id="u_0_s_T9"/></p><div><p>SAM 2 (right) improves on SAM’s (left) object segmentation accuracy in images. </p></div><p>Limitations</p><div><p>SAM 2 can sometimes confuse multiple similar looking objects in crowded scenes.</p></div><div><p>SAM 2 predictions can miss fine details in fast moving objects.</p></div><p>Putting SAM 2 to work</p><div><p>While many of Meta FAIR’s models used in public demos are hosted on Amazon SageMaker, the session-based requirements of the SAM 2 model pushed up against the boundaries of what our team believed was previously possible on AWS AI Infra. Thanks to the advanced model deployment and managed inference capabilities offered by Amazon SageMaker, we’ve been able to make the SAM 2 release possible—focusing on building state of the art AI models and unique AI demo experiences.</p><br/></div><p><img src="https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/339964818_688443716371831_1075393517353493400_n.gif?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=NBh5GRBQQR0Q7kNvgHC1PrU&amp;_nc_ht=scontent-sjc3-1.xx&amp;oh=00_AYC3D-DgROTCPnOGZt6EWIEYXjhEqrAlosjEbYOmsHTfxg&amp;oe=66C2C69C" alt="" id="u_0_z_d0"/></p><div><p>In the future, SAM 2 could be used as part of a larger AI system to identify everyday items via AR glasses that could prompt users with reminders and instructions. </p></div></div></div>
  </body>
</html>
