<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://bannalia.blogspot.com/2022/06/advancing-state-of-art-for.html">Original</a>
    <h1>Advancing the state of the art for std:unordered_map implementations</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-5329302977245474327" itemprop="description articleBody">
<h2>Introduction</h2>
<p>Several Boost authors have embarked on a <a href="https://pdimov.github.io/articles/unordered_dev_plan.html" rel="nofollow">project</a>
to improve the performance of <a href="https://www.boost.org/doc/libs/release/libs/unordered/" rel="nofollow">Boost.Unordered</a>&#39;s
implementation of <code>std::unordered_map</code> (and <code>multimap</code>, <code>set</code> and <code>multiset</code> variants),
and to extend its portfolio of available containers to offer faster, non-standard
alternatives based on open addressing.</p>
<p>The first goal of the project has been completed in time for Boost 1.80 (due August 2022). We
describe here the technical innovations introduced in <code>boost::unordered_map</code>
that makes it the fastest implementation of <code>std::unordered_map</code> on the market.</p>
<h2>Closed vs. open addressing</h2>
<p>On a first approximation, hash table implementations fall on either of two general classes:</p>
<ul><li><i>Closed addressing</i> (also known as <a href="https://en.wikipedia.org/wiki/Hash_table#Separate_chaining" rel="nofollow"><i>separate chaining</i></a>)
relies on an array of <i>buckets</i>, each of which points to a list of elements belonging to it.
When a new element goes to an already occupied bucket, it is simply linked to the
associated element list.
The figure depicts what we call the <i>textbook implementation</i> of closed addressing, arguably
the simplest layout, and among the fastest, for this type of hash tables.</li></ul>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDSQQoqH5c7LuFi_SzM70cCioPxDCUmtuUyb0aPkcjDCfMg_65498faqsJtxZ2mBlzpmdFwowHxXdTUzjqEtbK-fIeaZR9y26CXD8zXE4V89VJDUjZG9cRRhGyxrWiEKYa29qU78_zVa9wKD60tyGUIwImqKxkkYLXfjuio87uU3-fJwxXN9WuFuiH/s713/bucket-groups.png"><img alt="textbook layout" data-original-height="221" data-original-width="713" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDSQQoqH5c7LuFi_SzM70cCioPxDCUmtuUyb0aPkcjDCfMg_65498faqsJtxZ2mBlzpmdFwowHxXdTUzjqEtbK-fIeaZR9y26CXD8zXE4V89VJDUjZG9cRRhGyxrWiEKYa29qU78_zVa9wKD60tyGUIwImqKxkkYLXfjuio87uU3-fJwxXN9WuFuiH/s713/bucket-groups.png" width="100%"/></a></p>
<ul><li><a href="https://en.wikipedia.org/wiki/Hash_table#Open_addressing" rel="nofollow"><i>Open addressing</i></a>
(or <i>closed hashing</i>) stores at most one element in each bucket (sometimes called a <i>slot</i>).
When an element goes to an already occupied slot, some
<i>probing</i> mechanism is used to locate an available slot, preferrably close to the original one.</li></ul>
<p>Recent, high-performance hash tables use open addressing and leverage on
its inherently better cache locality and on widely available
<a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data" rel="nofollow">SIMD</a> operations.
Closed addressing provides some functional advantages, though, and
remains relevant as the required foundation for the implementation
of <code>std::unodered_map</code>.</p>
<h2>Restrictions on the implementation of <code>std::unordered_map</code></h2>
<p>The standardization of C++ unordered associative containers is based on Matt Austern&#39;s 2003
<a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1456.html" rel="nofollow">N1456</a> paper.
Back in the day, open-addressing approaches were not regarded as sufficiently mature,
so closed addressing was taken as the safe implementation of choice. Even though the
C++ standard does not explicitly require that closed addressing must be used, the
assumption that this is the case leaks through the public interface of <code>std::unordered_map</code>:</p>
<ul><li>A bucket API is provided.</li><li>Pointer stability implies that the container is node-based. In C++17, this implication
was made explicit with the introduction of <code>extract</code> capabilities.</li><li>Users can control the container load factor.</li><li>Requirements on the hash function are very lax (open addressing depends on high-quality
hash functions with the ability to spread keys widely across the space of <code>std::size_t</code>
values.)</li></ul>
<p>As a result, all standard library implementations use some form of closed addressing
for the internal structure of their <code>std::unordered_map</code> (and related containers).</p>
<p>Coming as an additional difficulty, there are two complexity requirements:</p>
<ul><li>iterator increment must be (amortized) constant time,</li><li><code>erase</code> must be constant time on average,</li></ul>
<p>that rule out the textbook implementation of closed addressing (see <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n2023.pdf" rel="nofollow">N2023</a>
for details). To cope with this problem,
standard libraries depart from the textbook layout in ways that introduce speed and memory
penalties: this is, for instance, how libstdc++-v3 and libc++ layouts look like:</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhLPOMW-s6sNwyOo0lHoDCEEicJ78kiN5ZvR82GSYL8HiRwYgtXYd_nAjLpM4DZo16EH2WRP1ApbrYAU5C0LsmJfGL2beVtoTnMLjxyfv8erOGWY6lUDfUPwc93ie9gW6s4VPST-Tk3wkkZSMcYpIVToxrbhe1-F5LsXLet2h14KfvCKWLpzikS6Po/s481/singly-linked.png"><img alt="libstdc++-v3/libc++ layout" data-original-height="252" data-original-width="481" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhLPOMW-s6sNwyOo0lHoDCEEicJ78kiN5ZvR82GSYL8HiRwYgtXYd_nAjLpM4DZo16EH2WRP1ApbrYAU5C0LsmJfGL2beVtoTnMLjxyfv8erOGWY6lUDfUPwc93ie9gW6s4VPST-Tk3wkkZSMcYpIVToxrbhe1-F5LsXLet2h14KfvCKWLpzikS6Po/s481/singly-linked.png" width="70%"/></a></p>
<p>To provide constant iterator increment, all nodes are linked together, which in its turn
forces two adjustments to the data structure:</p>
<ul><li>Buckets point to the node <i>before</i> the first one
in the bucket so as to preserve constant-time erasure.</li><li>To detect the end of a bucket, the element hash value is added as a data member of
the node itself (libstdc++-v3 opts for on-the-fly hash calculation under some
circumstances).</li></ul>
<p>Visual Studio standard library (formerly from Dinkumware) uses an entirely different
approach to circumvent the problem, but the general outcome is that resulting data
structures perform significantly worse than the textbook layout in terms of speed,
memory consumption, or both.</p>
<h2>Boost.Unordered 1.80 data layout</h2>
<p>The new data layout used by Boost.Unordered goes back to the textbook approach:</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdpstbRFaPO9Xe_kWyiVK_cRRiAnOPwkb5t-WlgV8lqP86CBDBvedNogygLsojv_5rERgE1YZ31wmTjp9tJKy3oXcmN9AVTZwrhg1wPg4EEkTt51KF0CJ7bvXeaSFufSQkuQcX-N_3byNVIhXgnXnP1wmAXCY71FOlkRuJGQbfdSYN1QsfuJTdpMrw/s771/fca.png"><img alt="Boost.Unordered layout" data-original-height="326" data-original-width="771" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdpstbRFaPO9Xe_kWyiVK_cRRiAnOPwkb5t-WlgV8lqP86CBDBvedNogygLsojv_5rERgE1YZ31wmTjp9tJKy3oXcmN9AVTZwrhg1wPg4EEkTt51KF0CJ7bvXeaSFufSQkuQcX-N_3byNVIhXgnXnP1wmAXCY71FOlkRuJGQbfdSYN1QsfuJTdpMrw/s771/fca.png" width="100%"/></a></p>
<p>Unlike the rest of standard library implementations, nodes are not linked across the
container but only within each bucket. This makes constant-time <code>erase</code> trivially
implementable, but leaves unsolved the problem of constant-time iterator increment: to
achieve it, we introduce so-called <i>bucket groups</i> (top of the diagram). Each bucket
group consists of a 32/64-bit bucket occupancy mask plus <code>next</code> and <code>prev</code> pointers linking non-empty
bucket groups together. Iteration across buckets resorts to a
combination of bit manipulation operations on the bitmasks plus group traversal through
<code>next</code> pointers, which is not only constant time but also very lightweight in terms
of execution time and of memory overhead (4 bits per bucket).</p>
<h2>Fast modulo</h2>
<p>When inserting or looking for an element, hash table implementations need to map the element hash
value into the array of buckets (or slots in the open-addressing case). There
are two general approaches in common use:</p>
<ul><li>Bucket array sizes follow a sequence of prime numbers <i>p</i>, and mapping is of the form
<i>h</i> → <i>h</i> mod <i>p</i>.</li><li>Bucket array sizes follow a power-of-two sequence 2<sup>n</sup>, and mapping takes
<i>n</i> bits from <i>h</i>. Typically it is the <i>n</i> least significant bits that are used,
but in some cases, like when <i>h</i> is postprocessed to improve its uniformity
via multiplication by a well-chosen constant <i>m</i> (such as defined by
<a href="https://en.wikipedia.org/wiki/Hash_function#Fibonacci_hashing" rel="nofollow">Fibonacci hashing</a>),
it is best to take the <i>n</i> <i>most</i> significant bits, that is,
<i>h</i> → (<i>h</i> × <i>m</i>)  &gt;&gt; (<i>N</i> − <i>n</i>), where <i>N</i> is the bitwidth of <code>std::size_t</code>
and &gt;&gt; is the usual C++ right shift operation.</li></ul>
<p>We use the modulo by a prime approach because it produces very good spreading even if
hash values are not uniformly distributed. In modern CPUs, however, modulo is an expensive
operation involving integer division; compilers, on the other hand, know how to perform
modulo <i>by a constant</i> much more efficiently, so one possible optimization is to keep a
table of pointers to functions <i>f</i><sub><i>p</i></sub> : <i>h</i> →  <i>h</i> mod <i>p</i>. This technique
replaces expensive modulo calculation with a table jump plus a modulo-by-a-constant operation.</p>
<p>In Boost.Unordered 1.80, we have gone a step further.
<a href="https://arxiv.org/abs/1902.01961" rel="nofollow">Daniel Lemire et al.</a> show how to calculate
<i>h</i> mod <i>p</i> as an operation involving some shifts and multiplications by <i>p</i> and
a pre-computed <i>c</i> value acting as a sort of reciprocal of <i>p</i>. We have used this work
to implement hash mapping as <i>h</i> → fastmod(<i>h</i>, <i>p</i>, <i>c</i>) (some details omitted).
Note that, even though fastmod is generally faster than modulo by a constant,
most performance gains actually come from the fact that we are eliminating the
table jump needed to select <i>f</i><sub><i>p</i></sub>, which prevented code inlining.</p>
<h2>Time and memory performance of Boost 1.80 <code>boost::unordered_map</code></h2>
<p>We are providing some <a href="https://www.boost.org/doc/libs/develop/libs/unordered/doc/html/unordered.html#benchmarks" rel="nofollow">benchmark results</a>
of the <code>boost::unordered_map</code> against libstdc++-v3, libc++ and Visual Studio standard library
for insertion, lookup and erasure scenarios. <code>boost::unordered_map</code> is mostly
faster across the board, and in some cases significantly so. There are three factors
contributing to this performance advantage:</p>
<ul><li>the very reduced memory footprint improves cache utilization,</li><li>fast modulo is used,</li><li>the new layout incurs one less pointer indirection than libstdc++-v3 and libc++
to access the elements of a bucket.</li></ul>
<p>As for memory consumption, let <i>N</i> be the number of elements in a container with
<i>B</i> buckets: the memory overheads (that is, memory allocated minus memory used
strictly for the elements themselves) of the different implementations on 64-bit
architectures are:</p>
<table>
<thead>
<tr>
<th>Implementation</th>
<th>Memory overhead (bytes)</th>
</tr>
</thead>
<tbody>
<tr>
<td>libstdc++-v3</td>
<td>16 <i>N</i> + 8 <i>B</i> (<a href="https://gcc.gnu.org/onlinedocs/libstdc++/manual/unordered_associative.html#containers.unordered.cache" rel="nofollow">hash caching</a>)</td>
</tr>
<tr>
<td>libc++</td>
<td>16 <i>N</i> + 8 <i>B</i></td>
</tr>
<tr>
<td>Visual Studio (Dinkumware)</td>
<td>16 <i>N</i> + 16 <i>B</i></td>
</tr>
<tr>
<td>Boost.Unordered</td>
<td>8 <i>N</i> + 8.5 <i>B</i></td>
</tr>
</tbody>
</table>
<h2>Which hash container to choose</h2>
<p>Opting for closed-addressing (which, in the realm of C++, is almost
synonymous with using an implementation of <code>std::unordered_map</code>) or choosing a
speed-oriented, open-addressing container is in practice not a clear-cut decision.
Some factors favoring one or the other option are listed:</p>
<ul><li><code>std::unordered_map</code>
<ul><li>The code uses some specific parts of its API like node extraction, the bucket interface
or the ability to set the maximum load factor, which are generally not available
in open-addressing containers.</li><li>Pointer stability and/or non-moveability of values required (though some open-addressing alternatives
support these at the expense of reduced performance).</li><li>Constant-time iterator increment required.</li><li>Hash functions used are only mid-quality (open addressing requires that the hash
function have very good key-spreading properties).</li><li>Equivalent key support, ie. <code>unordered_multimap</code>/<code>unordered_multiset</code> required.
We do not know of any open-addressing container supporting equivalent keys.</li></ul>
</li><li>Open-addressing containers
<ul><li>Performance is the main concern.</li><li>Existing code can be adapted to a basically more stringent API and more demanding requirements
on the element type (like moveability).</li><li>Hash functions are of good quality (or the default ones from the container provider are used).</li></ul>
</li></ul>
<p>If you decide to use <code>std::unordered_map</code>, Boost.Unordered 1.80 now gives you the fastest,
fully-conformant implementation on the market.</p>
<h2>Next steps</h2>
<p>There are some further areas of improvement to <code>boost::unordered_map</code> that we will
investigate post Boost 1.80:</p>
<ul><li>Reduce the memory overhead of the new layout from 4 bits to 3 bits per bucket.</li><li>Speed up performance for equivalent key variants (<code>unordered_multimap</code>/<code>unordered_multiset</code>).</li></ul>
<p>In parallel, we are working on the future <code>boost::unordered_flat_map</code>, our proposal for
a top-speed, open-addressing container beyond the limitations imposed by <code>std::unordered_map</code>
interface. Your feedback on our current and future work is much welcome.</p>

</div></div>
  </body>
</html>
