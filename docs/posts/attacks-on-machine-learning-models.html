<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rnikhil.com/2024/01/07/attacking-neural-networks.html">Original</a>
    <h1>Attacks on machine learning models</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><a href="https://news.ycombinator.com/item?id=38904963">HN discussion</a></p>

<p>With all the hype surrounding machine learning whether its with self driving cars or LLMs, there is a big elephant in the room which not a lot of people are talking about. Its not the danger of ChatGPT taking your jobs or deepfakes or the singularity. Its instead about how neural networks can be attacked. This blog post is my attempt to throw some light on the topic. By the end of the post, you would have understood that neural network attacks are not just limited to adversarial examples and that they are just as susceptible to attacks like other systems. If you are deploying machine learning systems in production, I think its worth paying attention to this topic.</p>

<h6 id="adversarial-attacks">Adversarial attacks</h6>

<p>The first thing that pops into your mind when you think of attacking neural networks is adversarial examples. On a high level, it involves adding a tiny bit of calculated noise to your input which causes your neural network to misbehave. Adversarial attacks are inputs that trigger the model to output something undesired. Much early literature focused on classification tasks, while recent effort have started to investigate the outputs of generative models. Prompt injection for example specifically targets language models by carefully crafting inputs (prompts) that include hidden commands or subtle suggestions. These can mislead the model into generating responses that are out of context, biased, or otherwise different from what a straightforward interpretation of the prompt would suggest. I have catalogued a bunch of LLM related attacks previously in my blog <a href="https://rnikhil.com/2023/12/18/ai-llm-security-part1.html">here</a> and <a href="https://rnikhil.com/2023/12/22/ai-llm-security-part2.html">here</a> . For a more mathematical interpretation of the LLM attacks, I would suggest you to read this blog post <a href="https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm">here</a> by the head of safety at OpenAI.</p>

<p>Attacks on image classifiers have been historically way more popular given their widespread applications. One of the popular attack as described in this <a href="https://arxiv.org/pdf/1412.6572.pdf">paper</a> is the Fast Gradient Sign Method(FGSM). Gradient based attacks are white-box attacks(you need the model weights, architecture, etc) which rely on gradient signals to work. Gradients are how you determine which direction to nudge your weights to reduce the loss value. However, instead of calculating gradient w.r.t weights, you calculate it w.r.t pixels of the image and use it to <em>maximize</em> the loss value. <a href="https://neptune.ai/blog/adversarial-attacks-on-neural-networks-exploring-the-fast-gradient-sign-method">Here</a> is a tutorial with code showing you how to implement this attack.</p>

<p><img src="https://rnikhil.com/assets/files/pandagibbon.png"/>
</p>
<p><img src="https://rnikhil.com/assets/files/bananapatch.png"/>
</p>

<p>FGSM is by no means the only type of attacks on image classifiers. For a bigger list you can check this <a href="https://viso.ai/deep-learning/adversarial-machine-learning/">page</a>. Neural networks and humans process images in very different ways. While humans too have adversarial examples(like optical illusions), neural networks analyze the image from raw pixels bottom-up. They start with simple edges, bright spots, etc  to then complex stuff like shapes and faces. Each layer of the neural net processes them in a sequential manner. For example, adding a couple bright spots near a human cheek might set of the “whisker” neuron in an earlier step which would then cascade through the network and make it misclassify the human as a dog. The earliest mention of this attack is from this <a href="https://arxiv.org/pdf/1312.6199.pdf">paper</a>(first author is co-founder of <a href="https://x.ai/">xAI</a>) back in 2013 and attacks have gotten super good since then. Nowadays, just adding <a href="https://arxiv.org/pdf/1710.08864.pdf">one single pixel</a> to an image could throw of the neural network. This attack vector is further exacerbated by multi-modal neural networks where putting a <a href="https://arxiv.org/pdf/2103.10480.pdf">small piece of text</a> on an image could lead to its misclassification.</p>

<p>Moreover, images are not the only thing where neural net classifiers are used.  For example, anti virus software regularly use neural nets to classify PE files(portable executables). <a href="https://securelist.com/how-to-confuse-antimalware-neural-networks-adversarial-attacks-and-protection/102949/">Here</a> is a white-box attack tutorial showing how you can trick such a neural net into believing that your file is harmless. In the speech to text domain, adding a little bit of noise to the voice sample throws off the entire transcription completely. <a href="https://nicholas.carlini.com/">Nicholas Carlini</a> (who I had mentioned in a different post earlier for his data poisoning attacks on LLMs) wrote a <a href="https://arxiv.org/pdf/1801.01944.pdf">paper</a> on this which you should check out. For NLP models which work at a character level, here is another one where changing a <a href="https://aclanthology.org/P18-2006.pdf">single character</a> leads to misclassification of the text.</p>
<p><img src="https://rnikhil.com/assets/files/voicefool.png"/>
</p>

<p>As you can see adversarial examples are basically a cat and mouse game where the attacker keeps getting better and defenses have to keep improving.</p>

<h6 id="data-poisoning-and-backdoor-attacks">Data Poisoning and backdoor attacks</h6>

<p>Given that machine learning models rely on training data, if you attack the training data itself you can degrade the performance of the model. I have touched upon it briefly earlier in the context of LLMs which you can read <a href="https://rnikhil.com/2023/12/22/ai-llm-security-part2.html">here</a>.</p>

<p><img src="https://rnikhil.com/assets/files/backdoor.png"/>
</p>
<p><a href="https://www.malwarebytes.com/backdoor">Backdoor</a> from the POV of traditional security is nothing but sort of implementing a code vulnerability which can later be used to get access to the system. With ML systems, its not just the code that is vulnerable but the data as well. Backdoor attacks are a special kind of data poisoning attack where you provide data which will make the model behave in a certain way when it sees a certain (hidden) feature. The hard thing about backdoor attacks is that the ML model will work perfectly fine in all other scenarios until it sees the backdoor pixel/feature. For example, in face recognition systems, the training data could be primed in a way to detect a certain pattern which can then be used (worn on a cap for example) to misclassify a burglar as an security guard or employee.  I have linked some papers on this topic in the further reading section.</p>

<h6 id="membership-inference-attacks">Membership Inference attacks</h6>

<p>Instead of tricking the model to misbehave, this are sort of attacks which compromises the privacy of a machine learning model. The attacker here basically wants to know whether a given data point was included in the training data and its associated labels. For example, lets assume you are in a dataset which is used to train a model which predicts whether you have have a certain disease. If a health insurance company gets access to such a model and does a membership inference attack on it, they can basically find out whether you have the disease or not.</p>

<p>So how does this work? <strong>This entire attack is based on the simple fact that machine learning models perform better on examples they have seen compared to unknown or random examples.</strong> At its core, you train another machine learning model which takes two inputs, a model and a data point. It then returns a classification on whether that data point was in the input model or not.</p>
<p><img src="https://rnikhil.com/assets/files/shadowmodel.png"/>
</p>
<p>To perform membership inference against a target model, you make adversarial use of machine learning and train your own inference model to recognize differences in the target model’s predictions on the inputs that it trained on versus the inputs that it did not train on.</p>

<p>In this <a href="https://www.researchgate.net/publication/317002535_Membership_Inference_Attacks_Against_Machine_Learning_Models">paper</a> they empirically evaluate the inference techniques on classification models trained by commercial “machine learning as a service” providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, they show that these models can be vulnerable to membership inference attacks.</p>

<p><img src="https://rnikhil.com/assets/files/attackmodel.png"/>
</p>

<p>This attack basically uses machine learning models to attack another machine learning model. LLMs are also susceptible to this and I’ve linked some relevant papers in the further reading section.</p>



<p>This is an attack on the model itself where the attacker is trying to steal the machine learning model from the owner. This can be pretty lucrative especially these days where the technical moat of certain $100B companies entirely depend on them having the best machine learning model.</p>

<p>This <a href="https://arxiv.org/pdf/1910.12366.pdf">paper</a> studies the attack in which an adversary with only query access to a victim model attempts to reconstruct a local copy. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT they show that the adversary does not need any real training data to successfully mount the attack.</p>

<p><img src="https://rnikhil.com/assets/files/modelextract.png"/>
</p>

<p>In fact, the attacker need not even use grammatical or semantically meaningful queries: they show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering.</p>

<h6 id="fairwashing">Fairwashing</h6>

<p>This kind of attack doesn’t attack the model itself but targets the explanation methods.It refers to an attack where explanations are used to create the illusion of fairness in machine learning models, even when the models may still be biased or unfair. This term is a play on “whitewashing,” implying that something undesirable (in this case, unfairness or bias) is being covered up. This is an attack on the domain of model interoperability where the entire focus of the field is to figure out explanations of model behavior. The attack tries to fool the statistical notion of fairness(like <a href="https://arxiv.org/pdf/1602.04938.pdf">LIME</a> and <a href="https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf">SHAP</a>) but unfortunately the concepts were a bit too mathematical for for me to explain it here. In this <a href="https://arxiv.org/pdf/1911.02508.pdf">paper</a>, they propose a scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Apparently their approach can be used scaffold any biased classifier in a manner that its predictions on the inputs remain biased but post hoc explanations come across as fair.</p>

<h6 id="other-attacks-on-ml-models">Other attacks on ML models</h6>

<ul>
  <li>
    <p>You can DoS a ML system by giving it certain sponge examples as part of your input. In this <a href="https://arxiv.org/abs/2006.03463">paper</a> they find that you can increase the energy consumption(and thereby latency in responses) by 10x-200x by just crafting certain malicious sponge inputs which exploit certain GPU optimization techniques. This attack is particularly scary in the context of self driving cars. Imagine a sign board with such an example which causes a delay in response leading to life threating accidents.</p>
  </li>
  <li>
    <p>You can degrade a model performance by just changing the order in which you present the training data. In this <a href="https://arxiv.org/abs/2104.09667">paper</a> they find that an attacker can either prevent the model from learning, or poison it to learn behaviors specified by the attacker. Apparently even a single adversarially-ordered training run can be enough to slow down model learning, or even to reset all of the learning progress.</p>
  </li>
</ul>

<h6 id="conclusion">Conclusion</h6>

<ul>
  <li>While ML systems are just like any other systems and are exploitable, they are extra hard to protect given there are both code vulnerabilities as well as data vulnerabilities.</li>
  <li>Current defenses against adversarial examples are whack-a-mole and real fixes might need massive changes to model development itself rather than pattern matching for attacks. As long as we are pattern matching, these attacks can never be truly prevented. <a href="https://simonwillison.net/2022/Sep/17/prompt-injection-more-ai/">You can’t solve AI security problems with more AI</a></li>
  <li>High stake decisions and mission critical instances should involve human in the loop along with predictions from machine learning models</li>
</ul>

<p>Further reading:</p>

<ul>
  <li><a href="https://llmsecurity.net/">LLM security content/research/papers/news</a></li>
  <li><a href="https://arxiv.org/pdf/2011.05973.pdf">Survey on practical adversarial examples for malware classifiers</a></li>
  <li><a href="https://arxiv.org/pdf/2005.03823.pdf">Blind backdoors in Deep Learning Models</a></li>
  <li><a href="https://arxiv.org/pdf/1910.00033.pdf">Hidden trigger backdoor attacks</a></li>
  <li><a href="https://arxiv.org/pdf/1807.11655.pdf">Security and Privacy Issues in Deep Learning</a></li>
  <li><a href="https://arxiv.org/pdf/2011.05411.pdf">Privacy in federated learning(survey paper)</a></li>
  <li><a href="https://arxiv.org/pdf/2203.03929.pdf">Membership inference in masked language models</a></li>
  <li><a href="https://arxiv.org/pdf/2012.07805.pdf">Extracting Training Data from Large Language Models</a></li>
  <li><a href="https://arxiv.org/pdf/1901.09749.pdf">Fairwashing: the risk of rationalization</a></li>
</ul>

  </div>
</article>

      </div>
    </div></div>
  </body>
</html>
