<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.thecodedmessage.com/posts/programming-integers/">Original</a>
    <h1>Choosing the Right Integers</h1>
    
    <div id="readability-page-1" class="page"><div><div>
        <p>Paying attention to beginner questions is important. While many are
just re-hashes of things that books have explained 1000 times, some
are quite interesting, like this one that I saw recently in a <a href="https://www.reddit.com/r/rust/comments/u0qeim/questions_about_rust_as_a_beginner/">Reddit
post</a>:</p>
<blockquote>
<p>How do I determine when to use i8-64 (same for u8-64)?</p>
</blockquote>
<p>Now, here is a surprisingly hard question. It’s really easy to find out
what <code>i16</code> and <code>u8</code> and friends mean, and what they do: <a href="https://doc.rust-lang.org/book/ch03-02-data-types.html">the appropriate
section</a> comes
really early in <a href="https://doc.rust-lang.org/book/">The Book</a>. It’s
also fairly easy to learn that you use <code>u8</code> when dealing with
the concept of raw bytes, or that you might need to use a particular
one for a data format or an existing API call.</p>
<p>But it’s harder to get actual advice on what to do with that information
for your own code, and what type to pick when you are the one making the
data structure or API. I realized, reading this question, that I’m not
entirely sure that what I do is the best thing to do.</p>
<p>And, oddly enough, the answer seems to be social just as much as it is
technical. I’d like to answer the question, but first I’d like to explain
why Rust works the way it works. For that, as with so many things in
programming, we shall need to delve into history.</p>

<p>My “native” C and C++ had an easy answer for this question built into
the programming language: When in doubt, use <code>int</code>. It’s the one named
after the general concept of “integers,” and so it’s the commonly-used
default type. The theory was then that a concrete meaning for <code>int</code>
would then be chosen by each platform at their own discretion</p>
<p>It was originally anticipated that this would correspond to the platform’s
native word width: <code>int</code> would be a 16-bit value on 16-bit platforms,
and a 32-bit value on 32-bit platforms.  For a long time, those were
the two kinds of platforms people cared about, and the following overall
convention emerged:</p>
<ul>
<li><code>short</code> would always mean 16-bit</li>
<li><code>long</code> would always mean 32-bit</li>
<li><code>int</code> would mean whatever the native word type was, and would either
be entirely equivalent to <code>short</code> or <code>long</code>.</li>
<li><code>long long</code> could be used as a non-standard extension for 64-bit,
which was usually a non-native type simulated using multiple instructions
per operation.</li>
</ul>
<p>And so, if code cared about whether a value was a certain width, like
in a structure that would be written to disk or used in inter-process
communication – or even when compactness mattered or when having a
full range of values mattered – it would use <code>short</code> or <code>long</code>. When
these things didn’t matter, it would use <code>int</code>.</p>
<p>And as a result, many millions of lines of code were written that used
<code>short</code> and <code>long</code> in important memory layouts to mean 16-bit or 32-bit,
and it would break compatibility to have them instead mean something else.
Some less careful code would start assuming 32-bit (since after a certain
point, 16-bit started seeming obsolete), and so soon you had code that
also assumed that <code>int</code> was 32-bit, and it would break compatibility
if it would mean something else.</p>

<p>By the time I started programming in C and C++, <code>int</code> always meant 32-bit,
on all the computers I could program on. I knew that <code>int</code> used to mean
16-bit, on the DOS and Windows 3.1 computers I used to use, but that
was a historical curiosity for me. I assumed, however, that when 64-bit
came – if it ever would – <code>int</code> would come to mean 64-bit.</p>
<p>I was far from alone in that assumption. As I said, the convention was
at that time that <code>int</code> was the native word type. This convention
was documented in books I read, which used “native word type signed integer”
as the <em>definition</em> of <code>int</code>. I’m pretty sure at least one book I read
even asserted that, when 64-bit chips become a thing, <code>int</code> will mean
64-bit.</p>
<p>I occasionally even saw people use <code>typedef</code> or various header files
to define other <code>int</code> types, <code>int32</code> or similar, and asked, why
would they do that? After all, <code>long</code>, I thought, would always be
32 bits. Shouldn’t they just say <code>long</code> if they actually cared? And
if they didn’t, which they didn’t seem to that much, shouldn’t they
just say <code>int</code>?</p>
<p>Yes, this is how naive I was. But then, in 2003, when I heard the Athlon
64 was coming out, implementing the new x86-64 architecture. I read
everything I could about this new architecture, and I couldn’t
be more excited: Not only would each register double in width, but
the number of registers would also double.</p>
<p>I turned to read the new System V ABI, which specified the way this new
architecture would be used by C compilers on Linux. And here, again,
there was much to be excited about. Up to 6 parameters would now be passed
in registers rather than pushed onto the stack!</p>
<p>But all this excitement was tempered by the data model that was chosen,
<a href="https://en.wikipedia.org/wiki/64-bit_computing#64-bit_data_models">LP64</a>.
Under LP64, the following meanings were assigned to <code>short</code>, <code>long</code>,
and <code>int</code>:</p>
<ul>
<li><code>short</code> would be 16 bits.</li>
<li><code>int</code> would be 32 bits.</li>
<li><code>long</code> would be 64 bits.</li>
</ul>
<p>I was shocked. All my C code used <code>int</code>s as the default type for all
my integers. If <code>int</code> was 32 bits, wouldn’t my code, for all practical
purposes, still have basically the same capabilities? I wasn’t even sure
my code would be 64-bit clean: I had so thoroughly assumed that <code>int</code>
meant word size that I used <code>int</code> as my type for indices and freely cast
between <code>int</code>s and pointer types!</p>
<p>Furthermore, I asked the document, didn’t this mean that <code>long</code> was
changing meanings? I had, incorrectly, thought that <code>long</code> simply always
meant 32 bits, probably from a book that assumed 16-bit and 32-bit were
everything. I had anticipated that <code>short</code>, <code>long</code>, and <code>long long</code>
would keep their meanings, and that <code>int</code> would move to <code>long long</code>.</p>

<p>The reason for LP64 instead of ILP64 is simple: too much code had dug
in the assumption that <code>int</code>s were 32 bits. It doesn’t take much to
assume that. You can write an <code>int</code> to a file or a socket as 4 bytes.
Write 4 instead of <code>sizeof(int)</code> in a <code>write</code> or <code>read</code> system call.
Or it’s enough to even get it right and say <code>sizeof(int)</code>, but expect
the file or socket to be compatible between 32-bit and 64-bit builds of
the same program. It’s easy to make assumptions in C.</p>
<p>Apparently more code made that assumption than made the assumption that
pointers could be cast to <code>int</code>s and back (which was undefined behavior
anyway, and which I was now forced to repent of). And using <code>int</code>s
as an index wasn’t that bad, if no longer fully correct; how many
data structures really do have more than 2 billion items, even if
it is theoretically possible now.</p>
<p>As for <code>long</code>, well, it was deemed reasonable that there should be
some primitive type that was equivalent to the native word length.
There was also some precedent: <code>long</code> already meant this in Java.
And besides, the word pretty much called for it. And this way,
code that casted to <code>int</code> and back could just use <code>long</code> instead.</p>
<p>Well, I wanted my values to be 64-bit. So when I finally got my hands
on a 64-bit computer, for a while, I just replaced all my uses of <code>int</code>
with <code>long</code> (including writing <code>long main(long argc,...)</code> – I was a
stubborn teenager). But ultimately, that felt silly, and I tried to find
a better solution.</p>

<p>Eventually, I figured out the real solution.  Eventually I came to
understand the header files full of <code>typedef</code>s so that code could say
things like <code>int32</code>. The C standards committee recently had adopted
something similar themselves: C99 came with a <code>stdint.h</code>, that would
define types like <code>int64_t</code> or <code>uint32_t</code>.</p>
<p>For various specific use cases, there were also specialized types,
which would automatically resolve to the right type on your platform
for that use case.  For storing indices, there was <code>size_t</code>. <code>ssize_t</code>
was its signed equivalent: more on that later. For being the same width
as a pointer, there was <code>intptr_t</code>/<code>uintptr_t</code>.</p>
<p>But for general-purpose integers, which fit none of these use
cases, after my brief flirtation with <code>long</code> (because 64-bit!)
I continued to use <code>int</code>. It was still the default; the one
I used if there was no reason to use anything else, the one C
APIs would treat as the most normal.</p>

<p>What do I mean by general-purpose? What is there to do with <code>int</code>s
besides indexing? This struck me when I saw the original question:
Even though I use <code>int</code>s all the time in C++, and use various integer
types all the time in Rust, for reasons other than indexing an array
or vector, I had never enumerated or thought deeply about what
situations call for such a thing.</p>
<p>Now I have thought about it, and here are some general-purpose use
cases for <code>int</code>:</p>
<ul>
<li>Assigning unique IDs</li>
<li>Locating widgets on a screen in pixels</li>
<li>Bit fields of options (e.g. <code>O_RDONLY</code>, <code>O_APPEND</code> in <code>open</code> syscall)</li>
<li>Indices that will be low enough and can’t take up 8 bytes</li>
<li>Counting how many times something has happened</li>
<li>Counting how many times you want something to happen</li>
<li>Counting how many times you want to try something before giving up</li>
<li>Counting how many milliseconds to wait before giving up</li>
<li>Counting how many milliseconds to sleep for</li>
<li>Counting, in general. Computers, it turns out, do a lot of that.</li>
<li>(Please let me know if you think of more)</li>
</ul>
<p>Here’s some less generic ones that you definitely want to use special
<code>typedef</code>s for, and for which you should make a conscious decision
about what width to use:</p>
<ul>
<li>Encoding unicode code points</li>
<li>Pointers/words in simulated architectures</li>
<li>Fields in structs that are serialized in wire formats or file formats
<ul>
<li>Go ahead and make them <code>int</code> but maybe write <code>int32_t</code> for these</li>
</ul>
</li>
<li>Port numbers (<code>uint16_t</code>/<code>u16</code>) or other OS-level constructs</li>
<li>Values for hardware registers</li>
<li>Enumerations (in fact, use <code>enum</code>, in Rust or C)</li>
<li>Values for color or sound samples in image or sound formats
<ul>
<li>Where it’s in a large collection</li>
<li>Where the value is only measureable to a certain level</li>
<li>Where it’s effectively a fixed-point value</li>
</ul>
</li>
<li>Individual bytes as bytes: this is always <code>u8</code></li>
<li>(Please let me know if you think of more)</li>
</ul>

<p>In Rust, there is no type with the generic name <code>int</code>. The
programming language is, compared to C, neutral in naming the various
widths.</p>
<p><code>usize</code> is available for indices, and is effectively
considered to be the same as <code>uintptr_t</code> from C as well
(even though some people are <a href="https://github.com/rust-lang/rust/issues/65473">interested in changing
that</a>).</p>
<p>But there’s no named type for when you “just” want an integer.  If you’re
counting things, or assigning IDs, and you want to write out the type,
you have to not only pick a width, you have to actively type it. So
what “should” you do?  Do you go with the default word width of the
processor? Well, your code may be multi-platform, so that might lead
to inconsistencies.</p>
<p>Well, what I do, it turns out, is I think <code>int</code>, C’s default,
and I just write <code>i32</code>. And, as far as choosing <code>32</code> goes, a quick
scan of checked out code indicates I’m far from the only one. And
this convention is endorsed by the programming language, but
more subtly than C: <code>i32</code> is the default type for integer literals
when the type inferencer isn’t constrained to choose another one.</p>
<p>So <code>i32</code> is the Rust convention. But it’s a convention that comes from C.
It comes from the C <code>int</code> type on both 32-bit and 64-bit architectures
being <code>i32</code>.</p>
<p>Choosing an integer width a complicated choice. It’s a trade-off between
compactness and space enough for a wide range of numbers, but also,
it’s a matter of convention: More ints being the same width reduces the
cognitive load. Having a default reduces the cognitive load, even if
the programming language isn’t “in on” the default like C is. And so,
we simply inherited a default from C.</p>
<p>So that’s my advice: use <code>usize</code> for indices, and <code>i32</code> for your
default. Use either types when explicitly called for by a situation.</p>
<p>(This section has been corrected thanks to a
<a href="https://www.reddit.com/r/rust/comments/u3yfiw/thoughts_about_integer_choice/i4t6ith/">Reddit
comment</a>.
Previously, it implied that the Rust programming language itself did nothing
to privilege <code>i32</code> over other types. This is false; it is used as
a default by the inferencer.)</p>

<p>I wish that was it, but it isn’t. We have to talk about signedness.
Which is a whole ‘nother kettle of fish, isn’t it? What is <code>isize</code>
even for? Should you use <code>u32</code> if your value can never be negative?</p>
<p>Again, the literal distinction between signed and unsigned types
is straight-forward and well-known: Signed types have a special
interpretation of the <em>most-significant bit</em> (the MSB). Instead of
indicating <code>2**31</code>, the MSB in a <code>i32</code> indicates <code>-2**31</code> when the bit is
<code>1</code>, making it the <a href="https://en.wikipedia.org/wiki/Sign_bit">“sign bit”</a>
(all Rust platforms and all commonly-used C/C++ platforms use <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s complement
arithmetic</a> for
integers). But the practical implications are less clear.</p>
<h3 id="signedness-in-cc">Signedness in C/C++<a href="#signedness-in-cc" arialabel="Anchor">#</a> </h3>
<p>C/C++ has a weird relationship with signed and unsigned.</p>
<p>For example, if a value overflows or underflows in a signed value,
this is considered undefined behavior. In an unsigned value, it is
defined, and it will wrap around, so that <code>uint32_t</code> arithmetic was
defined modulo <code>2**32</code>.</p>
<p>In general, unsigned integers are more raw. They are supposed to
represent, abstractly, the underlying types used by the processor.
Unsigned integers are for bit fields, for “raw” memory accesses,
for simulating processors, and for other low-level technical
purposes.</p>
<p>For all other purposes, signed integers are really a better choice. If
you are doing application programming in C/C++, unsigned integers might
<a href="http://soundsoftware.ac.uk/c-pitfall-unsigned.html">never</a> be called for.</p>
<p>Here are some specific reasons:</p>
<ul>
<li>As “signed integers” exhibited undefined behavior, compilers are
allowed to make extra optimizations. Some loops just ran faster if
you used signed types. This happened surprisingly often, though
more often with <code>int</code> vs <code>uint32_t</code> than with <code>size_t</code>.</li>
<li>Use of unsigned integers could lead to surprises in loops:</li>
</ul>
<pre><code>for (uint32_t i = 12; i &gt;= 0; --i) {
    // loop infinitely
}
</code></pre><ul>
<li>-1 is often used as a sentinel number, for errors or invalid
values, even if values are expected to be signed. This necessitated
the need for <code>ssize_t</code> as well, as a return type for the <code>read</code> and
<code>write</code> system calls.</li>
</ul>
<p>For all these reasons, signed is generally preferred to unsigned in
C or C++ when correspondence with the machine type is not required,
even if the number is not expected to be negative.</p>
<h3 id="signedness-in-rust">Signedness in Rust<a href="#signedness-in-rust" arialabel="Anchor">#</a> </h3>
<p>In Rust, both signed and unsigned have defined (but configurable) behavior
on overflow. They both either panic on overflow (default in debug builds),
or they wrap around with two’s complement (default in release builds).
This means the optimization difference from C/C++ is moot, and that
argument can be entirely set aside.</p>
<p>I haven’t played around with different loop types to know what’s fully
best, but I do know that Rust’s support for ranges somewhat mitigates
the dangers manual fiddling with <code>for</code>-loop conditions. I suspect
unsigned still comes out looking worse though, as underflow around 0,
very close to the numbers that come up all the time, is way easier
to trigger by accident than overflow or underflow around <code>MAX</code> or
<code>MIN</code> on a signed value.</p>
<p>Similarly, using -1 as a sentinel or error value is not good Rust code
hygiene: <code>Option&lt;u32&gt;</code> or <code>Result&lt;u32, E&gt;</code> could just as easily be
used instead. However, in compact data structures, or working with other
programming languages that use it, -1 as a sentinel value still makes perfect
sense, and is easy to check for.</p>
<p>Old habits die hard. I still use <code>i32</code>, the signed version, as my
default in Rust. Besides, I work with a lot of other programming
languages that might use -1 as a sentinel; it’s shockingly common.</p>
<p>But I’ve also seen a lot of code that uses <code>u32</code> whenever it doesn’t
make sense for the number to be negative. This is in line with Rust’s
philosophy of making invalid values unrepresentable in the type.</p>
<p>In the end, I’m not entirely sure. I think I’ll continue to use <code>i32</code>,
but I feel like I need to think and learn more on the issue. What a
deep question!</p>

<p>What do other people think? This is something I think about way less
often than I should. Do we agree that <code>i32</code> is a good default for
non-indices, <code>usize</code> is a good choice for indices? Has anyone had
need for <code>isize</code> in Rust?</p>
<p>What about the <code>i32</code>/<code>u32</code> debaucle? In the end, this Redditor’s
question made me realize that I’m still figuring out what to do about
Rust’s signedness.</p>

<ul>
<li>A commenter suggested <a href="http://www.catb.org/esr/structure-packing/">The Lost Art of Structure
Packing</a>, a true classic</li>
</ul>

      </div></div></div>
  </body>
</html>
