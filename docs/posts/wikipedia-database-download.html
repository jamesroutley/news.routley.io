<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download">Original</a>
    <h1>Wikipedia: Database Download</h1>
    
    <div id="readability-page-1" class="page"><div id="mw-content-text"><div lang="en" dir="ltr">


<p>Not to be confused with <a href="https://en.wikipedia.org/wiki/Wikipedia:DDD" title="Wikipedia:DDD">WP:DDD</a>.</p>


<p>Wikipedia offers free copies of all available content to interested users. These databases can be used for <a href="https://en.wikipedia.org/wiki/Wikipedia:Mirrors_and_forks" title="Wikipedia:Mirrors and forks">mirroring</a>, personal use, informal backups, offline use or database queries (such as for <a href="https://en.wikipedia.org/wiki/Wikipedia:Maintenance" title="Wikipedia:Maintenance">Wikipedia:Maintenance</a>). All text content is licensed under the <a href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a> (CC-BY-SA), and most is additionally licensed under the <a href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a> (GFDL).<sup id="cite_ref-1"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup> Images and other files are available under <a href="https://en.wikipedia.org/wiki/Wikipedia:Image_copyright_tags" title="Wikipedia:Image copyright tags">different terms</a>, as detailed on their description pages. For our advice about complying with these licenses, see <a href="https://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Wikipedia:Copyrights</a>.
</p>
<meta property="mw:PageProp/toc"/>
<p><h2 id="Offline_Wikipedia_readers" data-mw-thread-id="h-Offline_Wikipedia_readers"><span data-mw-comment-start="" id="h-Offline_Wikipedia_readers"></span>Offline Wikipedia readers<span data-mw-comment-end="h-Offline_Wikipedia_readers"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Offline_Wikipedia_readers","replies":[]}}--></p>
<p>Some of the many ways to read Wikipedia while offline:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Kiwix" title="Kiwix">Kiwix</a>: (<a href="#Kiwix">§ Kiwix</a>) – <a rel="nofollow" href="https://library.kiwix.org/#lang=eng">index of images</a> (2024)</li>
<li><a href="https://en.wikipedia.org/wiki/XOWA" title="XOWA">XOWA</a>: (<a href="#XOWA">§ XOWA</a>) – <a rel="nofollow" href="http://xowa.org/home/wiki/Dashboard/Image_databases.html">index of images</a> (2015)</li>
<li>WikiTaxi: <a href="#WikiTaxi_(for_Windows)">§ WikiTaxi (for Windows)</a></li>
<li>aarddict: <a href="#Aard_Dictionary_/_Aard_2">§ Aard Dictionary / Aard 2</a></li>
<li>BzReader: <a href="#BzReader_and_MzReader_(for_Windows)">§ BzReader and MzReader (for Windows)</a></li>
<li>WikiFilter: <a href="#WikiFilter">§ WikiFilter</a></li>
<li>Wikipedia on rockbox: <a href="#Wikiviewer_for_Rockbox">§ Wikiviewer for Rockbox</a></li>
<li>Selected Wikipedia articles as a printed document: <a href="https://en.wikipedia.org/wiki/Help:Printing" title="Help:Printing">Help:Printing</a></li></ul>
<p>Some of them are mobile applications – see &#34;<a href="https://en.wikipedia.org/wiki/List_of_Wikipedia_mobile_applications" title="List of Wikipedia mobile applications">List of Wikipedia mobile applications</a>&#34;.
</p>
<p><h2 id="Where_do_I_get_the_dumps?" data-mw-thread-id="h-Where_do_I_get_the_dumps?"><span id="Where_do_I_get_the_dumps.3F"></span><span data-mw-comment-start="" id="h-Where_do_I_get_the_dumps?"></span>Where do I get the dumps?<span data-mw-comment-end="h-Where_do_I_get_the_dumps?"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Where_do_I_get_the_dumps?","replies":["h-English-language_Wikipedia-Where_do_I_get_the_dumps?","h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"]}}--></p>
<p><h3 id="English-language_Wikipedia" data-mw-thread-id="h-English-language_Wikipedia-Where_do_I_get_the_dumps?"><span data-mw-comment-start="" id="h-English-language_Wikipedia-Where_do_I_get_the_dumps?"></span>English-language Wikipedia<span data-mw-comment-end="h-English-language_Wikipedia-Where_do_I_get_the_dumps?"></span></h3></p>
<ul><li>Dumps from any Wikimedia Foundation project: <span><a href="https://dumps.wikimedia.org/">dumps<wbr/>.wikimedia<wbr/>.org</a></span> and the <a href="https://archive.org/details/wikimediadownloads" title="iarchive:wikimediadownloads">Internet Archive</a></li>
<li>English Wikipedia dumps in SQL and XML: <span><a href="https://dumps.wikimedia.org/enwiki/">dumps<wbr/>.wikimedia<wbr/>.org<wbr/>/enwiki<wbr/>/</a></span> and the <a rel="nofollow" href="https://archive.org/search.php?query=subject%3A%22enwiki%22%20AND%20subject%3A%22data%20dumps%22%20AND%20collection%3A%22wikimediadownloads%22">Internet Archive</a>
<ul><li><a href="https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia">Download</a> the data dump using a BitTorrent client (torrenting has many benefits and reduces server load, saving bandwidth costs).</li>
<li>pages-articles-multistream.xml.bz2 – Current revisions only, no talk or user pages; this is probably what you want, and is over 19 GB compressed (expands to over 86 GB when decompressed).</li>
<li>pages-meta-current.xml.bz2 – Current revisions only, all pages (including talk)</li>
<li>abstract.xml.gz – page abstracts</li>
<li>all-titles-in-ns0.gz – Article titles only (with redirects)</li>
<li>SQL files for the pages and links are also available</li>
<li>All revisions, all pages: <b>These files expand to multiple <a href="https://en.wikipedia.org/wiki/Terabyte" title="Terabyte">terabytes</a> of text. Please only download these if you know you can cope with this quantity of data.</b> Go to <a href="https://dumps.wikimedia.org/enwiki/latest/">Latest Dumps</a> and look out for all the files that have &#39;pages-meta-history&#39; in their name.</li></ul></li>
<li>To download a subset of the database in XML format, such as a specific category or a list of articles see: <a href="https://en.wikipedia.org/wiki/Special:Export" title="Special:Export">Special:Export</a>, usage of which is described at <a href="https://en.wikipedia.org/wiki/Help:Export" title="Help:Export">Help:Export</a>.</li>
<li>Wiki front-end software: <a href="https://en.wikipedia.org/wiki/MediaWiki" title="MediaWiki">MediaWiki</a> <a href="https://www.mediawiki.org">[1]</a>.</li>
<li>Database backend software: <a href="https://en.wikipedia.org/wiki/MySQL" title="MySQL">MySQL</a>.</li>
<li>Image dumps: See below.</li></ul>
<p><h3 id="Other_Wikipedia_editions" data-mw-thread-id="h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"><span data-mw-comment-start="" id="h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"></span>Other Wikipedia editions<span data-mw-comment-end="h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"></span></h3></p>
<ul><li>Wikipedia dumps in SQL and XML: <span><a href="https://dumps.wikimedia.org/">dumps<wbr/>.wikimedia<wbr/>.org</a></span></li></ul>
<p><h2 id="Should_I_get_multistream?" data-mw-thread-id="h-Should_I_get_multistream?"><span id="Should_I_get_multistream.3F"></span><span data-mw-comment-start="" id="h-Should_I_get_multistream?"></span>Should I get multistream?<span data-mw-comment-end="h-Should_I_get_multistream?"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Should_I_get_multistream?","replies":["h-How_to_use_multistream?-Should_I_get_multistream?","h-Other_languages-Should_I_get_multistream?"]}}--></p>
<p>:
<b>GET THE MULTISTREAM VERSION!</b> (and the corresponding index file, <i>pages-articles-multistream-index.txt.bz2</i>)
</p><p><i>pages-articles.xml.bz2</i> and <i>pages-articles-multistream.xml.bz2</i> both contain the same <i>xml</i> contents. So if you unpack either, you get the same data. But with multistream, it is possible to get an article from the archive without unpacking the whole thing. Your reader should handle this for you; if your reader doesn&#39;t support it, it will work anyway since multistream and non-multistream contain the same <i>xml</i>. The only downside to multistream is that it is marginally larger. You might be tempted to get the smaller non-multistream archive, but this will be useless if you don&#39;t unpack it. And it will unpack to ~5–10 times its original size. Penny wise, pound foolish. Get multistream.
</p><p>NOTE THAT the multistream dump file contains multiple bz2 &#39;streams&#39; (bz2 header, body, footer) concatenated together into one file, in contrast to the vanilla file which contains one stream. Each separate &#39;stream&#39; (or really, file) in the multistream dump contains 100 pages, except possibly the last one. 
</p>
<p><h3 id="How_to_use_multistream?" data-mw-thread-id="h-How_to_use_multistream?-Should_I_get_multistream?"><span id="How_to_use_multistream.3F"></span><span data-mw-comment-start="" id="h-How_to_use_multistream?-Should_I_get_multistream?"></span>How to use multistream?<span data-mw-comment-end="h-How_to_use_multistream?-Should_I_get_multistream?"></span></h3></p>
<p>For multistream, you can get an index file, <i>pages-articles-multistream-index.txt.bz2</i>. The first field of this index is the number of bytes to seek into the compressed archive <i>pages-articles-multistream.xml.bz2</i>, the second is the article ID, the third the article title. 
</p><p>Cut a small part out of the archive with dd using the byte offset as found in the index. You could then either bzip2 decompress it or use bzip2recover, and search the first file for the article ID.
</p><p>See <a rel="nofollow" href="https://docs.python.org/3/library/bz2.html#bz2.BZ2Decompressor">https://docs.python.org/3/library/bz2.html#bz2.BZ2Decompressor</a> for info about such multistream files and about how to decompress them with python; see also <a href="https://gerrit.wikimedia.org/r/plugins/gitiles/operations/dumps/+/ariel/toys/bz2multistream/README.txt">https://gerrit.wikimedia.org/r/plugins/gitiles/operations/dumps/+/ariel/toys/bz2multistream/README.txt</a> and related files for an old working toy.
</p>
<p><h3 id="Other_languages" data-mw-thread-id="h-Other_languages-Should_I_get_multistream?"><span data-mw-comment-start="" id="h-Other_languages-Should_I_get_multistream?"></span>Other languages<span data-mw-comment-end="h-Other_languages-Should_I_get_multistream?"></span></h3></p>
<p>In the <span><a href="https://dumps.wikimedia.org/">dumps<wbr/>.wikimedia<wbr/>.org</a></span> directory you will find the latest SQL and XML dumps for the projects, not just English. The sub-directories are named for the <a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes" title="List of ISO 639-1 codes">language code</a> and the appropriate project. Some other directories (e.g. simple, nostalgia) exist, with the same structure. These dumps are also available from the <a href="https://archive.org/details/wikimediadownloads" title="iarchive:wikimediadownloads">Internet Archive</a>.
</p>
<p><h2 id="Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?" data-mw-thread-id="h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?"><span id="Where_are_the_uploaded_files_.28image.2C_audio.2C_video.2C_etc..29.3F"></span><span data-mw-comment-start="" id="h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?"></span>Where are the uploaded files (image, audio, video, etc.)?<span data-mw-comment-end="h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?","replies":[]}}--></p>
<p>Images and other uploaded media are available from mirrors in addition to being served directly from Wikimedia servers. Bulk download is (as of September 2013) available from mirrors but not offered directly from Wikimedia servers. See the <a href="https://meta.wikimedia.org/wiki/Mirroring_Wikimedia_project_XML_dumps#Media" title="m:Mirroring Wikimedia project XML dumps">list of current mirrors</a>. You should <a href="https://en.wikipedia.org/wiki/Rsync" title="Rsync">rsync</a> from the mirror, then fill in the missing images from <a href="https://upload.wikimedia.org">upload.wikimedia.org</a>; when downloading from <code>upload.wikimedia.org</code> you should throttle yourself to 1 cache miss per second (you can check headers on a response to see if was a hit or miss and then back off when you get a miss) and you shouldn&#39;t use more than one or two simultaneous HTTP connections. In any case, make sure you have an accurate <a href="https://en.wikipedia.org/wiki/User_agent" title="User agent">user agent</a> string with contact info (email address) so ops can contact you if there&#39;s an issue. You should be getting checksums from the mediawiki API and verifying them. The <a href="https://www.mediawiki.org/wiki/API:Etiquette" title="mw:API:Etiquette">API Etiquette</a> page contains some guidelines, although not all of them apply (for example, because upload.wikimedia.org isn&#39;t MediaWiki, there is no <code>maxlag</code> parameter).
</p><p>Unlike most article text, images are not necessarily licensed under the GFDL &amp; CC-BY-SA-4.0. They may be under one of many <a href="https://en.wikipedia.org/wiki/Wikipedia:File_copyright_tags/Free_licenses" title="Wikipedia:File copyright tags/Free licenses">free licenses</a>, in the <a href="https://en.wikipedia.org/wiki/Wikipedia:File_copyright_tags/Public_domain" title="Wikipedia:File copyright tags/Public domain">public domain</a>, believed to be <a href="https://en.wikipedia.org/wiki/Wikipedia:File_copyright_tags/Non-free" title="Wikipedia:File copyright tags/Non-free">fair use</a>, or even copyright infringements (which should be <a href="https://en.wikipedia.org/wiki/Wikipedia:IFD" title="Wikipedia:IFD">deleted</a>). In particular, use of fair use images outside the context of Wikipedia or similar works may be illegal. Images under most licenses require a credit, and possibly other attached copyright information. This information is included in image description pages, which are part of the text dumps available from <a href="https://dumps.wikimedia.org/">dumps.wikimedia.org</a>. In conclusion, download these images at your own risk (<a href="https://dumps.wikimedia.org/legal.html">Legal</a>).
</p>
<p><h2 id="Dealing_with_compressed_files" data-mw-thread-id="h-Dealing_with_compressed_files"><span data-mw-comment-start="" id="h-Dealing_with_compressed_files"></span>Dealing with compressed files<span data-mw-comment-end="h-Dealing_with_compressed_files"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Dealing_with_compressed_files","replies":[]}}--></p>
<p>Compressed dump files are significantly compressed, thus after being decompressed will take up <b>large</b> amounts of drive space.  A large list of decompression programs are described in <a href="https://en.wikipedia.org/wiki/Comparison_of_file_archivers" title="Comparison of file archivers">comparison of file archivers</a>. The following  programs in particular can be used to decompress bzip2, <a href="https://en.wikipedia.org/wiki/.bz2" title=".bz2">.bz2</a>, <a href="https://en.wikipedia.org/wiki/.zip" title=".zip">.zip</a>, and <a href="https://en.wikipedia.org/wiki/.7z" title=".7z">.7z</a> files.
</p>
<dl><dt><a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a></dt></dl>
<p>Beginning with <a href="https://en.wikipedia.org/wiki/Windows_XP" title="Windows XP">Windows XP</a>, a basic decompression program enables decompression of zip files.<sup id="cite_ref-2"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup><sup id="cite_ref-3"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup> Among others, the following can be used to decompress bzip2 files.
</p>
<ul><li><a rel="nofollow" href="https://sourceware.org/pub/bzip2/v102/bzip2-102-x86-win32.exe">bzip2 (command-line)</a> (from <a rel="nofollow" href="https://www.sourceware.org/bzip2/">here</a>) is available for free under a BSD license.</li>
<li><a href="https://en.wikipedia.org/wiki/7-Zip" title="7-Zip">7-Zip</a> is available for free under an <a href="https://en.wikipedia.org/wiki/GNU_Lesser_General_Public_License" title="GNU Lesser General Public License">LGPL</a> license.</li>
<li><a href="https://en.wikipedia.org/wiki/WinRAR" title="WinRAR">WinRAR</a></li>
<li><a href="https://en.wikipedia.org/wiki/WinZip" title="WinZip">WinZip</a></li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Macintosh" title="Macintosh">Macintosh</a> (Mac)</dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/MacOS" title="MacOS">macOS</a> ships with the command-line bzip2 tool.</li></ul>
<dl><dt>GNU/<a href="https://en.wikipedia.org/wiki/Linux" title="Linux">Linux</a></dt></dl>
<ul><li>Most GNU/Linux distributions ship with the command-line bzip2 tool.</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution" title="Berkeley Software Distribution">Berkeley Software Distribution</a> (BSD)</dt></dl>
<ul><li>Some BSD systems ship with the command-line bzip2 tool as part of the operating system. Others, such as <a href="https://en.wikipedia.org/wiki/OpenBSD" title="OpenBSD">OpenBSD</a>, provide it as a package which must first be installed.</li></ul>
<dl><dt>Notes</dt></dl>
<ol><li>Some older versions of bzip2 may not be able to handle files larger than 2 GB, so make sure you have the latest version if you experience any problems.</li>
<li>Some older archives are compressed with gzip, which is compatible with PKZIP (the most common Windows format).</li></ol>
<p><h2 id="Dealing_with_large_files" data-mw-thread-id="h-Dealing_with_large_files"><span data-mw-comment-start="" id="h-Dealing_with_large_files"></span>Dealing with large files<span data-mw-comment-end="h-Dealing_with_large_files"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Dealing_with_large_files","replies":["h-File_system_limits-Dealing_with_large_files","h-Operating_system_limits-Dealing_with_large_files","h-Tips-Dealing_with_large_files"]}}--></p>
<p>As files grow in size, so does the likelihood they will exceed some limit of a computing device. Each operating system, file system, hard storage device, and software (application) has a maximum file size limit. Each one of these will likely have a different maximum, and the lowest limit of all of them will become the file size limit for a storage device.
</p><p>The older the software in a computing device, the more likely it will have a 2 GB file limit somewhere in the system. This is due to older software using 32-bit integers for file indexing, which limits file sizes to 2^31 bytes (2 GB) (for signed integers), or 2^32 (4 GB) (for unsigned integers). Older <a href="https://en.wikipedia.org/wiki/C_(programming_language)" title="C (programming language)">C</a> <a href="https://en.wikipedia.org/wiki/Library_(computing)" title="Library (computing)">programming libraries</a> have this 2 or 4 GB limit, but the newer file libraries have been converted to 64-bit integers thus supporting file sizes up to 2^63 or 2^64 bytes (8 or 16 <a href="https://en.wikipedia.org/wiki/Exabyte" title="Exabyte">EB</a>).
</p><p>Before starting a download of a large file, check the storage device to ensure its file system can support files of such a large size, check the amount of free space to ensure that it can hold the downloaded file, and make sure the device(s) you&#39;ll use the storage with are able to read your chosen file system.
</p>
<p><h3 id="File_system_limits" data-mw-thread-id="h-File_system_limits-Dealing_with_large_files"><span data-mw-comment-start="" id="h-File_system_limits-Dealing_with_large_files"></span>File system limits<span data-mw-comment-end="h-File_system_limits-Dealing_with_large_files"></span></h3></p>
<p>There are two limits for a file system: the file system size limit, and the file system limit. In general, since the file size limit is less than the file system limit, the larger file system limits are a moot point. A large percentage of users assume they can create files up to the size of their storage device, but are wrong in their assumption. For example, a 16 GB storage device formatted as FAT32 file system has a file limit of 4 GB for any single file. The following is a list of the most common file systems, and see <a href="https://en.wikipedia.org/wiki/Comparison_of_file_systems#Limits" title="Comparison of file systems">Comparison of file systems</a> for additional detailed information.
</p>
<dl><dt><a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a></dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/File_Allocation_Table" title="File Allocation Table">FAT16</a> supports files up to 4 <a href="https://en.wikipedia.org/wiki/Gigabyte" title="Gigabyte">GB</a>. FAT16 is the factory format of smaller <a href="https://en.wikipedia.org/wiki/USB" title="USB">USB</a> drives and all <a href="https://en.wikipedia.org/wiki/SD_card" title="SD card">SD cards</a> that are 2 GB or smaller.</li>
<li><a href="https://en.wikipedia.org/wiki/File_Allocation_Table" title="File Allocation Table">FAT32</a> supports files up to 4 GB. FAT32 is the factory format of larger <a href="https://en.wikipedia.org/wiki/USB" title="USB">USB</a> drives and all <a href="https://en.wikipedia.org/wiki/SD_card" title="SD card">SDHC</a> cards that are 4 GB or larger.</li>
<li><a href="https://en.wikipedia.org/wiki/ExFAT" title="ExFAT">exFAT</a> supports files up to 127 <a href="https://en.wikipedia.org/wiki/Petabyte" title="Petabyte">PB</a>. exFAT is the factory format of all <a href="https://en.wikipedia.org/wiki/SD_card" title="SD card">SDXC</a> cards, but is incompatible with most flavors of UNIX due to licensing problems.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources.">citation needed</span></a></i>]</sup></li>
<li><a href="https://en.wikipedia.org/wiki/NTFS" title="NTFS">NTFS</a> supports files up to 16 <a href="https://en.wikipedia.org/wiki/Terabyte" title="Terabyte">TB</a>. NTFS is the default file system for modern <a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a> computers, including Windows 2000, Windows XP, and all their successors to date. Versions after Windows 8 can support larger files if the file system is formatted with a larger cluster size.</li>
<li><a href="https://en.wikipedia.org/wiki/ReFS" title="ReFS">ReFS</a> supports files up to 16 <a href="https://en.wikipedia.org/wiki/Exabyte" title="Exabyte">EB</a>.</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Macintosh" title="Macintosh">Macintosh</a> (Mac)</dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/HFS_Plus" title="HFS Plus">HFS Plus</a> (HFS+) (Also known as Mac OS Extended) supports files up to 8 EiB (8 exbibytes) (2^63 bytes).<sup id="cite_ref-AppleVolumecomparison_4-0"><a href="#cite_note-AppleVolumecomparison-4"><span>[</span>4<span>]</span></a></sup> An exbibyte is similar to an <a href="https://en.wikipedia.org/wiki/Exabyte" title="Exabyte">exabyte</a>. HFS Plus is supported on <a href="https://en.wikipedia.org/wiki/MacOS" title="MacOS">macOS</a> 10.2+ and <a href="https://en.wikipedia.org/wiki/IOS" title="IOS">iOS</a>. It was the default file system for <a href="https://en.wikipedia.org/wiki/MacOS" title="MacOS">macOS</a> computers prior to the release of <a href="https://en.wikipedia.org/wiki/MacOS_High_Sierra" title="MacOS High Sierra">macOS High Sierra</a> in 2017 when it was replaced as default with <a href="https://en.wikipedia.org/wiki/Apple_File_System" title="Apple File System">Apple File System</a>, <a href="https://en.wikipedia.org/wiki/APFS" title="APFS">APFS</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/APFS" title="APFS">APFS</a> supports files up to 8 exbibytes (2^63 bytes).<sup id="cite_ref-AppleVolumecomparison_4-1"><a href="#cite_note-AppleVolumecomparison-4"><span>[</span>4<span>]</span></a></sup></li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Linux" title="Linux">Linux</a></dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/Ext2" title="Ext2">ext2</a> and <a href="https://en.wikipedia.org/wiki/Ext3" title="Ext3">ext3</a> supports files up to 16 GB, but up to 2 TB with larger block sizes. See <a rel="nofollow" href="https://users.suse.com/~aj/linux_lfs.html">https://users.suse.com/~aj/linux_lfs.html</a> for more information.</li>
<li><a href="https://en.wikipedia.org/wiki/Ext4" title="Ext4">ext4</a> supports files up to 16 TB, using 4 KB block size. (<a rel="nofollow" href="https://fedoraproject.org/wiki/Features/F17Ext4Above16T">limit removed in e2fsprogs-1.42 (2012)</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/XFS" title="XFS">XFS</a> supports files up to 8 EB.</li>
<li><a href="https://en.wikipedia.org/wiki/ReiserFS" title="ReiserFS">ReiserFS</a> supports files up to 1 EB, 8 TB on 32-bit systems.</li>
<li><a href="https://en.wikipedia.org/wiki/JFS_(file_system)" title="JFS (file system)">JFS</a> supports files up to 4 PB.</li>
<li><a href="https://en.wikipedia.org/wiki/Btrfs" title="Btrfs">Btrfs</a> supports files up to 16 EB.</li>
<li><a href="https://en.wikipedia.org/wiki/NILFS" title="NILFS">NILFS</a> supports files up to 8 EB.</li>
<li><a href="https://en.wikipedia.org/wiki/YAFFS" title="YAFFS">YAFFS</a>2 supports files up to 2 GB</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/FreeBSD" title="FreeBSD">FreeBSD</a></dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/ZFS" title="ZFS">ZFS</a> supports files up to 16 EB.</li></ul>
<dl><dt>FreeBSD and other BSDs</dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/Unix_File_System" title="Unix File System">Unix File System</a> (UFS) supports files up to 8 ZiB.</li></ul>
<p><h3 id="Operating_system_limits" data-mw-thread-id="h-Operating_system_limits-Dealing_with_large_files"><span data-mw-comment-start="" id="h-Operating_system_limits-Dealing_with_large_files"></span>Operating system limits<span data-mw-comment-end="h-Operating_system_limits-Dealing_with_large_files"></span></h3></p>
<p>Each operating system has internal file system limits for file size and drive size, which is independent of the file system or physical media. If the operating system has any limits lower than the file system or physical media, then the OS limits will be the real limit.
</p>
<dl><dt><a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a></dt></dl>
<ul><li>Windows 95, 98, ME have a 4 GB limit for all file sizes.</li>
<li>Windows XP has a 16 TB limit for all file sizes.</li>
<li>Windows 7 has a 16 TB limit for all file sizes.</li>
<li>Windows 8, 10, and Server 2012 have a 256 TB limit for all file sizes.</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Linux" title="Linux">Linux</a></dt></dl>
<ul><li>32-bit kernel 2.4.x systems have a 2 TB limit for all file systems.</li>
<li>64-bit kernel 2.4.x systems have an 8 EB limit for all file systems.</li>
<li>32-bit kernel 2.6.x systems without option CONFIG_LBD have a 2 TB limit for all file systems.</li>
<li>32-bit kernel 2.6.x systems with option CONFIG_LBD and all 64-bit kernel 2.6.x systems have an 8 ZB limit for all file systems.<sup id="cite_ref-5"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup></li></ul>
<p><a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a>:
Android is based on Linux, which determines its base limits.
</p>
<ul><li>Internal storage:
<ul><li><a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a> 2.3 and later uses the <a href="https://en.wikipedia.org/wiki/Ext4" title="Ext4">ext4</a> file system.<sup id="cite_ref-6"><a href="#cite_note-6"><span>[</span>6<span>]</span></a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a> 2.2 and earlier uses the <a href="https://en.wikipedia.org/wiki/YAFFS" title="YAFFS">YAFFS</a>2 file system.</li></ul></li>
<li>External storage slots:
<ul><li>All Android devices should support FAT16, FAT32, ext2 file systems.</li>
<li>Android 2.3 and later supports ext4 file system.</li></ul></li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Apple_Inc." title="Apple Inc.">Apple</a> <a href="https://en.wikipedia.org/wiki/IOS" title="IOS">iOS</a> (see <a href="https://en.wikipedia.org/wiki/List_of_iPhone_models" title="List of iPhone models">List of iPhone models</a>)</dt>
<dd></dd></dl>
<ul><li>All devices support <a href="https://en.wikipedia.org/wiki/HFS_Plus" title="HFS Plus">HFS Plus</a> (HFS+) for internal storage. No devices have external storage slots. Devices on 10.3 or later run <a href="https://en.wikipedia.org/wiki/Apple_File_System" title="Apple File System">Apple File System</a> supporting a max file size of 8 EB.</li></ul>
<p><h3 id="Tips" data-mw-thread-id="h-Tips-Dealing_with_large_files"><span data-mw-comment-start="" id="h-Tips-Dealing_with_large_files"></span>Tips<span data-mw-comment-end="h-Tips-Dealing_with_large_files"></span></h3></p>
<p><h4 id="Detect_corrupted_files" data-mw-thread-id="h-Detect_corrupted_files-Tips"><span data-mw-comment-start="" id="h-Detect_corrupted_files-Tips"></span>Detect corrupted files<span data-mw-comment-end="h-Detect_corrupted_files-Tips"></span></h4></p>
<p>It is useful to check the <a href="https://en.wikipedia.org/wiki/MD5" title="MD5">MD5</a> sums (provided in a file in the download directory) to make sure the download was complete and accurate. This can be checked by running the &#34;md5sum&#34; command on the files downloaded. Given their sizes, this may take some time to calculate. Due to the technical details of how files are stored, <i>file sizes</i> may be reported differently on different filesystems, and so are not necessarily reliable. Also, corruption may have occurred during the download, though this is unlikely.
</p>
<p><h4 id="Linux_and_Unix" data-mw-thread-id="h-Linux_and_Unix-Tips"><span data-mw-comment-start="" id="h-Linux_and_Unix-Tips"></span>Linux and Unix<span data-mw-comment-end="h-Linux_and_Unix-Tips"></span></h4></p>
<p>If you seem to be hitting the 2 GB limit, try using <a href="https://en.wikipedia.org/wiki/Wget" title="Wget">wget</a> version 1.10 or greater, <a href="https://en.wikipedia.org/wiki/CURL" title="CURL">cURL</a> version 7.11.1-1 or greater, or a recent version of <a href="https://en.wikipedia.org/wiki/Lynx_(web_browser)" title="Lynx (web browser)">lynx</a> (using -dump). Also, you can resume downloads (for example wget -c).
</p>

<p>Suppose you are building a piece of software that at certain points displays information that came from Wikipedia. If you want your program to display the information in a different way than can be seen in the live version, you&#39;ll probably need the wikicode that is used to enter it, instead of the finished HTML.
</p><p>Also, if you want to get all the data, you&#39;ll probably want to transfer it in the most efficient way that&#39;s possible. The wikipedia.org servers need to do quite a bit of work to convert the wikicode into HTML. That&#39;s time consuming both for you and for the wikipedia.org servers, so simply spidering all pages is not the way to go.
</p><p>To access any article in XML, one at a time, access <a href="https://en.wikipedia.org/wiki/Special:Export/Title_of_the_article" title="Special:Export/Title of the article">Special:Export/Title of the article</a>.
</p><p>Read more about this at <a href="https://en.wikipedia.org/wiki/Special:Export" title="Special:Export">Special:Export</a>.
</p><p>Please be aware that live mirrors of Wikipedia that are dynamically loaded from the Wikimedia servers are prohibited. Please see <a href="https://en.wikipedia.org/wiki/Wikipedia:Mirrors_and_forks" title="Wikipedia:Mirrors and forks">Wikipedia:Mirrors and forks</a>.
</p>
<p><h3 id="Please_do_not_use_a_web_crawler" data-mw-thread-id="h-Please_do_not_use_a_web_crawler-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"><span data-mw-comment-start="" id="h-Please_do_not_use_a_web_crawler-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span>Please do not use a web crawler<span data-mw-comment-end="h-Please_do_not_use_a_web_crawler-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span></h3></p>
<p>Please do not use a <a href="https://en.wikipedia.org/wiki/Web_crawler" title="Web crawler">web crawler</a> to download large numbers of articles. Aggressive crawling of the server can cause a dramatic slow-down of Wikipedia.
</p>
<p><h4 id="Sample_blocked_crawler_email" data-mw-thread-id="h-Sample_blocked_crawler_email-Please_do_not_use_a_web_crawler"><span data-mw-comment-start="" id="h-Sample_blocked_crawler_email-Please_do_not_use_a_web_crawler"></span>Sample blocked crawler email<span data-mw-comment-end="h-Sample_blocked_crawler_email-Please_do_not_use_a_web_crawler"></span></h4></p>
<dl><dd>IP address <i><b>nnn.nnn.nnn.nnn</b></i> was retrieving up to 50 pages per second from wikipedia.org addresses. Something like at least a second delay between requests is reasonable. Please respect that setting. If you must exceed it a little, do so only during the least busy times shown in our site load graphs at <b><span><a href="https://stats.wikimedia.org/#/all-wikipedia-projects">stats<wbr/>.wikimedia<wbr/>.org<wbr/>#<wbr/>/all-wikipedia-projects</a></span></b>. It&#39;s worth noting that to crawl the whole site at one hit per second will take several weeks. The originating IP is now blocked or will be shortly. Please contact us if you want it unblocked. Please don&#39;t try to circumvent it – we&#39;ll just block your whole IP range.</dd></dl>
<dl><dd>If you want information on how to get our content more efficiently, we offer a variety of methods, including weekly database dumps which you can load into MySQL and crawl locally at any rate you find convenient. Tools are also available which will do that for you as often as you like once you have the infrastructure in place.</dd></dl>
<dl><dd>Instead of an email reply you may prefer to visit <span>#mediawiki</span> <sup><a rel="nofollow" href="https://web.libera.chat/?channel=#mediawiki"><span>connect</span></a></sup> at irc.libera.chat to discuss your options with our team.</dd></dl>
<p><h3 id="Doing_SQL_queries_on_the_current_database_dump" data-mw-thread-id="h-Doing_SQL_queries_on_the_current_database_dump-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"><span data-mw-comment-start="" id="h-Doing_SQL_queries_on_the_current_database_dump-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span>Doing SQL queries on the current database dump<span data-mw-comment-end="h-Doing_SQL_queries_on_the_current_database_dump-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span></h3></p>
<p>You can do SQL queries on the current database dump using <a rel="nofollow" href="https://quarry.wmflabs.org">Quarry</a> (as a replacement for the disabled <a href="https://en.wikipedia.org/wiki/Special:Asksql" title="Special:Asksql (page does not exist)">Special:Asksql</a> page).
</p>
<p><h2 id="Database_schema" data-mw-thread-id="h-Database_schema"><span data-mw-comment-start="" id="h-Database_schema"></span>Database schema<span data-mw-comment-end="h-Database_schema"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Database_schema","replies":["h-SQL_schema-Database_schema","h-XML_schema-Database_schema"]}}--></p>
<p><h3 id="SQL_schema" data-mw-thread-id="h-SQL_schema-Database_schema"><span data-mw-comment-start="" id="h-SQL_schema-Database_schema"></span>SQL schema<span data-mw-comment-end="h-SQL_schema-Database_schema"></span></h3></p>
<p><i>See also: <a href="https://www.mediawiki.org/wiki/Manual:Database_layout" title="mw:Manual:Database layout">mw:Manual:Database layout</a></i>
</p><p>The sql file used to initialize a MediaWiki database can be found <a href="https://phabricator.wikimedia.org/source/mediawiki/browse/master/sql/mysql/tables-generated.sql">here</a>.
</p>
<p><h3 id="XML_schema" data-mw-thread-id="h-XML_schema-Database_schema"><span data-mw-comment-start="" id="h-XML_schema-Database_schema"></span>XML schema<span data-mw-comment-end="h-XML_schema-Database_schema"></span></h3></p>
<p>The XML schema for each dump is defined at the top of the file and described in the <a href="https://www.mediawiki.org/wiki/Help:Export#Export_format" title="mw:Help:Export">MediaWiki export help page</a>.
</p>
<p><h2 id="Help_to_parse_dumps_for_use_in_scripts" data-mw-thread-id="h-Help_to_parse_dumps_for_use_in_scripts"><span data-mw-comment-start="" id="h-Help_to_parse_dumps_for_use_in_scripts"></span>Help to parse dumps for use in scripts<span data-mw-comment-end="h-Help_to_parse_dumps_for_use_in_scripts"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Help_to_parse_dumps_for_use_in_scripts","replies":["h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"]}}--></p>
<ul><li><a href="https://en.wikipedia.org/wiki/Wikipedia:Computer_help_desk/ParseMediaWikiDump" title="Wikipedia:Computer help desk/ParseMediaWikiDump">Wikipedia:Computer help desk/ParseMediaWikiDump</a> describes the <a href="https://en.wikipedia.org/wiki/Perl" title="Perl">Perl</a> Parse::MediaWikiDump library, which can parse XML dumps.</li>
<li><a rel="nofollow" href="https://web.archive.org/web/20070907074625/http://www.cs.technion.ac.il/~gabr/resources/code/wikiprep">Wikipedia preprocessor (wikiprep.pl)</a> is a <a href="https://en.wikipedia.org/wiki/Perl" title="Perl">Perl</a> script that preprocesses raw XML dumps and builds link tables, category hierarchies, collects anchor text for each article etc.</li>
<li><a rel="nofollow" href="https://github.com/svick/Wikipedia-SQl-dump-parser">Wikipedia SQL dump parser</a> is a .NET library to read MySQL dumps without the need to use MySQL database</li>
<li><a rel="nofollow" href="https://github.com/MartinRichards23/WikiDumpParser">WikiDumpParser</a> – a .NET Core library to parse the database dumps.</li>
<li><a rel="nofollow" href="https://github.com/newca12/dictionary-builder">Dictionary Builder</a> is a Rust program that can parse XML dumps and extract entries in files</li>
<li><a rel="nofollow" href="https://github.com/napsternxg/WikiUtils">Scripts for parsing Wikipedia dumps</a> ­– Python based scripts for parsing sql.gz files from wikipedia dumps.</li>
<li><a rel="nofollow" href="https://crates.io/crates/parse-mediawiki-sql/">parse-mediawiki-sql</a> – a Rust library for quickly parsing the SQL dump files with minimal memory allocation</li>
<li><a rel="nofollow" href="https://gitlab.com/tozd/go/mediawiki">gitlab.com/tozd/go/mediawiki</a> – a Go package providing utilities for processing Wikipedia and Wikidata dumps.</li></ul>
<p><h3 id="Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump" data-mw-thread-id="h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"><span data-mw-comment-start="" id="h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"></span>Doing Hadoop MapReduce on the Wikipedia current database dump<span data-mw-comment-end="h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"></span></h3></p>
<p>You can do Hadoop MapReduce queries on the current database dump, but you will need an extension to the InputRecordFormat to
have each &lt;page&gt; &lt;/page&gt; be a single mapper input. A working set of java methods (jobControl, mapper, reducer, and XmlInputRecordFormat) is available at <a rel="nofollow" href="https://tpmoyer-gallery.appspot.com/hadoopWikipedia">Hadoop on the Wikipedia</a>
</p>
<p><h2 id="Help_to_import_dumps_into_MySQL" data-mw-thread-id="h-Help_to_import_dumps_into_MySQL"><span data-mw-comment-start="" id="h-Help_to_import_dumps_into_MySQL"></span>Help to import dumps into MySQL<span data-mw-comment-end="h-Help_to_import_dumps_into_MySQL"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Help_to_import_dumps_into_MySQL","replies":[]}}--></p>
<p>See:
</p>
<ul><li><a href="https://www.mediawiki.org/wiki/Manual:Importing_XML_dumps" title="mw:Manual:Importing XML dumps">mw:Manual:Importing XML dumps</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Data_dumps" title="m:Data dumps">m:Data dumps</a></li></ul>

<p>Access to recent article update dumps (Snapshot API) or individual article retrieval (On-demand API) are available via <i><a href="https://enterprise.wikimedia.com/">Wikimedia Enterprise</a></i> with a free account (<a href="https://meta.wikimedia.org/wiki/Wikimedia_Enterprise" title="m:Wikimedia Enterprise">documentation on Meta wiki</a>). Alternatively, use your developer account to access APIs within Wikimedia Cloud Services.
</p>
<p><h2 id="Static_HTML_tree_dumps_for_mirroring_or_CD_distribution" data-mw-thread-id="h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span data-mw-comment-start="" id="h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Static HTML tree dumps for mirroring or CD distribution<span data-mw-comment-end="h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","replies":["h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","h-Aard_Dictionary_\/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","h-Wikiviewer_for_Rockbox-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"]}}--></p>
<p>MediaWiki 1.5 includes routines to dump a wiki to HTML, rendering the HTML with the same parser used on a live wiki. As the following page states, putting one of these dumps on the web unmodified will constitute a trademark violation. They are intended for private viewing in an intranet or desktop installation.
</p>
<ul><li>If you want to draft a traditional website in Mediawiki and dump it to HTML format, you might want to try <a rel="nofollow" href="https://barnesc.blogspot.com/2005/10/mw2html-export-mediawiki-to-static.html">mw2html</a> by <a href="https://en.wikipedia.org/wiki/User:Connelly" title="User:Connelly">User:Connelly</a>.</li>
<li>If you&#39;d like to help develop dump-to-static HTML tools, please drop us a note on <a href="https://en.wikipedia.org/wiki/Wikipedia:Mailing_lists" title="Wikipedia:Mailing lists">the developers&#39; mailing list</a>.</li>
<li>Static HTML dumps as of 2008 are available <a href="https://dumps.wikimedia.org/other/static_html_dumps/">here</a>.</li></ul>
<p><b>See also:</b>
</p>
<ul><li><a href="https://www.mediawiki.org/wiki/Alternative_parsers" title="mw:Alternative parsers">mw:Alternative parsers</a> lists some other not working options for getting static HTML dumps</li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:Snapshots" title="Wikipedia:Snapshots">Wikipedia:Snapshots</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:TomeRaider_database" title="Wikipedia:TomeRaider database">Wikipedia:TomeRaider database</a></li></ul>
<p><h3 id="Kiwix" data-mw-thread-id="h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span data-mw-comment-start="" id="h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Kiwix<span data-mw-comment-end="h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h3></p>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Kiwix_on_Android.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Kiwix_on_Android.jpg/250px-Kiwix_on_Android.jpg" decoding="async" width="250" height="141" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Kiwix_on_Android.jpg/500px-Kiwix_on_Android.jpg 1.5x" data-file-width="4608" data-file-height="2592"/></a><figcaption>Kiwix on an Android tablet</figcaption></figure>
<p><a href="https://en.wikipedia.org/wiki/Kiwix" title="Kiwix">Kiwix</a> is by far the largest offline distribution of <a href="https://en.wikipedia.org/wiki/Wikipedia" title="Wikipedia">Wikipedia</a> to date. As an offline reader, Kiwix works with a library of contents that are zim files: you can pick &amp; choose whichever <a href="https://en.wikipedia.org/wiki/Wikimedia_Foundation#Wikimedia_projects" title="Wikimedia Foundation">Wikimedia project</a> (Wikipedia in any language, <a href="https://en.wikipedia.org/wiki/Wiktionary" title="Wiktionary">Wiktionary</a>, <a href="https://en.wikipedia.org/wiki/Wikisource" title="Wikisource">Wikisource</a>, etc.), as well as <a href="https://en.wikipedia.org/wiki/TED_Talks" title="TED Talks">TED Talks</a>, <a href="https://en.wikipedia.org/wiki/PhET_Interactive_Simulations" title="PhET Interactive Simulations">PhET Interactive Maths &amp; Physics simulations</a>, <a href="https://en.wikipedia.org/wiki/Project_Gutenberg" title="Project Gutenberg">Project Gutenberg</a>, etc.
</p><p>It is free and open source, and currently available for download on: 
</p>
<ul><li><a rel="nofollow" href="https://play.google.com/store/apps/details?id=org.kiwix.kiwixmobile">Android</a></li>
<li><a rel="nofollow" href="https://itunes.apple.com/us/app/kiwix/id997079563?mt=8">iOS</a></li>
<li><a rel="nofollow" href="https://apps.apple.com/us/app/kiwix-desktop/id1275066656">macOS</a></li>
<li><a rel="nofollow" href="https://download.kiwix.org/release/kiwix-desktop/kiwix-desktop_windows_x64.zip">Windows</a> &amp; <a rel="nofollow" href="https://www.microsoft.com/store/apps/9P8SLZ4J979J">Windows 10 (UWP)</a></li>
<li><a rel="nofollow" href="https://flathub.org/apps/details/org.kiwix.desktop">GNU/Linux</a></li></ul>
<p>... as well as extensions for <a rel="nofollow" href="https://chrome.google.com/webstore/detail/kiwix/donaljnlmapmngakoipdmehbfcioahhk">Chrome</a> &amp; <a rel="nofollow" href="https://addons.mozilla.org/fr/firefox/addon/kiwix-offline/">Firefox</a> browsers, server solutions, etc. See <a rel="nofollow" href="https://www.kiwix.org/en/">official Website</a> for the complete Kiwix portfolio.
</p>
<p><h3 id="Aard_Dictionary_/_Aard_2" data-mw-thread-id="h-Aard_Dictionary_/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span id="Aard_Dictionary_.2F_Aard_2"></span><span data-mw-comment-start="" id="h-Aard_Dictionary_/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Aard Dictionary / Aard 2<span data-mw-comment-end="h-Aard_Dictionary_/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h3></p>
<p><a rel="nofollow" href="https://github.com/aarddict">Aard Dictionary</a> is an offline Wikipedia reader. No images. Cross-platform for Windows, Mac, Linux, Android, Maemo. Runs on rooted Nook and Sony PRS-T1 eBooks readers.
</p><p>It also has a successor <a rel="nofollow" href="http://aarddict.org/">Aard 2</a>.
</p>

<p>The wikiviewer plugin for rockbox permits viewing converted Wikipedia dumps on many <a href="https://en.wikipedia.org/wiki/Rockbox" title="Rockbox">Rockbox</a> devices.
It needs a custom build and conversion of the wiki dumps using the instructions available at <a rel="nofollow" href="http://www.rockbox.org/tracker/4755">http://www.rockbox.org/tracker/4755</a> . The conversion recompresses the file and splits it into 1 <a href="https://en.wikipedia.org/wiki/Gigabyte" title="Gigabyte">GB</a> files and an index file which all need to be in the same folder on the device or micro sd card.
</p>
<p><h3 id="Old_dumps" data-mw-thread-id="h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span data-mw-comment-start="" id="h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Old dumps<span data-mw-comment-end="h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h3></p>
<ul><li>The static version of Wikipedia created by Wikimedia: <a href="https://static.wikipedia.org/">http://static.wikipedia.org/</a> Feb. 11, 2013 – This is apparently offline now. There was no content.</li>
<li><a rel="nofollow" href="http://www.tommasoconforti.com/">Wiki2static</a> (site down as of October 2005) <b>was</b> an experimental program set up by <a href="https://en.wikipedia.org/wiki/User:Alfio" title="User:Alfio">User:Alfio</a> to generate html dumps, inclusive of images, search function and alphabetical index. At the linked site experimental dumps and the script itself can be downloaded. As an example it was used to generate these copies of <a rel="nofollow" href="http://fixedreference.org/en/20040424/wikipedia/Main_Page">English WikiPedia 24 April 04</a>, <a rel="nofollow" href="https://web.archive.org/web/20040618150011/http://fixedreference.org/simple/20040501/wikipedia/Main_Page">Simple WikiPedia 1 May 04</a>(old database) format and <a rel="nofollow" href="http://july.fixedreference.org/en/20040724/wikipedia/Main_Page">English WikiPedia 24 July 04</a><a rel="nofollow" href="http://july.fixedreference.org/simple/20040724/wikipedia/Main_Page">Simple WikiPedia 24 July 04</a>, <a rel="nofollow" href="http://july.fixedreference.org/fr/20040727/wikipedia/Accueil">WikiPedia Francais 27 Juillet 2004</a> (new format). <a href="https://en.wikipedia.org/wiki/User:BozMo" title="User:BozMo">BozMo</a> uses a version to generate periodic static copies at <a rel="nofollow" href="http://fixedreference.org/">fixed reference</a> (site down as of October 2017).</li></ul>
<p><h2 id="Dynamic_HTML_generation_from_a_local_XML_database_dump" data-mw-thread-id="h-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>Dynamic HTML generation from a local XML database dump<span data-mw-comment-end="h-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Dynamic_HTML_generation_from_a_local_XML_database_dump","replies":["h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"]}}--></p>
<p>Instead of converting a database dump file to many pieces of static HTML, one can also use a dynamic HTML generator. Browsing a wiki page is just like browsing a Wiki site, but the content is fetched and converted from a local dump file on request from the browser.
</p>
<p><h3 id="XOWA" data-mw-thread-id="h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>XOWA<span data-mw-comment-end="h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a href="https://en.wikipedia.org/wiki/XOWA" title="XOWA">XOWA</a> is a free, open-source application that helps download Wikipedia to a computer. Access all of Wikipedia offline, without an internet connection!
It is currently in the beta stage of development, but is functional. It is available for download <a rel="nofollow" href="http://xowa.org/home/wiki/Help/Download_XOWA.html">here</a>.
</p>
<p><h4 id="Features" data-mw-thread-id="h-Features-XOWA"><span data-mw-comment-start="" id="h-Features-XOWA"></span>Features<span data-mw-comment-end="h-Features-XOWA"></span></h4></p>
<ul><li>Displays all articles from Wikipedia without an internet connection.</li>
<li>Download a complete, recent copy of English Wikipedia.</li>
<li>Display 5.2+ million articles in full HTML formatting.</li>
<li>Show images within an article. Access 3.7+ million images using the offline image databases.</li>
<li>Works with any Wikimedia wiki, including Wikipedia, Wiktionary, Wikisource, Wikiquote, Wikivoyage (also some non-wmf dumps)</li>
<li>Works with any non-English language wiki such as French Wikipedia, German Wikisource, Dutch Wikivoyage, etc.</li>
<li>Works with other specialized wikis such as Wikidata, Wikimedia Commons, Wikispecies, or any other MediaWiki generated dump</li>
<li>Set up over 660+ other wikis including:
<ul><li>English Wiktionary</li>
<li>English Wikisource</li>
<li>English Wikiquote</li>
<li>English Wikivoyage</li>
<li>Non-English wikis, such as French Wiktionary, German Wikisource, Dutch Wikivoyage</li>
<li>Wikidata</li>
<li>Wikimedia Commons</li>
<li>Wikispecies</li>
<li>... and many more!</li></ul></li>
<li>Update your wiki whenever you want, using Wikimedia&#39;s database backups.</li>
<li>Navigate between offline wikis. Click on &#34;Look up this word in Wiktionary&#34; and instantly view the page in Wiktionary.</li>
<li>Edit articles to remove vandalism or errors.</li>
<li>Install to a flash memory card for portability to other machines.</li>
<li>Run on Windows, Linux and Mac OS X.</li>
<li>View the HTML for any wiki page.</li>
<li>Search for any page by title using a Wikipedia-like Search box.</li>
<li>Browse pages by alphabetical order using Special:AllPages.</li>
<li>Find a word on a page.</li>
<li>Access a history of viewed pages.</li>
<li>Bookmark your favorite pages.</li>
<li>Downloads images and other files on demand (when connected to the internet)</li>
<li>Sets up Simple Wikipedia in less than 5 minutes</li>
<li>Can be customized at many levels: from keyboard shortcuts to HTML layouts to internal options</li></ul>
<p><h4 id="Main_features" data-mw-thread-id="h-Main_features-XOWA"><span data-mw-comment-start="" id="h-Main_features-XOWA"></span>Main features<span data-mw-comment-end="h-Main_features-XOWA"></span></h4></p>
<ol><li>Very fast searching</li>
<li>Keyword (actually, title words) based searching</li>
<li>Search produces multiple possible articles: you can choose amongst them</li>
<li>LaTeX based rendering for mathematical formulae</li>
<li>Minimal space requirements: the original .bz2 file plus the index</li>
<li>Very fast installation (a matter of hours) compared to loading the dump into MySQL</li></ol>
<p><h3 id="WikiFilter" data-mw-thread-id="h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>WikiFilter<span data-mw-comment-end="h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a rel="nofollow" href="http://wikifilter.sourceforge.net/">WikiFilter</a> is a program which allows you to browse over 100 dump files without visiting a Wiki site.
</p>
<p><h4 id="WikiFilter_system_requirements" data-mw-thread-id="h-WikiFilter_system_requirements-WikiFilter"><span data-mw-comment-start="" id="h-WikiFilter_system_requirements-WikiFilter"></span>WikiFilter system requirements<span data-mw-comment-end="h-WikiFilter_system_requirements-WikiFilter"></span></h4></p>
<ul><li>A recent Windows version (Windows XP is fine; Windows 98 and ME won&#39;t work because they don&#39;t have NTFS support)</li>
<li>A fair bit of hard drive space (to install you will need about 12–15 Gigabytes; afterwards you will only need about 10 Gigabytes)</li></ul>
<p><h4 id="How_to_set_up_WikiFilter" data-mw-thread-id="h-How_to_set_up_WikiFilter-WikiFilter"><span data-mw-comment-start="" id="h-How_to_set_up_WikiFilter-WikiFilter"></span>How to set up WikiFilter<span data-mw-comment-end="h-How_to_set_up_WikiFilter-WikiFilter"></span></h4></p>
<ol><li>Start downloading a Wikipedia database dump file such as an <a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2">English Wikipedia dump</a>. It is best to use a download manager such as <a href="https://en.wikipedia.org/wiki/GetRight" title="GetRight">GetRight</a> so you can resume downloading the file even if your computer crashes or is shut down during the download.</li>
<li>Download XAMPPLITE from <a rel="nofollow" href="http://sourceforge.net/project/showfiles.php?group_id=61776&amp;package_id=89552">[2]</a> (you must get the 1.5.0 version for it to work). Make sure to pick the file whose filename ends with .exe</li>
<li>Install/extract it to C:\XAMPPLITE.</li>
<li>Download WikiFilter 2.3 from this site: <a rel="nofollow" href="http://sourceforge.net/projects/wikifilter">http://sourceforge.net/projects/wikifilter</a>. You will have a choice of files to download, so make sure that you pick the 2.3 version. Extract it to C:\WIKIFILTER.</li>
<li>Copy the WikiFilter.so into your C:\XAMPPLITE\apache\modules folder.</li>
<li>Edit your C:\xampplite\apache\conf\httpd.conf file, and add the following line:
<ul><li>LoadModule WikiFilter_module &#34;C:/XAMPPLITE/apache/modules/WikiFilter.so&#34;</li></ul></li>
<li>When your Wikipedia file has finished downloading, uncompress it into your C:\WIKIFILTER folder. (I used WinRAR <a rel="nofollow" href="https://www.rarlab.com/">http://www.rarlab.com/</a> demo version – BitZipper <a rel="nofollow" href="http://www.bitzipper.com/winrar.html">http://www.bitzipper.com/winrar.html</a> works well too.)</li>
<li>Run WikiFilter (WikiIndex.exe), and go to your C:\WIKIFILTER folder, and drag and drop the XML file into the window, click Load, then Start.</li>
<li>After it finishes, exit the window, and go to your C:\XAMPPLITE folder. Run the setup_xampp.bat file to configure xampp.</li>
<li>When you finish with that, run the Xampp-Control.exe file, and start Apache.</li>
<li>Browse to <a rel="nofollow" href="http://localhost/wiki">http://localhost/wiki</a> and see if it works
<ul><li>If it doesn&#39;t work, see the <a rel="nofollow" href="http://sourceforge.net/forum/forum.php?forum_id=495411">forums</a>.</li></ul></li></ol>
<p><h3 id="WikiTaxi_(for_Windows)" data-mw-thread-id="h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span id="WikiTaxi_.28for_Windows.29"></span><span data-mw-comment-start="" id="h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>WikiTaxi (for Windows)<span data-mw-comment-end="h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a rel="nofollow" href="https://www.yunqa.de/delphi/apps/wikitaxi/index">WikiTaxi</a> is an offline-reader for wikis in MediaWiki format. It enables users to search and browse popular wikis like Wikipedia, Wikiquote, or WikiNews, without being connected to the Internet. WikiTaxi works well with different languages like English, German, Turkish, and others but has a problem with right-to-left language scripts. WikiTaxi does not display images.
</p>
<p><h4 id="WikiTaxi_system_requirements" data-mw-thread-id="h-WikiTaxi_system_requirements-WikiTaxi_(for_Windows)"><span data-mw-comment-start="" id="h-WikiTaxi_system_requirements-WikiTaxi_(for_Windows)"></span>WikiTaxi system requirements<span data-mw-comment-end="h-WikiTaxi_system_requirements-WikiTaxi_(for_Windows)"></span></h4></p>
<ul><li>Any Windows version starting from Windows 95 or later. Large File support (greater than 4 GB which requires an exFAT filesystem) for the huge wikis (English only at the time of this writing).</li>
<li>It also works on Linux with <a href="https://en.wikipedia.org/wiki/Wine_(software)" title="Wine (software)">Wine</a>.</li>
<li>16 MB RAM minimum for the WikiTaxi reader, 128 MB recommended for the importer (more for speed).</li>
<li>Storage space for the WikiTaxi database. This requires about 11.7 GiB for the English Wikipedia (as of 5 April 2011), 2 GB for German, less for other Wikis. These figures are likely to grow in the future.</li></ul>
<p><h4 id="WikiTaxi_usage" data-mw-thread-id="h-WikiTaxi_usage-WikiTaxi_(for_Windows)"><span data-mw-comment-start="" id="h-WikiTaxi_usage-WikiTaxi_(for_Windows)"></span>WikiTaxi usage<span data-mw-comment-end="h-WikiTaxi_usage-WikiTaxi_(for_Windows)"></span></h4></p>
<ol><li>Download WikiTaxi and extract to an empty folder. No installation is otherwise required.</li>
<li>Download the XML database dump (*.xml.bz2) of your favorite wiki.</li>
<li>Run WikiTaxi_Importer.exe to import the database dump into a WikiTaxi database. The importer takes care to uncompress the dump as it imports, so make sure to save your drive space and do not uncompress beforehand.</li>
<li>When the import is finished, start up WikiTaxi.exe and open the generated database file. You can start searching, browsing, and reading immediately.</li>
<li>After a successful import, the XML dump file is no longer needed and can be deleted to reclaim disk space.</li>
<li>To update an offline Wiki for WikiTaxi, download and import a more recent database dump.</li></ol>
<p>For WikiTaxi reading, only two files are required: WikiTaxi.exe and the .taxi database. Copy them to any storage device (memory stick or memory card) or burn them to a CD or DVD and take your Wikipedia with you wherever you go!
</p>
<p><h3 id="BzReader_and_MzReader_(for_Windows)" data-mw-thread-id="h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span id="BzReader_and_MzReader_.28for_Windows.29"></span><span data-mw-comment-start="" id="h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>BzReader and MzReader (for Windows)<span data-mw-comment-end="h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a rel="nofollow" href="https://code.google.com/archive/p/bzreader/">BzReader</a> is an offline Wikipedia reader with fast search capabilities. It renders the Wiki text into HTML and doesn&#39;t need to decompress the database. Requires Microsoft .NET framework 2.0.
</p><p><a rel="nofollow" href="http://homepage.ntlworld.com/bharat.vadera/MzReader/">MzReader</a> by <a href="https://en.wikipedia.org/wiki/User_talk:Mun206" title="User talk:Mun206">Mun206</a> works with (though is not affiliated with) BzReader, and allows further rendering of wikicode into better HTML, including an interpretation of the monobook skin. It aims to make pages more readable. Requires Microsoft Visual Basic 6.0 Runtime, which is not supplied with the download. Also requires Inet Control and Internet Controls (Internet Explorer 6 ActiveX), which are packaged with the download.
</p>
<p><h3 id="EPWING" data-mw-thread-id="h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>EPWING<span data-mw-comment-end="h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p>Offline Wikipedia database in EPWING dictionary format, which is common and an out-dated <a href="https://en.wikipedia.org/wiki/Japanese_Industrial_Standards" title="Japanese Industrial Standards">Japanese Industrial Standards</a> (JIS) in Japan, can be read including thumbnail images and tables with some rendering limits, on any systems where a reader is available (<a rel="nofollow" href="https://sites.google.com/site/boookends">Boookends</a>). There are many free and commercial readers for Windows (including Mobile), Mac OS X, iOS (iPhone, iPad), Android, Unix-Linux-BSD, DOS, and Java-based browser applications (<a rel="nofollow" href="http://maximilk.web.fc2.com/viewers.htm">EPWING Viewers</a>).
</p>
<p><h2 id="Mirror_building" data-mw-thread-id="h-Mirror_building"><span data-mw-comment-start="" id="h-Mirror_building"></span>Mirror building<span data-mw-comment-end="h-Mirror_building"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Mirror_building","replies":["h-WP-MIRROR-Mirror_building"]}}--></p>
<p><h3 id="WP-MIRROR" data-mw-thread-id="h-WP-MIRROR-Mirror_building"><span data-mw-comment-start="" id="h-WP-MIRROR-Mirror_building"></span>WP-MIRROR<span data-mw-comment-end="h-WP-MIRROR-Mirror_building"></span></h3></p>
<dl><dd><i><b>Important:</b></i> <i>WP-mirror hasn&#39;t been supported since 2014, and community verification is needed that it actually works. <a href="https://en.wikipedia.org/wiki/Wikipedia_talk:Database_download#Does_WP-mirror_work?" title="Wikipedia talk:Database download">See talk page</a>.</i></dd></dl>
<p><a href="https://www.mediawiki.org/wiki/Wp-mirror" title="mw:Wp-mirror">WP-MIRROR</a> is a free utility for mirroring any desired set of WMF wikis. That is, it builds a wiki farm that the user can browse locally. WP-MIRROR builds a complete mirror with original size media files. WP-MIRROR is available for <a rel="nofollow" href="http://www.nongnu.org/wp-mirror/">download</a>.
</p>
<p><h2 id="See_also" data-mw-thread-id="h-See_also"><span data-mw-comment-start="" id="h-See_also"></span>See also<span data-mw-comment-end="h-See_also"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-See_also","replies":[]}}--></p>
<ul><li><a href="https://en.wikipedia.org/wiki/DBpedia" title="DBpedia">DBpedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/WikiReader" title="WikiReader">WikiReader</a></li>
<li><a href="https://www.mediawiki.org/wiki/Help:Export" title="mw:Help:Export">mw:Help:Export</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Help:Downloading_pages" title="m:Help:Downloading pages">m:Help:Downloading pages</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Help:Import" title="m:Help:Import">m:Help:Import</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Data_dumps/Other_tools" title="meta:Data dumps/Other tools">Meta:Data dumps/Other tools</a>, for related tools, e.g. extractors and &#34;dump readers&#34;</li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_CD_Selection" title="Wikipedia:Wikipedia CD Selection">Wikipedia:Wikipedia CD Selection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia" title="Wikipedia:Size of Wikipedia">Wikipedia:Size of Wikipedia</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Mirroring_Wikimedia_project_XML_dumps" title="meta:Mirroring Wikimedia project XML dumps">meta:Mirroring Wikimedia project XML dumps</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Static_version_tools" title="meta:Static version tools">meta:Static version tools</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Offline_Projects">Wikimedia offline projects</a></li></ul>
<p><h2 id="References" data-mw-thread-id="h-References"><span data-mw-comment-start="" id="h-References"></span>References<span data-mw-comment-end="h-References"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-References","replies":[]}}--></p>

<p><h2 id="External_links" data-mw-thread-id="h-External_links"><span data-mw-comment-start="" id="h-External_links"></span>External links<span data-mw-comment-end="h-External_links"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-External_links","replies":[]}}--></p>
<ul><li><a href="https://dumps.wikimedia.org/">Wikimedia downloads</a>.</li>
<li><a rel="nofollow" href="http://dammit.lt/wikistats/">Domas visits logs</a> (<a rel="nofollow" href="http://infodisiac.com/blog/2010/07/wikimedia-page-views-some-good-and-bad-news/">read this!</a>). Also, <a rel="nofollow" href="https://archive.org/details/wikipedia_visitor_stats_200712">old data</a> in <a href="https://en.wikipedia.org/wiki/Internet_Archive" title="Internet Archive">the Internet Archive</a>.</li>
<li><a href="https://meta.wikimedia.org/wiki/Mailing_lists/Overview" title="meta:Mailing lists/Overview">Wikimedia mailing lists</a> archives.</li>
<li><a href="https://en.wikipedia.org/wiki/User:Emijrp/Wikipedia_Archive" title="User:Emijrp/Wikipedia Archive">User:Emijrp/Wikipedia Archive</a>. An effort to find all the Wiki[mp]edia available data, and to encourage people to download it and save it around the globe.</li>
<li><a rel="nofollow" href="https://github.com/WikiTeam/wikiteam/blob/master/wikipediadownloader.py">Script to download all Wikipedia 7z dumps</a>.</li></ul>
<!-- 
NewPP limit report
Parsed by mw‐web.codfw.main‐767c977f85‐f7chn
Cached time: 20250419113710
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
DiscussionTools time usage: 0.033 seconds
CPU time usage: 0.354 seconds
Real time usage: 0.459 seconds
Preprocessor visited node count: 1674/1000000
Post‐expand include size: 25045/2097152 bytes
Template argument size: 933/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 27959/5000000 bytes
Lua time usage: 0.156/10.000 seconds
Lua memory usage: 4618696/52428800 bytes
Number of Wikibase entities loaded: 0/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  327.664      1 -total
 21.89%   71.741      1 Template:Reader_help
 21.72%   71.163      3 Template:Cite_web
 20.95%   68.644      1 Template:Sidebar
 15.11%   49.519      1 Template:Wikipedia_how-to
 11.87%   38.883      1 Template:Ombox
 11.01%   36.084      1 Template:Short_description
  7.89%   25.867      1 Template:Pagetype
  7.69%   25.212      1 Template:Cn
  7.26%   23.801      1 Template:Startflatlist
-->

<!-- Saved in parser cache with key enwiki:pcache:68321:|#|:idhash:canonical and timestamp 20250419113710 and revision id 1284893687. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> -->
</div></div>
  </body>
</html>
