<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/sahil280114/codealpaca">Original</a>
    <h1>CodeAlpaca â€“ Instruction following code generation model</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/b0cf55f6e9e9b09db260a68b5fce208098d724d440fb8b1c3ca8b3c7fb2d7cd7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d677265656e2e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache_2.0-green.svg"/></a>
<a href="https://www.python.org/downloads/release/python-390/" rel="nofollow"><img src="https://camo.githubusercontent.com/5d5155a412962d313548b087c622702b96e9f7564f137f91173a3a3aa8ed4a29/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e392b2d626c75652e737667" alt="Python 3.9+" data-canonical-src="https://img.shields.io/badge/python-3.9+-blue.svg"/></a>
<a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"/></a></p>
<p dir="auto">This is the repo for the Code Alpaca project, which aims to build and share an instruction-following LLaMA model for code generation. This repo is fully based on <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca</a> ,and only changes the data used for training. Training approach is the same.</p>
<p dir="auto">The repo contains:</p>
<ul dir="auto">
<li>The <a href="#data-release">20K data</a> used for fine-tuning the model</li>
<li>The code for <a href="#data-generation-process">generating the data</a></li>
<li>The code for <a href="#fine-tuning">fine-tuning the model</a></li>
</ul>
<p dir="auto">Demo for the model can be found <a href="https://code-alpaca-demo.vercel.app/" rel="nofollow">https://code-alpaca-demo.vercel.app/</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-overview" aria-hidden="true" href="#overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h2>
<p dir="auto">The Code Alpaca models are fine-tuned from a 7B and 13B LLaMA model on 20K instruction-following data generated by the techniques in the Self-Instruct [1] paper, with some modifications that we discuss in the next section.
Evals are still a todo.</p>
<p dir="auto">The model is not finetuned to be safe and harmless, so be cautious.</p>
<p dir="auto">Current release contains the data generation procedure, dataset, and training code. Model weights aren&#39;t part of the release for now, to respect OpenAI TOS and LLaMA license.</p>
<p dir="auto">[1]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. <a href="https://arxiv.org/abs/2212.10560" rel="nofollow">https://arxiv.org/abs/2212.10560</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-data-release" aria-hidden="true" href="#data-release"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Data Release</h2>
<p dir="auto"><a href="https://github.com/sahil280114/codealpaca/blob/master/data/code_alpaca_20k.json"><code>data/code_alpaca_20k.json</code></a> contains 20K instruction-following data used for fine-tuning the Code Alpaca model.
This JSON file is a list of dictionaries, each dictionary contains the following fields:</p>
<ul dir="auto">
<li><code>instruction</code>: <code>str</code>, describes the task the model should perform. Each of the 20K instructions is unique.</li>
<li><code>input</code>: <code>str</code>, optional context or input for the task. For example, when the instruction is &#34;Amend the following SQL query to select distinct elements&#34;, the input is the SQL query. Around 40% of the examples have an input.</li>
<li><code>output</code>: <code>str</code>, the answer to the instruction as generated by <code>text-davinci-003</code>.</li>
</ul>
<p dir="auto">We used the following prompts for fine-tuning the model:</p>
<ul dir="auto">
<li>for examples with a non-empty input field:</li>
</ul>
<div data-snippet-clipboard-copy-content="Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:"><pre><code>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
</code></pre></div>
<ul dir="auto">
<li>for examples with an empty input field:</li>
</ul>
<div data-snippet-clipboard-copy-content="Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:"><pre><code>Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
</code></pre></div>
<p dir="auto">During inference (eg for the web demo), we use the user instruction with an empty input field (second option).</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-data-generation-process" aria-hidden="true" href="#data-generation-process"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Data Generation Process</h2>
<details>
<summary> <strong> Running the code </strong> </summary>
<ol dir="auto">
<li>Set environment variables <code>OPENAI_API_KEY</code> to your OpenAI API key.</li>
<li>Install the dependencies with <code>pip install -r requirements.txt</code>.</li>
<li>Run <code>python -m generate_instruction generate_instruction_following_data</code> to generate the data.</li>
</ol>
</details>
Data generation pipeline had minor changes from [Stanford Alpaca](<a href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a>)
- Modified prompt to focus on code generation/editing/optimization tasks instead of general tasks.
- Modified seed tasks to only be related to code generation.
<p dir="auto">This produced an instruction-following dataset with 20K examples obtained at a much lower cost (less than $200). Also including a smaller 2k samples dataset which was used to derisk the approach and quality of the model.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-fine-tuning" aria-hidden="true" href="#fine-tuning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Fine-tuning</h2>
<p dir="auto">Finetuned the models using standard Hugging Face training code and deepspeed with the following hyperparameters:</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning rate</td>
<td>2e-5</td>
</tr>
<tr>
<td>Epochs</td>
<td>3</td>
</tr>
<tr>
<td>Max length</td>
<td>512</td>
</tr>
<tr>
<td>Weight decay</td>
<td>0</td>
</tr>
</tbody>
</table>
<p dir="auto">Given Hugging Face hasn&#39;t officially supported the LLaMA models, we fine-tuned LLaMA with Hugging Face&#39;s transformers library by installing it from a particular fork (i.e. this <a href="https://github.com/huggingface/transformers/pull/21955" data-hovercard-type="pull_request" data-hovercard-url="/huggingface/transformers/pull/21955/hovercard">PR</a> to be merged).
The hash of the specific commit we installed was <code>68d640f7c368bcaaaecfc678f11908ebbd3d6176</code>.</p>
<p dir="auto">The code runs on a 8xA100 80GB, but can also run on 8xA10040GB or 4xA100 with lower batch size and gradient accumulation steps. To get the GPUs, I suggest using <a href="https://cloud.lambdalabs.com/login?redirect_to=/instances?" rel="nofollow">Lambda Labs</a>, best pricing for the best hardware.</p>
<p dir="auto">To reproduce the fine-tuning runs for LLaMA, first install the requirements</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">Then, install the particular fork of Hugging Face&#39;s transformers library.</p>
<p dir="auto">Below is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP <code>full_shard</code> mode.
We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using <strong>Python 3.10</strong>.
Replace <code>&lt;your_random_port&gt;</code> with a port of your own, <code>&lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt;</code> with the
path to your converted checkpoint and tokenizer (following instructions in the PR), and <code>&lt;your_output_dir&gt;</code> with where you want to store your outputs.</p>
<div dir="auto" data-snippet-clipboard-copy-content="torchrun --nproc_per_node=8 --master_port=&lt;your_random_port&gt; train.py \
    --model_name_or_path &lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt;
    --data_path ./data/code_alpaca_20k.json \
    --fp16 True \
    --output_dir &lt;your_output_dir&gt; \
    --num_train_epochs 3 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --gradient_accumulation_steps 4 \
    --evaluation_strategy &#34;no&#34; \
    --save_strategy &#34;steps&#34; \
    --save_steps 500 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type &#34;cosine&#34; \
    --logging_steps 1 \
    --deepspeed ds_config.json
    --tf32 False"><pre>torchrun --nproc_per_node=8 --master_port=<span>&lt;</span>your_random_port<span>&gt;</span> train.py \
    --model_name_or_path <span>&lt;</span>your_path_to_hf_converted_llama_ckpt_and_tokenizer<span>&gt;</span>
    --data_path ./data/code_alpaca_20k.json \
    --fp16 True \
    --output_dir <span>&lt;</span>your_output_dir<span>&gt;</span> \
    --num_train_epochs 3 \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 8 \
    --gradient_accumulation_steps 4 \
    --evaluation_strategy <span><span>&#34;</span>no<span>&#34;</span></span> \
    --save_strategy <span><span>&#34;</span>steps<span>&#34;</span></span> \
    --save_steps 500 \
    --save_total_limit 1 \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type <span><span>&#34;</span>cosine<span>&#34;</span></span> \
    --logging_steps 1 \
    --deepspeed ds_config.json
    --tf32 False</pre></div>
<p dir="auto">Note the given training script is meant to be simple and easy to use, and is not particularly optimized.</p>
<p dir="auto">For convenience I have included the <a href="https://github.com/sahil280114/codealpaca/blob/master/convert_to_hf.py"><code>convert_to_hf.py</code></a> to covnert llama checkpoints to huggingface compatible checkpoints. (This file is taken from the hugginface transformers repo)</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-citation" aria-hidden="true" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h3>
<p dir="auto">Cite this repo if you want to, or don&#39;t, both are fine.</p>
<div data-snippet-clipboard-copy-content="@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}"><pre><code>@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}
</code></pre></div>
<p dir="auto">Naturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2] and the <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca repo</a>.</p>
</article>
          </div></div>
  </body>
</html>
