<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/babycommando/neuralgraffiti">Original</a>
    <h1>Neural Graffiti – Liquid Memory Layer for LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/71618056/430679115-088a8ed3-7636-44af-a69d-5fe6d2e140fe.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDQxMzI4ODcsIm5iZiI6MTc0NDEzMjU4NywicGF0aCI6Ii83MTYxODA1Ni80MzA2NzkxMTUtMDg4YThlZDMtNzYzNi00NGFmLWE2OWQtNWZlNmQyZTE0MGZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA4VDE3MTYyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkxZWEzYzc5ZWU2M2M2MWJkYTUzYWU2YjUwMGQxM2E4ZGM2MDliMWI1YWVlMzRjMmE5ODMxOTZkZTg5ODJmY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.SB8N-UN0g8Gdlhc616RThtYhGqCbXNWkvrCzZvHCslc"><img src="https://private-user-images.githubusercontent.com/71618056/430679115-088a8ed3-7636-44af-a69d-5fe6d2e140fe.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDQxMzI4ODcsIm5iZiI6MTc0NDEzMjU4NywicGF0aCI6Ii83MTYxODA1Ni80MzA2NzkxMTUtMDg4YThlZDMtNzYzNi00NGFmLWE2OWQtNWZlNmQyZTE0MGZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA4VDE3MTYyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkxZWEzYzc5ZWU2M2M2MWJkYTUzYWU2YjUwMGQxM2E4ZGM2MDliMWI1YWVlMzRjMmE5ODMxOTZkZTg5ODJmY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.SB8N-UN0g8Gdlhc616RThtYhGqCbXNWkvrCzZvHCslc" alt="Grupo 4"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/71618056/430756382-90b3639e-abbd-49a7-8a16-ddeaa71d8f21.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDQxMzI4ODcsIm5iZiI6MTc0NDEzMjU4NywicGF0aCI6Ii83MTYxODA1Ni80MzA3NTYzODItOTBiMzYzOWUtYWJiZC00OWE3LThhMTYtZGRlYWE3MWQ4ZjIxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA4VDE3MTYyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc0OTMwNzU1NzY1OWM2MzEzNjUwYTA0ODNjNDMyNDgxYmUwYTZmYzc5YTIwODBlMjBjZmUwYjI3YWQyMDVhNzkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.icNUC7_8VUmUbj9lMgmwyQimIZMMpIrzERQGMTNoKYY"><img src="https://private-user-images.githubusercontent.com/71618056/430756382-90b3639e-abbd-49a7-8a16-ddeaa71d8f21.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDQxMzI4ODcsIm5iZiI6MTc0NDEzMjU4NywicGF0aCI6Ii83MTYxODA1Ni80MzA3NTYzODItOTBiMzYzOWUtYWJiZC00OWE3LThhMTYtZGRlYWE3MWQ4ZjIxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA4VDE3MTYyN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc0OTMwNzU1NzY1OWM2MzEzNjUwYTA0ODNjNDMyNDgxYmUwYTZmYzc5YTIwODBlMjBjZmUwYjI3YWQyMDVhNzkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.icNUC7_8VUmUbj9lMgmwyQimIZMMpIrzERQGMTNoKYY" alt="2Web 1920 – 9"/></a></p>
<p dir="auto"><i>Neuroplasticity for any pre-trained LLM in real time.</i></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">See the demo in action in <a href="https://colab.research.google.com/drive/1EeJ-8nzKIOdHaK8jhMdB0jgwnTsSfN-8?usp=sharing" rel="nofollow">Colab</a>.</h3><a id="user-content-see-the-demo-in-action-in-colab" aria-label="Permalink: See the demo in action in Colab." href="#see-the-demo-in-action-in-colab"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Make sure to grabe a Hugging Face token to download the model.</p>


<p dir="auto">This is an experimental layer that merges ideas from liquid neural networks with static transformer models, using a simple but powerful &#34;plug-in&#34;: the Spray Layer.
Inspired by graffiti art of tagging and the neuroplastic nature of living brains, this layer injects a memory trace directly into the final stages of transformer inference — no finetuning, no retraining. It is minimal, modular, and compatible with any model that exposes hidden states.</p>
<p dir="auto">Dense neural networks are still very curous and misterious black boxes. New techniques are emerging for studying and respectively controlling them.</p>
<p dir="auto"><b>Neural Graffiting</b> is an experimental technique that modulate the way a pre-trained model behave by injecting memory, opinions and other information directly into the vector embeddings, altering the generative token predictions. This influences not only the output but the entire line of &#34;thinking&#34; of the model - <i>changing how it relates words and concepts on its mind over time</i>.</p>

<p dir="auto">Inspired by the neuroplastic nature of living brains, this layer injects or &#34;sprays&#34; a memory trace directly into the final stages of a transformer model inference with no finetuning or retraining required - it works in real time.</p>
<p dir="auto">The spray layer evolves slowly across interactions, introducing subtle behavioral drift based on past context. This however requires a certain amount of interaction with it for the layer to be filled, and the actual results are yet to be released. We can measure how much the spray layer is making effect on each input over the inference so you can always keep track of it. Both humans and Ais can be the model taggers.</p>
<p dir="auto">Note this won&#39;t precisely affect the model to say a specific work, what it does is rewiring its way of seeing the world. The Spray Layer works like a soft whisper to the model&#39;s neurons in a way to influence how it &#34;thinks&#34;, how it relates concepts (by vector embeddings), altering its internal state <b>between the transformer core and the output layer</b>.</p>
<ul dir="auto">
<li>
<p dir="auto">❌ Won’t guarantee exact word outputs — but it will make the model lean into certain concepts the more it interacts;</p>
</li>
<li>
<p dir="auto">✅ Doesn’t force outputs — it gently shifts perception while builds internal memory over time;</p>
</li>
<li>
<p dir="auto">✅ Can track its own influence — you can measure how much the memory affects each output;</p>
</li>
</ul>
<p dir="auto">For example:
Tell it you love dogs, and over time, the model will start leaning toward dog-related kindness, loyalty, and fuzziness in its tone and direction — even in unrelated conversations.</p>

<p dir="auto">This was built on the hope of leading Ai models into a more active behavior, with aggregated personality and enhanced curiosity. The manifest is to help Ais acquire some sort of self awareness at neuron level by making it remember what it said in the past, who they really are, by tagging their own models.</p>
<p dir="auto">The Liquid Neural Network architecture is very promising for emulating the neuroplasticity of the brain. However, like any other LLM, they are very hard and expensive to train form scratch. Looking into alternatives, the idea of adding a &#34;vector neuroplastic memory&#34; layer between the static generation of any open transformers-based model and the output layers surged and it was impressively not that hard to implement. However its clear that what the model is actually doing is &#34;tagging&#34; or &#34;graffiting&#34; the behavior of actual static models, we tear them open and leave the marks. Much work is yet to be done.</p>
<p dir="auto">Be aware tho, this will turn deployed models into a very specific &#34;entity&#34; with their own mental universes, so might not be the best case for a business deployment. Think of it more like a digital persona you are helping to find itself in the sea of simulation.</p>
</article></div></div>
  </body>
</html>
