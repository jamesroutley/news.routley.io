<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/SubscriberLink/905164/e1f4d4c1ce35f8b9/">Original</a>
    <h1>The container orchestrator landscape</h1>
    
    <div id="readability-page-1" class="page"><div>
<!-- $Id: slink-none,v 1.2 2005-11-04 22:11:18 corbet Exp $ -->
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://podviaznikov.com/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>
<div>
           <p>August 23, 2022</p>
           <p>This article was contributed by Jordan Webb</p>
           </div>
<p><a href="https://podviaznikov.com/Articles/902049/">Docker and other container
engines</a> can greatly simplify many aspects of deploying a server-side
application, but numerous applications consist of more than one container.
Managing a group of containers only gets harder as additional applications
and services are deployed; this has led to the development of a class of
tools called container orchestrators.  The best-known of these by far is <a href="https://kubernetes.io">Kubernetes</a>; the history of container
orchestration can 
be divided into what came before it and what came after. </p>

<p>The convenience offered by containers comes with some trade-offs;
someone who adheres strictly to Docker&#39;s idea that <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#decouple-applications">each
service should have its own container</a> will end up running a large
number of them.  Even a simple web interface to a database might require
running separate containers for the database server and the
application; it might also include a separate container for a web server to
handle serving static files, a proxy server to terminate SSL/TLS connections, a
key-value store to serve as a cache, or even a second application container
to handle background jobs and scheduled tasks.  </p>

<p>An administrator who is responsible for several such applications will
quickly find themselves wishing for a tool to make their job easier; this
is where container orchestrators step in.  A container orchestrator is a
tool that can manage a group of multiple containers as a single unit.
Instead of operating on a single server, orchestrators allow combining
multiple servers into a cluster, and automatically distribute container
workloads among the cluster nodes.   </p>

<h4>Docker Compose and Swarm</h4>

<p><a href="https://github.com/docker/compose">Docker Compose</a> is not
quite an orchestrator, but it was Docker&#39;s first attempt to create a tool
to make it easier to manage applications that are made out of several
containers.  It consumes a <a href="https://docs.docker.com/compose/compose-file/">YAML-formatted
file</a>, which 
is almost always named <tt>docker-compose.yml</tt>.  Compose reads this
file and uses the <a href="https://docs.docker.com/engine/api/">Docker
API</a> to create the resources that it declares; Compose also adds labels to
all of the resources, so that they can be managed as a group after they are
created.  In effect, it is an alternative to the <a href="https://docs.docker.com/engine/reference/commandline/cli/">Docker
command-line interface</a> (CLI) that operates on groups of containers.
Three types of resources 
can be defined in a Compose file: </p>

<ul>

<li> <tt>services</tt> contains declarations of containers to be
launched. Each entry in <tt>services</tt> is equivalent to a
<tt>docker run</tt> command.</li> 

<li> <tt>networks</tt> declares networks that can be attached to the
containers defined in the Compose file. Each entry in <tt>networks</tt> is
equivalent to a <tt>docker network create</tt> command.</li>

<li> <tt>volumes</tt> defines named volumes that can be attached to the
containers. In Docker parlance, a volume is persistent storage that is
mounted into the container. Named volumes are managed by the Docker
daemon. Each entry in <tt>volumes</tt> is equivalent to a
<tt>docker volume create</tt> command.</li>

</ul>

<p>Networks and volumes can be directly connected to networks and
filesystems on the host that Docker is running on, or they can be provided
by a <a href="https://docs.docker.com/engine/extend/legacy_plugins/">plugin</a>.
Network plugins allow things like connecting containers to VPNs; a
volume plugin might allow storing a volume on an NFS server or an
object storage service.  </p>

<p>Compose provides a much more convenient way to manage an application
that consists of multiple containers, but, at least in its original
incarnation, it only worked with a single host; all of the containers that
it created were run on the same machine.  To extend its reach across
multiple hosts, Docker introduced <a href="https://docs.docker.com/engine/swarm/">Swarm mode</a> in 2016.  This
is actually the second product from Docker to bear the name &#34;Swarm&#34; — a <a href="https://github.com/docker-archive/classicswarm">product from 2014</a>
implemented a <a href="https://dockerlabs.collabnix.com/intermediate/swarm/difference-between-docker-swarm-vs-swarm-mode-vs-swarmkit.html">completely
different approach</a> to running containers across multiple hosts, but it
is no longer maintained.  It was replaced by <a href="https://github.com/moby/swarmkit">SwarmKit</a>, which provides the
underpinnings of the current version of Docker Swarm.  </p>

<p>Swarm mode is included in Docker; no additional software is required.
Creating a cluster is a simple matter of running <a href="https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/"><tt>docker swarm init</tt></a>
on an initial node, and then <a href="https://docs.docker.com/engine/swarm/swarm-tutorial/add-nodes/"><tt>docker swarm join</tt></a>
on each additional node to be added.  Swarm clusters contain
two types of nodes.  Manager nodes provide an API to launch containers on
the cluster, and communicate with each other using a protocol based on the
<a href="https://raft.github.io/">Raft Consensus Algorithm</a> in order to
synchronize the state of the cluster across all managers.  Worker nodes do
the actual work of running containers.  It is unclear how large these
clusters can be; Docker&#39;s documentation says that a cluster should have <a href="https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes">no
more than 7 manager nodes</a> but does not specify a limit on the number of
worker nodes.  Bridging container networks across nodes is built-in, but
sharing storage between nodes is not; third-party volume plugins need to be
used to provide shared persistent storage across nodes.  </p>

<p>Services are deployed on a swarm using Compose files.  Swarm extended
the Compose format by adding a <a href="https://docs.docker.com/compose/compose-file/deploy/"><tt>deploy</tt></a>
key to each service that specifies how many instances of the service should
be running and which nodes they should run on.  Unfortunately, this led to
a divergence between Compose and Swarm, which caused <a href="https://stackoverflow.com/questions/43099408/whats-the-difference-between-a-stack-file-and-a-compose-file">some
confusion</a> because options like CPU and memory quotas needed to be
specified 
in different ways depending on which tool was being used.  During this
period of divergence, a file intended for Swarm was referred to as a &#34;stack
file&#34; instead of a Compose file in an attempt to disambiguate the two;
thankfully, these differences appear to have been smoothed over in the
current versions of Swarm and Compose, and any references to a stack file
being distinct from a Compose file seem to have largely been scoured from
the Internet.  The Compose format now has an <a href="https://compose-spec.io/">open specification</a> and its own <a href="https://github.com/compose-spec/">GitHub organization</a> providing
reference implementations.  </p>

<p>There is some level of uncertainty about the future of Swarm.  It
once formed the backbone of a service called Docker Cloud, but the
service was <a href="https://web.archive.org/web/20200611102535/http://success.docker.com/article/cloud-migration">suddenly
shut down in 2018</a>.  It was also touted as a key feature of Docker&#39;s
Enterprise Edition, but that product has since been <a href="https://www.mirantis.com/blog/mirantis-acquires-docker-enterprise-platform-business/">sold
to another company</a> and is now marketed as <a href="https://www.mirantis.com/software/mirantis-kubernetes-engine/">Mirantis
<em>Kubernetes</em> Engine</a>.  Meanwhile, recent versions of Compose have
gained the ability to deploy containers to services hosted by <a href="https://docs.docker.com/cloud/ecs-integration/">Amazon</a> and <a href="https://docs.docker.com/cloud/aci-integration/">Microsoft</a>.  There
has been no deprecation announcement, but there also hasn&#39;t been any
announcement of any other type in recent memory; <a href="https://www.docker.com/search/?_sf_s=swarm">searching for the word
&#34;Swarm&#34; on Docker&#39;s website</a> only turns up passing mentions.  </p>

<h4>Kubernetes</h4>

<p>Kubernetes (sometimes known as k8s) is a project inspired by an internal
Google tool called <a href="https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/">Borg</a>.
Kubernetes manages resources and coordinates running workloads on clusters
of up to thousands of nodes; it dominates container orchestration like
Google dominates search.  Google <a href="https://www.theinformation.com/articles/when-docker-said-no-to-google">wanted
to collaborate with Docker</a> on Kubernetes development in 2014, but Docker
decided to go its own way with Swarm.  Instead, Kubernetes grew up under
the auspices of the <a href="https://www.cncf.io/certification">Cloud
Native Computing Foundation</a> (CNCF).  By 2017, Kubernetes had grown so
popular that Docker announced that it would be <a href="https://web.archive.org/web/20190923110648/https://blog.docker.com/2017/10/kubernetes-docker-platform-and-moby-project/">integrated
into Docker&#39;s own product</a>.  </p>

<p>Aside from its popularity, Kubernetes is primarily known for its <a href="https://www.jeffgeerling.com/blog/2018/kubernetes-complexity">complexity</a>.
Setting up a new cluster by hand is an <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/">involved
task</a>, which requires the administrator to select and configure several
third-party components in addition to Kubernetes itself.  Much like the
Linux kernel needs to be combined with additional software
to make a complete operating system, Kubernetes is only an orchestrator and
needs to be combined with additional software to make a complete cluster.
It needs a
container engine to run its containers;  it also needs plugins for <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">networking</a>
and <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">persistent
volumes</a>.  </p>

<p><a href="https://containerjournal.com/topics/container-ecosystems/kubernetes-distribution-what-it-is-and-what-it-isnt/">Kubernetes
distributions</a> exist to fill this gap.  Like a Linux distribution, a
Kubernetes distribution bundles Kubernetes with an installer and a
curated selection of third-party components.  Different distributions
exist to fill different niches; seemingly every tech company of a certain
size has its own distribution and/or hosted offering to cater to
enterprises.  The <a href="https://minikube.sigs.k8s.io/docs/start/">minikube</a> project offers
an easier on-ramp for developers looking for a local environment to
experiment with.  Unlike their Linux counterparts, Kubernetes distributions are
<a href="https://www.cncf.io/certification/software-conformance/">certified
for conformance</a> by the CNCF; each distribution must implement the same
baseline of functionality in order to obtain the certification, which
allows them to use
the &#34;Certified Kubernetes&#34; badge.  </p>

<p>A Kubernetes cluster contains several software <a href="https://kubernetes.io/docs/concepts/overview/components/">components</a>.
Every node in the cluster runs an agent called the <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
to maintain membership in the cluster and accept work from it, a container
engine, and <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a>
to enable network communication with containers running on other nodes.


</p><p> The components that maintain the state of the cluster and make
decisions about resource allocations are collectively referred to as the
control plane — these include a distributed key-value store called <a href="https://etcd.io/">etcd</a>, a scheduler that assigns work to cluster
nodes, and one or more controller processes that react to changes in the
state of the cluster and trigger any actions needed to make the actual
state match the desired state.  Users and cluster nodes interact with the
control plane through the Kubernetes <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">API
server</a>.  To effect changes, users set the desired state of the cluster
through the API server, while the kubelet reports the actual state of each
cluster node to the controller processes.  </p>

<p>Kubernetes runs containers inside an abstraction called a <a href="https://kubernetes.io/docs/concepts/workloads/pods/">pod</a>, which
can contain one or more containers, although running containers for more
than one service in a pod is discouraged.  Instead, a pod will generally
have a single main container that provides a service, and possibly one or
more &#34;sidecar&#34; containers that collect metrics or logs from the service
running in the main container.  All of the containers in a pod will be
scheduled together on the same machine, and will share a <a href="https://podviaznikov.com/Articles/580893/">network namespace</a> — containers
running within the same pod can communicate with each other over the
loopback interface.  Each pod receives its own unique IP address within
the cluster.  Containers running in different pods can communicate with
each other using their cluster IP addresses.  </p>

<p>A pod specifies a set of containers to run, but the definition of a pod
says nothing about where to run those containers, or how long to run them
for — without this information, Kubernetes will start the containers
somewhere on the cluster, but will not restart them when they exit, and may
abruptly terminate them if the control plane decides the resources they are
using are needed by another workload.  For this reason, pods are rarely
used alone; instead, the definition of a pod is usually wrapped in a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>
object, which is
used to define a persistent service.  Like Compose
and Swarm, the objects managed by Kubernetes are declared in YAML; for
Kubernetes, the YAML declarations are submitted to the cluster 
using the <a href="https://kubernetes.io/docs/reference/kubectl/"><tt>kubectl</tt></a>
tool.  </p>

<p>In addition to pods and
Deployments, Kubernetes can <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#-strong-api-groups-strong-">manage
many other types of objects</a>, like load balancers and authorization
policies.  The list of supported APIs is continually evolving, and will vary
depending on which version of Kubernetes and which distribution a cluster is
running.  <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom
resources</a>
can be used to add APIs to a cluster to manage additional types of objects.
<a href="https://kubevirt.io/">KubeVirt</a> adds APIs to enable Kubernetes
to run virtual machines, for example.  The complete list of APIs supported by a
particular cluster can be discovered with the <a href="https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_api-versions/"><tt>kubectl api-versions</tt></a> command.  </p>

<p>Unlike Compose, each of these objects is declared in a separate YAML
document, although multiple YAML documents can be inlined in the same file
by separating them with &#34;<tt>---</tt>&#34;, as seen in the <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#organizing-resource-configurations">Kubernetes
documentation</a>.  A complex application might consist of many objects
with their definitions spread across multiple files; keeping all of these
definitions in sync with each other when maintaining such an application
can be quite a chore.  In order to make this easier, some Kubernetes
administrators have turned to templating tools like <a href="https://jsonnet.org/articles/kubernetes.html">Jsonnet</a>.  </p>

<p><a href="https://helm.sh/">Helm</a> takes the templating approach a step
further.  Like Kubernetes, development of Helm takes place under the aegis
of the CNCF; it is billed as &#34;the package manager for Kubernetes&#34;.  Helm
generates YAML configurations for Kubernetes from a collection of
templates and variable declarations called a <a href="https://helm.sh/docs/topics/charts/">chart</a>.  Its <a href="https://helm.sh/docs/chart_template_guide/#the-chart-template-developer-s-guide">template
language</a> is distinct from the <a href="https://jinja.palletsprojects.com/en/3.1.x/">Jinja</a> templates used
by <a href="https://www.ansible.com/">Ansible</a> but looks fairly similar
to them; people who are familiar with <a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html">Ansible
Roles</a> will likely feel at home with Helm Charts.  
</p>

<p> Collections of Helm charts can be published in <a href="https://helm.sh/docs/topics/chart_repository/">Helm repositories</a>;
<a href="https://artifacthub.io/">Artifact Hub</a> provides a large directory of
public Helm repositories.  Administrators can add these repositories to
their Helm configuration and use the ready-made Helm charts to deploy
prepackaged versions of popular applications to their cluster.  Recent
versions of Helm also support pushing and pulling charts to and from
<a href="https://helm.sh/docs/topics/registries/">container registries</a>,
giving 
administrators the option to store charts in the same place that they store
container images.  </p>

<p>Kubernetes shows no signs of losing momentum any time soon.  It is
designed to manage any type of resource; this flexibility, as demonstrated
by the KubeVirt virtual-machine controller,  gives it the
potential to remain relevant even if containerized workloads should
eventually fall out of favor.  Development proceeds at a healthy clip and
new <a href="https://kubernetes.io/releases/">major releases come out
regularly</a>.  Releases are supported for a year; there doesn&#39;t seem to be
a long-term support version available.  <a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/">Upgrading
a cluster</a> is supported, but some prefer to <a href="https://aws.amazon.com/blogs/containers/kubernetes-cluster-upgrade-the-blue-green-deployment-strategy/">bring
up a new cluster</a> and migrate their services over to it.  </p>

<h4>Nomad</h4>

<p><a href="https://www.hashicorp.com/products/nomad">Nomad</a> is an
orchestrator from <a href="https://www.hashicorp.com/">HashiCorp</a>, which
is marketed as a simpler alternative to Kubernetes.  Nomad is an <a href="https://github.com/hashicorp/nomad">open source
project</a>, like Docker and Kubernetes. It consists of a single
binary called <tt>nomad</tt>, which can be used to start a daemon
called the agent and also serves as a <a href="https://www.nomadproject.io/docs/commands">CLI</a> to communicate
with an agent.  Depending on how it is configured, the agent process can
run in one of two modes.  Agents running in server mode accept jobs and
allocate cluster resources for them.  Agents running in client mode contact
the servers to receive jobs, run them, and report their status back to the
servers.  The agent can also run in development mode, where it takes on the
role of both client and server to form a single-node cluster that can be
used for testing purposes.  </p>

<p><a href="https://learn.hashicorp.com/tutorials/nomad/clustering">Creating a
Nomad cluster</a> can be quite simple.  In Nomad&#39;s most basic mode of operation,
the initial server agent must be started, then additional nodes can be added to
the cluster using the <a href="https://www.nomadproject.io/docs/commands/server/join"><tt>nomad server join</tt></a>
command.  HashiCorp also provides <a href="https://www.consul.io/">Consul</a>, which is a general-purpose
service mesh and discovery tool.  While it can be used standalone, Nomad is
probably at its best when used in combination with Consul. The Nomad agent
can use Consul to automatically discover and join a cluster, and
can also perform health checks, serve DNS records, and provide HTTPS
proxies to services running on the cluster.  </p>

<p>Nomad supports complex cluster topologies.  Each cluster is divided into
one or more &#34;data centers&#34;.  Like Swarm, server agents within a single
data center communicate with each other using a <a href="https://www.nomadproject.io/docs/concepts/consensus">protocol</a>
based on Raft; this protocol has <a href="https://www.nomadproject.io/docs/install/production/requirements#network-topology">tight
latency requirements</a>, but multiple data centers may be <a href="https://www.nomadproject.io/docs/concepts/architecture#high-level-overview">linked
together</a> using a <a href="https://www.nomadproject.io/docs/concepts/gossip">gossip protocol</a>
that allows information to propagate through the cluster without each
server having to maintain a direct connection to every other.
Data centers linked together in this way can act as one cluster from a
user&#39;s perspective.  This architecture gives Nomad an advantage when scaled
up to enormous clusters.  Kubernetes officially supports <a href="https://kubernetes.io/docs/setup/best-practices/cluster-large/">up to
5,000 nodes and 300,000 containers</a>, whereas Nomad&#39;s documentation cites
example of clusters containing <a href="https://www.nomadproject.io/docs/nomad-vs-kubernetes#scalability">over
10,000 nodes</a> and <a href="https://www.hashicorp.com/c2m">2,000,000
containers</a>.  </p>

<p>Like Kubernetes, Nomad doesn&#39;t include a container engine or runtime.
It uses <a href="https://www.nomadproject.io/docs/drivers">task drivers</a>
to run jobs.  Task drivers that use <a href="https://www.nomadproject.io/docs/drivers/docker">Docker</a> and <a href="https://www.nomadproject.io/plugins/drivers/podman">Podman</a> to run
containers are included; community-supported drivers are available for
other container engines.  Also like Kubernetes, Nomad&#39;s ambitions are not
limited to containers; there are also task drivers for other types of
workloads, including a <a href="https://www.nomadproject.io/docs/drivers/raw_exec">fork/exec
driver</a> that 
simply runs a command on the host, a <a href="https://www.nomadproject.io/docs/drivers/qemu">QEMU driver</a> for
running virtual machines, and a <a href="https://www.nomadproject.io/docs/drivers/java">Java driver</a> for
launching Java applications.  <a href="https://www.nomadproject.io/plugins/drivers/community">Community-supported
task drivers</a> connect Nomad to other types of workloads.  </p>

<p>Unlike Docker or Kubernetes, Nomad eschews YAML in favor of <a href="https://github.com/hashicorp/hcl">HashiCorp Configuration
Language</a> (HCL), which was originally created for another HashiCorp
project for provisioning cloud resources called <a href="https://www.terraform.io/">Terraform</a>. HCL is used across the
HashiCorp 
product line, although it has limited
adoption elsewhere.  Documents written in HCL can easily be converted
to JSON, but it aims to provide a syntax that is more finger-friendly than
JSON and less <a href="https://noyaml.com/">error-prone</a> than YAML.
</p>

<p>HashiCorp&#39;s equivalent to Helm is called <a href="https://learn.hashicorp.com/tutorials/nomad/nomad-pack-intro?in=nomad%2Fnomad-pack">Nomad
Pack</a>.  Like Helm, Nomad Pack processes a directory full of templates
and variable declarations to generate job configurations.  Nomad also has a
<a href="https://github.com/hashicorp/nomad-pack-community-registry">community
registry</a> of pre-packaged applications, but the selection is <a href="https://github.com/hashicorp/nomad-pack-community-registry/tree/main/packs">much
smaller</a> than what is available for Helm at Artifact Hub.  </p>

<p>Nomad does not have the same level of popularity as Kubernetes.  Like
Swarm, its development appears to be primarily driven by its creators;
although it has been deployed by <a href="https://www.nomadproject.io/docs/who-uses-nomad">many large
companies</a>, HashiCorp is still very much the center of the community
around Nomad.  At this point, it seems unlikely the project has gained
enough momentum to have a life independent from its corporate parent.
Users can perhaps find assurance in the fact that HashiCorp is much more
clearly committed to the development and promotion of Nomad than Docker is
to Swarm.  </p>

<h4>Conclusion</h4>

<p>Swarm, Kubernetes, and Nomad are not the only container orchestrators,
but they are the three most viable.  <a href="https://mesos.apache.org/">Apache Mesos</a> can also be used to run
containers, but it was <a href="https://lists.apache.org/thread/ysvw7bb1rd8p88fk32okkzr75mscdjo8">nearly
mothballed</a> in 2021; <a href="https://dcos.io/">DC/OS</a> is based on
Mesos, but much like Docker Enterprise Edition, the company that backed its
development is now <a href="https://d2iq.com/">focused on Kubernetes</a>.
Most &#34;other&#34; container orchestration projects, like <a href="https://www.redhat.com/en/technologies/cloud-computing/openshift">OpenShift</a>
and <a href="https://rancher.com/">Rancher</a>, are actually just enhanced
(and certified) Kubernetes distributions, even if they don&#39;t have
Kubernetes in their name. 
</p>

<p> Despite (or perhaps, <a href="https://www.appvia.io/blog/why-is-kubernetes-so-complicated">because
of</a>) its 
complexity, Kubernetes currently enjoys the most popularity by far, but
HashiCorp&#39;s successes with Nomad show that there is still room for
alternatives.  Some users remain loyal to the simplicity of Docker Swarm,
but its 
future is uncertain.  Other alternatives appear to be largely abandoned at
this point.
It would seem that the landscape has largely settled around these three
players, but container orchestration is a still a
relatively immature area.  Ten years ago, very little of this technology even
existed, and things are still evolving quickly.  There are likely many
exciting new ideas and developments in container orchestration that are
still to come. 
</p>


<p>[Special thanks to Guinevere Saenger for educating me with regard to
some of the finer points of Kubernetes and providing some important
corrections for this article.]</p></div></div>
  </body>
</html>
