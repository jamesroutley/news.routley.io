<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://scrollprize.org/overview">Original</a>
    <h1>The Vesuvius Challenge</h1>
    
    <div id="readability-page-1" class="page"><div><main><div><div><div><div><article><div><header></header><p>The objective of the Vesuvius Challenge is to make history by reading an unopened Herculaneum scroll for the very first time. We believe that an open competition will accelerate progress and enable us to achieve this goal in 2023.</p><p>We are providing all contestants with the following resources:</p><ul><li><strong><a href="https://scrollprize.org/overview#grand-prize-150000">Prizes</a></strong>: $1,000,000+ in prizes, including a $700,000 Grand Prize and the rest in <a href="https://scrollprize.org/overview#progress-prizes-100000">Progress Prizes</a>. The first progress prize is a $100,000 ink detection competition on <a href="https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection" target="_blank" rel="noopener noreferrer">Kaggle</a>. We also have some smaller <a href="https://scrollprize.org/overview#open-source-prizes-3x-2000">Open Source Prizes</a>.</li><li><strong><a href="https://scrollprize.org/data">Data</a></strong>: 3D X-ray scans of two unopened scrolls from Herculaneum, and scans and images of three papyrus fragments</li><li><strong><a href="https://scrollprize.org/tutorial1">Tutorials and tools</a></strong>: the current best tools and techniques for virtually unwrapping papyrus scrolls</li><li><strong><a href="https://scrollprize.org/participate">Community</a></strong>: a Discord server where you can connect with other contestants and the Vesuvius Challenge team</li></ul><ul><li>Join our <a href="https://discord.gg/6FgWYNjb4N" target="_blank" rel="noopener noreferrer">Discord server</a> to ask questions, learn about what is going on, hang out with fellow contestants and interested onlookers, and perhaps find teammates.</li><li>Receive updates every 1-2 weeks from our <a href="https://scrollprize.substack.com" target="_blank" rel="noopener noreferrer">mailing list on Substack</a>.</li><li>Follow <a href="https://twitter.com/scrollprize" target="_blank" rel="noopener noreferrer">@scrollprize</a> on Twitter.</li></ul><p>Our team is available for any questions and feedback on Discord. You can also email us at <a href="https://scrollprize.org/cdn-cgi/l/email-protection#a9ddccc8c4e9dacadbc6c5c5d9dbc0d3cc87c6dbce" target="_blank" rel="noopener noreferrer"><span data-cfemail="f08495919db08393829f9c9c8082998a95de9f8297">[email protected]</span></a>.</p><h3 id="grand-prize-700000">Grand Prize ($700,000)<a href="#grand-prize-700000" title="Direct link to heading">​</a></h3><p>The Grand Prize will go to the first team to read four passages of text from the inside of the two intact scrolls. More details on the qualifying criteria are available <a href="https://scrollprize.org/participate">here</a>.</p><p>Here are the scrolls in question:</p><div><div><p><img loading="lazy" src="https://scrollprize.org/img/overview/scroll1-small.jpg"/></p><figcaption>Scroll 1</figcaption></div><div><p><img loading="lazy" src="https://scrollprize.org/img/overview/scroll2-small.jpg"/></p><figcaption>Scroll 2</figcaption></div></div><p>We have provided you with 8µm 3D X-ray scans of each of these scrolls, which you can find <a href="https://scrollprize.org/data">here</a>. Your job is to extract the text from these scans.</p><p>You can approach this challenge through any means necessary: machine learning, computer vision, or machine-assisted tools operated by humans.</p><figure><video autoplay="" playsinline="" loop="" muted="" poster="/img/overview/scroll-inside-animation-4.jpg"><source src="/img/overview/scroll-inside-animation-4.webm" type="video/webm"/><source src="/img/overview/scroll-inside-animation-4.mp4" type="video/mp4"/></video><figcaption>3D X-ray scan of a scroll <a href="https://www.youtube.com/watch?v=PpNq2cFotyY" target="_blank" rel="noopener noreferrer">(source)</a></figcaption></figure><h4 id="why-is-this-difficult">Why is this difficult?<a href="#why-is-this-difficult" title="Direct link to heading">​</a></h4><p>As you will read in the <a href="https://scrollprize.org/tutorial1">tutorials</a>, advanced tools and techniques exist for virtually unwrapping papyrus scrolls. This was demonstrated in 2015 when Dr. Seales&#39;s team <a href="https://www2.cs.uky.edu/dri/the-scroll-from-en-gedi/" target="_blank" rel="noopener noreferrer">unwrapped the En-Gedi scroll</a>, and in their recent result <a href="https://raw.githubusercontent.com/educelab/EduceLab-Scrolls/main/paper/EduceLab-Scrolls.pdf" target="_blank" rel="noopener noreferrer">identifying ink from 3D X-ray scans in the Herculaneum scrolls</a>.</p><p>But the Herculaneum scrolls have proved more challenging. The remaining challenges include:</p><ul><li>Segmenting the scrolls. The Herculaneum scrolls are especially long, tightly wrapped, damaged, and distorted. To date, no one has successfully done a large-scale segmentation of these scrolls to identify the surfaces of all the rolled layers.</li><li>Finding the ink. The ink used in the Herculaneum scrolls is <a href="https://en.wikipedia.org/wiki/Radiodensity" target="_blank" rel="noopener noreferrer">radiolucent</a>, making it difficult to see in the scans. Recently, Dr. Seales&#39;s team has trained a machine learning model which can detect the ink from subtle patterns in the 3D X-rays. This works in the fragments, but these models are not yet perfect and will probably need to be improved to work at the scale of an entire scroll.</li><li>Putting it all together. Applying the ink detection models to the segmented scroll has not yet been successfully demonstrated.</li></ul><p>Based on the <a href="https://raw.githubusercontent.com/educelab/EduceLab-Scrolls/main/paper/EduceLab-Scrolls.pdf" target="_blank" rel="noopener noreferrer">landmark results</a> that Dr. Seales and his team have recently produced, we believe that it is possible to read the Herculaneum scrolls using the <a href="https://scrollprize.org/data">scans</a> that we already have and the tools and tecniques that they have developed. And that is the Vesuvius Challenge!</p><p>The Grand Prize deadline is 11:59pm Pacific, December 31st, 2023.</p><h2 id="progress-prizes">Progress prizes<a href="#progress-prizes" title="Direct link to heading">​</a></h2><p>The purpose of progress prizes is to encourage contestants to solve important subproblems and release their work. We also hope that they attract more participants who are interested in starting with a narrower task, and then go on to try to win the Grand Prize.</p><p>Progress prizes are optional! You can choose to skip them and work directly on the Grand Prize.</p><h3 id="ink-detection-on-kaggle-100000-total">Ink Detection on Kaggle ($100,000 total)<a href="#ink-detection-on-kaggle-100000-total" title="Direct link to heading">​</a></h3><p>As you will learn in the <a href="https://scrollprize.org/tutorial1">tutorials</a>, one of the big challenges of reading the Herculaneum Papyri is detecting the ink in the 3D X-ray scans.</p><figure><video autoplay="" playsinline="" loop="" muted="" poster="/img/tutorials/ink-detection-anim2-dark.jpg"><source src="/img/tutorials/ink-detection-anim2-dark.webm" type="video/webm"/><source src="/img/tutorials/ink-detection-anim2-dark.mp4" type="video/mp4"/></video></figure><p>For this important subproblem, we are offering a $100,000 <a href="https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/" target="_blank" rel="noopener noreferrer">Ink Detection Progress Prize</a>. This progress prize is hosted on Kaggle, and the final submission deadline is June 14th, 2023.</p><h3 id="open-source-prizes-3x-2000">Open Source Prizes (3x $2,000)<a href="#open-source-prizes-3x-2000" title="Direct link to heading">​</a></h3><p>All of us organizing the Vesuvius Challenge are big believers in open source and  incremental progress. We want to encourage building in the open and benefiting the whole community — something that is typically disincentivized in a competition format.</p><p>That’s why we are introducing “open source prizes”. These are small prizes that may be awarded for any open source or publicly available contributions: software, documentation, research notes, Kaggle notebooks, anything goes.</p><ul><li><strong>Anything that you release in public</strong> can qualify for an open source prize. You need to publicly release your work before you can win an open source prize.</li><li>It has to be accessible and usable by other contestants (e.g. licensed under MIT, Apache, GPL, Creative Commons, etc).</li><li>You may make multiple submissions. Please edit the form when doing so. We may award a single prize to a set of multiple submissions by one person or team.</li><li>Your submission will be judged subjectively: the Technical Team (Dr. Brent Seales, Nat Friedman, Stephen Parsons, Seth Parker, JP Posma, and Daniel Havíř) will decide what they think are the best open source contributions.</li><li>The first batch of prizes will be <strong>3 prizes of $2,000 each</strong> (plus a fun surprise!), which you must submit for by <strong>April 11th 11:59pm PT</strong>.</li><li>If you are submitting as a team, the team leader should make the submission, and is responsible for distributing the prize money (and fun surprise) among the team.</li><li>Make your submissions <a href="https://forms.gle/4qiDBW4uxKET87oh8" target="_blank" rel="noopener noreferrer">here</a>.</li></ul><h3 id="future-progress-prizes">Future Progress Prizes<a href="#future-progress-prizes" title="Direct link to heading">​</a></h3><p>We have some ideas of what future progress prizes might best accelerate overall progress on the Vesuvius Challenge, but we’re also keeping an open mind. If you have suggestions, please let us know!</p></div></article></div></div></div></div></main></div></div>
  </body>
</html>
