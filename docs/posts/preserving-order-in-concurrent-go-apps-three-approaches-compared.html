<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://destel.dev/blog/preserving-order-in-concurrent-go">Original</a>
    <h1>Preserving Order in Concurrent Go Apps: Three Approaches Compared</h1>
    
    <div id="readability-page-1" class="page"><div> <p>Concurrency is one of Go’s greatest strengths, but it comes with a fundamental trade-off: when multiple goroutines process data simultaneously, the natural ordering gets scrambled. Most of the time, this is fine – unordered processing is enough, it’s faster and simpler.</p>
<p>But sometimes, order matters.</p>
<h2 id="when-order-matters">When Order Matters</h2>
<p>Here are three real-world scenarios where preserving order becomes critical:</p>
<p><strong>Real-time Log Enrichment</strong>: You’re processing a high-volume log stream, enriching each entry with user metadata from a database or external API. Sequential processing can’t keep up with the incoming rate, but concurrent processing breaks the sequence, making the enriched logs unusable for downstream consumers that depend on chronological order.</p>
<p><strong>Finding the First Match in a File List</strong>: You need to download a list of files from cloud storage and find the first one containing a specific string. Concurrent downloads are much faster, but they complete out of order – the 50th file might finish before the 5th file, so you can’t simply return the first match you find without knowing if an earlier file also contains the string.</p>
<p><strong>Time Series Data Processing</strong>: This scenario inspired my original implementation. I needed to download 90 days of transaction logs (~600MB each), extract some data, then compare consecutive days for trend analysis. Sequential downloads took hours; concurrent downloads could give an order of magnitude speedup, but would destroy the temporal relationships I needed for comparison.</p>
<p>The challenge is clear: we need the speed benefits of concurrent processing without sacrificing the predictability of ordered results. This isn’t just a theoretical problem – it’s a practical constraint that affects real systems at scale.</p>
<p>In this article, we’ll explore three approaches I’ve developed and used in production Go applications. We’ll build a concurrent <code>OrderedMap</code> function that transforms a channel of inputs into a channel of outputs while preserving order, and supporting potentially <strong>infinite streams</strong> with backpressure. Through benchmarks of each approach, we’ll understand their trade-offs and discover surprising performance insights along the way.</p>
<h2 id="the-problem-why-concurrency-breaks-order">The Problem: Why Concurrency Breaks Order</h2>
<p><img src="https://andrewkelley.me/_astro/ordering-meme.BbptM_mi_ZqCeTn.webp" alt="Why are my results out of order? Concurrency." width="1200" height="630" loading="lazy" decoding="async"/></p>
<p>Let’s quickly recall why concurrency messes up ordering. One of the reasons is that goroutines process tasks at different speeds. Another common reason – we can’t predict how exactly goroutines will be scheduled by the Go runtime.</p>
<p>For example, goroutine #2 might finish processing item #50 before goroutine #1 finishes item #10, causing results to arrive out of order. This is the natural behavior of concurrent processing.</p>
<p>If you want to see this in action, here’s a quick <a href="https://goplay.tools/snippet/hG1xdZvT9FX" rel="nofollow" target="_blank">demo</a> the Go playground.</p>
<h2 id="design-philosophy-backpressure-vs-buffering">Design Philosophy: Backpressure vs Buffering</h2>
<p>The classic approach to ordered concurrency uses some sort of reorder buffer or queue. When a worker calculates a result but it’s too early to write it to the output, the result gets stored in that buffer until it can be written in the correct order.</p>
<p>In such designs buffers can typically grow without bound. This happens when:</p>
<ul>
<li>The input is skewed – early items take longer to process than later items</li>
<li>Downstream consumers are slow</li>
</ul>
<p>Another common approach is to accumulate all results in memory (slice/map/etc) then sort them. But we’re building a streaming design here. We want to:</p>
<ul>
<li><strong>Emit results as soon as they’re ready</strong> – The only valid reason to delay emission is to preserve the order</li>
<li><strong>Handle infinite input streams</strong> – Support arbitrarily large or even infinite inputs (think reading from stdin or network streams)</li>
<li><strong>Memory-bounded processing</strong> – Avoid accumulating results in memory unless it’s necessary to preserve the order</li>
</ul>
<p>Having said that, the algorithms presented below are backpressure-first. If a worker can’t yet write its result to the output channel, it blocks. This design is memory-bound and preserves the behavior developers expect from Go channels.</p>
<blockquote>
<p>Technically speaking, such algorithms also do buffering, but here out-of-order items are held on the stacks of running goroutines. So, to get a larger “buffer” in these algorithms, you can simply increase the concurrency level. This works well in practice since typically when applications need larger buffers they also need higher concurrency levels.</p>
</blockquote>
<h2 id="establishing-a-performance-baseline">Establishing a Performance Baseline</h2>
<p>To understand the true cost of ordering, we first need a baseline to measure against.
Let’s implement and benchmark a basic concurrent <code>Map</code> function that doesn’t preserve order – this will show us exactly what overhead the ordering approaches add.</p>
<p>Our <code>Map</code> function transforms an input channel into an output channel using a user-supplied function <code>f</code>. It’s built on top of a simple worker pool, which spawns multiple goroutines to process input items concurrently.</p>
<pre tabindex="0" data-language="go"><code><span><span>// Map transforms items from the input channel using n goroutines, and the</span></span>
<span><span>// provided function f. Returns a new channel with transformed items.</span></span>
<span><span>func</span><span> Map</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>f</span><span> func</span><span>(</span><span>A</span><span>) </span><span>B</span><span>) </span><span>&lt;-chan</span><span> B</span><span> {</span></span>
<span><span>	out </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> B</span><span>)</span></span>
<span><span>	Loop</span><span>(in, n, out, </span><span>func</span><span>(</span><span>a</span><span> A</span><span>) {</span></span>
<span><span>		out </span><span>&lt;-</span><span> f</span><span>(a)</span></span>
<span><span>	})</span></span>
<span><span>	return</span><span> out</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Loop is a worker pool implementation. It calls function f for each </span></span>
<span><span>// item from the input channel using n goroutines. This is a non-blocking function </span></span>
<span><span>// that signals completion by closing the done channel when all work is finished.</span></span>
<span><span>func</span><span> Loop</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>done</span><span> chan&lt;-</span><span> B</span><span>, </span><span>f</span><span> func</span><span>(</span><span>A</span><span>)) {</span></span>
<span><span>	var</span><span> wg </span><span>sync</span><span>.</span><span>WaitGroup</span></span>
<span></span>
<span><span>	for</span><span> i </span><span>:=</span><span> 0</span><span>; i </span><span>&lt;</span><span> n; i</span><span>++</span><span> {</span></span>
<span><span>		wg.</span><span>Add</span><span>(</span><span>1</span><span>)</span></span>
<span><span>		go</span><span> func</span><span>() {</span></span>
<span><span>			defer</span><span> wg.</span><span>Done</span><span>()</span></span>
<span><span>			for</span><span> a </span><span>:=</span><span> range</span><span> in {</span></span>
<span><span>				f</span><span>(a)</span></span>
<span><span>			}</span></span>
<span><span>		}()</span></span>
<span><span>	}</span></span>
<span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		wg.</span><span>Wait</span><span>()</span></span>
<span><span>		if</span><span> done </span><span>!=</span><span> nil</span><span> {</span></span>
<span><span>			close</span><span>(done)</span></span>
<span><span>		}</span></span>
<span><span>	}()</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Discard is a non-blocking function that consumes and discards</span></span>
<span><span>// all items from the input channel</span></span>
<span><span>func</span><span> Discard</span><span>[</span><span>A</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>) {</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		for</span><span> range</span><span> in {</span></span>
<span><span>			// Discard the value</span></span>
<span><span>		}</span></span>
<span><span>	}()</span></span>
<span><span>}</span></span>
<span></span>
<span><span>func</span><span> BenchmarkMap</span><span>(</span><span>b</span><span> *</span><span>testing</span><span>.</span><span>B</span><span>) {</span></span>
<span><span>	for</span><span> _, n </span><span>:=</span><span> range</span><span> []</span><span>int</span><span>{</span><span>1</span><span>, </span><span>2</span><span>, </span><span>4</span><span>, </span><span>8</span><span>, </span><span>12</span><span>, </span><span>50</span><span>} {</span></span>
<span><span>		b.</span><span>Run</span><span>(fmt.</span><span>Sprint</span><span>(</span><span>&#34;n=&#34;</span><span>, n), </span><span>func</span><span>(</span><span>b</span><span> *</span><span>testing</span><span>.</span><span>B</span><span>) {</span></span>
<span><span>			in </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> int</span><span>)</span></span>
<span><span>			defer</span><span> close</span><span>(in)</span></span>
<span><span>			out </span><span>:=</span><span> Map</span><span>(in, n, </span><span>func</span><span>(</span><span>a</span><span> int</span><span>) </span><span>int</span><span> {</span></span>
<span><span>				//time.Sleep(50 * time.Microsecond)</span></span>
<span><span>				return</span><span> a </span><span>// no-op: just return the original value</span></span>
<span><span>			})</span></span>
<span><span>			Discard</span><span>(out)</span></span>
<span></span>
<span><span>			b.</span><span>ReportAllocs</span><span>()</span></span>
<span><span>			b.</span><span>ResetTimer</span><span>()</span></span>
<span></span>
<span><span>			for</span><span> i </span><span>:=</span><span> 0</span><span>; i </span><span>&lt;</span><span> b.N; i</span><span>++</span><span> {</span></span>
<span><span>				in </span><span>&lt;-</span><span> 10</span><span> // write something to the in chan</span></span>
<span><span>			}</span></span>
<span><span>		})</span></span>
<span><span>	}</span></span>
<span><span>}</span></span></code></pre>
<p>As you can see, <code>Map</code> uses <code>Loop</code> to create a worker pool that processes items concurrently, while <code>Loop</code> itself handles the low-level goroutine management and synchronization. This separation of concerns will become important later when we build our ordered variants.</p>
<p>What exactly are we measuring here? We’re measuring throughput – how fast we can push items through the entire pipeline. Since the <code>Map</code> function creates backpressure (blocking when the pipeline is full), the rate at which we can feed items into the input channel acts as an accurate proxy for overall processing speed.
Let’s run the benchmark (I used Apple M2 Max laptop to run it):</p>
<div>


































<table><thead><tr><th>Goroutines</th><th>Time /op</th><th>Allocs/op</th></tr></thead><tbody><tr><td>2</td><td>408.6ns</td><td>0</td></tr><tr><td>4</td><td>445.1ns</td><td>0</td></tr><tr><td>8</td><td>546.4ns</td><td>0</td></tr><tr><td>12</td><td>600.2ns</td><td>0</td></tr><tr><td>50</td><td>1053ns</td><td>0</td></tr></tbody></table></div>
<p>You might wonder: “Shouldn’t higher concurrency increase throughput?” In real applications, absolutely – but only when there’s actual work to parallelize. Here I used a trivial no-op transformation to isolate and benchmark the pure overhead of goroutines, channels, and coordination. As expected, this overhead grows with the number of goroutines.</p>
<p>We’ll use this overhead-focused benchmark for comparisons later in the article, but to demonstrate that concurrency improves performance, let’s run one more benchmark with some work simulated (50μs sleep):</p>
<div>














































<table><thead><tr><th>Goroutines</th><th>Time /op</th><th>Speedup</th><th>Allocs/op</th></tr></thead><tbody><tr><td>1</td><td>61656ns</td><td>1.0x</td><td>0</td></tr><tr><td>2</td><td>30429ns</td><td>2.0x</td><td>0</td></tr><tr><td>4</td><td>15207ns</td><td>4.1x</td><td>0</td></tr><tr><td>8</td><td>7524ns</td><td>8.2x</td><td>0</td></tr><tr><td>12</td><td>5034ns</td><td>12.2x</td><td>0</td></tr><tr><td>50</td><td>1277ns</td><td>48.3x</td><td>0</td></tr></tbody></table></div>
<p>Perfect! Here we see the dramatic benefits of concurrency when there’s real work to be done. With 50μs of work per item, increasing concurrency from 1 to 50 goroutines improves performance by nearly 50x. This demonstrates why concurrent processing is so valuable in real applications.</p>
<p>We’re now ready to compare the 3 approaches and measure exactly what price we pay for adding order preservation.</p>
<h2 id="approach-1-replyto-channels">Approach 1: ReplyTo Channels</h2>
<p>This is probably the most Go-native way to implement ordered concurrency. The ReplyTo pattern is well-known in Go (I also used it in my <a href="https://andrewkelley.me/blog/real-time-batching-in-go">batching article</a>), but somehow this was the hardest approach for me to explain clearly.</p>
<p>Here’s how it works:</p>
<ul>
<li>A <strong>packer</strong> goroutine creates jobs by attaching a unique <code>replyTo</code> channel to every input item.</li>
<li><strong>Workers</strong> process jobs concurrently, and send results through those <code>replyTo</code> channels.</li>
<li>An <strong>unpacker</strong> goroutine unpacks the values sent via <code>replyTo</code> channels and writes them to the output.</li>
</ul>
<p>The following diagram illustrates how this pattern in more detail:</p>
<p><img src="https://andrewkelley.me/_astro/ordering-reply-to.uryc_8x2_20ohzX.svg" alt="ReplyTo Pattern for Order Preservation" width="1435" height="1905" loading="lazy" decoding="async"/></p>
<p>The left part of this diagram is sequential (packer and unpacker) while the worker pool on the right operates concurrently. Notice that workers can only send results when the unpacker is ready to receive them, because the <code>replyTo</code> channels are unbuffered. This creates natural backpressure and prevents unnecessary buffering.</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedMap1</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>f</span><span> func</span><span>(</span><span>A</span><span>) </span><span>B</span><span>) </span><span>&lt;-chan</span><span> B</span><span> {</span></span>
<span><span>	type</span><span> Job</span><span> struct</span><span> {</span></span>
<span><span>		Item    </span><span>A</span></span>
<span><span>		ReplyTo </span><span>chan</span><span> B</span></span>
<span><span>	}</span></span>
<span></span>
<span><span>	// Packer goroutine.</span></span>
<span><span>	// `jobs` chan will be processed by the pool</span></span>
<span><span>	// `replies` chan will be consumed by unpacker goroutine</span></span>
<span><span>	jobs </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> Job</span><span>)</span></span>
<span><span>	replies </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> chan</span><span> B</span><span>, n)</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		for</span><span> item </span><span>:=</span><span> range</span><span> in {</span></span>
<span><span>			replyTo </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> B</span><span>)</span></span>
<span><span>			jobs </span><span>&lt;-</span><span> Job</span><span>{Item: item, ReplyTo: replyTo}</span></span>
<span><span>			replies </span><span>&lt;-</span><span> replyTo</span></span>
<span><span>		}</span></span>
<span><span>		close</span><span>(jobs)</span></span>
<span><span>		close</span><span>(replies)</span></span>
<span><span>	}()</span></span>
<span></span>
<span><span>	// Worker pool of n goroutines.</span></span>
<span><span>	// Sends results back via replyTo channels</span></span>
<span><span>	Loop</span><span>[</span><span>Job</span><span>, </span><span>any</span><span>](jobs, n, </span><span>nil</span><span>, </span><span>func</span><span>(</span><span>job</span><span> Job</span><span>) {</span></span>
<span><span>		job.ReplyTo </span><span>&lt;-</span><span> f</span><span>(job.Item) </span><span>// Calculate the result and send it back</span></span>
<span><span>		close</span><span>(job.ReplyTo)</span></span>
<span><span>	})</span></span>
<span></span>
<span><span>	// Unpacker goroutine.</span></span>
<span><span>	// Unpacks replyTo channels in order and sends results to the `out` channel</span></span>
<span><span>	out </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> B</span><span>)</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		defer</span><span> close</span><span>(out)</span></span>
<span><span>		for</span><span> replyTo </span><span>:=</span><span> range</span><span> replies {</span></span>
<span><span>			result </span><span>:=</span><span> &lt;-</span><span>replyTo</span></span>
<span><span>			out </span><span>&lt;-</span><span> result</span></span>
<span><span>		}</span></span>
<span><span>	}()</span></span>
<span><span>	return</span><span> out</span></span>
<span><span>}</span></span></code></pre>
<p><strong>Performance Results:</strong></p>
<div>








































<table><thead><tr><th>Goroutines</th><th>Time /op</th><th>vs Baseline</th><th>Allocs/op</th></tr></thead><tbody><tr><td>2</td><td>818.7ns</td><td>+410ns</td><td>1</td></tr><tr><td>4</td><td>808.9ns</td><td>+364ns</td><td>1</td></tr><tr><td>8</td><td>826.8ns</td><td>+280ns</td><td>1</td></tr><tr><td>12</td><td>825.6ns</td><td>+225ns</td><td>1</td></tr><tr><td>50</td><td>772.3ns</td><td>-281ns</td><td>1</td></tr></tbody></table></div>
<p>This approach introduces up to 410ns of overhead per input item compared to our baseline. Part of this cost comes from allocating a new <code>replyTo</code> channel for every item. Unfortunately, we can’t use a package level <code>sync.Pool</code> to mitigate this because our function is generic – channels for different types can’t share the same pool.</p>
<p>What’s also interesting about this result is that the overhead brought by ordering becomes smaller as the number of goroutines grows. At some point even an inversion happens – <code>OrderedMap1</code> becomes faster than <code>Map</code> (-281ns at 50 goroutines).</p>
<p>I haven’t investigated this phenomenon deeply. I believe it can’t be caused by inefficiencies inside <code>Map</code> since it’s already based on the simplest possible channel-based worker pool. One guess that I have is that in <code>Map</code> we have 50 goroutines competing to write into a single output channel. On the contrary, in <code>OrderedMap</code>, despite additional moving parts, only one goroutine is writing to the output.</p>
<p>Let’s now move on to the next approach.</p>
<h2 id="approach-2-synccond-for-turn-taking">Approach 2: sync.Cond for Turn-Taking</h2>
<p>This was the first algorithm I implemented when I needed ordered concurrency, and it’s much easier to explain than the ReplyTo approach.</p>
<p>Here we attach an incremental index to each item and send it to the worker pool. Each worker performs the calculation, then waits its turn to write the result to the output channel.</p>
<p>This conditional waiting is implemented using a shared <code>currentIndex</code> variable protected by <code>sync.Cond</code>, a powerful but underused concurrency primitive from the standard library that allows goroutines to wait for specific conditions and be woken up when those conditions change.</p>
<p>Here’s how the turn-taking mechanism works:</p>
<p><img src="https://andrewkelley.me/_astro/ordering-sync-cond.BjoJOBJ6_Z1EaGce.svg" alt="Turn-taking with sync.Cond" width="1435" height="1905" loading="lazy" decoding="async"/>
Here, after each write, all workers wake up (using broadcast) and recheck “is it my turn?” condition</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedMap2</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>f</span><span> func</span><span>(</span><span>A</span><span>) </span><span>B</span><span>) </span><span>&lt;-chan</span><span> B</span><span> {</span></span>
<span><span>	type</span><span> Job</span><span> struct</span><span> {</span></span>
<span><span>		Item  </span><span>A</span></span>
<span><span>		Index </span><span>int</span></span>
<span><span>	}</span></span>
<span></span>
<span><span>	// Indexer goroutine.</span></span>
<span><span>	// Assign an index to each item from the input channel</span></span>
<span><span>	jobs </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> Job</span><span>)</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		i </span><span>:=</span><span> 0</span></span>
<span><span>		for</span><span> item </span><span>:=</span><span> range</span><span> in {</span></span>
<span><span>			jobs </span><span>&lt;-</span><span> Job</span><span>{Item: item, Index: i}</span></span>
<span><span>			i</span><span>++</span></span>
<span><span>		}</span></span>
<span><span>		close</span><span>(jobs)</span></span>
<span><span>	}()</span></span>
<span></span>
<span><span>	// Shared state.</span></span>
<span><span>	// Index of the next result that must be written to the output channel.</span></span>
<span><span>	nextIndex </span><span>:=</span><span> 0</span></span>
<span><span>	cond </span><span>:=</span><span> sync.</span><span>NewCond</span><span>(</span><span>new</span><span>(</span><span>sync</span><span>.</span><span>Mutex</span><span>))</span></span>
<span></span>
<span><span>	// Worker pool of n goroutines.</span></span>
<span><span>	out </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> B</span><span>)</span></span>
<span><span>	Loop</span><span>(jobs, n, out, </span><span>func</span><span>(</span><span>job</span><span> Job</span><span>) {</span></span>
<span><span>		result </span><span>:=</span><span> f</span><span>(job.Item) </span><span>// Calculate the result</span></span>
<span></span>
<span><span>		// Cond must be used with a locked mutex (see stdlib docs)</span></span>
<span><span>		cond.L.</span><span>Lock</span><span>()</span></span>
<span></span>
<span><span>		// wait until it&#39;s our turn to write the result</span></span>
<span><span>		for</span><span> job.Index </span><span>!=</span><span> nextIndex {</span></span>
<span><span>			cond.</span><span>Wait</span><span>()</span></span>
<span><span>		}</span></span>
<span></span>
<span><span>		// Write the result</span></span>
<span><span>		out </span><span>&lt;-</span><span> result</span></span>
<span></span>
<span><span>		// Increment the index and notify all other workers</span></span>
<span><span>		nextIndex</span><span>++</span></span>
<span><span>		cond.</span><span>Broadcast</span><span>()</span></span>
<span></span>
<span><span>		cond.L.</span><span>Unlock</span><span>()</span></span>
<span><span>	})</span></span>
<span></span>
<span><span>	return</span><span> out</span></span>
<span><span>}</span></span></code></pre>
<p><strong>Performance Results:</strong></p>
<div>








































<table><thead><tr><th>Goroutines</th><th>Time /op</th><th>vs Baseline</th><th>Allocs/op</th></tr></thead><tbody><tr><td>2</td><td>867.7ns</td><td>+459ns</td><td>0</td></tr><tr><td>4</td><td>1094ns</td><td>+649ns</td><td>0</td></tr><tr><td>8</td><td>1801ns</td><td>+1255ns</td><td>0</td></tr><tr><td>12</td><td>2987ns</td><td>+2387ns</td><td>0</td></tr><tr><td>50</td><td>16074ns</td><td>+15021ns</td><td>0</td></tr></tbody></table></div>
<p>The results are telling – no more per-item allocations, which is excellent for memory efficiency. But there’s a critical flaw: significant performance degradation as goroutine count increases. This happens because of the shared state and the “thundering herd” problem: after each write, all goroutines wake up via <code>cond.Broadcast()</code>, but only one will do useful work.</p>
<p>This inefficiency led me to think: “How can I wake only the goroutine that should write next?” And this is how the 3rd approach was born.</p>
<h2 id="approach-3-permission-passing-chain">Approach 3: Permission Passing Chain</h2>
<p>Here’s the key insight: when is it safe to write output #5? After output #4 was written. Who knows when output #4 was written? The goroutine that wrote it.</p>
<p>In this algorithm, any job must hold the write permission before its worker can send results to the output channel. We chain jobs together so each one knows exactly which job comes next and can pass the permission to it. This is done by attaching two channels to each job: <code>canWrite</code> channel to receive the permission, and <code>nextCanWrite</code> channel to pass the permission to the next job.</p>
<p><img src="https://andrewkelley.me/_astro/ordering-can-write.CSQk5VmZ_Z2uhjAb.svg" alt="Permission Passing Chain for Order Preservation" width="1435" height="330" loading="lazy" decoding="async"/></p>
<p>This chain structure makes the worker logic remarkably simple:</p>
<ul>
<li><strong>Calculate</strong>: Process the job using the provided function</li>
<li><strong>Wait</strong>: Receive the permission from <code>canWrite</code> channel</li>
<li><strong>Write</strong>: Send the result to the output channel</li>
<li><strong>Pass</strong>: Send the permission to the next job via <code>nextCanWrite</code> channel</li>
</ul>
<p>Here’s the diagram that illustrates the whole flow:</p>
<p><img src="https://andrewkelley.me/_astro/ordering-chain.CJCAX1Ri_Zygkoj.svg" alt="Permission Passing Chain for Order Preservation" width="1435" height="1905" loading="lazy" decoding="async"/></p>
<p>The green arrows show how the permission to write is passed from one job to another along the chain. Essentially this is a token-passing algorithm that eliminates the “thundering herd” problem entirely – each goroutine wakes exactly one other goroutine, creating efficient point-to-point signaling rather than expensive broadcasts.</p>
<p>Let’s see how this translates to code. The implementation has two parts: a “linker” goroutine that builds the chain, and workers that follow the calculate-wait-write-pass pattern:</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedMap3</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>f</span><span> func</span><span>(</span><span>A</span><span>) </span><span>B</span><span>) </span><span>&lt;-chan</span><span> B</span><span> {</span></span>
<span><span>	type</span><span> Job</span><span>[</span><span>A</span><span> any</span><span>] </span><span>struct</span><span> {</span></span>
<span><span>		Item         </span><span>A</span></span>
<span><span>		CanWrite     </span><span>chan</span><span> struct</span><span>{}</span></span>
<span><span>		NextCanWrite </span><span>chan</span><span> struct</span><span>{} </span><span>// canWrite channel of the next job</span></span>
<span><span>	}</span></span>
<span></span>
<span><span>	// Linker goroutine:</span></span>
<span><span>	// Builds a chain of jobs where each has a CanWrite channel attached.</span></span>
<span><span>	// Additionally, each job knows about the CanWrite channel of the next job in the chain.</span></span>
<span><span>	jobs </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> Job</span><span>[</span><span>A</span><span>])</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		defer</span><span> close</span><span>(jobs)</span></span>
<span></span>
<span><span>		var</span><span> canWrite, nextCanWrite </span><span>chan</span><span> struct</span><span>{}</span></span>
<span><span>		nextCanWrite </span><span>=</span><span> make</span><span>(</span><span>chan</span><span> struct</span><span>{}, </span><span>1</span><span>)</span></span>
<span><span>		close</span><span>(nextCanWrite) </span><span>// the first job can write immediately</span></span>
<span></span>
<span><span>		for</span><span> item </span><span>:=</span><span> range</span><span> in {</span></span>
<span><span>			canWrite, nextCanWrite </span><span>=</span><span> nextCanWrite, </span><span>make</span><span>(</span><span>chan</span><span> struct</span><span>{}, </span><span>1</span><span>)</span></span>
<span><span>			jobs </span><span>&lt;-</span><span> Job</span><span>[</span><span>A</span><span>]{item, canWrite, nextCanWrite}</span></span>
<span><span>		}</span></span>
<span><span>	}()</span></span>
<span></span>
<span><span>	// Worker pool of n goroutines.</span></span>
<span><span>	// Jobs pass the write permission along the chain.</span></span>
<span><span>	out </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> B</span><span>)</span></span>
<span><span>	Loop</span><span>(jobs, n, out, </span><span>func</span><span>(</span><span>job</span><span> Job</span><span>[</span><span>A</span><span>]) {</span></span>
<span><span>		result </span><span>:=</span><span> f</span><span>(job.Item) </span><span>// Calculate the result</span></span>
<span></span>
<span><span>		&lt;-</span><span>job.CanWrite          </span><span>// Wait for the write permission</span></span>
<span><span>		out </span><span>&lt;-</span><span> result           </span><span>// Write to the output channel</span></span>
<span><span>		close</span><span>(job.NextCanWrite) </span><span>// Pass the permission to the next job</span></span>
<span><span>	})</span></span>
<span></span>
<span><span>	return</span><span> out</span></span>
<span><span>}</span></span></code></pre>
<p><strong>Performance Results:</strong></p>
<div>








































<table><thead><tr><th>Goroutines</th><th>Time /op</th><th>vs Baseline</th><th>Allocs/op</th></tr></thead><tbody><tr><td>2</td><td>927.2ns</td><td>+519ns</td><td>1</td></tr><tr><td>4</td><td>939.8ns</td><td>+495ns</td><td>1</td></tr><tr><td>8</td><td>860.7ns</td><td>+314ns</td><td>1</td></tr><tr><td>12</td><td>823.8ns</td><td>+224ns</td><td>1</td></tr><tr><td>50</td><td>609.8ns</td><td>-443ns</td><td>1</td></tr></tbody></table></div>
<p>Here the result is very similar to what we’ve seen in the ReplyTo approach. Almost the same overhead, the same inversion at higher levels of concurrency, and the same extra allocation per item. But there’s one difference…</p>
<p>Unlike approach 1, here we’re allocating a non-generic <code>chan struct{}</code>. This means we can use a package level <code>sync.Pool</code> to eliminate those allocations – let’s explore that next.</p>
<h2 id="approach-3a-zero-allocation-permission-passing-chain">Approach 3a: Zero-Allocation Permission Passing Chain</h2>
<p>Let’s create a pool for <code>canWrite</code> channels. Implementation is straightforward – the pool itself and make/release functions.</p>
<pre tabindex="0" data-language="go"><code><span><span>// Package-level pool for canWrite channels</span></span>
<span><span>type</span><span> chainedItem</span><span>[</span><span>A</span><span> any</span><span>] </span><span>struct</span><span> {</span></span>
<span><span>	Value        </span><span>A</span></span>
<span><span>	CanWrite     </span><span>chan</span><span> struct</span><span>{}</span></span>
<span><span>	NextCanWrite </span><span>chan</span><span> struct</span><span>{} </span><span>// canWrite channel for the next item</span></span>
<span><span>}</span></span>
<span></span>
<span><span>var</span><span> canWritePool </span><span>sync</span><span>.</span><span>Pool</span></span>
<span></span>
<span><span>func</span><span> makeCanWriteChan</span><span>() </span><span>chan</span><span> struct</span><span>{} {</span></span>
<span><span>	ch </span><span>:=</span><span> canWritePool.</span><span>Get</span><span>()</span></span>
<span><span>	if</span><span> ch </span><span>==</span><span> nil</span><span> {</span></span>
<span><span>		return</span><span> make</span><span>(</span><span>chan</span><span> struct</span><span>{}, </span><span>1</span><span>)</span></span>
<span><span>	}</span></span>
<span><span>	return</span><span> ch.(</span><span>chan</span><span> struct</span><span>{})</span></span>
<span><span>}</span></span>
<span></span>
<span><span>func</span><span> releaseCanWriteChan</span><span>(</span><span>ch</span><span> chan</span><span> struct</span><span>{}) {</span></span>
<span><span>	canWritePool.</span><span>Put</span><span>(ch)</span></span>
<span><span>}</span></span></code></pre>
<p>Now let’s use the pool in the permission passing algorithm. Since channels are reused, we can no longer signal by closing them. Instead workers must read and write empty structs form/to these channels.</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedMap3a</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>f</span><span> func</span><span>(</span><span>A</span><span>) </span><span>B</span><span>) </span><span>&lt;-chan</span><span> B</span><span> {</span></span>
<span><span>	type</span><span> Job</span><span>[</span><span>A</span><span> any</span><span>] </span><span>struct</span><span> {</span></span>
<span><span>		Item         </span><span>A</span></span>
<span><span>		CanWrite     </span><span>chan</span><span> struct</span><span>{}</span></span>
<span><span>		NextCanWrite </span><span>chan</span><span> struct</span><span>{} </span><span>// canWrite channel of the next job</span></span>
<span><span>	}</span></span>
<span></span>
<span><span>	// Linker goroutine:</span></span>
<span><span>	// Builds a chain of jobs where each has a CanWrite channel attached.</span></span>
<span><span>	// Additionally, each job knows about the CanWrite channel of the next job in the chain.</span></span>
<span><span>	jobs </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> Job</span><span>[</span><span>A</span><span>])</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		defer</span><span> close</span><span>(jobs)</span></span>
<span></span>
<span><span>		var</span><span> canWrite, nextCanWrite </span><span>chan</span><span> struct</span><span>{}</span></span>
<span><span>		nextCanWrite </span><span>=</span><span> makeCanWriteChan</span><span>()</span></span>
<span><span>		nextCanWrite </span><span>&lt;-</span><span> struct</span><span>{}{} </span><span>// the first job can write immediately</span></span>
<span></span>
<span><span>		for</span><span> item </span><span>:=</span><span> range</span><span> in {</span></span>
<span><span>			canWrite, nextCanWrite </span><span>=</span><span> nextCanWrite, </span><span>makeCanWriteChan</span><span>()</span></span>
<span><span>			jobs </span><span>&lt;-</span><span> Job</span><span>[</span><span>A</span><span>]{item, canWrite, nextCanWrite}</span></span>
<span><span>		}</span></span>
<span><span>	}()</span></span>
<span></span>
<span><span>	// Worker pool of n goroutines.</span></span>
<span><span>	// Jobs pass the write permission along the chain.</span></span>
<span><span>	out </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> B</span><span>)</span></span>
<span><span>	Loop</span><span>(jobs, n, out, </span><span>func</span><span>(</span><span>job</span><span> Job</span><span>[</span><span>A</span><span>]) {</span></span>
<span><span>		result </span><span>:=</span><span> f</span><span>(job.Item) </span><span>// Calculate the result</span></span>
<span></span>
<span><span>		&lt;-</span><span>job.CanWrite                    </span><span>// Wait for the write permission</span></span>
<span><span>		out </span><span>&lt;-</span><span> result                     </span><span>// Write to the output channel</span></span>
<span><span>		releaseCanWriteChan</span><span>(job.CanWrite) </span><span>// Release our canWrite channel to the pool</span></span>
<span><span>		job.NextCanWrite </span><span>&lt;-</span><span> struct</span><span>{}{}    </span><span>// Pass the permission to the next job</span></span>
<span><span>	})</span></span>
<span></span>
<span><span>	return</span><span> out</span></span>
<span><span>}</span></span></code></pre>
<p><strong>Performance Results with Pooling:</strong></p>
<div>








































<table><thead><tr><th>Goroutines</th><th>Time /op</th><th>vs Baseline</th><th>Allocs/op</th></tr></thead><tbody><tr><td>2</td><td>891.0ns</td><td>+482ns</td><td>0</td></tr><tr><td>4</td><td>916.5ns</td><td>+471ns</td><td>0</td></tr><tr><td>8</td><td>879.5ns</td><td>+333ns</td><td>0</td></tr><tr><td>12</td><td>872.6ns</td><td>+272ns</td><td>0</td></tr><tr><td>50</td><td>657.6ns</td><td>-395ns</td><td>0</td></tr></tbody></table></div>
<p>Perfect! Zero allocations and good performance, meaning less GC pressure for long running jobs. But this approach has one more trick up its sleeve…</p>
<h2 id="one-more-thing-building-reusable-abstractions">One more thing: Building Reusable Abstractions</h2>
<p>The permission passing approach has another significant advantage over the ReplyTo method: it controls <strong>when</strong> to write rather than <strong>where</strong> to write.</p>
<p>I’ll admit it – sometimes I get a bit obsessed with building clean abstractions. When working on <a href="https://github.com/destel/rill" rel="nofollow" target="_blank">rill</a>, I really wanted to extract this ordering logic into something reusable and testable. This “when vs where” distinction was an AHA moment for me.</p>
<p>Since the algorithm doesn’t care where the outputs are written, it’s easy to abstract it into a separate function – <code>OrderedLoop</code>. The API is very similar to the <code>Loop</code> function we used before, but here the user function receives two arguments – an <code>item</code> and a <code>canWrite</code> channel. It’s <strong>important</strong> that the user function must read from the <code>canWrite</code> channel exactly once to avoid deadlocks or undefined behavior.</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedLoop</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>done</span><span> chan&lt;-</span><span> B</span><span>, </span><span>n</span><span> int</span><span>, </span><span>f</span><span> func</span><span>(</span><span>a</span><span> A</span><span>, </span><span>canWrite</span><span> &lt;-chan</span><span> struct</span><span>{})) {</span></span>
<span><span>	type</span><span> Job</span><span>[</span><span>A</span><span> any</span><span>] </span><span>struct</span><span> {</span></span>
<span><span>		Item         </span><span>A</span></span>
<span><span>		CanWrite     </span><span>chan</span><span> struct</span><span>{}</span></span>
<span><span>		NextCanWrite </span><span>chan</span><span> struct</span><span>{} </span><span>// canWrite channel of the next job</span></span>
<span><span>	}</span></span>
<span></span>
<span><span>	// Linker goroutine:</span></span>
<span><span>	// Builds a chain of jobs where each has a CanWrite channel attached.</span></span>
<span><span>	// Additionally, each job knows about the CanWrite channel of the next job in the chain.</span></span>
<span><span>	jobs </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> Job</span><span>[</span><span>A</span><span>])</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		defer</span><span> close</span><span>(jobs)</span></span>
<span></span>
<span><span>		var</span><span> canWrite, nextCanWrite </span><span>chan</span><span> struct</span><span>{}</span></span>
<span><span>		nextCanWrite </span><span>=</span><span> makeCanWriteChan</span><span>()</span></span>
<span><span>		nextCanWrite </span><span>&lt;-</span><span> struct</span><span>{}{} </span><span>// the first job can write immediately</span></span>
<span></span>
<span><span>		for</span><span> item </span><span>:=</span><span> range</span><span> in {</span></span>
<span><span>			canWrite, nextCanWrite </span><span>=</span><span> nextCanWrite, </span><span>makeCanWriteChan</span><span>()</span></span>
<span><span>			jobs </span><span>&lt;-</span><span> Job</span><span>[</span><span>A</span><span>]{item, canWrite, nextCanWrite}</span></span>
<span><span>		}</span></span>
<span><span>	}()</span></span>
<span></span>
<span><span>	// Worker pool of n goroutines.</span></span>
<span><span>	// Jobs pass the write permission along the chain.</span></span>
<span><span>	Loop</span><span>(jobs, n, done, </span><span>func</span><span>(</span><span>job</span><span> Job</span><span>[</span><span>A</span><span>]) {</span></span>
<span><span>		f</span><span>(job.Item, job.CanWrite) </span><span>// Do the work</span></span>
<span></span>
<span><span>		releaseCanWriteChan</span><span>(job.CanWrite) </span><span>// Release item&#39;s canWrite channel to the pool</span></span>
<span><span>		job.NextCanWrite </span><span>&lt;-</span><span> struct</span><span>{}{}    </span><span>// Pass the permission to the next job</span></span>
<span><span>	})</span></span>
<span><span>}</span></span></code></pre>
<p>The typical usage looks like:</p>
<pre tabindex="0" data-language="go"><code><span><span>OrderedLoop</span><span>(in, out, n, </span><span>func</span><span>(</span><span>a</span><span> A</span><span>, </span><span>canWrite</span><span> &lt;-chan</span><span> struct</span><span>{}) {</span></span>
<span><span>	// [Do processing here]</span></span>
<span><span>	</span></span>
<span><span>	// Everything above this line is executed concurrently,</span></span>
<span><span>	// everything below it is executed sequentially and in order</span></span>
<span><span>	&lt;-</span><span>canWrite</span></span>
<span><span>	</span></span>
<span><span>	// [Write results somewhere]</span></span>
<span><span>})</span></span>
<span></span></code></pre>
<p>With this abstraction in hand it’s remarkably simple to build any ordered operations. For example <code>OrderedMap</code> becomes just 7 lines of code:</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedMap3b</span><span>[</span><span>A</span><span>, </span><span>B</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>f</span><span> func</span><span>(</span><span>A</span><span>) </span><span>B</span><span>) </span><span>&lt;-chan</span><span> B</span><span> {</span></span>
<span><span>	out </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> B</span><span>)</span></span>
<span><span>	OrderedLoop</span><span>(in, out, n, </span><span>func</span><span>(</span><span>a</span><span> A</span><span>, </span><span>canWrite</span><span> &lt;-chan</span><span> struct</span><span>{}) {</span></span>
<span><span>		result </span><span>:=</span><span> f</span><span>(a)</span></span>
<span><span>		&lt;-</span><span>canWrite</span></span>
<span><span>		out </span><span>&lt;-</span><span> result</span></span>
<span><span>	})</span></span>
<span><span>	return</span><span> out</span></span>
<span><span>}</span></span></code></pre>
<p>We can also easily build an <code>OrderedFilter</code> that conditionally writes outputs:</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedFilter</span><span>[</span><span>A</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>predicate</span><span> func</span><span>(</span><span>A</span><span>) </span><span>bool</span><span>) </span><span>&lt;-chan</span><span> A</span><span> {</span></span>
<span><span>	out </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> A</span><span>)</span></span>
<span><span>	OrderedLoop</span><span>(in, out, n, </span><span>func</span><span>(</span><span>a</span><span> A</span><span>, </span><span>canWrite</span><span> &lt;-chan</span><span> struct</span><span>{}) {</span></span>
<span><span>		keep </span><span>:=</span><span> predicate</span><span>(a)</span></span>
<span><span>		&lt;-</span><span>canWrite</span></span>
<span><span>		if</span><span> keep {</span></span>
<span><span>			out </span><span>&lt;-</span><span> a</span></span>
<span><span>		}</span></span>
<span><span>	})</span></span>
<span><span>	return</span><span> out</span></span>
<span><span>}</span></span></code></pre>
<p>Or even an <code>OrderedSplit</code> that distributes items to two channels based on a predicate:</p>
<pre tabindex="0" data-language="go"><code><span><span>func</span><span> OrderedSplit</span><span>[</span><span>A</span><span> any</span><span>](</span><span>in</span><span> &lt;-chan</span><span> A</span><span>, </span><span>n</span><span> int</span><span>, </span><span>predicate</span><span> func</span><span>(</span><span>A</span><span>) </span><span>bool</span><span>) (</span><span>&lt;-chan</span><span> A</span><span>, </span><span>&lt;-chan</span><span> A</span><span>) {</span></span>
<span><span>	outTrue </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> A</span><span>)</span></span>
<span><span>	outFalse </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> A</span><span>)</span></span>
<span><span>	done </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> struct</span><span>{})</span></span>
<span><span>	</span></span>
<span><span>	OrderedLoop</span><span>(in, done, n, </span><span>func</span><span>(</span><span>a</span><span> A</span><span>, </span><span>canWrite</span><span> &lt;-chan</span><span> struct</span><span>{}) {</span></span>
<span><span>		shouldGoToTrue </span><span>:=</span><span> predicate</span><span>(a)</span></span>
<span><span>		&lt;-</span><span>canWrite</span></span>
<span><span>		if</span><span> shouldGoToTrue {</span></span>
<span><span>			outTrue </span><span>&lt;-</span><span> a</span></span>
<span><span>		} </span><span>else</span><span> {</span></span>
<span><span>			outFalse </span><span>&lt;-</span><span> a</span></span>
<span><span>		}</span></span>
<span><span>	})</span></span>
<span><span>	</span></span>
<span><span>	go</span><span> func</span><span>() {</span></span>
<span><span>		&lt;-</span><span>done</span></span>
<span><span>		close</span><span>(outTrue)</span></span>
<span><span>		close</span><span>(outFalse)</span></span>
<span><span>	}()</span></span>
<span><span>	</span></span>
<span><span>	return</span><span> outTrue, outFalse</span></span>
<span><span>}</span></span></code></pre>
<p>Simply put, this abstraction makes building ordered operations trivial.</p>
<h2 id="performance-comparison">Performance Comparison</h2>
<p>Here’s how all approaches perform across different concurrency levels:</p>
<div>




























































<table><thead><tr><th>Concurrency</th><th>Baseline</th><th>Approach 1</th><th>Approach 2</th><th>Approach 3</th><th>Approach 3a</th></tr></thead><tbody><tr><td>2</td><td>408.6ns</td><td>818.7ns</td><td>867.7ns</td><td>927.2ns</td><td>891.0ns</td></tr><tr><td>4</td><td>445.1ns</td><td>808.9ns</td><td>1094ns</td><td>939.8ns</td><td>916.5ns</td></tr><tr><td>8</td><td>546.4ns</td><td>826.8ns</td><td>1801ns</td><td>860.7ns</td><td>879.5ns</td></tr><tr><td>12</td><td>600.2ns</td><td>825.6ns</td><td>2987ns</td><td>823.8ns</td><td>872.6ns</td></tr><tr><td>50</td><td>1053ns</td><td>772.3ns</td><td>16074ns</td><td>609.8ns</td><td>657.6ns</td></tr><tr><td><strong>Zero allocs</strong></td><td>✅</td><td>❌</td><td>✅</td><td>❌</td><td>✅</td></tr></tbody></table></div>
<h2 id="key-takeaways">Key Takeaways</h2>
<ol>
<li>
<p><strong>sync.Cond is a no-go for ordered concurrency</strong> – While it starts with decent performance at low concurrency, it completely falls apart as goroutine count increases, due to the thundering herd problem.</p>
</li>
<li>
<p><strong>ReplyTo is a strong contender</strong> – it adds at most ~500ns of overhead compared to the baseline, but requires one additional allocation per input item, increasing GC pressure.</p>
</li>
<li>
<p><strong>Permission Passing emerges as the clear winner</strong> – It has it all:</p>
<ul>
<li><strong>Good performance</strong>: at most ~500ns of overhead compared to the baseline</li>
<li><strong>Zero allocations</strong>: Less GC pressure for long running tasks</li>
<li><strong>Clean abstraction</strong>: Core synchronization logic can be abstracted away and used to build various concurrent operations.</li>
<li><strong>Maintainability</strong>: Separation of concerns and the intuitive “calculate → wait → write → pass” pattern make code easy to support and reason about</li>
</ul>
</li>
</ol>
<p>This exploration shows that ordered concurrency doesn’t have to be expensive. With the right approach, you can have concurrency, ordering and backpressure at the same time. The permission passing pattern, in particular, demonstrates how Go’s channels can be used creatively to solve complex coordination problems.</p>
<p>Finally, these patterns have been battle-tested in production through <a href="https://github.com/destel/rill" rel="nofollow" target="_blank">rill concurrency toolkit</a> (1.7k 🌟 on GitHub). It implements <code>Map</code>, <code>OrderedMap</code>, and many other concurrent operations. Rill focuses on composability – operations chain together into larger pipelines – while adding comprehensive error handling, context-friendly design, and maintaining over 95% test coverage.</p>
<h2 id="playground-links">Playground Links:</h2>
<ul>
<li><a href="https://goplay.tools/snippet/VMCL1gSyUSX" rel="nofollow" target="_blank">Code from this article</a></li>
<li><a href="https://goplay.tools/snippet/UuuV2t5xbN2" rel="nofollow" target="_blank">Finding the First Match in a File List example</a></li>
</ul> </div></div>
  </body>
</html>
