<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cognition.ai/blog/blockdiff">Original</a>
    <h1>Blockdiff: We built our own file format for VM disk snapshots</h1>
    
    <div id="readability-page-1" class="page"><div id="blog-post__body"> <p><em>We made it open-source here:</em> <a href="https://github.com/CognitionAI/blockdiff">https://github.com/CognitionAI/blockdiff</a></p><p>Usually, I’m a researcher working on areas like <a href="https://cognition.ai/blog/kevin-32b">RL for coding agents</a> – but one day I became annoyed by our slow VM startup times. So I took the plunge into systems engineering and built the first version of our VM hypervisor called <code>otterlink</code>. It now powers both our research &amp; all of Devin production workloads.</p><p>Devin writes and runs code in a VM environment. Why VMs instead of Docker? For untrusted user workloads we require full isolation for <a href="https://unit42.paloaltonetworks.com/container-escape-techniques/">security purposes</a>. Moreover, many realistic dev environments require using Docker (e.g. to spin up a backend service or database). <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Good luck</a> running Docker inside Docker, so we needed VMs.</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/f7e4a26fb05a4f1f8cd16950c0f9a26a97e9224c-3160x730.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/f7e4a26fb05a4f1f8cd16950c0f9a26a97e9224c-3160x730.png" alt="" loading="lazy"/></a></p><p>Compared to <a href="https://aws.amazon.com/ec2/">EC2</a>, <code>otterlink</code> was able to bring down VM startup times by about 10x. The real pain point, however, were EC2’s long snapshot times. We want a lot of flexibility (e.g. forking, rollback and suspending VMs) that all require taking disk snapshots. On EC2 taking disk snapshots usually took a whopping 30+ minutes, which would be a terrible experience for our users. With <code>otterlink</code> we were able to bring this down to just a couple of seconds – a <strong>200x speed-up</strong>.</p><p>To achieve this, we built our own file format <code>blockdiff</code> for instant block-level diffs of VM disks. Creating block-level diffs of two files is a much broader problem that goes beyond VM disks. We assumed there must be an existing open-source solution. To our surprise we couldn’t find such a tool, so we’re open-sourcing our implementation today.</p><h2>Why incremental VM snapshots?</h2><p>There are three reasons why we want incremental snapshots of VM disks:</p><p><strong>Dev environments </strong></p><p>Our customers set up their dev environment in Devin’s VM, which we save to reuse via a disk snapshot. If most customers use just 1GB of additional disk space, we don’t want all the snapshots to redundantly store the entire 15GB operating system.</p><p><strong>Sleep &amp; wake up</strong></p><p>When Devin sleeps, we want to store the current session state without making another copy of the dev environment. The limiting factor isn’t even storage cost – it’s wake up time. Transferring a 50 MB snapshot of session state is much faster than a multi-GB snapshot of the entire dev environment.</p><p><strong>Disk rollback</strong></p><p>To enable rolling back the disk during a session, we want to stack many of these incremental snapshots on top of each other.</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/4b55ddc121a302ee02a402c82d6663f6172e041f-2058x1210.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/4b55ddc121a302ee02a402c82d6663f6172e041f-2058x1210.png" alt="" loading="lazy"/></a></p><h2>Design goals</h2><p>We tried very hard to find a way to implement disk snapshotting while satisfying all these criteria:</p><p><strong>Compact </strong></p><p>The snapshot file should grow proportional to the difference between the base image and the VM disk. It’s too expensive to snapshot the entire disk.</p><p><strong>Instantaneous</strong> </p><p>Taking a snapshot should be instant and should not require significant disk I/O. We design our file format so that creating snapshot operates mostly on file metadata.</p><p><strong>Zero overhead</strong> </p><p>The VM should not experience any overhead, e.g. slower reads or writes.</p><p><strong>Simplicity</strong> </p><p>Things like this can easily break and have thousands of edge cases, so we want a solution that’s as simple as possible, to spare ourselves lots of debugging time.</p><p>The implementation of the file format is a single, few-hundred line Rust file. It stands on the shoulders of giants: most of the complexity is handled by the Linux kernel’s excellent CoW implementation in the XFS filesystem. The core idea is simple: For two files A &amp; B, <code>blockdiff</code> stores only the blocks in B that are different from blocks in A.</p><h3>Why is this hard?</h3><p>To explain the difficulty of achieving all these design goals, let’s first explain the limitations of other solutions we considered:</p><p><strong>Why not just read the files and compute a binary diff? </strong></p><p>Trying to compute a binary diff directly based on file content would be quite slow. Even on the fastest SSDs, scanning an entire 128 GB disk image can take 30-60 seconds.</p><p><strong>Why not OverlayFS? </strong></p><p>For the first few weeks of <code>otterlink</code>’s existence we used OverlayFS. However, it had two issues: It didn’t have clean support for incremental snapshots without remounting. Moreover, it created big issues when users wanted to use Docker – which would fall back to the vfs storage driver, consume 17x more storage and be 6x slower.</p><p><strong>Why not ZFS? </strong></p><p>Before we started using <code>otterlink</code>, we had implemented a version of rollback using ZFS <em>inside</em> of the VM. It had multiple limitations: For reliability reasons, we mounted ZFS only on the home dir, so it wasn’t possible to roll back system-level changes like package installs. Moreover, the snapshot logic had to live inside of the VM, visible to the user. We also briefly considered using ZFS <em>outside</em> of the VM on the hypervisor. However, we concluded that the end-to-end performance of creating &amp; transferring ZFS snapshots (<a href="https://docs.oracle.com/cd/E18752_01/html/819-5461/gbchx.html">send/recv</a>) seemed to most likely be lower than what we can achieve with <code>blockdiff</code>.</p><p><strong>What about the qcow2 file format? </strong></p><p>We didn’t deeply consider qcow2 because our hypervisor only supports raw disk images. Further below in the bonus section, we show a performance comparison that shows an example of <code>qemu-img convert</code> becoming quite slow for large files. Evidently, qcow2 doesn’t operate on metadata only (unlike <code>blockdiff</code>).</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/f89d93dfebf47b2b2b2b9199e578d52249422a00-2842x1318.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/f89d93dfebf47b2b2b2b9199e578d52249422a00-2842x1318.png" alt="" loading="lazy"/></a></p><h2>Primer: Linux filesystem concepts</h2><p>Let’s first explain two Linux concepts that are necessary to understand the rest.</p><p><strong>Sparse files</strong></p><p>Sparse files only allocate disk space for non-zero data. This is particularly helpful for VM disk images that have mostly unused space. For a sparse file, the <em>logical size</em> of the file is different from the actual <em>disk usage.</em> In this example, <code>ls -hl disk.img</code> shows the logical size which is 32GB. However, it only uses 261 MB of actual disk space – which you can see with <code>du -h</code>.</p><pre><code>ubuntu@devin-box:~$ ls -hl disk.img
-rw-r--r-- 1 ubuntu ubuntu 32G Jan  9 07:57 disk.img
ubuntu@devin-box:~$ du -h disk.img
261M    disk.img</code></pre><p>You can create an empty sparse file with <code>truncate -s</code> and then format it as an <code>ext4</code> disk image using <code>mkfs.ext4</code>:</p><pre><code>ubuntu@devin-box:~$ truncate -s 32G disk.img
ubuntu@devin-box:~$ mkfs.ext4 disk.img
mke2fs 1.46.5 (30-Dec-2021)
Discarding device blocks: done                            
Creating filesystem with 8388608 4k blocks and 2097152 inodes
Filesystem UUID: e2fdd2d5-a1a7-4be1-a9d7-6fecdb57096c
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
        4096000, 7962624

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (65536 blocks): done
Writing superblocks and filesystem accounting information: done</code></pre><p><strong>Copy-on-write (CoW)</strong></p><p>Copy-on-write is a feature supported by many modern Linux filesystems (XFS, ZFS, btrfs). Instead of immediately copying data when requested, the system shares the original data and only creates a separate copy when modifications are made to either the original or the copy. This bookkeeping happens on a block-by-block basis: blocks are the fundamental unit of storage of modern file systems and typically 4KB in size.</p><p>See the difference between copying a file with &amp; without “reflink” (another name for copy-on-write) for a 128GB disk image on a very fast NVMe SSD:</p><pre><code>ubuntu@devin-box:~$ time cp --reflink=never base.img vm1.img

real    0m24.532s
user    0m0.142s
sys     0m18.785s

ubuntu@devin-box:~$ time cp --reflink=always base.img vm2.img

real    0m0.008s
user    0m0.001s
sys     0m0.004s</code></pre><p><strong>Disk images as files</strong></p><p>For our hypervisor <code>otterlink</code>, VM disks are just files on its filesystem. Each VM disk is a CoW copy of the base disk image (e.g. the operating system) – which means that it shares all blocks by default and greedily allocates new blocks on write.</p><p>It’s important to differentiate between the filesystem inside &amp; outside of the VMs: Inside our VMs we use ext4 as the filesystem because it’s most widespread and the default on Ubuntu. Outside, the hypervisor uses XFS as its filesystem – crucially with reflink (= copy-on-write) enabled.</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/1e8678309bdefb581bc0a6696a8dfd99f3f2bccb-1696x912.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/1e8678309bdefb581bc0a6696a8dfd99f3f2bccb-1696x912.png" alt="" loading="lazy"/></a></p><h2>Serializing the diff between two files</h2><p>Let’s say we have two files: <code>base.img</code> is a disk image of our operating system and <code>vm.img</code> is a CoW copy of <code>base.img</code>. The VM is reading &amp; writing from <code>vm.img</code>. Our goal is to create a separate file <code>snapshot.bdiff</code> that stores only the blocks from <code>vm.img</code> that are <em>different</em> from <code>base.img</code>.</p><p><strong>File extent maps</strong></p><p>Our objective is to <em>only</em> operate on the filesystem metadata and to never touch the actual contents of the files. To be precise, the key lies in the file extent maps which you can get using the <a href="https://docs.kernel.org/filesystems/fiemap.html">FIEMAP</a> syscall. The <code>blockdiff</code> tool can be used to view the syscall outputs in a nicely formatted way (or alternatively use the Linux utility <code>filefrag -v disk.img</code>):</p><pre><code>blockdiff view disk.img</code></pre><p>The file extent map represents the mapping from logical blocks in the file to physical blocks on the hard drive. This mapping is grouped in extents which are sequences of blocks that are allocated contiguously. You might’ve heard of the word (de)fragmentation before: In an ideal world, every file would just be a single extent, stored as one contiguous chunk on the hard drive. However, due to fragmentation files usually end up split across multiple extents scattered throughout the disk.</p><p><strong>Reading file extent maps from Rust</strong></p><p>Using the <code>fiemap</code> crate in Rust we have a clean wrapper around the underlying Linux syscall <a href="https://docs.kernel.org/filesystems/fiemap.html">FIEMAP IOCTL</a>. Getting the extents of the target file is as easy as:<br/></p><pre><code>let mut target_extents: Vec&lt;_&gt; = fiemap::fiemap(target_file)?.collect::<result<vec<_>, _&gt;&gt;()?;</result<vec<_></code></pre><p>Each extent looks as follows:</p><pre><code>pub struct FiemapExtent {
    pub fe_logical: u64, // logical offset (in bytes)
    pub fe_physical: u64, // physical offset (in bytes)
    pub fe_length: u64, // length of extent (in bytes)
    pub fe_flags: FiemapExtentFlags,
}</code></pre><p>The logical block addresses are the location of data in the file (i.e. in our VM disk). The physical block addresses are where the data is stored on the hypervisor disk. An <em>extent</em> is a sequence of contiguous logical blocks with contiguous physical addresses. If two logical blocks from different files, point to the same physical blocks, then they are the same.</p><p><em>Exercise for the reader: Write an algorithm that takes in file extent map A &amp; B and returns a list of extents from B that are different from A. Be careful that extent boundaries are in general not aligned between A &amp; B.</em></p><p><strong>Defining a file format (.bdiff)</strong></p><p>Now the last step is to serialize this into a file:</p><pre><code>/// - Header:
///   - 8 bytes: magic string (&#34;BDIFFv1\0&#34;)
///   - 8 bytes: target file size (little-endian)
///   - 8 bytes: base file size (little-endian)
///   - 8 bytes: number of ranges (little-endian)
///   - Ranges array, each range containing:
///     - 8 bytes: logical offset (little-endian)
///     - 8 bytes: length (little-endian)
/// - Padding to next block boundary (4 KiB)
/// - Range data (contiguous blocks of data)</code></pre><p>A small header contains information about which logical block ranges the file contains. After that, it stores all <em>differing</em> blocks contiguously. When creating (or applying) blockdiffs, writing the small header is the only disk I/O that needs to happen. All the actual data can share the same physical blocks with <code>vm.img</code>, i.e. creating the rest of the file is purely a “rewiring” of file metadata.</p><p><em>Exercise for the reader: What piece in the blockdiff codebase is responsible for the fact that the range data shares the same physical blocks as <code>vm.img</code>?</em></p><h3><strong>How the tool works</strong></h3><p>Install and build the binary:</p><pre><code>git clone https://github.com/cognitionai/blockdiff &amp;&amp; cd blockdiff
cargo install</code></pre><p>Reminder that we need to be on a filesystem with reflink enabled (e.g. XFS). Now, let’s create a snapshot of <code>vm1.img</code> against the base image <code>base.img</code></p><pre><code>blockdiff create snapshot.bdiff vm1.img --base base.img</code></pre><p>We can use that snapshot, to create a new disk image <code>vm2.img:</code></p><pre><code>blockdiff apply snapshot.bdiff vm2.img --base base.img</code></pre><p>This file should be identical to <code>vm1.img</code>. We can verify this with hashes:</p><pre><code>xxhsum vm1.img
xxhsum vm2.img</code></pre><p>Since this is only a file metadata operation, creating and applying snapshots is effectively instant – no matter how large the disk images. After creating the snapshot file locally, the file still needs to be transferred to storage which happens with about 2 GB/s.</p><pre><code>On our hypervisor machines:
Reading/writing 20 GB of data: ~6.5 s
Creating 20 GB snapshot with blockdiff: ~200 ms</code></pre><h2>Bonus: Compactifying sparse files using blockdiff</h2><p>A fun little challenge that we faced while building <code>otterlink</code> is “compactifying” sparse files. If you try to upload a sparse disk image to blob storage, it will upload the entire logical size since blob storage doesn’t natively understand sparse files. So we were looking for a way to turn a sparse file of logical size X &gt; disk usage Y into a “compact” file with logical size Y = disk usage Y.</p><p>Most online sources seemed to recommend using tar which ended up being extremely slow. You would usually expect network latency to be the main bottleneck but it turned out tar would be 5x slower than the network transfer.</p><p>Despite not using qcow2 in our hypervisor itself, it turned out that <code>qemu-img convert</code> gave us exactly what we wanted: converting a raw, sparse disk image into a compact one. Moreover, it did it 5x faster than tar. To be clear, this was a random hack of ours and it isn’t what <code>qemu-img convert</code> is intended to be used for. However, with larger disks it becomes clear that even qcow2 starts being slow – it clearly isn’t a metadata-only operation. Fortunately, <code>blockdiff</code> is super fast at all sizes!</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/4fd99d0465b98efe5d2dc6228d0f11c193f40e4a-1938x500.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/4fd99d0465b98efe5d2dc6228d0f11c193f40e4a-1938x500.png" alt="" loading="lazy"/></a></p><h2>Open Questions</h2><p>Working on VM hypervisors was a fun foray into systems programming. Hopefully, this gave you a glimpse of the work we do at Cognition. Of course, there are many more open questions:</p><ol><li>How to roll back (= swap out) the disk while the VM is running?</li><li>How to implement intelligent caching of dev environment disks on hypervisors?</li><li>How to perform memory snapshots &amp; rollback in an incremental &amp; space-efficient way?</li></ol><p>If you’d like to work on these problems, reach out to us or apply <a href="https://jobs.ashbyhq.com/cognition/e8086415-62bc-4cc0-96a4-84bb56182d35">here</a>!</p> </div></div>
  </body>
</html>
