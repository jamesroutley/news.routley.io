<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rachelbythebay.com/w/2024/01/01/chrono/">Original</a>
    <h1>C&#43;&#43; time_point wackiness across platforms</h1>
    
    
<p>
It&#39;s a new year, so let&#39;s talk about some more time-related shenanigans.  
This one comes from the world of writing C++ for multiple platforms.
A couple of weeks ago, I was looking at some of my code and couldn&#39;t 
remember why it did something goofy-looking.  It&#39;s a utility function 
that runs stat() on a target path and returns the mtime as a  
std::chrono::system_clock::time_point.  This is nicer than using a 
time_t since it has sub-second precision.
</p>
<p>
The trick is getting it out of a &#34;struct stat&#34; and into that time_point.  
The integer part is simple enough: you use from_time_t on the tv_sec 
field.  But then you have to get the nanoseconds (tv_nsec) from that 
struct into your time_point.  What do you do?
</p>
<p>
The &#34;obvious&#34; answer sounds something like this: add 
std::chrono::nanoseconds(foo.tv_nsec) to your time_point.  It even works 
in a few places!  It just doesn&#39;t work everywhere.  On a Mac, it&#39;ll 
blow up with a nasty compiler error.  Good luck trying to make sense of 
this the first time you see it:
</p>
<pre class="terminal">exp/tp.cc:14:6: error: no viable overloaded &#39;+=&#39;
  tp += std::chrono::nanoseconds(2345);
  ~~ ^  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/v1/__chrono/time_point.h:65:73: note: candidate function not viable: no known conversion from &#39;duration&lt;[...], ratio&lt;[...], 1000000000&gt;&gt;&#39; to &#39;const duration&lt;[...], ratio&lt;[...], 1000000&gt;&gt;&#39; for 1st argument
    _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR_SINCE_CXX17 time_point&amp; operator+=(const duration&amp; __d) {__d_ += __d; return *this;}
</pre>
<p>
Nice, right?  It tells you that there&#39;s something wrong, but the chances 
of someone figuring that out quickly are pretty slim.  For the benefit 
of anyone else who encounters this, it&#39;s basically this: a 
system_clock::time_point on that platform isn&#39;t fine enough to represent 
nanoseconds, and they&#39;re keeping you from throwing away precision.
</p>
<p>
To make it happy, you have to jam it through a duration_cast and just 
accept the lack of precision - you&#39;re basically shaving off the last 
three digits, so instead of something like 0.111222333 seconds, your 
time will appear as 0.111222 seconds.  The nanoseconds are gone.
</p>
<p>
I assume you might find other platforms out there which don&#39;t support 
microseconds or even milliseconds, and so you&#39;d hit the same problem 
with trying to &#34;just add&#34; them to system clock time point.
</p>
<p>
At any rate, here&#39;s a little bit of demo code to show what I&#39;m talking 
about.  As-is, it&#39;ll run on Linux boxes and Macs, and it&#39;ll show 
slightly different results.
</p>
<pre class="terminal">#include &lt;stdio.h&gt;

#include &lt;chrono&gt;

int main() {
  std::chrono::system_clock::time_point tp =
      std::chrono::system_clock::from_time_t(1234567890);

  // Okay.
  tp += std::chrono::milliseconds(1);

  // No problem here so far.
  tp += std::chrono::microseconds(1);

  // But... this fails on Macs:
  // tp += std::chrono::nanoseconds(123);

  // So you adapt, and this works everywhere.  It slices off some of that
  // precision without any hint as to why or when, and it&#39;s ugly too!

  tp += std::chrono::duration_cast&lt;std::chrono::system_clock::duration&gt;(
      std::chrono::nanoseconds(123));

  // Something like this swaps the horizontal verbosity for vertical
  // stretchiness (and still slices off that precision).

  using std::chrono::duration_cast;
  using std::chrono::system_clock;
  using std::chrono::nanoseconds;

  tp += duration_cast&lt;system_clock::duration&gt;(nanoseconds(123));

  // This is what you ended up with:

  auto tse = tp.time_since_epoch();

  printf(&#34;%lld\n&#34;, (long long) duration_cast&lt;nanoseconds&gt;(tse).count());

  // Output meaning when split up:
  //
  //        sec        ms  us  ns
  //
  // macOS: 1234567890 001 001 000  &lt;-- 000 = loss of precision (246 ns)
  //
  // Linux: 1234567890 001 001 246  &lt;-- 246 = 123 + 123 (expected)
  //

  return 0;
}
</pre>
<p>
To bring this full-circle, that&#39;s why I have that ugly thing in my code 
to handle the addition of the tv_nsec field.  Without it, the code 
doesn&#39;t even compile on a Mac.
</p>
<p>
Stuff like this is why comments can be very important after the fact.
</p>

  </body>
</html>
