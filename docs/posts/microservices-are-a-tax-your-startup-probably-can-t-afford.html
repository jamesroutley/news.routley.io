<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nexo.sh/posts/microservices-for-startups/">Original</a>
    <h1>Microservices Are a Tax Your Startup Probably Can&#39;t Afford</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody"><blockquote><p><em>Why splitting your codebase too early can quietly destroy your team’s velocity — and what to do instead</em>.</p></blockquote><p><img src="https://nexo.sh/posts/microservices-for-startups/frame.jpg" alt="microservice vs monolith"/></p><p>In a startup, <strong>your survival depends on how quickly you can iterate, ship features, and deliver value to end-users</strong>. This is where the foundational architecture of your startup plays a big role; additionally, things like your tech stack and choice of programming language directly affect your team’s velocity. The wrong architecture, especially premature microservices, can substantially hurt productivity and contribute to missed goals in delivering software.</p><p>I’ve had this experience when working on greenfield projects for early-stage startups, where questionable decisions were made in terms of software architecture that led to half-finished services and <em>brittle, over-engineered and broken local setups</em>, and <strong>demoralized teams</strong> who struggle maintaining unnecessary complexity.</p><p>Before diving into specific pitfalls, here’s what you’re actually signing up for when introducing microservices prematurely:</p><p><strong>Microservices Early On: What You’re Paying For</strong></p><table><thead><tr><th>Pain Point</th><th>Real-World Manifestation</th><th>Developer Cost</th></tr></thead><tbody><tr><td>Deployment Complexity</td><td>Orchestrating 5+ services for a single feature</td><td><strong>Hours lost per release</strong></td></tr><tr><td>Local Dev Fragility</td><td>Docker sprawl, broken scripts, platform-specific hacks</td><td><strong>Slow onboarding, frequent breakage</strong></td></tr><tr><td>CI/CD Duplication</td><td>Multiple pipelines with duplicated logic</td><td><strong>Extra toil per service</strong></td></tr><tr><td>Cross-Service Coupling</td><td>“Decoupled” services tightly linked by shared state</td><td><strong>Slower changes, coordination tax</strong></td></tr><tr><td>Observability Overhead</td><td>Distributed tracing, logging, monitoring</td><td><strong>Weeks to instrument properly</strong></td></tr><tr><td>Test Suite Fragmentation</td><td>Tests scattered across services</td><td><strong>Brittle tests, low confidence</strong></td></tr></tbody></table><p>Let’s unpack why microservices often backfire early on, where they genuinely help, and how to structure your startup’s systems for speed and survival.</p><h2 id="monoliths-are-not-the-enemy">Monoliths Are Not the Enemy</h2><figure><img src="https://nexo.sh/posts/microservices-for-startups/monolith.svg" alt="Monolith architecture" width="500"/></figure><p>If you’re building some SaaS product, even a simple SQL database wrapper eventually may bring a lot of internal complexity in the way your business logic works; additionally, you can get to various integrations and background tasks that let transform one set of data to another.</p><p>With time, sometimes unnecessary features, it’s inevitable that your app may grow messy. The great thing about monoliths is: they still work. <strong>Monoliths, even when messy, keep your team focused on what matters most</strong>:</p><ul><li><strong>Staying alive</strong></li><li><strong>Delivering customer value</strong></li></ul><p>The biggest advantage of monoliths is their simplicity in deployment. Generally, such projects are built around existing frameworks — it could be Django for Python, ASP.Net for C#, Nest.js for Node.js apps, etc. When sticking to monolithic architecture, you get the biggest advantage over fancy microservices — a wide support of the open source community and project maintainers who primarily designed those frameworks to work as a single process, monolithic app.</p><p>At one real-estate startup where I led the front-end team, and occasionally consulted the backend team on technology choices, we had an interesting evolution of a Laravel-based app. What started as a small dashboard for real-estate agents to manage deals gradually grew into a much larger system.</p><p>Over time, it evolved into a feature-rich suite that handled hundreds of gigabytes of documents and integrated with dozens of third-party services. Yet, it remained built on a fairly basic PHP stack running on Apache.</p><p>The team leaned heavily on best practices recommended by the Laravel community. That discipline paid off, we were able to scale the application’s capabilities significantly while still meeting the business’s needs and expectations.</p><p>Interestingly, we never needed to decouple the system into microservices or adopt more complex infrastructure patterns. We avoided a lot of accidental complexity that way. The simplicity of the architecture gave us leverage. This echoes what others have written — like Basecamp’s take on the <a href="https://signalvnoise.com/svn3/the-majestic-monolith/">“Majestic Monolith”</a>, which lays out why simplicity is a superpower early on.</p><p>People often point out that it’s hard to make monoliths scalable, but it’s bad modularization <em>inside</em> the monolith that may bring such problems.</p><p><strong>Takeaway: A well-structured monolith keeps your team focused on shipping, not firefighting.</strong></p><h2 id="but-isnt-microservices-best-practice">But Isn’t Microservices “Best Practice”?</h2><p>A lot of engineers reach for microservices early, thinking they’re “the right way.” And sure — at scale, they can help. But in a startup, that same complexity turns into drag.</p><p>Microservices only pay off when you have real scaling bottlenecks, large teams, or independently evolving domains. Before that? You’re paying the price without getting the benefit: duplicated infra, fragile local setups, and slow iteration. For example, <strong>Segment</strong> eventually <a href="https://segment.com/blog/goodbye-microservices/">reversed their microservice split</a> for this exact reason — too much cost, not enough value.</p><p><strong>Takeaway: Microservices are a scaling tool — not a starting template.</strong></p><h2 id="where-microservices-go-wrong-especially-early-on">Where Microservices Go Wrong (Especially Early On)</h2><p>In one early-stage team I advised, the decision to split services created more PM-engineering coordination overhead than technical gain. Architecture shaped not just code, but how we planned, estimated, and shipped. That organizational tax is easy to miss — until it’s too late.</p><figure><img src="https://nexo.sh/posts/microservices-for-startups/pm-coordination.svg" alt="Coordination between teams" width="600"/></figure><p><em><strong>Diagram:</strong> Coordination overhead grows linearly with services — and exponentially when you add product managers, deadlines, and misaligned timelines.</em></p><p>Here are the most common anti-patterns that creep in early.</p><h3 id="1-arbitrary-service-boundaries">1. Arbitrary Service Boundaries</h3><figure><img src="https://nexo.sh/posts/microservices-for-startups/premature-microservice.svg" alt="Arbitrary service boundaries in microservices" width="600"/></figure><p>In theory, you often see suggestions on splitting your applications by business logic domain — users service, products service, orders service, and so on. This often borrows from Domain-Driven Design or Clean Architecture concepts — which make sense at scale, but in early-stage products, they can ossify structure prematurely, before the product itself is stable or validated. You end up with:</p><ul><li>Shared databases</li><li>Cross-service calls for simple workflows</li><li>Coupling disguised as “separation”</li></ul><p>At one project, I watched a team separating user, authentication, and authorization into separate services, which led to deployment complexity and difficulties in service coordination for any API operation they were building.</p><p>In reality, business logic doesn’t directly map to service boundaries. Premature separation can make the system more fragile and often times difficult to introduce changes quickly.</p><p>Instead, isolate bottlenecks surgically — based on real scaling pain, not theoretical elegance.</p><p>When I’ve coached early-stage teams, we’ve sometimes used internal flags or deployment toggles to simulate future service splits — without the immediate operational burden. This gave product and engineering room to explore boundaries organically, before locking in premature infrastructure.</p><p><strong>Takeaway: Don’t split by theory — split by actual bottlenecks.</strong></p><h3 id="2-repository-and-infrastructure-sprawl">2. Repository and Infrastructure Sprawl</h3><p>When working on the application, typically a next set of things is important:</p><ul><li>Code style consistency (linting)</li><li>Testing infrastructure, including integration testing</li><li>Local environment configuration</li><li>Documentation</li><li>CI/CD configuration</li></ul><p>When dealing with microservices, you need to multiply those requirements by the number of services. If your project is structured as a monorepo, you can simplify your life by having a central CI/CD configuration (when working with GitHub Actions or GitLab CI). Some teams separate microservices into separate repositories, which makes it way harder to maintain the code consistency and the same set of configurations without extra effort or tools.</p><p><img src="https://nexo.sh/posts/microservices-for-startups/tests.svg" alt="tests environments diagram"/></p><p>For a three-person team, this is brutal. Context switching across repositories and tooling adds up to the development time of every feature that is shipped.</p><h4 id="mitigating-issues-by-using-monorepos-and-a-single-programming-language">Mitigating issues by using monorepos and a single programming language</h4><p>There are various ways to mitigate this problem. For early projects, the single most important thing is — keeping your code in a monorepo. This ensures that there’s a single version of code that exists on prod, and it’s much easier to coordinate code reviews and collaborate for smaller teams.</p><p>For Node.js projects, I strongly recommend using a monorepo tool like <code>nx</code> or <code>turborepo</code>. Both:</p><ul><li>Simplify CI/CD config across subprojects</li><li>Support dependency graph-based build caching</li><li>Let you treat internal services as TypeScript libraries (via ES6 imports)</li></ul><p>These tools save time otherwise spent writing glue code or reinventing orchestration. That said, they come with real tradeoffs:</p><ul><li>Complex dependency trees can grow fast</li><li>CI performance tuning is non-trivial</li><li>You may need faster tooling (like bun) to keep build times down</li></ul><p>To summarize: Tooling like <code>nx</code> or <code>turborepo</code> gives small teams monorepo velocity — if you’re willing to invest in keeping them clean.</p><p>When developing <code>go</code>-based microservices, a good idea early in the development is to use a single <code>go</code> workspace with the <code>replace</code> directive in <code>go.mod</code>. Eventually, as the software scales, it’s possible to effortlessly separate <code>go</code> modules into separate repositories.</p><p><strong>Takeaway: A monorepo with shared infra buys you time, consistency, and sanity.</strong></p><h3 id="3-broken-local-dev--broken-velocity">3. Broken Local Dev = Broken Velocity</h3><p><strong>If it takes three hours, a custom shell script, and a Docker marathon just to run your app locally, you’ve already lost velocity.</strong></p><p>Early projects often suffer from:</p><ul><li>Missing documentation</li><li>Obsolete dependencies</li><li>OS-specific hacks (hello, Linux-only setups)</li></ul><p>In my experience, when I received projects from past development teams, they were often developed for a single operating system. Some devs preferred building on macOS and never bothered supporting their shell scripts on Windows. In my past teams, I had engineers working on Windows machines, and often it required rewriting shell scripts or fully understanding and reverse engineering the process of getting the local environment running. With time, we standardized environment setup across dev OSes to reduce onboarding friction — a small investment that saved hours per new engineer. It was frustrating — but it taught a lasting lesson on how important it is to get the code running on any laptop your new developer may be using.</p><p>At another project, a solo dev had created a fragile microservice setup, that the workflow of running Docker containers mounted to a local file system. Of course, you pay a little price for running processes as containers when your computer runs Linux.</p><p>But onboarding a new front-end dev with an older Windows laptop turned into a nightmare. They had to spin up ten containers just to view the UI. Everything broke — volumes, networking, container compatibility — and the setup very poorly documented. This created a major friction point during onboarding.</p><p>We ended up hacking together a Node.js proxy that mimicked the nginx/Docker configuration without containers. It wasn’t elegant, but it let the dev get unblocked and start contributing.</p><figure><img src="https://nexo.sh/posts/microservices-for-startups/dev-env.svg" alt="complex dev environment in microservices" width="600"/></figure><p><strong>Takeaway: If your app only runs on one OS, your team’s productivity is one laptop away from disaster.</strong></p><p><strong>Tip:</strong> Ideally, aim for <code>git clone &lt;repo&gt; &amp;&amp; make up</code> to have the project running locally. If it’s not possible, then maintaining an up-to-date README file with instructions for Windows/macOS/Linux is a must. Nowadays, there are some programming languages and toolchains that don’t work well on Windows (like OCaml), but the modern widely popular stack runs just fine on every widely used operating system; by limiting your local setup to a single operating system, it’s often a symptom of under-investment in DX.</p><h3 id="4-technology-mismatch">4. Technology Mismatch</h3><p>Beyond architecture, your tech stack also shapes how painful microservices become — not every language shines in a microservice architecture.</p><ul><li><strong>Node.js and Python:</strong> Great for rapid iteration, but managing build artifacts, dependency versions, and runtime consistency across services gets painful fast.</li><li><strong>Go:</strong> Compiles to static binaries, fast build times, and low operational overhead. More natural fit when splitting is truly needed.</li></ul><p>It’s very important to pick the right technical stack early on. If you look for performance, maybe look for the JVM and its ecosystem and ability to deploy artifacts at scale and run them in microservice-based architectures. If you do very fast iterations and prototype quickly without worrying about scaling your deployment infrastructure — you’re good with something like Python.</p><p>It’s quite often for teams to realise that there are big issues with their choice of technology that wasn’t apparent initially, and they had to pay the price of rebuilding the back-end in a different programming language (like <a href="https://blog.khanacademy.org/go-services-one-goliath-project/?utm_source=blog.quastor.org&amp;utm_medium=referral&amp;utm_campaign=khan-academy-s-migration-from-python-to-go">those guys</a> were forced to do something about legacy Python 2 codebase and migrated to Go).</p><p>But on the contrary, if you really need to, you can bridge multiple programming languages with protocols like <strong>gRPC</strong> or async message communication. And it’s often the way to go about things. When you get to the point that you want to enrich your feature set with Machine Learning functionality or ETL-based jobs, you would just separately build your ML-based infrastructure in Python, due to its rich ecosystem of domain-specific libraries, that naturally any other programming language lacks. But such decisions should be made when there’s enough head count to justify this venture; otherwise, the small team will be eternally drawn into the endless complexity of bridging multiple software stacks together.</p><p><strong>Takeaway: Match the tech to your constraints, not your ambition.</strong></p><h3 id="5-hidden-complexity-communication-and-monitoring">5. Hidden Complexity: Communication and Monitoring</h3><p>Microservices introduce an invisible web of needs:</p><ul><li>Service discovery</li><li>API versioning</li><li>Retries, circuit breakers, fallbacks</li><li>Distributed tracing</li><li>Centralized logging and alerting</li></ul><p>In a monolith, a bug might be a simple stack trace. In a distributed system, it’s “why does service A fail when B’s deployment lags C by 30 seconds?”
You would have to thoroughly invest in your observability stack. To do it “properly”, it requires instrumenting your applications in specific ways, e.g. integrating OpenTelemetry to support tracing, or relying on your cloud provider’s tools like AWS XRay if you go with a complex serverless system. But at this point, you have to completely shift your focus from application code towards building complex monitoring infrastructure that to validate whether your architecture is <strong>actually</strong> functioning in production.</p><p>Of course, some of the observability instrumentation is needed to be performed on monolith apps, but it’s way simpler than doing that in terms of the number of services in a consistent way.</p><p><strong>Tip:</strong> Understand that <strong>distributed systems <em>aren’t free.</em></strong> They’re a commitment to a whole new class of engineering challenges.</p><h2 id="when-microservices-do-make-sense">When Microservices <em>Do</em> Make Sense</h2><p>Despite the mentioned difficulties with microservices, there are times where service-level decoupling actually is very beneficial. There are cases where it definitely helps:</p><ul><li><strong>Workload Isolation</strong>: a common example for that would be in AWS best practices on using S3 event notifications — when an image gets loaded to S3, trigger an image resizing/OCR process, etc. Why it is useful: we can decouple obscure data processing libraries in a self-isolated service and make it API focused solely on image processing and generating output from the uploaded data. Your upstream clients that upload data to S3 aren’t coupled with this service, and there’s less overhead in instrumenting such a service because of its relative simplicity.</li><li><strong>Divergent Scalability Needs:</strong> — Imagine you are building an AI product. One part of the system (<strong>web API</strong>) that triggers ML workloads and shows past results isn’t resource intensive, it’s lightweight, because it interacts mostly with the database. On the contrary, ML model runs on GPUs is actually heavy to run and requires special machines with GPU support with additional configuration. By splitting these parts of the application into separate services running on different machines, you can scale them independently.</li><li><strong>Different Runtime Requirements:</strong> — Let’s say you’ve got some legacy part of code written in C++. You have 2 choices — magically convert it to your core programming language or find ways to integrate it with a codebase. Depending on the complexity of that legacy app, you would have to write glue code, implementing additional networking/protocols to establish interactions with that service, but the bottom line is — you will likely have to separate this app as a separate service due to runtime incompatibilities. I would say even more, you could write your main app in C++ as well, but because of different compiler configurations and library dependencies, you wouldn’t be able to easily compile things together as a single binary.</li></ul><p>Large-scale engineering orgs have wrestled with similar challenges. For instance, Uber’s engineering team <a href="https://www.uber.com/en-HR/blog/microservice-architecture/">documented their shift to a domain-oriented microservice architecture</a> — not out of theoretical purity, but in response to real complexity across teams and scaling boundaries. Their post is a good example of how microservices can work when you have the organizational maturity and operational overhead to support them.</p><p>At one project, that also happens to be a real-estate one, we had code from a previous team that runs Python-based analytics workloads that loads data into MS-SQL db, we found that it would be a waste to build on top of it a Django app. The code had different runtime dependencies and was pretty self-isolated, so we kept it separate and only revisited it when something wasn’t working as expected. This worked for us even for a small team, because this analytics generation service was a part that required rare changes or maintenance.</p><p><strong>Takeaway: Use microservices when workloads diverge — not just because they sound clean.</strong></p><h2 id="practical-guidance-for-startups">Practical Guidance for Startups</h2><p>If you’re shipping your first product, here’s the playbook I’d recommend:</p><ul><li><strong>Start monolithic.</strong> Pick a common framework and focus on getting the features done. All known frameworks are more than good enough to build some API or website and serve the users. Don’t follow the hype, stick to the boring way of doing things; you can thank yourself later.</li><li><strong>Single repo.</strong> Don’t bother splitting your code into multiple repositories. I’ve worked with founders who wanted to separate repos to reduce the risk of contractors copying IP — a valid concern. But in practice, it added more friction than security: slower builds, fragmented CI/CD, and poor visibility across teams. The marginal IP protection wasn’t worth the operational drag, especially when proper access controls inside a monorepo were easier to manage. For early-stage teams, clarity and speed tend to matter more than theoretical security gains.</li><li><strong>Dead-simple local setup.</strong> Make <code>make up</code> work. If it takes more, be very specific on the steps, record a video/Loom, and add screenshots. If your code is going to be run by an intern or junior dev, they’ll likely hit a roadblock, and you’ll spend time explaining how to troubleshoot an issue. I found that documenting every possible issue for every operating system eliminates time spent clarifying why certain things in a local setup didn’t work.</li><li><strong>Invest early in CI/CD.</strong> Even if it’s a simple HTML that you could just <code>scp</code> to a server manually, you could automate this and rely on source control with CI/CD to do it. When the setup is properly automated, you just forget about your continuous integration infrastructure and focus on features. I’ve seen many teams and founders when working with outsourced teams often be cheap on CI/CD, and that results in the team being demoralized and annoyed by manual deployment processes.</li><li><strong>Split surgically.</strong> Only split when it clearly solves a painful bottleneck. Otherwise, invest in modularity and tests inside the monolith — it’s faster and easier to maintain.</li></ul><p>And above all: <strong>optimize for developer velocity.</strong></p><p><strong>Velocity is your startup’s oxygen.</strong> Premature microservices leak that oxygen slowly — until one day, you can’t breathe.</p><p><strong>Takeaway: Start simple, stay pragmatic, and split only when you must.</strong></p><h2 id="if-you-go-with-a-microservice-based-approach">If you go with a microservice-based approach</h2><p>I had micro-service-based projects created earlier than they should have been done, and here are the next recommendations that I could give on that:</p><ul><li><strong>Evaluate your technical stack</strong> that powers your micro-service-based architecture. Invest in developer experience tooling. When you have service-based separation, you now need to think about automating your microservice stack, automating config across both local and production environments. In certain projects, I had to build a separate CLI that does administrative tasks on the monorepository. One project I had contained 15-20 microservice deployments, and for the local environment, I had to create a cli-tool for generating docker-compose.yml files dynamically to achieve seamless one-command start-up for the regular developer.</li><li><strong>Focus on reliable communication protocols</strong> around service communication. If it’s async messaging, make sure your message schemas are consistent and standardized. If it’s REST, focus on OpenAPI documentation. Inter-service communication clients must implement many things that don’t come out-of-the-box: retries with exponential backoff, timeouts. A typical bare-bones gRPC client requires you to manually factor those additional things to make sure you don’t suffer from transient errors.</li><li><strong>Ensure that your unit, integration testing, and end-to-end testing setup</strong> is stable and scales with the amount of service-level separations you introduce into your codebase.</li><li>On smaller projects that use micro-service-based workloads, you would likely default to a shared library with common helpers for instrumenting your observability, communication code in a consistent way. An important consideration here — <strong>keep your shared library as small as possible</strong>. Any major change forces a rebuild across all dependent services — even if unrelated.<figure><img src="https://nexo.sh/posts/microservices-for-startups/shared-dep.svg" alt="shared library dependency" width="600"/></figure></li><li><strong>Look into observability earlier on.</strong> Add structured-JSON logs and create various correlation IDs for debugging things once your app is deployed. Even basic helpers that output rich logging information (until you instrumented your app with proper logging/tracing facilities) often save time figuring out flaky user flows.</li></ul><p>To summarize: if you’re still going for microservices, you should beforehand understand the tax you’re going to pay in terms of additional development time and maintenance to make the setup workable for every engineer in your team.</p><p><strong>Takeaway: If you embrace complexity, invest fully in making it manageable.</strong></p><h2 id="conclusion">Conclusion</h2><p><strong>Premature microservices are a tax you can’t afford. Stay simple. Stay alive.</strong> Split only when the pain makes it obvious.</p><p><strong>Survive first. Scale later. Choose the simplest system that works — and earn every layer of complexity you add.</strong></p><ul><li><a href="https://martinfowler.com/bliki/MonolithFirst.html">Monolith First</a> — Martin Fowler</li><li><a href="https://signalvnoise.com/svn3/the-majestic-monolith">The Majestic Monolith</a> — DHH / Basecamp</li><li><a href="https://segment.com/blog/goodbye-microservices">Goodbye Microservices: From 100s of problem children to 1 superstar</a> — Segment Eng.</li><li><a href="https://shopify.engineering/deconstructing-monolith-designing-software-maximizes-developer-productivity">Deconstructing the Monolith</a> — Shopify Eng.</li><li><a href="https://www.uber.com/blog/microservice-architecture/">Domain‑Oriented Microservice Architecture</a> — Uber Eng.</li><li><a href="https://blog.khanacademy.org/go-services-one-goliath-project/">Go + Services = One Goliath Project</a> — Khan Academy</li></ul></div></div>
  </body>
</html>
