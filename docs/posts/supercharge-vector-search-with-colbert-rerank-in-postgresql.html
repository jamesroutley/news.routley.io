<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.vectorchord.ai/supercharge-vector-search-with-colbert-rerank-in-postgresql">Original</a>
    <h1>Supercharge vector search with ColBERT rerank in PostgreSQL</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content-parent"><div id="post-content-wrapper"><p>Traditional vector search methods typically employ sentence embeddings to locate similar content. However, generating sentence embeddings through pooling token embeddings can potentially sacrifice fine-grained details present at the token level. <a target="_blank" href="https://github.com/stanford-futuredata/ColBERT">ColBERT</a> overcomes this by representing text as token-level multi-vectors rather than a single, aggregated vector. This approach, leveraging contextual late interaction at the token level, allows ColBERT to retain more nuanced information and improve search accuracy compared to methods relying solely on sentence embeddings.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1736414634494/e50e0b90-74da-49d9-813d-5c5ec00317ee.jpeg?auto=compress,format&amp;format=webp" alt="ColBERT structure (from the original paper)"/></p>
<p>As illustrated in the above image, ColBERT encodes each document/query into a list of token vectors and computes the MaxSim during the query time.</p>
<p>Token-level late interaction requires more computing power and storage. This makes using ColBERT search in large datasets challenging, especially when low latency is important.</p>
<p>One possible solution is to combine sentence-level vector search with token-level late interaction rerank, which leverages the efficiency of approximate vector search and the high quality of multi-vector similarity search.</p>
<p>The multi-vector approach is not limited to pure text retrieval tasks; it can also be used in visual document understanding. For multimodal retrieval models, state-of-the-art models like <a target="_blank" href="https://huggingface.co/vidore/colpali-v1.3">ColPali</a> and <a target="_blank" href="https://huggingface.co/vidore/colqwen2-v1.0-merged">ColQwen</a> directly encode document images into multi-vectors and demonstrate stronger performance compared to OCR-to-text approaches.</p>
<p>This blog will demonstrate using the PostgreSQL extension <a target="_blank" href="https://github.com/tensorchord/VectorChord/">VectorChord</a> and pgvector with ColBERT rerank.</p>
<h2 id="heading-tutorial">Tutorial</h2>
<p>Assume we already have the documents, let’s create a table to store all of them:</p>
<pre><code><span>import</span> psycopg
<span>from</span> pgvector.psycopg <span>import</span> register_vector

<span><span>class</span> <span>PgClient</span>:</span>
    <span><span>def</span> <span>__init__</span>(<span>self, url: str, dataset: str, sentence_emb_dim: int, token_emb_dim: int</span>):</span>
        self.dataset = dataset
        self.sentence_emb_dim = sentence_emb_dim
        self.token_emb_dim = token_emb_dim
        self.conn = psycopg.connect(url, autocommit=<span>True</span>)
        <span>with</span> self.conn.cursor() <span>as</span> cursor:
            cursor.execute(<span>&#34;CREATE EXTENSION IF NOT EXISTS vchord CASCADE;&#34;</span>)
        register_vector(self.conn)

    <span><span>def</span> <span>create</span>(<span>self</span>):</span>
        <span>with</span> self.conn.cursor() <span>as</span> cursor:
            cursor.execute(
                <span>f&#34;CREATE TABLE IF NOT EXISTS <span>{self.dataset}</span>_corpus &#34;</span>
                <span>&#34;(id INT BY DEFAULT AS IDENTITY PRIMARY KEY, text TEXT, &#34;</span>
                <span>f&#34;emb vector(<span>{self.sentence_emb_dim}</span>), embs vector(<span>{self.token_emb_dim}</span>)[]);&#34;</span>
            )
</code></pre>
<p>Here we created a table with sentence-level embedding and token-level embeddings.</p>
<p>There are numerous embedding APIs and <a target="_blank" href="https://huggingface.co/spaces/mteb/leaderboard">open-source models</a>. You can choose the one that fits your use case.</p>
<p>For token-level embedding:</p>
<pre><code><span>from</span> colbert.infra <span>import</span> ColBERTConfig
<span>from</span> colbert.modeling.checkpoint <span>import</span> Checkpoint

<span><span>class</span> <span>TokenEncoder</span>:</span>
    <span><span>def</span> <span>__init__</span>(<span>self</span>):</span>
        self.config = ColBERTConfig(doc_maxlen=<span>220</span>, query_maxlen=<span>32</span>)
        self.checkpoint = Checkpoint(
            <span>&#34;colbert-ir/colbertv2.0&#34;</span>, colbert_config=self.config, verbose=<span>0</span>
        )

    <span><span>def</span> <span>encode_doc</span>(<span>self, doc: str</span>):</span>
        <span>return</span> self.checkpoint.docFromText([doc], keep_dims=<span>False</span>)[<span>0</span>].numpy()

    <span><span>def</span> <span>encode_query</span>(<span>self, query: str</span>):</span>
        <span>return</span> self.checkpoint.queryFromText([query])[<span>0</span>].numpy()
</code></pre>
<p>ColBERT model produces 128-dim vectors by default.</p>
<p>To insert the data:</p>
<pre><code><span><span>class</span> <span>PgClient</span>:</span>
    ...
    <span><span>def</span> <span>insert</span>(<span>self, documents: list[str]</span>):</span>
        <span>with</span> self.conn.cursor() <span>as</span> cursor:
            <span>for</span> doc <span>in</span> tqdm(documents):
                sentence_emb = sentence_encoder.encode_doc(doc)
                token_embs = [emb <span>for</span> emb <span>in</span> token_encoder.encode(doc)]
                cursor.execute(
                    <span>f&#34;INSERT INTO <span>{self.dataset}</span>_corpus (text, emb, embs) VALUES (%s, %s, %s)&#34;</span>
                    (doc, sentence_emb, token_embs)
                )
</code></pre>
<p>For the vector search part, we can use VectorChord to build a high-performance RaBitQ index:</p>
<pre><code><span><span>class</span> <span>PgClient</span>:</span>
    ...
    <span><span>def</span> <span>index</span>(<span>self, num_doc: int, workers: int</span>):</span>
        n_cluster = <span>1</span> &lt;&lt; math.ceil(math.log2(num_doc ** <span>0.5</span> * <span>4</span>))
        config = <span>f&#34;&#34;&#34;
        residual_quantization = true
        [build.internal]
        lists = [<span>{n_cluster}</span>]
        build_threads = <span>{workers}</span>
        spherical_centroids = false
        &#34;&#34;&#34;</span>
        <span>with</span> self.conn.cursor() <span>as</span> cursor:
            cursor.execute(<span>f&#34;SET max_parallel_maintenance_workers TO <span>{workers}</span>&#34;</span>)
            cursor.execute(<span>f&#34;SET max_parallel_workers TO <span>{workers}</span>&#34;</span>)
            cursor.execute(
                <span>f&#34;CREATE INDEX <span>{self.dataset}</span>_rabitq ON <span>{self.dataset}</span>_corpus USING &#34;</span>
                <span>f&#34;vchordrq (emb vector_l2_ops) WITH (options = $$<span>{config}</span>$$)&#34;</span>
            )
</code></pre>
<p>To speed up the indexing building process, we can utilize the external centroids build. Check “<a target="_blank" href="https://blog.vectorchord.ai/benefits-and-steps-of-external-centroids-building-in-vectorchord">Benefits and Steps of External Centroids Building in VectorChord</a>” for more details.</p>
<p>Now, we can query the PostgreSQL:</p>
<pre><code><span><span>class</span> <span>PgClient</span>:</span>
    ...
    <span><span>def</span> <span>query</span>(<span>self, doc: str, topk: int</span>):</span>
        sentence_emb = sentence_encoder.encode_query(doc)
        <span>with</span> self.conn.cursor() <span>as</span> cursor:
            cursor.execute(
                <span>f&#34;SELECT id, text FROM <span>{self.dataset}</span>_corpus ORDER BY emb &lt;-&gt; %s LIMIT <span>{topk}</span>&#34;</span>
            )
            res = cursor.fetchall()
        <span>return</span> res
</code></pre>
<p>To support <code>MaxSim</code> rerank, we’ll need to create a function:</p>
<pre><code><span><span>class</span> <span>PgClient</span>:</span>
    <span><span>def</span> <span>__init__</span>(<span>self, url: str, dataset: str, sentence_emb_dim: int, token_emb_dim: int</span>):</span>
        ...
        self.conn.execute(<span>&#34;&#34;&#34;
            CREATE OR REPLACE FUNCTION max_sim(document vector[], query vector[]) RETURNS double precision AS $$
            WITH queries AS (
                SELECT row_number() OVER () AS query_number, * FROM (SELECT unnest(query) AS query)
            ),
            documents AS (
                SELECT unnest(document) AS document
            ),
            similarities AS (
                SELECT query_number, document &lt;=&gt; query AS similarity FROM queries CROSS JOIN documents
            ),
            max_similarities AS (
                SELECT MAX(similarity) AS max_similarity FROM similarities GROUP BY query_number
            )
            SELECT SUM(max_similarity) FROM max_similarities
            $$ LANGUAGE SQL
        &#34;&#34;&#34;</span>)
</code></pre>
<p>Now, we can rerank the documents retrieved by vector search:</p>
<pre><code><span><span>class</span> <span>PgClient</span>:</span>
    <span><span>def</span> <span>rerank</span>(<span>self, query: str, ids: list[int], topk: int</span>):</span>
        token_embs = [emb <span>for</span> emb <span>in</span> token_encoder.encode_query(query)]
        <span>with</span> self.conn.cursor() <span>as</span> cursor:
            cursor.execute(
                <span>f&#34;SELECT id, text FROM <span>{self.dataset}</span>_corpus WHERE id = ANY(%s) ORDER BY &#34;</span>
                <span>f&#34;max_sim(embs, %s) DESC LIMIT <span>{topk}</span>&#34;</span>
                (ids, token_embs)
            )
            res = cursor.fetchall()
        <span>return</span> res
</code></pre>
<h2 id="heading-evaluation">Evaluation</h2>
<p>We have tested this method on several <a target="_blank" href="https://github.com/beir-cellar/beir">BEIR</a> datasets. Here are the results:</p>
<div>
<table>
<thead>
<tr>
<td>Dataset</td><td>Search NDCG@10</td><td>Rerank NDCG@10</td></tr>
</thead>
<tbody>
<tr>
<td>fiqa</td><td>0.23211</td><td>0.3033</td></tr>
<tr>
<td>quora</td><td>0.31599</td><td>0.3934</td></tr>
</tbody>
</table>
</div><p>This shows that ColBERT reranking can significantly enhance the results of vector searches.</p>
<p>All the related benchmark codes can be found <a target="_blank" href="https://github.com/kemingy/vectorchord-colbert">here</a>.</p>
<h2 id="heading-future-work">Future work</h2>
<p>Vector search and full-text search with ColBERT rerank can further improve performance. We’re also working on the <a target="_blank" href="https://github.com/tensorchord/VectorChord-bm25">PostgreSQL BM25 extensions</a>. Stay tuned.</p>
<h3 id="heading-references">References</h3>
<ul>
<li><p><a target="_blank" href="https://www.answer.ai/posts/colbert-pooling.html">https://www.answer.ai/posts/colbert-pooling.html</a></p>
</li>
<li><p><a target="_blank" href="https://github.com/stanford-futuredata/ColBERT">https://github.com/stanford-futuredata/ColBERT</a></p>
</li>
<li><p><a target="_blank" href="https://github.com/tensorchord/vectorChord/">https://github.com/tensorchord/vectorChord/</a></p>
</li>
<li><p><a target="_blank" href="https://github.com/tensorchord/VectorChord-bm25">https://github.com/tensorchord/VectorChord-bm25</a></p>
</li>
<li><p><a target="_blank" href="https://huggingface.co/blog/manu/colpali">https://huggingface.co/blog/manu/colpali</a></p>
</li>
</ul>
</div></div></div>
  </body>
</html>
