<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://medium.com/mindsoft/rats-in-doom-eb6c52c73aca">Original</a>
    <h1>Rats in Doom VR</h1>
    
    <div id="readability-page-1" class="page"><div><div><p id="289e"><strong><em>TL;DR</em></strong> I built a VR setup for rodents from scratch and trained three rats in an automated fashion, without manual intervention, to traverse a corridor rendered in the DOOM II engine. Although I did implement the mechanisms to further train rats to shoot monsters in-game, I lacked the time to actually reinforce the behavior. The promise of the project is a relatively cheap (&lt;$2000) VR setup that automatically trains rodents to traverse 3D environments without restraining them too much, while refraining from surgical procedures to provide the least stressful circumstances for them. Rodent VR rigs have had their presence in neuroscience experiments in the past; I hope to see more such studies to come! The following project aims to ease those experiments by automating some tasks of the experimenter.</p><p id="cf52"><em>This article describes the implementation of the automated training procedure proposed in my previous post: </em><a rel="noopener" href="https://medium.com/mindsoft/rats-play-doom-eb0d9c691547"><em>A Neuroengineer’s Guide on Training Rats to Play Doom</em></a></p><p id="ca8b">As a personal side project, building the rig took around 4 months and costed me more than two times of what the final product amounts to. Several rodent VR setup designs have been developed [1], however, most are difficult to get right or they require special equipment to assemble. As far as tools go, I had access to a medium-sized <a href="https://www.creality3dofficial.com/products/creality-ender-3-pro-3d-printer" target="_blank" rel="noopener ugc nofollow">3D printer</a>, a soldering iron, a drilling machine and a screwdriver set. Wielding my Prime membership, I received 99 individual Amazon orders — special thanks to Yuniel for relentlessly delivering these packages to the lab!</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*PpvwjAGRyCvmzQ9CEfnaEQ.png?q=20" width="700" height="640" role="presentation"/></p><p><img alt="" width="700" height="640" role="presentation"/></p></div></div></div></div></figure><p id="aab4">Rodent VR setups usually consists of a (polystyrene) ball, on which the rat runs suspended in a harness, along with motion sensors tracking the movement of the ball. Either a screen or a projector displays the virtual environment. The ball could be levitated on airflow or just be rolled on ball bearings. I tried both options and the latter prevailed with its robustness despite the higher friction it imposed. I housed the whole system on an aluminum frame precut and ordered from Misumi.</p><figure><div></div></figure><p id="3c45">Rats’ fields of vision extends to 300°; covering the whole field is not necessary [2, 3], but wrapping a wider screen around the animal ups the immersion factor. To simplify the setup, I avoided using projectors and went for a very curved commercial Dell screen that somewhat hugged the visual space.</p><p id="d814">Custom parts (attachments, items holding electrical components, mechanical contraptions) were 3D designed in Blender then printed. Dozens of versions were tested to arrive at the final rig.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*MCrmUaAw06bp113GIOYKMw.png?q=20" width="700" height="618" role="presentation"/></p><p><img alt="" width="700" height="618" role="presentation"/></p></div></div></div></div></figure><p id="6bc1">A <a href="https://flexsealproducts.com/products/flex-seal-liquid?variant=32929853538413" target="_blank" rel="noopener ugc nofollow">rubber</a>-coated 16&#34; (40 cm) foam ball was held in place by three <a href="https://www.amazon.com/TruePower-Roller-Ball-Transfer-Bearings/dp/B009KASQZW" target="_blank" rel="noopener ugc nofollow">ball bearings</a> on the bottom, two at the front and one on the left side. The guiding principles for designing the base were as follows: 1) the ball bearings should contact the spherical treadmill at a 90° angle to minimize friction, 2) the arms should be long enough to support the treadmill from the sides preventing it from rolling off the base, 3) the base should be able to hold up foam balls of different sizes without manual redesign — at this point the diameter of the ball had not been fixed.</p><p id="98f5">To meet all these requirements I wrote a <a href="https://drive.google.com/file/d/14oJaiRonh6eouMhLuJVtvOE0g5tcCg76/view?usp=sharing" target="_blank" rel="noopener ugc nofollow">Blender Python script</a> that spawned the arms in the 3D editor just in the right position above the base, hugging the ball, given the dimensions of the foam ball, of the 3D printer and of the ball bearings. After changing the radius of the spherical treadmill in the script, a new arm arrangement can be spawned at the right size, angle and position. The arms were printed separately from the base to maximize their length.</p><p id="92f2">The motion laser sensors were fashioned from gutted <a href="https://www.amazon.com/HEWLETT-PACKARD-3-Button-USB-Laser-Mouse/dp/B00971Y1N0" target="_blank" rel="noopener ugc nofollow">computer mice</a>, and were installed at the front and left side to cover all three axes of ball movement. The motion detection range of the laser mouse maxed out around 1 cm. To account for irregularities on the surface of the foam ball, frontal bearings were positioned on <a href="https://www.amazon.com/Neewer-centimeters-Adjustable-Compatible-Microphone/dp/B08YN2XNM5" target="_blank" rel="noopener ugc nofollow">magic arms</a> distancing the ball from the front sensor. Magic arms further held the motion sensors and the reward stations, which made all this equipment easy to (re)position.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/56/1*ynaUKPkIb_1Ty-kABddZxA.png?q=20" width="700" height="738" role="presentation"/></p><p><img alt="" width="700" height="738" role="presentation"/></p></div></div></div></div></figure><p id="7f0d">Rats were trained to move on the spherical treadmill by means of operant conditioning, which required a feedback mechanism that reinforces the desired behavior. To deliver positive feedback, I chose feeding them sugary water, as it’s relatively simple to dose in small amounts (tens of <em>μL</em>): took a <a href="https://www.theleeco.com/products/electro-fluidic-systems/solenoid-valves/control-valves/lhl-series-solenoid-valves/3-port/ported/" target="_blank" rel="noopener ugc nofollow">precision solenoid valve</a>, gravity-fed it (check the above intro video from 0:42) and drove it with a microcontroller. The above shown 3D printed tiny bowl functioned as a buffer for any excess water, while Quidditch goal posts held the tubing tight.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*Py3kdiuGD_5hUP8oPeJMIw.png?q=20" width="700" height="356" role="presentation"/></p><p><img alt="" width="700" height="356" role="presentation"/></p></div></div></div></div></figure><p id="0fd7">The two larger tubes conducted air puffs from the <a href="https://www.allelectronics.com/item/pmp-23/micro-air-pump-12-volt-dc/1.html" target="_blank" rel="noopener ugc nofollow">12V pumps</a>. Air puffs in the face or the butt are often used as negative feedback in rodent VR experiments [4]; specifically to signal when the animal hits the wall, from either side, while running. The foam padding were added to absorb the vibration of the pumps, which reduced noise. Reducing noise across the setup is in general of importance, as rodents can easily get startled by sounds of sudden onset. Although the air pumps worked fine, in the lack of time and out of caution, I refrained from applying air puffs in my experiments not to discourage rats from staying on the treadmill.</p><p id="d929">The equipment showed so far is part of most VR setups and is paramount for rodent VR experiments to work in the scenario of operant conditioning. The novel aspect of this project lies in the automatic training of rodents to traverse and perform actions in virtual spaces.</p><h2 id="cbb8">Train to Walk</h2><p id="7245">Training rats to walk in VR is conceptually simple and has been done in a manual method: if the animal is stuck, roll the ball in the right direction, forcing the rat to take steps, then provide positive feedback to reinforce this behavior. When done manually, the experimenter has to roll the ball by hand before triggering the reward. Conversely, an automatic solution necessitates rolling the ball along two axes by the use of motors: forward-backward and left-right.</p><figure><div></div></figure><p id="d7d2">The video above first shows how the motor attempts (but fails) to rotate the ball: a <a href="https://www.amazon.com/Electric-Actuator-lbs-12V-High-Speed-sec-Weight/dp/B07ZJ4R2NR?th=1" target="_blank" rel="noopener ugc nofollow">linear actuator</a> was extended until the <a href="https://www.adafruit.com/product/3766" target="_blank" rel="noopener ugc nofollow">wheel</a> reached the ball, the <a href="https://www.adafruit.com/product/3801" target="_blank" rel="noopener ugc nofollow">motor </a>rotated the wheel for two seconds before the actuator was detracted. Failing to roll the ball in this case was both a bug and a feature. If the rat put all its weight lying on the treadmill, the traction of the silicone wheel on the rubber-coated polystyrene ball turned out to be insufficient. The rat was not entirely forced to walk, only encouraged, which, I can anecdotally say, eased the acclimation process.</p><p id="d824">The second portion of the video demonstrates the 2D movement training. As you may notice, the rat was slightly confused about the second motor, which kept turning him to the right to face the exit sign — this was the first time Romero (that’s his name) ever encountered the enforced left-right rotation of the ball.</p><p id="9e5b">The last part of the video (from 0:32, turn the audio on) records a later part of the training, where Romero already learnt to take advantage of the motor and roll with it. He also just began to walk on his own there.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*tFzu92dFqvR5dkHi8rx7Zg.png?q=20" width="700" height="439" role="presentation"/></p><p><img alt="" width="700" height="439" role="presentation"/></p></div></div></div></div></figure><p id="3878">Placing a spring between the actuator and the wheel helped to maintain the traction of the wheel on the imperfect spherical surface — the distance from the center and the surface of the ball varied. A spring allowed for such variance without pushing the ball too much to the side.</p><h2 id="3785">Train to Shoot</h2><p id="ee4a">Shooting is the in-game action that has to be performed in the context of a monster being in front of the player. Consistently training such a behavior is difficult manually, as monsters may appear in different spots and in large numbers. One could incrementally train a rat to shoot a single paralyzed monster, then a moving one, then multiple ones moving around, but again, to consistently induce the action associated with shooting, manual interventions would just not cut it.</p><p id="13bb">Let’s say the behavioral action associated with shooting is the rat raising its body by extending the front legs (a form of <a href="https://imgur.com/vl3CIn7" target="_blank" rel="noopener ugc nofollow">rearing</a>). It should be trained to perform this action every time a monster appears ahead. Every such time, the experimenter could <em>manually </em>lift the rat by the harness, initiate an in-game shot and release some sugary water for positive feedback; the timing of all these operations are essential to shorten the training period as much as possible, and to avoid confusing the animal by giving inconsistent feedback. So, how could this be automated?</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/46/1*KBBS_sLD_FvEQSjCkccXuA.png?q=20" width="700" height="898" role="presentation"/></p><p><img alt="" width="700" height="898" role="presentation"/></p></div></div></div></div></figure><p id="1d9f">A <a href="https://www.amazon.com/Stock-Show-Outdoor-Squirrel-Accessory/dp/B07G23Y57W" target="_blank" rel="noopener ugc nofollow">cute little harness</a> (selected from a set of 5) was attached to a 3D printed arm allowing for vertical movement. A <a href="https://www.adafruit.com/product/3992" target="_blank" rel="noopener ugc nofollow">solenoid push-pull actuator</a>, secured to the base of the arm, served as the training mechanism to teach the shooting behavior. Just above the actuator a button sat. When pushed, either by activating, pulling up the solenoid itself, or by the animal raising its posture and thus lifting the inactive solenoid shaft, an in-game shot was discharged. Note: the actuator was activated using pulse-width modulation to drive a more gradual pull not to scare the animal.</p><p id="5a58">Simply put, the training procedure would go as follows: the rat walks into a monster → the software detects that the monster is in the proximity of the player (and for now, let’s assume that the player is facing it) → initially the rat has no idea what to do in this situation, so the training software activates the push-pull solenoid lifting the animal slightly upwards → the head of the actuator then touches the button → monster gets shot down → reward in the form of sugary water is released to reinforce the behavior.</p><p id="c4a0">Ideally, the rat learns on its own to rear up the body in front of every monster, which then lifts the shaft of the solenoid, pushes the button, the monster gets shot and the reward is dispensed.</p><p id="bd4c">Two <a href="https://www.amazon.com/ELEGOO-Board-ATmega328P-ATMEGA16U2-Compliant/dp/B01EWOE0UU/" target="_blank" rel="noopener ugc nofollow">Arduino microcontrollers</a> managed all the electronics: 2 <em>×</em> (linear actuator + motor), 2 × air pump, a solenoid water valve, a push-pull solenoid, and the fire button. The motion sensors and the microcontrollers were wired to the PC via USB, each Arduino board communicating on a serial port, while motion data was received through the input event interface of Linux using the Python <a href="https://python-evdev.readthedocs.io/en/latest/" target="_blank" rel="noopener ugc nofollow">evdev</a> library.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*GsgzEl9S2H1bxg5TBWeESg.png?q=20" width="700" height="331" role="presentation"/></p><p><img alt="" width="700" height="331" role="presentation"/></p></div></div></div></div></figure><p id="a804">I wanted to push as much logic to the PC (Python) as possible without compromising on the response time of the setup. The PC sent small packages to the microcontrollers as commands: first byte being the message type (e.g. extend the linear actuator, dispense sugary water, etc.), then the packet load (how long to extend the actuator in seconds, amount of water to dispense in <em>μL</em>). The experiment described below was implemented in a <a href="https://gist.github.com/csiki/dd2c990aa2cbdf5533d0daba6fb04b8b" target="_blank" rel="noopener ugc nofollow">single Python script</a> (&lt;300 lines), concurrently receiving movement and shot info, sending out commands to the devices and interfacing the game.</p><p id="85ab">I designed a map containing a corridor, an exit door and an exit button in <a href="http://www.doombuilder.com/" target="_blank" rel="noopener ugc nofollow">Doom Builder 2</a>. The door and the button were activated on touch. Later to train the shooting behavior, I added an <a href="https://doomwiki.org/wiki/Imp" target="_blank" rel="noopener ugc nofollow">imp</a> monster around 1/3rd of the corridor. The assets were taken from the first map of Doom II, <a href="https://doom.fandom.com/wiki/MAP01:_Entryway_(Doom_II)" target="_blank" rel="noopener ugc nofollow">Entryway</a>. As I described in my <a rel="noopener" href="https://medium.com/mindsoft/rats-play-doom-eb0d9c691547">previous post</a>, this map enabled the first stage of training only requiring forward movement, and shooting a paralyzed, docile monster.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*IBFvk3On787hOJdR6Ggh7w.png?q=20" width="700" height="215" role="presentation"/></p><p><img alt="" width="700" height="215" role="presentation"/></p></div></div></div></div></figure><p id="a32b">As the player pushed the button, the animal was teleported to the beginning of the map to start all over again. A <a href="https://gist.github.com/csiki/aa344f7c119e216580c8c5d2d3a916ab" target="_blank" rel="noopener ugc nofollow">Doom (ACS) script</a> registered the player’s position along the corridor and the number of monsters it killed, and exposed all this info to the Python experiment script.</p><p id="33e1">I ran Doom, accessed game states and sent game actions using the <a href="https://github.com/mwydmuch/ViZDoom" target="_blank" rel="noopener ugc nofollow">ViZDoom</a> engine. ViZDoom was primarily built for training reinforcement learning algorithms, but its seamless Doom API made it perfect for the job. From motion data that evdev supplied, I calculated the speed of movement over time, scaled and relayed it to ViZDoom at every iteration of the game to translate rat steps into in-game steps. ViZDoom in return exposed the game state, sprinkled with some extra variables that were defined in the ACS script.</p><p id="afac">I received three 8-week-old male Long Evans rats, Romero, Carmack and Tom. Romero was fearless (more like thrill-seeking) and loved grapes. Carmack was a real architect building around its home keeping it tidy; he was fond of bananas. Tom began shy, but held the most surprises in learning performance.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*lDZXnz5t-mnlA6Sb_vApkg.png?q=20" width="700" height="386" role="presentation"/></p><p><img alt="" width="700" height="386" role="presentation"/></p></div></div></div></div></figure><p id="de02">I spent around 6 weeks with them during their (and my) night cycle, ~1 hour with each daily. It took two weeks to habituate them to me — this process could be accelerated, I was just a little slow with them. One week to teach them to stay put while I was dressing them; I also tested multiple harnesses and their combinations during this time.</p><figure><div role="button" tabindex="0"><div><div><div><p><img alt="" src="https://miro.medium.com/max/60/1*FGHTqNko5Nbftt2d5TRjLw.png?q=20" width="500" height="295" role="presentation"/></p><p><img alt="" width="500" height="295" role="presentation"/></p></div></div></div></div></figure><p id="5835">I spend another week of habituation to the spherical treadmill and feeding from the bowl. First, I kept filling the bowl with baby food to make them stay on top of the ball. Baby food has a more compelling scent to recognize than sugary water; also, I could avoid instantiating water restriction for this period.</p><p id="cc75">For the remaining 11 days I ran the actual VR Doom training. Romero outperformed the others in general, though Tom was the first to actually walk on its own without the help of the motorized ball roller. I had two sessions to train shooting with Romero, who was pretty confident running in VR at that point. Two sessions were far from enough; getting stuck into the monster and pulled up by the solenoid confused him. In the last part of the video below, you can hear the push-pull solenoid activating, and see it pulling Romero slightly upwards before the shots are discharged.</p><figure><div></div></figure><p id="ce36">Amid the VR training I kept improving the software simultaneously with the hardware, which inconsistency most probably interfered with the learning progress, but was necessary to finalize the setup.</p><p id="361a">At this point you might rightfully ask, why? Playing games, whether a human does it or rats, involves the interplay of a wide variety of cognitive processes. A rat, being stationary relative to the setup, can express a wide variety of behavior, while also being recorded by a neural interface. Thus, movement and complex in-game actions can be correlated with neural activity in a virtual, yet more natural, lively set of experiments, than in simple maze setups for instance.</p><p id="6844">Computer games, virtual worlds are abundant and easy to make. Once we can reliably train animals to play games, designing an experiment becomes a software problem instead of an often costly hardware problem. However, consistent training of animals to perform complex, context-dependent actions in VR happens to be pretty difficult. This project was an attempt to automate this process. However, even after 11 days of VR training, I had to be present and help the rats to get on or off the ball, as they desired. Although their training was software-based, it still required my presence.</p><p id="2001">Unfortunately, I did not have the time to carry out the experiment in its entirety — I gave myself a hard deadline out of personal reasons. Rats lacking the ability to shoot is an obvious major limitation of this study. In hindsight, I would much rather implement an already tested response method to train shooting, like a <a href="https://youtu.be/h7_hwgRWtOE?t=35" target="_blank" rel="noopener ugc nofollow">nose-poking system</a>, than the button-pushing contraption. Although the mechanisms to train left-right turning was present, I did not have time to train movements with turns included, so in most sessions only one axis of motion was registered.</p></div></div></div>
  </body>
</html>
