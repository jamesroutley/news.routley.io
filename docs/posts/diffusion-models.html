<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://andrewkchan.dev/posts/diffusion.html">Original</a>
    <h1>Diffusion Models</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p id="toc"><h4>Contents</h4></p>
      
      <h2>Notes on the theory behind models like Stable Diffusion and their applications.</h2>
      <dt-byline></dt-byline>
      <p>
        I spent 2022 learning to draw and was blindsided by the rise of AI art<dt-fn>There is lots to say about this, whether it has been or will be a good thing for 
        artists and society in the long run. I hope to write about it in another post.</dt-fn> models like Stable Diffusion. Suddenly, the computer was a better artist than I could ever hope to be.
      </p>
      <p>
        It&#39;s been two years, and image generation with diffusion is better than ever. It&#39;s also led to breakthroughs in animation, 
        video generation, 3D modeling, protein structure prediction, and even robot trajectory planning. Where did it come from, how does it work and where is it going?
      </p>
      <p>
        This post collects my notes on the theory of diffusion and applications to image generation and other tasks. Readers should know some probability theory (Bayes&#39; rule, Gaussian distributions).
        Examples and code using PyTorch are provided.
      </p>
      <dt-byline></dt-byline>
      
      <p>
        The basic problem of generative modeling is: given a set of samples from an unknown distribution \( \mathbf{x} \sim p(\mathbf{x}) \), we want to generate new samples from that distribution.
      </p>
      <p>
        <a href="https://poloclub.github.io/ganlab/">Generative adversarial networks</a> treat this as a game: a generator model taking a random seed is trained to fool a discriminator, which is simultaneously trained to tell real samples from the 
        dataset from fake. GANs can <a href="https://thispersondoesnotexist.com/">synthesize amazing images</a> but are notoriously hard to train. They do not explicitly model \( p(\mathbf{x}) \) and in practice end up incapable<dt-cite key="grover2018flowgan"></dt-cite> of generating substantial subsets of it.
        <dt-fn>In the extreme case we get <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network#Mode_collapse">mode collapse</a>, where the generator learns to cycle between a small subset of possible outputs to fool the discriminator.</dt-fn>
      </p>
      <p>
        A more explicit approach is to learn a deterministic, reversible mapping from the samples we have to a distribution which we know how to sample from, like the unit gaussian. Then we can sample 
        a point from the known distribution and apply the inverse mapping to get a sample from \( p(\mathbf{x}) \). This is conceptually attractive and is called  
        <a href="https://blog.evjang.com/2018/01/nf1.html">normalizing flows</a>. Flows have also been used for images: <a href="https://openai.com/index/glow/">OpenAI&#39;s 2018 Glow</a> 
        generated realistic images of faces with a semantically meaningful latent space.
      </p>
      <p><img src="https://andrewkchan.dev/posts/diffusion-assets/normalizing_flow.png" alt="Normalizing flow" id="normalizing-flow-png"/>  
        <img src="https://andrewkchan.dev/posts/diffusion-assets/normalizing_flow.gif" alt="Normalizing flow" id="normalizing-flow-gif"/>
      </p>
      <p>
        <b>Hover to play.</b>
        <i>Image via <a href="https://blog.evjang.com/2019/07/nf-jax.html">Eric Jang&#39;s blog</a>. A normalizing flow learns a deterministic, probability-density-preserving mapping between the normal distribution and a 2D dataset.</i>
      </p>

      <h2 id="section-1.1">1.1 Denoising diffusion models</h2>
      <p>
        What if instead of mapping data points to a normal distribution deterministically, we mapped points stochastically, by blending random noise into them?
      </p>
      <p>
        This seems weird at first. Technically this mapping wouldn&#39;t be reversible, because a given data point could map to any point in the target space.
      </p>
      <p>
        But suppose we were to do this over many steps, where we start with a clean data point, then blend in a small amount of noise, repeating many times until we have 
        something that looks like pure noise.
      </p>
      <ul>
        <li>
            At any given time, looking at a single noisy datapoint, you can sort of tell where the datapoint might have been in the previous step.
        </li>
        <li>
            And given any point \( \mathbf{y} \) in the target space and any point \( \mathbf{x} \) in our original space, \( \mathbf{y} \) comes from
            \( \mathbf{x} \) with probability arbitrarily close to \( p(\mathbf{x}) \), depending on how much noise we choose to blend into our data. 
            So if we learn to reverse the many-step process, we should be able to sample from \( p(\mathbf{x}) \). This is the idea of <em>denoising diffusion</em>.
        </li>
      </ul>
      <p>
        This is like the physical process of diffusion, where a drop of ink slowly diffuses out to fill a tank by the random motion of individual ink particles.
      </p>
      <div>
        <p>
            A 2D dataset being mapped to the unit gaussian over 50 noising steps.
            Adjust the slider or click the previews below to see it in action.
        </p>
        <p>
            <b>Left:</b> our 2D dataset with noise added at the current step. <b>Right:</b>
            the expected direction over all the directions a noisy point might have come from in the previous step.
        </p>
      </div>
      
      <p>
        Why might this stochastic mapping work better than the deterministic one that we get from normalizing flows?
        One answer is that in practice, the invertibility requirement for flows is highly limiting. Not only does each layer of the flow network need to be invertible, 
        but the determinant of the Jacobian for each layer must be fast to compute.<dt-fn><a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">Computing the determinant</a> 
        of an arbitrary \(N \times N\) Jacobian is \( O(N^3) \), which is unacceptably slow. Much research focuses on finding specific functions for which this can be faster.</dt-fn> 
        This limits what you can express with a given model size, which could be why flows weren&#39;t the first model type to scale to Stable Diffusion levels of fidelity. In contrast, 
        denoising diffusion models only need to learn a mapping that goes in one direction.
      </p>
      <p>
        Training works by adding random noise to each data point in our training set, having the model predict the noise, then minimizing the L2 loss between the 
        prediction and the actual noise direction via gradient descent.
      </p>
      <p>
        There are a few ways to sample from a pre-trained model. They boil down to:
        </p><ol>
            <li>Start with a pure noise image.</li>
            <li>Predict the noise in it, and subtract a predefined fraction of it.</li>
            <li>Repeat (2) many times (10-1000 depending on the sampler), get a noise-free image.</li>
        </ol>
      
      <p>
        If you&#39;re like me, you may be wondering a few things:
        </p><ul>
            <li>
                Why do we estimate the noise direction rather than the de-noised image directly? In other words, how does estimating noise help us learn the distribution?
            </li>
            <li>
                Why does this process require so many steps?
            </li>
            <li>
                Why are there multiple ways to sample, and what&#39;s the difference between them?
            </li>
        </ul>
      

      <dt-byline></dt-byline>

      
      <p>
        Let&#39;s take a look at the original approach, Denoising Diffusion Probabilistic Models<dt-cite key="ho2020denoising"></dt-cite>.
        Newer advances build on the language and math of this paper.
      </p>
      <h2 id="section-2.1">2.1 Noising and de-noising</h2>
      <p>
        Given an input image \( \mathbf{x}_0 \), we map it to a point in the unit normal distribution by iteratively blending noise to it in a forward diffusion process over \(t=1,2,â€¦,T\) timesteps.
        Each timestep generates a new image by blending in a small amount of random noise to the previous one:
        $$
        \mathbf{x}_t = \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1-\alpha_t}\epsilon
        $$

        where:
        </p><ul>
            <li>\(\epsilon \sim \mathcal{N}(0, \mathbf{I})\)</li>
            <li>\(\alpha_t\) is less than but close to \(1\), and \(\prod_{t=1}^T \alpha_t \approx 0\)</li>
            <li>The terms in square roots ensure that the variance remains the same after each step<dt-fn>We assume that the dataset is standardized, so that the variance of \(\mathbf{x}_0\) is 1 over all dimensions.</dt-fn>. Notice how we are adding noise but shrinking the dataset at the same time.</li>
        </ul>
      
      <p>
        We can write the probability density of the forward step as:
        $$
        q(\mathbf{x}_t | \mathbf{x}_{t-1}) := \mathcal{N}(\sqrt{\alpha_t}\mathbf{x}_{t-1}, (1 - \alpha_t)\mathbf{I})
        $$
      </p>
      <h4>Recurrence property</h4>
      <p>
        Each step depends only on the last timestep, and the noise blended in is independent of all previous noise samples. So we can expand the recurrence and derive
        an equation to obtain \(\mathbf{x}_t\) in one step from \(\mathbf{x}_0\) by blending in a single gaussian noise vector, since sums of independent gaussians 
        are also gaussian:
        $$
        \mathbf{x}_t = \sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\epsilon
        $$
        where \(\bar\alpha_t = \prod_{i=1}^t \alpha_i\) and \(\epsilon \sim \mathcal{N}(0, \mathbf{I})\).

        This is used to derive the reverse process which we want to learn, and the training objective where we predict the noise that we add to images.
      </p>
      <img src="https://andrewkchan.dev/posts/diffusion-assets/ddpm_markov_diagram.png" alt="Noising and denoising processes in DDPM"/>
      <p><i>Image via <dt-cite key="weng2021diffusion"></dt-cite><dt-cite key="ho2020denoising"></dt-cite></i>.</p>
      <p>
        Now consider the reverse process. Given a noisy image \( \mathbf{x}_t \), what&#39;s the distribution of the previous, less-noisy version of it \(q(\mathbf{x}_{t-1} | \mathbf{x}_t)\)?
      </p>
      <p>
        This is easier if we know the original image \( \mathbf{x}_0 \). By Bayes&#39; rule, we have:
        $$
        q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t | \mathbf{x}_{t-1}) q(\mathbf{x}_{t-1} | \mathbf{x}_0) q(\mathbf{x}_0)}{q(\mathbf{x}_t | \mathbf{x}_0) q(\mathbf{x}_0)}
        $$
        Subbing in the distribution formulas and doing the algebra we get...
        $$
        q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mu(\mathbf{x}_t, \mathbf{x}_0), \Sigma(t)\mathbf{I})
        $$
        where
        $$
        \mathbf{\mu}(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})\mathbf{x}_t + \sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)\mathbf{x}_0}{1-\bar{\alpha}_t} \\
        \Sigma(t) = \frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}
        $$

        That is, given a noisy image and the known original image, the distribution of the previous, less-noisy version of it is gaussian.
      </p>
      <p>
        What can we do with this information? When we&#39;re de-noising a noisy image we won&#39;t know the original corresponding to it. We want \( q(\mathbf{x}_{t-1} | \mathbf{x}_t) \).
      </p>
      <p>
        Since we have a closed form solution for \(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0)\), if we could use the entire dataset at generation time, we could use the law of total
        probability to compute \(q(\mathbf{x}_{t-1} | \mathbf{x}_t)\) as a mixture of gaussians, but we can&#39;t (billions of images!) and moreover that would not give us the novelty 
        we want, since if we followed it for all timesteps, we would just end up recovering the training samples. We want to learn some underlying distribution function which gives
        us novelty in generated samples by compressing the dataset.
      </p>
      <h2 id="section-2.2">2.2 Learning to de-noise</h2>
      <p>
        It turns out that \(q(\mathbf{x}_{t-1} | \mathbf{x}_t)\) is approximately gaussian for very small amounts of noise. This is an old result from statistical physics<dt-cite key="sohldickstein2015deep"></dt-cite>.
        This gives us a way to learn a reverse distribution: we can estimate the parameters \(\mu_\theta, \Sigma_\theta\) of a gaussian, and take the KL divergence to all of the distributions 
        \(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0)\) for every training example \(\mathbf{x}_0\).
      </p>
      <p>
        Recall that the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> is a metric measuring the difference between two probability distributions. 
        It&#39;s easy to compute for us because we are computing it between two gaussians with known parameters, so it has a closed form<dt-fn>For arbitrary continuous distributions, the KL 
        divergence requires taking an integral. This is a special case. See the formula and a short proof <a href="https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/">here</a>.</dt-fn>. 
        And as it turns out, minimizing this gives us a distribution which is most likely to generate all our training samples.
      </p>
      <img src="https://andrewkchan.dev/posts/diffusion-assets/ddpm_kl.svg" alt="The reverse distributions q conditioned on training samples, and the distribution p that we learn."/>
      <p>
        <i>
            The reverse distributions <span>\(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0^{(1)})\)</span> and 
            <span>\(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0^{(2)})\)</span> conditioned on training samples 
            \(\mathbf{x}_0^{(1)},\mathbf{x}_0^{(2)}\), and the <span>distribution \(p_\theta\) that we learn</span> by minimizing KL divergence to them.
        </i>
      </p>
      <div>
          <p>
            ðŸ‘‰ We can prove that minimizing \( L \) maximizes the likelihood of generating the dataset because it optimizes a lower bound for the same, through a process called variational inference.
          </p>
          <p>
              For a proof, see the derivation of \(L_\text{VLB}\) on <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process">Lilian Weng&#39;s blog</a>.
          </p>
      </div>
      <p>
        Concretely, let our training objective be:
        $$
        L = \mathbb{E}_{\mathbf{x}_{0:T} \sim q}[\sum_{t=1}^TD_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))]
        $$

        where \(D_{KL}(q || p_\theta)\) is an expression<dt-fn>Note the <a href="https://x.com/ari_seff/status/1303741288911638530">KL divergence is asymmetric</a>, so minimizing \(D_{KL}(q || p_\theta)\) over \(p_\theta\) (which squeezes \(q\) under \(p_\theta\)) gives a different result than \(D_{KL}(p_\theta || q)\)
        (which does the opposite). But as we see next this doesn&#39;t ultimately matter.</dt-fn> involving the variances \(\Sigma_\theta,\Sigma(t)\) and means \(\mu_\theta,\mu(\mathbf{x}_t,\mathbf{x}_0)\) of the two gaussians.
      </p>
      <p>
        Ho 2020<dt-cite key="ho2020denoising"></dt-cite> fixed the \(\Sigma_\theta\) to be equal to \(\Sigma(t)\), since they found that trying to learn it made training too unstable, and this gave good results. So in practice
        we only learn the means \(\mu_\theta\). After substituting in the KL divergence formula for gaussians, we end up with an objective to minimize the L2 distance between estimated
        and actual means:
        $$
        L = \sum_{t=1}^T\mathbb{E}_{\mathbf{x}_{0:T} \sim q}[\frac{1}{2\Sigma(t)}||\mu(\mathbf{x}_t, \mathbf{x}_0) - \mu_\theta(\mathbf{x}_t)||^2]
        $$
      </p>
      <p>
        We can simplify further and take advantage of the fact that \(\mathbf{x}_t\) can be written as a blending of \(\mathbf{x}_0\) with gaussian noise
        \(\epsilon\). 
      </p>
      <p>
        This means we can rewrite<dt-fn>Much thanks to <a href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html#three-equivalent-interpretations">Calvin Luo&#39;s blog</a> for providing detailed derivations. I learned while writing this post that I like seeing detailed proofs only a little more than I dislike math.</dt-fn>
        $$
        \mathbf{\mu}(\mathbf{x}_t, \mathbf{x}_0) = \frac{1}{\sqrt{\alpha_t}}\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}\sqrt{\alpha_t}}\epsilon
        $$

        And we can define \(\mu_\theta(\mathbf{x}_t)\) in terms of an estimator \(\epsilon_\theta\) to match:
        $$
        \mathbf{\mu}_\theta(\mathbf{x}_t) = \frac{1}{\sqrt{\alpha_t}}\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}\sqrt{\alpha_t}}\epsilon_\theta(\mathbf{x}_t, t)
        $$
      </p>
      <p>
        Plugging this in turns our mean prediction problem into a noise prediction problem:
        $$
        L = \sum_{t=1}^T\mathbb{E}_{\mathbf{x}_{0} \sim q,\epsilon}[\frac{(1-\alpha_t)^2}{2\Sigma(t)\alpha_t(1-\bar{\alpha}_t)}||\epsilon-\epsilon_\theta(\sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\epsilon,t)||^2]
        $$
      </p>
      <p>
        It turns out ignoring the weighting improves the quality of results<dt-cite key="ho2020denoising"></dt-cite>. You could view this as down-weighting loss terms at small \(t\) so that the network focuses on learning the more difficult problem 
        of denoising images with lots of noise. So the final loss function is
        $$
        L_\text{simple} = \mathbb{E}_{t \sim [1, T], \mathbf{x}_{0} \sim q,\epsilon}[||\epsilon-\epsilon_\theta(\sqrt{\bar\alpha_t}\mathbf{x}_0 + \sqrt{1-\bar\alpha_t}\epsilon,t)||^2]
        $$

        In code, our training loop is:
      </p>
    <!-- <dt-code block language="python"> -->
        <pre>            <code>
def train(model, train_data, alpha_min=0.98, alpha_max=0.999, T=1000, n_epochs=5):
    opt = torch.optim.SGD([model.parameters()], lr=0.1)
    alpha = torch.linspace(alpha_max, alpha_min, T)
    alpha_bar = torch.cumprod(alpha, dim=-1)

    for _ in range(n_epochs):
        for x0s in train_data:
            eps = torch.randn_like(x0s)
            t = torch.randint(T, (x0s.shape[0],))

            xts = alpha_bar[t].sqrt() * x0s +  (1.-alpha_bar[t]).sqrt() * eps
            eps_pred = model(xts, t)

            loss = torch.nn.functional.mse_loss(eps_pred, eps)
            loss.backward()
            opt.step()
            opt.zero_grad()
            </code>
        </pre>
    <!-- </dt-code> -->

      <h2 id="section-2.3">2.3 Sampling</h2>
      <p>
        Once we&#39;ve learned a noise estimation model \( \epsilon_\theta(\mathbf{x}_t, t) \), we&#39;ve effectively learned the reverse process. 
        Then we can use this learned model to sample an image \( \mathbf{x}_0 \) from the image distribution by:
        </p><ol>
            <li>
                Sampling a random noise image \(x_T \sim \mathcal{N}(0, \mathbf{I})\).
            </li>
            <li>
                <p>
                    For timesteps \(t\) from \(T\) to \(1\):
                </p>
                <ol type="a">
                    <li>Predict the noise \(\hat\epsilon_t = \epsilon_\theta(\mathbf{x}_t, t)\).</li>
                    <li>Sample the de-noised image \(\mathbf{x}_{t-1} \sim \mathcal{N}(\frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}}\hat\epsilon_t), \Sigma_\theta)\).</li>
                </ol>
            </li>
        </ol>
      
      <p>
        In code:
      </p>
    <!-- <dt-code block language="python"> -->
        <pre>            <code>
def sample(model, img_size, alpha, alpha_bar):
    xt = torch.randn(img_size)
    for t in reversed(range(T)):
        with torch.no_grad():
            eps_pred = model(xt, t)

        alpha_bar_t = alpha_bar[t]
        alpha_bar_t1 = alpha_bar[t-1] if t &gt; 0 else 1.
        sigma = ((1.-alpha[t])*(1.-alpha_bar_t1)/(1.-alpha_bar_t)).sqrt()
        z = torch.randn(img_size)
        
        mu_pred = (xt - (1.-alpha[t])/(1.-alpha_bar[t]).sqrt()*eps_pred)/alpha[t].sqrt()
        xt = mu_pred + sigma*z
    return xt
            </code>
        </pre>
    <!-- </dt-code> -->

      <h2 id="section-2.4">2.4 Summary and example</h2>

      <p>
        Let&#39;s summarize what we&#39;ve learned about DDPM:
      </p>
      <ul>
        <li>
            We want to learn an underlying distribution for a dataset of images.
        </li>
        <li>
            <p>
                We do this by defining a forward noising process where we gradually turn an image \(\mathbf{x}_0\) into pure noise \(\mathbf{x}_T\) over many steps, and we 
                learn to reverse the process by estimating the distribution of \(\mathbf{x}_{t-1}\) given \(\mathbf{x}_T\), which is feasible because:
            </p>
            <ul>
                <li>
                    It&#39;s approximately gaussian when \(T\) is large.
                </li>
                <li>
                    We know exactly what the distribution is if we assume the original image is some \(\mathbf{x}_0\) from our dataset.
                </li>
                <li>
                    We can use the KL divergence to ensure what we learn is as close to these known distributions as possible for every \(\mathbf{x}_0\) in our dataset.
                </li>
                <li>
                    This also provably maximizes the likelihood of re-generating our dataset. 
                </li>
            </ul>
        </li>
        <li>
            Finally, we can simplify the objective so it becomes a noise estimation problem.
        </li>
      </ul>
      <p>
        Let&#39;s train a DDPM network on a 2D dataset. We will use the <a href="http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html">Datasaurus</a>
        dataset<dt-fn>Inspired by tanelp&#39;s <a href="https://github.com/tanelp/tiny-diffusion">tiny-diffusion</a>.</dt-fn> of 142 points, plotted below.
        Follow along via Colab:
        <a target="_blank" href="https://colab.research.google.com/github/andrewkchan/very-tiny-diffusion/blob/main/very-tiny-diffusion.ipynb">
            <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
          </a>
      </p>
      <img src="https://andrewkchan.dev/posts/diffusion-assets/trex-viz/step000.svg" alt="Datasaurus" id="trex"/>
      <p>
        The neural network will be a function from \(\mathbb{R}^2 \mapsto \mathbb{R}^2\). We&#39;ll start with a bog-standard MLP with 3 hidden layers of size 64 with ReLU activations.
        This architecture has 12,000+ parameters, so one might think there is a high chance of memorizing the dataset (284 numbers), but as we&#39;ll see, the distribution we learn 
        will be pretty good: it will not only fit the training samples but will have high diversity.
      </p>
      <p>
        After training, we can sample 1000 points to see how well it learned the distribution:
      </p>
      <img src="https://andrewkchan.dev/posts/diffusion-assets/trex-viz/mlp_identity_zero.svg" alt="Datasaurus" id="trex-mlp-identity-zero"/>
      <p>
        Oh no! That doesn&#39;t look anything like the dinosaur we wanted. What happened?
      </p>
      <p>
        One problem is that we&#39;re not passing any timestep information to the model. The noise drift vectors look pretty different at higher timesteps compared to lower
        timesteps. Let&#39;s try passing the timestep \(t=0,...,50\) normalized to between \(0\) and \(1\) to our model, which now map \(\mathbb{R}^3 \mapsto \mathbb{R}^2\).
      </p>
      <img src="https://andrewkchan.dev/posts/diffusion-assets/trex-viz/mlp_identity_linear.svg" alt="Datasaurus" id="trex-mlp-identity-linear"/>
      <p>
        That&#39;s much better. But we can do better by using input encodings. These are fixed functions that transform the input before feeding them to the neural network, 
        and they can make a big difference. We will use a <a href="https://bmild.github.io/fourfeat/">fourier encoding</a>, since we know the distribution underlying our 
        data is like an image - a high-frequency signal in a low-dimensional (2D) space<dt-cite key="tancik2020fourfeat"></dt-cite>.
      </p>
      <p>
        For an input \(D\)-dimensional point \( \mathbf{x} \), we will encode it as:
        $$
        \text{FourierEncoding}(\mathbf{x}) = \left[ \cos(2\pi\mathbf{Bx}), \sin(2\pi\mathbf{Bx}) \right]^T
        $$

        here \(\mathbf{B}\) is a random \(L \times D\) Gaussian matrix, where each entry is drawn independently from a normal distribution.
        What we are doing is transforming the input space into a \(L\)-dimensional space of random frequency features. We&#39;ll set the hyperparameter \(L\)
        to 32.
      </p>
      <img src="https://andrewkchan.dev/posts/diffusion-assets/trex-viz/mlp_fourier_fourier.svg" alt="Datasaurus" id="trex-mlp-fourier-fourier"/>
      <p>
        Nice! Our distribution is looking pretty good. One more thing we can do is tweak our noising schedule. This can be crucial for performance.
        </p><ul>
            <li>
                Our noising schedule is based on Ho 2020<dt-cite key="ho2020denoising"></dt-cite>, who use a linearly decreasing sequence of \(\alpha_t\) where \(\bar\alpha_T=\prod_{t=1}^T\alpha_t\approx0\) so that the model spends a bit more time
                learning how to reverse lower noise levels, and the last timestep is close to pure noise. This works well for high-resolution images.
            </li>
            <li>
                But our dataset is low-dimensional, and from the forward process visualization in <a href="#section-1.1">Â§1.1</a>, it already looks a lot like noise once we get about halfway through our process, 
                and subsequent steps don&#39;t seem to destroy much more signal.
            </li>
        </ul>
      
      <img src="https://andrewkchan.dev/posts/diffusion-assets/noising_importance.png" alt="Noised images at different resolution with the same noise level"/>
      <p>
        <i>Image via <dt-cite key="chen2023importance"></dt-cite>. The same amount of noise in different resolution images yields very different looking results, with low-res images looking much noisier than high-res ones.</i>
      </p>
      <p>
        Let&#39;s adjust our schedule so that the model trains on more high-signal examples. This improves performance on lower-dimensional data while doing the opposite for higher-dimensional data<dt-cite key="chen2023importance"></dt-cite><dt-cite key="nichol2021improved"></dt-cite>.
        It gets us our best dinosaur yet:
      </p>
      <div id="trex-noise-schedule-gutter">
        <p>
            <b>Left:</b> our original and new \(\bar\alpha_t\) schedules.
            <b>Right:</b> 1000 samples from the trained model.
        </p>
        <p>
            The original schedule already didn&#39;t take us to pure noise, with \(\bar\alpha_T \approx 0.28 \). The new schedule ends at where the old schedule was halfway, at \(0.6\).
        </p>
      </div>
      <p><img src="https://andrewkchan.dev/posts/diffusion-assets/trex-viz/alpha_bar_modified.svg" alt="Datasaurus"/>
        <img src="https://andrewkchan.dev/posts/diffusion-assets/trex-viz/mlp_fourier_fourier_rescheduled.svg" alt="Datasaurus" id="trex-mlp-fourier-fourier-rescheduled"/>
      </p>
      

      <dt-byline></dt-byline>

      
      </div></div>
  </body>
</html>
