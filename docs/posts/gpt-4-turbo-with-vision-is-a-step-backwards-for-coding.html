<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aider.chat/2024/04/09/gpt-4-turbo.html">Original</a>
    <h1>GPT-4 Turbo with Vision is a step backwards for coding</h1>
    
    <div id="readability-page-1" class="page"><div id="content" role="main">
      

<p><a href="https://twitter.com/OpenAIDevs/status/1777769463258988634">OpenAI just released GPT-4 Turbo with Vision</a>
and it performs worse on aider’s coding benchmark suites than all the previous GPT-4 models.
In particular, it seems much more prone to “lazy coding” than the
existing GPT-4 Turbo “preview” models.</p>

<h2 id="code-editing-skill">Code editing skill</h2>

<p><a href="https://aider.chat/assets/2024-04-09-gpt-4-turbo.svg"><img src="https://aider.chat/assets/2024-04-09-gpt-4-turbo.svg" alt="benchmark results"/></a></p>

<p>Aider relies on a
<a href="https://aider.chat/docs/benchmarks.html#the-benchmark">code editing benchmark</a>
to quantitatively evaluate how well
an LLM can make changes to existing code.
The benchmark uses aider to try and complete
<a href="https://github.com/exercism/python">133 Exercism Python coding exercises</a>.</p>

<p>For each exercise, the LLM gets two tries to solve each problem:</p>

<ol>
  <li>On the first try, it gets initial stub code and the English description of the coding task. If the tests all pass, we are done.</li>
  <li>If any tests failed, aider sends the LLM the failing test output and gives it a second try to complete the task.</li>
</ol>

<p><strong>GPT-4 Turbo with Vision
scores only 62% on this benchmark,
the lowest score of any of the existing GPT-4 models.</strong>
The other models scored 63-66%, so this represents only a small
regression, and is likely statistically insignificant when compared
against <code>gpt-4-0613</code>.</p>

<h2 id="lazy-coding">Lazy coding</h2>

<p><a href="https://aider.chat/assets/2024-04-09-gpt-4-turbo-laziness.svg"><img src="https://aider.chat/assets/2024-04-09-gpt-4-turbo-laziness.svg" alt="benchmark results"/></a></p>

<p>The GPT-4 Turbo “preview” models have been widely criticized for being “lazy”
when coding.
They often omit needed code
and instead leave comments with homework assignments like “implement method here”.</p>

<div><div><pre><code>def some_complex_method(foo, bar):
    # ... implement method here ...
</code></pre></div></div>

<p>Aider uses a <a href="https://github.com/paul-gauthier/refactor-benchmark">“laziness” benchmark suite</a>
which is designed to both provoke and quantify lazy coding.
It consists of
89 python refactoring tasks
which tend to make GPT-4 Turbo code in that lazy manner.</p>

<p><strong>The new GPT-4 Turbo with Vision model scores only 34% on aider’s
refactoring benchmark, making it the laziest coder of all the GPT-4 Turbo models
by a significant margin.</strong></p>



<p>Aider has full support for the new GPT-4 Turbo with Vision
model, which you can access using the switch <code>--model gpt-4-turbo-2024-04-09</code>.
But aider will continue to use <code>gpt-4-1106-preview</code> by default,
as it is by far the strongest coder of the GPT-4 models.</p>



      
    </div></div>
  </body>
</html>
