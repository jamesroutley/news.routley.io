<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2303.09752">Original</a>
    <h1>CoLT5: Faster Long-Range Transformers With Conditional Computation</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ainslie%2C+J">Joshua Ainslie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lei%2C+T">Tao Lei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=de+Jong%2C+M">Michiel de Jong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Onta%C3%B1%C3%B3n%2C+S">Santiago Ontañón</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brahma%2C+S">Siddhartha Brahma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zemlyanskiy%2C+Y">Yury Zemlyanskiy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Uthus%2C+D">David Uthus</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo%2C+M">Mandy Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee-Thorp%2C+J">James Lee-Thorp</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tay%2C+Y">Yi Tay</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sung%2C+Y">Yun-Hsuan Sung</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sanghai%2C+S">Sumit Sanghai</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2303.09752">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Many natural language processing tasks benefit from long inputs, but
processing long documents with Transformers is expensive -- not only due to
quadratic attention complexity but also from applying feedforward and
projection layers to every token. However, not all tokens are equally
important, especially for longer documents. We propose CoLT5, a long-input
Transformer model that builds on this intuition by employing conditional
computation, devoting more resources to important tokens in both feedforward
and attention layers. We show that CoLT5 achieves stronger performance than
LongT5 with much faster training and inference, achieving SOTA on the
long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably
make use of extremely long inputs, showing strong gains up to 64k input length.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Joshua Ainslie [<a href="https://arxiv.org/show-email/3fbe01c0/2303.09752">view email</a>]
      </p></div></div>
  </body>
</html>
