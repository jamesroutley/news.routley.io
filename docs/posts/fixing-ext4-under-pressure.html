<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sdomi.pl/weblog/18-fixing-ext4-under-pressure/">Original</a>
    <h1>Fixing Ext4 Under Pressure</h1>
    
    <div id="readability-page-1" class="page"><div><h3>Stakes bigger than life: fixing ext4 under pressure</h3><p>Last September, we&#39;ve had an <a href="https://t.me/uwuenterprises/73">unplanned reboot</a> on sakamoto (one of my servers - likely the one serving you this page). This in itself isn&#39;t that big of a deal, but the chaos that ensued afterwards got <i>weird</i> really quickly.</p>

<p>As a preface, the relevant parts of sakamoto currently work more or less like this:</p>

<ul>
	<li>Alpine Linux on host</li>
	<li>/ on a SATA SSD, ext4 (defaults)</li>
	<li>array of spinning rust, with ZFS (including native encryption)</li>
	<li>&#34;band-aid&#34; SSD LUKS solution: huge file under / that&#39;s LUKS encrypted and has an ext4 partition underneath</li>
</ul>

<p>The last part isn&#39;t even close to best practices, but I deemed it &#34;good enough&#34; as a temporary solution. And, as we all know, temporary solutions last the longest... :p</p>

<h3>Leading to the incident: early 2023</h3>

<p>In late March of last year, I decided to move our (then fully unencrypted) postgres db to a LUKS-backed store; Due to our partition layout (and to minimize downtime), I went with a LUKS-in-a-file based approach, as outlined above.</p>

<p><span>
&gt; I will be pulling postgres down for some LONG overdue maintenance in ~10 minutes. I expect that it will be down for around 10-20 minutes.
</span></p><p>~ <i>me</i>, March 29th 2023, 17:27 CET</p>

<p>
	Initially, I created a 64GB file, which seemed plenty big at the time.
	Everything went well, and I didn&#39;t care for a month or two.
	Then, we wanted to migrate the Matrix db from sqlite to PostgreSQL;
	That database ended up being almost twice the size of the original
	partition - so I grew the file with 
	<span>dd oflag=append conv=notrunc</span>,
	then the LUKS part with <span>cryptsetup resize</span>,
	and finally, the ext4 FS itself with <span>resize2fs</span>.
	It worked like a charm; In the coming months, I likely did that once more.
</p>

<h3>The incident: September 19-20th, 2023</h3>

<p>Around 23:22 CEST, the system rebooted. Immediately afterwards, I started bringing up some of the services (a few things have to wait for the filesystems to get decrypted and properly mounted). Then, I hit a brick wall:</p>



<p><code>mount: /postgres_luks.img: cannot mount; probably corrupted filesystem on /dev/loop0.
dmesg(1) may have more information after failed mount system call.
</code></p></div><p>Wow, this reminds me old times when I was trying to understand in what order to assemble mdadm devices after a crash. 200% of adrenaline in the blood for 2h,  but the respect of other admins when I succeeded was priceless ðŸ˜Ž</p></div>
  </body>
</html>
