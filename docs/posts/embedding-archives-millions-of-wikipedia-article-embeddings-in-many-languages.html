<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://txt.cohere.com/embedding-archives-wikipedia/">Original</a>
    <h1>Embedding Archives: Millions of Wikipedia Article Embeddings in Many Languages</h1>
    
    <div id="readability-page-1" class="page"><section>
        <p>There’s no denying that we’re in the midst of a revolutionary time for Language AI. Developers are waking up to the vast emerging capabilities of language understanding and generation models. One of the key building blocks for this new generation of applications are the embeddings that power search systems.<br/></p><p>To aid developers in rapidly getting started with commonly used datasets, we are releasing a massive archive of embedding vectors that can be freely downloaded and used to power your applications.<br/></p><p>Using Cohere’s <a href="https://txt.cohere.com/multilingual/">Multilingual embedding model</a>, we have embedded millions of Wikipedia articles in many languages. The articles are broken down into passages, and an embedding vector is calculated for each passage.<br/></p><figure><img src="https://lh6.googleusercontent.com/jwH5eoIwIMNt4asY5kIYiWJHAVIF_HbmsAbPkloi-66rFaSYAt2emjQcEWJUCV8WG4xitutre7yeTkn5Hx_lz44JltxzBSDFxT21GHOorkJSZPoTZI9DO3JSAn6iJypFkM51GqpDeeAKTM5aq0Guftk" alt="" loading="lazy" width="624" height="316"/></figure><p>The archives are available for download on <a href="https://huggingface.co/Cohere?ref=txt.cohere.com">Hugging Face Datasets</a>, and contain both the text, embedding vector, and additional metadata values.<br/></p><!--kg-card-begin: markdown--><pre><code>from datasets import load_dataset
docs = load_dataset(f&#34;Cohere/wikipedia-22-12-simple-embeddings&#34;, split=&#34;train&#34;)
</code></pre>
<!--kg-card-end: markdown--><p>This downloads the entire dataset (Simple English Wikipedia in this instance). The schema looks like this:</p><figure><img src="https://lh6.googleusercontent.com/x9c3Gl3BAUH7dl6vHvX1DDV3j8bj-ZbLgEKtVVAFpzs79PVWSrOqD_20ZBpcW8swp62Zvwa1W97mJUVKfhZxjr92AnH_ai7O8S_cy_YQ9Gcxk--kuAcWR2D2CrJl6m32eEZVUsAvEnwo8pLGPRZ6kAw" alt="" loading="lazy" width="624" height="219"/></figure><p>The <code>emb</code> column contains the embedding of that passage of text (with the title of the article appended to its beginning). This is an array of 768 floats (the embedding dimension of Cohere’s multilingual-22-12 embedding model).<br/></p><!--kg-card-begin: html--><table><colgroup><col width="170"/><col width="221"/></colgroup><tbody><tr><td><p dir="ltr"><span>Wikipedia</span></p></td><td><p dir="ltr"><span>Number of vectors / embedded passages</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings?ref=txt.cohere.com"><span>English</span></a><span> </span></p></td><td><p dir="ltr"><span>35 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-de-embeddings?ref=txt.cohere.com"><span>German</span></a></p></td><td><p dir="ltr"><span>15 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-fr-embeddings?ref=txt.cohere.com"><span>French</span></a></p></td><td><p dir="ltr"><span>13 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-es-embeddings?ref=txt.cohere.com"><span>Spanish</span></a></p></td><td><p dir="ltr"><span>10 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-it-embeddings?ref=txt.cohere.com"><span>Italian</span></a></p></td><td><p dir="ltr"><span>8 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-ja-embeddings?ref=txt.cohere.com"><span>Japanese</span></a></p></td><td><p dir="ltr"><span>5 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-ar-embeddings?ref=txt.cohere.com"><span>Arabic</span></a></p></td><td><p dir="ltr"><span>3 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-zh-embeddings?ref=txt.cohere.com"><span>Chinese (Simplified)</span></a></p></td><td><p dir="ltr"><span>2 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-ko-embeddings?ref=txt.cohere.com"><span>Korean</span></a></p></td><td><p dir="ltr"><span>1 million</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings?ref=txt.cohere.com"><span>Simple English</span></a></p></td><td><p dir="ltr"><span>486 Thousand</span></p></td></tr><tr><td><p dir="ltr"><a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-hi-embeddings?ref=txt.cohere.com"><span>Hindi</span></a></p></td><td><p dir="ltr"><span>432 Thousand</span></p></td></tr><tr><td><p dir="ltr"><span>Total</span></p></td><td><p dir="ltr"><span>94 Million</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>Read more about how this data was prepared and processed in the <a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12?ref=txt.cohere.com">dataset card</a>.</p><h2 id="what-can-you-build-with-this">What Can You Build with This?</h2><p>The sky&#39;s the limit to what you can build with this. A few common use cases include:</p><p><strong>Neural Search Systems</strong></p><p>Wikipedia is one of the world’s most valuable knowledge stores. This embedding archive can be used to build search systems that retrieve relevant knowledge based on a user query. </p><figure><img src="https://txt.cohere.com/content/images/2023/04/image-5.png" alt="" loading="lazy" width="2000" height="991" srcset="https://txt.cohere.com/content/images/size/w600/2023/04/image-5.png 600w, https://txt.cohere.com/content/images/size/w1000/2023/04/image-5.png 1000w, https://txt.cohere.com/content/images/size/w1600/2023/04/image-5.png 1600w, https://txt.cohere.com/content/images/2023/04/image-5.png 2354w" sizes="(min-width: 720px) 720px"/></figure><p>In this example, to conduct a search, the query is first embedded using <code>co.Embed()</code> , and then the similarity is calculated using dot product multiplication.</p><!--kg-card-begin: markdown--><pre><code># Get the query, then embed it
query = &#39;Who founded youtube&#39;
query_embedding = co.embed(texts=[query], model=&#39;multilingual-22-12&#39;).embeddings 


# Compute dot score between query embedding and document embeddings
# &#39;doc_embeddings&#39; is the list of vectors in the archive
dot_scores = torch.mm(query_embedding, doc_embeddings.transpose(0, 1))
top_k = torch.topk(dot_scores, k=3)
</code></pre>
<!--kg-card-end: markdown--><p>Now, <code>topk</code> contains the indices of the most relevant results. Look at the actual code example <a href="https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings?ref=txt.cohere.com">here</a> [<a href="https://colab.research.google.com/github/cohere-ai/notebooks/blob/main/notebooks/Wikipedia_Semantic_Search_With_Cohere_Embeddings_Archives.ipynb?ref=txt.cohere.com">Colab</a>/<a href="https://github.com/cohere-ai/notebooks/blob/main/notebooks/Wikipedia_Semantic_Search_With_Cohere_Embeddings_Archives.ipynb?ref=txt.cohere.com">notebook</a>]. <br/></p><p><strong>Weaviate: Neural Search with a Vector Database</strong></p><p>Beyond a certain scale, it becomes useful to employ a vector database for more scalable and advanced retrieval functionality.</p><p>A subset of this embedding archive is hosted publicly by <a href="https://weaviate.io/?ref=txt.cohere.com">Weaviate</a>. You can query it directly without having to download the dataset or process it in any way. It contains 10 million of these vectors comprised of 1 million each from the languages: <code>en</code>, <code>de</code>, <code>fr</code>, <code>es</code>, <code>it</code>, <code>ja</code>, <code>ar</code>, <code>zh</code>, <code>ko</code>, <code>hi</code>.</p><p>You can find this code in this <a href="https://colab.research.google.com/github/cohere-ai/notebooks/blob/main/notebooks/Wikipedia_search_demo_cohere_weaviate.ipynb?ref=txt.cohere.com">colab</a>/<a href="https://github.com/cohere-ai/notebooks/blob/main/notebooks/Wikipedia_search_demo_cohere_weaviate.ipynb?ref=txt.cohere.com">notebook</a>. You can query the dataset with</p><pre><code>query_result = semantic_serch(&#34;time travel plot twist&#34;)</code></pre><p>And get the results:</p><figure><img src="https://txt.cohere.com/content/images/2023/04/image-6.png" alt="" loading="lazy" width="1302" height="692" srcset="https://txt.cohere.com/content/images/size/w600/2023/04/image-6.png 600w, https://txt.cohere.com/content/images/size/w1000/2023/04/image-6.png 1000w, https://txt.cohere.com/content/images/2023/04/image-6.png 1302w" sizes="(min-width: 720px) 720px"/></figure><p>You can also filter the results for a specific language, say Japanese:</p><pre><code>query_result = semantic_serch(&#34;time travel plot twist&#34;, results_lang=&#39;ja&#39;)</code></pre><p>And get results only in that one language.</p><figure><img src="https://txt.cohere.com/content/images/2023/04/image-7.png" alt="" loading="lazy" width="1274" height="692" srcset="https://txt.cohere.com/content/images/size/w600/2023/04/image-7.png 600w, https://txt.cohere.com/content/images/size/w1000/2023/04/image-7.png 1000w, https://txt.cohere.com/content/images/2023/04/image-7.png 1274w" sizes="(min-width: 720px) 720px"/></figure><p><strong>Use More Than One Language</strong></p><p>Because these archives were embedded with a model with cross-lingual properties, you can use multiple languages in your application and rely on the property that sentences that are similar in meaning will have similar embeddings, even if they are in different languages.</p><figure><img src="https://lh4.googleusercontent.com/Wgza7B_orsTcX_-_pVBLv4mzFz0gbWcbqSk_a-Ke46F7S8VSsqU7MK_S4o9NFd1GpQNzpE8XyqHDMVGlzZXUv2ON2aoFwqkXaE2JhMnfQqg8Diuo9R8F3mVF6nZm1PmTqzq1ZsMW7K4hkt3fH_edlUE" alt="" loading="lazy" width="624" height="351"/></figure><p><strong>Search specific sections of Wikipedia</strong></p><p>Beyond global Wikipedia exploration, a dataset like this opens the door to searching specific topics if you curate several pages on a relevant topic. Examples include :</p><ul><li> All the episode pages of Breaking Bad (Get the page titles from <a href="https://en.wikipedia.org/wiki/List_of_Breaking_Bad_episodes?ref=txt.cohere.com">List of Breaking Bad Episodes</a>) or other TV series.</li><li>Utilize Wikipedia information boxes to collect the titles of a specific topic, say Electronics (from the bottom of the <a href="https://en.wikipedia.org/wiki/Computer?ref=txt.cohere.com">Computers</a> page)</li></ul><figure><img src="https://txt.cohere.com/content/images/2023/04/image-9.png" alt="" loading="lazy" width="1742" height="548" srcset="https://txt.cohere.com/content/images/size/w600/2023/04/image-9.png 600w, https://txt.cohere.com/content/images/size/w1000/2023/04/image-9.png 1000w, https://txt.cohere.com/content/images/size/w1600/2023/04/image-9.png 1600w, https://txt.cohere.com/content/images/2023/04/image-9.png 1742w" sizes="(min-width: 720px) 720px"/></figure><p>Due to the size of the dataset, an interim step can be to import the text into a database like Postgres and use that to extract interesting subsets for each project you want to build.</p><h2 id="lets-build">Let&#39;s build!</h2><p>Drop by the <a href="https://discord.com/channels/954421988141711382/1098599231612276836?ref=txt.cohere.com">Embedding Archives: Wikipedia thread</a> on the Cohere Discord (join <a href="https://discord.gg/co-mmunity?ref=txt.cohere.ai">here</a>) if you have any questions, ideas, or if you want to share something cool you build with this.</p>
    </section></div>
  </body>
</html>
