<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://erthalion.info/2022/12/30/bpf-performance/">Original</a>
    <h1>Running fast and slow: experiments with BPF programs&#39; performance</h1>
    
    <div id="readability-page-1" class="page"><div>

<p><span>30 Dec 2022</span></p><h2 id="1-introduction">1. Introduction</h2>
<p>My own personal white spot regarding BPF subsystem in Linux kernel was always
programs performance and an overall introspection. Or to formulate it more
specifically, I wasn’t sure if there is any difference in how we reason about
an abstract program performance versus a BPF program? Could we use the same
technics and approaches?</p>
<p>You may wonder why even bother when BPF programs are so small and fast?
Generally speaking you would be right, but there are cases when BPF programs
are not small any more and placed on the hot execution path, e.g. if we talk
about a security system monitoring syscalls. In such situations even small
overhead is drastically multiplied and accumulated, and it only makes sense to
fully understand the system performance to avoid nasty surprises.</p>
<p>It seems many other people also would like to know more about this topic, thus
want to share results of my investigation.</p>
<ul>
<li>
<a href="#1-introduction">Introduction</a>
</li>
<li>
<a href="#2-current-state-of-things">Current state of things</a>
</li>
<ul>
<li>
<a href="#21-bpf-instruction-set">BPF Instruction Set</a>
</li>
<li>
<a href="#22-batching-of-map-operations">Batching of map operations</a>
</li>
<li>
<a href="#23-bloom-filter-map">Bloom filter map</a>
</li>
<li>
<a href="#24-task-local-storage">Task local storage</a>
</li>
<li>
<a href="#25-bpf-program-pack-allocator">BPF program pack allocator</a>
</li>
<li>
<a href="#26-bpf-2-bpf">BPF 2 BPF</a>
</li>
</ul>
<li>
<a href="#3-how-to-analyze-bpf-performance">How to analyze BPF performance?</a>
</li>
<ul>
<li>
<a href="#31-talking-to-the-compiler">Talking to the compiler</a>
</li>
<li>
<a href="#32-aggregated-counters">Aggregated counters</a>
</li>
<li>
<a href="#33-manual-instrumentation">Manual instrumentation</a>
</li>
<li>
<a href="#34-top-down-approach">Top-down approach</a>
</li>
<li>
<a href="#35-profiling-of-bpf-programs">Profiling of BPF programs</a>
</li>
</ul>
<li>
<a href="#4-modeling-of-bpf-programs">Modeling of BPF programs</a>
</li>
</ul>

<h2 id="2-current-state-of-things">2. Current state of things</h2>
<p>Whenever we analyse performance of some system, it’s always useful to get an
understanding of which features and parameters could improve efficiency or make
it worse. What is important to keep in mind when writing BPF programs? Looking
around I found a couple of interesting examples.</p>
<h3 id="21-bpf-instruction-set">2.1 BPF Instruction Set</h3>
<p>It turns out that there are several versions of BPF instruction set available,
namely v1, v2 and v3. Unsurprisingly they feature different set of supported
instructions, which could affect how the final program is performing. An
example from the <a href="https://www.kernel.org/doc/Documentation/bpf/bpf_design_QA.rst">documentation</a>:</p>
<div><div><pre><code>Q: Why BPF_JLT and BPF_JLE instructions were not
   introduced in the beginning?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
A: Because classic BPF didn&#39;t have them and BPF authors
felt that compiler workaround would be acceptable.
Turned out that programs lose performance due to lack of
these compare instructions and they were added.
</code></pre></div></div>
<p>This is a quite interesting example of differences between two ISA versions.
First, it shows the historical perspective and answers the question of how
the versions were evolving over time. Second, it mentions two particular
instructions <code>BFP_JLT</code> (jump if less than) and <code>BPF_JLE</code> (jump if less than or
equal), which were added into v2. Those are not something you would simply miss
among dozens of other instructions, those are basic comparison instructions!
For example, you write something like:</p>

<p>In this case ideally compiler would like to use <code>BPF_JLT</code> in order to produce
an optimal program, but it’s not available in BPF ISA v1. Instead, the compiler
has to do some extra mile and reverse the condition using only “jump if greater
than” instruction:</p>
<pre><code>; if (delta &lt; ts)
  cmp    %rsi,%rdi
; jump if above or equal
  jae    0x0000000000000068
</code></pre>
<p>As soon as we switch BPF ISA to v2 the same code will produce something more
expected:</p>
<pre><code>; if (delta &lt; ts)
  cmp    %rdi,%rsi
; jump is below or equal
  jbe    0x0000000000000065
</code></pre>
<p>In the case above both versions are equivalent, but if we’re going to compare
the value with a constant, the resulting implementation will require one more
register load. Interestingly enough I wasn’t able to produce such example
exactly, the compiler was stubbornly generating it only one way around for both
versions, but you can find an example in <a href="https://pchaigno.github.io/bpf/2021/10/20/ebpf-instruction-sets.html">this</a> blogpost.
But probably more dramatic changes one can notice between v1 and v3, which adds
32-bit variants of existing conditional 64-bit jumps. Using v1 to compare a
variable with a constant will produce the following result, when we have to
clean the 32 most-significant bits:</p>

<div><div><pre><code>; if (delta &lt; 107)
  15: (bf) r2 = r1
  16: (67) r2 &lt;&lt;= 32
  17: (c7) r2 s&gt;&gt;= 32
  18: (65) if r2 s&gt; 0x6a goto pc+3
</code></pre></div></div>
<pre><code>; if (delta &lt; 107)
  31:   mov    %r13d,%edi
  34:   shl    $0x20,%rdi
  38:   sar    $0x20,%rdi
  3c:   cmp    $0x6a,%rdi
  40:   jg     0x0000000000000048
</code></pre>
<p>Whereas using v3 there is no need to do so:</p>
<div><div><pre><code>; if (pid &lt; 107)
  15: (66) if w1 s&gt; 0x6a goto pc+3
</code></pre></div></div>
<pre><code>; if (pid &lt; 107)
  31:   cmp    $0x6a,%r13d
  35:   jg     0x000000000000003d
</code></pre>
<p>Those two flavours come with different performance characteristics, the former
version of the program has to do a bit more work to include the workaround. It
also makes the program bigger, using instruction cache less efficiently. Again,
you can get more numbers and the support matrix for Linux and LLVM version in
<a href="https://pchaigno.github.io/bpf/2021/10/20/ebpf-instruction-sets.html">this</a> great blogpost.</p>
<p>Unsurprisingly the default BPF ISA being used normally is the lowest one, v1
also known as “generic”, which means one has to configure it explicitly to use
something higher:</p>
<div><div><pre><code>$ llc probe.bc -mcpu=v2 -march=bpf -filetype=obj -o probe.o
</code></pre></div></div>
<p>With clang one can use <code>-mllvm -mcpu=v2</code> to do the same.</p>
<p>Few closing notes for this section. Instead of pinning one particular version
one could use <code>probe</code> value to tell the compiler to pick up highest available
ISA for <em>current</em> machine you’re compiling the program on (see the
corresponding <a href="https://github.com/llvm/llvm-project/blob/5ee13e6c65276fe03d9fd82aaf870d99cc9c7256/llvm/lib/TargetParser/Host.cpp#L419">part of the implementation</a>). Keep in mind it
could be a different machine, not the one which is going to run the program.
I’ve got quite curious about this feature, but for whatever reason it’s not
always work in those examples I was experimenting with, still defaulting to v1
where v2 was available. And last but not least: needless to say that the best
practice is always use the latest available instructions set, which is v3 at
the moment.</p>
<h3 id="22-batching-of-map-operations">2.2 Batching of map operations</h3>
<p>Maps are one of the core parts of BPF subsystem, and they could be used quite
extensively. One way of making data processing a bit faster (almost
independently of the context) is to batch operations. Sure enough there is
something like that for BPF maps as well, for example a generic support for
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=cb4d03ab499d4c040f4ab6fd4389d2b49f42b5a5">lookups</a> and <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=aa2e93b8e58e18442edfb2427446732415bc215e">modifications</a>.
One could use a corresponding subcommands:</p>
<div><div><pre><code><span>BPF_MAP_LOOKUP_BATCH</span>
<span>BPF_MAP_LOOKUP_AND_DELETE_BATCH</span>
<span>BPF_MAP_UPDATE_BATCH</span>
<span>BPF_MAP_DELETE_BATCH</span>
</code></pre></div></div>
<p>and parameters:</p>
<div><div><pre><code><span>struct</span> <span>{</span> <span>/* struct used by BPF_MAP_*_BATCH commands */</span>
    <span>__aligned_u64</span>   <span>in_batch</span><span>;</span>       <span>/* start batch,
                                     * NULL to start from beginning
                                     */</span>
    <span>__aligned_u64</span>   <span>out_batch</span><span>;</span>      <span>/* output: next start batch */</span>
    <span>__aligned_u64</span>   <span>keys</span><span>;</span>
    <span>__aligned_u64</span>   <span>values</span><span>;</span>
    <span>__u32</span>           <span>count</span><span>;</span>          <span>/* input/output:
                                     * input: # of key/value
                                     * elements
                                     * output: # of filled elements
                                     */</span>
    <span>__u32</span>           <span>map_fd</span><span>;</span>
    <span>__u64</span>           <span>elem_flags</span><span>;</span>
    <span>__u64</span>           <span>flags</span><span>;</span>
<span>}</span> <span>batch</span><span>;</span>
</code></pre></div></div>
<p>An alternative way would be to use <a href="https://lore.kernel.org/bpf/20200115184308.162644-8-brianvv@google.com/">libbpf</a> support.</p>
<p>Batching BPF map operations one can save on user/kernel space interaction. The
ballpark numbers mentioned in the mailing list were visible improvement for ~1M
record with the batch size 10, 1000, etc. For more details check out the
<a href="https://lore.kernel.org/bpf/20190829064517.2751629-1-yhs@fb.com/">original patch</a>.</p>
<h3 id="23-bloom-filter-map">2.3 Bloom filter map</h3>
<p>Continue the topic of diverse BPF map features we find out something very
curious among the typical structures like hash, array, queue –
a <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9330986c03006ab1d33d243b7cfe598a7a3c1baa">bloom filter</a> map. To remind you the classical definition:</p>
<div><div><pre><code>    A Bloom filter is a space-efficient probabilistic data structure
    that is used to test whether an element is a member of a set.
    False positive matches are possible, but false negatives are not
    – in other words, a query returns either &#34;possibly in set&#34; or
    &#34;definitely not in set&#34;. Elements can be added to the set,
    but not removed (though this can be addressed with the counting
    Bloom filter variant); the more items added, the larger the
    probability of false positives.
</code></pre></div></div>
<p>Independently of the context this is a great instrument in our hands, which
gives us possibility to trade off size of the data structure (we now have to
maintain both the hash map and the filter) for more efficient lookups. In case
of BPF normally checking the bloom filter before accessing the hash map would
improve the <a href="https://lore.kernel.org/bpf/20211027234504.30744-6-joannekoong@fb.com/">overall performance</a>, since a costly
hash map lookup will be avoided if the element doesn’t exist. We can turn this
on when needed by specifying the new map type:</p>
<div><div><pre><code><span>bpf_map_create</span><span>(</span><span>BPF_MAP_TYPE_BLOOM_FILTER</span><span>,</span>
    <span>NULL</span><span>,</span> <span>0</span><span>,</span> <span>sizeof</span><span>(</span><span>value</span><span>),</span> <span>100</span><span>,</span> <span>NULL</span><span>);</span>
</code></pre></div></div>
<h3 id="24-task-local-storage">2.4 Task local storage</h3>
<p>But there is more to it! Looking closely we also find something called <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a10787e6d58c24b51e91c19c6d16c5da89fcaa4b">task
local storage</a>, which gives us another
trade-off to play with for one particular use case. Imagine you would like to
write a BPF program that works with information about processes. To store
anything you would normally create a hash map and use PID as the key, then you
realize that large hash maps could be slow. To prevent that (and some other
inconveniences) you can use a task local storage instead, which is, surprise
surprise, a storage <em>local</em> to the owning task. And this means we’re getting a
performance boost accessing such local data on the account of its allocation
in a different place. The API is quite straightforward:</p>
<div><div><pre><code><span>ptr</span> <span>=</span> <span>bpf_task_storage_get</span><span>(</span><span>&amp;</span><span>start</span><span>,</span> <span>t</span><span>,</span> <span>0</span><span>,</span> <span>BPF_LOCAL_STORAGE_GET_F_CREATE</span><span>);</span>
</code></pre></div></div>
<p>Interesting enough that this feature is not unique one, there are
bpf_local_storage exists for sockets and inodes. For tasks, it was originally
implemented only for BPF LSM (Security Audit and Enforcement using BPF), and
then extended for tracing programs as well.</p>
<h3 id="25-bpf-program-pack-allocator">2.5 BPF program pack allocator</h3>
<p>One more topic somewhat related to the memory allocation is the program pack
allocator. It turns out that originally every single BPF program was consuming
one memory page, no matter how small the program is. Which means that if you
end up having a log of such small programs on your server you can observe high
instruction TLB (iTLB) pressure in the system, degrading the overall
performance. To solve this <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=57631054fae6dcc9c892ae6310b58bbb6f6e5048">bpf_prog_allocator</a> was
introduced, with the idea to pack programs together on a page when allocating
them. This happens transparently, nothing has to be done or configured, and the
only thing you may need to think about is whether there are available huge
pages on your machines (yes, the packer will try to pack on a huge page as
well). For more details check out the <a href="https://lwn.net/Articles/915005/">LWN article</a>.</p>
<h3 id="26-bpf-2-bpf">2.6 BPF 2 BPF</h3>
<p>Experimenting with the code layout is generally important for performance, and
BPF is not an exception. The thing I didn’t know was that prior to
<a href="https://docs.cilium.io/en/v1.12/bpf/#bpf-to-bpf-calls">Linux kernel 4.16 and LLVM 6.0</a> one always had to inline
any reusable shared pieces of code in a BPF program:</p>
<div><div><pre><code><span>#ifndef __inline
# define __inline                         \
   inline __attribute__((always_inline))
#endif
</span>
<span>static</span> <span>__inline</span> <span>int</span> <span>test_bpf2bpf</span><span>(</span><span>void</span><span>)</span> <span>{}</span>
</code></pre></div></div>
<p>If we look at this via <code>bpftool</code> this would obviously produce a single chunk of
code:</p>
<div><div><pre><code># 0xffffffffc1513a68:
  nopl   0x0(%rax,%rax,1)
  xor    %eax,%eax
  push   %rbp
# [...]
</code></pre></div></div>
<p>What’s the problem here? The same as with regular code inlining, it’s not
always the best choice: it eliminates a call, but increases the code size using
instruction cache less efficiently. Fortunately with modern versions of the
kernel you have an option to decide inline or not inline, where the latter will
show you two chunks of code with <code>bpftool</code>:</p>
<div><div><pre><code># 0xffffffffc15b810c:
  nopl   0x0(%rax,%rax,1)
  xor    %eax,%eax
# [...]
  callq  0x0000000000002370

# 0xffffffffc15ba47c:
  nopl   0x0(%rax,%rax,1)
  xchg   %ax,%ax
# [...]
</code></pre></div></div>
<h2 id="3-how-to-analyze-bpf-performance">3. How to analyze BPF performance?</h2>
<p>In the previous sections we’ve learned there are many interesting techniques on
the table to improve performance of your programs. But how to evaluate them,
how to introspect the program and understand what those approaches have
changed?</p>
<h3 id="31-talking-to-the-compiler">3.1 Talking to the compiler</h3>
<p>Things are getting started even before the program is executed. What if we ask
the compiler for an opinion about performance of our program? It’s not so
strange as it sounds, clang can generate an
<a href="https://clang.llvm.org/docs/UsersManual.html#options-to-emit-optimization-reports">Optimization Report</a> when compiling a program:</p>
<div><div><pre><code><span>$ </span>clang <span>-O2</span>
    <span>-Rpass</span><span>=</span>.<span>*</span>
    <span>-Rpass-analysis</span><span>=</span>.<span>*</span>
    <span>-Rpass-missed</span><span>=</span>.<span>*</span>
    <span># ...</span>
</code></pre></div></div>
<p>The report will contain quite many potential missed optimizations like this
one from Global Value Numering pass:</p>
<div><div><pre><code>remark: load of type i32
not eliminated [-Rpass-missed=gvn]
</code></pre></div></div>
<p>This record says that the compiler found a load operation that potentially
could be eliminated. That’s it, the compiler thinks the load is unnecessary, but
doesn’t have enough information to prove this for all possible use cases. In my
experiments this type of records were most frequent in the report, and I
speculate that this happens because in BPF programs we often pass some values
like an execution context via an opaque pointer:</p>
<div><div><pre><code><span>static</span> <span>__always_inline</span> <span>int</span> <span>bpf_example_fn</span><span>(</span><span>void</span> <span>*</span><span>ctx</span><span>)</span>
</code></pre></div></div>
<p>In such situations the compiler has an opportunity to apply certain
eliminations, but they are not allowed in case of overlapping pointers (i.e.
there are multiple pointers referring to the same memory). This is a hand-waving
explanation, but one thing points in its favour – I’ve manage to make compiler
to take some number of those missed optimizations by scattering <code>restrict</code>
keyword around:</p>
<div><div><pre><code><span>static</span> <span>__always_inline</span> <span>int</span> <span>bpf_example_fn</span><span>(</span><span>void</span> <span>*</span> <span>restrict</span> <span>ctx</span><span>)</span>
</code></pre></div></div>
<p>This keyword is the way of letting compiler known that there are no other
overlapping pointers exist.</p>
<h3 id="32-aggregated-counters">3.2 Aggregated counters</h3>
<p>Well, we’re finally running our BPF program. What is the first thing that comes
in mind for regular applications at this stage? Right, we check some global
counters like uptime and average load. Could we do this for the BPF program?</p>
<p>Fortunately for us Linux kernel collects BPF execution counters, when
instructed to do so:</p>
<div><div><pre><code><span>$ </span>sysctl <span>-w</span> kernel.bpf_stats_enabled<span>=</span>1
</code></pre></div></div>
<p>Those counters could be read via <code>bpftool</code>:</p>
<div><div><pre><code><span>$ </span>bpftool prog

379: raw_tracepoint <span>[</span>...] run_time_ns 35875602162 run_cnt 160512637
</code></pre></div></div>
<p>Another hacky way of getting how many times the BPF program was invoked would
be to use <a href="https://www.man7.org/linux/man-pages/man1/perf-record.1.html">memory events</a> with perf. Those events allow
monitoring access to certain address in memory, and finding out at which
address the BPF program was loaded we can use this feature to get the number of
hits (of course you can use <code>record</code> command instead of <code>trace</code>):</p>
<div><div><pre><code><span>$ </span><span>cat</span> /proc/kallsyms | <span>grep </span>bpf_prog
<span>[</span>...]
ffffffffc0201990 t bpf_prog_31e86e7ee100ebfd_test <span>[</span>bpf]

<span>$ </span>perf trace <span>-e</span> mem:0xffffffffc0201990:x
<span>[</span> trigger the BPF program <span>in </span>another session <span>]</span>
18446744073790.551 <span>ls</span>/242 mem:0xffffffffc0201990:x<span>()</span>
</code></pre></div></div>
<p>Those numbers are overall counters for the program, which is useful to get some
ballpark numbers about the performance to make some sanity check or verify the
results obtained using other methods.</p>
<h3 id="33-manual-instrumentation">3.3 Manual instrumentation</h3>
<p>The simplest next step in understanding what’s going on inside the BPF program
would be to instrument it manually. I mean, nobody will prevent you from
modifying your own program, right?</p>
<p>You could write something like this inside the program:</p>
<div><div><pre><code><span>bpf_trace_printk</span><span>(</span><span>&#34;Timestamp: %lld&#34;</span><span>,</span> <span>ts</span><span>);</span>
</code></pre></div></div>
<p>Then fetch the timestamps from either a trace_pipe, or using <code>bpftool</code>:</p>
<div><div><pre><code><span>$ </span><span>cat</span> /sys/kernel/debug/tracing/trace_pipe
<span>$ </span>bpftool prog tracelog
</code></pre></div></div>
<p>This is rather flexible and easy approach, but unfortunately it introduces a
lot of overhead due to constant user- to kernel space communication, which
makes it not quite practical in most of the situations. If something like this
is really needed, instead of using trace_pipe one can introduce the map to
buffer data on the kernel side, then periodically read this map.</p>
<h3 id="34-top-down-approach">3.4 Top-down approach</h3>
<p>Now lets step back and think. What cool kids are using to analyse the
application performance? For example the BPF program pack allocator from the
previous section, how people have identified that the high iTLB cache pressure
is the issue?</p>
<p>Normally there are two options:</p>
<ul>
<li>
<p>Either one have enough years of experience, making it possible to stare at
the screen for an hour and then exclaim “A-ha! We need to check iTLB
counters!”</p>
</li>
<li>
<p>Or one uses some method to figure out the solution step by step, for example
Top-Down approach.</p>
</li>
</ul>
<p>Top-Down analysis method (see
<a href="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html">Intel® 64 and IA-32 Architectures Optimization Reference Manual, Appendix B.1</a>
) helps to identify performance bottlenecks correlating to major functional
blocks of modern out-of-order microarchitectures. At the first level they’re
falling into four categories (see more details in
<a href="https://book.easyperf.net/perf_book">Performance analysis and tuning on modern CPUS</a> book):</p>
<ul>
<li>
<p>Front-End Bound. The modern CPU Front-End main purpose is to efficiently
fetch and decode instructions from memory and feed them to the CPU
Back-End. Front-End Bound most of the time means the Back-End is waiting for
instructions to execute, while Front-End is not able to provide them. An
example is an instruction cache miss.</p>
</li>
<li>
<p>Back-End Bound. The CPU Back-End is responsible for actual execution of
prepared instructions employing Out-Of-Order engine and store results.
Back-End Bound means the Front-End has fetched and decoded instructions, but
the Back-End is overloaded and can’t take more to do. An example is
a data cache miss.</p>
</li>
<li>
<p>Bad Speculation</p>
</li>
<li>
<p>Retiring</p>
</li>
</ul>
<figure>
<img src="https://blog.jonnew.com/public/img/bpf-perf/top-down-level-1.small.png" width="80%"/>
<figcaption>
<p><small>Intel® 64 and IA-32 Architectures Optimization Reference Manual, Appendix B.1</small></p>
</figcaption>
</figure>

<p>Now, armed with this method, if we face a system with multitude of BPF programs
loaded, we can start categorizing bottlenecks and find out that the system is
“Front End Bound”. Digging few levels deeper we can discover that the reason is
iTLB cache pressure, which contributes to the “Front End Bound” load. The
only question is how to actually do that?</p>
<p>Well, theoretically <code>perf</code> does support the first level of Top-Down approach
and can collect stats for specified BPF programs:</p>
<div><div><pre><code><span>$ </span>perf <span>stat</span> <span>-b</span> &lt;prog <span>id</span><span>&gt;</span> <span>--topdown</span>
</code></pre></div></div>
<p>This machinery works thanks to <code>fentry</code>/<code>fexit</code> attachment points, that allows
to attach an instrumentation BPF program at the start/end of another BPF
program. It’s worth talking bit more about this approach, as it works in
quite an interesting way. Here is what happens with the BPF program when one
has attached an <code>fentry</code> program to it:</p>
<div><div><pre><code># original program instructions
nopl   0x0(%rax,%rax,1
xchg   %ax,%ax
push   %rbp
mov    %rsp,%rbp
sub    0x20,%rsp
...

# instructions after attaching fentry
callq  0xffffffffffe0096c
xchg   %ax,%ax
push   %rbp
mov    %rsp,%rbp
sub    0x20,%rsp
...
</code></pre></div></div>
<p>Kernel modifies the BPF program prologue to execute the <code>fentry</code>, which is a
remarkably flexible and powerful approach. There are of course certain
limitations one have to keep in mind: the target BPF program has to be compiled
with BTF, and one cannot attach <code>fentry</code>/<code>fexit</code> BPF programs to another
programs of the same type (see the verifier for both <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/bpf/verifier.c#n16488">here</a>
and <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/bpf/verifier.c#n16531">here</a>).</p>
<p>At the end of the day using this approach should give us something like this:</p>
<div><div><pre><code>Performance counter stats <span>for</span> <span>&#39;system wide&#39;</span>:

             retiring  bad speculation  frontend bound  backend bound
S0-D0-C0  1  64.2%             8.3%           19.1%          8.5%
</code></pre></div></div>
<p>Unfortunately this didn’t work for me, perf was always returning nulls. Digging
a bit deeper I’ve noticed that reading the actual perf events was returning
empty result, still not sure why (if you have any ideas, let me know).</p>
<p>Trying to figure out if it’s an intrinsic limitation or simply a strange bug in
my setup I’ve picked up a <code>bpftool prog profile</code> command. It’s a great
instrument for our purposes which works using <code>fentry</code>/<code>fexit</code> as well,
allowing us to collect certain set of metrics:</p>
<div><div><pre><code>bpftool prog profile &lt;PROG&gt; [duration &lt;sec&gt;] &lt;METRICS&gt;
</code></pre></div></div>
<p>The metrics are one of <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/bpf/bpftool/prog.c#n1937">following</a>:</p>
<ul>
<li>
<p>cycles / <code>PERF_COUNT_HW_CPU_CYCLES</code></p>
</li>
<li>
<p>instructions per cycle / <code>PERF_COUNT_HW_INSTRUCTIONS</code></p>
</li>
<li>
<p>l1d_loads / <code>PERF_COUNT_HW_CACHE_L1D</code> + <code>RESULT_ACCESS</code></p>
</li>
<li>
<p>llc_misses per million instructions / <code>PERF_COUNT_HW_CACHE_LL</code> + <code>RESULT_MISS</code></p>
</li>
<li>
<p>itlb_misses per million instructions / <code>PERF_COUNT_HW_CACHE_ITLB</code> + <code>RESULT_MISS</code></p>
</li>
<li>
<p>dtlb_misses per million instructions / <code>PERF_COUNT_HW_CACHE_DTLB</code> + <code>RESULT_MISS</code></p>
</li>
</ul>
<p>This is a lot of information we could collect doing something like this:</p>
<div><div><pre><code><span>$ </span>bpftool prog profile <span>id</span> &lt;prog <span>id</span><span>&gt;</span> duration 10 cycles instructions

                11 run_cnt
            258161 cycles
             50634 instructions        <span>#     0.20 insns per cycle</span>
</code></pre></div></div>
<p>Alas, it doesn’t help in case of Top-Down approach. As an experiment I’ve added
few perf counters from Top-Down to <code>bpftool</code> and it was able to do the thing
and read those counters (the full implementation would be of course much more
involving). At the end it means one can successfully apply Top-Down approach in
case of a BPF program, which is great news.</p>
<h3 id="35-profiling-of-bpf-programs">3.5 Profiling of BPF programs</h3>
<p>This brings us forward in our quest of understanding what’s going on inside a
BPF program, but we’re still missing one point. To get most out of it we need
not only to sample certain events between two points in the program (e.g.
<code>fentry</code>/<code>fexit</code>), but also be able to profile the program in general, i.e.
sample needed events and attribute them to the corresponding parts of the
program. Can we do that?</p>
<p>The question has two parts: can we do stack sampling, and is it possible to
correlate results with the BPF program code? Surely we can do stack sampling,
as our BPF program is sort of a “dynamic” extension of the kernel, sampling the
kernel will sample the program as well. How to relate it to the code?
Fortunately for us <code>perf</code> is smart enough to do that for us if the BPF program
build with BTF support. Here is an example of a perf report collecting cycles
without actually retired uops:</p>
<div><div><pre><code>$ perf report
  Percent | uops_retired.stall_cycles
          :
          : if (duration_ns &lt; min_duration_ns)
     0.00 :    9f:movabs $0xffffc9000009e000,%rdi
     0.00 :    a9:mov    0x0(%rdi),%rsi
          :
          : e = bpf_ringbuf_reserve(...)
    21.74 :    ad:movabs $0xffff888103e70e00,%rdi
     0.00 :    b7:mov    $0xa8,%esi
     0.00 :    bc:xor    %edx,%edx
     0.00 :    be:callq  0xffffffffc0f9fbb8
</code></pre></div></div>
<p>From this report we see there is a hot spot of stall events around
<code>bpf_ringbuf_reserve</code>, so we may want to look deeper into this helper to
understand what’s going on.</p>
<p>But there is a small catch, I’ve mentioned before that we sample our BPF
program by sampling the kernel. Now, if you have relatively busy system that
actually does something useful except only running some BPF stuff, you will get
a lot of data this way, like tons and tons of samples, only fraction of which
is actually of any interest to you. To make it manageable one can try to
capture only kernel part of stacks, maybe isolate the BPF program on a single
core if the environment allows that, but it still would be quite a lot.</p>
<p>At some point I’ve realized there is a <code>--filter</code> perf option
to do an efficient filtering of the data using <a href="https://man7.org/linux/man-pages/man1/perf-intel-pt.1.html">Intel PT</a>,
and in theory one could do something like this:</p>
<div><div><pre><code><span># with the program name</span>
<span>$ </span>perf record <span>-e</span> intel_pt// <span>--filter</span> <span>&#39;filter bpf_prog_9baac7ecffdb457d&#39;</span>

<span># or with the raw address</span>
<span>$ </span>perf record <span>-e</span> intel_pt// <span>--filter</span> <span>&#39;start 0xffffffffc1544172&#39;</span>
</code></pre></div></div>
<p>Unfortunately it seems this is not supported for BPF:</p>
<div><div><pre><code>failed to set filter &#34;start 0xffffffffc1544172&#34; on event intel_pt// with 95 (Operation not supported)
</code></pre></div></div>
<p>Nevertheless, having both Top-Down approach and profiling for BPF opens quite
interesting opportunities. Let’s take a look at the previous example with the
program allocator packer. The idea was to reduce overhead (in the form of iTLB
pressure) via packing BPF programs together on a page, without knowing anything
else about those programs. Now let’s make one more step forward and try to
apply this in the real world, where quite often it’s not a single BPF program,
but a chain of programs processing different part of an event and connected
together via tail calls. Packing such programs together may produce a situation
when some more frequently invoked (hot) pieces are located on the same page
with less demanded (cold):</p>
<p>
<img src="https://blog.jonnew.com/public/img/bpf-perf/pack_allocator.png" width="80%"/>
</p>
<p>Such placement would not be as optimal as having most of the hot programs
together, increasing number of pages that have to be frequently reached to
execute the full chain of processing. But to define an “optimal” placing we
need to know what are the hot and cold programs, so we end up with applying
both approaches mentioned above: first identify the largest bottleneck (iTLB),
then use profiling to find out how to pack everything more efficiently. Note,
that nothing like this is of course implemented – although one can try to
manipulate the order in which programs are loaded to influence the final
placement.</p>
<h2 id="4-modeling-of-bpf-programs">4. Modeling of BPF programs</h2>
<p>We were talking about great many details that could affect performance of a BPF
program. There are even more factors (not necessarily BPF only) we were not
talking about. In this complex picture of many actors interacting in different
ways, how to get a prediction about performance of your program? An intuition
may fail you, because sometimes improvements in one area reveal an unexpected
bottleneck in another. One can benchmark the program under the required
conditions, but often it’s resource intensive to cover many cases. And
obviously it could be done only when the program is already written, and it’s
even more problematic to fix a performance issue. The last option left on the
table is to model the program behaviour and experiment with the model instead
of an actual implementation. This would be much faster, but the catch is that
the results are only as good as the model allows it.</p>
<figure>
<img src="https://blog.jonnew.com/public/img/bpf-perf/build-model.png" width="50%"/>
<figcaption>
<p><small>&#34;The Thrilling Adventures of Lovelace and Babbage&#34;, Sydney Padua, 2015</small></p>
</figcaption>
</figure>

<p>Nevertheless, even <a href="https://brooker.co.za/blog/2022/04/11/simulation.html">simplest models</a> could give very valuable
feedback. Inspired by the article above from Marc Brooker I wanted to see what
it takes to simulate a simple BPF program execution. As an example I took
<code>runqslower</code> program from the Linux kernel repository, that uses a BPF program
to trace scheduling delays. Its implementation was recently switched to use
task local storage instead of a hash map, which is an interesting study case.
To make it even simpler for my purposes I’ve tried to simulate not the full
difference between two implementations (task local storage is simply faster),
but how much effect memory access in the hash map (slower with more elements in
the map) has on the final performance.</p>
<p>In few words the simulation goes like this:</p>
<ul>
<li>The program behaviour is specified via a state machine.</li>
<li>Transitions between states are handled in the event loop.</li>
<li>Every transition is associated with a certain “resource” consumed, in our
case simply execution time.</li>
<li>Initial state as well as execution time for every state are specified as
random values with a distribution of a certain type.</li>
</ul>
<p>The state machine from the task point of view looks like this:</p>
<p>
<img src="https://blog.jonnew.com/public/img/bpf-perf/state-machine.png" width="20%"/>
</p>
<ul>
<li>When the task is ready to be executed, it’s being added into a scheduler
queue.</li>
<li>The BPF program adds a record into the hash map containing the PID and the
timestamp of the new task.</li>
<li>The task spends some time in the queue, then gets dequeued to run for a slice
of time.</li>
<li>The BFP program picks up the recorded PID and timestamp to report them.</li>
<li>The task finishes the time slice, then waits to get enqueued again.</li>
</ul>
<p>Every event from the state machine needs to have an associated latency
distribution, which would tell how much time the system has spent in this
state. For simplicity, I’ve taken the normal distribution with the following
mean/stddev:</p>
<ul>
<li>Time spend in the queue: ~4/1</li>
<li>Process enqueue in the BPF program: ~400/10</li>
<li>Process dequeue in the BPF program: ~200/10</li>
<li>Time slice for the task to run: 3000000 (sched_min_granularity_ns)/1000</li>
<li>Report using <code>bpf_perf_event_output</code> helper: ~24/8</li>
</ul>
<p>Now a warning. No, not like this – a <em>WARNING</em>: It’s hard to come up with
real numbers – pretty much like in quantum physics if we try to measure
something it will be skewed. It means that the values above should be
considered only as an approximation, roughly measured on my laptop under
certain conditions (to be more precise when <code>runqslower</code> measures latencies
slower than 100ns and some processes are getting spawn in a loop). This is the
part when we do quality trade-offs about the model.</p>
<p>Another point is the distribution type. Of course using normal distribution is
unrealistic, as most likely those have to be some long-tailed distribution if
we’re dealing with a queue-like process or an exponential distribution if some
events are happening independently at a certain constant average rate. This is
one of the parts that makes our model “simple”.</p>
<p>None of those points stops us from experimenting, or should prevent you from
improving the model to be more realistic.</p>
<p>We encode few more important factors in the model, together with the state
machine itself:</p>
<ul>
<li>
<p>First, without some type of process contention it’s not going to be
interesting. To represent that lets introduce an additional overhead, which
will be linearly growing with the number of processes running on a CPU code
starting from a certain threshold. This would be similar to the saturation,
when the execution queue is full. It means the latency of running the process
would be a random variable taken from normal distribution normalized by
contention level. The latter one is simplified to the CPU load when it’s
higher than a certain constant (in the implementation CPU load is expressed
via number of events happening on the core, and I’m using 4 as the threshold,
probably instinctively thinking about the pipeline width of modern CPUs).</p>
</li>
<li>
<p>Second, to represent the difference between a hash map and a task local
storage we add another type of overhead depending on the size of hash map.
The constants here are pretty arbitrary, I’ve implemented it as the map size
divided by 2<sup>10</sup> in the second degree.</p>
</li>
</ul>
<p>Here is how my final implementation looks like:
<a href="https://github.com/erthalion/runqslower-simulation">runqslower-simulation</a>.
Warning, it’s the lowest quality Haskell code you’ve seen, riddled with bugs
and inconsistencies, but I hope it shows the point. To summarize:</p>
<ul>
<li>
<p>We record time spent in various states, taking into account all the quirks
like contention in the system.</p>
</li>
<li>
<p>Sum all the latencies related to BPF events (plus reporting if the value we
want to record is higher than the threshold set for <code>runqslower</code>).</p>
</li>
<li>
<p>Sum all the processing latencies.</p>
</li>
<li>
<p>To calculate overhead of the BPF program we compare these two sums to see
which portion of runtime was spent doing bookkeeping.</p>
</li>
</ul>
<p>Having everything in place we’ve defined the model, now let’s talk about the
input and the output. The initial conditions to start the simulation are
completely up to us, so let’s set it to a bunch of processes starting together
and doing something for a certain amount of time. The output of the model for
us would be records of time spent doing actual job (positive payload) versus
time spent running the BPF parts (bookkeeping overhead). The results would look
like this:</p>
<p>
<img src="https://blog.jonnew.com/public/img/bpf-perf/overhead-modeling.png" width="80%"/>
</p>
<p>As expected on the graph we observe that hash map performs slower than task
local storage, and even can derive at which point in time this difference
become significant. The dynamics of this model could be directed via number of
processes and their lifetime, producing results in the phase space that are
either converging to null (when there is not enough work to do for BPF, and
process runtime dominates), exploding into the infinity (when there are too
many short-lived processes and BPF overhead is getting more and more
significant), or oscillating around certain value (when both BPF and process
runtime are balancing each other, producing stable system).</p>
<h2 id="5-summary">5. Summary</h2>
<p>In this blog post we were talking great deal about performance of BPF programs,
the current state of things and various analysis approaches starting from
simplest to most intricate. I’m glad to see that BPF subsystem lately started
to provide flexible set of features to tune efficiency for different use cases,
and maturing introspection techniques. Those experiment I’ve conducted along
the lines were driven mostly by curiosity, but I hope the results will help you
to reason about your BPF programs with more confidence.</p>
<h2 id="acknowlegements">Acknowlegements</h2>
<p>Huge thanks to Yonghong Song, Joanne Koong and Artem Savkov for reviews and
commentaries!</p>
</div></div>
  </body>
</html>
