<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model">Original</a>
    <h1>Stable Video Diffusion</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
      
      
      
      
      
      
      <div data-content-field="main-content" data-item-id="">
  <article id="article-">
  
    <div>
      

      <div>
        <div><div data-layout-label="Post Body" data-type="item" id="item-655c90f932dda45e84c36b21"><div><div><div data-block-type="2" id="block-8551f07839a99bc18bfc"><div>

<div>
  <p>Today, we are releasing Stable Video Diffusion, our first foundation model for generative video based on the image model Stable Diffusion. </p><p>Now available in research preview, this state-of-the-art generative AI video model represents a significant step in our journey toward creating models for everyone of every type.</p><p>With this research release, we have made the code for Stable Video Diffusion available on our <a href="https://github.com/Stability-AI/generative-models" target="_blank"><span>GitHub repository</span> &amp; t</a>he weights required to run the model locally can be found on our <a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt" target="_blank"><span>Hugging Face page</span></a>. Further details regarding the technical capabilities of the model can be found in our <a href="https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets"><span>research paper</span></a>.</p>
</div>



</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1700590321475_5634"><div>

<div>
  <p><strong>Adaptable to Numerous Video Applications</strong></p><p>Our video model can be easily adapted to various downstream tasks, including multi-view synthesis from a single image with finetuning on multi-view datasets. We are planning a variety of models that build on and extend this base, similar to the ecosystem that has built around stable diffusion.</p>
</div>



</div></div><div data-block-json="{&#34;blockAnimation&#34;:&#34;none&#34;,&#34;layout&#34;:&#34;caption-below&#34;,&#34;overlay&#34;:false,&#34;description&#34;:{&#34;html&#34;:&#34;&lt;p class=\&#34;\&#34; style=\&#34;white-space:pre-wrap;\&#34;&gt;Sample multi-view generations from our finetuned video model&lt;/p&gt;&#34;},&#34;hSize&#34;:null,&#34;floatDir&#34;:null,&#34;isOldBlock&#34;:false,&#34;settings&#34;:{&#34;muted&#34;:true,&#34;autoPlay&#34;:true,&#34;loop&#34;:true,&#34;controls&#34;:&#34;full&#34;},&#34;url&#34;:&#34;&#34;,&#34;resolvedBy&#34;:&#34;native&#34;,&#34;nativeVideo&#34;:&#34;655d01210e1a04696b737615&#34;}" data-block-type="32" id="block-yui_3_17_2_1_1700593418987_8015"><div><div><div><p>Sample multi-view generations from our finetuned video model</p></div></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1700590128063_5646"><div>

<p>In addition, today, you can sign up for our waitlist <a href="http://harihareswara.net/contact"><span>here</span></a> to access a new upcoming web experience featuring a Text-To-Video interface. This tool showcases the practical applications of Stable Video Diffusion in numerous sectors, including Advertising, Education, Entertainment, and beyond.</p>



</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1700591052078_6172"><div>

<div>
  <p><strong>Competitive in Performance</strong></p><p>Stable Video Diffusion is released in the form of two image-to-video models, capable of generating 14 and 25 frames at customizable frame rates between 3 and 30 frames per second. At the time of release in their foundational form, through external evaluation, we have found these models surpass the leading closed models in user preference studies.</p>
</div>



</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1700589340688_4726"><div>










































  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg" data-image-dimensions="2032x825" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg" width="2032" height="825" alt="" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"/>

                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1700589340688_3627"><div>

<div>
  <p><strong>Exclusively for Research</strong></p><p>While we eagerly update our models with the latest advancements and work to incorporate your feedback, we emphasize that this model is not intended for real-world or commercial applications at this stage. Your insights and feedback on safety and quality are important to refining this model for its eventual release. </p><p>This aligns with our previous releases in new modalities, and we look forward to sharing the full release with you all. </p><p><strong>Our Ever-Expanding Suite of AI Models</strong></p><p>Stable Video Diffusion is a proud addition to our diverse range of open-source models. Spanning across modalities including image, language, audio, 3D, and code, our portfolio is a testament to Stability AI’s dedication to amplifying human intelligence.</p><p>Stay updated on our progress by signing up for our <a href="https://stability.ai/home#newsletter"><span>newsletter</span></a> and discovering more about commercial applications by contacting us <a href="https://stability.ai/contact"><span>here</span></a>.</p><p>Follow us on<a href="https://twitter.com/stabilityai" target="_blank"> <span>Twitter</span></a>,<a href="https://www.instagram.com/stability.ai/" target="_blank"> <span>Instagram</span></a>, <a href="https://www.linkedin.com/company/stability-ai" target="_blank"><span>LinkedIn</span></a>, and join our<a href="https://discord.gg/stablediffusion" target="_blank"> <span>Discord Community</span></a>.<br/></p>
</div>



</div></div></div></div></div></div>

        

        
        
          
        
      </div>

      
    </div>
  
</article>

</div>
    </div>
  
  </div></div>
  </body>
</html>
