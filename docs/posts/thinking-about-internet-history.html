<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://content.cooperate.com/post/internet_history/">Original</a>
    <h1>Thinking About Internet History</h1>
    
    <div id="readability-page-1" class="page"><div><main><div><div><ul><li><em></em>
<span>30/12/2023</span></li><li><em></em>
<span>15-minute read</span></li></ul><p>I could have happily been a librarian.</p><p>I spent the glorious summer of 1988 as an intern in my university’s library system,
learning what it was like to work in the various departments. I got to sample it all:
the patient craft of the book restorers, the voices of the dead in Manuscripts and Archives,
the exotic tastes of the special collections managers, the ever-present fear of <a href="https://lithub.com/no-beinecke-library-is-not-specially-designed-to-suffocate-humans-in-the-event-of-a-fire/">halon
suffocation</a> in the sealed glass cages of the rare book collection.</p><p>But what I remember most was the impossible challenge of the acquisitions department. In this
pre-Internet era, a major research library
might aspire to acquire a percent or two of the world’s printed output each year, using
paper and pencils, phone calls and fax machines.</p><p>Figuring out what to acquire, and how to integrate it into the collection, was a largely
manual process that relied on the personal global networks of dozens of professional
librarians, driven by the bibliographic hunger of thousands of affiliated academics.
And this was just one university library out of thousands in the world, all working toward
that same goal, which would outlive any of the people who worked on it: to sample the
most important parts of the world’s recorded knowledge, index and organize it, and make it
available to researchers, <em>forever.</em></p><p>In short, a research librarian’s task was (and is) both gloriously inspiring, and hilariously
impossible. If you stopped to think about it for even a moment, you’d give up and choose
another profession that paid better. The people who successfully dedicated long careers
to the library system were therefore very special people.</p><h2 id="an-argument-for-internet-history">An Argument for Internet History</h2><p>I thought about these librarians recently as I was contemplating what should be a much easier
challenge: piecing together the recorded history of the Internet as a resource for future
historians.</p><p>The Internet has now grown to something like a billion connected hosts, and is inextricably
woven together with politics, economics, public health, education, .. in short, <em>civilization.</em>
An artifact that valuable has to fend off continuous capture by money and power interests.
To avoid accidentally breaking what made it great, we need to
understand and communicate about that greatness using more than anecdotes – we’re going to
need to build an <a href="https://ssir.org/articles/entry/the_next_era_of_evidence_based_policymaking/">evidence-based</a>
case for preserving the Internet.</p><p>Historians who study our times will desperately want to quantify the
effects the Internet had on all aspects of society, in all the places on earth,
as the network of networks came ashore and become central to every aspect of everyone’s life.
But what primary sources will they use to understand the Internet’s expansion during our lifetimes?</p><h2 id="three-steps">Three Steps</h2><p>If we want to make sure the Internet’s story is preserved for future scholars in a
quantifiable way, and pull together the data to defend it against irreversible damage,
we basically have three big collective tasks to undertake before we all forget how it worked:</p><ul><li><p><em>Preserve</em> the history, by gathering the irreplaceable records of how the Internet grew</p></li><li><p><em>Curate</em> the history to interpret it and make it accessible and meaningful for future scholars</p></li><li><p><em>Explore</em> the history, creating tools and visualizations that everyone can enjoy and celebrate</p></li></ul><p>What follows is a brief overview of what I think that process might entail, mostly in the form of notes
to myself to help me figure out what I should be working on in 2024. If any of this strikes
a chord with you, <a href="mailto:jim.nh.us@gmail.com">drop me a note</a>
and I’ll keep you in the loop.</p><h2 id="step-1-preservation">Step 1: Preservation</h2><p>Today’s Internet consists of about a
billion communicating hosts (things that have their own IP address), arranged into about a
million routed networks (groups of IP addresses with common reachability), collectively
managed by voluntary interconnection among about a hundred thousand autonomous systems
(organizations that are responsible for their own Internet routing policy).</p><p>Fortunately, the Internet is somewhat self-documenting, because it can’t help talking
about itself constantly. BGP (Border Gateway Protocol) encourages all of those autonomous
systems to continuously whisper and compare notes with each other about the best ways packets
should traverse the global network to reach their destinations.</p><p>A timestamped recording of this “BGP whispering” provides what human historians always hope to
find when they visit Manuscripts and Archives: a contemporaneous record of exactly what the
participants in history were experiencing, in their own voice, at the moment when it happened.</p><p>That means that if you play a set of BGP update streams back today, you can reconstruct a
version of the “state of the Internet” as it appeared
at any second in history – at least, to a level of fidelity constrained by the
number of independent BGP perspectives you managed to record from around the world on that day.
At least in terms of reconstructing the interprovider relationships, which document how all the IP
addresses are connected to each other administratively, this job is, dare I say,
moderately straightforward.</p><h3 id="some-good-news-and-bad-news-about-bgp-preservation">Some Good News and Bad News about BGP Preservation</h3><p>The good news is that there are at least two major surviving repositories of historical BGP data
that we can combine to get the best understanding of Internet history:
the <a href="https://www.ripe.net/analyse/internet-measurements/routing-information-service-ris">Routing Information Service</a> (maintained by the RIPE NCC in Amsterdam, starting in 1999) and the
<a href="https://routeviews.org/">Oregon Routeviews</a> project (from 2001, with some data back to 1997).
But if anything were to happen
to these projects, due to disaster or financial constraints, their data would be literally
irreplaceable.</p><p>The existential threat to our common history is not hypothetical.
<a href="https://www.businesswire.com/news/home/20120423005244/en/Renesys-Granted-Patent-for-Monitoring-Internet-Routing-by-United-States-Patent-Office">Renesys</a>, the company
I cofounded in 2000, once managed a third significant repository with more than a decade
of historical BGP data, including hundreds of <a href="https://www.peeringdb.com/net/21">BGP peers</a> hand-selected to offer complementary
perspectives to the RIPE and Oregon peer sets. Sadly, those datasets did not survive
multiple corporate acquisitions and are now believed to have been
<a href="https://docs.oracle.com/en-us/iaas/releasenotes/changes/d55c9503-ff50-41d1-b0c1-069ba869de22/">lost.</a></p><p>In 2024, I’d like to find additional homes for the repos of BGP data that survive, for the sake of
preservation. In the wise words of the librarians,
<a href="https://perma.cc/D8KY-B93F">Lots Of Copies Keep Stuff Safe.</a></p><h3 id="what-else-should-we-preserve">What Else Should We Preserve?</h3><p>Having preserved the basic shape of interdomain routing, there are plenty of other
historical Internet datasets that we’d like to have in order to put flesh on the bones,
particularly when it comes to interpreting what all those billion hosts were actually <em>doing</em>
within society as the years ticked by.</p><p>Every time someone in the Internet records a measurement (resolves a DNS domain to an IP address,
runs a ping to see whether a given host is alive, runs a traceroute to see what path the
packets are taking to reach a given host, retrieves a web page to see how long content takes
to arrive), they’ve performed a natural experiment that can never be run again.</p><p>Several foundational repositories of active measurements exist, including
<a href="https://catalog.caida.org/software/archipelago">CAIDA’s Ark project</a>
(since 2007), <a href="https://atlas.ripe.net/docs/faq/general.html">RIPE’s Atlas</a>
(since 2010), and the <a href="https://www.measurementlab.net/">MLab</a>
trace set (since 2013). There may be
earlier sets of measurements that were collected by the public, or by individual network operators,
that could be used to push the horizon of active performance measurement back before 2007. If
you have any of these <a href="http://archives.real-time.com/tclug-list/2000/Mar/msg00928.html">dusty tarballs</a>
of data lurking in your backup tapes, please consider
their preservation – you almost certainly have unique observations of how packets actually
crossed the historical Internet over time.</p><p>Besides active measurements, of course, we’ll also need to preserve the records of registry
data — who each of these network resources was assigned to on each day in
history, from <a href="https://www.arin.net/reference/materials/data/">ARIN</a>, <a href="https://www.ripe.net/manage-ips-and-asns/db">RIPE</a>, and <a href="https://www.apnic.net/about-apnic/whois_search/about/what-is-in-whois/">APNIC</a> — as well as anything we can find about the DNS names that
were associated with each IP address on a given day. These are the collective clues to what
all these Internet hosts were up to, as well as providing clues to where on earth they were
likely located.</p><h3 id="reconstructing-the-internet-as-a-point-in-time-database">Reconstructing the Internet as a Point-in-Time Database</h3><p>Finally, all this DNS and registry data is strongly ephemeral, meaning that it can change from
day to day without warning. That makes it imperative that we keep track of the time of each
of our ephemeral observations, if we want to later build credible metrics for things like the
density of Internet hosts within a given region.</p><p>Recall that in the 2010s, IPv4 exhaustion triggered waves of sales and international
reassignments of network address blocks, so that (for example) a block of network addresses
that had been hosting DSL customers in
<a href="https://www.pcworld.com/article/426844/how-romanias-patchwork-internet-helped-spawn-an-ip-address-industry.html">Romania</a>
might vanish from the Internet for a while,
and then reappear serving web pages in a datacenter in Saudi Arabia. Internet geography
changes quickly, so we don’t just need a geolocation map of all the IP addresses, and some
sense of what each IP address was being used for. We also need to know what that
map looked like on each day in history over decades, as the hosts and resources associated with
each IP address moved around and changed their functions.</p><p>Are you disheartened yet? No? Are you excited about scouring the world to find and fit
together all the pieces of this enormous space-and-time puzzle? Excellent, you’re
showing signs of “librarian spirit.” Let’s keep going!</p><h2 id="step-2-expose-the-narrative">Step 2: Expose the Narrative</h2><p>Once we’ve succeeded in preserving all of our endangered digital datasets,
we can get down to the business of curation and interpretation. Most Internet
measurement research has focused on the operational questions of the here and now:
monitoring for slowdowns and shutdowns within and between providers, figuring
out how the Internet is routing traffic around that damage. Questions of historical
evolution tended to be secondary.</p><p>I predict that we can find new ways to look at the Internet through historical eyes,
to get past this “operations trap.” If we regard the Internet as just another complex
process that was informing, and being informed by, everything else that was going on
in society at the time, we’re going to need some stable metrics and metaphors that we can
carry forward through time for a couple decades, as input to all the other models
of what was going on simultaneously.</p><p>If we do it right, we might even be able to perform statistical tests for
<a href="https://en.wikipedia.org/wiki/Transfer_entropy"><em>information transfer</em></a>
among our time series, and we might finally be able to answer some
tantalizing social science
questions: does the evolving structure of your Internet environment exert a quantifiable
influence on the growth of your economy, or the probability of violence against civilians,
or levels of voter participation, or secondary school graduation rates, or life expectancy?</p><h3 id="rethinking-measurement-and-starting-over">Rethinking Measurement and Starting Over</h3><p>Here’s the fun part: I don’t know with certainty what metrics-and-metaphors I would
choose to extract from the historical data to characterize Internet structure,
if I were starting over today with the raw stuff. Everything we did to date was
either operationally oriented (“the Internet is broken! now it’s fixed!”) or focused on
geopolitically reductive metrics that don’t really describe how the Internet works
(“the Russian Internet grew by 12% in the last decade!”)</p><p>I am personally to blame for promulgating many of the latter sorts of statistics. Over
and over and over in my career, I’ve made <a href="https://content.cooperate.com/presentations/menog9/">presentations</a>
that purported to compare one national Internet with another, to see who was “growing
faster,” and who was “lagging behind.” We did this in part to exhort slow-growing,
low-diversity parts of the Internet
to grow faster, and it’s true that national regulatory environments
(and the central role of national providers in many countries) do induce some parts of
the Internet to behave in ways that are country-specific. But I hope that
for future historians’ sake, we can find better ways to preserve geographic intuition
without falling into the cognitive trap of somehow regarding
national Internet footprints as just another sovereign border to be defended.</p><h3 id="documenting-historical-slices-of-internet-activity">Documenting Historical “Slices” of Internet Activity</h3><p>My guess about where this is going is that historians will show up with questions about
what I call <em>workload slices</em> of the Internet: what was it like in a given time and place for
a specific set of users having a specific client-server experience on the Internet? This
is similar in spirit to the way, for example, the <a href="https://ooni.org/">OONI</a> team
categorizes Internet impairment
according to the category of website or communications protocol that becomes unreachable to
users in a particular part of the Internet, due to someone’s censorship.</p><p>Again, we return to the idea of careful curation of all the ‘flesh on the bones’ of
interdomain routing: the idea that we will have preserved a lot of the clues about what
all those Internet hosts were being used for on a particular day. Registry data gives
us a first clue about the organizations to whom resources were assigned; DNS
data give added clues about what roles the individual hosts were performing, how
networks were organized functionally, perhaps even where the hosts were physically
located. We’ll have lots of contemporaneous evidence from people who made, or retained,
maps of their organizations’ networks, or who crawled the base of available content.
It may be possible to reconstruct the functional host-level Internet footprints of
particular companies, or regions, or industries, at specific points in time.</p><p>Some of these “workload slices” will be very specific in time and place for people who want
to understand the Internet connectivity coincident with historical events. What was it
like for Chinese academic users in 2009 to use Google search? What was
it like for mobile users in Cairo who tried to get to Wikipedia in 2011? What was it like
for the financial sector in South America to connect to Bloomberg and Reuters throughout the
2000s? How diversely hosted were Ethereum nodes in 2020, or Mastodon servers in 2023,
relative to Internet consumers around the world? Some of these slices are a bit on the nose –
we may be able to map the embedding of hosts in the Internet, and visualize the interprovider
connectivity that would have supported a given workload slice, but without a lot of related active
measurement data from the period, detailed answers about very narrow slices of
user experience may never be knowable.</p><p>This will be hard curation work, not research that will be easy or (often) automatable. So
we’ll need to let potential users of this data guide us to the worthwhile problems to study,
and show them where to dig, if they want to help excavate patterns of interest from the
data we’ve managed to preserve.</p><h3 id="regional-internet-connectivity">Regional Internet Connectivity</h3><p>To generate time series of broader interest for our historians’ perspective, perhaps
we could open the lens a bit on these slices to consider broader trends in regional
interprovider connectivity. For example, we might construct <em>random-workload</em> slices that
could usefully approximate actual regional experiences, while also being consistently computable
over time given our available measurements.</p><p>For example, we might ask: what would <em>typical</em> paths look like for a <em>randomly selected</em>
enduser in the Middle East, connecting to a <em>randomly selected</em> server hosted somewhere else
within the same region? If this random selection were run over and over, the statistics about
the set of available paths might converge to something approximating the actual consumer
experience years ago (which we can no longer measure directly).</p><p>Between our random client and server, was there a good supply of relatively dense, direct,
low-latency local connections through Internet Exchange Points, or would they be subjected to
long, roundabout paths through exchange points in Western Europe or Singapore? We might begin
by approximating these sorts of in-region and cross-region distributions of potential connectivity
on each day throughout history, based on randomly sliced workload models. Then we could
validate those models against the vastly smaller set of actual contemporaneous measurements
that might exist from the period.</p><h3 id="regional-internet-stability">Regional Internet Stability</h3><p>Once we have the daily snapshots of how the Internet generally works to connect each region
with itself, and with other regions, we could start computing longer-term metrics
representing things like the diversity and stability of the available distribution of paths
for a given Internet workload slice.</p><p>In terms of the latency between two regions, or within a region, we could then ask:
how many “stable modes” are there for daily Internet experience, looking back over the trailing
twelve months? Does the Internet experience change frequently, or is it more or less stable?
How often do new modes appear, based
on the addition of new kinds of connectivity serving the region? These might be the sort of
‘process metrics’ that end up having predictive power outside the technical domain,
rather than the simpler structural metrics. But I mistrust my intuition here, because I’m
thinking like a technologist, rather than being guided by the more interesting questions that
might be posed by researchers outside the Internet community, looking in.</p><p>Remember, no single model, metric, or metaphor is going to work consistently over a period of
decades to characterize the growth of the Internet. Workloads on today’s Internet are vastly
different from workloads in 2000, as we’ve lived through everything from the
runout of IPv4, to the growth of IPv6, the rise of cloud computing, the growth of content
distribution networks at the edge, and the centrality of specific content megaproviders
like Google and Facebook. How we summarize that change into eras, how we add new metrics
and metaphors to our descriptive blend, or retire old ones as being less meaningful .. these
are great long-term curation questions to engage in with historians. For now, we can afford
to be agnostic.</p><h2 id="step-3-explore-and-celebrate-the-internets-history">Step 3: Explore and Celebrate the Internet’s History.</h2><p>This is the payoff for the librarian’s work. The reason we fight to preserve and curate
the history of the Internet as a technological artifact is to help make the case for its
preservation to a public that (to be fair) barely understands how the Internet works its
magic. Today’s Internet works unbelievably well in no small part because of the specific
conditions under which it grew and evolved, under multistakeholder governance rather than a
multilateral treaty system, often valuing decentralized openness and innovation where
a centralized set of authorities might have preferred to prioritize safety, predictability,
and control.</p><p>Once we’ve preserved the history of the Internet, and we’ve enlisted thoughtful
scientists who can help us quantify some of the Internet’s social benefits (net of
social costs), we’ll need tools to help tell those stories. Visualizations mostly, perhaps
immersive walkthroughs, certainly the kind of interactive exhibits that
<a href="https://open.nytimes.com/tagged/data-journalism">data journalists</a> use to inform and
entertain. Our investments in making these datasets available will open the door
to a vastly larger collaboration with artists, journalists, and visual storytellers.</p><p>That’s as far as I’d like to speculate about the work that I’d like to
get underway for the coming year. We can confidently predict that just as
the Internet has changed society, society will surely continue to change the Internet,
through some competing combination of top-down regulation and bottom-up innovation and
popular demand.</p><p>For those who care about the Internet’s future, the race is now on to
be better librarians of its history, so that we can preserve and tell the story of what
made it great.</p><hr/><p><em>To stay in touch about these or other research ideas,
<a href="https://social.secret-wg.org/@jimcowie">find me on Mastodon</a>
or <a href="mailto:jim.nh.us@gmail.com">drop me a line.</a></em></p></div></div></main></div></div>
  </body>
</html>
