<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.glyph.im/2022/12/potato-programming.html">Original</a>
    <h1>Potato Programming</h1>
    
    <div id="readability-page-1" class="page"><div>
  <blockquote>
<p>One potato, two potato, three potato, four</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/One_potato,_two_potato">Traditional Children’s Counting Rhyme</a></p>
<blockquote>
<p>Programmers waste enormous amounts of time thinking about, or worrying about,
the speed of noncritical parts of their programs, and these attempts at
efficiency actually have a strong negative impact when debugging and
maintenance are considered. We should forget about small efficiencies, say
about 97% of the time: premature optimization is the root of all evil. <strong>Yet we
should not pass up our opportunities in that critical 3%</strong>.</p>
</blockquote>
<p>Knuth, Donald</p>
<p>Knuth’s admonition about premature optimization is such a cliché among software
developers at this point that even the correction to include the full context
of the quote is <em>itself</em> a a cliché.</p>
<p>Still, it’s a cliché for a reason: the speed at which software can be written
is in tension — if not necessarily in conflict — with the speed at which it
executes.  As Nelson Elhage has explained, <a href="https://blog.nelhage.com/post/reflections-on-performance/">software can be qualitatively worse
when it is slow</a>,
but spending time optimizing an algorithm before getting any feedback from
users or profiling the system as a whole can lead one down many blind alleys of
wasted effort.</p>
<p>In that same essay, Nelson further elaborates that <a href="https://blog.nelhage.com/post/reflections-on-performance/#performant-foundations-simplify-architecture">performant foundations
simplify
architecture</a><sup id="fnref:1:potato-programming-2022-12"><a href="#fn:1:potato-programming-2022-12" id="fnref:1">1</a></sup>.
He then follows up with several bits of architectural advice that is highly
specific to parsing—compilers and type-checkers specifically—which, while good,
is hard to generalize beyond “optimizing performance early can also be good”.</p>
<p>So, here I will endeavor to generalize that advice.  <em>How</em> does one provide a
performant architectural foundation without necessarily wasting a lot of time
on early micro-optimization?</p>

<p>Many years before Nelson wrote his excellent aforementioned essay, <a href="https://youtu.be/h5fmhYc4U-Y?t=1875">my
father</a> coined a related term: “Potato
Programming”.</p>
<p>In modern vernacular, a
<a href="https://linguaholic.com/linguablog/potato-computer-definition/">potato</a> is
very slow hardware, and “potato programming” is the software equivalent of the
same.</p>
<p>The term comes from the rhyme that opened this essay, and is meant to evoke a
slow, childlike counting of individual elements as an algorithm operates upon
them. it is an unfortunately quite common <em>software-architectural idiom</em>
whereby interfaces are provided in terms of scalar values.  In other words,
APIs that require you to use <code>for</code> loops or other forms of explicit,
individual, non-parallelized iteration.  But this is all very abstract; an
example might help.</p>
<p>For a generic business-logic example, let’s consider the problem of monthly
recurring billing.  Every month, we pull in the list of all of all
subscriptions to our service, and we bill them.</p>
<p>Since our hypothetical company has an account-management team that owns the UI
which updates subscriptions and a billing backend team that writes code to
interface with 3rd-party payment providers, we’ll create 2 backends, here
represented by some <code>Protocol</code>s.</p>
<p>Finally, we’ll have an orchestration layer that puts them together to actually
run the billing. I will use <code>async</code> to indicate which things require a network
round trip:</p>
<div><table><tbody><tr><td></td><td><div><pre><span></span><code><span>class</span> <span>SubscriptionService</span><span>(</span><span>Protocol</span><span>):</span>
    <span>async</span> <span>def</span> <span>all_subscriptions</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>AsyncIterable</span><span>[</span><span>Subscription</span><span>]:</span>
        <span>...</span>

<span>class</span> <span>Subscription</span><span>(</span><span>Protocol</span><span>):</span>
    <span>account_id</span><span>:</span> <span>str</span>
    <span>to_charge_per_month</span><span>:</span> <span>money</span>

<span>class</span> <span>BillingService</span><span>(</span><span>Protocol</span><span>):</span>
    <span>async</span> <span>def</span> <span>bill_amount</span><span>(</span><span>self</span><span>,</span> <span>account_id</span><span>:</span> <span>str</span><span>,</span> <span>amount</span><span>:</span> <span>money</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>
</code></pre></div></td></tr></tbody></table></div>

<p>To many readers, this may look like an entirely reasonable interface
specification; indeed, it looks like a <em>lot</em> of real, public-facing “REST”
APIs.  An equally apparently-reasonable implementation of our orchestration
between them might look like this:</p>
<div><table><tbody><tr><td></td><td><div><pre><span></span><code><span>async</span> <span>def</span> <span>billing</span><span>(</span><span>s</span><span>:</span> <span>SubscriptionService</span><span>,</span> <span>b</span><span>:</span> <span>BillingService</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>async</span> <span>for</span> <span>sub</span> <span>in</span> <span>s</span><span>.</span><span>all_subscriptions</span><span>():</span>
        <span>await</span> <span>b</span><span>.</span><span>bill_amount</span><span>(</span><span>sub</span><span>.</span><span>account_id</span><span>,</span> <span>sub</span><span>.</span><span>to_charge_per_month</span><span>)</span>
</code></pre></div></td></tr></tbody></table></div>

<p>This is, however, just about the slowest implementation of this functionality
that it’s possible to implement.  So, this is the bad version.  Let’s talk
about the good version: no-tato programming, if you will.  But first, some backstory.</p>
<h2 id="some-backstory">Some Backstory</h2>
<p>My father began his career as an
<a href="https://en.wikipedia.org/wiki/APL_(programming_language)">APL</a> programmer, and
one of the key insights he took away from APL’s architecture is that, as he
puts it:</p>
<blockquote>
<p>Computers like to do things over and over again. They like to do things on
<em>arrays</em>. They don’t want to do things on scalars. So, in fact, it’s not
<em>possible</em> to write a program that only does things on a scalar.  [...]  You
can’t have an ‘integer’ in APL, you can only have an ‘<em>array</em> of integers’.
There’s no ‘<code>loop</code>’s, there’s no ‘<code>map</code>’s.</p>
</blockquote>
<p>APL, like Python<sup id="fnref:2:potato-programming-2022-12"><a href="#fn:2:potato-programming-2022-12" id="fnref:2">2</a></sup>, is typically executed via an interpreter.  Which means, like
Python, execution of basic operations like calling functions can be quite slow.
However, unlike Python, its pervasive reliance upon arrays meant that almost
all of its operations could be safely parallelized, and would only get more and
more efficient as more and more parallel hardware was developed.</p>
<p>I said ‘unlike Python’ there, but in fact, my father first related this concept
to me regarding a part of the Python ecosystem which follows APL’s design
idiom: <a href="https://numpy.org">NumPy</a>.  NumPy takes a similar approach: it cannot
itself do anything to speed up Python’s fundamental interpreted execution
speed<sup id="fnref:3:potato-programming-2022-12"><a href="#fn:3:potato-programming-2022-12" id="fnref:3">3</a></sup>, but it <em>can</em> move the intensive numerical operations that it
implements into operations on arrays, rather than operations on individual
objects, whether numbers or not.</p>
<p>The performance difference involved in these two styles is not small.  Consider
<a href="https://realpython.com/numpy-tensorflow-performance/">this case study</a> which
shows a <strong>5828% improvement</strong><sup id="fnref:4:potato-programming-2022-12"><a href="#fn:4:potato-programming-2022-12" id="fnref:4">4</a></sup> when taking an algorithm from idiomatic pure
Python to NumPy.</p>
<p>This idiom is also more or less how GPU programming works. GPUs cannot operate
on individual values. You submit a program<sup id="fnref:5:potato-programming-2022-12"><a href="#fn:5:potato-programming-2022-12" id="fnref:5">5</a></sup> to the GPU, as well as a large
array of data<sup id="fnref:6:potato-programming-2022-12"><a href="#fn:6:potato-programming-2022-12" id="fnref:6">6</a></sup>, and the GPU executes the program on that data in parallel
across hundreds of tiny cores.  Submitting individual values for the GPU to
work on would actually be much <em>slower</em> than just doing the work on the CPU
directly, due to the bus latency involved to transfer the data back and forth.</p>
<h2 id="back-from-the-backstory">Back from the Backstory</h2>
<p>This is all interesting for a class of numerical software — and indeeed it
works very well there — but it may seem a bit abstract to web backend
developers just trying to glue together some internal microservice APIs, or
indeed most app developers who aren’t working in those specialized fields.
It’s not like <a href="https://stripe.com/">Stripe</a> is going to let you run their
payment service on your GPU.</p>
<p>However, the lesson generalizes quite well: anywhere you see an API defined in
terms of one-potato, two-potato iteration, ask yourself: “how can this be
turned into an array”?  Let’s go back to our example.</p>
<p>The simplest change that we can make, as a <em>consumer</em> of these potato-shaped
APIs, is to submit them in parallel.  So if we have to do the optimization in
the orchestration layer, we might get something more like this:</p>
<div><table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span>
<span>16</span>
<span>17</span>
<span>18</span>
<span>19</span>
<span>20</span>
<span>21</span>
<span>22</span>
<span>23</span>
<span>24</span>
<span>25</span></pre></div></td><td><div><pre><span></span><code><span>from</span> <span>asyncio</span> <span>import</span> <span>Semaphore</span><span>,</span> <span>AbstractEventLoop</span>

<span>async</span> <span>def</span> <span>one_bill</span><span>(</span>
    <span>loop</span><span>:</span> <span>AbstractEventLoop</span><span>,</span>
    <span>sem</span><span>:</span> <span>Semaphore</span><span>,</span>
    <span>sub</span><span>:</span> <span>Subscription</span><span>,</span>
    <span>b</span><span>:</span> <span>BillingService</span><span>,</span>
<span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>await</span> <span>sem</span><span>.</span><span>acquire</span><span>()</span>
    <span>async</span> <span>def</span> <span>work</span><span>()</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>try</span><span>:</span>
            <span>await</span> <span>b</span><span>.</span><span>bill_amount</span><span>(</span><span>sub</span><span>.</span><span>account_id</span><span>,</span> <span>sub</span><span>.</span><span>to_charge_per_month</span><span>)</span>
        <span>finally</span><span>:</span>
            <span>sem</span><span>.</span><span>release</span><span>()</span>
    <span>loop</span><span>.</span><span>create_task</span><span>(</span><span>work</span><span>)</span>

<span>async</span> <span>def</span> <span>billing</span><span>(</span>
    <span>loop</span><span>:</span> <span>AbstractEventLoop</span><span>,</span>
    <span>s</span><span>:</span> <span>SubscriptionService</span><span>,</span>
    <span>b</span><span>:</span> <span>BillingService</span><span>,</span>
    <span>batch_size</span><span>:</span> <span>int</span><span>,</span>
<span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>sem</span> <span>=</span> <span>Semaphore</span><span>(</span><span>batch_size</span><span>)</span>
    <span>async</span> <span>for</span> <span>sub</span> <span>in</span> <span>s</span><span>.</span><span>all_subscriptions</span><span>():</span>
        <span>await</span> <span>one_bill</span><span>(</span><span>loop</span><span>,</span> <span>sem</span><span>,</span> <span>sub</span><span>,</span> <span>b</span><span>)</span>
</code></pre></div></td></tr></tbody></table></div>

<p>This is an improvement, but it’s a bit of a brute-force solution; a
multipotato, if you will.  We’ve moved the work to the billing service faster,
but it still has to do just as much work.  Maybe even <em>more</em> work, because now
it’s potentially got a lot more lock-contention on its end.  And we’re still
waiting for the <code>Subscription</code> objects to dribble out of the
<code>SubscriptionService</code> potentially one request/response at a time.</p>
<p>In other words, we have used network concurrency as a hack to <em>simulate</em> a
performant design.  But the back end that we have been given here is not
actually <em>optimizable</em>; we do not have a performant foundation.  As you can
see, we have even had to change our local architecture a little bit here, to
include a <code>loop</code> parameter and a <code>batch_size</code> which we had not previously
contemplated.</p>
<p>A better-designed interface in the first place would look like this:</p>
<div><table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span>
<span>16</span>
<span>17</span>
<span>18</span>
<span>19</span>
<span>20</span>
<span>21</span></pre></div></td><td><div><pre><span></span><code><span>class</span> <span>SubscriptionService</span><span>(</span><span>Protocol</span><span>):</span>
    <span>async</span> <span>def</span> <span>all_subscriptions</span><span>(</span>
        <span>self</span><span>,</span> <span>batch_size</span><span>:</span> <span>int</span><span>,</span>
    <span>)</span> <span>-&gt;</span> <span>AsyncIterable</span><span>[</span><span>Sequence</span><span>[</span><span>Subscription</span><span>]]:</span>
        <span>...</span>

<span>class</span> <span>Subscription</span><span>(</span><span>Protocol</span><span>):</span>
    <span>account_id</span><span>:</span> <span>str</span>
    <span>to_charge_per_month</span><span>:</span> <span>money</span>

<span>@dataclass</span>
<span>class</span> <span>BillingRequest</span><span>:</span>
    <span>account_id</span><span>:</span> <span>str</span>
    <span>amount</span><span>:</span> <span>money</span>

<span>class</span> <span>BillingService</span><span>(</span><span>Protocol</span><span>):</span>
    <span>async</span> <span>def</span> <span>submit_bills</span><span>(</span>
        <span>self</span><span>,</span>
        <span>bills</span><span>:</span> <span>Sequence</span><span>[</span><span>BillingRequest</span><span>],</span>
    <span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>
</code></pre></div></td></tr></tbody></table></div>

<p>Superficially, the implementation here looks <em>slightly</em> more awkward than our naive first attempt:</p>
<div><table><tbody><tr><td></td><td><div><pre><span></span><code><span>async</span> <span>def</span> <span>billing</span><span>(</span><span>s</span><span>:</span> <span>SubscriptionService</span><span>,</span> <span>b</span><span>:</span> <span>BillingService</span><span>,</span> <span>batch_size</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>async</span> <span>for</span> <span>sub_batch</span> <span>in</span> <span>s</span><span>.</span><span>all_subscriptions</span><span>(</span><span>batch_size</span><span>):</span>
        <span>await</span> <span>b</span><span>.</span><span>submit_bills</span><span>(</span>
            <span>[</span>
                <span>BillingRequest</span><span>(</span><span>sub</span><span>.</span><span>account_id</span><span>,</span> <span>sub</span><span>.</span><span>to_charge_per_month</span><span>)</span>
                <span>for</span> <span>sub</span> <span>in</span> <span>subs</span>
            <span>]</span>
        <span>)</span>
</code></pre></div></td></tr></tbody></table></div>

<p>However, while the implementation with batching in the backend is approximately
as performant as our parallel orchestration implementation, backend batching
has a number of advantages over parallel orchestration.</p>
<p>First, backend batching has less internal complexity; no need to have a
<code>Semaphore</code> in the orchestration layer, or to create tasks on an event loop.
There’s less surface area here for bugs.</p>
<p>Second, and more importantly: backend batching permits for future optimizations
<em>within the backend services</em>, which are much closer to the relevant data and
can achieve more substantial gains than we can as a client without knowledge of
their implementation.</p>
<p>There are many ways this might manifest, but consider that each of these
services has their own database, and have got to submit queries and execute
transactions on those databases.</p>
<p>In the subscription service, it’s faster to run a single <code>SELECT</code> statement
that returns a bunch of results than to select a single result at a time.  On
the billing service’s end, it’s much faster to issue a single <code>INSERT</code> or
<code>UPDATE</code> and then <code>COMMIT</code> for N records at once than to concurrently issue a
ton of potentially related modifications in separate transactions.</p>

<p>The initial implementation within each of these backends can be as naive and
slow as necessary to achieve an MVP.  You can do a <code>SELECT … LIMIT 1</code>
internally, if that’s easier, and performance is not important at first.  There
can be a mountain of potatoes hidden behind the veil of that batched list.  In
this way, you can avoid the potential trap of premature optimization.  Maybe
this is a terrible factoring of services for your application in the first
place; best to have that prototype in place and functioning quickly so that you
can throw it out faster!</p>
<p>However, by initially designing an interface based on <em>lists</em> of things rather
than <em>individual</em> things, it’s much easier to hide irrelevant implementation
details from the client, and to achieve <em>meaningful</em> improvements when
optimizing.</p>

<p>This is the first post supported by my
<a href="https://www.patreon.com/creatorglyph">Patreon</a>, with a topic suggested by a
patron.</p>

</div></div>
  </body>
</html>
