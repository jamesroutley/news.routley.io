<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lepisma.xyz/2024/09/12/emacs-dictation-mode/index.html">Original</a>
    <h1>Speech Dictation Mode for Emacs</h1>
    
    <div id="readability-page-1" class="page"><div id="content">

<p>
<span>T</span>here is a wide range of input mechanisms for computers, starting with keyboards
(which are relatively mature) and extending to various types of neural
interfaces (currently under research). Speech lies somewhere on this spectrum
with a lot of promises but still not much to show for. Keeping accessibility
aspects aside, I think speech is mature enough to be used for drafting ideas and
taking notes. Maybe not so much for structured writing like programming or final
versions of most prose.
</p>

<p>
We have transcription tools that can work as dictation interfaces but they
inevitably make mistakes and mishandle <a href="https://en.wikipedia.org/wiki/Speech_disfluency">speech disfluencies</a>, fixing which offline
takes a lot of additional time. I wanted to explore augmenting transcription
tools with LLMs to enable <i>real-time edits</i>. If done well this should feel as if
you are talking to a human writer.
</p>

<p>
I had this idea for some time but kept delaying this for many months. Finally I
stopped thinking about it altogether when I saw <a href="https://news.ycombinator.com/item?id=39828686">Aqua Voice</a> launch. Recently I
got some free time so attempted this as I needed a dictation tool for Emacs,
which is where I do all of my writings.
</p>

<div id="outline-container-sec-esi-dictate.el">
<h2 id="sec-esi-dictate.el"><span>1.</span> esi-dictate.el</h2>
<div id="text-sec-esi-dictate.el">



<p>
I wrote a small Emacs package <a href="https://github.com/lepisma/emacs-speech-input">here</a> that works by adding a minor mode where
spoken words enter your buffer at an independent <i>voice cursor</i><sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup> and the entered text is corrected in real time
based on your [voice] instructions. Here are a few screencasts:
</p>

<figure>
  <video controls="" width="100%"><source src="./plain-input.mp4" type="video/mp4"/></video>
  <figcaption>
    <span>Figure 1:</span>
    [With audio] The â¤³ symbol shows the <em>voice cursor</em>. Underlined text is
    used as <em>voice context</em> for the LLM. I can move around with my text cursor to
    make fixes if needed. The LLM does general fixes as well as following explicit
    instructions.
  </figcaption>
</figure>

</div>
</div>

<div id="outline-container-sec-future-improvements">
<h2 id="sec-future-improvements"><span>2.</span> Future Improvements</h2>
<div id="text-sec-future-improvements">

<p>
From UX perspective<sup><a id="fnr.2" href="#fn.2" role="doc-backlink">2</a></sup>, I think
this flow is good enough for me but I suspect that I am not at the sweet spot
yet. A major gain at this point will come from latency improvements in both ASR
and LLM. Some of that is achievable by making good use of asynchronous calls,
caching, etc.
</p>

<p>
Furthermore, while the code for this package is open-source it&#39;s still not a
<i>nice</i> FOSS since it depends on <a href="https://deepgram.com/">Deepgram</a> and <a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">OpenAI&#39;s</a> services, both of which I
want to swap out with lightweight on-device or self-hosted alternatives. There
are also a few minor bugs that pop up when voice and text cursor&#39;s actions
conflict. I will resolve these in due time.
</p>
</div>
</div>
</div></div>
  </body>
</html>
