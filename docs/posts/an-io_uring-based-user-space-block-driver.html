<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/Articles/903855/">Original</a>
    <h1>An io_uring-based user-space block driver</h1>
    
    <div id="readability-page-1" class="page"><div>
<center>
           <div><b>LWN.net needs you!</b><p>Without subscribers, LWN would simply not exist.  Please consider
       <a href="https://lwn.net/subscribe/">signing up for a subscription</a> and helping
       to keep LWN publishing</p></div>
           </center>
           <p>
The addition of the ublk driver during the 6.0 merge window would have been
easy to miss; it was buried deeply within an io_uring pull request and is
entirely devoid of any sort of documentation that might indicate why it
merits a closer look.  Ublk is intended to facilitate the implementation of
high-performance block drivers in user space; to that end, it uses <a href="https://lwn.net/Articles/776703/">io_uring</a> 
for its communication with the kernel.  This driver is considered
experimental for now; if it is successful, it might just be a harbinger of
more significant changes to come to the kernel in the future.
</p><p>
Your editor has spent a fair amount of time beating his head against <a href="https://lwn.net/ml/io-uring/20220713140711.97356-1-ming.lei@redhat.com/">the source
for the ublk driver</a>, as well as the <a href="https://github.com/ming1/ubdsrv">ubdsrv server</a> that comprises the
user-space component.  The picture that has emerged from this exploration
of that uncommented
and vowel-deficient realm is doubtless incorrect
in some details, though the overall shape should be close enough to
reality.
</p><h4>How ublk works</h4>
<p>
The ublk driver starts by creating a special device called
<tt>/dev/ublk-control</tt>.  The user-space server (or servers, there can
be more than one) starts by opening that device and setting up an io_uring
ring to communicate with it.  Operations at this level are essentially
<tt>ioctl()</tt> commands, but <tt>/dev/ublk-control</tt> has no
<tt>ioctl()</tt> handler; all operations are, instead, sent as commands
through io_uring.  Since the purpose is to implement a device behind
io_uring, the reasoning seems to be, there is no reason to not use it from
the beginning.
</p><p>
A server will typically start with a <tt>UBLK_CMD_ADD_DEV</tt> command; as
one might expect, it adds a new ublk device to the system.  The server can
describe various aspects of this device, including the number of hardware
queues it claims to implement, its block size, the maximum transfer size,
and the number of blocks the 
device can hold.  Once this command succeeds, the device exists as far as
the ublk driver is concerned and is visible as <tt>/dev/ublkc<i>N</i></tt>,
where 
<tt><i>N</i></tt> is the device ID returned when the device is created.
The device has not yet been added to the block layer, though.
</p><p>
The server should open the new <tt>/dev/ublkc<i>N</i></tt> device for the
following steps, the first of which is to map a region from the device into
the server&#39;s 
address space with an <tt>mmap()</tt> call.  This region is an array of
<tt>ublksrv_io_desc</tt> structures describing I/O requests:
</p><pre>    struct ublksrv_io_desc {
	/* op: bit 0-7, flags: bit 8-31 */
	__u32		op_flags;
	__u32		nr_sectors;
	__u64		start_sector;
	__u64		addr;
    };
</pre>
<p>
Notification of new I/O requests will be received via io_uring.  To
get to that point, the server must enqueue a set of
<tt>UBLK_IO_FETCH_REQ</tt> requests on the newly created device; normally
there will be one for each &#34;hardware queue&#34; declared for the device, which
may also correspond to each thread running within the server.  Among other
things, this request must provide a memory buffer that can hold the maximum
request size declared when the device was created.
</p><p>
Once this setup is complete, a separate <tt>UBLK_CMD_START_DEV</tt>
operation will cause the ublk driver to actually create a block device
visible to the rest of the system.  When the block subsystem sends a
request to this device, one of the queued <tt>UBLK_IO_FETCH_REQ</tt> operations
will complete.  The completion data returned to the user-space server will
include the index of the 
<tt>ublkserv_io_desc</tt> structure describing the request, which the
server should now execute.  For a write request, the data to be written
will be in the buffer that was provided by the server; for a read, the data
should be placed in that same buffer.
</p><p>
When the operation is complete, the server must inform the kernel of that
fact; this is done by placing a <tt>UBLK_IO_COMMIT_AND_FETCH_REQ</tt>
operation into the ring.  It will give the result of the operation back to
the block subsystem, but will also enqueue the buffer to receive the next
request, thus avoiding the need to do that separately.
</p><p>
There are the expected <tt>UBLK_CMD_STOP_DEV</tt> and
<tt>UBLK_CMD_DEL_DEV</tt> operations to make existing devices go away, and
a couple of other operations to query information about existing devices.
There are also a number of details that have not been covered here, mostly
aimed at increased performance.  Among other things, the ublk protocol is
set up to enable zero-copy I/O, but that is not implemented in the current
code.
</p><p>
The server code implements two targets: null and loop.  The null target is,
as one might expect, an overly complicated, block-oriented version of
<tt>/dev/null</tt>; it is useless but makes it possible to see how things
work with a minimum of unrelated details.  The loop target uses an existing
file as the backing store for a virtual block device.  According to author
Ming Lei, with this loop implementation, &#34;<q>the performance is
is even better than kernel loop with same setting</q>&#34;.
</p><h4>Implications</h4>
<p>
One might wonder why this work has been done (and evidently supported by
Red Hat); if the world has been clamoring for an io_uring-based,
user-space, faster loop block device, it has done so quietly.  One
advantage cited in the patch cover letter is that development of
block-driver code is more easily done in user space; another is
high-performance <a href="https://en.wikipedia.org/wiki/Qcow">qcow2</a>
support.  The patch cover letter also cites interest expressed by other
developers in having a fast user-space block-device mechanism available.

</p><p>
An interesting question, though, is whether this mechanism might ultimately
facilitate the movement of a number of device drivers out of the kernel â€”
perhaps not just block drivers.  Putting device drivers into user-space
code is a fundamental concept in a number of secure-system designs,
including microkernel systems.  But one of the problems with those designs
has always been the communication overhead between the two components once
they are no longer running within the same address space.  Io_uring might
just be a convincing answer to that problem.
</p><p>
Should that scenario play out, kernels of the future could look
significantly different from what we have today; they could be smaller,
with much of the complicated logic running in separate, user-space
components.  Whether this is part of Lei&#39;s vision for ublk is unknown, and
things may never get anywhere near that point.  But ublk is clearly an
interesting experiment that could lead to big changes down the line.
Something will need to be done about that complete absence of
documentation, though, on the way toward world domination.</p><hr/><p>
           (<a href="https://lwn.net/Login/?target=/Articles/903855/">Log in</a> to post comments)
           </p></div></div>
  </body>
</html>
