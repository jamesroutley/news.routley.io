<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jackogrady.me/reinforcement-learning-solar/research-summary">Original</a>
    <h1>I built an open-source, AI-powered solar panel that&#39;s 95% optimal</h1>
    
    <div id="readability-page-1" class="page"><div id="pageWrapper" role="main">
        <!-- CATEGORY NAV -->
        
      <section id="page">

      <div data-content-field="main-content">
        <div data-type="page" data-updated-on="1651162993326" id="page-6256e9945ce3664a884a2ae9"><div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650770775249_29196"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650382549050_322559"><div><p><strong>Jack O’Grady<br/></strong>April 28th, 2022</p><p>This research demonstrates a model-free approach to optimize the energy produced by a dual-axis solar panel using reinforcement learning. Specifically, a softmax actor-critic agent optimizes energy production in a simulated, dynamic lighting environment which is generated from real power data. All schematics, algorithms, and code are provided as open-source materials.</p>


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650382549050_397820"><div><h2>Introduction</h2><p>Photovoltaic solar is playing a key role in the world’s transition to renewable energy, making up 46% of all new electric capacity added to the U.S. grid in 2021 [1]. While primary factors driving optimal panel positioning are readily modeled (<em>i.e</em>., the sun’s position at each time of day), site-specific and panel-specific factors are less so. Elements like dynamic shading from nearby trees or structures, localized panel defects, drift in axis positions as systems degrade, <em>etc.</em> can have significant impacts on energy production. Rather than modeling, calibrating, and updating all of these factors for each dual-axis solar installation, this research seeks to demonstrate how an AI agent employing reinforcement learning (RL) can optimize energy production without directly knowing any of these factors.</p><h2>Controlling Systems with RL</h2><p>RL is a paradigm shift in how we model and control complex systems. The designers of an RL agent neither need to model the dynamics of the system being controlled, nor do they need to understand how to control for each of these dynamics. In our solar energy optimization problem, our objective is to maximize energy production from the panel. To do that, optimal behavior would entail positioning the panel to generate the most power at any point of time during the day as the environment (lighting) changes.</p><p>Using traditional methods, we would take a development approach like the below, where we try to model each factor that affects the energy production of the system then develop control logic to account for each of those factors. We try different things, tune our controls and models, and ultimately attempt to reach optimal behavior.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1650820324500_264047"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650820324500_264400"><p>For RL, the development approach is quite different—we simply define our objective, and the RL agent learns how to achieve optimal behavior for the problem without the designers of the RL agent having to know how to do so. The RL agent is either directly or indirectly learning the underlying dynamics of the system and responding optimally to each situation it finds itself in.</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1650820324500_551384"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650820324500_551725"><div><p>At a high level, the agent learns via the following process:</p><ol data-rte-list="default"><li><p>The agent interacts with the environment and receives some reward</p></li><li><p>The agent learns from the reward by updating its likelihood of selecting actions in a given state</p></li><li><p>The process repeats</p></li></ol>


</div></div><div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650820324500_665248"></div></div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650820324500_661327"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650820324500_673865"></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650820324500_661679"><div><p>With this background, we now discuss the specific RL implementation for our energy optimization problem.</p><p><em>For those interested, you can find more in-depth pages about reinforcement learning concepts on the following pages:<br/></em><a href="https://blog.plover.com/reinforcement-learning-solar/rl-fundamentals" target="_blank"><span><em>RL Fundamentals</em></span></a><em> | </em><a href="https://blog.plover.com/reinforcement-learning-solar/helpful-rl-resources" target="_blank"><span><em>Helpful RL Resources</em></span></a></p><h2>RL Implementation</h2><p>We first review the hardware of the system we are optimizing, then discuss the software and algorithms.</p><h3>Dual-Axis Panel</h3><p>In our problem, we have a dual-axis solar panel that we are trying to maximize the energy output of. Each axis is operated by a servo motor, which can rotate 180°, and the panel is able to measure the power produced for each position that it is in. At each time step, it broadcasts the motor positions and power being produced to the RL agent via serial communication over a USB port. Below, you can see a blueprint compared to the actual panel hardware.</p>


</div></div><div data-block-json="{&#34;hSize&#34;:null,&#34;floatDir&#34;:null,&#34;methodOption&#34;:&#34;transient&#34;,&#34;existingGallery&#34;:null,&#34;newWindow&#34;:false,&#34;active-alignment&#34;:&#34;center&#34;,&#34;aspect-ratio&#34;:&#34;square&#34;,&#34;auto-crop&#34;:true,&#34;autoplay&#34;:false,&#34;blockAnimation&#34;:&#34;none&#34;,&#34;collectionId&#34;:&#34;6265a55c9838821c958b50fc&#34;,&#34;controls&#34;:true,&#34;design&#34;:&#34;slider&#34;,&#34;lightbox&#34;:false,&#34;padding&#34;:20,&#34;show-meta&#34;:true,&#34;show-meta-basic&#34;:true,&#34;show-meta-only-title&#34;:false,&#34;show-meta-only-description&#34;:false,&#34;square-thumbs&#34;:true,&#34;thumbnails-per-row&#34;:4,&#34;vSize&#34;:null,&#34;transientGalleryId&#34;:&#34;6265a55c9838821c958b50fc&#34;}" data-block-type="8" id="block-yui_3_17_2_1_1650820324500_1945603"><div>





<div>
  <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/1650828636908-Z2QTG862UTDPN90GURXM/panel_v1_design.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/1650828636908-Z2QTG862UTDPN90GURXM/panel_v1_design.png" data-image-dimensions="2871x2154" data-image-focal-point="0.5,0.5" alt="panel_v1_design.png" data-load="false" data-image-id="6265a55c5ffadb5f98c2434d" data-type="image"/>
            
          
          
        

        

      

        

        
          
            
              <img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/1650828654262-TXH4A1JD01T9TYTQNEPW/panel_approach_angle.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/1650828654262-TXH4A1JD01T9TYTQNEPW/panel_approach_angle.jpg" data-image-dimensions="2437x3518" data-image-focal-point="0.5,0.5" alt="panel_approach_angle.jpg" data-load="false" data-image-id="6265a56bbe929828d2e7e332" data-type="image"/>
            
          
          
        

        

      
    
  </p>

  
    <div>
    
    
      
      
        
      
    
    
    </div> <!-- END .sqs-gallery-meta-container -->
  

</div>






  
  

</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1650820324500_1939048"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>Detailed view of the dual-axis panel hardware</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650856400358_48548"><div><p><em>For those interested, you can find more detailed material on the dual-axis panel design on the following pages:<br/></em><a href="https://blog.plover.com/reinforcement-learning-solar/system-design" target="_blank"><span><em>System Design</em></span></a><em> | </em><a href="https://blog.plover.com/reinforcement-learning-solar/circuit-schematic" target="_blank"><span><em>Circuit Schematic</em></span></a><em> | </em><a href="https://blog.plover.com/reinforcement-learning-solar/dual-axis-panel-design" target="_blank"><span><em>Dual-Axis Panel Design</em></span></a></p><h3>Agent-Environment Interaction</h3><p>At each time step, the agent must decide how the motors should be positioned to produce the most energy. Each motor can be positioned from 0-180°, but the 3D printed gears used in this project only allow about 5° of precision in panel positioning. To account for this, I map the 0-180° positions to indices of 5° increments. This means that each motor can be in position 0 to 36. To simplify the agent’s memory structure, we convert the 2D index of motor position (say [36, 1]) into a flat mapped 1D index position (say [1333]), which conveys the state of both motors with a single index.</p><p>In addition to the motor positions, as the environment shifts throughout the day, the agent also tracks time of day as part of its state. There is a configurable time-of-day partition value which can increase or decrease the discretization of distinct states the agent learns, but in this implementation it is set to 24 to map to each hour of the day.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1650820324500_2438357"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/901301d2-4380-4443-b4ed-a0b719c16d69/agent-env-interaction.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/901301d2-4380-4443-b4ed-a0b719c16d69/agent-env-interaction.png" data-image-dimensions="3399x1401" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6265aef8d240466a7d60a9d8" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>The agent interacts with the environment by requesting motor index positions, then learns from the power produced at those indices at that time of day</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650831019237_624914"><div><p>This is the nexus of the information the agent has and the actions that it can take to optimize energy. We now discuss how the agent uses this information to achieve that objective.</p><h3>Agent Design</h3><p>The agent in this optimization problem employs a softmax actor-critic algorithm with tabular representations of the actor and critic networks. I chose this implementation after initially experimenting with a Q-learning agent using an epsilon-greedy policy, which had difficulty efficiently exploring the large action space (of which there are 1369 actions in any state). Given the continuous nature of this task, our actor and critic update equations also use average reward rather than a discount factor and future state values. </p><p>The agent’s update equations are shown below:</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1650986509006_1804539"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/c047f77c-90e5-43cf-82fb-a936ba681542/delta_t_eqn.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/c047f77c-90e5-43cf-82fb-a936ba681542/delta_t_eqn.png" data-image-dimensions="536x44" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="626815868a43ce7ac126ae51" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Delta equation for our agent</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1650986509006_1809691"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/04ccad3e-91e7-4273-9f09-abc8d1217b48/policy_update_final.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/04ccad3e-91e7-4273-9f09-abc8d1217b48/policy_update_final.png" data-image-dimensions="656x55" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6268159d34910814fe5af275" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <div><p>Actor update equation for our agent. See <a href="https://blog.plover.com/reinforcement-learning-solar/rl-agent-softmax-actor-critic" target="_blank">RL Agent - Softmax Actor-Critic</a> for details on this gradient simplification. </p></div>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1650986509006_1814880"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/4c6b87f3-9aca-4487-a537-c9b7059412c0/critic_update_eqn.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/4c6b87f3-9aca-4487-a537-c9b7059412c0/critic_update_eqn.png" data-image-dimensions="517x40" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="626815abb45ca03bd1477fd5" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Critic update equation for our agent</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_1798845"><p>In each state, the agent must select which motor position index to transition to next, one of which corresponds to staying in the same position. When the agent makes its decision, the environment transitions the panel positions and returns reward, which is the amount of power produced in the new state less some energy required to rotate the motors. In the graphic below, we visualize how the agent is selecting which action to take:</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1650831019237_1287610"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/11b36bfd-b50e-4339-aa8b-dae6329a05a3/action_selection_process.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/11b36bfd-b50e-4339-aa8b-dae6329a05a3/action_selection_process.png" data-image-dimensions="2652x1032" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62660e8fd240466a7d6ae2b0" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>A visualization of how the actor’s learned values of each action influence its probability of being selected in a softmax policy</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650831019237_1287953"><p>The actor and critic networks are three dimensional and of size [time of day partitions, number of states, number of actions]. In this implementation, that equates to a size of [24, 1369, 1369]. The values inside of the actor network correspond not to the actual power expected in each state, but rather the probability with which that action should be selected. The critic has a similar three-dimensional memory structure; however, it is learning the expected power from each action given the state. </p></div><div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650831019237_1699152"></div></div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650831019237_1632598"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/e83d8edc-b3b6-4fae-bd30-670ca8109a90/new_index_representation.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/e83d8edc-b3b6-4fae-bd30-670ca8109a90/new_index_representation.png" data-image-dimensions="3505x2323" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62661012e3e24278738ffeec" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>A visualization of the three-dimensional structure of the actor and critic networks</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650831019237_1711084"></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650831019237_1903943"><div><p>With the agent design covered, we now transition to the training environment in which the agent learns optimal behavior. </p><p><em>For those interested, you can find more detailed, technical pages on agent implementations in this project at the following links: <br/></em><a href="https://blog.plover.com/reinforcement-learning-solar/rl-agent-softmax-actor-critic" target="_blank"><span><em>RL Agent - Softmax Actor-Critic</em></span></a><em> | </em><a href="https://blog.plover.com/reinforcement-learning-solar/rl-agent-q-learning" target="_blank"><span><em>RL Agent - Q-Learning</em></span></a></p><h3>Generating Simulated Environments</h3><p>RL agents learn from exploration, and exploration takes time. We typically navigate this in RL by using simulated environments wherein one step of simulation time is significantly shorter than one step of real time. This allows us to quickly learn whether or not a given agent implementation is well-suited for a problem, as well as conduct a hyperparameter study.</p><p>In this problem, since we do not want to model all the underlying dynamics that affect real power generation in a given place at a given time, we generate a simulated environment from a real light-scan using the dual-axis panel. This is done using the following high-level process:</p>


</div></div><div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650856400358_1060522"></div></div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650856400358_1020347"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/ba82eb1d-ea0c-46d7-beb6-77494fbe89ac/light_scan_process.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/ba82eb1d-ea0c-46d7-beb6-77494fbe89ac/light_scan_process.png" data-image-dimensions="1503x246" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62663be7c57da541670fdd8a" data-type="image"/>
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650856400358_1073150"></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650856400358_1020696"><div><p>Visually, this looks like:</p><p><strong>Collect power measurement at all motor indices</strong></p>


</div></div><div data-block-json="{&#34;blockAnimation&#34;:&#34;none&#34;,&#34;layout&#34;:&#34;caption-below&#34;,&#34;overlay&#34;:false,&#34;description&#34;:{&#34;html&#34;:&#34;&lt;p class=\&#34;\&#34; style=\&#34;white-space:pre-wrap;\&#34;&gt;The above is a timelapse capture, not real-time&lt;/p&gt;&#34;},&#34;hSize&#34;:null,&#34;floatDir&#34;:null,&#34;isOldBlock&#34;:false,&#34;html&#34;:&#34;&lt;iframe class=\&#34;embedly-embed\&#34; src=\&#34;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FCb24tlY8zVY%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DCb24tlY8zVY&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FCb24tlY8zVY%2Fhqdefault.jpg&amp;key=61d05c9d54e8455ea7a9677c366be814&amp;type=text%2Fhtml&amp;schema=youtube\&#34; width=\&#34;854\&#34; height=\&#34;480\&#34; scrolling=\&#34;no\&#34; title=\&#34;YouTube embed\&#34; frameborder=\&#34;0\&#34; allow=\&#34;autoplay; fullscreen\&#34; allowfullscreen=\&#34;true\&#34;&gt;&lt;/iframe&gt;&#34;,&#34;url&#34;:&#34;https://www.youtube.com/watch?v=Cb24tlY8zVY&#34;,&#34;width&#34;:854,&#34;height&#34;:480,&#34;providerName&#34;:&#34;YouTube&#34;,&#34;thumbnailUrl&#34;:&#34;https://i.ytimg.com/vi/Cb24tlY8zVY/hqdefault.jpg&#34;,&#34;resolvedBy&#34;:&#34;youtube&#34;}" data-block-type="32" id="block-yui_3_17_2_1_1650902653927_25420"><div><div><div></div><div><p>The above is a timelapse capture, not real-time</p></div></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650902653927_25486"><p><strong>Convert to 2D array</strong></p></div><div><div><div data-block-type="5" id="block-65eff317f013818a2fbc"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/ee257ab1-27bf-491f-b42c-726619522c70/3d_light_scan.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/ee257ab1-27bf-491f-b42c-726619522c70/3d_light_scan.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6267255156ae8b2e232405b5" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>The light scan from the video above in 3D</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="5" id="block-7b4bb7804e7c84be00ec"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/6475ef6b-43b3-4d80-9d58-084431036964/2d_light_scan.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/6475ef6b-43b3-4d80-9d58-084431036964/2d_light_scan.png" data-image-dimensions="600x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6267255db45ca03bd12f50b2" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>The light scan from the video above as the 2D reward array passed to the environment class</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650856400358_1737046"><div><p><strong>Add to environment class</strong></p><p>The environment class emulates the dynamic lighting environment that the agent will actually face in deployment by both shifting the array every N steps (sun moving in the sky), and resetting it to its initial position every M steps (days). N and M are currently set to 3,600 and 86,400, respectively, to represent the seconds in an hour and in a day. Thus, in our simulation 3,600 steps corresponds to 1 hour of real time, and 86,400 steps corresponds to 1 day of real time.</p>


</div></div><div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650856400358_1232676"></div></div><div><div data-block-json="{&#34;blockAnimation&#34;:&#34;none&#34;,&#34;layout&#34;:&#34;caption-hidden&#34;,&#34;overlay&#34;:false,&#34;description&#34;:{},&#34;hSize&#34;:null,&#34;floatDir&#34;:null,&#34;isOldBlock&#34;:false,&#34;html&#34;:&#34;&lt;iframe class=\&#34;embedly-embed\&#34; src=\&#34;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FzliwDgidwsg%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzliwDgidwsg&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FzliwDgidwsg%2Fhqdefault.jpg&amp;key=61d05c9d54e8455ea7a9677c366be814&amp;type=text%2Fhtml&amp;schema=youtube\&#34; width=\&#34;640\&#34; height=\&#34;480\&#34; scrolling=\&#34;no\&#34; title=\&#34;YouTube embed\&#34; frameborder=\&#34;0\&#34; allow=\&#34;autoplay; fullscreen\&#34; allowfullscreen=\&#34;true\&#34;&gt;&lt;/iframe&gt;&#34;,&#34;url&#34;:&#34;https://www.youtube.com/watch?v=zliwDgidwsg&#34;,&#34;width&#34;:640,&#34;height&#34;:480,&#34;providerName&#34;:&#34;YouTube&#34;,&#34;thumbnailUrl&#34;:&#34;https://i.ytimg.com/vi/zliwDgidwsg/hqdefault.jpg&#34;,&#34;resolvedBy&#34;:&#34;youtube&#34;}" data-block-type="32" id="block-2904e584f84ae508441d"><div><div><div></div></div></div></div></div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650856400358_1244513"></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650856400358_1256544"><div><p>The environment class also internally adds a power draw penalty for moving the motors. For testing the agent, it’s fairly arbitrary how accurate the motor power draw per angle is—we just want to have some penalty for the agent moving the motors so it has to learn how to balance exploration (which consumes motor power) with exploitation (staying in the current position and not drawing current for the motors). </p><p>Thus, the environment’s reward is:</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1650986509006_2378020"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/61739ba0-eb14-4254-95d7-8c277a4509cf/eqn_reward_function.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/61739ba0-eb14-4254-95d7-8c277a4509cf/eqn_reward_function.png" data-image-dimensions="400x33" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6268184077cdc5222263af2b" data-type="image"/>
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_2378367"><p>Where the motor power draw is defined as:</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1650986509006_2404916"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/996fe1b1-0223-4595-987f-7306ca21eb6c/eqn_motor_draw.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/996fe1b1-0223-4595-987f-7306ca21eb6c/eqn_motor_draw.png" data-image-dimensions="979x42" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6268185f08fb8a08234cc120" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Sigma is defined as some constant. In this implementation, sigma is set to 0.0001W per degree of movement per motor</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_2405256"><div><p><em>For those interested, you can find more detailed, technical pages on the environment and simulation generation at the following links: <br/></em><a href="https://blog.plover.com/reinforcement-learning-solar/experiment-design" target="_blank"><span><em>Experiment Design</em></span></a><em> | </em><a href="https://blog.plover.com/reinforcement-learning-solar/experiment-one-shifting-env" target="_blank"><span><em>Experiment I: Shifting Env</em></span></a><em> | </em><a href="https://blog.plover.com/reinforcement-learning-solar/experiment-two-shifting-env" target="_blank"><span><em>Experiment II: Shifting Env with Time of Day</em></span></a></p><h2>Results</h2><p>I first conduct a hyperparameter study by sweeping the agent’s temperature, actor step size, critic step size, and average reward step size from 1e-5 to 1e0 in 10x increments, which leads to 625 permutations of agent hyperparameters. In the simulation for the hyperparameter study, I use 1M steps and assess performance based on the rolling average of power generation at the end of the simulation.</p><p>Based on this study, the below hyperparameter set was determined to be the most performant:</p><ul data-rte-list="default"><li><pre><code>temperature: 0.001</code></pre></li><li><pre><code>actor_step_size: 1.0</code></pre></li><li><pre><code>critic_step_size: 0.1</code></pre></li><li><pre><code>avg_reward_step_size: 1.0</code></pre></li></ul><p>To understand the performance of our system, we simulate one year’s worth of data and assess the energy produced by the agent as compared to the energy that would have been generated if the panel was perfectly positioned at each time step. Since we’re using 1 simulation step = 1 second, we need 365 * 86,400 = 31,536,000 simulation steps. Running this, we get the following agent performance:</p>


</div></div><div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650930762943_39520"></div></div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650930762943_24356"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650930762943_52897"></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650934633229_124622"><div><p>Within 1 month of operation, our agent is producing over 90% of the true optimal energy each day—all without needing any models or human inputs to capture and control for the environment dynamics. As the year goes on, the agent continues to learn and improve its energy production, achieving 96% of true optimal energy per day by the end of the year.</p><p>Examining agent behavior at the beginning of the simulation, we can see how the agent explores many actions the first time it encounters each state, then starts to narrow down its action selections as the hours go on. Below, we visualize the first 5 days of operation by hour:</p>


</div></div><div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650986509006_247618"></div></div><div><div data-block-json="{&#34;blockAnimation&#34;:&#34;none&#34;,&#34;layout&#34;:&#34;caption-hidden&#34;,&#34;overlay&#34;:false,&#34;description&#34;:{},&#34;hSize&#34;:null,&#34;floatDir&#34;:null,&#34;isOldBlock&#34;:false,&#34;html&#34;:&#34;&lt;iframe class=\&#34;embedly-embed\&#34; src=\&#34;//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FVhvFOGnJIug%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DVhvFOGnJIug&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FVhvFOGnJIug%2Fhqdefault.jpg&amp;key=61d05c9d54e8455ea7a9677c366be814&amp;type=text%2Fhtml&amp;schema=youtube\&#34; width=\&#34;640\&#34; height=\&#34;480\&#34; scrolling=\&#34;no\&#34; title=\&#34;YouTube embed\&#34; frameborder=\&#34;0\&#34; allow=\&#34;autoplay; fullscreen\&#34; allowfullscreen=\&#34;true\&#34;&gt;&lt;/iframe&gt;&#34;,&#34;url&#34;:&#34;https://www.youtube.com/watch?v=VhvFOGnJIug&#34;,&#34;width&#34;:640,&#34;height&#34;:480,&#34;providerName&#34;:&#34;YouTube&#34;,&#34;thumbnailUrl&#34;:&#34;https://i.ytimg.com/vi/VhvFOGnJIug/hqdefault.jpg&#34;,&#34;resolvedBy&#34;:&#34;youtube&#34;}" data-block-type="32" id="block-yui_3_17_2_1_1650935147997_96419"><div><div><div></div></div></div></div></div><div><div data-block-type="21" id="block-yui_3_17_2_1_1650986509006_260468"></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_343004"><p>Comparing the rolling power that the agent generates for the first million steps versus the last million steps, we also see how the agent’s behavior converges to optimal while the agent continues to learn.</p></div><div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650930762943_29305"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650930762943_64915"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_547540"><p>To further validate agent performance, we pass in a different light scan and use the same agent algorithm and hyperparameters. This light scan has much more defined regions of high power and lower power states:</p></div><div><div><div data-aspect-ratio="83.57843137254902" data-block-type="5" id="block-yui_3_17_2_1_1650986509006_799343"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/651bb053-556e-4429-9a6c-0561ddeb1fc6/scan_with_peak.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/651bb053-556e-4429-9a6c-0561ddeb1fc6/scan_with_peak.png" data-image-dimensions="1322x1106" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6268108027321e30ff3807b4" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>A separate light scan with more pronounced peaks and troughs.</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="5" id="block-yui_3_17_2_1_1650986509006_794959"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/f1e73e6f-7c4e-4035-9272-7c9ae960d0c4/heatmap_shifting.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/f1e73e6f-7c4e-4035-9272-7c9ae960d0c4/heatmap_shifting.png" data-image-dimensions="1064x900" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6268106c94a57b562d1f1a0b" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>The 2D reward array based on the light scan.</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_944217"><p>Running the simulation for 31,536,000 steps (a year in simulation), the agent again achieves around 95% of true optimal energy per day by the end of the year.</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1650986509006_2617330"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/66914c01-fdc9-4e79-96b7-19164e00099c/agent_indoor_performance.png" data-image="https://images.squarespace-cdn.com/content/v1/5bba8415c2ff6147bdcf9a89/66914c01-fdc9-4e79-96b7-19164e00099c/agent_indoor_performance.png" data-image-dimensions="800x600" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="62688b84e90b45221df6e4ec" data-type="image"/>
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_2617671"><div><h2>Conclusion</h2><p>This research demonstrates that a softmax actor-critic agent can optimize energy production for a dual-axis solar panel with a completely model-free, domain-knowledge-free approach. The agent optimizes for power generation, handles multiple dynamic lightning environments, and, in simulations emulating a year of operation, consistently achieves 95% of the fully optimal energy production per day.</p><p>While traditional methods and domain knowledge could theoretically model and solve an optimization problem like this, they don’t necessarily scale as the number of panels in an installation grows (consider inter-panel shading dynamics), nor are the solutions applicable to other problems we face in our transition to renewable energy. This contrast between domain-knowledge-based methods and AI-based methods points to a larger question: </p><p><em>As we face an increasing number of engineering challenges with the world’s transition to renewable energy, how can we scale humanity’s problem-solving capability? </em></p><p>I believe that RL enables us to solve a greater number of optimal-control problems for complex energy systems in less time, with less human effort, and more optimally so than traditional, non-AI based methods. </p>


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1650986509006_2649466"><div><h2>Open-Source Project Materials</h2><p>All schematics, algorithms, and code that went into this project are provided as open-source materials:</p><ul data-rte-list="default"><li><p>For detailed, technical content about each phase of the project, head to: <a href="https://blog.plover.com/reinforcement-learning-solar/project-overview" target="_blank"><span>Project Overview</span></a></p></li><li><p>To access the code used in this project, head to: <a href="https://github.com/jogrady23/rl-solar" target="_blank"><span>Code Repository on Github</span></a></p></li></ul><h2>Future Directions</h2><p>While I won’t be developing this research further, I see three promising future directions for it:</p><ul data-rte-list="default"><li><p><strong>Update the Q-Network and Policy Network to Deep Neural Networks: </strong>By getting rid of tabular state and action representations, the agent becomes much more general and could be easily adapted to control multiple panels.</p></li><li><p><strong>Multi-Panel Control: </strong>Utilize the underlying agent algorithm to control multiple panels, wherein certain panel positions shade the other panels in the solar array.</p></li><li><p><strong>Convert the Agent to C/C++ for Standalone, Online Deployment: </strong>While the agent as is can technically be operated as an online agent, it requires having a laptop running Python connected to the Arduino. By porting the code to C/C++, the agent could be flashed onto the Arduino and run as a standalone, online agent whenever the Arduino is powered on.</p></li></ul>


</div></div><div data-block-type="2" id="block-a3b7181fb8268fd42b2c"><div><h3>References</h3><p>[1] https://www.seia.org/solar-industry-research-data</p>


</div></div></div></div></div>
      </div>

      

      </section>
      </div></div>
  </body>
</html>
