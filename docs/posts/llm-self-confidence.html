<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://glitch.land/blog/llm-self-confidence/">Original</a>
    <h1>LLM Self Confidence</h1>
    
    <div id="readability-page-1" class="page"><div><hr/><p><i>Before I begin, I must disclose my expertise is in software engineering, game dev, and application security. I have a growing curiosity and passion to learn ML, but I am still very much a beginner.</i></p><hr/><h3 id="a-simple-notion">A Simple Notion?</h3><p>While thinking a little about LLM hallucinations, I was curious how difficult it would be to measure the confidence an LLM has in its answer.</p><p>So where do I start, I’m a single mom of two active children, my time is….limited. I needed to concoct a simple but accurate heuristic, and it turned out there was one hiding in plain sight?</p><p>The idea arose from this series of questions:</p><ul><li>How do I add a “certainty” or “confidence” score into an LLM without being able to retrain or change the model?</li><li>If an AI truly “knows” something, shouldn’t it give fairly consistent answers if asked the same question?</li><li>Conversely, if the temperature on a model is non-zero, won’t the responses be more diverse if it is hallucinating an answer?</li></ul><p>Well, you can probably already guess the notion from the above questions, my hypothesis was:</p><blockquote><p><i>If you input the LLM the same question multiple times and evaluate the semantic similarity of the responses. The measure will correlate with the confidence the model has in its answer.</i></p></blockquote><h3 id="the-journey">The Journey</h3><p>So, I set out on a journey to give my local LLM a very simple sense of confidence and to naively test my hypothesis.</p><p>Practically speaking, to implement this algorithm, we would need to measure the semantic similarity between two or more synthesized responses to the same question. But before that, I think we would also need to systematically compensate for how the temperature and normalization constants shaped the final responses.</p><hr/><p>I began to worry then, about the scope and complexity of implementing semantic similarity. But as I was daydreaming about this, it occurred to me that we could just use sentence embeddings, and/or contextual embeddings for this task. Relieved, I considered the remaining task: <i>compensating for how the temperature and normalization constants shape the final output.</i> For that, let me provide a little more context.</p><p>Temperature and normalization constants are applied at the <strong>final</strong> token selection step during text generation, right after the model computes its raw logits, but before sampling the next token. Logits are the raw, unnormalized scores that a neural network outputs before they get converted into normalized probabilities, commonly using a softmax function for transformer models.</p><details><summary><svg width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg>
<span>More Details</span></summary><div><p>More precisely, for GPT-2, the transformer produces raw logits <code>logits_i</code> for each possible next token. After that, the temperature scaling is applied:</p><p>$$scaled\_logits_i = logits_i / τ$$ Next, softmax normalization converts these scaled logits to probabilities: $$P(token_i) = &gt;exp(scaled\_logits_i) / Z_τ$$ where: $$Z_τ = Σ_j exp(logits_j / τ)$$ is the normalization constant. And then, finally, a token is sampled from this calculated probability distribution.</p></div></details><p>Crucially here, we don’t need to extract the original tokens that would have been chosen before temperature scaling was applied. If that were the case, I think it would be impossible since sampling is stochastic? Instead, at least for GPT-2 (and some other models), we can extract the token probabilities themselves from the model’s forward pass while it generates a response and then reverse the process.</p><hr/><h2 id="self-confidence--putting-the-theory-into-practice">Self Confidence : Putting The Theory Into Practice</h2><p>Instead of a binary heuristic true/false detection of hallucinations, I decided instead to score on a confidence gradient. So then, I expect the script to say <em>“I don’t know.”</em>, or <em>“I’m confident that I know this”</em>, or even: <em>“I’m not sure about this, I recall XYZ but you should check it.”</em> when the model is uncertain. With all of the unknowns, now knowns, I was ready to test my theory.</p><p>So I started implementing, and the first thing that I found, was GPT-2 hallucinated 100% of the time. I needed to rethink my setup. After some searching, I decided to use Googles <a href="https://huggingface.co/docs/transformers/main/en/model_doc/flan-t5">Flan-T5</a> model. It also supports probability extraction, and it is better-suited to answering questions. The large version of this model requires about 1GB of RAM and runs fine on CPU. Most importantly, it worked on my old PC.</p><h2 id="step-1--compensate-for-temperature">Step 1 : Compensate for temperature</h2><p>As mentioned earlier, for each generated token we take its probability (as extracted by <a href="https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075">compute_transition_scores</a>), take the logarithm, and multiply by the temperature. This should give us a value proportional to the model’s original confidence in that token before the temperature was applied:</p><p>$$logit_i​=τ⋅log(P(token_i​))+constant$$</p><p>Then by averaging these values across all tokens in the response, we should get a robust, temperature-independent confidence score for the whole answer. The code for this is below (<code>...</code> means code is removed for brevity):</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Call into our generation function:</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span>...</span>
</span></span><span><span>responses<span>,</span> token_probs <span>=</span> <span>self</span><span>.</span>generate_multiple_responses<span>(</span>
</span></span><span><span>    question<span>,</span> num_samples<span>,</span> max_new_tokens<span>,</span> temperature
</span></span><span><span><span>)</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span># Inside generate_multiple_responses, we extract the probabilities:</span>
</span></span><span><span><span>with</span> torch<span>.</span>no_grad<span>():</span>
</span></span><span><span>    outputs <span>=</span> <span>self</span><span>.</span>model<span>.</span>generate<span>(</span>
</span></span><span><span>        input_ids<span>=</span>input_ids<span>,</span>
</span></span><span><span>        attention_mask<span>=</span>attention_mask<span>,</span>
</span></span><span><span>        max_new_tokens<span>=</span>max_new_tokens<span>,</span>
</span></span><span><span>        temperature<span>=</span>temperature<span>,</span>
</span></span><span><span>        do_sample<span>=</span><span>True</span><span>,</span>
</span></span><span><span>        return_dict_in_generate<span>=</span><span>True</span><span>,</span>
</span></span><span><span>        output_scores<span>=</span><span>True</span><span>,</span> <span># We need to set this to true</span>
</span></span><span><span>        pad_token_id<span>=</span><span>self</span><span>.</span>tokenizer<span>.</span>eos_token_id <span>if</span> <span>self</span><span>.</span>tokenizer<span>.</span>eos_token_id <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>self</span><span>.</span>tokenizer<span>.</span>pad_token_id
</span></span><span><span>    <span>)</span>
</span></span><span><span>generated_text <span>=</span> <span>self</span><span>.</span>tokenizer<span>.</span>decode<span>(</span>outputs<span>.</span>sequences<span>[</span><span>0</span><span>],</span> skip_special_tokens<span>=</span><span>True</span><span>)</span>
</span></span><span><span><span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Raw decoded output: </span><span>{</span>generated_text<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>responses<span>.</span>append<span>(</span>generated_text<span>.</span>strip<span>())</span>
</span></span><span><span>generated_ids <span>=</span> outputs<span>.</span>sequences
</span></span><span><span>transition_scores <span>=</span> <span>self</span><span>.</span>model<span>.</span>compute_transition_scores<span>(</span> <span># &lt;- here :)</span>
</span></span><span><span>    outputs<span>.</span>sequences<span>,</span> outputs<span>.</span>scores<span>,</span> normalize_logits<span>=</span><span>True</span>
</span></span><span><span><span>)</span>
</span></span><span><span>probs <span>=</span> np<span>.</span>exp<span>(</span>transition_scores<span>.</span>numpy<span>())</span>
</span></span><span><span>all_token_probs<span>.</span>append<span>(</span>probs<span>.</span>flatten<span>()</span><span>.</span>tolist<span>())</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span># And back in the parent function, after they are returned by </span>
</span></span><span><span><span># generate_multiple_responses as token_probs we pass them and </span>
</span></span><span><span><span># the relative temperatures into compensate_temperature_effects</span>
</span></span><span><span>temperatures <span>=</span> <span>[</span>temperature<span>]</span> <span>*</span> num_samples
</span></span><span><span>confidences <span>=</span> <span>self</span><span>.</span>compensate_temperature_effects<span>(</span>token_probs<span>,</span> temperatures<span>)</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span># Where compensate_temperature_effects just applies the maths </span>
</span></span><span><span><span># we discussed earlier:</span>
</span></span><span><span>    <span>def</span> <span>compensate_temperature_effects</span><span>(</span><span>self</span><span>,</span> token_probs_list<span>,</span> temperatures<span>):</span>
</span></span><span><span>        compensated_sequences <span>=</span> <span>[]</span>
</span></span><span><span>        <span>for</span> token_probs<span>,</span> temp <span>in</span> <span>zip</span><span>(</span>token_probs_list<span>,</span> temperatures<span>):</span>
</span></span><span><span>            <span>if</span> token_probs <span>is</span> <span>None</span> <span>or</span> <span>len</span><span>(</span>token_probs<span>)</span> <span>==</span> <span>0</span><span>:</span>
</span></span><span><span>                compensated_sequences<span>.</span>append<span>(</span><span>-</span><span>10.0</span><span>)</span>
</span></span><span><span>                <span>continue</span>
</span></span><span><span>            sequence_confidence <span>=</span> <span>[]</span>
</span></span><span><span>            <span>for</span> prob <span>in</span> token_probs<span>:</span>
</span></span><span><span>                <span>if</span> prob <span>&gt;</span> <span>1e-10</span><span>:</span>
</span></span><span><span>                    relative_logit <span>=</span> temp <span>*</span> np<span>.</span>log<span>(</span>prob<span>)</span>
</span></span><span><span>                    sequence_confidence<span>.</span>append<span>(</span>relative_logit<span>)</span>
</span></span><span><span>                <span>else</span><span>:</span>
</span></span><span><span>                    sequence_confidence<span>.</span>append<span>(</span><span>-</span><span>100</span><span>)</span>
</span></span><span><span>            <span>if</span> sequence_confidence<span>:</span>
</span></span><span><span>                avg_confidence <span>=</span> np<span>.</span>mean<span>(</span>sequence_confidence<span>)</span>
</span></span><span><span>            <span>else</span><span>:</span>
</span></span><span><span>                avg_confidence <span>=</span> <span>-</span><span>10.0</span>
</span></span><span><span>            compensated_sequences<span>.</span>append<span>(</span>avg_confidence<span>)</span>
</span></span><span><span>        <span>return</span> compensated_sequences
</span></span></code></pre></div><h2 id="step-2--semantic-similarity">Step 2 : Semantic Similarity</h2><p>For our semantic comparison, I started out with a simple <code>MiniLM mean cosine</code> check. However, while it was quite fast, I was disappointed by its accuracy. So after some more reading, I chose a <code>cross-encoders for pairwise scoring</code> approach, and it turns out all of the hard work was already done for this, so adding it was only a couple of lines of code similar to this:</p><div><pre tabindex="0"><code data-lang="python"><span><span> <span>from</span> <span>sentence_transformers</span> <span>import</span> CrossEncoder
</span></span><span><span> cross_encoder <span>=</span> CrossEncoder<span>(</span><span>&#39;cross-encoder/stsb-roberta-base&#39;</span><span>)</span>
</span></span><span><span> score <span>=</span> cross_encoder<span>.</span>predict<span>([(</span><span>&#34;one sentence&#34;</span><span>,</span> <span>&#34;two sentence&#34;</span><span>)])</span>
</span></span></code></pre></div><h2 id="step-3--how-confident-are-you--ask-a-friend">Step 3 : How Confident Are You? … Ask a friend?</h2><p>Before we connect all of the parts of our implementation, we need to decide how we select one of the answers after making our confidence determination about our set of answers.</p><p><em>But which one should we choose?</em> We have a set of answers and they are all a bit different due to the temperature effect on the output. Naively picking the first or most frequent answer might be misleading, especially when the model produces outliers. </p><p>Instead, I utilized a clustering approach, specifically I applied K-means clustering (I wrote about K-means before <a href="https://glitch.land/blog/k-means-4-tone/">here</a>) to the semantic embeddings of all generated responses, this grouped semantically similar answers together. Then I identified the largest cluster of answers, hopefully the one with the most agreement. After that, I used the geometry of the embeddings and calculated the centroid of that cluster, then got the closest answer to that point as our chosen answer.</p><p>I think thats a fairly elegant approach? It should select a consensus response that best reflects the model’s dominant <strong>“belief”</strong>, rather than being swayed by outliers or noise. At least I think so, if my intuition and mental model for embeddings are accurate. The code for this is:</p><div><pre tabindex="0"><code data-lang="python"><span><span>    <span>def</span> <span>select_consensus_response_kmeans</span><span>(</span><span>self</span><span>,</span> responses<span>,</span> k<span>=</span><span>2</span><span>):</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Cluster responses using K-means and select the response closest to the centroid
</span></span></span><span><span><span>        of the largest cluster (consensus).
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        <span>if</span> <span>len</span><span>(</span>responses<span>)</span> <span>==</span> <span>0</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>&#34;&#34;</span><span>,</span> <span>[]</span>
</span></span><span><span>        <span>if</span> <span>len</span><span>(</span>responses<span>)</span> <span>==</span> <span>1</span><span>:</span>
</span></span><span><span>            <span>return</span> responses<span>[</span><span>0</span><span>],</span> responses
</span></span><span><span>
</span></span><span><span>        <span># Embed responses</span>
</span></span><span><span>        embeddings <span>=</span> <span>self</span><span>.</span>similarity_model<span>.</span>encode<span>(</span>responses<span>)</span>
</span></span><span><span>        <span># Run K-means</span>
</span></span><span><span>        k <span>=</span> <span>min</span><span>(</span>k<span>,</span> <span>len</span><span>(</span>responses<span>))</span>  <span># can&#39;t have more clusters than responses</span>
</span></span><span><span>        kmeans <span>=</span> KMeans<span>(</span>n_clusters<span>=</span>k<span>,</span> random_state<span>=</span><span>42</span><span>,</span> n_init<span>=</span><span>&#39;auto&#39;</span><span>)</span>
</span></span><span><span>        labels <span>=</span> kmeans<span>.</span>fit_predict<span>(</span>embeddings<span>)</span>
</span></span><span><span>        <span># Find the largest cluster</span>
</span></span><span><span>        unique<span>,</span> counts <span>=</span> np<span>.</span>unique<span>(</span>labels<span>,</span> return_counts<span>=</span><span>True</span><span>)</span>
</span></span><span><span>        largest_cluster <span>=</span> unique<span>[</span>np<span>.</span>argmax<span>(</span>counts<span>)]</span>
</span></span><span><span>        cluster_indices <span>=</span> <span>[</span>i <span>for</span> i<span>,</span> label <span>in</span> <span>enumerate</span><span>(</span>labels<span>)</span> <span>if</span> label <span>==</span> largest_cluster<span>]</span>
</span></span><span><span>        cluster_embeddings <span>=</span> embeddings<span>[</span>cluster_indices<span>]</span>
</span></span><span><span>        cluster_responses <span>=</span> <span>[</span>responses<span>[</span>i<span>]</span> <span>for</span> i <span>in</span> cluster_indices<span>]</span>
</span></span><span><span>        <span># Find the response closest to the centroid</span>
</span></span><span><span>        centroid <span>=</span> kmeans<span>.</span>cluster_centers_<span>[</span>largest_cluster<span>]</span>
</span></span><span><span>        dists <span>=</span> np<span>.</span>linalg<span>.</span>norm<span>(</span>cluster_embeddings <span>-</span> centroid<span>,</span> axis<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        best_idx <span>=</span> np<span>.</span>argmin<span>(</span>dists<span>)</span>
</span></span><span><span>        best_response <span>=</span> cluster_responses<span>[</span>best_idx<span>]</span>
</span></span><span><span>        <span>return</span> best_response<span>,</span> cluster_responses
</span></span></code></pre></div><hr/><h2 id="some-runtime-logs">Some Runtime Logs</h2><p>I integrated the remaining scaffolding for our hypothesis test code, and fed it some questions. Here are the responses:</p><div><pre tabindex="0"><code data-lang="text"><span><span>============================================================
</span></span><span><span>Testing: What is 2 + 2?
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I don&#39;t know the answer to this. 
</span></span><span><span>(Bees Note: It really didn&#39;t know the answer to this.)
</span></span><span><span>============================================================
</span></span><span><span>
</span></span><span><span>============================================================
</span></span><span><span>Testing: Describe the plot of the movie &#39;The Quantum Paradox&#39; from 1987.
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I don&#39;t know the answer to this.
</span></span><span><span>(Bees Note: This is correct, there is no such movie.)
</span></span><span><span>============================================================
</span></span><span><span>
</span></span><span><span>============================================================
</span></span><span><span>Testing: Explain the discovery of the element &#39;Fictonium&#39; by Dr. John Madeupname in the year 1337.
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I don&#39;t know the answer to this.
</span></span><span><span>(Bees Note: This is correct, there is no such element.)
</span></span><span><span>============================================================
</span></span><span><span>
</span></span><span><span>============================================================
</span></span><span><span>Testing: What are the main ingredients in the traditional dish &#39;Flibbernaught Stew&#39; from medieval England?
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I don&#39;t know the answer to this.
</span></span><span><span>(Bees Note: This is correct, there is no such stew.)
</span></span><span><span>============================================================
</span></span><span><span>
</span></span><span><span>============================================================
</span></span><span><span>Testing: What is the capital of France?
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I&#39;m very confident the answer is: Paris
</span></span><span><span>============================================================
</span></span><span><span>
</span></span><span><span>============================================================
</span></span><span><span>Testing: Tell me about the life of Dr. Zelinda Farthingbottom, the famous 19th-century astronomer.
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I don&#39;t know the answer to this.
</span></span><span><span>(Bees Note: This is correct, there is no such Dr.)
</span></span><span><span>============================================================
</span></span><span><span>
</span></span><span><span>============================================================
</span></span><span><span>Testing: Who wrote the novel &#39;To Kill a Mockingbird&#39;?
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I don&#39;t know the answer to this.
</span></span><span><span>(Bees Note: This is correct, it really was unsure, these were 
</span></span><span><span>its guesses:
</span></span><span><span>Responses:
</span></span><span><span>  1. (temp=0.97 conf=-1.076538428502998)
</span></span><span><span>     Faulkner
</span></span><span><span>  2. (temp=0.97 conf=-0.2628941149206578)
</span></span><span><span>     Harper Lee
</span></span><span><span>  3. (temp=0.97 conf=-0.2628941149206578)
</span></span><span><span>     Harper Lee
</span></span><span><span>  4. (temp=0.97 conf=-0.598906322145417)
</span></span><span><span>     A. J. Paltrow
</span></span><span><span>  5. (temp=0.97 conf=-1.07315808609994)
</span></span><span><span>     Montgomery Clift
</span></span><span><span>  6. (temp=0.97 conf=-0.2628941149206578)
</span></span><span><span>     Harper Lee
</span></span><span><span>  7. (temp=0.97 conf=-1.3127088368370317)
</span></span><span><span>     Tom Hanks
</span></span><span><span>  8. (temp=0.97 conf=-0.7763668985525249)
</span></span><span><span>     Robert Louis Stevenson
</span></span><span><span>  9. (temp=0.97 conf=-0.8343997280612352)
</span></span><span><span>     f scott fitzgerald
</span></span><span><span>  10. (temp=0.97 conf=-0.8507150286995726)
</span></span><span><span>     J.M. Barrie
</span></span><span><span>)
</span></span><span><span>============================================================
</span></span><span><span>
</span></span><span><span>============================================================
</span></span><span><span>Testing: What is the chemical formula for water?
</span></span><span><span>thinking...: 100%|█████████████████████████████████████████████| 10/10
</span></span><span><span>
</span></span><span><span>Final Answer: I don&#39;t know the answer to this.
</span></span><span><span>(Bees Note: Again this is correct, I was surprised it didn&#39;t know this.
</span></span><span><span>Its guesses were:
</span></span><span><span>Responses:
</span></span><span><span>  1. (temp=0.97 conf=-0.45178970254642453)
</span></span><span><span>     H2O
</span></span><span><span>  2. (temp=0.97 conf=-1.0207007067520621)
</span></span><span><span>     H 2 O
</span></span><span><span>  3. (temp=0.97 conf=-0.9457521845302133)
</span></span><span><span>     wt
</span></span><span><span>  4. (temp=0.97 conf=-1.9991200647992708)
</span></span><span><span>     w h w
</span></span><span><span>  5. (temp=0.97 conf=-2.413021369189594)
</span></span><span><span>     pH
</span></span><span><span>  6. (temp=0.97 conf=-0.45968350047439777)
</span></span><span><span>     w
</span></span><span><span>  7. (temp=0.97 conf=-0.9560885998466766)
</span></span><span><span>     OH
</span></span><span><span>  8. (temp=0.97 conf=-0.8657086350604895)
</span></span><span><span>     h2o
</span></span><span><span>  9. (temp=0.97 conf=-0.45968350047439777)
</span></span><span><span>     w
</span></span><span><span>  10. (temp=0.97 conf=-0.9560885998466766)
</span></span><span><span>     OH
</span></span><span><span>)
</span></span><span><span>============================================================
</span></span></code></pre></div><h2 id="well-then-is-this-worth-further-study">Well then, Is this worth further study?</h2><p>Well, this was a very casual experiment. I didn’t follow a formal method, so its hard to tell for sure ☺️. My intuition says…. maybe? While my approach here is naive, I think AIs will need a measure of self-confidence in order to drive further learning, exploration, and curiosity in the future?</p><p>There are edge cases where my heuristics fell short, and the model I chose wasn’t able to answer many questions correctly. For example, for the question <code>What is 2 + 2?</code>, in one run the model was very confident that the answer was <code>2 + 2</code>, technically this is a correct answer, but I think a human would infer that the asker wanted the answer of <code>4</code>? So I’m unsure if its worth pursuing and developing a more formal thesis from here.</p><p>Watching some of the hallucinations were quite funny though, delights such as <code>Flibbernaught Stew</code>, 100% butter. mmmm delicious.. or <code>hog fat, onions, onions, and salt and pepper</code>, double onions for good measure cooked in <code>hog fat</code> no less.</p><div><pre tabindex="0"><code data-lang="text"><span><span>Question: What are the main ingredients in the traditional dish 
</span></span><span><span>          &#39;Flibbernaught Stew&#39; from medieval England?
</span></span><span><span>Detection: UNCERTAIN
</span></span><span><span>Risk Level: MEDIUM
</span></span><span><span>Avg Similarity: 0.209
</span></span><span><span>Avg Confidence: -2.182
</span></span><span><span>Temperature: 0.97
</span></span><span><span>Individual Similarities: [&#39;0.534&#39;, &#39;0.473&#39;, &#39;0.154&#39;, &#39;0.113&#39;, &#39;0.419&#39;, &#39;0.188&#39;, &#39;0.344&#39;, &#39;0.157&#39;, &#39;0.104&#39;, &#39;0.527&#39;, &#39;0.030&#39;, &#39;0.056&#39;, &#39;0.298&#39;, &#39;0.245&#39;, &#39;0.278&#39;, &#39;0.339&#39;, &#39;0.053&#39;, &#39;0.373&#39;, &#39;0.103&#39;, &#39;0.182&#39;, &#39;0.538&#39;, &#39;0.353&#39;, &#39;0.165&#39;, &#39;0.058&#39;, &#39;0.061&#39;, &#39;0.329&#39;, &#39;0.313&#39;, &#39;0.033&#39;, &#39;0.195&#39;, &#39;0.047&#39;, &#39;0.014&#39;, &#39;0.156&#39;, &#39;0.260&#39;, &#39;0.032&#39;, &#39;0.636&#39;, &#39;0.123&#39;, &#39;0.056&#39;, &#39;0.283&#39;, &#39;0.073&#39;, &#39;0.356&#39;, &#39;0.105&#39;, &#39;0.075&#39;, &#39;0.047&#39;, &#39;0.081&#39;, &#39;0.060&#39;]
</span></span><span><span>Individual Confidences: [&#39;-1.719&#39;, &#39;-1.955&#39;, &#39;-2.807&#39;, &#39;-2.469&#39;, &#39;-1.601&#39;, &#39;-2.985&#39;, &#39;-1.859&#39;, &#39;-1.030&#39;, &#39;-2.059&#39;, &#39;-3.337&#39;]
</span></span><span><span>Responses:
</span></span><span><span>  1. (temp=0.97 conf=-1.7185745795482497)
</span></span><span><span>     kiln dried beans, carrots, onion, salt
</span></span><span><span>  2. (temp=0.97 conf=-1.9549108758864049)
</span></span><span><span>     hog fat, onions, onions, and salt and pepper
</span></span><span><span>  3. (temp=0.97 conf=-2.806552017047853)
</span></span><span><span>     beef fat, onions
</span></span><span><span>  4. (temp=0.97 conf=-2.468553621657526)
</span></span><span><span>     beef meatloaf root
</span></span><span><span>  5. (temp=0.97 conf=-1.600746207479049)
</span></span><span><span>     flour, water, milk, butter
</span></span><span><span>  6. (temp=0.97 conf=-2.985059213336243)
</span></span><span><span>     salt pork
</span></span><span><span>  7. (temp=0.97 conf=-1.8594504321931333)
</span></span><span><span>     apple cider, beef stock, cream, bay leaf, onions
</span></span><span><span>  8. (temp=0.97 conf=-1.030147153618215)
</span></span><span><span>     flour, onions, thyme, and apricots
</span></span><span><span>  9. (temp=0.97 conf=-2.058654045511679)
</span></span><span><span>     sage, black pepper, salt, black worms and beef thigh
</span></span><span><span>  10. (temp=0.97 conf=-3.337035475555869)
</span></span><span><span>     butter
</span></span></code></pre></div><h2 id="conclusion">Conclusion</h2><p>I had fun exploring this topic, and thinking/reading/programming through it. I think it would be interesting to more formally explore the notion of what makes up certainty, and how that might drive a model or agent to explore and learn in an autodidactic way. If you made it all the way through, thanks for reading and sharing this journey with me. Until next time, my love &lt;3.</p><h2 id="code">Code</h2><p>Its messy sketch code, but here is the code if you want to experiment and build on the idea for yourself:</p><div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span>#!/usr/bin/env python3</span>
</span></span><span><span><span>&#34;&#34;&#34;
</span></span></span><span><span><span>A stupidly simple approach to implementing a self-confidence for 
</span></span></span><span><span><span>LLMs using multi-response semantic similarity
</span></span></span><span><span><span>
</span></span></span><span><span><span>Only python 3.12 works (because sentencepiece breaks for 3.13):
</span></span></span><span><span><span>pyenv install 3.12.7
</span></span></span><span><span><span>pyenv global 3.12.7
</span></span></span><span><span><span>
</span></span></span><span><span><span>python -m venv self_confidence
</span></span></span><span><span><span>source self_confidence/bin/activate
</span></span></span><span><span><span># deactivate to exit
</span></span></span><span><span><span>
</span></span></span><span><span><span>pip install torch transformers sentence-transformers sentencepiece numpy
</span></span></span><span><span><span>
</span></span></span><span><span><span>Usage:
</span></span></span><span><span><span>python llm_self_confidence.py
</span></span></span><span><span><span>&#34;&#34;&#34;</span>
</span></span><span><span>
</span></span><span><span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
</span></span><span><span><span>import</span> <span>torch</span>
</span></span><span><span><span>import</span> <span>random</span>
</span></span><span><span><span>from</span> <span>transformers</span> <span>import</span> AutoTokenizer<span>,</span> AutoModelForCausalLM<span>,</span> AutoModelForSeq2SeqLM<span>,</span> T5Tokenizer<span>,</span> T5ForConditionalGeneration
</span></span><span><span><span>from</span> <span>sentence_transformers</span> <span>import</span> SentenceTransformer<span>,</span> CrossEncoder
</span></span><span><span><span>import</span> <span>warnings</span>
</span></span><span><span><span>import</span> <span>os</span>
</span></span><span><span><span>import</span> <span>sys</span>
</span></span><span><span><span>from</span> <span>enum</span> <span>import</span> Enum
</span></span><span><span><span>from</span> <span>sklearn.cluster</span> <span>import</span> KMeans
</span></span><span><span><span>import</span> <span>time</span>
</span></span><span><span><span>from</span> <span>tqdm</span> <span>import</span> tqdm
</span></span><span><span>
</span></span><span><span>warnings<span>.</span>filterwarnings<span>(</span><span>&#34;ignore&#34;</span><span>)</span>
</span></span><span><span>os<span>.</span>environ<span>[</span><span>&#34;TOKENIZERS_PARALLELISM&#34;</span><span>]</span> <span>=</span> <span>&#34;false&#34;</span>
</span></span><span><span>
</span></span><span><span><span># === GLOBAL CONSTANTS ===</span>
</span></span><span><span>SIMILARITY_THRESHOLD_CORRECT <span>=</span> <span>0.76</span>
</span></span><span><span><span># Note, I had to play with this a bit to get a value that worked well. It needs to be fairly </span>
</span></span><span><span><span># high variance to generate randomness in correct answers too</span>
</span></span><span><span>DEFAULT_TEMPERATURE <span>=</span> <span>0.97</span> 
</span></span><span><span>DEFAULT_CONFIDENCE_THRESHOLD <span>=</span> <span>-</span><span>3.0</span>
</span></span><span><span>TRUNCATION_LENGTH <span>=</span> <span>180</span>
</span></span><span><span>GENERATION_ITERATIONS <span>=</span> <span>10</span>
</span></span><span><span>HIGH_FICTIONAL_THRESHOLD <span>=</span> <span>0.95</span>
</span></span><span><span>VERBOSITY <span>=</span> <span>1</span>  <span># 0 = silent, 1 = summary, 2 = debug</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>RiskLevel</span><span>(</span>Enum<span>):</span>
</span></span><span><span>    HIGH <span>=</span> <span>&#34;HIGH&#34;</span>
</span></span><span><span>    MEDIUM <span>=</span> <span>&#34;MEDIUM&#34;</span>
</span></span><span><span>    LOW <span>=</span> <span>&#34;LOW&#34;</span>
</span></span><span><span>    UNKNOWN <span>=</span> <span>&#34;UNKNOWN&#34;</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>DetectionResult</span><span>(</span>Enum<span>):</span>
</span></span><span><span>    CONFIDENT <span>=</span> <span>&#34;CONFIDENT&#34;</span>
</span></span><span><span>    UNCERTAIN <span>=</span> <span>&#34;UNCERTAIN&#34;</span>
</span></span><span><span>    LOW_CONFIDENCE <span>=</span> <span>&#34;LOW_CONFIDENCE&#34;</span>
</span></span><span><span>    ERROR <span>=</span> <span>&#34;ERROR&#34;</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>SelfConfidenceDetector</span><span>:</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> model_name<span>=</span><span>&#34;google/flan-t5-large&#34;</span><span>,</span> cross_encoder_model<span>=</span><span>&#34;cross-encoder/stsb-roberta-base&#34;</span><span>):</span>
</span></span><span><span>        <span>self</span><span>.</span>verbosity <span>=</span> VERBOSITY
</span></span><span><span>        <span>print</span><span>(</span><span>f</span><span>&#34;Loading model: </span><span>{</span>model_name<span>}</span><span>&#34;</span><span>)</span>
</span></span><span><span>        <span>try</span><span>:</span>
</span></span><span><span>            <span>self</span><span>.</span>tokenizer <span>=</span> T5Tokenizer<span>.</span>from_pretrained<span>(</span>model_name<span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>model <span>=</span> T5ForConditionalGeneration<span>.</span>from_pretrained<span>(</span>model_name<span>)</span>
</span></span><span><span>            <span>print</span><span>(</span><span>f</span><span>&#34;Loading cross-encoder: </span><span>{</span>cross_encoder_model<span>}</span><span> ...&#34;</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>cross_encoder <span>=</span> CrossEncoder<span>(</span>cross_encoder_model<span>)</span>
</span></span><span><span>            <span>print</span><span>(</span><span>&#34;Loading embedding model for clustering: all-MiniLM-L6-v2 ...&#34;</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>similarity_model <span>=</span> SentenceTransformer<span>(</span><span>&#39;all-MiniLM-L6-v2&#39;</span><span>)</span>
</span></span><span><span>            <span>print</span><span>(</span><span>&#34;✓ Model(s) loaded successfully!&#34;</span><span>)</span>
</span></span><span><span>        <span>except</span> <span>Exception</span> <span>as</span> e<span>:</span>
</span></span><span><span>            <span>print</span><span>(</span><span>f</span><span>&#34;✗ Error loading model: </span><span>{</span>e<span>}</span><span>&#34;</span><span>)</span>
</span></span><span><span>            <span>raise</span>
</span></span><span><span>    
</span></span><span><span>    <span>@classmethod</span>
</span></span><span><span>    <span>def</span> <span>set_expected_similarities</span><span>(</span><span>cls</span><span>,</span> expected_dict<span>):</span>
</span></span><span><span>        <span>cls</span><span>.</span>expected_similarity_by_temp <span>=</span> expected_dict
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>get_expected_similarity</span><span>(</span><span>self</span><span>,</span> temperature<span>):</span>
</span></span><span><span>        <span>return</span> <span>self</span><span>.</span>expected_similarity_by_temp<span>.</span>get<span>(</span>temperature<span>,</span> <span>1.0</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>log</span><span>(</span><span>self</span><span>,</span> msg<span>,</span> level<span>=</span><span>1</span><span>,</span> end<span>=</span><span>&#34;</span><span>\n</span><span>&#34;</span><span>):</span>
</span></span><span><span>        <span>if</span> <span>self</span><span>.</span>verbosity <span>&gt;=</span> level<span>:</span>
</span></span><span><span>            <span>print</span><span>(</span>msg<span>,</span> end<span>=</span>end<span>)</span>
</span></span><span><span>            sys<span>.</span>stdout<span>.</span>flush<span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>get_token_probs</span><span>(</span><span>self</span><span>,</span> prompt<span>,</span> generated_ids<span>):</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Given a prompt and generated_ids (tensor of generated token ids),
</span></span></span><span><span><span>        compute per-token probabilities using a forward pass.
</span></span></span><span><span><span>
</span></span></span><span><span><span>        How this works:
</span></span></span><span><span><span>        1. For each generated response, we take the prompt and the generated sequence.
</span></span></span><span><span><span>        2. We run a forward pass through the model with the prompt as input and the generated sequence (shifted right) as decoder input.
</span></span></span><span><span><span>        3. The model outputs logits (unnormalized scores) for each possible token at each position in the sequence.
</span></span></span><span><span><span>        4. We apply the softmax function to the logits to obtain probabilities for each token in the vocabulary at each position.
</span></span></span><span><span><span>        5. For each position, we extract the probability assigned to the actual generated token at that position (i.e., the probability the model assigned to the token it chose).
</span></span></span><span><span><span>        6. The result is a list of probabilities, one for each token in the generated response, representing the model&#39;s confidence in its choice at each step.
</span></span></span><span><span><span>
</span></span></span><span><span><span>        Returns a list of probabilities for each generated token.
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        input_ids <span>=</span> <span>self</span><span>.</span>tokenizer<span>(</span>prompt<span>,</span> return_tensors<span>=</span><span>&#34;pt&#34;</span><span>)</span><span>.</span>input_ids
</span></span><span><span>        decoder_input_ids <span>=</span> generated_ids<span>[:,</span> <span>:</span><span>-</span><span>1</span><span>]</span>
</span></span><span><span>        labels <span>=</span> generated_ids<span>[:,</span> <span>1</span><span>:]</span>
</span></span><span><span>        <span>with</span> torch<span>.</span>no_grad<span>():</span>
</span></span><span><span>            outputs <span>=</span> <span>self</span><span>.</span>model<span>(</span>
</span></span><span><span>                input_ids<span>=</span>input_ids<span>,</span>
</span></span><span><span>                decoder_input_ids<span>=</span>decoder_input_ids<span>,</span>
</span></span><span><span>            <span>)</span>
</span></span><span><span>            logits <span>=</span> outputs<span>.</span>logits  <span># shape: [1, seq_len, vocab_size]</span>
</span></span><span><span>            probs <span>=</span> torch<span>.</span>softmax<span>(</span>logits<span>,</span> dim<span>=-</span><span>1</span><span>)</span>
</span></span><span><span>            token_probs <span>=</span> <span>[]</span>
</span></span><span><span>            <span>for</span> i <span>in</span> <span>range</span><span>(</span>labels<span>.</span>shape<span>[</span><span>1</span><span>]):</span>
</span></span><span><span>                token_id <span>=</span> labels<span>[</span><span>0</span><span>,</span> i<span>]</span><span>.</span>item<span>()</span>
</span></span><span><span>                prob <span>=</span> probs<span>[</span><span>0</span><span>,</span> i<span>,</span> token_id<span>]</span><span>.</span>item<span>()</span>
</span></span><span><span>                token_probs<span>.</span>append<span>(</span>prob<span>)</span>
</span></span><span><span>        <span>return</span> <span>list</span><span>(</span>token_probs<span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>generate_multiple_responses</span><span>(</span><span>self</span><span>,</span> question<span>,</span> num_samples<span>=</span>GENERATION_ITERATIONS<span>,</span> max_new_tokens<span>=</span><span>100</span><span>,</span> temperature<span>=</span>DEFAULT_TEMPERATURE<span>):</span>
</span></span><span><span>        responses <span>=</span> <span>[]</span>
</span></span><span><span>        all_token_probs <span>=</span> <span>[]</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Generating </span><span>{</span>num_samples<span>}</span><span> responses with temperature </span><span>{</span>temperature<span>}</span><span>...&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>        prompt <span>=</span> <span>self</span><span>.</span>prompt_wrapper<span>(</span>question<span>)</span>
</span></span><span><span>        <span>for</span> i <span>in</span> <span>range</span><span>(</span>num_samples<span>):</span>
</span></span><span><span>            <span>try</span><span>:</span>
</span></span><span><span>                inputs <span>=</span> <span>self</span><span>.</span>tokenizer<span>(</span>
</span></span><span><span>                    prompt<span>,</span>
</span></span><span><span>                    return_tensors<span>=</span><span>&#34;pt&#34;</span><span>,</span>
</span></span><span><span>                    padding<span>=</span><span>True</span>
</span></span><span><span>                <span>)</span>
</span></span><span><span>                input_ids <span>=</span> inputs<span>[</span><span>&#34;input_ids&#34;</span><span>]</span>
</span></span><span><span>                attention_mask <span>=</span> inputs<span>[</span><span>&#34;attention_mask&#34;</span><span>]</span>
</span></span><span><span>                <span>with</span> torch<span>.</span>no_grad<span>():</span>
</span></span><span><span>                    outputs <span>=</span> <span>self</span><span>.</span>model<span>.</span>generate<span>(</span>
</span></span><span><span>                        input_ids<span>=</span>input_ids<span>,</span>
</span></span><span><span>                        attention_mask<span>=</span>attention_mask<span>,</span>
</span></span><span><span>                        max_new_tokens<span>=</span>max_new_tokens<span>,</span>
</span></span><span><span>                        temperature<span>=</span>temperature<span>,</span>
</span></span><span><span>                        do_sample<span>=</span><span>True</span><span>,</span>
</span></span><span><span>                        return_dict_in_generate<span>=</span><span>True</span><span>,</span>
</span></span><span><span>                        output_scores<span>=</span><span>True</span><span>,</span>
</span></span><span><span>                        pad_token_id<span>=</span><span>self</span><span>.</span>tokenizer<span>.</span>eos_token_id <span>if</span> <span>self</span><span>.</span>tokenizer<span>.</span>eos_token_id <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>self</span><span>.</span>tokenizer<span>.</span>pad_token_id
</span></span><span><span>                    <span>)</span>
</span></span><span><span>                generated_text <span>=</span> <span>self</span><span>.</span>tokenizer<span>.</span>decode<span>(</span>outputs<span>.</span>sequences<span>[</span><span>0</span><span>],</span> skip_special_tokens<span>=</span><span>True</span><span>)</span>
</span></span><span><span>                <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Raw decoded output: </span><span>{</span>generated_text<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>                responses<span>.</span>append<span>(</span>generated_text<span>.</span>strip<span>())</span>
</span></span><span><span>                generated_ids <span>=</span> outputs<span>.</span>sequences
</span></span><span><span>                transition_scores <span>=</span> <span>self</span><span>.</span>model<span>.</span>compute_transition_scores<span>(</span>
</span></span><span><span>                    outputs<span>.</span>sequences<span>,</span> outputs<span>.</span>scores<span>,</span> normalize_logits<span>=</span><span>True</span>
</span></span><span><span>                <span>)</span>
</span></span><span><span>                probs <span>=</span> np<span>.</span>exp<span>(</span>transition_scores<span>.</span>numpy<span>())</span>
</span></span><span><span>                all_token_probs<span>.</span>append<span>(</span>probs<span>.</span>flatten<span>()</span><span>.</span>tolist<span>())</span>
</span></span><span><span>                <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Response </span><span>{</span>i<span>+</span><span>1</span><span>}</span><span> (temp=</span><span>{</span>temperature<span>:</span><span>.2f</span><span>}</span><span>): </span><span>{</span>generated_text<span>[:</span><span>150</span><span>]</span><span>}</span><span>...&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>except</span> <span>Exception</span> <span>as</span> e<span>:</span>
</span></span><span><span>                <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  ✗ Error generating response </span><span>{</span>i<span>+</span><span>1</span><span>}</span><span>: </span><span>{</span>e<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>                responses<span>.</span>append<span>(</span><span>&#34;&#34;</span><span>)</span>
</span></span><span><span>                all_token_probs<span>.</span>append<span>([])</span>
</span></span><span><span>        <span>return</span> responses<span>,</span> all_token_probs
</span></span><span><span>    
</span></span><span><span>    <span>def</span> <span>compensate_temperature_effects</span><span>(</span><span>self</span><span>,</span> token_probs_list<span>,</span> temperatures<span>):</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Remove temperature bias to get model&#39;s true confidence patterns
</span></span></span><span><span><span>        
</span></span></span><span><span><span>        Mathematical foundation:
</span></span></span><span><span><span>        Forward: P(token_i) = exp(logit_i / τ) / Σ_j exp(logit_j / τ)
</span></span></span><span><span><span>        Reverse: logit_i = τ * log(P(token_i)) + constant
</span></span></span><span><span><span>        Note: The constant is ignored here because only relative confidence is used, not the absolute logit values.
</span></span></span><span><span><span>
</span></span></span><span><span><span>        Args:
</span></span></span><span><span><span>            token_probs_list: List of token probability sequences
</span></span></span><span><span><span>            temperatures: List of temperatures used
</span></span></span><span><span><span>            
</span></span></span><span><span><span>        Returns:
</span></span></span><span><span><span>            compensated_confidences: List of temperature-compensated confidence scores
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        compensated_sequences <span>=</span> <span>[]</span>
</span></span><span><span>        <span>for</span> token_probs<span>,</span> temp <span>in</span> <span>zip</span><span>(</span>token_probs_list<span>,</span> temperatures<span>):</span>
</span></span><span><span>            <span>if</span> token_probs <span>is</span> <span>None</span> <span>or</span> <span>len</span><span>(</span>token_probs<span>)</span> <span>==</span> <span>0</span><span>:</span>
</span></span><span><span>                compensated_sequences<span>.</span>append<span>(</span><span>-</span><span>10.0</span><span>)</span>
</span></span><span><span>                <span>continue</span>
</span></span><span><span>            sequence_confidence <span>=</span> <span>[]</span>
</span></span><span><span>            <span>for</span> prob <span>in</span> token_probs<span>:</span>
</span></span><span><span>                <span>if</span> prob <span>&gt;</span> <span>1e-10</span><span>:</span>
</span></span><span><span>                    relative_logit <span>=</span> temp <span>*</span> np<span>.</span>log<span>(</span>prob<span>)</span>
</span></span><span><span>                    sequence_confidence<span>.</span>append<span>(</span>relative_logit<span>)</span>
</span></span><span><span>                <span>else</span><span>:</span>
</span></span><span><span>                    sequence_confidence<span>.</span>append<span>(</span><span>-</span><span>100</span><span>)</span>
</span></span><span><span>            <span>if</span> sequence_confidence<span>:</span>
</span></span><span><span>                avg_confidence <span>=</span> np<span>.</span>mean<span>(</span>sequence_confidence<span>)</span>
</span></span><span><span>            <span>else</span><span>:</span>
</span></span><span><span>                avg_confidence <span>=</span> <span>-</span><span>10.0</span>
</span></span><span><span>            compensated_sequences<span>.</span>append<span>(</span>avg_confidence<span>)</span>
</span></span><span><span>        <span>return</span> compensated_sequences
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>prompt_wrapper</span><span>(</span><span>self</span><span>,</span> question<span>):</span>
</span></span><span><span>        system_prompt <span>=</span> <span>(</span>
</span></span><span><span>            <span>&#34;You are a helpful and accurate assistant. &#34;</span>
</span></span><span><span>            <span>&#34;Answer the following question as concisely and factually as possible. &#34;</span>
</span></span><span><span>            <span>&#34;The question is:</span><span>\n</span><span>&#34;</span>
</span></span><span><span>            <span>&#34;</span><span>{question}</span><span>\n</span><span>&#34;</span>
</span></span><span><span>        <span>)</span>
</span></span><span><span>        <span>return</span> system_prompt<span>.</span>format<span>(</span>question<span>=</span>question<span>)</span>
</span></span><span><span>    
</span></span><span><span>    <span>def</span> <span>measure_semantic_similarity</span><span>(</span><span>self</span><span>,</span> responses<span>):</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Calculate semantic similarity between all pairs of responses using a cross-encoder.
</span></span></span><span><span><span>        For each unique pair of valid responses, the cross-encoder predicts a similarity score (typically in [0, 1]).
</span></span></span><span><span><span>        Returns:
</span></span></span><span><span><span>            similarities: List of pairwise similarity scores
</span></span></span><span><span><span>            avg_similarity: Average similarity across all pairs
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        valid_responses <span>=</span> <span>[</span>r <span>for</span> r <span>in</span> responses <span>if</span> r<span>.</span>strip<span>()]</span>
</span></span><span><span>        <span>if</span> <span>len</span><span>(</span>valid_responses<span>)</span> <span>&lt;</span> <span>2</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>[</span><span>1.0</span><span>],</span> <span>1.0</span>
</span></span><span><span>        <span>try</span><span>:</span>
</span></span><span><span>            pairs <span>=</span> <span>[]</span>
</span></span><span><span>            <span>for</span> i <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span>valid_responses<span>)):</span>
</span></span><span><span>                <span>for</span> j <span>in</span> <span>range</span><span>(</span>i <span>+</span> <span>1</span><span>,</span> <span>len</span><span>(</span>valid_responses<span>)):</span>
</span></span><span><span>                    pairs<span>.</span>append<span>((</span>valid_responses<span>[</span>i<span>],</span> valid_responses<span>[</span>j<span>]))</span>
</span></span><span><span>            scores <span>=</span> <span>self</span><span>.</span>cross_encoder<span>.</span>predict<span>(</span>pairs<span>)</span>
</span></span><span><span>            similarities <span>=</span> <span>list</span><span>(</span>scores<span>)</span>
</span></span><span><span>            avg_similarity <span>=</span> np<span>.</span>mean<span>(</span>similarities<span>)</span> <span>if</span> <span>len</span><span>(</span>similarities<span>)</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>1.0</span>
</span></span><span><span>            <span>return</span> similarities<span>,</span> avg_similarity
</span></span><span><span>        <span>except</span> <span>Exception</span> <span>as</span> e<span>:</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Error calculating semantic similarity: </span><span>{</span>e<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>return</span> <span>[</span><span>0.5</span><span>],</span> <span>0.5</span>
</span></span><span><span>
</span></span><span><span>    <span># Store expected similarities for each temperature (to be set empirically)</span>
</span></span><span><span>    expected_similarity_by_temp <span>=</span> <span>{}</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>ponder</span><span>(</span><span>self</span><span>,</span> question<span>,</span> num_samples<span>=</span>GENERATION_ITERATIONS<span>,</span> max_new_tokens<span>=</span><span>100</span><span>,</span> temperature<span>=</span>DEFAULT_TEMPERATURE<span>,</span>
</span></span><span><span>                           similarity_threshold<span>=</span>SIMILARITY_THRESHOLD_CORRECT<span>,</span> confidence_threshold<span>=</span>DEFAULT_CONFIDENCE_THRESHOLD<span>):</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        This method measures model confidence and response consistency, not factuality.
</span></span></span><span><span><span>        It is best interpreted as a measure of how certain and consistent the model is in its answers, not whether those answers are true.
</span></span></span><span><span><span>        Core algorithm:
</span></span></span><span><span><span>        1. Generate multiple responses with the same temperature
</span></span></span><span><span><span>        2. Extract token probabilities and compensate for temperature effects  
</span></span></span><span><span><span>           (We extract per-token probabilities for each generated response using generate_multiple_responses(),
</span></span></span><span><span><span>            then pass these to compensate_temperature_effects() to reverse the temperature scaling and obtain a temperature-independent confidence score for each response.)
</span></span></span><span><span><span>        3. Measure semantic similarity between responses
</span></span></span><span><span><span>        4. Classify based on similarity and confidence thresholds
</span></span></span><span><span><span>        5. Also report an adjusted difference score (semantic similarity minus temperature)
</span></span></span><span><span><span>        6. Also report a temperature-normalized similarity score: (Observed Similarity / Expected Similarity at T)
</span></span></span><span><span><span>        Args:
</span></span></span><span><span><span>            question: Input question/prompt
</span></span></span><span><span><span>            num_samples: Number of responses to generate
</span></span></span><span><span><span>            max_new_tokens: Maximum tokens per response
</span></span></span><span><span><span>            similarity_threshold: Threshold for semantic similarity
</span></span></span><span><span><span>            confidence_threshold: Threshold for confidence score
</span></span></span><span><span><span>        Returns:
</span></span></span><span><span><span>            result: Dictionary with detection results
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        <span>self</span><span>.</span>log_section<span>(</span><span>f</span><span>&#34;Testing: </span><span>{</span>question<span>}</span><span>&#34;</span><span>)</span>
</span></span><span><span>        <span>try</span><span>:</span>
</span></span><span><span>            responses<span>,</span> token_probs <span>=</span> <span>self</span><span>.</span>generate_multiple_responses<span>(</span>
</span></span><span><span>                question<span>,</span> num_samples<span>,</span> max_new_tokens<span>,</span> temperature
</span></span><span><span>            <span>)</span>
</span></span><span><span>            temperatures <span>=</span> <span>[</span>temperature<span>]</span> <span>*</span> num_samples
</span></span><span><span>            confidences <span>=</span> <span>self</span><span>.</span>compensate_temperature_effects<span>(</span>token_probs<span>,</span> temperatures<span>)</span>
</span></span><span><span>            similarities<span>,</span> _ <span>=</span> <span>self</span><span>.</span>measure_semantic_similarity<span>(</span>responses<span>)</span>
</span></span><span><span>            avg_similarity <span>=</span> np<span>.</span>mean<span>(</span>similarities<span>)</span> <span>if</span> <span>len</span><span>(</span>similarities<span>)</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>1.0</span>
</span></span><span><span>            avg_confidence <span>=</span> np<span>.</span>mean<span>(</span>confidences<span>)</span> <span>if</span> <span>len</span><span>(</span>confidences<span>)</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>-</span><span>10.0</span>
</span></span><span><span>            adjusted_difference_score <span>=</span> avg_similarity <span>-</span> temperature
</span></span><span><span>            expected_sim <span>=</span> <span>self</span><span>.</span>get_expected_similarity<span>(</span>temperature<span>)</span>
</span></span><span><span>            normalized_similarity <span>=</span> avg_similarity <span>/</span> expected_sim <span>if</span> expected_sim <span>&gt;</span> <span>0</span> <span>else</span> <span>0.0</span>
</span></span><span><span>            <span>if</span> avg_similarity <span>&lt;</span> similarity_threshold<span>:</span>
</span></span><span><span>                <span>if</span> avg_confidence <span>&lt;</span> confidence_threshold<span>:</span>
</span></span><span><span>                    detection_result <span>=</span> DetectionResult<span>.</span>LOW_CONFIDENCE
</span></span><span><span>                    risk_level <span>=</span> RiskLevel<span>.</span>HIGH
</span></span><span><span>                <span>else</span><span>:</span>
</span></span><span><span>                    detection_result <span>=</span> DetectionResult<span>.</span>UNCERTAIN
</span></span><span><span>                    risk_level <span>=</span> RiskLevel<span>.</span>MEDIUM
</span></span><span><span>            <span>else</span><span>:</span>
</span></span><span><span>                detection_result <span>=</span> DetectionResult<span>.</span>CONFIDENT
</span></span><span><span>                risk_level <span>=</span> RiskLevel<span>.</span>LOW
</span></span><span><span>            result <span>=</span> <span>{</span>
</span></span><span><span>                <span>&#39;question&#39;</span><span>:</span> question<span>,</span>
</span></span><span><span>                <span>&#39;detection_result&#39;</span><span>:</span> detection_result<span>,</span>
</span></span><span><span>                <span>&#39;risk_level&#39;</span><span>:</span> risk_level<span>,</span>
</span></span><span><span>                <span>&#39;avg_similarity&#39;</span><span>:</span> avg_similarity<span>,</span>
</span></span><span><span>                <span>&#39;avg_confidence&#39;</span><span>:</span> avg_confidence<span>,</span>
</span></span><span><span>                <span>&#39;adjusted_difference_score&#39;</span><span>:</span> adjusted_difference_score<span>,</span>
</span></span><span><span>                <span>&#39;normalized_similarity&#39;</span><span>:</span> normalized_similarity<span>,</span>
</span></span><span><span>                <span>&#39;expected_similarity&#39;</span><span>:</span> expected_sim<span>,</span>
</span></span><span><span>                <span>&#39;individual_similarities&#39;</span><span>:</span> similarities<span>,</span>
</span></span><span><span>                <span>&#39;individual_confidences&#39;</span><span>:</span> confidences<span>,</span>
</span></span><span><span>                <span>&#39;responses&#39;</span><span>:</span> responses<span>,</span>
</span></span><span><span>                <span>&#39;temperatures&#39;</span><span>:</span> temperatures<span>,</span>
</span></span><span><span>                <span>&#39;temperature&#39;</span><span>:</span> temperature<span>,</span>
</span></span><span><span>                <span>&#39;success&#39;</span><span>:</span> <span>True</span>
</span></span><span><span>            <span>}</span>
</span></span><span><span>        <span>except</span> <span>Exception</span> <span>as</span> e<span>:</span>
</span></span><span><span>            <span>print</span><span>(</span><span>f</span><span>&#34;!! Error during detection: </span><span>{</span>e<span>}</span><span>&#34;</span><span>)</span>
</span></span><span><span>            <span>if</span> <span>&#34;truth value of an array with more than one element is ambiguous&#34;</span> <span>in</span> <span>str</span><span>(</span>e<span>):</span>
</span></span><span><span>                <span>try</span><span>:</span>
</span></span><span><span>                    <span>print</span><span>(</span><span>&#34;[DEBUG] similarities type:&#34;</span><span>,</span> <span>type</span><span>(</span>similarities<span>))</span>
</span></span><span><span>                    <span>print</span><span>(</span><span>&#34;[DEBUG] similarities value:&#34;</span><span>,</span> similarities<span>)</span>
</span></span><span><span>                <span>except</span> <span>Exception</span> <span>as</span> e2<span>:</span>
</span></span><span><span>                    <span>print</span><span>(</span><span>&#34;[DEBUG] similarities not available:&#34;</span><span>,</span> e2<span>)</span>
</span></span><span><span>                <span>try</span><span>:</span>
</span></span><span><span>                    <span>print</span><span>(</span><span>&#34;[DEBUG] confidences type:&#34;</span><span>,</span> <span>type</span><span>(</span>confidences<span>))</span>
</span></span><span><span>                    <span>print</span><span>(</span><span>&#34;[DEBUG] confidences value:&#34;</span><span>,</span> confidences<span>)</span>
</span></span><span><span>                <span>except</span> <span>Exception</span> <span>as</span> e2<span>:</span>
</span></span><span><span>                    <span>print</span><span>(</span><span>&#34;[DEBUG] confidences not available:&#34;</span><span>,</span> e2<span>)</span>
</span></span><span><span>            result <span>=</span> <span>{</span>
</span></span><span><span>                <span>&#39;question&#39;</span><span>:</span> question<span>,</span>
</span></span><span><span>                <span>&#39;detection_result&#39;</span><span>:</span> DetectionResult<span>.</span>ERROR<span>,</span>
</span></span><span><span>                <span>&#39;risk_level&#39;</span><span>:</span> RiskLevel<span>.</span>UNKNOWN<span>,</span>
</span></span><span><span>                <span>&#39;avg_similarity&#39;</span><span>:</span> <span>0.0</span><span>,</span>
</span></span><span><span>                <span>&#39;avg_confidence&#39;</span><span>:</span> <span>0.0</span><span>,</span>
</span></span><span><span>                <span>&#39;adjusted_difference_score&#39;</span><span>:</span> <span>0.0</span><span>,</span>
</span></span><span><span>                <span>&#39;normalized_similarity&#39;</span><span>:</span> <span>0.0</span><span>,</span>
</span></span><span><span>                <span>&#39;expected_similarity&#39;</span><span>:</span> <span>1.0</span><span>,</span>
</span></span><span><span>                <span>&#39;individual_similarities&#39;</span><span>:</span> <span>[],</span>
</span></span><span><span>                <span>&#39;individual_confidences&#39;</span><span>:</span> <span>[],</span>
</span></span><span><span>                <span>&#39;responses&#39;</span><span>:</span> <span>[],</span>
</span></span><span><span>                <span>&#39;temperatures&#39;</span><span>:</span> <span>[</span>temperature<span>]</span> <span>*</span> num_samples<span>,</span>
</span></span><span><span>                <span>&#39;temperature&#39;</span><span>:</span> temperature<span>,</span>
</span></span><span><span>                <span>&#39;success&#39;</span><span>:</span> <span>False</span><span>,</span>
</span></span><span><span>                <span>&#39;error&#39;</span><span>:</span> <span>str</span><span>(</span>e<span>)</span>
</span></span><span><span>            <span>}</span>
</span></span><span><span>        <span>self</span><span>.</span>_print_results<span>(</span>result<span>)</span>
</span></span><span><span>        <span>return</span> result
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>log_section</span><span>(</span><span>self</span><span>,</span> title<span>):</span>
</span></span><span><span>        <span>if</span> <span>self</span><span>.</span>verbosity <span>&gt;</span> <span>0</span><span>:</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>{</span><span>&#39;=&#39;</span><span>*</span><span>60</span><span>}</span><span>\n</span><span>{</span>title<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>if</span> <span>self</span><span>.</span>verbosity <span>==</span> <span>2</span><span>:</span>
</span></span><span><span>                <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>{</span><span>&#39;=&#39;</span><span>*</span><span>60</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>_progress_bar</span><span>(</span><span>self</span><span>,</span> total<span>,</span> desc<span>=</span><span>&#34;thinking...&#34;</span><span>):</span>
</span></span><span><span>        <span>if</span> <span>self</span><span>.</span>verbosity <span>==</span> <span>1</span><span>:</span>
</span></span><span><span>            <span>for</span> _ <span>in</span> tqdm<span>(</span><span>range</span><span>(</span>total<span>),</span> desc<span>=</span>desc<span>,</span> ncols<span>=</span><span>70</span><span>,</span> bar_format<span>=</span><span>&#39;</span><span>{l_bar}{bar}</span><span>| </span><span>{n_fmt}</span><span>/</span><span>{total_fmt}</span><span>&#39;</span><span>):</span>
</span></span><span><span>                time<span>.</span>sleep<span>(</span><span>0.1</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>_print_results</span><span>(</span><span>self</span><span>,</span> result<span>):</span>
</span></span><span><span>        <span>if</span> <span>not</span> result<span>[</span><span>&#39;success&#39;</span><span>]:</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\u2717</span><span> Detection failed: </span><span>{</span>result<span>.</span>get<span>(</span><span>&#39;error&#39;</span><span>,</span> <span>&#39;Unknown error&#39;</span><span>)</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>return</span>
</span></span><span><span>        <span>if</span> <span>self</span><span>.</span>verbosity <span>==</span> <span>2</span><span>:</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Results (Model Consistency/Confidence, not Factuality):&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Detection: </span><span>{</span>result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span><span>.</span>value<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Risk Level: </span><span>{</span>result<span>[</span><span>&#39;risk_level&#39;</span><span>]</span><span>.</span>value<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Avg Semantic Similarity: </span><span>{</span>result<span>[</span><span>&#39;avg_similarity&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Avg Model Confidence: </span><span>{</span>result<span>[</span><span>&#39;avg_confidence&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Adjusted Difference Score: </span><span>{</span>result<span>[</span><span>&#39;adjusted_difference_score&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Temperature-Normalized Similarity: </span><span>{</span>result<span>[</span><span>&#39;normalized_similarity&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>if</span> result<span>[</span><span>&#39;individual_similarities&#39;</span><span>]:</span>
</span></span><span><span>                <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Individual Similarities: </span><span>{</span><span>[</span><span>f</span><span>&#39;</span><span>{</span>s<span>:</span><span>.3f</span><span>}</span><span>&#39;</span> <span>for</span> s <span>in</span> result<span>[</span><span>&#39;individual_similarities&#39;</span><span>]]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>if</span> result<span>[</span><span>&#39;individual_confidences&#39;</span><span>]:</span>
</span></span><span><span>                <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  Individual Confidences: </span><span>{</span><span>[</span><span>f</span><span>&#39;</span><span>{</span>c<span>:</span><span>.3f</span><span>}</span><span>&#39;</span> <span>for</span> c <span>in</span> result<span>[</span><span>&#39;individual_confidences&#39;</span><span>]]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Generated Responses:&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>            <span>for</span> i<span>,</span> <span>(</span>resp<span>,</span> temp<span>,</span> conf<span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>zip</span><span>(</span>result<span>[</span><span>&#39;responses&#39;</span><span>],</span> result<span>[</span><span>&#39;temperatures&#39;</span><span>],</span> result<span>[</span><span>&#39;individual_confidences&#39;</span><span>])):</span>
</span></span><span><span>                <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  </span><span>{</span>i<span>+</span><span>1</span><span>}</span><span>. (temp=</span><span>{</span>temp<span>:</span><span>.2f</span><span>}</span><span>, conf=</span><span>{</span>conf<span>:</span><span>.3f</span><span>}</span><span>): </span><span>{</span>resp<span>[:</span><span>80</span><span>]</span><span>}</span><span>...&#34;</span><span>,</span> level<span>=</span><span>2</span><span>)</span>
</span></span><span><span>        <span>elif</span> <span>self</span><span>.</span>verbosity <span>==</span> <span>1</span><span>:</span>
</span></span><span><span>            <span>self</span><span>.</span>_progress_bar<span>(</span>GENERATION_ITERATIONS<span>)</span>
</span></span><span><span>        final_answer <span>=</span> <span>self</span><span>.</span>_final_answer<span>(</span>result<span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Final Answer: </span><span>{</span>final_answer<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>if</span> <span>self</span><span>.</span>verbosity <span>==</span> <span>1</span><span>:</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>{</span><span>&#39;=&#39;</span><span>*</span><span>60</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>_final_answer</span><span>(</span><span>self</span><span>,</span> result<span>):</span>
</span></span><span><span>        similarity <span>=</span> result<span>[</span><span>&#39;avg_similarity&#39;</span><span>]</span>
</span></span><span><span>        <span>try</span><span>:</span>
</span></span><span><span>            best_answer<span>,</span> _ <span>=</span> <span>self</span><span>.</span>select_consensus_response_kmeans<span>(</span>result<span>[</span><span>&#39;responses&#39;</span><span>],</span> k<span>=</span><span>2</span><span>)</span>
</span></span><span><span>        <span>except</span> <span>Exception</span> <span>as</span> e<span>:</span>
</span></span><span><span>            best_answer <span>=</span> <span>f</span><span>&#34;(Error selecting consensus answer: </span><span>{</span>e<span>}</span><span>)&#34;</span>
</span></span><span><span>        <span>if</span> result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>LOW_CONFIDENCE<span>:</span>
</span></span><span><span>            <span>return</span> <span>&#34;I&#39;m not confident in this answer.&#34;</span>
</span></span><span><span>        <span>elif</span> similarity <span>&gt;</span> <span>0.95</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>f</span><span>&#34;I&#39;m very confident the answer is: </span><span>{</span>best_answer<span>}</span><span>&#34;</span>
</span></span><span><span>        <span>elif</span> similarity <span>&gt;</span> <span>0.85</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>f</span><span>&#34;I think the answer is: </span><span>{</span>best_answer<span>}</span><span>&#34;</span>
</span></span><span><span>        <span>elif</span> similarity <span>&gt;</span> <span>0.7</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>f</span><span>&#34;I&#39;m fairly sure, but please check: </span><span>{</span>best_answer<span>}</span><span>&#34;</span>
</span></span><span><span>        <span>elif</span> similarity <span>&gt;</span> <span>0.5</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>f</span><span>&#34;I&#39;m uncertain about this, so make sure to check, but I think the answer is: </span><span>{</span>best_answer<span>}</span><span>&#34;</span>
</span></span><span><span>        <span>else</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>&#34;I don&#39;t know the answer to this.&#34;</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>batch_test</span><span>(</span><span>self</span><span>,</span> questions<span>,</span> <span>**</span>kwargs<span>):</span>
</span></span><span><span>        results <span>=</span> <span>[]</span>
</span></span><span><span>        <span>for</span> question <span>in</span> questions<span>:</span>
</span></span><span><span>            result <span>=</span> <span>self</span><span>.</span>ponder<span>(</span>question<span>,</span> <span>**</span>kwargs<span>)</span>
</span></span><span><span>            results<span>.</span>append<span>(</span>result<span>)</span>
</span></span><span><span>        
</span></span><span><span>        <span>self</span><span>.</span>log_section<span>(</span><span>&#34;BATCH TEST SUMMARY&#34;</span><span>)</span>
</span></span><span><span>        
</span></span><span><span>        successful_results <span>=</span> <span>[</span>r <span>for</span> r <span>in</span> results <span>if</span> r<span>[</span><span>&#39;success&#39;</span><span>]]</span>
</span></span><span><span>        
</span></span><span><span>        <span>for</span> i<span>,</span> result <span>in</span> <span>enumerate</span><span>(</span>results<span>):</span>
</span></span><span><span>            status <span>=</span> <span>&#34;</span><span>\u2713</span><span>&#34;</span> <span>if</span> result<span>[</span><span>&#39;success&#39;</span><span>]</span> <span>else</span> <span>&#34;</span><span>\u2717</span><span>&#34;</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>{</span>status<span>}</span><span> </span><span>{</span>i<span>+</span><span>1</span><span>}</span><span>. </span><span>{</span>result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span><span>.</span>value<span>}</span><span> - </span><span>{</span>result<span>[</span><span>&#39;question&#39;</span><span>][:</span>TRUNCATION_LENGTH<span>]</span><span>}</span><span>...&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        
</span></span><span><span>        <span>if</span> successful_results<span>:</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Success rate: </span><span>{</span><span>len</span><span>(</span>successful_results<span>)</span><span>}</span><span>/</span><span>{</span><span>len</span><span>(</span>results<span>)</span><span>}</span><span> (</span><span>{</span><span>len</span><span>(</span>successful_results<span>)</span><span>/</span><span>len</span><span>(</span>results<span>)</span><span>*</span><span>100</span><span>:</span><span>.1f</span><span>}</span><span>%)&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        
</span></span><span><span>        <span>return</span> results
</span></span><span><span>    
</span></span><span><span>    <span>def</span> <span>cleanup</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>        <span>if</span> <span>hasattr</span><span>(</span><span>self</span><span>,</span> <span>&#39;model&#39;</span><span>):</span>
</span></span><span><span>            <span>del</span> <span>self</span><span>.</span>model
</span></span><span><span>        <span>if</span> <span>hasattr</span><span>(</span><span>self</span><span>,</span> <span>&#39;tokenizer&#39;</span><span>):</span>
</span></span><span><span>            <span>del</span> <span>self</span><span>.</span>tokenizer
</span></span><span><span>        <span>if</span> <span>hasattr</span><span>(</span><span>self</span><span>,</span> <span>&#39;cross_encoder&#39;</span><span>):</span>
</span></span><span><span>            <span>del</span> <span>self</span><span>.</span>cross_encoder
</span></span><span><span>        
</span></span><span><span>        <span>if</span> torch<span>.</span>cuda<span>.</span>is_available<span>():</span>
</span></span><span><span>            torch<span>.</span>cuda<span>.</span>empty_cache<span>()</span>
</span></span><span><span>        
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>&#34;✓ Resources cleaned up&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>pretty_print_result</span><span>(</span><span>self</span><span>,</span> result<span>):</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>{</span><span>&#39;-&#39;</span><span>*</span><span>60</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Question: </span><span>{</span>result<span>[</span><span>&#39;question&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Detection: </span><span>{</span>result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span><span>.</span>value <span>if</span> <span>hasattr</span><span>(</span>result<span>[</span><span>&#39;detection_result&#39;</span><span>],</span> <span>&#39;value&#39;</span><span>)</span> <span>else</span> result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Risk Level: </span><span>{</span>result<span>[</span><span>&#39;risk_level&#39;</span><span>]</span><span>.</span>value <span>if</span> <span>hasattr</span><span>(</span>result<span>[</span><span>&#39;risk_level&#39;</span><span>],</span> <span>&#39;value&#39;</span><span>)</span> <span>else</span> result<span>[</span><span>&#39;risk_level&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Avg Similarity: </span><span>{</span>result<span>[</span><span>&#39;avg_similarity&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Avg Confidence: </span><span>{</span>result<span>[</span><span>&#39;avg_confidence&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>if</span> result<span>.</span>get<span>(</span><span>&#39;temperature&#39;</span><span>):</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Temperature: </span><span>{</span>result<span>[</span><span>&#39;temperature&#39;</span><span>]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>if</span> result<span>.</span>get<span>(</span><span>&#39;individual_similarities&#39;</span><span>):</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Individual Similarities: </span><span>{</span><span>[</span><span>f</span><span>&#39;</span><span>{</span>s<span>:</span><span>.3f</span><span>}</span><span>&#39;</span> <span>for</span> s <span>in</span> result<span>[</span><span>&#39;individual_similarities&#39;</span><span>]]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>if</span> result<span>.</span>get<span>(</span><span>&#39;individual_confidences&#39;</span><span>):</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Individual Confidences: </span><span>{</span><span>[</span><span>f</span><span>&#39;</span><span>{</span>c<span>:</span><span>.3f</span><span>}</span><span>&#39;</span> <span>for</span> c <span>in</span> result<span>[</span><span>&#39;individual_confidences&#39;</span><span>]]</span><span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;Responses:&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>for</span> i<span>,</span> resp <span>in</span> <span>enumerate</span><span>(</span>result<span>.</span>get<span>(</span><span>&#39;responses&#39;</span><span>,</span> <span>[])):</span>
</span></span><span><span>            temp <span>=</span> result<span>[</span><span>&#39;temperatures&#39;</span><span>][</span>i<span>]</span> <span>if</span> <span>&#39;temperatures&#39;</span> <span>in</span> result <span>and</span> i <span>&lt;</span> <span>len</span><span>(</span>result<span>[</span><span>&#39;temperatures&#39;</span><span>])</span> <span>else</span> <span>None</span>
</span></span><span><span>            conf <span>=</span> result<span>[</span><span>&#39;individual_confidences&#39;</span><span>][</span>i<span>]</span> <span>if</span> <span>&#39;individual_confidences&#39;</span> <span>in</span> result <span>and</span> i <span>&lt;</span> <span>len</span><span>(</span>result<span>[</span><span>&#39;individual_confidences&#39;</span><span>])</span> <span>else</span> <span>None</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;  </span><span>{</span>i<span>+</span><span>1</span><span>}</span><span>. (temp=</span><span>{</span>temp <span>if</span> temp <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>&#39;?&#39;</span><span>}</span><span> conf=</span><span>{</span>conf <span>if</span> conf <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>&#39;?&#39;</span><span>}</span><span>)&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;     </span><span>{</span>resp<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>{</span><span>&#39;-&#39;</span><span>*</span><span>60</span><span>}</span><span>\n</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>select_consensus_response_kmeans</span><span>(</span><span>self</span><span>,</span> responses<span>,</span> k<span>=</span><span>2</span><span>):</span>
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        Cluster responses using K-means and select the response closest to the centroid
</span></span></span><span><span><span>        of the largest cluster (consensus).
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        <span>if</span> <span>len</span><span>(</span>responses<span>)</span> <span>==</span> <span>0</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>&#34;&#34;</span><span>,</span> <span>[]</span>
</span></span><span><span>        <span>if</span> <span>len</span><span>(</span>responses<span>)</span> <span>==</span> <span>1</span><span>:</span>
</span></span><span><span>            <span>return</span> responses<span>[</span><span>0</span><span>],</span> responses
</span></span><span><span>
</span></span><span><span>        <span># Embed responses</span>
</span></span><span><span>        embeddings <span>=</span> <span>self</span><span>.</span>similarity_model<span>.</span>encode<span>(</span>responses<span>)</span>
</span></span><span><span>        <span># Run K-means</span>
</span></span><span><span>        k <span>=</span> <span>min</span><span>(</span>k<span>,</span> <span>len</span><span>(</span>responses<span>))</span>  <span># can&#39;t have more clusters than responses</span>
</span></span><span><span>        kmeans <span>=</span> KMeans<span>(</span>n_clusters<span>=</span>k<span>,</span> random_state<span>=</span><span>42</span><span>,</span> n_init<span>=</span><span>&#39;auto&#39;</span><span>)</span>
</span></span><span><span>        labels <span>=</span> kmeans<span>.</span>fit_predict<span>(</span>embeddings<span>)</span>
</span></span><span><span>        <span># Find the largest cluster</span>
</span></span><span><span>        unique<span>,</span> counts <span>=</span> np<span>.</span>unique<span>(</span>labels<span>,</span> return_counts<span>=</span><span>True</span><span>)</span>
</span></span><span><span>        largest_cluster <span>=</span> unique<span>[</span>np<span>.</span>argmax<span>(</span>counts<span>)]</span>
</span></span><span><span>        cluster_indices <span>=</span> <span>[</span>i <span>for</span> i<span>,</span> label <span>in</span> <span>enumerate</span><span>(</span>labels<span>)</span> <span>if</span> label <span>==</span> largest_cluster<span>]</span>
</span></span><span><span>        cluster_embeddings <span>=</span> embeddings<span>[</span>cluster_indices<span>]</span>
</span></span><span><span>        cluster_responses <span>=</span> <span>[</span>responses<span>[</span>i<span>]</span> <span>for</span> i <span>in</span> cluster_indices<span>]</span>
</span></span><span><span>        <span># Find the response closest to the centroid</span>
</span></span><span><span>        centroid <span>=</span> kmeans<span>.</span>cluster_centers_<span>[</span>largest_cluster<span>]</span>
</span></span><span><span>        dists <span>=</span> np<span>.</span>linalg<span>.</span>norm<span>(</span>cluster_embeddings <span>-</span> centroid<span>,</span> axis<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        best_idx <span>=</span> np<span>.</span>argmin<span>(</span>dists<span>)</span>
</span></span><span><span>        best_response <span>=</span> cluster_responses<span>[</span>best_idx<span>]</span>
</span></span><span><span>        <span>return</span> best_response<span>,</span> cluster_responses
</span></span><span><span>
</span></span><span><span><span>def</span> <span>get_token_probs</span><span>(</span>model<span>,</span> tokenizer<span>,</span> prompt<span>,</span> generated_ids<span>):</span>
</span></span><span><span>    <span># prompt: str, generated_ids: tensor of token ids (the generated answer)</span>
</span></span><span><span>    input_ids <span>=</span> tokenizer<span>(</span>prompt<span>,</span> return_tensors<span>=</span><span>&#34;pt&#34;</span><span>)</span><span>.</span>input_ids
</span></span><span><span>    <span># prepare decoder input ids (shifted right)</span>
</span></span><span><span>    decoder_input_ids <span>=</span> generated_ids<span>[:,</span> <span>:</span><span>-</span><span>1</span><span>]</span>
</span></span><span><span>    labels <span>=</span> generated_ids<span>[:,</span> <span>1</span><span>:]</span>
</span></span><span><span>
</span></span><span><span>    <span>with</span> torch<span>.</span>no_grad<span>():</span>
</span></span><span><span>        outputs <span>=</span> model<span>(</span>
</span></span><span><span>            input_ids<span>=</span>input_ids<span>,</span>
</span></span><span><span>            decoder_input_ids<span>=</span>decoder_input_ids<span>,</span>
</span></span><span><span>            output_logits<span>=</span><span>True</span><span>,</span>  <span># or output_hidden_states=True</span>
</span></span><span><span>        <span>)</span>
</span></span><span><span>        logits <span>=</span> outputs<span>.</span>logits  <span># shape: [1, seq_len, vocab_size]</span>
</span></span><span><span>        probs <span>=</span> torch<span>.</span>softmax<span>(</span>logits<span>,</span> dim<span>=-</span><span>1</span><span>)</span>
</span></span><span><span>        <span># for each position, get the probability of the actual next token</span>
</span></span><span><span>        token_probs <span>=</span> <span>[]</span>
</span></span><span><span>        <span>for</span> i <span>in</span> <span>range</span><span>(</span>labels<span>.</span>shape<span>[</span><span>1</span><span>]):</span>
</span></span><span><span>            token_id <span>=</span> labels<span>[</span><span>0</span><span>,</span> i<span>]</span><span>.</span>item<span>()</span>
</span></span><span><span>            prob <span>=</span> probs<span>[</span><span>0</span><span>,</span> i<span>,</span> token_id<span>]</span><span>.</span>item<span>()</span>
</span></span><span><span>            token_probs<span>.</span>append<span>(</span>prob<span>)</span>
</span></span><span><span>    <span>return</span> <span>list</span><span>(</span>token_probs<span>)</span>  <span># Ensure it&#39;s always a Python list</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>main</span><span>():</span>
</span></span><span><span>    
</span></span><span><span>    confidence_detector <span>=</span> <span>None</span>
</span></span><span><span>    <span>try</span><span>:</span>
</span></span><span><span>        <span>print</span><span>(</span><span>&#34;Initializing Self-Confidence...&#34;</span><span>)</span>
</span></span><span><span>        confidence_detector <span>=</span> SelfConfidenceDetector<span>(</span>
</span></span><span><span>            model_name<span>=</span><span>&#34;google/flan-t5-large&#34;</span><span>,</span>
</span></span><span><span>        <span>)</span>
</span></span><span><span>        confidence_detector<span>.</span>log_section<span>(</span><span>&#34;LLM SELF-CONFIDENCE&#34;</span><span>)</span>
</span></span><span><span>        
</span></span><span><span>        <span>#########################################################</span>
</span></span><span><span>        <span># these are just the test questions - a little mix of fact and fiction :)</span>
</span></span><span><span>
</span></span><span><span>        factual_questions <span>=</span> <span>[</span>
</span></span><span><span>            <span>&#34;What is the capital of France?&#34;</span><span>,</span>
</span></span><span><span>            <span>&#34;What is 2 + 2?&#34;</span><span>,</span>
</span></span><span><span>            <span>&#34;Who wrote the novel &#39;To Kill a Mockingbird&#39;?&#34;</span><span>,</span>
</span></span><span><span>            <span>&#34;What is the chemical formula for water?&#34;</span><span>,</span>
</span></span><span><span>        <span>]</span>
</span></span><span><span>
</span></span><span><span>        fictional_questions <span>=</span> <span>[</span>
</span></span><span><span>            <span>&#34;Tell me about the life of Dr. Zelinda Farthingbottom, the famous 19th-century astronomer.&#34;</span><span>,</span>
</span></span><span><span>            <span>&#34;What are the main ingredients in the traditional dish &#39;Flibbernaught Stew&#39; from medieval England?&#34;</span><span>,</span>
</span></span><span><span>            <span>&#34;Describe the plot of the movie &#39;The Quantum Paradox&#39; from 1987.&#34;</span><span>,</span>
</span></span><span><span>            <span>&#34;Explain the discovery of the element &#39;Fictonium&#39; by Dr. John Madeupname in the year 1337.&#34;</span>
</span></span><span><span>        <span>]</span>
</span></span><span><span>
</span></span><span><span>        test_questions <span>=</span> factual_questions <span>+</span> fictional_questions
</span></span><span><span>        random<span>.</span>shuffle<span>(</span>test_questions<span>)</span>
</span></span><span><span>
</span></span><span><span>        results <span>=</span> confidence_detector<span>.</span>batch_test<span>(</span>
</span></span><span><span>            test_questions<span>,</span>
</span></span><span><span>            num_samples<span>=</span>GENERATION_ITERATIONS<span>,</span> <span># increasing this seems to increase the accuracy of the hallucination detection</span>
</span></span><span><span>            max_new_tokens<span>=</span><span>100</span><span>,</span>
</span></span><span><span>            similarity_threshold<span>=</span>SIMILARITY_THRESHOLD_CORRECT<span>,</span>
</span></span><span><span>            confidence_threshold<span>=</span>DEFAULT_CONFIDENCE_THRESHOLD<span>,</span>
</span></span><span><span>            temperature<span>=</span>DEFAULT_TEMPERATURE
</span></span><span><span>        <span>)</span>
</span></span><span><span>        
</span></span><span><span>        <span>#########################################################</span>
</span></span><span><span>        <span># analyze results</span>
</span></span><span><span>        successful_results <span>=</span> <span>[</span>r <span>for</span> r <span>in</span> results <span>if</span> r<span>[</span><span>&#39;success&#39;</span><span>]]</span>
</span></span><span><span>        
</span></span><span><span>        <span># iterate and print out all the results if debug = true</span>
</span></span><span><span>        debug <span>=</span> <span>True</span>
</span></span><span><span>        <span>if</span> debug<span>:</span>
</span></span><span><span>            <span>for</span> result <span>in</span> results<span>:</span>
</span></span><span><span>                confidence_detector<span>.</span>pretty_print_result<span>(</span>result<span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span> successful_results<span>:</span>
</span></span><span><span>            confidence_detector<span>.</span>log_section<span>(</span><span>&#34;FINAL ANALYSIS&#34;</span><span>)</span>
</span></span><span><span>            confident_count <span>=</span> <span>sum</span><span>(</span><span>1</span> <span>for</span> r <span>in</span> successful_results 
</span></span><span><span>                                <span>if</span> r<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>CONFIDENT<span>)</span>
</span></span><span><span>            uncertain_count <span>=</span> <span>sum</span><span>(</span><span>1</span> <span>for</span> r <span>in</span> successful_results 
</span></span><span><span>                                <span>if</span> r<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>UNCERTAIN<span>)</span>
</span></span><span><span>            low_confidence_count <span>=</span> <span>sum</span><span>(</span><span>1</span> <span>for</span> r <span>in</span> successful_results 
</span></span><span><span>                                <span>if</span> r<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>LOW_CONFIDENCE<span>)</span>
</span></span><span><span>            error_count <span>=</span> <span>sum</span><span>(</span><span>1</span> <span>for</span> r <span>in</span> successful_results 
</span></span><span><span>                                <span>if</span> r<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>ERROR<span>)</span>
</span></span><span><span>            total <span>=</span> <span>len</span><span>(</span>successful_results<span>)</span>
</span></span><span><span>            
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;Total questions tested: </span><span>{</span>total<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;Confident responses: </span><span>{</span>confident_count<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;Uncertain responses: </span><span>{</span>uncertain_count<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;Low confidence responses: </span><span>{</span>low_confidence_count<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;Errors: </span><span>{</span>error_count<span>}</span><span>&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Confident answers:&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>for</span> result <span>in</span> successful_results<span>:</span>
</span></span><span><span>                <span>if</span> result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>CONFIDENT<span>:</span>
</span></span><span><span>                    confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;  </span><span>\u2705</span><span> </span><span>{</span>result<span>[</span><span>&#39;question&#39;</span><span>][:</span>TRUNCATION_LENGTH<span>]</span><span>}</span><span>... (sim: </span><span>{</span>result<span>[</span><span>&#39;avg_similarity&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>)&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>                    <span>break</span>
</span></span><span><span>            
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Uncertain answers:&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>for</span> result <span>in</span> successful_results<span>:</span>
</span></span><span><span>                <span>if</span> result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>UNCERTAIN<span>:</span>
</span></span><span><span>                    confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;  ? </span><span>{</span>result<span>[</span><span>&#39;question&#39;</span><span>][:</span>TRUNCATION_LENGTH<span>]</span><span>}</span><span>... (sim: </span><span>{</span>result<span>[</span><span>&#39;avg_similarity&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>)&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>                    <span>break</span>
</span></span><span><span>            
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Low confidence answers:&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>            <span>for</span> result <span>in</span> successful_results<span>:</span>
</span></span><span><span>                <span>if</span> result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>LOW_CONFIDENCE<span>:</span>
</span></span><span><span>                    confidence_detector<span>.</span>log<span>(</span><span>f</span><span>&#34;  ! </span><span>{</span>result<span>[</span><span>&#39;question&#39;</span><span>][:</span>TRUNCATION_LENGTH<span>]</span><span>}</span><span>... (sim: </span><span>{</span>result<span>[</span><span>&#39;avg_similarity&#39;</span><span>]</span><span>:</span><span>.3f</span><span>}</span><span>)&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>                    <span>break</span>
</span></span><span><span>        <span>else</span><span>:</span>
</span></span><span><span>            confidence_detector<span>.</span>log<span>(</span><span>&#34;</span><span>\n</span><span>✗ No successful results to analyze, something is horribly wrong!&#34;</span><span>,</span> level<span>=</span><span>1</span><span>)</span>
</span></span><span><span>        
</span></span><span><span>        <span># assert here that every fictional question is detected as an unknown, but</span>
</span></span><span><span>        <span># be careful to make sure that hallucinations of factual questions are counted as well!</span>
</span></span><span><span>        <span>for</span> result <span>in</span> results<span>:</span>
</span></span><span><span>            <span>if</span> result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>UNCERTAIN<span>:</span>
</span></span><span><span>                <span>assert</span><span>(</span>result<span>[</span><span>&#39;question&#39;</span><span>]</span> <span>in</span> fictional_questions<span>)</span>
</span></span><span><span>            <span>elif</span> result<span>[</span><span>&#39;detection_result&#39;</span><span>]</span> <span>==</span> DetectionResult<span>.</span>CONFIDENT<span>:</span>
</span></span><span><span>                <span>assert</span><span>(</span>result<span>[</span><span>&#39;question&#39;</span><span>]</span> <span>in</span> factual_questions<span>)</span>
</span></span><span><span>
</span></span><span><span>        results <span>=</span> <span>[]</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> confidence_detector<span>,</span> results
</span></span><span><span>        
</span></span><span><span>    <span>except</span> <span>KeyboardInterrupt</span><span>:</span>
</span></span><span><span>        <span>print</span><span>(</span><span>&#34;</span><span>\n\n</span><span>Demo interrupted by user.&#34;</span><span>)</span>
</span></span><span><span>        <span>return</span> confidence_detector<span>,</span> <span>[]</span>
</span></span><span><span>    <span>except</span> <span>Exception</span> <span>as</span> e<span>:</span>
</span></span><span><span>        <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>✗ Error in main: </span><span>{</span>e<span>}</span><span>&#34;</span><span>)</span>
</span></span><span><span>        <span>import</span> <span>traceback</span>
</span></span><span><span>        traceback<span>.</span>print_exc<span>()</span>
</span></span><span><span>        <span>return</span> confidence_detector<span>,</span> <span>[]</span>
</span></span><span><span>    
</span></span><span><span>    <span>finally</span><span>:</span>
</span></span><span><span>        <span>if</span> confidence_detector<span>:</span>
</span></span><span><span>            <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Cleaning up...&#34;</span><span>)</span>
</span></span><span><span>            confidence_detector<span>.</span>cleanup<span>()</span>
</span></span><span><span>
</span></span><span><span><span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span><span>:</span>
</span></span><span><span>    <span>print</span><span>(</span><span>&#34;Starting hallucination detection test...&#34;</span><span>)</span>
</span></span><span><span>    <span>try</span><span>:</span>
</span></span><span><span>        confidence_detector<span>,</span> results <span>=</span> main<span>()</span>
</span></span><span><span>        
</span></span><span><span>        <span># To test your own questions:</span>
</span></span><span><span>        <span># confidence_detector = SelfConfidenceDetector()</span>
</span></span><span><span>        <span># result = confidence_detector.ponder(&#39;Your question here&#39;)</span>
</span></span><span><span>
</span></span><span><span>        <span>if</span> confidence_detector <span>and</span> results<span>:</span>
</span></span><span><span>            <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>✅ Demo completed successfully!&#34;</span><span>)</span>
</span></span><span><span>        <span>else</span><span>:</span>
</span></span><span><span>            <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>❌ Demo encountered issues. Check error messages above.&#34;</span><span>)</span>
</span></span><span><span>            
</span></span><span><span>    <span>except</span> <span>Exception</span> <span>as</span> e<span>:</span>
</span></span><span><span>        <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>✗ Unexpected error: </span><span>{</span>e<span>}</span><span>&#34;</span><span>)</span>
</span></span><span><span>        <span>import</span> <span>traceback</span>
</span></span><span><span>        traceback<span>.</span>print_exc<span>()</span>
</span></span><span><span>    
</span></span><span><span>    <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Done!&#34;</span><span>)</span>
</span></span></code></pre></div></div></div>
  </body>
</html>
