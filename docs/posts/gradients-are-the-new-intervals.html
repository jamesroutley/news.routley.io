<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mattkeeter.com/blog/2025-05-14-gradients/">Original</a>
    <h1>Gradients Are the New Intervals</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
<!-- End header -->









<p>At the <a href="https://nesg.graphics/">New England Symposium on Graphics</a>,
<a href="https://jamestompkin.com/">James Tompkin</a> compared graphics researchers to
magpies: they&#39;re easily distracted by shiny objects and pretty renderings.</p>
<p>While this is true, the analogy also holds from a different angle: when I&#39;m
reading graphics papers, I&#39;m constantly looking for ideas to <del>steal</del>
bring back to my nest.</p>
<p>Researchers at <a href="https://www.irit.fr/en/home/">IRIT</a> and
<a href="https://research.adobe.com/">Adobe Research</a> recently published a
<a href="https://research.adobe.com/publication/lipschitz-pruning-hierarchical-simplification-of-primitive-based-sdfs/">paper</a>
that&#39;s <em>full</em> of interesting ideas, and I&#39;d like to talk about it.</p>
<p>
<a href="https://wbrbr.org/publications/LipschitzPruning/"><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/paper.png" alt="Picture of the paper, showing the abstract and title"/></a>
</p>
<p>This blog post assumes a vague understanding of implicit surface rasterization,
and how interval arithmetic is used to both skip regions of space and simplify
complex expressions.
See <a href="https://www.mattkeeter.com/projects/fidget#intervals">this section of the Fidget writeup</a> for a short
introduction, or
<a href="https://www.youtube.com/watch?v=UxGxsGnbyJ4">my colloquium talk</a>
for a longer explanation.</p>
<p>Here&#39;s the key sentence from the paper&#39;s abstract:</p>
<blockquote>
<p>We introduce an efficient hierarchical tree pruning method based on the
Lipschitz property of SDFs, which is compatible with hard and smooth CSG
operators.</p>
</blockquote>
<p>In this case, &#34;the Lipschitz property&#34; means that the gradient of the distance
value is bounded, e.g. $||\nabla f(x, y, z)|| \lt 1$.  You&#39;ll also see it
referred to as
<a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuity</a>.</p>
<p>Lipschitz continuity provides a <strong>bound</strong> on the maximum change in the distance
value between two points: if you have points $\vec{p}$ and $\vec{q}$ in 3D
space, then</p>
<p>$$|f(\vec{p}) - f(\vec{q})| \le ||\vec{p} - \vec{q}||$$</p>
<p>Given this requirement on the distance fields, the authors present two core
tricks that can be applied to CSG-tree-flavored models:</p>
<ul>
<li><strong>Pruning</strong> identifies primitives which are inactive within a particular
region of space, and builds a simplified shape with only active primitives</li>
<li><strong>Far-field culling</strong> finds primitives which are far from a particular
region (and therefore not relevant), and replaces them with a simpler
expression.</li>
</ul>
<p>These optimizations make rendering dramatically cheaper, allowing for
interactive visualizations of complex models.</p>
<p>The first optimization also sounds familiar – I presented something
similar <a href="https://www.mattkeeter.com/research/mpr/">back in 2020</a> (which
itself builds on <a href="https://fab.cba.mit.edu/classes/S62.12/docs/Duff_interval_CSG.pdf">work from 1992</a>):</p>
<blockquote>
<p>To render a model, its underlying expression is evaluated in a shallow
hierarchy of spatial regions, using a high branching factor for efficient
parallelization. Interval arithmetic is used to both skip empty regions and
construct reduced versions of the expression. The latter is the optimization
that makes our algorithm practical: in one benchmark, expression complexity
decreases by two orders of magnitude between the original and reduced
expressions.</p>
</blockquote>
<p>Here&#39;s the big difference: my implementation used
<a href="https://en.wikipedia.org/wiki/Interval_arithmetic">interval arithmetic</a>
on arbitrary math expressions, while this new paper uses single-point evaluation
on Lipschitz-continuous primitives.</p>
<p>Single-point evaluation is cheaper, and also doesn&#39;t suffer from the
conservative nature of interval arithmetic (where intervals tend to grow over
time).  However, their shapes are limited to a set of well-behaved primitives
and transforms; something as simple as scaling a model can break Lipschitz
continuity if not handled carefully.</p>
<p>After thinking about it for a few days, I ended up taking home a slightly
different conclusion than the paper presented:</p>
<p><em>If your distance field is Lipschitz-continuous, then you can use single-point
evaluation to get interval-ish results, then apply all the usual tricks which
normally require interval arithmetic.</em></p>
<p>I&#39;ve now gone far too long without showing a pretty picture (the magpies in the
audience are growing rowdy), so let&#39;s dig into some examples.  I&#39;m going to
take this idea — of using single-point evaluations to get pseudo-intervals — and
see how it applies to my usual box of tools.</p>
<p>Note that I&#39;m not hewing to the implementation from the paper (which you should
go read, it&#39;s great!); I&#39;m taking this idea and running in a slightly different
direction based on my background knowledge.</p>
<hr/>
<h3>Basic rendering</h3>
<p>Here&#39;s a circle of radius 1, $f(x, y) = \sqrt{x^2 + y^2} - 1$:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle.png" alt="Picture of a circle with field lines shown"/></p>
<p>It&#39;s rendered with orange / blue indicating whether we&#39;re inside or outside the
shape, and field lines showing the distance field values.  I&#39;m not sure who
developed the original coloring strategy, but it&#39;s pervasive on Shadertoy; I
borrowed it from <a href="https://www.shadertoy.com/view/t3X3z4">Inigo Quilez</a>.</p>
<p>My typical strategy for rendering is to do several rounds of evaluation using
interval arithmetic, then pixel-by-pixel evaluation of the remaining ambiguous
regions.  The goal is to produce a binary image: each pixel is either inside or
outside, depending on sign.</p>
<p>Interval evaluation takes intervals for $x$ and $y$, and returns an interval
output.</p>
<p>$$f([0.5, 1.5], [0.5, 1.5]) = [-0.3, 1.12]$$</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_rect.png" alt="Picture of a circle with field lines and a rectangle region"/></p>
<p>Because the interval result is ambiguous (contains 0), we can&#39;t prove this
region inside or outside the shape, so we subdivide and recurse.  If it was
unambiguously less than or greater than zero, we could stop processing right
away!</p>
<p>The full algorithm produces a set of colored regions (proven inside / outside),
and pixel-level results for ambiguous regions below a certain size:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_interval.png" alt="Circle with interval regions shown"/></p>
<p>Intervals are shown as large uniformly-shaded squares; pixel-by-pixel evaluation
is concentrated at the edge of the circle.</p>
<h3>Using gradients instead</h3>
<p>Our circle model has a gradient of 1 everywhere (proof is left as an exercise to
the reader).
If we want to evaluate an interval region
$[x_\text{min}, x_\text{max}], [y_\text{min}, y_\text{max}]$,
we can sample a single point in the center:</p>
<p>$$v = f\left((x_\text{min} + x_\text{max}) / 2, (y_\text{min} + y_\text{max}) / 2\right)$$</p>
<p>From this point, the maximum distance to a corner is given by
$$d = \frac{\sqrt{(x_\text{max} - x_\text{min})^2 + (y_\text{max} - y_\text{min})^2}}{2}$$</p>
<p>Because our gradient is bounded, we know that the value anywhere in the region
must be in the range $[v - d, v + d]$.</p>
<p>In other words, we can evaluate a single point in the center of the region,
then construct a <em>pseudo-interval</em> result by adding the distance to a corner.</p>
<p>Visually, this is like drawing a circle over the rectangular region:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_region.png" alt="Region with a circle around it"/></p>
<p>The red box is our target region (i.e. intervals in X and Y); the green elements
shows our center point, radius, and effective circle.</p>
<p>This is a drop-in replacement for our previous interval evaluator; we&#39;ll call it
a <strong>&#34;pseudo-interval evaluator&#34;</strong>.  We can run the exact same algorithm to
produce an image, swapping in this new evaluator.  For the original circle, it
produces the same picture:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_pseudo.png" alt="Pseudo-interval evaluation of the circle SDF"/></p>
<p>By carefully positioning the circle, we can see a case where pseudo-interval
evaluation requires more subdivision (interval evaluation on the left,
pseudo-interval evaluation on the right):</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/comparison.png" alt="Comparison"/></p>
<p>Because the pseudo-interval considers the green circle (rather than the red
square of the interval evaluator), it can&#39;t prove this entire region outside of
the shape, and therefore has to subdivide it.</p>
<h3>Downsides to interval evaluation</h3>
<p>There are also cases where the pseudo-interval evaluator has the upper hand!</p>
<p>Consider rotating a model by 45°: this is equivalent to a new evaluation</p>
<p>$$ f_\text{rot}(x, y) = f\left(\frac{x + y}{\sqrt{2}}, \frac{y - x}{\sqrt{2}}\right) $$</p>
<p>The &#34;natural&#34; axes of the model are now at 45° angles from our image axes.
Unfortunately, our intervals are in terms of the image&#39;s $x$ and $y$
coordinates, which remain horizontal and vertical.   During evaluation, this
transform enlarges the sampled region by a factor of $\sqrt{2}$:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/rectangle_rot.png" alt="Rotated rectangle"/></p>
<p>You can now imagine situations where pseudo-interval evaluation is more precise,
because its circle circumscribes the <em>original</em> region:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/rectangle_rot_circle.png" alt="Rotated rectangle with a circle"/></p>
<p>Sure enough, there are cases where interval arithmetic recurses down to
individual pixels, while pseudo-interval evaluation succeeds in proving
regions entirely inside the shape:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/rot_compared.png" alt="Comparison of rotated renderings"/></p>
<p>More generally, interval evalution gets worse and worse as you stack up more
transforms.  One way to think about it is that interval arithmetic &#34;forgets&#34; any
correlations between its inputs.  For example, if you evaluate $x^2$ as $x
\times x$ (instead of with a dedicated power operation), interval arithmetic
produces a much wider result:</p>
<p>$$[-1, 1]^2 = [0, 1]$$
$$[-1, 1] \times [-1, 1] = [-1, 1]$$</p>
<p>We know that the value must be the same on each side of the multiplication, so
it&#39;s impossible to get -1 as a result; however, the implementation of
multiplication assumes that the two sides are totally uncorrelated.</p>
<p>You can work around this failure mode (to some extent) by automatically
<a href="https://www.mattkeeter.com/blog/2016-03-20-affine/">detecting and collapsing affine transforms</a>, but
you certainly don&#39;t get it for free.</p>
<p>Using point samples on a Lipschitz-continuous function means that this problem
disappears: we know that the stack of transforms hasn&#39;t changed the bounds on
the gradient, so any transforms of the model don&#39;t affect sampling!</p>
<h3>Normalization</h3>
<p>Here&#39;s a more complex example, built from about 400 math expressions:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello.png" alt="SDF reading &#39;hello world&#39;"/></p>
<p>Looking at the field lines, it&#39;s clear that this is a less well-behaved distance
field!  Specifically, the gradient at the <code>w</code> is very low; the field lines are
<em>extremely</em> stretched out.</p>
<p>Interval evaluation shows the effects of the badly-behaved field: there&#39;s a
bunch of regions around the <code>w</code> where the renderer recurses farther than
necessary.</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_interval.png" alt="Interval evaluation of the above SDF"/></p>
<p>Having a badly-behaved field means that we can&#39;t assume Lipschitz continuity.
In this model, most of the field has a magnitude $\ge 1$:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/grad_mag.png" alt="Gradient magnitude"/></p>
<p>Our evaluation is using <a href="https://www.mattkeeter.com/projects/fidget">Fidget</a>, which provides a
forward-mode automatic differentiation evaluator.  This suggests a dumb
potential solution: find the <code>(value, dx, dy, dz)</code> tuple using automatic
differentiation, then normalize by dividing the value by the magnitude of the
partial derivatives:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_norm_bad.png" alt="Normalized distance field (which is subtly wrong)"/></p>
<p>The gradient looks <em>vaguely</em> correct – field lines advance at the same rate
everywhere – but there&#39;s a subtle issue here.  Looking above the <code>e</code>, you can
spot a dramatic shift in brightness, indicating that the distance field believes
it&#39;s suddenly much further away from the model.  This clearly won&#39;t work: a
sample taken above the <code>e</code> could falsely report that it&#39;s very far from the
model.</p>
<p>So, how did this go wrong?</p>
<p>Above the <code>e</code>, two fields are combined with <code>min</code>.  We&#39;ll call these two fields
the $e$ and $w$ fields, based on the letter than generated the field.  At the
point where the fields are exactly equal, they have the same value but
<strong>different gradient magnitudes</strong>.</p>
<p>$$
\begin{aligned}
f(x, y) &amp;= \min(e(x, y), w(e, y)) \\
e(x, y) &amp;= 0.1,\hspace{0.1in}||\nabla e(x, y)|| = 1.2 \\
w(x, y) &amp;= 0.1,\hspace{0.1in}||\nabla w(x, y)|| = 0.5 \\
\end{aligned}
$$</p>
<p>Now consider the modified $f_\text{norm}(x, y) = f(x, y) / ||\nabla f(x, y)||$,
slightly above and below the $\min$ transition:</p>
<p>$$
\begin{aligned}
f_\text{norm}(x, y - \epsilon) &amp;= 0.1 / 1.2 = 0.083 \\
f_\text{norm}(x, y + \epsilon) &amp;= 0.1 / 0.5 = 0.2
\end{aligned}
$$</p>
<p>Normalization instroduces a discontinuity in the <strong>value</strong> here!  Even though
our gradient is guaranteed to be 1 everywhere (because of normalization), there
are instantaneous transitions in the value itself.  This means that the
Lipschitz property doesn&#39;t hold, so we can&#39;t use pseudo-interval evaluation.</p>
<h3>Normalization, once more</h3>
<p>What&#39;s to be done?</p>
<p>Non-rigorously, it seems like discontinuities in the gradient magnitude lead to
discontinuities in the value after normalization.  The only expresions which
introduce discontinuities in the gradient magnitude are <code>min</code> and <code>max</code>, which
suggests a simple solution: normalize <em>before</em> those operations.</p>
<p>Of course, this changes the values, but it doesn&#39;t change their sign.  We&#39;re
using <code>min</code> and <code>max</code> to perform <code>union</code> and <code>intersection</code> operations, so the
sign is the only thing that matters.</p>
<pre><code>min(a, b) =&gt;     union(a, b) = min(normalize(a), normalize(b))
max(a, b) =&gt; intersect(a, b) = max(normalize(a), normalize(b))
</code></pre>
<p>Here&#39;s the new normalization:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_norm_good.png" alt="Normalized distance field (which is fine)"/></p>
<p>Everything looks good: the field lines are evenly spaced, and there are no
discontinuities.</p>
<p>Plugging this into our psuedo-interval evaluator, we can get a perfectly
reasonable result.  It ends up evaluating slightly more tiles than the interval
evaluator, but still produces a correct image:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_pseudo.png" alt="Hello, world"/></p>
<h3>Expression simplification</h3>
<p>Interval evaluation has a second benefit: at each <code>min</code> and <code>max</code> node, we may
be able to prove that one branch is <em>always taken</em> within a particular spatial
region.  If this happens, then we can simplify the shape, generating a
shorter expression which is valid within that spatial region (and less expensive
to evaluate).</p>
<p>With intervals, we check that they are not overlapping: <code>min([0, 1], [2, 3])</code>
will always pick the left-hand (<code>[0, 1]</code>) argument.</p>
<p>A similar strategy works for Lipschitz-bounded point samples: within a region of
radius $r$, the value can only differ by $\pm r$ from a point sample in the
center.  The equivalent condition for selecting the left-hand argument of a
<code>min</code> operation is</p>
<p>$$ \min(a, b) \text{ where } a + r \lt b - r$$</p>
<p>Similar rules apply for the right-hand argument and <code>max</code> expressions.</p>
<p>I find it helpful to visualize how much the expression is simplified at each
tile.  For our <code>hello, world</code> model, here&#39;s a comparison of tape lengths at the
final tile, with the original tape being 390 clauses:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/tape_lens.png" alt="Tape lengths in each tile"/></p>
<p>It&#39;s definitely working in both cases!  There are a few tiles where they differ,
but there&#39;s not a clear winner; average tape lengths are 120.2 (for interval
evaluation) versus 121.5 (for pseudo-intervals).</p>
<h3>Performance impacts</h3>
<p>We&#39;ve presented three different evaluation strategies, which can be used to both
prove entire regions inside / outside and provide data for expression
simplification.</p>
<p><strong>Interval arithmetic</strong> is conservative and works on any kind of distance field.
The evaluator has to do extra work for each arithmetic operation to track
intervals, compared to floating-point (value-only) evaluation.  If many
transforms are stacked up, it tends to provide results that are overly broad.</p>
<p>For Lipschitz-continuous distance fields, you can sample at the center of the
region, then add the radius to calculate a <strong>pseudo-interval</strong>.  This is a
single floating-point evaluation, which is cheaper than interval arithmetic.  In
addition, it doesn&#39;t have issues with stacked transforms.</p>
<p>Finally, for distance fields that are not Lipschitz-continuous, you can evaluate
them using forward-mode automatic differentiation, then <strong>normalize</strong> by
dividing by the magnitude of the gradient before <code>min</code> and <code>max</code> nodes, creating
a new Lipschitz-continuous field.  Like single-point sampling, this doesn&#39;t have
issues with stacked transforms; however, automatic differentiation is more
expensive than floating-point evaluation.  Normalization also produces a
different numerical result, so this strategy is only valid if you just care
about the sign.</p>
<p>It&#39;s pretty obvious that single-point sampling will beat interval arithmetic:
it&#39;s doing less work, and doesn&#39;t have issues with overly conservative
estimates.  (Indeed, the paper shows this in Figure 13)</p>
<p>However, it&#39;s not immediately obvious whether interval arithmetic or
<em>normalized</em> single-point samples (using forward-mode automatic differentiation)
would be more expensive!  Obviously, the answer is going to be
implementation-dependent, but I can provide one data point.</p>
<p><a href="https://www.mattkeeter.com/projects/prospero">The Prospero Challenge</a> presents a large expression (7668
clauses), and challenges readers to write an efficient renderer.  The underlying
expression is <em>not</em> Lipschitz-continuous, so we have to use the normalized
evaluator.</p>
<p>Testing these two evaluators on this model, I see the following results:</p>
<table>
    <tbody><tr><th></th><th>Interval</th><th>Normalized</th></tr><tr><td>512×512</td><td>12 ms</td><td>18 ms
    </td></tr><tr><td>1024×1024</td><td>20 ms</td><td>28 ms
    </td></tr><tr><td>1536×1536</td><td>29 ms</td><td>38 ms
    </td></tr><tr><td>2048×2048</td><td>37 ms</td><td>48 ms
</td></tr></tbody></table>
<p>It looks like interval arithmetic is slightly faster <em>in this case</em>, but they&#39;re
relatively close to each other – there&#39;s certainly no order-of-magnitude
differences here.</p>
<h3>Wrapping up</h3>
<p>This has been a fun exercise, and I&#39;d encourage you to go read
<a href="https://wbrbr.org/publications/LipschitzPruning/"><em>Lipschitz Pruning: Hierarchical Simplification of Primitive-Based SDFs</em></a>
yourself and see their full system laid out on paper.</p>
<p>Suggested further reading:</p>
<ul>
<li>Blake Courter is building up a theory of
<a href="https://www.blakecourter.com/ugfs/">unit gradient fields</a>,
which would be amenable to these techniques (since by definition they have a
gradient of 1 everywhere)</li>
<li><a href="https://dl.acm.org/doi/10.1145/3386569.3392374"><em>Monte Carlo geometry processing</em></a>
and subsequent papers discuss using <a href="https://en.wikipedia.org/wiki/Walk-on-spheres_method">walk-on-spheres</a> to
solve geometry problems without discretization, using the same strategy of
point samples on a Lipschitz-continuous distance field</li>
</ul>
<p>I&#39;ve published the code used to generate figures in this blog post at
<a href="https://github.com/mkeeter/gradients-and-intervals"><code>mkeeter/gradients-and-intervals</code></a>,
along with <a href="https://www.mattkeeter.com/blog/2025-05-14-gradients/run.py"><code>run.py</code></a> for post-processing.  This is Research Quality
Code™, so take it with a grain of salt!</p>

<!-- Begin footer -->
</div></div>
  </body>
</html>
