<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/sail-sg/understand-r1-zero">Original</a>
    <h1>Understanding R1-Zero-Like Training: A Critical Perspective</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<ul dir="auto">
<li>21/03/2025: ðŸŽ‰ We release our paper, models and codebase. Our R1-Zero training is implemented with ðŸŒ¾ <a href="https://github.com/sail-sg/oat">Oat</a>, a highly modular, research-friendly and efficient LLM RL framework.</li>
</ul>

<ul dir="auto">
<li>
<p dir="auto"><strong>Understanding R1-Zero-Like Training</strong></p>
<ul dir="auto">
<li>ðŸ“„ <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf">Paper</a></li>
<li>ðŸ¤— <a href="https://huggingface.co/collections/sail/oat-zero-understanding-r1-zero-like-training-67dcdb07b9f3eb05f1501c4a" rel="nofollow">Models</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>There May Not Be Aha Moment in R1-Zero-like Training â€” A Pilot Study</strong></p>
<ul dir="auto">
<li>ðŸ“„ <a href="https://oatllm.notion.site/oat-zero" rel="nofollow">Blog</a></li>
<li>ðŸ’» <a href="https://github.com/sail-sg/oat-zero">Code</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>OAT: A research-friendly framework for LLM online alignment</strong></p>
<ul dir="auto">
<li>ðŸ’» <a href="https://github.com/sail-sg/oat">Codebase</a></li>
</ul>
</li>
</ul>

<p dir="auto">To understand R1-Zero-like training, we critically examine two core components: <strong>base models</strong>
and <strong>reinforcement learning</strong>. We highlight our findings below.</p>

<ol dir="auto">
<li><strong>DeepSeek-V3-Base already exhibit &#34;Aha moment&#34;</strong>.</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/deepseek-base-aha.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/deepseek-base-aha.png" width="70%/"/></a>
</p>
<ol start="2" dir="auto">
<li>As the popular choice for R1-Zero-like training, Qwen2.5 base models demonstrate strong reasoning capabilities
even <strong>without</strong> prompt templates: the average benchmark scores improve by <strong>~60%</strong> (compared to the traditional 4-shot prompting)!</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/qwen-math-base-scores.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/qwen-math-base-scores.png" width="70%/"/></a>
</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">On reinforcement learning:</h3><a id="user-content-on-reinforcement-learning" aria-label="Permalink: On reinforcement learning:" href="#on-reinforcement-learning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol start="3" dir="auto">
<li>GRPO leads to <strong>biased</strong> optimization! We propose a simple fix that improves token efficiency
while maintaining reasoning performance, termed as Dr. GRPO (GRPO <strong>D</strong>one <strong>R</strong>ight).</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/drgrpo.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/drgrpo.png" width="80%/"/></a>
</p>
<ol start="4" dir="auto">
<li>In R1-Zero-like training, the template and the question set perform a duet to affect the RL dynamics
<ul dir="auto">
<li>(Left Plot) For Qwen2.5-Math-1.5B, a mismatched template (e.g., R1 template) in fact <strong>destructs the reasoning capabilities before RL reconstructing it</strong>. This makes the improvement impressive on the surface.</li>
<li>(Middle Plot) However, if a template does not deviate from the pretraining distribution too far, even a small and completely o.o.d. question set (e.g., GSM8K) could induce the reasoning ability equally well, by reinforcing correct reasoning behaviors instead of infusing new knowledge.</li>
</ul>
</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/template-data-duet.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/template-data-duet.png" width="80%/"/></a>
</p>
<ol start="5" dir="auto">
<li>Beyond Qwen, Llama can also be RL-tuned from base models. In this case, domain-specific pretraining will improves RL ceiling.
<ul dir="auto">
<li>(Right Plot) GRPO can even make Llama with math knowledge &#34;Aha&#34; by increasing the output length; however, it is likely due to its length bias, which can be removed by Dr. GRPO.</li>
</ul>
</li>
</ol>
 <p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/llama-r1-zero.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/llama-r1-zero.png" width="70%/"/></a>
</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Our minimalist R1-Zero recipe:</h3><a id="user-content-our-minimalist-r1-zero-recipe" aria-label="Permalink: Our minimalist R1-Zero recipe:" href="#our-minimalist-r1-zero-recipe"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Our analysis suggests a minimalist recipe for R1-Zero-like training:</p>
<p dir="auto">We RL-tune Qwen2.5-
Math-7B using the (unbiased) Dr. GRPO algorithm on MATH level 3-5 questions with the Qwen-Math template, and achieve state-of-the-art performance with only 27 hours compute on 8Ã— A100 GPUs.</p>
 <p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/benchmark.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/benchmark.png" width="90%/"/></a>
</p>
<p dir="auto">If you are interested in more details, please check out our <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf">paper</a>!</p>


<p dir="auto">We recommend a clean <code>python==3.10</code> environment for development.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install vllm &amp; oat, the LLM RL framework we developed r1-zero training on.
pip install vllm==0.7.2 &amp;&amp; pip install oat-llm==0.0.9

# Install this package locally to use the math grader.
git clone git@github.com:sail-sg/understand-r1-zero.git &amp;&amp; cd understand-r1-zero
pip install -e ."><pre><span><span>#</span> Install vllm &amp; oat, the LLM RL framework we developed r1-zero training on.</span>
pip install vllm==0.7.2 &amp;&amp; pip install oat-llm==0.0.9

<span><span>#</span> Install this package locally to use the math grader.</span>
git clone git@github.com:sail-sg/understand-r1-zero.git &amp;&amp; cd understand-r1-zero
pip install -e .</pre></div>

<p dir="auto">We implement R1-Zero training by extending Oat&#39;s Learner and Actor components. Please see <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/train_zero_math.py">train_zero_math.py</a> for a step-by-step guide.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Patch LD_LIBRARY_PATH to avoid dependency errors:
export LD_LIBRARY_PATH=$(python -c &#34;import sysconfig; print(sysconfig.get_config_var(&#39;LIBDIR&#39;))&#34;):$LD_LIBRARY_PATH

# Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:
# (change to `--critic_type grpo` for running GRPO)
python train_zero_math.py \
    --critic_type drgrpo \
    --gpus 8 \
    --enable_prefix_caching \
    --collocate \
    --vllm_sleep \
    --vllm_gpu_ratio 0.35 \
    --gradient-checkpointing \
    --flash-attn \
    --bf16 \
    --rnd-seed \
    --learning_rate 0.000001 \
    --lr_scheduler constant \
    --num_ppo_epochs 1 \
    --beta 0 \
    --oracle_type reward \
    --oracle math \
    --pretrain Qwen/Qwen2.5-Math-1.5B \
    --prompt_template r1 \
    --zero-stage 2 \
    --ref_offload \
    --prompt_data ./datasets/train/math_12k \
    --train_split train \
    --input_key problem \
    --output_key answer \
    --max-train 9999999 \
    --num_prompt_epoch 20 \
    --prompt_max_length 1024 \
    --num_samples 8 \
    --temperature 1 \
    --top_p 1 \
    --generate_max_length 3000 \
    --save_steps -1 \
    --train_batch_size 128 \
    --rollout_batch_size 128 \
    --rollout_batch_size_per_device 16 \
    --pi_buffer_maxlen_per_device 128 \
    --eval_batch_size 200 \
    --eval_steps 16 \
    --eval_temperature 0 \
    --eval_generate_max_length 3000 \
    --eval_data ./datasets/evaluation_suite \
    --eval_input_key input \
    --use-wb \
    --wb-run-name qwen2.5-Math-1.5b-r1-zero \
    --wb_project oat-zero"><pre><span><span>#</span> Patch LD_LIBRARY_PATH to avoid dependency errors:</span>
export LD_LIBRARY_PATH=$(python -c &#34;import sysconfig; print(sysconfig.get_config_var(&#39;LIBDIR&#39;))&#34;):$LD_LIBRARY_PATH

<span><span>#</span> Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:</span>
<span><span>#</span> (change to `--critic_type grpo` for running GRPO)</span>
python train_zero_math.py \
    --critic_type drgrpo \
    --gpus 8 \
    --enable_prefix_caching \
    --collocate \
    --vllm_sleep \
    --vllm_gpu_ratio 0.35 \
    --gradient-checkpointing \
    --flash-attn \
    --bf16 \
    --rnd-seed \
    --learning_rate 0.000001 \
    --lr_scheduler constant \
    --num_ppo_epochs 1 \
    --beta 0 \
    --oracle_type reward \
    --oracle math \
    --pretrain Qwen/Qwen2.5-Math-1.5B \
    --prompt_template r1 \
    --zero-stage 2 \
    --ref_offload \
    --prompt_data ./datasets/train/math_12k \
    --train_split train \
    --input_key problem \
    --output_key answer \
    --max-train 9999999 \
    --num_prompt_epoch 20 \
    --prompt_max_length 1024 \
    --num_samples 8 \
    --temperature 1 \
    --top_p 1 \
    --generate_max_length 3000 \
    --save_steps -1 \
    --train_batch_size 128 \
    --rollout_batch_size 128 \
    --rollout_batch_size_per_device 16 \
    --pi_buffer_maxlen_per_device 128 \
    --eval_batch_size 200 \
    --eval_steps 16 \
    --eval_temperature 0 \
    --eval_generate_max_length 3000 \
    --eval_data ./datasets/evaluation_suite \
    --eval_input_key input \
    --use-wb \
    --wb-run-name qwen2.5-Math-1.5b-r1-zero \
    --wb_project oat-zero</pre></div>
<p dir="auto">Please see <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/examples">here</a> for more example scripts.</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Evaluate our models:
python evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero
python evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero
python evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1

# Evaluate baseline models:
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B
python evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero
python evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero
python evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B"><pre><span><span>#</span> Evaluate our models:</span>
python evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero
python evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero
python evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1

<span><span>#</span> Evaluate baseline models:</span>
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B
python evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero
python evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero
python evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B</pre></div>

<p dir="auto">If you find our work useful for your research, please consider citing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{liu2025understanding,
  title={Understanding R1-Zero-Like Training: A Critical Perspective},
  author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},
  year={2025},
  howpublished={\url{https://github.com/sail-sg/understand-r1-zero}},
}"><pre><span>@misc</span>{<span>liu2025understanding</span>,
  <span>title</span>=<span><span>{</span>Understanding R1-Zero-Like Training: A Critical Perspective<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
  <span>howpublished</span>=<span><span>{</span>\url{https://github.com/sail-sg/understand-r1-zero}<span>}</span></span>,
}</pre></div>

<ul dir="auto">
<li>This work is supported by <a href="https://sail.sea.com/" rel="nofollow">Sea AI Lab</a> for computing resources.</li>
<li>The training codes are built on <a href="https://github.com/sail-sg/oat">Oat</a>, which employs <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> and <a href="https://github.com/google-deepmind/launchpad">launchpad</a>.</li>
<li>The base models are from <a href="https://huggingface.co/Qwen/Qwen2.5-Math-7B" rel="nofollow">Qwen2.5-Math</a>, <a href="https://huggingface.co/meta-llama/Llama-3.2-3B" rel="nofollow">Llama</a>, and <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" rel="nofollow">DeepSeek</a>.</li>
<li>We thank Qingfeng Lan for his time in thoroughly reviewing our code.</li>
</ul>
</article></div></div>
  </body>
</html>
