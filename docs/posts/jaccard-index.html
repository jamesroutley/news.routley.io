<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://en.wikipedia.org/wiki/Jaccard_index">Original</a>
    <h1>Jaccard Index</h1>
    
    <div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div id="mw-content-text" lang="en" dir="ltr"><div>

<div><div><div><div><p><a href="https://en.wikipedia.org/wiki/File:Intersection_of_sets_A_and_B.svg"><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Intersection_of_sets_A_and_B.svg/200px-Intersection_of_sets_A_and_B.svg.png" decoding="async" width="200" height="153" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Intersection_of_sets_A_and_B.svg/300px-Intersection_of_sets_A_and_B.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Intersection_of_sets_A_and_B.svg/400px-Intersection_of_sets_A_and_B.svg.png 2x" data-file-width="371" data-file-height="284"/></a></p></div></div><div><div><p><a href="https://en.wikipedia.org/wiki/File:Union_of_sets_A_and_B.svg"><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Union_of_sets_A_and_B.svg/200px-Union_of_sets_A_and_B.svg.png" decoding="async" width="200" height="153" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Union_of_sets_A_and_B.svg/300px-Union_of_sets_A_and_B.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Union_of_sets_A_and_B.svg/400px-Union_of_sets_A_and_B.svg.png 2x" data-file-width="371" data-file-height="284"/></a></p></div></div><div><p>Intersection and union of two sets A and B</p></div></div></div>

<p>The <b>Jaccard index</b>, also known as the <b>Jaccard similarity coefficient</b>, is a <a href="https://en.wikipedia.org/wiki/Statistic" title="Statistic">statistic</a> used for gauging the <a href="https://en.wikipedia.org/wiki/Similarity_measure" title="Similarity measure">similarity</a> and <a href="https://en.wikipedia.org/wiki/Diversity_index" title="Diversity index">diversity</a> of <a href="https://en.wikipedia.org/wiki/Sample_(statistics)" title="Sample (statistics)">sample</a> sets. It was developed by <a href="https://en.wikipedia.org/wiki/Grove_Karl_Gilbert" title="Grove Karl Gilbert">Grove Karl Gilbert</a> in 1884 as his <b>ratio of verification (v)</b><sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> and now is frequently referred to as the <b>Critical Success Index</b> in meteorology.<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> It was later developed independently by <a href="https://en.wikipedia.org/wiki/Paul_Jaccard" title="Paul Jaccard">Paul Jaccard</a>, originally giving the French name <i>coefficient de communauté</i>,<sup id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup> and independently formulated again by T. Tanimoto.<sup id="cite_ref-:1_4-0"><a href="#cite_note-:1-4">[4]</a></sup> Thus, the <b>Tanimoto index</b> or <b>Tanimoto coefficient</b> are also used in some fields. However, they are identical in generally taking the ratio of <b>Intersection over Union</b>. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the <a href="https://en.wikipedia.org/wiki/Intersection_(set_theory)" title="Intersection (set theory)">intersection</a> divided by the size of the <a href="https://en.wikipedia.org/wiki/Union_(set_theory)" title="Union (set theory)">union</a> of the sample sets:
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eaef5aa86949f49e7dc6b9c8c3dd8b233332c9e7" aria-hidden="true" alt=" J(A,B) = {{|A \cap B|}\over{|A \cup B|}} = {{|A \cap B|}\over{|A| + |B| - |A \cap B|}}."/></span></dd></dl>
<p>Note that by design, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/896adf7fa55a30dfc437230e64c34524e278dc5c" aria-hidden="true" alt=" 0\le J(A,B)\le 1."/></span> If <i>A</i> intersection <i>B</i> is empty, then <i>J</i>(<i>A</i>,<i>B</i>) = 0. The Jaccard coefficient is widely used in computer science, ecology, genomics, and other sciences, where <a href="https://en.wikipedia.org/wiki/Binary_data" title="Binary data">binary or binarized data</a> are used. Both the exact solution and approximation methods are available for hypothesis testing with the Jaccard coefficient.<sup id="cite_ref-:0_5-0"><a href="#cite_note-:0-5">[5]</a></sup>
</p><p>Jaccard similarity also applies to bags, i.e., <a href="https://en.wikipedia.org/wiki/Multiset" title="Multiset">Multisets</a>.  This has a similar formula,<sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup> but the symbols mean 
bag intersection and bag sum (not union). The maximum value is 1/2. 
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2889a77ca1108a6d8648f85a6b05417f36e67423" aria-hidden="true" alt="{\displaystyle J(A,B)={{|A\cap B|} \over {|A\uplus B|}}={{|A\cap B|} \over {|A|+|B|}}.}"/></span></dd></dl>
<p>The <b>Jaccard distance</b>, which measures <i>dis</i>similarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3d17a48a5fb6cea57b076200de6edbccbc1c38f9" aria-hidden="true" alt=" d_J(A,B) = 1 - J(A,B) = { { |A \cup B| - |A \cap B| } \over |A \cup B| }."/></span></dd></dl>
<p>An alternative interpretation of the Jaccard distance is as the ratio of the size of the <a href="https://en.wikipedia.org/wiki/Symmetric_difference" title="Symmetric difference">symmetric difference</a> <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/22453ac41837fe3f74b74840d03fc83a8ed17ed4" aria-hidden="true" alt="A \triangle B = (A \cup B) - (A \cap B)"/></span> to the union. 
Jaccard distance is commonly used to calculate an <i>n</i> × <i>n</i> matrix for <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">clustering</a> and <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a>  of <i>n</i> sample sets.
</p><p>This distance is a <a href="https://en.wikipedia.org/wiki/Distance_function" title="Distance function">metric</a> on the collection of all finite sets.<sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup><sup id="cite_ref-lipkus_8-0"><a href="#cite_note-lipkus-8">[8]</a></sup><sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>
</p><p>There is also a version of the Jaccard distance for <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)" title="Measure (mathematics)">measures</a>, including <a href="https://en.wikipedia.org/wiki/Probability_measure" title="Probability measure">probability measures</a>. If <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9fd47b2a39f7a7856952afec1f1db72c67af6161" aria-hidden="true" alt="\mu "/></span> is a measure on a <a href="https://en.wikipedia.org/wiki/Measurable_space" title="Measurable space">measurable space</a> <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="X"/></span>, then we define the Jaccard coefficient by
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e82cec4924e1c62bb9510ead933fe38e895184d" aria-hidden="true" alt="{\displaystyle J_{\mu }(A,B)={{\mu (A\cap B)} \over {\mu (A\cup B)}},}"/></span></dd></dl>
<p>and the Jaccard distance by
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b9fb1db1c785cb4597c668190f80621532bd4e9b" aria-hidden="true" alt="{\displaystyle d_{\mu }(A,B)=1-J_{\mu }(A,B)={{\mu (A\triangle B)} \over {\mu (A\cup B)}}.}"/></span></dd></dl>
<p>Care must be taken if <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/61c39954b33e842d36490496bb5f2d50f0a5e0e1" aria-hidden="true" alt="\mu(A \cup B) = 0"/></span> or <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c26c105004f30c27aa7c2a9c601550a4183b1f21" aria-hidden="true" alt="\infty "/></span>, since these formulas are not well defined in these cases.
</p><p>The <a href="https://en.wikipedia.org/wiki/MinHash" title="MinHash">MinHash</a> min-wise independent permutations <a href="https://en.wikipedia.org/wiki/Locality_sensitive_hashing" title="Locality sensitive hashing">locality sensitive hashing</a> scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by a constant-sized signature derived from the minimum values of a <a href="https://en.wikipedia.org/wiki/Hash_function" title="Hash function">hash function</a>.
</p>
<meta property="mw:PageProp/toc"/>
<h2><span id="Similarity_of_asymmetric_binary_attributes">Similarity of asymmetric binary attributes</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=1" title="Edit section: Similarity of asymmetric binary attributes">edit</a><span>]</span></span></h2>
<p>Given two objects, <i>A</i> and <i>B</i>, each with <i>n</i> <a href="https://en.wikipedia.org/wiki/Binary_numeral_system" title="Binary numeral system">binary</a> attributes, the Jaccard coefficient is a useful measure of the overlap that <i>A</i> and <i>B</i> share with their attributes.  Each attribute of <i>A</i> and <i>B</i> can either be 0 or 1.  The total number of each combination of attributes for both <i>A</i> and <i>B</i> are specified as follows:
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/83f4effa72a74f4d82e6122f986c61c7a4a12318" aria-hidden="true" alt="M_{11}"/></span> represents the total number of attributes where <i>A</i> and <i>B</i> both have a value of 1.</dd>
<dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2cda7b9fde8a1388081f4eb4830b3416c1fe7d40" aria-hidden="true" alt="M_{01}"/></span> represents the total number of attributes where the attribute of <i>A</i> is 0 and the attribute of <i>B</i> is 1.</dd>
<dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0757dc79ee53ea111396ae1e9f01bd5021c55c8" aria-hidden="true" alt="M_{10}"/></span> represents the total number of attributes where the attribute of <i>A</i> is 1 and the attribute of <i>B</i> is 0.</dd>
<dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e3e73b7d9853b5c1ec725d38277514d780a02c8a" aria-hidden="true" alt="M_{00}"/></span> represents the total number of attributes where <i>A</i> and <i>B</i> both have a value of 0.</dd></dl>
<table>

<tbody><tr>
<th><p><i>A</i></p><p><i>B</i></p></th>
<th>0</th>
<th>1
</th></tr>
<tr>
<th>0
</th>
<td><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e3e73b7d9853b5c1ec725d38277514d780a02c8a" aria-hidden="true" alt="M_{00}"/></span>
</td>
<td><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0757dc79ee53ea111396ae1e9f01bd5021c55c8" aria-hidden="true" alt="M_{10}"/></span>
</td></tr>
<tr>
<th>1
</th>
<td><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2cda7b9fde8a1388081f4eb4830b3416c1fe7d40" aria-hidden="true" alt="M_{01}"/></span>
</td>
<td><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/83f4effa72a74f4d82e6122f986c61c7a4a12318" aria-hidden="true" alt="M_{11}"/></span>
</td></tr></tbody></table>
<p>Each attribute must fall into one of these four categories, meaning that
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/daa513fe09ffeb5bcda856c23fd8c49a4cd81fa5" aria-hidden="true" alt="M_{11} + M_{01} + M_{10} + M_{00} = n."/></span></dd></dl>
<p>The Jaccard similarity coefficient, <i>J</i>, is given as
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fe0b4dede26761822f844a98192ee69ec52030f9" aria-hidden="true" alt="J = {M_{11} \over M_{01} + M_{10} + M_{11}}."/></span></dd></dl>
<p>The Jaccard distance, <i>d</i><sub><i>J</i></sub>, is given as
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/056a75a4fff56290013904fee98e122b232f0440" aria-hidden="true" alt="d_J = {M_{01} + M_{10} \over M_{01} + M_{10} + M_{11}} = 1 - J."/></span></dd></dl>
<p>Statistical inference can be made based on the Jaccard similarity coefficients, and consequently related metrics.<sup id="cite_ref-:0_5-1"><a href="#cite_note-:0-5">[5]</a></sup> Given two sample sets <i>A</i> and <i>B</i> with <i>n</i> attributes, a statistical test can be conducted to see if an overlap is <a href="https://en.wikipedia.org/wiki/Statistical_significance" title="Statistical significance">statistically significant</a>. The exact solution is available, although computation can be costly as <i>n</i> increases.<sup id="cite_ref-:0_5-2"><a href="#cite_note-:0-5">[5]</a></sup> Estimation methods are available either by approximating a <a href="https://en.wikipedia.org/wiki/Multinomial_distribution" title="Multinomial distribution">multinomial distribution</a> or by bootstrapping.<sup id="cite_ref-:0_5-3"><a href="#cite_note-:0-5">[5]</a></sup>
</p>
<h3><span id="Difference_with_the_simple_matching_coefficient_.28SMC.29"></span><span id="Difference_with_the_simple_matching_coefficient_(SMC)">Difference with the simple matching coefficient (SMC)</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=2" title="Edit section: Difference with the simple matching coefficient (SMC)">edit</a><span>]</span></span></h3>
<p>When used for binary attributes, the Jaccard index is very similar to the <a href="https://en.wikipedia.org/wiki/Simple_matching_coefficient" title="Simple matching coefficient">simple matching coefficient</a>. The main difference is that the SMC has the term <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e3e73b7d9853b5c1ec725d38277514d780a02c8a" aria-hidden="true" alt="M_{00}"/></span> in its numerator and denominator, whereas the Jaccard index does not. Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets.
</p><p>In <a href="https://en.wikipedia.org/wiki/Affinity_analysis" title="Affinity analysis">market basket analysis</a>, for example, the basket of two consumers who we wish to compare might only contain a small fraction of all the available products in the store, so the SMC will usually return very high values of similarities even when the baskets bear very little resemblance, thus making the Jaccard index a more appropriate measure of similarity in that context. For example, consider a supermarket with 1000 products and two customers. The basket of the first customer contains salt and pepper and the basket of the second contains salt and sugar. In this scenario, the similarity between the two baskets as measured by the Jaccard index would be 1/3, but the similarity becomes 0.998 using the SMC.
</p><p>In other contexts, where 0 and 1 carry equivalent information (symmetry), the SMC is a better measure of similarity. For example, vectors of demographic variables stored in <a href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)" title="Dummy variable (statistics)">dummy variables</a>, such as gender, would be better compared with the SMC than with the Jaccard index since the impact of gender on similarity should be equal, independently of whether male is defined as a 0 and female as a 1 or the other way around. However, when we have symmetric dummy variables, one could replicate the behaviour of the SMC by splitting the dummies into two binary attributes (in this case, male and female), thus transforming them into asymmetric attributes, allowing the use of the Jaccard index without introducing any bias. The SMC remains, however, more computationally efficient in the case of symmetric dummy variables since it does not require adding extra dimensions.
</p>
<h2><span id="Weighted_Jaccard_similarity_and_distance">Weighted Jaccard similarity and distance</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=3" title="Edit section: Weighted Jaccard similarity and distance">edit</a><span>]</span></span></h2>
<p>If <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ae4a713bd817434a854daeb28d6aa8909c05be6c" aria-hidden="true" alt="\mathbf{x} = (x_1, x_2, \ldots, x_n)"/></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68e23e3f9a7a0f339527dab3e613447552d87125" aria-hidden="true" alt="\mathbf{y} = (y_1, y_2, \ldots, y_n)"/></span> are two vectors with all real <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/44a42071a75783328dbe8c21994f7f2f9a863d37" aria-hidden="true" alt="x_i, y_i \geq 0"/></span>, then their Jaccard similarity coefficient (also known then as Ruzicka similarity) is defined as
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a5c2f402da5e73885154e474ea156381703485d" aria-hidden="true" alt="{\displaystyle J_{\mathcal {W}}(\mathbf {x} ,\mathbf {y} )={\frac {\sum _{i}\min(x_{i},y_{i})}{\sum _{i}\max(x_{i},y_{i})}},}"/></span></dd></dl>
<p>and Jaccard distance (also known then as Soergel distance)
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a852ba5e8c8eeb97ef3553b03664cdf3078c67a" aria-hidden="true" alt="{\displaystyle d_{J{\mathcal {W}}}(\mathbf {x} ,\mathbf {y} )=1-J_{\mathcal {W}}(\mathbf {x} ,\mathbf {y} ).}"/></span></dd></dl>
<p>With even more generality, if <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" aria-hidden="true" alt="f"/></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3556280e66fe2c0d0140df20935a6f057381d77" aria-hidden="true" alt="g"/></span> are two non-negative measurable functions on a measurable space <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="X"/></span> with measure <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9fd47b2a39f7a7856952afec1f1db72c67af6161" aria-hidden="true" alt="\mu "/></span>, then we can define
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/033df94fac26f64b3a738130ce28d0f202d0b3e3" aria-hidden="true" alt="{\displaystyle J_{\mathcal {W}}(f,g)={\frac {\int \min(f,g)d\mu }{\int \max(f,g)d\mu }},}"/></span></dd></dl>
<p>where <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8e49fca3e322708b32d21eaa8b095dc05f09538" aria-hidden="true" alt="\max "/></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/695d28931288a686335c3969dfd15bb76ea873db" aria-hidden="true" alt="\min "/></span> are pointwise operators. Then Jaccard distance is
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a06db8607d8c054bb96a458345782f4d7952f6eb" aria-hidden="true" alt="{\displaystyle d_{J{\mathcal {W}}}(f,g)=1-J_{\mathcal {W}}(f,g).}"/></span></dd></dl>
<p>Then, for example, for two measurable sets <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/77cbebe3113e44afef7d4a0fd3f424efef2855a6" aria-hidden="true" alt="A, B \subseteq X"/></span>, we have <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7fc3a81e843358ebe350bd76f3cebf91f5829bf2" aria-hidden="true" alt="J_\mu(A,B) = J(\chi_A, \chi_B),"/></span> where <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/49baf1caaa804f2d77bfc7570d102ee4a3cafa26" aria-hidden="true" alt="\chi _{A}"/></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ccc5d247c0324706579cf85da51d154eb46a6ea8" aria-hidden="true" alt="\chi_B"/></span> are the characteristic functions of the corresponding set.
</p>
<h2><span id="Probability_Jaccard_similarity_and_distance">Probability Jaccard similarity and distance</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=4" title="Edit section: Probability Jaccard similarity and distance">edit</a><span>]</span></span></h2>
<p>The weighted Jaccard similarity described above generalizes the Jaccard Index to positive vectors, where a set corresponds to a binary vector given by the <a href="https://en.wikipedia.org/wiki/Indicator_function" title="Indicator function">indicator function</a>, i.e. <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07dda71da2a630762c7b21b51ea54f86f422f951" aria-hidden="true" alt="x_{i}\in \{0,1\}"/></span>. However, it does not generalize the Jaccard Index to probability distributions, where a set corresponds to a uniform probability distribution, i.e.
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b72a41a226fe1ce9fd7c9eba68918f17982959ec" aria-hidden="true" alt="{\displaystyle x_{i}={\begin{cases}{\frac {1}{|X|}}&amp;i\in X\\0&amp;{\text{otherwise}}\end{cases}}}"/></span></dd></dl>
<p>It is always less if the sets differ in size. If <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f80f01e262afdfc81608fd43cfd047898e3b84f7" aria-hidden="true" alt="{\displaystyle |X|&gt;|Y|}"/></span>, and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d929a2c7b2f69498be5f5e9508fd6b432d637f0a" aria-hidden="true" alt="{\displaystyle x_{i}=\mathbf {1} _{X}(i)/|X|,y_{i}=\mathbf {1} _{Y}(i)/|Y|}"/></span> then
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b83220d677d5516ded16506a0b7c827d0c5a6579" aria-hidden="true" alt="{\displaystyle J_{\mathcal {W}}(x,y)={\frac {|X\cap Y|}{|X\setminus Y|+|X|}}&lt;J(X,Y).}"/></span></dd></dl>
<div><div><p><a href="https://en.wikipedia.org/wiki/File:Geometric_interpretation_of_the_Probability_Jaccard_Index_as_Simplices.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Geometric_interpretation_of_the_Probability_Jaccard_Index_as_Simplices.png/390px-Geometric_interpretation_of_the_Probability_Jaccard_Index_as_Simplices.png" decoding="async" width="390" height="216" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Geometric_interpretation_of_the_Probability_Jaccard_Index_as_Simplices.png/585px-Geometric_interpretation_of_the_Probability_Jaccard_Index_as_Simplices.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Geometric_interpretation_of_the_Probability_Jaccard_Index_as_Simplices.png/780px-Geometric_interpretation_of_the_Probability_Jaccard_Index_as_Simplices.png 2x" data-file-width="1140" data-file-height="631"/></a></p><div><p>The probability Jaccard index can be interpreted as intersections of simplices.</p></div></div></div>
<p>Instead, a generalization that is continuous between probability distributions and their corresponding support sets is
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/94a8beb34e9f6ca3dca58d674d0dac4c9a6e4ac0" aria-hidden="true" alt="{\displaystyle J_{\mathcal {P}}(x,y)=\sum _{x_{i}\neq 0,y_{i}\neq 0}{\frac {1}{\sum _{j}\max \left({\frac {x_{j}}{x_{i}}},{\frac {y_{j}}{y_{i}}}\right)}}}"/></span></dd></dl>
<p>which is called the &#34;Probability&#34; Jaccard.<sup id="cite_ref-pminhash_10-0"><a href="#cite_note-pminhash-10">[10]</a></sup> It has the following bounds against the Weighted Jaccard on probability vectors.
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dcd309ea068fecc2b1a66e59b3ce5febe7cefb82" aria-hidden="true" alt="{\displaystyle J_{\mathcal {W}}(x,y)\leq J_{\mathcal {P}}(x,y)\leq {\frac {2J_{\mathcal {W}}(x,y)}{1+J_{\mathcal {W}}(x,y)}}}"/></span></dd></dl>
<p>Here the upper bound is the (weighted) <a href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient#Difference_from_Jaccard" title="Sørensen–Dice coefficient">Sørensen–Dice coefficient</a>.
The corresponding distance, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4db543bbe400f192b641842b33198afae5f1921b" aria-hidden="true" alt="{\displaystyle 1-J_{\mathcal {P}}(x,y)}"/></span>, is a metric over probability distributions, and a <a href="https://en.wikipedia.org/wiki/Pseudometric_space" title="Pseudometric space">pseudo-metric</a> over non-negative vectors.
</p><p>The Probability Jaccard Index has a geometric interpretation as the area of an intersection of <a href="https://en.wikipedia.org/wiki/Simplex" title="Simplex">simplices</a>. Every point on a unit <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" aria-hidden="true" alt="k"/></span>-simplex corresponds to a probability distribution on <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/552a558062ed4c0486297b5b5531c5ee044dbd9b" aria-hidden="true" alt="k+1"/></span> elements, because the unit <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" aria-hidden="true" alt="k"/></span>-simplex is the set of points in <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/552a558062ed4c0486297b5b5531c5ee044dbd9b" aria-hidden="true" alt="k+1"/></span> dimensions that sum to 1. To derive the Probability Jaccard Index geometrically, represent a probability distribution as the unit simplex divided into sub simplices according to the mass of each item. If you overlay two distributions represented in this way on top of each other, and intersect the simplices corresponding to each item, the area that remains is equal to the Probability Jaccard Index of the distributions.
</p>
<h3><span id="Optimality_of_the_Probability_Jaccard_Index">Optimality of the Probability Jaccard Index</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=5" title="Edit section: Optimality of the Probability Jaccard Index">edit</a><span>]</span></span></h3>
<div><div><p><a href="https://en.wikipedia.org/wiki/File:Visual_proof_of_the_optimality_of_the_Probability_Jaccard_Index_on_Three_element_distributions.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Visual_proof_of_the_optimality_of_the_Probability_Jaccard_Index_on_Three_element_distributions.png/390px-Visual_proof_of_the_optimality_of_the_Probability_Jaccard_Index_on_Three_element_distributions.png" decoding="async" width="390" height="230" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Visual_proof_of_the_optimality_of_the_Probability_Jaccard_Index_on_Three_element_distributions.png/585px-Visual_proof_of_the_optimality_of_the_Probability_Jaccard_Index_on_Three_element_distributions.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Visual_proof_of_the_optimality_of_the_Probability_Jaccard_Index_on_Three_element_distributions.png/780px-Visual_proof_of_the_optimality_of_the_Probability_Jaccard_Index_on_Three_element_distributions.png 2x" data-file-width="1140" data-file-height="671"/></a></p><div><p>A visual proof of the optimality of the Probability Jaccard Index on three element distributions.</p></div></div></div>
<p>Consider the problem of constructing random variables such that they collide with each other as much as possible. That is, if <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db9201671c9de928babaa1a7a44ef01bbc85a583" aria-hidden="true" alt="{\displaystyle X\sim x}"/></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/136ff2783ccafbd2d133e1155729e04a91f6192e" aria-hidden="true" alt="{\displaystyle Y\sim y}"/></span>, we would like to construct <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="X"/></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" aria-hidden="true" alt="Y"/></span> to maximize <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9c35204086ef689f4c1ed3bbeedaa8a8ac942793" aria-hidden="true" alt="{\displaystyle \Pr[X=Y]}"/></span>. If we look at just two distributions <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5ea0abffd33a692ded22accc104515a032851dff" aria-hidden="true" alt="x,y"/></span> in isolation, the highest <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9c35204086ef689f4c1ed3bbeedaa8a8ac942793" aria-hidden="true" alt="{\displaystyle \Pr[X=Y]}"/></span> we can achieve is given by <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b5b753e9ee8a4f3ee86e0bc180c0270bf17e286a" aria-hidden="true" alt="{\displaystyle 1-{\text{TV}}(x,y)}"/></span> where <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/99041a68bea2c26461ec460324c60e9eab8d5b61" aria-hidden="true" alt="{\displaystyle {\text{TV}}}"/></span> is the <a href="https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures" title="Total variation distance of probability measures">Total Variation distance</a>. However, suppose we weren&#39;t just concerned with maximizing that particular pair, suppose we would like to maximize the collision probability of any arbitrary pair. One could construct an infinite number of random variables one for each distribution <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" aria-hidden="true" alt="x"/></span>, and seek to maximize <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9c35204086ef689f4c1ed3bbeedaa8a8ac942793" aria-hidden="true" alt="{\displaystyle \Pr[X=Y]}"/></span> for all pairs <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5ea0abffd33a692ded22accc104515a032851dff" aria-hidden="true" alt="x,y"/></span>. In a fairly strong sense described below, the Probability Jaccard Index is an optimal way to align these random variables.
</p><p>For any sampling method <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f3c8921a3b352de45446a6789b104458c9f90b" aria-hidden="true" alt="G"/></span> and discrete distributions <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5ea0abffd33a692ded22accc104515a032851dff" aria-hidden="true" alt="x,y"/></span>, if <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d51bcb8456e58c7bfdc742dc68a54693ed5dfacf" aria-hidden="true" alt="{\displaystyle \Pr[G(x)=G(y)]&gt;J_{\mathcal {P}}(x,y)}"/></span> then for some <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bf368e72c009decd9b6686ee84a375632e11de98" aria-hidden="true" alt="z"/></span> where <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c89e01bc62b77de3b856828f0d87ce662ce9ae81" aria-hidden="true" alt="{\displaystyle J_{\mathcal {P}}(x,z)&gt;J_{\mathcal {P}}(x,y)}"/></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3234eceee4240845fb0873ca3f433f9625df49c3" aria-hidden="true" alt="{\displaystyle J_{\mathcal {P}}(y,z)&gt;J_{\mathcal {P}}(x,y)}"/></span>, either <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/66ca35d4656cc676fc5ee3a8b24755c9da1d58ac" aria-hidden="true" alt="{\displaystyle \Pr[G(x)=G(z)]&lt;J_{\mathcal {P}}(x,z)}"/></span> or <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a5863ec699546c3b1799683031e35c6a15017f0b" aria-hidden="true" alt="{\displaystyle \Pr[G(y)=G(z)]&lt;J_{\mathcal {P}}(y,z)}"/></span>.<sup id="cite_ref-pminhash_10-1"><a href="#cite_note-pminhash-10">[10]</a></sup>
</p><p>That is, no sampling method can achieve more collisions than <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dd7bebf9691b5acc698e4c0204b1eaff7f1092e7" aria-hidden="true" alt="{\displaystyle J_{\mathcal {P}}}"/></span> on one pair without achieving fewer collisions than <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dd7bebf9691b5acc698e4c0204b1eaff7f1092e7" aria-hidden="true" alt="{\displaystyle J_{\mathcal {P}}}"/></span> on another pair, where the reduced pair is more similar under <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dd7bebf9691b5acc698e4c0204b1eaff7f1092e7" aria-hidden="true" alt="{\displaystyle J_{\mathcal {P}}}"/></span> than the increased pair. This theorem is true for the Jaccard Index of sets (if interpreted as uniform distributions) and the probability Jaccard, but not of the weighted Jaccard. (The theorem uses the word &#34;sampling method&#34; to describe a joint distribution over all distributions on a space, because it derives from the use of <a href="https://en.wikipedia.org/wiki/MinHash#Incorporating_Weights" title="MinHash">weighted minhashing algorithms</a> that achieve this as their collision probability.)
</p><p>This theorem has a visual proof on three element distributions using the simplex representation.
</p>
<h2><span id="Tanimoto_similarity_and_distance">Tanimoto similarity and distance</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=6" title="Edit section: Tanimoto similarity and distance">edit</a><span>]</span></span></h2>
<p>Various forms of functions described as  Tanimoto similarity  and Tanimoto distance occur  in the literature and on the Internet. Most of these are synonyms for Jaccard similarity and Jaccard distance, but some are mathematically different. Many sources<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> cite an IBM Technical Report<sup id="cite_ref-:1_4-1"><a href="#cite_note-:1-4">[4]</a></sup> as the seminal reference. The report is available from <a rel="nofollow" href="https://www.worldcat.org/oclc/10917698">several libraries</a>.
</p><p>In &#34;A Computer Program for Classifying Plants&#34;, published in October 1960,<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> a method of classification based on a similarity ratio, and a derived distance function, is given. It seems that this is  the most authoritative  source for the meaning of the terms &#34;Tanimoto similarity&#34; and &#34;Tanimoto Distance&#34;. The similarity ratio is equivalent to Jaccard similarity, but the distance function is <i>not</i> the same as Jaccard distance.
</p>
<h3><span id="Tanimoto.27s_definitions_of_similarity_and_distance"></span><span id="Tanimoto&#39;s_definitions_of_similarity_and_distance">Tanimoto&#39;s definitions of similarity and distance</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=7" title="Edit section: Tanimoto&#39;s definitions of similarity and distance">edit</a><span>]</span></span></h3>
<p>In that paper, a &#34;similarity ratio&#34; is  given over <a href="https://en.wikipedia.org/wiki/Bit_array" title="Bit array">bitmaps</a>, where each bit of a fixed-size array represents the presence or absence of a characteristic in the plant being modelled. The definition of the ratio is the number of common bits, divided by the number of bits set (<i>i.e.</i> nonzero) in either sample.
</p><p>Presented in mathematical terms, if samples <i>X</i> and <i>Y</i> are bitmaps, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af4a0955af42beb5f85aa05fb8c07abedc13990d" aria-hidden="true" alt="X_{i}"/></span> is the <i>i</i>th bit of <i>X</i>, and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7d8d919d13846514d6b41b0d8b4f5e25de101f2" aria-hidden="true" alt=" \land , \lor "/></span> are <a href="https://en.wikipedia.org/wiki/Bitwise_operation" title="Bitwise operation">bitwise</a> <i><a href="https://en.wikipedia.org/wiki/Logical_conjunction" title="Logical conjunction">and</a></i>, <i><a href="https://en.wikipedia.org/wiki/Logical_disjunction" title="Logical disjunction">or</a></i> operators respectively, then the similarity ratio <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/942b5ac7697c72b9e3bc1e3b340daf7189fdf61f" aria-hidden="true" alt="T_{s}"/></span> is
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2e50229a192465739b8532e85292c8803ff3fff3" aria-hidden="true" alt=" T_s(X,Y) =  \frac{\sum_i ( X_i \land Y_i)}{\sum_i ( X_i \lor Y_i)}"/></span></dd></dl>
<p>If each sample is modelled instead as a set of attributes, this value is  equal to the Jaccard coefficient of the two sets. Jaccard is not cited in the paper, and it seems likely that the authors were not aware of it.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="says who? (March 2022)">citation needed</span></a></i>]</sup>
</p><p>Tanimoto goes on to define a &#34;distance coefficient&#34; based on this ratio, defined for bitmaps with non-zero similarity:
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d6803d80804b0c51d4d3b1359af5c8f92f98fe2f" aria-hidden="true" alt="T_d(X,Y) = -\log_2 ( T_s(X,Y) ) "/></span></dd></dl>
<p>This coefficient is, deliberately, not a distance metric. It is chosen to allow the possibility of two specimens, which are quite different from each other, to both be similar to a third. It is  easy to construct an example which disproves the property of <a href="https://en.wikipedia.org/wiki/Triangle_inequality#Metric_space" title="Triangle inequality">triangle inequality</a>.
</p>
<h3><span id="Other_definitions_of_Tanimoto_distance">Other definitions of Tanimoto distance</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=8" title="Edit section: Other definitions of Tanimoto distance">edit</a><span>]</span></span></h3>
<p>Tanimoto distance is often referred to, erroneously, as a synonym for Jaccard distance <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc70714f30fa9960778249a5baa664031ea30744" aria-hidden="true" alt="{\displaystyle 1-T_{s}}"/></span>. This function is a proper distance metric. &#34;Tanimoto Distance&#34; is often stated as being a proper distance metric, probably because of its confusion with Jaccard distance.
</p><p>If Jaccard or Tanimoto similarity is expressed over a bit vector, then it can be written as
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/80905696bb2abde3715ff49e11173631d1cd5b8d" aria-hidden="true" alt="{\displaystyle f(A,B)={\frac {A\cdot B}{\|A\|^{2}+\|B\|^{2}-A\cdot B}}}"/></span></dd></dl>
<p>where the same calculation is expressed in terms of vector scalar product and magnitude. This representation relies on the fact that, for a bit vector (where the value of each dimension is either 0 or 1) then
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6dadb24b34213d00ffeff3de3c6cbd0df5a4e226" aria-hidden="true" alt="A \cdot B = \sum_i A_iB_i = \sum_i ( A_i \land B_i)"/></span></dd></dl>
<p>and
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/25bad6395ff24ec87264ab335563055e89cb415a" aria-hidden="true" alt="{\displaystyle \|A\|^{2}=\sum _{i}A_{i}^{2}=\sum _{i}A_{i}.}"/></span></dd></dl>
<p>This is a potentially confusing representation, because the function as expressed over vectors is more general, unless its domain is explicitly restricted. Properties of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/942b5ac7697c72b9e3bc1e3b340daf7189fdf61f" aria-hidden="true" alt=" T_s "/></span> do not necessarily extend to <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" aria-hidden="true" alt="f"/></span>. In particular, the difference function <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32ba41af59057abb76dfce3220ab25807741daf5" aria-hidden="true" alt="1-f"/></span> does not preserve <a href="https://en.wikipedia.org/wiki/Triangle_inequality" title="Triangle inequality">triangle inequality</a>, and is not therefore a proper distance metric, whereas <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc70714f30fa9960778249a5baa664031ea30744" aria-hidden="true" alt="1-T_{s}"/></span> is.
</p><p>There is a real danger that the combination of &#34;Tanimoto Distance&#34; being defined using this formula, along with the statement &#34;Tanimoto Distance is a proper distance metric&#34; will lead to the false conclusion that the function <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32ba41af59057abb76dfce3220ab25807741daf5" aria-hidden="true" alt="1-f"/></span> is in fact a distance metric over vectors or <a href="https://en.wikipedia.org/wiki/Multiset" title="Multiset">multisets</a> in general, whereas its use in similarity search or clustering algorithms may fail to produce correct results.
</p><p>Lipkus<sup id="cite_ref-lipkus_8-1"><a href="#cite_note-lipkus-8">[8]</a></sup> uses a definition of Tanimoto similarity which is equivalent to <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61" aria-hidden="true" alt="f"/></span>, and refers to Tanimoto distance as the function <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32ba41af59057abb76dfce3220ab25807741daf5" aria-hidden="true" alt="1-f"/></span>. It is, however, made clear within the paper that the context is restricted by the use of a (positive) weighting vector <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54a9c4c547f4d6111f81946cad242b18298d70b7" aria-hidden="true" alt="W"/></span> such that, for any vector <i>A</i> being considered, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d128a7cc4524cca9d4f15e860d035cb745d81a4" aria-hidden="true" alt="{\displaystyle A_{i}\in \{0,W_{i}\}.}"/></span> Under these circumstances, the  function is a proper distance metric, and so a set of vectors governed by such a weighting vector forms a <a href="https://en.wikipedia.org/wiki/Metric_space" title="Metric space">metric space</a> under this function.
</p>
<h2><span id="Jaccard_index_in_binary_classification_confusion_matrices">Jaccard index in binary classification confusion matrices</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=9" title="Edit section: Jaccard index in binary classification confusion matrices">edit</a><span>]</span></span></h2>
<p>In <a href="https://en.wikipedia.org/wiki/Confusion_matrix" title="Confusion matrix">confusion matrices</a> employed for <a href="https://en.wikipedia.org/wiki/Binary_classification" title="Binary classification">binary classification</a>, the Jaccard index can be framed in the following formula:
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7b89006c0b38eda6bea009986c6a485391a81b2c" aria-hidden="true" alt="{\displaystyle {\text{Jaccard index}}={\frac {TP}{TP+FP+FN}}}"/></span></dd></dl>
<p>where TP are the true positives, FP the false positives and FN the false negatives.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=10" title="Edit section: See also">edit</a><span>]</span></span></h2>

<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=11" title="Edit section: References">edit</a><span>]</span></span></h2>
<div>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFMurphy1996">Murphy, Allan H. (1996). <a rel="nofollow" href="https://doi.org/10.1175/1520-0434(1996)011%3C0003:TFAASE%3E2.0.CO;2">&#34;The Finley Affair: A Signal Event in the History of Forecast Verification&#34;</a>. <i>Weather and Forecasting</i>. <b>11</b> (1): 3. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1996WtFor..11....3M">1996WtFor..11....3M</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1175%2F1520-0434%281996%29011%3C0003%3ATFAASE%3E2.0.CO%3B2">10.1175/1520-0434(1996)011&lt;0003:TFAASE&gt;2.0.CO;2</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a rel="nofollow" href="https://www.worldcat.org/issn/1520-0434">1520-0434</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:54532560">54532560</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Weather+and+Forecasting&amp;rft.atitle=The+Finley+Affair%3A+A+Signal+Event+in+the+History+of+Forecast+Verification&amp;rft.volume=11&amp;rft.issue=1&amp;rft.pages=3&amp;rft.date=1996&amp;rft_id=info%3Adoi%2F10.1175%2F1520-0434%281996%29011%3C0003%3ATFAASE%3E2.0.CO%3B2&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A54532560%23id-name%3DS2CID&amp;rft.issn=1520-0434&amp;rft_id=info%3Abibcode%2F1996WtFor..11....3M&amp;rft.aulast=Murphy&amp;rft.aufirst=Allan+H.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1175%2F1520-0434%281996%29011%253C0003%3ATFAASE%253E2.0.CO%3B2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><a rel="nofollow" href="https://www.swpc.noaa.gov/sites/default/files/images/u30/Forecast%20Verification%20Glossary.pdf">https://www.swpc.noaa.gov/sites/default/files/images/u30/Forecast%20Verification%20Glossary.pdf</a><sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Bare_URLs" title="Wikipedia:Bare URLs"><span title="A full citation of this PDF document is required to prevent link rot. (May 2022)">bare URL PDF</span></a></i>]</sup></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite id="CITEREFJaccard1912">Jaccard, Paul (February 1912). <a rel="nofollow" href="http://doi.wiley.com/10.1111/j.1469-8137.1912.tb05611.x">&#34;The Distribution of the Flora in the Alpine Zone.1&#34;</a>. <i>New Phytologist</i>. <b>11</b> (2): 37–50. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1111%2Fj.1469-8137.1912.tb05611.x">10.1111/j.1469-8137.1912.tb05611.x</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a> <a rel="nofollow" href="https://www.worldcat.org/issn/0028-646X">0028-646X</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Phytologist&amp;rft.atitle=The+Distribution+of+the+Flora+in+the+Alpine+Zone.1&amp;rft.volume=11&amp;rft.issue=2&amp;rft.pages=37-50&amp;rft.date=1912-02&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1469-8137.1912.tb05611.x&amp;rft.issn=0028-646X&amp;rft.aulast=Jaccard&amp;rft.aufirst=Paul&amp;rft_id=http%3A%2F%2Fdoi.wiley.com%2F10.1111%2Fj.1469-8137.1912.tb05611.x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-:1-4"><span>^ <a href="#cite_ref-:1_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_4-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFTanimoto1958">Tanimoto TT (17 Nov 1958). &#34;An Elementary Mathematical theory of Classification and Prediction&#34;. <i>Internal IBM Technical Report</i>. <b>1957</b> (8?).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Internal+IBM+Technical+Report&amp;rft.atitle=An+Elementary+Mathematical+theory+of+Classification+and+Prediction&amp;rft.volume=1957&amp;rft.issue=8%3F&amp;rft.date=1958-11-17&amp;rft.aulast=Tanimoto&amp;rft.aufirst=TT&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-:0-5"><span>^ <a href="#cite_ref-:0_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_5-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:0_5-3"><sup><i><b>d</b></i></sup></a></span> <span><cite id="CITEREFChungMiasojedowStartekGambin2019">Chung NC, Miasojedow B, Startek M, Gambin A (December 2019). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6929325">&#34;Jaccard/Tanimoto similarity test and estimation methods for biological presence-absence data&#34;</a>. <i>BMC Bioinformatics</i>. <b>20</b> (Suppl 15): 644. <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/1903.11372">1903.11372</a></span>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1186%2Fs12859-019-3118-5">10.1186/s12859-019-3118-5</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6929325">6929325</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/31874610">31874610</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BMC+Bioinformatics&amp;rft.atitle=Jaccard%2FTanimoto+similarity+test+and+estimation+methods+for+biological+presence-absence+data&amp;rft.volume=20&amp;rft.issue=Suppl+15&amp;rft.pages=644&amp;rft.date=2019-12&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6929325%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F31874610&amp;rft_id=info%3Aarxiv%2F1903.11372&amp;rft_id=info%3Adoi%2F10.1186%2Fs12859-019-3118-5&amp;rft.aulast=Chung&amp;rft.aufirst=NC&amp;rft.au=Miasojedow%2C+B&amp;rft.au=Startek%2C+M&amp;rft.au=Gambin%2C+A&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6929325&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><cite id="CITEREFLeskovecRajaramanUllman2020">Leskovec J, Rajaraman A, Ullman J (2020). <i>Mining of Massive Datasets</i>. Cambridge. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/9781108476348" title="Special:BookSources/9781108476348"><bdi>9781108476348</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mining+of+Massive+Datasets&amp;rft.pub=Cambridge&amp;rft.date=2020&amp;rft.isbn=9781108476348&amp;rft.aulast=Leskovec&amp;rft.aufirst=J&amp;rft.au=Rajaraman%2C+A&amp;rft.au=Ullman%2C+J&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span> and p. 76-77 in an earlier version  <a rel="nofollow" href="http://infolab.stanford.edu/~ullman/mmds/ch3.pdf">http://infolab.stanford.edu/~ullman/mmds/ch3.pdf</a></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite id="CITEREFKosub2019">Kosub S (April 2019). &#34;A note on the triangle inequality for the Jaccard distance&#34;. <i>Pattern Recognition Letters</i>. <b>120</b>: 36–8. <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/1612.02696">1612.02696</a></span>. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/2019PaReL.120...36K">2019PaReL.120...36K</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2Fj.patrec.2018.12.007">10.1016/j.patrec.2018.12.007</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:564831">564831</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition+Letters&amp;rft.atitle=A+note+on+the+triangle+inequality+for+the+Jaccard+distance.&amp;rft.volume=120&amp;rft.pages=36-8&amp;rft.date=2019-04&amp;rft_id=info%3Aarxiv%2F1612.02696&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A564831%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patrec.2018.12.007&amp;rft_id=info%3Abibcode%2F2019PaReL.120...36K&amp;rft.aulast=Kosub&amp;rft.aufirst=S&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-lipkus-8"><span>^ <a href="#cite_ref-lipkus_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lipkus_8-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFLipkus1999">Lipkus AH (1999). &#34;A proof of the triangle inequality for the Tanimoto distance&#34;. <i>Journal of Mathematical Chemistry</i>. <b>26</b> (1–3): 263–265. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1023%2FA%3A1019154432472">10.1023/A:1019154432472</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:118263043">118263043</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Mathematical+Chemistry&amp;rft.atitle=A+proof+of+the+triangle+inequality+for+the+Tanimoto+distance&amp;rft.volume=26&amp;rft.issue=1%E2%80%933&amp;rft.pages=263-265&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1019154432472&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A118263043%23id-name%3DS2CID&amp;rft.aulast=Lipkus&amp;rft.aufirst=AH&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite id="CITEREFLevandowskyWinter1971">Levandowsky M, Winter D (1971). &#34;Distance between sets&#34;. <i>Nature</i>. <b>234</b> (5): 34–35. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1971Natur.234...34L">1971Natur.234...34L</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1038%2F234034a0">10.1038/234034a0</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:4283015">4283015</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Distance+between+sets&amp;rft.volume=234&amp;rft.issue=5&amp;rft.pages=34-35&amp;rft.date=1971&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A4283015%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1038%2F234034a0&amp;rft_id=info%3Abibcode%2F1971Natur.234...34L&amp;rft.aulast=Levandowsky&amp;rft.aufirst=M&amp;rft.au=Winter%2C+D&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-pminhash-10"><span>^ <a href="#cite_ref-pminhash_10-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pminhash_10-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFMoultonJiang2018">Moulton R, Jiang Y (2018). &#34;Maximally Consistent Sampling and the Jaccard Index of Probability Distributions&#34;. <i>International Conference on Data Mining, Workshop on High Dimensional Data Mining</i>: 347–356. <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/1809.04052">1809.04052</a></span>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1109%2FICDM.2018.00050">10.1109/ICDM.2018.00050</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-5386-9159-5" title="Special:BookSources/978-1-5386-9159-5"><bdi>978-1-5386-9159-5</bdi></a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a> <a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:49746072">49746072</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Data+Mining%2C+Workshop+on+High+Dimensional+Data+Mining&amp;rft.atitle=Maximally+Consistent+Sampling+and+the+Jaccard+Index+of+Probability+Distributions&amp;rft.pages=347-356&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1809.04052&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A49746072%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FICDM.2018.00050&amp;rft.isbn=978-1-5386-9159-5&amp;rft.aulast=Moulton&amp;rft.aufirst=R&amp;rft.au=Jiang%2C+Y&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span>For example <cite id="CITEREFHuihuanXinyuYangsheng2011">Huihuan Q, Xinyu W, Yangsheng X (2011). <i>Intelligent Surveillance Systems</i>. Springer. p. 161. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/978-94-007-1137-2" title="Special:BookSources/978-94-007-1137-2"><bdi>978-94-007-1137-2</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Intelligent+Surveillance+Systems&amp;rft.pages=161&amp;rft.pub=Springer&amp;rft.date=2011&amp;rft.isbn=978-94-007-1137-2&amp;rft.aulast=Huihuan&amp;rft.aufirst=Q&amp;rft.au=Xinyu%2C+W&amp;rft.au=Yangsheng%2C+X&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite id="CITEREFRogersTanimoto1960">Rogers DJ, Tanimoto TT (October 1960). &#34;A Computer Program for Classifying Plants&#34;. <i>Science</i>. <b>132</b> (3434): 1115–8. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1960Sci...132.1115R">1960Sci...132.1115R</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1126%2Fscience.132.3434.1115">10.1126/science.132.3434.1115</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/17790723">17790723</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=A+Computer+Program+for+Classifying+Plants&amp;rft.volume=132&amp;rft.issue=3434&amp;rft.pages=1115-8&amp;rft.date=1960-10&amp;rft_id=info%3Apmid%2F17790723&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.132.3434.1115&amp;rft_id=info%3Abibcode%2F1960Sci...132.1115R&amp;rft.aulast=Rogers&amp;rft.aufirst=DJ&amp;rft.au=Tanimoto%2C+TT&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFAziz_Taha2015">Aziz Taha, Abdel (2015). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4533825">&#34;Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool&#34;</a>. <i>BMC Medical Imaging</i>. <b>15</b> (29): 1–28. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1186%2Fs12880-015-0068-x">10.1186/s12880-015-0068-x</a></span>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a> <span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4533825">4533825</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a> <a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/26263899">26263899</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BMC+Medical+Imaging&amp;rft.atitle=Metrics+for+evaluating+3D+medical+image+segmentation%3A+analysis%2C+selection%2C+and+tool&amp;rft.volume=15&amp;rft.issue=29&amp;rft.pages=1-28&amp;rft.date=2015&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4533825%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F26263899&amp;rft_id=info%3Adoi%2F10.1186%2Fs12880-015-0068-x&amp;rft.aulast=Aziz+Taha&amp;rft.aufirst=Abdel&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4533825&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></span>
</li>
</ol></div></div>
<h2><span id="Further_reading">Further reading</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=12" title="Edit section: Further reading">edit</a><span>]</span></span></h2>
<div>
<ul><li><cite id="CITEREFTanSteinbachKumar2005">Tan PN, Steinbach M, Kumar V (2005). <i>Introduction to Data Mining</i>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/0-321-32136-7" title="Special:BookSources/0-321-32136-7"><bdi>0-321-32136-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Data+Mining&amp;rft.date=2005&amp;rft.isbn=0-321-32136-7&amp;rft.aulast=Tan&amp;rft.aufirst=PN&amp;rft.au=Steinbach%2C+M&amp;rft.au=Kumar%2C+V&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></li>
<li><cite id="CITEREFJaccard1901"><a href="https://en.wikipedia.org/wiki/Paul_Jaccard" title="Paul Jaccard">Jaccard P</a> (1901). &#34;Étude comparative de la distribution florale dans une portion des Alpes et des Jura&#34;. <i>Bulletin de la Société vaudoise des sciences naturelles</i>. <b>37</b>: 547–579.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+de+la+Soci%C3%A9t%C3%A9+vaudoise+des+sciences+naturelles&amp;rft.atitle=%C3%89tude+comparative+de+la+distribution+florale+dans+une+portion+des+Alpes+et+des+Jura&amp;rft.volume=37&amp;rft.pages=547-579&amp;rft.date=1901&amp;rft.aulast=Jaccard&amp;rft.aufirst=P&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></li>
<li><cite id="CITEREFJaccard1912"><a href="https://en.wikipedia.org/wiki/Paul_Jaccard" title="Paul Jaccard">Jaccard P</a> (1912). &#34;The Distribution of the flora in the alpine zone&#34;. <i>New Phytologist</i>. <b>11</b> (2): 37–50. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1111%2Fj.1469-8137.1912.tb05611.x">10.1111/j.1469-8137.1912.tb05611.x</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Phytologist&amp;rft.atitle=The+Distribution+of+the+flora+in+the+alpine+zone&amp;rft.volume=11&amp;rft.issue=2&amp;rft.pages=37-50&amp;rft.date=1912&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1469-8137.1912.tb05611.x&amp;rft.aulast=Jaccard&amp;rft.aufirst=P&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AJaccard+index"></span></li></ul>
</div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Jaccard_index&amp;action=edit&amp;section=13" title="Edit section: External links">edit</a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap2_data.pdf">Introduction to Data Mining lecture notes from Tan, Steinbach, Kumar</a></li>
<li><a rel="nofollow" href="http://sourceforge.net/projects/simmetrics/">SimMetrics a sourceforge implementation of Jaccard index and many other similarity metrics</a></li>
<li><a rel="nofollow" href="http://www.planetcalc.com/1664/">A web-based calculator for finding the Jaccard Coefficient</a></li>
<li><a rel="nofollow" href="http://www.gettingcirrius.com/2011/01/calculating-similarity-part-2-jaccard.html">Tutorial on how to calculate different similarities</a></li>
<li><a rel="nofollow" href="http://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">Intersection over Union (IoU) for object detection</a></li>
<li><a rel="nofollow" href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection#evaluation">Kaggle Dstl Satellite Imagery Feature Detection - Evaluation</a></li>
<li><a rel="nofollow" href="https://towardsdatascience.com/17-types-of-similarity-and-dissimilarity-measures-used-in-data-science-3eb914d2681">Similarity and dissimilarity measures used in data science</a></li></ul>

<!-- 
NewPP limit report
Parsed by mw2269
Cached time: 20230314042915
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc, no‐toc‐conversion]
CPU time usage: 0.582 seconds
Real time usage: 0.748 seconds
Preprocessor visited node count: 2123/1000000
Post‐expand include size: 62413/2097152 bytes
Template argument size: 1918/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 61484/5000000 bytes
Lua time usage: 0.280/10.000 seconds
Lua memory usage: 7144764/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  479.990      1 -total
 38.77%  186.072      1 Template:Reflist
 30.46%  146.214     12 Template:Cite_journal
 18.79%   90.187      1 Template:Machine_learning_evaluation_metrics
 18.21%   87.388      1 Template:Navbox
 13.37%   64.187      1 Template:More_footnotes_needed
 11.98%   57.493      1 Template:Short_description
 11.66%   55.951      1 Template:Ambox
  6.58%   31.581      2 Template:Pagetype
  4.50%   21.603      2 Template:Fix
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:2203756-0!canonical and timestamp 20230314042914 and revision id 1144140835. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> -->
</div></div>
  </body>
</html>
