<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md">Original</a>
    <h1>DeepSeek-R1-671B-Q4_K_M with 1 or 2 Arc A770 on Xeon</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><section aria-labelledby="file-name-id-wide file-name-id-mobile"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
   &lt; <b>English</b> | <a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.zh-CN.md">中文</a> &gt;
</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p dir="auto">You can now run <strong>DeepSeek-R1-671B-Q4_K_M</strong> with 1 or 2 Arc A770 on Xeon using the latest <a href="#flashmoe-for-deepseek-v3r1">llama.cpp Portable Zip</a>.</p>
</div>
<p dir="auto">This guide demonstrates how to use <a href="https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly">llama.cpp portable zip</a> to directly run llama.cpp on Intel GPU with <code>ipex-llm</code> (without the need of manual installations).</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">llama.cpp portable zip has been verified on:</p>
<ul dir="auto">
<li>Intel Core Ultra processors</li>
<li>Intel Core 11th - 14th gen processors</li>
<li>Intel Arc A-Series GPU</li>
<li>Intel Arc B-Series GPU</li>
</ul>
</div>

<ul dir="auto">
<li><a href="#windows-quickstart">Windows Quickstart</a>
<ul dir="auto">
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#step-1-download-and-unzip">Step 1: Download and Unzip</a></li>
<li><a href="#step-2-runtime-configuration">Step 2: Runtime Configuration</a></li>
<li><a href="#step-3-run-gguf-models">Step 3: Run GGUF models</a></li>
</ul>
</li>
<li><a href="#linux-quickstart">Linux Quickstart</a>
<ul dir="auto">
<li><a href="#prerequisites-1">Prerequisites</a></li>
<li><a href="#step-1-download-and-extract">Step 1: Download and Extract</a></li>
<li><a href="#step-2-runtime-configuration-1">Step 2: Runtime Configuration</a></li>
<li><a href="#step-3-run-gguf-models-1">Step 3: Run GGUF models</a></li>
<li><a href="#flashmoe-for-deepseek-v3r1">(New) FlashMoE for DeepSeek V3/R1 671B using llama.cpp</a></li>
</ul>
</li>
<li><a href="#tips--troubleshooting">Tips &amp; Troubleshooting</a>
<ul dir="auto">
<li><a href="#error-detected-different-sycl-devices">Error: Detected different sycl devices</a></li>
<li><a href="#multi-gpus-usage">Multi-GPUs usage</a></li>
<li><a href="#performance-environment">Performance Environment</a></li>
</ul>
</li>
<li><a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md">More Details</a></li>
</ul>


<p dir="auto">Check your GPU driver version, and update it if needed:</p>
<ul dir="auto">
<li>
<p dir="auto">For Intel Core Ultra processors (Series 2) or Intel Arc B-Series GPU, we recommend updating your GPU driver to the <a href="https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html" rel="nofollow">latest</a></p>
</li>
<li>
<p dir="auto">For other Intel iGPU/dGPU, we recommend using GPU driver version <a href="https://www.intel.com/content/www/us/en/download/785597/834050/intel-arc-iris-xe-graphics-windows.html" rel="nofollow">32.0.101.6078</a></p>
</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 1: Download and Unzip</h3><a id="user-content-step-1-download-and-unzip" aria-label="Permalink: Step 1: Download and Unzip" href="#step-1-download-and-unzip"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Download IPEX-LLM llama.cpp portable zip for Windows users from the <a href="https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly">link</a>.</p>
<p dir="auto">Then, extract the zip file to a folder.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 2: Runtime Configuration</h3><a id="user-content-step-2-runtime-configuration" aria-label="Permalink: Step 2: Runtime Configuration" href="#step-2-runtime-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Open &#34;Command Prompt&#34; (cmd), and enter the extracted folder through <code>cd /d PATH\TO\EXTRACTED\FOLDER</code></li>
<li>To use GPU acceleration, several environment variables are required or recommended before running <code>llama.cpp</code>.
<div dir="auto" data-snippet-clipboard-copy-content="set SYCL_CACHE_PERSISTENT=1"><pre><span>set</span> <span>SYCL_CACHE_PERSISTENT</span><span>=</span><span>1</span></pre></div>
</li>
<li>For multi-GPUs user, go to <a href="#multi-gpus-usage">Tips</a> for how to select specific GPU.</li>
</ul>

<p dir="auto">Here we provide a simple example to show how to run a community GGUF model with IPEX-LLM.</p>

<p dir="auto">Before running, you should download or copy community GGUF model to your local directory. For instance,  <code>DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf</code> of <a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf" rel="nofollow">bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF</a>.</p>

<p dir="auto">Please change <code>PATH\TO\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf</code> to your model path before your run below command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="llama-cli.exe -m PATH\TO\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf -p &#34;A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here &lt;/answer&gt;. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: &lt;think&gt;&#34; -n 2048  -t 8 -e -ngl 99 --color -c 2500 --temp 0"><pre>llama-cli.exe -m PATH\TO\DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf -p <span><span>&#34;</span>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here &lt;/answer&gt;. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: &lt;think&gt;<span>&#34;</span></span> -n <span>2048</span>  -t <span>8</span> -e -ngl <span>99</span> --color -c <span>2500</span> --temp <span>0</span></pre></div>
<p dir="auto">Part of outputs:</p>
<div data-snippet-clipboard-copy-content="Found 1 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                     Intel Arc Graphics|  12.71|    128|    1024|   32| 13578M|            1.3.27504|
llama_kv_cache_init:      SYCL0 KV buffer size =   138.25 MiB
llama_new_context_with_model: KV self size  =  138.25 MiB, K (f16):   69.12 MiB, V (f16):   69.12 MiB
llama_new_context_with_model:  SYCL_Host  output buffer size =     0.58 MiB
llama_new_context_with_model:      SYCL0 compute buffer size =  1501.00 MiB
llama_new_context_with_model:  SYCL_Host compute buffer size =    58.97 MiB
llama_new_context_with_model: graph nodes  = 874
llama_new_context_with_model: graph splits = 2
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 22 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |

sampler seed: 341519086
sampler params:
        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000
        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist

generate: n_ctx = 2528, n_batch = 4096, n_predict = 2048, n_keep = 1

&lt;think&gt;
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
&lt;/think&gt;

&lt;answer&gt;XXXX&lt;/answer&gt; [end of text]


llama_perf_sampler_print:    sampling time =     xxx.xx ms /  1386 runs   (    x.xx ms per token, xxxxx.xx tokens per second)
llama_perf_context_print:        load time =   xxxxx.xx ms
llama_perf_context_print: prompt eval time =     xxx.xx ms /   129 tokens (    x.xx ms per token,   xxx.xx tokens per second)
llama_perf_context_print:        eval time =   xxxxx.xx ms /  1256 runs   (   xx.xx ms per token,    xx.xx tokens per second)
llama_perf_context_print:       total time =   xxxxx.xx ms /  1385 tokens"><pre><code>Found 1 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                     Intel Arc Graphics|  12.71|    128|    1024|   32| 13578M|            1.3.27504|
llama_kv_cache_init:      SYCL0 KV buffer size =   138.25 MiB
llama_new_context_with_model: KV self size  =  138.25 MiB, K (f16):   69.12 MiB, V (f16):   69.12 MiB
llama_new_context_with_model:  SYCL_Host  output buffer size =     0.58 MiB
llama_new_context_with_model:      SYCL0 compute buffer size =  1501.00 MiB
llama_new_context_with_model:  SYCL_Host compute buffer size =    58.97 MiB
llama_new_context_with_model: graph nodes  = 874
llama_new_context_with_model: graph splits = 2
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 22 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |

sampler seed: 341519086
sampler params:
        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000
        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist

generate: n_ctx = 2528, n_batch = 4096, n_predict = 2048, n_keep = 1

&lt;think&gt;
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
&lt;/think&gt;

&lt;answer&gt;XXXX&lt;/answer&gt; [end of text]


llama_perf_sampler_print:    sampling time =     xxx.xx ms /  1386 runs   (    x.xx ms per token, xxxxx.xx tokens per second)
llama_perf_context_print:        load time =   xxxxx.xx ms
llama_perf_context_print: prompt eval time =     xxx.xx ms /   129 tokens (    x.xx ms per token,   xxx.xx tokens per second)
llama_perf_context_print:        eval time =   xxxxx.xx ms /  1256 runs   (   xx.xx ms per token,    xx.xx tokens per second)
llama_perf_context_print:       total time =   xxxxx.xx ms /  1385 tokens
</code></pre></div>


<p dir="auto">Check your GPU driver version, and update it if needed; we recommend following <a href="https://dgpu-docs.intel.com/driver/client/overview.html" rel="nofollow">Intel client GPU driver installation guide</a> to install your GPU driver.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 1: Download and Extract</h3><a id="user-content-step-1-download-and-extract" aria-label="Permalink: Step 1: Download and Extract" href="#step-1-download-and-extract"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Download IPEX-LLM llama.cpp portable tgz for Linux users from the <a href="https://github.com/intel/ipex-llm/releases/tag/v2.2.0-nightly">link</a>.</p>
<p dir="auto">Then, extract the tgz file to a folder.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 2: Runtime Configuration</h3><a id="user-content-step-2-runtime-configuration-1" aria-label="Permalink: Step 2: Runtime Configuration" href="#step-2-runtime-configuration-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Open a &#34;Terminal&#34;, and enter the extracted folder through <code>cd /PATH/TO/EXTRACTED/FOLDER</code></li>
<li>To use GPU acceleration, several environment variables are required or recommended before running <code>llama.cpp</code>.
<div dir="auto" data-snippet-clipboard-copy-content="export SYCL_CACHE_PERSISTENT=1"><pre><span>export</span> SYCL_CACHE_PERSISTENT=1</pre></div>
</li>
<li>For multi-GPUs user, go to <a href="#multi-gpus-usage">Tips</a> for how to select specific GPU.</li>
</ul>

<p dir="auto">Here we provide a simple example to show how to run a community GGUF model with IPEX-LLM.</p>

<p dir="auto">Before running, you should download or copy community GGUF model to your local directory. For instance,  <code>DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf</code> of <a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf" rel="nofollow">bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF</a>.</p>

<p dir="auto">Please change <code>/PATH/TO/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf</code> to your model path before your run below command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="./llama-cli -m /PATH/TO/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf -p &#34;A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here &lt;/answer&gt;. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: &lt;think&gt;&#34; -n 2048  -t 8 -e -ngl 99 --color -c 2500 --temp 0"><pre>./llama-cli -m /PATH/TO/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf -p <span><span>&#34;</span>A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here &lt;/answer&gt;. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: &lt;think&gt;<span>&#34;</span></span> -n 2048  -t 8 -e -ngl 99 --color -c 2500 --temp 0</pre></div>
<p dir="auto">Part of outputs:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Found 1 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                     Intel Arc Graphics|  12.71|    128|    1024|   32| 13578M|            1.3.27504|
llama_kv_cache_init:      SYCL0 KV buffer size =   138.25 MiB
llama_new_context_with_model: KV self size  =  138.25 MiB, K (f16):   69.12 MiB, V (f16):   69.12 MiB
llama_new_context_with_model:  SYCL_Host  output buffer size =     0.58 MiB
llama_new_context_with_model:      SYCL0 compute buffer size =  1501.00 MiB
llama_new_context_with_model:  SYCL_Host compute buffer size =    58.97 MiB
llama_new_context_with_model: graph nodes  = 874
llama_new_context_with_model: graph splits = 2
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 22 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |

sampler seed: 341519086
sampler params:
        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000
        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist

generate: n_ctx = 2528, n_batch = 4096, n_predict = 2048, n_keep = 1

&lt;think&gt;
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
&lt;/think&gt;

&lt;answer&gt;XXXX&lt;/answer&gt; [end of text]"><pre>Found 1 SYCL devices:
<span>|</span>  <span>|</span>                   <span>|</span>                                       <span>|</span>       <span>|</span>Max    <span>|</span>        <span>|</span>Max  <span>|</span>Global <span>|</span>                     <span>|</span>
<span>|</span>  <span>|</span>                   <span>|</span>                                       <span>|</span>       <span>|</span>compute<span>|</span>Max work<span>|</span>sub  <span>|</span>mem    <span>|</span>                     <span>|</span>
<span>|</span>ID<span>|</span>        Device Type<span>|</span>                                   Name<span>|</span>Version<span>|</span>units  <span>|</span>group   <span>|</span>group<span>|</span>size   <span>|</span>       Driver version<span>|</span>
<span>|</span>--<span>|</span>-------------------<span>|</span>---------------------------------------<span>|</span>-------<span>|</span>-------<span>|</span>--------<span>|</span>-----<span>|</span>-------<span>|</span>---------------------<span>|</span>
<span>|</span> 0<span>|</span> [level_zero:gpu:0]<span>|</span>                     Intel Arc Graphics<span>|</span>  12.71<span>|</span>    128<span>|</span>    1024<span>|</span>   32<span>|</span> 13578M<span>|</span>            1.3.27504<span>|</span>
llama_kv_cache_init:      SYCL0 KV buffer size =   138.25 MiB
llama_new_context_with_model: KV self size  =  138.25 MiB, K (f16):   69.12 MiB, V (f16):   69.12 MiB
llama_new_context_with_model:  SYCL_Host  output buffer size =     0.58 MiB
llama_new_context_with_model:      SYCL0 compute buffer size =  1501.00 MiB
llama_new_context_with_model:  SYCL_Host compute buffer size =    58.97 MiB
llama_new_context_with_model: graph nodes  = 874
llama_new_context_with_model: graph splits = 2
common_init_from_params: warming up the model with an empty run - please <span>wait</span> ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 22 <span>|</span> CPU <span>:</span> SSE3 = 1 <span>|</span> SSSE3 = 1 <span>|</span> AVX = 1 <span>|</span> AVX2 = 1 <span>|</span> F16C = 1 <span>|</span> FMA = 1 <span>|</span> LLAMAFILE = 1 <span>|</span> OPENMP = 1 <span>|</span> AARCH64_REPACK = 1 <span>|</span>

sampler seed: 341519086
sampler params:
        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.000
        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -<span>&gt;</span> logit-bias -<span>&gt;</span> penalties -<span>&gt;</span> dry -<span>&gt;</span> top-k -<span>&gt;</span> typical -<span>&gt;</span> top-p -<span>&gt;</span> min-p -<span>&gt;</span> xtc -<span>&gt;</span> temp-ext -<span>&gt;</span> dist

generate: n_ctx = 2528, n_batch = 4096, n_predict = 2048, n_keep = 1

<span>&lt;</span>think<span>&gt;</span>
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
<span>&lt;</span>/think<span>&gt;</span>

<span>&lt;</span>answer<span>&gt;</span>XXXX<span>&lt;</span>/answer<span>&gt;</span> [end of text]</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">FlashMoE for DeepSeek V3/R1</h3><a id="user-content-flashmoe-for-deepseek-v3r1" aria-label="Permalink: FlashMoE for DeepSeek V3/R1" href="#flashmoe-for-deepseek-v3r1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">FlashMoE is a command-line tool built on llama.cpp, optimized for mixture-of-experts (MoE) models such as DeepSeek V3/R1. Now, it&#39;s available for Linux platforms.</p>
<p dir="auto">Tested MoE GGUF Models (other MoE GGUF models are also supported):</p>
<ul dir="auto">
<li><a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M" rel="nofollow">DeepSeek-V3-Q4_K_M</a></li>
<li><a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q6_K" rel="nofollow">DeepSeek-V3-Q6_K</a></li>
<li><a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q4_K_M" rel="nofollow">DeepSeek-R1-Q4_K_M.gguf</a></li>
<li><a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q6_K" rel="nofollow">DeepSeek-R1-Q6_K</a></li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">Run DeepSeek V3/R1 with FlashMoE</h4><a id="user-content-run-deepseek-v3r1-with-flashmoe" aria-label="Permalink: Run DeepSeek V3/R1 with FlashMoE" href="#run-deepseek-v3r1-with-flashmoe"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Requirements:</p>
<ul dir="auto">
<li>380GB CPU Memory</li>
<li>1-8 ARC A770</li>
<li>500GB Disk</li>
</ul>
<p dir="auto">Note:</p>
<ul dir="auto">
<li>Larger models and other precisions may require more resources.</li>
<li>For 1 ARC A770 platform, please reduce context length (e.g., 1024) to avoid OOM. Add this option <code>-c 1024</code> at the end of below command.</li>
</ul>
<p dir="auto">Before running, you should download or copy community GGUF model to your local directory. For instance,  <code>DeepSeek-R1-Q4_K_M.gguf</code> of <a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-Q4_K_M" rel="nofollow">DeepSeek-R1-Q4_K_M.gguf</a>.</p>
<p dir="auto">Change <code>/PATH/TO/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf</code> to your model path, then run <code>DeepSeek-R1-Q4_K_M.gguf</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="./flash-moe -m /PATH/TO/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf --prompt &#34;What&#39;s AI?&#34;"><pre>./flash-moe -m /PATH/TO/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf --prompt <span><span>&#34;</span>What&#39;s AI?<span>&#34;</span></span></pre></div>
<p dir="auto">Part of outputs</p>
<div dir="auto" data-snippet-clipboard-copy-content="llama_kv_cache_init:      SYCL0 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL1 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL2 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL3 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL4 KV buffer size =  1120.00 MiB
llama_kv_cache_init:      SYCL5 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL6 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL7 KV buffer size =   960.00 MiB
llama_new_context_with_model: KV self size  = 9760.00 MiB, K (i8): 5856.00 MiB, V (i8): 3904.00 MiB
llama_new_context_with_model:  SYCL_Host  output buffer size =     0.49 MiB
llama_new_context_with_model: pipeline parallelism enabled (n_copies=1)
llama_new_context_with_model:      SYCL0 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL1 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL2 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL3 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL4 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL5 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL6 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL7 compute buffer size =  3264.00 MiB
llama_new_context_with_model:  SYCL_Host compute buffer size =  1332.05 MiB
llama_new_context_with_model: graph nodes  = 5184 (with bs=4096), 4720 (with bs=1)
llama_new_context_with_model: graph splits = 125
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 48

system_info: n_threads = 48 (n_threads_batch = 48) / 192 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |

sampler seed: 2052631435
sampler params:
        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist
generate: n_ctx = 4096, n_batch = 4096, n_predict = -1, n_keep = 1

&lt;think&gt;
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
&lt;/think&gt;

&lt;answer&gt;XXXX&lt;/answer&gt; [end of text]"><pre>llama_kv_cache_init:      SYCL0 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL1 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL2 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL3 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL4 KV buffer size =  1120.00 MiB
llama_kv_cache_init:      SYCL5 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL6 KV buffer size =  1280.00 MiB
llama_kv_cache_init:      SYCL7 KV buffer size =   960.00 MiB
llama_new_context_with_model: KV self size  = 9760.00 MiB, K (i8): 5856.00 MiB, V (i8): 3904.00 MiB
llama_new_context_with_model:  SYCL_Host  output buffer size =     0.49 MiB
llama_new_context_with_model: pipeline parallelism enabled (n_copies=1)
llama_new_context_with_model:      SYCL0 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL1 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL2 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL3 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL4 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL5 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL6 compute buffer size =  2076.02 MiB
llama_new_context_with_model:      SYCL7 compute buffer size =  3264.00 MiB
llama_new_context_with_model:  SYCL_Host compute buffer size =  1332.05 MiB
llama_new_context_with_model: graph nodes  = 5184 (with bs=4096), 4720 (with bs=1)
llama_new_context_with_model: graph splits = 125
common_init_from_params: warming up the model with an empty run - please <span>wait</span> ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 48

system_info: n_threads = 48 (n_threads_batch = 48) / 192 <span>|</span> CPU <span>:</span> SSE3 = 1 <span>|</span> SSSE3 = 1 <span>|</span> AVX = 1 <span>|</span> AVX_VNNI = 1 <span>|</span> AVX2 = 1 <span>|</span> F16C = 1 <span>|</span> FMA = 1 <span>|</span> LLAMAFILE = 1 <span>|</span> OPENMP = 1 <span>|</span> AARCH64_REPACK = 1 <span>|</span>

sampler seed: 2052631435
sampler params:
        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
        dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
        top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -<span>&gt;</span> logit-bias -<span>&gt;</span> penalties -<span>&gt;</span> dry -<span>&gt;</span> top-k -<span>&gt;</span> typical -<span>&gt;</span> top-p -<span>&gt;</span> min-p -<span>&gt;</span> xtc -<span>&gt;</span> temp-ext -<span>&gt;</span> dist
generate: n_ctx = 4096, n_batch = 4096, n_predict = -1, n_keep = 1

<span>&lt;</span>think<span>&gt;</span>
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
<span>&lt;</span>/think<span>&gt;</span>

<span>&lt;</span>answer<span>&gt;</span>XXXX<span>&lt;</span>/answer<span>&gt;</span> [end of text]</pre></div>

<div dir="auto"><h3 tabindex="-1" dir="auto">Error: Detected different sycl devices</h3><a id="user-content-error-detected-different-sycl-devices" aria-label="Permalink: Error: Detected different sycl devices" href="#error-detected-different-sycl-devices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You will meet error log like below:</p>
<div data-snippet-clipboard-copy-content="Found 3 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 1| [level_zero:gpu:1]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 2| [level_zero:gpu:2]|                 Intel UHD Graphics 770|   12.2|     32|     512|   32| 63218M|     1.6.31907.700000|
Error: Detected different sycl devices, the performance will limit to the slowest device. 
If you want to disable this checking and use all of them, please set environment SYCL_DEVICE_CHECK=0, and try again.
If you just want to use one of the devices, please set environment like ONEAPI_DEVICE_SELECTOR=level_zero:0 or ONEAPI_DEVICE_SELECTOR=level_zero:1 to choose your devices.
If you want to use two or more deivces, please set environment like ONEAPI_DEVICE_SELECTOR=&#34;level_zero:0;level_zero:1&#34;
See https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Overview/KeyFeatures/multi_gpus_selection.md for details. Exiting."><pre><code>Found 3 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 1| [level_zero:gpu:1]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 2| [level_zero:gpu:2]|                 Intel UHD Graphics 770|   12.2|     32|     512|   32| 63218M|     1.6.31907.700000|
Error: Detected different sycl devices, the performance will limit to the slowest device. 
If you want to disable this checking and use all of them, please set environment SYCL_DEVICE_CHECK=0, and try again.
If you just want to use one of the devices, please set environment like ONEAPI_DEVICE_SELECTOR=level_zero:0 or ONEAPI_DEVICE_SELECTOR=level_zero:1 to choose your devices.
If you want to use two or more deivces, please set environment like ONEAPI_DEVICE_SELECTOR=&#34;level_zero:0;level_zero:1&#34;
See https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Overview/KeyFeatures/multi_gpus_selection.md for details. Exiting.
</code></pre></div>
<p dir="auto">Because the GPUs are not the same, the jobs will be allocated according to device&#39;s memory. Upon example, the iGPU(Intel UHD Graphics 770) will get 2/3 of the computing tasks. The performance will be quit bad. So you have below two choices:</p>
<ol dir="auto">
<li>Disable the iGPU will get the best performance. Visit <a href="#multi-gpus-usage">Multi-GPUs usage</a> for details.</li>
<li>Disable this check and use all of them, you can run below command:
<ul dir="auto">
<li><code>set SYCL_DEVICE_CHECK=0</code> (Windows user)</li>
<li><code>export SYCL_DEVICE_CHECK=0</code> (Linux user)</li>
</ul>
</li>
</ol>

<p dir="auto">If your machine has multiple Intel GPUs, llama.cpp will by default runs on all of them. If you are not clear about your hardware configuration, you can get the configuration when you run a GGUF model. Like:</p>
<div data-snippet-clipboard-copy-content="Found 3 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 1| [level_zero:gpu:1]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 2| [level_zero:gpu:2]|                 Intel UHD Graphics 770|   12.2|     32|     512|   32| 63218M|     1.6.31907.700000|"><pre><code>Found 3 SYCL devices:
|  |                   |                                       |       |Max    |        |Max  |Global |                     |
|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 1| [level_zero:gpu:1]|                Intel Arc A770 Graphics|  12.55|    512|    1024|   32| 16225M|     1.6.31907.700000|
| 2| [level_zero:gpu:2]|                 Intel UHD Graphics 770|   12.2|     32|     512|   32| 63218M|     1.6.31907.700000|
</code></pre></div>
<p dir="auto">To specify which Intel GPU you would like llama.cpp to use, you could set environment variable <code>ONEAPI_DEVICE_SELECTOR</code> <strong>before starting llama.cpp command</strong>, as follows:</p>
<ul dir="auto">
<li>For <strong>Windows</strong> users:
<div dir="auto" data-snippet-clipboard-copy-content="set ONEAPI_DEVICE_SELECTOR=level_zero:0 (If you want to run on one GPU, llama.cpp will use the first GPU.) 
set ONEAPI_DEVICE_SELECTOR=&#34;level_zero:0;level_zero:1&#34; (If you want to run on two GPUs, llama.cpp will use the first and second GPUs.)"><pre><span>set</span> <span>ONEAPI_DEVICE_SELECTOR</span><span>=</span>level_zero:0 (If you want to run on one GPU, llama.cpp will use the first GPU.) 
<span>set</span> <span>ONEAPI_DEVICE_SELECTOR</span><span>=</span><span><span>&#34;</span>level_zero:0;level_zero:1<span>&#34;</span></span> (If you want to run on two GPUs, llama.cpp will use the first and second GPUs.)</pre></div>
</li>
<li>For <strong>Linux</strong> users:
<div dir="auto" data-snippet-clipboard-copy-content="export ONEAPI_DEVICE_SELECTOR=level_zero:0 (If you want to run on one GPU, llama.cpp will use the first GPU.) 
export ONEAPI_DEVICE_SELECTOR=&#34;level_zero:0;level_zero:1&#34; (If you want to run on two GPUs, llama.cpp will use the first and second GPUs.)"><pre><span>export</span> ONEAPI_DEVICE_SELECTOR=level_zero:0 (If you want to run on one GPU, llama.cpp will use the first GPU.) 
<span>export</span> ONEAPI_DEVICE_SELECTOR=<span><span>&#34;</span>level_zero:0;level_zero:1<span>&#34;</span></span> (If you want to run on two GPUs, llama.cpp will use the first and second GPUs.)</pre></div>
</li>
</ul>

<div dir="auto"><h4 tabindex="-1" dir="auto">SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS</h4><a id="user-content-sycl_pi_level_zero_use_immediate_commandlists" aria-label="Permalink: SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS" href="#sycl_pi_level_zero_use_immediate_commandlists"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To enable SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS, you can run below command:</p>
<ul dir="auto">
<li><code>set SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1</code>(Windows user)</li>
<li><code>export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1</code>(Linux user)</li>
</ul>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">The environment variable SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS determines the usage of immediate command lists for task submission to the GPU. While this mode typically enhances performance, exceptions may occur. Please consider experimenting with and without this environment variable for best performance. For more details, you can refer to <a href="https://www.intel.com/content/www/us/en/developer/articles/guide/level-zero-immediate-command-lists.html" rel="nofollow">this article</a>.</p>
</div>
</article></div></section></div></div></div></div>
  </body>
</html>
