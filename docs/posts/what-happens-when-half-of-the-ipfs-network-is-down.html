<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.ipfs.tech/2023-ipfs-unresponsive-nodes/">Original</a>
    <h1>What happens when half of the IPFS network is down?</h1>
    
    <div id="readability-page-1" class="page"><div><!----> <div itemprop="articleBody"><p>It depends on what type of system/network you‚Äôre running. In 90% of networks, or networked systems, this is a grand-scale disaster. Alerts are popping up everywhere, engineers go far beyond ‚Äúday-time work‚Äù to get things back to normal, customers are panicking and potentially leaving the platform and the customer care lines are on fire. Half of the network is a large fraction, but I would bet that the same would happen even when 10% or 20% of the network experiences an outage.</p> <p>It‚Äôs not like that when you run your services on a decentralized, distributed P2P network, such as IPFS! At the beginning of 2023, a critical component of the IPFS network, namely the public IPFS DHT, experienced a large-scale incident. <em>During this incident, <a href="https://github.com/protocol/network-measurements/blob/master/reports/2023/calendar-week-04/ipfs/plots/crawl-unresponsive.png" target="_blank" rel="noopener noreferrer">60% of the IPFS DHT Server nodes became unresponsive<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>.</em> Interestingly, <strong>no content became unreachable and almost nothing in the network looked like the majority of the network was basically down</strong>. We did observe a significant increase in the content routing/resolution latency (in the order of 25% initially), but this in no way reflected the scale of the event.</p> <p>In this blog post, we‚Äôll go through the timeline of the event from ‚ÄúDetection‚Äù to ‚ÄúRoot Cause Analysis‚Äù and give details about the engineering team‚Äôs response. A summarizing talk on the content of this blog post was given at <a href="https://2023.ipfs-thing.io/" target="_blank" rel="noopener noreferrer">IPFS Thing 2023<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> and can be found <a href="https://youtu.be/8cGEjdCfm14" target="_blank" rel="noopener noreferrer">on YouTube<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>.</p> <h2 id="‚ùóÔ∏èdetection-we-ve-got-a-problem"><a href="#‚ùóÔ∏èdetection-we-ve-got-a-problem">#</a> ‚ùóÔ∏èDetection: we&#39;ve got a problem!</h2> <blockquote><p>At the beginning of 2023, a critical component of the IPFS network, namely the public IPFS DHT, experienced a large-scale malfunction. <em>During this situation, <a href="https://github.com/protocol/network-measurements/blob/master/reports/2023/calendar-week-04/ipfs/plots/crawl-unresponsive.png" target="_blank" rel="noopener noreferrer">60% of the IPFS DHT Server nodes became unresponsive<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>.</em></p></blockquote> <p>Unresponsive here means that nodes would seem to be online, they would accept connections from other nodes, but they wouldn‚Äôt reply to requests. Basically, when a node would try to write to one of the unresponsive nodes, the unresponsive node would terminate the connection immediately.</p> <p>Given that these nodes seemed to be functional, they occupied several places in other nodes‚Äô routing tables, when in fact they shouldn‚Äôt have.</p> <p>The problem came down to a misconfiguration of the go-libp2p resource manager - a new feature that shipped with <code>kubo-v0.17</code>. The problematic configuration which was applied manually (i.e. was not based on the default values of <code>kubo-v0.17</code>) was set to such values that any attempt to interact with the nodes would be flagged as a resource exhaustion event and would trigger the corresponding ‚Äúdefense‚Äù mechanism. In practice, this materialized as a connection tear-down. It is worth noting that <code>kubo</code> is the most prevalent IPFS implementation using the public IPFS DHT with ~80% of nodes in the DHT being <code>kubo</code> nodes (see most recent <a href="https://github.com/protocol/network-measurements/tree/master/reports/2023/calendar-week-17/ipfs#agent-version-analysis" target="_blank" rel="noopener noreferrer">stats<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>).</p> <p>Content was still findable through kubo, so no alarms were raised. However, some of our research teams observed unusual error messages:</p> <p>Since PUT and GET operations were completing successfully, the error didn‚Äôt seem like one that would trigger widespread panic. We were seeing slower performance than normal and had been investigating whether <a href="https://discuss.ipfs.tech/t/dht-hydra-peers-dialling-down-non-bridging-functionality-on-2022-12-01/15567" target="_blank" rel="noopener noreferrer">recent changes with Hydra boosters<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> had bigger impacts than we were expecting. It was at this time that we had a physical meeting of our engineering teams and one of the items on the agenda was to figure out where this error was coming from.</p> <h2 id="‚ùì-diagnosis-what-was-happening"><a href="#‚ùì-diagnosis-what-was-happening">#</a> ‚ùì Diagnosis: what was happening?</h2> <p>We quickly realized that <a href="https://github.com/libp2p/go-libp2p/issues/1928" target="_blank" rel="noopener noreferrer">there was a resource manager issue where the remote node was hitting a limit and closing the connection<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>. After looking into the details of the resource manager and the error itself (i.e., <code>cannot reserve **in**bound connection</code>), we realized that the root cause of the issue was related to the remote node. It turned out that the resource manager was manually misconfigured by a very large percentage of nodes to values that were not in the default configuration by the ‚Äúvanilla‚Äù version of the resource manager that shipped with <code>kubo-v0.17</code>.</p> <p>As mentioned earlier, the GET and PUT operations were completing successfully, so our next step was to identify the scale of the problem. Our main goals were to figure out:</p> <ul><li>what percentage of nodes in the network were affected</li> <li>if there was a performance penalty in either the PUT or the GET operation, or both</li></ul> <p>Through a combination of crawling the network and attempting connections to all ~50k DHT Server nodes (i.e., those that store and serve provider records and content), we found that close to 60% of the network had been affected by the misconfiguration. Clearly this was a very large percentage of the network, which made it urgent to look into the performance impact. We followed the below methodology:</p> <ol><li><p>We wanted to figure out which buckets in the nodes‚Äô routing tables did the affected nodes occupy. We found that they occupied the higher buckets of the nodes‚Äô routing tables, which meant that most likely PUTs would get slower, but GETs should not be affected too much. This is because the DHT lookup from the GET operation terminates when it hits <em>one</em> of the 20 closest peers to the target key, while the PUT operation terminates when it has found <em>all</em> the 20 closest peers. Since a significant portion of the network was unresponsive, the PUT operation hit at least one unresponsive node, but the GET operation had good chances of finding at least one responsive node within the 20 closest.</p> <p><img src="https://blog.ipfs.tech/assets/img/output.f139e8e4.png" alt="output.png" loading="lazy"/></p></li> <li><p>After further investigation and given the very large percentage of nodes that were affected by the resource manager misconfiguration, we started looking into the impact of the incident to the GET performance.</p> <p>A GET request that hits one of the affected, unresponsive nodes would get the connection shut down by the remote, but would get stuck there until it timed out, at which point it would re-issue the request to another peer. The relatively high concurrency factor of the IPFS DHT (<code>alpha = 10</code>) helps in this case, as it means that for any given request up to 10 concurrent requests can be in flight. This helps a lot even with a high percentage of unresponsive nodes as it means that at least one of the 10 peers contacted will respond.</p> <blockquote><p>This is because the DHT lookup from the GET operation terminates when it hits one of the 20 closest peers to the target key, when the PUT operation terminates when it has found all the 20 closest peers.</p></blockquote> </li> <li><p>To quantify the impact, we crawled the network and gathered the PeerIDs of unresponsive nodes. We set up six kubo nodes in several locations around the globe and attempted to: i) publish content (PUT), and, ii) retrieve content (GET) for two cases: 1) when interacting with all nodes in the network, and, 2) when ignoring all responses from the unresponsive peers, whose PeerIDs we knew and were cross-checking with in real time.</p> <ul><li>The results we found were as follows:
<ul><li>The PUT operation was slowed down by approximately 10%</li> <li>The GET operation was also disrupted (in contrast to our initial assumption) and was slowed down by approximately 15%, at times reaching closer to 20%.</li></ul></li></ul></li> <li><p>We also experimented with even higher concurrency factors, in particular with <code>alpha = 20</code>, as a potential mitigation strategy. We repeated the same experiment with one extra set of runs: the case where we interact with all nodes in the network (i.e., we do not ignore unresponsive peers), but have higher concurrency factor.</p> </li></ol> <h2 id="üöë-mitigation-how-we-stopped-the-bleeding"><a href="#üöë-mitigation-how-we-stopped-the-bleeding">#</a> üöë Mitigation: how we stopped the bleeding.</h2> <p>The team‚Äôs immediate focus became:</p> <ol><li><a href="https://github.com/ipfs/kubo/blob/master/docs/libp2p-resource-management.md" target="_blank" rel="noopener noreferrer">Adding/updating documentation on Kubo‚Äôs resource manager integration<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a></li> <li>Triaging and responding to user questions/issues (<a href="https://github.com/ipfs/kubo/issues/9432" target="_blank" rel="noopener noreferrer">example<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>)</li> <li>Preparing a new kubo release (<code>v0.18.1</code>), where the default settings for the resource manager were set to more appropriate values. This reduced the likelihood that someone would need to adjust the resource manager configuration manually, thus avoiding the configuration ‚Äúfootguns‚Äù.</li> <li>Encouraging as many nodes as possible to upgrade through public forums and direct relationships with known larger scale operators.</li></ol> <p>In parallel, we kept monitoring the situation by instrumenting a PUT and GET measurement experiment that was running since before the <code>kubo-v0.18.1</code> update, when the affected nodes started updating gradually.</p> <p><code>kubo-v0.18.1</code> was <a href="https://github.com/ipfs/kubo/releases/tag/v0.18.1" target="_blank" rel="noopener noreferrer">released on the 2023-01-30<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> and within the first 10 days, more than 8.5k nodes updated to this release. Our monitoring software allowed us to have an accurate view of the state of the network and observed that the update to the new kubo release brought significant performance increase for the GET operation - more than 40% at the 95th percentile on a sample of ~2k requests, compared to the situation before the <code>kubo-v0.18.1</code> release.</p> <p><img src="https://blog.ipfs.tech/assets/img/output_2.96f968bb.png" alt="output.png" loading="lazy"/></p> <p>We also monitored the situation compared to the pre-incident performance by running the experiment where we ignored the set of PeerIDs that were identified as affected by the misconfiguration. As a sample from more than 20k GET operations, in the figure below we show that the impact has reduced to ~5% (mid-February 2023).</p> <p><img src="https://blog.ipfs.tech/assets/img/output_3.8197e92e.png" alt="output.png" loading="lazy"/></p> <h2 id="üîß-addressing-the-root-cause"><a href="#üîß-addressing-the-root-cause">#</a> üîß Addressing the Root Cause</h2> <p>Our immediate actions managed to stop the bleeding and bring the network back to normal quickly. However, it was clear that we had to implement longer term fixes to protect the nodes‚Äô routing tables from unresponsive peers and to avoid inadvertently making nodes unresponsive. Specifically this translated to:</p> <ol><li>Revamping the Kubo resource manager UX to further reduce the likelihood of catastrophic misconfiguration. This was completed in <a href="https://github.com/ipfs/kubo/releases/tag/v0.19.0#improving-the-libp2p-resource-management-integration" target="_blank" rel="noopener noreferrer">Kubo 0.19<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>.</li> <li>Only adding peers to the routing table that are responsive requests <a href="https://github.com/libp2p/go-libp2p-kad-dht/pull/810" target="_blank" rel="noopener noreferrer">during the routing table refresh<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> (done) and <a href="https://github.com/libp2p/go-libp2p-kad-dht/issues/811" target="_blank" rel="noopener noreferrer">upon adding a node to the routing table<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a> (in progress - targeting <a href="https://github.com/ipfs/kubo/issues/9814" target="_blank" rel="noopener noreferrer">Kubo 0.21 in May<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>).</li></ol> <h2 id="üìñ-lessons-learned"><a href="#üìñ-lessons-learned">#</a> üìñ Lessons Learned</h2> <p>In the days since, we have come away from this experience with several important learnings:</p> <p>üóíÔ∏è Significant fundamental changes to the codebase (such as retroactively adding resource accounting) is ripe for disruption. This increases the necessity for documentation, announcements, and clear recommendations to node operators.</p> <p>‚è±Ô∏è¬†Monitoring software should always be in place to help identify such events from the start.</p> <p>üì£ It is challenging to monitor and apply changes directly to the software that runs on nodes of a decentralized network. Well-established communication channels go a long way and help the engineering teams communicate directly with the community. In IPFS, we use a variety of channels including the Discord Server [<a href="https://discord.gg/ipfs" target="_blank" rel="noopener noreferrer">invite link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>], Filecoin Slack [<a href="https://filecoin.io/slack" target="_blank" rel="noopener noreferrer">invite link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>] (mostly in <code>#engres-ip-stewards</code> channel), the <a href="https://discuss.ipfs.tech/" target="_blank" rel="noopener noreferrer">Discourse discussion forum<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>, and the <a href="https://blog.ipfs.tech/" target="_blank" rel="noopener noreferrer">blog<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span>(opens new window)</span></span></a>.</p> <p>üöÄ¬†Last, but certainly not least, the decentralized, P2P nature of IPFS kept the network running with all important operations completing successfully (albeit slower than normal). It is exactly because of the structure of the network that there are no single points of failure and performance is not catastrophically disrupted even when more than half of the network nodes are essentially unresponsive.</p></div> <!----></div></div>
  </body>
</html>
