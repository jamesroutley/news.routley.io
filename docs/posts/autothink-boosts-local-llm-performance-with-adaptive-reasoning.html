<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=44112326">Original</a>
    <h1>Show HN: AutoThink â€“ Boosts local LLM performance with adaptive reasoning</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>I built AutoThink, a technique that makes local LLMs reason more efficiently by adaptively allocating computational resources based on query complexity.</p><p>The core idea: instead of giving every query the same &#34;thinking time,&#34; classify queries as HIGH or LOW complexity and allocate thinking tokens accordingly. Complex reasoning gets 70-90% of tokens, simple queries get 20-40%.</p><p>I also implemented steering vectors derived from Pivotal Token Search (originally from Microsoft&#39;s Phi-4 paper) that guide the model&#39;s reasoning patterns during generation. These vectors encourage behaviors like numerical accuracy, self-correction, and thorough exploration.</p><p>Results on DeepSeek-R1-Distill-Qwen-1.5B:</p><p>- GPQA-Diamond: 31.06% vs 21.72% baseline (+43% relative improvement)</p><p>- MMLU-Pro: 26.38% vs 25.58% baseline</p><p>- Uses fewer tokens than baseline approaches</p><p>Works with any local reasoning model - DeepSeek, Qwen, custom fine-tuned models. No API dependencies.</p><p>The technique builds on two things I developed: an adaptive classification framework that can learn new complexity categories without retraining, and an open source implementation of Pivotal Token Search.</p><p>Technical paper: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327" rel="nofollow">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327</a></p><p>Code and examples: <a href="https://github.com/codelion/optillm/tree/main/optillm/autothink">https://github.com/codelion/optillm/tree/main/optillm/autoth...</a></p><p>PTS implementation: <a href="https://github.com/codelion/pts">https://github.com/codelion/pts</a></p><p>I&#39;m curious about your thoughts on adaptive resource allocation for AI reasoning. Have you tried similar approaches with your local models?</p></div></div></div>
  </body>
</html>
