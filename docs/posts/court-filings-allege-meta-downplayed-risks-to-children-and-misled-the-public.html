<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://time.com/7336204/meta-lawsuit-files-child-safety/">Original</a>
    <h1>Court filings allege Meta downplayed risks to children and misled the public</h1>
    
    <div id="readability-page-1" class="page"><div><article id="article-body"><p data-testid="paragraph-content">Sex trafficking on Meta platforms was both difficult to report and widely tolerated, according to a court filing unsealed Friday. In a plaintiffs’ brief filed as part of a major lawsuit against four social media companies, Instagram’s former head of safety and well-being Vaishnavi Jayakumar testified that when she joined Meta in 2020 she was shocked to learn that the company had a “17x” strike policy for accounts that reportedly engaged in the “trafficking of humans for sex.” </p><div><div data-ad-wrapper="true"></div></div><div><p data-testid="paragraph-content">“You could incur 16 violations for prostitution and sexual solicitation, and upon the 17th violation, your account would be suspended,” Jayakumar reportedly testified, adding that “by any measure across the industry, [it was] a very, very high strike threshold.” The plaintiffs claim that this testimony is corroborated by internal company documentation.</p><p data-testid="paragraph-content">The brief, filed by plaintiffs in the Northern District of California, alleges that Meta was aware of serious harms on its platform and engaged in a broad pattern of deceit to downplay risks to young users. According to the brief, Meta was aware that millions of adult strangers were contacting minors on its sites; that its products exacerbated mental health issues in teens; and that content related to eating disorders, suicide, and child sexual abuse was frequently detected, yet rarely removed. According to the brief, the company failed to disclose these harms to the public or to Congress, and refused to implement safety fixes that could have protected young users.  </p></div><div><div data-ad-wrapper="true"><p>Advertisement</p><div></div></div></div><div><p data-testid="paragraph-content">“Meta has designed social media products and platforms that it is aware are addictive to kids, and they’re aware that those addictions lead to a whole host of serious mental health issues,” says Previn Warren, the co-lead attorney for the plaintiffs in the case. “Like tobacco, this is a situation where there are dangerous products that were marketed to kids,” Warren adds. “They did it anyway, because more usage meant more profits for the company.” </p><p data-testid="paragraph-content">The following allegations against Meta come from the brief filed in an unprecedented multidistrict litigation. More than 1,800 plaintiffs—including children and parents, school districts, and state attorneys general—have joined together in a suit alleging that the parent companies behind Instagram, TikTok, Snapchat, and YouTube “relentlessly pursued a strategy of growth at all costs, recklessly ignoring the impact of their products on children’s mental and physical health,” according to their master complaint. The newly unsealed allegations about Meta are just one small part of the sprawling suit. (TIME filed a motion to intervene in the case to ensure public access to court records; the motion was denied.)</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">The plaintiffs’ brief, first reported by TIME, purports to be based on sworn depositions of current and former Meta executives, internal communications, and company research and presentations obtained during the lawsuit’s discovery process. It includes quotes and excerpts from thousands of pages of testimony and internal company documents. TIME was not able to independently view the underlying testimony or research quoted in the brief, since those documents remain under seal.  </p><p data-testid="paragraph-content"><strong>Read More:</strong> <em><a href="https://time.com/7334078/matthew-bergman-social-media-victims-lawsuits/">The Lawyer Suing Social Media Companies On Behalf of Kids</a></em></p><p data-testid="paragraph-content">But the brief still paints a damning picture of the company’s internal research and deliberations about issues that have long plagued its platforms. Plaintiffs claim that since 2017, Meta has aggressively pursued young users, even as its internal research suggested its social media products could be addictive and dangerous to kids. Meta employees proposed multiple ways to mitigate these harms, according to the brief, but were repeatedly blocked by executives who feared that new safety features would hamper teen engagement or user growth.</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">“We strongly disagree with these allegations, which rely on cherry-picked quotes and misinformed opinions in an attempt to present a deliberately misleading picture,&#34; a Meta spokesperson said in a statement to TIME. &#34;The full record will show that for over a decade, we have listened to parents, researched issues that matter most, and made real changes to protect teens – like introducing Teen Accounts with built-in protections and providing parents with controls to manage their teens’ experiences. We’re proud of the progress we’ve made and we stand by our record.”</p><p data-testid="paragraph-content">In the years since the lawsuit was filed, Meta has implemented new safety features designed to address some of the problems described by plaintiffs. In 2024, Meta unveiled Instagram Teen Accounts, which defaults any user between 13 and 18 into an account that is automatically private, limits sensitive content, turns off notifications at night, and doesn’t allow messaging from unconnected adults. “We know parents are worried about their teens having unsafe or inappropriate experiences online, and that’s why we’ve significantly reimagined the Instagram experience for tens of millions of teens with new Teen Accounts,” a Meta spokeswoman told TIME in June. “These accounts provide teens with built-in protections to automatically limit who’s contacting them and the content they’re seeing, and teens under 16 need a parent’s permission to change those settings. We also give parents oversight over their teens’ use of Instagram, with ways to see who their teens are chatting with and block them from using the app for more than 15 minutes a day, or for certain periods of time, like during school or at night.”</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">And yet the plaintiffs’ brief suggests that Meta resisted safety changes like these for years. </p><p data-testid="paragraph-content">The brief quotes testimony from Brian Boland, Meta’s former vice president of partnerships who worked at the company for 11 years and resigned in 2020. “My feeling then and my feeling now is that they don’t meaningfully care about user safety,” he allegedly said. “It’s not something that they spend a lot of time on. It’s not something they think about. And I really think they don’t care.”</p><p data-testid="paragraph-content">After the plaintiffs’ brief was unsealed late Friday night, Meta did not immediately respond to TIME’s requests for comment. </p><p data-testid="paragraph-content">Here are some of the most notable allegations from the plaintiffs’ omnibus brief: </p><h2>Allegation: Meta had a high threshold for &#34;sex trafficking&#34; content—and no way to report child sexual content</h2><p data-testid="paragraph-content">Despite Instagram’s “zero tolerance” policy for child sexual abuse material, the platform did not offer users a simple way to report child sexual abuse content, according to the brief. Plaintiffs allege that Jayakumar raised the issue multiple times when she joined Meta in 2020, but was told it would be too difficult to address. Yet Instagram allowed users to easily report far less serious violations, like “spam,” “intellectual property violation” and “promotion of firearms,” according to plaintiffs.</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">Jayakumar was even more shocked to learn that Instagram had a disturbingly high tolerance for sex trafficking on the platform. According to the brief, she testified that Meta had a “17x” strike policy for accounts that reportedly engaged in the “trafficking of humans for sex,” meaning it would take at least 16 reports for an account to be deleted.   </p><p data-testid="paragraph-content">“Meta never told parents, the public, or the Districts that it doesn’t delete accounts that have engaged over fifteen times in sex trafficking,” the plaintiffs wrote. </p><p data-testid="paragraph-content">A Meta spokesperson disputed this allegation to TIME, saying the company has for years removed accounts immediately if it suspects them of human trafficking or exploitation and has made it easier over time for users to report content that violates child-exploitation policies. </p><h2>Allegation: Meta &#34;lied to Congress&#34; about its knowledge of harms on the platform</h2><p data-testid="paragraph-content">For years, plaintiffs allege, Meta’s internal research had found that teenagers who frequently use Instagram and Facebook have higher rates of anxiety and depression. </p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">In late 2019, according to the brief, Meta designed a “deactivation study,” which found that users who stopped using Facebook and Instagram for a week showed lower rates of anxiety, depression, and loneliness. Meta halted the study and did not publicly disclose the results, stating that the research study was biased by the “existing media narratives around the company.” (A Meta spokesperson told TIME that the study was initially conceived as a pair of one-weeks pilots, and researchers declined to continue it because it found that the only reductions in feelings of depression, anxiety, and loneliness were among people who already believed Facebook was bad for them.)</p><p data-testid="paragraph-content">At least one Meta employee was uncomfortable with the implications of this decision: “If the results are bad and we don’t publish and they leak,” this employee wrote, according to the brief, “is it going to look like tobacco companies doing research and knowing cigs were bad and then keeping that info to themselves?”</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">Indeed, in December 2020, when the Senate Judiciary Committee asked the company in a set of written questions whether it was “able to determine whether increased use of its platform among teenage girls has any correlation with increased signs of depression” and “increased signs of anxiety,” the company offered only a one-word answer: “No.”</p><p data-testid="paragraph-content">To the plaintiffs in the case, the implication is clear: “The company never publicly disclosed the results of its deactivation study. Instead, Meta lied to Congress about what it knew.”</p><h2>Allegation: The company knew Instagram was letting adult strangers connect with teenagers</h2><p data-testid="paragraph-content">For years <a href="https://www.theatlantic.com/technology/archive/2018/10/instagram-has-massive-harassment-problem/572890/">Instagram has had </a>a well-documented problem of adults harassing teens. Around 2019, company researchers recommended making all teen accounts private by default in order to prevent adult strangers from connecting with kids, according to the plaintiffs’ brief. Instead of implementing this recommendation, Meta asked its growth team to study the potential impact of making all teen accounts private. The growth team was pessimistic, according to the brief, and responded that the change would likely reduce engagement. </p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">By 2020, the growth team had determined that a private-by-default setting would result in a loss of 1.5 million monthly active teens a year on Instagram. The plaintiffs’ brief quotes an unnamed employee as saying: “taking away unwanted interactions… is likely to lead to a potentially untenable problem with engagement and growth.” Over the next several months, plaintiffs allege, Meta’s policy, legal, communications, privacy, and well-being teams all recommended making teen accounts private by default, arguing that the switch “will increase teen safety” and was in line with expectations from users, parents, and regulators. But Meta did not launch the feature that year. </p><p data-testid="paragraph-content">Safety researchers were dismayed, according to excerpts of an internal conversation quoted in the filing. One allegedly grumbled: “Isn’t safety the whole point of this team?” </p><p data-testid="paragraph-content">“Meta knew that placing teens into a default-private setting would have eliminated 5.4 million unwanted interactions a day,” the plaintiffs wrote. Still, Meta didn’t make the fix. Instead, inappropriate interactions between adults and kids on Instagram skyrocketed to 38 times that on Facebook Messenger, according to the brief. The launch of Instagram Reels allegedly compounded the problem. It allowed young teenagers to broadcast short videos to a wide audience, including adult strangers.</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content"><strong>Read More:</strong> <em><a href="https://time.com/7327229/raul-torrez-new-mexico-meta-lawsuit/">The AG Putting Big Tech On Trial</a></em>.</p><p data-testid="paragraph-content">An internal 2022 audit allegedly found that Instagram’s Accounts You May Follow feature recommended 1.4 million potentially inappropriate adults to teenage users in a single day. By 2023, according to the plaintiffs, Meta knew that they were recommending minors to potentially suspicious adults and vice versa. </p><p data-testid="paragraph-content">It wasn’t until 2024 that Meta rolled out default privacy settings to all teen accounts. In the four years it took the company to implement their own safety recommendations, teens experienced billions of unwanted interactions with strangers online. Inappropriate encounters between teens and adults were common enough, according to the brief, that the company had an acronym for them: “IIC,” or “inappropriate interactions with children.” </p><p data-testid="paragraph-content">A Meta spokesperson said the company has defaulted teens under 16 to private accounts since 2021, began defaulting teens under 18 into private accounts with the introduction of its Teen Accounts program, and has <a href="https://about.fb.com/news/2023/12/combating-online-predators/">taken steps</a> to protect users from online predators. </p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><h2>Allegation: Meta aggressively targeted young users</h2><p data-testid="paragraph-content">Meta feared young users would abandon Facebook and Instagram for their competitors. Acquiring and keeping young users became a central business goal. Meta CEO Mark Zuckerberg suggested that “teen time spent be our top goal of 2017,” according to a company executive quoted in the brief. That has remained the case, plaintiffs allege; internal company documents from 2024 stated that “acquiring new teen users is mission critical to the success of Instagram.” (A Meta spokesperson said time spent on its platforms is not currently a company goal.)</p><p data-testid="paragraph-content">Meta launched a campaign to connect with school districts and paid organizations like the National Parent Teacher Association and Scholastic to conduct outreach to schools and families. Meanwhile, according to the brief, Meta used location data to push notifications to students in “school blasts,” presumably as part of an attempt to increase youth engagement during the school day. As one employee allegedly put it: “One of the things we need to optimize for is sneaking a look at your phone under your desk in the middle of Chemistry :)”.</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">Though Meta aggressively pursued young users, it may not have known exactly how old those new users were. Whistleblower <a href="https://www.judiciary.senate.gov/imo/media/doc/6ccd5abd-cbb9-5107-d48d-b99e821eb244/2025-09-09%20-%20QFR%20Responses%20-%20Sattizahn.pdf">Jason Sattizahn recently</a> testified to Congress that Meta does not reliably know the age of its users. (Meta pushed back on Sattizahn’s testimony, <a href="https://www.nbcnews.com/tech/tech-news/meta-whistleblower-research-kids-vr-former-employees-stock-rcna230131">saying in a statement to NBC</a> that his claims were “nonsense” and “based on selectively leaked internal documents that were picked specifically to craft a false narrative.”) In 2022, according to the plaintiffs’ brief, there were 216 million users on Meta platforms whose age was “unknown.”</p><p data-testid="paragraph-content">Federal law requires social media platforms to observe various data-privacy safeguards for users under 13, and Meta policy states that users under 13 are not allowed on its platforms. Yet the plaintiffs’ court filing claims Meta knew that children under 13 used the company’s products anyway. Internal research cited in the brief suggested there were 4 million users under 13 on Instagram in 2015; by 2018, the plaintiffs claim, Meta knew that roughly 40% of children aged 9 to 12 said they used Instagram daily.</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">The plaintiffs allege that this was a deliberate business strategy. The brief describes a coordinated effort to acquire young users that included studying the psychology and digital behavior of “tweens” and exploring new products designed for “users as young as 5-10.” </p><p data-testid="paragraph-content">Internally, some employees expressed disgust at the attempt to target preteens. “Oh good, we’re going after &lt;13 year olds now?” one wrote, according to the brief. “Zuck has been talking about that for a while...targeting 11 year olds feels like tobacco companies a couple decades ago (and today). Like we’re seriously saying ‘we have to hook them young’ here.”</p><h2>Allegation: Meta&#39;s executives initially shelved efforts to make Instagram less toxic for teens</h2><p data-testid="paragraph-content">To combat toxic “social comparison,” in 2019 Instagram CEO Adam Mosseri announced a new product feature that would “hide” likes on posts. Meta researchers had determined that hiding likes would make users “significantly less likely to feel worse about themselves,” according to the plaintiffs’ brief. The initiative was code-named Project Daisy. </p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">But after a series of tests, Meta backtracked on Project Daisy. It determined the feature was “pretty negative to FB metrics,” including ad revenue, according to the plaintiffs’ brief, which quotes an unnamed employee on the growth team insisting: “It’s a social comparison app, fucking get used to it.” </p><p data-testid="paragraph-content">A similar debate took place over the app’s beauty filters. Plaintiffs claim that an internal review concluded beauty filters exacerbated the “risk and maintenance of several mental health concerns, including body dissatisfaction, eating disorders, and body dysmorphic disorder,” and that Meta knew that “children are particularly vulnerable.” Meta banned beauty filters in 2019, only to roll them back out the following year after the company realized that banning beauty filters would have a “negative growth impact,” according to the plaintiffs’ brief. </p><p data-testid="paragraph-content">Other company researchers allegedly built an AI “classifier” to identify content that would lead to negative appearance comparison, so that Meta could avoid recommending it to vulnerable kids. But Mosseri allegedly killed the project, disappointing developers who “felt like they had a solution” to “a big problem.”</p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><h2>Allegation: Meta doesn&#39;t automatically remove harmful content, including self-harm content</h2><p data-testid="paragraph-content">While Meta developed AI tools to monitor the platforms for harmful content, the company didn’t automatically delete that content even when it determined with “100% confidence” that it violated Meta’s policies against child sexual-abuse material or eating-disorder content. Meta’s AI classifiers did not automatically delete posts that glorified self-harm unless they were 94% certain they violated platform policy, according to the plaintiffs’ brief. As a result, most of that content remained on the platform, where teenage users often discovered it. In a 2021 internal company survey cited by plaintiffs, more than 8% of respondents aged 13 to 15 reported having seen someone harm themselves, or threaten to do so, on Instagram during the past week.</p><p data-testid="paragraph-content"><strong>Read More: </strong><em><a href="https://time.com/7310444/instagram-lawsuit-self-harm/">‘Everything I Learned About Suicide, I Learned On Instagram.’</a></em></p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><div><p data-testid="paragraph-content">A Meta spokesperson said the company reports more child sexual-abuse material than any other service and uses an array of tools to proactively find that content, including photo and video-matching technologies as well as machine learning. The spokesperson said human reviewers assess content flagged before it is deleted to ensure it violates policies, prevent mistakes that could affect users, and maintain the integrity of the company&#39;s detection databases. </p><h2>Allegation: Meta knew its products were addictive, but publicly downplayed the harms</h2><p data-testid="paragraph-content">The addictive nature of the company’s products wasn’t a secret internally. “Oh my gosh yall IG is a drug,” one of the company’s user-experience researchers allegedly wrote to a colleague. “We’re basically pushers.” </p><p data-testid="paragraph-content">Meta does not officially study addiction to its products, plaintiffs allege; it studies “problematic use.” In 2018, company researchers surveyed 20,000 Facebook users in the U.S. and found that 58% had some level of “problematic use”—55% mild, and 3.1% severe. But when Meta published an account of this research the following year, only the smaller number of users with “severe” problematic use was mentioned. “We estimate (as an upper bound) that 3.1% of Facebook users in the U.S. experience problematic use,” <a href="https://scontent-sjc6-1.xx.fbcdn.net/v/t39.8562-6/240848801_924166131530538_1308218914254273445_n.pdf?_nc_cat=111&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=C_tgb_3384YQ7kNvwHuZaN0&amp;_nc_oc=Adm09HFJ1ktJoo9WIE0m4qDw6-OlF_2k7EQF8rnKbmwRLRsOB1cDAsP3XLwyT8q2accez_oKoOZRXzLKpBMUCsHU&amp;_nc_zt=14&amp;_nc_ht=scontent-sjc6-1.xx&amp;_nc_gid=WZWa2peYFXQxwOygthrZ6g&amp;oh=00_AfhNhtKp-IiYwptqBR42Y2exRT2A9v9zp5xlCdNIzxAJ_A&amp;oe=6925C501">wrote the researchers</a>. The other 55% of users are not mentioned anywhere in the public report. </p></div><div><div data-ad-wrapper="true"><p>Advertisement</p></div></div><p data-testid="paragraph-content">Plaintiffs allege that Meta’s safety team proposed features designed to lessen addiction, only to see them set aside or watered down. One employee who helped develop a “quiet mode” feature said it was shelved because Meta was concerned that this feature would negatively impact metrics related to growth and usage.</p><p data-testid="paragraph-content">Around the same time, another user-experience researcher at Instagram allegedly recommended that Meta inform the public about its research findings: “Because our product exploits weaknesses in the human psychology to promote product engagement and time spent,” the researcher wrote, Meta needed to “alert people to the effect that the product has on their brain.” </p><p data-testid="paragraph-content">Meta did not. </p><p data-testid="paragraph-content"><em>This story has been updated to reflect additional comments from Meta. </em></p></article></div></div>
  </body>
</html>
