<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mcyoung.xyz/2025/10/21/ssa-1/">Original</a>
    <h1>Why SSA Compilers?</h1>
    
    <div id="readability-page-1" class="page"><div> <p>If you’ve read anything about compilers in the last two decades or so, you have almost certainly heard of <em>SSA compilers</em>, a popular architecture featured in many optimizing compilers, including ahead-of-time compilers such as LLVM, GCC, Go, CUDA (and various shader compilers), Swift<sup id="fnref:swift" role="doc-noteref"><a href="#fn:swift" rel="footnote">1</a></sup>, and MSVC<sup id="fnref:msvc" role="doc-noteref"><a href="#fn:msvc" rel="footnote">2</a></sup>, and just-in-time compilers such as HotSpot C2<sup id="fnref:hotspot" role="doc-noteref"><a href="#fn:hotspot" rel="footnote">3</a></sup>, V8<sup id="fnref:v8" role="doc-noteref"><a href="#fn:v8" rel="footnote">4</a></sup>, SpiderMonkey<sup id="fnref:spidermonkey" role="doc-noteref"><a href="#fn:spidermonkey" rel="footnote">5</a></sup>, LuaJIT, and the Android Runtime<sup id="fnref:art" role="doc-noteref"><a href="#fn:art" rel="footnote">6</a></sup>.</p> <p>SSA is hugely popular, to the point that most compiler projects no longer bother with other IRs for optimization<sup id="fnref:ghc" role="doc-noteref"><a href="#fn:ghc" rel="footnote">7</a></sup>. This is because SSA is incredibly nimble at the types of program analysis and transformation that compiler optimizations want to do on your code. But <em>why</em>? Many of my friends who don’t do compilers often say that compilers seem like opaque magical black boxes, and SSA, as it often appears in the literature, is impenetrably complex.</p> <p>But it’s not! SSA is actually very simple once you forget everything you think your programs are actually doing. We will develop the concept of SSA form, a simple SSA IR, prove facts about it, and design some optimizations on it.</p> <blockquote id="note:1"> <p><a href="#note:1"><span>note</span></a></p> <p>I have <a href="https://mcyoung.xyz/2023/08/01/llvm-ir">previously written</a> about the granddaddy of all modern SSA compilers, LLVM. This article is about SSA in general, and won’t really have anything to do with LLVM. However, it may be helpful to read that article to make some of the things in this article feel more concrete.</p> </blockquote> <h2 id="what-is-ssa"><a href="#what-is-ssa">What Is SSA?</a></h2> <p>SSA is a property of <em>intermediate representations</em> (IRs), primarily used by compilers for optimizing imperative code that target a <em>register machine</em>. Register machines are computers that feature a fixed set of <em>registers</em> that can be used as the operands for instructions: this includes virtually all physical processors, including CPUs, GPUs, and weird tings like DSPs.</p> <p>SSA is most frequently found in compiler <em>middle-ends</em>, the optimizing component between the <em>frontend</em> (which deals with the <em>surface language</em> programmers write, and lowers it into the middle-end’s IR), and the <em>backend</em> (which takes the optimized IR and lowers it into the target platform’s assembly).</p> <p>SSA IRs, however, often have little resemblance to the surface language they lower out of, or the assembly language they target. This is because neither of these representations make it easy for a compiler to intuit optimization opportunities.</p> <h3 id="imperative-code-is-hard"><a href="#imperative-code-is-hard">Imperative Code Is Hard</a></h3> <p>Imperative code consists of a sequence of operations that mutate the executing machine’s state to produce a desired result. For example, consider the following C program:</p> <div id="code:1"><figure><pre><code data-lang="c"><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span> <span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>argc</span><span>;</span>
  <span>int</span> <span>b</span> <span>=</span> <span>a</span> <span>+</span> <span>1</span><span>;</span>
  <span>a</span> <span>=</span> <span>b</span> <span>+</span> <span>2</span><span>;</span>
  <span>b</span> <span>+=</span> <span>2</span><span>;</span>
  <span>a</span> <span>-=</span> <span>b</span><span>;</span>
  <span>return</span> <span>a</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>This program returns <code>0</code> no matter what its input is, so we can optimize it down to this:</p> <div id="code:2"><figure><pre><code data-lang="c"><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span> <span>{</span>
  <span>return</span> <span>0</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>But, how would you write a general algorithm to detect that all of the operations cancel out? You’re forced to keep in mind <em>program order</em> to perform the necessary dataflow analysis, following mutations of <code>a</code> and <code>b</code> through the program. But this isn’t very general, and traversing all of those paths makes the search space for large functions very big. Instead, you would like to rewrite the program such that <code>a</code> and <code>b</code> gradually get replaced with the expression that calculates the most recent value, like this:</p> <div id="code:3"><figure><pre><code data-lang="c"><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span> <span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>argc</span><span>;</span>
  <span>int</span> <span>b</span> <span>=</span> <span>a</span> <span>+</span> <span>1</span><span>;</span>
  <span>int</span> <span>a2</span> <span>=</span> <span>b</span> <span>+</span> <span>2</span><span>;</span>
  <span>int</span> <span>b2</span> <span>=</span> <span>b</span> <span>+</span> <span>2</span><span>;</span>
  <span>int</span> <span>a3</span> <span>=</span> <span>a2</span> <span>-</span> <span>b2</span><span>;</span>
  <span>return</span> <span>a3</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>Then we can replace each occurrence of a variable with its right-hand side recursively…</p> <div id="code:4"><figure><pre><code data-lang="c"><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span> <span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>argc</span><span>;</span>
  <span>int</span> <span>b</span> <span>=</span> <span>argc</span> <span>+</span> <span>1</span><span>;</span>
  <span>int</span> <span>a2</span> <span>=</span> <span>argc</span> <span>+</span> <span>1</span> <span>+</span> <span>2</span><span>;</span>
  <span>int</span> <span>b2</span> <span>=</span> <span>argc</span> <span>+</span> <span>1</span> <span>+</span> <span>2</span><span>;</span>
  <span>int</span> <span>a3</span> <span>=</span> <span>(</span><span>argc</span> <span>+</span> <span>1</span> <span>+</span> <span>2</span><span>)</span> <span>-</span> <span>(</span><span>argc</span> <span>+</span> <span>1</span> <span>+</span> <span>2</span><span>);</span>
  <span>return</span> <span>(</span><span>argc</span> <span>+</span> <span>1</span> <span>+</span> <span>2</span><span>)</span> <span>-</span> <span>(</span><span>argc</span> <span>+</span> <span>1</span> <span>+</span> <span>2</span><span>);</span>
<span>}</span></code></pre></figure></div> <p>Then fold the constants together…</p> <div id="code:5"><figure><pre><code data-lang="c"><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span> <span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>argc</span><span>;</span>
  <span>int</span> <span>b</span> <span>=</span> <span>argc</span> <span>+</span> <span>1</span><span>;</span>
  <span>int</span> <span>a2</span> <span>=</span> <span>argc</span> <span>+</span> <span>3</span><span>;</span>
  <span>int</span> <span>b2</span> <span>=</span> <span>argc</span> <span>+</span> <span>3</span><span>;</span>
  <span>int</span> <span>a3</span> <span>=</span> <span>argc</span> <span>-</span> <span>argc</span>
  <span>return</span> <span>argc</span> <span>-</span> <span>argc</span>
<span>}</span></code></pre></figure></div> <p>And finally, we see that we’re returning <code>argc - argc</code>, and can replace it with <code>0</code>. All the other variables are now unused, so we can delete them.</p> <p>The reason this works so well is because we took a function with mutation, and converted it into a <em>combinatorial circuit</em>, a type of digital logic circuit that has no state, and which is very easy to analyze. The dependencies between <em>nodes</em> in the circuit (corresponding to primitive operations such as addition or multiplication) are obvious from its structure. For example, consider the following circuit diagram for a one-bit multiplier:</p> <figure> <p><img src="https://mcyoung.xyz/public/images/multiplier.png"/></p> <figcaption>A binary multiplier (Wikipedia)</figcaption> </figure> <p>This graph representation of an operation program has two huge benefits:</p> <ol> <li> <p>The powerful tools of graph theory can be used to algorithmically analyze the program and discover useful properties, such as operations that are independent of each other or whose results are never used.</p> </li> <li> <p>The operations are not ordered with respect to each other except when there is a dependency; this is useful for reordering operations, something compilers really like to do.</p> </li> </ol> <p>The reason combinatorial circuits are the best circuits is because they are <em>directed acyclic graphs</em> (DAGs) which admit really nice algorithms. For example, longest path in a graph is <a href="https://en.wikipedia.org/wiki/NP-hardness">NP-hard</a> (and because <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo mathvariant="normal">≠</mo><mi>N</mi><mi>P</mi></mrow><annotation encoding="application/x-tex">P \neq NP</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span><span></span><span><span><span><span><span><span></span><span><span><span></span></span></span><span></span></span></span></span></span><span>=</span></span><span></span></span><span><span></span><span>NP</span></span></span></span></span><sup id="fnref:p-np" role="doc-noteref"><a href="#fn:p-np" rel="footnote">8</a></sup>, has complexity <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>2</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span></span></span></span></span><span>)</span></span></span></span></span>). However, if the graph is a DAG, it admits an <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span>)</span></span></span></span></span> solution!</p> <p>To understand this benefit, consider another program:</p> <div id="code:6"><figure><pre><code data-lang="c"><span>int</span> <span>f</span><span>(</span><span>int</span> <span>x</span><span>)</span> <span>{</span>
  <span>int</span> <span>y</span> <span>=</span> <span>x</span> <span>*</span> <span>2</span><span>;</span>
  <span>x</span> <span>*=</span> <span>y</span><span>;</span>
  <span>const</span> <span>int</span> <span>z</span> <span>=</span> <span>y</span><span>;</span>
  <span>y</span> <span>*=</span> <span>y</span><span>;</span>
  <span>return</span> <span>x</span> <span>+</span> <span>z</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>Suppose we wanted to replace each variable with its definition like we did before. We can’t <em>just</em> replace each constant variable with the expression that defines it though, because we would wind up with a different program!</p> <div id="code:7"><figure><pre><code data-lang="c"><span>int</span> <span>f</span><span>(</span><span>int</span> <span>x</span><span>)</span> <span>{</span>
  <span>int</span> <span>y</span> <span>=</span> <span>x</span> <span>*</span> <span>2</span><span>;</span>
  <span>x</span> <span>*=</span> <span>y</span><span>;</span>
  <span>// const int z = y; // Replace z with its definition.</span>
  <span>y</span> <span>*=</span> <span>y</span><span>;</span>
  <span>return</span> <span>x</span> <span>+</span> <span>y</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>Now, we pick up an extra <code>y</code> term because the squaring operation is no longer unused! We can put this into circuit form, but it requires inserting new variables for every mutation.</p> <p>But we can’t do this when complex control flow is involved! So all of our algorithms need to carefully account for mutations and program order, meaning that we don’t get to use the nice graph algorithms without careful modification.</p> <h2 id="the-ssa-invariant"><a href="#the-ssa-invariant">The SSA Invariant</a></h2> <p>SSA stands for “static single assignment”, and was developed in the 80s as a way to enhance the existing three-argument code (where every statement is in the form <code>x = y op z</code>) so that every program was circuit-like, using a very similar procedure to the one described above.</p> <p>The SSA invariant states that every variable in the program is assigned to by precisely one operation. If every operation in the program is visited once, they form a combinatorial circuit. Transformations are required to respect this invariant. In circuit form, a program is a graph where operations are nodes, and “registers” (which is what variables are usually called in SSA) are edges (specifically, each output of an operation corresponds to a register).</p> <p>But, again, control flow. We can’t hope to circuitize a loop, right? The key observation of SSA is that <em>most</em> parts of a program are circuit-like. A <em>basic block</em> is a maximal circuital component of a program. Simply put, it is a sequence of non-control flow operations, and a final <em>terminator</em> operation that transfers control to another basic block.</p> <p>The basic blocks themselves form a graph, the <em>control flow graph</em>, or CFG. This formulation of SSA is sometimes called SSA-CFG<sup id="fnref:non-cfg" role="doc-noteref"><a href="#fn:non-cfg" rel="footnote">9</a></sup>. This graph is <em>not</em> a DAG in general; however, separating the program into basic blocks conveniently factors out the “non-DAG” parts of the program, allowing for simpler analysis within basic blocks.</p> <p>There are two equivalent formalisms for SSA-CFG. The traditional one uses special “phi” operations (often called <em>phi nodes</em>, which is what I will call them here) to link registers across basic blocks. This is the formalism LLVM uses. A more modern approach, used by MLIR, is <em>block arguments</em>: each basic block specifies parameters, like a function, and blocks transferring control flow to it must pass arguments of those types to it.</p> <h3 id="my-first-ir"><a href="#my-first-ir">My First IR</a></h3> <p>Let’s look at some code. First, consider the following C function which calculates Fibonacci numbers using a loop.</p> <div id="code:8"><figure><pre><code data-lang="c"><span>int</span> <span>fib</span><span>(</span><span>int</span> <span>n</span><span>)</span> <span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>0</span><span>,</span> <span>b</span> <span>=</span> <span>1</span><span>;</span>
  <span>for</span> <span>(;</span> <span>n</span> <span>&gt;</span> <span>0</span><span>;</span> <span>--</span><span>n</span><span>)</span> <span>{</span>
    <span>int</span> <span>c</span> <span>=</span> <span>a</span> <span>+</span> <span>b</span><span>;</span>
    <span>a</span> <span>=</span> <span>b</span><span>,</span> <span>b</span> <span>=</span> <span>c</span><span>;</span>
  <span>}</span>
  <span>return</span> <span>a</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>How might we express this in an SSA-CFG IR? Let’s start inventing our SSA IR! It will look a <em>little bit</em> like LLVM IR, since that’s what I’m used to looking at.</p> <div id="code:fib-ir"><figure><pre><code data-lang="go"><span>// Globals (including functions) start with $, registers with %.</span>
<span>// Each function declares a signature.</span>
<span>func</span> <span>fib</span><span>(</span><span>%</span><span>n</span><span>:</span> <span>i32</span><span>)</span> <span>-&gt;</span> <span>(</span><span>i32</span><span>)</span> <span>{</span>
    <span>// The first block has no label and can&#39;t be &#34;jumped to&#34;.</span>
    <span>//</span>
    <span>// Single-argument goto jumps directly into a block with</span>
    <span>// the given arguments.</span>
    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>n</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>)</span>

  <span>// Block labels start with a `!`, can contain dots, and</span>
  <span>// define parameters. Register names are scoped to a block.</span>
  <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>n</span><span>,</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span><span>:</span> <span>i32</span><span>)</span><span>:</span>
    <span>// Integer comparison: %n &gt; 0.</span>
    <span>%</span><span>cont</span> <span>=</span> <span>cmp</span><span>.</span><span>gt</span> <span>%</span><span>n</span><span>,</span> <span>0</span>

    <span>// Multi-argument goto is a switch statement. The compiler</span>
    <span>// may assume that `%cont` is among the cases listed in the</span>
    <span>// goto.</span>
    <span>goto</span> <span>%</span><span>cont</span> <span>{</span>
      <span>0</span> <span>-&gt;</span> <span>@</span><span>ret</span><span>(</span><span>%</span><span>a</span><span>),</span> <span>// Goto can jump to the function exit.</span>
      <span>1</span> <span>-&gt;</span> <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>n</span><span>,</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span><span>),</span>
    <span>}</span>

  <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>n</span><span>,</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span><span>:</span> <span>i32</span><span>)</span><span>:</span>
    <span>// Addition and subtraction.</span>
    <span>%</span><span>c</span>    <span>=</span> <span>add</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span>
    <span>%</span><span>n</span><span>.2</span>  <span>=</span> <span>sub</span> <span>%</span><span>n</span><span>,</span> <span>1</span>

    <span>// Note the assignments in @loop.start:</span>
    <span>// %n = %n.2, %a = %b, %b = %c.</span>
    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>n</span><span>.2</span><span>,</span> <span>%</span><span>b</span><span>,</span> <span>%</span><span>c</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>Every block ends in a <code>goto</code>, which transfers control to one of several possible blocks. In the process, it calls that block with the given arguments. One can think of a basic block as a tiny function which <em>tails</em><sup id="fnref:tail-call" role="doc-noteref"><a href="#fn:tail-call" rel="footnote">10</a></sup> into other basic blocks in the same function.</p> <blockquote id="aside:phi-nodes"> <p><a href="#aside:phi-nodes"><span>aside</span><span>Phi Nodes</span></a></p> <p>LLVM IR is… older, so it uses the older formalism of phi nodes. “Phi” comes from “phony”, because it is an operation that doesn’t do anything; it just links registers from predecessors.</p> <p>A <code>phi</code> operation is essentially a switch-case on the predecessors, each case selecting a register from that predecessor (or an immediate). For example, <code>@loop.start</code> has two predecessors, the implicit entry block <code>@entry</code>, and <code>@loop.body</code>. In a phi node IR, instead of taking a block argument for <code>%n</code>, it would specify</p> <div id="code:9"><figure><pre><code data-lang="go"><span>&gt;</span>   <span>%</span><span>n</span> <span>=</span> <span>phi</span> <span>{</span> <span>@</span><span>entry</span> <span>-&gt;</span> <span>0</span><span>,</span> <span>@</span><span>loop</span><span>.</span><span>body</span> <span>-&gt;</span> <span>%</span><span>n</span><span>.2</span> <span>}</span></code></pre></figure></div> <p>The value of the <code>phi</code> operation is the value from whichever block jumped to this one.</p> <p>This can be awkward to type out by hand and read, but is a more convenient representation for describing algorithms (just “add a phi node” instead of “add a parameter and a corresponding argument”) and for the in-memory representation, but is otherwise completely equivalent.</p> </blockquote> <p>It’s a bit easier to understand the transformation from C to our IR if we first rewrite the C to use goto instead of a for loop:</p> <div id="code:10"><figure><pre><code data-lang="c"><span>int</span> <span>fib</span><span>(</span><span>int</span> <span>n</span><span>)</span> <span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>0</span><span>,</span> <span>b</span> <span>=</span> <span>1</span><span>;</span>
 <span>start:</span>
  <span>if</span> <span>(</span><span>n</span> <span>&gt;</span> <span>0</span><span>)</span> <span>goto</span> <span>ret</span><span>;</span>

  <span>int</span> <span>c</span> <span>=</span> <span>a</span> <span>+</span> <span>b</span><span>;</span>
  <span>a</span> <span>=</span> <span>b</span><span>,</span> <span>b</span> <span>=</span> <span>c</span><span>;</span>
  <span>goto</span> <span>start</span>

<span>ret:</span>
  <span>return</span> <span>a</span><span>;</span>
<span>}</span></code></pre></figure></div> <p>However, we still have mutation in the picture, so this isn’t SSA. To get into SSA, we need to replace every assignment with a new register, and somehow insert block arguments…</p> <h3 id="entering-ssa-form"><a href="#entering-ssa-form">Entering SSA Form</a></h3> <p>The <a href="#code:fib-ir">above IR code</a> is already partially optimized; the named variables in the C program have been <em>lifted</em> out of memory and into registers. If we represent each named variable in our C program with a pointer, we can avoid needing to put the program into SSA form immediately. This technique is used by frontends that lower into LLVM, like Clang.</p> <p>We’ll enhance our IR by adding a <code>stack</code> declaration for functions, which defines scratch space on the stack for the function to use. Each stack slot produces a pointer that we can <code>load</code> from and <code>store</code> to.</p> <p>Our Fibonacci function would now look like so:</p> <div id="code:fib-memory"><figure><pre><code data-lang="go"><span>func</span> <span>&amp;</span><span>fib</span><span>(</span><span>%</span><span>n</span><span>:</span> <span>i32</span><span>)</span> <span>-&gt;</span> <span>(</span><span>i32</span><span>)</span> <span>{</span>
    <span>// Declare stack slots.</span>
    <span>%</span><span>np</span> <span>=</span> <span>stack</span> <span>i32</span>
    <span>%</span><span>ap</span> <span>=</span> <span>stack</span> <span>i32</span>
    <span>%</span><span>bp</span> <span>=</span> <span>stack</span> <span>i32</span>

    <span>// Load initial values into them.</span>
    <span>store</span> <span>%</span><span>np</span><span>,</span> <span>%</span><span>n</span>
    <span>store</span> <span>%</span><span>ap</span><span>,</span> <span>0</span>
    <span>store</span> <span>%</span><span>bp</span><span>,</span> <span>1</span>

    <span>// Start the loop.</span>
    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>)</span>

  <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>:</span> <span>ptr</span><span>)</span><span>:</span>
    <span>%</span><span>n</span> <span>=</span> <span>load</span> <span>%</span><span>np</span>
    <span>%</span><span>cont</span> <span>=</span> <span>cmp</span><span>.</span><span>gt</span> <span>%</span><span>n</span><span>,</span> <span>0</span>

    <span>goto</span> <span>%</span><span>cont</span> <span>{</span>
      <span>0</span> <span>-&gt;</span> <span>@</span><span>exit</span><span>(</span><span>%</span><span>ap</span><span>)</span>
      <span>1</span> <span>-&gt;</span> <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>n</span><span>,</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span><span>),</span>
    <span>}</span>

  <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>:</span> <span>ptr</span><span>)</span><span>:</span>
    <span>%</span><span>a</span> <span>=</span> <span>load</span> <span>%</span><span>ap</span>
    <span>%</span><span>b</span> <span>=</span> <span>load</span> <span>%</span><span>bp</span>
    <span>%</span><span>c</span> <span>=</span> <span>add</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span>
    <span>store</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>b</span>
    <span>store</span> <span>%</span><span>bp</span><span>,</span> <span>%</span><span>c</span>

    <span>%</span><span>n</span>   <span>=</span> <span>load</span> <span>%</span><span>np</span>
    <span>%</span><span>n</span><span>.2</span> <span>=</span> <span>sub</span> <span>%</span><span>n</span><span>,</span> <span>1</span>
    <span>store</span> <span>%</span><span>np</span><span>,</span> <span>%</span><span>n</span><span>.2</span>

    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>)</span>

  <span>@</span><span>exit</span><span>(</span><span>%</span><span>ap</span><span>:</span> <span>ptr</span><span>)</span><span>:</span>
    <span>%</span><span>a</span> <span>=</span> <span>load</span> <span>%</span><span>ap</span>
    <span>goto</span> <span>@</span><span>ret</span><span>(</span><span>%</span><span>ap</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>Any time we reference a named variable, we load from its stack slot, and any time we assign it, we store to that slot. This is very easy to get into from C, but the code sucks because it’s doing lots of unnecessary pointer operations. How do we get from this to the register-only function I showed earlier?</p> <blockquote id="aside:program-order"> <p><a href="#aside:program-order"><span>aside</span><span>Program Order</span></a></p> <p>We want program order to not matter for the purposes of reordering, but as we’ve written code here, program order <em>does</em> matter: loads depend on prior stores but stores don’t produce a value that can be used to link the two operations.</p> <p>We can restore not having program order by introducing operands representing an “address space”; loads and stores take an address space as an argument, and stores return a new address space. An address space, or <code>mem</code>, represents the state of some region of memory. Loads and stores are independent when they are not connected by a <code>mem</code> argument.</p> <p>This type of enhancement is used by Go’s SSA IR, for example. However, it adds a layer of complexity to the examples, so instead I will hand-wave this away.</p> </blockquote> <h2 id="the-dominance-relation"><a href="#the-dominance-relation">The Dominance Relation</a></h2> <p>Now we need to prove some properties about CFGs that are important for the definition and correctness of our optimization passes.</p> <p>First, some definitions.</p> <blockquote id="def:1"> <p><a href="#def:1"><span>definition</span></a></p> <p>The <strong>predecessors</strong> (or “preds”) of a basic block is the set of blocks with an outgoing edge <strong>to</strong> that block. A block may be its own predecessors.</p> </blockquote> <p>Some literature calls the above “direct” or immediate predecessors. For example, the preds of in our <a href="#code:fib-memory">example</a> are <code>@loop.start</code> are <code>@entry</code> (the special name for the function entry-point) <code>@loop.body</code>.</p> <blockquote id="def:2"> <p><a href="#def:2"><span>definition</span></a></p> <p>The <strong>successors</strong> (no, not “succs”) of a basic block is the set of blocks with an outgoing edge <strong>from</strong> that block. A block may be its own successors.</p> </blockquote> <p>The sucessors of <code>@loop.start</code> are <code>@exit</code> and <code>@loop.body</code>. The successors are listed in the loop’s <code>goto</code>.</p> <p>If a block <code>@a</code> is a transitive pred of a block <code>@b</code>, we say that <code>@a</code> <em>weakly dominates</em> <code>@b</code>, or that it is a <em>weak dominator</em> of <code>@b</code>. For example, <code>@entry</code>, <code>@loop.start</code> and <code>@loop.body</code> both weakly dominate <code>@exit</code>.</p> <p>However, this is not usually an especially useful relationship. Instead, we want to speak of dominators:</p> <blockquote id="def:3"> <p><a href="#def:3"><span>definition</span></a></p> <p>A block <code>@a</code> is a <strong>dominator</strong> (or <strong>dominates</strong>) <code>@b</code> if every pred of <code>@b</code> is dominated by <code>@a</code>, or if <code>@a</code> is <code>@b</code> itself.</p> <p>Equivalently, the dominator set of <code>@b</code> is the intersection of the dominator sets of its preds, plus <code>@b</code>.</p> </blockquote> <p>The dominance relation has some nice order properties that are necessary for defining the core graph algorithms of SSA.</p> <h3 id="some-graph-theory"><a href="#some-graph-theory">Some Graph Theory</a></h3> <p>We only consider CFGs which are flowgraphs, that is, all blocks are reachable from the root block <code>@entry</code>, which has no preds. This is necessary to eliminate some pathological graphs from our proofs. Importantly, we can always ask for an acyclic path<sup id="fnref:acyclic" role="doc-noteref"><a href="#fn:acyclic" rel="footnote">11</a></sup> from <code>@entry</code> to any block <code>@b</code>.</p> <p>An equivalent way to state the dominance relationship is that from every path from <code>@entry</code> to <code>@b</code> contains all of <code>@b</code>’s dominators.</p> <blockquote id="prop:1"> <p><a href="#prop:1"><span>proposition</span></a></p> <p><code>@a</code> dominates <code>@b</code> iff every path from <code>@entry</code> to <code>@b</code> contains <code>@a</code>.</p> <blockquote id="proof:1"> <p><a href="#proof:1"><span>proof</span></a></p> <p>First, assume every <code>@entry</code> to <code>@b</code> path contains <code>@a</code>. If <code>@b</code> is <code>@a</code>, we’re done. Otherwise we need to prove each predecessor of <code>@b</code> is dominated by <code>@a</code>; we do this by induction on the length of acyclic paths from <code>@entry</code> to <code>@b</code>. Consider preds <code>@p</code> of <code>@b</code> that are not <code>@a</code>, and consider all acyclic paths <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span> from <code>@entry</code> to <code>@p</code>; by appending <code>@b</code> to them, we have an acyclic path <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">p&#39;</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span> from <code>@entry</code> to <code>@b</code>, which must contain <code>@a</code>. Because both the last and second-to-last elements of this are not <code>@a</code>, it must be within the shorter path <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span> which is shorter than <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">p&#39;</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span>. Thus, by induction, <code>@a</code> dominates <code>@p</code> and therefore <code>@b</code></p> <p>Going the other way, if <code>@a</code> dominates <code>@b</code>, and consider a path <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span> from <code>@entry</code> to <code>@b</code>. The second-to-last element of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span> is a pred <code>@p</code> of <code>@b</code>; if it is <code>@a</code> we are done. Otherwise, we can consider the path <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span> made by deleting <code>@b</code> at the end. <code>@p</code> is dominated by <code>@a</code>, and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">p&#39;</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span> is shorter than <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span>, so we can proceed by induction as above.</p> </blockquote> </blockquote> <p>Onto those nice properties. Dominance allows us to take an arbitrarily complicated CFG and extract from it a DAG, composed of blocks ordered by dominance.</p> <blockquote id="thm:1"> <p><a href="#thm:1"><span>theorem</span></a></p> <p>The dominance relation is a partial order.</p> <blockquote id="proof:2"> <p><a href="#proof:2"><span>proof</span></a></p> <p>Dominance is reflexive and transitive by definition, so we only need to show blocks can’t dominate each other.</p> <p>Suppose distinct <code>@a</code> and <code>@b</code> dominate each other.Pick an acyclic path<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span> from <code>@entry</code> to <code>@a</code>. Because <code>@b</code> dominates <code>@a</code>, there is a prefix <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">p&#39;</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span> of this path ending in <code>@b</code>. But because <code>@a</code> dominates <code>@b</code>, some prefix <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mrow><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo></mrow></msup></mrow><annotation encoding="application/x-tex">p&#39;&#39;</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>′′</span></span></span></span></span></span></span></span></span></span></span></span></span> of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">p&#39;</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span> ends in <code>@a</code>. But now <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span> must contain <code>@a</code> twice, contradicting that it is acyclic.</p> </blockquote> </blockquote> <p>This allows us to write <code>@a &lt; @b</code> when <code>@a</code> dominates <code>@b</code>. There is an even more refined graph structure that we can build out of dominators, which follows immediately from the partial order theorem.</p> <blockquote id="cor:1"> <p><a href="#cor:1"><span>corollary</span></a></p> <p>The dominators of a basic block are totally ordered by the dominance relation.</p> <blockquote id="proof:3"> <p><a href="#proof:3"><span>proof</span></a></p> <p>Suppose <code>@a1 &lt; @b</code> and <code>@a2 &lt; @b</code>, but neither dominates the other. Then, there must exist acyclic paths from <code>@entry</code> to <code>@b</code> which contain both, but in different orders. Take the subpaths of those paths which follow <code>@entry ... @a1</code>, and <code>@a1 ... @b</code>, neither of which contains <code>@a2</code>. Concatenating these paths yields a path from <code>@entry</code> to <code>@b</code> that does not contain <code>@a2</code>, a contradiction.</p> </blockquote> </blockquote> <p>This tells us that the DAG we get from the dominance relation is actually a tree, rooted at <code>@entry</code>. The parent of a node in this tree is called its <em>immediate dominator</em>.</p> <p>Computing dominators can be done iteratively: the dominator set of a block <code>@b</code> is the intersection the dominator sets of its preds, plus <code>@b</code>. This algorithm runs in quadratic time.</p> <p>A better algorithm is the Lengauer-Tarjan algorithm[^lta]. It is relatively simple, but explaining how to implement it is a bit out of scope for this article. I found a nice treatment of it <a href="https://www.cs.utexas.edu/~misra/Lengauer+Tarjan.pdf">here</a>.</p> <p>What’s important is we can compute the dominator tree without breaking the bank, and given any node, we can ask for its immediate dominator. Using immediate dominators, we can introduce the final, important property of dominators.</p> <blockquote id="def:4"> <p><a href="#def:4"><span>definition</span></a></p> <p>The <em>dominance frontier</em> of a block <code>@a</code> is the set of all blocks not dominated by <code>@a</code> with at least one pred which <code>@a</code> dominates.</p> </blockquote> <p>These are points where control flow merges from <em>distinct</em> paths: one containing <code>@a</code> and one not. The dominance frontier of <code>@loop.body</code> is <code>@loop.start</code>, whose preds are <code>@entry</code> and <code>@loop.body</code>.</p> <p>There are many ways to calculate dominance frontiers, but with a dominance tree in hand, we can do it like this:</p> <blockquote id="alg:dominance-frontiers"> <p><a href="#alg:dominance-frontiers"><span>algorithm</span><span>Dominance Frontiers.</span></a></p> <p>For each block <code>@b</code> with more than one pred, for each of its preds, let <code>@p</code> be that pred. Add <code>@b</code> to the dominance frontier of <code>@p</code> and all of its dominators, stopping when encountering <code>@b</code>’ immediate dominator.</p> <blockquote id="proof:4"> <p><a href="#proof:4"><span>proof</span></a></p> <p>We need to prove that every block examined by the algorithm winds up in the correct frontiers.</p> <p>First, we check that every examined block <code>@b</code> is added to the correct frontier. If <code>@a &lt; @p</code>, where <code>@p</code> is a pred of <code>@b</code>, and a <code>@d</code> is <code>@b</code>’s immediate dominator, then if <code>@a &lt; @d</code>, <code>@b</code> is not in its frontier, because <code>@a</code> must dominate <code>@b</code>. Otherwise, <code>@b</code> must be in <code>@a</code>’s frontier, because <code>@a</code> dominates a pred but it cannot dominate <code>@b</code>, because then it would be dominated by <code>@i</code>, a contradiction.</p> <p>Second, we check that every frontier is complete. Consider a block <code>@a</code>. If an examined block <code>@b</code> is in its frontier, then <code>@a</code> must be among the dominators of some pred <code>@p</code>, and it must be dominated by <code>@b</code>’s immediate dominator; otherwise, <code>@a</code> would dominate <code>@b</code> (and thus <code>@b</code> would not be in its frontier). Thus, <code>@b</code> gets added to <code>@a</code>’s dominator.</p> </blockquote> </blockquote> <p>You might notice that all of these algorithms are quadratic. This is actually a very good time complexity for a compilers-related graph algorithm. Cubic and quartic algorithms are not especially uncommon, and yes, your optimizing compiler’s time complexity is probably cubic or quartic in the size of the program!</p> <h2 id="lifting-memory"><a href="#lifting-memory">Lifting Memory</a></h2> <p>Ok. Let’s construct an optimization. We want to figure out if we can replace a load from a pointer with the most recent store to that pointer. This will allow us to fully lift values out of memory by cancelling out store/load pairs.</p> <p>This will make use of yet another implicit graph data structure.</p> <blockquote id="def:5"> <p><a href="#def:5"><span>definition</span></a></p> <p>The <strong>dataflow graph</strong> is the directed graph made up of the internal circuit graphs of each each basic block, connected along block arguments.</p> <p>To <strong>follow a use-def chain</strong> is to walk this graph forward from an operation to discover operations that potentially depend on it, or backwards to find operations it potentially depends on.</p> </blockquote> <p>It’s important to remember that the dataflow graph, like the CFG, does <em>not</em> have a well defined “up” direction. Navigating it and the CFG requires the dominator tree.</p> <p>One other important thing to remember here is that every instruction in a basic block always executes if the block executes. In much of this analysis, we need to appeal to “program order” to select the last load in a block, but we are always able to do so. This is an important property of basic blocks that makes them essential for constructing optimizations.</p> <h3 id="forward-dataflow"><a href="#forward-dataflow">Forward Dataflow</a></h3> <p>For a given <code>store %p, %v</code>, we want to identify all loads that depend on it. We can follow the use-def chain of <code>%p</code> to find which blocks contain loads that potentially depend on the store (call it <code>%s</code>).</p> <p>First, we can eliminate loads within the same basic block (call it <code>@a</code>). Replace all <code>load %p</code> instructions after <code>s</code> (but before any other <code>store %p, _</code>s, in program order) with <code>%v</code>’s def. If <code>s</code> is not the last store in this block, we’re done.</p> <p>Otherwise, follow the use-def chain of <code>%p</code> to successors which use <code>%p</code>, i.e., successors whose <code>goto</code> case has <code>%p</code> as at least one argument. Recurse into those successors, and now replacing the pointer <code>%p</code> of interest with the parameters of the successor which were set to <code>%p</code> (more than one argument may be <code>%p</code>).</p> <p>If successor <code>@b</code> loads from one of the registers holding <code>%p</code>, replace all such loads before a store to <code>%p</code>. We also now need to send <code>%v</code> into <code>@b</code> somehow.</p> <p>This is where we run into something of a wrinkle. If <code>@b</code> has exactly one predecessor, we need to add a new block argument to pass whichever register is holding <code>%v</code> (which exists by induction). If <code>%v</code> is already passed into <code>@b</code> by another argument, we can use that one.</p> <p>However, if <code>@b</code> has multiple predecessors, we need to make sure that every path from <code>@a</code> to <code>@b</code> sends <code>%v</code>, and canonicalizing those will be tricky. Worse still, if <code>@b</code> is in <code>@a</code>’s domination frontier, a <em>different</em> store could be contributing to that load! For this reason, dataflow from stores to loads is not a great strategy.</p> <p>Instead, we’ll look at dataflow from loads backwards to stores (in general, dataflow from uses to defs tends to be more useful), which we can use to augment the above forward dataflow analysis to remove the complex issues around domination frontiers.</p> <h3 id="dependency-analysis"><a href="#dependency-analysis">Dependency Analysis</a></h3> <p>Let’s analyze loads instead. For each <code>load %p</code> in <code>@a</code>, we want to determine all stores that could potentially contribute to its value. We can find those stores as follows:</p> <p>We want to be able to determine which register in a given block corresponds to the value of <code>%p</code>, and then find its last store in that block.</p> <p>To do this, we’ll flood-fill the CFG backwards in BFS order. This means that we’ll follow preds (through the use-def chain) recursively, visiting each pred before visiting their preds, and never revisiting a basic block (except we may need to come back to <code>@a</code> at the end).</p> <p>Determining the “equivalent”<sup id="fnref:equiv-reg" role="doc-noteref"><a href="#fn:equiv-reg" rel="footnote">12</a></sup> of <code>%p</code> in <code>@b</code> (we’ll call it <code>%p.b</code>) can be done recursively: while examining <code>@b</code>, follow the def of <code>%p.b</code>. If <code>%p.b</code> is a block parameter, for each pred <code>@c</code>, set <code>%p.c</code> to the corresponding argument in the <code>@b(...)</code> case in <code>@c</code>’s <code>goto</code>.</p> <p>Using this information, we can collect all stores that the load potentially depends on. If a predecessor <code>@b</code> stores to <code>%p.b</code>, we add the last such store in <code>@b</code> (in program order) to our set of stores, and do not recurse to <code>@b</code>’s preds (because this store overwrites all past stores). Note that we <em>may</em> revisit <code>@a</code> in this process, and collect a store to <code>%p</code> from it occurs in the block. This is necessary in the case of loops.</p> <p>The result is a set <code>stores</code> of <code>(store %p.s %v.s, @s)</code> pairs. In the process, we also collected a set of all blocks visited, <code>subgraph</code>, which are dominators of <code>@a</code> which we need to plumb a <code>%v.b</code> through. This process is called <em>memory dependency analysis</em>, and is a key component of many optimizations.</p> <blockquote id="note:2"> <p><a href="#note:2"><span>note</span></a></p> <p>Not all contributing operations are stores. Some may be references to globals (which we’re disregarding), or function arguments or the results of a function call (which means we probably can’t lift this load). For example <code>%p</code> gets traced all the way back to a function argument, there is a code path which loads from a pointer whose stores we can’t see.</p> </blockquote> <p>It may also trace back to a stack slot that is potentially not stored to. This means there is a code path that can potentially load uninitialized memory. Like LLVM, we can assume this is not observable behavior, so we can discount such dependencies. If all of the dependencies are uninitialized loads, we can potentially delete not just the load, but operations which depend on it (reverse dataflow analysis is the origin of so-called “time-traveling” UB).</p> <h3 id="lifting-loads"><a href="#lifting-loads">Lifting Loads</a></h3> <p>Now that we have the full set of dependency information, we can start lifting loads. Loads can be safely lifted when all of their dependencies are stores in the current function, or dependencies we can disregard thanks to UB in the surface language (such as <code>null</code> loads or uninitialized loads).</p> <blockquote id="note:3"> <p><a href="#note:3"><span>note</span></a></p> <p>There is a lot of fuss in this algorithm about plumbing values through block arguments. A lot of IRs make a simplifying change, where every block implicitly receives the registers from its dominators as block arguments.</p> <p>I am keeping the fuss because it makes it clearer what’s going on, but in practice, most of this plumbing, except at dominance frontiers, would be happening in the background.</p> </blockquote> <p>Suppose we can safely lift some load. Now we need to plumb the stored values down to the load. For each block <code>@b</code> in <code>subgraph</code> (all other blocks will now be in <code>subgraph</code> unless stated otherwise). We will be building two mappings: one <code>(@s, @b) -&gt; %v.s.b</code>, which is the register equivalent to <code>%v.s</code> in that block. We will also be building a map <code>@b -&gt; %v.b</code>, which is the value that <code>%p</code> must have in that block.</p> <ol> <li> <p>Prepare a work queue, with each <code>@s</code> in it initially.</p> </li> <li> <p>Pop a block <code>@a</code> form the queue. For each successor <code>@b</code> (in <code>subgraph</code>):</p> <ol> <li> <p>If <code>%v.b</code> isn’t already defined, add it as a block argument. Have <code>@a</code> pass <code>%v.a</code> to that argument.</p> </li> <li> <p>If <code>@b</code> hasn’t been visited yet, and isn’t the block containing the load we’re deleting, add it to the queue.</p> </li> </ol> </li> </ol> <p>Once we’re done, if <code>@a</code> is the block that contains the load, we can now replace all loads to <code>%p</code> before any stores to <code>%p</code> with <code>%v.a</code>.</p> <blockquote id="tip:1"> <p><a href="#tip:1"><span>tip</span></a></p> <p>There are cases where this whole process can be skipped, by applying a “peephole” optimization. For example, stores followed by loads within the same basic block can be optimized away locally, leaving the heavy-weight analysis for cross-block store/load pairs.</p> </blockquote> <h3 id="worked-example"><a href="#worked-example">Worked Example</a></h3> <p>Here’s the result of doing dependency analysis on our Fibonacci function. Each load is annotated with the blocks and stores in <code>stores</code>.</p> <div id="code:11"><figure><pre><code data-lang="go"><span>func</span> <span>&amp;</span><span>fib</span><span>(</span><span>%</span><span>n</span><span>:</span> <span>i32</span><span>)</span> <span>-&gt;</span> <span>(</span><span>i32</span><span>)</span> <span>{</span>
    <span>%</span><span>np</span> <span>=</span> <span>stack</span> <span>i32</span>
    <span>%</span><span>ap</span> <span>=</span> <span>stack</span> <span>i32</span>
    <span>%</span><span>bp</span> <span>=</span> <span>stack</span> <span>i32</span>

    <span>store</span> <span>%</span><span>np</span><span>,</span> <span>%</span><span>n</span>  <span>// S1</span>
    <span>store</span> <span>%</span><span>ap</span><span>,</span> <span>0</span>   <span>// S2</span>
    <span>store</span> <span>%</span><span>bp</span><span>,</span> <span>1</span>   <span>// S3</span>

    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>)</span>

  <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>:</span> <span>ptr</span><span>)</span><span>:</span>
    <span>// @entry: S1</span>
    <span>// @loop.body: S6</span>
    <span>%</span><span>n</span> <span>=</span> <span>load</span> <span>%</span><span>np</span>  <span>// L1</span>
    <span>%</span><span>cont</span> <span>=</span> <span>cmp</span><span>.</span><span>gt</span> <span>%</span><span>n</span><span>,</span> <span>0</span>

    <span>goto</span> <span>%</span><span>cont</span> <span>{</span>
      <span>0</span> <span>-&gt;</span> <span>@</span><span>exit</span><span>(</span><span>%</span><span>ap</span><span>)</span>
      <span>1</span> <span>-&gt;</span> <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>),</span>
    <span>}</span>

  <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>:</span> <span>ptr</span><span>)</span><span>:</span>
    <span>// @entry: S1</span>
    <span>// @loop.body: S4</span>
    <span>%</span><span>a</span> <span>=</span> <span>load</span> <span>%</span><span>ap</span>  <span>// L2</span>
    <span>// @entry: S2</span>
    <span>// @loop.body: S5</span>
    <span>%</span><span>b</span> <span>=</span> <span>load</span> <span>%</span><span>bp</span>  <span>// L3</span>
    <span>%</span><span>c</span> <span>=</span> <span>add</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span>
    <span>store</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>b</span> <span>// S4</span>
    <span>store</span> <span>%</span><span>bp</span><span>,</span> <span>%</span><span>c</span> <span>// S5</span>

    <span>// @entry: S1</span>
    <span>// @loop.body: S6</span>
    <span>%</span><span>n</span>   <span>=</span> <span>load</span> <span>%</span><span>np</span>  <span>// L3</span>
    <span>%</span><span>n</span><span>.2</span> <span>=</span> <span>sub</span> <span>%</span><span>n</span><span>,</span> <span>1</span>
    <span>store</span> <span>%</span><span>np</span><span>,</span> <span>%</span><span>n</span><span>.2</span>  <span>// S6</span>

    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>)</span>

  <span>@</span><span>exit</span><span>(</span><span>%</span><span>ap</span><span>:</span> <span>ptr</span><span>)</span><span>:</span>
    <span>// @entry: S2</span>
    <span>// @loop.body: S5</span>
    <span>%</span><span>a</span> <span>=</span> <span>load</span> <span>%</span><span>ap</span>  <span>// L4</span>
    <span>goto</span> <span>@</span><span>ret</span><span>(</span><span>%</span><span>ap</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>Let’s look at <code>L1</code>. Is contributing loads are in <code>@entry</code> and <code>@loop.body</code>. So we add a new parameter <code>%n</code>: in <code>@entry</code>, we call that parameter with <code>%n</code> (since that’s stored to it in <code>@entry</code>), while in <code>@loop.body</code>, we pass <code>%n.2</code>.</p> <p>What about L4? The contributing loads are also in <code>@entry</code> and <code>@loop.body</code>, but one of those isn’t a pred of <code>@exit</code>. <code>@loop.start</code> is also in the subgraph for this load, though. So, starting from <code>@entry</code>, we add a new parameter <code>%a</code> to <code>@loop.body</code> and feed <code>0</code> (the stored value, an immediate this time) through it. Now looking at <code>@loop.body</code>, we see there is already a parameter for this load (<code>%a</code>), so we just pass <code>%b</code> as that argument. Now we process <code>@loop.start</code>, which <code>@entry</code> pushed onto the queue. <code>@exit</code> gets a new parameter <code>%a</code>, which is fed <code>@loop.start</code>’s own <code>%a</code>. We do not re-process <code>@loop.body</code>, even though it also appears in <code>@loop.start</code>’s gotos, because we already visited it.</p> <p>After doing this for the other two loads, we get this:</p> <div id="code:12"><figure><pre><code data-lang="go"><span>func</span> <span>&amp;</span><span>fib</span><span>(</span><span>%</span><span>n</span><span>:</span> <span>i32</span><span>)</span> <span>-&gt;</span> <span>(</span><span>i32</span><span>)</span> <span>{</span>
    <span>%</span><span>np</span> <span>=</span> <span>stack</span> <span>i32</span>
    <span>%</span><span>ap</span> <span>=</span> <span>stack</span> <span>i32</span>
    <span>%</span><span>bp</span> <span>=</span> <span>stack</span> <span>i32</span>

    <span>store</span> <span>%</span><span>np</span><span>,</span> <span>%</span><span>n</span>  <span>// S1</span>
    <span>store</span> <span>%</span><span>ap</span><span>,</span> <span>0</span>   <span>// S2</span>
    <span>store</span> <span>%</span><span>bp</span><span>,</span> <span>1</span>   <span>// S3</span>

    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>,</span> <span>%</span><span>n</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>)</span>

  <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>:</span> <span>ptr</span><span>,</span> <span>%</span><span>n</span><span>,</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span><span>:</span> <span>i32</span><span>)</span><span>:</span>
    <span>// @entry: S1</span>
    <span>// @loop.body: S6</span>
    <span>// %n = load %np  // L1</span>
    <span>%</span><span>cont</span> <span>=</span> <span>cmp</span><span>.</span><span>gt</span> <span>%</span><span>n</span><span>,</span> <span>0</span>

    <span>goto</span> <span>%</span><span>cont</span> <span>{</span>
      <span>0</span> <span>-&gt;</span> <span>@</span><span>exit</span><span>(</span><span>%</span><span>ap</span><span>,</span> <span>%</span><span>a</span><span>)</span>
      <span>1</span> <span>-&gt;</span> <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>,</span> <span>%</span><span>n</span><span>,</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span><span>),</span>
    <span>}</span>

  <span>@</span><span>loop</span><span>.</span><span>body</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>:</span> <span>ptr</span><span>,</span> <span>%</span><span>n</span><span>,</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span><span>:</span> <span>i32</span><span>)</span><span>:</span>
    <span>// @entry: S1</span>
    <span>// @loop.body: S4</span>
    <span>// %a = load %ap  // L2</span>
    <span>// @entry: S2</span>
    <span>// @loop.body: S5</span>
    <span>// %b = load %bp  // L3</span>
    <span>%</span><span>c</span> <span>=</span> <span>add</span> <span>%</span><span>a</span><span>,</span> <span>%</span><span>b</span>
    <span>store</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>b</span> <span>// S4</span>
    <span>store</span> <span>%</span><span>bp</span><span>,</span> <span>%</span><span>c</span> <span>// S5</span>

    <span>// @entry: S1</span>
    <span>// @loop.body: S6</span>
    <span>// %n   = load %np  // L3</span>
    <span>%</span><span>n</span><span>.2</span> <span>=</span> <span>sub</span> <span>%</span><span>n</span><span>,</span> <span>1</span>
    <span>store</span> <span>%</span><span>np</span><span>,</span> <span>%</span><span>n</span><span>.2</span>  <span>// S6</span>

    <span>goto</span> <span>@</span><span>loop</span><span>.</span><span>start</span><span>(</span><span>%</span><span>np</span><span>,</span> <span>%</span><span>ap</span><span>,</span> <span>%</span><span>bp</span><span>,</span> <span>%</span><span>n</span><span>.2</span><span>,</span> <span>%</span><span>b</span><span>,</span> <span>%</span><span>c</span><span>)</span>

  <span>@</span><span>exit</span><span>(</span><span>%</span><span>ap</span><span>:</span> <span>ptr</span><span>,</span> <span>%</span><span>a</span><span>:</span> <span>i32</span><span>)</span><span>:</span>
    <span>// @entry: S2</span>
    <span>// @loop.body: S5</span>
    <span>// %a = load %ap  // L4</span>
    <span>goto</span> <span>@</span><span>ret</span><span>(</span><span>%</span><span>a</span><span>)</span>
<span>}</span></code></pre></figure></div> <p>After lifting, if we know that a stack slot’s pointer does not escape (i.e., none of its uses wind up going into a function call<sup id="fnref:calls-only" role="doc-noteref"><a href="#fn:calls-only" rel="footnote">13</a></sup>) or a write to a global (or a pointer that escapes), we can delete every store to that pointer. If we delete every store to a stack slot, we can delete the stack slot altogether (there should be no loads left for that stack slot at this point).</p> <h3 id="complications"><a href="#complications">Complications</a></h3> <p>This analysis is simple, because it assumes pointers do not alias in general. Alias analysis is necessary for more accurate dependency analysis. This is necessary, for example, for lifting loads of fields of structs through subobject pointers, and dealing with pointer arithmetic in general.</p> <p>However, our dependency analysis <em>is</em> robust to passing different pointers as arguments to the same block from different predecessors. This is the case that is specifically handled by all of the fussing about with dominance frontiers. This robustness ultimately comes from SSA’s circuital nature.</p> <p>Similarly, this analysis needs to be tweaked to deal with something like <code>select %cond, %a, %b</code> (a ternary, essentially). <code>select</code>s of pointers need to be replaced with <code>select</code>s of the loaded values, which means we need to do the lifting transformation “all at once”: lifting some liftable loads will leave the IR in an inconsistent state, until all of them have been lifted.</p> <h2 id="cleanup-passes"><a href="#cleanup-passes">Cleanup Passes</a></h2> <p>Many optimizations will make a mess of the CFG, so it’s useful to have simple passes that “clean up” the mess left by transformations. Here’s some easy examples.</p> <h3 id="unused-result-elimination"><a href="#unused-result-elimination">Unused Result Elimination</a></h3> <p>If an operation’s result has zero uses, and the operation has no side-effects, it can be deleted. This allows us to then delete operations that it depended on that now have no side effects. Doing this is very simple, due to the circuital nature of SSA: collect all instructions whose outputs have zero uses, and delete them. Then, examine the defs of their operands; if those operations now have no uses, delete them, and recurse.</p> <p>This bubbles up all the way to block arguments. Deleting block arguments is a bit trickier, but we can use a work queue to do it. Put all of the blocks into a work queue.</p> <ol> <li> <p>Pop a block from the queue.</p> </li> <li> <p>Run unused result elimination on its operations.</p> </li> <li> <p>If it now has parameters with no uses, remove those parameters.</p> </li> <li> <p>For each pred, delete the corresponding arguments to this block. Then, Place those preds into the work queue (since some of their operations may have lost their last use).</p> </li> <li> <p>If there is still work left, go to 1.</p> </li> </ol> <h3 id="simplifying-the-cfg"><a href="#simplifying-the-cfg">Simplifying the CFG</a></h3> <p>There are many CFG configurations that are redundant and can be simplified to reduce the number of basic blocks.</p> <p>For example, unreachable code can help delete blocks. Other optimizations may cause the <code>goto</code> at the end of a function to be empty (because all of its successors were optimized away). We treat an empty <code>goto</code> as being unreachable (since it has no cases!), so we can delete every operation in the block up to the last non-pure operation. If we delete every instruction in the block, we can delete the block entirely, and delete it from its preds’ <code>goto</code>s. This is a form of <em>dead code elimination</em>, or DCE, which combines with the previous optimization to aggressively delete redundant code.</p> <p>Some jumps are redundant. For example, if a block has exactly one pred and one successor, the pred’s <code>goto</code> case for that block can be wired directly to the successor. Similarly, if two blocks are each other’s unique predecessor/successor, they can be <em>fused</em>, creating a single block by connecting the input blocks’ circuits directly, instead of through a <code>goto</code>.</p> <p>If we have a ternary <code>select</code> operation, we can do more sophisticated fusion. If a block has two successors, both of which the same unique successor, and those successors consist only of gotos, we can fuse all four blocks, replacing the CFG diamond with a <code>select</code>. In terms of C, this is this transformation:</p> <div> <div id="code:13"><figure><pre><code data-lang="c"><span>// Before.</span>
<span>int</span> <span>x</span><span>;</span>
<span>if</span> <span>(</span><span>cond</span><span>)</span> <span>{</span>
  <span>x</span> <span>=</span> <span>a</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
  <span>x</span> <span>=</span> <span>0</span><span>;</span>
<span>}</span></code></pre></figure></div> <div id="code:14"><figure><pre><code data-lang="c"><span>// After.</span>
<span>int</span> <span>x</span> <span>=</span> <span>cond</span> <span>?</span> <span>a</span> <span>:</span> <span>0</span><span>;</span></code></pre></figure></div> </div> <p>LLVM’s CFG simplification pass is very sophisticated and can eliminate complex forms of control flow.</p> <h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2> <p>I am hoping to write more about SSA optimization passes. This is a very rich subject, and viewing optimizations in isolation is a great way to understand how a sophisticated optimization pipeline is built out of simple, dumb components.</p> <p>It’s also a practical application of graph theory that shows just how powerful it can be, and (at least in my opinion), is an intuitive setting for understanding graph theory, which can feel very abstract otherwise.</p> <p>In the future, I’d like to cover CSE/GVN, loop optimizations, and, if I’m feeling brave, getting out of SSA into a finite-register machine (backends are not my strong suit!).</p>  </div></div>
  </body>
</html>
