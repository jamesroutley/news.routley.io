<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/LearningCircuit/local-deep-research">Original</a>
    <h1>Local Deep Research ‚Äì ArXiv, wiki and other searches included</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">A powerful AI-powered research assistant that performs deep, iterative analysis using multiple LLMs and web searches. The system can be run locally for privacy or configured to use cloud-based LLMs for enhanced capabilities.</p>

<ul dir="auto">
<li>
<p dir="auto">üîç <strong>Advanced Research Capabilities</strong></p>
<ul dir="auto">
<li>Automated deep research with intelligent follow-up questions</li>
<li>Citation tracking and source verification</li>
<li>Multi-iteration analysis for comprehensive coverage</li>
<li>Full webpage content analysis (not just snippets)</li>
</ul>
</li>
<li>
<p dir="auto">ü§ñ <strong>Flexible LLM Support</strong></p>
<ul dir="auto">
<li>Local AI processing with Ollama models</li>
<li>Cloud LLM support (Claude, GPT)</li>
<li>Supports all Langchain models</li>
<li>Configurable model selection based on needs</li>
</ul>
</li>
<li>
<p dir="auto">üìä <strong>Rich Output Options</strong></p>
<ul dir="auto">
<li>Detailed research findings with citations</li>
<li>Comprehensive research reports</li>
<li>Quick summaries for rapid insights</li>
<li>Source tracking and verification</li>
</ul>
</li>
<li>
<p dir="auto">üîí <strong>Privacy-Focused</strong></p>
<ul dir="auto">
<li>Runs entirely on your machine when using local models</li>
<li>Configurable search settings</li>
<li>Transparent data handling</li>
</ul>
</li>
<li>
<p dir="auto">üåê <strong>Enhanced Search Integration</strong></p>
<ul dir="auto">
<li><strong>Auto-selection of search sources</strong>: The &#34;auto&#34; search engine intelligently analyzes your query and selects the most appropriate search engine based on the query content</li>
<li>Wikipedia integration for factual knowledge</li>
<li>arXiv integration for scientific papers and academic research</li>
<li>DuckDuckGo integration for web searches (may experience rate limiting)</li>
<li>SerpAPI integration for Google search results (requires API key)</li>
<li>The Guardian integration for news articles and journalism (requires API key)</li>
<li><strong>Local RAG search for private documents</strong> - search your own documents with vector embeddings</li>
<li>Full webpage content retrieval</li>
<li>Source filtering and validation</li>
<li>Configurable search parameters</li>
</ul>
</li>
<li>
<p dir="auto">üìë <strong>Local Document Search (RAG)</strong></p>
<ul dir="auto">
<li>Vector embedding-based search of your local documents</li>
<li>Create custom document collections for different topics</li>
<li>Privacy-preserving - your documents stay on your machine</li>
<li>Intelligent chunking and retrieval</li>
<li>Compatible with various document formats (PDF, text, markdown, etc.)</li>
<li>Automatic integration with meta-search for unified queries</li>
</ul>
</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Example Research: Fusion Energy Developments</h2><a id="user-content-example-research-fusion-energy-developments" aria-label="Permalink: Example Research: Fusion Energy Developments" href="#example-research-fusion-energy-developments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The repository includes complete research examples demonstrating the tool&#39;s capabilities. For instance, our <a href="https://github.com/LearningCircuit/local-deep-research/blob/main/examples/fusion-energy-research-developments.md">fusion energy research analysis</a> provides a comprehensive overview of:</p>
<ul dir="auto">
<li>Latest scientific breakthroughs in fusion research (2022-2025)</li>
<li>Private sector funding developments exceeding $6 billion</li>
<li>Expert projections for commercial fusion energy timelines</li>
<li>Regulatory frameworks being developed for fusion deployment</li>
<li>Technical challenges that must be overcome for commercial viability</li>
</ul>
<p dir="auto">This example showcases the system&#39;s ability to perform multiple research iterations, follow evidence trails across scientific and commercial domains, and synthesize information from diverse sources while maintaining proper citation.</p>

<ol dir="auto">
<li>Clone the repository:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/local-deep-research.git
cd local-deep-research"><pre>git clone https://github.com/yourusername/local-deep-research.git
<span>cd</span> local-deep-research</pre></div>
<ol start="2" dir="auto">
<li>Install dependencies:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ol start="3" dir="auto">
<li>Install Ollama (for local models):</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Install Ollama from https://ollama.ai
ollama pull mistral  # Default model - many work really well choose best for your hardware (fits in GPU)"><pre><span><span>#</span> Install Ollama from https://ollama.ai</span>
ollama pull mistral  <span><span>#</span> Default model - many work really well choose best for your hardware (fits in GPU)</span></pre></div>
<ol start="4" dir="auto">
<li>Configure environment variables:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Copy the template
cp .env.template .env

# Edit .env with your API keys (if using cloud LLMs)
ANTHROPIC_API_KEY=your-api-key-here  # For Claude
OPENAI_API_KEY=your-openai-key-here  # For GPT models
GUARDIAN_API_KEY=your-guardian-api-key-here  # For The Guardian search"><pre><span><span>#</span> Copy the template</span>
cp .env.template .env

<span><span>#</span> Edit .env with your API keys (if using cloud LLMs)</span>
ANTHROPIC_API_KEY=your-api-key-here  <span><span>#</span> For Claude</span>
OPENAI_API_KEY=your-openai-key-here  <span><span>#</span> For GPT models</span>
GUARDIAN_API_KEY=your-guardian-api-key-here  <span><span>#</span> For The Guardian search</span></pre></div>

<p dir="auto">Terminal usage (not recommended):</p>


<p dir="auto">The project includes a web interface for a more user-friendly experience:</p>

<p dir="auto">This will start a local web server, accessible at <code>http://127.0.0.1:5000</code> in your browser.</p>

<ul dir="auto">
<li><strong>Dashboard</strong>: Intuitive interface for starting and managing research queries</li>
<li><strong>Real-time Updates</strong>: Track research progress with live updates</li>
<li><strong>Research History</strong>: Access and manage past research queries</li>
<li><strong>PDF Export</strong>: Download completed research reports as PDF documents</li>
<li><strong>Research Management</strong>: Terminate ongoing research processes or delete past records</li>
</ul>

<p dir="auto">Key settings in <code>config.py</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# LLM Configuration
DEFAULT_MODEL = &#34;mistral&#34;  # Change based on your needs
DEFAULT_TEMPERATURE = 0.7
MAX_TOKENS = 8000

# Search Configuration
MAX_SEARCH_RESULTS = 40
SEARCH_REGION = &#34;us-en&#34;
TIME_PERIOD = &#34;y&#34;
SAFE_SEARCH = True
SEARCH_SNIPPETS_ONLY = False

# Choose search tool: &#34;wiki&#34;, &#34;arxiv&#34;, &#34;duckduckgo&#34;, &#34;guardian&#34;, &#34;serp&#34;, &#34;local_all&#34;, or &#34;auto&#34;
search_tool = &#34;auto&#34;  # &#34;auto&#34; will intelligently select the best search engine for your query"><pre><span># LLM Configuration</span>
<span>DEFAULT_MODEL</span> <span>=</span> <span>&#34;mistral&#34;</span>  <span># Change based on your needs</span>
<span>DEFAULT_TEMPERATURE</span> <span>=</span> <span>0.7</span>
<span>MAX_TOKENS</span> <span>=</span> <span>8000</span>

<span># Search Configuration</span>
<span>MAX_SEARCH_RESULTS</span> <span>=</span> <span>40</span>
<span>SEARCH_REGION</span> <span>=</span> <span>&#34;us-en&#34;</span>
<span>TIME_PERIOD</span> <span>=</span> <span>&#34;y&#34;</span>
<span>SAFE_SEARCH</span> <span>=</span> <span>True</span>
<span>SEARCH_SNIPPETS_ONLY</span> <span>=</span> <span>False</span>

<span># Choose search tool: &#34;wiki&#34;, &#34;arxiv&#34;, &#34;duckduckgo&#34;, &#34;guardian&#34;, &#34;serp&#34;, &#34;local_all&#34;, or &#34;auto&#34;</span>
<span>search_tool</span> <span>=</span> <span>&#34;auto&#34;</span>  <span># &#34;auto&#34; will intelligently select the best search engine for your query</span></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Local Document Search (RAG)</h2><a id="user-content-local-document-search-rag" aria-label="Permalink: Local Document Search (RAG)" href="#local-document-search-rag"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The system includes powerful local document search capabilities using Retrieval-Augmented Generation (RAG). This allows you to search and retrieve content from your own document collections.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Setting Up Local Collections</h3><a id="user-content-setting-up-local-collections" aria-label="Permalink: Setting Up Local Collections" href="#setting-up-local-collections"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Create a file named <code>local_collections.py</code> in the project root directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# local_collections.py
import os
from typing import Dict, Any

# Registry of local document collections
LOCAL_COLLECTIONS = {
    # Research Papers Collection
    &#34;research_papers&#34;: {
        &#34;name&#34;: &#34;Research Papers&#34;,
        &#34;description&#34;: &#34;Academic research papers and articles&#34;,
        &#34;paths&#34;: [os.path.abspath(&#34;local_search_files/research_papers&#34;)],  # Use absolute paths
        &#34;enabled&#34;: True,
        &#34;embedding_model&#34;: &#34;all-MiniLM-L6-v2&#34;,
        &#34;embedding_device&#34;: &#34;cpu&#34;,
        &#34;embedding_model_type&#34;: &#34;sentence_transformers&#34;,
        &#34;max_results&#34;: 20,
        &#34;max_filtered_results&#34;: 5,
        &#34;chunk_size&#34;: 800,  # Smaller chunks for academic content
        &#34;chunk_overlap&#34;: 150,
        &#34;cache_dir&#34;: &#34;.cache/local_search/research_papers&#34;
    },
    
    # Personal Notes Collection
    &#34;personal_notes&#34;: {
        &#34;name&#34;: &#34;Personal Notes&#34;,
        &#34;description&#34;: &#34;Personal notes and documents&#34;,
        &#34;paths&#34;: [os.path.abspath(&#34;local_search_files/personal_notes&#34;)],  # Use absolute paths
        &#34;enabled&#34;: True,
        &#34;embedding_model&#34;: &#34;all-MiniLM-L6-v2&#34;,
        &#34;embedding_device&#34;: &#34;cpu&#34;,
        &#34;embedding_model_type&#34;: &#34;sentence_transformers&#34;,
        &#34;max_results&#34;: 30,
        &#34;max_filtered_results&#34;: 10,
        &#34;chunk_size&#34;: 500,  # Smaller chunks for notes
        &#34;chunk_overlap&#34;: 100,
        &#34;cache_dir&#34;: &#34;.cache/local_search/personal_notes&#34;
    }
}

Create the directories for your collections:
```bash
mkdir -p local_search_files/research_papers
mkdir -p local_search_files/personal_notes"><pre><span># local_collections.py</span>
<span>import</span> <span>os</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Dict</span>, <span>Any</span>

<span># Registry of local document collections</span>
<span>LOCAL_COLLECTIONS</span> <span>=</span> {
    <span># Research Papers Collection</span>
    <span>&#34;research_papers&#34;</span>: {
        <span>&#34;name&#34;</span>: <span>&#34;Research Papers&#34;</span>,
        <span>&#34;description&#34;</span>: <span>&#34;Academic research papers and articles&#34;</span>,
        <span>&#34;paths&#34;</span>: [<span>os</span>.<span>path</span>.<span>abspath</span>(<span>&#34;local_search_files/research_papers&#34;</span>)],  <span># Use absolute paths</span>
        <span>&#34;enabled&#34;</span>: <span>True</span>,
        <span>&#34;embedding_model&#34;</span>: <span>&#34;all-MiniLM-L6-v2&#34;</span>,
        <span>&#34;embedding_device&#34;</span>: <span>&#34;cpu&#34;</span>,
        <span>&#34;embedding_model_type&#34;</span>: <span>&#34;sentence_transformers&#34;</span>,
        <span>&#34;max_results&#34;</span>: <span>20</span>,
        <span>&#34;max_filtered_results&#34;</span>: <span>5</span>,
        <span>&#34;chunk_size&#34;</span>: <span>800</span>,  <span># Smaller chunks for academic content</span>
        <span>&#34;chunk_overlap&#34;</span>: <span>150</span>,
        <span>&#34;cache_dir&#34;</span>: <span>&#34;.cache/local_search/research_papers&#34;</span>
    },
    
    <span># Personal Notes Collection</span>
    <span>&#34;personal_notes&#34;</span>: {
        <span>&#34;name&#34;</span>: <span>&#34;Personal Notes&#34;</span>,
        <span>&#34;description&#34;</span>: <span>&#34;Personal notes and documents&#34;</span>,
        <span>&#34;paths&#34;</span>: [<span>os</span>.<span>path</span>.<span>abspath</span>(<span>&#34;local_search_files/personal_notes&#34;</span>)],  <span># Use absolute paths</span>
        <span>&#34;enabled&#34;</span>: <span>True</span>,
        <span>&#34;embedding_model&#34;</span>: <span>&#34;all-MiniLM-L6-v2&#34;</span>,
        <span>&#34;embedding_device&#34;</span>: <span>&#34;cpu&#34;</span>,
        <span>&#34;embedding_model_type&#34;</span>: <span>&#34;sentence_transformers&#34;</span>,
        <span>&#34;max_results&#34;</span>: <span>30</span>,
        <span>&#34;max_filtered_results&#34;</span>: <span>10</span>,
        <span>&#34;chunk_size&#34;</span>: <span>500</span>,  <span># Smaller chunks for notes</span>
        <span>&#34;chunk_overlap&#34;</span>: <span>100</span>,
        <span>&#34;cache_dir&#34;</span>: <span>&#34;.cache/local_search/personal_notes&#34;</span>
    }
}

<span>Create</span> <span>the</span> <span>directories</span> <span>for</span> <span>your</span> <span>collections</span>:
<span>``</span>`<span>bash</span>
<span>mkdir</span> <span>-</span><span>p</span> <span>local_search_files</span><span>/</span><span>research_papers</span>
<span>mkdir</span> <span>-</span><span>p</span> <span>local_search_files</span><span>/</span><span>personal_notes</span></pre></div>
<p dir="auto">Add your documents to these folders, and the system will automatically index them and make them available for searching.</p>

<p dir="auto">You can use local search in several ways:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Auto-selection</strong>: Set <code>search_tool = &#34;auto&#34;</code> in <code>config.py</code> and the system will automatically use your local collections when appropriate for the query.</p>
</li>
<li>
<p dir="auto"><strong>Explicit Selection</strong>: Set <code>search_tool = &#34;research_papers&#34;</code> to search only that specific collection.</p>
</li>
<li>
<p dir="auto"><strong>Search All Local Collections</strong>: Set <code>search_tool = &#34;local_all&#34;</code> to search across all your local document collections.</p>
</li>
<li>
<p dir="auto"><strong>Query Syntax</strong>: Use <code>collection:collection_name your query</code> to target a specific collection within a query.</p>
</li>
</ol>

<p dir="auto">The system supports multiple search engines that can be selected by changing the <code>search_tool</code> variable in <code>config.py</code>:</p>
<ul dir="auto">
<li><strong>Auto</strong> (<code>auto</code>): Intelligent search engine selector that analyzes your query and chooses the most appropriate source (Wikipedia, arXiv, local collections, etc.)</li>
<li><strong>Wikipedia</strong> (<code>wiki</code>): Best for general knowledge, facts, and overview information</li>
<li><strong>arXiv</strong> (<code>arxiv</code>): Great for scientific and academic research, accessing preprints and papers</li>
<li><strong>DuckDuckGo</strong> (<code>duckduckgo</code>): General web search that doesn&#39;t require an API key</li>
<li><strong>The Guardian</strong> (<code>guardian</code>): Quality journalism and news articles (requires an API key)</li>
<li><strong>SerpAPI</strong> (<code>serp</code>): Google search results (requires an API key)</li>
<li><strong>Local Collections</strong>: Any collections defined in your <code>local_collections.py</code> file</li>
</ul>
<blockquote>
<p dir="auto"><strong>Note:</strong> The &#34;auto&#34; option will intelligently select the best search engine based on your query. For example, if you ask about physics research papers, it might select arXiv or your research_papers collection, while if you ask about current events, it might select The Guardian or DuckDuckGo.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Support Free Knowledge:</strong> If you frequently use the search engines in this tool, please consider making a donation to these organizations. They provide valuable services and rely on user support to maintain their operations:</p>
<ul dir="auto">
<li><a href="https://donate.wikimedia.org" rel="nofollow">Donate to Wikipedia</a></li>
<li><a href="https://support.theguardian.com" rel="nofollow">Support The Guardian</a></li>
<li><a href="https://arxiv.org/about/give" rel="nofollow">Support arXiv</a></li>
<li><a href="https://duckduckgo.com/donations" rel="nofollow">Donate to DuckDuckGo</a></li>
</ul>
</blockquote>

<p dir="auto">This project is licensed under the MIT License - see the <a href="https://jvns.ca/LearningCircuit/local-deep-research/blob/main/LICENSE">LICENSE</a> file for details.</p>

<ul dir="auto">
<li>Built with <a href="https://ollama.ai" rel="nofollow">Ollama</a> for local AI processing</li>
<li>Search powered by multiple sources:
<ul dir="auto">
<li><a href="https://www.wikipedia.org/" rel="nofollow">Wikipedia</a> for factual knowledge (default search engine)</li>
<li><a href="https://arxiv.org/" rel="nofollow">arXiv</a> for scientific papers</li>
<li><a href="https://duckduckgo.com" rel="nofollow">DuckDuckGo</a> for web search</li>
<li><a href="https://www.theguardian.com/" rel="nofollow">The Guardian</a> for quality journalism</li>
<li><a href="https://serpapi.com" rel="nofollow">SerpAPI</a> for Google search results (requires API key)</li>
</ul>
</li>
<li>Built on <a href="https://github.com/hwchase17/langchain">LangChain</a> framework</li>
<li>Uses <a href="https://github.com/miso-belica/justext">justext</a> for content extraction</li>
<li><a href="https://playwright.dev" rel="nofollow">Playwright</a> for web content retrieval</li>
<li>Uses <a href="https://github.com/facebookresearch/faiss">FAISS</a> for vector similarity search</li>
<li>Uses <a href="https://github.com/UKPLab/sentence-transformers">sentence-transformers</a> for embeddings</li>
</ul>

<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create your feature branch (<code>git checkout -b feature/AmazingFeature</code>)</li>
<li>Commit your changes (<code>git commit -m &#39;Add some AmazingFeature&#39;</code>)</li>
<li>Push to the branch (<code>git push origin feature/AmazingFeature</code>)</li>
<li>Open a Pull Request</li>
</ol>
</article></div></div>
  </body>
</html>
