<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html">Original</a>
    <h1>Overfitting and the strong version of Goodhartâ€™s law</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <meta charset="utf-8"/>
<!-- Google tag (gtag.js) -->



<pre>Increased efficiency can sometimes, counterintuitively, lead to worse outcomes. 
This is true almost everywhere.  
We will name this phenomenon the strong version of [Goodhart&#39;s law](https://en.wikipedia.org/wiki/Goodhart%27s_law). 
As one example, more efficient centralized tracking of student progress by standardized testing 
seems like such a good idea that well-intentioned laws [mandate it](https://en.wikipedia.org/wiki/No_Child_Left_Behind_Act). 
However, testing also incentivizes schools to focus more on teaching students to test well, and less on teaching broadly useful skills.
As a result, it can cause overall educational outcomes to become worse. Similar examples abound, in politics, economics, health, science, and many other fields.

This same counterintuitive relationship between efficiency and outcome occurs in machine learning, where it is called overfitting.
Overfitting is heavily studied, somewhat theoretically understood, and has well known mitigations.
This connection between the strong version of Goodhart&#39;s law in general, and overfitting in machine learning, provides a new 
lens for understanding bad outcomes, and new ideas for fixing them.


Overfitting and Goodhart&#39;s law
==========================

In machine learning (ML), **overfitting** is a pervasive phenomenon. We want to train an ML model to achieve some goal. We can&#39;t directly fit the model to the goal, so we instead train the model using some proxy which is *similar* to the goal.

![](/assets/cartoon-conversation.png width=&#34;300px&#34; border=&#34;1&#34;)

For instance,
as an occasional computer vision researcher,
my 
goal is sometimes to
prove that my new image classification model works well.
I accomplish this by measuring its accuracy, after asking 
it to label images (is this image a cat or a dog or a frog or a truck or a ...)
from a standardized 
[test dataset of images](https://paperswithcode.com/dataset/cifar-10).
I&#39;m not allowed to train my model on the test dataset though (that would be cheating), 
so I instead train the model on a *proxy* dataset, called the training dataset.
I also can&#39;t directly target prediction accuracy during training[^accuracytarget], so I instead target a *proxy* objective which is only related to accuracy.
So rather than training my model on the goal I care about -- classification accuracy on a test dataset -- I instead train it using a *proxy objective* on a *proxy dataset*.

At first everything goes as we hope -- the proxy improves, and since the goal is similar to the proxy, it also improves.

![](/assets/cartoon-early.png width=&#34;444px&#34; border=&#34;1&#34;)

As we continue optimizing the proxy though, we eventually exhaust the useable similarity between proxy and goal. The proxy keeps on getting better, but the goal stops improving. In machine learning we call this overfitting, but it is also an example of Goodhart&#39;s law.

![](/assets/cartoon-mid.png width=&#34;444px&#34; border=&#34;1&#34;)

[Goodhart&#39;s law](https://en.wikipedia.org/wiki/Goodhart%27s_law) states that, *when a measure becomes a target, it ceases to be a good measure*[^strathern]. 
Goodhart proposed this in the context of monetary policy, but it applies far more broadly. In the context of overfitting in machine learning, it describes how the proxy objective we optimize ceases to be a good measure of the objective we care about.

The strong version of Goodhart&#39;s law: as we become too efficient, the thing we care about grows worse
==========================

If we keep on optimizing the proxy objective, even after our goal stops improving, something more worrying happens. The goal often starts getting *worse*, even as our proxy objective continues to improve. Not just a little bit worse either -- often the goal will diverge towards infinity.

This is an [extremely](https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture6_overfitting.pdf) [general](https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf) [phenomenon](https://scholar.google.com/scholar?hl=en&amp;q=overfitting) in machine learning. It mostly doesn&#39;t matter what our goal and proxy are, or what model architecture we use[^overfittinggenerality]. If we are very efficient at optimizing a proxy, then we make the thing it is a proxy for grow worse.

![](/assets/cartoon-late.png width=&#34;444px&#34; border=&#34;1&#34;)

Though this pheonomenon is often discussed, it doesn&#39;t seem to be named[^notoverfitting]. Let&#39;s call it **the strong version of Goodhart&#39;s law**[^strongunintended]. 
We can state it as:
&gt; *When a measure becomes a target,
&gt; if it is effectively optimized,
&gt; then the thing it is designed to measure will grow worse.*

Goodhart&#39;s law says that if you optimize a proxy, eventually the goal you care about will stop improving. 
The strong version of Goodhart&#39;s law differs 
in that it says that as you over-optimize, the goal you care about won&#39;t just stop improving,
but will instead grow much worse than if you had done nothing at all.

Goodhart&#39;s law applies well beyond economics, where it was originally proposed. Similarly, the strong version of Goodhart&#39;s law applies well beyond machine learning. I believe it can help us understand failures in economies, governments, and social systems.

Increasing efficiency and overfitting are happening everywhere
==========================

Increasing efficiency is permeating almost every aspect of our society. If the thing that is being made more efficient is beneficial, then the increased efficiency makes the world a better place (overall, the world [seems to be becoming a better place](https://ourworldindata.org/a-history-of-global-living-conditions-in-5-charts)). If the thing that is being made more efficient is socially harmful, then the consequences of greater efficiency are scary or depressing (think mass surveillance, or robotic weapons). What about the most common case though -- where the thing we are making more efficient is related, but not identical, to beneficial outcomes? What happens when we get better at something which is merely correlated with outcomes we care about?

In that case, we can overfit, the same as we do in machine learning. The outcomes we care about will improve for a while ... and then they will grow dramatically worse.

Below are a few, possibly facile, examples applying this analogy.

&gt; **Goal:** Educate children well  </pre>

<!-- Markdeep: -->





  </div></div>
  </body>
</html>
