<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/babysor/MockingBird">Original</a>
    <h1>Mocking Bird ‚Äì Realtime Voice Clone for Chinese</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
          <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg"><img src="https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg" alt="mockingbird"/></a></p>
<p dir="auto"><a href="http://choosealicense.com/licenses/mit/" rel="nofollow"><img src="https://camo.githubusercontent.com/dd1c858e94a371529a0a4c359bc95f18f09ba4a5fc0e658950bcb1383ea40fc9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e7376673f7374796c653d666c6174" alt="MIT License" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg?style=flat"/></a></p>
<blockquote>
<p dir="auto">English | <a href="https://github.com/babysor/MockingBird/blob/main/README-CN.md">‰∏≠Êñá</a></p>
</blockquote>
<h2 dir="auto"><a id="user-content-features" aria-hidden="true" href="#features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Features</h2>
<p dir="auto"><g-emoji alias="earth_africa" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png">üåç</g-emoji> <strong>Chinese</strong> supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.</p>
<p dir="auto"><g-emoji alias="star_struck" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f929.png">ü§©</g-emoji> <strong>PyTorch</strong> worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060</p>
<p dir="auto"><g-emoji alias="earth_africa" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png">üåç</g-emoji> <strong>Windows + Linux</strong> run in both Windows OS and linux OS (even in M1 MACOS)</p>
<p dir="auto"><g-emoji alias="star_struck" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f929.png">ü§©</g-emoji> <strong>Easy &amp; Awesome</strong> effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder</p>
<p dir="auto"><g-emoji alias="earth_africa" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png">üåç</g-emoji> <strong>Webserver Ready</strong> to serve your result with remote calling</p>
<h3 dir="auto"><a id="user-content-demo-video" aria-hidden="true" href="#demo-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://www.bilibili.com/video/BV17Q4y1B7mY/" rel="nofollow">DEMO VIDEO</a></h3>
<h2 dir="auto"><a id="user-content-quick-start" aria-hidden="true" href="#quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quick Start</h2>
<h3 dir="auto"><a id="user-content-1-install-requirements" aria-hidden="true" href="#1-install-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1. Install Requirements</h3>
<blockquote>
<p dir="auto">Follow the original repo to test if you got all environment ready.
**Python 3.7 or higher ** is needed to run the toolbox.</p>
</blockquote>
<ul dir="auto">
<li>Install <a href="https://pytorch.org/get-started/locally/" rel="nofollow">PyTorch</a>.</li>
</ul>
<blockquote>
<p dir="auto">If you get an <code>ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )</code> This error is probably due to a low version of python, try using 3.9 and it will install successfully</p>
</blockquote>
<ul dir="auto">
<li>Install <a href="https://ffmpeg.org/download.html#get-packages" rel="nofollow">ffmpeg</a>.</li>
<li>Run <code>pip install -r requirements.txt</code> to install the remaining necessary packages.</li>
<li>Install webrtcvad <code>pip install webrtcvad-wheels</code>(If you need)</li>
</ul>
<blockquote>
<p dir="auto">Note that we are using the pretrained encoder/vocoder but synthesizer, since the original model is incompatible with the Chinese sympols. It means the demo_cli is not working at this moment.</p>
</blockquote>
<h3 dir="auto"><a id="user-content-2-prepare-your-models" aria-hidden="true" href="#2-prepare-your-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2. Prepare your models</h3>
<p dir="auto">You can either train your models or use existing ones:</p>
<h4 dir="auto"><a id="user-content-21-train-encoder-with-your-dataset-optional" aria-hidden="true" href="#21-train-encoder-with-your-dataset-optional"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1 Train encoder with your dataset (Optional)</h4>
<ul dir="auto">
<li>
<p dir="auto">Preprocess with the audios and the mel spectrograms:
<code>python encoder_preprocess.py &lt;datasets_root&gt;</code> Allowing parameter <code>--dataset {dataset}</code> to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.</p>
</li>
<li>
<p dir="auto">Train the encoder: <code>python encoder_train.py my_run &lt;datasets_root&gt;/SV2TTS/encoder</code></p>
</li>
</ul>
<blockquote>
<p dir="auto">For training, the encoder uses visdom. You can disable it with <code>--no_visdom</code>, but it&#39;s nice to have. Run &#34;visdom&#34; in a separate CLI/process to start your visdom server.</p>
</blockquote>
<h4 dir="auto"><a id="user-content-22-train-synthesizer-with-your-dataset" aria-hidden="true" href="#22-train-synthesizer-with-your-dataset"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.2 Train synthesizer with your dataset</h4>
<ul dir="auto">
<li>
<p dir="auto">Download dataset and unzip: make sure you can access all .wav in folder</p>
</li>
<li>
<p dir="auto">Preprocess with the audios and the mel spectrograms:
<code>python pre.py &lt;datasets_root&gt;</code>
Allowing parameter <code>--dataset {dataset}</code> to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.</p>
</li>
<li>
<p dir="auto">Train the synthesizer:
<code>python synthesizer_train.py mandarin &lt;datasets_root&gt;/SV2TTS/synthesizer</code></p>
</li>
<li>
<p dir="auto">Go to next step when you see attention line show and loss meet your need in training folder <em>synthesizer/saved_models/</em>.</p>
</li>
</ul>
<h4 dir="auto"><a id="user-content-23-use-pretrained-model-of-synthesizer" aria-hidden="true" href="#23-use-pretrained-model-of-synthesizer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.3 Use pretrained model of synthesizer</h4>
<blockquote>
<p dir="auto">Thanks to the community, some models will be shared:</p>
</blockquote>
<table>
<thead>
<tr>
<th>author</th>
<th>Download link</th>
<th>Preview Video</th>
<th>Info</th>
</tr>
</thead>
<tbody>
<tr>
<td>@author</td>
<td><a href="https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g" rel="nofollow">https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g</a>  <a href="https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g" rel="nofollow">Baidu</a> 4j5d</td>
<td></td>
<td>75k steps trained by multiple datasets</td>
</tr>
<tr>
<td>@author</td>
<td><a href="https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw" rel="nofollow">https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw</a>  <a href="https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw" rel="nofollow">Baidu</a> codeÔºöom7f</td>
<td></td>
<td>25k steps trained by multiple datasets, only works under version 0.0.1</td>
</tr>
<tr>
<td>@FawenYo</td>
<td><a href="https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing" rel="nofollow">https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing</a> <a href="https://u.teknik.io/AYxWf.pt" rel="nofollow">https://u.teknik.io/AYxWf.pt</a></td>
<td><a href="https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3">input</a> <a href="https://github.com/babysor/MockingBird/wiki/audio/export.wav">output</a></td>
<td>200k steps with local accent of Taiwan, only works under version 0.0.1</td>
</tr>
<tr>
<td>@miven</td>
<td><a href="https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ" rel="nofollow">https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ</a> codeÔºö2021</td>
<td><a href="https://www.bilibili.com/video/BV1uh411B7AD/" rel="nofollow">https://www.bilibili.com/video/BV1uh411B7AD/</a></td>
<td>only works under version 0.0.1</td>
</tr>
</tbody>
</table>
<h4 dir="auto"><a id="user-content-24-train-vocoder-optional" aria-hidden="true" href="#24-train-vocoder-optional"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.4 Train vocoder (Optional)</h4>
<blockquote>
<p dir="auto">note: vocoder has little difference in effect, so you may not need to train a new one.</p>
</blockquote>
<ul dir="auto">
<li>Preprocess the data:
<code>python vocoder_preprocess.py &lt;datasets_root&gt; -m &lt;synthesizer_model_path&gt;</code></li>
</ul>
<blockquote>
<p dir="auto"><code>&lt;datasets_root&gt;</code> replace with your dataset rootÔºå<code>&lt;synthesizer_model_path&gt;</code>replace with directory of your best trained models of sythensizer, e.g. <em>sythensizer\saved_mode\xxx</em></p>
</blockquote>
<ul dir="auto">
<li>
<p dir="auto">Train the wavernn vocoder:
<code>python vocoder_train.py mandarin &lt;datasets_root&gt;</code></p>
</li>
<li>
<p dir="auto">Train the hifigan vocoder
<code>python vocoder_train.py mandarin &lt;datasets_root&gt; hifigan</code></p>
</li>
</ul>
<h3 dir="auto"><a id="user-content-3-launch" aria-hidden="true" href="#3-launch"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3. Launch</h3>
<h4 dir="auto"><a id="user-content-31-using-the-web-server" aria-hidden="true" href="#31-using-the-web-server"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3.1 Using the web server</h4>
<p dir="auto">You can then try to run:<code>python web.py</code> and open it in browser, default as <code>http://localhost:8080</code></p>
<h4 dir="auto"><a id="user-content-32-using-the-toolbox" aria-hidden="true" href="#32-using-the-toolbox"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3.2 Using the Toolbox</h4>
<p dir="auto">You can then try the toolbox:
<code>python demo_toolbox.py -d &lt;datasets_root&gt;</code></p>
<h2 dir="auto"><a id="user-content-reference" aria-hidden="true" href="#reference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reference</h2>
<blockquote>
<p dir="auto">This repository is forked from <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">Real-Time-Voice-Cloning</a> which only support English.</p>
</blockquote>
<table>
<thead>
<tr>
<th>URL</th>
<th>Designation</th>
<th>Title</th>
<th>Implementation source</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1803.09017" rel="nofollow">1803.09017</a></td>
<td>GlobalStyleToken (synthesizer)</td>
<td>Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis</td>
<td>This repo</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/2010.05646" rel="nofollow">2010.05646</a></td>
<td>HiFi-GAN (vocoder)</td>
<td>Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</td>
<td>This repo</td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"><strong>1806.04558</strong></a></td>
<td><strong>SV2TTS</strong></td>
<td><strong>Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</strong></td>
<td>This repo</td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow">1802.08435</a></td>
<td>WaveRNN (vocoder)</td>
<td>Efficient Neural Audio Synthesis</td>
<td><a href="https://github.com/fatchord/WaveRNN">fatchord/WaveRNN</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1703.10135.pdf" rel="nofollow">1703.10135</a></td>
<td>Tacotron (synthesizer)</td>
<td>Tacotron: Towards End-to-End Speech Synthesis</td>
<td><a href="https://github.com/fatchord/WaveRNN">fatchord/WaveRNN</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow">1710.10467</a></td>
<td>GE2E (encoder)</td>
<td>Generalized End-To-End Loss for Speaker Verification</td>
<td>This repo</td>
</tr>
</tbody>
</table>
<h2 dir="auto"><a id="user-content-f-qa" aria-hidden="true" href="#f-qa"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>F Q&amp;A</h2>
<h4 dir="auto"><a id="user-content-1where-can-i-download-the-dataset" aria-hidden="true" href="#1where-can-i-download-the-dataset"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1.Where can I download the dataset?</h4>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Original Source</th>
<th>Alternative Sources</th>
</tr>
</thead>
<tbody>
<tr>
<td>aidatatang_200zh</td>
<td><a href="http://www.openslr.org/62/" rel="nofollow">OpenSLR</a></td>
<td><a href="https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing" rel="nofollow">Google Drive</a></td>
</tr>
<tr>
<td>magicdata</td>
<td><a href="http://www.openslr.org/68/" rel="nofollow">OpenSLR</a></td>
<td><a href="https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing" rel="nofollow">Google Drive (Dev set)</a></td>
</tr>
<tr>
<td>aishell3</td>
<td><a href="https://www.openslr.org/93/" rel="nofollow">OpenSLR</a></td>
<td><a href="https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing" rel="nofollow">Google Drive</a></td>
</tr>
<tr>
<td>data_aishell</td>
<td><a href="https://www.openslr.org/33/" rel="nofollow">OpenSLR</a></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p dir="auto">After unzip aidatatang_200zh, you need to unzip all the files under <code>aidatatang_200zh\corpus\train</code></p>
</blockquote>
<h4 dir="auto"><a id="user-content-2what-isdatasets_root" aria-hidden="true" href="#2what-isdatasets_root"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.What is<code>&lt;datasets_root&gt;</code>?</h4>
<p dir="auto">If the dataset path is <code>D:\data\aidatatang_200zh</code>,then <code>&lt;datasets_root&gt;</code> is<code>D:\data</code></p>
<h4 dir="auto"><a id="user-content-3not-enough-vram" aria-hidden="true" href="#3not-enough-vram"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3.Not enough VRAM</h4>
<p dir="auto">Train the synthesizerÔºöadjust the batch_size in <code>synthesizer/hparams.py</code></p>
<div data-snippet-clipboard-copy-content="//Before
tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule
                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)
                (2,  2e-4,  80_000,  12),   #
                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames
                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)
                (2,  1e-5, 640_000,  12)],  # lr = learning rate
//After
tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule
                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)
                (2,  2e-4,  80_000,  8),   #
                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames
                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)
                (2,  1e-5, 640_000,  8)],  # lr = learning rate"><pre><code>//Before
tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule
                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)
                (2,  2e-4,  80_000,  12),   #
                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames
                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)
                (2,  1e-5, 640_000,  12)],  # lr = learning rate
//After
tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule
                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)
                (2,  2e-4,  80_000,  8),   #
                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames
                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)
                (2,  1e-5, 640_000,  8)],  # lr = learning rate
</code></pre></div>
<p dir="auto">Train Vocoder-Preprocess the dataÔºöadjust the batch_size in <code>synthesizer/hparams.py</code></p>
<div data-snippet-clipboard-copy-content="//Before
### Data Preprocessing
        max_mel_frames = 900,
        rescale = True,
        rescaling_max = 0.9,
        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.
//After
### Data Preprocessing
        max_mel_frames = 900,
        rescale = True,
        rescaling_max = 0.9,
        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference."><pre><code>//Before
### Data Preprocessing
        max_mel_frames = 900,
        rescale = True,
        rescaling_max = 0.9,
        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.
//After
### Data Preprocessing
        max_mel_frames = 900,
        rescale = True,
        rescaling_max = 0.9,
        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.
</code></pre></div>
<p dir="auto">Train Vocoder-Train the vocoderÔºöadjust the batch_size in <code>vocoder/wavernn/hparams.py</code></p>
<div data-snippet-clipboard-copy-content="//Before
# Training
voc_batch_size = 100
voc_lr = 1e-4
voc_gen_at_checkpoint = 5
voc_pad = 2

//After
# Training
voc_batch_size = 6
voc_lr = 1e-4
voc_gen_at_checkpoint = 5
voc_pad =2"><pre><code>//Before
# Training
voc_batch_size = 100
voc_lr = 1e-4
voc_gen_at_checkpoint = 5
voc_pad = 2

//After
# Training
voc_batch_size = 6
voc_lr = 1e-4
voc_gen_at_checkpoint = 5
voc_pad =2
</code></pre></div>
<h4 dir="auto"><a id="user-content-4if-it-happens-runtimeerror-errors-in-loading-state_dict-for-tacotron-size-mismatch-for-encoderembeddingweight-copying-a-param-with-shape-torchsize70-512-from-checkpoint-the-shape-in-current-model-is-torchsize75-512" aria-hidden="true" href="#4if-it-happens-runtimeerror-errors-in-loading-state_dict-for-tacotron-size-mismatch-for-encoderembeddingweight-copying-a-param-with-shape-torchsize70-512-from-checkpoint-the-shape-in-current-model-is-torchsize75-512"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>4.If it happens <code>RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).</code></h4>
<p dir="auto">Please refer to issue <a href="https://github.com/babysor/MockingBird/issues/37">#37</a></p>
<h4 dir="auto"><a id="user-content-5-how-to-improve-cpu-and-gpu-occupancy-rate" aria-hidden="true" href="#5-how-to-improve-cpu-and-gpu-occupancy-rate"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>5. How to improve CPU and GPU occupancy rate?</h4>
<p dir="auto">Adjust the batch_size as appropriate to improve</p>
<h4 dir="auto"><a id="user-content-6-what-if-it-happens-the-page-file-is-too-small-to-complete-the-operation" aria-hidden="true" href="#6-what-if-it-happens-the-page-file-is-too-small-to-complete-the-operation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>6. What if it happens <code>the page file is too small to complete the operation</code></h4>
<p dir="auto">Please refer to this <a href="https://www.youtube.com/watch?v=Oh6dga-Oy10&amp;ab_channel=CodeProf" rel="nofollow">video</a> and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.</p>
<h4 dir="auto"><a id="user-content-7-when-should-i-stop-during-training" aria-hidden="true" href="#7-when-should-i-stop-during-training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>7. When should I stop during training?</h4>
<p dir="auto">FYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps.
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png"><img src="https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png" alt="attention_step_20500_sample_1"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png"><img src="https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png" alt="step-135500-mel-spectrogram_sample_1"/></a></p>
</article>
        </div></div>
  </body>
</html>
