<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://terriblesoftware.org/2025/01/22/the-7-most-influential-papers-in-computer-science-history/">Original</a>
    <h1>Most Influential Papers in Computer Science History</h1>
    
    <div id="readability-page-1" class="page"><div>




<h3></h3>



<p><strong>Author:</strong> Alan Turing</p>



<p>It’s the 1930s, and a “programmable machine” sounds like something out of a sci-fi novel. Then along comes Alan Turing, laying the groundwork for what computers can theoretically do. He sketches out a hypothetical “Turing Machine,” proving that, if something is computable at all, a machine (in principle) can handle it.</p>



<h4><strong>The big idea</strong></h4>



<p>Turing’s simple model — just a tape, a head for reading/writing, and a finite set of states, turned into the granddaddy of all modern computation. It defined what’s solvable (and what’s not) in a purely mechanical sense, basically giving us the “rules of the game” for digital problem-solving.</p>



<h4><strong>Why it matters today</strong></h4>



<p>Every single programming language, every single piece of code out there, is playing by Turing’s rules. Even when we talk about quantum computing, we’re still referencing the boundaries Turing described. That’s a huge testament to the power of one paper published in the mid-1930s.</p>



<h4><strong>Learn more</strong></h4>



<ul>
<li><a href="https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf">https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf</a></li>



<li><a href="https://en.wikipedia.org/wiki/Turing%27s_proof">https://en.wikipedia.org/wiki/Turing%27s_proof</a></li>



<li><a href="https://www.youtube.com/watch?v=dNRDvLACg5Q">https://www.youtube.com/watch?v=dNRDvLACg5Q</a></li>
</ul>



<h3></h3>



<p><strong>Author:</strong> Claude Shannon</p>



<p>Now that Turing showed us what machines can (and can’t) do, how do we actually move information around? Enter Claude Shannon, who basically invented <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a> so we could talk about bits, entropy, and noisy channels in a rigorous way.</p>



<h4><strong>The big idea</strong></h4>



<p>Shannon took the abstract notion of “information” and turned it into something a little <em>bit</em> (pun intended) more measurable. This helped us figure out how to pack data more efficiently (compression) and how to protect it from errors (error-correcting codes), whether we’re sending signals into space or streaming Netflix on a Friday night.</p>



<h4><strong>Why it matters today</strong></h4>



<p>Every single time you send a text, stream a video, or call your mom on FaceTime, you’re using Shannon’s ideas. Without them, you’d be dealing with a lot more scrambled audio and jumbled data, trust me.</p>



<h4><strong><strong>Learn more</strong></strong></h4>



<ul>
<li><a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</a></li>



<li><a href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication">https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication</a></li>



<li><a href="https://www.youtube.com/watch?v=b6VdGHSV6qg">https://www.youtube.com/watch?v=b6VdGHSV6qg</a></li>



<li><a href="https://www.youtube.com/watch?v=kP0zi5lX-Fo">https://www.youtube.com/watch?v=kP0zi5lX-Fo</a></li>
</ul>



<h3></h3>



<p><strong>Author:</strong> Edgar F. Codd</p>



<p>So, we can compute and communicate — awesome. But eventually, we’re buried under mountains of data. Edgar F. Codd saw this coming and introduced the relational model, which is basically the reason we’re able to store and query data.</p>



<h4><strong>The big idea</strong></h4>



<p>Codd said, “Let’s store data in tables and manipulate it with logical operations.” This might sound obvious now, but at the time it was revolutionary. His blueprint led to SQL and the huge family of relational databases that power, oh, basically every bank, retail website, and enterprise system you can imagine.</p>



<h4><strong>Why it matters today</strong></h4>



<p>Even in the NoSQL era, the underlying concepts of how we organize data (tables, schemas, consistency) trace right back to Codd. If you ever wrote a SQL query in your life — it’s all thanks to him.</p>



<h4><strong>Learn more</strong></h4>



<ul>
<li><a href="https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf">https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf</a></li>



<li><a href="https://en.wikipedia.org/wiki/Codd%27s_12_rules">https://en.wikipedia.org/wiki/Codd%27s_12_rules</a></li>
</ul>



<h3></h3>



<p><strong>Author:</strong> Stephen A. Cook</p>



<h4><strong>The big idea</strong></h4>



<p>Cook showed that the <a href="https://en.wikipedia.org/wiki/Boolean_satisfiability_problem">Boolean satisfiability problem</a> (SAT) is NP-complete, meaning if you magically solve SAT quickly, you’ve instantly cracked a whole bunch of other seemingly impossible problems. This created a universal language for talking about problem difficulty.</p>



<h4><strong>Why it matters today</strong></h4>



<p>Whenever you see “NP-hard” in a problem description, or wonder why route optimization kills your CPU, that’s Cook’s legacy. It led to huge developments in algorithms, cryptography, and the hunt for efficient solutions (or at least decent approximations).</p>



<h4><strong>Learn more</strong></h4>



<ul>
<li><a href="https://www.inf.unibz.it/~calvanese/teaching/14-15-tc/material/cook-1971-NP-completeness-of-SAT.pdf">https://www.inf.unibz.it/~calvanese/teaching/14-15-tc/material/cook-1971-NP-completeness-of-SAT.pdf</a></li>



<li><a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">https://en.wikipedia.org/wiki/P_versus_NP_problem</a></li>



<li><a href="https://www.youtube.com/watch?v=dJUEkjxylBw">https://www.youtube.com/watch?v=dJUEkjxylBw</a></li>
</ul>



<h3></h3>



<p><strong>Authors:</strong> Vinton G. Cerf and Robert E. Kahn</p>



<p>Great, we have tough problems to solve and data to store — but how do we hook all these computers together? Cerf and Kahn’s TCP turned isolated networks into an interconnected web, letting data hop around the planet in tiny packets.</p>



<h3><strong>The big idea</strong></h3>



<p>They created a universal language for different networks to talk. Packets get split up, zipped through various routes, and reassembled on the other side. This flexibility opened the door for global connectivity — no single monolithic network required.</p>



<h3><strong>Why it matters today</strong></h3>



<p>Short answer? Pretty much the entire internet. Whenever you browse the web, send an email, or securely log in to your bank’s website, you’re leaning on TCP/IP to move those bits around reliably. Sure, some real-time applications might use UDP, but the core idea of IP-based networking — laid out by Cerf and Kahn — still unites all our devices under one global network.</p>



<h3><strong>Learn more</strong></h3>



<ul>
<li><a href="https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf">https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf</a></li>



<li><a href="https://www.youtube.com/watch?v=PG9oKZdFb7w">https://www.youtube.com/watch?v=PG9oKZdFb7w</a></li>
</ul>



<h3></h3>



<p><strong>Author:</strong> Tim Berners-Lee</p>



<p>And speaking of TCP/IP — once machines could talk to each other easily, Tim Berners-Lee asked, “How about we make this thing friendly for everyone?” That’s where the World Wide Web was born.</p>



<h3><strong>The big idea</strong></h3>



<p>Berners-Lee pitched a global hypertext system, complete with hyperlinks, URLs, and HTTP. Suddenly, documents around the world were no longer isolated; they were “webbed” together, turning the internet into something normal people (not just scientists) could navigate.</p>



<h3><strong>Why it matters today</strong></h3>



<p>We live on the web. Whether it’s social media, online shopping, or reading obscure blog posts at 3 a.m., the seeds of it all came from this straightforward proposal. It changed how we share knowledge forever.</p>



<h3>Learn more</h3>



<ul>
<li><a href="https://cds.cern.ch/record/369245/files/dd-89-001.pdf">https://cds.cern.ch/record/369245/files/dd-89-001.pdf</a></li>



<li><a href="https://time.com/21039/tim-berners-lee-web-proposal-at-25/">https://time.com/21039/tim-berners-lee-web-proposal-at-25/</a></li>



<li><a href="https://www.youtube.com/watch?v=qJNrvVv7SdU">https://www.youtube.com/watch?v=qJNrvVv7SdU</a></li>
</ul>



<h3></h3>



<p><strong>Authors:</strong> Sergey Brin and Larry Page</p>



<p>Once Berners-Lee’s web blew up, it became a jungle of links, pages, and cat memes. Sergey Brin and Larry Page decided to tame that jungle. Their approach, based on link analysis, evolved into the search engine we now call “Google.”</p>



<h3><strong>The big idea</strong></h3>



<p>They introduced PageRank, which viewed links as votes of confidence rather than just a new dimension to count keywords. The result was a seismic jump in relevant search results, making the web feel, well…searchable.</p>



<h3><strong><strong>Why it matters today</strong></strong></h3>



<p>Type a question into Google and get an instant answer? That’s PageRank (and a lot of subsequent innovation) at work. It redefined how we navigate information online and kicked off a new era of data-driven tech—ads, analytics, machine learning, you name it.</p>



<h3>Learn more</h3>



<ul>
<li><a href="https://snap.stanford.edu/class/cs224w-readings/Brin98Anatomy.pdf">https://snap.stanford.edu/class/cs224w-readings/Brin98Anatomy.pdf</a></li>



<li><a href="https://en.wikipedia.org/wiki/PageRank">https://en.wikipedia.org/wiki/PageRank</a></li>



<li><a href="https://www.youtube.com/watch?v=v7n7wZhHJj8&amp;t=184s">https://www.youtube.com/watch?v=v7n7wZhHJj8&amp;t=184s</a></li>
</ul>



<hr/>



<h2>Bonuses (5 That Almost Made the List)</h2>



<p>1. <strong>“Recursive Functions of Symbolic Expressions and Their Computation by Machine” (1960) – John McCarthy</strong></p>



<p>Introduced Lisp and the functional programming style that still sneaks into modern languages and frameworks.</p>



<p>2. <strong>“Go To Statement Considered Harmful” (1968) – Edsger Dijkstra</strong></p>



<p>A short but fiery editorial that argued <strong>goto</strong> leads to messy, unstructured code, sparking the structured programming revolution.</p>



<p>3. <strong>“Time, Clocks, and the Ordering of Events in a Distributed System” (1978) – Leslie Lamport</strong></p>



<p>You can’t sync real clocks perfectly in distributed systems, so you need logical ones. This is a must-read if you’re into distributed computing.</p>



<p>4. <strong><strong>“No Silver Bullet—Essence and Accident in Software Engineering” (1986) – Fred Brooks</strong></strong></p>



<p>Brooks argued that there’s no single magical fix for the inherent complexity of software development. Decades later, as we chase the “next big thing” in frameworks or methodologies, his message remains a sobering reminder that some problems are just hard.</p>



<p>5. <strong>“Attention Is All You Need” (2017) – Vaswani et al.</strong></p>



<p>The transformer architecture behind GPT and other big-name AI models. If you’re impressed by large language models, here’s your blueprint.</p>



<h2><strong>Conclusion</strong></h2>



<p>These days, we’re flooded with new stuff: fresh languages, mind-blowing AI breakthroughs, quantum leaps, and the JavaScript framework of the week. It’s all super exciting, but here’s the thing: foundations matter. Without them, we’re just piling on new toys without fully understanding the ground we’re building on. The papers in this post are a reminder of where our core concepts — data structures, algorithms, the very web — came from.</p>
</div></div>
  </body>
</html>
