<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.peerdb.io/how-can-we-make-pgdump-and-pgrestore-5-times-faster">Original</a>
    <h1>Parallel Snapshotting: make pg_dump and pg_restore multi-threaded per table</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content-parent"><div id="post-content-wrapper"><p><a target="_blank" href="https://www.postgresql.org/docs/current/app-pgdump.html">pg_dump</a> and <a target="_blank" href="https://www.postgresql.org/docs/current/app-pgrestore.html">pg_restore</a> are reliable tools for backing up and restoring <a target="_blank" href="https://www.postgresql.org/">Postgres</a> databases. They&#39;re essential for database migrations, disaster recovery and so on. They offer precise control over object selection for backup/restore, dump format options (plain or compressed), parallel table processing and so on. They ensure a consistent database snapshot is dumped and restored.</p>
<p>However, they are single-threaded at the table level. This significantly slows down the dump and restore of databases with a star schema common in real-world applications such as Time series and IoT. For databases over 1 TB, <code>pg_dump</code> and <code>pg_restore</code> can take days, increasing downtime during migrations and RTOs in disaster recovery scenarios.</p>
<p>In this blog, we&#39;ll discuss an idea called <strong>&#34;Parallel Snapshotting&#34;</strong>. This idea could be integrated into Postgres upstream in the future to make <code>pg_dump</code> and <code>pg_restore</code> parallelizable at a single table level. Parallel Snapshotting has already been implemented in <a target="_blank" href="https://github.com/PeerDB-io/peerdb">PeerDB</a>, an open-source Postgres replication tool. We will also cover a few interesting benchmarks of migrating a large table of 1.5TB from one Postgres Database to another with and without Parallel Snapshotting.</p>
<h2 id="heading-a-quick-primer-on-pgdump-and-pgrestore">A quick primer on pg_dump and pg_restore</h2>
<p><code>pg_dump</code> is the most reliable way to back up a PostgreSQL database. It enables the backup of a database at a consistent snapshot; that is, the backup guarantees a state that existed previously. The backup generated by <code>pg_dump</code> is a logical representation of the data in PostgreSQL, not a copy of the PostgreSQL data directory. It captures objects as they appear in PostgreSQL.</p>
<p><code>pg_restore</code> is the most reliable way to restore a backup generated by pg_dump from one PostgreSQL database to another.</p>
<p>Both <code>pg_dump</code> and <code>pg_restore</code> are Postgres-native; that is, they come packaged with community Postgres and can be used as command-line utilities, similar to <a target="_blank" href="https://www.postgresql.org/docs/current/app-psql.html">psql</a>.</p>
<h3 id="heading-pgdump-and-pgrestore-offer-fine-grain-control">pg_dump and pg_restore offer fine grain control</h3>
<p>They provide fine-grained control to manage the backup and restore processes. Below are a few flags that are commonly used:</p>
<ol>
<li><p>You have the <code>-f</code> flag, which lets you decide on data formats such as plain text or compressed gzip. Compressed dumps are quite helpful when you have limited network bandwidth or want to save on network costs.</p>
</li>
<li><p>To speed up the backup and restore process, you can use the <code>-j</code> flag to dump and restore multiple tables in parallel.</p>
</li>
<li><p>You can pick and choose specific database objects you want to backup and restore, including tables and schemas.</p>
</li>
<li><p>You can also choose to dump only the schema or only the data using the <code>schema-only</code> and <code>data-only</code> flags.</p>
</li>
<li><p>There are many more flags that they provide that can be found in community <a target="_blank" href="https://www.postgresql.org/docs/current/app-pgdump.html">docs</a>.</p>
</li>
</ol>
<h3 id="heading-pgdump-and-pgrestore-can-be-very-slow-for-large-tables">pg_dump and pg_restore can be very slow for large tables</h3>
<p><strong>pg_dump and pg_restore are single threaded at a table level</strong></p>
<p>There is a painful issue that users often run into with <code>pg_dump</code> and <code>pg_restore</code>. <code>pg_dump</code> and <code>pg_restore</code> can be very slow for large tables. This is because they are single threaded at table level. They can dump and restore multiple tables in parallel but for a single table they are single threaded.</p>
<p>This means that in use cases where you have a single fact table and multiple dimension tables, <code>pg_dump</code> and <code>pg_restore</code> can get bottlenecked on the large fact table. This is very common in the star schema data-model which is used by multiple real-world use-cases such as IoT, Timeseries, Data Warehousing and so on.</p>
<h3 id="heading-migrating-a-15tb-table-can-take-15-days">Migrating a 1.5TB table can take 1.5 days</h3>
<p>The impact of the problem described above can be significant. Using <code>pg_dump</code> and <code>pg_restore</code> to migrate a 1.5 TB <a target="_blank" href="https://www.postgresql.org/docs/current/pgbench.html">pgbench_accounts</a> table from one Postgres database to another took 1.5 days. The benchmark was conducted under optimal conditions, i.e., using the correct flags and region collocating the source, target, and the VM on which <code>pg_dump</code> and <code>pg_restore</code> were running, among other factors. This 1.5-day downtime is substantial when migrating or recovering mission-critical databases.</p>
<h2 id="heading-parallel-snapshotting-to-make-pgdump-amp-pgrestore-multi-threaded-per-table">Parallel Snapshotting to make pg_dump &amp; pg_restore multi-threaded per table</h2>
<p>Now, let&#39;s explore a concept called Parallel Snapshotting, which could make <code>pg_dump</code> and <code>pg_restore</code> multi-threaded at the single table level. Note that Parallel Snapshotting is not currently implemented in the upstream versions of <code>pg_dump</code> and <code>pg_restore</code>. It represents an idea/design that could enhance <code>pg_dump</code> and <code>pg_restore</code> in the future.</p>


<p>Below video captures migrating 5GB of data from Postgres to Postgres within a min using the Parallel Snapshotting feature in PeerDB.</p>
<h3 id="heading-ctid-forms-the-basis-of-parallel-snapshotting">CTID forms the basis of Parallel Snapshotting</h3>
<p><a target="_blank" href="https://www.postgresql.org/docs/current/ddl-system-columns.html#:~:text=ctid">CTID</a> forms the basis of Parallel Snapshotting. Every row in a Postgres table has an internal column called CTID, also known as the tuple identifier. CTID is unique for each row of the table. It represents the exact location of the row on disk—it is the combination of the page/block number and the page offset. You can also query the CTID column for a table through a simple SELECT as you are seeing in the below image.</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1714054057800/03e06d8e-2368-453b-b8c0-58d7fbe16ac4.png?auto=compress,format&amp;format=webp" alt=""/></p>
<h3 id="heading-parallel-snapshotting-logically-partition-the-table-by-ctid-and-copy-multiple-partitions-simultaneously">Parallel Snapshotting - Logically Partition the Table by CTID and COPY Multiple Partitions Simultaneously</h3>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1714054167383/34f72a09-a6bb-48c2-813e-0b2eb48a467d.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p>Let&#39;s dive into the design of Parallel Snapshotting:</p>
<ol>
<li><p>First, create a Postgres Snapshot using the function <code>pg_export_snapshot()</code>. This ensures that the dump and restore operate on a consistent snapshot of the database.</p>
</li>
<li><p>Second, using that snapshot, logically partition the large table based on CTIDs, i.e., create CTID ranges that encapsulate the table.</p>
</li>
<li><p>Once that is done, copy multiple such logical partitions in parallel from the source to the target.</p>
<ol>
<li><p>Essentially, you are running SELECT statements with these CTID ranges to read data from the source and write it to the target.</p>
</li>
<li><p>The SELECT statements with CTID ranges are very efficient because they use tid range scans, which are similar to index lookups on the CTID column.</p>
</li>
<li><p>Also, note that you are reading data in the order of how it is stored on the disk.</p>
</li>
</ol>
</li>
<li><p>We are using <code>COPY WITH BINARY</code> to <code>STDOUT</code> and from <code>STDIN</code>, which makes the dump and restore simultaneous.</p>
</li>
<li><p>We are also using cursors to ensure that the dump doesn’t exhaust memory.</p>
</li>
</ol>
<h3 id="heading-migrating-a-15tb-table-5-times-faster-with-parallel-snapshotting">Migrating a 1.5TB table 5 times faster with Parallel Snapshotting</h3>
<p>At PeerDB, we are building a Postgres replication tool to provide a fast and cost-effective way to move data from Postgres to data warehouses such as <a target="_blank" href="https://www.snowflake.com/en/">Snowflake</a>, BigQuery, <a target="_blank" href="https://clickhouse.com/">ClickHouse</a>, <a target="_blank" href="https://www.postgresql.org/">PostgreSQL</a>, and queues like <a target="_blank" href="https://kafka.apache.org/">Kafka</a>, <a target="_blank" href="https://redpanda.com/">Redpanda</a>, <a target="_blank" href="https://cloud.google.com/pubsub?hl=en">Google PubSub</a>, <a target="_blank" href="https://azure.microsoft.com/en-us/products/event-hubs">Azure Event Hubs</a>, etc.</p>
<p>To enable faster migrations from one Postgres database to another, we have implemented Parallel Snapshotting within our product. Through this feature, our customers are able to move terabytes of data in a few hours versus days.</p>
<p>We did the same above benchmark to move a 1.5 TB <a target="_blank" href="https://www.postgresql.org/docs/current/pgbench.html">pgbench_accounts</a> table from one Postgres database to another, and it took just 7 hours with PeerDB. This was 5x faster than using <code>pg_dump</code> and <code>pg_restore</code>. This speedup was possible through the Parallel Snapshotting feature. The performance can be further improved by increasing the number of parallel threads for the migration and by using more beefier Postgres source and target databases.</p>
<h2 id="heading-conclusion-and-references">Conclusion and References</h2>
<p>The intent of this blog is to share the design principles we followed to enable faster database migrations and discuss how they can be extended to enhance <code>pg_dump</code> and <code>pg_restore</code> in the future. Hope you enjoyed reading the blog. Sharing a few relevant links for reference:</p>
<ol>
<li><p><a target="_blank" href="https://tech.gadventures.com/speeding-up-postgres-restores-de575149d17a">Speeding up Postgres restores</a></p>
</li>
<li><p><a target="_blank" href="https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/faster-data-migrations-in-postgres/ba-p/2150850">Faster Data Migrations in Postgres</a></p>
</li>
<li><p><a target="_blank" href="https://blog.peerdb.io/faster-postgres-migrations-using-peerdb-part-1">Faster Postgres Migrations using PeerDB</a></p>
</li>
<li><p><a target="_blank" href="https://www.youtube.com/watch?v=pgJwT9vcwI8">Podcast on Logical replication common issues</a></p>
</li>
<li><p>Try <a target="_blank" href="https://github.com/PeerDB-io/peerdb">PeerDB Open Source</a> for fast Postgres migration and replication</p>
</li>
<li><p>Try <a target="_blank" href="https://app.peerdb.cloud/">PeerDB Cloud</a>, the fully managed offering of PeerDB</p>
</li>
</ol>
</div></div></div>
  </body>
</html>
