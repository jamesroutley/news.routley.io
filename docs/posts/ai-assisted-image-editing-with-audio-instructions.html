<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ShaShekhar/aaiela">Original</a>
    <h1>Show HN: AI assisted image editing with audio instructions</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">AAIELA: AI Assisted Image Editing with Language and Audio</h2><a id="user-content-aaiela-ai-assisted-image-editing-with-language-and-audio" aria-label="Permalink: AAIELA: AI Assisted Image Editing with Language and Audio" href="#aaiela-ai-assisted-image-editing-with-language-and-audio"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This project empowers users to modify images using just audio commands.</p>
<p dir="auto">By leveraging open-source AI models for computer vision, speech-to-text, large language models (LLMs),
and text-to-image inpainting, we have created a seamless editing experience that bridges the gap between
spoken language and visual transformation.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/36410320/344484843-cc267599-beb9-4451-8c8b-138e223291f4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk4NjE0OTYsIm5iZiI6MTcxOTg2MTE5NiwicGF0aCI6Ii8zNjQxMDMyMC8zNDQ0ODQ4NDMtY2MyNjc1OTktYmViOS00NDUxLThjOGItMTM4ZTIyMzI5MWY0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzAxVDE5MTMxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2ZWNiMTBhMDQxZTllMTU4MzQwYTUxMzg0MTUxZjE4OWUyMzJmYzIwYjRlMGQ5YWRjYjQ5MWNkMDk1NThiOGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.q1Re889mnMN8YIjyGWifG3eE3EHRRoTll8rhGohrHJ0" data-canonical-src="https://private-user-images.githubusercontent.com/36410320/344484843-cc267599-beb9-4451-8c8b-138e223291f4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk4NjE0OTYsIm5iZiI6MTcxOTg2MTE5NiwicGF0aCI6Ii8zNjQxMDMyMC8zNDQ0ODQ4NDMtY2MyNjc1OTktYmViOS00NDUxLThjOGItMTM4ZTIyMzI5MWY0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA3MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNzAxVDE5MTMxNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2ZWNiMTBhMDQxZTllMTU4MzQwYTUxMzg0MTUxZjE4OWUyMzJmYzIwYjRlMGQ5YWRjYjQ5MWNkMDk1NThiOGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.q1Re889mnMN8YIjyGWifG3eE3EHRRoTll8rhGohrHJ0" controls="controls" muted="muted">

  </video>
</details>


<ul dir="auto">
<li><strong>detectron2</strong>: The Detectron2 library for object detection, keypoint detection, instance/panoptic segmentation etc.</li>
<li><strong>faster_whisper</strong>: Contains the faster_whisper which is implementation of OpenAI Whisper for audio transcription/translation.</li>
<li><strong>language_model</strong>: Using small Language model like Phi3 or any of the LLM API: Gemini, Claude, GPT4 etc to extract object, action and prompt from natural language instruction.</li>
<li><strong>sd_inpainting</strong>: Include Text conditioned Stable Diffusion v1.5 Inpainting model.</li>
</ul>

<p dir="auto">See <a href="https://github.com/ShaShekhar/aaiela/blob/main/INSTALL.md">installation instructions</a>.</p>
<p dir="auto">API Keys: Create a <code>.env</code> file in the root directory of the project. Fill in API keys if intend to use API-based
language models. Use the provided <code>.env.example</code> file as a template.</p>
<p dir="auto">Or to use a small language model like Phi-3, set the <code>active_model:local</code> in config file.</p>
<p dir="auto">To run individual test files:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ python -m tests.&lt;test_file_name&gt;"><pre>$ python -m tests.<span>&lt;</span>test_file_name<span>&gt;</span></pre></div>
<p dir="auto">Configuration: adjust some settings in the <code>aaiela.yaml</code> config file e.g., device, active_model.
Toggle between using an API-based model or a local LLM by modifying the <code>active_model</code> parameter.</p>
<ul dir="auto">
<li>
<p dir="auto">Run the project&#39;s main script to load the model and start the web interface.</p>
<p dir="auto"><code>python app.py</code></p>
</li>
</ul>

<ol dir="auto">
<li>Upload: User uploads an image.</li>
<li>Segmentation: Detectron2 performs segmentation.</li>
<li>Audio Input: User records an audio command (e.g., &#34;Replace the sky with a starry night.&#34;).</li>
<li>Transcription: Faster Whisper transcribes the audio into text.</li>
<li>Language Understanding: The LLM (Gemini, GPT4, Phi3 etc.) to extracts object, action, and prompt from the text.</li>
<li>Image Inpainting:
<ul dir="auto">
<li>Relevant masks are selected from the segmentation results.</li>
<li>Stable Diffusion Inpainting apply the desired changes.</li>
</ul>
</li>
<li>Output: The inpainted image.</li>
</ol>

<ol dir="auto">
<li>
<p dir="auto">The SDXL-Inpainting model requires retraining on a substantially larger dataset to achieve satisfactory results. The current model trained by HuggingFace shows limitations.</p>
</li>
<li>
<p dir="auto">context aware automatic mask generation for prompt like this &#34;Add a cat sitting on the wooden chair.&#34; Incorporate domain knowledge or external knowledge bases (e.g., object attributes, spatial relationships) to guide mask generation.</p>
</li>
<li>
<p dir="auto">&#39;Segment Anything&#39; model that could generate masks from text input was explored in research paper. <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">This remains an active area of research</a>.</p>
</li>
<li>
<p dir="auto">Contextual Reasoning: Understand relationships between objects and actions (e.g., &#34;sitting&#34; implies the cat should be on top of the chair).</p>
</li>
<li>
<p dir="auto">Multi-Object Mask generation: &#34;Put a cowboy hat on the person in the right and a red scarf around their neck.&#34;</p>
</li>
<li>
<p dir="auto">Integrate Visual Language model such as BLIP, to provide another layer of interaction for the users.</p>
<ul dir="auto">
<li>If a voice command is unclear or ambiguous, the VLM can analyze the image and offer
suggestions or ask clarifying questions.</li>
<li>The VLM can suggest adjustments to numerical parameters based on the image content.
etc.</li>
</ul>
</li>
</ol>

<ul>
<li>
<p dir="auto"> The current <a href="https://github.com/NVIDIA/TensorRT/tree/release/10.0/demo/Diffusion">TensorRT integration for Stable Diffusion models</a> lacks a working example of the text-to-image inpainting pipeline.</p>
</li>
<li>
<p dir="auto"> Integrate ControlNet conditioned on keypoints, depth, input scribbles, and other modalities.</p>
</li>
<li>
<p dir="auto"> Integrate Mediapipe Face Mesh to enable facial landmark detection, face geometry estimation,
eye tracking, and other features for modifying facial features in response to audio commands (e.g., &#34;Make me smile,&#34; &#34;Change my eye color&#34;).</p>
</li>
<li>
<p dir="auto"> Integrate pose landmark detection capabilities.</p>
</li>
<li>
<p dir="auto"> Incorporate a super-resolution model for image upscaling.</p>
</li>
<li>
<p dir="auto"> Implement interactive mask editing using Segment Anything with simple click-based interactions followed by inpainting using audio instructions.</p>
</li>
</ul>
</article></div></div>
  </body>
</html>
