<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/d41586-025-01266-x">Original</a>
    <h1>Huge reproducibility project fails to validate biomedical studies</h1>
    
    <div id="readability-page-1" class="page"><div data-test="access-teaser"> <figure><picture><source type="image/webp" srcset="//media.nature.com/lw767/magazine-assets/d41586-025-01266-x/d41586-025-01266-x_50902208.jpg?as=webp 767w, //media.nature.com/lw319/magazine-assets/d41586-025-01266-x/d41586-025-01266-x_50902208.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"/><img alt="Two female researchers wearing full PPE sit working at extraction units in the lab, with their faces reflected in the glass" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-01266-x/d41586-025-01266-x_50902208.jpg"/><figcaption><p><span>A replication drive focused on results that lean on three methods commonly used in biomedical research in Brazil. </span><span>Credit: Mauro Pimentel/AFP/Getty </span></p></figcaption></picture></figure><p>In an unprecedented effort, <a href="https://www.nature.com/articles/d41586-019-01485-z" data-track="click" data-label="https://www.nature.com/articles/d41586-019-01485-z" data-track-category="body text link">a coalition of more than 50 research teams</a> has surveyed a swathe of Brazilian biomedical studies to double-check their findings — with dismaying results.</p><p>The teams were able to replicate the results of less than half of the tested experiments<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. That rate is in keeping with <a href="https://www.nature.com/articles/nature.2015.18248" data-track="click" data-label="https://www.nature.com/articles/nature.2015.18248" data-track-category="body text link">that found by other large-scale attempts to reproduce scientific findings</a>. But the latest work is unique in focusing on papers that use specific methods and in examining the research output of a specific country, according to the research teams.</p><p>The results provide an impetus to strengthen the country’s science, the study’s authors say. “We now have the material to start making changes from within — whether through public policies or within universities,” says Mariana Boechat de Abreu, a metascience researcher at the Federal University of Rio de Janeiro (UFRJ) in Brazil and one of the coordinators of the project.</p><p>The work was posted on 8 April to the bioRxiv preprint server and has not yet been peer reviewed.</p><h2>Ambitious undertaking</h2><p>The massive experiment was coordinated by the Brazilian Reproducibility Initiative, a collaborative effort launched in 2019 by researchers at the UFRJ. The scientists wanted to assess publications “based on methods, rather than research area, perceived importance or citation counts”, de Abreu says. And they wanted to do so on a large scale. Ultimately, 213 scientists at 56 laboratories in Brazil were involved in the work.</p><p>The project unfolded during the COVID-19 pandemic, which brought numerous logistical challenges. And teams disagreed about how closely to follow the tested protocols. “It was like trying to turn dozens of garage bands, each with its own way of playing, into an orchestra,” says project coordinator Olavo Bohrer Amaral, a physician at the UFRJ.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-023-03177-1" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-01266-x/d41586-025-01266-x_26179086.jpg"/><p>Reproducibility trial: 246 biologists get different results from same data sets</p></a></article><p>The authors began by reviewing a random sample of life-sciences articles to determine the most common biomedical research methods used in Brazil, ensuring that any biomedical lab interested in joining the project would be capable of reproducing the experiments.</p><p>They ended up selecting three of these methods: an assay of cell metabolism, a technique for amplifying genetic material and a type of maze test for rodents. Then the authors randomly selected biomedical papers that relied on those methods and were published from 1998 to 2017 by research teams in which at least half the contributors had a Brazilian affiliation.</p><p>The collaborators initially chose a subset of 60 papers for replication, guided by factors such as whether a paper included certain statistical information. Three labs tested each experiment, and an independent committee judged which of those tests was a valid replication. The coalition performed 97 valid replication attempts of 47 experiments.</p><h2>Falling short</h2><p>The authors judged a paper’s replicability by five criteria, including whether at least half of the replication attempts had statistically significant results in the same direction as the original paper. Only 21% of the experiments were replicable using at least half of the applicable criteria.</p><p>The authors also found that the effect size — the magnitude of the observed impact in the experiments — was, on average, 60% larger in the original papers than in the experimental follow-ups, indicating that published results tend to overestimate the effects of the interventions tested.</p></div></div>
  </body>
</html>
