<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://carlineng.com/?postid=malloy-intro#blog">Original</a>
    <h1>A sequel to SQL? An intro to Malloy</h1>
    
    <div id="readability-page-1" class="page"><div>
    <section id="home">
      <!-- HOME -->
      <figure>
        <a href="#img-home">
          <img loading="lazy" alt="" src="https://carlineng.com/img/site-image.jpg"/>
        </a>
      </figure>
      <p> Former Data Engineer and Engineering Manager at <a href="https://strava.com/">Strava</a>; then spent two years
        as a Sales Engineer and Data Scientist at <a href="https://www.snowflake.com/">Snowflake</a>. Currently Head of
        Data Engineering at <a href="https://www.geteppo.com/">Eppo</a>. Avid cyclist, and proud member of the Dolphin
        Club in San Francisco, California. </p>
      <div><p> Made with <span id="emoji-carousel">üö¥‚Äç‚ôÇÔ∏è</span> by Carlin Eng </p></div>
    </section>
    <section id="about">
      
      <p> This page is based off of the <a href="https://github.com/cadars/john-doe/">John Doe</a> template, a
        single-file website written only in HTML and CSS, to which I&#39;ve added small Javascript snippets for toy
        functionality. Source code for this website can be found at <a href="https://github.com/carlineng/carlineng.github.io">Github</a>. <a href="#home">‚Ü©Ô∏è</a>
      </p>
    </section>
    <section id="blog">
      <!-- BLOG -->
      <article>
        <label for="malloy-intro">A Sequel to SQL? An introduction to Malloy</label>
        <time datetime="2022-09-05">09.05.2022</time>
        
        <div>
          <p>The foundations of SQL were laid at the dawn of the relational database. Back then, there was no such thing
            as a data warehouse, no such thing as a BI tool, and certainly no such thing as an <a href="https://www.getdbt.com/what-is-analytics-engineering/">Analytics Engineer</a>. And yet, SQL is still
            the primary user interface by which most data professionals interact with their raw materials. The
            underlying technologies have improved immeasurably, but aside from a handful of updates to the ANSI
            standard, the core of the language remains untouched. It‚Äôs practically a miracle that after 40+ years of use
            by countless data professionals, our interface to data is effectively the same. </p>
          <p>Since its inception, many computer scientists and database researchers have expressed their disdain for
            SQL, and their critiques are usually well-founded; however, every serious attempt to replace it as the
            de-facto standard has failed. Most attempts to replace SQL primarily address the language‚Äôs awkward syntax;
            for example putting the <code>FROM</code> clause first or removing the need for <code>HAVING</code> and
            <code>QUALIFY</code> clauses. Unfortunately, the reality is that SQL is ‚Äúgood enough‚Äù for most use cases,
            and according to <a href="https://youtu.be/IVlMB9akD1A?t=420">Aiken‚Äôs Law</a>, programmer training is the
            dominant cost for a programming language. It seems that syntactic sugar is simply not enough to overcome
            SQL‚Äôs entrenchment. </p>
          <p>This brings me to <a href="https://github.com/looker-open-source/malloy">Malloy</a>, a new query language
            for data analysis currently being developed by <a href="https://twitter.com/lloydtabb">Lloyd Tabb</a>,
            founder and former CTO of <a href="https://www.looker.com/">Looker</a>. Malloy addresses many of the
            aesthetic concerns that plague SQL, but far more interesting in my opinion is its integration of a query
            language and a semantic layer into a single language. But what is a semantic layer, and why is it important?
          </p>
          <h3>Semantic Layers</h3>
          <p>A semantic layer‚Äôs purpose is to codify domain-specific logic on top of database tables, and to prevent
            users from issuing queries that are syntactically valid, but semantically meaningless. For example, what
            happens if you write a join between two tables with incorrect keys? In most relational databases, primary
            and foreign keys are just strings or numbers. Some databases may enforce referential integrity, but most
            will not complain if you attempt to join, say, <code>CUSTOMER_ID</code> with <code>ORDER_ID</code>. As
            another example, it‚Äôs often the case that primary key columns are integers, and databases will happily let
            you use them as inputs to any aggregate function that takes a number input, like <code>SUM</code> or
            <code>AVG</code>, even though the result is nonsense. Lastly, every organization has special rules that must
            be applied to their datasets in order to correctly calculate key metrics. For example, what inputs and
            adjustments go into calculating the revenue that Investor Relations reports out to Wall Street? Someone with
            access only to the raw data, but not the requisite domain-specific knowledge will not be able to accurately
            reproduce these metrics. The semantic layer provides a place to set these rules and require queries to abide
            by them; e.g., which joins are valid, which columns can be grouped on, or what inputs go into a particular
            aggregate function. </p>
          <p>A semantic layer usually takes the form of an application that sits on top of the database, along with
            configuration files that define the rules described above. Examples of products on the market today include
            SAP‚Äôs <a href="https://help.sap.com/docs/SAP_BUSINESSOBJECTS_BUSINESS_INTELLIGENCE_PLATFORM/3d4f417fd0764f909c0ef7931e19fe1a/e4af1c39d2b94ca5bc8a991b4ff26f5f.html?locale=en-US">Business
              Objects Universe</a>, Looker‚Äôs <a href="https://cloud.google.com/looker/docs/what-is-lookml">LookML</a>
            and <a href="https://cube.dev/">Cube</a>. I was an early user of Looker, and that experience left a strong
            impression. LookML allowed me to define the logic necessary to package all of our data sources together as a
            cohesive single source of truth. Our data warehouse turned from a tangled mess of tables that only a skilled
            data scientist could operate, into a trusted repository that the typical PM or business partner could pull
            insights from. </p>
          <p>Despite the huge value we got out of Looker and LookML, the analytics and data science group never loved it
            as much as I did. In comparison with <a href="https://www.tableau.com/">Tableau</a>, Looker‚Äôs interactive
            data exploration and analysis capabilities are relatively limited, and most data scientists saw exploration
            and analysis as their primary activity. They viewed writing LookML configuration as a chore, and the user
            experience did not help. To add a new dimension or measure, a data scientist would have to edit a YAML
            configuration file, check their changes into source control, and reload their configuration before they
            could view any results. A simple workflow that could take 2 seconds in Tableau might take over a minute in
            Looker. When trying to explore a dataset, the ability to iterate ‚Äúat the speed of thought‚Äù is critical, and
            that extra latency was a source of frustration for many. As a result, data modeling and data exploration
            were viewed as two entirely separate disciplines. Data scientists greatly preferred tools that aided the
            latter, much to the detriment of everyone who was NOT a data scientist. </p>
          <h3>Malloy</h3>
          <p>So finally we return to Malloy. As I mentioned previously, Malloy is a query language that compiles to SQL,
            and looks very familiar to anyone who has used SQL. The team has also built a VSCode extension that allows
            users to connect to a database and start writing queries, currently with support for BigQuery, DuckDB and
            Postgres. The semantic layer within Malloy is accessed via writing Sources. From the <a href="https://looker-open-source.github.io/malloy/documentation/language/source.html#sources">docs</a>:
          </p>
          <blockquote>
            <p>A source can be thought of as a table and a collection of computations and relationships which are
              relevant to that table. These computations can consist of measures (aggregate functions), dimensions
              (scalar calculations) and query definitions; joins are relationships between sources.</p>
          </blockquote>
          <p> Let‚Äôs take a look at an example Source from the <a href="https://github.com/looker-open-source/malloy/blob/2135a51c46f3076f9d28c2136f9d8ca4cf9a101b/samples/duckdb/faa/2_flights.malloy#L8-L32">Malloy
              Github repo</a>, looking at a database of flights.: </p>
          <pre><code>source: flights is table(&#39;duckdb:data/flights.parquet&#39;) + {
  primary_key: id2

  // rename some fields as from their physical names
  rename: origin_code is origin
  rename: destination_code is destination

  // join all the data sources
  join_one: carriers with carrier
  join_one: origin is airports with origin_code
  join_one: destination is airports with destination_code
  join_one: aircraft with tail_num

  // declare some resusable aggregate calculations
  measure:
    flight_count is count()
    total_distance is sum(distance)
  }
}</code></pre>
          
          <p>This Source has a few key components: the name of a table in the database, the primary key of the table,
            which tables can be joined to it and via which columns, and which aggregations (aka measures) are valid.
            Within the VSCode extension, I can use this source as the starting point for data exploration or analysis
            simply by writing a query referencing it, and hitting the ‚ÄúRun‚Äù CodeLens button: </p>
          <p>
            <img loading="lazy" alt="" src="https://carlineng.com/img/malloy_src.png"/>
          </p>
          <p>The VSCode extension compiles the Malloy query to SQL, issues it against the database, and renders the
            results in another window. </p>
          <p>That exploration may spur me to update the Source in some way, which I can do from right within the IDE,
            and immediately re-execute the query. This sort of read-eval-print loop enables fast iteration in a way that
            simply isn‚Äôt possible with LookML or any other semantic layer that requires both SQL and YAML for
            configuration (and even worse, oftentimes a separate API for actually querying the data). Malloy‚Äôs semantic
            layer marries two separate but intricately related disciplines ‚Äì exploring data and codifying rules around
            it. This not only removes the overhead of context switching between the two tasks, but it actually improves
            the individual experience of each. Implementing a data model actively improves the data exploration
            experience, and vice versa. </p>
          <p>While the semantic layer is the main attraction for me, there are many other noteworthy features. One
            particularly nice feature is its handling of nested data. Rollup queries with nested subtotals can be
            painful to write in SQL, and the
            <code><a href="https://docs.snowflake.com/en/sql-reference/constructs/group-by-rollup.html">GROUP BY ROLLUP</a></code> function in most SQL
            dialects produces awkward output that is very difficult to work with. Malloy‚Äôs nest clause makes this
            trivial. The previous query simply counts the number of flights by month. Let‚Äôs say I now want to compute
            the top 3 destination airports for each month: </p>
          <p>
            <img loading="lazy" alt="" src="https://carlineng.com/img/malloy_nest.png"/>
          </p>
          <p>For each row in the original by-month query, there‚Äôs a new column that contains a nested table with flight
            counts by destination. Note also that since the original flights Source has pre-defined the join with
            destination, accessing fields from the destination table is as simple as referencing them with dot notation:
            destination.name in the above example. Getting this same information in a single SQL query is quite a bit
            more verbose and requires either a window function or a self-join: </p>
          <pre><code>WITH flights_by_destination_and_month AS (
  SELECT
    DATE_TRUNC(‚Äòmonth‚Äô, dep_time) AS dep_month,
    d.name AS destination_name,
    COUNT(*) AS flight_count
  FROM flights f
  LEFT JOIN destination d
  GROUP BY 1,2
)
SELECT
  dep_month,
  destination_name,
  SUM(flight_count) OVER (PARTITION BY dep_month) 
    AS flight_count_by_month,
  flight_count AS flight_count_by_dest_and_month,
FROM flights_by_destination_and_month
QUALIFY ROW_NUMBER() OVER (
  PARTITION BY dep_month ORDER BY flight_count DESC
) &lt;= 3
ORDER BY 1,2
</code></pre>
          
          <p> There‚Äôs a lot more to Malloy than what I‚Äôve shared here, so I encourage you to take a look at the <a href="https://github.com/looker-open-source/malloy">Github repository</a> and the <a href="https://looker-open-source.github.io/malloy/documentation/language/basic.html">documentation</a>.
          </p>
          <h3>Summary</h3>
          <p> Could Malloy finally be the language that replaces SQL? It‚Äôs a nigh impossible task, but I am very much
            hoping that it succeeds. Unlike other projects that only seek to improve on the aesthetics of the language,
            Malloy takes a more ambitious approach and tackles a critical missing piece of the stack. Its marriage of
            query language and semantic layer has the potential to radically change the disciplines of analytics and
            business intelligence for the better. Other trends, such as the rise and dominance of a few data platform
            products, namely BigQuery and Snowflake, mean that for Malloy to really succeed, there are relatively few
            database targets that it must support. It‚Äôs still very early days for the project, and who knows where it
            will go from here, but this is one that I‚Äôll be keeping a very close eye on. </p>
        </div>
      </article>
      <article>
        <label for="sql-critique">A Critique of SQL, 40 Years Later</label>
        <time datetime="2022-08-11">08.11.2022</time>
        
        <div>
          <p><i>Author&#39;s note: this post has found it&#39;s way to the front-page of Hacker News -- follow along with the <a href="https://news.ycombinator.com/item?id=32578725">discussion there</a>.</i></p>
          <p>The SQL language made its first appearance in 1974, as part of IBM‚Äôs System R database. It is now nearly 50
            years later, and SQL is the de facto language for operating the majority of industrial grade databases. Its
            usage has bifurcated into two domains ‚Äì application programming and data analysis. The majority of my 12
            year career (data engineer and data scientist) has been concerned with the latter, and SQL is by far the
            language that I have used the most. I love SQL for the productivity it has afforded me, but over time I‚Äôve
            also become aware of its many flaws and idiosyncrasies. My perspective is primarily from a practitioner‚Äôs
            standpoint, and I have always been curious if those ‚Äúreal world‚Äù issues have more fundamental or theoretical
            underpinnings. This brought me to <a href="https://courses.cs.duke.edu/compsci516/cps216/spring03/papers/date-1983.pdf">A Critique of the SQL
              Database Language</a>, by mathematician and computer scientist CJ Date. Date was a former IBM employee, a
            well known database researcher, and friend of EF Codd. The SQL standard has received many major updates
            since this critique was first published, but which of those critiques are still valid today? </p>
          <p> A Critique of the SQL Database Language was first published in November 1984 in The ACM SIGMOD Record. It
            examines the dialect of SQL implemented by several IBM systems (SQL/DS, DB2, and QMF) which provided the
            basis for the initial SQL standard. Having no direct experience with any of these systems, reading the SQL
            examples from the paper is a bit like trying to read 17th century English ‚Äì it has a stilting, unfamiliar
            cadence that requires an extra bit of effort to understand. In the examples below, I‚Äôll use the terms
            SQL[1983] and SQL[2022] to distinguish between the older dialect, and what is available today. Use of the
            unqualified term ‚ÄúSQL‚Äù means my comment could apply to both. </p>
          <p> The paper consists of eight sections, each one describing a different category of criticism: lack of
            orthogonality in expressions, lack of orthogonality in functions, miscellaneous lack of orthogonality,
            formal definition, mismatch with host language, missing functions, mistakes, and missing aspects of the
            relational model. In the rest of this post, I‚Äôll go through each of those sections, describe the critique in
            informal terms, and give my interpretation on whether the critique is still relevant. </p>
          <h3>Lack of Orthogonality: Expressions</h3>
          <p> Orthogonality with respect to programming languages means roughly that the constructs of the language are
            like Lego blocks ‚Äì a small number of basic pieces can be recombined in simple and intuitive ways. Lack of
            orthogonality (again, informally speaking) means the language has lots of special cases and exceptions in
            how the components can be put together, which make it complex to learn and unintuitive to use. </p>
          <p> This section begins with a definition of table-expression, column-expression, row-expression, and
            scalar-expression. Respectively, these are expressions in SQL that return a table, column, row, and scalar
            value. In SQL[1983], the <code>FROM</code> clause of a <code>SELECT</code> statement was restricted to only
            specifying table or view names, and not general table-expressions, i.e., subqueries or common-table
            expressions (CTE). This made constructing nested expressions, one of the key features of Relational Algebra,
            nearly impossible. Modern SQL provides the capability to reference a CTE or subquery in a <code>FROM</code>
            clause, so this concern is mostly irrelevant today; however, the idea that a table-expression can take the
            form of ‚Äútablename‚Äù in some contexts, but must be <code>SELECT * FROM tablename</code> in others is
            interesting.</p>
          <p>For example, why not allow the following expression as a legal statement:</p>
          <pre><code>tablename;</code></pre>
          <p>which would return identical results to:</p>
          <pre><code>SELECT * FROM tablename;</code></pre>
          <p>Both are table-expressions (statements that return a table), and thus should be allowed anywhere that
            accepts a table-expression, e.g., the <code>FROM</code> clause of a <code>SELECT</code> statement, or a
            statement itself. </p>
          <p> While <code>SELECT</code> statements in SQL[1983] are not allowed in the <code>FROM</code> clause, they
            are required as an argument to an <code>EXISTS</code> clause. Furthermore, the <code>SELECT</code> statement
            here is required to be a column-expression (selecting only a single column) ‚Äì a statement that returns a
            table, a row, or a scalar will not work. When is a <code>SELECT</code> statement a table-expression, a
            column-expression, a row-expression or a scalar-expression? The language itself provides no guidance here,
            and it is wholly dependent on the query itself; e.g.: </p>
          <pre><code>SELECT a FROM tablename;</code></pre> 
          <p> is a column-expression, but </p>
          <pre><code>SELECT a,b FROM tablename;</code></pre>
          
          <p> is a table-expression. This bit of arbitrariness still exists in SQL[2022]. </p>
          <h3>Lack of Orthogonality: Functions</h3>
          <p>While some of the concerns in this section are mitigated by the introduction of subqueries and CTEs, a lot
            of them still hold true today. Column functions in SQL take a column of scalars as input, and return either
            a column of scalar values (e.g., the MD5 function or a type-casting function), or a single scalar (e.g.,
            aggregate functions like <code>SUM</code>). The author argues here that since column functions take a column
            of scalar values as input, any valid column-expression should be allowed. An example where this is not the
            case is as follows:</p>
          <pre><code>SELECT SUM(val) FROM tbl</code></pre>
          <p>is allowed, but </p>
          <pre><code>SELECT SUM( SELECT val FROM tbl )</code></pre>
          <p>is not, even though <code>SELECT val FROM tbl</code> is a valid column-expression ‚Äì it returns a single
            column, <code>val</code>, from table <code>tbl</code>.</p>
          <p>The key problem here is that the input to the <code>SUM</code> function in the first example is a column
            name, but that column name alone does not define the column-expression. Instead, we must look at the context
            (i.e., the full query) to understand that the ‚Äúval‚Äù column comes from ‚Äútable‚Äù. Said another way, in SQL,
            F(X) is not dependent only on X, but on contextual information surrounding F:</p>
          <pre><code>SELECT SUM(amount) FROM purchases;</code></pre>
          <p>and</p>
          <pre><code>SELECT SUM(amount) FROM purchases WHERE 1 = 0;</code></pre>
          <p>Are two very different queries, even though the column-function invocation <code>SUM(amount)</code> is
            identical.</p>
          <p>This also makes it difficult to nest aggregations. Consider the following example: we have a database of
            purchases for an ecommerce website, and want to retrieve (1) the total amount spent for each customer, and
            (2) the average spend across all customers. SQL[1983] could not solve this in a single statement. SQL[2022]
            can solve it with the use of CTEs:</p>
          <pre><code>  WITH spend_per_customer AS (
    SELECT
      SUM(amount) AS customer_total
    FROM purchases
    GROUP BY customer
  )
  SELECT AVG(customer_total) FROM spend_per_customer</code></pre>
          
          <p>However, the following (arguably more natural) statement is not allowed:</p>
          <pre><code>  SELECT
    AVG(
        SELECT SUM(amount) FROM purchases GROUP BY customer
    )</code></pre>
          
          <p>In the above query, the inner <code>SELECT</code> is a column-expression (<code>SELECT</code> statement
            returning a single column), and <code>AVG</code> is a function that takes a single column; however, the
            above statement does not work in most databases. In Snowflake, the above query responds with the error
            message ‚ÄúSingle-row subquery returns more than one row‚Äù, which I find confusing, since the <code>AVG</code>
            function clearly expects input of more than one row. </p>
          <p>Another interesting consequence here is the necessity of the <code>HAVING</code> clause. The
            <code>HAVING</code> clause is a favorite ‚Äúgotcha‚Äù of SQL interviewers everywhere. Why and how is it
            different from a <code>WHERE</code> clause? The answer is not immediately obvious to someone looking at SQL
            for the first time. Specialized knowledge like this certainly serves a purpose as an indicator for
            experience, but it can just as easily be seen as a deficiency of the SQL language. The <code>HAVING</code>
            clause provides a scoping hint to a column-function to indicate that the function input must make use of the
            <code>GROUP BY</code> clause. The author does not mince words here: ‚ÄúThe <code>HAVING</code> clause and the
            <code>GROUP BY</code> clause are needed in SQL only as a consequence of the column-function argument scoping
            rules.‚Äù</p>
          <p>The author also describes table-functions (functions that take a table as input, rather than just a
            column), and laments several instances of arbitrary and non-orthogonal syntax. First, the
            <code>EXISTS</code> function (takes a table-expression, returns a scalar) can only be used in a
            <code>WHERE</code> clause, whereas orthogonality would dictate that it should be allowed anywhere that the
            language accepts a scalar. Second, the <code>UNION</code> function is represented by an in-fix operator, and
            since SQL[1983] did not allow arbitrary table-expressions in <code>FROM</code> clauses, it was impossible to
            compute a column-function over a <code>UNION</code> of two tables. This problem is solved in SQL[2022], as
            the following syntax is now legal:</p>
          <pre><code>  SELECT
    SUM(val)
  FROM (
    SELECT val FROM instore_purchases
    UNION ALL
    SELECT val FROM online_purchases
  )</code></pre>
          <h3>Lack of Orthogonality: Miscellaneous Items</h3>
          <p>This section contains a grab-bag of items related to functionality and implementation details of the
            underlying systems ‚Äì host/indicator variables, cursors, ‚Äúlong‚Äù fields (e.g., character fields with length
            greater than 254). Some of the limitations are indeed very frightening (a ‚Äúlong‚Äù field could not be
            referenced in a <code>WHERE</code> or <code>GROUP BY</code> clause!), but modern database systems are no
            longer subject to these restrictions. Other items in this section have been addressed by updates to the SQL
            standard. In no particular order, the following limitations are no longer applicable: </p>
          <ul>
            <li> Only simple expressions (column names) allowed in <code>GROUP BY</code></li>
            <li>NULL literal could not be used in places where a scalar constant was expected</li>
            <li>No concept of <code>UNION ALL</code></li>
            <li>Only possible to aggregate at one level with the <code>GROUP BY</code> construct</li>
          </ul>
          <p> While much of the discussion here is no longer relevant, the discussion of <code>NULL</code> values
            remains as scary today as it ever was. Inconsistency in <code>NULL</code> handling gives rise to some truly
            unexpected and frightening results, most notably in aggregate functions. Aggregate functions ignore
            <code>NULL</code> values, leading to the unfortunate fact that for a column X with values
            <code>x1, x2, ‚Ä¶, xn</code>, <code></code></p><pre><code>x1 + x2 + ‚Ä¶ + xn != SUM(X)</code></pre><p>and
            <code><pre>(X1 + X2) != SUM(X1) + SUM(X2)</pre></code>See the <a href="https://www.db-fiddle.com/f/hUeLXcYP38eEqZmvLbdumZ/1">following example</a> in Postgres:</p>
          <pre><code>  WITH v AS (
    SELECT * FROM ( 
        VALUES 
          (1, 5),
          (null, 10) 
      ) AS t (column1, column2) 
  )
  SELECT 
    SUM(column1 + column2) AS sum_of_addition 
    , SUM(column1) + SUM(column2) AS addition_of_sum 
  FROM v;</code></pre>
          <p>which outputs</p>
          <pre><code>    sum_of_addition | addition_of_sum 
   -----------------+-----------------
                  6 |              16
   (1 row)
          </code></pre>
          <h3>Formal Definition, Mismatch with Host Language, and Missing Functions</h3>
          <p>These three sections are taken together, as I found none of them to be of particular relevance to modern
            databases, modern SQL, or analytical query processing.</p>
          <p><em><b>Formal Definition: </b></em>This section highlights areas where the developing SQL[1983] standard
            either disagreed with the IBM implementation, or was not precise enough ‚Äì cursor positioning, lock
            statements, alias scoping rules, and more. I understand this section more to be a critique of the standard,
            as opposed to the language itself. Furthermore, many of these issues (cursors, locks) are not as relevant to
            analytical processing, and are thus not as interesting to me personally.</p>
          <p><em><b>Mismatch with Host Language: </b></em>Similar to the previous section, I found this one mostly
            irrelevant. The author points out many differences between SQL and the host language (e.g., IBM PL/I) that
            cause friction for the programmer. Today, there are so many potential host languages (Python, Ruby,
            Javascript. Java just to name a few), each with their own idiosyncrasies, that it would be impossible for
            SQL to conform to all of them. Technologies like <a href="https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/">LINQ</a> aim to
            address some of these concerns, but as with above, these primarily target application programming use cases.
          </p>
          <p><em><b>Missing Functions: </b></em>Most of the bullet points here are related to cursors and locking, which
            I view as implementation-specific details related to underlying systems.</p>
          <h3>Mistakes</h3>
          <p>This section describes several things that the author views as simply a mistake in the language design.
            Here again, <code>NULL</code> is the prime example:</p>
          <blockquote>
            <p>In my opinion the null value concept is far more trouble than it is worth‚Ä¶ The system should never
              produce a (spuriously) precise answer to a query when the data involved in that query is itself imprecise.
              At least the system should offer the user the explicit option either to ignore nulls or to treat their
              presence as an exception</p>
          </blockquote>
          <p>It is interesting to note that this was far from the consensus view, even amongst the original developers
            of the relational model. EF Codd himself sanctioned the use of <code>NULL</code> in his <a href="https://en.wikipedia.org/wiki/Codd%27s_12_rules">12 Rules</a> (rule no. 3).</p>
          <p>Other &#34;mistakes&#34; included are:</p>
          <ul>
            <li>Primary Key is specified as part of an Index as opposed to at table creation time.<ul>
                <li>The reasoning here is that a Primary Key is really a logical property of a table, and should not be
                  intermingled with an index, which deals primarily with the physical access path of that data. Today,
                  most databases allow a <code>CREATE TABLE</code> statement to include a Primary Key, so this concern
                  has largely been rectified. </li>
              </ul>
            </li>
            <li> <code>SELECT *</code> is undoubtedly convenient for interactive querying, but extremely prone to errors
              when used in programs. <ul>
                <li>Date argues that <code>SELECT *</code> should only be allowed in interactive sessions. I largely
                  agree with this sentiment, but defining ‚Äúinteractive session‚Äù is by no means a trivial problem.</li>
              </ul>
            </li>
          </ul>
          <h3>Aspects of the Relational Model Not Supported</h3>
          <p>This section is another list of miscellaneous items, unified by the fact that each of them prevented
            SQL[1983] from truly being ‚Äúrelational‚Äù.</p>
          <p><em><b>Primary Keys and Foreign Keys:</b></em> Primary Keys could easily be ignored by SQL[1983] and
            Foreign Keys did not even exist. While SQL[2022] does allow for Foreign Keys, and many databases enforce
            referential integrity, SQL[2022] still does not fully understand the semantics of Primary Keys and Foreign
            Keys. Two examples:</p>
          <ul>
            <li>When performing a <code>GROUP BY</code> on the Primary Key of a table, and including other columns from
              that table, because the Primary Key guarantees uniqueness, it is guaranteed that those other columns will
              also be unique; however, SQL requires that those columns also be included in the <code>GROUP BY</code>.
            </li>
            <li>A join between a Foreign Key and its corresponding Primary Key could easily be implicit, but SQL still
              requires the join condition to be explicitly written out.</li>
          </ul>
          <p><em><b>Domains:</b> </em>Domain is another word for ‚Äútype‚Äù. Type systems in SQL[1983] only permitted
            primitive types (int, char, float, etc.). Today, Postgres provides support for user-defined types of
            arbitrary complexity, as well as check-constraints that allow users to restrict primitive types to
            acceptable values. Unfortunately, most OLAP data warehouses don‚Äôt support user-defined types, and SQL itself
            doesn‚Äôt have much to say on the topic.</p>
          <p>To take a simple example of how this can be dangerous, many databases in the wild have tables with integer
            Primary Key ID columns. Clearly not all of the operations that are legal for integers should be allowed on
            Primary Key columns ‚Äì what does it mean to add, multiply, or divide two PK IDs? SQL, and most databases,
            will happily let you perform these operations.</p>
          <p>
            <em><b>Relation Assignment: </b></em>The critique here is a single sentence ‚Äì
          </p>
          <blockquote>
            <p>A limited form of relation assignment is supported via <code>INSERT ... SELECT</code>, but that operation
              does not overwrite the previous content of the target table, and the source of the assignment cannot be an
              arbitrary algebraic expression (or <code>SELECT</code> equivalent).</p>
          </blockquote>
          <p>This is no longer true. Relation assignment can be done via <code>CREATE OR REPLACE TABLE AS</code>. With
            subqueries and CTEs, the source can be any arbitrary algebraic expression.</p>
          <p><em><b>Explicit <code>JOIN</code>, <code>INTERSECT</code>, and <code>DIFFERENCE</code> operators:</b></em>
            SQL[1983] did not support these. SQL[2022] does. JOIN was added to the SQL92 standard.
            <code>INTERSECT</code> and <code>MINUS</code> are supported by most databases, and even if they aren‚Äôt, the
            operators have semantically identical equivalents using <code>JOIN</code>.</p>
          <h3>Summary</h3>
          <p>While many of the critiques of SQL have been fixed by updates to the ANSI standard, many are still present.
            Lack of orthogonality in many places still exists, which makes SQL clunky to learn and use; however, I
            suspect the learning curve here is not actually all that high, judging by the number of people out there who
            can write SQL. By contrast, missing components of the relational model and issues arising from
            <code>NULL</code> values are likely the cause of many queries that look correct but provide wrong answers,
            especially by folks who are confident in their ability to write queries, but unfamiliar with some of the
            nastier traps.</p>
          <p>Despite the improvements listed above, in a <a href="https://www.red-gate.com/simple-talk/opinion/opinion-pieces/chris-date-and-the-relational-model/">2014
              interview</a>, CJ Date said ‚Äúwe didn‚Äôt realize how truly awful SQL was or would turn out to be (note that
            it‚Äôs much worse now than it was then, though it was pretty bad right from the outset).‚Äù This quote leaves me
            wondering ‚Äì if Date himself were to write an updated critique, what would it look like? My best guess is
            most of his criticism would revolve around further departures of SQL from the relational model, but specific
            examples escape me.</p>
          <p>SQL‚Äôs market dominance means every DBMS vendor is strongly incentivized to implement a SQL interface, and
            every aspiring programmer must learn it. So does this mean that despite all its problems, we‚Äôre stuck with
            SQL for good? I think SQL will continue to live on in some form for a very long time, probably even as the
            dominant query language; however, I strongly believe there‚Äôs still room for the development of new query
            languages that have learned the lessons of the past. Furthermore, I think the time is now better than ever
            for such a language to succeed. My reasons for believing so are beyond the scope of this essay, perhaps a
            good topic for the next one.</p>
        </div>
      </article>
      <article>
        <label for="post1">My Time on the Job Market as a Data Engineer</label>
        <time datetime="2019-04-17">04.17.2019</time>
        
        <div>
          <p> In December 2018, I had an amazing job at a company I loved, doing work I was immensely proud of. This
            made my decision to quit extraordinarily difficult. This decision could itself be the topic of a long and
            rambling blog post, but that&#39;s a story for another day. In this post, I&#39;ll talk through my experience being
            unemployed, my approach to the job search, and how I ultimately made the decision for my next career move.
          </p>
          <h3>Unemployment</h3>
          <p> I left my prior job without a new one lined up. When my friends heard I was entering a phase of
            funemployment, they all assumed I would be taking off to backpack around the world for 6 months. I take that
            to mean that I do a pretty good job of hiding my true workaholic anxious nature. By the time the New Year
            rolled around, I was already irrationally worrying about my employability and felt like I was behind
            schedule with everything ‚Äî interview prep, networking, and everything in-between. Soliciting advice from a
            few friends who had taken extended time off, I was able to assuage the rising feeling of panic and settle
            into a loose routine for my time off. I would get a full night&#39;s sleep every day, spend mornings catching up
            on the latest tech news and analysis (I read a LOT of Stratechery), and take at least one weekday each week
            to do absolutely nothing job related (this usually meant riding my bike all day). The rest of the time would
            be split between catching up with old friends and former colleagues, gathering information about companies I
            was interested in, and practicing my programming and technical whiteboarding skills. </p>
          <h3>The Job Search</h3>
          <p> From both my experience as a hiring manager as well as preliminary conversations with a few peers, I
            understood very quickly that it was a buyer&#39;s market, and I could easily make job hunting a full time
            effort. I also wanted to give myself the opportunity to truly evaluate a broad spectrum of companies of all
            shapes and sizes, across many different industries. In total, I spoke to 26 different companies, had tech
            screens with 11 (withdrew my application from the rest), did 9 onsite interviews, and ended up with 7
            offers, the majority of which were for individual contributor roles as a data engineer. </p>
          <p> I thought this process would be exhausting, but it turned out to be far more fun and exciting than I
            anticipated. It was fascinating to learn about all the different organizations and businesses, and the
            unique challenges faced by each of them. As I talked to more and more people, I started to develop a better
            sense of how to extract real signal from my conversations. During interviews, most folks are either in
            evaluation mode or sales mode. In both cases, they&#39;re likely to stick to an HR-approved script. My goal in
            every interview was to build enough rapport with the interviewer that I could successfully navigate the
            conversation away from the typical clich√©s. I also did my best to ask very similar questions to each
            interviewer. By listening closely to their individual answers, I could then evaluate them each in the
            context of the whole, which would often paint a much more telling picture of the organization than any
            individual answer. Do individual contributors understand the vision set forth by their managers? Are the
            pains of the ICs being heard by management? Are different business units aligned on the company mission?
          </p>
          <p> Every company I talked to had extremely aggressive hiring goals. Most were looking to double their
            engineering headcount by the end of the year, and more than double the size of their data engineering teams.
            More often than not, when I asked engineering leaders about their biggest challenges, hiring was #1 on the
            list. I began to evaluate prospective companies through this lens, asking ‚Äúhow will this company
            differentiate from all the others when competing for talent?‚Äù Every company had a different angle for this,
            some leveraging recent fundraising events or a high profile consumer brand, others leaning heavily on their
            social-impact oriented mission. I tried to understand not only how their answers appealed to me, but how
            they might appeal to the broader segment of job-seekers. </p>
          <h3> Key Takeaways </h3>
          <p> I learned a lot during my interviews. Rather than try and tie them all together into a neat narrative,
            I&#39;ll just list a few things that stood out to me as noteworthy: </p>
          <ul>
            <li> The technical bar for data engineering is reasonable. I did quite a bit of prep using the standard
              books and websites like Cracking the Coding Interview and leetcode. Never was I asked anything I felt was
              overly difficult or unfair. </li>
            <li> I don&#39;t consider myself a great programmer, but do think I have better than average soft skills for an
              engineer. Based on my success during the interview process, I suspect this combination is more valuable
              than the inverse. </li>
            <li> A 5 hour onsite interview is simply not enough time to effectively evaluate a workplace. During 5 hours
              of interviewing, a candidate has at most 1 hour available for asking questions about the company. How can
              someone possibly learn enough about a company during that time to make an informed decision? Doing
              pre-interview prep and intelligence gathering is absolutely critical, as is being efficient with your
              time. </li>
            <li> I really enjoyed all of my conversations with companies, except declining offers. It&#39;s emotionally
              draining to let someone down immediately after they&#39;ve congratulated you and told you how excited everyone
              is about the possibility of you joining. It also forced me to confront the reality that the choice to go
              through one door meant closing many others. </li>
          </ul>
          <h3>The Decision</h3>
          <p> I count myself as extraordinarily fortunate to have had my pick of some of the best technology companies
            in San Francisco. I was looking for a company with aggressive growth, a great product, and awesome
            leadership, and while many of the companies I talked to met these criteria, Snowflake was a clear cut above.
            When I first started using Snowflake as a customer at my previous job, I was totally blown away by their
            product. The Snowflake data warehouse was critical to my job as a data engineer, and it was obvious to me
            how revolutionary a technology they had developed.</p>
          <p> The job at Snowflake was in sales engineering, a big change from my prior role as an in-house data
            engineer. As a sales engineer, the responsibilities are primarily around evaluating the data architecture of
            potential customers, helping prove out the value of Snowflake within that architecture, and scoping and
            executing on a proof-of-concept. The chance to get a glimpse of data teams of all shapes and sizes across
            the San Francisco tech scene and beyond seemed like a unique opportunity. </p>
          <p> From a team perspective, I knew Snowflake&#39;s sales and sales engineering org fairly well from my time as a
            customer. Both groups were great to work with ‚Äî their sales engineering lead was enormously valuable in
            helping us with our initial implementation, and the regional sales director struck me as an ambitious,
            driven individual who would likely push me to realize more of my potential. This gave me a high degree of
            confidence in the general quality of the team over at Snowflake, which was confirmed yet again during my
            interview process. </p>
          <p> As I alluded to earlier, I spent a fair amount of my funemployment reading through the back catalogue of
            Ben Thompson&#39;s Stratechery blog. Stratechery focuses primarily on consumer technology, with decidedly fewer
            articles on enterprise software, especially a product as technical as Snowflake. Even so, many of the themes
            he emphasizes over and over when discussing consumer tech apply just as well to enterprise. In this light,
            many of Snowflake&#39;s initiatives made sense as part of a broader strategy. I didn&#39;t see any other players in
            the space operating at the same level, and this combination of superior product and thought leadership made
            it an extremely compelling opportunity. </p>
          <p> I&#39;m only a few weeks into my new role as a sales engineer at Snowflake, and so far it has not
            disappointed. The energy around what we&#39;re building, both in terms of the product and the business is
            absolutely incredible. Funemployment is finally over, but now the real fun begins! </p>
        </div>
      </article>
      <article>
        <label for="post2">Fort Bragg 600k Ride Report</label>
        <time datetime="2017-05-13">05.13.2017</time>
        
        <div>
          <p> The crown jewel of the <a href="https://www.sfrandonneurs.org/">San Francisco Randonneurs</a> brevet
            series is the Fort Bragg 600k. I‚Äôd never ridden a 600k before, and this would be my longest ride by a good
            margin. I was confident my legs were strong enough, but a 600k is ridden on the strength of a rider‚Äôs
            stomach, not his legs. The real challenge would be keeping myself adequately fueled while preventing my
            stomach from revolting.</p>
          <p> To that end, I enlisted the help of <a href="https://www.hammernutrition.com/perpetuem">Hammer
              Perpetuem</a>, a powdered energy drink mix. One scoop of Perpetuem, 70 grams, delivers 135 calories -- 87%
            simple carbohydrates in the form of maltodextrin (simple carbohydrate energy source), 10% soy protein (to
            prevent cannibalization of lean muscle tissue), and 3% fat. It‚Äôs a bland substance, without the ultra-sweet
            kick of energy gels, and a consistency somewhat reminiscent of sidewalk chalk, but it keeps the engine
            running and burns relatively clean. I brought along 10 scoops, and would keep a water bottle filled with a
            2-scoop mix at all times. Theoretically enough Perpetuem for 10 hours of saddle time. </p>
          <p> Pt. Reyes Station is the first stop along the ride, and most other riders made the standard rush for a
            pastry at Bovine Bakery. As they heaped praises on their scones and danishes, I sipped my Strawberry-Vanilla
            Perpetuem in silence. By Petaluma, I‚Äôd nearly finished my second bottle, but was starting to crave salty
            foods. Unwilling to stray too far from my liquid diet, I grabbed some string cheese to satisfy my craving,
            and diligently mixed up my third bottle of powdered fuel. As we rode to Healdsburg, the thought of fried
            chicken tenders from Safeway lodged itself stubbornly in my imagination. By the time I‚Äôd reached my fourth
            bottle of Perpetuem, each successive sip was becoming more and more laborious. When eating on the bike
            becomes a chore, it‚Äôs a sure sign that bad times are ahead. My fifth and final Perpetuem shake was meant to
            last all the way out to Fort Bragg at mile 182, but halfway through the bottle, the thought of drinking more
            induced mild nausea. Unable to eat take in calories on my preferred schedule, the 40 miles from Fort Bragg
            to the Indian Creek campground were an absolute slog. My legs began to run out of energy, but my stomach
            remained closed off, shutting out the possibility of refueling. </p>
          <p> I arrived at Indian Creek at around 10 PM in a ragged state, but was immediately welcomed by lots of
            familiar volunteer faces. I collapsed into a camp chair around a blazing fire, and was handed a variety of
            hot foods including homemade potato vegetable soup and crispy quesadillas. I had planned to sleep for around
            four hours at the campground, but really wanted to eat a full meal before going to bed to allow my body to
            digest and recover. I stared blankly into the distance, waiting for my appetite to recover. It never fully
            did, but I was able to force down the soup and one of the quesadillas before passing out in my tent. </p>
          <p> I closed my eyes for what felt like five seconds, but my 3:30 AM alarm rang loudly. Despite going to bed
            feeling pretty awful, I woke up in much better spirits. I wolfed down an egg and cheese sandwich, and was
            back on my bike, ready to go by 4:15. Riding through Anderson Valley just before dawn was undoubtedly the
            highlight of my ride. Rte 128 is beautiful, but plagued by an endless stream of cars shuttling back and
            forth from the coast. Combined with a small shoulder, it‚Äôs not a pleasant daytime ride. Before sunrise, I
            had the entire road all to myself, with a gibbous moon lighting the valley. </p>
          <p> Once I hit Healdsburg, I was essentially on autopilot. I deliberately chose to ride solo on the second day
            to relieve the pressure of trying to keep up with a group or pull through at an appropriate pace. With a
            more relaxed pace, I could also afford to be less draconian with my diet, and as an added bonus, I was
            completely out of Perpetuem. A few Clif bars and Larabars held me over until Freestone where I stopped in at
            <a href="https://wildflourbread.com/">Wild Flour bakery</a>. Coffee with two scones -- cheddar/bacon/onion
            and meyer lemon/cherry/almond -- did wonders for my mood. From Wild Flour, I‚Äôm a skip, hop and a jump away
            from San Francisco on familiar roads. I developed some pain in my right knee right after Pt Reyes Station
            (mile 330), but at that point, I felt so close to home that spirits were high, even limping along at a
            severely diminished pace. </p>
          <p> I rolled into the finish at Crissy Field just before 4 PM, greeted by a small crew of volunteers holding
            down the fort on a windy day at the shore. Too tired to socialize much, I grabbed a quick bite to eat,
            remounted my bike, and rode the final few miles back to my apartment. </p>
        </div>
      </article>
    </section>
    <section id="pbp">
      <!-- PORTFOLIO -->
      <div>
        <figure>
          
          <p> Paris-Brest-Paris has a credible claim to being the greatest cycling event in the world. If this is true,
            it&#39;s not because the route is the most beautiful or the most challenging (though it has its fair share of
            both), but because of the sheer number of people participating. Over 6,000 riders attempt the 1200 km
            course, and the drama, triumph, and despair of each individual ride is on full display to onlookers. </p>
          <p> No ride report can accurately recreate the experience of the ride, so rather than a play-by-play account,
            I&#39;ll instead tell a series of vignettes about my sensory experiences ‚Äî the sights, sounds, smells, tastes,
            and aches of my 88 hours on the bike. It&#39;s a terribly incomplete picture, but telling the full story is an
            impossible task. Paris-Brest-Paris is something you have to ride to truly understand. Scroll down to
            continue reading ‚¨áÔ∏è </p>
          <p>üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure>
          
          <p> Riding PBP means riding at all times of day, including the dead of night. My wave rolled at 7:30pm, with
            the sun hanging inches above the horizon. The chaos and the rush of the start made it tough to appreciate
            the sunset, but night fell around us and my jitters calmed. We began to catch riders in prior start waves,
            and the infinite trail of red tail lights ahead looked like <span>glowing coals</span>
            marking the way to Brest. Every now and then I&#39;d be tempted to look back, but it was always a mistake.
            Modern headlights are <em>viciously bright</em>. Even a brief glance would make my eyes wince in pain and
            leave spots in my vision. I had to glean whatever information about the situation behind me by watching the
            shadows dance. When the lines of my shadow sharpened, it meant someone was approaching from behind. As they
            <span>faded</span>, it meant we were separating. </p>
          <p> Three hours into the ride, I stopped on the side of the road to relieve myself. As I stretched my back and
            neck, I looked up at the sky and for the first time noticed the half-moon and the stars of the Milky Way
            above. I took a few extra seconds to soak in the beauty of the moment ‚Äî red lights marching on ahead, white
            lights slowly approaching from behind, and the cosmos above, twinkling softly like any other night. I
            remounted my bike and rejoined the endless stream of red and white making its way west. </p>
          <p>üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure>
          
          <p> At our first stop in the town of Mortagne-au-Perche, my first remark was ‚Äúthis whole town smells like
            brie.‚Äù Our evening start meant that for the first part of the ride, the French countryside was completely
            cloaked in darkness. Without visual stimulation aside from the lights of the other riders, my primary
            sensory experience was that of smell. </p>
          <p> Rain from the previous day had moistened the ground, and every forest, farm, and town that we sped through
            was an explosion of odors. Four primary smells left a major impression on me, and all but one I think was
            best described by a different type of cheese: brie, feta, and ch√®vre. The fourth was manure with some sour
            notes. Though none were unpleasant by any means, 770 miles of riding incurred fatigue in every part of my
            body, nose included. The occasional whiff of wild lavender was always a welcome reprieve, and early morning
            towns with boulangeries baking fresh croissants and baguettes were absolutely divine. </p>
          <p> Many US brevets travel on roads with moderate to heavy auto traffic, so I&#39;m mostly accustomed to the
            smells of gasoline and diesel exhaust while I ride. I will take the smells of livestock, wildflowers, and
            pastries any day. </p>
          <p>üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure>
          
          <p> The PBP course rolls up and down through the tiny villages of Normandy and Brittany. The scenery, while at
            times quite lovely, can also become tiresome. The following pattern repeats endlessly: ride through a flat
            stretch lined with either cow pastures or corn fields. Turn into a small climb through a deciduous forest,
            and descend until you reach the outskirts of town. The road pitches up again, and cobblestone houses begin
            to appear, getting denser and denser until you reach the top of the hill, invariably marked by a large
            church. Begin the descent through the center of town over cobbled roads, past the boulangerie and the
            pharmacy, and before you know it, you&#39;re back in the corn fields. Qu√©dillac, M√©dr√©ac, Merl√©ac, R√©t√©ac,
            Loud√©ac. Rinse and repeat. </p>
          <p>
            <img loading="lazy" alt="Sunflowers on the return leg, near Louvigny" src="https://carlineng.com/img/pbp_scenery.jpg"/>
          </p><figcaption>Sunflowers on the return leg, near Louvigny</figcaption>
          üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥
        </figure>
        <figure>
          
          <p>What does it sound like when 6000+ riders from every country on earth attempt to ride 1200 km?
          </p><ul>
            <li> The steady whirring of my bike&#39;s drivetrain </li>
            <li> The creaks, squeaks, and rattles of the bikes owned by 6000 people who all have wildly varying bike
              maintenance standards </li>
            <li> The low rumble of an approaching peloton moving twice my speed. I nervously anticipate the rush of wind
              as they blow past </li>
            <li> Heavy labored breathing as we grind up the endless rolling hills on our way to Brest </li>
            <li> The sound of Ian&#39;s voice up ahead as he chatters happily away with one of our many friends on the ride
            </li>
            <li> Cheers of ‚ÄúBonne route!‚Äù and ‚ÄúBon courage!‚Äù from spectating French villagers </li>
            <li> Crinkling of space blankets as riders toss and turn, trying to steal a few precious minutes of sleep on
              the floor of a control point </li>
            <li> Constant chirping from Garmins, Wahoos, or other bike computers, alerting their owners to who knows
              what </li>
            <li> The clopping of bike cleats on asphalt and tile floors </li>
            <li> A cacophony of every language on Earth. French, German, English, Italian, Hindi, Korean, Japanese,
              Portuguese, Spanish, Chinese and more </li>
          </ul>
          
          <p>üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
        </figure>
        <figure id="discomforts">
          
          <p><span>üõë</span> <span>‚è≠Ô∏è</span></p>
          <p>
            <b>Sunday 7:30 PM:</b> Feeling great. Start wave goes. Body feels amazing.
          </p>
          <p>
            <b>Sunday 10 PM:</b> riding in same position for awhile in a fast group. Feel the typical aches associated
            with riding bikes. Some soreness in the saddle and a bit of numbness in toes, but nothing unusual. Knees
            holding up great. Finally catch up to Irving, Ian and Ben who started 30 minutes before me. Grateful for the
            chance to sit up and ride with them and stave off any potential muscle soreness from hammering with this
            group.
          </p>
          <p>
            <b>Monday 4 AM:</b> rolling out of the control point at Villaine-ah-Juhel. We made the 200k mark in under 8
            hours and in celebration I over-ate. Bolognese topped with gruyere. Stomach not happy. Burps taste like
            gruyere.
          </p>
          <p>
            <b>Monday 11 PM:</b> a few hours of sleep at the hotel in Loud√©ac. Getting up and I am stiff in the legs,
            but a few minutes on the bike and I&#39;m feeling good again. 450 km in, knee is holding up.
          </p>
          <p>
            <b>Tuesday 6 AM:</b> descending in freezing fog down into the town of Sizun. Cold air rushing over exposed
            skin is painful, but my time swimming in the SF bay with the Dolphin Club has made me far more tolerant of
            cold. Espresso and croissant in Sizun, and it&#39;s off to Brest.
          </p>
          <p>
            <b>Tuesday 10 AM:</b> we reach Brest and I am ecstatic. 600 km in, and no physical problems, just fatigue
            and hunger.
          </p>
          <p>
            <b>Tuesday 1 PM:</b> left knee starts acting up. Haven&#39;t had issues with this one before, so am getting
            worried. I stop, stretch out my legs, and attempt to manage the pain by easing back on the pace and trying
            different pedaling positions. Eventually start riding much slower, and with more emphasis on the right leg.
          </p>
          <p>
            <b>Tuesday 6 PM:</b> left knee pain persists, but isn&#39;t getting too much worse from stop to stop. As
            expected, pedaling asymmetrically also starts to cause problems for my right knee. IT band begins to tighten
            up and generate soreness on my right side. Start limping at controls and having trouble walking down stairs.
          </p>
          <p>
            <b>Tuesday 9 PM:</b> Right knee decently swollen and left knee still feeling moderate pain. Riding slowly
            has taken a huge toll on me. Without the rush of physical exertion, staying focused on the road takes an
            enormous amount of mental energy, leading to boredom and frustration. I swear up and down that I am never
            doing this, ever again. To stay awake and alert I fall back on caffeinated gels, which make me nervous and
            jittery. In a bout of frustration, just as we are hitting Loud√©ac for the second time, I turn up the gas.
            For a solid 15 minutes, I&#39;m flying. Lots of pent up energy from the gels and caffeine. Passing dozens of
            riders like they are standing still, and momentarily the joy of riding returns. I stop briefly at a town
            corner and notice that even though I am not feeling tired or breathing heavily, my heart is absolutely
            racing. I stop, take thirty deep breaths, and try to down-regulate both my heart rate and my exhilaration.
            Oddly enough, both knees felt fine. Take a mental note of that, but fully expect this foolishness to put my
            knees further into a hole.
          </p>
          <p>
            <b>Wednesday 2 AM:</b> wake up in the hotel, and everything is stiff as hell. Open my mouth to yawn and
            immediately yelp in pain due to chapped lips. Tongue and mouth are raw and abraded from constantly chewing
            crusty baguettes. Legs feel moderately better, but can tell that knees still won&#39;t be happy back on the
            bike. 450 km to go. How will I manage?
          </p>
          <p>
            <b>Wednesday 10 AM:</b> the heat of the day is driving me crazy, and still bothersome knee pain means I&#39;m
            moving super slowly. After 3 hours of crawling, I look back and a huge group of cyclists is coming up on us
            fast. I can&#39;t resist the temptation, and jump on the train and start hammering to keep up with them.
            Japanese power couple pulls us all the way to Villaine. Wait a second, knee suddenly feels great...
          </p>
          <p>
            <b>Wednesday 4 PM:</b> Decide my knee only feels good when I&#39;m hammering. The last two days of riding slowly
            means my legs have a good deal of snap left. Passing a continuous stream of riders, and begin collecting a
            decent sized train of riders in my slipstream. Get swept up by a group of twelve or so serious 84-hour
            riders and tuck in. This is some of the most fun I&#39;ve ever had on a bike.
          </p>
          <p>
            <b>Wednesday 11 PM:</b> arrive in Mortagne-au-Perche. Legs start to ache from the effort, but knees feel
            decent, especially after completing a regular regimen of hip and hamstring stretches. My renewed focus on
            riding hard means I&#39;m paying less attention to my hands, and at this control I start to notice some of my
            fingers going numb, a classic symptom of ulner nerve compression. I&#39;ll take that any day over knee pain.
          </p>
          <p>
            <b>Wednesday 3 AM:</b> leaving the Mortagne Airbnb, I start developing a pulsing headache, which I suspect
            is from not drinking enough water before going to sleep. Both knees still ache slightly, but somehow in a
            way that doesn&#39;t concern me.
          </p>
          <p>
            <b>Wednesday 11 AM:</b> the finish. Nothing is terribly painful, but nothing feels great either. Contact
            points (butt, hands, feet), joints (knees, ankles), supporting tissues and small muscles (neck, Achilles)
            are all straining from nearly 90 hours of continuous effort. We are elated to have finished successfully,
            and all ignore the aches and pains in the glow of the finish.
          </p>
          <p>
            <b>Thursday 8 AM:</b> the aftermath. The worst after-effects are chapped lips and sores/abrasions in my
            mouth and tongue, which make it hard to eat. Of course legs and butt are sore. Not about to jump on the bike
            again today, but after 1200 km of riding, I&#39;m pleasantly surprised at how good I feel. I can sense the
            randonnesia setting in, and thoughts begin to form in my head ‚Äî what might a 2023 ride look like?
          </p><p>üö¥‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÄÔ∏èüö¥üèΩüö¥üèº‚Äç‚ôÇÔ∏èüö¥</p>
          
        </figure>
      </div>
      
    </section>
    <section id="another-page">
      <!-- ANOTHER PAGE -->
      <p>This page is not referenced in the menu, yet it exists.</p>
      <p><a href="#about">‚Üê back</a></p>
    </section>
  </div></div>
  </body>
</html>
