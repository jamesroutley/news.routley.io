<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/grokking-with-weights-decay/">Original</a>
    <h1>Grokking With Weights Decay</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>Say hi to our new bestiary friend, Grok.</p>
<figure><a href="https://recursiverecipes.schollz.com/ogre.png" title="ogre" data-thumbnail="ogre.png" data-sub-html="&lt;h2&gt;Our lovely ogre: Grok the cruel&lt;/h2&gt;&lt;p&gt;ogre&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="ogre.png" data-srcset="ogre.png, ogre.png 1.5x, ogre.png 2x" data-sizes="auto" alt="ogre.png"/>
    </a><figcaption>Our lovely ogre: Grok the cruel</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>Let’s explore how a network can generalize the solution after already reaching perfect loss.</p>
<h2 id="grokking">Grokking</h2>
<p>Grokking is the model’s ability to move beyond rote learning of training data and develop a broader understanding that allows it to generalize well to unseen inputs.</p>
<h3 id="the-model">The Model</h3>
<p>We’ll try to reproduce this effect using a model trained to predict modular addition <code>(a + b) % vocab</code>.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>NN</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
</span></span><span><span>
</span></span><span><span>        <span>self</span><span>.</span><span>embed</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>n_vocab</span><span>,</span> <span>n_embed</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>layers</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>n_embed</span><span>,</span> <span>n_hidden</span><span>),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>ReLU</span><span>(</span><span>n_hidden</span><span>),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>n_hidden</span><span>,</span> <span>n_hidden</span><span>),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>ReLU</span><span>(</span><span>n_hidden</span><span>),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>n_hidden</span><span>,</span> <span>n_vocab</span><span>))</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>        <span>a</span><span>,</span> <span>b</span> <span>=</span> <span>x</span><span>[:,</span> <span>0</span><span>],</span> <span>x</span><span>[:,</span> <span>1</span><span>]</span>
</span></span><span><span>        <span># compute embedding</span>
</span></span><span><span>        <span>a_embed</span> <span>=</span> <span>self</span><span>.</span><span>embed</span><span>(</span><span>a</span><span>)</span>
</span></span><span><span>        <span>b_embed</span> <span>=</span> <span>self</span><span>.</span><span>embed</span><span>(</span><span>b</span><span>)</span>
</span></span><span><span>        <span># merge them</span>
</span></span><span><span>        <span>embd</span> <span>=</span> <span>a_embed</span> <span>+</span> <span>b_embed</span>
</span></span><span><span>        <span>return</span> <span>self</span><span>.</span><span>layers</span><span>(</span><span>embd</span><span>)</span>
</span></span></code></pre></div><h3 id="dataset">Dataset</h3>
<p>The training is run over a random subset of 30% of the pairs of integers from our vocabulary and we train the model on full batches.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>X</span> <span>=</span> <span>[(</span><span>i</span><span>,</span> <span>j</span><span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>n_vocab</span><span>)</span> <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>n_vocab</span><span>)]</span>
</span></span><span><span><span>random</span><span>.</span><span>shuffle</span><span>(</span><span>X</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>X_train</span> <span>=</span> <span>X</span><span>[:</span> <span>int</span><span>(</span><span>len</span><span>(</span><span>X</span><span>)</span> <span>*</span> <span>split</span><span>)]</span>
</span></span><span><span><span>X_test</span> <span>=</span> <span>X</span><span>[</span><span>int</span><span>(</span><span>len</span><span>(</span><span>X</span><span>)</span> <span>*</span> <span>split</span><span>):]</span>
</span></span><span><span><span>Y_train</span> <span>=</span> <span>[(</span><span>i</span> <span>+</span> <span>j</span><span>)</span> <span>%</span> <span>n_vocab</span> <span>for</span> <span>i</span><span>,</span> <span>j</span> <span>in</span> <span>X_train</span><span>]</span>
</span></span><span><span><span>Y_test</span> <span>=</span> <span>[(</span><span>i</span> <span>+</span> <span>j</span><span>)</span> <span>%</span> <span>n_vocab</span> <span>for</span> <span>i</span><span>,</span> <span>j</span> <span>in</span> <span>X_test</span><span>]</span>
</span></span><span><span>
</span></span><span><span><span># train on full batch</span>
</span></span><span><span><span>def</span> <span>get_batch</span><span>(</span><span>name</span><span>=</span><span>&#39;train&#39;</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>{</span>
</span></span><span><span>        <span>&#39;train&#39;</span><span>:</span> <span>(</span><span>torch</span><span>.</span><span>tensor</span><span>(</span><span>X_train</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>),</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>Y_train</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)),</span>
</span></span><span><span>        <span>&#39;test&#39;</span><span>:</span> <span>(</span><span>torch</span><span>.</span><span>tensor</span><span>(</span><span>X_test</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>),</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>Y_test</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>))</span>
</span></span><span><span>    <span>}[</span><span>name</span><span>]</span>
</span></span></code></pre></div><h3 id="training">Training</h3>
<p>For the training we use AdamW as an optimizer and aim to minimize cross entropy.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>optimizer</span> <span>=</span> <span>optim</span><span>.</span><span>AdamW</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>learning_rate</span><span>,</span> <span>weight_decay</span><span>=</span><span>weight_decay</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>evaluate</span><span>(</span><span>name</span><span>):</span>
</span></span><span><span>    <span>X</span><span>,</span> <span>Y</span> <span>=</span> <span>get_batch</span><span>(</span><span>name</span><span>)</span>
</span></span><span><span>    <span>Y_hat</span> <span>=</span> <span>model</span><span>(</span><span>X</span><span>)</span>
</span></span><span><span>    <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>cross_entropy</span><span>(</span><span>Y_hat</span><span>,</span> <span>F</span><span>.</span><span>one_hot</span><span>(</span><span>Y</span><span>,</span> <span>n_vocab</span><span>)</span><span>.</span><span>float</span><span>())</span>
</span></span><span><span>    <span>accuracy</span> <span>=</span> <span>(</span><span>Y_hat</span><span>.</span><span>argmax</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>)</span> <span>==</span> <span>Y</span><span>)</span><span>.</span><span>float</span><span>()</span><span>.</span><span>mean</span><span>()</span>
</span></span><span><span>    <span>return</span> <span>loss</span><span>,</span> <span>accuracy</span>
</span></span><span><span>
</span></span><span><span><span>@torch.no_grad</span><span>()</span>
</span></span><span><span><span>def</span> <span>evaluate_test</span><span>():</span>
</span></span><span><span>    <span>return</span> <span>evaluate</span><span>(</span><span>&#39;test&#39;</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>evaluate_train</span><span>():</span>
</span></span><span><span>    <span>return</span> <span>evaluate</span><span>(</span><span>&#39;train&#39;</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span># training loop</span>
</span></span><span><span><span>for</span> <span>i</span> <span>in</span> <span>tqdm</span><span>(</span><span>range</span><span>(</span><span>epoch</span><span>)):</span>
</span></span><span><span>    <span>loss</span><span>,</span> <span>accuracy</span> <span>=</span> <span>evaluate_train</span><span>()</span>
</span></span><span><span>    <span>test_loss</span><span>,</span> <span>test_accuracy</span> <span>=</span> <span>evaluate_test</span><span>()</span>
</span></span><span><span>    <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
</span></span><span><span>    <span>loss</span><span>.</span><span>backward</span><span>()</span>
</span></span><span><span>    <span>optimizer</span><span>.</span><span>step</span><span>()</span>
</span></span></code></pre></div><h2 id="results">Results</h2>
<p>The model rapidly overfit the training data (in blue) and achieve perfect accuracy/loss by memorizing the inputs. Meanwhile performing terrible on the testing set (in orange). But after some tens of thousands of epochs something happen and the model “Grok” and generalize the solution to work on unseen inputs.</p>
<figure><a href="https://recursiverecipes.schollz.com/loss.png" title="loss" data-thumbnail="loss.png" data-sub-html="&lt;h2&gt;Loss over time&lt;/h2&gt;&lt;p&gt;loss&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="loss.png" data-srcset="loss.png, loss.png 1.5x, loss.png 2x" data-sizes="auto" alt="loss.png"/>
    </a><figcaption>Loss over time</figcaption>
    </figure>
<figure><a href="https://recursiverecipes.schollz.com/accuracy.png" title="accuracy" data-thumbnail="accuracy.png" data-sub-html="&lt;h2&gt;Accuracy over time&lt;/h2&gt;&lt;p&gt;accuracy&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="accuracy.png" data-srcset="accuracy.png, accuracy.png 1.5x, accuracy.png 2x" data-sizes="auto" alt="accuracy.png"/>
    </a><figcaption>Accuracy over time</figcaption>
    </figure>
<h3 id="how">How?</h3>
<p>How can the model keep training when the loss is already perfect. This is where <code>AdamW</code> kicks-in. from epoch <code>~1000</code> and onward all the model is doing is trying to reduce the weights.</p>
<figure><a href="https://recursiverecipes.schollz.com/weights.png" title="weights" data-thumbnail="weights.png" data-sub-html="&lt;h2&gt;Weights and Squared Weights over time&lt;/h2&gt;&lt;p&gt;weights&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="weights.png" data-srcset="weights.png, weights.png 1.5x, weights.png 2x" data-sizes="auto" alt="weights.png"/>
    </a><figcaption>Weights and Squared Weights over time</figcaption>
    </figure>
<p>And by putting pressure onto minimizing the weights the model is forced to find a generic solution instead of memorizing each entry.</p>
<h3 id="inside-the-model">Inside the model</h3>
<p>While I can’t claim to really understand exactly what the model is doing under the hood, looking at the neurons activation for the first hidden layer gives some feeling for structure emerging in the later stages of training.</p>
<p>Going from untrained to a memorized solution on the training set.</p>
<figure><a href="https://recursiverecipes.schollz.com/memorized.png" title="memorized" data-thumbnail="memorized.png" data-sub-html="&lt;h2&gt;From untrained to memorized&lt;/h2&gt;&lt;p&gt;memorized&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="memorized.png" data-srcset="memorized.png, memorized.png 1.5x, memorized.png 2x" data-sizes="auto" alt="memorized.png"/>
    </a><figcaption>From untrained to memorized</figcaption>
    </figure>
<p>And from a memorized to a generalized solution working for the testing set.</p>
<figure><a href="https://recursiverecipes.schollz.com/generalized.png" title="generalized" data-thumbnail="generalized.png" data-sub-html="&lt;h2&gt;From memorized to generalized&lt;/h2&gt;&lt;p&gt;generalized&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="generalized.png" data-srcset="generalized.png, generalized.png 1.5x, generalized.png 2x" data-sizes="auto" alt="generalized.png"/>
    </a><figcaption>From memorized to generalized</figcaption>
    </figure>
<h2 id="open-questions-to-me">Open Questions (to me)</h2>
<p>Some of the arcane are still inscrutable to my magic. So I’ll leave a note scribbled on the margin of my bestiary to be solved at a later date.</p>
<h3 id="why-does-it-spike">Why does it spike</h3>
<p>I tried running the training for a million epochs just to see what would happen to the activations neurons after a long training, and I can’t explain why the loss/accuracy keep spiking over time.</p>
<figure><a href="https://recursiverecipes.schollz.com/loss-spikes.png" title="loss-spikes" data-thumbnail="loss-spikes.png" data-sub-html="&lt;h2&gt;Loss spikes&lt;/h2&gt;&lt;p&gt;loss-spikes&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="loss-spikes.png" data-srcset="loss-spikes.png, loss-spikes.png 1.5x, loss-spikes.png 2x" data-sizes="auto" alt="loss-spikes.png"/>
    </a><figcaption>Loss spikes</figcaption>
    </figure>
<figure><a href="https://recursiverecipes.schollz.com/accuracy-spikes.png" title="accuracy-spikes" data-thumbnail="accuracy-spikes.png" data-sub-html="&lt;h2&gt;Accuracy spikes&lt;/h2&gt;&lt;p&gt;accuracy-spikes&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="accuracy-spikes.png" data-srcset="accuracy-spikes.png, accuracy-spikes.png 1.5x, accuracy-spikes.png 2x" data-sizes="auto" alt="accuracy-spikes.png"/>
    </a><figcaption>Accuracy spikes</figcaption>
    </figure>
<figure><a href="https://recursiverecipes.schollz.com/weights-spikes.png" title="weights-spikes" data-thumbnail="weights-spikes.png" data-sub-html="&lt;h2&gt;Weights spikes&lt;/h2&gt;&lt;p&gt;weights-spikes&lt;/p&gt;">
        <img src="https://recursiverecipes.schollz.com/svg/loading.min.svg" data-src="weights-spikes.png" data-srcset="weights-spikes.png, weights-spikes.png 1.5x, weights-spikes.png 2x" data-sizes="auto" alt="weights-spikes.png"/>
    </a><figcaption>Weights spikes</figcaption>
    </figure>
<h3 id="concatenated-embeddings-do-not-grok">Concatenated embeddings do not grok</h3>
<p>In the model, the embeddings are computed for both numbers, and they are merged</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># compute embedding</span>
</span></span><span><span><span>a_embed</span> <span>=</span> <span>self</span><span>.</span><span>embed</span><span>(</span><span>a</span><span>)</span>
</span></span><span><span><span>b_embed</span> <span>=</span> <span>self</span><span>.</span><span>embed</span><span>(</span><span>b</span><span>)</span>
</span></span><span><span><span># merge them</span>
</span></span><span><span><span>embd</span> <span>=</span> <span>a_embed</span> <span>+</span> <span>b_embed</span>
</span></span></code></pre></div><p>Encoding the <code>+</code> in the forward pass feels like cheating, it feels like giving the network a freebie. So I experimented with just concatenating the embeddings to let the network do it’s own thing instead.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># compute embedding</span>
</span></span><span><span><span>a_embed</span> <span>=</span> <span>self</span><span>.</span><span>embed</span><span>(</span><span>a</span><span>)</span>
</span></span><span><span><span>b_embed</span> <span>=</span> <span>self</span><span>.</span><span>embed</span><span>(</span><span>b</span><span>)</span>
</span></span><span><span><span># merge them</span>
</span></span><span><span><span>embd</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>((</span><span>a_embed</span><span>,</span> <span>b_embed</span><span>),</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
</span></span></code></pre></div><p>And it never grokked. This is super counter intuitive to me. It would be trivial for the linear layer to emulate <code>a_embed + b_embed</code> by just setting the bias to zero and the weights as mostly zeros with a few ones at the correct positions.</p>
<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/grokking-modular-addition" target="_blank" rel="noopener noreffer ">https://github.com/peluche/grokking-modular-addition</a> and additional graphs for the spikes in the <a href="https://github.com/peluche/grokking-modular-addition/tree/long-training" target="_blank" rel="noopener noreffer ">long-training branch</a>.</p>


<h2 id="sources">Sources</h2>
<p>This entry was inspired by a paper from <a href="https://www.neelnanda.io/" target="_blank" rel="noopener noreffer ">Neel Nanda</a> and <a href="https://chanlawrence.me/" target="_blank" rel="noopener noreffer ">Lawrence Chan</a> <em>Progress measures for grokking via mechanistic</em> interpretability <a href="https://arxiv.org/abs/2301.05217" target="_blank" rel="noopener noreffer ">https://arxiv.org/abs/2301.05217</a>.</p>
</div></div>
  </body>
</html>
