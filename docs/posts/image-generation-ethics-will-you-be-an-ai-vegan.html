<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2022/Aug/29/stable-diffusion/#ai-vegan">Original</a>
    <h1>Image generation ethics: Will you be an AI vegan?</h1>
    
    <div id="readability-page-1" class="page"><div id="primary">

<div>




<p>If you haven’t been paying attention to what’s going on with Stable Diffusion, you really should be.</p>
<p>Stable Diffusion is a new “text-to-image diffusion model” that was <a href="https://stability.ai/blog/stable-diffusion-public-release">released to the public</a> by <a href="https://stability.ai/">Stability.ai</a> six days ago, on August 22nd.</p>
<p>It’s similar to models like Open AI’s DALL-E, but with one crucial difference: they released the whole thing.</p>
<p>You can try it out online at <a href="https://beta.dreamstudio.ai/">beta.dreamstudio.ai</a> (currently for free). Type in a text prompt and the model will generate an image.</p>
<p>You can download and run the model on your own computer (if you have a powerful enough graphics card). Here’s <a href="https://www.reddit.com/r/StableDiffusion/comments/wuyu2u/how_do_i_run_stable_diffusion_and_sharing_faqs/">an FAQ</a> on how to do that.</p>
<p>You can use it for commercial and non-commercial purposes, under the terms of the <a href="https://huggingface.co/spaces/CompVis/stable-diffusion-license">Creative ML OpenRAIL-M license</a>—which lists some usage restrictions that include avoiding using it to break applicable laws, generate false information, discriminate against individuals or provide medical advice.</p>
<p>In just a few days, there has been an <strong>explosion</strong> of innovation around it. The things people are building are absolutely astonishing.</p>
<p>I’ve been tracking the <a href="https://www.reddit.com/r/StableDiffusion/">r/StableDiffusion</a> subreddit and following Stability.ai founder <a href="https://twitter.com/EMostaque">Emad Mostaque</a> on Twitter.</p>
<h4>img2img</h4>
<p>Generating images from text is one thing, but generating images from other images is a whole new ballgame.</p>
<p>My favourite example so far comes <a href="https://www.reddit.com/r/StableDiffusion/comments/wzlmty/its_some_kind_of_black_magic_i_swear/">from Reddit user argaman123</a>. They created this image:</p>
<p><img alt="A simple looking Microsoft Paint style image made of flat colours: a sky blue background, a rough yellow desert in the foreground, a semi-circle black line representing a half dome over five shapes in two shades of grey representing buildings inside the dome. A yellow circle represents the sun in the top right of the image, above the dome." src="https://static.simonwillison.net/static/2022/stable-diffusion-img2img.png"/></p>
<p>And added this prompt (or &#34;<a href="https://www.reddit.com/r/StableDiffusion/comments/wzlmty/comment/im35vwf/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3">something along those lines</a>&#34;):</p>
<blockquote>
<p>A distant futuristic city full of tall buildings inside a huge transparent glass dome, In the middle of a barren desert full of large dunes, Sun rays, Artstation, Dark sky full of stars with a shiny sun, Massive scale, Fog, Highly detailed, Cinematic, Colorful</p>
</blockquote>
<p>The model produced the following two images:</p>
<p><img alt="A gorgeous image of a futuristic city under a glass domb, in front of a wind-swept desert. The composition matches the Microsoft Paint input, but everything is rendered in great detail." src="https://static.simonwillison.net/static/2022/stable-diffusion-1.jpg"/></p>
<p><img alt="A second image, similar to the first but this time the domb is composed of triangle sheets of glass. The composition remains the same." src="https://static.simonwillison.net/static/2022/stable-diffusion-2.jpg"/></p>
<p>These are amazing. In my <a href="https://simonwillison.net/2022/Jun/23/dall-e/">previous experiments with DALL-E</a> I’ve tried to recreate photographs I have taken, but getting the exact composition I wanted has always proved impossible using just text. With this new capability I feel like I could get the AI to do pretty much exactly what I have in my mind.</p>
<p>Imagine having an on-demand concept artist that can generate anything you can imagine, and can iterate with you towards your ideal result. For free (or at least for very-cheap).</p>
<p>You can run this today on your own computer, if you can figure out how to set it up. You can <a href="https://replicate.com/stability-ai/stable-diffusion">try it in your browser</a> using Replicate, <a href="https://huggingface.co/spaces/huggingface/diffuse-the-rest">or Hugging Face</a>. This capability is apparently coming to the DreamStudio interface <a href="https://twitter.com/EMostaque/status/1563632874091421697">next week</a>.</p>
<p>There’s so much more going on.</p>
<p><a href="https://github.com/hlky/stable-diffusion-webui">stable-diffusion-webui</a> is an open source UI you can run on your own machine providing a powerful interface to the model. <a href="https://twitter.com/altryne/status/1563452692399214594">Here’s a Twitter thread</a> showing what it can do.</p>
<p>Reddit user alpacaAI shared <a href="https://old.reddit.com/r/StableDiffusion/comments/wyduk1/show_rstablediffusion_integrating_sd_in_photoshop/">a video demo</a> of a Photoshop plugin they are developing which has to be seen to be believed. They have a registration form up on <a href="https://www.getalpaca.io/">getalpaca.io</a> for people who want to try it out once it’s ready.</p>
<p><img alt="A screenshot of Photoshop - a complex image on multiple layers is shown in the background. The user has open a dialog where they have entered the prompt &#34;a dog seating on a path going up in a hill&#34; - with modifiers of &#34;studio ghibli::3&#34;, &#34;highly detailed::1&#34;, &#34;mang anime::1&#34;, &#34;cel-shading::1&#34; and &#34;game characters::1&#34;." src="https://static.simonwillison.net/static/2022/stable-diffusion-alpaca.jpg"/></p>
<p>Reddit user Hoppss ran a 2D animated clip from Disney’s Aladdin through <code>img2img</code> frame-by frame, using the following parameters:</p>
<p><code>--prompt &#34;3D render&#34; --strength 0.15 --seed 82345912 --n_samples 1 --ddim_steps 100 --n_iter 1 --scale 30.0 --skip_grid</code></p>
<p>The result was <a href="https://www.reddit.com/r/StableDiffusion/comments/wys3w5/applying_img2img_to_video_3d_aladdin/">a 3D animated video</a>. Not a great quality one, but pretty stunning for a shell script and a two word prompt!</p>
<h4>And there’s so much more to come</h4>
<p>All of this happened in just six days since the model release. Emad Mostaque <a href="https://twitter.com/EMostaque/status/1564011883572633600">on Twitter</a>:</p>
<blockquote>
<p>We use as much compute as stable diffusion used every 36 hours for our upcoming open source models</p>
</blockquote>
<p>This made me think of <a href="https://parti.research.google/">Google’s Parti paper</a>, which included a demonstration that showed that once the model was trained to 200bn parameters it could generate images with correctly spelled text!</p>
<p><img alt="Four images of a kangaroo holding a sign generated by Parti. In the 350M and 750M parameter images the text on the sign is garbage symbols. At 3B parameters it does at least look like words, but is still not correct. At 20B parametecs the sign reads &#34;Welcome friends&#34;." src="https://static.simonwillison.net/static/2022/stable-diffusion-parti.jpg"/></p>
<h4 id="ai-vegan">Ethics: will you be an AI vegan?</h4>
<p>I’m finding the ethics of all of this extremely difficult.</p>
<p>Stable Diffusion has been trained on millions of copyrighted images scraped from the web.</p>
<p>The <a href="https://github.com/CompVis/stable-diffusion/blob/69ae4b35e0a0f6ee1af8bb9a5d0016ccb27e36dc/Stable_Diffusion_v1_Model_Card.md">Stable Diffusion v1 Model Card</a> has the full details, but the short version is that it uses <a href="https://laion.ai/blog/laion-5b/">LAION-5B</a> (5.85 billion image-text pairs) and its <a href="https://laion.ai/blog/laion-aesthetics/">laion-aesthetics v2 5+</a> subset (which I think is ~600M pairs filtered for aesthetics). These images were scraped from the web.</p>
<p>I’m not qualified to speak to the legality of this. I’m personally more concerned with the morality.</p>
<p>The final model is I believe around 9GB of data—a binary blob of floating point numbers. The fact that it can compress such an enormous quantity of visual information into such a small space is itself a fascinating detail.</p>
<p>As such, each image in the training set contributes only a tiny amount of information—a few tweaks to some numeric weights spread across the entire network.</p>
<p>But... the people who created these images did not give their consent. And the model can be seen as a direct threat to their livelihoods. No-one expected creative AIs to come for the artist jobs first, but here we are!</p>
<p>I’m still thinking through this, and I’m eager to consume more commentary about it. But my current mental model is to think about this in terms of veganism, as an analogy for people making their own personal ethical decisions.</p>
<p>I know many vegans. They have access to the same information as I do about the treatment of animals, and they have made informed decisions about their lifestyle, which I fully respect.</p>
<p>I myself remain a meat-eater.</p>
<p>There will be many people who will decide that the  AI models trained on copyrighted images are incompatible with their values. I understand and respect that decision.</p>
<p>But when I look at that img2img example of the futuristic city in the dome, I can’t resist imagining what I could do with that capability.</p>
<p>If someone were to create <a href="https://twitter.com/simonw/status/1563201333821288452">a vegan model</a>, trained entirely on out-of-copyright images, I would be delighted to promote it and try it out. If its results were good enough, I might even switch to it entirely.</p>
<h4>Indistinguishable from magic</h4>
<p>Just a few months ago, if I’d seen someone on a fictional TV show using an interface like that Photoshop plugin I’d have grumbled about how that was a step too far even by the standards of American network TV dramas.</p>
<p>Science fiction is real now. Machine learning generative models are here, and the rate with which they are improving is unreal. It’s worth paying real attention to what they can do and how they are developing.</p>
<p>I’m tweeting about this stuff a lot these days. <a href="https://twitter.com/simonw">Follow @simonw on Twitter</a> for more.</p>




</div>

</div></div>
  </body>
</html>
