<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/rag-and-tatters-document-chunking-and-retrieval-with-ann-using-nsw/">Original</a>
    <h1>RAG and Tatters Document Chunking and Retrieval with ANN Using NSW</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p><em>At first glance, the cloak appears to be nothing more than a collection of rags stitched together haphazardly, frayed edges fluttering like whispers in the wind. Yet, as the old wizard wraps it tightly around his shoulders, the air shimmers with a soft, silvery glow. Each tattered piece of fabric is imbued with arcane runes, barely visible, that hum with ancient magic, shielding him from the piercing cold and prying eyes alike. This patchwork garment, a tapestry of secrets, has safeguarded many a hidden wisdom through the ages.</em></p>
<figure><a href="https://github.com/caddyserver/caddy/releases/tag/cloak.png" title="cloak" data-thumbnail="cloak.png" data-sub-html="&lt;h2&gt;Cloak in tatters&lt;/h2&gt;&lt;p&gt;cloak&lt;/p&gt;">
        <img src="https://github.com/svg/loading.min.svg" data-src="cloak.png" data-srcset="cloak.png, cloak.png 1.5x, cloak.png 2x" data-sizes="auto" alt="cloak.png"/>
    </a><figcaption>Cloak in tatters</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>Create a threadbare implementation of the Retrieval-Augmented Generation (RAG) toolkit.</p>
<h2 id="why">Why?</h2>
<p>Using RAG lets us search documents by meaning instead of exact keywords. The matching documents are used to supplement the LLM context. This is useful for workflows that require asking questions from local documents on which the model hasn’t been trained. Or isolate a specific paragraph in a book or paper to reduce the cost of long context queries.</p>
<h2 id="compare-embeddings">Compare Embeddings</h2>
<p>I’ll use a random off-the-shelf model to generate embeddings. If you want to read more about how Embeddings are created, take a look at my <a href="https://github.com/posts/embeddings-necronomicon/" rel="">Embeddings Necronomicon</a>.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>langchain_community.embeddings</span> <span>import</span> <span>HuggingFaceEmbeddings</span>
</span></span><span><span>
</span></span><span><span><span>embeddings</span> <span>=</span> <span>HuggingFaceEmbeddings</span><span>()</span>
</span></span><span><span><span>def</span> <span>get_embedding</span><span>(</span><span>text</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>embeddings</span><span>.</span><span>embed_query</span><span>(</span><span>text</span><span>))</span>
</span></span></code></pre></div><p>Embeddings are N-dimensional vectors, and can be compared using cosine_similarity (or cosine_distance) which boils down to a dot product of normalized vectors.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>cosine_similarity</span><span>(</span><span>embed1</span><span>,</span> <span>embed2</span><span>):</span>
</span></span><span><span>    <span>norm_embed1</span> <span>=</span> <span>embed1</span> <span>/</span> <span>embed1</span><span>.</span><span>norm</span><span>()</span>
</span></span><span><span>    <span>norm_embed2</span> <span>=</span> <span>embed2</span> <span>/</span> <span>embed2</span><span>.</span><span>norm</span><span>()</span>
</span></span><span><span>    <span>return</span> <span>norm_embed1</span> <span>@</span> <span>norm_embed2</span><span>.</span><span>T</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>cosine_distance</span><span>(</span><span>embed1</span><span>,</span> <span>embed2</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>1</span> <span>-</span> <span>cosine_similarity</span><span>(</span><span>embed1</span><span>,</span> <span>embed2</span><span>)</span>
</span></span></code></pre></div><p>This gives us a way to compare how similar two blocks of text are:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>lemon</span> <span>=</span> <span>get_embedding</span><span>(</span><span>&#39;lemons are yellow&#39;</span><span>)</span>
</span></span><span><span><span>lime</span> <span>=</span> <span>get_embedding</span><span>(</span><span>&#39;limes are green&#39;</span><span>)</span>
</span></span><span><span><span>tomato</span> <span>=</span> <span>get_embedding</span><span>(</span><span>&#39;tomatoes are red&#39;</span><span>)</span>
</span></span><span><span><span>rain</span> <span>=</span> <span>get_embedding</span><span>(</span><span>&#39;today is rainy&#39;</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;distance between &#34;lemon&#34; and &#34;lime&#34; is   </span><span>{</span><span>cosine_distance</span><span>(</span><span>lemon</span><span>,</span> <span>lime</span><span>)</span><span>}</span><span>&#39;</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;distance between &#34;lemon&#34; and &#34;tomato&#34; is </span><span>{</span><span>cosine_distance</span><span>(</span><span>lemon</span><span>,</span> <span>tomato</span><span>)</span><span>}</span><span>&#39;</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;distance between &#34;lemon&#34; and &#34;rain&#34; is   </span><span>{</span><span>cosine_distance</span><span>(</span><span>lemon</span><span>,</span> <span>rain</span><span>)</span><span>}</span><span>&#39;</span><span>)</span>
</span></span></code></pre></div><pre tabindex="0"><code>distance between &#34;lemon&#34; and &#34;lime&#34; is   0.3547552824020386
distance between &#34;lemon&#34; and &#34;tomato&#34; is 0.5043934583663940
distance between &#34;lemon&#34; and &#34;rain&#34; is   0.7150339484214783
</code></pre><h2 id="document-chunking">Document Chunking</h2>
<p>Once we identify the documents matching our query we can add them to the context. This approach has several drawbacks, including scalability issues, because attention is quadratic on the size of the context. The documents might not be relevent in their entirety diluting the focus of the query. For that reason we will look at how we can split the documents into smaller chunks.</p>
<p>For this section, I’ll use the Wikipedia article on wizards as our document.</p>
<h3 id="fixed-size">Fixed Size</h3>
<p>The simplest strategy is to split the documents into equally sized chunks. Here for simplicity of a fixed string size, but in practice it would be smarter to chunk it into a fixed number of tokens.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>chunk_fixed_size</span><span>(</span><span>text</span><span>,</span> <span>size</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>[</span><span>text</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>size</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> <span>len</span><span>(</span><span>text</span><span>),</span> <span>size</span><span>)]</span>
</span></span><span><span>
</span></span><span><span><span>chunks</span> <span>=</span> <span>chunk_fixed_size</span><span>(</span><span>document</span><span>,</span> <span>50</span><span>)</span>
</span></span><span><span><span>chunks</span><span>[:</span><span>4</span><span>]</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>[</span><span>&#39;Magicians appearing in fantasy fiction</span><span>\n\n</span><span>For other &#39;</span><span>,</span>
</span></span><span><span> <span>&#39;uses, see [Magician</span><span>\n</span><span>(disambiguation)](/wiki/Magici&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;an_</span><span>\\</span><span>(disambiguation</span><span>\\</span><span>) &#34;Magician</span><span>\n\\</span><span>(disambiguation</span><span>\\</span><span>)&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;&#34;) and [Magi (disambiguation)](/wiki/Magi_</span><span>\\</span><span>(disamb&#39;</span><span>]</span>
</span></span></code></pre></div><p>Optionally we could also allow for overlaps between chunks.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>chunk_fixed_size_overlap</span><span>(</span><span>text</span><span>,</span> <span>size</span><span>,</span> <span>overlap</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>[</span><span>text</span><span>[</span><span>i</span><span>:</span><span>i</span><span>+</span><span>size</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> <span>len</span><span>(</span><span>text</span><span>),</span> <span>size</span> <span>-</span> <span>overlap</span><span>)]</span>
</span></span><span><span>
</span></span><span><span><span>chunks</span> <span>=</span> <span>chunk_fixed_size_overlap</span><span>(</span><span>document</span><span>,</span> <span>50</span><span>,</span> <span>10</span><span>)</span>
</span></span><span><span><span>chunks</span><span>[:</span><span>4</span><span>]</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>[</span><span>&#39;Magicians appearing in fantasy fiction</span><span>\n\n</span><span>For other &#39;</span><span>,</span>
</span></span><span><span> <span>&#39;For other uses, see [Magician</span><span>\n</span><span>(disambiguation)](/w&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;ation)](/wiki/Magician_</span><span>\\</span><span>(disambiguation</span><span>\\</span><span>) &#34;Magicia&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;) &#34;Magician</span><span>\n\\</span><span>(disambiguation</span><span>\\</span><span>)&#34;) and [Magi (disamb&#39;</span><span>]</span>
</span></span></code></pre></div><h3 id="recursive-character-split-rcs">Recursive Character Split (RCS)</h3>
<p>A more useful approach is to split the text based on a hierarchy of specific landmarks (e.g. <code>&#39;\n\n&#39;</code>, <code>&#39;\n&#39;</code>, <code>&#39; &#39;</code>) until we reach the desired size. This is meant to preserve more structure than simple fixed size split.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>chunk_recursive_character_split</span><span>(</span><span>text</span><span>,</span> <span>size</span><span>,</span> <span>separators</span><span>=</span><span>[</span><span>&#39;</span><span>\n\n</span><span>&#39;</span><span>,</span> <span>&#39;</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39; &#39;</span><span>]):</span>
</span></span><span><span>    <span>if</span> <span>len</span><span>(</span><span>text</span><span>)</span> <span>&lt;=</span> <span>size</span><span>:</span> <span>return</span> <span>[</span><span>text</span><span>]</span>
</span></span><span><span>    <span>for</span> <span>separator</span> <span>in</span> <span>separators</span> <span>+</span> <span>[</span><span>&#39;&#39;</span><span>]:</span>
</span></span><span><span>        <span>if</span> <span>(</span><span>index</span> <span>:=</span> <span>text</span><span>[:</span><span>size</span><span>]</span><span>.</span><span>rfind</span><span>(</span><span>separator</span><span>))</span> <span>!=</span> <span>-</span><span>1</span><span>:</span>
</span></span><span><span>            <span>index</span> <span>+=</span> <span>len</span><span>(</span><span>separator</span><span>)</span>
</span></span><span><span>            <span>return</span> <span>[</span><span>text</span><span>[:</span><span>index</span><span>]]</span> <span>+</span> <span>chunk_recursive_character_split</span><span>(</span><span>text</span><span>[</span><span>index</span><span>:],</span> <span>size</span><span>,</span> <span>separators</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>chunks</span> <span>=</span> <span>chunk_recursive_character_split</span><span>(</span><span>document</span><span>,</span> <span>50</span><span>)</span>
</span></span><span><span><span>chunks</span><span>[:</span><span>4</span><span>]</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>[</span><span>&#39;Magicians appearing in fantasy fiction</span><span>\n\n</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;For other uses, see [Magician</span><span>\n</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;(disambiguation)](/wiki/Magician_</span><span>\\</span><span>(disambiguation</span><span>\\</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;) &#34;Magician</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span></code></pre></div><p>It’s a good default for chunking. In practice we’d use longer than 50 characters chunks depending on the capabilities of the embedding model we are using as well as the context size of our LLM.</p>
<h3 id="document-specific-splitting">Document Specific Splitting</h3>
<p>Or split based on the document specific grammar (e.g. markdown, HTML, PDF …).</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>chunk_markdown</span><span>(</span><span>text</span><span>,</span> <span>size</span><span>,</span> <span>offset</span><span>=</span><span>0</span><span>):</span>
</span></span><span><span>    <span>&#39;&#39;&#39; piggyback on recursive character split for the demo but it should use a markdown parser &#39;&#39;&#39;</span>
</span></span><span><span>    <span>separators</span> <span>=</span> <span>[</span>
</span></span><span><span>        <span>&#39;</span><span>\n</span><span># &#39;</span><span>,</span> <span>&#39;</span><span>\n</span><span>## &#39;</span><span>,</span> <span>&#39;</span><span>\n</span><span>### &#39;</span><span>,</span> <span>&#39;</span><span>\n</span><span>#### &#39;</span><span>,</span> <span>&#39;</span><span>\n</span><span>##### &#39;</span><span>,</span> <span>&#39;</span><span>\n</span><span>###### &#39;</span><span>,</span> <span># headings</span>
</span></span><span><span>        <span>&#39;```</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;</span><span>\n\n</span><span>&#39;</span><span>,</span> <span># blocks</span>
</span></span><span><span>        <span>&#39;</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;`&#39;</span><span>,</span> <span>&#39;[&#39;</span><span>,</span> <span>&#39;]&#39;</span><span>,</span> <span>&#39;(&#39;</span><span>,</span> <span>&#39;)&#39;</span><span>,</span> <span>&#39;*&#39;</span><span>,</span> <span>&#39;_&#39;</span><span>,</span> <span># inline</span>
</span></span><span><span>        <span>&#39; &#39;</span><span>,</span> <span># words</span>
</span></span><span><span>    <span>]</span>
</span></span><span><span>    <span>if</span> <span>len</span><span>(</span><span>text</span><span>)</span> <span>&lt;=</span> <span>size</span><span>:</span> <span>return</span> <span>[</span><span>text</span><span>]</span>
</span></span><span><span>    <span>for</span> <span>separator</span> <span>in</span> <span>separators</span> <span>+</span> <span>[</span><span>&#39;&#39;</span><span>]:</span>
</span></span><span><span>        <span>if</span> <span>(</span><span>index</span> <span>:=</span> <span>text</span><span>[</span><span>offset</span><span>:</span><span>size</span><span>]</span><span>.</span><span>rfind</span><span>(</span><span>separator</span><span>))</span> <span>!=</span> <span>-</span><span>1</span><span>:</span>
</span></span><span><span>            <span>index</span> <span>+=</span> <span>offset</span>
</span></span><span><span>            <span>return</span> <span>[</span><span>text</span><span>[:</span><span>index</span><span>]]</span> <span>+</span> <span>chunk_markdown</span><span>(</span><span>text</span><span>[</span><span>index</span><span>:],</span> <span>size</span><span>,</span> <span>offset</span><span>=</span><span>len</span><span>(</span><span>separator</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span>doc</span> <span>=</span> <span>&#39;&#39;&#39;
</span></span></span><span><span><span># The Enigmatic Life of Wizard Eldrath
</span></span></span><span><span><span>
</span></span></span><span><span><span>## Introduction
</span></span></span><span><span><span>Eldrath the Wise, a wizard of great renown, has fascinated scholars and adventurers alike with his mysterious powers and secretive nature.
</span></span></span><span><span><span>
</span></span></span><span><span><span>## Notable Achievements
</span></span></span><span><span><span>Eldrath is known for many great deeds, including the discovery of the lost city of Aranthar and the creation of the spell of eternal light.
</span></span></span><span><span><span>&#39;&#39;&#39;</span>
</span></span><span><span><span>chunks</span> <span>=</span> <span>chunk_markdown</span><span>(</span><span>doc</span><span>,</span> <span>100</span><span>)</span>
</span></span><span><span><span>chunks</span><span>[:</span><span>6</span><span>]</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>[</span><span>&#39;</span><span>\n</span><span># The Enigmatic Life of Wizard Eldrath</span><span>\n</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;</span><span>\n</span><span>## Introduction&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;</span><span>\n</span><span>Eldrath the Wise, a wizard of great renown, has fascinated scholars and adventurers alike with his&#39;</span><span>,</span>
</span></span><span><span> <span>&#39; mysterious powers and secretive nature.</span><span>\n</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;</span><span>\n</span><span>## Notable Achievements&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;</span><span>\n</span><span>Eldrath is known for many great deeds, including the discovery of the lost city of Aranthar and&#39;</span><span>,</span>
</span></span></code></pre></div><h3 id="semantic-splitting">Semantic Splitting</h3>
<p>A more interesting concept is to use embeddings themselves to determine how to chunk the document by meaning. A naive aproach is to split the document into sentences (here I’ll re-use recursive character split) compute their embeddings. Use the embeddings to find topics boundary in the text and merge the rest together.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>cluster_chunks</span><span>(</span><span>chunks</span><span>,</span> <span>indices</span><span>,</span> <span>size</span><span>=</span><span>500</span><span>):</span>
</span></span><span><span>    <span>&#39;&#39;&#39; cluster chunks such that:
</span></span></span><span><span><span>      - each cluster is smaller or equal to size
</span></span></span><span><span><span>      - chunks are clustered according to their relative similarities
</span></span></span><span><span><span>    &#39;&#39;&#39;</span>
</span></span><span><span>    <span>indices</span> <span>=</span> <span>[</span><span>i</span> <span>+</span> <span>1</span> <span>for</span> <span>i</span> <span>in</span> <span>indices</span><span>]</span> <span># shift to the right</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>rec</span><span>(</span><span>start</span><span>,</span> <span>end</span><span>,</span> <span>idx</span><span>=</span><span>0</span><span>):</span>
</span></span><span><span>        <span># shortcircuit if the entire chunk fits</span>
</span></span><span><span>        <span>if</span> <span>sum</span><span>(</span><span>len</span><span>(</span><span>c</span><span>)</span> <span>for</span> <span>c</span> <span>in</span> <span>chunks</span><span>[</span><span>start</span><span>:</span><span>end</span><span>])</span> <span>&lt;=</span> <span>size</span><span>:</span>
</span></span><span><span>            <span>return</span> <span>[</span><span>&#39;&#39;</span><span>.</span><span>join</span><span>(</span><span>chunks</span><span>[</span><span>start</span><span>:</span><span>end</span><span>])]</span>
</span></span><span><span>        <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>idx</span><span>,</span> <span>len</span><span>(</span><span>indices</span><span>)):</span>
</span></span><span><span>            <span>index</span> <span>=</span> <span>indices</span><span>[</span><span>i</span><span>]</span>
</span></span><span><span>            <span>if</span> <span>start</span> <span>&lt;</span> <span>index</span> <span>&lt;</span> <span>end</span><span>:</span>
</span></span><span><span>                <span>return</span> <span>rec</span><span>(</span><span>start</span><span>,</span> <span>index</span><span>,</span> <span>i</span> <span>+</span> <span>1</span><span>)</span> <span>+</span> <span>rec</span><span>(</span><span>index</span><span>,</span> <span>end</span><span>,</span> <span>i</span> <span>+</span> <span>1</span><span>)</span>
</span></span><span><span>    
</span></span><span><span>    <span>return</span> <span>rec</span><span>(</span><span>0</span><span>,</span> <span>len</span><span>(</span><span>chunks</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>chunk_semantic</span><span>(</span><span>text</span><span>,</span> <span>size</span><span>=</span><span>100</span><span>):</span>
</span></span><span><span>    <span>mini_chunks</span> <span>=</span> <span>chunk_recursive_character_split</span><span>(</span><span>text</span><span>,</span> <span>size</span><span>)</span>
</span></span><span><span>    <span>embeddings</span> <span>=</span> <span>[</span><span>get_embedding</span><span>(</span><span>c</span><span>)</span> <span>for</span> <span>c</span> <span>in</span> <span>mini_chunks</span><span>]</span>
</span></span><span><span>    <span>similarities</span> <span>=</span> <span>t</span><span>.</span><span>cosine_similarity</span><span>(</span><span>t</span><span>.</span><span>stack</span><span>(</span><span>embeddings</span><span>[:</span><span>-</span><span>1</span><span>]),</span> <span>t</span><span>.</span><span>stack</span><span>(</span><span>embeddings</span><span>[</span><span>1</span><span>:]))</span>
</span></span><span><span>    <span>_</span><span>,</span> <span>indices</span> <span>=</span> <span>t</span><span>.</span><span>sort</span><span>(</span><span>similarities</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>cluster_chunks</span><span>(</span><span>mini_chunks</span><span>,</span> <span>indices</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>chunks</span> <span>=</span> <span>chunk_semantic</span><span>(</span><span>document</span><span>)</span>
</span></span><span><span><span>chunks</span><span>[:</span><span>4</span><span>]</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>[</span><span>&#39;Magicians appearing in fantasy fiction</span><span>\n\n</span><span>For other uses, see [Magician</span><span>\n</span><span>(disambiguation)](/wiki/Magician_</span><span>\\</span><span>(disambiguation</span><span>\\</span><span>) &#34;Magician</span><span>\n\\</span><span>(disambiguation</span><span>\\</span><span>)&#34;) and [Magi (disambiguation)](/wiki/Magi_</span><span>\\</span><span>(disambiguation</span><span>\\</span><span>)</span><span>\n</span><span>&#34;Magi </span><span>\\</span><span>(disambiguation</span><span>\\</span><span>)&#34;).</span><span>\n\n</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;&#34;Wizard (fantasy)&#34; redirects here. For other uses, see [Wizard</span><span>\n</span><span>(disambiguation)](/wiki/Wizard_</span><span>\\</span><span>(disambiguation</span><span>\\</span><span>) &#34;Wizard</span><span>\n\\</span><span>(disambiguation</span><span>\\</span><span>)&#34;).</span><span>\n\n</span><span>[![](//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-</span><span>\n</span><span>new.svg/50px-Question_book-new.svg.png)](/wiki/File:Question_book-new.svg)|</span><span>\n</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;This article **needs additional citations</span><span>\n</span><span>&#39;</span><span>,</span>
</span></span><span><span> <span>&#39;for[verification](/wiki/Wikipedia:Verifiability &#34;Wikipedia:Verifiability&#34;)**.</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span></code></pre></div><p>It a tradeoff that yields better quality chunks than RCS but is more computationally expensive to run.</p>
<h2 id="document-retrieval">Document Retrieval</h2>
<p>Let’s emulate a vector database in 5 lines of code.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># create a dummy database for our embegginds / chunks pairs</span>
</span></span><span><span><span>def</span> <span>create_db</span><span>(</span><span>documents</span><span>):</span>
</span></span><span><span>    <span>chunks</span> <span>=</span> <span>[</span><span>chunk</span> <span>for</span> <span>document</span> <span>in</span> <span>documents</span> <span>for</span> <span>chunk</span> <span>in</span> <span>chunk_recursive_character_split</span><span>(</span><span>document</span><span>,</span> <span>100</span><span>)]</span>
</span></span><span><span>    <span>db</span> <span>=</span> <span>t</span><span>.</span><span>stack</span><span>([</span><span>get_embedding</span><span>(</span><span>chunk</span><span>)</span> <span>for</span> <span>chunk</span> <span>in</span> <span>chunks</span><span>])</span>
</span></span><span><span>    <span>return</span> <span>chunks</span><span>,</span> <span>db</span>
</span></span><span><span>
</span></span><span><span><span>chunks</span><span>,</span> <span>db</span> <span>=</span> <span>create_db</span><span>([</span><span>document</span><span>])</span>
</span></span></code></pre></div><h3 id="exhaustive-search">Exhaustive Search</h3>
<p>The simplest and exact way to retrieve chunks from the vector database is to perform an exhaustive search.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>retrieve</span><span>(</span><span>query</span><span>,</span> <span>k</span><span>=</span><span>3</span><span>,</span> <span>threshold</span><span>=</span><span>0.5</span><span>):</span>
</span></span><span><span>    <span>query_embedding</span> <span>=</span> <span>get_embedding</span><span>(</span><span>query</span><span>)</span>
</span></span><span><span>    <span>similarities</span> <span>=</span> <span>t</span><span>.</span><span>cosine_similarity</span><span>(</span><span>db</span><span>,</span> <span>query_embedding</span><span>)</span>
</span></span><span><span>    <span>values</span><span>,</span> <span>indices</span> <span>=</span> <span>t</span><span>.</span><span>topk</span><span>(</span><span>similarities</span><span>,</span> <span>k</span><span>=</span><span>k</span><span>)</span>
</span></span><span><span>    <span>indices</span> <span>=</span> <span>indices</span><span>[</span><span>values</span> <span>&gt;</span> <span>threshold</span><span>]</span>
</span></span><span><span>    <span>return</span> <span>[</span><span>chunks</span><span>[</span><span>i</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>indices</span><span>]</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>retrieve</span><span>(</span><span>&#39;dnd&#39;</span><span>))</span>
</span></span><span><span><span>print</span><span>(</span><span>retrieve</span><span>(</span><span>&#39;banana&#39;</span><span>))</span>
</span></span><span><span><span>print</span><span>(</span><span>retrieve</span><span>(</span><span>&#39;merlin the enchanter&#39;</span><span>))</span>
</span></span><span><span><span>print</span><span>(</span><span>retrieve</span><span>(</span><span>&#39;harry potter&#39;</span><span>))</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>[</span><span>&#39;  * _[Dungeons&amp; Dragons](/wiki/Dungeons_%26_Dragons &#34;Dungeons &amp; Dragons&#34;)_</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;the _[Dungeons&amp; Dragons](/wiki/Dungeons_%26_Dragons &#34;Dungeons &amp; Dragons&#34;)_</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span><span><span><span>[]</span>
</span></span><span><span><span>[</span><span>&#39;Pyle_The_Enchanter_Merlin.JPG)_The Enchanter Merlin_ , by [Howard</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;Pyle_The_Enchanter_Merlin.JPG/170px-Arthur-</span><span>\n</span><span>Pyle_The_Enchanter_Merlin.JPG)](/wiki/File:Arthur-</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;&#34;Mentor&#34;), with [Merlin](/wiki/Merlin &#34;Merlin&#34;) from the [_King Arthur_</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span><span><span><span>[</span><span>&#39;series of books by [J. K. Rowling](/wiki/J._K._Rowling &#34;J. K. Rowling&#34;).</span><span>\n\n</span><span>&#39;</span><span>,</span> <span>&#39;the Rings_ or [Lord Voldemort](/wiki/Lord_Voldemort &#34;Lord Voldemort&#34;) from</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;Lord of the Rings](/wiki/The_Lord_of_the_Rings &#34;The Lord of the Rings&#34;)_ and</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span></code></pre></div><p>So the wizard wikipage doesn’t mention banana much, but searching for <code>harry potter</code> surface J.K. Rowling and Voldemort.</p>
<h3 id="approximate-nearest-neighbor-ann">Approximate Nearest Neighbor (ANN)</h3>
<p>Making it scale! Instead of doing exhaustive search <code>O(n)</code>, we can perform an approximate nearest neighbor search using a greedy approach with navigable small world <code>O(log(n))</code>.</p>
<h4 id="building-a-navigable-small-world-nsw">Building a Navigable Small World (NSW)</h4>
<p>You might have heard the theory that you could reach anyone with 6 or fewer connections (e.g. you know your mom, she knows the mayor, he knows someone in parliment, which in turn knows Obama. Therefore you are less than 6 connections away from Obama). The “small-world” theory is a probabilistic approach to retrieval. We greedily explore our neighbor nodes that are closest to the target and reach a good approximation in logarithmic time.</p>
<p>Here’s one way to build the NSW graph.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>build_nsw_graph</span><span>(</span><span>db</span><span>,</span> <span>k_near</span><span>=</span><span>3</span><span>,</span> <span>k_random</span><span>=</span><span>3</span><span>):</span>
</span></span><span><span>    <span>&#39;&#39;&#39;
</span></span></span><span><span><span>    build a graph by piggybacking on the KNN search and random nodes for far away connections.
</span></span></span><span><span><span>    the way it is implemented early nodes will have more neighbors (this is not ideal but it&#39;ll do for a toy example).
</span></span></span><span><span><span>    &#39;&#39;&#39;</span>
</span></span><span><span>    <span>graph</span> <span>=</span> <span>[]</span>
</span></span><span><span>    <span># create node and add approximate nearest neighbors</span>
</span></span><span><span>    <span>for</span> <span>idx</span><span>,</span> <span>embedding</span> <span>in</span> <span>enumerate</span><span>(</span><span>db</span><span>):</span>
</span></span><span><span>        <span>node</span> <span>=</span> <span>Node</span><span>(</span><span>idx</span><span>,</span> <span>embedding</span><span>)</span>
</span></span><span><span>        <span>graph</span><span>.</span><span>append</span><span>(</span><span>node</span><span>)</span>
</span></span><span><span>        <span>if</span> <span>not</span> <span>idx</span><span>:</span> <span>continue</span>
</span></span><span><span>        <span>start_node</span> <span>=</span> <span>graph</span><span>[</span><span>random</span><span>.</span><span>randint</span><span>(</span><span>0</span><span>,</span> <span>idx</span> <span>-</span> <span>1</span><span>)]</span>
</span></span><span><span>        <span>nearests</span> <span>=</span> <span>greedy_k_nearest_neighbors</span><span>(</span><span>graph</span><span>,</span> <span>start_node</span><span>,</span> <span>embedding</span><span>,</span> <span>k</span><span>=</span><span>k_near</span><span>)</span>
</span></span><span><span>        <span>for</span> <span>near</span> <span>in</span> <span>nearests</span><span>:</span>
</span></span><span><span>            <span>node</span><span>.</span><span>neighbors</span><span>.</span><span>add</span><span>(</span><span>near</span><span>.</span><span>idx</span><span>)</span>
</span></span><span><span>            <span>near</span><span>.</span><span>neighbors</span><span>.</span><span>add</span><span>(</span><span>idx</span><span>)</span>
</span></span><span><span>    <span># add random connections</span>
</span></span><span><span>    <span>for</span> <span>idx</span> <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span><span>db</span><span>)):</span>
</span></span><span><span>        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>k_random</span><span>):</span>
</span></span><span><span>            <span>neighbor</span> <span>=</span> <span>random</span><span>.</span><span>randint</span><span>(</span><span>0</span><span>,</span> <span>len</span><span>(</span><span>db</span><span>)</span> <span>-</span> <span>1</span><span>)</span>
</span></span><span><span>            <span>graph</span><span>[</span><span>idx</span><span>]</span><span>.</span><span>neighbors</span><span>.</span><span>add</span><span>(</span><span>neighbor</span><span>)</span>
</span></span><span><span>            <span>graph</span><span>[</span><span>neighbor</span><span>]</span><span>.</span><span>neighbors</span><span>.</span><span>add</span><span>(</span><span>idx</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>graph</span>
</span></span></code></pre></div><p>We insert nodes one at a time. For each node we estimate the closest K nodes and add them as our neighbors as well as K’ random other nodes. It produces a network with high clustering (we have K very closes neighbors) and low average distance (we have K’ long distance jumps).</p>
<figure><a href="https://github.com/caddyserver/caddy/releases/tag/build_nsw.gif" title="build_nsw" data-thumbnail="build_nsw.gif" data-sub-html="&lt;h2&gt;Building the NSW graph&lt;/h2&gt;&lt;p&gt;build_nsw&lt;/p&gt;">
        <img src="https://github.com/svg/loading.min.svg" data-src="build_nsw.gif" data-srcset="build_nsw.gif, build_nsw.gif 1.5x, build_nsw.gif 2x" data-sizes="auto" alt="build_nsw.gif"/>
    </a><figcaption>Building the NSW graph</figcaption>
    </figure>
<h4 id="k-nearest-neighbors-knn-in-a-nsw">K-Nearest Neighbors (KNN) in a NSW</h4>
<p>In order to find an approximation of the K-Nearest Neighbors we traverse the graph starting from a random node (or ideally a set of random nodes). Greedily traverse toward the closest node to our target until we get stuck in a local minimum and return our result.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>greedy_k_nearest_neighbors</span><span>(</span><span>graph</span><span>,</span> <span>node</span><span>,</span> <span>target</span><span>,</span> <span>k</span><span>=</span><span>3</span><span>,</span> <span>compute_dist</span><span>=</span><span>compute_dist</span><span>):</span>
</span></span><span><span>    <span>seen</span> <span>=</span> <span>set</span><span>([</span><span>node</span><span>.</span><span>idx</span><span>])</span>
</span></span><span><span>    <span>dist</span> <span>=</span> <span>compute_dist</span><span>(</span><span>node</span><span>,</span> <span>target</span><span>)</span>
</span></span><span><span>    <span>nearests</span> <span>=</span> <span>[(</span><span>-</span><span>dist</span><span>,</span> <span>node</span><span>)]</span> <span># treat as a maxheap with bounded size `k`</span>
</span></span><span><span>    <span>q</span> <span>=</span> <span>[(</span><span>dist</span><span>,</span> <span>node</span><span>)]</span> <span># minheap</span>
</span></span><span><span>    
</span></span><span><span>    <span>while</span> <span>q</span><span>:</span>
</span></span><span><span>        <span>_</span><span>,</span> <span>node</span> <span>=</span> <span>heapq</span><span>.</span><span>heappop</span><span>(</span><span>q</span><span>)</span>
</span></span><span><span>        <span>for</span> <span>neighbor_idx</span> <span>in</span> <span>node</span><span>.</span><span>neighbors</span><span>:</span>
</span></span><span><span>            <span>if</span> <span>neighbor_idx</span> <span>in</span> <span>seen</span><span>:</span> <span>continue</span>
</span></span><span><span>            <span>neighbor</span> <span>=</span> <span>graph</span><span>[</span><span>neighbor_idx</span><span>]</span>
</span></span><span><span>            <span>dist</span> <span>=</span> <span>compute_dist</span><span>(</span><span>neighbor</span><span>,</span> <span>target</span><span>)</span>
</span></span><span><span>            <span>if</span> <span>len</span><span>(</span><span>nearests</span><span>)</span> <span>&lt;</span> <span>k</span><span>:</span>
</span></span><span><span>                <span>heapq</span><span>.</span><span>heappush</span><span>(</span><span>nearests</span><span>,</span> <span>(</span><span>-</span><span>dist</span><span>,</span> <span>neighbor</span><span>))</span>
</span></span><span><span>            <span>elif</span> <span>dist</span> <span>&lt;</span> <span>-</span><span>nearests</span><span>[</span><span>0</span><span>][</span><span>0</span><span>]:</span>
</span></span><span><span>                <span>heapq</span><span>.</span><span>heapreplace</span><span>(</span><span>nearests</span><span>,</span> <span>(</span><span>-</span><span>dist</span><span>,</span> <span>neighbor</span><span>))</span>
</span></span><span><span>            <span>else</span><span>:</span>
</span></span><span><span>                <span>continue</span>
</span></span><span><span>            <span>seen</span><span>.</span><span>add</span><span>(</span><span>neighbor_idx</span><span>)</span>
</span></span><span><span>            <span>heapq</span><span>.</span><span>heappush</span><span>(</span><span>q</span><span>,</span> <span>(</span><span>dist</span><span>,</span> <span>neighbor</span><span>))</span>
</span></span><span><span>    <span>return</span> <span>[</span><span>n</span><span>[</span><span>1</span><span>]</span> <span>for</span> <span>n</span> <span>in</span> <span>nearests</span><span>]</span>
</span></span></code></pre></div><p>In this animation the <code>X</code> mark the target, nodes with a purple rim are the current best candidate for the KNN and nodes get marked in black once they have been traversed.</p>
<figure><a href="https://github.com/caddyserver/caddy/releases/tag/knn.gif" title="knn" data-thumbnail="knn.gif" data-sub-html="&lt;h2&gt;K-Nearest Neighbor in NSW&lt;/h2&gt;&lt;p&gt;knn&lt;/p&gt;">
        <img src="https://github.com/svg/loading.min.svg" data-src="knn.gif" data-srcset="knn.gif, knn.gif 1.5x, knn.gif 2x" data-sizes="auto" alt="knn.gif"/>
    </a><figcaption>K-Nearest Neighbor in NSW</figcaption>
    </figure>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>nsw_retrieve</span><span>(</span><span>query</span><span>,</span> <span>k</span><span>=</span><span>3</span><span>,</span> <span>threshold</span><span>=</span><span>0.5</span><span>):</span>
</span></span><span><span>    <span>query_embedding</span> <span>=</span> <span>get_embedding</span><span>(</span><span>query</span><span>)</span>
</span></span><span><span>    <span>start_node</span> <span>=</span> <span>graph</span><span>[</span><span>random</span><span>.</span><span>randint</span><span>(</span><span>0</span><span>,</span> <span>len</span><span>(</span><span>graph</span><span>)</span> <span>-</span> <span>1</span><span>)]</span>
</span></span><span><span>    <span>nearests</span> <span>=</span> <span>greedy_k_nearest_neighbors</span><span>(</span><span>graph</span><span>,</span> <span>start_node</span><span>,</span> <span>query_embedding</span><span>,</span> <span>k</span><span>=</span><span>k</span><span>)</span>
</span></span><span><span>    <span>res</span> <span>=</span> <span>[]</span>
</span></span><span><span>    <span>for</span> <span>near</span> <span>in</span> <span>nearests</span><span>:</span>
</span></span><span><span>        <span>if</span> <span>compute_dist</span><span>(</span><span>near</span><span>,</span> <span>query_embedding</span><span>)</span> <span>&gt;</span> <span>threshold</span><span>:</span> <span>continue</span>
</span></span><span><span>        <span>res</span><span>.</span><span>append</span><span>(</span><span>chunks</span><span>[</span><span>near</span><span>.</span><span>idx</span><span>])</span>
</span></span><span><span>    <span>return</span> <span>res</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>nsw_retrieve</span><span>(</span><span>&#39;dnd&#39;</span><span>))</span>
</span></span><span><span><span>print</span><span>(</span><span>nsw_retrieve</span><span>(</span><span>&#39;banana&#39;</span><span>))</span>
</span></span><span><span><span>print</span><span>(</span><span>nsw_retrieve</span><span>(</span><span>&#39;merlin the enchanter&#39;</span><span>))</span>
</span></span><span><span><span>print</span><span>(</span><span>nsw_retrieve</span><span>(</span><span>&#39;harry potter&#39;</span><span>))</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>[</span><span>&#39;  * _[Dungeons&amp; Dragons](/wiki/Dungeons_%26_Dragons &#34;Dungeons &amp; Dragons&#34;)_</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;the _[Dungeons&amp; Dragons](/wiki/Dungeons_%26_Dragons &#34;Dungeons &amp; Dragons&#34;)_</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span><span><span><span>[]</span>
</span></span><span><span><span>[</span><span>&#39;&#34;Mentor&#34;), with [Merlin](/wiki/Merlin &#34;Merlin&#34;) from the [_King Arthur_</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;Pyle_The_Enchanter_Merlin.JPG)_The Enchanter Merlin_ , by [Howard</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;Pyle_The_Enchanter_Merlin.JPG/170px-Arthur-</span><span>\n</span><span>Pyle_The_Enchanter_Merlin.JPG)](/wiki/File:Arthur-</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span><span><span><span>[</span><span>&#39;Rowling](/wiki/J._K._Rowling &#34;J. K. Rowling&#34;)</span><span>\&#39;</span><span>s _Harry Potter_ novels or</span><span>\n</span><span>&#39;</span><span>,</span> <span>&#39;series of books by [J. K. Rowling](/wiki/J._K._Rowling &#34;J. K. Rowling&#34;).</span><span>\n\n</span><span>&#39;</span><span>,</span> <span>&#39;the Rings_ or [Lord Voldemort](/wiki/Lord_Voldemort &#34;Lord Voldemort&#34;) from</span><span>\n</span><span>&#39;</span><span>]</span>
</span></span></code></pre></div><p>We retain the ability to retrieve most of the same documents as earlier but it is now possible to scale better.</p>
<p>This algorithm can be improved further by using Hierarchical Navigable Small Worlds (HNSW). Which is similar to using a skip-list but applied to a graph. Instead of having one NSW we can traverse. We have multiple layers of NSW starting from very sparse and getting increasingly denser. Each time we reach a local minimum we fall down to the denser layer below. This permits to travel further on initial jumps, reducing the average query time.</p>
<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/RAG-and-tatters/blob/master/RAG-and-tatters.ipynb" target="_blank" rel="noopener noreffer ">https://github.com/peluche/RAG-and-tatters/blob/master/RAG-and-tatters.ipynb</a></p>
</div></div>
  </body>
</html>
