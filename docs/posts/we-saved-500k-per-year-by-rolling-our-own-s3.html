<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://engineering.nanit.com/how-we-saved-500-000-per-year-by-rolling-our-own-s3-6caec1ee1143">Original</a>
    <h1>We saved $500k per year by rolling our own &#34;S3&#34;</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><div><div><div><div><div><div><div><div><div><div><div role="tooltip"><div tabindex="-1"><a href="https://medium.com/@miedwar" rel="noopener follow"><div><div><p><img alt="Miedwar Meshbesher" src="https://miro.medium.com/v2/resize:fill:64:64/0*qdWi2jKtBNgnIVC6." width="32" height="32" loading="lazy" data-testid="authorPhoto"/></p></div></div></a></div></div></div></div></div><p><span><div><p><span data-testid="storyReadTime">12 min read</span></p><p><span aria-hidden="true"><span>·</span></span></p><p><span data-testid="storyPublishDate">Oct 19, 2025</span></p></div></span></p></div></div></div></div></div><h2 id="a1d9"><strong>tl;dr</strong></h2><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*id30a4pMME5E4DrmujuTew.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*id30a4pMME5E4DrmujuTew.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*id30a4pMME5E4DrmujuTew.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*id30a4pMME5E4DrmujuTew.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*id30a4pMME5E4DrmujuTew.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*id30a4pMME5E4DrmujuTew.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*id30a4pMME5E4DrmujuTew.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*id30a4pMME5E4DrmujuTew.png 640w, https://miro.medium.com/v2/resize:fit:720/1*id30a4pMME5E4DrmujuTew.png 720w, https://miro.medium.com/v2/resize:fit:750/1*id30a4pMME5E4DrmujuTew.png 750w, https://miro.medium.com/v2/resize:fit:786/1*id30a4pMME5E4DrmujuTew.png 786w, https://miro.medium.com/v2/resize:fit:828/1*id30a4pMME5E4DrmujuTew.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*id30a4pMME5E4DrmujuTew.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*id30a4pMME5E4DrmujuTew.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="312" loading="eager" role="presentation"/></picture></div></div><figcaption>Camera → N3 → S3 Fallback</figcaption></figure><p id="e5be">We used <strong>S3</strong> as a landing zone for Nanit’s video processing pipeline (baby sleep-state inference), but at thousands of uploads/second, <strong>S3</strong>’s <strong>PutObject</strong> request fees dominated costs. Worse, <strong>S3</strong>’s auto-cleanup (Lifecycle rules) has a 1-day minimum; we paid for 24 hours of storage on objects processed in ~2 seconds. We built <strong>N3</strong>, a <strong>Rust-based in-memory</strong> landing zone that eliminates both issues, using <strong>S3 </strong>only as an overflow buffer.</p><p id="6592"><strong>Result: meaningful cost reduction (~$0.5M/year).</strong></p><h2 id="f127">Part 1: Background</h2><h2 id="822d">High-Level Overview of Our Video Processing Pipeline</h2><ul><li id="5785">Cameras record video <strong>chunks</strong> (configurable duration).</li><li id="a7fe">For each chunk, the camera requests an <strong>S3 presigned URL</strong> from the <strong>Camera Service</strong> and uploads <strong>directly to S3</strong>.</li><li id="42d2">An AWS <strong>Lambda</strong> posts the object key to an <strong>SQS FIFO</strong> queue (sharded by baby_uid).</li><li id="6a3a">Video processing pods consume from <strong>SQS</strong>, download from <strong>S3</strong>, and produce sleep states.</li></ul><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*EOaIvWuNlaR-RISBXC3asQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*EOaIvWuNlaR-RISBXC3asQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*EOaIvWuNlaR-RISBXC3asQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*EOaIvWuNlaR-RISBXC3asQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*EOaIvWuNlaR-RISBXC3asQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*EOaIvWuNlaR-RISBXC3asQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOaIvWuNlaR-RISBXC3asQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*EOaIvWuNlaR-RISBXC3asQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*EOaIvWuNlaR-RISBXC3asQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*EOaIvWuNlaR-RISBXC3asQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*EOaIvWuNlaR-RISBXC3asQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*EOaIvWuNlaR-RISBXC3asQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*EOaIvWuNlaR-RISBXC3asQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*EOaIvWuNlaR-RISBXC3asQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="378" loading="lazy" role="presentation"/></picture></div></div><figcaption>High Level Overview</figcaption></figure><p id="40d9">For a deeper dive, see this<a rel="noopener ugc nofollow" href="https://engineering.nanit.com/nanits-high-scale-video-analysis-pipeline-dea0e3fae895" target="_blank" data-discover="true"> post</a>.</p><h2 id="ab05">What We Like About This Setup</h2><ul><li id="bce0">Landing on <strong>S3</strong> + queuing to <strong>SQS </strong>decouples camera uploads from video processing. During maintenance or temporary downtime, we don’t lose videos; if queues grow, we scale processing.</li><li id="818a">With <strong>S3</strong>, we don’t manage availability or durability.</li><li id="702c"><strong>SQS FIFO</strong> + group IDs preserve per-baby ordering, keeping processing nodes mostly stateless (coordination happens in SQS).</li><li id="6a83"><strong>S3 Lifecycle</strong> rules offload GC: objects expire after one day, so we don’t track processed videos.</li></ul><h2 id="1494">Why We Changed</h2><p id="5b6f"><strong>PutObject costs dominated.</strong> Our objects are <strong>short-lived: </strong>videos land for <strong>seconds</strong>, then get processed. At our scale (thousands of uploads/s), the <strong>per-object request charge</strong> was the largest cost driver. Increasing <strong>chunking frequency</strong> (i.e., sending <strong>more, smaller chunks</strong>) to cut latency <strong>raises costs linearly</strong>, because each additional chunk is another PutObject request.</p><p id="6ab6"><strong>Storage was a secondary tax.</strong> Even when processing finished in ~2 s, <strong>Lifecycle</strong> deletes meant paying for <strong>~24 h</strong> of storage.</p><p id="7283">We needed a design that <strong>kept reliability and strict ordering</strong> while <strong>avoiding per-object costs on the happy path</strong> and minimizing “pay-to-wait” storage.</p><h2 id="0525">Part 2: Planning</h2><h2 id="3b11">Guiding Principles</h2><ol><li id="802d"><strong>Simplicity through architecture</strong>: Eliminate complexity at the design level, not through clever implementations.</li><li id="c079"><strong>Correctness</strong>: A true drop-in replacement that’s transparent to the rest of the pipeline.</li><li id="ec58"><strong>Optimize for the happy path</strong>: Design for the normal case and use S3 as a safety net for edge cases. Our processing algorithms are robust to occasional gaps, so we can prioritize simplicity over building complex guarantees; S3 provides reliability when needed.</li></ol><h2 id="6b10">Design Drivers</h2><ul><li id="aa42"><strong>Short-lived objects:</strong> segments live on the landing zone for seconds, not hours.</li><li id="bdc5"><strong>Ordering:</strong> strict per-baby sequencing (no processing newer before older).</li><li id="804b"><strong>Throughput:</strong> thousands of uploads/second; 2–6 MB per segment.</li><li id="67e9"><strong>Client limits:</strong> cameras have <strong>limited retries</strong>; don’t assume retransmits.</li><li id="53e1"><strong>Operations:</strong> tolerate <strong>multi-million-item backlogs</strong> during maintenance/scale-ups.</li><li id="4a69"><strong>No firmware changes:</strong> must work with existing cameras.</li><li id="ab91"><strong>Loss tolerance:</strong> very small gaps are acceptable; algorithms mask them.</li><li id="ea76"><strong>Cost:</strong> avoid per-object S3 costs on the happy path; minimize “pay-to-wait” storage.</li></ul><h2 id="40b7">Design at a Glance (N3 Happy Path + S3 Overflow)</h2><h3 id="8361">The Architecture</h3><p id="5e92">N3 is a custom landing zone that holds videos in memory just long enough for processing to drain them (~2 seconds). S3 is used only when N3 can’t handle the load.</p><p id="7453"><strong>Two components:</strong></p><ul><li id="afb3"><strong>N3-Proxy</strong> (<em>stateless, dual interfaces</em>): </li><li id="ed74"><strong>N3-Storage</strong> (<em>stateful, internal-only</em>): Stores uploaded segments in RAM and enqueues <strong>SQS</strong> with a pod-addressable <strong>download URL.</strong></li></ul><p id="bf99">Video processing pods consume from SQS FIFO and download from whichever storage the URL points to: N3 or S3.</p><p id="4449"><strong>Normal Flow (Happy Path)</strong></p><ol><li id="8114">Camera requests an upload URL from Camera Service.</li><li id="cf94">Camera Service calls N3-Proxy’s internal API for a presigned URL.</li><li id="1dda">Camera uploads video to N3-Proxy’s external endpoint.</li><li id="631a">N3-Proxy forwards to N3-Storage.</li><li id="aaa4">N3-Storage holds video in memory and enqueues to SQS with a download URL pointing to itself.</li><li id="9410">Processing pod downloads from N3-Storage and processes.</li></ol><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*vkMdlWZ_bvNnbU9k7oKbJA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*vkMdlWZ_bvNnbU9k7oKbJA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*vkMdlWZ_bvNnbU9k7oKbJA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*vkMdlWZ_bvNnbU9k7oKbJA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*vkMdlWZ_bvNnbU9k7oKbJA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vkMdlWZ_bvNnbU9k7oKbJA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkMdlWZ_bvNnbU9k7oKbJA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*vkMdlWZ_bvNnbU9k7oKbJA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*vkMdlWZ_bvNnbU9k7oKbJA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*vkMdlWZ_bvNnbU9k7oKbJA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*vkMdlWZ_bvNnbU9k7oKbJA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*vkMdlWZ_bvNnbU9k7oKbJA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*vkMdlWZ_bvNnbU9k7oKbJA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*vkMdlWZ_bvNnbU9k7oKbJA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="417" loading="lazy" role="presentation"/></picture></div></div><figcaption>N3 Happy Path</figcaption></figure><p id="2601"><strong>Two-Tier Fallback</strong></p><ul><li id="7ce3">Tier 1: Proxy-level fallback (per-request): </li></ul><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*uIy7lKdR-gkwAzrzwf7J-Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*uIy7lKdR-gkwAzrzwf7J-Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*uIy7lKdR-gkwAzrzwf7J-Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*uIy7lKdR-gkwAzrzwf7J-Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*uIy7lKdR-gkwAzrzwf7J-Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*uIy7lKdR-gkwAzrzwf7J-Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uIy7lKdR-gkwAzrzwf7J-Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*uIy7lKdR-gkwAzrzwf7J-Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*uIy7lKdR-gkwAzrzwf7J-Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*uIy7lKdR-gkwAzrzwf7J-Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*uIy7lKdR-gkwAzrzwf7J-Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*uIy7lKdR-gkwAzrzwf7J-Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*uIy7lKdR-gkwAzrzwf7J-Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*uIy7lKdR-gkwAzrzwf7J-Q.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="431" loading="lazy" role="presentation"/></picture></div></div><figcaption>Proxy Level Fallback</figcaption></figure><ul><li id="12d0">Tier 2: Cluster-level reroute (all traffic): </li></ul><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9PmDtETxMGfRk1gW1KmeZA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9PmDtETxMGfRk1gW1KmeZA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9PmDtETxMGfRk1gW1KmeZA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9PmDtETxMGfRk1gW1KmeZA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9PmDtETxMGfRk1gW1KmeZA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9PmDtETxMGfRk1gW1KmeZA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PmDtETxMGfRk1gW1KmeZA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*9PmDtETxMGfRk1gW1KmeZA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*9PmDtETxMGfRk1gW1KmeZA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*9PmDtETxMGfRk1gW1KmeZA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*9PmDtETxMGfRk1gW1KmeZA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*9PmDtETxMGfRk1gW1KmeZA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*9PmDtETxMGfRk1gW1KmeZA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*9PmDtETxMGfRk1gW1KmeZA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="414" loading="lazy" role="presentation"/></picture></div></div></figure><p id="a677"><strong>Why Two Components?</strong></p><p id="d197">We split N3-Proxy and N3-Storage because they have different requirements:</p><ul><li id="7e4f"><strong>Blast radius:</strong> If storage crashes, proxy can still route to S3. If proxy crashes, only that node’s traffic is affected; not the entire storage cluster.</li><li id="58cc"><strong>Resource profiles: </strong>Proxy is CPU/network-heavy (TLS termination). Storage is memory-heavy (holding videos). Different instance types and scaling requirments.</li><li id="5052"><strong>Security:</strong> Storage never touches the Internet.</li><li id="7c2f"><strong>Rollout safety:</strong> We can update proxy (stateless) without touching storage (holding active data).</li></ul><h2 id="1703">Validating the Design</h2><p id="b722">The architecture made sense on paper, but we had critical unknowns:</p><ul><li id="532d"><strong>Capacity &amp; sizing:</strong> real <strong>upload durations</strong> across client networks; how much compute and upload <strong>buffer size</strong> we need?</li><li id="5093"><strong>Storage model:</strong> can we keep everything in <strong>RAM</strong>, or do we need <strong>disks</strong>?</li><li id="6923"><strong>Resilience:</strong> how to <strong>load balance</strong> cheaply and handle <strong>failed nodes</strong>?</li><li id="110c"><strong>Operational policy:</strong> <strong>GC</strong> needs, <strong>retry</strong> expectations, and whether <strong>delete-on-GET</strong> is sufficient.</li><li id="096b"><strong>Unknown unknowns</strong>: what edge cases would emerge when idea meet reality?</li></ul><p id="286b">To de-risk decisions, we ran two tracks <strong>during planning</strong>:</p><h2 id="2874">Approach 1: Synthetic Stress Tests</h2><p id="0c73">We built a load generator to push the system to its limits: varying concurrency, slow clients, sustained load, and processing downtime.</p><p id="1bfb"><strong>Goal:</strong> Find breaking points. Surface bottlenecks we hadn’t anticipated. Get deterministic baselines for capacity planning.</p><h2 id="475f">Approach 2: Production PoC (Mirror Mode)</h2><p id="fc84">Synthetic tests can’t replicate real camera behavior: flaky Wi-Fi, diverse firmware versions, unpredictable network conditions. We needed <strong>in-the-wild </strong>data without risking production.</p><ul><li id="299f"><strong>Mirror mode:</strong> n3-proxy wrote to <strong>S3 first</strong> (preserving prod), then also to a PoC <strong>N3-Storage</strong> wired to a <strong>canary SQS + video processors</strong>.</li><li id="9f7b"><strong>Targeted cohorts:</strong> by firmware version / Baby-UID lists</li><li id="6228"><strong>Data parity:</strong> compared <strong>sleep states</strong> PoC vs. production; investigated any diffs.</li><li id="cc7e"><strong>Observability:</strong> per-path dashboards (N3 vs. S3), queue depth, latency/RPS, error budgets, egress breakdown.</li></ul><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*FiLhsohXhoK_7L68ZaDPew.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*FiLhsohXhoK_7L68ZaDPew.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*FiLhsohXhoK_7L68ZaDPew.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*FiLhsohXhoK_7L68ZaDPew.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*FiLhsohXhoK_7L68ZaDPew.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*FiLhsohXhoK_7L68ZaDPew.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FiLhsohXhoK_7L68ZaDPew.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*FiLhsohXhoK_7L68ZaDPew.png 640w, https://miro.medium.com/v2/resize:fit:720/1*FiLhsohXhoK_7L68ZaDPew.png 720w, https://miro.medium.com/v2/resize:fit:750/1*FiLhsohXhoK_7L68ZaDPew.png 750w, https://miro.medium.com/v2/resize:fit:786/1*FiLhsohXhoK_7L68ZaDPew.png 786w, https://miro.medium.com/v2/resize:fit:828/1*FiLhsohXhoK_7L68ZaDPew.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*FiLhsohXhoK_7L68ZaDPew.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*FiLhsohXhoK_7L68ZaDPew.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="446" loading="lazy" role="presentation"/></picture></div></div><figcaption>PoC setup</figcaption></figure><p id="8470">Feature flags (via <a href="https://www.getunleash.io/" rel="noopener ugc nofollow" target="_blank">Unleash</a>) were critical. We could flip cohorts on/off in real-time; no deployments; letting us test narrow slices (older firmware, weak Wi-Fi cameras) and revert instantly if issues appeared.</p><h2 id="d040">What We Discovered</h2><ol><li id="a505"><strong>Bottlenecks</strong>: TLS termination consumed most CPU, and AWS burstable networking throttled us after credits expired.</li><li id="9006"><strong>Memory-only storage was viable.</strong> Real upload-time distributions and concurrency showed we could <strong>fit the working set in RAM</strong> with safe headroom; disks not required.</li><li id="f221"><strong>Delete-on-GET is safe.</strong> We <strong>did not</strong> observe re-downloads; retries happen downstream in the processor, so N3 doesn’t need to support download retries.</li><li id="f903"><strong>We need lightweight GC.</strong> Some segments get <strong>skipped</strong> by processing and would never be downloaded/deleted; added a <strong>TTL GC</strong> pass to clean stragglers.</li></ol><p id="59b6">These findings shaped our implementation: memory-backed storage, network- optimized instances with TLS optimization, and delete-on-GET with TTL GC for stragglers.</p><h2 id="042d">Part 3: Implementation Details</h2><h2 id="f526">DNS Load Balancing</h2><p id="69f4"><code>n3-proxy</code> is a <strong>DaemonSet</strong> on dedicated nodes, one pod per node to maximize network and CPU resources for TLS termination. We need node-level load balancing and graceful restarts.</p><p id="4c47">An AWS Network Load Balancer would work, but at our throughput (thousands of uploads/second, sustained multi-GB/s), the combination of fixed costs plus per-GB processed fees becomes expensive. Instead, we use DNS-based load balancing via Route53 multi-value A records, which is significantly cheaper.</p><ul><li id="95a5">For each node we create a <strong>MultiValue</strong> record that contains a single IP.</li><li id="19bb">Each record has a health check that hits an external readiness endpoint.</li><li id="5952">A records use a short <strong>30-second TTL</strong>.</li></ul><p id="1c2d">This gives us:</p><ul><li id="b5e3">If a node fails, it’s taken out of the pool and cameras stop uploading to it.</li><li id="c5ae">Because the external readiness endpoint is also used as the Kubernetes readiness probe, marking a pod Not Ready during rollouts automatically removes it from DNS.</li></ul><h3 id="da9e">Rollout process</h3><p id="c6c1"><code>n3-proxy</code> pods have a graceful shutdown mechanism:</p><ol><li id="9454">On <strong>SIGTERM</strong>, the pod enters <strong>paused</strong> mode.</li><li id="bf3b">Readiness becomes <strong>Not Ready</strong>, but uploads are still accepted.</li><li id="62e4">Wait <strong>2× DNS TTL</strong> (e.g., 60s) so the DNS health check removes the node and camera DNS caches update.</li><li id="6542">Drain active connections, then restart.</li><li id="4d0b">On startup, wait for health checks to pass and for client DNS TTLs to expire before rolling to the next pod (lets the node rejoin the pool).</li></ol><h2 id="de1c">Networking Limitations</h2><p id="2fbc">When doing initial benchmarks to size the cluster, we saw a surprising pattern: runs started near <strong>~1k RPS</strong>, then dropped to <strong>~70 RPS</strong> after ~1 minute. Restarting didn’t help; after waiting and rerunning, we briefly saw ~1k RPS again.</p><p id="d6e0">It turns out that when AWS says an instance can do <strong>“Up to 12.5 Gbps”</strong>, that’s <strong>burstable</strong> networking backed by credits; when you’re below the baseline, you accrue credits and can burst for short periods.</p><p id="bf8b">Baseline depends on instance family and vCPUs:</p><ul><li id="694a"><strong>Non–network-optimized:</strong> ~<strong>0.375 Gbps/vCPU</strong></li><li id="aa02"><strong>Network-optimized (suffix “n”):</strong> ~<strong>3.125 Gbps/vCPU</strong></li></ul><p id="f339">And for instances that don’t say “Up to,” you get the <strong>stated Gbps</strong> continuously.</p><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*SuqJn5mJw2ZUYVCvJJiCbg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*SuqJn5mJw2ZUYVCvJJiCbg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*SuqJn5mJw2ZUYVCvJJiCbg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*SuqJn5mJw2ZUYVCvJJiCbg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*SuqJn5mJw2ZUYVCvJJiCbg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*SuqJn5mJw2ZUYVCvJJiCbg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SuqJn5mJw2ZUYVCvJJiCbg.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*SuqJn5mJw2ZUYVCvJJiCbg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*SuqJn5mJw2ZUYVCvJJiCbg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*SuqJn5mJw2ZUYVCvJJiCbg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*SuqJn5mJw2ZUYVCvJJiCbg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*SuqJn5mJw2ZUYVCvJJiCbg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*SuqJn5mJw2ZUYVCvJJiCbg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*SuqJn5mJw2ZUYVCvJJiCbg.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="213" loading="lazy" role="presentation"/></picture></div></div><figcaption>AWS instance and Network Performance</figcaption></figure><p id="3a09"><strong>Conclusion:</strong> our workload is steady, so bursts don’t help. We moved to network optimized <strong>c8gn.4xlarge</strong> nodes, which provide <strong>50 Gbps</strong> each, giving us the sustained throughput we need.</p><h2 id="e55f">HTTPS, rustls, and Graviton4</h2><p id="7433">Initially, for simplicity, we used a <code>stunnel</code> sidecar for HTTPS termination, but early stress testing showed HTTPS was the main CPU consumer and primary bottleneck. We made three changes:</p><ol><li id="fa5d">Moved from <code>stunnel</code> to native <strong>rustls</strong>.</li><li id="9599">Upgraded from <strong>Graviton3</strong> to <strong>Graviton4</strong> instances.</li><li id="001a">Compiled <code>n3-proxy</code> with <strong>target-cpu</strong> and crypto features enabled.</li></ol><pre><span id="de6d">[profile.release]</span></pre><p id="8f9a">These changes yielded ~<strong>30%</strong> higher RPS at the same cost.</p><h2 id="96c7">Outgoing Traffic Costs</h2><p id="d798">We assumed that since we only receive uploads (ingress is free) and don’t send payloads to clients, egress would be negligible. Post-launch, we saw non-trivial outbound traffic.</p><h3 id="1193">TLS handshakes</h3><p id="4cd0">Each upload opens a new TLS connection, so a full handshake runs and sends ~<strong>7 KB</strong> of certificates. In theory we could reduce this with smaller (e.g., ECDSA) certs, session resumption/tickets, or long-lived connections; but given our constraint of not changing camera behavior, we accept this overhead for now.</p><h3 id="2043">ACKs</h3><p id="c7f7">Surprisingly, TLS handshakes were only a small part of the outbound bytes. A <code>tcpdump</code> showed many <code>66-byte</code> ACKs:</p><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_DmRRO5aimORRW-Tx4YnnQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_DmRRO5aimORRW-Tx4YnnQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_DmRRO5aimORRW-Tx4YnnQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_DmRRO5aimORRW-Tx4YnnQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_DmRRO5aimORRW-Tx4YnnQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_DmRRO5aimORRW-Tx4YnnQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DmRRO5aimORRW-Tx4YnnQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*_DmRRO5aimORRW-Tx4YnnQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_DmRRO5aimORRW-Tx4YnnQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_DmRRO5aimORRW-Tx4YnnQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_DmRRO5aimORRW-Tx4YnnQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_DmRRO5aimORRW-Tx4YnnQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_DmRRO5aimORRW-Tx4YnnQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_DmRRO5aimORRW-Tx4YnnQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="280" loading="lazy" role="presentation"/></picture></div></div><figcaption>Wireshark — ACKs</figcaption></figure><pre><span id="a671">tshark -r n3-3.pcap \</span></pre><p id="13e2">This was a short traffic capture:</p><ul><li id="1582"><strong>total_bytes</strong> = 37,014,432</li><li id="cfac"><strong>ack_frame_bytes</strong> = 31,258,550 (<strong>84.4%</strong>)</li><li id="cc84"><strong>data_frame_bytes</strong> = 5,755,882 (<strong>15.6%</strong>)</li></ul><p id="369c"><strong>~85% </strong>of outbound bytes were ACK frames.</p><p id="5a6f">With ~1500-byte MTUs and frequent ACKs, overhead adds up. While we can’t easily reduce the number of ACKs, we can make each ACK smaller by removing TCP timestamps (−12 bytes/ACK):</p><pre><span id="acf2">sysctl -w net.ipv4.tcp_timestamps=0</span></pre><p id="ef04">Kubernetes init-container:</p><pre><span id="8084">spec:</span></pre><p id="5baf"><strong>This isn’t without risk:</strong> with high byte counts on the <strong>same socket</strong>, sequence numbers can wrap and delayed packets may be mis-merged, causing corruption.</p><h2 id="a197">Memory Leak</h2><p id="7f5b">After the initial launch, we saw steady n3-proxy memory growth. Even after traffic stopped, the process returned to an ever-higher baseline — so it wasn’t just the OS holding freed pages.</p><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*AJV1pBkeF4waAsYN_urlrw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*AJV1pBkeF4waAsYN_urlrw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*AJV1pBkeF4waAsYN_urlrw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*AJV1pBkeF4waAsYN_urlrw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*AJV1pBkeF4waAsYN_urlrw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AJV1pBkeF4waAsYN_urlrw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJV1pBkeF4waAsYN_urlrw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*AJV1pBkeF4waAsYN_urlrw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*AJV1pBkeF4waAsYN_urlrw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*AJV1pBkeF4waAsYN_urlrw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*AJV1pBkeF4waAsYN_urlrw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*AJV1pBkeF4waAsYN_urlrw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*AJV1pBkeF4waAsYN_urlrw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*AJV1pBkeF4waAsYN_urlrw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="182" loading="lazy" role="presentation"/></picture></div></div><figcaption>Memory Usage Increasing</figcaption></figure><p id="443d"><code>jemalloc</code> stats showed <strong>referenced memory</strong> constantly increasing.</p><figure><div role="button" tabindex="0"><p><span>Press enter or click to view image in full size</span></p><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*GAIHihcYYAH1Gxvk4DhQVw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*GAIHihcYYAH1Gxvk4DhQVw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*GAIHihcYYAH1Gxvk4DhQVw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*GAIHihcYYAH1Gxvk4DhQVw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*GAIHihcYYAH1Gxvk4DhQVw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*GAIHihcYYAH1Gxvk4DhQVw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GAIHihcYYAH1Gxvk4DhQVw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*GAIHihcYYAH1Gxvk4DhQVw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*GAIHihcYYAH1Gxvk4DhQVw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*GAIHihcYYAH1Gxvk4DhQVw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*GAIHihcYYAH1Gxvk4DhQVw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*GAIHihcYYAH1Gxvk4DhQVw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*GAIHihcYYAH1Gxvk4DhQVw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*GAIHihcYYAH1Gxvk4DhQVw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="220" loading="lazy" role="presentation"/></picture></div></div><figcaption>jemalloc — Allocated memory</figcaption></figure><p id="3dc7">Using <a href="https://github.com/polarsignals/rust-jemalloc-pprof" rel="noopener ugc nofollow" target="_blank">rust-jemalloc-pprof</a> we profiled memory in production and identified growth in per-connection <code>hyper</code> <code><strong>BytesMut</strong></code> buffers.</p><p id="5902">Since we handle large uploads over variable networks, some client connections <strong>stalled mid-transfer</strong> and never cleaned up. The per-connection <code>hyper</code> buffers (<code>BytesMut</code>) stuck around and memory kept climbing. When we <strong>Terminated connections idle &gt;10 minutes</strong>, memory dropped by <strong>~1 GB</strong> immediately; confirming the leak was from <strong>dangling sockets</strong>.</p><p id="629f"><strong>Fix:</strong> make sockets short-lived and enforce time limits.</p><ul><li id="1eff"><strong>Disable keep-alive:</strong> close the connection immediately after each upload completes.</li><li id="1219"><strong>Tighten timeouts:</strong> set header/socket timeouts so stalled uploads are terminated and buffers are freed.</li></ul><pre><span id="7b27">fn make_listener(addr: &amp;str) -&gt; std::io::Result&lt;std::net::TcpListener&gt; {</span></pre><h2 id="c1c4">Storage</h2><p id="a1bd">We started with the simplest path: <strong>in-memory storage</strong>. It avoids I/O tuning and lets us use straightforward data structures.</p><pre><span id="269a">type Store = Arc&lt;DashMap&lt;Ulid, Bytes&gt;&gt;;</span></pre><p id="213f">Each video upload increments <code>bytes_used</code> ; each download deletes the video and decrements it.</p><p id="8c66">Above ~80% capacity, we start rejecting uploads to avoid OOM and signal n3-proxy to stop signing upload URLs.</p><p id="da0e">The <code>control</code> handle lets us manually pause uploads and garbage collection.</p><h2 id="9529">Graceful Restart</h2><p id="3b60">With memory-only storage, restarts must not drop in-flight data. Our <strong>graceful restart</strong> process:</p><ol><li id="cb0c"><code>SIGTERM</code> to a pod (StatefulSet rolls one pod at a time).</li><li id="03f9">Pod becomes <strong>Not Ready</strong> and leaves the Service (no new uploads).</li><li id="244a">It continues serving downloads for already-uploaded videos.</li><li id="d1f9">Once downloads quiesce (no recent reads → processing drained),</li><li id="6727">Wait for any open requests to complete</li><li id="b768">Restart and move to the next pod.</li></ol><p id="f385">Under normal operation pods drain in seconds.</p><h2 id="5d81">GC</h2><p id="cefa">We use two cleanup mechanisms:</p><h3 id="1fa4">Delete on download</h3><p id="f1da">We delete videos immediately after download. In the PoC, we saw zero re-downloads; video processors retry internally. This eliminates the need to hold data or track “processed” state.</p><h3 id="bdf7">TTL GC for stragglers</h3><p id="b06a">Deleting on download doesn’t cover segments <strong>skipped</strong> by the processor (never downloaded → never deleted). We added a lightweight <strong>TTL GC</strong>: periodically scan the in-memory <strong>DashMap</strong> and remove entries older than a configurable threshold (e.g., a few hours).</p><h3 id="1279">Maintenance mode</h3><p id="c8d0">During planned processing downtime, we can <strong>temporarily pause GC</strong> via an internal control so videos aren’t deleted while consumption is stopped.</p><h2 id="d7a3">Part 4: Conclusion</h2><p id="434c">By using S3 as a fallback buffer and N3 as the primary landing zone, we eliminated ~$0.5M/year in costs while keeping the system simple and reliable.</p><p id="eca2"><strong>The key insight</strong>: most “<em>build vs. buy</em>” decisions focus on features, but at scale, economics shift the calculus. For short-lived objects (~2 seconds in normal operation), we don’t need replication or sophisticated durability; simple in-memory storage works. But when processing lags or maintenance extends object lifetime, we need <strong>S3</strong>’s reliability guarantees. We get the best of both worlds: N3 handles the happy path efficiently, while S3 provides durability when objects need to live longer. If N3 has any issues; memory pressure, pod crashes, or cluster problems; uploads seamlessly fail over to S3.</p><p id="adbc"><strong>What Made This Work</strong></p><p id="1209">Defining the problem clearly upfront: <a href="https://theengineeringmanager.substack.com/p/the-beauty-of-constraints" rel="noopener ugc nofollow" target="_blank">constraints</a>, assumptions, and boundaries prevented scope creep. Validating early with a mirror-mode PoC let us discover bottlenecks (TLS, network throttling) and validate assumptions before committing. This prevented overengineering and backtracking.</p><p id="43aa"><strong>When Should You Build Something Like This?</strong></p><p id="0326">Consider custom infrastructure when you have both: sufficient scale for meaningful cost savings, and specific constraints that enable a simple solution. The engineering effort to build and maintain your system must be less than the infrastructure costs it eliminates. In our case, specific requirements (ephemeral storage, loss tolerance, S3 fallback) let us build something simple enough that maintenance costs stay low. Without both factors, stick with managed services.</p><p id="6880"><strong>Would we do it again? </strong>Yes. The system has been running reliably in production, and the fallback design lets us avoid complexity without sacrificing reliability.</p></div></div></div></div></section></div>
  </body>
</html>
