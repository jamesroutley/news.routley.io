<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/princeton-nlp/MeZO">Original</a>
    <h1>MeZO: Fine-Tuning Language Models with Just Forward Passes</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This is the implementation for the paper <a href="https://arxiv.org/pdf/2305.17333.pdf" rel="nofollow">Fine-Tuning Language Models with Just Forward Passes</a>.
In this paper we propose a memory-efficient zeroth-order optimizer (<strong>MeZO</strong>),
adapting the classical zeroth-order SGD method to operate in-place, thereby fine-tuning language models (LMs) with the same memory footprint as inference.</p>
<p dir="auto">With a single A100 80GB GPU, MeZO can train a 30-billion parameter OPT model, whereas fine-tuning with Adam can train only a 2.7B LM.
MeZO demonstrates comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12Ã— memory reduction. MeZO is also compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning. We also show that MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/princeton-nlp/MeZO/blob/main/assets/fig2.png?raw=true"><img src="https://github.com/princeton-nlp/MeZO/raw/main/assets/fig2.png?raw=true" alt="Fig" width="100%"/></a>
  <em>
  GPU memory usage comparison between zero-shot, in-context learning (ICL), Adam fine-tuning (FT), and our proposed MeZO.
  </em>
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/princeton-nlp/MeZO/blob/main/assets/fig1.png?raw=true"><img src="https://github.com/princeton-nlp/MeZO/raw/main/assets/fig1.png?raw=true" alt="Fig" width="100%"/></a>
  <em>
  OPT-13B results with zero-shot, in-context learning (ICL), MeZO (we report the best among MeZO/MeZO (LoRA)/MeZO (prefix)), and fine-tuning with Adam (FT). MeZO demonstrates superior results over zero-shot and ICL and performs on par with FT (within 1%) on 7 out of 11 tasks, despite using only 1/12 memory.
  </em>
</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-reproduce-our-paper-results" aria-hidden="true" href="#reproduce-our-paper-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reproduce our paper results</h2>
<p dir="auto">For reproducing RoBERTa-large experiments, please refer to the <a href="https://github.com/princeton-nlp/MeZO/tree/main/medium_models">medium_models</a> folder. For autoregressive LM (OPT) experiments, please refer to the <a href="https://github.com/princeton-nlp/MeZO/tree/main/large_models">large_models</a> folder. If you want to learn more about how MeZO works and how we implement it, we recommend you to read the <a href="https://github.com/princeton-nlp/MeZO/tree/main/large_models">large_models</a> folder as the implementation is clearer and more extensible. If you want to explore more variants of MeZO, we recommend trying out <a href="https://github.com/princeton-nlp/MeZO/tree/main/medium_models">medium_models</a> as it&#39;s faster and has more variants implemented.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-to-add-mezo-to-my-own-code" aria-hidden="true" href="#how-to-add-mezo-to-my-own-code"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to add MeZO to my own code?</h2>
<p dir="auto">Our implementation of MeZO is based on <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py">HuggingFace&#39;s Trainer</a>. We add MeZO to the official implementation of trainer with minimum editing. Please refer to &#34;How to add MeZO to my own code?&#34; section in <a href="https://github.com/princeton-nlp/MeZO/tree/main/large_models">large_models</a> README for more details.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-bugs-or-questions" aria-hidden="true" href="#bugs-or-questions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Bugs or questions?</h2>
<p dir="auto">If you have any questions related to the code or the paper, feel free to email Sadhika (<code>smalladi@princeton.edu</code>) or Tianyu (<code>tianyug@princeton.edu</code>). If you encounter any problems when using the code, or want to report a bug, you can open an issue. Please try to specify the problem with details so we can help you better and quicker!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-citation" aria-hidden="true" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h2>
<div dir="auto" data-snippet-clipboard-copy-content="@article{malladi2023mezo,
   title={Fine-Tuning Large Language Models with Just Forward Passes},
   author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
   year={2023}
}"><pre><span>@article</span>{<span>malladi2023mezo</span>,
   <span>title</span>=<span><span>{</span>Fine-Tuning Large Language Models with Just Forward Passes<span>}</span></span>,
   <span>author</span>=<span><span>{</span>Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev<span>}</span></span>,
   <span>year</span>=<span><span>{</span>2023<span>}</span></span>
}</pre></div>
</article>
          </div></div>
  </body>
</html>
