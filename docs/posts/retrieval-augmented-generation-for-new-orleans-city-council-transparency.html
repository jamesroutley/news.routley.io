<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eyeonsurveillance.org/blog/rag-for-new-orleans-city-council-transparency">Original</a>
    <h1>Retrieval Augmented Generation for New Orleans City Council Transparency</h1>
    
    <div id="readability-page-1" class="page"><div><p><a href="https://sawt.us">Sawt</a> is an <a href="https://github.com/eye-on-surveillance/sawt">open source</a> AI assistant trained with recordings from New Orleans City Council meetings. Today, any resident can ask Sawt a question about what’s going on in the council. Our long-term aim is to develop an ethical, community-controlled large language model (LLM).</p><p>If you live in New Orleans, consider attending our annual meeting. You can RSVP <a href="https://bit.ly/2024EOS">here</a>. If you don’t live in New Orleans, we accept crypto <a href="https://omo.so/sawt">donations</a>. Our OpenAI bill is starting to grow and any help would be greatly appreciated.</p><p>‍</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/620ef7a71c9f7648fc963845/65957f5b1febe84b8639a5df_sawt_card.png" loading="lazy" alt=""/></p><figcaption>Figure 1: Screenshot of Sawt <a href="https://www.sawt.us/s/8a03906d-d257-45d2-935a-efdfea3ace7c">response</a></figcaption></figure><p>‍</p><h2>Results so far</h2><p>Although we&#39;ve just publicly launched the beta version of Sawt, a select group of local residents have been testing it since the summer. To date, 319 questions have been asked by New Orleanians. They have also provided feedback on 201 responses generated by the system.</p><h3>Retention</h3><p>Despite targeted outreach, we&#39;ve noticed that community members are not returning to use the tool after their initial interaction. There has been minimal engagement apart from a spike during a mid-November focus group. This indicates that Sawt is not helpful enough yet for people to want to come back.</p><p>‍</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/620ef7a71c9f7648fc963845/6595804deca8d8d960aba5de_questions_by_week.png" loading="lazy" alt=""/></p><figcaption>Figure 2: Questions asked on Sawt, by week</figcaption></figure><p>‍</p><p>We&#39;ve approached Sawt&#39;s development with caution, understanding that technology alone isn&#39;t a catalyst for change. Genuine grassroots engagement is paramount. While we remain critical of the project, we&#39;re also excited about its potential. Upcoming features aimed at increasing Sawt&#39;s usefulness include instant data uploads for new council meetings, alerts when registered keywords are discussed during meetings, a larger dataset, and more precise citations.</p><h3>Accuracy &amp; bias</h3><p>Improving accuracy and reducing bias are critical to making Sawt a valuable tool. Through focus groups with organizers, non-profit workers, and community members from diverse areas, we&#39;ve confirmed that Sawt&#39;s responses are not always accurate and sometimes contain biases. When asked to give feedback on a scale of 1 - 5, people currently rate Sawt around a 3.</p><p>‍</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/620ef7a71c9f7648fc963845/659582ed4d4bf68137e1fae4_feedback_by_version.png" loading="lazy" alt=""/></p><figcaption>Figure 3: Feedback on Sawt across two different releases of the tool (v0.0.1 and v0.0.2)</figcaption></figure><p>‍</p><p>These feedback sessions also surfaced some counterintuitive patterns about real world usage. For example, people feel that generated responses are the most accurate when the number of source documents (k) is lowest. In other words, people find that responses are most accurate when there is less information being considered. </p><p>‍</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/620ef7a71c9f7648fc963845/6595831dc4893eca6b23f0f6_feedback_details.png" loading="lazy" alt=""/></p><figcaption>Figure 4: Detailed feedback on Sawt. Figure 4 contains the same results as Figure 3, but with responses normalized per respondent. Figure 4 shows how people scored responses by accuracy, helpfulness, balance, and overall. The k parameter corresponds to the number of documents used to inform the generated response. k=5 means that five video clips, meeting minutes, and/or articles were used to inform the response. k=15 means that fifteen such documents were used.</figcaption></figure><p>‍</p><p>Ultimately, the sample size is small, so there’s limited statistical significance to these results. If you are a New Orleans resident, you can <a href="https://www.sawt.us/feedback">help</a> by submitting some feedback yourself.</p><p>‍</p><h2>How it’s made</h2><p>The initial Sawt <a href="https://colab.research.google.com/drive/1DpEuim10ZxngSQ_m3hAWjnFETqO9_Zaf?usp=sharing">prototype</a> was developed over a weekend by <a href="https://github.com/ayyubibrahimi">@ayyubibrahimi</a>. The core logic has remained consistent since then. We first ingest raw data, create embeddings, and setup FAISS. When someone asks Sawt a question, we identify the most relevant documents using FAISS, combine them with the query, and send them to OpenAI for a response.</p><p>‍</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/620ef7a71c9f7648fc963845/659583cf4d4bf68137e2ae7b_sawt_flow.png" loading="lazy" alt=""/></p><figcaption>Figure 5: How Sawt works</figcaption></figure><p>‍</p><p>In the preprocessing/FAISS phase, we implemented the Hypothetical Document Embeddings (HyDE) methodology from &#39;<a href="https://arxiv.org/abs/2212.10496">Precise Zero-Shot Dense Retrieval without Relevance Labels</a> to create our embedding space for Retrieval-Augmented Generation. RAG is a technique that combines the strengths of large language models with external data retrieval, aiming to enhance the relevance and accuracy of the responses generated by the AI model. The hypothetical document for generating these embeddings was produced using a zero-shot prompt. This prompt <a href="https://github.com/eye-on-surveillance/sawt/blob/ec64a5f92e2ec978131df55832bdaab4707df5d9/packages/backend/src/preprocessor.py#L29C4-L29C134">reads:</a></p><p>‍</p><blockquote> As an AI assistant, utilize the New Orleans City Council transcript data from your training to deliver a detailed and impartial response to the following query: &#34;{user_query}&#34;.</blockquote><p>‍</p><p>This approach ensures that the model uses the specific dataset it was trained on, aiming to provide a comprehensive and impartial answer to the question.</p><p>The prompt sent to OpenAI during the final phase is more complex and detailed. It contains several instructions emphasizing the need to format responses clearly. It&#39;s designed to balance the breadth of response with conciseness. A significant part of the prompt is dedicated to explicitly investigating and addressing potential biases in the response. Additionally, the prompt instructs the model to define uncommon words, making the responses more accessible and informative for people who may not be familiar with specific terms related to city council activities. </p><p>‍</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/620ef7a71c9f7648fc963845/659584906950ea4dc287eabd_template.png" loading="lazy" alt=""/></p><figcaption>Figure 5: <a href="https://github.com/eye-on-surveillance/sawt/blob/ec64a5f92e2ec978131df55832bdaab4707df5d9/packages/googlecloud/functions/getanswer/inquirer.py#L214">Prompt</a> used for queries</figcaption></figure><p>‍</p><p>Although Sawt is an Eye on Surveillance project, we worked with <a href="https://sse.tulane.edu/aron-culotta">Dr Culotta</a> and three undergraduate students at Tulane University. Their work has focused on addressing bias and they built out the current feedback mechanism. Dr Culotta also generated the analysis used in Figure 4.</p><p>‍</p><h3>Understanding our context</h3><p>In December 2020, after the New Orleans Police Department (NOPD) had been caught <a href="https://thelensnola.org/2020/11/12/new-orleans-police-department-using-facial-recognition-despite-years-of-denial/">lying for years</a> about its use of facial recognition, Eye on Surveillance a coalition of local organizations united to divest from surveillance and invest in communities, <a href="https://www.wwno.org/latest-news/2020-12-18/city-council-bans-police-from-using-facial-recognition-technology">pushed the New Orleans City Council to adopt a data privacy ordinance</a> which included a ban on facial recognition and other surveillance technologies. However, merely a year later, the city repealed the ban on facial recognition and cell-site simulators. Moreover, the council member who sponsored the ordinance is now <a href="https://thelensnola.org/2023/11/15/the-das-office-wants-to-use-predictive-analytics-software-to-direct-city-resources-to-places-that-drive-crime-will-it-work/">piloting a predictive policing tech program</a> as district attorney, which is banned under the ordinance he passed. </p><p>Most members of Eye on Surveillance are volunteers with day jobs. It’s hard to keep up with all the regressive steps the council is taking during their meetings. While official council meetings are public and theoretically accessible, they often occur during work hours, last all day, and have agendas that change last-minute. This makes it challenging for us to keep up. Especially when the city strategically schedules announcements of controversial decisions/meetings the day before a holiday. </p><p>Throughout the year, there have been numerous instances where we only became informed about significant surveillance initiatives months after they were raised in city council. We hope Sawt will be a useful tool for us and other New Orleanians moving forward.</p><p>‍</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/620ef7a71c9f7648fc963845/65958b9ff5ccbd1e812d6b12_battles.jpeg" loading="lazy" alt=""/></p><figcaption>Figure 6: New Orleans took on surveillance throughout 2023 without transparency</figcaption></figure><p>‍</p><ul role="list"><li><a href="https://www.wwltv.com/article/news/crime/new-orleans-city-council-police-shotspotter-technology-crime-reduction-shootings/289-966251f7-4097-4c61-bbba-f4c1138e81de">ShotSpotter</a>: Council considered using it in January and we found out in February.</li><li><a href="https://youtu.be/PwiJYkLNzZA?t=9953">Facial Recognition</a>: Reporting was available as of February, but the city rejected several public records requests until July. Sawt itself was helpful in providing information that led to our public records request finally being accepted.</li><li><a href="https://thelensnola.org/2023/11/15/the-das-office-wants-to-use-predictive-analytics-software-to-direct-city-resources-to-places-that-drive-crime-will-it-work/">Predictive Policing</a>: Former council member who sponsored ban on predictive policing, is now DA and piloting the illegal technology. We found out from the newspaper. </li><li><a href="https://www.fox8live.com/2023/11/29/nopd-solicits-public-feedback-proposed-drone-program/">Drones</a>: NOPD seeks feedback on drones they already purchased. </li></ul><p>‍</p><h2>Next steps</h2><p>As we move into 2024, we&#39;re reflecting on the broader implications of Sawt. We&#39;re questioning the extent to which we might be perpetuating AI biases and considering the specific biases of Eye on Surveillance members that could hinder our efforts. Nevertheless, we&#39;re excited about the continued progress and the prospect of launching an official version in the summer of 2024. We&#39;re committed to creating community-owned and operated AI models. </p><p>If you want to get involved:</p><ul role="list"><li><a href="https://bit.ly/2024EOS">RSVP</a> to attend our annual meeting </li><li><a href="https://actionnetwork.org/forms/join-eye-on-surveillances-mailing-list/">Sign up</a> for our mailing list </li><li><a href="https://github.com/eyeonsurveillance/sawt">Contribute</a> some code</li><li><a href="https://omo.so/sawt">Donate</a></li><li>     BTC: bc1qkseneu5cv9g6u4gpmnlen3q3at59r6sj6kn07q</li><li>     SOL: CawuhzDxyytazxywF942VsLwi4RKEWqryLYDsv4hndNa</li><li>     ETH: 0xAA37b8a54e49e6c61De9904985e2887dfEABBA20</li></ul></div></div>
  </body>
</html>
