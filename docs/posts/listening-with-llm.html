<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://paul.mou.dev/posts/2023-12-31-listening-with-llm/">Original</a>
    <h1>Listening with LLM</h1>
    
    <div id="readability-page-1" class="page"><div>
			<h2 id="overview">Overview</h2>
<p>This is the first part of many posts I am writing to consolidate learnings on how to finetune Large Language Models (LLMs) to process audio, with the eventual goal of being able to build and host a LLM able to describe human voices.</p>
<p>I am motivated to gain hands-on experience tinkering LLMs so, as much as practical, I tried to recreate utilities and functions with pytorch from scratch rather than rely on 3rd party libraries.</p>
<blockquote>
<p><strong>tl;dr</strong> I chronicle and share the steps I took to learn how to finetune a LLM model to describe a given audio file on Google’s MusicCaps dataset</p>
</blockquote>
<h2 id="background">Background</h2>
<p>Recently, I came across two papers</p>
<ul>
<li><a href="https://arxiv.org/abs/2310.13289">SALMONN: Towards Generic Hearing Abilities for Large Language Models</a></li>
<li><a href="https://arxiv.org/abs/2311.07919">Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models</a></li>
</ul>
<p>to give LLMs audio understanding capabilities.</p>
<p>Broadly speaking, both papers explored leveraging an audio encoder to transform sound to embeddings that is then fed into LLMs along with text embeddings.</p>
<p>In SALMONN’s case, they combined <a href="https://github.com/openai/whisper">OpenAI’s Whisper</a> and <a href="https://arxiv.org/abs/2212.09058">BEATS encoder</a>, performed pretraining on the combined encoder, then leveraged <a href="https://arxiv.org/abs/2106.09685">LoRA</a> for finetuning the LLM. Qwen-Audio bootstrapped its audio encoder from OpenAI’s Whisper; after pretraining, Qwen-Audio performs a full finetuning on the LLM.</p>
<p>These two papers gave me a great overview on how to adapt cross domain encoders and combine them with LLMs.
Excited by the idea of a LLM with general audio understanding ability and itching to gain hands-on experience, I decided to try and build a minimal viable LLM with audio processing capability.</p>
<h2 id="setup">Setup</h2>
<p>To get started, I hopped over to HuggingFace to find a good base LLM and a medium-sized dataset. I wanted to do as much work locally as possible so everythign must run on a local RTX 3090.</p>
<p>After testing and comparing a few different models, I settled on <a href="https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca">Mistral OpenOrca</a>.</p>
<p>For audio encoder, I went with <a href="https://github.com/openai/whisper">OpenAI’s Whisper</a>.</p>
<p>For dataset, I chose <a href="https://huggingface.co/datasets/google/MusicCaps">MusicCaps</a>. I did not see any convenient links to download processed/segmented audio files, so I wrote a <a href="https://gist.github.com/moomou/d5ff6af6716d20b33026f53f209502af">small script</a> to download the Youtube videos.</p>
<h2 id="one-mini-step-at-a-time">One Mini Step at a Time</h2>
<p>With the basic dependencies out of the way, I fired up my Jupyter notebook and started tinkering.</p>
<h3 id="sampling-from-scratch">Sampling from Scratch</h3>
<p>The first step I took is to ensure I can load the base LLM and perform inference correctly. Instead of leveraging transformers library’s <a href="https://huggingface.co/docs/transformers/main_classes/text_generation#generation">generation utilities</a>, I implemented my own sampling function to verify my understanding as well as to learn how to sample using embeddings directly, which will come in handy when feeding in audio embeddings.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@torch.no_grad</span>
</span></span><span><span><span>def</span> <span>sampler</span>(input_ids):
</span></span><span><span>    outputs <span>=</span> []
</span></span><span><span>
</span></span><span><span>    <span>for</span> _ <span>in</span> range(<span>50</span>):
</span></span><span><span>        inputs_embeds <span>=</span> model<span>.</span>llm<span>.</span>model<span>.</span>embed_tokens(input_ids)
</span></span><span><span>        res <span>=</span> model<span>.</span>llm(inputs_embeds<span>=</span>inputs_embeds)
</span></span><span><span>        <span># res.logits shape is (batch, seq_len, logits)</span>
</span></span><span><span>        <span># sample using multinomial using the last logits </span>
</span></span><span><span>        sampled <span>=</span> torch<span>.</span>multinomial(res<span>.</span>logits[:,<span>-</span><span>1</span>,:]<span>.</span>softmax(dim<span>=-</span><span>1</span>), <span>1</span>)
</span></span><span><span>        <span># repeatedly concat the `sampled` to the `input_ids` for next sampling</span>
</span></span><span><span>        input_ids <span>=</span> torch<span>.</span>cat((input_ids, sampled), dim<span>=-</span><span>1</span>)
</span></span><span><span>
</span></span><span><span>    <span>return</span> input_ids
</span></span></code></pre></div><p>Using the <code>tokenizer</code> class obtained from Transformer’s <code>AutoTokenizer</code> class, I was able to verify sampling worked as expected! Running</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>tokenizer<span>.</span>decode(sampler(tokenizer(<span>&#34;tell me a story&#34;</span>, return_tensors<span>=</span><span>&#34;pt&#34;</span>)<span>.</span>input_ids<span>.</span>to(<span>&#34;cuda:0&#34;</span>))[<span>0</span>])
</span></span></code></pre></div><p>yields (as an example output)</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>&#39;&lt;s&gt;tell me a story is a film and video production company, tell me a story is a concept that was created to allow people to come together through the power of storytelling.\n and so, with this massive power in storytelling, the founders and creat&#39;
</span></span></code></pre></div><h3 id="debugging-nans-and-infs">Debugging NaNs and Infs</h3>
<p>So far so good. However, I soon noticed that, occasionally, the sampling function would fail by complaining that softmax function encountered an <code>inf</code> or <code>NaN</code>. I followed this <a href="https://github.com/TimDettmers/bitsandbytes/issues/165#issuecomment-1560496240">insightful thread</a> and learnt to identify the source of <code>NaN</code> by using the following adapted Pytorch hooks</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch
</span></span><span><span><span>from</span> functools <span>import</span> partial
</span></span><span><span>
</span></span><span><span>__registered_hook_refs <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> h <span>in</span> __registered_hook_refs:
</span></span><span><span>    h<span>.</span>remove()
</span></span><span><span>
</span></span><span><span>__global <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>def</span> <span>nan_hook</span>(module, args, output, name<span>=</span><span>None</span>):
</span></span><span><span>    <span>if</span> <span>not</span> isinstance(output, tuple):
</span></span><span><span>        outputs <span>=</span> [output]
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        outputs <span>=</span> output
</span></span><span><span>
</span></span><span><span>    <span>for</span> i, out <span>in</span> enumerate(outputs):
</span></span><span><span>        <span>if</span> out <span>is</span> <span>None</span>:
</span></span><span><span>            <span>continue</span>
</span></span><span><span>        <span>if</span> isinstance(out, tuple):
</span></span><span><span>            <span>for</span> j, out2 <span>in</span> enumerate(out):
</span></span><span><span>                nan_mask <span>=</span> torch<span>.</span>isnan(out2)
</span></span><span><span>                <span>if</span> nan_mask<span>.</span>any():
</span></span><span><span>                    __global<span>.</span>append((module, args, output))
</span></span><span><span>                    <span>raise</span> <span>RuntimeError</span>(<span>f</span><span>&#34;In module </span><span>{</span>name<span>}</span><span> of name </span><span>{</span>module<span>.</span>__class__<span>.</span>__name__<span>}</span><span>, Found NAN in output </span><span>{</span>j<span>}</span><span> at indices: &#34;</span>, nan_mask<span>.</span>nonzero(), <span>&#34;where:&#34;</span>,
</span></span><span><span>                               out[nan_mask<span>.</span>nonzero()[:, <span>0</span>]<span>.</span>unique(sorted<span>=</span><span>True</span>)])
</span></span><span><span>                               
</span></span><span><span>        <span>elif</span> torch<span>.</span>is_tensor(out):
</span></span><span><span>            nan_mask <span>=</span> torch<span>.</span>isnan(out)
</span></span><span><span>            
</span></span><span><span>            <span>if</span> nan_mask<span>.</span>any():
</span></span><span><span>                __global<span>.</span>append((module, args, output))
</span></span><span><span>                <span>raise</span> <span>RuntimeError</span>(<span>f</span><span>&#34;In module </span><span>{</span>name<span>}</span><span> of name </span><span>{</span>module<span>.</span>__class__<span>.</span>__name__<span>}</span><span>, Found NAN in output </span><span>{</span>i<span>}</span><span> at indices: &#34;</span>, nan_mask<span>.</span>nonzero(), <span>&#34;where:&#34;</span>,
</span></span><span><span>                                   out[nan_mask<span>.</span>nonzero()[:, <span>0</span>]<span>.</span>unique(sorted<span>=</span><span>True</span>)])
</span></span><span><span>
</span></span><span><span><span>def</span> <span>register_nan_hook</span>(model: torch<span>.</span>nn<span>.</span>Module):
</span></span><span><span>    <span>for</span> name, submodule <span>in</span> model<span>.</span>named_modules():
</span></span><span><span>        new_hook <span>=</span> partial(nan_hook, name<span>=</span>name<span>+</span><span>&#39;.back&#39;</span>)
</span></span><span><span>        hook_ref <span>=</span> submodule<span>.</span>register_full_backward_hook(new_hook)
</span></span><span><span>        __registered_hook_refs<span>.</span>append(hook_ref)
</span></span><span><span>        new_hook <span>=</span> partial(nan_hook, name<span>=</span>name<span>+</span><span>&#39;.fwd&#39;</span>)
</span></span><span><span>        hook_ref <span>=</span> submodule<span>.</span>register_forward_hook(new_hook)
</span></span><span><span>        __registered_hook_refs<span>.</span>append(hook_ref)
</span></span><span><span>
</span></span><span><span>debug <span>=</span> <span>True</span>
</span></span><span><span>register_nan_hook(model) <span>if</span> debug <span>else</span> <span>None</span>
</span></span></code></pre></div><p>Leveraging these hooks narrowed down the source of issue to a particular layer and from there I was able to trace the problem to an <code>inf</code> value in the model weights. Digging further, I traced the source of <code>inf</code> to <a href="https://paul.mou.dev/posts/2023-12-27-ohmy">bad RAM sticks</a>! After mitigation, I wrote a small script to verify the model weights and confirmed sampling function worked as expected.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># verify model weight</span>
</span></span><span><span><span>from</span> collections <span>import</span> Counter
</span></span><span><span>pbytype <span>=</span> Counter()
</span></span><span><span><span>for</span> name, p <span>in</span> (model<span>.</span>named_parameters()):
</span></span><span><span>    <span>if</span> torch<span>.</span>isinf(p)<span>.</span>any() <span>or</span> torch<span>.</span>isnan(p)<span>.</span>any():
</span></span><span><span>        print(name, p)
</span></span><span><span>        <span>raise</span> <span>ValueError</span>(<span>&#34;invalid weight&#34;</span>)
</span></span><span><span>    <span>else</span>:
</span></span><span><span>        pbytype[p<span>.</span>dtype] <span>+=</span> <span>1</span>
</span></span><span><span>print(<span>&#34;OK&#34;</span>, pbytype)
</span></span></code></pre></div><h3 id="adapting-whisper-to-mistral">Adapting Whisper to Mistral</h3>
<p>After gaining confidence with debugging Pytorch modules, I focused on adapting Whisper model so audio files can be transformed into an embedding that can then be fed into Mistral.</p>
<p>OpenAI’s Whisper <a href="https://github.com/openai/whisper/blob/ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab/whisper/model.py#L225-L238">model</a> is composed of two major components, an <code>AudioEncoder</code> and a <code>TextDecoder</code>.
For the purpose of translating audio into embeddings, I only need the <code>AudioEncoder</code> component.</p>
<p>Therefore, I loaded up a full Whisper model and extracted the <code>AudioEncoder</code> weights using the following snippets</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> whisper
</span></span><span><span>
</span></span><span><span>model <span>=</span> whisper<span>.</span>load_model(<span>&#34;large-v3&#34;</span>)
</span></span><span><span>audio_encoder <span>=</span> model<span>.</span>encoder
</span></span><span><span>torch<span>.</span>save(
</span></span><span><span>    audio_encoder<span>.</span>state_dict(),
</span></span><span><span>    <span>&#34;&lt;output_location&gt;&#34;</span>,
</span></span><span><span>)
</span></span></code></pre></div><p>I adapted the Whisper AudioEncoder into a <code>TunableWhisperAudioEncoder</code> with an extra projection layer to map from Whisper’s audio embedding (size 1280) to mistral’s token embedding (size 4096).</p>
<p>I ensured <code>proj</code> is the only trainable network by explicitly freezing the audio encoder’s parameters. Note that <code>TrainableSubmodule</code> is a hyperparameter and any model that maps the output embedding to size 4096 will work. Later in the post, I will describe what I found to work for me.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>TunableWhisperAudioEncoder</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>def</span> __init__(self, <span>*</span>, output_embedding_size<span>=</span><span>4096</span>):
</span></span><span><span>        <span>&#34;&#34;&#34;
</span></span></span><span><span><span>        args
</span></span></span><span><span><span>            output_embedding_size: int = 4096 / mistral default embedding size
</span></span></span><span><span><span>        &#34;&#34;&#34;</span>
</span></span><span><span>        super()<span>.</span>__init__()
</span></span><span><span>
</span></span><span><span>        self<span>.</span>audio_encoder <span>=</span> load_whisper_v3_audio_encoder()
</span></span><span><span>        self<span>.</span>proj <span>=</span> TrainableSubmodule(output_embedding_size<span>=</span>output_embedding_size)
</span></span><span><span>
</span></span><span><span>        <span># # Freeze all parameters</span>
</span></span><span><span>        <span>for</span> param <span>in</span> audio_encoder<span>.</span>parameters():
</span></span><span><span>            param<span>.</span>requires_grad <span>=</span> <span>False</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, mels):
</span></span><span><span>        res <span>=</span> self<span>.</span>audio_encoder(mels)
</span></span><span><span>        res <span>=</span> self<span>.</span>proj(res)
</span></span><span><span>        <span>return</span> res
</span></span><span><span>
</span></span><span><span><span>def</span> <span>load_whisper_v3_audio_encoder</span>(
</span></span><span><span>    <span>*</span>,
</span></span><span><span>    n_mels<span>=</span><span>128</span>,
</span></span><span><span>    n_audio_ctx<span>=</span><span>1500</span>,
</span></span><span><span>    n_audio_state<span>=</span><span>1280</span>,
</span></span><span><span>    n_audio_head<span>=</span><span>20</span>,
</span></span><span><span>    n_audio_layer<span>=</span><span>32</span>,
</span></span><span><span>):
</span></span><span><span>    m <span>=</span> whisper<span>.</span>model<span>.</span>AudioEncoder(
</span></span><span><span>        n_mels, n_audio_ctx, n_audio_state, n_audio_head, n_audio_layer
</span></span><span><span>    )
</span></span><span><span>    m<span>.</span>load_state_dict(torch<span>.</span>load(WHISPER_AUDIO_BIN))
</span></span><span><span>    <span>return</span> m
</span></span></code></pre></div><p>Finally, I build up the model I am going to use for training as follows</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Model</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>def</span> __init__(self, audio_encoder: <span>&#34;Whisper.AudioEncoder&#34;</span>, llm: <span>&#34;Mistral&#34;</span>):
</span></span><span><span>        super()<span>.</span>__init__()
</span></span><span><span>
</span></span><span><span>        self<span>.</span>audio_encoder <span>=</span> audio_encoder
</span></span><span><span>        self<span>.</span>llm <span>=</span> llm
</span></span><span><span>
</span></span><span><span>        <span># freeze the LLM weights</span>
</span></span><span><span>        <span>for</span> p <span>in</span> self<span>.</span>llm<span>.</span>parameters():
</span></span><span><span>            p<span>.</span>requires_grad <span>=</span> <span>False</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, batch):
</span></span><span><span>        audio_mels <span>=</span> batch[<span>&#34;audio_mels&#34;</span>]
</span></span><span><span>        <span># caption token ids</span>
</span></span><span><span>        cap_ids <span>=</span> batch[<span>&#34;cap_ids&#34;</span>]
</span></span><span><span>        <span># caption attention mask</span>
</span></span><span><span>        cap_ids_attention_mask <span>=</span> batch[<span>&#34;cap_attention_mask&#34;</span>]
</span></span><span><span>        prompt_ids <span>=</span> batch[<span>&#34;prompt_ids&#34;</span>]
</span></span><span><span>        prompt_ids_attention_mask <span>=</span> batch[<span>&#34;prompt_attention_mask&#34;</span>]
</span></span><span><span>        end_prompt_ids <span>=</span> batch[<span>&#34;end_prompt_ids&#34;</span>]
</span></span><span><span>        end_prompt_ids_attention_mask <span>=</span> batch[<span>&#34;end_prompt_attention_mask&#34;</span>]
</span></span><span><span>
</span></span><span><span>        audio_embeds <span>=</span> self<span>.</span>audio_encoder(audio_mels)
</span></span><span><span>        <span># audio_embeds: (batch, audio_seq_len, audio_embedding_size)</span>
</span></span><span><span>        bs, audio_seq <span>=</span> audio_embeds<span>.</span>shape[:<span>2</span>]
</span></span><span><span>
</span></span><span><span>        attention_mask <span>=</span> torch<span>.</span>concat(
</span></span><span><span>            (
</span></span><span><span>                prompt_ids_attention_mask,
</span></span><span><span>                torch<span>.</span>ones(bs, audio_seq)<span>.</span>to(cap_ids<span>.</span>device),
</span></span><span><span>                end_prompt_ids_attention_mask,
</span></span><span><span>                cap_ids_attention_mask,
</span></span><span><span>            ),
</span></span><span><span>            dim<span>=</span><span>1</span>,
</span></span><span><span>        )
</span></span><span><span>
</span></span><span><span>        cap_embeds <span>=</span> self<span>.</span>llm<span>.</span>model<span>.</span>embed_tokens(cap_ids)
</span></span><span><span>        prompt_embeds <span>=</span> self<span>.</span>llm<span>.</span>model<span>.</span>embed_tokens(prompt_ids)
</span></span><span><span>        end_prompt_embeds <span>=</span> self<span>.</span>llm<span>.</span>model<span>.</span>embed_tokens(end_prompt_ids)
</span></span><span><span>
</span></span><span><span>        <span># build the inputs_embeds by concating all the token embeddings</span>
</span></span><span><span>        <span># with audio_embeddings</span>
</span></span><span><span>        inputs_embeds <span>=</span> torch<span>.</span>concat(
</span></span><span><span>            (
</span></span><span><span>                prompt_embeds,
</span></span><span><span>                audio_embeds<span>.</span>to(cap_embeds<span>.</span>dtype),
</span></span><span><span>                end_prompt_embeds,
</span></span><span><span>                cap_embeds,
</span></span><span><span>            ),
</span></span><span><span>            dim<span>=</span><span>1</span>,
</span></span><span><span>        )
</span></span><span><span>
</span></span><span><span>        mout <span>=</span> self<span>.</span>llm(
</span></span><span><span>            inputs_embeds<span>=</span>inputs_embeds,
</span></span><span><span>            attention_mask<span>=</span>attention_mask,
</span></span><span><span>        )
</span></span><span><span>
</span></span><span><span>        <span>return</span> mout, audio_embeds<span>.</span>shape[<span>1</span>]
</span></span></code></pre></div><p>The model itself is quite simple in that it simply holds reference to the Mistral LLM and <code>TunableWhisperAudioEncoder</code>. The <code>forward</code> method encapsulates the logic of converting audio mel-spectrogram into audio embeddings, then concatenating the audio embeddings with text/token embeddings to feeding those into Mistral LLM.</p>
<h3 id="sampling-with-audio-from-scratch">Sampling with Audio from Scratch</h3>
<p>With the basic model in place, the next step is to try and sample from this model with audio inputs. Here is the audio sampling function I came up with.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># note, full gist is available at https://gist.github.com/moomou/7df8345d79a0063d67d1fa2b4cf55db8</span>
</span></span><span><span>
</span></span><span><span><span>@torch.no_grad</span>()
</span></span><span><span><span>def</span> <span>sample_with_audio</span>(model, tokenizer, prompt, audio_file, device<span>=</span><span>&#34;cuda:0&#34;</span>, iteration<span>=</span><span>50</span>):
</span></span><span><span>    audio_mels <span>=</span> load_audio_mels(audio_file)<span>.</span>to(device)<span>.</span>half()
</span></span><span><span>    end_prompt_ids, end_prompt_attention_mask <span>=</span> text_2_ids_and_attention_mask(
</span></span><span><span>        tokenizer,
</span></span><span><span>        end_template(),
</span></span><span><span>        truncate<span>=</span><span>True</span>,
</span></span><span><span>    )
</span></span><span><span>    prompt_ids, prompt_attention_mask <span>=</span> text_2_ids_and_attention_mask(
</span></span><span><span>        tokenizer,
</span></span><span><span>        prompt,
</span></span><span><span>    )
</span></span><span><span>
</span></span><span><span>    prompt_ids <span>=</span> prompt_ids<span>.</span>to(device)
</span></span><span><span>    prompt_attention_mask <span>=</span> prompt_attention_mask<span>.</span>to(device)
</span></span><span><span>    end_prompt_attention_mask <span>=</span> end_prompt_attention_mask<span>.</span>to(device)
</span></span><span><span>    end_prompt_ids <span>=</span> end_prompt_ids<span>.</span>to(device)
</span></span><span><span>    sampled_ids <span>=</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>    prompt_embeds <span>=</span> <span>None</span>
</span></span><span><span>    end_prompt_embeds <span>=</span> <span>None</span>
</span></span><span><span>    audio_embeds <span>=</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>    <span>with</span> torch<span>.</span>amp<span>.</span>autocast(device_type<span>=</span><span>&#34;cuda&#34;</span>, dtype<span>=</span>torch<span>.</span>float16): <span># use float16 to reduce GPU memory</span>
</span></span><span><span>        <span>if</span> audio_embeds <span>is</span> <span>None</span>:
</span></span><span><span>            audio_embeds <span>=</span> model<span>.</span>audio_encoder(audio_mels)
</span></span><span><span>        bs, audio_seq <span>=</span> audio_embeds<span>.</span>shape[:<span>2</span>]
</span></span><span><span>        
</span></span><span><span>        mask_concat_args <span>=</span> [
</span></span><span><span>            prompt_attention_mask,
</span></span><span><span>            torch<span>.</span>ones(bs, audio_seq)<span>.</span>to(audio_embeds<span>.</span>device),
</span></span><span><span>            end_prompt_attention_mask,
</span></span><span><span>        ]
</span></span><span><span>
</span></span><span><span>        <span>for</span> _ <span>in</span> range(iteration):
</span></span><span><span>            <span>if</span> sampled_ids <span>is</span> <span>not</span> <span>None</span>:
</span></span><span><span>                mask_concat_args<span>.</span>append(torch<span>.</span>ones(bs, sampled_ids<span>.</span>shape[<span>1</span>])<span>.</span>to(audio_embeds<span>.</span>device))
</span></span><span><span>                
</span></span><span><span>            attention_mask <span>=</span> torch<span>.</span>concat(
</span></span><span><span>                tuple(mask_concat_args),
</span></span><span><span>                dim<span>=</span><span>1</span>,
</span></span><span><span>            )
</span></span><span><span>
</span></span><span><span>            <span>if</span> prompt_embeds <span>is</span> <span>None</span>:
</span></span><span><span>                prompt_embeds <span>=</span> model<span>.</span>llm<span>.</span>model<span>.</span>embed_tokens(prompt_ids)
</span></span><span><span>            <span>if</span> end_prompt_embeds <span>is</span> <span>None</span>:
</span></span><span><span>                end_prompt_embeds <span>=</span> model<span>.</span>llm<span>.</span>model<span>.</span>embed_tokens(end_prompt_ids)
</span></span><span><span>                
</span></span><span><span>            sampled_ids_embeds <span>=</span> <span>None</span>
</span></span><span><span>            <span>if</span> sampled_ids <span>is</span> <span>not</span> <span>None</span>:
</span></span><span><span>                sampled_ids_embeds <span>=</span> model<span>.</span>llm<span>.</span>model<span>.</span>embed_tokens(sampled_ids)
</span></span><span><span>                
</span></span><span><span>            embeds_concat_args <span>=</span> [
</span></span><span><span>                prompt_embeds,
</span></span><span><span>                audio_embeds<span>.</span>to(prompt_embeds<span>.</span>dtype),
</span></span><span><span>                end_prompt_embeds,
</span></span><span><span>            ]
</span></span><span><span>            <span>if</span> sampled_ids_embeds <span>is</span> <span>not</span> <span>None</span>:
</span></span><span><span>                embeds_concat_args<span>.</span>append(sampled_ids_embeds)
</span></span><span><span>                
</span></span><span><span>            inputs_embeds <span>=</span> torch<span>.</span>concat(
</span></span><span><span>                tuple(embeds_concat_args),
</span></span><span><span>                dim<span>=</span><span>1</span>,
</span></span><span><span>            )
</span></span><span><span>    
</span></span><span><span>            mout <span>=</span> model<span>.</span>llm(
</span></span><span><span>                inputs_embeds<span>=</span>inputs_embeds,
</span></span><span><span>                attention_mask<span>=</span>attention_mask,
</span></span><span><span>            )
</span></span><span><span>    
</span></span><span><span>            logits <span>=</span> mout<span>.</span>logits
</span></span><span><span>            sampled <span>=</span> torch<span>.</span>multinomial(logits[:, <span>-</span><span>1</span>, :]<span>.</span>softmax(dim<span>=-</span><span>1</span>), <span>1</span>)
</span></span><span><span>            
</span></span><span><span>            <span>if</span> sampled_ids <span>is</span> <span>None</span>:
</span></span><span><span>                sampled_ids <span>=</span> sampled
</span></span><span><span>            <span>else</span>:
</span></span><span><span>                sampled_ids <span>=</span> torch<span>.</span>cat((sampled_ids, sampled), dim<span>=-</span><span>1</span>)<span>.</span>to(device)
</span></span><span><span>
</span></span><span><span>    <span>return</span> torch<span>.</span>concat((
</span></span><span><span>        prompt_ids, 
</span></span><span><span>        end_prompt_ids,
</span></span><span><span>        sampled_ids,
</span></span><span><span>    ),dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><p>Putting the function to use via</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>dataloader <span>=</span> <span>...</span> <span># standard pytorch dataloader</span>
</span></span><span><span>local_batch <span>=</span> next(iter(dataloader))
</span></span><span><span>tokenizer<span>.</span>decode(sample_with_audio(model, tokenizer, prompt_template_fn(), audio_file, iteration<span>=</span><span>60</span>)[<span>0</span>])
</span></span></code></pre></div><p>produces gibberish as expected since <code>TunableWhisperAudioEncoder</code> projection layer is untrained.</p>
<pre>&#39;&lt;s&gt; &lt;|im_start|&gt;  system\n    You are a helpful AI who follows instruction carefully&lt;|im_end|&gt; &lt;|im_start|&gt;  user\n    Describe the sound of the given file \n    &lt;|im_end|&gt; &lt;|im_start|&gt;  assistant\n     war&lt;|im_end|&gt; clockunits ]andfirst4Iftektime爆R Cur&lt;|im_end|&gt; United&lt;|im_end|&gt; ’daysIn“Never&lt;|im_end|&gt; thenAnd,and VI&lt;|im_end|&gt; Islo&lt;|im_end|&gt; GOkaydown&lt;|im_end|&gt; JainteYoulfailedLabelsEvenfacevC,rest&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt; q&lt;|im_end|&gt; Xs&lt;|im_end|&gt; h&lt;|im_end|&gt;&lt;|im_end|&gt;&#39;
</pre>
<h3 id="defining-loss-function">Defining Loss Function</h3>
<p>The loss function here is the standard cross entropy loss on the logits output; the only trick is that the loss should only be calculated on the caption portion. Specifically,</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># calculate loss</span>
</span></span><span><span><span># local_batch: (b, seq, C)</span>
</span></span><span><span>prompt_ids_seq <span>=</span> local_batch[<span>&#34;prompt_ids&#34;</span>]<span>.</span>shape[<span>1</span>]
</span></span><span><span>end_prompt_ids_seq <span>=</span> local_batch[<span>&#34;end_prompt_ids&#34;</span>]<span>.</span>shape[<span>1</span>]
</span></span><span><span>logits_start <span>=</span> prompt_ids_seq <span>+</span> audio_seq <span>+</span> end_prompt_ids_seq
</span></span><span><span>
</span></span><span><span><span># remove the last output</span>
</span></span><span><span>logits <span>=</span> <span>...</span> <span># model output</span>
</span></span><span><span><span># remove the prompt and audio seq from logits</span>
</span></span><span><span><span># calculation; additionally, remove the final item</span>
</span></span><span><span>logits <span>=</span> logits[:, logits_start:<span>-</span><span>1</span>, :]<span>.</span>contiguous()
</span></span><span><span>
</span></span><span><span><span># calculate target using only `cap_ids`</span>
</span></span><span><span>targets <span>=</span> batch[<span>&#34;cap_ids&#34;</span>][:]
</span></span><span><span>targets <span>=</span> targets[:, <span>1</span>:]
</span></span><span><span>
</span></span><span><span>loss <span>=</span> nn<span>.</span>functional<span>.</span>cross_entropy(
</span></span><span><span>    logits<span>.</span>view(<span>-</span><span>1</span>, logits<span>.</span>shape[<span>-</span><span>1</span>]), targets<span>.</span>view(<span>-</span><span>1</span>)
</span></span><span><span>)
</span></span></code></pre></div><h3 id="training-overfitting-and-debugging-gradients">Training, Overfitting and Debugging Gradients</h3>
<p>Finally, all the pieces are in place for training the model. The objective I had in mind is to make the frozen LLM describe a given audio file by training only <code>TunableWhisperAudioEncoder</code>; achieving this will not give LLM general audio understanding ability since the training data is small but will give me great confidence that I performed all the basic steps right.</p>
<p>In order to ensure training is setup correctly, I started small and one step at a time. Specifically, I interactively stepped through the training steps manually, recorded and plotted the weight update relative to weight data in <code>TunableWhisperAudioEncoder</code>, and ensured there is no <code>inf</code> or <code>NaN</code> using the Pytorch hooks described previously. These steps were repeated for varous combination of learning rate, model architecture, and optimizer.</p>
<p><img src="https://paul.mou.dev/weight_update.png" alt="Weight Update"/></p><p>Keeping the setup as simple as possible, I found Adam (without momentum), a constant learning rate of 1.5e-3, and using the following simple <code>TrainableSubmodule</code>, I achieved stable training.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>TrainableSubmodule</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>def</span> __init__(self, output_embedding_size<span>=</span><span>4096</span>):
</span></span><span><span>        super()<span>.</span>__init__()
</span></span><span><span>
</span></span><span><span>        self<span>.</span>pool <span>=</span> nn<span>.</span>AdaptiveAvgPool1d(<span>250</span>)
</span></span><span><span>        self<span>.</span>proj <span>=</span> nn<span>.</span>Linear(<span>1280</span>, output_embedding_size, bias<span>=</span><span>False</span>)
</span></span><span><span>        self<span>.</span>ln1 <span>=</span> nn<span>.</span>LayerNorm(<span>1280</span>)
</span></span></code></pre></div><p>I ran training over the course of ~4days and by the time I stopped training, the loss was still going down. By the time I stopped, I achieved ~0.46 loss, which translates to approximately 66% probability for the correct token!</p>
<p><img src="https://paul.mou.dev/avg_loss.png" alt="Average Loss"/></p><p>Rerunning the <code>sample_with_audio</code> with the same audio file that produced gibberish pretraining, I now obtain</p>
<pre>&#34;&lt;s&gt; &lt;|im_start|&gt;  system\n    You are a helpful AI who follows instruction carefully&lt;|im_end|&gt; &lt;|im_start|&gt;  user\n    Describe the sound of the given file \n    &lt;|im_end|&gt; &lt;|im_start|&gt; assistant\n     The electronica song features a crisp acoustic kick, snap snare and hat along with a deep bass. The male vocal is rapping syncopated along with a male background vocal. The song is fast paced and there is a limited frequency range of the synths. The song&#34;
</pre>
<p>Compare this against the ground truth</p>
<pre>&#34;This is a K-pop music piece performed by a boy band. Initially, a male vocalist is singing in a rap-like manner. Then, it switches to another male vocal that is singing more melodically. The melody is being played by a crisp synth sound. The rhythmic background consists of an energetic electronic drum beat. There is a danceable feel to it. This piece could be playing at Korean nightclubs and dance clubs.&#34;
</pre>
<p>The result is <em>pretty</em> good!</p>
<p>It’s worth repeating this is achieved by only training on the audio encoder projection without modifying the LLM weights or the Whisper AudioEncoder weights.</p>
<h2 id="next-steps">Next Steps</h2>
<p>With the fundamentals in place, I am planning to scale up training by incorporating more audio tasks such as transcription, speaker identification, etc. as well as apply finetuning to LLM to work my way toward replicating “emergent” behaviors described in the referenced papers.</p>
<p>Assuming sufficient data and with a proper training regime, LLM should be able to perform original audio tasks such as say identify the speaker age or gender without having been explicitly trained on such task.</p>
<p>More work to be done!</p>
<h2 id="acknowledgement">Acknowledgement</h2>
<p>I would not been able to do any of this without learning from the <a href="https://github.com/karpathy/nn-zero-to-hero/">excellent lectures by Karpathy</a>.</p>

		</div></div>
  </body>
</html>
