<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://interjectedfuture.com/lab-note-070-vibe-coding-is-half-a-skill-issue/">Original</a>
    <h1>Lab note #070 Vibe-coding is half a skill issue</h1>
    
    <div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://interjectedfuture.com/tag/lab-notes/">lab notes</a>
            

            <div>
                <p><a href="https://interjectedfuture.com/author/wil/">
                                <img src="https://www.gravatar.com/avatar/915aac3dfde2fb502ce415d77643a72d?s=250&amp;d=mm&amp;r=x" alt="Wil Chung"/>
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2025-06-10">10 Jun 2025</time>
                            <span><span>â€”</span> 5 min read</span>
                    </p>
                </div>
            </div>

                <figure>
        <img srcset="/content/images/size/w320/2025/06/ChatGPT-Image-Jun-10--2025--10_15_02-PM.png 320w,
                    /content/images/size/w600/2025/06/ChatGPT-Image-Jun-10--2025--10_15_02-PM.png 600w,
                    /content/images/size/w960/2025/06/ChatGPT-Image-Jun-10--2025--10_15_02-PM.png 960w,
                    /content/images/size/w1200/2025/06/ChatGPT-Image-Jun-10--2025--10_15_02-PM.png 1200w,
                    /content/images/size/w2000/2025/06/ChatGPT-Image-Jun-10--2025--10_15_02-PM.png 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://interjectedfuture.com/content/images/size/w1200/2025/06/ChatGPT-Image-Jun-10--2025--10_15_02-PM.png" alt="Lab note #070 Vibe-coding is half a skill issue"/>
    </figure>

        </header>

        <section>
            <p>I&#39;ve been doing an implementation of DBSP, which is a way of doing incremental computation built up from some basic concepts from digital signal processing. While I use LLMs in my daily work to ask it questions, spitball with it, and do some basic stuff, I hardly ever ask it to explicitly write code for me.</p><p>But the fact that some people (not just beginners, but programmers that are better than me) have had success with it made me wonder if I could also leverage it to speed me up. However, the summary from the last three weeks was one of struggle with getting AI to write code for me. </p><p>I had written the DBSP implementation and got to a point where I realized it was much better if the description of the circuit was separated from the state of the circuit during execution. So the task was a refactor to separate these two out. Codex had just come out, so I thought I&#39;d give it a shot. </p><p>The short version of the story is that it simply didn&#39;t work with the way I approached it. I would write what I wanted in the agent CLI, and then tried to review each change that the agent was making. It didn&#39;t matter if I switched to Claude Code, or if I wrote out a codex.md/CLAUDE.md file to give it overarching guidelines. Even if it got to a point where the code worked, it was just slightly wrong in subtle ways that didn&#39;t take into account where I was going with it later on. </p><p>Are the great results limited to certain domains? Was it the nature of the work I was doing? Was there not enough training data for reactive and incremental code? Or was this a skill issue on my part? Do I not know how to prompt or scaffold it? The entire experience was not only frustrating, but also demoralizing. I&#39;d lose days just trying to get it to work, with nothing to show for it and $100 in the hole. </p><p>What did finally work is what I heard someone mention before: writing product description docs (PRD). <strong>Except that I wouldn&#39;t write it myself.</strong> Originally, when I heard about doing PRDs, I thought it&#39;d be faster if I&#39;d just implement things myself. But for some reason, I didn&#39;t think about just getting LLMs to write it for me. Here&#39;s what I did. </p><ul><li>I&#39;d first spitball with o3 on the design of the next thing I wanted to implement or fix. </li><li>And then after I was satisfied with all the design decisions that we made, I&#39;d have it write me a PRD.</li><li>Then I&#39;d make sure I was starting a fresh commit in the repo, so I can come back to it if I need to.</li><li>Then I&#39;d paste the PRD into a new chat in an agent on its own repo or gittree branch and just let it go to town with auto-accept all the way until it was done.</li><li>Then I&#39;d review the code and run it against my tests (or ask it to fix any bugs that failed the test, making sure it didn&#39;t change tests to pass them).</li><li>If it didn&#39;t work after three tries, I&#39;d just blow the whole implementation away and try again fresh. </li><li>If it worked, I&#39;d checkpoint it by checking it into version control.</li><li>Finally, if it almost worked, but it&#39;s a little bit off, I&#39;d correct it myself, rather than prompting it again, since it might break other things.</li></ul><p>The workflow is a little odd, since I don&#39;t usually work in large companies, so I never had to do this sort of thing. But I believe it&#39;s a sort of skill issue now. Also, there are some devs out there that are willing to just try and steer it again and again, which builds their intuition for what works and what doesn&#39;t.</p><p>This reminds me of when Sri and I were doing the Technium Podcast in 2022, and we also had the idea of turning the transcript into social media posts. But we found it lackluster and lacking back in the GPT-3 days, and we believed that it simply wasn&#39;t good enough yet. I had no idea that RAG existed, and I didn&#39;t push the prompting either. And guess what? <a href="https://spiral.computer/?ref=interjectedfuture.com" rel="noreferrer">spiral.computer</a> is just that exact idea.</p><p>On the other hand, in my work doing illustrations for <a href="https://forestfriends.tech/?ref=interjectedfuture.com" rel="noreferrer">forestfriends.tech</a>, I&#39;d spend 8 hours a day for two or three weeks just generating hundreds of illustrations. By the end, I got a pretty good idea of what would work and what wouldn&#39;t. </p><p>My point is that we didn&#39;t push prompting hard enough to get it to work, and I&#39;m probably not pushing prompting hard enough to get vibe coding to work for me. That said, I don&#39;t think people should rely on it completely. There are some things where it&#39;s just faster (and more fun) for you to implement yourself. </p><p>And I think Jensen Huang is wrong about not needing engineers in the future. It seems like he does more press tours than running a company nowadays. For things that I have taste in, such as programming, there is still a wide gap between what LLMs can do and what I can do. I&#39;m sure it&#39;ll get better, but I think we&#39;ll always need Geordi LaForges in the future. &#34;Computer&#34; can&#39;t do it by itself, and Data is on the bridge.</p><p>So my takeaways:</p><ol><li>Have a reasoning model rubberduck with you and then have it write the PRD prompt as a result of the conversation. It knows how to prompt itself better than you do.</li><li>Don&#39;t be afraid to blow the results away and start over when it&#39;s not working.</li><li>If it got most of the way, just check point it, and follow up with corrections later.</li><li>Don&#39;t abdicate your seat at the thinking table. The moment you do, it&#39;s a spiral of despair. You&#39;ll spend money on tokens and time to goal with nothing to show for it.</li></ol><p>So I think it&#39;s half and half. I couldn&#39;t get vibe-coding to work for me before is because half of it is a skill issue, and half of it is that LLMs aren&#39;t very good at the domain I&#39;m working in, and need a lot of scaffolding to get descent results on a task with very limited scope.</p><p>Finally, I found a thoughtful piece this prompting stuff, and why career developers have such a disparate experience with vibe-coding.: <a href="https://ferd.ca/the-gap-through-which-we-praise-the-machine.html?ref=interjectedfuture.com" rel="noreferrer">The gap through which we praise the machine</a>. </p><hr/><p>I&#39;ve not been reading as much since I&#39;ve just been focused on the DBSP implementation. But no worries, I have a backlog.</p><ul><li><a href="https://ferd.ca/the-gap-through-which-we-praise-the-machine.html?ref=interjectedfuture.com">The Gap Through Which We Praise the Machine</a> #[[Prompt Engineering]]</li><li><a href="https://every.to/chain-of-thought/how-to-figure-out-what-people-want-90fad8a3-fb2e-4a54-b355-44d35b6d3e86?ph_email=iamwil%40duck.com&amp;ref=interjectedfuture.com">How to Figure Out What People Want</a> #[[Customer Dev]] #[[Buying Decisions]]</li><li><a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude?ref=interjectedfuture.com">Field Notes From Shipping Real Code With Claude - diwank&#39;s space</a> #[[Prompt Engineering]]</li><li><a href="https://github.com/anthropics/prompt-eng-interactive-tutorial?ref=interjectedfuture.com">anthropics/prompt-eng-interactive-tutorial: Anthropic&#39;s Interactive Prompt Engineering Tutorial</a> #[[Prompt Engineering]]</li><li><a href="https://www.youtube.com/watch?v=lLFCtg4DqrE&amp;t=2765s&amp;ref=interjectedfuture.com">What is a Witness-Like? - YouTube</a> #[[Game Design]]</li><li><a href="https://www.mattkeeter.com/blog/2025-05-14-gradients/?ref=interjectedfuture.com">Gradients are the new intervals</a> #[[Implicit Surfaces]] #[[Matt Keeter]]</li><li><a href="https://jhellerstein.github.io/blog/crdt-dont-read/?ref=interjectedfuture.com">CRDTs #3: Do Not Read! | Async Stream</a> #[[CRDT]] #[[Joseph Hellerstein]]</li><li><a href="https://www.youtube.com/watch?v=IhNSINolcSM&amp;t=5s&amp;ref=interjectedfuture.com">Design Pressure: The Invisible Hand That Shapes Your Code - Hynek Schlawack - YouTube</a></li><li><a href="https://www.youtube.com/watch?v=AF3XJT9YKpM&amp;ref=interjectedfuture.com">Prof. Judy Fan: Cognitive Tools for Making the Invisible Visible - YouTube</a> #[[Visual Programming]]</li></ul>
        </section>

    </article>


</div></div>
  </body>
</html>
