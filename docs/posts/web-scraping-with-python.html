<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.scrapingbee.com/blog/web-scraping-101-with-python/">Original</a>
    <h1>Web Scraping with Python</h1>
    
    <div id="readability-page-1" class="page"><div>
            
                <h2 id="introduction">Introduction:</h2>
<p>In this post, which can be read as a follow-up to our guide about <a href="https://www.scrapingbee.com/blog/web-scraping-without-getting-blocked/">web scraping without getting blocked</a>, we will cover almost all of the tools Python offers to scrape the web. We will go from the basic to advanced ones, covering the pros and cons of each. Of course, we won&#39;t be able to cover every aspect of every tool we discuss, but this post should give you a good idea of what each tool does, and when to use one.</p>
<p><em>Note: When I talk about Python in this blog post you should assume that I talk about Python3.</em></p>
<h2 id="0-web-fundamentals">0. Web Fundamentals</h2>
<p>The Internet is <strong>complex</strong>: there are many underlying technologies and concepts involved to view a simple web page in your browser. The goal of this article is not to go into excruciating detail on every single of those aspects, but to provide you with the most important parts for extracting data from the web with Python.</p>
<h3 id="hypertext-transfer-protocol">HyperText Transfer Protocol</h3>
<p>HyperText Transfer Protocol (HTTP) uses a <strong>client/server</strong> model. An HTTP client (a browser, your Python program, cURL, libraries such as Requests...) opens a connection and sends a message (“I want to see that page : /product”) to an HTTP server (Nginx, Apache...). Then the server answers with a response (the HTML code for example) and closes the connection.</p>
<blockquote>
<p>HTTP is called a <em><strong>stateless protocol</strong></em> because each transaction (request/response) is independent. FTP, for example, is stateful because it maintains the connection.</p>
</blockquote>
<p>Basically, when you type a website address in your browser, the HTTP request looks like this:</p>
<pre tabindex="0"><code>GET /product/ HTTP/1.1
Host: example.com
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Accept-Encoding: gzip, deflate, sdch, br
Connection: keep-alive
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36
</code></pre><p>In the first line of this request, you can see the following:</p>
<ul>
<li>The <strong>HTTP method</strong> or verb. In our case <code>GET</code>, indicating that we would like to fetch data. There are quite a few other HTTP methods available as (e.g. for uploading data) and a full list is available <a href="https://www.w3schools.com/tags/ref_httpmethods.asp">here</a>.</li>
<li>The <strong>path of the file, directory, or object</strong> we would like to interact with. In the case here the directory <code>product</code> right beneath the root directory.</li>
<li>The <strong>version of the HTTP</strong> protocol. In this tutorial we will focus on HTTP 1.</li>
<li>Multiple <strong>headers fields</strong>: Connection, User-Agent... Here is an exhaustive list of <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers">HTTP headers</a></li>
</ul>
<p>Here are the most important header fields :</p>
<ul>
<li><strong>Host:</strong> This header indicates the hostname for which you are sending the request. This header is particularly important for name-based <a href="https://en.wikipedia.org/wiki/Virtual_hosting#Name-based">virtual hosting</a>, which is the standard in today&#39;s hosting world.</li>
<li><strong>User-Agent:</strong> This contains information about the client originating the request, including the OS. In this case, it is my web browser (Chrome) on macOS. This header is important because it is either used for statistics (how many users visit my website on mobile vs desktop) or to prevent violations by bots. Because these headers are sent by the clients, they can be modified (<em>“Header Spoofing”</em>). This is exactly what we will do with our scrapers - <strong>make our scrapers look like a regular web browser</strong>.</li>
<li><strong>Accept:</strong> This is a list of <a href="https://en.wikipedia.org/wiki/Media_type">MIME types</a>, which the client will accept as response from the server. There are lots of different content types and sub-types: <strong>text/plain, text/html, image/jpeg, application/json</strong> ...</li>
<li><strong>Cookie</strong> : This header field contains a list of name-value pairs (name1=value1;name2=value2). Cookies are one way how websites can store data on your machine. This could be either up to a certain date of expiration (standard cookies) or only temporarily until you close your browser (session cookies). Cookies are used for a number of different purposes, ranging from authentication information, to user preferences, to more nefarious things such as user-tracking with personalised, unique user identifiers. However, they are a <strong>vital browser feature</strong> for mentioned authentication. When you submit a login form, the server will verify your credentials and, if you provided a valid login, issue a session cookie, which clearly identifies the user session for your particular user account. Your browser will receive that cookie and will pass it along with all subsequent requests.</li>
<li><strong>Referer</strong>: The referrer header (please note <a href="https://en.wikipedia.org/wiki/HTTP_referer#Etymology">the typo</a>) contains the URL from which the actual URL has been requested. This header is important because websites use this header to change their behavior based on where the user came from. For example, lots of news websites have a paying subscription and let you view only 10% of a post, but if the user comes from a news aggregator like Reddit, they let you view the full content. They use the referrer to check this. Sometimes we will have to spoof this header to get to the content we want to extract.</li>
</ul>
<p>And the list goes on...you can find the full header list <a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">here</a>.</p>
<p>A server will respond with something like this:</p>
<pre tabindex="0"><code>HTTP/1.1 200 OK
Server: nginx/1.4.6 (Ubuntu)
Content-Type: text/html; charset=utf-8
Content-Length: 3352

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&#34;utf-8&#34; /&gt; ...[HTML CODE]
</code></pre><p>On the first line, we have a new piece of information, the HTTP code <code>200 OK</code>. A code of 200 means the request was properly handled. You can find a full list of all available codes on <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">Wikipedia</a>. Following the status line, you have the response headers, which serve the same purpose as the request headers we just discussed. After the response headers, you will have a blank line, followed by the actual data sent with this response.</p>
<p>Once your browser received that response, it will parse the HTML code, fetch all embedded assets (JavaScript and CSS files, images, videos), and render the result into the main window.</p>
<p>We will go through the different ways of performing HTTP requests with Python and extract the data we want from the responses.</p>
<h2 id="1-manually-opening-a-socket-and-sending-the-http-request">1. Manually Opening a Socket and Sending the HTTP Request</h2>
<h3 id="socket">Socket</h3>
<p>The most basic way to perform an HTTP request in Python is to open a <a href="https://docs.python.org/3/howto/sockets.html">TCP socket</a> and manually send the HTTP request.</p>
<div><pre tabindex="0"><code data-lang="python"><span>import</span> socket

HOST <span>=</span> <span>&#39;www.google.com&#39;</span>  <span># Server hostname or IP address</span>
PORT <span>=</span> <span>80</span>                <span># The standard port for HTTP is 80, for HTTPS it is 443</span>

client_socket <span>=</span> socket<span>.</span>socket(socket<span>.</span>AF_INET, socket<span>.</span>SOCK_STREAM)
server_address <span>=</span> (HOST, PORT)
client_socket<span>.</span>connect(server_address)

request_header <span>=</span> <span>b</span><span>&#39;GET / HTTP/1.0</span><span>\r\n</span><span>Host: www.google.com</span><span>\r\n\r\n</span><span>&#39;</span>
client_socket<span>.</span>sendall(request_header)

response <span>=</span> <span>&#39;&#39;</span>
<span>while</span> <span>True</span>:
    recv <span>=</span> client_socket<span>.</span>recv(<span>1024</span>)
    <span>if</span> <span>not</span> recv:
        <span>break</span>
    response <span>+=</span> str(recv)

print(response)
client_socket<span>.</span>close()  
</code></pre></div><p>Now that we have the HTTP response, the most basic way to extract data from it is to use regular expressions.</p>
<h3 id="regular-expressions">Regular Expressions</h3>
<p>Regular expressions (or also regex) are an extremely versatile tool for handling, parsing, and validating arbitrary text. A regular expression is essentially a string which defines a search pattern using a standard syntax. For example, you could quickly identify all phone numbers in a web page.</p>
<p>Combined with classic <em>search and replace</em>, regular expressions also allow you to perform string substitution on dynamic strings in a relatively straightforward fashion. The easiest example, in a web scraping context, may be to replace uppercase tags in a poorly formatted HTML document with the proper lowercase counterparts.</p>
<p>You may be, now, wondering why it is important to understand regular expressions when doing web scraping. That&#39;s a fair question and after all, there are many different Python modules to parse HTML, with XPath and CSS selectors.</p>
<p>In an ideal <a href="https://en.wikipedia.org/wiki/Semantic_Web">semantic world,</a> data is easily machine-readable, and the information is embedded inside relevant HTML elements, with meaningful attributes. But the real world is messy. You will often find huge amounts of text inside a <code>&lt;p&gt;</code> element. For example, if you want to extract specific data inside a large text (a price, a date, a name...), you will have to use regular expressions.</p>
<blockquote>
<p><strong>Note:</strong> Here is a great website to test your regex: <a href="https://regex101.com/">https://regex101.com/</a>. Also, here is an <a href="https://www.rexegg.com/">awesome blog</a> to learn more about them. This post will only cover a small fraction of what you can do with regex.</p>
</blockquote>
<p>Regular expressions can be useful when you have this kind of data:</p>
<p>We could select this text node with an XPath expression and then use this kind of regex to extract the price:</p>
<pre tabindex="0"><code>^Price\s*:\s*(\d+\.\d{2})\$
</code></pre><p>If you only have the HTML, it is a bit trickier, but not all that much more after all. You can simply specify in your expression the tag as well and then use a capturing group for the text.</p>
<div><pre tabindex="0"><code data-lang="python"><span>import</span> re

html_content <span>=</span> <span>&#39;&lt;p&gt;Price : 19.99$&lt;/p&gt;&#39;</span>

m <span>=</span> re<span>.</span>match(<span>&#39;&lt;p&gt;(.+)&lt;\/p&gt;&#39;</span>, html_content)
<span>if</span> m:
	print(m<span>.</span>group(<span>1</span>))
</code></pre></div><p>As you can see, manually sending the HTTP request with a socket and parsing the response with regular expression can be done, but it&#39;s complicated and there are higher-level API that can make this task easier.</p>
<h2 id="2-urllib3--lxml">2. urllib3 &amp; LXML</h2>
<p><strong>Disclaimer</strong>: It is easy to get lost in the urllib universe in Python. The standard library contains urllib and urllib2 (and sometimes urllib3). In Python3 urllib2 was split into multiple modules and urllib3 won&#39;t be part of the standard library anytime soon. This confusing situation will be the subject of another blog post. In this section, I&#39;ve decided to only talk about urllib3 because it is widely used in the Python world, including by Pip and Requests.</p>
<p>Urllib3 is a high-level package that allows you to do pretty much whatever you want with an HTTP request. With urllib3, we could do what we did in the previous section with way fewer lines of code.</p>
<div><pre tabindex="0"><code data-lang="python"><span>import</span> urllib3
http <span>=</span> urllib3<span>.</span>PoolManager()
r <span>=</span> http<span>.</span>request(<span>&#39;GET&#39;</span>, <span>&#39;http://www.google.com&#39;</span>)
print(r<span>.</span>data)
</code></pre></div><p>As you can see, this is much more concise than the socket version. Not only that, the API is straightforward. Also, you can easily do many other things, like adding HTTP headers, using a proxy, POSTing forms ...</p>
<p>For example, had we decided to set some headers and use a proxy, we would only have to do the following (you can learn more about proxy servers at <a href="https://www.bestproxyreviews.com/proxy-server/">bestproxyreviews.com</a>):</p>
<div><pre tabindex="0"><code data-lang="python"><span>import</span> urllib3
user_agent_header <span>=</span> urllib3<span>.</span>make_headers(user_agent<span>=</span><span>&#34;&lt;USER AGENT&gt;&#34;</span>)
pool <span>=</span> urllib3<span>.</span>ProxyManager(<span>f</span><span>&#39;&lt;PROXY IP&gt;&#39;</span>, headers<span>=</span>user_agent_header)
r <span>=</span> pool<span>.</span>request(<span>&#39;GET&#39;</span>, <span>&#39;https://www.google.com/&#39;</span>)
</code></pre></div><p>See? There are exactly the same number of lines. However, there are some things that urllib3 does not handle very easily. For example, if we want to add a cookie, we have to manually create the corresponding headers and add it to the request.</p>
<p>There are also things that urllib3 can do that Requests can&#39;t: creation and management of a pool and proxy pool, as well as managing the retry strategy, for example.</p>
<p>To put it simply, urllib3 is between Requests and Socket in terms of abstraction, although it&#39;s way closer to Requests than Socket.</p>
<p>Next, to parse the response, we are going to use the LXML package and XPath expressions.</p>
<h3 id="xpath">XPath</h3>
<p>XPath is a technology that uses path expressions to select nodes or node-sets in an XML document (or HTML document). If you are familiar with the concept of CSS selectors, then you can imagine it as something relatively similar.</p>
<p>As with the Document Object Model, XPath has been a W3C standard since 1999. Although XPath is not a programming language in itself, it allows you to write expressions that can directly access a specific node, or a specific node-set, without having to go through the entire HTML tree (or XML tree).</p>
<p>To extract data from an HTML document with XPath we need three things:</p>
<ul>
<li>an HTML document</li>
<li>some XPath expressions</li>
<li>an XPath engine that will run those expressions</li>
</ul>
<p>To begin, we will use the HTML we got from urllib3. And now we would like to extract all of the links from the Google homepage. So, we will use one simple XPath expression, <code>//a</code>, and we will use LXML to run it. LXML is a fast and easy to use XML and HTML processing library that supports XPath.</p>
<p><em>Installation</em>:</p>
<p>Below is the code that comes just after the previous snippet:</p>
<div><pre tabindex="0"><code data-lang="python"><span>from</span> lxml <span>import</span> html

<span># We reuse the response from urllib3</span>
data_string <span>=</span> r<span>.</span>data<span>.</span>decode(<span>&#39;utf-8&#39;</span>, errors<span>=</span><span>&#39;ignore&#39;</span>)

<span># We instantiate a tree object from the HTML</span>
tree <span>=</span> html<span>.</span>fromstring(data_string)

<span># We run the XPath against this HTML</span>
<span># This returns an array of element</span>
links <span>=</span> tree<span>.</span>xpath(<span>&#39;//a&#39;</span>)

<span>for</span> link <span>in</span> links:
    <span># For each element we can easily get back the URL</span>
    print(link<span>.</span>get(<span>&#39;href&#39;</span>))
</code></pre></div><p>And the output should look like this:</p>
<div><pre tabindex="0"><code data-lang="python">https:<span>//</span>books<span>.</span>google<span>.</span>fr<span>/</span>bkshp<span>?</span>hl<span>=</span>fr<span>&amp;</span>tab<span>=</span>wp
https:<span>//</span>www<span>.</span>google<span>.</span>fr<span>/</span>shopping<span>?</span>hl<span>=</span>fr<span>&amp;</span>source<span>=</span>og<span>&amp;</span>tab<span>=</span>wf
https:<span>//</span>www<span>.</span>blogger<span>.</span>com<span>/</span><span>?</span>tab<span>=</span>wj
https:<span>//</span>photos<span>.</span>google<span>.</span>com<span>/</span><span>?</span>tab<span>=</span>wq<span>&amp;</span>pageId<span>=</span>none
http:<span>//</span>video<span>.</span>google<span>.</span>fr<span>/</span><span>?</span>hl<span>=</span>fr<span>&amp;</span>tab<span>=</span>wv
https:<span>//</span>docs<span>.</span>google<span>.</span>com<span>/</span>document<span>/</span><span>?</span>usp<span>=</span>docs_alc
<span>...</span>
https:<span>//</span>www<span>.</span>google<span>.</span>fr<span>/</span>intl<span>/</span>fr<span>/</span>about<span>/</span>products<span>?</span>tab<span>=</span>wh
</code></pre></div><p>Keep in mind that this example is really really simple and doesn&#39;t show you how powerful XPath can be (Note: we could have also used <code>//a/@href</code>, to point straight to the <code>href</code> attribute). If you want to learn more about XPath, you can read <a href="https://librarycarpentry.org/lc-webscraping/02-xpath/index.html">this helpful introduction</a>. The LXML documentation is also <a href="https://lxml.de/tutorial.html">well-written and is a good starting point</a>.</p>
<p>XPath expressions, like regular expressions, are powerful and one of the fastest way to extract information from HTML. And like regular expressions, XPath can quickly become messy, hard to read, and hard to maintain.</p>
<p>If you&#39;d like to learn more about XPath, do not hesitate to read my dedicated blog post about <a href="https://www.scrapingbee.com/blog/practical-xpath-for-web-scraping/">XPath applied to web scraping</a>.</p><p>
toto
</p><h2 id="3-requests--beautifulsoup">3. Requests &amp; BeautifulSoup</h2>
<h3 id="requests">Requests</h3>
<p><em><a href="https://docs.python-requests.org">Requests</a></em> is the king of Python packages. With more than 11,000,000 downloads, it is the most widely used package for Python.</p>
<p>Installation:</p>
<p>Making a request with - pun intended - <em>Requests</em> is easy:</p>
<div><pre tabindex="0"><code data-lang="python"><span>import</span> requests

r <span>=</span> requests<span>.</span>get(<span>&#39;https://www.scrapingninja.co&#39;</span>)
print(r<span>.</span>text)
</code></pre></div><p>With Requests, it is easy to perform POST requests, handle cookies, query parameters... You can also <a href="https://www.scrapingbee.com/blog/download-image-python/">download images with Requests</a>.</p>
<p>On the following page, you will learn to <a href="https://www.scrapingbee.com/blog/python-requests-proxy/">use Requests with proxies</a>. This is almost mandatory for scraping the web at scale.</p>
<p><strong>Authentication to Hacker News</strong></p>
<p>Let&#39;s say we want to create a tool to automatically submit our blog post to Hacker news or any other forum, like Buffer. We would need to authenticate on those websites before posting our link. That&#39;s what we are going to do with Requests and BeautifulSoup!</p>
<p>Here is the Hacker News login form and the associated DOM:</p>






















<div>
    <svg width="1812" height="1476" aria-hidden="true" style="background-color:white"></svg>
    <p><img data-sizes="auto" data-srcset="
    
      , /blog/web-scraping-101-with-python/screenshot_hn_login_form_hu473e87119dfc02f5ff573cd876eebf05_248908_825x0_resize_catmullrom_3.png 825w
    
    
      , /blog/web-scraping-101-with-python/screenshot_hn_login_form_hu473e87119dfc02f5ff573cd876eebf05_248908_1200x0_resize_catmullrom_3.png 1200w
    
    
      , /blog/web-scraping-101-with-python/screenshot_hn_login_form_hu473e87119dfc02f5ff573cd876eebf05_248908_1500x0_resize_catmullrom_3.png 1500w 
    " data-src="/blog/web-scraping-101-with-python/screenshot_hn_login_form.png" width="1812" height="1476" alt="Hacker News login form"/>
    
</p></div>

</div></div>
  </body>
</html>
