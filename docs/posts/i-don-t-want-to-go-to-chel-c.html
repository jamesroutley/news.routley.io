<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://applied-langua.ge/posts/i-dont-want-to-go-to-chel-c.html">Original</a>
    <h1>I don&#39;t want to go to Chel-C</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
<header>

</header><nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>

</nav>
<p>
We are constantly reading of stories with one main theme where people
turn to seemingly &#34;simple&#34; systems in order to manage complexity;
except that these systems manage to produce complex problems, without
having to acquire historical cruft, or suffering from any of the usual
issues that produce non-essential complexity. There is a trend in
programming language design, and a particular language that we will
focus on, that are both seemingly gaining popularity today, for their
minimalism and supposed simplicity. However, neither is awfully simple
in practise; it appears there are good reasons why, and we will give
some reasons at the end of this article.
</p>

<div id="outline-container-org334c23c">
<h2 id="org334c23c">Barbarism begins at <code>$HOME</code></h2>
<div id="text-org334c23c">
<p>
In <a href="https://www.youtube.com/watch?v=ZSRHeXYDLko">a DevGAMM</a> presentation, Jon Blow managed to convince himself and
other game developers that high level languages are going to cause the
&#34;collapse of civilisation&#34; due to a loss of some sort of &#34;capability&#34;
to do low-level programming, and that abstraction will make people
forget how to do things. We shall start off with a description of the
alternative by Charles Antony Richard Hoare, and then work our way
through some arguments we have heard against that line of thought:
</p>

<blockquote>
<p>
We asked our customers whether they wished us to provide an option to
switch off these checks in the interests of efficiency on production
runs. Unanimously, they urged us not to - they already knew how
frequently subscript errors occur on production runs where failure to
detect them could be disastrous. I note with fear and horror that even
in 1980, language designers and users have not learned this lesson. In
any respectable branch of engineering, failure to observe such
elementary precautions would have long been against the
law.<sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup>
</p>
</blockquote>

<p>
We note with fear and horror that even in 2022, language designers
and users have not learned this lesson.
</p>

<p>
While Jon argues that safety features are useless to &#34;professional
programmers&#34;<sup><a id="fnr.2" href="#fn.2" role="doc-backlink">2</a></sup> or &#34;programmers that know what they&#39;re
doing&#34;, and programmers are &#34;afraid of pointers&#34; or such, we derive
our fear from how even <i>the most talented</i> computer scientists that we
know make mistakes. Indeed the CWE team claims that &#34;out-of-bounds
writes&#34; and &#34;out-of-bounds reads&#34; are the first and third most
dangerous software weaknesses, respectively, and &#34;use after free&#34; is
the seventh most dangerous.<sup><a id="fnr.3" href="#fn.3" role="doc-backlink">3</a></sup> Jon must have found other company,
who manage to never write buggy software, and do it without any
techniques for systematically avoiding bugs. He mentions the design of
highly reliable software systems, but merely writing perfect software
is not enough, at the level of reliability he discusses.
</p>
</div>

<div id="outline-container-org5a52fe4">
<h3 id="org5a52fe4">Expanding redundancy</h3>
<div id="text-org5a52fe4">
<p>
Good physical engineering crucially relies on <i>redundancy</i>, which is
the property of multiple ways to ensure that something doesn&#39;t go
wrong. Such redundancy protects against expected and unexpected forms
of failure. Software engineering also relies on introducing
redundancy,<sup><a id="fnr.4" href="#fn.4" role="doc-backlink">4</a></sup> and redundancy may be achieved with static
analyses such as static type checking and theorem proving, runtime
checks such as array bounds<sup><a id="fnr.5" href="#fn.5" role="doc-backlink">5</a></sup> and dynamic type
checks, and system design with <i>fallback</i> and <i>replicated</i> subsystems.
</p>

<p>
Highly reliable software systems are possible, but they require those
forms of redundancy that Jon argues are unnecessary for &#34;real
programmers&#34;. In particular, such systems require fault <i>isolation</i>,
which can only be achieved by pervasive runtime checks, as not
checking allows faults to propagate uncontrollably. While Jon insists
that defects should never exist, Joe Armstrong insists in his infamous
thesis, with some humility, that &#34;from the outset, we know that faults
will occur&#34;.<sup><a id="fnr.6" href="#fn.6" role="doc-backlink">6</a></sup> Even if all programmers were perfect, the
latter mindset also allows for minimising the effects of hardware
faults; while Jon is aware that physics influences what sort of
programs can be run fast, ignoring hardware faults is ignoring that
physics dictates how a program run <i>at all</i>; programs are run on real
electronic circuits, which can have faults, can lose connections and
power, and so on. Even formal reasoning about programs cannot (easily)
model random bit-flips in defective memory chips, but fault isolation
and redundancy can attempt to work around it. These sorts of issues,
while occuring with low probability, do appear in redundant systems
which are around long enough to encounter many issues;<sup><a id="fnr.7" href="#fn.7" role="doc-backlink">7</a></sup> thus
high reliability systems do require more than simply not making
mistakes while programming.
</p>

<p>
However, there appear to be reasons to avoid such forms of
redundancy. We have heard that runtime checks are prohibitively slow,
but modern processors with <i>branch prediction</i> can easily predict that
errors will not occur. Furthermore, the cost of the system running
into an unexpected state is drastically worse than the loss of
efficiency in many situations (especially when reliability is such a great
concern), and thus detecting an error is invaluable in comparison. We
have similarly heard that safe languages impair writing some algorithms,
but we never heard just <i>which</i> algorithms those were, and we never
encountered any algorithms which require using unsafe code ourselves.
Furthermore, any programs using such algorithms would rarely be valid
programs written in those languages, either, as unsafe code typically invokes
some sort of <i>undefined behaviour</i> in the language. Not invoking UB does
not necessarily produce a correct or reliable program, but a program
invoking UB is <i>never</i> correct, as it has no defined behaviour to consider
&#34;correct&#34;.
</p>

<p>
There is also the argument that these issues only occur with &#34;complex&#34;
code, and &#34;simple&#34; code can be shown to not have out-of-bounds
accesses. However, the reasoning applied, on average, is incredibly
sloppy and should not be confused with any vigorous formal analysis of
the code, which would be substantially more difficult. For example,
the specification framework for the C language by Andronick et
al<sup><a id="fnr.8" href="#fn.8" role="doc-backlink">8</a></sup> does not support &#34;references to local variables,
goto statements, expressions with uncontrolled side-effects, switch
statements using fall-through, unions, floating point arithmetic, or
calls to function pointers&#34;. If someone does not want to use a
language which provides for bounds checks, due to a lack of some sort
of &#34;capability&#34;, then we cannot imagine that they will want to use any
subset of C that can be formally modelled!
</p>

<p>
A notion of code complexity is also influenced by the choice of
language for that code; the C language does not provide for bounds or
pointer checks, so introducing explicit checks will necessarily result
in more code and more control flow paths, and code without these
checks will necessarily appear to be simpler.<sup><a id="fnr.9" href="#fn.9" role="doc-backlink">9</a></sup>
Thus the idea of &#34;simple&#34; C code not requiring redundancy requires
circular reasoning; the only simple code that can be written in C does
not have any redundancy. Though, indeed, the idea was false to start
with. The combination of how formal reasoning for even a subset of C
is difficult, and the burden being put on the programmer to explicitly
insert runtime checks, leads to &#34;simple&#34; C code ironically being very
undesirable.
</p>
</div>
</div>

<div id="outline-container-org4c98684">
<h3 id="org4c98684">Abstract reasoning about abstractions</h3>
<div id="text-org4c98684">
<p>
Using abstractions extensively actively encourages study into how to
implement them efficiently, contrary to Blow&#39;s claim that their use
will lead to few understanding their implementation. The
implementation of higher level languages rather provides a context in
which better study of how to optimise for a given computer is possible.
A programmer could indeed break abstractions, and write low-level code
themselves, to generate the best-performing code that they can think
of. The programmer would then be effectively running the perfect
optimising compiler in their head. But if a programmer is instead
motivated to stick to these higher level constructs, they have to
express their mental model precisely, in the form of code for the
compiler to use. The latter requires deeper thought, as the programmer
has to formalise and &#34;explain&#34; their knowledge, in a way similar to
&#34;learning by teaching&#34;. Hence abstractions encourage <i>deeper thought</i>
into how to get consistent performance out of high-level code, rather
than one-off low-level trickery.
</p>

<p>
One practical example of optimising such abstractions is the
implementation of <i>dynamic dispatch</i> in object-oriented programming
languages. The C++ programming language offers dynamic dispatch, but
it is almost implemented with a &#34;virtual table&#34; lookup. If the lookup
is too slow, a programmer may avoid dynamic dispatch and instead use
regular functions to regain performance. However, this is not an
option for implementations of the Smalltalk programming language,
where every method call requires dynamic dispatch. Thus
high-performance Smalltalk systems make use of inline
caching.<sup><a id="fnr.10" href="#fn.10" role="doc-backlink">10</a></sup> The Self programming language additionally
uses method dispatch for &#34;instance&#34; or &#34;local&#34; variables, as they are
only exposed via reader and writer methods, demanding the better
technique of polymorphic inline caching.<sup><a id="fnr.11" href="#fn.11" role="doc-backlink">11</a></sup> The general trend is
that if an abstraction offers appealing principles, attempting to
follow the principle consistently requires investigating how to make
the abstraction <i>convenient</i>, as inconvenience will cause someone to
abandon the abstraction; and the performance of an abstraction can be
a crucial part of its convenience to a programmer.
</p>

<p>
Another property of abstractions is that improving the performance of
abstractions improves performance for all users of the
abstraction. Having such abstractions reduces the amount of code that
needs to be written, producing simpler code, and allows for the
implementor to fine-tune implementations for maximum performance on
every machine they target, rather than every user doing it themselves
for every program. As Robert Bernecky states on his time implementing
APL systems:
</p>

<blockquote>
<p>
In the late 1970’s, I was manager of the APL development department at
I.P. Sharp Associates Limited. A number of users of our system were
concerned about the performance of the <code>∨.∧</code> inner product [for
transitive closures] on large Boolean arrays in graph computations. I
realized that a permuted loop order would permit vectorization of the
Boolean calculations, even on a non-vector machine. David Allen
implemented the algorithm and obtained a thousand-fold speedup factor
on the problem. This made all Boolean matrix products immediately
practical in APL, and our user (and many others) went away very happy.
</p>

<p>
What made things even better was that the work had benefit for all
inner products, not just the Boolean ones. The standard <code>+.×</code> [matrix
multiplication] now ran 2.5—3 times faster than Fortran. […] So,
rather than merely speeding up one library subroutine, we sped up a
whole family of hundreds of such routines (even those that had never
been used yet!), with no more effort than would have been required for
one.<sup><a id="fnr.12" href="#fn.12" role="doc-backlink">12</a></sup>
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org1b6d7d1">
<h3 id="org1b6d7d1">Conclusion</h3>
<div id="text-org1b6d7d1">
<blockquote>
<p>
You&#39;re quite right. C is memory unsafe, and the large quantity of C code
in IoT devices is, as you rightly say, a disaster - only it has already
happened; no waiting is involved.<sup><a id="fnr.13" href="#fn.13" role="doc-backlink">13</a></sup>
</p>
</blockquote>

<p>
The supposed collapse of civilisation due to bad software is already
here, and Mr. Blow doesn&#39;t want you to know why. Even great
programmers still use redundancy measures to allow them to make
mistakes without causing harm; we&#39;d even say that the greatest
programmers are so because of how they produce redundancy. The
persistent use of redundancy and abstraction encourages research into
how to make it efficient, which is quite the opposite of forgetting
how any low-level trickery is performed.
</p>

<p>
As Henry Baker reminded us, there are many contexts &#34;in which bad
programming style can kill people&#34;.<sup><a id="fnr.9.100" href="#fn.9" role="doc-backlink">9</a></sup> We can
only hope that people figure out what to blame, before the idiotic
view of producing high-quality software that Blow et al promote
eventually <i>does</i> kill people.
</p>
</div>
</div>
</div>

<div id="outline-container-org12582b4">
<h2 id="org12582b4">The <i>uxn</i> disaster: how to make a 9000 MIPS desktop run like a 100MHz Pentium 1</h2>
<div id="text-org12582b4">
<p>
<a href="https://wiki.xxiivv.com/site/uxn.html">The <i>uxn</i> machine</a> is a stack machine with byte-addressed memory, and
is programmed in <i>uxntal</i>, an assembler for the machine. It is claimed
this assembler is like Forth, but it is not interactive, nor it have
the ability to define new <i>immediate</i> words; calling and returning are
explicit instructions. The uxntal language is merely an assembler for
a stack machine.
</p>

<p>
The author insists on a relation to <i>permacomputing</i>. <a href="https://wiki.xxiivv.com/site/permacomputing.html">Their page on
permacomputing</a> describes <i>frugal computing</i> and <i>salvage computing</i> as
principles of permacomputing, defining them as &#34;utilizing
computational resources as finite and precious, to be utilised only
when necessary, and as effectively as possible&#34;, and &#34;utilizing only
already available computational resources, to be limited by that which
is already produced.&#34; The author is part of a collective that <a href="https://100r.co/site/story.html">wanted
to replace all the &#34;bloated&#34; software they used</a>, due to having little
energy storage on their sailboat. Using software design techniques to
reduce power usage, and to allow continued use of old computers is a
good idea,<sup><a id="fnr.14" href="#fn.14" role="doc-backlink">14</a></sup> but the <i>uxn</i> machine has quite the opposite
effect, due to inefficient implementations and a poorly designed
virtual machine, which does not lend itself to writing an efficient
implementation easily.
</p>

<p>
Our remarks are similar to those by Michal Jirků and Stanislav
Datskovskiy on the Urbit platform.<sup><a id="fnr.15" href="#fn.15" role="doc-backlink">15</a></sup><sup>, </sup><sup><a id="fnr.16" href="#fn.16" role="doc-backlink">16</a></sup> Interestingly,
both platforms are posited as being <a href="https://wiki.xxiivv.com/site/varvara.html">&#34;clean-slate computing stacks&#34;</a>,
though they are both hosted on some other stack; typically with
an implementation written in C on a Unix-like system. The uxn platform
has been ported to other non-Unix-like systems, but it is still not
self-hosting, which has been <a href="https://merveilles.town/@neauoire/107628143935144047">routinely ignored</a> as a part of bootstrapping.
Thus uxn does not really escape mainstream computing stacks, much like
the Urbit VM.
</p>
</div>

<div id="outline-container-org14bea5a">
<h3 id="org14bea5a">Implementation strategy</h3>
<div id="text-org14bea5a">
<p>
The most significant performance issue is that all uxn implementations
we have found use naïve interpretation. A typical estimation is that
an interpreter is between one and two magnitudes slower than a good
compiler. We instead propose the use of <i>dynamic translation</i> to
generate native code, which should eliminate the overhead associated
with interpreting instructions.
</p>

<p>
The use of dynamic translation to improve performance is not new; its
use for hardware emulation is at least as old as emulation of the
HP 3000<sup><a id="fnr.17" href="#fn.17" role="doc-backlink">17</a></sup> and emulation of the VAX on Alpha machines,<sup><a id="fnr.18" href="#fn.18" role="doc-backlink">18</a></sup>
and its use for dynamic languages (for which static approaches alone
do not improve much) is at least as old as the APL\3000 implementation
of APL.<sup><a id="fnr.19" href="#fn.19" role="doc-backlink">19</a></sup> A translator can be as sophisticated as an optimising
compiler (and indeed can use an optimising compiler backend, such as
LLVM), but a simple translator can also suffice in avoiding overhead due
to interpretation.
</p>

<p>
To demonstrate, we will compare a C program compiled to x86-64 code, a
translation of the program running under a uxn interpreter, and the
program translated naïvely to x86-64 assembler. It may seem odd to
compare to a C program, after stating our thoughts on the C language,
but the C and uxn languages both use fixed-size integers and are both
unsafe (as we will discuss later), so the performance should only
differ due to implementation strategy.
</p>

<p>
We test with a recursive function that computes the 32nd Fibonacci
number.
</p>

<div>
<p><label><span>Listing 1: </span>Computing Fibonacci numbers in C</label></p><pre>




<span>unsigned</span> <span>int</span> <span>fib</span>(<span>unsigned</span> <span>int</span> <span>n</span>) {
  <span>if</span> (n &lt; 2) <span>return</span> n;
  <span>return</span> fib(n - 1) + fib(n - 2);
}

<span>int</span> <span>main</span>() {
  <span>unsigned</span> <span>int</span> <span>sum</span> = 0;
  <span>for</span> (<span>int</span> <span>x</span> = 0; x &lt; 100; x++)
    


    sum += fib(0x20);
  <span>return</span> sum;
}
</pre>
</div>

<div>
<p><label><span>Listing 2: </span>Computing Fibonacci numbers in uxntal</label></p><pre>



%RET { JMP2r }
%DONE { #01 #0f DEO BRK }
|0100

#0020 ;fib JSR2 DONE
@fib 
  
  DUP2 #0001 GTH2 ,&amp;inductive-case; JCN RET
  &amp;inductive-case;
    DUP2 #0001 SUB2 ;fib JSR2 
    SWP2 #0002 SUB2 ;fib JSR2 
  ADD2 RET
</pre>
</div>

<p>
Our translation is done solely with the macro feature of the <code>nasm</code>
assembler. Each uxn instruction is translated to a sequence of x86-64
instructions, with no optimisation between instructions performed. The
translation (including macros) consists of 91 lines of assembly code,
so we have put it in <a href="https://applied-langua.ge/posts/uxn.S">another file</a>. Note that the <a href="https://git.sr.ht/~rabbits/uxn/tree/main/item/build.sh#L97">build script for the
uxn implementation tested</a> defaults to building with the compilation
flag <code>-Os</code> which optimizes for size, so we replaced it with <code>-O2</code> to
optimize for speed, which reduces the time taken to run our program by
about 30%. It is unlikely that any applications benefit from optimizing for
size, as the interpreter loop only uses 6,526 bytes when compiled with
<code>-O2</code>, and should easily fit in L1 instruction cache.
</p>

<p>
We observed that the C program takes 5.8ms to call <code>fib(0x20)</code>, the
uxn virtual machine 537ms, and the translation 82ms. (As per the
title, the program executed at about 9 billion instructions per
second.) In other words, the uxn virtual machine introduces <i>two
magnitudes of time overhead</i>, and a simple translator is successful at
improving throughput by about 6.5 times. However, the translated code
is still 14 times as slow as the output of an optimising compiler.
</p>

<p>
A more sophisticated translator is necessary to reduce the remaining
overhead, but the simplest translator still produces a considerable
speedup. There are some optimisations which are not too hard to write,
nonetheless. For example, mapping some part of the data stack into
registers, for which the arrangement is known at compile-time, merely
requires the translator to maintain a &#34;virtual stack&#34;.<sup><a id="fnr.20" href="#fn.20" role="doc-backlink">20</a></sup> A
colleague also told us that they considered <i>peephole</i> optimisations
&#34;fair game&#34; for a simple translator, as they consist of replacing a
particular sequence of instructions with another sequence, without
performing any sophisticated analysis. The <a href="https://applied-langua.ge/posts/uxn2.S">use of peepholing</a> reduces
runtime to 34ms, which is only 5.9 times slower than an optimising
compiler.
</p>
</div>
</div>

<div id="outline-container-org60fd78e">
<h3 id="org60fd78e">Word sizes</h3>
<div id="text-org60fd78e">
<p>
The careful reader will note that the uxn program, and thus our
translation, use 16-bit arithmetic, and the result overflows an
unsigned 16-bit integer. The uxn virtual machine only exposes 8-bit
and 16-bit instructions, so we cannot perform 32-bit instructions
&#34;natively&#34;. Writing a decent big-integer implementation ourselves
requires some design which we would rather not perform, if
possible. Instead, we can use <a href="https://gist.github.com/non/76da1f19f5f0d819535a3c72618ce9ee">a library</a> for simulated 32-bit
arithmetic. We can modify our definition of the <code>fib</code> function to use
it:
</p>

<div>
<p><label><span>Listing 3: </span>Computing Fibonacci numbers with the 32-bit integer library</label></p><pre>@fib 
  
  DUP2 #0001 GTH2 ,&amp;inductive-case; JCN #0000 SWP RET
  &amp;inductive-case;
    DUP2 #0001 SUB2 ;fib JSR2 
    ROT2 #0002 SUB2 ;fib JSR2 
  ;add32 JSR2 RET
</pre>
</div>

<p>
The resulting program takes 1.95 seconds to run in the &#34;official&#34; uxn
implementation, or 390 times slower than native code! This amount of
overhead greatly restricts what sort of computations can be done at an
acceptable speed on <i>modern</i> computers; and using older computers
would be out of the question.<sup><a id="fnr.21" href="#fn.21" role="doc-backlink">21</a></sup>
</p>

<p>
It could be argued that the use of small operations frees 16-bit
machines from simulating larger arithmetic. But there are few machines
with small arithmetic units; we estimate that most desktop computers
since 1990 can perform most 32-bit operations no slower than 16-bit
operations, and most since 2010 can perform most 64-bit operations no
slower than 32-bit operations.<sup><a id="fnr.22" href="#fn.22" role="doc-backlink">22</a></sup> There is also no end to the
number of 32-bit microcontrollers being manufactured, either; thus
there are not many computers that cannot perform 32-bit arithmetic
without loss of performance.
</p>

<p>
Of course, the size of the arithmetic unit is not the only factor affecting
performance. Larger integers require more bits to store, thus reducing
the effectiveness of cache memory and vectorisation. But it could be
worthwhile to use integers even smaller than 8 bits to conserve cache,
and clever algorithms can effectively manipulate &#34;packs&#34; of such
integers.<sup><a id="fnr.23" href="#fn.23" role="doc-backlink">23</a></sup> Vectorisation is also out of the question,
because there are no compilers for uxn code, let alone <i>vectorising</i>
compilers. We can only conclude that the provided instruction sizes
are arbitrary, and not optimised for performance or portability, yet
they are not suitable for many applications either.
</p>

<p>
While we believe dynamic translation is the best approach to improving
the performance of uxn programs, because we cannot change the
instruction set, we note that one way to reduce interpretation
overhead in a purely interpreted system is to have instructions
perform more work, so that fewer instructions and <i>instruction
dispatches</i> are required. If someone did find a computer for which
arithmetic on integers larger than 16 bits would be slow, emulating
arithmetic in the host language, rather than in uxn, would still be
faster by avoiding interpretation overhead. Such a computer would be
old and slow enough that improving performance would be very
important, in order to have a usable computer. (We believe that
finding such a computer would be very difficult, compared to the
myriad of machines that have been produced more recently, and so
computers that are <i>that</i> old are not worth the effort targeting. But
other people have different ideas of what is worth what effort, so we
still make the point irregardless of our opinion there.)
</p>

<p>
As an example of reducing overhead in this manner, an APL
implementation may perform element-wise addition of two large matrices
after dispatching on just one <code>+</code>; resulting in the time spent in operator
dispatch being negligible. The implementor of such operators can use
whatever algorithms they know are suited for the hardware; the Dyalog
APL implementation has many of these.<sup><a id="fnr.24" href="#fn.24" role="doc-backlink">24</a></sup> However, emulating
larger integers with smaller integers in uxn has the opposite effect;
what could be a single instruction is instead <i>dozens</i> of
instructions, and the resulting interpretation overhead is very large.
</p>
</div>
</div>

<div id="outline-container-org7ae00a3">
<h3 id="org7ae00a3">Redundancy and computing platforms</h3>
<div id="text-org7ae00a3">
<p>
It is also argued that the <i>uxn</i> virtual machine can be simulated with pen
and paper. Following from our discussion of word sizes, humans are
terrible at reasoning about modular arithmetic, which is manifested in
the occurrence of <a href="http://cwe.mitre.org/data/definitions/190.html">integer overflow bugs</a>. The Von Neumann model is also
tricky to work with on paper; the program state includes 64 kibibytes of
memory that the virtual machine may access arbitrarily. A human
computer would have to simulate &#34;paging&#34; and try to only keep memory
that has been used in the computation on paper.
</p>

<p>
We didn&#39;t use 8-bit instructions in our program, but the uxn virtual
machine also allows for using 8-bit instructions and data. Incorrectly
&#34;typed&#34; instructions can also lead to confusing behaviour, producing
an error somewhere in the program far detached from where the
programming error is. Combining 8-bit and 16-bit instructions which
operate the stack leads to further confusion, as the individual bytes
of a 16-bit integer can be manipulated separately; i.e. a 16-bit
integer is not an &#34;atomic&#34; object, and the only atomic objects are
8-bit integers. Rearranging the stack with a mixture of 8-bit and
16-bit data is also more difficult, and requires thinking about a
16-bit value as two 8-bit values. Thus even the abstraction of 16-bit
integers can be defeated, and the reader must continue reasoning at
the level of 8-bit integers, for their reasoning to be sound.
</p>

<p>
The essentially untyped<sup><a id="fnr.25" href="#fn.25" role="doc-backlink">25</a></sup> instruction set and memory also
are an issue for security, and hamper reasoning about programs with
erroneous inputs. For example, C programs compiled to WebAssembly may
have even fewer safety checks than C programs compiled to native
code.<sup><a id="fnr.26" href="#fn.26" role="doc-backlink">26</a></sup> However, security flaws in programs compiled to
WebAssembly may not be as disastrous as, say, security flaws in
programs compiled to native code, because the WebAssembly
implementation acts as a <i>sandbox</i>, and no information outside the
virtual memory of the implementation can be compromised. The uxn
system could also be a sandbox, and prevent damage outside of its
memory, but <a href="https://wiki.xxiivv.com/site/varvara.html#file">a filesystem device</a> is specified for the platform, and is
often implemented, and so a uxn program can destroy information
outside its virtual machine. Any isolation would have to be performed
outside of the virtual machine; thus <i>running a random uxn program is
as insecure as running a random native executable</i>.
</p>

<p>
The lack of redundancy also makes life more complicated for a
translating implementation of uxn, as an implementation must preserve
the semantics of every action performed by the virtual machine,
including those which would typically trip type and bounds checks in
other programming languages. <a href="https://pointersgonewild.com/2022/06/08/typed-vs-untyped-virtual-machines/">Some have argued</a> that a lack of
redundancy (which is somewhat incorrectly called &#34;untyped&#34; in the
article) allows for a simpler implementation, and further that a
dynamic translator for such a machine would be simpler, but the latter
claim is far from reality. One simple example is that any memory-write
instruction may modify bytecode, which would require the
implementation to invalidate any machine code that was translated from
the bytecode.
</p>

<p>
Modern compilers use <i>alias analysis</i> to determine which references
can reference the same memory; often this analysis is helped by
redundancy in the language, such as type information and array bounds,
to prove that two references cannot alias. Such analysis could be used
to prove that writes to memory cannot affect bytecode that has been
translated; however the uxn language has no types, nor any sort of
notion of a reference that isn&#39;t an index into memory, so alias
analysis can only be performed using low-level information, by
performing more complex reasoning. The aforementioned 32-bit arithmetic
library writes into memory to store its arguments, though the
locations are constant and so alias analysis is not
difficult.<sup><a id="fnr.27" href="#fn.27" role="doc-backlink">27</a></sup> Writing to any location that is not constant
would require very general and very precise analysis. For example,
even writing to an element of an array at a constant location might
require determining a tight range for the index written to; whereas a
language with bounds checks can assume that such a write cannot affect
any other memory, as out-of-bounds access instead signals an
error.<sup><a id="fnr.28" href="#fn.28" role="doc-backlink">28</a></sup>
</p>

<p>
It appears that only a subset of uxn seems useful for simulation by
humans, which is no different to any other low-level language. The uxn
platform also offers no safety mechanisms, including bounds checks
inside the virtual machine, or capabilities for what external state
the machine may affect; thus programs in uxn may unintentionally or
intentionally cause irreversible damage. The same lack of redundancy
exposed to the virtual machine may make for a simpler naive
interpreter, but an efficient implementation of uxn has to either be
prepared to de-optimize almost anywhere, or to use unnecessarily
complicated and precise analyses to prove that optimisations still
preserve the semantics of the machine.
</p>
</div>
</div>

<div id="outline-container-org23332a0">
<h3 id="org23332a0">Conclusion</h3>
<div id="text-org23332a0">
<p>
The design of uxn makes it unsuitable for personal computing, be it on
new or old hardware, or pen-and-paper simulations. Some people think
these constraints are fun. We can&#39;t argue with personal preferences,
but we can still argue that these constraints coincide with weaknesses
of both humans and computers. The machine model is hard for humans to
simulate, and programs using uxn are as unsafe and insecure as
programs written in mainstream contemporary low-level languages. The
only simple implementations of uxn are quite slow, which is
undesirable, especially on older machines which are slow already; a
performant implementation of uxn requires much of the complexity of
modern optimising compilers. Yet even a simple implementation must be
compiled using a C compiler on a mainstream computing platform, so we
can only conclude that either implementation simplicity (of the
toolchain producing the virtual machine) is not important, or we have
not meaningfully escaped mainstream computing platforms.
</p>

<p>
The principles of uxn, and this promise of a &#34;clean-slate computing
platform,&#34; cannot be appreciated with these issues. Few programs can
be written in uxn, so it does not greatly improve the portability of
software. Even fewer programs can be written in a way that a human can
reason about, so it does not reduce dependence on modern digital
computers, nor improve the use of digital computers in any way.
</p>
</div>
</div>
</div>

<div id="outline-container-orgfaa565d">
<h2 id="orgfaa565d">Conclusion</h2>
<div id="text-orgfaa565d">
<p>
How did this all happen then? A quick detour is necessary for context.
Murray Bookchin names a concept of <i>futurism</i> while discussing
predictions of the future, where things just seem to get larger, while
staying the same otherwise.
</p>

<blockquote>
<p>
What is futurism? Futurism is the present as it exists today, projected,
one hundred years from now. That’s what futurism is. If you have a
population of X billions of people, how are you going to have food, how
are you going to do this… nothing has changed. All they do is they make
everything either bigger, or they change the size—you’ll live in thirty
story buildings, you’ll live in sixty-story buildings. Frank Lloyd Wright
was going to build an office building that was one mile high. That
was futurism.<sup><a id="fnr.29" href="#fn.29" role="doc-backlink">29</a></sup>
</p>
</blockquote>

<p>
This sort of futurism is characterised by growing structures, without
considering how and if they can be grown. The proportions of things
are merely made larger, regardless of if they can be made larger
without some other sort of failure. Alan Kay also brings up a similar
issue with software design, but doesn&#39;t name it as such:
</p>

<blockquote>
<p>
Now, somebody could come along and look at a dog house and say,
&#34;Wow! If we could just expand that by a factor of a hundred we could
make ourselves a cathedral.&#34; It&#39;s about three feet high. That would
give us something thirty stories high, and that would be really
impressive. We could get a lot of people in there.
[… However,] when you blow something up [by] a factor of a hundred, it
gets a factor of hundred weaker in its ability, and in fact, […]
it would just collapse into a pile of rubble.<sup><a id="fnr.30" href="#fn.30" role="doc-backlink">30</a></sup>
</p>
</blockquote>

<p>
But whereas futurism is mindless <i>growth</i>, the stories we told involve
mindless <i>degrowth</i>. We doubt we can make a physical metaphor as
elegant as building a cathedral by attempting to build a large dog
house, without any concern for physics, but we can try: this sort of
degrowth is akin to taking only one arbitrary machine from a
complicated factory, and hoping to do much with it. A single machine
plainly does not achieve much when it is not part of a larger
production chain. Instead, a drastically different design is necessary
to have smaller and decentralised means of production.<sup><a id="fnr.31" href="#fn.31" role="doc-backlink">31</a></sup>
</p>

<p>
Software suffers the same fate as physical production: writing
&#34;simple&#34; software that does not achieve much on its own will does not
lead to the user benefiting from simplicity. Simplicity needs to be
pervasive through the system, and further in the context it resides
in, for one to benefit from it. Invoking that &#34;simple code is easier
to reason about&#34; to defend unsafe programming languages rings hollow,
when the language used produces many edge cases that either require
more complex code to check for, or for all users of the code to
carefully avoid those cases. It similarly does not help to use a
programming platform which is simple to write a slow implementation
for, where there are no forms of redundancy to help detect bugs, where
writing secure code is difficult, and executing programs efficiently
is no easier (if not harder) than with mainstream platforms.
</p>

<p>
Minimalist computing is theoretically about &#34;more with less&#34;, but
rather than being provided with &#34;more&#34;, we are instead being
guilt-tripped and told that any &#34;more&#34; is sinful: that it is going to
cause the collapse of civilisation, that it is going to ruin the
environment, that it increases the labour required by programmers, and
so on. Yet it is precisely those minimalist devices which are
committing these sins <i>right now</i>; the hypocritical Church of
Minimalism calls <i>us</i> the sinners, while it harbours its most sinful
priests, and gives them a promotion every so often.
</p>

<p>
Such is life, we suppose.
</p>
</div>
</div>


</div></div>
  </body>
</html>
