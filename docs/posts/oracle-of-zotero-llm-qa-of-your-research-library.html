<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Frost-group/The-Oracle-of-Zotero">Original</a>
    <h1>Oracle of Zotero: LLM QA of Your Research Library</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<ul dir="auto">
<li><a href="#paper-qa--paper-qa">Paper QA- Paper QA</a>
<ul dir="auto">
<li><a href="#output-example">Output Example</a>
<ul dir="auto">
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#hugging-face-demo">Hugging Face Demo</a></li>
<li><a href="#install">Install</a></li>
<li><a href="#usage">Usage</a>
<ul dir="auto">
<li><a href="#adding-documents">Adding Documents</a></li>
<li><a href="#choosing-model">Choosing Model</a></li>
<li><a href="#adjusting-number-of-sources">Adjusting number of sources</a></li>
<li><a href="#using-code-or-html">Using Code or HTML</a></li>
</ul>
</li>
<li><a href="#version-3-changes">Version 3 Changes</a>
<ul dir="auto">
<li><a href="#new-features">New Features</a></li>
<li><a href="#naming">Naming</a></li>
<li><a href="#breaking-changes">Breaking Changes</a></li>
</ul>
</li>
<li><a href="#notebooks">Notebooks</a></li>
<li><a href="#where-do-i-get-papers">Where do I get papers?</a>
<ul dir="auto">
<li><a href="#zotero">Zotero</a></li>
<li><a href="#paper-scraper">Paper Scraper</a></li>
</ul>
</li>
<li><a href="#pdf-reading-options">PDF Reading Options</a></li>
<li><a href="#typewriter-view">Typewriter View</a></li>
<li><a href="#llmembedding-caching">LLM/Embedding Caching</a>
<ul dir="auto">
<li><a href="#caching-embeddings">Caching Embeddings</a></li>
</ul>
</li>
<li><a href="#customizing-prompts">Customizing Prompts</a>
<ul dir="auto">
<li><a href="#pre-and-post-prompts">Pre and Post Prompts</a></li>
</ul>
</li>
<li><a href="#faq">FAQ</a>
<ul dir="auto">
<li><a href="#how-is-this-different-from-llamaindex">How is this different from LlamaIndex?</a></li>
<li><a href="#how-is-this-different-from-langchain">How is this different from LangChain?</a></li>
<li><a href="#can-i-use-different-llms">Can I use different LLMs?</a></li>
<li><a href="#where-do-the-documents-come-from">Where do the documents come from?</a></li>
<li><a href="#can-i-save-or-load">Can I save or load?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p dir="auto"><a href="https://github.com/whitead/paper-qa"><img src="https://camo.githubusercontent.com/f6d50128cb007f85916b7a899da5d94f654dce35a37331c8d28573aef46f4274/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6769746875622d2532333132313031312e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" alt="GitHub" data-canonical-src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white"/></a>
<a href="https://github.com/whitead/paper-qa"><img src="https://github.com/whitead/paper-qa/actions/workflows/tests.yml/badge.svg" alt="tests"/></a>
<a href="https://badge.fury.io/py/paper-qa" rel="nofollow"><img src="https://camo.githubusercontent.com/ad0dc79db40b9f40b17f1e756894726f05824c5156ebfd7a5da19b04ceb254f9/68747470733a2f2f62616467652e667572792e696f2f70792f70617065722d71612e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/paper-qa.svg"/></a></p>
<p dir="auto">This is a minimal package for doing question and answering from
PDFs or text files (which can be raw HTML). It strives to give very good answers, with no hallucinations, by grounding responses with in-text citations.</p>
<p dir="auto">By default, it uses <a href="https://platform.openai.com/docs/guides/embeddings" rel="nofollow">OpenAI Embeddings</a> with a vector DB called <a href="https://github.com/facebookresearch/faiss">FAISS</a> to embed and search documents. However, via <a href="https://github.com/hwchase17/langchain">langchain</a> you can use open-source models or embeddings (see details below).</p>
<p dir="auto">PaperQA uses the process shown below:</p>
<ol dir="auto">
<li>embed docs into vectors</li>
<li>embed query into vector</li>
<li>search for top k passages in docs</li>
<li>create summary of each passage relevant to query</li>
<li>put summaries into prompt</li>
<li>generate answer with prompt</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content-output-example" aria-hidden="true" tabindex="-1" href="#output-example"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Output Example</h2>
<p dir="auto">Question: How can carbon nanotubes be manufactured at a large scale?</p>
<p dir="auto">Carbon nanotubes can be manufactured at a large scale using the electric-arc technique (Journet6644). This technique involves creating an arc between two electrodes in a reactor under a helium atmosphere and using a mixture of a metallic catalyst and graphite powder in the anode. Yields of 80% of entangled carbon filaments can be achieved, which consist of smaller aligned SWNTs self-organized into bundle-like crystallites (Journet6644). Additionally, carbon nanotubes can be synthesized and self-assembled using various methods such as DNA-mediated self-assembly, nanoparticle-assisted alignment, chemical self-assembly, and electro-addressed functionalization (Tulevski2007). These methods have been used to fabricate large-area nanostructured arrays, high-density integration, and freestanding networks (Tulevski2007). 98% semiconducting CNT network solution can also be used and is separated from metallic nanotubes using a density gradient ultracentrifugation approach (Chen2014). The substrate is incubated in the solution and then rinsed with deionized water and dried with N2 air gun, leaving a uniform carbon network (Chen2014).</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-references" aria-hidden="true" tabindex="-1" href="#references"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h3>
<p dir="auto">Journet6644: Journet, Catherine, et al. &#34;Large-scale production of single-walled carbon nanotubes by the electric-arc technique.&#34; nature 388.6644 (1997): 756-758.</p>
<p dir="auto">Tulevski2007: Tulevski, George S., et al. &#34;Chemically assisted directed assembly of carbon nanotubes for the fabrication of large-scale device arrays.&#34; Journal of the American Chemical Society 129.39 (2007): 11964-11968.</p>
<p dir="auto">Chen2014: Chen, Haitian, et al. &#34;Large-scale complementary macroelectronics using hybrid integration of carbon nanotubes and IGZO thin-film transistors.&#34; Nature communications 5.1 (2014): 4097.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-hugging-face-demo" aria-hidden="true" tabindex="-1" href="#hugging-face-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hugging Face Demo</h2>
<p dir="auto"><a href="https://huggingface.co/spaces/whitead/paper-qa" rel="nofollow">Hugging Face Demo</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-install" aria-hidden="true" tabindex="-1" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install</h2>
<p dir="auto">Install with pip:</p>

<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" tabindex="-1" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Make sure you have set your OPENAI_API_KEY environment variable to your <a href="https://platform.openai.com/account/api-keys" rel="nofollow">openai api key</a></p>
<p dir="auto">To use paper-qa, you need to have a list of paths (valid extensions include: .pdf, .txt) and a list of citations (strings) that correspond to the paths. You can then use the <code>Docs</code> class to add the documents and then query them. If you don&#39;t have citations, <code>Docs</code> will try to guess them from the first page of your docs.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
from paperqa import Docs

# get a list of paths

docs = Docs()
for d in my_docs:
    docs.add(d)

answer = docs.query(&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;)
print(answer.formatted_answer)"><pre><span>from</span> <span>paperqa</span> <span>import</span> <span>Docs</span>

<span># get a list of paths</span>

<span>docs</span> <span>=</span> <span>Docs</span>()
<span>for</span> <span>d</span> <span>in</span> <span>my_docs</span>:
    <span>docs</span>.<span>add</span>(<span>d</span>)

<span>answer</span> <span>=</span> <span>docs</span>.<span>query</span>(<span>&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;</span>)
<span>print</span>(<span>answer</span>.<span>formatted_answer</span>)</pre></div>
<p dir="auto">The answer object has the following attributes: <code>formatted_answer</code>, <code>answer</code> (answer alone), <code>question</code>, <code>context</code> (the summaries of passages found for answer), <code>references</code> (the docs from which the passages came), and <code>passages</code> which contain the raw text of the passages as a dictionary.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-adding-documents" aria-hidden="true" tabindex="-1" href="#adding-documents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Adding Documents</h3>
<p dir="auto"><code>add</code> will add from paths. You can also use <code>add_file</code> (expects a file object) or <code>add_url</code> to work with other sources.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-choosing-model" aria-hidden="true" tabindex="-1" href="#choosing-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Choosing Model</h3>
<p dir="auto">By default, it uses a hybrid of <code>gpt-3.5-turbo</code> and <code>gpt-4</code>. If you don&#39;t have gpt-4 access or would like to save money, you can adjust:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docs = Docs(llm=&#39;gpt-3.5-turbo&#39;)"><pre><span>docs</span> <span>=</span> <span>Docs</span>(<span>llm</span><span>=</span><span>&#39;gpt-3.5-turbo&#39;</span>)</pre></div>
<p dir="auto">or you can use any other model available in <a href="https://github.com/hwchase17/langchain">langchain</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain.chat_models import ChatAnthropic, ChatOpenAI
model = ChatOpenAI(model=&#39;gpt-4&#39;)
summary_model = ChatAnthropic(model=&#34;claude-instant-v1-100k&#34;, anthropic_api_key=&#34;my-api-key&#34;)
docs = Docs(llm=model, summary_llm=summary_model)"><pre><span>from</span> <span>langchain</span>.<span>chat_models</span> <span>import</span> <span>ChatAnthropic</span>, <span>ChatOpenAI</span>
<span>model</span> <span>=</span> <span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#39;gpt-4&#39;</span>)
<span>summary_model</span> <span>=</span> <span>ChatAnthropic</span>(<span>model</span><span>=</span><span>&#34;claude-instant-v1-100k&#34;</span>, <span>anthropic_api_key</span><span>=</span><span>&#34;my-api-key&#34;</span>)
<span>docs</span> <span>=</span> <span>Docs</span>(<span>llm</span><span>=</span><span>model</span>, <span>summary_llm</span><span>=</span><span>summary_model</span>)</pre></div>
<h4 tabindex="-1" dir="auto"><a id="user-content-locally-hosted" aria-hidden="true" tabindex="-1" href="#locally-hosted"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Locally Hosted</h4>
<p dir="auto">You can also use any other models (or embeddings) available in <a href="https://github.com/hwchase17/langchain">langchain</a>. Here&#39;s an example of using <code>llama.cpp</code> to have locally hosted paper-qa:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import paperscraper
from paperqa import Docs
from langchain.llms import LlamaCpp
from langchain import PromptTemplate, LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.embeddings import LlamaCppEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Make sure the model path is correct for your system!
llm = LlamaCpp(
    model_path=&#34;./ggml-model-q4_0.bin&#34;, callbacks=[StreamingStdOutCallbackHandler()]
)
embeddings = LlamaCppEmbeddings(model_path=&#34;./ggml-model-q4_0.bin&#34;)

docs = Docs(llm=llm, embeddings=embeddings)

keyword_search = &#39;bispecific antibody manufacture&#39;
papers = paperscraper.search_papers(keyword_search, limit=2)
for path,data in papers.items():
    try:
        docs.add(path,chunk_chars=500)
    except ValueError as e:
        print(&#39;Could not read&#39;, path, e)

answer = docs.query(&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;)
print(answer)"><pre><span>import</span> <span>paperscraper</span>
<span>from</span> <span>paperqa</span> <span>import</span> <span>Docs</span>
<span>from</span> <span>langchain</span>.<span>llms</span> <span>import</span> <span>LlamaCpp</span>
<span>from</span> <span>langchain</span> <span>import</span> <span>PromptTemplate</span>, <span>LLMChain</span>
<span>from</span> <span>langchain</span>.<span>callbacks</span>.<span>manager</span> <span>import</span> <span>CallbackManager</span>
<span>from</span> <span>langchain</span>.<span>embeddings</span> <span>import</span> <span>LlamaCppEmbeddings</span>
<span>from</span> <span>langchain</span>.<span>callbacks</span>.<span>streaming_stdout</span> <span>import</span> <span>StreamingStdOutCallbackHandler</span>

<span># Make sure the model path is correct for your system!</span>
<span>llm</span> <span>=</span> <span>LlamaCpp</span>(
    <span>model_path</span><span>=</span><span>&#34;./ggml-model-q4_0.bin&#34;</span>, <span>callbacks</span><span>=</span>[<span>StreamingStdOutCallbackHandler</span>()]
)
<span>embeddings</span> <span>=</span> <span>LlamaCppEmbeddings</span>(<span>model_path</span><span>=</span><span>&#34;./ggml-model-q4_0.bin&#34;</span>)

<span>docs</span> <span>=</span> <span>Docs</span>(<span>llm</span><span>=</span><span>llm</span>, <span>embeddings</span><span>=</span><span>embeddings</span>)

<span>keyword_search</span> <span>=</span> <span>&#39;bispecific antibody manufacture&#39;</span>
<span>papers</span> <span>=</span> <span>paperscraper</span>.<span>search_papers</span>(<span>keyword_search</span>, <span>limit</span><span>=</span><span>2</span>)
<span>for</span> <span>path</span>,<span>data</span> <span>in</span> <span>papers</span>.<span>items</span>():
    <span>try</span>:
        <span>docs</span>.<span>add</span>(<span>path</span>,<span>chunk_chars</span><span>=</span><span>500</span>)
    <span>except</span> <span>ValueError</span> <span>as</span> <span>e</span>:
        <span>print</span>(<span>&#39;Could not read&#39;</span>, <span>path</span>, <span>e</span>)

<span>answer</span> <span>=</span> <span>docs</span>.<span>query</span>(<span>&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;</span>)
<span>print</span>(<span>answer</span>)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-adjusting-number-of-sources" aria-hidden="true" tabindex="-1" href="#adjusting-number-of-sources"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Adjusting number of sources</h3>
<p dir="auto">You can adjust the numbers of sources (passages of text) to reduce token usage or add more context. <code>k</code> refers to the top k most relevant and diverse (may from different sources) passages. Each passage is sent to the LLM to summarize, or determine if it is irrelevant. After this step, a limit of <code>max_sources</code> is applied so that the final answer can fit into the LLM context window. Thus, <code>k</code> &gt; <code>max_sources</code>  and <code>max_sources</code> is the number of sources used in the final answer.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docs.query(&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;, k = 5, max_sources = 2)"><pre><span>docs</span>.<span>query</span>(<span>&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;</span>, <span>k</span> <span>=</span> <span>5</span>, <span>max_sources</span> <span>=</span> <span>2</span>)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-using-code-or-html" aria-hidden="true" tabindex="-1" href="#using-code-or-html"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Using Code or HTML</h3>
<p dir="auto">You do not need to use papers -- you can use code or raw HTML. Note that this tool is focused on answering questions, so it won&#39;t do well at writing code. One note is that the tool cannot infer citations from code, so you will need to provide them yourself.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
import glob

source_files = glob.glob(&#39;**/*.js&#39;)

docs = Docs()
for f in source_files:
    # this assumes the file names are unique in code
    docs.add(f, citation=&#39;File &#39; + os.path.name(f), docname=os.path.name(f))
answer = docs.query(&#34;Where is the search bar in the header defined?&#34;)
print(answer)"><pre><span>import</span> <span>glob</span>

<span>source_files</span> <span>=</span> <span>glob</span>.<span>glob</span>(<span>&#39;**/*.js&#39;</span>)

<span>docs</span> <span>=</span> <span>Docs</span>()
<span>for</span> <span>f</span> <span>in</span> <span>source_files</span>:
    <span># this assumes the file names are unique in code</span>
    <span>docs</span>.<span>add</span>(<span>f</span>, <span>citation</span><span>=</span><span>&#39;File &#39;</span> <span>+</span> <span>os</span>.<span>path</span>.<span>name</span>(<span>f</span>), <span>docname</span><span>=</span><span>os</span>.<span>path</span>.<span>name</span>(<span>f</span>))
<span>answer</span> <span>=</span> <span>docs</span>.<span>query</span>(<span>&#34;Where is the search bar in the header defined?&#34;</span>)
<span>print</span>(<span>answer</span>)</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-version-3-changes" aria-hidden="true" tabindex="-1" href="#version-3-changes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Version 3 Changes</h2>
<p dir="auto">Version 3 includes many changes to type the code, make it more focused/modular, and enable performance to very large numbers of documents. The major breaking changes are documented below:</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-new-features" aria-hidden="true" tabindex="-1" href="#new-features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>New Features</h3>
<p dir="auto">The following new features are in v3:</p>
<ol dir="auto">
<li>Memory is now possible in <code>query</code> by setting <code>Docs(memory=True)</code> - this means follow-up questions will have a record of the previous question and answer.</li>
<li><code>add_url</code> and <code>add_file</code> are now supported for adding from URLs and file objects</li>
<li>Prompts can be customized, and now can be executed pre and post query</li>
<li>Consistent use of <code>dockey</code> and <code>docname</code> for unique and natural language names enable better tracking with external databases</li>
<li>Texts and embeddings are no longer required to be part of <code>Docs</code> object, so you can use external databases or other strategies to manage them</li>
<li>Various simplifications, bug fixes, and performance improvements</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-naming" aria-hidden="true" tabindex="-1" href="#naming"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Naming</h3>
<p dir="auto">The following table shows the old names and the new names:</p>
<table>
<thead>
<tr>
<th>Old Name</th>
<th>New Name</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>key</code></td>
<td><code>name</code></td>
<td>Name is a natural language name for text.</td>
</tr>
<tr>
<td><code>dockey</code></td>
<td><code>docname</code></td>
<td>Docname is a natural language name for a document.</td>
</tr>
<tr>
<td><code>hash</code></td>
<td><code>dockey</code></td>
<td>Dockey is a unique identifier for the document.</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto"><a id="user-content-breaking-changes" aria-hidden="true" tabindex="-1" href="#breaking-changes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Breaking Changes</h3>
<h4 tabindex="-1" dir="auto"><a id="user-content-pickled-objects" aria-hidden="true" tabindex="-1" href="#pickled-objects"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Pickled objects</h4>
<p dir="auto">The pickled objects are not compatible with the new version.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-agents" aria-hidden="true" tabindex="-1" href="#agents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Agents</h4>
<p dir="auto">The agent functionality has been removed, as it&#39;s not a core focus of the library</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-caching" aria-hidden="true" tabindex="-1" href="#caching"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Caching</h4>
<p dir="auto">Caching has been removed because it&#39;s not a core focus of the library. See FAQ below for how to use caching.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-answers" aria-hidden="true" tabindex="-1" href="#answers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Answers</h4>
<p dir="auto">Answers will not include passages, but instead return dockeys that can be used to retrieve the passages. Tokens/cost will also not be counted since that is built into langchain by default (see below for an example).</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-search-query" aria-hidden="true" tabindex="-1" href="#search-query"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Search Query</h4>
<p dir="auto">The search query chain has been removed. You can use langchain directly to do this.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-notebooks" aria-hidden="true" tabindex="-1" href="#notebooks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Notebooks</h2>
<p dir="auto">If you want to use this in an jupyter notebook or colab, you need to run the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import nest_asyncio
nest_asyncio.apply()"><pre><span>import</span> <span>nest_asyncio</span>
<span>nest_asyncio</span>.<span>apply</span>()</pre></div>
<p dir="auto">Also - if you know how to make this automated, please let me know!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-where-do-i-get-papers" aria-hidden="true" tabindex="-1" href="#where-do-i-get-papers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Where do I get papers?</h2>
<p dir="auto">Well that&#39;s a really good question! It&#39;s probably best to just download PDFs of papers you think will help answer your question and start from there.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-zotero" aria-hidden="true" tabindex="-1" href="#zotero"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Zotero</h3>
<p dir="auto">If you use <a href="https://www.zotero.org/" rel="nofollow">Zotero</a> to organize your personal bibliography,
you can use the <code>paperqa.contrib.ZoteroDB</code> to query papers from your library,
which relies on <a href="https://github.com/urschrei/pyzotero">pyzotero</a>.</p>
<p dir="auto">Install <code>pyzotero</code> to use this feature:</p>

<p dir="auto">First, note that <code>paperqa</code> parses the PDFs of papers to store in the database,
so all relevant papers should have PDFs stored inside your database.
You can get Zotero to automatically do this by highlighting the references
you wish to retrieve, right clicking, and selecting <em>&#34;Find Available PDFs&#34;</em>.
You can also manually drag-and-drop PDFs onto each reference.</p>
<p dir="auto">To download papers, you need to get an API key for your account.</p>
<ol dir="auto">
<li>Get your library ID, and set it as the environment variable <code>ZOTERO_USER_ID</code>.
<ul dir="auto">
<li>For personal libraries, this ID is given <a href="https://www.zotero.org/settings/keys" rel="nofollow">here</a> at the part &#34;<em>Your userID for use in API calls is XXXXXX</em>&#34;.</li>
<li>For group libraries, go to your group page <code>https://www.zotero.org/groups/groupname</code>, and hover over the settings link. The ID is the integer after /groups/. (<em>h/t pyzotero!</em>)</li>
</ul>
</li>
<li>Create a new API key <a href="https://www.zotero.org/settings/keys/new" rel="nofollow">here</a> and set it as the environment variable <code>ZOTERO_API_KEY</code>.
<ul dir="auto">
<li>The key will need read access to the library.</li>
</ul>
</li>
</ol>
<p dir="auto">With this, we can download papers from our library and add them to <code>paperqa</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from paperqa.contrib import ZoteroDB

docs = paperqa.Docs()
zotero = ZoteroDB(library_type=&#34;user&#34;)  # &#34;group&#34; if group library

for item in zotero.iterate(limit=20):
    if item.num_pages &gt; 30:
        continue  # skip long papers
    docs.add(item.pdf, docname=item.key)"><pre><span>from</span> <span>paperqa</span>.<span>contrib</span> <span>import</span> <span>ZoteroDB</span>

<span>docs</span> <span>=</span> <span>paperqa</span>.<span>Docs</span>()
<span>zotero</span> <span>=</span> <span>ZoteroDB</span>(<span>library_type</span><span>=</span><span>&#34;user&#34;</span>)  <span># &#34;group&#34; if group library</span>

<span>for</span> <span>item</span> <span>in</span> <span>zotero</span>.<span>iterate</span>(<span>limit</span><span>=</span><span>20</span>):
    <span>if</span> <span>item</span>.<span>num_pages</span> <span>&gt;</span> <span>30</span>:
        <span>continue</span>  <span># skip long papers</span>
    <span>docs</span>.<span>add</span>(<span>item</span>.<span>pdf</span>, <span>docname</span><span>=</span><span>item</span>.<span>key</span>)</pre></div>
<p dir="auto">which will download the first 20 papers in your Zotero database and add
them to the <code>Docs</code> object.</p>
<p dir="auto">We can also do specific queries of our Zotero library and iterate over the results:</p>
<div dir="auto" data-snippet-clipboard-copy-content="for item in zotero.iterate(
        q=&#34;large language models&#34;,
        qmode=&#34;everything&#34;,
        sort=&#34;date&#34;,
        direction=&#34;desc&#34;,
        limit=100,
):
    print(&#34;Adding&#34;, item.title)
    docs.add(item.pdf, docname=item.key)"><pre><span>for</span> <span>item</span> <span>in</span> <span>zotero</span>.<span>iterate</span>(
        <span>q</span><span>=</span><span>&#34;large language models&#34;</span>,
        <span>qmode</span><span>=</span><span>&#34;everything&#34;</span>,
        <span>sort</span><span>=</span><span>&#34;date&#34;</span>,
        <span>direction</span><span>=</span><span>&#34;desc&#34;</span>,
        <span>limit</span><span>=</span><span>100</span>,
):
    <span>print</span>(<span>&#34;Adding&#34;</span>, <span>item</span>.<span>title</span>)
    <span>docs</span>.<span>add</span>(<span>item</span>.<span>pdf</span>, <span>docname</span><span>=</span><span>item</span>.<span>key</span>)</pre></div>
<p dir="auto">You can read more about the search syntax by typing <code>zotero.iterate?</code> in IPython.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-paper-scraper" aria-hidden="true" tabindex="-1" href="#paper-scraper"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Paper Scraper</h3>
<p dir="auto">If you want to search for papers outside of your own collection, I&#39;ve found an unrelated project called <a href="https://github.com/blackadad/paper-scraper">paper-scraper</a> that looks
like it might help. But beware, this project looks like it uses some scraping tools that may violate publisher&#39;s rights or be in a gray area of legality.</p>
<div dir="auto" data-snippet-clipboard-copy-content="keyword_search = &#39;bispecific antibody manufacture&#39;
papers = paperscraper.search_papers(keyword_search)
docs = paperqa.Docs()
for path,data in papers.items():
    try:
        docs.add(path)
    except ValueError as e:
        # sometimes this happens if PDFs aren&#39;t downloaded or readable
        print(&#39;Could not read&#39;, path, e)
answer = docs.query(&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;)
print(answer)"><pre><span>keyword_search</span> <span>=</span> <span>&#39;bispecific antibody manufacture&#39;</span>
<span>papers</span> <span>=</span> <span>paperscraper</span>.<span>search_papers</span>(<span>keyword_search</span>)
<span>docs</span> <span>=</span> <span>paperqa</span>.<span>Docs</span>()
<span>for</span> <span>path</span>,<span>data</span> <span>in</span> <span>papers</span>.<span>items</span>():
    <span>try</span>:
        <span>docs</span>.<span>add</span>(<span>path</span>)
    <span>except</span> <span>ValueError</span> <span>as</span> <span>e</span>:
        <span># sometimes this happens if PDFs aren&#39;t downloaded or readable</span>
        <span>print</span>(<span>&#39;Could not read&#39;</span>, <span>path</span>, <span>e</span>)
<span>answer</span> <span>=</span> <span>docs</span>.<span>query</span>(<span>&#34;What manufacturing challenges are unique to bispecific antibodies?&#34;</span>)
<span>print</span>(<span>answer</span>)</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-pdf-reading-options" aria-hidden="true" tabindex="-1" href="#pdf-reading-options"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>PDF Reading Options</h2>
<p dir="auto">By default <a href="https://pypi.org/project/pypdf/" rel="nofollow">PyPDF</a> is used since it&#39;s pure python and easy to install. For faster PDF reading, paper-qa will detect and use <a href="https://pymupdf.readthedocs.io/en/latest/" rel="nofollow">PymuPDF (fitz)</a>:</p>

<h2 tabindex="-1" dir="auto"><a id="user-content-typewriter-view" aria-hidden="true" tabindex="-1" href="#typewriter-view"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Typewriter View</h2>
<p dir="auto">To stream the completions as they occur (giving that ChatGPT typewriter look), you can simply instantiate models with those properties:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from paperqa import Docs
from langchain.callbacks.manager import CallbackManager
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

my_llm = ChatOpenAI(callbacks=[StreamingStdOutCallbackHandler()], streaming=True)
docs = Docs(llm=my_llm)"><pre><span>from</span> <span>paperqa</span> <span>import</span> <span>Docs</span>
<span>from</span> <span>langchain</span>.<span>callbacks</span>.<span>manager</span> <span>import</span> <span>CallbackManager</span>
<span>from</span> <span>langchain</span>.<span>chat_models</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>langchain</span>.<span>callbacks</span>.<span>streaming_stdout</span> <span>import</span> <span>StreamingStdOutCallbackHandler</span>

<span>my_llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>callbacks</span><span>=</span>[<span>StreamingStdOutCallbackHandler</span>()], <span>streaming</span><span>=</span><span>True</span>)
<span>docs</span> <span>=</span> <span>Docs</span>(<span>llm</span><span>=</span><span>my_llm</span>)</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-llmembedding-caching" aria-hidden="true" tabindex="-1" href="#llmembedding-caching"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LLM/Embedding Caching</h2>
<p dir="auto">You can using the builtin langchain caching capabilities. Just run this code at the top of yours:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain.cache import InMemoryCache
langchain.llm_cache = InMemoryCache()"><pre><span>from</span> <span>langchain</span>.<span>cache</span> <span>import</span> <span>InMemoryCache</span>
<span>langchain</span>.<span>llm_cache</span> <span>=</span> <span>InMemoryCache</span>()</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-caching-embeddings" aria-hidden="true" tabindex="-1" href="#caching-embeddings"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Caching Embeddings</h3>
<p dir="auto">In general, embeddings are cached when you pickle a <code>Docs</code> regardless of what vector store you use. If you would like to manage caching embeddings via an external database or other strategy,
you can populate a <code>Docs</code> object directly via
the <code>add_texts</code> object. That can take chunked texts and documents, which are serializable objects, to populate <code>Docs</code>.</p>
<p dir="auto">You also can simply use a separate vector database by setting the <code>doc_index</code> and <code>texts_index</code> explicitly when building the <code>Docs</code> object.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-customizing-prompts" aria-hidden="true" tabindex="-1" href="#customizing-prompts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Customizing Prompts</h2>
<p dir="auto">You can customize any of the prompts, using the <code>PromptCollection</code> class. For example, if you want to change the prompt for the question, you can do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from paperqa import Docs, Answer, PromptCollection
from langchain.prompts import PromptTemplate

my_qaprompt = PromptTemplate(
    input_variables=[&#34;context&#34;, &#34;question&#34;],
    template=&#34;Answer the question &#39;{question}&#39; &#34;
    &#34;Use the context below if helpful. &#34;
    &#34;You can cite the context using the key &#34;
    &#34;like (Example2012). &#34;
    &#34;If there is insufficient context, write a poem &#34;
    &#34;about how you cannot answer.\n\n&#34;
    &#34;Context: {context}\n\n&#34;)
prompts=PromptCollection(qa=my_qaprompt)
docs = Docs(prompts=prompts)"><pre><span>from</span> <span>paperqa</span> <span>import</span> <span>Docs</span>, <span>Answer</span>, <span>PromptCollection</span>
<span>from</span> <span>langchain</span>.<span>prompts</span> <span>import</span> <span>PromptTemplate</span>

<span>my_qaprompt</span> <span>=</span> <span>PromptTemplate</span>(
    <span>input_variables</span><span>=</span>[<span>&#34;context&#34;</span>, <span>&#34;question&#34;</span>],
    <span>template</span><span>=</span><span>&#34;Answer the question &#39;{question}&#39; &#34;</span>
    <span>&#34;Use the context below if helpful. &#34;</span>
    <span>&#34;You can cite the context using the key &#34;</span>
    <span>&#34;like (Example2012). &#34;</span>
    <span>&#34;If there is insufficient context, write a poem &#34;</span>
    <span>&#34;about how you cannot answer.<span>\n</span><span>\n</span>&#34;</span>
    <span>&#34;Context: {context}<span>\n</span><span>\n</span>&#34;</span>)
<span>prompts</span><span>=</span><span>PromptCollection</span>(<span>qa</span><span>=</span><span>my_qaprompt</span>)
<span>docs</span> <span>=</span> <span>Docs</span>(<span>prompts</span><span>=</span><span>prompts</span>)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-pre-and-post-prompts" aria-hidden="true" tabindex="-1" href="#pre-and-post-prompts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Pre and Post Prompts</h3>
<p dir="auto">Following the syntax above, you can also include prompts that
are executed after the query and before the query. For example, you can use this to critique the answer.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-faq" aria-hidden="true" tabindex="-1" href="#faq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FAQ</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-how-is-this-different-from-llamaindex" aria-hidden="true" tabindex="-1" href="#how-is-this-different-from-llamaindex"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How is this different from LlamaIndex?</h3>
<p dir="auto">It&#39;s not that different! This is similar to the tree response method in LlamaIndex. I just have included some prompts I find useful, readers that give page numbers/line numbers, and am focused on one task - answering technical questions with cited sources.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-how-is-this-different-from-langchain" aria-hidden="true" tabindex="-1" href="#how-is-this-different-from-langchain"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How is this different from LangChain?</h3>
<p dir="auto">It&#39;s not! We use langchain to abstract the LLMS, and the process is very similar to the <code>map_reduce</code> chain in LangChain.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-can-i-use-different-llms" aria-hidden="true" tabindex="-1" href="#can-i-use-different-llms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Can I use different LLMs?</h3>
<p dir="auto">Yes, you can use any LLMs from <a href="https://langchain.readthedocs.io/" rel="nofollow">langchain</a> by passing the <code>llm</code> argument to the <code>Docs</code> class. You can use different LLMs for summarization and for question answering too.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-where-do-the-documents-come-from" aria-hidden="true" tabindex="-1" href="#where-do-the-documents-come-from"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Where do the documents come from?</h3>
<p dir="auto">You can provide your own. I use some of my own code to pull papers from Google Scholar. This code is not included because it may enable people to violate Google&#39;s terms of service and publisher&#39;s terms of service.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-can-i-save-or-load" aria-hidden="true" tabindex="-1" href="#can-i-save-or-load"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Can I save or load?</h3>
<p dir="auto">The <code>Docs</code> class can be pickled and unpickled. This is useful if you want to save the embeddings of the documents and then load them later.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import pickle

# save
with open(&#34;my_docs.pkl&#34;, &#34;wb&#34;) as f:
    pickle.dump(docs, f)

# load
with open(&#34;my_docs.pkl&#34;, &#34;rb&#34;) as f:
    docs = pickle.load(f)"><pre><span>import</span> <span>pickle</span>

<span># save</span>
<span>with</span> <span>open</span>(<span>&#34;my_docs.pkl&#34;</span>, <span>&#34;wb&#34;</span>) <span>as</span> <span>f</span>:
    <span>pickle</span>.<span>dump</span>(<span>docs</span>, <span>f</span>)

<span># load</span>
<span>with</span> <span>open</span>(<span>&#34;my_docs.pkl&#34;</span>, <span>&#34;rb&#34;</span>) <span>as</span> <span>f</span>:
    <span>docs</span> <span>=</span> <span>pickle</span>.<span>load</span>(<span>f</span>)</pre></div>
</article>
          </div></div>
  </body>
</html>
