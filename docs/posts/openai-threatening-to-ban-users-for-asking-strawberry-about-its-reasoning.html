<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://futurism.com/the-byte/openai-ban-strawberry-reasoning">Original</a>
    <h1>OpenAI Threatening to Ban Users for Asking Strawberry About Its Reasoning</h1>
    
    <div id="readability-page-1" class="page"><div id="incArticle"><h2>&#34;Additional violations of this policy may result in loss of access to GPT-4o with Reasoning.&#34;</h2><h2>Ban Hammer</h2><p>OpenAI claims that its latest AI model, code-named &#34;Strawberry&#34; and released as o1-preview, is supposed to be capable of &#34;reasoning.&#34; But understanding how its <a href="https://futurism.com/openai-strawberry-thought-process-scheming">thought process</a> works, apparently, is something that the ChatGPT maker is serious about keeping off-limits.</p><p>As <a href="https://arstechnica.com/information-technology/2024/09/openai-threatens-bans-for-probing-new-ai-models-reasoning-process/"><i>Ars Technica </i>reports</a>, OpenAI is now threatening to ban users that try to get the large language model to reveal how it thinks — a glaring example of how the company has long since <a href="https://futurism.com/openai-sleazy-company-creating-agi">abandoned its original vision</a> of championing open source AI.</p><p>According to accounts on social media, users are <a href="https://x.com/MarcoFigueroa/status/1834741170024726628">receiving emails</a> from the Microsoft-backed startup informing them that their requests made to <a href="https://futurism.com/the-byte/chatgpt-voice-mode-scream">ChatGPT</a> have been flagged for &#34;attempting to circumvent safeguards.&#34;</p><p>&#34;Additional violations of this policy may result in loss of access to GPT-4o with Reasoning,&#34; the emails state.</p><h2>Hush Hush</h2><p>This clampdown is more than a bit ironic given that a lot of the hype around Strawberry was built around its &#34;chain-of-thought&#34; reasoning that allowed the AI to articulate how it arrived at an answer, step by step. OpenAI chief technology officer Mira Murati <a href="https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/">called this</a> a &#34;new paradigm&#34; for the technology.</p><p>Reports vary on what triggers the violations. As <i>Ars </i>found, some users claim that using the term &#34;<a href="https://x.com/voooooogel/status/1834536216160768377">reasoning trace</a>&#34; is what got them in trouble. Others say that even using the word &#34;<a href="https://x.com/dyushag/status/1834379249731444820">reasoning</a>&#34; on its own was enough to alert OpenAI&#39;s systems. Users can still see what is essentially a summary of Strawberry&#39;s thought process, but it&#39;s cobbled together by a second AI model and is heavily watered-down.</p><p>In a <a href="https://openai.com/index/learning-to-reason-with-llms/#hiding-the-chains-of-thought">blog post</a>, OpenAI argues that it needs to hide the chain-of-thought so that it wouldn&#39;t need to put a filter on how its AI thinks, in case it says stuff that isn&#39;t compliant with safety policies while thinking out loud. That way, developers can safely see its &#34;raw&#34; thought process behind-the-scenes.</p><p>But as the company freely admits, this measure also helps it maintain a &#34;competitive advantage,&#34; staving off competitors from trying to ride its coattails.</p><h2>Red Alert</h2><p>The flipside of this approach, however, is that concentrates more responsibility for aligning the language language model into the hands of OpenAI, instead of democratizing it. That poses a problem for red-teamers, or programmers that try to hack AI models to make them safer.</p><p>&#34;I&#39;m not at all happy about this policy decision,&#34; AI researcher Simon Willison <a href="https://simonwillison.net/2024/Sep/12/openai-o1/">wrote on his blog</a>, as quoted by <i>Ars</i>. &#34;As someone who develops against LLMs, interpretability and transparency are everything to me — the idea that I can run a complex prompt and have key details of how that prompt was evaluated hidden from me feels like a big step backwards.&#34;</p><p>As it stands, it seems that OpenAI is continuing down a path of keeping its AI models an ever more opaque black box.</p><p><strong>More on OpenAI: </strong><em><a href="https://futurism.com/openai-strawberry-thought-process-scheming">OpenAI&#39;s Strawberry &#34;Thought Process&#34; Sometimes Shows It Scheming to Trick Users</a></em></p></div></div>
  </body>
</html>
