<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://francisbach.com/jensen-inequality/">Original</a>
    <h1>Revisiting the Classics: Jensen&#39;s Inequality (2023)</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
						<div>
	<div id="primary">
		<main id="main">

				<article id="post-8823">
	<!-- .entry-header -->

	<div>
		
<p>There are a few mathematical results that any researcher in applied mathematics uses on a daily basis. One of them is Jensen’s inequality, which allows bounding expectations of functions of random variables. This really happens a lot in any probabilistic arguments but also as a tool to generate inequalities and optimization algorithms. In this blog post, I will present a collection of fun facts about the inequality, from very classical to more obscure. If you know other cool ones, please add them as comments.</p>



<p>But before, <em><strong>let me be very clear</strong></em>: Jensen’s inequality is often not in the direction that you would hope it to be. So, to avoid embarrassing mistakes, I always draw at least in my mind the figure below before using it.  </p>


<div>
<figure><img fetchpriority="high" decoding="async" src="https://francisbach.com/wp-content/uploads/2023/03/jensen-3.png" alt="" width="546" height="243" srcset="https://francisbach.com/wp-content/uploads/2023/03/jensen-3.png 1616w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-300x134.png 300w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-1024x456.png 1024w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-768x342.png 768w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-1536x684.png 1536w, https://francisbach.com/wp-content/uploads/2023/03/jensen-3-850x379.png 850w" sizes="(max-width: 546px) 100vw, 546px"/></figure></div>


<h2>Simplest formulation and proof</h2>



<p>Given a convex function defined on a convex subset \(C\) of \(\mathbb{R}^d\), and a random vector \(X\) with values in \(C\), then $$ f\big( \mathbb{E}[X] \big) \leqslant \mathbb{E} \big[ f(X) \big],$$ as soon as the expectations exist. For a strictly convex function, there is equality if and only if \(X\) is almost surely constant. This is often stated with \(\mu\) taking finitely many values, like in the plot below.</p>



<p><strong>Proof.</strong> Starting with the standard definition of convexity that corresponds to random variables that take only two values in \(C\), this can be extended by recursion to all random variables taking finitely many values, and then by a density argument, to all random variables. See the <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Wikipedia</a> page.</p>



<p><strong>Proof without words.</strong> As nicely explained in this <a href="https://mark.reid.name/blog/behold-jensens-inequality.html">blog post</a> by Mark Reid, a simple argument based on epigraphs leads to the inequality for discrete measures supported on \(x_1,\dots,x_n\), with non-negative weights \(\lambda_1, \dots,\lambda_n\) that sum to one, with an illustration below for \(n=4\): any convex combination of points \((x_i,f(x_i))\) has to be in the red convex polygon, which is above the function.</p>


<div>
<figure><img decoding="async" src="https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-1024x474.png" alt="" width="542" height="250" srcset="https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-1024x474.png 1024w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-300x139.png 300w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-768x355.png 768w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-1536x711.png 1536w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3-850x393.png 850w, https://francisbach.com/wp-content/uploads/2023/03/jensen_multi-3.png 1549w" sizes="(max-width: 542px) 100vw, 542px"/></figure></div>


<h2>A bit of history</h2>



<p>The result is typically attributed to the Danish mathematician <a href="https://fr.wikipedia.org/wiki/Johan_Jensen">Johan Jensen</a> [<a href="https://zenodo.org/record/2371297/files/article.pdf">1</a>, in French] who proved in 1906 the result for convex functions on the real line (in fact all continuous <a href="https://en.wikipedia.org/wiki/Convex_function">mid-point convex</a> functions), but <a href="https://en.wikipedia.org/wiki/Otto_H%C3%B6lder">Otto Hölder</a> had shown it earlier for twice differentiable functions [<a href="http://gdz.sub.uni-goettingen.de/dms/resolveppn/?PPN=GDZPPN00252421X">2</a>, in German]. It turns out this was known thirty years earlier for uniform measures on finite sets, as shown by Jules Grolous [<a href="https://books.google.fr/books?id=z_BFnsBAx18C&amp;hl=fr&amp;pg=PA401#v=onepage&amp;q&amp;f=true">3</a>], a relatively unknown former student from Ecole Polytechnique. See also [<a href="https://www.jstor.org/stable/pdf/43667702.pdf?refreqid=excelsior%3A29d60497a1ddcba74004c27f25c39cd1&amp;ab_segments=&amp;origin=&amp;initiator=">4</a>] for more details on the history of Jensen’s inequality.</p>



<h2>Classical applications</h2>



<p>Jensen’s inequality can be used to derive many other classical inequalities, typically applied to the exponential, logarithm or powers.</p>



<p><strong>Arithmetic, harmonic, and geometric means.</strong> For \(X\) with positive real values, we have: $$ \mathbb{E}[X]\geqslant \exp \Big( \mathbb{E}\big[ \log(X)\big]\Big)  \ \mbox{ and } \  \mathbb{E}[X]  \geqslant \frac{1}{\mathbb{E}\big[\frac{1}{X}\big]},$$ which corresponds for empirical measures to classical <a href="https://en.wikipedia.org/wiki/HM-GM-AM-QM_inequalities">inequalities between means</a>.</p>



<p><strong>Young’s inequality.</strong> For \(p,q&gt;1\) such that \(\frac{1}{p}+\frac{1}{q}=1\), and two non-negative real numbers \(x,y\), we get by Jensen’s inequality, $$ \log\big(\frac{1}{p} x^p + \frac{1}{q} y^q \big) \geqslant \frac{1}{p} \log(x^p) + \frac{1}{q} \log(y^q) = \log(xy),$$ leading to <a href="https://en.wikipedia.org/wiki/Young%27s_inequality_for_products">Young’s inequality</a> \(\displaystyle xy \leqslant \frac{1}{p} x^p + \frac{1}{q} y^q.\)</p>



<p><strong>Hölder’s inequality.</strong> For any positive \(x_1,\dots,x_n,y_1,\dots,y_n\), we can write $$\sum_{i=1}^n x_i y_i = \sum_{j=1}^n y_j^q \cdot \sum_{i=1}^n x_i y_i^{1-q} \frac{y_i^q}{\sum_{j=1}^n y_j^q} \leqslant \sum_{j=1}^n y_j^q \cdot \Big( \sum_{i=1}^n (x_i y_i^{1-q})^p \frac{y_i^q}{\sum_{j=1}^n y_j^q} \Big)^{1/p},$$ leading to <a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Hölder’s inequality</a> (with the same relationship between \(p\) and \(q\) as above): $$\sum_{i=1}^n x_i y_i \leqslant \Big( \sum_{j=1}^n y_j^q \Big)^{1/q} \Big( \sum_{j=1}^n x_j^p \Big)^{1/p}.$$ This includes also <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a> if \(p=q=2\), and also multiple versions of the “<a href="https://francisbach.com/the-%ce%b7-trick-or-the-effectiveness-of-reweighted-least-squares/">eta-trick</a>“.</p>



<h2>Majorization-minimization</h2>



<p>Within data science, Jensen’s inequality is often used to derive auxiliary functions used in <a href="https://en.wikipedia.org/wiki/MM_algorithm">majorization-minimization</a> algorithms, with two classical examples below.</p>



<p><strong>Non-negative matrix factorization (NMF).</strong> Given a non-negative matrix \(V \in \mathbb{R}_+^{n \times d}\), the goal of NMF is to decompose it as \(V = WH\) with \(W \in \mathbb{R}_+^{n \times m}\) and \(H \in \mathbb{R}_+^{m \times d}\). This has many applications, in particular in source separation [<a href="https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf">5</a>, <a href="http://perso.ens-lyon.fr/patrice.abry/ENSEIGNEMENTS/14M2SCExam/Bertin.pdf">6</a>].</p>



<p> A classical cost function which is used to estimate \(W\) and \(H\) is the Kullback-Leibler divergence [<a href="https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf">5</a>] $$ D(V \| WH) = \sum_{i=1}^n \sum_{j=1}^d \Big\{ V_{ij} \log \frac{ V_{ij} }{ ( WH)_{ij} }\  – V_{ij} + (WH)_{ij} \Big\}.$$ To minimize the cost function above with respect to \(H\) only, the problematic term is \(\log  ( WH)_{ij} = \log \big( \sum_{k=1}^m W_{ik} H_{kj} \big)\), which is a “log of a sum”. To turn it into a “sum of logs”, we use Jensen’s inequality for the logarithm, by introducing a probability vector \(q^{ij} \in \mathbb{R}_+^m\) (with non-negative values that sum to one), and lower-bounding $$ \log  ( WH)_{ij} = \log \Big( \sum_{k=1}^m q^{ij}_k \frac{W_{ik} H_{kj} }{q^{ij}_k} \Big) \geqslant \sum_{k=1}^n q^{ij}_{k} \log \frac{W_{ik} H_{kj}}{q^{ij}_k}.$$ For a fixed \(H\), the bound is tight for \(\displaystyle q^{ij}_k = \frac{W_{ik} H_{kj}}{(WH)_{ij}},\) and given all \(q\)’s, we can minimize with respect to \(H_{ki}\) in closed form to get the update $$H_{kj} \leftarrow H_{kj}    \frac{\sum_{i=1}^n \! V_{ij} W_{ik}  \, /\,  (WH)_{ij} }{\sum_{i’=1}^n \! W_{i’k}}.$$ Because we had a tight upper bound at the current \(H\) (before the update), this is a descent algorithm. We can derive a similar update for \(W\). As shown in [<a href="https://papers.nips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf">5</a>], this is a simple parameter-free descent algorithm that converges to a stationary point, often referred to as a multiplicative update algorithm. See a convergence analysis in [<a href="https://perso.telecom-paristech.fr/rbadeau/assets/ieee-tnn-10.pdf">7</a>] and alternatives based on relative smoothness [<a href="https://publications.ut-capitole.fr/id/eprint/25852/1/25852.pdf">8</a>] or on primal-dual formulations [<a href="https://arxiv.org/pdf/1608.01264">9</a>, <a href="https://hal.science/hal-01079229/document">10</a>].</p>



<p><strong>Expectation-maximization (EM).</strong> The exact same technique of introducing a probability vector within the log and using Jensen’s inequality is at the core of <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM for latent variable models</a> and <a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">variational inference</a> (in fact NMF is simply a particular instance for a Poisson likelihood), which are two good topics for future posts (see <a href="https://lips.cs.princeton.edu/the-elbo-without-jensen-or-kl/">here</a> for a simple derivation of the “<a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">evidence lower bound</a>“).</p>



<h2>Information theory</h2>



<p>Within <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, the concavity of the logarithm and the use of Jensen’s inequality play a major role in most classical results, e.g., positivity of the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leilbler divergence</a> or <a href="https://en.wikipedia.org/wiki/Data_processing_inequality">data processing inequality</a>. This also extends to all <a href="https://en.wikipedia.org/wiki/F-divergence">f-divergences</a>.</p>



<h2>Operator convexity</h2>



<p>When considering convexity with respect to a generalized inequaility (such as based on the Löwner order), we can extend many of the classical formulas above (relationship between means, Young’s and Hölder’s inequality) to matrices. See <a href="https://francisbach.com/matrix-monotony-and-convexity/">earlier post</a> for an introduction. For a certain set of functions (such as the square or the negative logarithm) \(f: \mathbb{R} \to \mathbb{R}\), then for a random symmetric matrix \(X\), we have (for the <a href="https://en.wikipedia.org/wiki/Loewner_order">Löwner order</a>): $$ f\big( \mathbb{E}[X] \big) \preccurlyeq  \mathbb{E} \big[ f(X) \big].$$ An intriguing extension is the operator version of Jensen’s inequality [<a href="https://arxiv.org/pdf/math/0204049">10</a>], for potentially dependent random variables \((X,Y)\), where \(X\) is symmetric, and the sizes of \(X\) and \(Y\) are compatible: $$ f \Big( \mathbb{E} \big[ Y^\top X Y \big] \Big) \preccurlyeq \mathbb{E} \big[ Y^\top f(X) Y \big]  \ \mbox{ as soon as } \ \mathbb{E}[  Y^\top Y ] = I.$$</p>



<h2>Exact expression of the remainder</h2>



<p>There is a large literature on extensions, refinements on Jensen’s inequality. I have a cute one of my own, which has probably been derived before. For twice differentiable functions \(f\), we can use <a href="https://en.wikipedia.org/wiki/Taylor%27s_theorem">Taylor formula with integral remainder</a> on the segment between \(X\) and \(\mathbb{E}[X]\), leading to, with \(g(t) =  f\big( t X + (1-t) \mathbb{E}[X]\big)\), $$g(1) = g(0) + g&#39;(0) + \int_0^1 \! g^{\prime \prime}(t)(1-t)dt.$$ Taking expectations, this leads to $$\mathbb{E} \big[f(X)\big] – f\big( \mathbb{E}[X]\big) = \mathbb{E} \bigg[ \int_0^1 \! ( X – \mathbb{E}[X])^\top f^{\prime\prime}\big( t X + (1-t) \mathbb{E}[X]\big) ( X – \mathbb{E}[X]) (1-t) dt \bigg].$$ From this expression, we recover traditional refinements or reversing of the order if \(f^{\prime\prime}\) has bounded eigenvalues. This can for example be used also for characterizing the equality cases in non-strictly convex situations [<a href="https://arxiv.org/pdf/2202.08545.pdf">11</a>, page 31].</p>



<h2>References</h2>



<p>[1] <a href="https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)"></a>Johan L. Jensen. <a href="https://zenodo.org/record/2371297/files/article.pdf">Sur les fonctions convexes et les inégalités entre les valeurs moyennes.</a> <em>Acta Mathematica</em>, 30(1): 175–193, 1906.</p>





			</div><!-- .entry-content -->
</article><!-- #post-8823 -->

<!-- #comments -->

		</main><!-- #main -->
	</div><!-- #primary -->


<!-- #secondary -->
</div>
</div></div>
  </body>
</html>
