<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/apple/pico-banana-400k">Original</a>
    <h1>Pico-Banana-400k</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto"><strong>Pico-Banana-400K</strong> is a large-scale dataset of <strong>~400K text‚Äìimage‚Äìedit triplets</strong> designed to advance research in <strong>text-guided image editing</strong>.</p>
<ul dir="auto">
<li>an <strong>original image</strong> (from <a href="https://storage.googleapis.com/openimages/web/factsfigures.html" rel="nofollow">Open Images</a>),</li>
<li>a <strong>human-like edit instruction</strong>, and</li>
<li>the <strong>edited result</strong> generated and verified by the <em>Nano-Banana</em> model.</li>
</ul>
<p dir="auto">The dataset spans <strong>35 edit operations</strong> across <strong>8 semantic categories</strong>, covering diverse transformations‚Äîfrom low-level color adjustments to high-level object, scene, and stylistic edits.</p>
<hr/>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Total Samples</strong></td>
<td>~257K single-turn text‚Äìimage‚Äìedit triplets for SFT, ~56K single-turn text-image(positive) - image(negative)-edit for preference learning, and ~72K multi-turn texts-images-edits for multi-turn applications</td>
</tr>
<tr>
<td><strong>Source</strong></td>
<td><a href="https://storage.googleapis.com/openimages/web/factsfigures.html" rel="nofollow">Open Images</a></td>
</tr>
<tr>
<td><strong>Edit Operations</strong></td>
<td>35 across 8 semantic categories</td>
</tr>
<tr>
<td><strong>Categories</strong></td>
<td>Pixel &amp; Photometric, Object-Level, Scene Composition, Stylistic, Text &amp; Symbol, Human-Centric, Scale &amp; Perspective, Spatial/Layout</td>
</tr>
<tr>
<td><strong>Image Resolution</strong></td>
<td>512‚Äì1024 px</td>
</tr>
<tr>
<td><strong>Prompt Generator</strong></td>
<td><a href="https://deepmind.google/discover/blog/gemini-2-5/" rel="nofollow">Gemini-2.5-Flash</a></td>
</tr>
<tr>
<td><strong>Editing Model</strong></td>
<td>Nano-Banana</td>
</tr>
<tr>
<td><strong>Self-Evaluation</strong></td>
<td>Automated judging pipeline using Gemini-2.5-Pro for edit quality</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üèóÔ∏è Dataset Construction</h2><a id="user-content-Ô∏è-dataset-construction" aria-label="Permalink: üèóÔ∏è Dataset Construction" href="#Ô∏è-dataset-construction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Pico-Banana-400K is built using a <strong>two-stage multimodal generation pipeline</strong>:</p>
<ol dir="auto">
<li><strong>Instruction Generation</strong></li>
<li><strong>Editing + Self-Evaluation</strong>
The Nano-Banana model performs the edit, then automatically evaluates the result using a structured quality prompt that measures:
Instruction Compliance (40%)
Editing Realism (25%)
Preservation Balance (20%)
Technical Quality (15%)
Only edits scoring above a strict threshold (~0.7) are labeled as successful, forming the main dataset; the remaining ~56K are retained as failure cases for robustness and preference learning.</li>
</ol>

<p dir="auto"><strong>Nano-Banana-400K</strong> contains <strong>~400K image editing data</strong>, covering a wide visual and semantic range drawn from real-world imagery.</p>
<hr/>
<div dir="auto"><h3 tabindex="-1" dir="auto">üß≠ Category Distribution</h3><a id="user-content--category-distribution" aria-label="Permalink: üß≠ Category Distribution" href="#-category-distribution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Category</th>
<th>Description</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Object-Level Semantic</strong></td>
<td>Add, remove, replace, or relocate objects</td>
<td><strong>35%</strong></td>
</tr>
<tr>
<td><strong>Scene Composition &amp; Multi-Subject</strong></td>
<td>Contextual and environmental transformations</td>
<td><strong>20%</strong></td>
</tr>
<tr>
<td><strong>Human-Centric</strong></td>
<td>Edits involving clothing, expression, or appearance</td>
<td><strong>18%</strong></td>
</tr>
<tr>
<td><strong>Stylistic</strong></td>
<td>Domain and artistic style transfer</td>
<td><strong>10%</strong></td>
</tr>
<tr>
<td><strong>Text &amp; Symbol</strong></td>
<td>Edits involving visible text, signs, or symbols</td>
<td><strong>8%</strong></td>
</tr>
<tr>
<td><strong>Pixel &amp; Photometric</strong></td>
<td>Brightness, contrast, and tonal adjustments</td>
<td><strong>5%</strong></td>
</tr>
<tr>
<td><strong>Scale &amp; Perspective</strong></td>
<td>Zoom, viewpoint, or framing changes</td>
<td><strong>2%</strong></td>
</tr>
<tr>
<td><strong>Spatial / Layout</strong></td>
<td>Outpainting, composition, or canvas extension</td>
<td><strong>2%</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr/>

<ul dir="auto">
<li><strong>Single-Turn SFT samples (successful edits):</strong> ~257K</li>
<li><strong>Single-Turn Preference samples (failure cases):</strong> ~56K</li>
<li><strong>Multi-Turn SFT samples (successful cases):</strong> ~72K</li>
<li><strong>Gemini-generated instructions:</strong> concise, natural, and image-aware</li>
<li><strong>Edit coverage:</strong> 35 edit types across 8 semantic categories</li>
<li><strong>Image diversity:</strong> includes humans, objects, text-rich scenes, etc from Open Images</li>
</ul>
<hr/>

<p dir="auto">Below are representative examples from different categories:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Category</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Object-Level</td>
<td>‚ÄúReplace the red apple with a green one.‚Äù</td>
</tr>
<tr>
<td>Scene Composition</td>
<td>‚ÄúAdd sunlight streaming through the window.‚Äù</td>
</tr>
<tr>
<td>Human-Centric</td>
<td>‚ÄúChange the person‚Äôs expression to smiling.‚Äù</td>
</tr>
<tr>
<td>Text &amp; Symbol</td>
<td>‚ÄúUppercase the text on the billboard.‚Äù</td>
</tr>
<tr>
<td>Stylistic</td>
<td>‚ÄúConvert the image to a Van Gogh painting style.‚Äù</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr/>
<p dir="auto">Pico-Banana-400K provides both <strong>breadth</strong> (diverse edit operations) and <strong>depth</strong> (quality-controlled multimodal supervision), making it a strong foundation for training and evaluating text-guided image editing models.</p>

<p dir="auto"><strong>Pico-Banana-400K</strong> serves as a versatile resource for advancing controllable and instruction-aware image editing.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">üì¶ Dataset Download Guide</h2><a id="user-content--dataset-download-guide" aria-label="Permalink: üì¶ Dataset Download Guide" href="#-dataset-download-guide"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <strong>Pico-Banana-400K</strong> dataset is hosted on Apple‚Äôs public CDN.</p>
<hr/>
<div dir="auto"><h3 tabindex="-1" dir="auto">üñºÔ∏è 1. Single-Turn Edited Images</h3><a id="user-content-Ô∏è-1-single-turn-edited-images" aria-label="Permalink: üñºÔ∏è 1. Single-Turn Edited Images" href="#Ô∏è-1-single-turn-edited-images"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Manifest files: <a href="https://ml-site.cdn-apple.com/datasets/pico-banana-300k/nb/manifest/sft_manifest.txt" rel="nofollow">sft link</a> and <a href="https://ml-site.cdn-apple.com/datasets/pico-banana-300k/nb/manifest/preference_manifest.txt" rel="nofollow">preference link</a></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">üñºÔ∏è 2. Multi-Turn Edited Images</h3><a id="user-content-Ô∏è-2-multi-turn-edited-images" aria-label="Permalink: üñºÔ∏è 2. Multi-Turn Edited Images" href="#Ô∏è-2-multi-turn-edited-images"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Manifest file: <a href="https://ml-site.cdn-apple.com/datasets/pico-banana-300k/nb/manifest/multi_turn_manifest.txt" rel="nofollow">multi-turn link</a></p>

<p dir="auto">Urls to download source images are provided along with edit instructions in <a href="https://ml-site.cdn-apple.com/datasets/pico-banana-300k/nb/jsonl/sft.jsonl" rel="nofollow">sft link</a>, <a href="https://ml-site.cdn-apple.com/datasets/pico-banana-300k/nb/jsonl/preference.jsonl" rel="nofollow">preference link</a>, and <a href="https://ml-site.cdn-apple.com/datasets/pico-banana-300k/nb/jsonl/multi-turn.jsonl" rel="nofollow">multi-turn link</a>. If you hit rate limit with Flickr when downloading images, you can either request higher rate limit with Flickr or follow steps below.</p>
<p dir="auto">Another way to download the source images is to download packed files train_0.tar.gz and train_1.tar.gz from <a href="https://github.com/cvdfoundation/open-images-dataset#download-images-with-bounding-boxes-annotations">Open Images</a>, then map with the urls we provide. We also provide a sample mapping code <a href="https://blog.veitheller.de/apple/pico-banana-400k/blob/main/map_openimage_url_to_local.py">here</a>. Due to legal requirements, we cannot provide the source image files directly.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install awscli(https://aws.amazon.com/cli/)
# Download Open Images packed files 
aws s3 --no-sign-request --endpoint-url https://s3.amazonaws.com cp s3://open-images-dataset/tar/train_0.tar.gz . 
aws s3 --no-sign-request --endpoint-url https://s3.amazonaws.com cp s3://open-images-dataset/tar/train_1.tar.gz . 

# Create folder for extracted images 
mkdir openimage_source_images

# Extract the tar files 
tar -xvzf train_0.tar.gz -C openimage_source_images
tar -xvzf train_1.tar.gz -C openimage_source_images

# Download metadata CSV (ImageID ‚Üî OriginalURL mapping)  
wget https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv

# Map urls to local paths
python map_openimage_url_to_local.py #please modify variable is_multi_turn and file paths as needed"><pre><span><span>#</span> install awscli(https://aws.amazon.com/cli/)</span>
<span><span>#</span> Download Open Images packed files </span>
aws s3 --no-sign-request --endpoint-url https://s3.amazonaws.com cp s3://open-images-dataset/tar/train_0.tar.gz <span>.</span> 
aws s3 --no-sign-request --endpoint-url https://s3.amazonaws.com cp s3://open-images-dataset/tar/train_1.tar.gz <span>.</span> 

<span><span>#</span> Create folder for extracted images </span>
mkdir openimage_source_images

<span><span>#</span> Extract the tar files </span>
tar -xvzf train_0.tar.gz -C openimage_source_images
tar -xvzf train_1.tar.gz -C openimage_source_images

<span><span>#</span> Download metadata CSV (ImageID ‚Üî OriginalURL mapping)  </span>
wget https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv

<span><span>#</span> Map urls to local paths</span>
python map_openimage_url_to_local.py <span><span>#</span>please modify variable is_multi_turn and file paths as needed</span></pre></div>

<p dir="auto">Pico-Banana-400K is released under the Creative Commons Attribution‚ÄìNonCommercial‚ÄìNoDerivatives (CC BY-NC-ND 4.0) license.
‚úÖ Free for research and non-commercial use
‚ùå Commercial use and derivative redistribution are not permitted
üñºÔ∏è Source images follow the Open Images (CC BY 2.0) license
By using this dataset, you agree to comply with the terms of both licenses.</p>

<p dir="auto">If you use <strong>üçå Pico-Banana-400K</strong> in your research, please cite it as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{Qian2025PicoBanana400KAL,
  title={Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing},
  author={Yusu Qian and Eli Bocek-Rivele and Liangchen Song and Jialing Tong and Yinfei Yang and Jiasen Lu and Wenze Hu and Zhe Gan},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:282272484}
}

"><pre><span>@inproceedings</span>{<span>Qian2025PicoBanana400KAL</span>,
  <span>title</span>=<span><span>{</span>Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Yusu Qian and Eli Bocek-Rivele and Liangchen Song and Jialing Tong and Yinfei Yang and Jiasen Lu and Wenze Hu and Zhe Gan<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
  <span>url</span>=<span><span>{</span>https://api.semanticscholar.org/CorpusID:282272484<span>}</span></span>
}

</pre></div>
</article></div></div>
  </body>
</html>
