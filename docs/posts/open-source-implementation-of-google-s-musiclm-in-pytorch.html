<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lucidrains/musiclm-pytorch">Original</a>
    <h1>Open source implementation of Google&#39;s MusicLM in PyTorch</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://healeycodes.com/lucidrains/musiclm-pytorch/blob/main/musiclm.png"><img src="https://healeycodes.com/lucidrains/musiclm-pytorch/raw/main/musiclm.png" width="450px"/></a></p>
<h2 dir="auto"><a id="user-content-musiclm---pytorch-wip" aria-hidden="true" href="#musiclm---pytorch-wip"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MusicLM - Pytorch (wip)</h2>
<p dir="auto">Implementation of <a href="https://google-research.github.io/seanet/musiclm/examples/" rel="nofollow">MusicLM</a>, Google&#39;s new SOTA model for music generation using attention networks, in Pytorch.</p>
<p dir="auto">They are basically using text-conditioned <a href="https://github.com/lucidrains/audiolm-pytorch">AudioLM</a>, but surprisingly with the embeddings from a text-audio contrastive learned model named <a href="https://arxiv.org/abs/2208.12415" rel="nofollow">MuLan</a>. MuLan is what will be built out in this repository, with AudioLM modified from the other repository to support the music generation needs here.</p>
<h2 dir="auto"><a id="user-content-appreciation" aria-hidden="true" href="#appreciation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Appreciation</h2>
<ul dir="auto">
<li><a href="https://stability.ai/" rel="nofollow">Stability.ai</a> for the generous sponsorship to work and open source cutting edge artificial intelligence research</li>
</ul>
<h2 dir="auto"><a id="user-content-citations" aria-hidden="true" href="#citations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Citations</h2>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{Agostinelli2023MusicLMGM,
  title     = {MusicLM: Generating Music From Text},
  author    = {Andrea Agostinelli and Timo I. Denk and Zal{\&#39;a}n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matthew Sharifi and Neil Zeghidour and C. Frank},
  year      = {2023}
}"><pre><span>@inproceedings</span>{<span>Agostinelli2023MusicLMGM</span>,
  <span>title</span>     = <span><span>{</span>MusicLM: Generating Music From Text<span>}</span></span>,
  <span>author</span>    = <span><span>{</span>Andrea Agostinelli and Timo I. Denk and Zal{\&#39;a}n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matthew Sharifi and Neil Zeghidour and C. Frank<span>}</span></span>,
  <span>year</span>      = <span><span>{</span>2023<span>}</span></span>
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@article{Huang2022MuLanAJ,
    title   = {MuLan: A Joint Embedding of Music Audio and Natural Language},
    author  = {Qingqing Huang and Aren Jansen and Joonseok Lee and Ravi Ganti and Judith Yue Li and Daniel P. W. Ellis},
    journal = {ArXiv},
    year    = {2022},
    volume  = {abs/2208.12415}
}"><pre><span>@article</span>{<span>Huang2022MuLanAJ</span>,
    <span>title</span>   = <span><span>{</span>MuLan: A Joint Embedding of Music Audio and Natural Language<span>}</span></span>,
    <span>author</span>  = <span><span>{</span>Qingqing Huang and Aren Jansen and Joonseok Lee and Ravi Ganti and Judith Yue Li and Daniel P. W. Ellis<span>}</span></span>,
    <span>journal</span> = <span><span>{</span>ArXiv<span>}</span></span>,
    <span>year</span>    = <span><span>{</span>2022<span>}</span></span>,
    <span>volume</span>  = <span><span>{</span>abs/2208.12415<span>}</span></span>
}</pre></div>
</article>
          </div></div>
  </body>
</html>
