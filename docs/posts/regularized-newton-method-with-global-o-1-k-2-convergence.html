<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2112.02089">Original</a>
    <h1>Regularized Newton Method with Global $O(1/k^2)$ Convergence</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/2112.02089">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  We present a Newton-type method that converges fast from any initialization
and for arbitrary convex objectives with Lipschitz Hessians. We achieve this by
merging the ideas of cubic regularization with a certain adaptive
Levenberg--Marquardt penalty. In particular, we show that the iterates given by
$x^{k+1}=x^k - \bigl(\nabla^2 f(x^k) + \sqrt{H\|\nabla f(x^k)\|}
\mathbf{I}\bigr)^{-1}\nabla f(x^k)$, where $H&gt;0$ is a constant, converge
globally with a $\mathcal{O}(\frac{1}{k^2})$ rate. Our method is the first
variant of Newton&#39;s method that has both cheap iterations and provably fast
global convergence. Moreover, we prove that locally our method converges
superlinearly when the objective is strongly convex. To boost the method&#39;s
performance, we present a line search procedure that does not need
hyperparameters and is provably efficient.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Konstantin Mishchenko [<a href="https://arxiv.org/show-email/70bc0207/2112.02089">view email</a>]
      </p></div></div>
  </body>
</html>
