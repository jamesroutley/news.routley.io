<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hazyresearch.stanford.edu/blog/2024-10-29-tk2">Original</a>
    <h1>ThunderKittens: Simple, fast, and adorable AI kernels</h1>
    
    <div id="readability-page-1" class="page"><div><p><a href="https://arxiv.org/abs/2410.20399"><strong>Paper</strong></a> | <a href="https://github.com/HazyResearch/ThunderKittens"><strong>Code</strong></a> | <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-quick-tk"><strong>TK Part 1</strong></a> | <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk"><strong>Brr</strong></a></p>
<hr/>
<p>Fiveish months ago, we put out our posts on <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-quick-tk">ThunderKittens</a> and <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">GPUs</a>, and were pleasantly surprised by their warm reception on The Platform Formerly Known as Twitter.</p>
<p>We now return in a continuation of our long-standing efforts to throw compute to the wolves. We offer you, dearest reader, several new and improved gifts:</p>
<ol>
<li>Lots of new, exciting kernels;</li>
<li>Talking models;</li>
<li>Extra adorable kittens;</li>
<li>Attention kernels: faster and more flexible;</li>
<li>Us!</li>
</ol>
<p>And purrfect medieval kitten masterpieces throughput!</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/fav.png"/></figure>
<h3>Moar Architectures</h3>
<p>The main raison d’etre for TK has always been to help us write kernels for -- and do research on -- new architectures. Correspondingly, we’re releasing a bunch of kernels for various new operations (and some old ones, too); these range from “a bit” to “much” faster than comparable implementations.</p>
<ul>
<li>Fused Mamba-2, several times faster than the current Triton implementation, mostly through more aggressive kernel fusions. (Note it uses a slightly different layout than the normal Triton implementation, but it might be handy.)</li>
</ul>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/mamba-2_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/mambakitty.png"/></figure>
<ul>
<li>Long convolutions -- at sequence length 4096, we can get up to ~9x over the FlashFFTConv implementation!</li>
</ul>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/conv_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/rar.png"/></figure>
<ul>
<li>Linear attentions – Based and LoLCATS Hedgehog linear attention 14x and 6.5x faster than Fast Linear Attention Triton implementations, by being careful about register usage and using H100 features.</li>
</ul>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/based_fwd_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/hedgehog_fwd_len2tflops.png"/></figure>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/basedkitty.png"/></figure>
<ul>
<li>Rope, LayerNorm, Linear layers -- each competitive with (or sometimes faster than) existing implementations, while being pretty concise and readable.</li>
</ul>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/gemm_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/rotary_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/fusednorm_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/shock_cropped.png"/></figure>
<h3>Meow meow meow. Meow meow meow MEOW!</h3>
<p>Talking kittens are the best kittens. We’ve pushed a couple of demo integrations to help your kittens talk to you, and even to teach them a thing or two!</p>
<ul>
<li>Attention -- TK kernels support Llama3 8B and Qwen 2.5 7B. We’ve integrated some demo scripts, to make running these models as easy as <code>cd demos/llama_demo &amp;&amp; bash demo_8b.sh</code>. We’ve added example training integrations with both nanoGPT and PyTorch Lightning, and conducted several successful training runs.</li>
<li>Following up on our recent LoLCATs work, we’ve included a forwards prefill kernel and a demo integration -- just run <code>cd demos/lolcats_demo &amp;&amp; bash demo_8b.sh</code>.</li>
<li>Based is a linear attention architecture that combines short sliding window attentions with large-state-size linear attentions. To use TK’s prefill kernel (optimized for high-throughput scenarios), there are several example scripts in <code>demos/based_demo</code>.</li>
</ul>
<h3>Enhanced Kittens</h3>
<p>Finally, we’re excited to release our first major batch of improvements to ThunderKittens to take it from, “seems cool,” (0.0) to, “actually useful” (0.01). In no particular order. ThunderKittens really is now easier, better, faster, and most importantly, cuter!</p>
<ul>
<li>A proper build system. Just <code>python setup.py install</code> and use prebuilt ThunderKittens kernels in your code!</li>
<li>No more shared layouts. We’ll automatically arrange and swizzle your shared memory tiles for you; just tell us how big.</li>
<li>This also comes with a pretty significant performance boost, because we finally figured out how to get rid of the stupid interleave format, reduce bank conflicts, and improve L2 transaction coalescing.</li>
<li>Global layout descriptors, so that you can just pretend everything is a tensor like it is in Pytorch. No more stride calculations! And significantly reduced padding requirements, too.</li>
<li>Broader type support, including (serious, not-hacked) global and shared FP16 and FP32 support. FP8... eventually.</li>
<li>Many more robust tests. Tens of thousands!</li>
<li>Templates to handle much of the boilerplate setup and coordination of barriers. Just write load, compute, and store functions! Here’s an example non-causal attention kernel within this framework, which is a good deal faster than the ones we released in May, and pretty much matches FA-3.</li>
</ul>

<p>For more on these demos -- an an FA2 demo, too -- take a look <a href="https://github.com/HazyResearch/ThunderKittens/tree/main/kernels/attn/demo">here</a>.</p>
<p>We’re particularly excited about ThunderKittens because, in addition to enabling fast and diverse kernels, ThunderKittens remains quite transparent to the underlying hardware. We want you to actually know what your code is really doing!</p>
<h3>All Your Attention Are Belong to Us</h3>
<p>We were excited to see Tri&amp;team <a href="https://tridao.me/publications/flash3/flash3.pdf">release FA3</a> a few months ago. We had a lot of fun taking apart the kernel and <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">learned a few tricks</a>! Correspondingly, we’re releasing a swath of kernels for attention, including some that exceed FA3’s performance -- especially on the backwards pass!</p>
<p>A surprising result we found is that very little of the speedup found in FA-3 comes from the complex ping-ponging algorithm overlapping matrix multiplies and non-tensor ops. Instead, almost all of it comes from just using the GPU better! It’s really all standard things, like being careful with the register file, minimizing memory movement, etc. Consequently, our current implementation actually skips most of the complexity found in FA-3 -- but it still retains most of the performance on the forwards pass, and is noticeably faster on the backwards.</p>
<p>Our kernels are prebuilt, so that if you don&#39;t want to know how the sausage is made, you can still enjoy ThunderKittens. We support major open-source models (e.g. Llama’s, Qwen’s etc), with demo integrations in the repo. And if you want to add your favorite new feature, we’ve made it as easy as we know how. Here’s how ThunderKittens currently stacks up against FA3.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/attn_c_fwd_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/attn_c_bwd_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/attn_nc_fwd_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/attn_nc_bwd_len2tflops.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2024-10-29-tk2/sunny.png"/></figure>
<h3>Us!</h3>
<p>This coming Thursdsay (Oct 31) in celebration of Halloween, we’ll (Ben, Simran, and Aaryan) be doing a second livestream of ThunderKittens (<a href="https://www.youtube.com/watch?v=xcpEl0cGCC4">previous one</a>). Come hang out!</p>
<p>If you can’t make it, fear not: we’ll also be putting out a series of in-depth posts on more of our learnings about kernel optimization over the next few weeks, too. We&#39;ll also be on the GPU Mode discord, come hang out and write TK kernels with us!</p></div></div>
  </body>
</html>
