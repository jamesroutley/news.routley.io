<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://akashgoswami.dev/posts/automating-my-now-page/">Original</a>
    <h1>Automating My /Now Page</h1>
    
    <div id="readability-page-1" class="page"><div><p>Here‚Äôs a small deep dive into how I automated <a href="https://akashgoswami.com/now/">my now page</a> on my personal website using Golang and GitHub Actions.</p>
<p>First of all, you might be wondering what a now page actually is. Most websites have an ‚Äòabout‚Äô page which explains the background of a certain individual or a business.</p>
<p>A ‚Äònow‚Äô page is a page that tells you what a person is focused on currently. Now pages have become more popular on personal websites and you can find several people with now pages on <a href="https://nownownow.com/">nownownow</a> (<a href="https://nownownow.com/p/1M0p">including my own</a>).</p>
<p>So a while ago I added a now page to share what I was up to, what I‚Äôm currently learning, some fitness goals, places I‚Äôve travelled to this year and in the last year as well as some stats around what media (books, movies, TV shows and video games) I‚Äôve recently been reading/watching/playing.</p>
<p>Wanting to share all of this information led to one issue‚Ä¶</p>
<h2 id="manual-updates-are-difficult">Manual updates are difficult</h2>
<p>My website is a static website built using <a href="https://gohugo.io/">Hugo</a>. That means to make a change to the now page, I‚Äôd need to manually update the now file, commit the new changes made to GitHub and then merge the new changes into my main branch on GitHub.</p>
<p>Doing this once or twice wasn‚Äôt so bad, but with how frequently I needed to update the media section of my now page, the process started getting repetitive and time-consuming.</p>
<p>So I thought, what if I could automate the now page? üöÄ</p>
<h2 id="inspiration">Inspiration</h2>
<p>Before getting started, I had a look around online to see if anyone else had managed to do something like this and if they had open-sourced their code.</p>
<p>I found that another blogger/software engineer, Robb Knight had already made a post, ‚Äò<a href="https://rknight.me/blog/automating-my-now-page/">Automating My Now Page</a>‚Äô that was trying to solve the exact problem I had with my now page.</p>
<p>Robb‚Äôs solution was to use other services to track what he was reading, watching, listening to and playing, fetching that data periodically and then formatting and using that data in a way that was useful for his now page.</p>
<p>This made me realise I was going to need a good ‚Äòsource‚Äô or service which would allow me to track these things. Exploring this, led me to another blog post, this time written by Sophie Koonin about how ‚Äò<a href="https://localghost.dev/blog/everything-should-have-an-api-adventures-in-trying-to-automate-stuff/">Everything should have an API</a>‚Äô which helped me find some services that I could use to track some of my media consumption.</p>
<p>One key difference I found with Robb and Sophie‚Äôs websites compared to my own was that theirs were made using another static site generator called ‚Äò<a href="https://www.11ty.dev/">Eleventy</a>‚Äô which was made with JavaScript compared to Hugo‚Äôs Go codebase. I wanted to work on my Golang scripting skills so I realised pretty early on that I was going to have to start this project from scratch if I wanted to write a Go script to automate my now page.</p>
<p>Before doing so, I ended up searching for some examples of how others had used Go scripts to automate stuff and came across a great post by <a href="https://victoria.dev/">Victoria Drake</a> on how you could <a href="https://victoria.dev/blog/go-automate-your-github-profile-readme/">update your GitHub Profile README using Golang</a>. This tutorial led me to discover a handy little Go package called, ‚Äò<a href="https://github.com/mmcdole/gofeed">gofeed</a>‚Äô that can be used to read RSS feeds in Go. Victoria also covered how to run the Go script on a schedule using GitHub Actions in the later part of the tutorial.</p>
<p>To put my understanding to the test, I ended up following the tutorial to automate my own GitHub Profile README by pulling in the latest posts from my websites using RSS. I added some extra parts to the script and ended up with a <a href="https://github.com/skyth3r">fantastic README</a> that updated automatically.</p>
<p>Armed with some confidence from this mini side-project, I decided to start looking into the best way of sourcing the data for each media type I wanted to track‚Ä¶</p>
<h2 id="automating">Automating!</h2>
<p>Given that the gofeed package made things super simple to fetch data using RSS feeds, I decided to first look into services that offered RSS feeds for tracking things. This search eventually led me to a service for tracking movies‚Ä¶</p>
<h3 id="movies">Movies</h3>
<p>To track movies I‚Äôd watched, I first looked into a free service that I knew a few of my friends were already using, <a href="https://letterboxd.com/">Letterboxd</a>. Immediately, I managed to spot that profiles on Letterboxd had a small RSS icon at the end of the sub-menu. Clicking this revealed an RSS URL that contained a list of movies the user had recently watched. After doing a bit of testing, I found the only way to add items to this RSS feed is to click the ‚ÄòReview or log‚Äô button on a movie, rather than just marking it as ‚Äòwatched‚Äô.</p>
<p>Once I figured out how to add movies to the RSS feed, I wrote a function to return a slice of elements (specifically of gofeed.Item‚Äôs) from the RSS feed via the gofeed package.</p>
<pre tabindex="0"><code>func getLetterboxdItems(input string) ([]gofeed.Item, error) {
	var items []gofeed.Item

	feedParser := gofeed.NewParser()
	feed, err := feedParser.ParseURL(input)

	if err != nil {
		return nil, err
	}

	for _, item := range feed.Items {
		items = append(items, *item)
	}

	return items, nil
}
</code></pre><h4 id="obtaining-titles-and-urls">Obtaining titles and URLs</h4>
<p>After getting back the slice of elements from this function, the only data I needed from each element in the slice was the movie‚Äôs title and the URL of the movie‚Äôs page on the Letterboxd website.</p>
<p>I wrote a function to loop through the elements of the slice and store the title and URL of the element in a new map, which was then added to a slice of maps.</p>
<pre tabindex="0"><code>func latestItems(items []gofeed.Item, count int) []map[string]string {
	var itemSlice []map[string]string

	for i := 0; i &lt; count; i++ {
		item := make(map[string]string)
		item[&#34;title&#34;] = items[i].Title
		item[&#34;url&#34;] = items[i].Link
		itemSlice = append(itemSlice, item)
	}
	return itemSlice
}
</code></pre><h4 id="parsing-titles-and-urls">Parsing titles and URLs</h4>
<p>While I was able to get the title and URL for each movie, I noticed that some issues with both would prevent me from being able to use the data on my now page.</p>
<p>The movie title field would contain the movie name followed by a comma and the year that the movie was released. Additionally, if a user reviewed or rated the movie, then the stars were also included in the title. E.g. If a movie with the title ‚ÄòDeadpool &amp; Wolverine‚Äô, released in 2024, was logged on a user‚Äôs Letterboxd page with a 5-star review then the title for the RSS feed item would be ‚ÄòDeadpool &amp; Wolverine, 2024 - ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚Äô.</p>
<p>I wanted to omit the comma followed by the year and optional star rating, so I created a new function to parse these parts out from the title using regex (regex pattern was created with the help of ChatGPT).</p>
<pre tabindex="0"><code>const movieTitlePattern = `, (\d{4})(?: - ?[‚òÖ]{0,5}(¬Ω)?)?$`

func GetMovieTitle(input string) string {
	// Removes &#39;, YYYY - ‚òÖ‚òÖ‚òÖ‚òÖ&#39; from movie titles
	// The regex pattern looks for the following in a movie title:
	// - `, 2020` (No rating given)
	// - `, 2020 - ‚òÖ‚òÖ‚òÖ‚òÖ` (rating given)
	re := regexp.MustCompile(movieTitlePattern)
	title := re.Split(input, -1)
	return title[0]
}
</code></pre><p>To ensure that my regex was correct, I ended up also writing a unit test (using the <a href="https://github.com/stretchr/testify">testify package</a>) with a table of test cases of movie title variations to check the regex extracted just the title of the movie.</p>
<pre tabindex="0"><code>func TestGetMovieTitle(t *testing.T) {
	tests := []struct {
		title    string
		expected string
	}{
		{&#34;Movie Title, 2024&#34;, &#34;Movie Title&#34;},
		{&#34;Movie Title, the sequel, 2023 - ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ&#34;, &#34;Movie Title, the sequel&#34;},
		{&#34;Movie - Title, 2022 - ‚òÖ‚òÖ‚òÖ‚òÖ&#34;, &#34;Movie - Title&#34;},
		{&#34;Movie Title and the movie title, 2021 - ‚òÖ‚òÖ‚òÖ&#34;, &#34;Movie Title and the movie title&#34;},
		{&#34;Movie, Title, 2020 - ‚òÖ‚òÖ&#34;, &#34;Movie, Title&#34;},
		{&#34;Movie, - Title, 2019 - ‚òÖ&#34;, &#34;Movie, - Title&#34;},
		{&#34;The Movie, 2023 - ‚òÖ‚òÖ¬Ω&#34;, &#34;The Movie&#34;},
		{&#34;Movie Title, 2018 - &#34;, &#34;Movie Title&#34;},
		{&#34;Movie Title&#34;, &#34;Movie Title&#34;},                 // Edge case: No year or rating
		{&#34;2024, Movie Title&#34;, &#34;2024, Movie Title&#34;},     // Edge case: Year at the start
		{&#34;Movie Title - ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ&#34;, &#34;Movie Title - ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ&#34;}, // Edge case: Rating but no year
	}

	for i := range tests {
		title := tests[i].title
		expected := tests[i].expected

		actual := GetMovieTitle(title)
		require.Equal(t, expected, actual)
	}
}
</code></pre><p>Once I was happy with this I then looked into my second issue. The URL field contained a URL for the user‚Äôs log of the movie rather than just the regular URL of the movie‚Äôs Letterboxd page.</p>
<p>For example, if a user called ‚ÄòMovieWatcher‚Äô logged the movie ‚ÄòDeadpool &amp; Wolverine‚Äô, then the URL field would be ‚Äò<a href="https://letterboxd.com/MovieWatcher/film/deadpool-wolverine/&#39;">https://letterboxd.com/MovieWatcher/film/deadpool-wolverine/&#39;</a> rather than ‚Äò<a href="https://letterboxd.com/film/deadpool-wolverine/&#39;">https://letterboxd.com/film/deadpool-wolverine/&#39;</a>.</p>
<p>I needed to remove the username part of the URL, so I created another function and made use of another regex expression to achieve this.</p>
<pre tabindex="0"><code>const (
	Url                  = &#34;https://letterboxd.com/&#34;
	movieUrlWithUsername = `https:\/\/letterboxd\.com\/([^\/]+)\/`
)

func GetMovieUrl(movieUrl string) string {
	// Get Letterboxd item link without the username
	// Replaces &#34;https://letterboxd.com/USERNAME_HERE/film/MOVIE_TITLE/&#34; with &#34;https://letterboxd.com/film/MOVIE_TITLE/&#34;
	usernamePattern := regexp.MustCompile(movieUrlWithUsername)
	formattedUrl := usernamePattern.ReplaceAllString(movieUrl, Url)
	return formattedUrl
}
</code></pre><p>Again, the same as the previous function, I wrote some unit tests for this new function as well to ensure it worked correctly.</p>
<pre tabindex="0"><code>func TestGetMovieUrl(t *testing.T) {
	tests := []struct {
		url      string
		expected string
	}{
		{&#34;https://letterboxd.com/USERNAME_HERE/film/Movie/&#34;, &#34;https://letterboxd.com/film/Movie/&#34;},
		{&#34;https://letterboxd.com/USERNAME_HERE/film/Movie-Title&#34;, &#34;https://letterboxd.com/film/Movie-Title&#34;},
		{&#34;https://letterboxd.com/USERNAME_HERE/film/Movie-Title-and-the-movie-title&#34;, &#34;https://letterboxd.com/film/Movie-Title-and-the-movie-title&#34;},
	}

	for i := range tests {
		url := tests[i].url
		expected := tests[i].expected

		actual := GetMovieUrl(url)
		require.Equal(t, expected, actual)
	}
}
</code></pre><p>Up next, displaying what I was reading‚Ä¶</p>
<h3 id="books">Books</h3>
<p>At the time I was already using <a href="https://www.goodreads.com/">GoodReads</a> (extremely infrequently) to track what I was reading. So I thought it was a good starting point to look into if I could get my reading history via RSS from GoodReads.</p>
<p>It turned out that GoodReads did provide an RSS feed, but it has a lot of issues. For some reason, the feed wasn‚Äôt a valid feed and it was lacking some granularity - I couldn‚Äôt group books in the way I wanted and in the end, I decided to start looking into a new book tracking service to track what I was reading/had already read.</p>
<p>Sophie mentioned a website called <a href="https://oku.club/">Oku</a> in her blog post, so I decided to look into that first. After playing around with the website, I found that it was perfect for what I wanted!</p>
<p>The UI for Oku is way better than GoodRead‚Äôs site (sorry to any hardcore GoodRead users), I could create ‚Äòcollections‚Äô to sort and organise books, and also track what I‚Äôd already read, what I wanted to read in the future as well as what I was currently reading. The best part of all of this was that each ‚Äòcollection‚Äô had a <strong>separate</strong> RSS feed!</p>
<p>Initially, I was a bit puzzled about how to find the RSS feed URL for a collection, but Oku had already <a href="https://oku.club/blog/oku-has-rss-feeds">written a guide</a> on how to find this URL. It was hidden away in the page source. Not a massive issue, but it would‚Äôve been nice to have an RSS icon somewhere in the front end instead. The guide also included a few examples of others who were displaying what they were reading on their own websites using these RSS feeds.</p>
<p>Since both books and movies were being fetched via RSS feeds, I decided to make the initial <code>getLetterboxdItems</code> function into a more generic function that could be used with any valid RSS feed link. I ended up renaming it to <code>getGoFeedItems</code>.</p>
<pre tabindex="0"><code>func getGoFeedItems(input string) ([]gofeed.Item, error) {
	var feedItems []gofeed.Item

	feedParser := gofeed.NewParser()
	feed, err := feedParser.ParseURL(input)

	if err != nil {
		return nil, err
	}

	for _, item := range feed.Items {
		feedItems = append(feedItems, *item)
	}

	return feedItems, nil
}
</code></pre><p>I then also adapted the <code>latestItems</code> function that would also work with non-Letterboxd RSS feeds. I decided to implement this via a branching logic flow that checked if a link for an item had a prefix of ‚Äú<a href="https://letterboxd.com">https://letterboxd.com</a>‚Äù to determine if it was a Letterboxd feed item. The final function was called <code>latestGoFeedItems</code>.</p>
<pre tabindex="0"><code>func latestGoFeedItems(items []gofeed.Item, count int) []map[string]string {
	var itemSlice []map[string]string

	for i := 0; i &lt; count; i++ {
		item := make(map[string]string)
		if strings.HasPrefix(items[i].Link, &#34;https://letterboxd.com&#34;) {
			item[&#34;title&#34;] = letterboxd.GetMovieTitle(items[i].Title)
			item[&#34;url&#34;] = letterboxd.GetMovieUrl(items[i].Link)
		} else {
			item[&#34;title&#34;] = items[i].Title
			item[&#34;url&#34;] = items[i].Link
		}
		itemSlice = append(itemSlice, item)
	}
	return itemSlice
}
</code></pre><h3 id="video-games">Video Games</h3>
<p>Next up was figuring out how to get a list of video games I was either currently playing or had recently played across all systems I own (at the time of writing, I play video games on my Nintendo Switch and my gaming PC).</p>
<p>Again, referring to what Robb and Sophie had already looked into, I found that Robb was scraping his latest trophies from <a href="https://psnprofiles.com/">psnprofiles.com</a> for PlayStation games and Sophie had looked into <a href="https://rawg.io/">rawg.io</a>. Unfortunately, neither of these would work for what I was looking to do with my now page. I don‚Äôt own a PlayStation so the functionality around PSN logging was a bit redundant for myself. Sophie mentioned in her post that scraping wasn‚Äôt possible with rawg.io as the whole site was a single-page app and that it didn‚Äôt actually log what games you were currently playing, but rather just games that you have.</p>
<p>My search led me to an alternative website called <a href="https://backloggd.com/">Backloggd</a>. The site is free to use and includes games from every platform (powered by <a href="https://www.igdb.com/">The Internet Game Database</a>).</p>
<p>I ended up creating a profile and started adding in games I‚Äôd already played, games I wanted to play (or more specifically games on my ‚Äòbacklog‚Äô) and then finally I added the games I was currently playing. The UI was easy to use, and finding games was a breeze (a manual process unfortunately but still great overall), but that was when I came across my main issue with the service‚Ä¶</p>
<p>There were no RSS feeds or API that I could use to fetch data on games I was currently playing üôÉ</p>
<p>I ended up joining the community Discord to see if this had been planned for development. I found a few mentions of other users requesting RSS feeds or an API but it didn‚Äôt seem to be prioritised and there was no official news from the site developer on if this was going to be added any time soon.</p>
<p>Despite this clear setback, I wasn‚Äôt giving up just yet. In the past, I‚Äôd done a lot of web scraping using Python and given I could access my profile page without having to log in, I started working if I could web scrape the page using Golang ü§î</p>
<p>Web scraping in Go isn‚Äôt something I‚Äôd done before, so I first started by checking if there was a package similar to Python‚Äôs selenium. My search led me to two packages. A version of <a href="https://github.com/tebeka/selenium">selenium for Go</a> and <a href="https://go-colly.org/">Colly</a>.</p>
<p>Using selenium would have required downloading/maintaining some dependencies (mainly a web driver) so I looked into Colly as a lighter-weight alternative. After going through some of the docs, and testing what I‚Äôd learned via some tutorials, I was able to put together a great little sub-function that scrapped the games under the ‚ÄòPlaying‚Äô section of my Backloggd profile üéâ</p>
<p>Similar to Letterboxd movies and Oku books, as I was also able to scrape the page URLs for each game, I added those to my now page as well for each game I was currently playing.</p>
<p>My web scraping function returns a map of items consisting of titles and URLs. Since I could be playing 3 or more games at a point of time (not all at the same time of course), I did not end up adding anything that limited the number of items included in the map.</p>
<pre tabindex="0"><code>func GetGames(url string) ([]map[string]string, error) {
	var games []map[string]string

	c := colly.NewCollector()

	c.OnHTML(&#34;div.rating-hover&#34;, func(e *colly.HTMLElement) {
		game := make(map[string]string)
		game[&#34;title&#34;] = e.ChildText(&#34;div.game-text-centered&#34;)
		game[&#34;url&#34;] = Url + e.ChildAttr(&#34;a&#34;, &#34;href&#34;)
		games = append(games, game)
	})

	err := c.Visit(url)
	if err != nil {
		return nil, err
	}

	if len(games) == 0 {
		err := errors.New(&#34;no games found&#34;)
		return nil, err
	}

	return games, nil
}
</code></pre><h3 id="tv-shows">TV Shows</h3>
<p>I watch quite a few TV shows and I‚Äôve always found keeping track of them difficult. I really liked Letterboxd but it didn‚Äôt really support TV shows and I really wanted something similar to track what I was watching.</p>
<p>I found two websites that offered what I wanted; <a href="https://trakt.tv/">Trakt</a> and <a href="https://www.serializd.com/">Serializd</a>.</p>
<p>Looking into Trakt first, I found that they did offer an RSS feed I could make use of, however, this was a feature that was only available to paid VIP users of the site. While the subscription wasn‚Äôt too much, I had little use of all the other features that were included in the VIP offering so I decided to look into Serializd instead.</p>
<p>Serializd has a great UI and an easy way to track and log what you‚Äôve watched. Unfortunately, it didn‚Äôt offer an RSS feed but I ended up doing some digging into the network calls the website made via Chrome dev tools and found some API endpoints that could be used to fetch recently watched TV shows and episodes via the diary API endpoint üéâ</p>

<p>Using this URL endpoint, I crafted a new HTTP GET request with some required headers and created a client to make the HTTP request and capture the response from the website.</p>
<pre tabindex="0"><code>req, err := http.NewRequest(&#34;GET&#34;, url, nil)
if err != nil {
	return nil, err
}

// Request headers
req.Header.Set(&#34;Accept&#34;, &#34;application/json, text/plain, */*&#34;)
req.Header.Set(&#34;Accept-Encoding&#34;, &#34;gzip, deflate, br, zstd&#34;)
req.Header.Set(&#34;Accept-Language&#34;, &#34;en-US,en;q=0.9&#34;)
req.Header.Set(&#34;Dnt&#34;, &#34;1&#34;)
req.Header.Set(&#34;Referer&#34;, url)
req.Header.Set(&#34;Sec-Ch-Ua&#34;, `&#34;Chromium&#34;;v=&#34;123&#34;, &#34;Not:A-Brand&#34;;v=&#34;8&#34;`)
req.Header.Set(&#34;Sec-Ch-Ua-Mobile&#34;, &#34;?1&#34;)
req.Header.Set(&#34;Sec-Ch-Ua-Platform&#34;, `&#34;Android&#34;`)
req.Header.Set(&#34;Sec-Fetch-Dest&#34;, &#34;empty&#34;)
req.Header.Set(&#34;Sec-Fetch-Mode&#34;, &#34;cors&#34;)
req.Header.Set(&#34;Sec-Fetch-Site&#34;, &#34;same-origin&#34;)
req.Header.Set(&#34;User-Agent&#34;, &#34;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Mobile Safari/537.36&#34;)
req.Header.Set(&#34;X-Requested-With&#34;, &#34;serializd_vercel&#34;)

client := &amp;http.Client{}
rsp, err := client.Do(req)
if err != nil {
   return nil, err
}
defer rsp.Body.Close()

if rsp.StatusCode != http.StatusOK {
	return nil, fmt.Errorf(&#34;unexpected status code: %v&#34;, rsp.StatusCode)
}
</code></pre><blockquote>
<p>This API exploration also led to me mapping out a number of these API endpoints the website had and I ended up putting together a small Golang package called <a href="https://github.com/Skyth3r/unserializd">unserializd</a>, as an unofficial way of accessing public data from profiles using Golang.</p>
</blockquote>
<p>I found that the contents of the response were not usable initially as the response was ‚Äògzipped‚Äô. To get around this, I added another part to the code to check if the response was gzipped and read the contents of the gzipped response using a NewReader function from the <a href="https://pkg.go.dev/compress/gzip">gzip package</a>.</p>
<pre tabindex="0"><code>// Check if the response is gzipped
var reader io.Reader
if rsp.Header.Get(&#34;Content-Encoding&#34;) == &#34;gzip&#34; {
	gz, err := gzip.NewReader(rsp.Body)
	if err != nil {
		return nil, err
	}
	defer gz.Close()
	reader = gz
} else {
	reader = rsp.Body
}

body, err := io.ReadAll(reader)
if err != nil {
	return nil, err
}
</code></pre><p>Now that the data was no longer gzip encoded, it was time to read the contents of the response. As the contents of the response were JSON encoded, I first created a <code>diary</code> struct in Go to match the data that was to be unmarshaled.</p>
<pre tabindex="0"><code>type SerializdDiary struct {
	Reviews      []SerializdDiaryReview `json:&#34;reviews&#34;`
	TotalPages   int                    `json:&#34;totalPages&#34;`
	TotalReviews int                    `json:&#34;totalReviews&#34;`
}

type SerializdDiaryReview struct {
	ID               int          `json:&#34;id&#34;`
	ShowID           int          `json:&#34;showId&#34;`
	SeasonID         int          `json:&#34;seasonId&#34;`
	SeasonName       string       `json:&#34;seasonName&#34;`
	DateAdded        string       `json:&#34;dateAdded&#34;`
	Rating           int          `json:&#34;rating&#34;`
	ReviewText       string       `json:&#34;reviewText&#34;`
	Author           string       `json:&#34;author&#34;`
	AuthorImageUrl   string       `json:&#34;authorImageUrl&#34;`
	ContainsSpoiler  bool         `json:&#34;containsSpoilers&#34;`
	BackDate         string       `json:&#34;backdate&#34;`
	ShowName         string       `json:&#34;showName&#34;`
	ShowBannerImage  string       `json:&#34;showBannerImage&#34;`
	ShowSeasons      []ShowSeason `json:&#34;showSeasons&#34;`
	ShowPremiereDate string       `json:&#34;showPremiereDate&#34;`
	IsRewatched      bool         `json:&#34;isRewatched&#34;`
	IsLogged         bool         `json:&#34;isLogged&#34;`
	EpisodeNumber    int          `json:&#34;episodeNumber&#34;`
}

type ShowSeason struct {
	ID           int    `json:&#34;id&#34;`
	Name         string `json:&#34;name&#34;`
	SeasonNumber int    `json:&#34;seasonNumber&#34;`
	PosterPath   string `json:&#34;posterPath&#34;`
}

var diary SerializdDiary

if err := json.Unmarshal(body, &amp;diary); err != nil {
	return nil, err
}
</code></pre><p>After unmarshalling the response, the part I was interested in was just the reviews section. So I stored the contents of that in a new variable and then looped through the slice of reviews to build a map of shows (with the season I watched) along with the show‚Äôs URL and stored this in the shows variable.</p>
<pre tabindex="0"><code>var shows []map[string]string

reviews := diary.Reviews

for r := range reviews {
	show := make(map[string]string)
	var showAndSeason string
	review := reviews[r]
	reviewSeasonID := review.SeasonID

	// Loop through review.showSeasons to find season name using review.SeasonID
	for s := range review.ShowSeasons {
		season := review.ShowSeasons[s]
		if reviewSeasonID == season.ID {
			review.SeasonName = season.Name
		}
	}

	// format showName with SeasonName and store in output
	showAndSeason = fmt.Sprintf(&#34;%v, %v&#34;, review.ShowName, review.SeasonName)
	show[&#34;title&#34;] = showAndSeason

	// get show url
	const showBaseUrl = &#34;https://www.serializd.com/show/&#34;
	showUrl := showBaseUrl + fmt.Sprint(review.ShowID)
	show[&#34;url&#34;] = showUrl

	// Append show to shows if shows[&#34;title&#34;] is not present in the map
	if !containsValue(shows, &#34;title&#34;, show[&#34;title&#34;]) {
		shows = append(shows, show)
	}
}
</code></pre><p>Before adding a show to the shows slice, I first wanted to check if the show‚Äôs title was already present in the shows slice and only add the show if it was not present to prevent duplicates. To help with this, I wrote a small utility function <code>containsValue</code>.</p>
<pre tabindex="0"><code>func containsValue(slice []map[string]string, key, value string) bool {
	for _, m := range slice {
		if _, ok := m[key]; ok {
			if val, ok := m[key]; ok &amp;&amp; val == value {
				return true
			}
		}
	}
	return false
}
</code></pre><p>To then limit the number of shows to display on my now page, I created a new function that would return the three latest shows in the map through the use of a count input variable.</p>
<pre tabindex="0"><code>func LatestShows(items []map[string]string, count int) []map[string]string {
	var shows []map[string]string
	for i := 0; i &lt; count; i++ {
		shows = append(shows, items[i])
	}
	return shows
}
</code></pre><h3 id="bonus---travel-stats">Bonus - Travel stats</h3>
<p>Finally, as a bonus, I wanted to see if I could also track places I‚Äôd visited automatically. I already made use of the trip tracker feature on <a href="https://nomadlist.com/">NomadList</a> and I noticed that there was an ‚ÄòExport as API‚Äô option on my NomadList profile‚Ä¶ üëÄ</p>

<p>Clicking this led me to a URL with my public data on the site in JSON, including previous trips!</p>
<p>Getting the data from this URL was no trouble, the main issues I faced were rather how I wanted to structure the data. With all the other previous media types, I had defaulted to only displaying up to three of the latest items for each media category. For travel stats, I wanted to include more than just the latest three trips I‚Äôd taken.</p>
<p>Initially, I thought about only displaying travel stats for the current year but realised this could remain empty towards the start of the year. So instead I thought about adding three years worth of travel stats but that also seemed a bit too much and would defeat the purpose of a ‚Äònow‚Äô page.</p>
<p>In the end, I compromised and added travel stats for the current year and the previous year. The code to handle this isn‚Äôt the prettiest and I have to admit, that parts of it are hard coded (boo) rather than being dynamic, but it got the job done. for 2023 and 2024 travel stats, the function <code>TripsInYear</code> takes the entire trips slice and a year in the format of a string and returns a subslice of the trips from that year only.</p>
<pre tabindex="0"><code>tripsIn2024 := nomadlist.TripsInYear(countries, &#34;2024&#34;)
tripsIn2023 := nomadlist.TripsInYear(countries, &#34;2023&#34;)

func TripsInYear(tripsInput []map[string]string, year string) []map[string]string {
	var tripsOutput []map[string]string

	for _, trip := range tripsInput {
		if trip[&#34;start_date&#34;][0:4] == year {
			tripsOutput = append(tripsOutput, trip)
		}
	}

	return tripsOutput
}
</code></pre><h4 id="travel-edge-cases">Travel edge cases</h4>
<p>I did come across a few minor edge cases with this travel data, so I created some mini functions to handle these.</p>
<p>My trip tracker from NomadList included when I was back in London but I didn‚Äôt want to pull through ‚Äôtrips‚Äô to London on my now page. The first mini function I made was to remove all trips from the slice that has a value of London for the ‚Äòplace‚Äô key.</p>
<pre tabindex="0"><code>tripsIn2024 = removeLondonTrips(tripsIn2024)
tripsIn2023 = removeLondonTrips(tripsIn2023)

func removeLondonTrips(countries []map[string]string) []map[string]string {
	var filteredCountries []map[string]string

	for _, trip := range countries {
		if trip[&#34;place&#34;] == &#34;London&#34; {
			continue
		}
		filteredCountries = append(filteredCountries, trip)
	}

	return filteredCountries
}
</code></pre><p>Next I wanted to remove duplicate countries from my trips slice. In this function I end up making use of the <code>containsValue</code> utility function I wrote earlier to only append countries to the slice if a trip[‚Äúname‚Äù] was not present in the slice.</p>
<pre tabindex="0"><code>countriesIn2024 := removeDupes(tripsIn2024)
countriesIn2023 := removeDupes(tripsIn2023)

func removeDupes(trips []map[string]string) []map[string]string {
	var countries []map[string]string

	// sorts trips from oldest to newest
	slices.Reverse(trips)

	for _, trip := range trips {
		// check if a trip[&#34;name&#34;] is present in the slice countries
		if !containsValue(countries, &#34;name&#34;, trip[&#34;name&#34;]) {
			countries = append(countries, trip)
		}
	}

	return countries
}
</code></pre><p>Removing the London edge case and duplicate countries was simple, however removing other trips in ‚ÄòEngland‚Äô wasn‚Äôt as easy as the default ‚Äòcountry‚Äô for England is actually just listed as ‚ÄòUK‚Äô on the NomadList website.</p>
<p>This led to a small issue with getting my travel stats on my visit to Scotland to pull through correctly, so I ended up <em>not</em> excluding UK trips from my 2023 travel stats (Scotland was the only non-London place I visited in 2023) through the use of a third mini function (I admit that this was a bit of a hacky fix).</p>
<pre tabindex="0"><code>tripsIn2023 = addScotlandTrip2023(tripsIn2023)

func addScotlandTrip2023(countries []map[string]string) []map[string]string {
	var filteredCountries []map[string]string

	for _, trip := range countries {
		if trip[&#34;name&#34;] == &#34;United Kingdom&#34; {
			trip[&#34;name&#34;] = &#34;Scotland&#34;
		}
		filteredCountries = append(filteredCountries, trip)
	}

	return filteredCountries
}
</code></pre><h4 id="country-flag-emojis">Country flag emojis</h4>
<p>Showing just the name of a country I visited on my now page seemed a bit boring. I wanted to also include a country‚Äôs emoji flag to each entry on my page. I found that in addition to being able to fetch countries from trip data on NomadList, there was also a field containing the country‚Äôs three/two-letter country code.</p>
<p>After a search online, I managed to find a go package called <a href="https://github.com/jayco/go-emoji-flag">Go Emoji Flag</a> that would be able to convert a country code into a flag emoji.</p>
<p>Using the country code from each trip, I performed a quick look-up and obtained the correct flag for each country that I visited and added that to the script as well.</p>
<pre tabindex="0"><code>const NoCountries  = &#34;Haven&#39;t visited any countries recently&#34;

func formatCountries(countries []map[string]string) string {
	var formattedText string
	var countryEmoji string

	if len(countries) == 0 {
		formattedText = NoCountries + &#34;\n\n&#34;
		return formattedText
	}

	for i := range countries {
		// UK country code needs to be GB to fetch correct emoji flag
		if countries[i][&#34;code&#34;] == &#34;UK&#34; {
			countries[i][&#34;code&#34;] = &#34;GB&#34;
		}
		// Handles Scotland edge case
		if countries[i][&#34;name&#34;] == &#34;Scotland&#34; {
			countryEmoji = &#34;\U0001F3F4\U000E0067\U000E0062\U000E0073\U000E0063\U000E0074\U000E007F&#34;
		} else {
			countryEmoji = emoji.GetFlag(countries[i][&#34;code&#34;])
		}
		countryText := fmt.Sprintf(&#34;%s %s\n\n&#34;, countryEmoji, countries[i][&#34;name&#34;])
		formattedText += countryText
	}

	return formattedText
}
</code></pre><p>With that wrapped up, I was then left with my more ‚Äòstatic‚Äô or ‚Äòinfrequently updated‚Äô parts of my now page‚Ä¶</p>
<h2 id="updating-static-content">Updating static content</h2>
<p>Static content is what I would consider parts of my now page that did not update very frequently and would still likely need to be updated manually by myself. This included the ‚ÄòWhat I‚Äôm up to, Learning and Fitness‚Äô sections on my now page.</p>
<p>Having already built the automation script I thought about how I could add this static content into the script. I decided that it was best to store the static contents in another markdown file, titled ‚Äòstatic.md‚Äô and then read the file as part of the automation script.</p>
<pre tabindex="0"><code>staticContent, err := os.ReadFile(&#34;static.md&#34;)
if err != nil {
	log.Fatalf(&#34;unable to read from static.md file. Error: %v&#34;, err)
}
</code></pre><p>Sure it meant that there was still an element of needing to manually update this file but given the nature of how infrequently it needed to be updated, I was happy with this solution.</p>
<h2 id="setting-up-github-actions">Setting up GitHub Actions</h2>
<p>After adding a bit more to format the now page how I wanted it to be, the Go script was finished üéâ</p>
<p>After compiling the Go script into an executable, I added it to my website‚Äôs repository in a new scripts directory.</p>
<p>Then using what I had learnt from the tutorial from Victoria, I put together a GitHub Actions workflow to run the script daily and add the update now.md file as a commit to my main branch. This then triggered a new build of the site being made via Cloudflare Pages and shortly after the now page on my site would be updated ü•≥</p>
<pre tabindex="0"><code>name: update-now

on:
  schedule:
    - cron: &#39;0 1 * * *&#39;
  push:
    branches:
      - master

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: get branch
        uses: actions/checkout@main
        with:
          fetch-depth: 1
      - name: run script
        run: |
          cd ${GITHUB_WORKSPACE}/scripts
          ./automate-now
      - name: deploy
        run: |
          git config user.name &#34;${GITHUB_ACTOR}&#34;
          git config user.email &#34;${GITHUB_ACTOR}@users.noreply.github.com&#34;
          git add . 
          git commit -m &#34;üç± Dynamic now page update&#34;
          git push --all -f https://${{ secrets.GITHUB_TOKEN }}@github.com/${GITHUB_REPOSITORY}.git
</code></pre><h2 id="manual-stat-tracking">Manual stat tracking</h2>
<p>Now you might have noticed that a lot of the tracking of various stats is still actually done manually, but instead of manually tracking things by editing my now page it‚Äôs via third-party services like Letterboxd, NomadList and Oku. This is something that I‚Äôm totally happy with.</p>
<p>The third-party services give me an easy of of tracking and logging things while also giving me the ability to select and pick what I want to share rather than sharing everything by default. For now, I think this is a good compromise.</p>
<h2 id="summary">Summary</h2>
<p>Since setting this all up, my now page has been updated automatically daily without any major issues, so I‚Äôm really happy with how this project turned out. I managed to learn quite a bit in the process (including writing my first generic function!), improved my Go scripting skills and created something that solved a real problem I had.</p>
<p>There are some things I‚Äôd like to add or improve to the script such as more unit tests (especially around API calls) and adding poster images for media to the now page but I‚Äôve decided to pause this work for now to work on other projects in the meantime.</p>
<p>Similar to what Sophie said in her post, I do wish that more apps/websites offered an RSS feed or an API. There are a ton of amazing things that could be built if data across these services were a bit more available to individuals.</p>
<p>Everything <em><strong>should</strong></em> have an API.</p>
<p>Interested in seeing how the code works?
You can find the <a href="https://github.com/skyth3r/automate-now/tree/main">source code here on GitHub</a>.</p>


  </div></div>
  </body>
</html>
