<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://opencoder-llm.github.io/">Original</a>
    <h1>OpenCoder: Open Cookbook for Top-Tier Code Large Language Models</h1>
    
    <div id="readability-page-1" class="page"><div>
          <p>
            <b>OpenCoder</b> is an open and reproducible code LLM family which includes 1.5B and 8B base and chat models, supporting both English and Chinese languages. Starting from scratch, OpenCoder is trained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, reaching the performance of top-tier code LLMs. We provide not only model weights and inference code, but also the reproducible training data, the complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols. Empowering researchers to build and innovate, OpenCoder is your open foundation for advancing code AI. 
            <!-- We further fine-tune the base model with open-source datasets to get instruction-tuned models, namedly Sailor-Chat. -->
          </p>
          <ul>
            <li><strong>OpenCoder</strong>: A completely open-source Code LLM, built on the transparent
            data process pipeline and reproducible dataset, which achieves top-tier performance on multiple code LLM evaluation benchmarks.</li>
            
            <li><strong>RefineCode</strong>: A high-quality, reproducible code pretraining corpus comprising 960 Billion tokens across 607 programming languages.</li>
            
            <li><strong>Instructive Ablation Studies</strong>: Several meaningful ablation experiments aiming at providing meaningful insight for various design choices and training strategies of code LLMs.</li>
            
            <li><strong>Released Resources</strong>: Final model weights, complete data processing pipeline, efficient evaluation pipeline, reproducible pretraining dataset, large-scale SFT dataset, and intermediate checkpoints.</li>
          </ul>
        </div></div>
  </body>
</html>
