<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://genesis-embodied-ai.github.io/">Original</a>
    <h1>Genesis: A generative and universal physics engine for robotics and beyond</h1>
    
    <div id="readability-page-1" class="page">


    <section>
        <div>
            <div>
                <div>
                    <div>


                        
                    
                        <div>
                            <p><span>
                                <a target="_blank" href="https://www.zhou-xian.com/">Zhou Xian</a><sup>*,1</sup>
                                    ,
                                <a target="_blank" href="https://ylqiao.net/">Yiling Qiao</a><sup>*,2</sup>,

                                <a target="_blank" href="https://www.zhenjiaxu.com/">Zhenjia Xu</a><sup>*,3,4,6</sup>,

                                <a target="_blank" href="https://zswang666.github.io/">Tsun-Hsuan Wang</a><sup>*,5</sup>,

                                <a target="_blank" href="https://acmlczh.github.io/">Zhehuan Chen</a><sup>*,7,8</sup>,

                                <a target="_blank" href="">Juntian Zheng</a><sup>*,1,9</sup>,
                                    
                                <a target="_blank" href="https://ziyanx02.github.io/">Ziyan Xiong</a><sup>*,9</sup>,
                                
                                <a target="_blank" href="https://wangyian-me.github.io/">
                                  Yian Wang</a><sup>*,7</sup>,
                                  
                                <a target="_blank" href="https://genesis-embodied-ai.github.io/erizmr.github.io">
                                    Mingrui Zhang</a><sup>*,10,11</sup>,

                                <a target="_blank" href="https://pingchuan.ma/">Pingchuan Ma</a><sup>*,5</sup>,
                                
                                <a target="_blank" href="https://yufeiwang63.github.io/">
                                  Yufei Wang</a><sup>*,1</sup>,
  
                                <a target="_blank" href="https://frank-zy-dou.github.io/">Zhiyang Dou</a><sup>*,13,14</sup>,
                                <a target="_blank" href="https://bc-kim.github.io/">Byungchul Kim</a><sup>5</sup>,
                                <a target="_blank" href="https://www.yunshengtian.com/">Yunsheng Tian</a><sup>5</sup>,
                                <a target="_blank" href="https://scholar.google.com/citations?user=ugn9uy4AAAAJ&amp;hl=en">Yipu Chen</a><sup>12</sup>,
                                <a target="_blank" href="">Xiaowen Qiu</a><sup>7</sup>,
                                <a target="_blank" href="https://xhrlyb.github.io/">
                                    Chunru Lin</a><sup>7</sup>,
                                <a target="_blank" href="https://tairanhe.com/">Tairan He</a><sup>1</sup>,
                                    
                                <a target="_blank" href="https://si-lynnn.github.io/">
                                    Zilin Si</a><sup>1</sup>,

                                <a target="_blank" href="https://yunchuzhang.github.io/">
                                  Yunchu Zhang</a><sup>16</sup>,
  
                                  <a target="_blank" href="">Zhanlue Yang</a><sup>11</sup>,

                              <a target="_blank" href="https://tiantianliu.cn/">
                                  Tiantian liu</a><sup>11</sup>,

                                  <a target="_blank" href="https://easypapersniper.github.io/">Tianyu Li</a><sup>12</sup>,
                                  
                                <a target="_blank" href="https://kashu7100.github.io/">Kashu Yamazaki</a><sup>1</sup>,
                                
                                <a target="_blank" href="https://icefoxzhx.github.io/">Hongxin Zhang</a><sup>7</sup>,
                                    
                                <a target="_blank" href="https://www.cs.columbia.edu/~huy/">Huy Ha</a><sup>3, 4</sup>,

                                <a target="_blank" href="https://yucrazing.github.io/resume/">Yu Zhang</a><sup>15</sup>,

                                <a target="_blank" href="">
                                  Michael Liu</a><sup>1,17</sup>,

                                <a target="_blank" href="">
                                  Shaokun Zheng</a><sup>9</sup>,

                                <a target="_blank" href="https://zipengfu.github.io/">Zipeng Fu</a><sup>4</sup>,
                                    
                                <a target="_blank" href="https://wooqi57.github.io/">Qi Wu</a><sup>4</sup>,


                                <a target="_blank" href="https://gengyiran.github.io/">
                                  Yiran Geng</a><sup>8</sup>,
  
                                <a target="_blank" href="">
                                    Feng Chen</a><sup>9,13</sup>,

                                <a target="_blank" href="https://zhou-xian.com/gallery#nono-milky">
                                  Milky</a><sup></sup>,

                                <a target="_blank" href="https://yuanming.taichi.graphics/">Yuanming Hu</a><sup>11</sup>,
                                    
                                <a target="_blank" href="https://www.gshi.me/">Guanya Shi</a><sup>1</sup>,
                                <a target="_blank" href="https://lingjie0206.github.io/">Lingjie Liu</a><sup>14</sup>,
                                <a target="_blank" href="https://i.cs.hku.hk/~taku/">Taku Komura</a><sup>13</sup>,
                                <a target="_blank" href="https://zackory.com/">Zackory Erickson</a><sup>1</sup>,
                                <a target="_blank" href="http://davheld.github.io/">David Held</a><sup>1</sup>,

                                <a target="_blank" href="https://www.cs.cmu.edu/~minchenl/">
                                  Minchen Li</a><sup>1</sup>,
  
                                <a target="_blank" href="https://jimfan.me/">Linxi &#34;Jim&#34; Fan</a><sup>6</sup>,
                                <a target="_blank" href="https://yukezhu.me/">Yuke Zhu</a><sup>6,18</sup>,
                                <a target="_blank" href="https://cdfg.csail.mit.edu/wojciech">Wojciech Matusik</a><sup>5</sup>,
                                <a target="_blank" href="https://research.ibm.com/people/dan-gutfreund">Dan Gutfreund</a><sup>19</sup>,
                                <a target="_blank" href="https://shurans.github.io/">Shuran Song</a><sup>3,4</sup>,
                                <a target="_blank" href="https://danielarus.csail.mit.edu/">Daniela Rus</a><sup>5</sup>,
                                <a target="_blank" href="https://www.cs.umd.edu/~lin/">Ming Lin</a><sup>2</sup>,
                                <a target="_blank" href="https://faculty.cc.gatech.edu/~bozhu/">Bo Zhu</a><sup>12</sup>,
    
                                <a target="_blank" href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a><sup>1</sup>,

                                <a target="_blank" href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a><sup>7,19</sup>
                            </span>
                        </p></div>

                        
                        <!-- <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1 </sup>CMU</span>,
                            <span class="author-block"><sup>2 </sup>UMD</span>,
                            <span class="author-block"><sup>3 </sup>Columbia University</span>,
                            <span class="author-block"><sup>4 </sup>Stanford</span>,
                            <span class="author-block"><sup>5 </sup>NVIDIA</span>,
                            <span class="author-block"><sup>6 </sup>MIT CSAIL</span>,
                            <span class="author-block"><sup>7 </sup>UMass Amherst</span>,
                            <span class="author-block"><sup>8 </sup>Peking University</span>,
                            <span class="author-block"><sup>9 </sup>Tsinghua IIIS</span>,
                            <span class="author-block"><sup>10 </sup>Imperial College London</span>,
                            <span class="author-block"><sup>11 </sup>Taichi Graphics</span>,
                            <span class="author-block"><sup>12 </sup>George Tech</span>,
                            <span class="author-block"><sup>13 </sup>HKU</span>,
                            <span class="author-block"><sup>14 </sup>UPenn</span>,
                            <span class="author-block"><sup>15 </sup>University of Washington</span>,
                            <span class="author-block"><sup>16 </sup>UMich</span>,
                            <span class="author-block"><sup>17 </sup>UT Austin</span>,
                            <span class="author-block"><sup>18 </sup>MIT-IBM AI Lab</span>,
                        </div> -->

                        <p><span><sup>1 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/1.jpg"/>  <img src="https://genesis-embodied-ai.github.io/assets/images/CMU_Logo_Stack_Red.png"/></span>
                          <span><sup>2 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/1.jpeg"/></span>
                          <span><sup>3 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/e679d5b662753f97ece0fd8b0d3eab26.jpg"/></span>
                          <span><sup>4 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/fc46c55e92901d55b8020aa867060a3c.63627c756928d.avif"/><img src="https://genesis-embodied-ai.github.io/assets/images/Stanford-Logo.png"/></span>
                          <span><sup>5 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/MIT_logo.svg.png"/><img src="https://genesis-embodied-ai.github.io/assets/images/channels4_profile.jpg"/></span>
                          <span><sup>6 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/NVIDIA_logo.svg"/></span>
                          <span><sup>7 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/67fefce70748390f1652c051c93d2c4d.png"/></span>
                          <span><sup>8 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/images.png"/></span>
                          <span><sup>9 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/tsinghua-university-logo.webp"/><img src="https://genesis-embodied-ai.github.io/assets/images/iiis.png"/></span>
                          <span><sup>10 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/Logo_for_Imperial_College_London.svg.png"/></span>
                          <span><sup>11 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/c19f6600-cfa3-11eb-89f5-7ff5113dab05.png"/></span>
                          <span><sup>12 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/georgia-institute-of-technology.svg"/></span>
                          <span><sup>13 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/university-of-hong-kong-logo.png"/></span>
                          <span><sup>14 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/upenn.png"/></span>
                          <span><sup>15 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/University-of-Utah-Logo.png"/></span>
                          <span><sup>16 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/University_of_Washington_Block_W_logo_RGB_brand_colors.SVG"/></span>
                          <span><sup>17 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/U-M_Logo-Hex.png"/></span>
                          <span><sup>18 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/utaustin.png"/></span>
                          <span><sup>19 </sup><img src="https://genesis-embodied-ai.github.io/assets/images/ibm.png"/></span>
                        </p>

                        <!-- <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>†</sup>Equal Contribution</span>
                        <span class="author-block"><sup>‡</sup>Equal Advising </span>
                    </div> -->

                        
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div>
            <!-- Abstract. -->
            <div>
                <div>
                    <div>
                        <p>
                        Genesis is a comprehensive physics simulation platform designed for general purpose <i>Robotics</i>, <i>Embodied AI</i>, &amp; <i>Physical AI</i> applications. It is simultaneously multiple things:
                      </p><ul>
                        <li>A <b>universal physics engine</b> re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.</li>
                        
                          <li>A <b>lightweight</b>, <b>ultra-fast</b>, <b>pythonic</b>, and <b>user-friendly</b> robotics simulation platform.</li>
                          <li>A powerful and fast <b>photo-realistic rendering</b> system.</li>
                          <li>A <b>generative data engine</b> that transforms user-prompted natural language description into various modalities of data.</li>
                          </ul>
                        
                        <p>
                          Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond. Our generative framework aims to automate generating data including the following modalities:
                          </p><ul>
                            <li> Physically-accurate &amp; spatially consistent videos</li>
                            <li> Camera motion &amp; parameters</li>
                            <li> Human and animal character motion</li>
                            <li> Robotic manipulation &amp; locomotion policy, deployable to real-world</li>
                            <li> Fully interactive 3D scene</li>
                            <li> Open-world articulated object generation</li>
                            <li> Speech audio, facial animation &amp; emotion</li>
                          </ul>
                        <p>
                          Currently, we are open-sourcing the underlying physics engine and the simulation platform. Access to the generative framework will be rolled out gradually in the near future.
</p>
<br/>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div>
            <div>
                <div>
                        <h2><span>Optimized Performance</span></h2>
                        <p>
                            Genesis is a highly-optimized physics engine that leverages GPU-accelerated parallel computation, with features like optimized collision checking, auto-hibernation, contact island, etc. Genesis delivers an unprecedented simulation speed in various scenes.

                        </p>
                        <p>
                            When simulating a manipulation scene (with a single plane and a Franka arm), Genesis runs at 43 million FPS, which is 430,000 times faster than in real time.

                        </p>
                        <p>
                            In large-scale simulation, Genesis utilized auto-hibernation to speed up simulation of entities that are in converged and static states. (This feature is under testing and will be released in version 0.1.1).
                        </p>
                    </div>
            </div>
        </div>
    </section>
    <section>
        <div>
            <div>
                <div>
                        <h2><span>Generating 4D dynamical &amp; physical world</span></h2>
                        
                        <p>Genesis&#39;s physics engine is empowered by a VLM-based generated agent that uses the APIs provided by the simulation infrastructure as tools to create 4D dynamic worlds, which can then be used as a foundational data source for extracting various modalities of data. Together with modules for generating camera and object motion, we are able to generate physically-accurate and view-consistent videos and other modalities of data.</p>
                    
                    
                    </div>

                    
                    </div>
                </div>
            
        
    </section>
    <section>
        <div>
            <div>
                <div>
                        <h2><span>Character Motion Generation</span></h2>
                    
                    
                    </div>

                    <div>
                    <div>
                      <div>
                        <div>
                          <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                            <source src="./videos/boxing.mp4" type="video/mp4"/>
                          </video>
                          <p>
                            <span>&#34;<i>A Japanese samurai performs boxing.</i>&#34;</span>
                          </p>
                        </div>
                      </div>
                      <div>
                        <div>
                          <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                            <source src="./videos/dance.mp4" type="video/mp4"/>
                          </video>
                          <p>
                            <span>&#34;<i>A Chinese soldier performs the Gangnam Style dance.</i>&#34;</span>
                          </p>
                        </div>
                      </div>
                      <div>
                        <div>
                          <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                            <source src="./videos/zombie.mp4" type="video/mp4"/>
                          </video>
                          <p>
                            <span>&#34;<i>A Roman soldier walks forward like a zombie.</i>&#34;</span>
                          </p>
                        </div>
                      </div>
                    </div>
                    
                    </div>
                </div>
            </div>
        
    </section>

    <section>
      <div>
          <div>
              <div>
                    <h2><span>Robotic Policy Generation</span></h2>
                <p><span>Genesis aims to use generative robotic agent and physics engine to automatically generate robotic policies and demonstration data for various skills under different scenarios. For the high-level motivation and more details behind the module, see <a href="https://robogen-ai.github.io/">RoboGen</a> and our upcoming paper.</span></p></div>
      </div>
  </div></section>

    <section>
      <div>
          <div>
              <div>
                    <h2><span>More Sim2Real Policy Transfer</span></h2>
                  <div>
                    <!-- <div class="column" style="padding-left: 2px; padding-right: 2px;">
                      <div class="content">
                        <video autoplay controls muted loop playsinline height="100%">
                          <source src="./videos/locomanipulation.mp4"
                                  type="video/mp4">
                        </video>
                        <p style="text-align:center;">
                          <span>Loco-manipulation of large & underactuated objects</span>
                          <span></span>
                        </p>
                      </div>
                    </div> -->
                    <div>
                      <div>
                        <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                          <source src="./videos/locomotion/quadraped/backflip.mp4" type="video/mp4"/>
                        </video>
                        <p>
                          <span>Quadraped single-backflip</span>
                        </p>
                      </div>
                    </div>
                    
                  </div>

                  <!-- <div class="columns is-centered">
                      <div class="content">
                        <p style="text-align:center;">
                        <video autoplay controls muted loop playsinline width="100%">
                          <source src="./videos/locomotion/quadraped/double_backflip.mp4"
                                  type="video/mp4">
                        </video>
                            <br>
                          <span>Quadraped double-backflip</span>
                        </p>
                      </div>
                    </div>
                  </div> -->

              </div>
          </div>
      </div>
  </section>
    <section>
      <div>
          <div>
              <div>
                    <h2><span>3D &amp; Fully Interactive Scene Generation </span></h2>
                  <div>
                    <div>
                      <div>
                        <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                          <source src="./videos/scene/house.mp4" type="video/mp4"/>
                        </video>
                        <p>
                          <span>&#34;<i>A home interior scene with a living room (including a dinning space), a restroom, a study and a bedroom.</i>&#34;</span>
                          <span></span>
                        </p>
                      </div>
                    </div>
                    
                  </div>


              </div>
          </div>
      </div>
  </section>
    <section>
      <div>
          <div>
              <div>
                    <h2><span>Open-world Articulated Object Generation </span></h2>
                    <div>
                      <div>
                        <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                          <source src="./videos/articulated.mp4" type="video/mp4"/>
                        </video>
                        <p>
                          <span>We generate articulated of open-set mesh assets, extending beyond categories contained in human-annotated articulated object asset. For more details, see our upcoming paper (under submission now).</span>
                        </p>
                      </div>
                    </div>
                  </div>


              </div>
          </div>
      
  </section>
  <section>
    <div>
        <div>
            <div>
                  <h2><span>Soft Robot </span></h2>
                  <p>
                    <span>Genesis can simulate generated soft or hybrid (soft skin and rigid skeleton) robot. To get started, check <a target="_blank" href="https://genesis-world.readthedocs.io/en/latest/user_guide/getting_started/soft_robots.html">tutorial</a>.</span>
                  </p>
                  </div>
        </div>
    </div>
</section>
    <section>
      <div>
          <div>
              <div>
                    <h2><span>Speech Audio, Facial Animation &amp; Emotion Generation </span></h2>
                  <div>
                    <div>
                      <div>
                        <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                          <source src="./videos/facial_single.mp4" type="video/mp4"/>
                        </video>
                        <p>
                          <span>&#34;<i>Facial animation and speech of the text [Genesis is a physical platform designed for general purpose Robotics/Embodied AI/Physical AI applications]. Emotion transitions from neutral to angry, then to happy</i>&#34;.</span>
                          <span></span>
                        </p>
                      </div>
                    </div>
                    <div>
                      <div>
                        <video autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                          <source src="./videos/facial_mix.mp4" type="video/mp4"/>
                        </video>
                        <p>
                          <span>Generalize across different facial characters and emotions</span>
                        </p>
                      </div>
                    </div>
                  </div>


              </div>
          </div>
      </div>
  </section>


   

<section id="BibTeX">
    <div>
        <h2>BibTeX</h2>
        <pre><code>@software{Genesis,
          author = {Genesis Authors},
          title = {Genesis: A Universal and Generative Physics Engine for Robotics and Beyond},
          month = {December},
          year = {2024},
          url = {https://github.com/Genesis-Embodied-AI/Genesis}
        }
</code></pre>
    </div>
</section>

    




</div>
  </body>
</html>
