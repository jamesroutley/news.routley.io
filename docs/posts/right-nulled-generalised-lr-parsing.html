<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.jeffsmits.net/generalised-lr-parsing/">Original</a>
    <h1>(Right-Nulled) Generalised LR Parsing</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>I hope you know a bit about LR parsing, otherwise this blog post won’t make much sense to you. You can read all about it in a <a href="https://blog.jeffsmits.net/lr-parsing-recursive-ascent/">previous post of mine</a>. Today I want to discuss the problems with getting your language parsed in LR(1), or even LR(<em>k</em>). And how an old way to solve those problems is with a more powerful algorithm, that can parse <em>any</em> context-free grammar, no restrictions, no complaints about conflicts.</p>
<p>Now in theory, any deterministic context-free language can be parsed with an LR(1) grammar. Writing one might be difficult though. If you can write an LR(<em>k</em>) grammar instead, you can mechanically transform it into an LR(1) grammar. But that doesn’t necessarily mean that your resulting grammar is <em>readable</em>. If it’s not very readable, like with any programming artefact, it’s going to be a pain in your behind at some point. Because you’ll make mistakes, or you’ll want to change it, and now you need to understand what’s going on again. While you may be able to describe your language within the restrictions of an LR(1) grammar, the <em>encoding</em> required (manual or automatic) will make your grammar less readable. There’s a reason why some programming language’s manuals have a “high level” or “natural” grammar describing the language and it’s intuitive structure, and separately an executable grammar that fits in some grammar class. Another complication is that the output of a parser generated from a grammar follows that grammar. And so, a worse grammar makes for a worse experience using the parser output.</p>
<p>For these reasons, generalised parsing is a popular prototyping tool, since it allows you to work with the natural grammar. Consider the following two grammars that describe the same language:</p>
<table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td><td>   </td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>E</mi><mo>+</mo><mi>T</mi></math></td><td>(2)</td><td></td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>E</mi><mo>+</mo><mi>E</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>T</mi></math></td><td>(3)</td><td></td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>E</mi><mo>*</mo><mi>E</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>T</mi><mo>=</mo><mi>T</mi><mo>*</mo><mi>V</mi></math></td><td>(4)</td><td></td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi></math></td><td>(4)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>T</mi><mo>=</mo><mi>V</mi></math></td><td>(5)</td><td></td><td></td><td></td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>V</mi><mo>=</mo><mi>a</mi></math></td><td>(5)</td><td></td><td></td><td></td></tr>
</tbody></table>
<p>The right one is what I’d call a natural grammar, while the left is a grammar with <em>encoded</em> priority and associativity information. In my opinion, the ideal grammar specification is a natural grammar, with some <em>separate</em> priority and associativity information. Of course without that information, the grammar would be ambiguous, so I do consider it part of the grammar specification. Popular parser generators will have some more or less clunky way of doing this kind of grammar specification for this simple example, but there are always more complicated situations where you must either write your grammar in an unnatural way (encodings), or have to make it accept a superset and do an analysis later. And even the latter one it’s always an elegant option, check out this example from C++:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>int </span><span>(a)</span><span>,</span><span> b</span><span>,</span><span> …</span><span>;
</span></code></pre>
<p>Depending on what you put in the …, the above might be a variable declaration list where the parentheses around <code>a</code> are superfluous, or it could be a comma-separated list of expressions where the first is a cast to <code>int</code>. Variable declaration lists and expressions are two conceptually quite different things in the grammar and in a natural grammar should be separate things. But if you want to deal with this input, you can’t use a finite amount of lookahead to distinguish the two. So your choice is to make the grammar ugly and complicated or to use a generalised parsing algorithm.</p>
<h2 id="an-introduction-to-generalised-lr-parsing">An Introduction to Generalised LR Parsing</h2>
<p>Let me start by mentioning my source here, it’s <a href="https://core.ac.uk/download/pdf/301667613.pdf"><em>Generalised LR Parsing Algorithms</em></a>, the PhD dissertation of Rob Economopoulos. If you want more detail than this blog post gives, or you want to learn more about the history of GLR parsing, I highly recommend spending some time reading that dissertation.</p>
<p>So what’s the big idea of Generalised LR Parsing? Well, ambiguous grammars give rise to LR parse table conflicts, right? In GLR, parse table conflicts are no problem: we’ll just execute multiple possible actions in parallel on different copies of the parser state. The naive way to do that could give an exponential blowup, so we’ll have to be smart about it, by sharing state as much as possible. This will save both memory and work.</p>
<p>Let’s work through an example to see how this works. We’ll be roughly be following Tomita’s Algorithm 1. The following grammar is an adaptation of grammar 4.1 from the dissertation:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi><mi>b</mi><mi>C</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi><mi>B</mi><mi>C</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo><mi>b</mi></math></td><td>(4)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>C</mi><mo>=</mo><mi>d</mi><mi>e</mi></math></td><td>(5)</td></tr>
</tbody></table>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit.svg" alt="The LALR(1) DFA for the adaptation of grammar 4.1"/></p>
<p>As you can see, state <code>S3</code> has a conflict. To make that clearer, I’ve added the follow set to rule <code>B = b •</code>, which is only <code>d</code>. So we have a shift-reduce conflict between that rule and <code>C = • d e</code>. Let’s see if we can’t parse an input of <code>abde</code> with this LR automaton and the ideas of GLR. The first part before <code>S3</code> should be easy:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step3.svg" alt="The stack at step 3"/></p>
<p>This is our “stack” for the LR automaton. As usual, we keep a reference to an automaton state on the stack. I’ve also labelled the edges in between with some extra information though. I think that makes things more readable, and eventually we’ll use it build parse trees so our algorithm functions as a proper parser instead of just a recogniser.</p>
<p>Now when we get to the conflicting state, we need to choose <em>both</em> actions. Since we want to do as little work as possible, we’ll use a tree-based datastructure for our stacks, where we can share the parts that are the same. In order to keep as little extra information around as possible, we’ll first do all the non-shift actions possible until every state is ready to shift the next input symbol. So first we reduce by <code>B = b</code> in <code>S3</code>:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step4.svg" alt="The (tree-based) stack at step 4"/></p>
<p>We can mark all the states we’ve checked for possible reductions. We just did all reductions in <code>S3</code> and new parallel state <code>S4</code> doesn’t do reductions. So we now shift input <code>d</code>. (If we have a state that can’t shift the next symbol in the input, we can just drop that state and stack. Only if we drop the last one do we actually have to reject the input). Let’s continue:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step5.svg" alt="The (tree-based) stack at step 5"/></p>
<p>Huh, now we’re in the same state twice, that seems redundant. Let’s merge whenever we get in the same parallel state multiple times:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step5-improved.svg" alt="The (graph-based) stack at step 5"/></p>
<p>So we have a directed acyclic graph based datastructure for the stacks now, where we can share any node with the same label, so long as we’ve shifted the same amount of input for those nodes (which I’m depicting by laying those nodes out vertically, while new nodes from shifting go horizontally to the right).</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step6.svg" alt="The graph-structured stack at step 6"/></p>
<p>Cool, now by sharing large parts of the stacks we’ve saved ourselves quite a bit of work. But while we reduce, we now need to search for paths in our <em>graph-structured stack</em> (GSS). This is different from normal LR, which only one stack, where you can just pop a number of nodes off. We know from which point in the graph we’re starting, but this is still a worst case <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>O</mi><mo>(</mo><msup><mi>n</mi><mi>m</mi></msup><mo>)</mo></math> operation where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>n</mi></math> in the input length and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>m</mi></math> is the length of the right-hand side of the rule.</p>
<p>At this point we’re reducing by <code>C = de</code> in state <code>S8</code>, which has two symbols in the right-hand side. The paths of length 2 are <code>S3 ← S6 ← S8</code> and <code>S4 ← S6 ← S8</code>:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step7.svg" alt="The graph-structured stack at step 7"/></p>
<p>Next let’s do the reduce in <code>S5</code>: <code>E = abC</code>.</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step8.svg" alt="The graph-structured stack at step 8"/></p>
<p>Cool, in <code>S1</code> we can accept the input. But there’s more, we can still reduce in <code>S7</code>: <code>E = aBC</code>.</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step9.svg" alt="The graph-structured stack at step 9"/></p>
<p>Yep, when we reduce from <code>S7</code> we find a second way to get to <code>S1</code>. Another goto from <code>S0</code> to <code>S1</code> means there’s multiple ways to parse the input (different right-most derivatives), i.e. there’s multiple parse trees, and so the grammar is <em>ambiguous</em> for this input. And with the GLR algorithm we were able to find all ways to parse the input.</p>
<h3 id="hidden-left-recursion">Hidden Left Recursion</h3>
<p>The GSS is not always an <em>acyclic</em> graph. In fact, cycles can be very important to make everything work. Let’s look at a grammar with so-called hidden left recursion, grammar 4.3 from the dissertation.</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>B</mi><mi>E</mi><mi>a</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>b</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo></math></td><td>(4)</td></tr>
</tbody></table>
<p>So rule 2 has hidden left recursion. Because <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi></math> is nullable, rule two can get from <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi></math> straight back to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi></math>. This gives multiple shift-reduce conflicts:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-43.svg" alt="The LALR(1) DFA for grammar 4.3"/></p>
<p>This is the LALR(1) automaton, the LR(1) is larger without helping make things less conflicted or simpler to parse so I’m sticking with LALR here. I’ve once again displayed the follow set for the reduce rule <code>B = •</code> in the conflicting states to make the conflict more explicit. The language that this grammar describes can be captured with regular expression <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>b</mi><msup><mi>a</mi><mo>*</mo></msup></math>. But with the way the grammar is written, assuming you want to get a proper parse tree that fits this grammar, your parser needs to speculate on how many <code>a</code>s will be in the input. It needs to go into rule 2 that many times before shifting the <code>b</code> at the start of the input. Thankfully this will not give non-termination issues, due to a self-loop that we can put into the GSS. What we’ll get before even shifting anything is a couple of reductions taking us from <code>S0</code> to <code>S3</code> and then <code>S3</code> again:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-43-step3.svg" alt="The graph-structured stack at step 3"/></p>
<p>If we now shift and do the two reduce actions we can, we get to this GSS:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-43-step6.svg" alt="The graph-structured stack at step 6"/></p>
<p>As you can see, reducing by <code>E = b</code> from <code>S2</code> is going perfectly fine and gets us into <code>S1</code> and <code>S4</code>. If <code>b</code> is our input, we can now accept the input since we’re in state <code>S1</code>. But let’s explore what happens if our input has at least one <code>a</code>. If we shift that, we should be able to reduce by <code>E = BEa</code> in different ways, where we do and do not take the self-loop of node <code>S3</code> in the GSS:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-43-step9.svg" alt="The graph-structured stack at step 9"/></p>
<p>Note how we could prune some tops of the GSS that had no way to shift an <code>a</code>. Pretty neat, everything seems to be working. We can accept <code>ba</code> as an input since we’re in <code>S1</code> again, but if there’s another <code>a</code> on the input we can continue as well from the new <code>S4</code> that we’re in.</p>
<p>Fun fact: if you take Tomita’s Algorithm 2, instead of Algorithm 1, it will get stuck in an infinite loop on this grammar. That’s a problem that wasn’t identified at the time of publication. When it was later found, people preferred to adapt Algorithm 1 as it was simpler. Of course, the whole reason for Tomita to show multiple algorithms was to build up to a usable but complex algorithm and explain the necessity of the complexity. Meaning there was something wrong with Algorithm 1 as well…</p>
<h3 id="hidden-right-recursion">Hidden Right Recursion</h3>
<p>We run into the real trouble with Algorithm 1 when trying to parse a grammar with hidden <em>right</em> recursion, exemplified in (an adaptation of) grammar 4.2 of the dissertation:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi><mi>E</mi><mi>B</mi><mi>B</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>b</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo></math></td><td>(4)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo><mi>c</mi></math></td><td>(5)</td></tr>
</tbody></table>
<p>The language is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msup><mi>a</mi><mo>*</mo></msup><mi>b</mi><msup><mi>c</mi><mi mathvariant="normal">?</mi></msup><msup><mi>c</mi><mi mathvariant="normal">?</mi></msup></math>. Here’s the automaton:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit.svg" alt="The LALR(1) DFA for grammar 4.2"/></p>
<p>Let’s see how our simple Algorithm 1 breaks on this with input <code>aab</code>:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-step5.svg" alt="The graph-structured stack at step 5"/></p>
<p>We’ve shifted the entire input, and started checking for reductions again. We’ve just reduced from <code>S2</code> to <code>S4</code>, and we basically never have to visit <code>S2</code> again since we can’t shift from it. So we’re technically still on 1 stack just like LALR, the depiction is just showing some old info to make things clearer. Next we can reduce again in <code>S4</code> with the epsilon rule:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-step6.svg" alt="The graph-structured stack at step 6"/></p>
<p>Still basically a single stack, we can’t do any more reductions from <code>S4</code>. We do all reductions from <code>S5</code>, nothing surprising there:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-step7.svg" alt="The graph-structured stack at step 7"/></p>
<p>Finally we get to the reduction of <code>E = aEBB</code>:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-step8.svg" alt="The graph-structured stack at step 8"/></p>
<p>According to the algorithm we’ve used so far, we’re done with our reductions in <code>S6</code>, and so are done with reductions. We’re also at the end of the input, so we cannot shift. And none of the parallel states at the tops of our GSS are the accepting state <code>S1</code>, so we’d reject the input. Meanwhile, with a normal LR parser we’d have popped off <code>S4</code>, <code>S5</code> and <code>S6</code> while reducing with <code>E = aEBB</code>, then taken a goto into <code>S4</code> again, followed by another re-deriving the <code>B</code>s and reducing again from <code>S6</code>.</p>
<p>There are several ways to deal with this problem. The first published solution was that if you reduce and you go to a state you’re already in (like the <code>S4</code> to <code>S3</code> edge we just saw appear), you should search for new reductions through the new edge in the graph from states you’ve checked off already. You can do some tricks to optimise that search a little, but it ends up being rather expensive regardless. So we’re not taking that approach here.</p>
<h2 id="right-nulled-glr-parsing">Right Nulled GLR parsing</h2>
<p>We’re going to short-circuit right-nulled rules. This is called Right-Nulled GLR (RNGLR) and is described in chapter 5 of the dissertation. To visualise short-circuiting, I will show the PDA of the LR automaton with explicit stack activity, similar to how I’ve done so in <a href="https://blog.jeffsmits.net/optimising-lr-automata/">a previous post</a>.</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-pda.svg" alt="The Push-Down Automaton for grammar 4.2"/></p>
<p>The notation <code>(a) b [C] ↓D ↑E</code> on the edges means: peek for <code>a</code> on the input, shift <code>b</code> from the input, peek at <code>C</code> on the top of the stack, push <code>D</code> on the stack, pop <code>E</code> off the stack.</p>
<p>What you can see here is that whenever we shift from the input, we also push the originating state number onto the stack. Whenever we reduce by a rule we pop numbers off the stack equal to the rule RHS length - 1. So <code>S2</code> reduces <code>E = b</code>, <em>no</em> stack pops, <code>S6</code> reduces <code>E = a E B B</code>, 3 stack pops, <code>S4</code> reduces <code>B =</code>, 1 stack <em>push</em>. Every non-terminal has a <code>_Goto</code> state that handles where to go next, based on the single number that’s still on the stack after the stack pops. That last one is left to represent the originating state of the non-terminal after the goto.</p>
<p>Now we continue with RNGLR, where we short-circuit right-nulled rules. What we’ll do is reduce when we’re in the middle of a rule, so long as the remaining part of the rule is nullable. For example: <code>E = a E • B B</code> in state <code>S4</code>, greyed out above, will be active again.</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-rn.svg" alt="The Right-Nulled Push-Down Automaton for grammar 4.2"/></p>
<p>As you can see with the <span>blue parts</span> of the diagram, we now short-circuit <code>E = a E • B B</code> in state <code>S4</code> and <code>E = a E B • B</code> in state <code>S5</code>. At those points we pop less off the stack, since less has been pushed onto the stack. We take the rule RHS length before the •, - 1. Note in a corresponding parse table, we similarly need to change the reduce action to remember how far into the rule we are instead of just the rule.</p>
<p>There are also some <span>mintgreen parts</span> in the diagram denoting epsilon rule reductions. These are to avoid doubling up on reductions that we’ve already short-circuited. We no longer try reductions at the end of a rule when the last thing we did to get to the end of the rule is reduce by an epsilon rule. So if we get from <code>S5</code> to <code>S6</code> by the epsilon rule <code>B =</code>, we shouldn’t try to reduce by <code>E = a E B B</code> in <code>S6</code>. In the diagram this is shown as mintgreen transitions for epsilon reductions, dashed mintgreen if there’s multiple options, and (dashed) greyed out transitions of reductions we’re avoiding. The algorithm is implemented as follows: Whenever we shift or reduce+goto, we create a new edge in the GSS or annotate another option on an existing GSS edge. Then, if it’s a new edge, we schedule the reduces possible through the edge which we just touched. So if we just did a reduce+goto of an epsilon rule, we’ll just not schedule new reduces except for more epsilon reductions.</p>
<p>Let’s see that in action by going back to our previous example:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-rn-step6.svg" alt="The graph-structured stack at step 6"/></p>
<p>We just reduced by <code>B =</code>, but with our RN parse table, there is a second reduce action we can take in state <code>S4</code> which short-circuits <code>E = a E • B B</code>:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-rn-step7.svg" alt="The graph-structured stack at step 7"/></p>
<p>We’ve now discovered the other <code>E</code> edge a lot earlier than with Algorithm 1. It’s even earlier than a normal LR parser would find this reduction. And starting from this new edge, we can short-circuit the same rule again:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-rn-step8.svg" alt="The graph-structured stack at step 8"/></p>
<p>At this point we know we can accept the input since we’re in <code>S1</code>, but we might still discover multiple derivations, so let’s continue:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-42-edit-rn-step9.svg" alt="The graph-structured stack at step 9"/></p>
<p>We can get to <code>S6</code>, but there’s no more reductions to do there because we’re not allowed to take reduction paths that start with an edge labeled with an epsilon reduction. We have finished parsing.</p>
<h3 id="reducing-conflicts-in-rnglr-automata">Reducing Conflicts in RNGLR Automata</h3>
<p>The upside of RNGLR is that we avoid searching for places to reduce again. The downside here is that we add more reduce actions to parse table cells, so we can get more reduce-reduce conflicts. Sometimes that’s just required, but there are specific cases where you can resolve the conflict. Identifying when you can safely resolve certain reduce-reduce conflicts caused by short-circuit reductions is a little involved though. The intuition is that you want to short-circuit as early as possible, so you would prefer to reduce by <code>E = a E B • B</code> over <code>B = •</code> in <code>S5</code> of the previous section’s example, since the <code>E</code> rule is higher up in the parse tree, whereas the <code>B</code> rules are only added to <code>S5</code> because we’re in front of a <code>B</code> in the <code>E</code> rule.</p>
<p>Let’s take a different example, from the paper on reducing conflicts in RNGLR:</p>
<table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td><td> </td><td></td><td></td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi><mi>A</mi></math></td><td>(2)</td><td></td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo><mi>C</mi><mi>C</mi></math></td><td>(6)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo></math></td><td>(3)</td><td></td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>C</mi><mo>=</mo></math></td><td>(7)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>A</mi><mo>=</mo><mi>B</mi><mi>C</mi><mi>D</mi></math></td><td>(4)</td><td></td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>D</mi><mo>=</mo><mi>d</mi></math></td><td>(8)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>A</mi><mo>=</mo><mi>b</mi></math></td><td>(5)</td><td></td><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>D</mi><mo>=</mo></math></td><td>(9)</td></tr>
</tbody></table>
<p>The corresponding RN PDA is as follows:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/sj04-41-rn1.svg" alt="The Right-Nulled Push-Down Automaton for grammar 4.1 from the paper"/></p>
<p>Note that some of the short-circuit rules now active are at the start and therefore themselves “nullable” rules (the RHS length before the • is zero). I’ve marked these mintgreen instead of blue.</p>
<p>This PDA is already partway into the process of reducing conflicts based on some of the reasoning we did before. If you just naively calculate the RN parse table, you’ll get conflicts in <code>S4</code> and <code>S5</code> due to the rules that are turned back on. However, when we trace the null reductions, we can find that those shouldn’t be considered. There are other resolvable conflicts in <code>S0</code>, <code>S2</code> and <code>S9</code> though.</p>
<p>Let’s look at state <code>S9</code>. In a normal LR automaton, the third item causes a shift of <code>b</code>, the fifth rule causes a reduce by <code>C =</code> when the lookahead is <code>d</code> or the end of the input. But with an RN parser, the other three rules are also active since they are all at a point where there are only nullable non-terminals right of the •. For lookahead <code>$</code>, we would prefer to use <code>E = a • A</code> since the way we got to this state is by shifting <code>a</code> and getting to that point of the rule. All four other rules in the itemset that can use <code>$</code> as lookahead are derived from expanding <code>A</code>, then <code>B</code> at the start of <code>A = • B D C</code>, then <code>C</code> at the start of <code>B = • C C</code>. So they can be safely disregarded. <strong>However</strong>, the rule <code>B = • C C</code> can also be used to reduce on a lookahead of <code>d</code>. So we should use that over <code>C = •</code> which also works for that lookahead since it’s in the itemset due to <code>B = • C C</code>.</p>
<p>Similar reasoning can be applied in <code>S2</code> and <code>S0</code> to prefer the short-circuit rules when they have the same follow set as later rules epsilon rules.</p>
<p>Settling the conflict by reducing with <code>E = a • A</code> on a <code>$</code> lookahead and by <code>B = • C C</code> with a <code>d</code> lookahead has knock-on effects in other places. We now know that <code>S5</code> can only be reached with a <code>d</code> lookahead, meaning we never reduce by <code>D = •</code>.</p>
<p>In the above example, all conflicts can be resolved with the method sketched here. If I understand the paper correctly, you can build up your itemsets for each state cleverly to order the items so later epsilon rules are always overridden by earlier ones with the same follow set symbol. Thereby resolving any reduce-reduce conflict with epsilon rules caused by the short-circuiting rules. Since the paper doesn’t make this claim explicitly though, I can’t be entirely sure of that last bit.</p>
<h3 id="parse-forests">Parse Forests</h3>
<p>In previous posts, I’ve mostly focused on the <em>recogniser</em> part of a parser. That’s the part that says yes/no based on an input, but doesn’t give you anything else. This is typically the most interesting part, and in <a href="https://blog.jeffsmits.net/ll-parsing-recursive-descent/">my LL</a> and <a href="https://blog.jeffsmits.net/lr-parsing-recursive-ascent/">LR parsing posts</a> I’ve mentioned semantic actions and parse trees as something to add in yourself in the sensible place. If you consider GLR parsing, there’s a bit more going on though that’s worth talking about. Given that you can create highly ambiguous grammars, the many parse trees that describe a parse can grow exponentially in the size of the input. We can’t just have our parser enumerate those. To keep a lid on the complexity of the algorithm, we need to apply sharing again, like we did with the GSS. If we share the common sub-trees of the parse tree, we can create a <em>parse forest</em>. So let’s have a look at the so-called Shared Packed Parse Forests (SPPFs) of GLR parsing.</p>
<p>For a parse forest we need an ambiguous grammar, so let’s get back to the typical expression grammar example:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>E</mi><mo>+</mo><mi>E</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>E</mi><mo>*</mo><mi>E</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi></math></td><td>(4)</td></tr>
</tbody></table>
<p>If we parse <code>a + a * a</code>, we get two trees:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/expr-parse-trees.svg" alt="Parse trees of a + a * a"/></p>
<p>We have terminal nodes and rule nodes. We can combine this into one SPPF like so:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/expr-sppf-v2.svg" alt="Parse forest of a + a * a"/></p>
<p>We have the two trees combined and sharing subtrees. And we have a <em>packed</em> node that points the two alternatives, called <em>packing</em> nodes. The left alternative corresponds to the left tree above, notice how it points to the <code>a + a</code> tree for its left child, <code>*</code> for the middle, and the rightmost <code>E = a</code> tree for the right child.</p>
<p>We can also build infinite trees using loops:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>E</mi><mi>E</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo></math></td><td>(4)</td></tr>
</tbody></table>
<p>Here’s the SPPF of input <code>a</code>:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/a-cyclic-sppf.svg" alt="Parse forest of a"/></p>
<p>Looks trippy, but if you take the time, you see that the top <em>packed</em> node has <code>E = a</code> as an alternative, along with that being the left child or right child of <code>E = E E</code>, where the other child is then either <code>E =</code> or a <code>E = E E</code> tree which eventually expands to <code>ε</code>.</p>
<h4 id="building-sppfs-and-improving-sharing">Building SPPFs and Improving Sharing</h4>
<p>The sharing that we want to get into the SPPF is automatically discovered by the GLR algorithm when it does sharing for the GSS, though it’s not perfect. Let’s return to an earlier grammar (4.1), example input and GSS:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi><mi>b</mi><mi>C</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi><mi>B</mi><mi>C</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo><mi>b</mi></math></td><td>(4)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>C</mi><mo>=</mo><mi>d</mi><mi>e</mi></math></td><td>(5)</td></tr>
</tbody></table>
<p>Our input is once again <code>abde</code>, and the GSS we found was:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-edit-step9.svg" alt="The graph-structured stack for abde with grammar 4.1"/></p>
<p>Our SPPF based on this GSS can very simply follow the structure, but there’s a basically free trick we can do immediately: make only one terminal node in the SPPF when we shift. Since the algorithm synchronises all stacks on a shift, this means that despite two edges in the GSS labeled <code>d</code>, we have both point to the same <code>d</code> terminal node in the SPPF. This gives us:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-41-sppf-1.svg" alt="The simple SPPF for abde with grammar 4.1"/></p>
<p>As you can see, we have a duplicate <code>C = de</code> node, which is a bit unfortunate. Whenever we have two nodes in the SPPF that are labeled the same and (transitively) point to the same set of terminal nodes (also called its span), we have an opportunity to share in the SPPF. That sharing is not always possible in the GSS, in this case since <code>S5</code> and <code>S7</code> are distinct states in the automaton that reduce different rules of <code>E</code>.</p>
<p>To de-duplication in the SPPF further, we’ll have to do more during the parsing algorithm. We’ll use the idea from the previous paragraph: Whenever we have two nodes in the SPPF that have the same label and span, they can be merged. If we can cheaply identify these opportunities, that would be great. As it so happens, whenever we reduce, we’re creating a new SPPF node whose span ends at the point in the input that we are currently at. So if for each SPPF rule node we keep around its <em>width</em> (no. of symbols from the input in its span), we should have enough information to determine any new rule node’s span. This is a slight tweak of the algorithm as presented in the dissertation.</p>
<p>So what we can do is keep around a map that maps <code>(label, width)</code> to the corresponding SPPF node. This map is emptied on each shift, and filled with each reduce that constructs a new SPPF node. During reduction—before creating a new node—we check the map for an existing one we can reuse.</p>
<h4 id="e-sppf">ε-SPPF</h4>
<p>There’s one last thing to address in the RNGLR parser algorithm. And that’s the parts we skip by short-circuiting rules. We know those parts are width 0, those are the nullable parts that we’re skipping. But they do have structure, they have an associated SPPF. In fact, we can statically derive the ε-SPPF for a grammar. Let’s check out the dissertation’s example, grammar 5.3:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>S</mi><mo>=</mo><mi>E</mi></math></td><td>(1)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>E</mi><mo>=</mo><mi>a</mi><mi>B</mi><mi>B</mi><mi>C</mi></math></td><td>(2)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo><mi>b</mi></math></td><td>(3)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>B</mi><mo>=</mo></math></td><td>(4)</td></tr>
<tr><td><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>C</mi><mo>=</mo></math></td><td>(5)</td></tr>
</tbody></table>
<p>The ε-SPPF for this grammar is something we can build up from a terminal node for ε. Then we add a rule node for each simple nullable rule (4 and 5). Now we have some nullable non-terminals, so we find every partial RHS with length &gt; 1 that’s nullable, and build up special nodes for these (<code>BC</code> and <code>BBC</code> from rule 2). For every full RHS that’s nullable and has a length &gt; 0, we can also create nodes, but we don’t have any in this example grammar. We end up with the following:</p>
<p><img src="https://blog.jeffsmits.net/generalised-lr-parsing/gre-53-epsilon-sppf.svg" alt="The ε-SPPF for grammar 5.3"/></p>
<p>This can be pre-computed and saved with the parse table. We can even save a reference to the relevant part of the ε-SPPF with every short-circuit reduction so we don’t have to go looking for it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>GLR parsing is cool, and useful, and fairly straightforward once you’ve wrapped your head around it. But there are some fun edge cases around, and dealing with those <em>efficiently</em> took quite some effort and research. I hope I’ve been able to highlight all of that in this blog post.</p>
<p>As usual, this post took way longer to write than expected (despite being able to source good examples from publications). It was also originally going to be longer and go into the other chapters of the dissertation that I’m basing this all on. But it’s probably better for everyone if I postpone that to another post. Here are some things to look forward to:</p>
<ol>
<li>BRNGLR is an adaptation of RNGLR, but the worst-case is limited to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>O</mi><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><mo>)</mo></math> instead of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>O</mi><mo>(</mo><msup><mi>n</mi><mi>m</mi></msup><mo>)</mo></math>.</li>
<li>RIGLR is a radically different approach that tries to identify parts of the grammar that don’t need the stack, and uses a normal DFA. Which gives you a PDA wrapping DFA(s) and reduced stack activity. It’ll be fun to compare this with the <a href="https://blog.jeffsmits.net/optimising-lr-automata/">old ideas of reduced stack activity for LR that I wrote about previously</a>.</li>
</ol>
<p>I also want to explore a generalised recursive ascent parser generator. Of course, I won’t guarantee that the next blog post will be about any of those topics, or when I will finish it or whatever ¯\_(ツ)_/¯</p>
<hr/>

  </div></div>
  </body>
</html>
