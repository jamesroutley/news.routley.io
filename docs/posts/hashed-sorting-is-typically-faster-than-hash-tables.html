<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://reiner.org/hashed-sorting">Original</a>
    <h1>Hashed sorting is typically faster than hash tables</h1>
    
    <div id="readability-page-1" class="page"><div id="maincontent">
  
  <p>Problem statement: count the unique values in a large array of mostly-unique uint64s. Two standard approaches are:</p>
  <ul>
  <li>Insert into a hash table and return the number of entries.</li>
  <li>Sort the array, then count positions that differ from their predecessor.</li>
  </ul>
  <p>Hash tables win the interview (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span>)</span></span></span></span> vs <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span></span><span>lo<span>g</span></span><span></span><span>n</span><span>)</span></span></span></span>), but sorting is typically faster in a well-tuned implementation. This problem and its variants are the inner loop of <a href="#applications">some of the world’s biggest CPU workloads</a>.</p>
  <h2 id="benchmark-highlights">Benchmark highlights</h2>
  <p>Here is performance on M2 Pro<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>, comparing our best-tuned hash table and our best-tuned sorting implementation. We also include Rust’s default implementations (<code>sort_unstable()</code>, and <code>HashSet</code> with <code>foldhash::fast</code>) as a reference point for high-quality general-purpose implementations:</p>
  <table>
  <colgroup>
  <col/>
  <col/>
  <col/>
  <col/>
  <col/>
  </colgroup>
  <thead>
  <tr>
  <th>Data size</th>
  <th>Baseline hash table</th>
  <th>Baseline sorting</th>
  <th>Tuned hash table</th>
  <th>Tuned sorting</th>
  </tr>
  </thead>
  <tbody>
  <tr>
  <td>8 KiB</td>
  <td>3.8 µs</td>
  <td>5.1 µs</td>
  <td><strong>1.6 µs</strong></td>
  <td>6.5 µs</td>
  </tr>
  <tr>
  <td>256 KiB</td>
  <td>219 µs</td>
  <td>264 µs</td>
  <td>193 µs</td>
  <td><strong>92 µs</strong></td>
  </tr>
  <tr>
  <td>8 MiB</td>
  <td>8.1 ms</td>
  <td>12.0 ms</td>
  <td>7.1 ms</td>
  <td><strong>3.9 ms</strong></td>
  </tr>
  <tr>
  <td>256 MiB</td>
  <td>875 ms</td>
  <td>464 ms</td>
  <td>269 ms</td>
  <td><strong>185 ms</strong></td>
  </tr>
  <tr>
  <td>2 GiB</td>
  <td>7.6 s</td>
  <td>4.3 s</td>
  <td>2.6 s</td>
  <td><strong>1.9 s</strong></td>
  </tr>
  </tbody>
  </table>
  <p>Tuned sorting beats our best hash table by ~1.5× on non-tiny sizes, and is up to 4× faster than the excellent <a href="https://www.youtube.com/watch?v=ncHmEUmJZf4">“Swiss Table”</a> hash tables that ship with Rust’s standard library.</p>
  <p><a href="https://github.com/reinerp/hashed-sorting-benchmark">Benchmark code is available</a>.</p>
  <h2 id="why-does-sorting-win">Why does sorting win?</h2>
  <p>Memory bandwidth: <em>even though sorting makes multiple passes through memory, each pass uses bandwidth far more efficiently than a hash table’s single pass.</em></p>
  <p>Once the dataset outgrows CPU caches (often around ~1 MiB on a single core, CPU-dependent), both hashing and sorting become limited by cache-line fetch bandwidth to main memory. Cache lines are typically 64 bytes, and the CPU fetches the entire line if you touch even a single byte.</p>
  <p>Hash tables waste bandwidth: each 8-byte key access pulls a full 64-byte cache line. So a hash table must incur at least 128 bytes of traffic per uint64 processed: 64 bytes read and 64 bytes written.</p>
  <p>For sorting we use a radix sort that splits into 1024 buckets per pass, needing just 3 passes for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">2^{30}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>2</span><span><span><span><span><span><span></span><span><span><span>30</span></span></span></span></span></span></span></span></span></span></span></span> elements<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Each pass reads and writes the entire array once, and the accesses have enough spatial locality that the <em>whole</em> cache line is used productively, unlike in hash tables. So processing a single uint64 takes only 48 bytes of memory traffic: 8 bytes × 3 passes of reads, 8 bytes × 3 passes of writes.</p>
  <p>This analysis would suggests a 2.7× speedup vs. hash tables: 128 bytes vs. 48 bytes of memory traffic per uint64. The measured speedup is only ~1.5×, primarily (as far as I can tell) because it’s harder to make the CPU cache system do exactly what you want for radix sort than for hash tables<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>. Hash tables get closer to their ideal than radix sort; radix sort’s ideal is better enough that it still wins.</p>
  <h2 id="making-radix-sort-robust">Making radix sort robust</h2>
  <p>Although radix sort is often the fastest sorting algorithm on random data, its performance degrades on data that isn’t uniformly spread out over the key space. One pass sorts one byte of the key into 256 buckets; if only a small subset of the 256 buckets are used by keys in practice—for example, if the top byte of a uint64 is always zero—then a lot of the work on that pass of radix sort may be wasted.</p>
  <p>To show this, I benchmarked sorting on (a) random uint64s and (b) uint64s where the random bits are “spread out”: even bits random, odd bits zero, like <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mi>a</mi><mn>0</mn><mi>b</mi><mn>0</mn><mi>c</mi><mn>0</mn><mi>d</mi><mo>…</mo></mrow><annotation encoding="application/x-tex">0a0b0c0d\ldots</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>0</span><span>a</span><span>0</span><span>b</span><span>0</span><span>c</span><span>0</span><span>d</span><span></span><span>…</span></span></span></span>. For the spread-out numbers, each radix pass uses only 16 of 256 buckets, hurting efficiency. Radix sort is ~2× faster than quicksort on random numbers, but ~1.5× slower on spread-out numbers:</p>
  <table>
  <thead>
  <tr>
  <th>Data size</th>
  <th>Quicksort</th>
  <th>Radix sort</th>
  </tr>
  </thead>
  <tbody>
  <tr>
  <td>8 KiB (random)</td>
  <td>5.1 µs</td>
  <td><strong>4.8 µs</strong></td>
  </tr>
  <tr>
  <td>256 KiB (random)</td>
  <td>262 µs</td>
  <td><strong>111 µs</strong></td>
  </tr>
  <tr>
  <td>8 MiB (random)</td>
  <td>11.9 ms</td>
  <td><strong>5.4 ms</strong></td>
  </tr>
  <tr>
  <td>256 MiB (random)</td>
  <td>466 ms</td>
  <td><strong>267 ms</strong></td>
  </tr>
  <tr>
  <td>2 GiB (random)</td>
  <td>4.2 s</td>
  <td><strong>2.8 s</strong></td>
  </tr>
  <tr>
  <td>8 KiB (spread-out)</td>
  <td><strong>5.2 µs</strong></td>
  <td>6.3 µs</td>
  </tr>
  <tr>
  <td>256 KiB (spread-out)</td>
  <td><strong>262 µs</strong></td>
  <td>459 µs</td>
  </tr>
  <tr>
  <td>8 MiB (spread-out)</td>
  <td><strong>12.2 ms</strong></td>
  <td>17.4 ms</td>
  </tr>
  <tr>
  <td>256 MiB (spread-out)</td>
  <td><strong>462 ms</strong></td>
  <td>628 ms</td>
  </tr>
  <tr>
  <td>2 GiB (spread-out)</td>
  <td>4.2 s</td>
  <td><strong>3.2 s</strong></td>
  </tr>
  </tbody>
  </table>
  <p>To avoid these slowdowns, we borrow an idea from hash tables: sort by <code>hash(key)</code> rather than <code>key</code>. For counting uniques we don’t care about final order, only grouping. Of course, this will change the sort order, but given that we’re only interested in counting uniques rather than the sort order per se, this doesn’t matter.</p>
  <p>Better yet, use an invertible (<a href="https://en.wikipedia.org/wiki/Bijection">bijective</a>) hash to transform keys in place. That avoids storing both key <em>and</em> hash, or recomputing the hash each pass. Many widely used hash functions are invertible on <code>uint64</code>; I used <a href="https://github.com/reinerp/hashed-sorting-benchmark/blob/174a64335e879ad367a5393892ebd8461529af58/src/hashers.rs#L19">Murmur3</a> and a cheaper variant, <a href="https://github.com/reinerp/hashed-sorting-benchmark/blob/174a64335e879ad367a5393892ebd8461529af58/src/hashers.rs#L43">MulSwapMul</a>.</p>
  <p>With a reasonably fast hasher, the cost of hashing is cheap and it fixes bad distributions. Using MulSwapMul, performance looks like this mostly regardless of data distribution:</p>
  <table>
  <thead>
  <tr>
  <th>Data size</th>
  <th>Hashed radix sort</th>
  </tr>
  </thead>
  <tbody>
  <tr>
  <td>8 KiB</td>
  <td>6.5 µs</td>
  </tr>
  <tr>
  <td>256 KiB</td>
  <td>92 µs</td>
  </tr>
  <tr>
  <td>8 MiB</td>
  <td>3.9 ms</td>
  </tr>
  <tr>
  <td>256 MiB</td>
  <td>185 ms</td>
  </tr>
  <tr>
  <td>2 GiB</td>
  <td>1.9 s</td>
  </tr>
  </tbody>
  </table>
  <p>This is the “best algorithm” shown at the top of this article: hash with MulSwapMul, then radix sort using <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">diverting LSD</a> <a href="https://en.wikipedia.org/wiki/Radix_sort">radix sort</a>. I fuse hashing with the first pass of radix sort, and counting with the last pass of radix sort, for a small additional speedup.</p>
  <h2 id="when-should-i-choose-hash-tables-versus-hashed-radix-sort">When should I choose hash tables versus hashed radix sort?</h2>
  <p>Some hash-table uses cannot be reformulated as hashed radix sort; the latter is more restrictive. Converting hash-table lookups/inserts to hashed radix sort fundamentally requires <em>batching</em>: you must issue many lookups without needing results until much later. Sometimes you can turn a one-pass algorithm (traverse a data structure and look up as you go) into two passes of the data structure: first gather keys from the data structure; do the radix sort; then traverse the data structure again to write results back. In some scenarios, such as <a href="https://en.wikipedia.org/wiki/Hash_consing">hash consing</a>, <a href="https://en.wikipedia.org/wiki/Common_subexpression_elimination">common subexpression elimination</a>, or parser lookup tables, the requirement for batching is a dealbreaker: sorting isn’t viable.</p>
  <p>Where batching <em>is</em> viable, hashed radix sorts are typically viable:</p>
  <ul>
  <li>Any key types usable with hash tables work with hashed radix sorts: apply the same hash.</li>
  <li>If you are <em>constructing</em> a hash table, the analog is building a sorted array by radix sort. If you are <em>querying</em> an existing table, the analog is: sort the queries, then do a linear-time merge with the existing sorted array of keys.</li>
  <li>Radix sort <a href="#appendix-4-parallelism">parallelizes at least as well</a> as hash tables, if not better.</li>
  </ul>
  <p>If hashed radix sorts are viable for your problem, will they be faster? The main determining factor seems to be the number of <em>repeat accesses per unique key</em>. If you perform many more inserts/lookups than there are unique keys, hash tables tend to win; if accesses are on the same order as unique keys—most keys touched only a few times—radix sort tends to win. This is because hash tables use O(unique keys) memory, while radix sort uses O(accesses); fitting into a smaller footprint often yields better memory-system performance.</p>
  <p>On my benchmarks, hash tables start to pull ahead when keys are accessed ~30 times on average. The threshold rises with problem size:</p>
  <table>
  <colgroup>
  <col/>
  <col/>
  <col/>
  <col/>
  <col/>
  <col/>
  <col/>
  </colgroup>
  <thead>
  <tr>
  <th>Size</th>
  <th>8 accesses per key</th>
  <th>8 accesses per key</th>
  <th>32 accesses per key</th>
  <th>32 accesses per key</th>
  <th>128 accesses per key</th>
  <th>128 accesses per key</th>
  </tr>
  </thead>
  <tbody>
  <tr>
  <td>8 KiB</td>
  <td><strong>1.9 µs</strong></td>
  <td>6.1 µs</td>
  <td><strong>1.9 µs</strong></td>
  <td>6.6 µs</td>
  <td><strong>1.7 µs</strong></td>
  <td>7.2 µs</td>
  </tr>
  <tr>
  <td>256 KiB</td>
  <td>125 µs</td>
  <td><strong>119 µs</strong></td>
  <td><strong>120 us</strong></td>
  <td>141 µs</td>
  <td><strong>149 µs</strong></td>
  <td>157 µs</td>
  </tr>
  <tr>
  <td>8 MiB</td>
  <td>6.7 ms</td>
  <td><strong>6.1 ms</strong></td>
  <td><strong>5.7 ms</strong></td>
  <td>7.0 ms</td>
  <td><strong>5.1 ms</strong></td>
  <td>7.1 ms</td>
  </tr>
  <tr>
  <td>256 MiB</td>
  <td>414 ms</td>
  <td><strong>246 ms</strong></td>
  <td><strong>240 ms</strong></td>
  <td><strong>242 ms</strong></td>
  <td><strong>200 ms</strong></td>
  <td>267 ms</td>
  </tr>
  <tr>
  <td>2 GiB</td>
  <td>3.9 s</td>
  <td><strong>2.6 s</strong></td>
  <td>3.2 s</td>
  <td><strong>2.7 s</strong></td>
  <td><strong>2.5 s</strong></td>
  <td>2.7 s</td>
  </tr>
  </tbody>
  </table>
  <h2 id="applications">Why does it matter?</h2>
  <p>Several high-performance systems have an inner loop equivalent to this; I’ve personally worked on two.</p>
  <p>First, extremely sparse unstructured matrix multiplication with e.g. sparsity of one in a billion and one scalar per nonzero. This applies to <a href="https://www.bigdatawire.com/2014/07/17/inside-sibyl-googles-massively-parallel-machine-learning-platform/">Google’s Sibyl</a>, which in 2015 was one of Google’s largest workloads and consumed several percent of fleetwide CPU. I and many others collectively spent years optimizing these inner loops.</p>
  <p>Second, the <a href="https://www-labs.iro.umontreal.ca/~simul/testu01/tu01.html">BigCrush test suite for random number generators</a> counts duplicates among n-grams of generated numbers to find anomalies.</p>
  <p>In my experience the default for problems like these is hash tables. The surprise to me, once I ran these benchmarks, is that radix sort can beat them by a substantial margin.</p>
  <hr/>
  <h2 id="appendix-1-tuning-radix-sort">Appendix 1: tuning radix sort</h2>
  <p>There’s a rich literature and set of libraries for fast radix sort. I primarily relied on and recommend <a href="https://axelle.me/2022/04/19/diverting-lsd-sort/">diverting LSD sort</a>, <a href="https://arxiv.org/pdf/2207.14334">diverting fast radix</a>, and the Rust library <a href="https://docs.rs/voracious_radix_sort/latest/voracious_radix_sort/">voracious_radix_sort</a>. The main characteristics of high-performance implementations like these are listed below:</p>
  <p>Use a <em>diverting</em> radix sort, which only performs a few passes of radix sort before falling back to insertion sort. If <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>radix</mtext></mrow><annotation encoding="application/x-tex">\text{radix}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>radix</span></span></span></span></span> is the number of buckets per pass (e.g., 1024), then after <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span> passes the array has been sorted into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>radix</mtext><mi>p</mi></msup></mrow><annotation encoding="application/x-tex">\text{radix}^p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>radix</span></span><span><span><span><span><span><span></span><span><span>p</span></span></span></span></span></span></span></span></span></span></span> buckets, with each bucket still needing sorting. After enough passes the buckets are tiny (average size &lt;1), so you stop doing radix passes and fix up locally with insertion sort. This makes diverting radix sort <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mtext>radix</mtext></msub><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n \log_{\text{radix}} n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span></span><span><span>lo<span>g</span></span><span><span><span><span><span><span></span><span><span><span><span>radix</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>n</span><span>)</span></span></span></span>, versus non-diverting radix sort’s <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo>⋅</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n \cdot w)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span></span><span>⋅</span><span></span></span><span><span></span><span>w</span><span>)</span></span></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>w</span></span></span></span> is word length. For large keys like uint64 you can typically save more than half the passes.</p>
  <p>Form the histograms for all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span> passes of radix sort in a single histogramming sweep, rather than one by one before each pass. This saves bandwidth: you only do <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p+1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span></span><span>+</span><span></span></span><span><span></span><span>1</span></span></span></span> instead of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>p</mi></mrow><annotation encoding="application/x-tex">2p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>p</span></span></span></span> sweeps through memory.</p>
  <p>Beyond the above techniques standard in the literature, I did two other optimizations.</p>
  <p>First, I fused the hashing pass (before the sort) into the histogramming pass (first step of the sort) to save memory bandwidth.</p>
  <p>I fall back to inlined insertion sort rather than a function call into the standard library. Calling into standard library sort, which needs to perform many branches on size to figure out that this is a very small array, adds a lot of overhead for buckets whose average size is 1. Inlined insertion sort finishes very quickly for small arrays.</p>
  <h2 id="appendix-2-other-memory-bandwidth-efficient-sorts">Appendix 2: other memory-bandwidth-efficient sorts</h2>
  <p>We prefer radix sort because it makes <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>1024</mn></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log_{1024}(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>lo<span>g</span></span><span><span><span><span><span><span></span><span><span><span>1024</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>n</span><span>)</span></span></span></span> passes through memory rather than quicksort/mergesort’s <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log_{2}(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>lo<span>g</span></span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>n</span><span>)</span></span></span></span> passes. Another way to reduce passes is <em>multi-way mergesort</em>: merge <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> arrays at once instead of 2, best implemented with a <a href="https://en.wikipedia.org/wiki/K-way_merge_algorithm#Tournament_Tree">tournament tree</a>.</p>
  <p>I implemented and benchmarked this. In my implementation it substantially underperforms quicksort and radix sort. Even though memory bandwidth usage is better, the instruction usage seems to be worse. However, especially here, where the literature appears sparser, it’s possible that my implementation was poorly tuned.</p>
  <h2 id="appendix-3-tuning-hash-tables">Appendix 3: tuning hash tables</h2>
  <p>Our tuned hash tables outperform baseline “Swiss Table” hash tables by up to 3× on this workload, largely because we can optimize for our specific case (huge tables, uint64 keys, prioritizing runtime time over memory footprint) whereas Swiss Tables must balance many use cases (strings, small tables, memory footprint).</p>
  <p>Our tuned hash table is different than Swiss Tables in the following ways.</p>
  <p><em>Single table vs. metadata+data.</em> Swiss Tables keep a “metadata” table (1-byte tags derived from 7-bit hash codes) in addition to the real data table. They probe metadata first (using SIMD instructions to probe 8–16 keys at once), then they probe the data table on a match. This allows efficient probing even when probe sequences are long, it has the disadvantage of requiring <em>two</em> cache misses per lookup: one for the metadata table, one for the data table. By contrast, we only use a data table: probing uint64 keys is already plenty fast. This allows us to pay just one cache miss per lookup<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
  <p>Swiss Table probing is not cacheline-aligned. A probe sequence may start in the middle of a cache line and continue to the next before finishing the current line, wasting bandwidth. Our tuned table also starts probing mid-line, but when we hit the end of the cache line we wrap to the beginning and probe the rest of the cache line before advancing to the next.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
  <p>Swiss Tables pack their data more densely than our table does: they target a max load factor of 78.5%, whereas we target 50%. This is a pure time-vs-memory tradeoff: Swiss Tables value memory more, we value time more. By using a lower load factor, we keep probe distances shorter, and increase the probability that a lookup can succeed with just one cache miss.</p>
  <p>We use <a href="https://doc.rust-lang.org/std/intrinsics/fn.prefetch_read_data.html">prefetching instructions</a> when accessing the table; Rust’s Swiss Tables implementation does not offer such functionality. Prefetching for hash tables is advanced user functionality, because users need to restructure their loops (and typically split into a loop prologue, loop body, loop epilogue) to support prefetching. Compared to any other API offered by Rust’s HashSet, this functionality is hugely more niche, and perhaps appropriately is not included in the standard library. That said, if you are willing to put in the effort to restructure your loops and tune them, prefetching can offer a huge speedup.</p>
  <p>Unlike the above differences, the hash function itself is the same between our tuned hash table and Swiss Tables: we use MulSwapMul in both. This is similar speed to <code>foldhash::fast</code> and hugely faster than Rust’s default <code>SipHash</code>.</p>
  <p>All of these differences together add up to a ~3× speedup over Swiss Tables on this workload:</p>
  <table>
  <thead>
  <tr>
  <th>Data size</th>
  <th>Swiss table</th>
  <th>Tuned hash table</th>
  </tr>
  </thead>
  <tbody>
  <tr>
  <td>8 KiB</td>
  <td>3.8 µs</td>
  <td>1.6 µs</td>
  </tr>
  <tr>
  <td>256 KiB</td>
  <td>219 µs</td>
  <td>193 µs</td>
  </tr>
  <tr>
  <td>8 MiB</td>
  <td>8.1 ms</td>
  <td>7.1 ms</td>
  </tr>
  <tr>
  <td>256 MiB</td>
  <td>875 ms</td>
  <td>269 ms</td>
  </tr>
  <tr>
  <td>2 GiB</td>
  <td>7.6 s</td>
  <td>2.6 s</td>
  </tr>
  </tbody>
  </table>
  <h2 id="appendix-4-parallelism">Appendix 4: parallelism</h2>
  <p>In many high-performance applications we want parallelism. Both radix sort and hash tables parallelize efficiently: for radix sort, parallelize within each pass and synchronize at pass boundaries; for hash tables, use fine-grained locking such as a lock per cacheline inside the table itself.</p>
  <p>From a theoretical analysis I expect both radix sort and hash tables to parallelize very well, and the advantage of radix sort to be sustained even in the parallel context. I benchmarked a few of the top Rust libraries for parallel radix sort and parallel hash tables, but I didn’t build any custom tuned versions myself.</p>
  <p>For radix sort, I found <a href="https://docs.rs/voracious_radix_sort/latest/voracious_radix_sort/">voracious_radix_sort</a> to have excellent parallel performance; it starts beating the best single-threaded implementation once the data exceeded 8MiB, and I suspect it could be improved further by porting over some of the single-threaded tuning work I did.</p>
  <table>
  <thead>
  <tr>
  <th>Data size</th>
  <th>Best sequential sorting</th>
  <th>Parallel sorting (8 cores)</th>
  </tr>
  </thead>
  <tbody>
  <tr>
  <td>8 KiB</td>
  <td>6.5 µs</td>
  <td>109 µs</td>
  </tr>
  <tr>
  <td>256 KiB</td>
  <td><strong>92 µs</strong></td>
  <td>386 µs</td>
  </tr>
  <tr>
  <td>8 MiB</td>
  <td><strong>3.9 ms</strong></td>
  <td>4.5 ms</td>
  </tr>
  <tr>
  <td>256 MiB</td>
  <td>185 ms</td>
  <td><strong>61 ms</strong></td>
  </tr>
  <tr>
  <td>2 GiB</td>
  <td>1.9 s</td>
  <td><strong>440 ms</strong></td>
  </tr>
  </tbody>
  </table>
  <p>For hash tables, I didn’t find an implementation that outperforms my best single-threaded implementation. This surprises me: in other contexts I’ve seen near-linear parallel speedups with a lock-per–cache-line design, and I suspect it’s possible here too. I didn’t spend long tuning this, and it’s possible I was using the existing libraries wrong or outside of their target workload profile.</p>
  
  </div></div>
  </body>
</html>
