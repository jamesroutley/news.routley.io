<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/alexpinel/Dot">Original</a>
    <h1>Dot: use of local LLMs and RAG in particular to interact with documents</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/93524949/313763477-e5983c61-d59c-45ac-86f6-9d62cffaf37b.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI0OTYwMzcsIm5iZiI6MTcxMjQ5NTczNywicGF0aCI6Ii85MzUyNDk0OS8zMTM3NjM0NzctZTU5ODNjNjEtZDU5Yy00NWFjLTg2ZjYtOWQ2MmNmZmFmMzdiLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDA3VDEzMTUzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY1OTkwNWViMWZkZDc5NmM3ZDhjM2E4NTRlYzJhNDU0NmNlNjYzMDMwMDk4ZjU4ZTg2MDkzMWIzMDZmN2M3ZGQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lOm54cT2Ak_6TGg4vb8YS_jjZ5P5ybVDLMm-gaO04dk"><img src="https://private-user-images.githubusercontent.com/93524949/313763477-e5983c61-d59c-45ac-86f6-9d62cffaf37b.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI0OTYwMzcsIm5iZiI6MTcxMjQ5NTczNywicGF0aCI6Ii85MzUyNDk0OS8zMTM3NjM0NzctZTU5ODNjNjEtZDU5Yy00NWFjLTg2ZjYtOWQ2MmNmZmFmMzdiLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDA3VDEzMTUzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY1OTkwNWViMWZkZDc5NmM3ZDhjM2E4NTRlYzJhNDU0NmNlNjYzMDMwMDk4ZjU4ZTg2MDkzMWIzMDZmN2M3ZGQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lOm54cT2Ak_6TGg4vb8YS_jjZ5P5ybVDLMm-gaO04dk" alt="ezgif-4-b96c0b5548" data-animated-image=""/></a></p>
<p dir="auto">This is Dot, a standalone open source app meant for easy use of local LLMs and RAG in particular to interact with documents and files similarly to Nvidia&#39;s Chat with RTX. Dot itself is completely standalone and is packaged with all dependencies including a copy of Mistral 7B, this is to ensure the app is as accessible as possible and no prior knowledge of programming or local LLMs is required to use it. You can install the app (available for Apple Silicon and Windows) here: <a href="https://dotapp.uk/" rel="nofollow">Dot website </a></p>

<p dir="auto">Dot can be used to load multiple documents into an llm and interact with them in a fully local environment through Retrieval Augmented Generation (RAG), supported documents are: pdf, docx, pptx, xlsx, and markdown. Apart from RAG, users can also switch to Big Dot for any interactions unrelated to their documents similarly to ChatGPT.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description Screen.Recording.2024-02-19.at.15.37.57.mov">Screen.Recording.2024-02-19.at.15.37.57.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/93524949/320215432-807fb58c-40e0-407e-afb3-a3813477ce9e.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI0OTYwMzcsIm5iZiI6MTcxMjQ5NTczNywicGF0aCI6Ii85MzUyNDk0OS8zMjAyMTU0MzItODA3ZmI1OGMtNDBlMC00MDdlLWFmYjMtYTM4MTM0NzdjZTllLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDA3VDEzMTUzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY3YmY3ZGU0NWE4NTE0OGY3MjEyNzVkMmQwNTk5YTM4NjZlMGI0ODQ2NGVjYjIzNTBhMTY3YjE1MjM4YzZiYTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nIE6fD_g-uEkpe_AxoOL80GJ3IhRS81lJNxyXji7hlE" data-canonical-src="https://private-user-images.githubusercontent.com/93524949/320215432-807fb58c-40e0-407e-afb3-a3813477ce9e.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI0OTYwMzcsIm5iZiI6MTcxMjQ5NTczNywicGF0aCI6Ii85MzUyNDk0OS8zMjAyMTU0MzItODA3ZmI1OGMtNDBlMC00MDdlLWFmYjMtYTM4MTM0NzdjZTllLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDA3VDEzMTUzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY3YmY3ZGU0NWE4NTE0OGY3MjEyNzVkMmQwNTk5YTM4NjZlMGI0ODQ2NGVjYjIzNTBhMTY3YjE1MjM4YzZiYTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nIE6fD_g-uEkpe_AxoOL80GJ3IhRS81lJNxyXji7hlE" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto">Dot is built with Electron JS, but its main functionalities come from a bundled install of Python that contains all libraries and necessary files. A multitude of libraries are used to make everything work, but perhaps the most important to be aware of are: llama.cpp to run the LLM, FAISS to create local vector stores, and Langchain &amp; Huggingface to setup the conversation chains and embedding process.</p>

<p dir="auto">You can either install the packaged app in the <a href="https://dotapp.uk/" rel="nofollow">Dot website </a> or can set up the project for development, to do so follow these steps:</p>
<ul dir="auto">
<li>Clone the repository <code>$ https://github.com/alexpinel/Dot.git</code></li>
<li>Install Node js and then run <code>npm install</code> inside the project repository, you can run <code>npm install --force</code> if you face any issues at this stage</li>
</ul>
<p dir="auto">Now, it is time to add a full python bundle to the app. The purpose of this is to create a distributable environment with all necessary libraries, if you only plan on using Dot from the console you might not need to follow this particular step but then make sure to replace the python path locations specified in <code>src/index.js</code>. Creating the python bundle is covered in detail here: <a href="https://til.simonwillison.net/electron/python-inside-electron" rel="nofollow">https://til.simonwillison.net/electron/python-inside-electron</a> , the bundles can also be installed from here: <a href="https://github.com/indygreg/python-build-standalone/releases/tag/20240224">https://github.com/indygreg/python-build-standalone/releases/tag/20240224</a></p>
<p dir="auto">Having created the bundle, please rename it to &#39;python&#39; and place it inside the <code>llm</code> directory. It is now time to get all necessary libraries, keep in mind that running a simple <code>pip install</code> will not work without specifying the actual path of the bundle so use this instead: <code>path/to/python/.bin/or/.exe -m pip install</code></p>
<p dir="auto">Required python libraries:</p>
<ul dir="auto">
<li>pytorch <a href="https://pytorch.org/get-started/locally/" rel="nofollow">link</a> (CPU version recommended as it is lighter than GPU)</li>
<li>langchain <a href="https://python.langchain.com/docs/get_started/quickstart" rel="nofollow">link</a></li>
<li>FAISS <a href="https://python.langchain.com/docs/integrations/vectorstores/faiss" rel="nofollow">link</a></li>
<li>HuggingFace <a href="https://python.langchain.com/docs/integrations/platforms/huggingface" rel="nofollow">link</a></li>
<li>llama.cpp <a href="https://github.com/abetlen/llama-cpp-python">link</a> (Use CUDA implementation if you have an Nvidia GPU!)</li>
<li>pypdf <a href="https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf" rel="nofollow">link</a></li>
<li>docx2txt <a href="https://python.langchain.com/docs/integrations/document_loaders/microsoft_word" rel="nofollow">link</a></li>
<li>Unstructured <a href="https://github.com/Unstructured-IO/unstructured">link</a> (Use <code>pip install &#34;unstructured[pptx, md, xlsx]</code> for the file formats)</li>
</ul>
<p dir="auto">Now python should be setup and running! However, there is still a few more steps left, now is the time to add the final magic to Dot! First, create a folder inside the <code>llm</code> directory and name it <code>mpnet</code>, there you will need to install sentence-transformers to use for the document embeddings, fetch all the files from the following link and place them inside the new folder: <a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2/tree/main" rel="nofollow">sentence-transformers/all-mpnet-base-v2</a></p>
<p dir="auto">Finally, download the Mistral 7B LLM from the following link and place it inside the <code>llm/scripts</code> directory alongside the python scripts used by Dot: <a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/blob/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf" rel="nofollow">TheBloke/Mistral-7B-Instruct-v0.2-GGUF</a></p>
<p dir="auto">That&#39;s it! If you follow these steps you should be able to get it all running, please let me know if you are facing any issues :)</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Future features I&#39;d like to add:</h3><a id="user-content-future-features-id-like-to-add" aria-label="Permalink: Future features I&#39;d like to add:" href="#future-features-id-like-to-add"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Linux support</li>
<li>Ability to choose LLM</li>
<li>Image support would be cool</li>
<li>Increased awarnes of documents apart from merely their content</li>
<li>Loading individual files instead of selecting a folder  (This is really needed, some users get confused by this and I cannot blame them at all)</li>
<li>Increased security considerations, after all this is the whole point of using a local LLM</li>
<li>Support for more docs</li>
<li>Storing file databases, allowing users to quickly switch between groups of files without having to load them all again</li>
<li>idk, will find out along the way</li>
</ul>

<p dir="auto">Please do! I am a busy student working on this as a side project so help is more than welcome!</p>
</article></div></div>
  </body>
</html>
