<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://render.com/blog/how-we-found-7-tib-of-memory-just-sitting-around">Original</a>
    <h1>How We Found 7 TiB of Memory Just Sitting Around</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><figure><div><p>“</p><blockquote><p>Debugging infrastructure at scale is rarely about one big aha moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.</p></blockquote></div></figure>

<figure><img alt="Getting ready to dissect what I like to call: the Kubernetes hypercube of bad vibes." loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/0681dc3bd3f1323e67c2a9275e216035d84a5b47-1184x864.png?w=1800&amp;fm=webp"/><figcaption>Getting ready to dissect what I like to call: the Kubernetes hypercube of bad vibes.</figcaption></figure> <p><em>Credits: Hyperkube from <a target="_blank" rel="noopener noreferrer" href="https://gregegan.net/APPLETS/29/29.html"><span><span></span><span><span>gregegan.net</span></span></span><span></span></a>, diagram (modified) from <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/scalability-envelope.png"><span><span></span><span><span>Kubernetes community repo</span></span></span><span></span></a></em></p>
<p>Plenty of teams run Kubernetes clusters bigger than ours. <a target="_blank" rel="noopener noreferrer" href="https://github.com/bchess/k8s-1m%20"><span><span></span><span><span>More nodes</span></span></span><span></span></a>, more pods, more ingresses, you name it. In most dimensions, someone out there has us beat.</p>
<p>There&#39;s one dimension where I suspect we might be near the very top: namespaces. I say that because we keep running into odd behavior in any process that has to keep track of them. In particular, anything that listwatches them ends up using a surprising amount of memory and puts real pressure on the apiserver. This has become one of those scaling quirks you only really notice once you hit a certain threshold. As this memory overhead adds up, efficiency decreases: each byte we have to use for management is a byte we can&#39;t put towards user services.</p>
<p>The problem gets significantly worse when a daemonset needs to listwatch namespaces or network policies (netpols, which we define per namespace). Since daemonsets run a pod on every node, each of those pods independently performs a listwatch on the same resources. As a result, memory usage increases with the number of nodes.</p>
<p>Even worse, these listwatch calls can put significant load on the apiserver. If many daemonset pods restart at once, such as during a rollout, they can overwhelm the server with requests and cause real disruption.</p>

<p>A few months ago, if you looked at our nodes, the largest memory consumers were often daemonsets. In particular, Calico and Vector which handle configuring networking and log collection respectively.</p>
<p>We had already done some work to reduce Calico’s memory usage, <a target="_blank" rel="noopener noreferrer" href="https://github.com/projectcalico/calico/pull/9514"><span><span></span><span><span>working closely</span></span></span><span></span></a> with the project’s maintainers to make it scale more efficiently. That optimization effort was a big win for us, and it gave us useful insight into how memory behaves when namespaces scale up.</p>
<figure><img alt="Memory profiling results" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/216c769005be1ea7fbf402921f0b0b915d9205d2-1999x749.png?w=1800&amp;fm=webp"/><figcaption>Memory profiling results</figcaption></figure>
<figure><img alt="Time-series graph of memory usage per pod for calico-node instances" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/923559ac8d19263b671db922ceaabbfeb24e5f89-1999x878.png?w=1800&amp;fm=webp"/><figcaption>Time-series graph of memory usage per pod for calico-node instances</figcaption></figure>
<p>To support that work, we set up a staging cluster with several hundred thousand namespaces. We knew that per-namespace network policies (netpols) were the scaling factor that stressed Calico, so we reproduced those conditions to validate our changes.</p>
<p>While running those tests, we noticed something strange. Vector, another daemonset, also started consuming large amounts of memory.</p>
<figure><img alt="Memory usage per pod graph showing Vector pods" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/81655caa5f70840c1f3cdfd6a6687e555f53f2ac-1999x372.png?w=1800&amp;fm=webp"/><figcaption>Memory usage per pod graph showing Vector pods</figcaption></figure>
<p>The pattern looked familiar, and we knew we had another problem to dig into. Vector obviously wasn’t looking at netpols but after poking around a bit we found it was listwatching namespaces from every node in order to allow referencing namespace labels per-pod in the <a target="_blank" rel="noopener noreferrer" href="https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/"><span><span></span><span><span>kubernetes logs source</span></span></span><span></span></a>.</p>

<p>That gave us an idea: what if Vector didn’t need to use namespaces at all? Was that even possible?</p>
<p>As it turns out, yes, they were in use in our configuration, but only to check whether a pod belonged to a user namespace.</p>
<!--$?--><template id="B:0"></template><!--/$-->
<p>Conveniently, we realized we could hackily describe that condition in another way, and the memory savings were absolutely worth it.</p>
<!--$?--><template id="B:1"></template><!--/$-->
<h2 id="building-the-fix-and-breaking-the-logs"><a href="#building-the-fix-and-breaking-the-logs"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill="currentColor" fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Building the fix (and breaking the logs)</h2>
<p>At that point, we were feeling a bit too lucky. We reached out to the Vector maintainers to ask whether disabling this behavior would actually work, and whether they would be open to accepting a contribution if we made it happen.</p>
<figure><img alt="GitHub comment proposing to make namespace list/watching in Vector an opt-in setting" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/08ccd23eba0aa36a1c4f028d7841cd0e73cfac78-1800x298.png?w=1800&amp;fm=webp"/><figcaption>GitHub comment proposing to make namespace list/watching in Vector an opt-in setting</figcaption></figure>
<p>From there, all that was left was to try it. The code change was straightforward. We added a new config option and threaded it through the relevant parts of the codebase.</p>
<!--$?--><template id="B:2"></template><!--/$-->
<p>After a few hours of flailing at rustc, a Docker image finally built and we were ready to test the theory. The container ran cleanly with no errors in the logs, which seemed promising.</p>
<p>But then we hit a snag. Nothing was being emitted. No logs at all. I couldn’t figure out why.</p>
<p>Thankfully, our pal Claude came to the rescue:</p>
<figure><img alt="Claude chatbot answer: Looking at the code, I can see the issue. When add_namespace_fields is set to false, the namespace watcher/reflector is not created (lines 722–741). However, there’s still a dependency on the namespace state in the K8sPathsProvider (line 768) and NamespaceMetadataAnnotator (line 774)" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/37b86cdfe8a37e07800504121db722d7c31be55a-1674x156.png?w=1800&amp;fm=webp"/><figcaption>Claude chatbot answer: Looking at the code, I can see the issue. When add_namespace_fields is set to false, the namespace watcher/reflector is not created (lines 722–741). However, there’s still a dependency on the namespace state in the K8sPathsProvider (line 768) and NamespaceMetadataAnnotator (line 774)</figcaption></figure><p> I rebuilt it (which took like 73 hours because Rust), generated a new image, updating staging, and watched nervously. This time, logs were flowing like normal and…</p><figure><img alt="Memory usage per pod graph" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/4329f1861ee643af743d5c398ebb1a870ae18f2d-1999x239.png?w=1800&amp;fm=webp"/><figcaption>Memory usage per pod graph</figcaption></figure>

<p>The change saved 50 percent of memory. A huge win. We were ready to wrap it up and ship to production.</p>
<p>But then Hieu, one of our teammates, asked a very good question.</p><figure><img alt="Slack conversation: Hieu: that sounds good. still concerning that it uses 1Gi in staging without namespace data, though.
Me: we can profile it more if we want. now that I can build images, I think we can do the Rust profiling the same way we get from Go for free.Hieu: cool! yeah, I think it’d be worth understanding where the RAM is going" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/a20c729139c4f4115413c5d720cfa2cd2fbf4c58-772x884.png?w=1800&amp;fm=webp"/><figcaption>Slack conversation: Hieu: that sounds good. still concerning that it uses 1Gi in staging without namespace data, though.
Me: we can profile it more if we want. now that I can build images, I think we can do the Rust profiling the same way we get from Go for free.Hieu: cool! yeah, I think it’d be worth understanding where the RAM is going</figcaption></figure>
<p>He was right, something didn’t add up.</p>
<p>A few hours later, after repeatedly running my head into a wall, I still hadn’t found anything. There was still a full gibibyte of memory unaccounted for. My whole theory about how this worked was starting to fall apart.</p>
<p>I even dropped into the channel to see if anyone had Valgrind experience:</p>
<p><em><strong>Me (later in channel):</strong> anybody got a background in valgrind? seems pretty straightforward to get working so far but it won’t end up interfacing with pyroscope. we’ll have to exec in and gdb manually.</em></p>
<p>In a last-ditch effort to profile it again, I finally saw the answer. It had been staring me in the face the whole time.</p>
<p>We actually had <em>two</em> kubernetes_logs sources on user nodes. I had only set the flag on one of them. Once I applied it to both, memory usage dropped to the level we had seen in staging before the extra namespaces were added.</p>
<!--$?--><template id="B:3"></template><!--/$-->

<p>I put together a full <a target="_blank" rel="noopener noreferrer" href="https://github.com/vectordotdev/vector/pull/23601"><span><span></span><span><span>pull request</span></span></span><span></span></a>, and after waiting a little while, <a target="_blank" rel="noopener noreferrer" href="https://vector.dev/releases/0.50.0/"><span><span></span><span><span>it shipped</span></span></span><span></span></a>!</p><figure><img alt="PR merged!" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/5b74dfa738353575d2cd06e9ecb078d208d0801c-1752x262.png?w=1800&amp;fm=webp"/><figcaption>PR merged!</figcaption></figure> <p><a target="_blank" rel="noopener noreferrer" href="https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/#insert_namespace_fields"><span><span></span><span><span><figure><img alt="Changelog noting new insert_namespace_fields. Click to see it in the Vector docs." loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/794359a77a47049bf34ed82d49aeac25adb905ba-960x142.png?w=1800&amp;fm=webp"/><figcaption>Changelog noting new insert_namespace_fields. Click to see it in the Vector docs.</figcaption></figure></span></span></span><span></span></a></p>
<p>Around the same time, our colleague Mark happened to be on-call. He did his usual magic — pulled everything together, tested the rollout in staging, and got it shipped to production.</p>
<p>I’ll let the results speak for themselves. </p><figure><img alt="Memory usage per pod dropped from nearly 4 GiB down to just a few tens of MiB not to mention the reduction in CPU and network IO" loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/d0d9e8c1b67c4c14952f989ca0a739918b933fb3-2944x1608.png?w=1800&amp;fm=webp"/><figcaption>Memory usage per pod dropped from nearly 4 GiB down to just a few tens of MiB not to mention the reduction in CPU and network IO</figcaption></figure>
<figure><img alt="The total memory usage across one of our larger clusters dropped by 1TiB." loading="lazy" width="2000" height="0" decoding="async" data-nimg="1" src="https://cdn.sanity.io/images/hvk0tap5/production/95d9bac099a1327dd9383e1eb5412ad122f05290-2294x836.png?w=1800&amp;fm=webp"/><figcaption>The total memory usage across one of our larger clusters dropped by 1TiB.</figcaption></figure><p> Our largest cluster saw a 1 TiB memory drop, with savings across our other clusters adding up to a total of just over 7 TiB.</p>

<p>Debugging infrastructure at scale is rarely about one big “aha” moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.</p>
<p>In this case, it started with a memory chart that didn’t look quite right, a teammate asking the right question at the right time, and a bit of persistence. When applied to our whole infrastructure, that simple fix freed up <strong>7 TiB of memory</strong>, reduced risk during rollouts, and made the system easier to reason about.</p>
<p>Huge thanks to Hieu for pushing the investigation forward, Mark for shipping it smoothly, and the Vector maintainers for being responsive and open to the change.</p>
<p>If you’re running daemonsets at scale and seeing unexplained memory pressure, it might be worth asking:</p>
<p>Do you really need those namespace labels?</p></div></div></div></div>
  </body>
</html>
