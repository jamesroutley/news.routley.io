<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.nelhage.com/post/cpython-tail-call/">Original</a>
    <h1>Performance of the Python 3.14 tail-call interpreter</h1>
    
    <div id="readability-page-1" class="page"><div>
  <p>About a month ago, the CPython project merged <a href="https://docs.python.org/3.14/whatsnew/3.14.html#whatsnew314-tail-call">a new implementation strategy</a> for their bytecode interpreter. The initial headline results <a href="https://github.com/python/cpython/pull/128718">were very impressive</a>, showing a 10-15% performance improvement <strong>on average</strong> across <a href="https://pyperformance.readthedocs.io/">a wide range of benchmarks</a> across a variety of platforms.</p>
<p>Unfortunately, as I will document in this post, these impressive performance gains turned out to be <strong>primarily due to inadvertently working around a regression in LLVM 19.</strong> When benchmarked against a better baseline (such GCC, clang-18, or LLVM 19 with certain tuning flags), the performance gain drops to 1-5% or so depending on the exact setup.</p>
<p>When the tail-call interpreter was announced, I was surprised and impressed by the performance improvements, but also confused: Iâ€™m not an expert, but Iâ€™m passingly-familiar with modern CPU hardware, compilers, and interpreter design, and I couldnâ€™t explain why this change would be so effective. I became curious â€“ and perhaps slightly obsessed â€“ and the reports in this post are the result of a few weeks of off-and-on compiling and benchmarking and disassembly of dozens of different Python binaries, in an attempt to understand what I was seeing.</p>
<p>At the end, I <a href="#reflections">will reflect</a> on this situation as a case study in some of the challenges of benchmarking, performance engineering, and software engineering in general.</p>
<p>I also want to be clear that I still think the tail-calling interpreter is a great piece of work, as well as a genuine speedup (albeit more modest than initially hoped). I am also optimistic itâ€™s a more robust approach than the older interpreter, in ways Iâ€™ll explain in this post. I also really donâ€™t want to blame anyone on the Python team for this error. This sort of confusion turns out to be very common â€“ Iâ€™ve certainly misunderstood many a benchmark myself â€“ and Iâ€™ll have some reflections on that topic at the end.</p>
<p>In addition, the impact of the LLVM regression doesnâ€™t seem to have been known prior to this work (and the bug still isnâ€™t fixed, as of this writing); thus, in that sense, the alternative (without this work) probably really was 10-15% slower, for builds using clang-19 or newer. For instance, Simon Willison <a href="https://simonwillison.net/2025/Feb/13/python-3140a5/">reproduced the 10% speedup</a> â€œin the wild,â€ as compared to Python 3.13, using builds from <a href="https://github.com/astral-sh/python-build-standalone"><code>python-build-standalone</code></a>.</p>

<p>Here are my headline results. I benchmarked several builds of the CPython interpreter, using multiple different compilers and different configuration options, on two machines: an Intel server (a <a href="https://www.intel.com/content/www/us/en/products/sku/230580/intel-core-i513500-processor-24m-cache-up-to-4-80-ghz/specifications.html">Raptor Lake i5-13500</a> I maintain in Hetzner), and my Apple M1 Macbook Air. You can reproduce these builds <a href="https://github.com/nelhage/cpython-interp-perf/">using my <code>nix</code> configuration</a>, which I found essential for managing so many different moving pieces at once.</p>
<p>All builds use LTO and PGO. These configurations are:</p>
<ul>
<li><code>clang18</code>: Built using Clang 18.1.8, using computed gotos.</li>
<li><code>gcc</code> (Intel only): Built with GCC 14.2.1, using computed gotos.</li>
<li><code>clang19</code>: Built using Clang 19.1.7, using compute gotos.</li>
<li><code>clang19.tc</code>: Built using Clang 19.1.7, using the new tail-call interpreter.</li>
<li><code>clang19.taildup</code>: Built using Clang 19.1.7, computed gotos and some <code>-mllvm</code> tuning flags which work around the regression.</li>
</ul>
<p>Iâ€™ve used <code>clang18</code> as the baseline, and reported the bottom-line â€œaverageâ€ reported by <code>pypeformance</code>/<code>pyperf compare_to</code>. You can find the complete output files and reports <a href="https://github.com/nelhage/cpython-interp-perf/tree/data/">on github</a>.</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>clang18</th>
<th>clang19</th>
<th>clang19.taildup</th>
<th>clang19.tc</th>
<th>gcc</th>
</tr>
</thead>
<tbody>
<tr>
<td>Raptor Lake i5-13500</td>
<td>(ref)</td>
<td>1.09x slower</td>
<td>1.01x faster</td>
<td>1.03x faster</td>
<td>1.02x faster</td>
</tr>
<tr>
<td>Apple M1 Macbook Air</td>
<td>(ref)</td>
<td>1.12x slower</td>
<td>1.02x slower</td>
<td>1.00x slower</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p>Observe that the tail-call interpreter still exhibits a speedup as compared to clang-18, but that itâ€™s far less dramatic than the slowdown from moving to clang-19. The Python team has also observed larger speedups than I have (after accounting for the bug) on some other platforms.</p>

<h2 id="a-brief-background">A brief backgroundÂ <a href="#a-brief-background"><i>	ğŸ”—ï¸</i></a> </h2>
<p>A classic bytecode interpreter consists of a <code>switch</code> statement inside of a <code>while</code> loop, looking something like so:</p>
<div><pre tabindex="0"><code data-lang="c++"><span><span><span>while</span> (true) {
</span></span><span><span>  opcode_t this_op <span>=</span> bytecode[pc<span>++</span>];
</span></span><span><span>  <span>switch</span> (this_op) {
</span></span><span><span>    <span>case</span> OP_IMM: {
</span></span><span><span>      <span>// push an immediate onto the stack
</span></span></span><span><span><span></span>      <span>break</span>;
</span></span><span><span>    }
</span></span><span><span>    <span>case</span> OP_ADD: {
</span></span><span><span>      <span>// handle the add
</span></span></span><span><span><span></span>      <span>break</span>;
</span></span><span><span>    }
</span></span><span><span>    <span>// etc
</span></span></span><span><span><span></span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>Most compilers will compile the <code>switch</code> into a jump table â€“ they will emit a table containing the address of each <code>case OP_xxx</code> block, index into it with the opcode, and perform an indirect jump.</p>
<p>Itâ€™s <a href="https://link.springer.com/content/pdf/10.1007/3-540-44681-8_59.pdf">long been known</a> that you can speed up a bytecode interpreter of this style by replicating the jump table dispatch into the body of each opcode. That is, instead of ending each opcode with a <code>jmp loop_top</code>, each opcode contains a separate instance of the â€œdecode next instruction and index through the jump tableâ€ logic.</p>
<p>Modern C compilers support <a href="https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html#Labels-as-Values">taking the address of labels</a>, and then using those labels in a â€œcomputed goto,â€ in order to implement this pattern. Thus, many modern bytecode interpreters, including CPython (before the tail-call work), employ an interpreter loop that looks something like:</p>
<div><pre tabindex="0"><code data-lang="c++"><span><span><span>static</span> <span>void</span> <span>*</span>opcode_table[<span>256</span>] <span>=</span> {
</span></span><span><span>    [OP_IMM] <span>=</span> <span>&amp;&amp;</span>TARGET_IMM,
</span></span><span><span>    [OP_ADD] <span>=</span> <span>&amp;&amp;</span>TARGET_ADD,
</span></span><span><span>    <span>// etc
</span></span></span><span><span><span></span>};
</span></span><span><span>
</span></span><span><span><span>#define DISPATCH() goto *opcode_table[bytecode[pc++]]
</span></span></span><span><span><span></span>
</span></span><span><span>DISPATCH();
</span></span><span><span>
</span></span><span><span>TARGET_IMM: {
</span></span><span><span>    <span>// push an immediate onto the stack
</span></span></span><span><span><span></span>    DISPATCH();
</span></span><span><span>}
</span></span><span><span>TARGET_ADD: {
</span></span><span><span>    <span>// handle the add
</span></span></span><span><span><span></span>    DISPATCH();
</span></span><span><span>}
</span></span></code></pre></div><h2 id="computed-goto-in-llvm">Computed goto in LLVMÂ <a href="#computed-goto-in-llvm"><i>	ğŸ”—ï¸</i></a> </h2>
<p>For performance reasons (performance of the compiler, not the generated code), it turns out that Clang and LLVM, internally, actually merges all of the <code>goto</code>s in the latter code into a <strong>single</strong> <a href="https://llvm.org/docs/LangRef.html#indirectbr-instruction"><code>indirectbr</code> LLVM instruction</a>, which each opcode will jump to. That is, the compiler takes our hard work, and deliberately rewrites into a control-flow-graph that looks essentially the same as the <code>switch</code>-based interpreter!</p>
<p>Then, during code generation, LLVM performs â€œtail duplication,â€ and copies the branch <strong>back</strong> into each location, restoring the original intent. This dance is documented, at a high level, <a href="https://blog.llvm.org/2010/01/address-of-label-and-indirect-branches.html">in an old LLVM blog post</a> introducing the new implementation.</p>
<h2 id="the-llvm-19-regression">The LLVM 19 regressionÂ <a href="#the-llvm-19-regression"><i>	ğŸ”—ï¸</i></a> </h2>
<p>The whole reason for the deduplicate-then-copy dance is that, for technical reasons, creating and manipulating the control-flow-graph containing many <code>indirectbr</code> instructions can be quite expensive.</p>
<p>In order to avoid catastrophic slowdowns (or memory usage) in certain cases, LLVM 19 implemented <a href="https://github.com/llvm/llvm-project/pull/78582">some limits on tail-duplication pass</a>, causing it to bail out if duplication would blow up the size of the IR past certain limits.</p>
<p>Unfortunately, on CPython those limits resulted in Clang <strong>leaving all of the dispatch jumps merged</strong>, and entirely undoing the whole purpose of the computed <code>goto</code>-based implementation! This bug was <a href="https://github.com/llvm/llvm-project/issues/106846">first identified</a> by another language implementation with a similar interpreter loop, but had not been known (as far as I can find) to affect CPython.</p>
<p>In addition to the performance impact, we can observe the bug directly by disassembling the resulting object code and counting the number of distinct indirect jumps:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ objdump -S --disassemble<span>=</span>_PyEval_EvalFrameDefault <span>${</span>clang18<span>}</span>/bin/python3.14 | <span>\
</span></span></span><span><span><span></span>  egrep -c <span>&#39;jmp\s+\*&#39;</span>
</span></span><span><span><span>332</span>
</span></span><span><span>
</span></span><span><span>$ objdump -S --disassemble<span>=</span>_PyEval_EvalFrameDefault <span>${</span>clang19<span>}</span>/bin/python3.14 | <span>\
</span></span></span><span><span><span></span>  egrep -c <span>&#39;jmp\s+\*&#39;</span>
</span></span><span><span><span>3</span>
</span></span></code></pre></div><h3 id="further-weirdness">Further weirdnessÂ <a href="#further-weirdness"><i>	ğŸ”—ï¸</i></a> </h3>
<p>I am confident that the change to the tail-call duplication logic caused the regression: if <a href="#the-fix">you fix it</a>, performance matches clang-18. However, I canâ€™t fully explain the <strong>magnitude</strong> of the regression.</p>
<p>Historically, the optimization of replicating the bytecode dispatch into each opcode has been cited to speed up interpreters anywhere from <a href="https://github.com/python/cpython/blob/c718c6be0f82af5eb0e57615ce323242155ff014/Misc/HISTORY#L15252-L15255">20%</a> to <a href="https://link.springer.com/content/pdf/10.1007/3-540-44681-8_59.pdf">100%</a>. However, on modern processors with improved branch predictors, <a href="https://inria.hal.science/hal-01100647/document">more recent work</a> finds a much smaller speedup, on the order of 2-4%.</p>
<p>We can verify this 2-4% number in practice, because Python still supports the â€œold-styleâ€ interpreter, which uses a single <code>switch</code> statement, via a configuration option. Hereâ€™s what we see if we benchmark that interpreter (&#34;.nocg&#34; for â€œno computed gotosâ€ in the following table):</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>clang18</th>
<th>clang18.nocg</th>
<th>clang19.nocg</th>
<th>clang19</th>
</tr>
</thead>
<tbody>
<tr>
<td>Performance change</td>
<td>(ref)</td>
<td>1.01x faster</td>
<td>1.02x slower</td>
<td>1.09x slower</td>
</tr>
</tbody>
</table>
<p>Notice that <code>clang19.nocg</code>&#34; is only 2% slower than <code>clang18</code>, even though the base <code>clang19</code> build is 9% slower! I interpret that â€œ2%â€ as a fairer estimate for the cost/benefit of duplicating opcode dispatch, alone, and I donâ€™t fully understand the other one.</p>
<h3 id="do-we-need-computed-gotos">Do we need computed gotos?Â <a href="#do-we-need-computed-gotos"><i>	ğŸ”—ï¸</i></a> </h3>
<p>I havenâ€™t mentioned the <code>clang19.nocg</code> benchmark, which you may notice claims to be <strong>faster</strong> than <code>clang19</code>&#34; It was at this point that I discovered an additional, and very funny, twist to the story.</p>
<p>I explained earlier that Clang and LLVM:</p>
<ol>
<li>Compiles the <code>switch</code> into a jump table and an indirect jump, very similar to the one we create by hand using computed gotos</li>
<li>Compiles computed gotos into a control-flow graph that closely resembles the classic <code>switch</code> graph, which a single instance of the opcode dispatch, and</li>
<li>Is able to reverse the transformation during codegen in order to duplicate the dispatch</li>
</ol>
<p>Those facts taken together, might lead you to ask, â€œCouldnâ€™t we just start with the <code>switch</code>-based interpreter, and have the <strong>compiler</strong> do tail-duplication, and get the same benefits?â€</p>
<p>And it turns out: Yes.</p>
<p>clang-18 (or clang-19 with appropriate flags), when presented with the â€œclassicâ€ <code>switch</code>-based interpreter, <strong>goes ahead and that replicates the dispatch logic into each the body of each opcode anyways</strong>. Hereâ€™s another table, showing the same builds with the number of indirect jumps, using the <code>objdump | grep</code> test from earlier:</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>clang18</th>
<th>clang18.nocg</th>
<th>clang19.nocg</th>
<th>clang19</th>
</tr>
</thead>
<tbody>
<tr>
<td># of indirect jumps</td>
<td>332</td>
<td>306</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Thus, thereâ€™s a case to be made that the entire â€œcomputed gotoâ€ interpreter turns out to be entirely unnecessary complexity (at least for modern Clang). The compiler is perfectly capable of performing the same transformation itself, and (apparently) the computed gotos donâ€™t even suffice to guarantee it!</p>
<p>That said, I did also test GCC, and GCC (at least as of 14.2.1) does not replicate the <code>switch</code>, but does implement the desired behavior for when using computed goto. So at least in that case we see the expected behavior.</p>
<h2 id="the-fix">The fixÂ <a href="#the-fix"><i>	ğŸ”—ï¸</i></a> </h2>
<p><a href="https://github.com/llvm/llvm-project/pull/114990">LLVM pull request 114990</a> is open and should fix this regression. It has not yet merged as of this writing, but hopefully will soon. I have also benchmarked it and confirmed it restores the expected performance.</p>
<p>In the meanwhile, the <a href="https://github.com/llvm/llvm-project/pull/78582">PR that caused the regression</a> added a tunable option to choose the threshold at which tail-duplication will abort. We can restore similar behavior on clang-19 by simply setting that limit to a very large number<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>

<p>I will freely admit that I got nerdsniped quite effectively by this topic, and have gone far deeper than was really necessary. That said, having done so, I think there are a number of interesting lessons and reflections to be taken away, which generalize to software engineering and performance engineering, and I will attempt to extract and meditate upon some of them.</p>
<h2 id="on-benchmarking">On benchmarkingÂ <a href="#on-benchmarking"><i>	ğŸ”—ï¸</i></a> </h2>
<p>When optimizing a system, we generally construct some set of benchmarks and benchmarking methodology, and then evaluate proposed changes using those benchmarks.</p>
<p>Any set of benchmarks or benchmark procedures embeds (often implicitly) what I like to call a â€œtheory of performance.â€ Your theory of performance is a set of beliefs and assumptions that answer questions like â€œwhich variables (may) effect performance, in what ways?â€ and â€œwhat is the relationship between results on benchmarks and the â€œtrueâ€ performance in â€œproductionâ€?â€</p>
<p>The benchmarks ran on the tail-call interpreter showed a 10-15% speedup when compared with the old computed-goto interpreter. Those benchmarks were accurate, in that they were accurately measuring (as far as I know) the performance difference between those builds. However, in order to generalize those specific data points into the statement â€œThe tail-call interpreter is 10-15% faster than the computed-goto interpreter, more generally,â€ or even â€œThe tail-call interpreter will speed up Python by 10-15% for our users,â€ we need to bring in more assumptions and beliefs about the world. In this case, it turns out the story was more complex, and those broader claims were not true in full generality.</p>
<p>(Once again, I really donâ€™t want to blame the Python developers! This stuff is <strong>hard</strong> and there are a million ways to get confused or to reach somewhat-incorrect conclusions. I had to do ~three weeks of intense benchmarking and experimenting to reach a better understanding. My point is that this is a very general challenge!)</p>
<h3 id="baselines">BaselinesÂ <a href="#baselines"><i>	ğŸ”—ï¸</i></a> </h3>
<p>This example highlights another recurring challenge, not only in software performance, but in many other domains: â€œWhat baseline do you compare against?â€</p>
<p>Any time you propose a new solution or method for some problem, you typically have a way of running your new method, and producing some relevant performance metrics.</p>
<p>Once you have metrics for <strong>your</strong> system, however, you need to know what to compare them against, in order to decide if theyâ€™re any good! Even if you score well on some absolute scale (assuming there <strong>is</strong> a sensible absolute scale to evaluate), if your method is worse than an existing solution, itâ€™s probably not that interesting.</p>
<p>Typically, you want to compare against â€œthe current best-known approach.â€ But sometimes that can be hard to do! Even if you understand the current approach in theory, you may or may not be an expert in applying it in practice. In the case of software this may mean something like, tuning your operating system or compiler options or other flags. The current-best approach may have published benchmarks, but theyâ€™re not always relevant to you; for instance, maybe it was published years ago on older hardware, and so you canâ€™t do an apples-to-apples comparison with the public numbers. Or maybe their tests were run at a scale you canâ€™t afford to replicate.</p>
<p>I work in machine learning at Anthropic these days, and we see this all the time in ML papers. When a paper comes out claiming some algorithmic improvement or other advance, Iâ€™ve noticed that the first detail our researchers ask is often not â€œWhat did they do?â€ but â€œWhat baseline did they compare against?â€ Itâ€™s easy to get impressive-looking results if youâ€™re comparing against a poorly-tuned baseline, and that observation turns out to explain a surprising fraction of supposed improvements.</p>
<h2 id="on-software-engineering">On software engineeringÂ <a href="#on-software-engineering"><i>	ğŸ”—ï¸</i></a> </h2>
<p>One other highlight, for me, is just how complex and interconnected our software systems are, and how rapidly-moving, and how hard it is to keep track of all the pieces.</p>
<p>If youâ€™d asked me, a month ago, to estimate the likelihood that an LLVM release caused a 10% performance regression in CPython and that no one noticed for five months, Iâ€™d have thought that a pretty unlikely state of affairs! Those are both widely-used projects, both of which care a fair bit about performance, and â€œsurelyâ€ someone would have tested and noticed.</p>
<p>And probably that <strong>particular</strong> situation was quite unlikely! However, with so many different software projects out there, each moving so rapidly and depending on and being used by so many other projects, it becomes practically-inevitable that <strong>some</strong> regressions â€œlike that oneâ€ happen, almost constantly.</p>
<h3 id="optimizing-compilers">Optimizing compilersÂ <a href="#optimizing-compilers"><i>	ğŸ”—ï¸</i></a> </h3>
<p>The saga of the computed-goto interpreter illustrates recurring tensions and unresolved questions around optimizers and optimizing compilers, to which we donâ€™t yet have agreed-upon answers as a field.</p>
<p>We generally expect our compilers to respect the programmerâ€™s intent, and to compile the code that was written in a way that preserves the programmerâ€™s intent.</p>
<p>We also, however, expect our compilers to optimize our code, and to transform it in potentially-complex-and-unintuitive ways in order to make it run faster.</p>
<p>These expectations are in tension, and we have a dearth of patterns and idioms to explain to the compiler â€œwhyâ€ we wrote code in various ways, and whether we were <strong>deliberately</strong> trying to trigger a certain output, or make a certain performance-related decision, or not.</p>
<p>Our compilers typically only <strong>promise</strong> to emit code with â€œthe same behaviorâ€ as the code we write; performance is something of a best-effort feature on top of that guarantee.</p>
<p>Thus, we end up in this odd world where clang-19 compiles the computed-goto interpreter â€œcorrectlyâ€ â€“ in the sense that the resulting binary produces all the same value we expect â€“ but at the same time it produces an output completely at odds with the intention of the optimization. Moreover, we also see other versions of the compiler applying optimizations to the â€œnaiveâ€ <code>switch()</code>-based interpreter, which implement the exact same optimization we â€œintendedâ€ to perform by rewriting the source code.</p>
<p>In hindsight, it appears that the â€œcomputed gotoâ€ interpreter, at a source code level, and â€œreplicating the dispatch at a machine-code levelâ€ end up being almost-orthogonal notions! Weâ€™ve seen examples of every instance of the resulting 2x2 matrix! Because all of those <code>python</code> binaries compute the same values when run, our current tools are essentially unable to talk about the distinctions between them in a coherent way.</p>
<p>This confusion is one way in which I think the tail-calling interpreter (and the compiler features behind it) represent a genuine, and useful, advance in the state of the art. The tail-call interpreter is built on <a href="https://clang.llvm.org/docs/AttributeReference.html#musttail">the <code>musttail</code> attribute</a>, which represents a relatively new kind of compiler feature. <code>musttail</code> does not affect the â€œobservable program behavior,â€ in the classic sense that compilers think, but is rather a conversation with the <strong>optimizer</strong>; it requires that the compiler be able to make certain optimizations, and requires that compilation fail if those optimizations donâ€™t happen.</p>
<p>Iâ€™m hopeful this framework will turn out to be a much more robust style for writing performance-sensitive code, especially over time and as compilers evolve. I look forward to continued experiments with features in that category.</p>
<p>Concretely, I find myself wondering if it would be viable to replace the computed-goto interpreter with something like a (hypothetical) <code>[[clang::musttailduplicate]]</code> attribute on the interpreter <code>while</code> loop. Iâ€™m not expert enough in all the relevant IRs and passes to have confidence in this proposal, but perhaps someone with more familiarity can weigh in on the feasibility.</p>
<h2 id="one-more-note-on-nix">One more note: on <code>nix</code>Â <a href="#one-more-note-on-nix"><i>	ğŸ”—ï¸</i></a> </h2>
<p>I want to close with a call-out of how helpful <code>nix</code> was for this project. I have been experimenting with nix and NixOS for my personal infrastructure over the last year or so, but they turned out to be a total lifesaver for this investigation.</p>
<p>In the course of these experiments, I have built and benchmarked <strong>dozens</strong> of different Python interpreters, across four different compilers (<code>gcc</code>, <code>clang-18</code>, <code>clang-19</code>, and <code>clang-20</code>) and using numerous combinations of compiler flags. Managing all of that by hand would have strained my sanity, and Iâ€™m <strong>certain</strong> I would have made numerous mistakes during which I mixed up which compiler and which flags went into which build, and so on.</p>
<p>Using <code>nix</code>, I was able to keep all of these parallel versions straight, and build them in a reproducible, hermetic style. I was able to write some short abstractions which made them very easy to define, and then know with absolute confidence <strong>where</strong> any given build in my <code>nix</code> store came from, with which compilers and which flags. After a small amount of work to build some helper functions, the core definitions of my build matrix is <a href="https://github.com/nelhage/cpython-interp-perf/blob/afd2123bef6ec3fd872d628f2bb519b20684e161/python.nix#L105-L143">shockingly concise</a>; hereâ€™s a taste:</p>
<div><pre tabindex="0"><code data-lang="nixos"><span><span>{
</span></span><span><span>    base <span>=</span> callPackage buildPython { python3 <span>=</span> python313; };
</span></span><span><span>    optimized <span>=</span> withOptimizations base;
</span></span><span><span>    optLTO <span>=</span> withLTO optimized;
</span></span><span><span>
</span></span><span><span>    clang18 <span>=</span> withLLVM llvmPackages_18 optLTO;
</span></span><span><span>    clang19 <span>=</span> withLLVM llvmPackages_19 optLTO;
</span></span><span><span>    clang20 <span>=</span> withLLVM llvmPackages_20 optLTO;
</span></span><span><span>
</span></span><span><span>    clang18nozero <span>=</span> noZeroCallUsed clang18;
</span></span><span><span>    clang18nocg <span>=</span> withoutCG clang18;
</span></span><span><span>
</span></span><span><span>    clang19taildup <span>=</span> withTailDup clang19;
</span></span><span><span>}
</span></span></code></pre></div><p>I was even able to build a custom version of LLVM (with the bugfix patch), and do Python builds using that compiler. Doing so required <a href="https://github.com/nelhage/cpython-interp-perf/blob/afd2123bef6ec3fd872d628f2bb519b20684e161/llvm.nix#L14-L25">all of about 10 lines of code</a>.</p>
<p>That said, not everything was rosy. For one, <code>nix</code> is, by necessity, â€œweirdâ€ in various ways compared to the ways â€œnormal peopleâ€ use software, and I worry that some of that weirdness may have affected some of my benchmarks or conclusions in ways I didnâ€™t notice. For instance, early on I discovered that nix (by default) builds projects using certain hardening flags that <a href="https://github.com/python/cpython/issues/130961">disproportionately impact the tail-call interpreter</a>. Iâ€™ve handled that one, but are there more?</p>
<p>In addition, Nix is incredible extensible and customizable, but figuring out how to make a specific customization can be a real uphill battle, and involve a lot of trial and error and source-diving. My patched LLVM build ended up being pretty short and clean, but getting there required me to read a lot of <code>nixpkgs</code> source code, mixing and matching two under-documented extensibility mechanisms (<code>extend</code> and <code>overrideAttrs</code> â€“ not to be confused with <code>override</code>, used elsewhere), and one failed attempt which successfully patched <code>libllvm</code>, but then silently built a new <code>clang</code> against the unpatched version.</p>
<p>Still, <code>nix</code> was clearly enormously helpful here, and on net it definitely made this kind of multi-version exploration and debugging <strong>much</strong> saner than any other approach I can imagine.</p>



</div></div>
  </body>
</html>
