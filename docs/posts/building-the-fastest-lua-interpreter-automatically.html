<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sillycross.github.io/2022/11/22/2022-11-22/">Original</a>
    <h1>Building the fastest Lua interpreter automatically</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
      
        <p>It is well-known that writing a good VM for a dynamic language is never an easy job. High-performance interpreters, such as the <a href="https://webkit.org/blog/10308/speculation-in-javascriptcore/" target="_blank" rel="noopener">JavaScript interpreter in Safari</a>, or <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html" target="_blank" rel="noopener">the Lua interpreter in LuaJIT</a>, are often hand-coded in assembly. If you want a JIT compiler for better performance, well, you’ve got some more assembly to write. And if you want the best possible performance with multiple-tier JIT compilation… Well, that’s assembly all the way down.</p>
<p>I have been working on a research project to make writing VMs easier. The idea arises from the following observation: writing a naive interpreter is not hard (just write a big switch-case), but writing a good interpreter (or JIT compiler) is hard, as it unavoidably involves hand-coding assembly. So why can’t we implement a special compiler to automatically <em>generate</em> a high-performance interpreter (and even the JIT) from “the big switch-case”, or more formally, a semantical description of what each bytecode does?</p>
<h3 id="The-LuaJIT-Remake-Project">The LuaJIT Remake Project</h3>
<p>I chose <a href="https://www.lua.org/" target="_blank" rel="noopener">Lua</a> as the experiment target for my idea, mainly because Lua is concise yet supports almost every language feature one can find in dynamic languages, including exotic ones like stackful coroutines. I named my project <a href="https://github.com/luajit-remake/luajit-remake" target="_blank" rel="noopener">LuaJIT Remake</a> (LJR) because in the long term, it will be a multi-tier method-based JIT compiler for Lua.</p>
<p>After months of work on the project, I’ve finally got some early results to share. LJR now has a feature-complete Lua 5.1 interpreter that is automatically generated at build time using a meta-compiler called <code>Deegen</code> (for “Dynamic language Execution Engine Generator”). More importantly, it is the world’s fastest Lua interpreter to date, outperforming LuaJIT’s interpreter by 28% and the official Lua interpreter by 171% on average on a variety of benchmarks.</p>
<p>The figure below illustrates the performance of our interpreter, the LuaJIT interpreter, and the official PUC Lua interpreter. PUC Lua’s performance is normalized to 1 as a baseline.</p>
<p><img src="https://sillycross.github.io/images/2022-11-22/interpreter-perf-comparison-2.png" alt=""/></p>
<p>As the figure shows, our interpreter performs better than LuaJIT’s hand-coded-in-assembly interpreter on 31 out of the 34 benchmarks, and on geometric average, we run 28% faster than LuaJIT interpreter, and almost 3x the speed of official PUC Lua.</p>
<p>Enough of the numbers, now I will dive a bit into how my approach works.</p>
<h3 id="Why-Assembly-After-All">Why Assembly After All?</h3>
<p>To explain how I built the fastest Lua interpreter, one needs to understand why (previously) the best interpreters have been hand-coded in assembly. This section is all about background. If you are already familiar with interpreters, feel free to skip to the next section.</p>
<p>Mike Pall, the author of LuaJIT, has explained this matter clearly in <a href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html" target="_blank" rel="noopener">this great email thread</a> back in 2011. The problem with the “big switch-case” approach is that C/C++ compilers simply cannot handle such code well. Although eleven years have passed, the situtation didn’t change much. Based on my experience, even if a function only has one fast path and one cold path, and the cold path has been nicely annotated with <code>unlikely</code>, LLVM backend will still pour a bunch of unnecessary register moves and stack spills into the fast path. And for the “big switch-case” interpreter loop with hundreds of fast-paths and cold-paths, it’s unsurprising that compilers fail to work well.</p>
<p><a href="https://en.wikipedia.org/wiki/Tail_call" target="_blank" rel="noopener">Tail call</a>, also known as <a href="https://dl.acm.org/doi/10.1145/800179.810196" target="_blank" rel="noopener">continuation-passing style</a>, is an alternative to switch-case-based interpreter loop. Basically each bytecode gets its own function that does the job, and when the job is done, control is transferred to the next function via a tail call dispatch (i.e., a jump instruction at machine code level). So despite that conceptually, the bytecode functions are calling each other, they are really jumping to each other at machine code level, and there will be no unbounded stack growth. An alternate way to look at it is that each “case” clause in the switch-case interpreter loop becomes a function. The “switch” will jump (i.e., tail call) to the corresponding “case” clause, and at the end of the case a jump (i.e., tail call) is executed to jump back to the switch dispatcher.</p>
<p>With the tail-call approach, each bytecode now gets its own function, and the pathological case for the C/C++ compiler is gone. And as shown by <a href="https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html" target="_blank" rel="noopener">the experience</a> of the Google protobuf developers, the tail-call approach can indeed be used to build very good interpreters. But can it push to the limit of hand-written assembly interpreters? Unfortunately, the answer is still no, at least at its current state.</p>
<p>The main blockade to the tail-call approach is the callee-saved registers. Since each bytecode function is still a function, it is required to abide to the calling convention, specifically, every callee-saved register must retain its old value at function exit. So if a bytecode function needs to use a callee-saved register, it needs to save the old value on the stack and restore it at the end. The only way to solve this problem is to use a calling convention with no callee-saved registers. Unfortunately, Clang is (to-date) the only compiler that offers guaranteed-tail-call intrinsic (<code>[[clang::musttail]]</code> annotation), but it has no such user-exposed calling convention with no callee-saved registers. So you lose 6 (or 8, depending on cconv) of the 15 registers for no reason on x86-64, which is clearly bad.</p>
<p>Another blockade to the tail-call approach is, again, the calling convention. No unbounded stack growth is a requirement, but tricky problems can arise when the caller and callee function prototype does not match, and some parameters are being passed in the stack. So Clang makes the compromise and requires the caller and callee to have <em>identical</em> function prototypes if <code>musttail</code> is used. This is extremely annoying in practice once you have tried to write anything serious under such limitation (for POC purpose I had hand-written a naive Lua interpreter using <code>musttail</code>, so I have first-hand experience on how annoying it is).</p>
<h3 id="Generating-the-Interpreter-Another-Level-Of-Indirection-Solves-Everything">Generating the Interpreter: Another Level Of Indirection Solves Everything</h3>
<p>As you might have seen, the root of all the difficulties is that our tool (C/C++) is not ideal for the problem we want to solve. So what’s the solution?</p>
<p>Of course, throwing the tool away and resort to sheer force (hand-coding assembly) is one solution, but doing so also results in high engineering cost. Can we do it more swiftly?</p>
<p>It is <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering" target="_blank" rel="noopener">well-known</a> that all problems in computer science can be solved by another level of indirection. In our case, C/C++ is a very good tool to describe the semantics of each bytecode (i.e., what each bytecode should do), but C/C++ is not a good tool to write the most efficient interpreter. So what if we add one level of indirection: we write the bytecode semantical description in C++, <em>compile it to LLVM IR</em>, and feed the IR into a special-purpose compiler. The special-purpose compiler will take care of all the dirty work, doing proper transformation to the IR and finally generate a nice tail-call-based interpreter.</p>
<p>For example, at LLVM IR level, it is trivial to make a function use <code>GHC</code> calling convention (a convention with no callee-saved registers) and properly transform the function to unify all the function prototype, thus solving the two major problems with <code>musttail</code> tail calls that is unsolvable at C/C++ level. In fact, <code>Deegen</code> (our meta-compiler that generates the interpreter) does a <em>lot</em> more than producing the tail calls, which we will cover in the rest of this post.</p>
<h3 id="Hide-All-the-Ugliness-Behind-Nice-APIs">Hide All the Ugliness Behind Nice APIs</h3>
<p>In Deegen framework, the semantics of each bytecode is described by a C++ function. One of the most important design philosophy of Deegen is to abstract away all the nasty parts of an interpreter. I will demonstrate with a simplified example for the <code>Add</code> bytecode:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span><span>void</span> <span>Add</span><span>(TValue lhs, TValue rhs)</span> </span>{</span></pre></td></tr></tbody></table></figure>
<p>The function <code>Add</code> takes two boxed values (a value along with its type) <code>lhs</code> and <code>rhs</code> as input. It first checks if both <code>lhs</code> and <code>rhs</code> are <code>double</code> (the <code>Is&lt;tDouble&gt;()</code> check). If not, we throw out an error. Otherwise, we add them together by casting the two boxed value to its actual type (<code>double</code>) and do a normal <code>double</code> addition. Finally, we create a new boxed value of <code>double</code> type using <code>TValue::Create&lt;tDouble&gt;()</code>, return it as the result of the bytecode and dispatch to the next bytecode, through the <code>Return()</code> API call (note that this is not the C keyword <code>return</code>).</p>
<p>Notice how much nasty work we have abstracted away: decoding the bytecode, loading the operands and constants, throwing out errors, storing results to the stack frame, and dispatching to the next bytecode. All of these interpreter details either happen automatically, or happen with a simple API call (e.g., <code>ThrowError</code> or <code>Return</code>).</p>
<p>Now let’s extend our <code>Add</code> to add support for the Lua <code>__add</code> metamethod semantics:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span><span>void</span> <span>AddContinuation</span><span>(TValue , TValue )</span> </span>{</span></pre></td></tr></tbody></table></figure>
<p>The <code>GetMMForAdd</code> is some arbitrary runtime function call that gets the metamethod. Deegen does not care about its implementation: the bytecode semantic description is just a normal C++ function, so it can do anything allowed by C++, of course including calling other C++ functions. The interesting part is the <code>MakeCall</code> API. It allows you to call other Lua functions with the specified parameters, and most importantly, a <em>return continuation</em>. The <code>MakeCall</code> API does not return. Instead, when the called function returns, control will be returned to the return continuation (the <code>AddContinuation</code> function). The return continuation function is similar to the bytecode function: it has access to all the bytecode operands, and additionally, it has access to all the values returned from the call. In our case, the semantics for Lua <code>__add</code> is to simply return the first value returned by the call as the result of the bytecode, so we use <code>GetReturnValueAtOrd(0)</code> to get that value, and use the <code>Return</code> API we have covered earlier to complete the <code>Add</code> bytecode and dispatch to the next bytecode.</p>
<p>Again, notice how much nasty work that we have abstracted away: all the details of creating the new Lua frame, adjusting the parameters and return values (overflowing arguments needs to go to variadic arg if callee accepts it, insufficient arguments need to get <code>nil</code>), transferring control to the callee functions, etc., are all hidden by a mere <code>MakeCall</code> API. Furthermore, all of these are language-neutral: if we were to target some other languages (e.g., Python), most of the Deegen code that implements the <code>MakeCall</code> could be reused.</p>
<p>The use of return continuation is designed to support Lua coroutines. Since Lua coroutines are stackful, and <code>yield</code> can happen anywhere (as <code>yield</code> is not a Lua keyword, but a library function), we need to make sure that the C stack is empty at any bytecode boundary, so we can simply tail call to the other continuation to accomplish a coroutine switch. This design also has a few advantages compared with PUC Lua’s coroutine implementation:</p>
<ol>
<li>We have no fragile <code>longjmp</code>s.</li>
<li>We can easily make any library function that calls into VM yieldable using this mechanism. In fact, the error message <code>cannot yield across C call frames</code> is gone completely in LJR: all Lua standard library functions, including exotic ones like <code>table.sort</code>, are redesigned to be yieldable using this mechanism.</li>
</ol>
<h3 id="Automation-Automation-and-More-Automation">Automation, Automation, and More Automation!</h3>
<p>The bytecode semantic function specifies the execution semantics of the bytecode, but one still needs to specify the definition of the bytecode. For example, one needs to know that <code>AddVN</code> takes two operands where LHS is a bytecode slot and RHS is a number value in the constant table, and that <code>AddVN</code> returns one value, and that it always fallthroughs to the next bytecode and cannot branch to anywhere else. In Deegen, this is achieved by a <em>bytecode specification language</em>.</p>
<p>Again, let’s use the <code>Add</code> as the example:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span>DEEGEN_DEFINE_BYTECODE(Add) {</span></pre></td></tr></tbody></table></figure>
<p>There are a few things going on here so we will go through them one by one. First of all, the <code>DEEGEN_DEFINE_BYTECODE</code> is a macro that tells us that you are defining a bytecode.</p>
<p>The <code>Operands(...)</code> API call tells us that the bytecode has two operands, with each can be either a bytecode slot (a slot in the call frame) or a constant in the constant table. Besides <code>BytecodeSlotOrConstant</code>, one can also use <code>Literal</code> to define literal operands, and <code>BytecodeRange</code> to define a range of bytecode values in the call frame.</p>
<p>The <code>Result(BytecodeValue)</code> API call tells us that the bytecode returns one value and does not branch. The enum key <code>BytecodeValue</code> means the bytecode returns one <code>TValue</code>. One can also use enum key <code>CondBr</code> to specify that the bytecode can branch, or just no argument to specify that the bytecode doesn’t return anything.</p>
<p>The <code>Implementation(...)</code> API specifies the execution semantics of the bytecode, which is the <code>Add</code> function we just covered.</p>
<p>The interesting part is the <code>Variant</code> API calls. It allows one to create different variants of the bytecode. For example, in Lua, we have the <code>AddVV</code> bytecode to add two bytecode values, or the <code>AddVN</code> bytecode to add a bytecode value with a constant <code>double</code>, or the <code>ADDNV</code> bytecode to add a constant <code>double</code> with a bytecode value. In a traditional interpreter implementation, the implementation of all of these bytecodes must be written by hand, which is not only laborious, but also error prone. However, in Deegen’s framework, all you need to do is to specify them as <code>Variant</code>s, and we will do all the work for you!</p>
<p>The <code>IsConstant</code> API allows optionally further specifying the type of the constant, as shown in the <code>IsConstant&lt;tDoubleNotNaN&gt;()</code> usage in the snippet. Deegen implemented special LLVM optimization pass to simplify the execution semantics function based on the known and speculated type information of the operands. For example, for the bytecode variant where <code>rhs</code> is marked as <code>IsConstant&lt;tDoubleNotNaN&gt;()</code>, Deegen will realize that the <code>rhs.Is&lt;tDouble&gt;()</code> check in the bytecode function must be <code>true</code>, and optimize it out. This allows us to automatically generate efficient specialized bytecode implementation, without adding engineering cost to the user. (And by the way, the <code>tDouble</code> and <code>tDoubleNotNaN</code> things, or more formally, the type lattice of the language, is also user-defined. Deegen is designed to be a generic meta-compiler: it is not welded to Lua).</p>
<p>Finally, Deegen will generate a user-friendly <code>CreateAdd</code> function for the user frontend parser to emit a <code>Add</code> bytecode. For example, the frontend parser can write the following code to generate an <code>Add</code> bytecode that adds bytecode slot <code>1</code> with constant <code>123.4</code>, and stores the output into slot <code>2</code>:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span>bytecodeBuilder.CreateAdd({</span></pre></td></tr></tbody></table></figure>
<p>The implementation of <code>CreateAdd</code> will automatically insert constants into the constant table, select the most suitable variant based in the input types (or throwing out an error if no satisfying variant can be found), and append the bytecode into the bytecode stream. The concrete layout of the bytecode in the bytecode stream is fully hidden from the user. This provides a maximally user-friendly and robust API for the user parser logic to build the bytecode stream.</p>
<p><a href="https://github.com/luajit-remake/luajit-remake/blob/master/annotated/bytecodes/arithmetic_bytecodes.cpp" target="_blank" rel="noopener">This link</a> is the real implementation of all the Lua arithemtic bytecodes in LuaJIT Remake. It used a few features that we haven’t covered yet: the <code>DEEGEN_DEFINE_BYTECODE_TEMPLATE</code> macro allows defining a template of bytecodes, so <code>Add</code>, <code>Sub</code>, <code>Mul</code>, etc., can all be defined at once, minimizing engineering cost. The <code>EnableHotColdSplitting</code> API allows automatically hot-cold-splitting based on speculated and proven input operand types, and splits out the cold path into a dedicated function, which improves the final code quality (recall the earlier discussion on the importance of hot-cold code splitting?).</p>
<p>And below is the actual disassembly of the interpreter generated by Deegen for Lua’s <code>AddVV</code> bytecode. Comments are manually added by me for exposition purposes:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span>__deegen_interpreter_op_Add_0:</span></pre></td></tr></tbody></table></figure>
<p>As one can see, thanks to all of our optimizations, the quality of the assembly generated by Deegen has no problem rivalling hand-written assembly.</p>
<h3 id="Inline-Caching-API-The-Tricks-of-the-Trade">Inline Caching API: The Tricks of the Trade</h3>
<p>A lot of LJR’s speedup over LuaJIT interpreter comes from our support of inline caching. We have rewritten the Lua runtime from scratch. In LJR, table objects are not stored as a plain hash table with an array part. Instead, our table implementation employed hidden classes, using a design mostly mirroring the hidden class design <a href="https://webkit.org/blog/10308/speculation-in-javascriptcore/" target="_blank" rel="noopener">in JavaScriptCore</a>.</p>
<p>Hidden class allows efficient <em>inline caching</em>, a technique that drastically speeds up table operations. Briefly speaking, one can think of a hidden class as a hash-consed metadata object that describes the layout of a table object, or (simplified for the purpose of exposition), a hash map from string key to the storage slot in the table storing the value of this string key.</p>
<p>Let’s use the <code>TableGetById</code> bytecode (aka, <code>TGETS</code> in LuaJIT) as example. <code>TableGetById</code> takes a table <code>T</code> and a fixed constant string <code>k</code> as input, and outputs <code>T[k]</code>.</p>
<p>Due to the natural use case of dynamic languages, for a fixed <code>TableGetById</code> bytecode, the tables it operates on are likely to have the same hidden class, or only a few different kinds of hidden classes. So <code>TableGetById</code> will cache the most recent hidden class <code>H</code> it saw, as well as <code>H[k]</code>, the storage slot in the table for the constant string key <code>k</code>. When <code>TableGetById</code> is executed on input <code>T</code>, it first check if the hidden class of <code>T</code> is just its cached hidden class <code>H</code>. If so (which is likely), it knows that the result must be stored in slot <code>H[k]</code> of <code>T</code>, so the expensive hash-lookup work (which queries hidden class <code>H</code> to obtain <code>H[k]</code>) can be elided.</p>
<p>In general, one can characterize the inline caching optimization as the following: there are some generic computation <code>λ : input -&gt; output</code> that can be split into two steps:</p>
<ol>
<li>An expensive but idempotent step <code>λ_i : icKey -&gt; ic</code> where <code>icKey</code> is a subset of the <code>input</code> data, and <code>ic</code> is an opaque result.</li>
<li>A cheap but effectful step <code>λ_e : &lt;input, ic&gt; -&gt; output</code>, that takes the <code>input</code> and the idempotent result <code>ic</code> for <code>input</code> in step 1, and outputs the final output.</li>
</ol>
<p><img src="https://sillycross.github.io/images/2022-11-22/ic.png" alt="Computation eligible for inline caching can be characterized as above."/></p>
<p>If the computation satisfies such constraint, then one can cache <code>icKey</code> and the corresponding <code>ic</code>. Then on new inputs, if the <code>icKey</code> matches, the expensive idempotent step of computing <code>ic</code> can be safely elided.</p>
<p>Deegen provided <em>generic inline caching APIs</em> to allow easy employment of inline caching optimization. Specifically:</p>
<ol>
<li>The full computation <code>λ</code> is specified as a C++ lambda (called the <code>body</code> lambda).</li>
<li>The effectful step <code>λ_e</code> is specified as C++ lambdas defined inside the <code>body</code> lambda (called the <code>effect</code> lambdas).</li>
</ol>
<p>We allow specifying multiple possible <code>effect</code> lambdas in the <code>body</code> lambda, since the <code>λ_e</code> to execute can often be dependent on the outcome of the idempotent step. However, we require that at most one <code>effect</code> lambda can be executed in each run of the <code>body</code> lambda.</p>
<p>For example, for <code>TableGetById</code>, the code that employs inline caching would look like the following (simplified for the purpose of exposition):</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span><span>void</span> <span>TableGetById</span><span>(TValue tab, TValue key)</span> </span>{</span></pre></td></tr></tbody></table></figure>
<p>The precise semantic of the inline caching APIs is the following:</p>
<ol>
<li>When <code>ic-&gt;Body()</code> executes for the first time, it will honestly execute the <code>body</code> lambda. However, during the execution, when a <code>ic-&gt;Effect</code> API call is executed, it will create an inline cache for this bytecode that records the IC key (defined by the <code>ic-&gt;Key()</code> API), as well as all captures of this <code>effect</code> lambda that are <em>defined within</em> the <code>body</code> lambda. These variables are treated as constants (the <code>ic</code> state).</li>
<li>Next time the <code>ic-&gt;Body</code> executes, compare the cached key against the actual key.</li>
<li>If the key matches, it will directly execute the previously recorded <code>effect</code> lambda. For each capture of the <code>effect</code> lambda, if the capture is defined inside the <code>body</code> lambda, it will see the cached value recorded in step 1. Otherwise (i.e., the capture is defined as a capture of the <code>body</code> lambda), it will see the fresh value.</li>
<li>If the key does not match, just execute step 1.</li>
</ol>
<p>The precise semantic might look a bit bewildering at first glance. A more intuitive way to understand is that one is only allowed to do idempotent computation inside the <code>body</code> lambda (idempotent is with respect to the cached key and other values known to be constants to this bytecode). All the non-idempotent computations must go to the <code>effect</code> lambda. As long as this rule is followed, Deegen will automatically generate correct implementation that employs the inline caching optimization.</p>
<p>Deegen also performs exotic optimizations that fuses the ordinal of the <code>effect</code> lambda into the opcode, to save an expensive indirect branch that branches to the correct <code>effect</code> implementation when the inline cache hits. Such optimizations would have required a lot of engineering efforts in a hand-written interpreter. But in Deegen, it is enabled by merely one line: <code>ic-&gt;FuseICIntoInterpreterOpcode()</code>.</p>
<p>Below is the actual disassembly of the interpreter generated by Deegen, for <code>TableGetById</code> bytecode. The assembly is for a “fused-IC” quickened variant (see above) where the table is known to have no metatable, and the property exists in the inline storage of the table. As before, comments are manually added by me for exposition purposes.</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span>__deegen_interpreter_op_TableGetById_0_fused_ic_1: </span></pre></td></tr></tbody></table></figure>
<p>As one can see, in the good case of an IC hit, a <code>TableGetById</code> is executed with a mere 2 branches (one that checks the operand is a heap object, and one that checks the hidden class of the heap object matches the inline-cached value).</p>
<p>LuaJIT’s hand-written assembly interpreter is highly optimized already. Our interpreter generated by Deegen is also highly optimized, and in many cases, slightly better-optimized than LuaJIT. However, the gain from those low-level optimizations are simply not enough to beat LuaJIT by a significant margin, especially on a modern CPU with very good instruction-level parallelism, where having a few more instructions, a few longer instructions, or even a few more L1-hitting loads have negligible impact on performance. The support of inline caching is one of the most important high-level optimizations we employed that contributes to our performance advantage over LuaJIT.</p>
<h3 id="Conclusion-Thoughts-and-Future-Works">Conclusion Thoughts and Future Works</h3>
<p>In this post, we demonstrated how we built the fastest interpreter for Lua (to date) through a novel meta-compiler framework.</p>
<p>However, automatically generating the fastest Lua interpreter is only the beginning of our story. <a href="https://github.com/luajit-remake/luajit-remake" target="_blank" rel="noopener">LuaJIT Remake</a> is designed to be a multi-tier method-based JIT compiler generated by the Deegen framework, and we will generate the baseline JIT, the optimizing JIT, the tiering-up/OSR-exit logic, and even a fourth-tier heavyweight optimizing JIT in the future.</p>
<p>Finally, Deegen is never designed to be welded to Lua, and maybe in the very far future, we can employ Deegen to generate high-performance VMs at a low engineering cost for other languages as well.</p>
<hr/>




      
    </div></div>
  </body>
</html>
