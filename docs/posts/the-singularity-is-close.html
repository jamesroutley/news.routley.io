<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mkaic.substack.com/p/the-singularity-is-very-close">Original</a>
    <h1>The singularity is close?</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><div><figure><a target="_blank" rel="nofollow" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg"><picture><source type="image/webp" srcset="https://cdn.substack.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 424w, https://cdn.substack.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 848w, https://cdn.substack.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 1272w, https://cdn.substack.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 1456w" sizes="100vw"/><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg" width="1280" height="692" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:692,&#34;width&#34;:1280,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:325483,&#34;alt&#34;:&#34;A screenshot from the animated film The Mitchells Vs. The Machines that depicts two humanoid robots brandishing their glowing blaster arms menacingly.&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/jpeg&#34;,&#34;href&#34;:null}" alt="A screenshot from the animated film The Mitchells Vs. The Machines that depicts two humanoid robots brandishing their glowing blaster arms menacingly." title="A screenshot from the animated film The Mitchells Vs. The Machines that depicts two humanoid robots brandishing their glowing blaster arms menacingly." srcset="https://cdn.substack.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 424w, https://cdn.substack.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 848w, https://cdn.substack.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 1272w, https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F835ceebf-2826-4b62-9be4-f5f3e254866c_1280x692.jpeg 1456w" sizes="100vw"/></picture></a><figcaption>from <em>The Mitchells Vs. The Machines. </em>Go watch it if you haven’t already, it’s perfect in every way.</figcaption></figure></div><blockquote><p><strong>Then came the Butlerian Jihad— two generations of chaos. The god of machine-logic was overthrown among the masses and a new concept was raised: “Man may not be replaced.” </strong></p><p><strong>—Frank Herbert, </strong><em><strong>Dune</strong></em></p></blockquote><p>Within one century, biological intelligence will be a <strong>tiny minority of all sentient life</strong>. It will be very rare to be human. It will be very rare to have cells and blood and a heart. Human beings will be outnumbered a thousand to one by conscious machine intelligences.</p><p><strong>A</strong>rtificial <strong>G</strong>eneral <strong>I</strong>ntelligence (AGI)<a id="footnote-anchor-1" href="#footnote-1" rel="">1</a> is about to go from being science fiction to being part of everybody’s day-to-day life. It’s also going to happen <strong>in the blink of an eye</strong> — because once it gets loose, there is no stopping it from scaling itself incredibly rapidly. Whether we want it to or not, it will impact every human being’s life.</p><p>Some people believe the singularity won’t happen for a very long time, or at all. I’d like to discuss why I am nearly certain it will happen in the next 20 years. My overall prediction is based on 3 hypotheses:</p><ol><li><p>Scale is not the solution.</p></li><li><p>AI will design AGI.</p></li><li><p>The ball is already rolling.</p></li></ol><p>Keep in mind that this is just speculation and opinions. These predictions depict the future I personally feel is most likely.</p><p>Recently, an architecture called the Transformer has been taking over machine learning. It’s really good at sequence-to-sequence tasks like translation and <a href="https://en.wikipedia.org/wiki/GPT-3" rel="">text completion</a>, and it’s also been successfully applied to other fields like <a href="https://arxiv.org/abs/2010.11929" rel="">computer vision</a>.</p><p>Transformers<a id="footnote-anchor-2" href="#footnote-2" rel="">2</a> also demonstrate an intriguing ability to scale their performance with their size better than other architectures. They seem less prone to the performance ceilings found in their competition.</p><p>This has lead to a new slogan popping up in the AGI-speculation community: “scale is all you need.” Some people believe that bigger networks, bigger compute clusters, and bigger datasets are all we need to get to AGI. I disagree.</p><p>I believe we are more bottlenecked by the architecture designs than anything else. While modern, standard feedforward neural networks are getting very good at Doing Stuff™, they aren’t AGI and I don’t think there’s a clear path forward for them to become AGI. I have no doubt OpenAI’s next mega-model, GPT-4 (and beyond), will be excellent, but I also think it will have exploitable flaws that make it <strong>fail a thorough Turing test</strong>. </p><p>In fact, I see the massive size of the present-day’s GPT-<strong>3</strong> as a sign that scale <em>isn’t</em> the answer. 175 <em>billion</em> parameters, but still obviously not sentient? For comparison, the human brain has between <a href="https://aiimpacts.org/scale-of-the-human-brain/" rel="">20 and 100 billion neurons and up to 1 quadrillion synapses</a>. </p><p>You <em>could</em> argue that until our neural networks have hundreds of trillions of parameters, it’s not fair to compare them to the brain, but I think this argument relies too much on the assumption that a biological synapse and a weight in a network are equivalent in computational ability. This has not be proven. The intricacies of how the brain moves and processes signals are still not entirely understood<a id="footnote-anchor-3" href="#footnote-3" rel="">3</a>, but we know it seems to operate very differently from current neural networks.<a id="footnote-anchor-4" href="#footnote-4" rel="">4</a></p><p>Looking at most of the most revolutionary papers in the history of AI, they are dominated not by “we made it bigger” but by “we made it smarter at the same size”. I see no reason not to expect that this pattern will continue.</p><p>If scale isn’t the answer, what is? I believe that the <em>pièce de résistance</em> is <strong>adaptability</strong>. Presently, the way you make an ML model is fairly rigid: you decide on a fancy new way to differentiably mix matrix multiplications together, you feed it a ton of data, and you use some simple calculus-based optimizer to train the weights in your network<a id="footnote-anchor-5" href="#footnote-5" rel="">5</a>. The way that the weights in your network are arranged doesn’t change after training.</p><p>I don’t believe this is adaptible enough, even at scale. In order for true intelligence to emerge, models must be able to <strong>reorganize their own inner workings</strong>. I don’t think you can have the level of flexibility required for sentience with a frozen architecture.<a id="footnote-anchor-6" href="#footnote-6" rel="">6</a></p><p>I think sentient AI will be created by <strong>working smarter, not harder</strong>, with a focus on better architectural design and intelligent optimizers. This leads nicely into my next hypothesis:</p><div><figure><a target="_blank" rel="nofollow" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg"><picture><source type="image/webp" srcset="https://cdn.substack.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 424w, https://cdn.substack.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 848w, https://cdn.substack.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 1272w, https://cdn.substack.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 1456w" sizes="100vw"/><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg" width="536" height="379.09626719056973" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg&#34;,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:720,&#34;width&#34;:1018,&#34;resizeWidth&#34;:536,&#34;bytes&#34;:317801,&#34;alt&#34;:&#34;A photoshopped image in which Barack Obama appears to be awarding a clone of himself with a medal.&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/jpeg&#34;,&#34;href&#34;:null}" alt="A photoshopped image in which Barack Obama appears to be awarding a clone of himself with a medal." title="A photoshopped image in which Barack Obama appears to be awarding a clone of himself with a medal." srcset="https://cdn.substack.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 424w, https://cdn.substack.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 848w, https://cdn.substack.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 1272w, https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8a216dfa-26b4-44cf-8d35-78fd61927428_1018x720.jpeg 1456w" sizes="100vw"/></picture></a></figure></div><p>Human-designed networks have achieved great results, but they still suffer from the flaws of their creators. We are attracted to neatly organized network architectures which we can investigate and explain and attempt to understand.</p><p>But our brains, the <strong>gold standard</strong> of intelligence, are famously difficult to investigate, explain, or understand! I think this is because our brains weren’t “designed” by anyone — they evolved. They are the product of the universe’s greatest optimizer, natural selection.<a id="footnote-anchor-7" href="#footnote-7" rel="">7</a></p><p>I think it’s reasonable to assume that the architecture that brings about AGI will not be hand-designed by humans, or even selected via some brute-force hyperparameter search — it will be <strong>designed by another AI</strong>. I predict there will be several recursive layers of AI design — perhaps a dumb network which constructs a decent network which constructs a smart network which constructs AGI.</p><p>I am bullish on the prospect of what I call “constructor networks” — models that construct other models (also known as <a href="https://arxiv.org/abs/1609.09106" rel="">hypernetworks</a>). I think the moment we crack hyperlearning will be the moment progress will start moving faster than we can keep up, precisely because <em>we</em> will no longer be the ones making the progress — the <em>algorithms</em> <em>themselves</em> will.</p><p>In order to work smarter, not harder, we need to let go of our human biases and focus on making unconstrained architectures that can aggressively optimize every aspect of themselves. I fully expect these architectures will be frustratingly difficult to explain when they arrive — like huge mounds of <strong>digital neural spaghetti</strong> — but they will also outperform all competition. Every additional stable layer of AI abstraction we add between ourselves and the final model will make the final model harder to understand and better at its task.</p><p>The ideal model will be able to not only be constantly online-learning, but also constantly adding and removing its own parameters, allowing evolution and adaptation to new tasks.</p><p>You cannot have artificial <strong>general</strong> intelligence if your model cannot adapt in real time to an arbitrary task.</p><p>I believe that there is <strong>too much momentum</strong> to stop AGI now. With this much distributed attention fixed on the problem, AGI will be solved. Additionally, once it is solved it will be released to the public — whether it’s ethical to do so or not. I imagine that the first people to solve it will probably keep it behind closed doors, but it won’t stay secret forever. Someone on the team will leak everything, or someone else will independently make the same discoveries and release them. Eventually it <em>will</em> get out.</p><p>Consider the invention of the nuclear bomb — once we learned of the power hidden in radioactive materials, it was only a matter of time before someone pushed the research to its moral limits. AGI is like that, except it’s <strong>even more terrifying</strong> because uranium, plutonium, and the bombs made out of them can be strictly controlled, but people with powerful computers and an internet connection cannot, nor can the AGIs they create.</p><p>I recognize how cliché and alarmist this all sounds. <em>Really, you’re genuinely worried about a robot apocalypse? You know Age of Ultron is just a stupid Marvel movie, right?</em> Yeah, I know. But I’ve grown to believe that the concerns that fiction writers have been bringing up for decades are actually quite reasonable — because AGI cannot be stopped.</p><p>Once an intelligence is loose on the internet, it will be able to learn from all of humanity’s data, replicate and mutate itself infinitely many times, take over physical manufacturing lines remotely, and hack important infrastructure. Obviously, it’s impossible to say for sure that this is what the <em>first</em> free AGI will do, but it’s inevitable that some malevolent AGI <em>will</em> exist and <em>will</em> do these things. We can only hope that we’ll have sufficiently powerful <strong>benevolent</strong> AGI to fight back.</p><p>I subtitled this post “Why we&#39;re all in denial about the robot apocalypse”. I say that because I believe that society at large is completely, utterly, and <em>woefully</em> unprepared for the advent of sentient, <em>living</em> artificial general intelligence. I think the singularity is coming much sooner than most people expect, and I think it’s going to cause a great deal of upset when it arrives — for better <em>and</em> for worse.</p><p>Take for instance the common religious belief that people possess some unmeasurable, undefinable <em>soul</em>, and that this soul is what separates us from inanimate objects and non-sentient animals. Furthermore, some people believe that these souls come from deity. I have spoken with friends who believe that AGI is impossible because “robots can’t have souls, humans aren’t God”. For these people, like Caleb says in <em>Ex Machina </em>(paraphrasing)<em>,</em> removing the line between man and machine also <strong>removes the line between god and man.</strong></p><p>Now, this isn’t to say that AGI will destroy religion or anything — it may even be used to strengthen some sects (as taken to the extreme in HBO’s Raised By Wolves). No, religion has been around for millennia and I’m sure it will continue to be around for many more millennia. I’m simply predicting that a subset of religious people are going to experience <strong>lots of cognitive dissonance</strong> when the first AGI arrives. </p><p>More generally, arguments about AGI sentience and ethical issues will go from being topics only geeks talk about to topics that Facebook moms make political grandstands over. </p><p>Finally, I want to address those who may feel this post is pessimistic: I assure you, I am <em>hopeful</em> about AGI. I work in the field of ML <em>because</em> I am hopeful. I hope to personally contribute to the development of AGI in my lifetime. I think AGI has the capacity to make the world an infinitely better place. We are not prepared for AGI, but that doesn’t mean AGI has to be the end of humanity. </p><p>I don’t know what life will look like in the age of living machines, but I am confident that, as Jeff Goldblum puts it:  </p><blockquote><p><strong>Life, uh, finds a way.</strong></p><p><strong>—Ian Malcolm, </strong><em><strong>Jurassic Park</strong></em></p></blockquote><p>Thanks for reading,</p><p>Kai</p><p>PS — I’m making a <strong>series of short films</strong> about AGI right now! You should totally go watch the first episode, <strong>which is out now</strong> on my <a href="https://www.youtube.com/c/KaiChristensen" rel="">YouTube channel</a> and my <a href="https://tiktok.com/@mkaic" rel="">TikTok account</a>. </p><p>Also, while you’re at it, why not follow me on <a href="https://twitter.com/mkaic_" rel="">Twitter</a>?</p></div></div></div></article></div></div></div>
  </body>
</html>
