<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://seed.bytedance.com/en/seedance">Original</a>
    <h1>Seedance 1.0</h1>
    
    <div id="readability-page-1" class="page"><section><header><p><time datetime="2025-06-07">June 7th, 2025</time> • ~800 words • 4 minute read</p></header><p><em>Note</em>: I accidentally published this before it was ready! I decided to tweak the typos, remove a couple terrible fragments and otherwise leave it as-is.</p><p>Ideas have always been cheap; it&#39;s the <a href="https://sive.rs/multiply">execution that&#39;s the multiplier</a>. In the AI world we&#39;re hurtling toward, which is driving down the cost of execution, I guess that means ideas are more devalued than ever?</p><p>Cheaper still than an idea is an <em>observation</em>. It&#39;s the Xerox of an idea run through a filter—artifacts and fidelity sometimes lost, but perhaps a bit of unique character introduced.</p><p>Everyone seems to have their own AI thoughts these days. Fresh off 4.5 years leading engineering efforts around a SaaS product through the birth of this LLM era we&#39;re all living in now, I thought I&#39;d offer a scattershot collection of my own half-baked ones:</p><h2>Abstractions on Abstractions</h2><p>Computing is the history of abstractions. We&#39;ve gone from punching holes in cards to perform calculations to talking to pocket devices in our mother tongue, asking what the weather will be—all in less than a century.</p><p>The advent of computer languages to create instructions led to software as we know it. From peeking and poking at individual memory addresses to writing scripts that imitate language to (now, with LLMs) <em>literally</em> using language as we&#39;re hard-wired to learn it from birth—conversing with machines.</p><p>What happens when the next &#34;hello world&#34; program is just: &#34;ask the computer to write a hello world program&#34;? Sure, behind the scenes it may literally write an entire operating system from scratch in some virtualized container, all in service of simply spitting out the words <em>Hello World</em>—but it doesn&#39;t matter.</p><p>I think there&#39;s an inevitable discomfort living through this kind of progress. You see what&#39;s lost. You see the depths of understanding falling short of your own. But there&#39;s progress on the other side of that that will outweigh anything lost. Feelings that end up bordering on nostalgia cannot slow this train.</p><p>And that&#39;s okay.</p><h2>Probabilistic Elevators</h2><p>At some point the interface to everything becomes a machine you talk to. In the same way microcontrollers have found their way into all manner of things. They&#39;re <a href="https://en.wikipedia.org/wiki/Fast,_Cheap_%26_Out_of_Control">fast, cheap, and out of control</a>. Wait until we have highly capable LLMs on chips. In that world, why create a unique electronic component for a button when you can manufacture a magical component that you kindly <em>ask</em> to behave like a button? Shrunken to microscopic sizes, how close does this start to resemble magical nanotechnologies capable of anything?</p><p>Imagine a grid of elevator buttons, floors 1–10. Each programmed with a simple prompt: &#34;Send the elevator to floor X.&#34;</p><p>Now, if you&#39;re thinking about the probabilistic nature of elevators, you might be thinking: &#34;That is insane. I would like a deterministic elevator, please.&#34;</p><p>But the truth is: if it&#39;s cheaper and easier, it will win.</p><p>And the other truth is: we&#39;re already living in a probabilistic world. This future is simply embracing it.</p><p>Also, <em>Probabilistic Elevators</em> would be a great band name. Please steal it.</p><h2>AGI</h2><p>The psychology of AGI, SGI, and trusting our robot overlords fascinates me. Science fiction has always been aspirational, but you have to remember that it&#39;s ultimately <em>fiction</em>. Real life always lands differently.</p><p>You achieve AGI when you&#39;re willing to trust the machine blindly. I think that&#39;s a product of comfort over time more than capability. At the end of the day, it&#39;s a marketing term for a highly capable version of this tool that hasn&#39;t quite landed yet. But I can squint and see it. We&#39;re talking about tools, not machines with will. We always have been.</p><p>It says more about us when we decide to feel comfortable making that leap than about the capabilities of the thing. I have a personal feeling that AGI is also only perceivable to the extent you&#39;ve given the brain-in-a-jar tools to act on its scattered thoughts. I can&#39;t really conceive or understand an intelligence in the abstract, independent of tools that help me recognize that level of cognizance—writing, speaking, actions.</p><p>There’s a whole era of study dedicated to pretty much the opposite of my intuition that goes back to the 1950s called <a href="https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence">symbolic AI</a>. I trust those people are much smarter than me—that my intuition is missing the point and that there’s something there—but I do suspect my ability to relate to this kind of intelligence is diminished absent relatable tools. A brain-in-a-jar absent the tool of language to talk to me is just a brain-in-a-jar, no matter what&#39;s going on in there. It feels more like philosophy than engineering something in the real world. At least today.</p><p>It’s like anthropomorphizing Excel or a bash script, to some extent. Or maybe asking Deep Blue or Watson to operate a forklift.</p><p>Anyway, debating AGI strikes me as a distraction. A fun distraction, but not a particularly useful one beyond feeding the marketing hype. Discussing the inevitable capabilities of these tools—and the agency we might feel comfortable granting them—is not.</p><p>AGI happens when you&#39;re ready.</p>--<p><em>Published on Saturday, June 7th 2025. Read this post in <a href="https://george.mand.is/2025/06/agi-elevators-and-other-thoughts-i-didnt-mean-to-publish/">Markdown</a> or <a href="https://george.mand.is/2025/06/agi-elevators-and-other-thoughts-i-didnt-mean-to-publish/">plain-text</a>.</em></p></section></div>
  </body>
</html>
