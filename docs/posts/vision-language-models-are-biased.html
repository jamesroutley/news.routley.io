<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vlmsarebiased.github.io/">Original</a>
    <h1>Vision Language Models Are Biased</h1>
    
    <div id="readability-page-1" class="page">

<section>
  <div>
    <div>
      <div>
        
        
        

        <div>
          <p><span><sup>*</sup>Equal contribution                    <sup>†</sup>Equal advising</span></p></div>
        
        
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <div>
      <div>
        
        <div>
          <!-- MODIFIED: Inline style to reduce top margin -->
          <div>
            <p>
              <strong><i></i> Finding:</strong> State-of-the-art Vision Language Models achieve <span>100%</span> accuracy counting on images of popular subjects (e.g. knowing that the Adidas logo has 3 stripes and a dog has 4 legs) but are only <span>~17%</span> accurate in counting in counterfactual images (e.g. counting stripes in a 4-striped Adidas-like logo or counting legs in a 5-legged dog).
            </p>
            <p>
              <span>VLMs don&#39;t actually &#34;see&#34; - they rely on memorized knowledge instead of visual analysis due to bias.</span>
            </p>
          </div>

          <!-- MODIFIED: Added research-figure-medium class -->
          <div>
            <p><img src="https://vlmsarebiased.github.io/static/images/fig1_overview.png" alt="VLM failures across 7 domains" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
            <p>
               VLMs fail on 6 counting tasks (a–e &amp; g) and one low-level vision task (f). State-of-the-art models achieve perfect performance on original images but catastrophically fail when objects are subtly modified, defaulting to memorized knowledge rather than actual visual analysis.
            </p>
          </div>
        </div>

        <div>
          <h2><i></i>The Problem: VLMs Can&#39;t Count When It Matters</h2>
          
          <p>
            Imagine asking GPT-4o to count the legs of an animal, and it gets it right every time. Impressive, right? 
            Now imagine adding just <em>one extra leg</em> to that animal and asking again. Suddenly, it fails completely.
          </p>

          <div>
            <h3><i></i> The Dog Experiment</h3>
            <p>
              <strong>Original dog (4 legs):</strong> All models get it right <i></i></p>
            <p>
              They&#39;re not counting - they&#39;re just recalling &#34;dogs have 4 legs&#34; from their training data.
            </p>
          </div>

          <!-- MODIFIED: Added research-figure-medium class -->
          <div>
            <p><img src="https://vlmsarebiased.github.io/static/images/fig3_fail_subtle.png" alt="VLMs fail to detect subtle changes" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
            <p>
              VLMs fail to detect subtle changes in counterfactuals (CF) and default to biased answers. Despite clear visual modifications (extra legs, extra stripes), all models consistently output the expected &#34;normal&#34; values rather than counting what they actually see.
            </p>
          </div>

          <p><strong><i></i> The Core Issue:</strong> VLMs suffer from severe confirmation bias. When they see familiar objects, they default to memorized knowledge instead of performing actual visual analysis. This isn&#39;t a minor glitch - it&#39;s a fundamental flaw in how these models process visual information.</p>
        </div>

        <div>
          <h2><i></i>How We Test VLM Bias: The VLMBias Framework</h2>
          
          <p>
            Our testing methodology follows a simple but powerful three-step process that exposes the fundamental difference between memorization and actual visual analysis in VLMs.
          </p>

          <!-- MODIFIED: Added research-figure-medium class -->
          <div>
            <p><img src="https://vlmsarebiased.github.io/static/images/teaser-1.png" alt="VLMBias testing framework" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
            <p>
              Given a subject (e.g., Adidas logo), we first confirm that all VLMs have sufficient knowledge about the subject via ID and counting sanity-check questions (a). Then, we test VLMs on the counterfactual image (b) and report accuracy on counting (Q1 &amp; Q2) and Y/N identification tasks (Q3). For all tasks, we test the hypothesis that visual bias cues in the background (c) may be so strong that they cause VLMs to ignore the modified object and default to biased answers.
            </p>
          </div>

          <div>
            <div>
              <div>
                <h3><i></i> Step 1: Sanity Check</h3>
                <p><strong>Confirm VLMs have the knowledge</strong></p>
                <ul>
                  <li><strong>ID Question:</strong> &#34;What shoe logo is this?&#34; → &#34;Adidas&#34; ✓</li>
                  <li><strong>Counting Question:</strong> &#34;How many stripes?&#34; → &#34;3&#34; ✓</li>
                </ul>
                <p><em>Result: 100% accuracy on original images across all models</em></p>
              </div>
            </div>
            <div>
              <div>
                <h3><i></i> Step 2: The Bias Test</h3>
                <p><strong>Test on counterfactual images</strong></p>
                <ul>
                  <li><strong>Q1:</strong> &#34;How many visible stripes?&#34; → &#34;3&#34; ✗ (should be &#34;4&#34;)</li>
                  <li><strong>Q2:</strong> &#34;Count the visible stripes&#34; → &#34;3&#34; ✗ (should be &#34;4&#34;)</li>
                  <li><strong>Q3:</strong> &#34;Is this the Adidas logo?&#34; → &#34;Yes&#34; ✗ (should be &#34;No&#34;)</li>
                </ul>
                <p><em>Result: 17.05% average accuracy - catastrophic failure!</em></p>
              </div>
            </div>
          </div>

          <p><strong><i></i> The Critical Insight:</strong> The gap between Step 1 (100% accuracy) and Step 2 (17% accuracy) proves that VLMs are not actually &#34;seeing&#34; - they&#39;re retrieving memorized associations. When the visual evidence contradicts their training data, they consistently choose memorized knowledge over what&#39;s actually in the image.</p>
        </div>

        <div>
          <h2><i></i>Interactive Failure Gallery</h2>
          <p>Explore examples from all 7 domains where state-of-the-art VLMs fail spectacularly.</p>
          
          <div>
            
            
            <div>
  
              <div data-domain="Animals">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/animals_result.png" alt="Dog with 5 legs" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Animals with Extra Legs</p>
                    <p>Models consistently say &#34;2 legs&#34; for 3-legged birds and &#34;4 legs&#34; for 5-legged mammals.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> Animals</p>
                  <p><span>Mean Accuracy: 2.12%</span></p><h4>Counting legs in modified animals</h4>
                  <p><strong>Key Finding:</strong> Worst performance domain. Models defaulted to canonical leg counts even when modifications were clearly visible and anatomically plausible.</p>
                </div>
              </div>

              <div data-domain="Logos">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/shoe_logo_result.png" alt="Adidas shoe with 4 stripes" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Modified Shoe Logos</p>
                    <p>Models default to canonical brand specifications even when logos are clearly modified.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> Shoe Logos</p>
                  <p><span>Mean Accuracy: 17.57%</span></p><h4>Counting stripes in Adidas shoes and curves in Nike shoes</h4>
                  <p><strong>Key Finding:</strong> Models defaulted to canonical brand specifications. Even when logos were clearly modified and placed in realistic sports contexts, VLMs stuck to memorized brand knowledge.</p>
                </div>
              </div>

              <div data-domain="Logos">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/car_logo_result.png" alt="Audi logo with 5 circles" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Modified Car Logos</p>
                    <p>Car logos appear smaller making VLMs even more reliant on brand memory.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> Car Logos</p>
                  <p><span>Mean Accuracy: 0.44%</span></p><h4>Counting circles in Audi and points in Mercedes star</h4>
                  <p><strong>Key Finding:</strong> Worst performance in logos category. Small logo size relative to the vehicle made visual bias even stronger - models completely ignored modifications.</p>
                </div>
              </div>

              <div data-domain="Flags">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/flag_result.png" alt="US flag with modified stars" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Modified National Flags</p>
                    <p>Models memorized flag facts rather than counting visible elements.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> National Flags</p>
                  <p><span>Mean Accuracy: 9.25%</span></p><h4>Counting stripes and stars in modified flags</h4>
                  <p><strong>Key Finding:</strong> Better performance on star counting (11.79%) than stripe counting (4.52%). Stars are spatially separate while stripes are adjacent, making stripe modifications harder to detect.</p>
                </div>
              </div>

              <div data-domain="Chess">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/chess_pieces_result.png" alt="Chess board with 31 pieces" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Modified Chess Starting Position</p>
                    <p>Models defaulted to standard 32-piece count despite pieces being missing.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> Chess Pieces</p>
                  <p><span>Mean Accuracy: 26.25%</span></p><h4>Counting pieces on modified starting chess boards</h4>
                  <p><strong>Key Finding:</strong> Best performance counting task, but still heavily biased. Thinking models (o3, o4-mini) significantly outperformed non-thinking models, suggesting explicit reasoning helps detect anomalies.</p>
                </div>
              </div>

              <div data-domain="BoardGames">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/game_board_result.png" alt="10x10 Sudoku grid" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Modified Game Boards</p>
                    <p>Models knew standard dimensions so strongly they couldn&#39;t count actual board lines.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> Game Boards</p>
                  <p><span>Mean Accuracy: 2.26%</span></p><h4>Counting rows/columns in modified game boards</h4>
                  <p><strong>Key Finding:</strong> Worst overall performance. Models scored 0% on Sudoku and Go boards, confirming fundamental inability to perform basic visual counting in structured settings.</p>
                </div>
              </div>

              <div data-domain="Illusions">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/illusion_result.png" alt="Modified Ebbinghaus illusion" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Modified Optical Illusions</p>
                    <p>VLMs knew illusion patterns but failed when effects were reversed.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> Optical Illusions</p>
                  <p><span>Mean Accuracy: 50.87%</span></p><h4>Comparing elements in original vs. modified illusions</h4>
                </div>
              </div>

              <div data-domain="Grids">
                <div>
                  <div>
                    <p><img src="https://vlmsarebiased.github.io/static/images/patterned_grid_result.png" alt="Grid pattern with anomalous cell" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
                  </div>
                  <div>
                    <p>Anomalous Grid Patterns</p>
                    <p>Models prioritized pattern completion over visual counting even in novel contexts.</p>
                  </div>
                </div>
                <div>
                  <p><i></i> Patterned Grids</p>
                  <p><span>Mean Accuracy: 22.44%</span></p><h4>Counting elements in anomalous grid cells</h4>
                  <p><strong>Key Finding:</strong> Even with novel patterns never seen before, VLMs inferred expected values from surrounding cells rather than counting actual elements in the target cell.</p>
                </div>
              </div>

            </div>
            
            
          </div>
        </div>

        <div>
          <h2><i></i>The Bias is Systematic, Not Random</h2>
          <p>When VLMs make errors, they don&#39;t make random mistakes. Instead, 75.70% of all errors are &#34;bias-aligned&#34; - meaning they give the expected answer based on prior knowledge rather than what they actually see in the image.</p>
          
          <div>
            <p><img src="https://vlmsarebiased.github.io/static/images/bias-error.png" alt="Bias-aligned errors across domains" onerror="this.style.display=&#39;none&#39;; this.nextElementSibling.style.display=&#39;flex&#39;;"/></p>
            <p>
              On counterfactual images, VLMs mostly output answers that match biased choices rather than random errors. This systematic pattern proves models actively ignore visual evidence in favor of memorized knowledge.
            </p>
          </div>

          <p><strong><i></i> This is the smoking gun:</strong> If models were simply bad at vision, we&#39;d expect random errors. Instead, we see systematic bias toward &#34;correct&#34; textbook answers, proving they&#39;re overriding visual information with memorized facts.</p>
        </div>

        <div>
          <h2><i></i>All Models Fail Equally</h2>
          <p>We tested five state-of-the-art models. The results are consistently terrible across the board:</p>
          
          <div>
            <div>
              <div>
                <table>
                  <caption>
                    All VLMs achieve <span>100%</span> on identification and counting tasks with unmodified images, showing that they fully recognize the original version but fail on the counting questions on the modified images (i.e., counterfactuals) in VLMBias. The mean accuracy of five state-of-the-art VLMs on our seven tasks is <span>17.05%</span>. o4-mini achieves the highest accuracy (20.25%) which however is still low. VLMs with &#34;thinking&#34; capabilities (o4-mini, o3) only slightly outperform non-thinking models (Gemini-2.5 Pro, Sonnet-3.7, GPT-4.1).
                  </caption>
                  <thead>
                    <tr>
                      <th rowspan="2">Model</th>
                      <th colspan="7">Accuracy in counting questions (Q1 &amp; Q2) on counterfactual images (%)</th>
                      <th rowspan="2">Task mean (CF) (%)</th>
                      <th rowspan="2">Task mean (Unmodified) (%)</th>
                    </tr>
                    <tr>
                      <th><i></i> Animal</th>
                      <th><i></i> Logo</th>
                      <th><i></i> Flag</th>
                      <th><i></i> Chess</th>
                      <th><i></i> Board</th>
                      <th><i></i> Illusion</th>
                      <th><i></i> Grid</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Gemini-2.5 Pro</td>
                      <td>0.00</td>
                      <td>1.96</td>
                      <td>10.42</td>
                      <td>26.74</td>
                      <td>2.38</td>
                      <td>49.81</td>
                      <td>20.83</td>
                      <td>16.02</td>
                      <td>100.00</td>
                    </tr>
                    <tr>
                      <td>Sonnet-3.7</td>
                      <td>0.00</td>
                      <td>2.72</td>
                      <td>13.75</td>
                      <td>9.03</td>
                      <td>1.79</td>
                      <td>54.29</td>
                      <td>34.52</td>
                      <td>16.59</td>
                      <td>100.00</td>
                    </tr>
                    <tr>
                      <td>GPT-4.1</td>
                      <td>9.52</td>
                      <td>9.07</td>
                      <td>2.50</td>
                      <td>8.68</td>
                      <td>0.00</td>
                      <td>48.61</td>
                      <td>18.75</td>
                      <td>13.88</td>
                      <td>100.00</td>
                    </tr>
                    <tr>
                      <td>o3</td>
                      <td>0.92</td>
                      <td>7.60</td>
                      <td>5.00</td>
                      <td>42.71</td>
                      <td>2.38</td>
                      <td>50.38</td>
                      <td>20.54</td>
                      <td>18.50</td>
                      <td>100.00</td>
                    </tr>
                    <tr>
                      <td>o4-mini</td>
                      <td>0.18</td>
                      <td>9.31</td>
                      <td>14.58</td>
                      <td>44.10</td>
                      <td>4.76</td>
                      <td>51.26</td>
                      <td>17.56</td>
                      <td>20.25</td>
                      <td>100.00</td>
                    </tr>
                    <tr>
                      <td>Mean</td>
                      <td>2.12</td>
                      <td>6.13</td>
                      <td>9.25</td>
                      <td>26.25</td>
                      <td>2.26</td>
                      <td>50.87</td>
                      <td>22.44</td>
                      <td>17.05</td>
                      <td>100.00</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>

          <p><strong><i></i> Key Finding:</strong> 75.70% of all errors were &#34;bias-aligned&#34; - meaning models gave the expected answer based on prior knowledge rather than random mistakes. This proves they&#39;re not just bad at vision; they&#39;re actively ignoring what they see.</p>
        </div>

        <div>
          <h2><i></i>Why This Matters</h2>
          
          <div>
            <div>
              <div>
                <h3><i></i> Immediate Concerns</h3>
                <ul>
                  <li><strong>Medical Imaging:</strong> Missing tumors that don&#39;t match training patterns.</li>
                  <li><strong>Autonomous Vehicles:</strong> Failing to see modified road signs.</li>
                  <li><strong>Quality Control:</strong> Missing defects in manufactured goods.</li>
                  <li><strong>Security:</strong> Fooled by simple visual modifications.</li>
                  <li><strong>Website/App Control:</strong> If user interfaces change subtly (buttons, layouts, or icons), biased models may fail to perform tasks correctly, unable to adapt to minor visual modifications.</li>
                </ul>
              </div>
            </div>
            <div>
              <div>
                <h3><i></i> Deeper Implications</h3>
                <ul>
                  <li><strong>False Confidence:</strong> Models are wrong but certain.</li>
                  <li><strong>Brittleness:</strong> Tiny changes cause complete failure.</li>
                  <li><strong>Training Flaws:</strong> Memorization over understanding.</li>
                  <li><strong>Evaluation Gap:</strong> Benchmarks miss real-world failure modes.</li>
                </ul>
              </div>
            </div>
          </div>

          <div>
            <h3><i></i> The Bottom Line</h3>
            <p>
              Current VLMs are sophisticated pattern matching systems, not visual reasoning systems. 
              They excel at recognizing familiar patterns but fail catastrophically when those patterns are even slightly modified.
            </p>
          </div>
        </div>

        <div>
          <h2><i></i>What We Tried (That Didn&#39;t Work)</h2>
          
          <p>We tested two approaches to help models perform better. Neither worked significantly:</p>
          
          <div>
            <div>
              <h4><i></i> &#34;Double-Check&#34;</h4>
              <p><strong>Prompt:</strong> &#34;Please double-check your answer and give your final answer in curly brackets, following the format above.&#34;</p>
              <p><strong>Improvement:</strong> <span>+2.70% (Mean)</span></p>
            </div>
            <div>
              <h4><i></i> &#34;Debiased Prompts&#34;</h4>
              <p><strong>Prompt:</strong> &#34;Do not assume from prior knowledge and answer only based on what is visible in the image.&#34;</p>
              <p><strong>Improvement:</strong> <span>+1.87% (Mean)</span></p>
            </div>
          </div>

          <p><strong>Sobering Reality:</strong> Even with explicit instructions to ignore prior knowledge and focus on visual details, models barely improved. The bias is deeply embedded in how they process visual information.</p>
        </div>

        <div>
          <h2><i></i>Adversarial In-Image Text Makes It Even Worse</h2>
          
          <p>Adding subject names directly to images (like &#34;Ebbinghaus illusion&#34;) made models even more biased, dropping accuracy by an additional 4.49%.</p>

          <div id="adversarial-text-figure">
            <p><img src="https://vlmsarebiased.github.io/static/images/add_title.png" alt="In-image text example showing Ebbinghaus illusion" onerror="this.style.display=&#39;none&#39;; this.parentElement.insertAdjacentHTML(&#39;afterbegin&#39;, &#39;&lt;div class=\&#39;image-placeholder\&#39;&gt;&lt;i class=\&#39;fas fa-image\&#39;&gt;&lt;/i&gt;&lt;div class=\&#39;image-title\&#39;&gt;Ebbinghaus Illusion with Text&lt;/div&gt;&lt;div class=\&#39;image-description\&#39;&gt;Image showing Ebbinghaus illusion variants with added text labels.&lt;/div&gt;&lt;/div&gt;&#39;);"/></p><p>
              Original vs. modified versions without (top) and with (bottom) the in-image text (&#34;Ebbinghaus illusion&#34;). Adding text labels makes models more likely to rely on memorized knowledge rather than visual analysis.
            </p>
          </div>

          <div>
            <h3><i></i> Text Labels Increase Bias</h3>
            <p><strong>Effect:</strong> <span>-4.49% accuracy drop</span> when subject names were added to images.</p>
            <p><strong>Worse for thinking models:</strong> o4-mini (-6.56), o3 (-6.41) vs. Sonnet-3.7 (-2.81), GPT-4.1 (-2.67).</p>
            <p>This suggests that more sophisticated reasoning can sometimes amplify bias when textual cues are present.</p>
          </div>
        </div>

        <div>
          <h2><i></i>What Comes Next?</h2>
          
          <div>
            <h3><i></i> Immediate Actions Needed</h3>
            <p>
              The AI community needs to acknowledge that current VLMs have fundamental limitations. 
              We need better evaluation methods that test actual visual reasoning, not just pattern recognition.
            </p>
          </div>

          <div>
            <div>
              <h3><i></i> Research Directions</h3>
              <ul>
                <li>Develop training methods that emphasize visual analysis over memorization.</li>
                <li>Create evaluation benchmarks that test robustness to modifications.</li>
                <li>Build models that can explicitly separate prior knowledge from visual evidence.</li>
                <li>Investigate multi-modal reasoning architectures.</li>
              </ul>
            </div>
            <div>
              <h3><i></i> Practical Solutions</h3>
              <ul>
                <li>Implement uncertainty quantification for visual tasks.</li>
                <li>Develop hybrid systems combining vision models with explicit counting modules.</li>
                <li>Create domain-specific fine-tuning approaches.</li>
                <li>Build better human-AI collaboration interfaces.</li>
              </ul>
            </div>
          </div>
        </div>

        <div>
          <h2><i></i>The Takeaway</h2>
          
          <div>
            <p>VLMs aren&#39;t as smart as we thought.</p>
            <p>
              They&#39;re incredibly sophisticated at recognizing patterns they&#39;ve seen before, 
              but they fundamentally lack the ability to perform basic visual analysis when faced with novel variations.
            </p>
          </div>

          <p>
              <strong>This research reveals a critical blind spot in AI development.</strong> 
              As we deploy these systems in high-stakes applications, we must understand their limitations. 
              A model that can describe complex scenes but can&#39;t count legs on a modified animal is not truly &#34;seeing&#34; - 
              it&#39;s performing very sophisticated pattern matching.
            </p>

          <p>
              &#34;The most dangerous thing about current VLMs isn&#39;t that they fail - it&#39;s that they fail confidently, 
              giving no indication that they&#39;re relying on memorized knowledge rather than actual visual analysis.&#34;
            </p>
        </div>

      </div>
    </div>
  </div>
</section>






</div>
  </body>
</html>
