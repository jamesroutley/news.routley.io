<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://brev.dev/blog/fine-tuning-llama-2">Original</a>
    <h1>A simple guide to fine-tuning Llama 2</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>In this guide, I show how you can fine-tune Llama 2 to be a dialog summarizer!</strong></p><p>Last weekend, I wanted to finetune Llama 2 (which now reigns supreme in the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM leaderboard</a>) on a dataset of my own collection of Google Keep notes; each one of my notes has both a title and a body so I wanted to train Llama to generate a body from a given title.</p><p>This first part of the tutorial covers finetuning Llama 2 on the <a href="https://huggingface.co/datasets/samsum">samsum dialog summarization dataset</a> using Huggingface libraries. I tend to find that while Huggingface has built a superb library in transformers, their guides tend to overcomplicate things for the average joe. The second part, fine-tuning on custom data, is coming at the end of the week!</p><p>To get started, get yourself either an A10, A10G, A100 (or any GPU with &gt;24GB GPU memory). If you&#39;re not sure where to start, the <a href="https://console.brev.dev/environment/new?instance=gpu_1x_a10">Brev Cloud</a> makes it easy to access each of these GPUs!</p><h2 id="1-download-the-model">1. Download the model</h2><p>Clone Meta&#39;s Llama inference repo (which contains the download script):</p><pre><code><span>git</span><span> clone https://github.com/facebookresearch/llama.git</span>
</code></pre><p>Then run the download script:</p><pre><code><span>bash</span><span> download.sh</span>
</code></pre><p>It&#39;ll prompt you to enter the URL you got sent by Meta in an email. If you haven&#39;t signed up, do it <a href="https://ai.meta.com/llama/">here</a>. They are surprisingly quick at sending you the email!</p><p>For this guide, you only need to download the 7B model.</p><h2 id="2-convert-model-to-hugging-face-format">2. Convert model to Hugging Face format</h2><pre><code><span>wget</span><span> https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py</span>
<span>python convert_llama_weights_to_hf.py </span><span>\</span><span></span>
<span>    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir models_hf/7B</span>
</code></pre><p>This now gives us a Hugging Face model that we can fine-tune leveraging Huggingface libraries!</p><h2 id="3-run-the-fine-tuning-notebook">3. Run the fine-tuning notebook:</h2><p>Clone the Llama-recipies repo:</p><pre><code><span>git</span><span> clone https://github.com/facebookresearch/llama-recipes.git</span>
</code></pre><p>Then open the <strong>quickstart.ipynb</strong> file in your preferred notebook interface:</p><p>(I use Jupyter lab like so):</p><pre><code><span>pip </span><span>install</span><span> jupyterlab</span>
<span>jupyter lab </span>
</code></pre><p>Then just run the whole notebook.</p><p>Make sure you change the line:</p><pre><code><span>model_id</span><span>=</span><span>&#34;./models_hf/7B&#34;</span>
</code></pre><p>to your actual model path that you converted. And that&#39;s that! You will end up with a Lora fine-tuned.</p><h2 id="4-run-inference-on-your-fine-tuned-model">4. Run inference on your fine-tuned model</h2><p>The issue here is that Huggingface only saves the adapter weights and not the full model. So we need to load the adapter weights into the full model. I struggled for a bit finding the right documentation to do this...But eventually worked it out!</p><p>Import libraries:</p><pre><code><span>import</span><span> torch</span>
<span></span><span>from</span><span> transformers </span><span>import</span><span> LlamaForCausalLM</span><span>,</span><span> LlamaTokenizer</span>
<span></span><span>from</span><span> peft </span><span>import</span><span> PeftModel</span><span>,</span><span> PeftConfig</span>
</code></pre><p>Load the tokenizer and model:</p><pre><code><span>model_id</span><span>=</span><span>&#34;./models_hf/7B&#34;</span><span></span>
<span>tokenizer </span><span>=</span><span> LlamaTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>)</span><span></span>
<span>model </span><span>=</span><span>LlamaForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_id</span><span>,</span><span> load_in_8bit</span><span>=</span><span>True</span><span>,</span><span> device_map</span><span>=</span><span>&#39;auto&#39;</span><span>,</span><span> torch_dtype</span><span>=</span><span>torch</span><span>.</span><span>float16</span><span>)</span>
</code></pre><p>Load the adapter from where you saved it post-train:</p><pre><code><span>model </span><span>=</span><span> PeftModel</span><span>.</span><span>from_pretrained</span><span>(</span><span>model</span><span>,</span><span> </span><span>&#34;/root/llama-recipes/samsungsumarizercheckpoint&#34;</span><span>)</span>
</code></pre><p>Run inference:</p><pre><code><span>eval_prompt </span><span>=</span><span> </span><span>&#34;&#34;&#34;</span>
<span>Summarize this dialog:</span>
<span>A: Hi Tom, are you busy tomorrow’s afternoon?</span>
<span>B: I’m pretty sure I am. What’s up?</span>
<span>A: Can you go with me to the animal shelter?.</span>
<span>B: What do you want to do?</span>
<span>A: I want to get a puppy for my son.</span>
<span>B: That will make him so happy.</span>
<span>A: Yeah, we’ve discussed it many times. I think he’s ready now.</span>
<span>B: That’s good. Raising a dog is a tough issue. Like having a baby ;-)</span>
<span>A: I&#39;ll get him one of those little dogs.</span>
<span>B: One that won&#39;t grow up too big;-)</span>
<span>A: And eat too much;-))</span>
<span>B: Do you know which one he would like?</span>
<span>A: Oh, yes, I took him there last Monday. He showed me one that he really liked.</span>
<span>B: I bet you had to drag him away.</span>
<span>A: He wanted to take it home right away ;-).</span>
<span>B: I wonder what he&#39;ll name it.</span>
<span>A: He said he’d name it after his dead hamster – Lemmy  - he&#39;s  a great Motorhead fan :-)))</span>
<span>---</span>
<span>Summary:</span>
<span>&#34;&#34;&#34;</span><span></span>
<!-- -->
<span>model_input </span><span>=</span><span> tokenizer</span><span>(</span><span>eval_prompt</span><span>,</span><span> return_tensors</span><span>=</span><span>&#34;pt&#34;</span><span>)</span><span>.</span><span>to</span><span>(</span><span>&#34;cuda&#34;</span><span>)</span><span></span>
<!-- -->
<span>model</span><span>.</span><span>eval</span><span>(</span><span>)</span><span></span>
<span></span><span>with</span><span> torch</span><span>.</span><span>no_grad</span><span>(</span><span>)</span><span>:</span><span></span>
<span>    </span><span>print</span><span>(</span><span>tokenizer</span><span>.</span><span>decode</span><span>(</span><span>model</span><span>.</span><span>generate</span><span>(</span><span>**</span><span>model_input</span><span>,</span><span> max_new_tokens</span><span>=</span><span>100</span><span>)</span><span>[</span><span>0</span><span>]</span><span>,</span><span> skip_special_tokens</span><span>=</span><span>True</span><span>)</span><span>)</span>
</code></pre><p>Next in this series, I&#39;ll show you how you can format your own dataset to train Llama 2 on a custom task! Message me on <a href="https://twitter.com/samlhuillier_">Twitter</a> if you want to get me to hurry up on this!</p></div></div>
  </body>
</html>
