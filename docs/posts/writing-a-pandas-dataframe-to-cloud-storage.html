<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://corinfaife.co/posts/write-pandas-google-cloud">Original</a>
    <h1>Writing a pandas dataframe to cloud storage</h1>
    
    <div id="readability-page-1" class="page"><div id="__next" data-reactroot=""><article><h2>In search of simple data sharing</h2>
<p>I&#39;ve often wanted a workflow where I could process data in a Jupyter notebook using pandas, then quickly upload the contents of the dataframe to the cloud to get an easily shareable URL.</p>
<p>This week I finally made that happen using <a href="https://cloud.google.com/storage">Google Cloud Storage</a> and some associated libraries. The most minimal version of the workflow is now as simple as:</p>
<pre><code>df = pd.DataFrame(...somedata)
df.to_csv(&#39;gs://bucket_name/filename.csv&#39;)
</code></pre>
<p>With the file then available at <code>storage.cloud.google.com/bucket_name/filename.csv</code></p>
<p>Since I had to get familiar with some new software along the way, I wanted to note down the process step-by-step.</p>
<h2>The Google Cloud part</h2>
<h3>Make a Google Cloud account</h3>
<p>Google Cloud is a set of entreprise and/or developer-focused cloud services, and so is not enabled for a standard Google account by default.</p>
<p>First I had to visit <a href="https://cloud.google.com/">cloud.google.com</a> and sign up for the service. This took me to the fairly complicated looking Google Cloud dashboard, where I created a new project named Datahub.</p>
<p><img src="https://blog.pypi.org/images/2024/Gcloud/cloud-dashboard.png" alt="Datahub cloud project"/></p>
<h3>Create a publicly accessible storage bucket</h3>
<p>I spent a while looking through all the different options until I found what I was looking for in the &#39;Storage&#39; button under the Resources tab.</p>
<p>This took me to a less complicated looking screen where I could create a new cloud storage bucket. At some point I was prompted to add my credit card info, because Google Cloud is a paid product – though it will only cost a few cents per month for the low volume of data transfer that I&#39;ll need.</p>
<p><img src="https://blog.pypi.org/images/2024/Gcloud/create-bucket.png" alt="New bucket"/></p>
<p>When creating the bucket I deselected the option to enforce public access prevention, meaning that data stored in the bucket will be viewable in read-only mode without authentication.</p>
<p>This would be risky security protocol for any sensitive information, but is fine for projects where we&#39;re sharing non-confidential data in order to collaborate on dataviz work.</p>
<p><img src="https://blog.pypi.org/images/2024/Gcloud/create-bucket-dialogue.png" alt="Bucket dialogue"/></p>
<h2>The Python part</h2>
<h3>Install Google Cloud&#39;s CLI and Python library</h3>
<p>Back on my local machine, I needed to <a href="https://cloud.google.com/sdk/docs/install">install the Google Cloud CLI</a>, and then set up <a href="https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev">Application Default Credentials</a> (ADC) with the command line tool:</p>
<pre><code>$ gcloud init
$ gcloud auth application-default login
</code></pre>
<p>The point of ADC is to store a set of credentials that can be accessed by the environment that the local code is running in, meaning there&#39;s no need to read a specific file containing application password, API key, etc.</p>
<p>After that, I needed to install the relevant libraries to interface with the cloud storage using Python.</p>
<p>First I installed the <code>google-cloud-storage</code> library into a new Conda environment (which is how I manage data journalism projects):</p>
<pre><code>$ conda create --name cloud
$ conda install conda-forge::google-cloud-storage
</code></pre>
<p>Then I tried a code sample provided by Google to <a href="https://cloud.google.com/docs/authentication/client-libraries#python">authenticate with a cloud storage bucket</a> using the ADC credentials.</p>
<pre><code>from google.cloud import storage

def authenticate_implicit_with_adc(project_id=&#34;your-project-id&#34;):
    storage_client = storage.Client(project=project_id)
    buckets = storage_client.list_buckets()
    print(&#34;Buckets:&#34;)
    for bucket in buckets:
        print(bucket.name)
    print(&#34;Listed all storage buckets.&#34;)
</code></pre>
<p>This returned the name of my newly created storage bucket. Success!</p>
<p>Next I wanted to figure out the file upload.</p>
<h3>Adapting examples from the Python documentation</h3>
<p>The Google Cloud team has created a GitHub repo with code samples for <a href="https://github.com/googleapis/python-storage/tree/main/samples">interacting with cloud storage using Python</a>.</p>
<p>After working through a few of those, I was able to write a simple, lightweight function to upload a dataframe to the cloud directly from memory (i.e. no need to read/write a local file first).</p>
<pre><code>import pandas as pd
from google.cloud import storage

def upload_csv_to_gcloud(df,filename,project,bucket):
    # Convert dataframe to CSV string buffer
    csv = df.to_csv(index=False)

    # Initialize GCS client
    client = storage.Client(project=project)
    bucket = client.get_bucket(bucket)
    blob = bucket.blob(filename)

    # Upload with content type specified
    blob.upload_from_string(csv, content_type=&#39;text/csv&#39;)

df = pd.DataFrame({&#39;letter&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;],
                    &#39;number&#39;:[1,2,3]})

upload_csv_to_gcloud(df,
                    &#39;data.csv&#39;,
                    &#39;my_cloud_project&#39;,
                    &#39;cloud_bucket_name&#39;)
</code></pre>
<h3>Bonus: use fs libraries to write directly from pandas</h3>
<p>The code above is a flexible and readable way to achieve the goal, as it&#39;s clear what the function is doing, and easy to build in more functionality if we need it. But after reading some Stack Overflow I found a <a href="https://stackoverflow.com/a/60644694/4151474">less explicit but much more simple way</a> to achieve the same result.</p>
<p>First we need to install two more libraries:</p>
<ul>
<li><a href="https://filesystem-spec.readthedocs.io/en/latest/"><code>fsspec</code></a> so that pandas can read from and write to remote file systems</li>
<li><a href="https://pypi.org/project/gcsfs/"><code>gcsfs</code></a> as a file system interface specifically for Google Cloud Storage</li>
</ul>
<p>Then all we need to do is:</p>
<pre><code>df.to_csv(&#39;gs://bucket/filepath.csv&#39;)
</code></pre>
<p>And we can write our CSV file directly to the cloud!</p>
<p>We can&#39;t explicitly specify a file type with this method, so the file in the storage bucket is stored as type <code>application/octet-stream</code> rather than CSV or plaintext:</p>
<p><img src="https://blog.pypi.org/images/2024/Gcloud/octet-stream.png" alt="Octet stream csv"/></p>
<p>But downloading the file will render it as a conventional CSV, so in some cases this might be an acceptable trade for the sheer simplicity.</p>
<p>Overall though, once the initial set up is done, it&#39;s very quick either way!</p><small><time>2024</time> © Corin Faife.<a href="https://blog.pypi.org/feed.xml">RSS</a></small></article></div></div>
  </body>
</html>
