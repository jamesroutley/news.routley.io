<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://engineering.skroutz.gr/blog/uncovering-a-24-year-old-bug-in-the-linux-kernel/">Original</a>
    <h1>A 24-year-old bug in the Linux Kernel TCP stack (2021)</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p>As part of our standard toolkit, we provide each developer at Skroutz
with a writable database snapshot against which she can develop. These
snapshots are updated daily through a pipeline that involves taking an
LVM snapshot of production data, anonymizing the dataset by stripping
all personal data, and transferring it via rsync to the development
database servers. The development servers in turn use ZFS snapshots to
expose a copy-on-write snapshot to each developer, with
self-service tools allowing rollback or upgrades to newer snapshots.</p>

<p>We use the same pipeline to expose MariaDB and MongoDB data, with a
full dataset size of 600GB and 200GB respectively, and a slightly
modified pipeline for Elasticsearch. While on-disk data changes
significantly for all data sources, rsync still saves significant time
by transferring roughly 1/3 of the full data set every night. This
setup has worked rather well for the better part of a decade and has
managed to scale from 15 developers to 150. However, as with most
systems, it has had its fair share of maintenance and has given us
some interesting moments.</p>

<p>One of the most interesting issues we encountered led to the discovery
of a fairly old bug in the Linux kernel TCP implementation: every now
and then, an rsync transfer from a source server would hang
indefinitely for no apparent reason, as — apart from the stuck transfer —
everything else seemed to be in order. What’s more, for reasons that became
apparent later, the issue could not be reproduced at will, although
some actions (e.g. adding an rsync-level rate limit) seemed to make
the issue less frequent, with frequency ranging from once or twice per
week to once every three months.</p>

<p>As is not unusual in these cases, we had more urgent systems and issues to
attend to, so we labeled this a “race condition in rsync” that we
should definitely look into at some point, and worked around it by
throttling the rsync transfers.</p>

<p>Until it started biting us every single day.</p>

<h2 id="rsync-as-a-pipeline">rsync as a pipeline</h2>

<p>While not strictly necessary, knowing how rsync works internally will help
understand the analysis that follows. The rsync site contains <a href="https://rsync.samba.org/how-rsync-works.html">a thorough
description</a> of rsync’s internal architecture, so I’ll try to
summarize the most important points here:</p>

<ol>
  <li>
    <p>rsync starts off as a single process on the client and a single
process on the server, communicating via a socket pair. When using
the rsync daemon, as in our case, communication is done over a
plain TCP connection</p>
  </li>
  <li>
    <p>Based on the direction of sync, after the initial handshake is
over, each end assumes a <em>role</em>, either that of the <em>sender</em>, or
that of the <em>receiver</em>. In our case the client is the receiver,
and the server is the sender.</p>
  </li>
  <li>
    <p>The receiver forks an additional process called the <em>generator</em>,
sharing the socket with the <em>receiver</em> process. The <em>generator</em>
figures out what it needs to ask from the <em>sender</em>, and the
<em>sender</em> subsequently sends the data to the <em>receiver</em>. What we
essentially have after this step is a pipeline, <em>generator</em> →
<em>sender</em> → <em>receiver</em>, where the arrows are the two directions of
<em>the same</em> TCP connection. While there is some signaling involved,
the pipeline operates in a <em>blocking</em> fashion and relies on OS
buffers and TCP receive windows to apply backpressure.</p>
  </li>
</ol>

<h2 id="a-ghost-in-the-network">A ghost in the network?</h2>

<p>Our first reaction when we encountered the issue was to suspect the
network for errors, which was a <em>reasonable</em> thing to do since we had
recently upgraded our servers and switches. After eliminating the
usual suspects (NIC firmware bugs involving TSO/GSO/GRO/VLAN
offloading, excessive packet drops or CRC errors at the switches etc),
we came to the conclusion that everything was normal and something
else had to be going on.</p>

<p>Attaching the hung processes using strace and gdb told us little: the
generator was hung on <code>send()</code> and the sender and receiver were hung
on <code>recv()</code>, yet no data was moving. However, turning to the kernel on
both systems revealed something interesting! On the client the rsync
socket shared between the <em>generator</em> and the <em>receiver</em> processes was
in the following state:</p>

<figure><pre><code data-lang="bash"><span>$ </span>ss <span>-mito</span> dst :873
State      Recv-Q Send-Q                  Local Address:Port                                 Peer Address:Port
ESTAB      0      392827 ❶             2001:db8:2a::3:38022                             2001:db8:2a::18:rsync                 timer:<span>(</span>persist,1min56sec,0<span>)</span>
	 skmem:<span>(</span>r0,rb4194304,t0,tb530944,f3733,w401771,o0,bl0,d757<span>)</span> ts sack cubic wscale:7,7 rto:204 backoff:15 rtt:2.06/0.541 ato:40 mss:1428 cwnd:10 ssthresh:46 bytes_acked:22924107 bytes_received:100439119971 segs_out:7191833 segs_in:70503044 data_segs_out:16161 data_segs_in:70502223 send 55.5Mbps lastsnd:16281856 lastrcv:14261988 lastack:3164 pacing_rate 133.1Mbps retrans:0/11 rcv_rtt:20 rcv_space:2107888 notsent:392827 minrtt:0.189</code></pre></figure>

<p>while on the server, the socket state was the following:</p>

<figure><pre><code data-lang="bash"><span>$ </span>ss <span>-mito</span> src :873
State      Recv-Q Send-Q                Local Address:Port                                 Peer Address:Port
ESTAB      0      0                   2001:db8:2a::18:rsync                              2001:db8:2a::3:38022                 timer:<span>(</span>keepalive,3min7sec,0<span>)</span>
 	 skmem:<span>(</span>r0,rb3540548,t0,tb4194304,f0,w0,o0,bl0,d292<span>)</span> ts sack cubic wscale:7,7 rto:204 rtt:1.234/1.809 ato:40 mss:1428 cwnd:1453 ssthresh:1431 bytes_acked:100439119971 bytes_received:22924106 segs_out:70503089 segs_in:7191833 data_segs_out:70502269 data_segs_in:16161 send 13451.4Mbps lastsnd:14277708 lastrcv:16297572 lastack:7012576 pacing_rate 16140.1Mbps retrans:0/794 rcv_rtt:7.5 rcv_space:589824 minrtt:0.026</code></pre></figure>

<p>The interesting thing here is that there are 3.5MB of data on the
client, queued to be sent (❶ in the first output) by the
<em>generator</em> to the server; however, while the server has an empty <code>Recv-Q</code>
and can accept data, nothing seems to be moving forward. If <code>Recv-Q</code>
in the second output was non-zero, we would be looking at rsync on the
server being stuck and not reading from the network, however here it
is obvious that rsync has consumed all incoming data and is not to
blame.</p>

<p>So why is data queued up on one end of the connection, while the other end is
obviously able to accept it? The answer is conveniently hidden in the <code>timer</code>
fields of both <code>ss</code> outputs, especially in
<code>timer:(persist,1min56sec,0)</code>. Quoting <code>ss(8)</code>:</p>

<figure><pre><code data-lang="man">       -o, --options
              Show timer information. For TCP protocol, the output format is:

              timer:(&lt;timer_name&gt;,&lt;expire_time&gt;,&lt;retrans&gt;)

              &lt;timer_name&gt;
                     the name of the timer, there are five kind of timer names:

                     on : means one of these  timers:  TCP  retrans  timer,  TCP
                     early retrans timer and tail loss probe timer

                     keepalive: tcp keep alive timer

                     timewait: timewait stage timer

                     persist: zero window probe timer

                     unknown: none of the above timers</code></pre></figure>

<p><code>persist</code> means that the connection has received a zero window
advertisement and is waiting for the peer to advertise a non-zero
window.</p>

<h2 id="tcp-zero-windows-and-zero-window-probes">TCP Zero Windows and Zero Window Probes</h2>

<p>TCP implements flow control by limiting the data in flight using a sliding
window called the <em>receive window</em>. Wikipedia has a <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Flow_control">good description</a>, but in short each end of a TCP connection advertises how much
data it is willing to buffer for the connection, i.e. how much data the other
end may send before waiting for an acknowledgment.</p>

<p>When one side’s receive buffer (<code>Recv-Q</code>) fills up (in this case
because the rsync process is doing disk I/O at a speed slower than the
network’s), it will send out a zero window advertisement, which will
put that direction of the connection on hold. When buffer space
eventually frees up, the kernel will send an unsolicited window update
with a non-zero window size, and the data transfer continues. To be
safe, just in case this unsolicited window update is lost, the other
end will regularly poll the connection state using the so-called Zero
Window Probes (the <code>persist</code> mode we are seeing here).</p>

<h2 id="the-window-is-stuck-closed">The window is stuck closed</h2>

<p>It’s now time to dive a couple of layers deeper and use <code>tcpdump</code> to
see what’s going on at the network level:</p>

<figure><pre><code data-lang="text">[…]
09:34:34.165148 0c:c4:7a:f9:68:e4 &gt; 0c:c4:7a:f9:69:78, ethertype IPv6 (0x86dd), length 86: (flowlabel 0xcbf6f, hlim 64, next-header TCP (6) payload length: 32) 2001:db8:2a::3.38022 &gt; 2001:db8:2a::18.873: Flags [.], cksum 0x711b (incorrect -&gt; 0x4d39), seq 4212361595, ack 1253278418, win 16384, options [nop,nop,TS val 2864739840 ecr 2885730760], length 0
09:34:34.165354 0c:c4:7a:f9:69:78 &gt; 0c:c4:7a:f9:68:e4, ethertype IPv6 (0x86dd), length 86: (flowlabel 0x25712, hlim 64, next-header TCP (6) payload length: 32) 2001:db8:2a::18.873 &gt; 2001:db8:2a::3.38022: Flags [.], cksum 0x1914 (correct), seq 1253278418, ack 4212361596, win 13831, options [nop,nop,TS val 2885760967 ecr 2863021624], length 0
[… repeats every 2 mins]</code></pre></figure>

<p>The first packet is the rsync client’s zero window probe, the second
packet is the server’s response. Surprisingly enough, the server is
advertising a non-zero window size of 13831 bytes¹ which the client
apparently ignores.</p>

<p>¹ actually multiplied by 128 because of a <a href="https://en.wikipedia.org/wiki/TCP_window_scale_option">window scaling</a> factor
of 7</p>

<p>We are finally making some progress and have a case to work on! At
some point the client encountered a zero window advertisement from the
server as part of regular TCP flow control, but then the window failed
to re-open for some reason. The client seems to be still ignoring
the new window advertised by the server and this is why the transfer
is stuck.</p>

<h2 id="linux-tcp-input-processing">Linux TCP input processing</h2>

<p>By now it’s obvious that the TCP connection is in a weird state on the
rsync client. Since TCP flow control happens at the kernel level, to
get to the root of this we need to look at how the Linux kernel
handles incoming TCP acknowledgments and try to figure out why it
ignores the incoming window advertisement.</p>

<p>Incoming TCP packet processing happens in
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/net/ipv4/tcp_input.c"><code>net/ipv4/tcp_input.c</code></a>.Despite
the <code>ipv4</code> component in the path, this is mostly shared code between
IPv4 and IPv6.</p>

<p>Digging a bit through the code we find out that incoming window
updates are handled in
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/net/ipv4/tcp_input.c?id=c3df39ac9b0e3747bf8233ea9ce4ed5ceb3199d3#n3552"><code>tcp_ack_update_window</code></a>
and actually updating the window is guarded by the following function:</p>

<figure><pre><code data-lang="c"><span>/* Check that window update is acceptable.
 * The function assumes that snd_una&lt;=ack&lt;=snd_next.
 */</span>
<span>static</span> <span>inline</span> <span>bool</span> <span>tcp_may_update_window</span><span>(</span><span>const</span> <span>struct</span> <span>tcp_sock</span> <span>*</span><span>tp</span><span>,</span>
					<span>const</span> <span>u32</span> <span>ack</span><span>,</span> <span>const</span> <span>u32</span> <span>ack_seq</span><span>,</span>
					<span>const</span> <span>u32</span> <span>nwin</span><span>)</span>
<span>{</span>
	<span>return</span>	<span>after</span><span>(</span><span>ack</span><span>,</span> <span>tp</span><span>-&gt;</span><span>snd_una</span><span>)</span> <span>||</span> <span>❶</span>
		<span>after</span><span>(</span><span>ack_seq</span><span>,</span> <span>tp</span><span>-&gt;</span><span>snd_wl1</span><span>)</span> <span>||</span> <span>❷</span>
		<span>(</span><span>ack_seq</span> <span>==</span> <span>tp</span><span>-&gt;</span><span>snd_wl1</span> <span>&amp;&amp;</span> <span>nwin</span> <span>&gt;</span> <span>tp</span><span>-&gt;</span><span>snd_wnd</span><span>);</span> <span>❸</span>
<span>}</span></code></pre></figure>

<p>The <code>ack</code>, <code>ack_seq</code>, <code>snd_wl1</code> and <code>snd_una</code> variables hold TCP
sequence numbers that are used in TCP’s sliding window to keep track
of the data exchanged over the wire. These sequence numbers are 32-bit
unsigned integers (<code>u32</code>) and are incremented by 1 for each byte that
is exchanged, beginning from an arbitrary initial value (<em>initial
sequence number</em>). In particular:</p>

<ul>
  <li><code>ack_seq</code> is the sequence number of the incoming segment</li>
  <li><code>ack</code> is the <em>acknowledgment number</em> contained in the incoming
 segment, i.e. it acknowledges the sequence number of the last
 segment the peer received from us.</li>
  <li><code>snd_wl1</code> is the sequence number of the incoming segment that last
 updated the peer’s receive window.</li>
  <li><code>snd_una</code> is the sequence number of the first <em>unacknowledged</em>
 segment, i.e. a segment we have sent but has not been yet
 acknowledged by the peer.</li>
</ul>

<p>Being fixed-size integers, the sequence numbers will eventually wrap
around, so the <code>after()</code> macro takes care of comparing two sequence
numbers <a href="https://en.wikipedia.org/wiki/Serial_number_arithmetic">in the face of wraparounds</a>.</p>

<p>For the record, the <code>snd_una</code> and <code>snd_wl1</code> names come directly from
the <a href="https://tools.ietf.org/html/rfc793#section-3.2">original TCP specification in RFC 793</a>, back in 1981!</p>

<p>Translating the rather cryptic check into plain English, we are
willing to accept a window update from a peer if:</p>

<dl>
  <dt>❶</dt>
  <dd>our peer acknowledges the receipt of data we previously sent; <em>or</em></dd>
  <dt>❷</dt>
  <dd>our peer is sending new data since the previous window update; <em>or</em></dd>
  <dt>❸</dt>
  <dd>our peer isn’t sending us new data since the previous window update,
but is advertising a larger window</dd>
</dl>

<p>Note that the comparison of <code>ack_seq</code> with <code>snd_wl1</code> is done to make
sure that the window is not accidentally updated by a
(retransmission of a) segment that was seen earlier.</p>

<p>In our case, at least condition ❸ should be able to re-open the window, but
apparently it doesn’t and we need access to these variables to figure out what
is happening. Unfortunately, these variables are part of the internal kernel
state and are not directly exposed to userspace, so it’s time to get a bit
creative.</p>

<h2 id="accessing-the-internal-kernel-state">Accessing the internal kernel state</h2>

<p>To get access to the kernel state, we somehow need to run code inside
the kernel. One way would be to patch the kernel with a few <code>printk()</code>
calls here and there, but that would require rebooting the machine and
waiting for rsync to hang again. Rather, we opted to live-patch the
kernel using <a href="https://sourceware.org/systemtap/">systemtap</a> with the following script:</p>

<figure><pre><code data-lang="perl"><span>probe</span> <span>kernel</span><span>.</span><span>statement</span><span>(&#34;</span><span>tcp_ack@./net/ipv4/tcp_input.c:3751</span><span>&#34;)</span>
<span>{</span>
  <span>if</span> <span>(</span><span>$sk</span><span>-&gt;</span><span>sk_send_head</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
	  <span>ack_seq</span> <span>=</span> <span>@cast</span><span>(</span><span>&amp;$skb</span><span>-&gt;</span><span>cb</span><span>[</span><span>0</span><span>],</span> <span>&#34;</span><span>tcp_skb_cb</span><span>&#34;,</span> <span>&#34;</span><span>kernel&lt;net/tcp.h&gt;</span><span>&#34;)</span><span>-&gt;</span><span>seq</span>
	  <span>printf</span><span>(&#34;</span><span>ack: %d, ack_seq: %d, prior_snd_una: %d</span><span>\n</span><span>&#34;,</span> <span>$ack</span><span>,</span> <span>ack_seq</span><span>,</span> <span>$prior_snd_una</span><span>)</span>
	  <span>seq</span> <span>=</span> <span>@cast</span><span>(</span><span>&amp;$sk</span><span>-&gt;</span><span>sk_send_head</span><span>-&gt;</span><span>cb</span><span>[</span><span>0</span><span>],</span> <span>&#34;</span><span>tcp_skb_cb</span><span>&#34;,</span> <span>&#34;</span><span>kernel&lt;net/tcp.h&gt;</span><span>&#34;)</span><span>-&gt;</span><span>seq</span>
	  <span>end_seq</span> <span>=</span> <span>@cast</span><span>(</span><span>&amp;$sk</span><span>-&gt;</span><span>sk_send_head</span><span>-&gt;</span><span>cb</span><span>[</span><span>0</span><span>],</span> <span>&#34;</span><span>tcp_skb_cb</span><span>&#34;,</span> <span>&#34;</span><span>kernel&lt;net/tcp.h&gt;</span><span>&#34;)</span><span>-&gt;</span><span>end_seq</span>
	  <span>printf</span><span>(&#34;</span><span>sk_send_head seq:%d, end_seq: %d</span><span>\n</span><span>&#34;,</span> <span>seq</span><span>,</span> <span>end_seq</span><span>)</span>

	  <span>snd_wnd</span> <span>=</span> <span>@cast</span><span>(</span><span>$sk</span><span>,</span> <span>&#34;</span><span>tcp_sock</span><span>&#34;,</span> <span>&#34;</span><span>kernel&lt;linux/tcp.h&gt;</span><span>&#34;)</span><span>-&gt;</span><span>snd_wnd</span>
	  <span>snd_wl1</span> <span>=</span> <span>@cast</span><span>(</span><span>$sk</span><span>,</span> <span>&#34;</span><span>tcp_sock</span><span>&#34;,</span> <span>&#34;</span><span>kernel&lt;linux/tcp.h&gt;</span><span>&#34;)</span><span>-&gt;</span><span>snd_wl1</span>
	  <span>ts_recent</span> <span>=</span> <span>@cast</span><span>(</span><span>$sk</span><span>,</span> <span>&#34;</span><span>tcp_sock</span><span>&#34;,</span> <span>&#34;</span><span>kernel&lt;linux/tcp.h&gt;</span><span>&#34;)</span><span>-&gt;</span><span>rx_opt</span><span>-&gt;</span><span>ts_recent</span>
	  <span>rcv_tsval</span> <span>=</span> <span>@cast</span><span>(</span><span>$sk</span><span>,</span> <span>&#34;</span><span>tcp_sock</span><span>&#34;,</span> <span>&#34;</span><span>kernel&lt;linux/tcp.h&gt;</span><span>&#34;)</span><span>-&gt;</span><span>rx_opt</span><span>-&gt;</span><span>rcv_tsval</span>
	  <span>printf</span><span>(&#34;</span><span>snd_wnd: %d, tcp_wnd_end: %d, snd_wl1: %d</span><span>\n</span><span>&#34;,</span> <span>snd_wnd</span><span>,</span> <span>$prior_snd_una</span> <span>+</span> <span>snd_wnd</span><span>,</span> <span>snd_wl1</span><span>)</span>
	  <span>printf</span><span>(&#34;</span><span>flag: %x, may update window: %d</span><span>\n</span><span>&#34;,</span> <span>$flag</span><span>,</span> <span>$flag</span> <span>&amp;</span> <span>0x02</span><span>)</span>
	  <span>printf</span><span>(&#34;</span><span>rcv_tsval: %d, ts_recent: %d</span><span>\n</span><span>&#34;,</span> <span>rcv_tsval</span><span>,</span> <span>ts_recent</span><span>)</span>
	  <span>print</span><span>(&#34;</span><span>\n</span><span>&#34;)</span>
     <span>}</span>
<span>}</span></code></pre></figure>

<p>Systemtap works by converting systemtap scripts into C and building a
kernel module that hot-patches the kernel and overrides specific
instructions. Here we overrode <code>tcp_ack()</code>, hooked at its end and
dumped the internal TCP connection state. The <code>$sk-&gt;sk_send_head !=
NULL</code> check is a quick way to only match connections that have a
non-empty <code>Send-Q</code>.</p>

<p>Loading the resulting module into the kernel gave us the following:</p>

<figure><pre><code data-lang="text">ack: 4212361596, ack_seq: 1253278418, prior_snd_una: 4212361596
sk_send_head seq:4212361596, end_seq: 4212425472
snd_wnd: 0, tcp_wnd_end: 4212361596, snd_wl1: 1708927328
flag: 4100, may update window: 0
rcv_tsval: 2950255047, ts_recent: 2950255047</code></pre></figure>

<p>The two things of interest here are <code>snd_wl1: 1708927328</code> and
<code>ack_seq: 1253278418</code>. Not only are they not identical as we would
expect, but actually <code>ack_seq</code> is <em>smaller</em> than <code>snd_wl1</code>, indicating
that <code>ack_seq</code> wrapped around at some point and <code>snd_wl1</code> has not been
updated for a while. Using the <a href="https://en.wikipedia.org/wiki/Serial_number_arithmetic">serial number arithmetic</a> rules, we can figure out that this end has
received (at least) 3.8 GB since the last update of <code>snd_wl1</code>.</p>

<p>We already saw that <code>snd_wl1</code> contains the last sequence number used
to update the peer’s receive window (and thus our send window), with
the ultimate purpose of guarding against window updates from old
segments. It should be okay if <code>snd_wl1</code> is not updated for a while,
but it should not lag too far behind <code>ack_seq</code>, or else we risk
rejecting valid window updates, as in this case. So it looks like the
Linux kernel fails to update <code>snd_wl1</code> under some circumstances, which
leads to an inability to recover from a zero-window condition.</p>

<p>Having tangible proof that something was going on in the kernel, it
was time to get people familiar with the networking code in the loop.</p>

<h2 id="taking-things-upstream">Taking things upstream</h2>

<p>After sleeping on this, we wrote a good summary of what we knew so far
and what we supposed was happening, and reached out to <a href="https://lore.kernel.org/netdev/87eelz4abk.fsf@marvin.dmesg.gr/T/#u">the Linux
networking maintainers</a>. Confirmation came less than two
hours later, <a href="https://lore.kernel.org/netdev/87eelz4abk.fsf@marvin.dmesg.gr/T/#mf568052a4f9d76d847ae192d3632b8e87083d75a">together with a patch by Neal
Cardwell</a>.</p>

<p>Apparently, the bug was in the <em>bulk receiver fast-path</em>, a code path
that skips most of the expensive, strict TCP processing to optimize
for the common case of bulk data reception. This is a significant
optimization, outlined 28 years ago² by Van Jacobson in his <a href="https://www.pdl.cmu.edu/mailinglists/ips/mail/msg00133.html">“TCP
receive in 30 instructions” email</a>. Apparently
the Linux implementation did not update <code>snd_wl1</code> while in the
receiver fast path. If a connection uses the fast path for too long,
<code>snd_wl1</code> will fall so far behind that <code>ack_seq</code> will wrap around with
respect to it. And if this happens while the receive window is zero,
there is no way to re-open the window, as demonstrated above. What’s
more, this bug had been present in Linux <a href="https://git.kernel.org/pub/scm/linux/kernel/git/history/history.git/commit/net/ipv4/tcp_input.c?h=2.1.8&amp;id=0f9cac5b27076f801b29a0867868e1bce7310e00&amp;ignorews=1">since v2.1.8</a>, dating
back to 1996!</p>

<p>² This optimization is still relevant today: a relatively recent
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=45f119bf936b1f9f546a0b139c5b56f9bb2bdc78">attempt</a> to remove the header prediction code and associated fast
paths to simplify the code was <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=31770e34e43d6c8dee129bfee77e56c34e61f0e5">reverted</a> on performance
regression grounds.</p>

<p>As soon as we got the patch, we applied it, rebuilt the kernel,
deployed it on the affected machines and waited to see if the issue
was fixed. A couple of days later we were certain that the fix was indeed
correct and did not cause any ill side-effects.</p>

<p>After a bit of discussion, the <a href="https://patchwork.ozlabs.org/project/netdev/patch/20201022143331.1887495-1-ncardwell.kernel@gmail.com/">final commit</a> landed in
<code>linux-net</code>, and from there it was merged into Linux mainline for 5.10-rc1.
Eventually it found its way to the stable 4.9 and 4.19 kernel series that we
use on our Debian systems, in 4.9.241 and 4.19.153 respectively.</p>

<h2 id="aftermath">Aftermath</h2>

<p>With the fix in place, we still had a couple of questions to answer,
namely:</p>

<ul>
  <li>
    <p>How is it possible for a TCP bug that leads to stuck connections to
go unnoticed for 24 years?</p>
  </li>
  <li>
    <p>Out of an infrastructure with more than 600 systems running all kinds of
software, how come we only witnessed this bug when using rsync?</p>
  </li>
</ul>

<p>It’s hard to give a definitive answer to these questions, but we can
reason about it this way:</p>

<ol>
  <li>
    <p>This bug will not be triggered by most L7 protocols. In
“synchronous” request-response protocols such as HTTP, usually
each side will consume all available data before sending. In this
case, even if <code>snd_wl1</code> wraps around, the bulk receiver will be
left with a non-zero window and will still be able to send out
data, causing the next acknowledgment to update the window and
adjust <code>snd_wl1</code> through check ❶ in <code>tcp_may_update_window</code>. <code>rsync</code> on the
other hand uses a pretty aggressive pipeline where the server might send
out multi-GB responses without consuming incoming data in the process.
Even in <code>rsync</code>’s case, using <code>rsync</code> over SSH (a rather common
combination) rather than the plain TCP transport would not expose this bug,
as SSH framing/signaling would most likely not allow data to queue up on
the server this way.</p>
  </li>
  <li>
    <p>Regardless of the application protocol, the receiver must remain
long enough (for at least 2GB) with a zero send window in the fast
path to cause a wrap-around — but not too long for <code>ack_seq</code>
to overtake <code>snd_wl1</code> again. For this to happen, there must be no
packet loss or other conditions that would cause the fast path’s header
prediction to fail. This is very unlikely to happen in practice as TCP
itself determines the network capacity by actually causing packets to be
lost.</p>
  </li>
  <li>
    <p>Most applications will care about network timeouts and will either fail or
reconnect, making it appear as a “random network glitch” and leaving no
trace to debug behind.</p>
  </li>
</ol>

<p>Finally, even if none of the above happens and you end up with a stuck
TCP connection, it takes a lot of annoyance to decide to deal with it
and drill deep in kernel code. And when you do, you are rewarded with
a nice adventure, where you get to learn about internet protocol
history, have a glimpse at kernel internals, and witness open source
work in motion!</p>

<hr/>

<p>If you enjoyed reading this post and you like hunting weird bugs and
looking at kernel code, you might want to drop us a line
— we are always looking for talented <a href="https://apply.workable.com/skroutz/j/485671FB1F/">SREs</a> and <a href="https://apply.workable.com/skroutz/j/9D8A0589DE/">DevOps
Engineers</a>!</p>


        
          <!-- /#disqus_thread -->
          




        
      </div></div>
  </body>
</html>
