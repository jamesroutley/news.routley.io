<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jamesakl.com/posts/cuda-ontology/">Original</a>
    <h1>CUDA Ontology</h1>
    
    <div id="readability-page-1" class="page"><div><main><article><div><p>CUDA’s terminology carries significant overloading: the word “CUDA” itself refers to at least five distinct concepts, “driver” means different things in different contexts, and version numbers reported by various tools measure different subsystems. This article provides a rigorous ontology of CUDA components: a systematic description of what exists in the CUDA ecosystem, how components relate to each other, their versioning semantics, compatibility rules, and failure modes. Each term is defined precisely to eliminate ambiguity. Understanding this structure is essential for diagnosing version incompatibilities and reasoning about CUDA system behavior.</p><h2 id="terminology-and-disambiguation">Terminology and disambiguation</h2><h3 id="the-term-cuda">The term “CUDA”</h3><p><code>CUDA</code> is overloaded across at least five distinct meanings:</p><ol><li><strong>CUDA as compute architecture:</strong> The parallel computing platform and programming model designed by NVIDIA.</li><li><strong>CUDA as instruction set:</strong> The GPU instruction set architecture (ISA) supported by NVIDIA hardware, versioned by compute capability (<code>compute_8.0</code>, <code>compute_9.0</code>, etc.).</li><li><strong>CUDA as source language:</strong> The C/C++ language extensions (<code>__global__</code>, <code>__device__</code>, etc.) used to write GPU code.</li><li><strong>CUDA Toolkit:</strong> The development package containing <code>nvcc</code>, libraries, headers, and development tools.</li><li><strong>CUDA Runtime:</strong> The runtime library (<code>libcudart</code>) that applications link against.</li></ol><p>When someone says “CUDA version,” they may be referring to toolkit version, runtime version, driver API version, or compute capability. Precision requires explicit qualification.</p><h3 id="the-term-kernel">The term “kernel”</h3><p><code>kernel</code> has two completely distinct meanings in GPU computing contexts:</p><ol><li><strong>Operating system kernel (<sup>OS</sup>kernel):</strong> The <sup>OS</sup>kernel running in privileged <sup>OS</sup>kernel-space. Examples: Linux <sup>OS</sup>kernel (e.g., version <code>6.6.87</code>), Windows NT <sup>OS</sup>kernel, macOS XNU <sup>OS</sup>kernel.</li><li><strong>CUDA kernel (<sup>CUDA</sup>kernel):</strong> A C++ function marked with <code>__global__</code> that executes on the GPU. When invoked from host code, the <sup>CUDA</sup>kernel launches as a grid of thread blocks.</li></ol><p>Throughout this article, <sup>OS</sup>kernel always refers to the operating system kernel (Linux, Windows, etc.), and <sup>CUDA</sup>kernel always refers to GPU functions.</p><h3 id="the-term-driver">The term “driver”</h3><p>In computing, a “driver” is software that allows the operating system to communicate with hardware devices. <code>driver</code> in the CUDA context is overloaded between:</p><ol><li><p><strong>NVIDIA GPU Driver</strong> (also called “NVIDIA Display Driver”): The <sup>OS</sup>kernel-space driver (<sup>OS</sup>kernel module on Linux, <sup>OS</sup>kernel driver on Windows) that manages GPU hardware. Despite the historical name “display driver,” this unified driver handles all GPU operations: graphics rendering, compute workloads, memory management, and scheduling. The name reflects NVIDIA’s evolution from graphics-focused GPUs to general-purpose compute accelerators.</p><ul><li>Installed as <sup>OS</sup>kernel modules: <code>nvidia.ko</code>, <code>nvidia-modeset.ko</code>, <code>nvidia-uvm.ko</code> (Linux), or as a Windows <sup>OS</sup>kernel driver.</li><li>Versioned independently: <code>535.104.05</code>, <code>550.54.15</code>, etc.</li></ul></li><li><p><strong>CUDA Driver API</strong>: A low-level C API (<code>libcuda.so</code> on Linux, <code>nvcuda.dll</code> on Windows) that provides direct access to GPU functionality. This is a user-space library provided by the NVIDIA GPU driver package.</p><ul><li>Located at: <code>/usr/lib/x86_64-linux-gnu/libcuda.so</code> (Linux).</li><li>API version distinct from GPU driver version but distributed together in the same driver package.</li></ul></li></ol><p>The NVIDIA GPU driver package includes both <sup>OS</sup>kernel components (<sup>OS</sup>kernel modules/drivers) and the <code>libcuda</code> user-space library.</p><h2 id="component-architecture">Component architecture</h2><p>The CUDA ecosystem consists of multiple layers, each with distinct responsibilities. Understanding this layering is fundamental to reasoning about version compatibility and system behavior.</p><h3 id="system-layers">System layers</h3><p>The CUDA stack spans <sup>OS</sup>kernel-space and user-space:</p><pre tabindex="0"><code data-lang="mermaid">graph TB
    A[&#34;&lt;b&gt;Application&lt;/b&gt; (PyTorch, TensorFlow, custom)&#34;]
    B[&#34;&lt;b&gt;libcudart.so&lt;/b&gt; (Runtime API, user-space)&#34;]
    C[&#34;&lt;b&gt;libcuda.so&lt;/b&gt; (Driver API, user-space)&#34;]
    D[&#34;&lt;b&gt;nvidia.ko&lt;/b&gt; (GPU Driver, &lt;sup&gt;OS&lt;/sup&gt;kernel-space)&#34;]
    E[&#34;&lt;b&gt;GPU Hardware&lt;/b&gt;&#34;]

    A --&gt;|calls| B
    B --&gt;|calls| C
    C --&gt;|ioctl syscalls| D
    D --&gt;|commands| E

    style A fill:#1a1a1a,stroke:#86efac,stroke-width:2px,color:#86efac,text-align:left
    style B fill:#1e2a3a,stroke:#60a5fa,stroke-width:2px,color:#60a5fa,text-align:left
    style C fill:#1e2a3a,stroke:#60a5fa,stroke-width:2px,color:#60a5fa,text-align:left
    style D fill:#3a2a1e,stroke:#fbbf24,stroke-width:3px,color:#fbbf24,text-align:left
    style E fill:#1a1a1a,stroke:#86efac,stroke-width:2px,color:#86efac,text-align:left

    classDef default font-family:Source Code Pro
</code></pre><h3 id="component-definitions">Component definitions</h3><p><strong><code>libcuda.so</code> / <code>nvcuda.dll</code></strong> (Driver API, backend):</p><ul><li>Provided by the NVIDIA GPU driver package.</li><li>Installed system-wide:<ul><li>Linux: <code>/usr/lib/x86_64-linux-gnu/libcuda.so</code> (or <code>/usr/lib64/libcuda.so</code>)</li><li>Windows: <code>C:\Windows\System32\nvcuda.dll</code></li></ul></li><li>Provides low-level primitives: <code>cuInit</code>, <code>cuMemAlloc</code>, <code>cuLaunchKernel</code>, etc.</li><li>Communicates with <sup>OS</sup>kernel driver via system calls:<ul><li>Linux: <code>ioctl</code> system calls (I/O control syscalls for <sup>OS</sup>kernel device communication)</li><li>Windows: <code>DeviceIoControl</code> calls to <sup>OS</sup>kernel driver</li></ul></li><li>Versioned by GPU driver version (e.g., driver <code>535.x</code> provides <code>libcuda.so</code>/<code>nvcuda.dll</code> with CUDA Driver API <code>12.2</code>).</li></ul><p><strong><code>libcudart.so</code> / <code>cudart64_*.dll</code></strong> (Runtime API, frontend):</p><ul><li>Provided by CUDA Toolkit (or bundled with applications like PyTorch).</li><li>File locations:<ul><li>Linux: <code>libcudart.so</code> (shared), <code>libcudart_static.a</code> (static)</li><li>Windows: <code>cudart64_&lt;version&gt;.dll</code> (shared), <code>cudart_static.lib</code> (static)</li></ul></li><li>Higher-level API: <code>cudaMalloc</code>, <code>cudaMemcpy</code>, <code>cudaLaunchKernel</code>, etc.</li><li>Internally calls <code>libcuda.so</code>/<code>nvcuda.dll</code> (Driver API) to perform operations.</li><li>Can be statically linked or dynamically linked.</li><li>Application code typically uses Runtime API, not Driver API directly.</li></ul><p><strong>CUDA Toolkit</strong>:</p><ul><li>Development package containing:<ul><li><code>nvcc</code>: Compiler for <sup>CUDA</sup>kernel code</li><li><code>libcudart</code>/<code>cudart64_*.dll</code>: Runtime library</li><li>Headers (<code>cuda.h</code>, <code>cuda_runtime.h</code>)</li><li>Math libraries: <code>cuBLAS</code>, <code>cuDNN</code>, <code>cuFFT</code>, etc.</li><li>Profiling and debugging tools: <code>nvprof</code>, <code>nsight</code>, <code>cuda-gdb</code></li></ul></li><li>Versioned independently from GPU driver: toolkit <code>12.1</code>, <code>12.4</code>, etc.</li><li>Required at build time to compile <sup>CUDA</sup>kernel code.</li><li>Runtime library (<code>libcudart</code>) required at execution time, but <code>nvcc</code> and headers are not.</li><li>Installation paths:<ul><li>Linux: <code>/usr/local/cuda/</code> (default)</li><li>Windows: <code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.x\</code></li></ul></li></ul><p><strong>NVIDIA GPU Driver</strong>:</p><ul><li>Operating system-level driver software managing GPU hardware.</li><li>Provides <sup>OS</sup>kernel modules (<code>nvidia.ko</code> on Linux, <sup>OS</sup>kernel driver on Windows) and user-space library (<code>libcuda.so</code> on Linux/macOS, <code>nvcuda.dll</code> on Windows).</li><li>Must be installed on all machines running CUDA applications.</li><li>Supports a range of CUDA Runtime API versions via forward compatibility (newer driver supports older runtime versions).</li><li><strong>Note:</strong> NVIDIA deprecated CUDA support for macOS as of CUDA 10.2 (2019). Modern CUDA development targets Linux and Windows.</li></ul><h3 id="layered-architecture-model">Layered architecture model</h3><p>The CUDA stack separates concerns between application-facing and system-facing layers:</p><ul><li><strong>Frontend (application layer):</strong> <code>libcudart.so</code> + application code. Provides high-level Runtime API (<code>cudaMalloc</code>, <code>cudaMemcpy</code>, etc.). Bundled with or linked by applications.</li><li><strong>Backend (system layer):</strong> <code>libcuda.so</code> + GPU driver (<code>nvidia.ko</code>). Provides low-level Driver API and hardware management. Installed system-wide, must be present at execution time.</li></ul><p>This separation allows applications to use a consistent high-level API while the backend handles hardware-specific details. <code>libcudart</code> translates Runtime API calls into Driver API calls, which <code>libcuda</code> executes via the <sup>OS</sup>kernel driver.</p><h3 id="build-time-vs-execution-time-components">Build-time vs. execution-time components</h3><p><strong>Note on terminology:</strong> Execution-time refers to when the application runs (at runtime). This is distinct from the Runtime API (<code>libcudart</code>), which is a specific CUDA library required at execution-time.</p><table><thead><tr><th>Component</th><th>Build-time (compilation)</th><th>Execution-time (at runtime)</th></tr></thead><tbody><tr><td><code>nvcc</code></td><td>Required to compile <sup>CUDA</sup>kernel code</td><td>Not required</td></tr><tr><td>CUDA headers (<code>cuda.h</code>, etc.)</td><td>Required</td><td>Not required</td></tr><tr><td><code>libcudart</code> (Runtime API)</td><td>Required to link application</td><td>Required to run application</td></tr><tr><td><code>libcuda</code> (Driver API)</td><td>Not directly used</td><td>Required (system-wide)</td></tr><tr><td>GPU driver (<code>nvidia.ko</code>)</td><td>Not required</td><td>Required</td></tr></tbody></table><p><strong>Example:</strong> PyTorch compilation:</p><ul><li>PyTorch is compiled against CUDA Toolkit <code>12.1</code> (meaning the build process links to and uses headers from toolkit <code>12.1</code>).</li><li>At build time: <code>nvcc</code>, headers, and <code>libcudart</code> from toolkit <code>12.1</code> are used.</li><li>At execution time: PyTorch bundles or depends on <code>libcudart.so</code> (often statically linked or packaged), and calls <code>libcuda.so</code> provided by the system’s GPU driver.</li><li>The system must have a GPU driver that supports CUDA Driver API version ≥ the version PyTorch expects (≥ <code>12.1</code> in this example).</li></ul><h2 id="version-semantics">Version semantics</h2><p>The CUDA ecosystem has multiple independent versioning schemes. Each version number measures a different aspect of the system. Conflating these versions is a primary source of confusion.</p><h3 id="compute-capability">Compute capability</h3><p>Compute capability (CC) defines the GPU’s instruction set and hardware features. This is a property of the GPU hardware itself, not software:</p><ul><li>Format: <code>X.Y</code> where <code>X</code> = major version, <code>Y</code> = minor version (e.g., <code>8.0</code>, <code>9.0</code>).</li><li>Determined by GPU hardware: RTX 4090 has CC <code>8.9</code>, H100 has CC <code>9.0</code>.</li><li>Query compute capability via <code>nvidia-smi</code> or <code>cudaGetDeviceProperties()</code>.</li></ul><p>GPU code can be compiled to two forms:</p><ul><li><strong>SASS (Shader Assembly):</strong> GPU-specific machine code compiled for a specific compute capability. Executable directly on hardware matching that CC, but not portable across different CCs.</li><li><strong>PTX (Parallel Thread Execution):</strong> NVIDIA’s virtual instruction set architecture (ISA) and intermediate representation. Platform-independent bytecode that can be JIT-compiled (Just-In-Time compiled) by the driver at execution time to SASS for any supported GPU architecture.</li></ul><p><strong>Binary compatibility rules:</strong></p><p>SASS (compiled machine code) has strict compatibility constraints:</p><ul><li><strong>Binary compatibility is NOT guaranteed between different compute capabilities.</strong> SASS compiled for CC <code>8.0</code> will generally NOT run on CC <code>8.6</code> hardware, even though both are major version 8.x. Each compute capability may have different instruction encoding.</li><li><strong>SASS cannot run on older hardware:</strong> SASS for CC <code>8.0</code> will NOT run on CC <code>7.5</code> hardware (older hardware lacks required instructions).</li><li><strong>SASS cannot run across major versions:</strong> SASS for CC <code>8.0</code> will NOT run on CC <code>9.0</code> hardware (different major version, different ISA).</li></ul><p>PTX (intermediate representation) provides forward compatibility:</p><ul><li><strong>PTX is portable across compute capabilities:</strong> PTX compiled for CC <code>8.0</code> can be JIT-compiled by the driver at execution time to native SASS for any supported GPU architecture (CC <code>8.6</code>, <code>8.9</code>, <code>9.0</code>, etc.).</li><li><strong>Requirements:</strong> The binary must include PTX, and the driver must support the target GPU architecture.</li><li><strong>Performance consideration:</strong> JIT compilation incurs a one-time cost at first <sup>CUDA</sup>kernel launch. Pre-compiled SASS avoids this cost.</li><li><strong>Recommendation:</strong> Include both SASS for known target architectures and PTX for forward compatibility with future GPUs.</li></ul><h3 id="gpu-driver-version">GPU driver version</h3><p>Format (Linux): <code>R.M.P</code> (e.g., <code>535.104.05</code>, <code>550.54.15</code>).</p><ul><li>R: Major release (corresponds to CUDA Driver API major version support)</li><li>M: Minor release</li><li>P: Patch</li></ul><p>Format (Windows): Display driver uses different versioning (e.g., <code>31.0.15.3623</code>), but CUDA components report similar <code>R.M</code> versioning.</p><p>Each driver version supports a maximum CUDA Driver API version. For example:</p><ul><li>Driver <code>535.x</code> supports CUDA Driver API <code>12.2</code></li><li>Driver <code>550.x</code> supports CUDA Driver API <code>12.4</code></li></ul><p><strong>Critical:</strong> Driver version determines maximum CUDA Driver API version, which must be ≥ Runtime API version used by applications.</p><p>Query driver version and maximum supported CUDA Driver API version via <code>nvidia-smi</code>.</p><h3 id="cuda-toolkit-version">CUDA Toolkit version</h3><p>Format: <code>X.Y.Z</code> (e.g., <code>12.1.0</code>, <code>12.4.1</code>).</p><ul><li>Corresponds to the toolkit package version installed during development.</li><li>Determines <code>nvcc</code> version, <code>libcudart</code> version, and available API features.</li><li>Query toolkit version via <code>nvcc --version</code> (requires toolkit installation).</li></ul><h3 id="cuda-runtime-api-version">CUDA Runtime API version</h3><ul><li>The API version supported by <code>libcudart</code>.</li><li>Typically matches toolkit version (toolkit <code>12.1</code> provides <code>libcudart</code> with Runtime API <code>12.1</code>).</li><li>Applications may bundle a specific <code>libcudart</code> version.</li><li>Query runtime API version via <code>cudaRuntimeGetVersion()</code> in application code.</li></ul><h3 id="cuda-driver-api-version">CUDA Driver API version</h3><ul><li>The API version provided by <code>libcuda.so</code>.</li><li>Determined by GPU driver version.</li><li>Must be ≥ the Runtime API version used by the application.</li><li>Query Driver API version via <code>cudaDriverGetVersion()</code> in application code, or via <code>nvidia-smi</code> (shown as “CUDA Version”).</li></ul><h3 id="pytorch-cuda-versions">PyTorch CUDA versions</h3><p>When PyTorch reports CUDA version, it refers to:</p><ul><li><strong>Build-time toolkit version:</strong> The CUDA Toolkit version PyTorch was compiled against (the build process linked to this version). Queried via <code>torch.version.cuda</code> (e.g., <code>&#34;12.1&#34;</code>).</li><li><strong>Runtime driver version:</strong> The CUDA Driver API version available on the system at runtime. Queried via <code>torch.cuda.is_available()</code> and driver checks.</li></ul><p>PyTorch may be compiled against toolkit <code>12.1</code> (linked at build time) but run on a system with driver supporting CUDA Driver API <code>12.4</code>. This is valid as long as Driver API version ≥ toolkit’s CUDA version (12.4 ≥ 12.1).</p><h2 id="version-compatibility">Version compatibility</h2><p>Version compatibility in CUDA follows specific rules. Understanding these rules is critical for ensuring applications run correctly across different systems.</p><p><strong>Terminology note:</strong> Compatibility is described from the perspective of the backend (driver + GPU hardware). “Forward compatible” means the backend can work with frontends built against older toolkit versions (example: driver version 12.4 works with libcudart from toolkit 12.1). “Backward compatible” means the backend can work with frontends built against newer toolkit versions (example: driver version 12.1 working with libcudart from toolkit 12.4). CUDA drivers are forward compatible (support older frontend versions) but not backward compatible (cannot support newer frontend versions).</p><h3 id="forward-compatibility-older-frontend-on-newer-backend">Forward compatibility: older frontend on newer backend</h3><p>CUDA maintains forward compatibility in specific dimensions:</p><p><strong>Driver API is forward-compatible with Runtime API:</strong></p><ul><li>A driver supporting CUDA Driver API <code>12.4</code> can run applications built with Runtime API <code>12.1</code>, <code>12.2</code>, <code>12.3</code>, or <code>12.4</code>.</li><li>Newer drivers support older runtime versions.</li><li>Applications run on systems with newer drivers without recompilation.</li></ul><p><strong>PTX provides forward compatibility across compute capabilities:</strong></p><ul><li>PTX compiled for CC <code>8.0</code> can be JIT-compiled by the driver to run on CC <code>8.6</code>, <code>8.9</code>, or <code>9.0</code> hardware (across major version boundaries).</li><li>Requirements: Binary must include PTX, and driver must support target GPU.</li><li>Applications can run on newer GPUs without recompilation, at the cost of JIT compilation overhead.</li></ul><h3 id="backward-compatibility-newer-frontend-on-older-backend">Backward compatibility: newer frontend on older backend</h3><p>Backward compatibility is not supported in CUDA:</p><p><strong>Driver API cannot support newer Runtime API:</strong></p><ul><li>A driver supporting CUDA Driver API <code>12.1</code> cannot run applications requiring Runtime API <code>12.4</code>.</li><li>Older drivers do not support newer runtime versions.</li><li><strong>Resolution:</strong> Upgrade GPU driver to support required Driver API version.</li></ul><p><strong>SASS cannot run on older hardware:</strong></p><ul><li>SASS compiled for CC <code>8.0</code> will not run on CC <code>7.5</code> hardware (older GPU lacks instructions).</li><li><strong>Resolution:</strong> Recompile for older compute capability, or include PTX for JIT compilation.</li></ul><p><strong>SASS is not portable across compute capabilities:</strong></p><ul><li>SASS compiled for CC <code>8.0</code> will generally not run on CC <code>8.6</code> or <code>9.0</code> hardware.</li><li><strong>Resolution:</strong> Include PTX in binary for forward compatibility, or compile SASS for all target architectures.</li></ul><h3 id="compatibility-requirements">Compatibility requirements</h3><p>For a CUDA application to execute successfully, two independent conditions must be satisfied:</p><p><strong>Condition 1: API version compatibility</strong></p><pre tabindex="0"><code>Driver API version ≥ Runtime API version
</code></pre><p>The Driver API version (provided by <code>libcuda.so</code>, determined by GPU driver) must be greater than or equal to the Runtime API version (provided by <code>libcudart</code>, bundled with or linked by application).</p><p><strong>Condition 2: GPU code availability</strong></p><p>At least one of the following must be true:</p><pre tabindex="0"><code>Binary contains SASS for GPU&#39;s compute capability
  OR
Binary contains PTX AND driver supports JIT compilation for GPU&#39;s architecture
</code></pre><p>The application binary must contain executable GPU code. This can be either:</p><ul><li>Pre-compiled SASS matching the GPU’s compute capability (fastest execution, no JIT overhead)</li><li>PTX intermediate representation that the driver can JIT-compile to SASS (enables forward compatibility, incurs one-time JIT cost)</li></ul><p><strong>Common failure modes:</strong></p><ul><li><code>cudaErrorInsufficientDriver</code>: Condition 1 violated (Driver API version &lt; Runtime API version)</li><li><code>cudaErrorNoKernelImageForDevice</code>: Condition 2 violated (no SASS or PTX available for GPU)</li></ul><p>Different tools report different version numbers. Understanding what each tool measures is essential for diagnosing compatibility issues.</p><h3 id="nvidia-smi"><code>nvidia-smi</code></h3><p><code>nvidia-smi</code> (NVIDIA System Management Interface) queries the GPU driver and reports driver-related information.</p><p><strong>Reports:</strong></p><ul><li>GPU driver version (e.g., <code>535.104.05</code>)</li><li>Maximum supported CUDA Driver API version (e.g., <code>12.2</code>)</li><li>GPU model (e.g., “NVIDIA GeForce RTX 4090”)</li><li>Compute capability (can be queried with <code>nvidia-smi --query-gpu=compute_cap --format=csv</code>)</li></ul><p><strong>Does NOT report:</strong></p><ul><li>CUDA Toolkit version (toolkit may not be installed on the system)</li><li>Runtime API version used by applications</li><li><code>nvcc</code> compiler version</li></ul><p><strong>Example output:</strong></p><pre tabindex="0"><code>Driver Version: 535.104.05    CUDA Version: 12.2
</code></pre><ul><li><code>535.104.05</code>: GPU driver version installed on the system</li><li><code>12.2</code>: Maximum CUDA Driver API version this driver supports (applications requiring ≤ 12.2 can run)</li></ul><h3 id="nvcc---version"><code>nvcc --version</code></h3><p><strong>Reports:</strong></p><ul><li>CUDA Toolkit version installed (e.g., <code>12.1.0</code>).</li><li><code>nvcc</code> compiler version (matches toolkit).</li></ul><p><strong>Does not report:</strong></p><ul><li>GPU driver version.</li><li>Runtime API version currently in use.</li><li>Driver API version currently in use.</li></ul><p><strong>May not be available if:</strong></p><ul><li>CUDA Toolkit is not installed (only driver installed).</li><li>Application bundles <code>libcudart</code> but not full toolkit.</li><li>Running in a container without toolkit (runtime-only image).</li></ul><h3 id="torchversioncuda"><code>torch.version.cuda</code></h3><p><strong>Reports:</strong></p><ul><li>CUDA Toolkit version PyTorch was compiled against (e.g., <code>&#34;12.1&#34;</code>).</li></ul><p><strong>Does not report:</strong></p><ul><li>Driver version.</li><li>Runtime driver API version available on the system.</li></ul><h3 id="torchcudais_available"><code>torch.cuda.is_available()</code></h3><p><strong>Reports:</strong></p><ul><li>Whether PyTorch can access a CUDA-capable GPU.</li><li>Requires compatible driver and runtime.</li></ul><p>Returns boolean indicating CUDA availability. Failure indicates version mismatch or missing driver.</p><h3 id="cudaruntimegetversion-and-cudadrivergetversion"><code>cudaRuntimeGetVersion()</code> and <code>cudaDriverGetVersion()</code></h3><p>Queried programmatically in application code:</p><ul><li><code>cudaRuntimeGetVersion()</code>: Runtime API version (from <code>libcudart</code>).</li><li><code>cudaDriverGetVersion()</code>: Driver API version (from <code>libcuda</code>).</li></ul><p><strong>Example:</strong></p><div><pre tabindex="0"><code data-lang="c"><span>int</span> runtimeVersion, driverVersion;
cudaRuntimeGetVersion(<span>&amp;</span>runtimeVersion);  <span>// e.g., 12010 (12.1)
</span><span></span>cudaDriverGetVersion(<span>&amp;</span>driverVersion);    <span>// e.g., 12040 (12.4)
</span></code></pre></div><h2 id="compilation-and-execution-model">Compilation and execution model</h2><h3 id="compilation-pipeline">Compilation pipeline</h3><p>When compiling CUDA code:</p><ol><li><p><strong>Source code (<code>.cu</code> file):</strong> Contains <sup>CUDA</sup>kernel definitions (<code>__global__</code> functions) and host code.</p></li><li><p><strong><code>nvcc</code> compilation:</strong></p><ul><li>Separates device code (GPU) from host code (CPU).</li><li>Device code compiled to PTX (intermediate representation) and/or SASS (GPU machine code) for specified compute capabilities.</li><li>Host code compiled with host compiler (e.g., <code>g++</code>, <code>cl.exe</code> on Windows).</li></ul></li><li><p><strong>Linking:</strong></p><ul><li>Object files linked with <code>libcudart</code> (Runtime API).</li><li>Resulting binary contains host code and embedded GPU code (PTX/SASS).</li></ul></li></ol><p><strong>Compute capability targeting:</strong> <code>nvcc</code> uses <code>-arch</code> and <code>-code</code> flags:</p><ul><li><code>-arch=compute_XY</code>: Sets the virtual architecture (PTX feature level). Determines which CUDA features are available during compilation.</li><li><code>-code=sm_XY</code>: Generates SASS (native machine code) for specific GPU architecture CC X.Y.</li><li><code>-code=compute_XY</code>: Embeds PTX for CC X.Y in the binary for forward compatibility.</li><li>Multiple <code>-code</code> targets can be comma-separated.</li></ul><p><strong>Default behavior:</strong> If only <code>-arch=compute_XY</code> is specified without <code>-code</code>, <code>nvcc</code> implicitly generates both <code>sm_XY</code> SASS and <code>compute_XY</code> PTX for that architecture.</p><p><strong>Best practice:</strong> Always specify both <code>-arch</code> (virtual architecture) and <code>-code</code> (real architectures). While <code>-code</code> can be used without <code>-arch</code>, <code>nvcc</code> will infer the PTX level, which may not be the intended behavior.</p><p><strong>Example:</strong></p><div><pre tabindex="0"><code data-lang="bash">nvcc -arch<span>=</span>compute_80 -code<span>=</span>sm_80,sm_86,sm_89,compute_80 kernel.cu -o app
</code></pre></div><p>This generates four outputs:</p><ul><li>SASS for CC <code>8.0</code> (A100), <code>8.6</code> (RTX 3090/3080), <code>8.9</code> (RTX 4090/4080)</li><li>PTX for CC <code>8.0</code> for forward compatibility with future GPUs</li></ul><p>At execution time:</p><ul><li>On A100 (CC 8.0): Loads sm_80 SASS directly</li><li>On RTX 3090 (CC 8.6): Loads sm_86 SASS directly</li><li>On RTX 4090 (CC 8.9): Loads sm_89 SASS directly</li><li>On H100 (CC 9.0): No matching SASS, so driver JIT-compiles PTX for CC 8.0 to SASS for CC 9.0</li></ul><h3 id="execution-model">Execution model</h3><p>When the application executes:</p><ol><li>Application calls <code>cudaMalloc</code>, <code>cudaMemcpy</code>, <code>cudaLaunchKernel</code>, etc. (Runtime API).</li><li><code>libcudart</code> translates these to Driver API calls (<code>cuMemAlloc</code>, <code>cuMemcpyHtoD</code>, <code>cuLaunchKernel</code>).</li><li><code>libcuda.so</code> communicates with <sup>OS</sup>kernel driver via <code>ioctl</code> syscalls.</li><li>Driver schedules <sup>CUDA</sup>kernel execution on GPU hardware.</li><li>GPU executes <sup>CUDA</sup>kernel (SASS instructions), processes data, returns results.</li></ol><p><strong>What is being transmitted:</strong> When <sup>CUDA</sup>kernel code is “launched,” the host does not send C++ source code to the GPU. Instead:</p><ul><li>Pre-compiled GPU machine code (SASS) or PTX is embedded in the application binary (created at compile time by <code>nvcc</code>).</li><li>At application startup, the driver loads the appropriate code into GPU memory:<ul><li>If SASS matching the GPU’s compute capability exists, driver loads SASS directly.</li><li>If only PTX is available, driver JIT-compiles PTX to SASS for the GPU’s architecture, then loads it.</li></ul></li><li>At <sup>CUDA</sup>kernel launch, the host specifies:<ul><li>Grid/block dimensions (how many thread blocks, how many threads per block)</li><li><sup>CUDA</sup>kernel parameters (function arguments passed to <sup>CUDA</sup>kernel)</li><li>Shared memory size</li></ul></li><li>The GPU’s hardware scheduler executes the SASS code across thread blocks.</li></ul><p>The execution model is not RPC (Remote Procedure Call) in the network sense, but shares conceptual similarities:</p><ul><li><strong>Command submission:</strong> Host enqueues commands (<sup>CUDA</sup>kernel launch, memory transfer) into a command buffer.</li><li><strong>Driver interpretation:</strong> Driver translates commands into GPU-specific operations.</li><li><strong>Asynchronous execution:</strong> GPU executes independently; host can continue or synchronize via <code>cudaDeviceSynchronize()</code>.</li></ul><p>The programming model resembles remote execution: host code invokes operations on a separate processor (GPU) with its own memory space and instruction set.</p><h2 id="version-mismatch-scenarios">Version mismatch scenarios</h2><h3 id="scenario-1-runtime-version--driver-version">Scenario 1: Runtime version &gt; Driver version</h3><p><strong>Setup:</strong></p><ul><li>GPU driver supports CUDA Driver API <code>12.1</code>.</li><li>Application built with Runtime API <code>12.4</code>.</li></ul><p><strong>Result:</strong></p><ul><li>Application calls <code>libcudart</code> (Runtime API version <code>12.4</code>).</li><li><code>libcudart</code> calls <code>libcuda.so</code> (Driver API version <code>12.1</code>, provided by GPU driver).</li><li><code>libcuda.so</code> does not support newer Driver API features required by Runtime API <code>12.4</code>.</li><li><strong>Failure:</strong> Application crashes or returns <code>cudaErrorInsufficientDriver</code>.</li></ul><p><strong>Possible solutions:</strong> Upgrade GPU driver to version supporting CUDA Driver API ≥ <code>12.4</code>.</p><h3 id="scenario-2-compiled-compute-capability--gpu-compute-capability">Scenario 2: Compiled compute capability &gt; GPU compute capability</h3><p><strong>Setup:</strong></p><ul><li>Code compiled for CC <code>8.0</code> (e.g., A100).</li><li>Running on CC <code>7.5</code> hardware (e.g., RTX 2080 Ti).</li></ul><p><strong>Result:</strong></p><ul><li>Driver attempts to load <sup>CUDA</sup>kernel code for CC <code>8.0</code>.</li><li>GPU does not support CC <code>8.0</code> instructions.</li><li><strong>Failure:</strong> <code>cudaErrorNoKernelImageForDevice</code> or similar.</li></ul><p><strong>Possible solutions:</strong></p><ul><li>Recompile code for CC <code>7.5</code> (<code>-arch=compute_75</code>).</li><li>Or include PTX in binary for JIT compilation (<code>-arch=compute_75</code> without <code>sm_</code> code).</li></ul><h3 id="scenario-3-missing-ptx-for-forward-compatibility">Scenario 3: Missing PTX for forward compatibility</h3><p><strong>Setup:</strong></p><ul><li>Code compiled with <code>-code=sm_80</code> (SASS only for CC <code>8.0</code>, no PTX included).</li><li>Running on newer GPU with CC <code>9.0</code> (e.g., H100).</li></ul><p><strong>Result:</strong></p><ul><li>Binary contains only SASS for CC <code>8.0</code>.</li><li>No PTX available for JIT compilation.</li><li>SASS for CC <code>8.0</code> is incompatible with CC <code>9.0</code> (different major version, different ISA).</li><li><strong>Failure:</strong> <code>cudaErrorNoKernelImageForDevice</code> - no compatible <sup>CUDA</sup>kernel image found in binary.</li></ul><p><strong>Possible solutions:</strong></p><ul><li>Recompile with PTX included: <code>-arch=compute_80 -code=sm_80,compute_80</code>.</li><li>The <code>compute_80</code> in <code>-code</code> ensures PTX is embedded in the binary.</li><li>At execution time on CC <code>9.0</code> hardware, driver JIT-compiles the PTX to SASS for CC <code>9.0</code>.</li></ul><h3 id="scenario-4-pytorch-toolkit-version-vs-driver-version">Scenario 4: PyTorch toolkit version vs. driver version</h3><p><strong>Setup:</strong></p><ul><li>PyTorch compiled against CUDA Toolkit <code>12.1</code>.</li><li>System driver supports CUDA <code>12.4</code>.</li></ul><p><strong>Result:</strong></p><ul><li>PyTorch bundles or links <code>libcudart</code> (version <code>12.1</code>).</li><li>Driver provides <code>libcuda.so</code> (version <code>12.4</code>).</li><li><code>12.4 ≥ 12.1</code>: <strong>Success.</strong> No issue.</li></ul><p><strong>Setup (reverse):</strong></p><ul><li>PyTorch compiled against CUDA Toolkit <code>12.4</code>.</li><li>System GPU driver supports CUDA Driver API <code>12.1</code>.</li></ul><p><strong>Result:</strong></p><ul><li>PyTorch runtime calls require Driver API <code>12.4</code> features.</li><li><code>libcuda.so</code> (Driver API version <code>12.1</code>) does not support them.</li><li><strong>Failure:</strong> <code>cudaErrorInsufficientDriver</code> or runtime error.</li></ul><p><strong>Possible solutions:</strong> Upgrade GPU driver to version supporting CUDA Driver API ≥ <code>12.4</code>.</p><h3 id="scenario-5-multiple-cuda-toolkits-installed">Scenario 5: Multiple CUDA toolkits installed</h3><p><strong>Setup:</strong></p><ul><li>System has toolkit <code>12.1</code> at <code>/usr/local/cuda-12.1</code>.</li><li>System has toolkit <code>12.4</code> at <code>/usr/local/cuda-12.4</code>.</li><li><code>PATH</code> points to <code>/usr/local/cuda-12.1/bin</code>.</li><li>Application compiled with toolkit <code>12.4</code>.</li></ul><p><strong>Result:</strong></p><ul><li><code>nvcc --version</code> reports <code>12.1</code> (from <code>PATH</code>).</li><li>Application uses <code>libcudart</code> from toolkit <code>12.4</code>.</li><li>Version reported by <code>nvcc</code> does not match application runtime.</li></ul><p><strong>Note:</strong> <code>nvcc --version</code> reports the toolkit in <code>PATH</code>, not the toolkit used by applications. Applications may bundle or link against a different toolkit version.</p><p><strong>Possible solutions:</strong> Check application’s linked libraries (<code>ldd ./app</code>) to determine actual <code>libcudart</code> version.</p><h3 id="scenario-6-docker-container-with-cuda-runtime-but-no-toolkit">Scenario 6: Docker container with CUDA runtime but no toolkit</h3><p><strong>Setup:</strong></p><ul><li>Container image based on <code>nvidia/cuda:12.1-runtime</code>.</li><li>Application requires <code>nvcc</code> to compile <sup>CUDA</sup>kernel code at runtime.</li></ul><p><strong>Result:</strong></p><ul><li>Runtime image includes <code>libcudart</code>, but not <code>nvcc</code> or headers.</li><li><strong>Failure:</strong> <code>nvcc</code> not found.</li></ul><p><strong>Possible solutions:</strong></p><ul><li>Use <code>nvidia/cuda:12.1-devel</code> image, which includes full toolkit.</li><li>Or install toolkit separately in runtime image.</li></ul><p><strong>Note:</strong> Runtime vs. devel images:</p><ul><li><strong>Runtime (<code>-runtime</code>):</strong> Includes <code>libcudart</code> and libraries needed to run CUDA applications. Does not include <code>nvcc</code> or headers.</li><li><strong>Devel (<code>-devel</code>):</strong> Includes full toolkit (<code>nvcc</code>, headers, libraries) for compiling CUDA code.</li></ul><h3 id="scenario-7-static-vs-dynamic-linking-of-libcudart">Scenario 7: Static vs. dynamic linking of <code>libcudart</code></h3><p><strong>Setup:</strong></p><ul><li>Application statically links <code>libcudart_static.a</code> (toolkit <code>12.1</code>).</li><li>System driver supports CUDA <code>12.4</code>.</li></ul><p><strong>Result:</strong></p><ul><li>Application contains embedded <code>libcudart</code> code (version <code>12.1</code>).</li><li><code>libcudart</code> calls <code>libcuda.so</code> (version <code>12.4</code>).</li><li><code>12.4 ≥ 12.1</code>: <strong>Success.</strong></li></ul><p><strong>Setup (reverse):</strong></p><ul><li>Application statically links <code>libcudart_static.a</code> (toolkit <code>12.4</code>).</li><li>System driver supports CUDA <code>12.1</code>.</li></ul><p><strong>Result:</strong></p><ul><li>Embedded <code>libcudart</code> (version <code>12.4</code>) calls <code>libcuda.so</code> (version <code>12.1</code>).</li><li><code>12.1 &lt; 12.4</code>: <strong>Failure.</strong></li></ul><p><strong>Note:</strong> Static linking bundles <code>libcudart</code> into the application binary. The version used is determined at build time, not runtime. Dynamic linking allows runtime version selection via library search paths (<code>LD_LIBRARY_PATH</code> on Linux, <code>PATH</code> on Windows) or system libraries.</p><h2 id="summary-of-component-relationships">Summary of component relationships</h2><table><thead><tr><th>Component</th><th>Provided by</th><th>Location (Linux / Windows)</th><th>Purpose</th><th>Version</th></tr></thead><tbody><tr><td><sup>OS</sup>kernel driver</td><td>NVIDIA GPU driver</td><td><sup>OS</sup>kernel-space: <code>nvidia.ko</code> / <code>nvlddmkm.sys</code></td><td>Manage GPU hardware</td><td>Driver</td></tr><tr><td>Driver API library</td><td>NVIDIA GPU driver</td><td><code>/usr/lib/libcuda.so</code> / <code>System32\nvcuda.dll</code></td><td>Driver API (low-level)</td><td>Driver</td></tr><tr><td>Runtime library</td><td>CUDA Toolkit</td><td><code>/usr/local/cuda/lib64</code> / <code>CUDA\v12.x\bin</code></td><td>Runtime API (high-level)</td><td>Toolkit</td></tr><tr><td><code>nvcc</code></td><td>CUDA Toolkit</td><td><code>/usr/local/cuda/bin</code> / <code>CUDA\v12.x\bin</code></td><td>Compile <sup>CUDA</sup>kernel</td><td>Toolkit</td></tr><tr><td>Headers</td><td>CUDA Toolkit</td><td><code>/usr/local/cuda/include</code> / <code>CUDA\v12.x\include</code></td><td>Build-time API definitions</td><td>Toolkit</td></tr><tr><td>Math libraries</td><td>CUDA Toolkit</td><td><code>/usr/local/cuda/lib64</code> / <code>CUDA\v12.x\lib</code></td><td>Optimized GPU operations</td><td>Toolkit</td></tr></tbody></table><h2 id="practical-guidelines">Practical guidelines</h2><h3 id="for-application-developers">For application developers</h3><ul><li><strong>Specify minimum driver version:</strong> Document required CUDA Driver API version.</li><li><strong>Bundle or specify runtime version:</strong> If statically linking <code>libcudart</code>, ensure driver compatibility. If dynamically linking, document required <code>libcudart</code> version.</li><li><strong>Compile for multiple compute capabilities:</strong> Use <code>-arch</code> and <code>-code</code> flags to support a range of GPUs. Include PTX for forward compatibility.</li><li><strong>Check versions at runtime:</strong> Use <code>cudaDriverGetVersion()</code> and <code>cudaRuntimeGetVersion()</code> to verify compatibility.</li></ul><h3 id="for-end-users">For end users</h3><ul><li><strong>Install appropriate GPU driver:</strong> Ensure GPU driver supports CUDA Driver API version ≥ application’s runtime requirement.</li><li><strong>Toolkit installation optional at runtime:</strong> Most applications do not require full toolkit (no need for <code>nvcc</code>). Only <code>libcudart</code> and GPU driver are necessary.</li><li><strong>Check compatibility:</strong> Run <code>nvidia-smi</code> to verify GPU driver version and supported CUDA Driver API version.</li></ul><h3 id="for-pytorch-users">For PyTorch users</h3><ul><li><strong>Build-time toolkit version (<code>torch.version.cuda</code>):</strong> The CUDA Toolkit version PyTorch was compiled against (linked at build time). Does not need to match system toolkit.</li><li><strong>Runtime driver version:</strong> System GPU driver must support CUDA Driver API ≥ PyTorch’s toolkit version.</li><li><strong>Example:</strong><ul><li>PyTorch built with CUDA Toolkit <code>12.1</code>: System GPU driver must support CUDA Driver API ≥ <code>12.1</code>.</li><li>System GPU driver supports CUDA Driver API <code>12.4</code>: Can run PyTorch built with CUDA Toolkit <code>12.1</code>, <code>12.2</code>, <code>12.3</code>, or <code>12.4</code>.</li></ul></li></ul><p><strong>Note:</strong> TensorFlow follows the same compatibility model. Check release notes for build toolkit version.</p><h3 id="for-dockercontainer-users">For Docker/container users</h3><ul><li><strong>Runtime-only images (<code>-runtime</code>):</strong> Include <code>libcudart</code> and libraries. Suitable for running pre-compiled CUDA applications. Do not include <code>nvcc</code>.</li><li><strong>Development images (<code>-devel</code>):</strong> Include full CUDA Toolkit (<code>nvcc</code>, headers, libraries). Required for compiling CUDA code.</li><li><strong>NVIDIA Container Toolkit:</strong> Ensures container has access to host GPU and GPU driver. GPU driver version on host determines maximum CUDA Driver API version available to container.</li></ul><h2 id="conclusion">Conclusion</h2><p>CUDA’s architecture is a layered system with distinct components serving different roles at build-time and execution-time. Rigorous understanding requires:</p><ol><li><p><strong>Disambiguation:</strong> “CUDA” refers to architecture, ISA, language, toolkit, or runtime. “Driver” refers to <sup>OS</sup>kernel-space driver (<code>nvidia.ko</code>) or user-space Driver API library (<code>libcuda.so</code>). “Kernel” refers to operating system kernel (<sup>OS</sup>kernel) or GPU function (<sup>CUDA</sup>kernel).</p></li><li><p><strong>Layering:</strong> <code>libcudart</code> (frontend, Runtime API, application-facing) calls <code>libcuda</code> (backend, Driver API, system-facing), which calls <sup>OS</sup>kernel driver (<code>nvidia.ko</code>), which manages GPU hardware.</p></li><li><p><strong>Versioning:</strong> Driver API version must be ≥ Runtime API version. GPU must have SASS for its compute capability OR binary must include PTX that driver can JIT-compile.</p></li><li><p><strong>Build vs. execution:</strong> Toolkit (including <code>nvcc</code>) required at build-time. Execution requires only <code>libcudart</code> (bundled or linked) and system GPU driver.</p></li></ol><p>Version mismatches produce specific failure modes: insufficient driver errors, missing <sup>CUDA</sup>kernel images, or unsupported API calls. This systematic understanding clarifies why <code>nvidia-smi</code>, <code>nvcc --version</code>, and <code>torch.version.cuda</code> report different numbers, what each number signifies, and how to diagnose version incompatibilities.</p></div></article></main></div></div>
  </body>
</html>
