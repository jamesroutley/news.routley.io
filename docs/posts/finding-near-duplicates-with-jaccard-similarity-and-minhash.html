<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.nelhage.com/post/fuzzy-dedup/">Original</a>
    <h1>Finding near-duplicates with Jaccard similarity and MinHash</h1>
    
    <div id="readability-page-1" class="page"><div>
  <p>Suppose we have a large collection of documents, and we wish you identify which documents are <strong>approximately</strong> the same as each other. For instance, we may have crawled the web over some period of time, and expect to have fetched the â€œsame pageâ€ several times, but to see slight differences in metadata, or that we have several revisions of a page following small edits.</p>
<p>In this post I want to explore the method of approximate deduplication via Jaccard similarity and the MinHash approximation trick. This is a commonly-used approach to this problem (e.g. the <a href="https://arxiv.org/pdf/2005.14165">GPT-3 paper</a> describes using it as part of their dataset preparation pipeline), but one I had not encountered until recently, and which I find to be pretty interesting.</p>

<figure><img src="https://dynomight.net/post/fuzzy-dedup/gpt-3-dedup.png" width="1172" height="156" alt="To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark&amp;rsquo;s MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%" title="Excerpt from the GPT-3 paper describing fuzzy deduplication"/>

<figcaption>Excerpt from the GPT-3 paper describing fuzzy deduplication</figcaption>

</figure>

<p>Our approach to approximate deduplication will be to define a notion of â€œsimilarityâ€ between any two documents, and then to search for pairs where their similarity value is above some threshold. So if we have some universe of possible documents \(U\), we might define a similarity measure between pairs of documents:
$$S: U \times U \rightarrow [0,1]$$
and consider two documents â€œapproximate duplicatesâ€ if \(S(A,B) \geq S_\textrm{crit}\).</p>
<p>Itâ€™s worth noticing that this definition is not in general transitive: we may well have three documents \(A, B, C\) such that \(S(A,B)\geq{}S_\textrm{crit}\) and \(S(B,C) \geq{} S_\textrm{crit}\) but \(S(A,C) &lt; S_\textrm{crit}\). That means that â€œapproximately identicalâ€ is not an <a href="https://en.wikipedia.org/wiki/Equivalence_relation">equivalence relation</a>, and is part of the reason that approximate deduplication is trickier to reason about, and to perform at scale, compared to finding exact matches.</p>
<h2 id="jaccard-similarity">Jaccard similarityÂ <a href="#jaccard-similarity"><i>	ğŸ”—ï¸</i></a> </h2>
<p>One measure of similarity widely used across several domains, including large-scale text processing is the <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a>, also known as the Jaccard similarity coefficient.</p>
<p>The Jaccard index is a function that compares <strong>sets</strong>, and characterizes the similarity of two finite sets as the ratio of their overlap to the size of their union:</p>
<p>$$J(A,B) = \frac{|A\cap{}B|}{|A\cup{}B|}$$</p>
<p>I find this calculation makes some intuitive sense: if two sets are similar, they should have <strong>mostly</strong> the same elements. This means the sets are of similar sizes, and their union is only slightly larger, and their intersection only slightly smaller. If the sets are very different, or of very different sizes, then the union will be large and the intersection small.</p>
<p>It has two very natural limit points, which define its range: For two disjoint sets, the numerator \(|A\cap{}B|\) is zero, and the index goes to zero. But if the sets are identical, \(A\cap{}B = A\cup{}B = A = B\), and the Jaccard similarity is 1.</p>
<p>Note that Jaccard similarity operates on sets, but weâ€™re starting with documents (typically represented as Unicode strings). Iâ€™ll return at the end to how we can turn textual documents into sets, but for now Iâ€™ll just assume that weâ€™ve done so. Iâ€™ll refer to these sets as â€œfeature sets,â€ where the individual elements are â€œfeaturesâ€ of the document.</p>

<p>We now have a <strong>definition</strong> of â€œapproximate similarityâ€: We convert documents into feature sets, and then search for sets with high Jaccard similarity.</p>
<p>For very small corpora, we could potentially apply that definition directly. However, considering every pair of documents scales as \(O(n^2)\) with the size of our corpus, which rapidly becomes infeasible.</p>
<p>For finding <strong>exact duplicates</strong>, we avoid the quadratic cost by hashing; we hash documents and group them by their hash value, which puts identical documents (and, with good probability, only identical documents) into the same hash bucket. We want to find a similar shortcut for <strong>approximate</strong> duplicates; in the language of the field, we want a <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">locality-sensitive hash</a>.</p>
<p>It turns out such techniques exist for Jaccard similarity! Letâ€™s see how it works.</p>
<h2 id="approximating-jaccard-similarity">Approximating Jaccard similarityÂ <a href="#approximating-jaccard-similarity"><i>	ğŸ”—ï¸</i></a> </h2>
<p>Weâ€™ll first consider the problem of approximating the Jaccard similarity between two documents. Weâ€™ll find an approximation that avoid examining the entire sets, and which only requires a small, fixed-size â€œsignature,â€ which we can precompute for each document independently. Then, weâ€™ll use the structure of that signature to find ways to group documents such that (with good probability) similar documents, and mostly-only similar documents, group together.</p>
<h3 id="minhash-signatures">MinHash signaturesÂ <a href="#minhash-signatures"><i>	ğŸ”—ï¸</i></a> </h3>
<p>Recall that the Jaccard similarity is the ratio of two sizes: the intersection and the union of our two input sets.</p>
<p>$$J(A,B) = \frac{|A\cap{}B|}{|A\cup{}B|}$$</p>
<p>To estimate a ratio of areas like this, one classic strategy is <strong>sampling</strong>. If we can generate random elements (uniformly in an appropriate sense), and we can query whether those elements are present in the two sides of the ratio, we can produce an empirical estimate which will approach the true value.</p>
<p>In this case, we know the union is at least as large as the intersection, so we want a uniformly-random sample from \(A\cup{}B\). Itâ€™s not clear how to do that given the sets themselves, but it turns out we can do so cheaply if weâ€™re allowed precomputation on each set!</p>
<ul>
<li>First, we make the problem apparently more complicated. Assume that features are integers in some finite range \(0 \leq f_i \leq F\), and then pick a random permutation on \(\mathbb{Z}_F\). Call that permutation \(P(x)\). Now, we can select a random element by selecting the feature in our set that has the smallest value under this permutation<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>:</li>
</ul>
<p>$$ x_{\textrm{random}} \leftarrow{} \argmin_{x\in{}A\cup{}B}{P(x)} $$</p>
<ul>
<li>Itâ€™s not really feasible to work with truly-random permutations, but we can <strong>approximate</strong> one using a good hash function. This also avoids the need for features to be represented as fixed-range integers; by accepting a miniscule risk of collision and storing only the hash values, we can map any reasonable featurespace into fixed-size hash values:</li>
</ul>
<p>$$ x_{\textrm{sig}} \leftarrow{} \min_{x\in{}A\cup{}B}{H(x)} $$</p>
<ul>
<li>Next, weâ€™ll exploit the fact that <code>min</code> is associative, and rewrite the above to pre-process each set individually:</li>
</ul><p>

\begin{align*}
a_{\textrm{min}} &amp;\leftarrow{} \min_{x\in{}A}H(x) \\
b_{\textrm{min}} &amp;\leftarrow{} \min_{x\in{}B}H(x) \\
x_{\textrm{sig}} &amp;\leftarrow{} \min(a_\textrm{min},b_\textrm{min})
\end{align*}


</p><p>Letâ€™s step back, and consider what weâ€™ve achieved. If we pick a good hash function on features, we can compute a â€œsignatureâ€ for <strong>each set,</strong> individually, consisting of the minimum hash value of all its features. Given any two sets, then, we can take the minimum of those signatures, and we have (the hash of) some element, drawn uniformly-at-random from their union.</p>
<p>We want to know whether that element is present in the intersection, or whether itâ€™s misssing from one side. But this construction also makes that trivial! We know that \(x_\textrm{sig}\) is the minimum hash value of any element in either set. Therefore, if it is present in, say, set \(A\), it must also be the minimum hash value in that set. But we know the minimum hash values of each set â€“ thatâ€™s precisely what we have!</p>
<p>Thus, we donâ€™t actually need to compute \(x_\textrm{sig}\); we can instead just ask whether \(a_\textrm{min} = b_\textrm{min}\)! For any two sets, this equality with hold with probability equal to \(J(A, B)\)!</p>
<h3 id="using-more-hash-functions">Using more hash functionsÂ <a href="#using-more-hash-functions"><i>	ğŸ”—ï¸</i></a> </h3>
<p>That probability is taken over the universe of permutations of \(\mathbb{Z}_F\) (aka, with some caveats, â€œover our choice of hash functionâ€). With a single hash function and a single min-hash, we only have a boolean estimate for each pair â€“ â€œequalâ€ or â€œnot equal.â€</p>
<p>We can improve on that by instead selecting \(k\) different hash functions from some appropriate hash family, and summarizing each document into a \(k\)-element vector:</p><p>

$$
A_\textrm{sig} =
\begin{pmatrix}
\displaystyle\min_{x\in{}A}H_1(x) &amp;
\displaystyle\min_{x\in{}A}H_2(x) &amp;
\cdots{} &amp;
\displaystyle\min_{x\in{}A}H_k(x)
\end{pmatrix}
$$


</p><p>Given two of these signatures, we can approximate the Jaccard similarity by counting how many hashes match:</p>
<p>$$
J(A,B) \approx{} \frac{1}{k}\sum_{i=1}^{k} (A_\textrm{sig}[i] = B_\textrm{sig}[i])
$$</p>
<p>One caveat to mention: the choice of the hash family function here is a bit subtle. We are attempting to approximate a random permutation over the universe of features, but the number of such permutations grows extremely quickly, and so our hash family will represent a tiny fraction of all <strong>possible</strong> permutations. We need to be sure that members of our hash family are not inappropriately correlated â€“ formally, the salient property here is referred to as <a href="https://en.wikipedia.org/wiki/MinHash#Practical_min-wise_independent_hash_functions">â€œmin-wise independenceâ€</a>. Fortunately, this problem is reasonably well-studied, and efficient solutions are available in the literature.</p>
<h2 id="comparing-all-documents">Comparing all documentsÂ <a href="#comparing-all-documents"><i>	ğŸ”—ï¸</i></a> </h2>
<p>Weâ€™ve now condensed each document into a \(k\)-element fingerprint of hash values, which allows efficient approximation of Jaccard similarities.</p>
<p>The next problem is to find approximate duplicates throughout our entire corpus â€“ documents with a high similarity â€“ <strong>without</strong> considering every pair of documents. As alluded to above, our strategy will be to define some set of keys that we can group documents by, and then only perform the full comparison within each group. We will aim to construct the grouping key so that similar documents group together with good probability, and dissimilar ones do not.</p>
<h3 id="using-the-full-signature">Using the full signatureÂ <a href="#using-the-full-signature"><i>	ğŸ”—ï¸</i></a> </h3>
<p>The simplest choice is to simply to use all \(k\) MinHash values together as a grouping key, and consider two documents â€œapproximate duplicatesâ€ iff all of their MinHash values match. Iâ€™m pretty sure is what the GPT-3 paper cited above means when they say â€œwe fuzzily deduplicated documents [â€¦] using Sparkâ€™s MinHashLSH implementation with 10 hashes.â€ They split each document into features, computed 10 MinHash values for each document (using 10 different hashes<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>), and then grouped documents by that 10-vector, and kept only one document per group.</p>
<p>The strongest virtue of this approach is its simplicity and efficiency. Grouping documents by a single high-cardinality bytestring is an efficient operation and easy to scale horizontally, and is offered as a basic primitive in essentially any data-processing toolkit (itâ€™s arguably <strong>the</strong> core primitive in <a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a>, taking the form of the â€œshuffleâ€ between the map and reduce stages).</p>
<p>How does this approach behave? For a single pair of documents, we expect each MinHash value to be equal with probability \(J(A,B)\), so we expect all 10 to match with \(p=J(A,B)^k\). For \(k=10\), hereâ€™s what that looks like:</p>


<p>
<img src="https://dynomight.net/post/fuzzy-dedup/probability-10-hashes.png" srcset="/post/fuzzy-dedup/probability-10-hashes@2x.png 2x" width="640" height="420" alt="Probability all 10 MinHashes match, as a function of similarity"/>
</p>


<p>As well as some quantiles:</p>
<table>
<thead>
<tr>
<th>p(all match)</th>
<th>1%</th>
<th>10%</th>
<th>25%</th>
<th>50%</th>
<th>75%</th>
<th>90%</th>
</tr>
</thead>
<tbody>
<tr>
<td>Jaccard</td>
<td>0.63</td>
<td>0.79</td>
<td>0.87</td>
<td>0.93</td>
<td>0.97</td>
<td>0.99</td>
</tr>
</tbody>
</table>
<p>We can see that documents with similarities below 0.6 or so will almost-never collide, and that the odds of matches become large around 0.95 or so. If weâ€™re primarily concerned about documents that are very close siblings, this approach may be sufficient. And in fact I suspect â€“ but havenâ€™t verified â€“ that in many corpora, we will encounter fairly bimodal Jaccard values â€“ two clusters, near 1 and 0. Unrelated documents have similarity close to 0, and similar documents will largely be â€œnearly-identicalâ€ â€“ e.g. two slight revisions of an article, or two copies of the same context with different timestamps or metadata.</p>
<p>Itâ€™s also worth noting that the \(J^{k}\) calculation holds for a <strong>single</strong> pair of documents. If we have many documents that are all similar, the pairwise probabilities are not at all independent. In practice, given many very-similar documents, theyâ€™re likely to end up hashed into at-most two or three buckets, and so we will find â€œalmost allâ€ of the duplication, in some sense.</p>
<h3 id="going-fuzzier">Going fuzzierÂ <a href="#going-fuzzier"><i>	ğŸ”—ï¸</i></a> </h3>
<p>(Note: this discussion is primarily sourced from <a href="http://infolab.stanford.edu/~ullman/mmds/booka.pdf">â€œMining of Massive Datasetsâ€</a> section 3.4. I havenâ€™t played with this strategy, and donâ€™t know whether or when itâ€™s used in practice, so far).</p>
<p>What if we want to detect â€œfuzzierâ€ duplicates? Perhaps after some empirical study, we determine that we want to find pairs with similarities above 0.8 or 0.7, instead of only â€œnear 1.â€</p>
<p>By using a subset of our \(k\) MinHash hashes as a grouping key, we can increase the likelihood of collisions at lower similarity values, and then compare the full signatures within each bucket to weed out false collisions. For instance, we might group by the first 4 MinHash values, and then â€“ within each colliding group â€“ use <strong>all</strong> of our MinHash values to estimate the trule similariy.</p>
<p>Using fewer hashes is helpful, but only so far; \(J^r\) will always be smaller than \(J\), and if we push \(r\) too small, the rate of spurious matches will become unacceptable.</p>
<p>What we can do instead is to generate <strong>multiple keys</strong> per document, and place each document into several buckets, one per key, using a different subset of MinHashes for each key. If we compute \(k=20\) hashes as our signature, we might place each document into \(b=4\) different buckets, using \(r=5\) hashes to construct each key, and then compare each pair within each bucket.</p>
<p>What are the odds that two documents end up hashed together in <strong>at least one</strong> bucket?</p>
<ul>
<li>The odds that two documents collide using a single key is \(J^r\)</li>
<li>So the odds they <strong>donâ€™t</strong> collide based on that key is \(1-J^r\)</li>
<li>So the odds that donâ€™t collide in <strong>any</strong> of the buckets is \(1-J^r)^b\)</li>
</ul>
<p>Thus, the odds that they collide at-least-once ends up as:</p>
<p>\[ p = 1 - (1-J^r)^b \]</p>
<p>For example, the above example â€“ using 4 groups of 5 hashes â€“ produces a probability curve like this:</p>


<p>
<img src="https://dynomight.net/post/fuzzy-dedup/probability-grouped-hashes.png" srcset="/post/fuzzy-dedup/probability-grouped-hashes@2x.png 2x" width="640" height="420" alt="Probability two documents collide in *any* bucket, for 4 groups of 5 hashes"/>
</p>


<p>The curve is not as steep as our earlier example, but weâ€™ve successfully shifted it to the left â€“ the odds of colliding become 50% at somewhere around \(J=0.7\).</p>
<p>It turns out, that for any choice of \(r\) and \(b\) greater than 1, the resulting curve is S-shaped in a broadly similar fashion, and so varying those values gives us a rich tradeoff space over sensitivity, recall, and performance costs.</p>

<p>Prior to working in AI, I believed myself to be relatively familiar with common algorithmic tricks, including most common <a href="https://www.cs.cornell.edu/content/sketching-algorithms">sketch</a> algorithms. But I had somehow never encountered MinHash, or even been aware that such algorithms (locality-sensitive hashing) existed or were practical!</p>
<p>I really enjoyed learning how this trick works and digging in. I hope this blog post introduces it to some more engineers for the first time, and/or that it helps fill in the gaps in someoneâ€™s understanding. I just love neat mathematical/algorithmic tricks!</p>

<p>While researching and writing this post, I realized that the core MinHash trick reminded me a bit of a classic, somewhat famous, sketch: <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>.</p>
<p>The core idea in HyperLogLog (going back to <a href="https://en.wikipedia.org/wiki/Flajolet%E2%80%93Martin_algorithm">a much older algorithm</a>) is to hash each element of a stream, and store a running maximum of â€œthe number of leading zerosâ€ in the resulting stream of hashes.</p>
<p>The algorithm is very different in the details, but thereâ€™s a clear similarity: In both cases, we use a hash function to map input elements into a uniform distribution, and then we compute a running extremum, which â€“ with appropriate calculation â€“ allows us to estimate some distributional property using only a constant-size summary of our input.</p>
<p>Indeed, I contend the algorithms are even more similar than they first appear: HyperLogLog counts leading zeros (zeros in the least-significant digits). However, weâ€™re assuming our hash function outputs uniform values in \([0, 2^N)\), and so we may as well reverse the bit order and count the number of zeros in the most-significant position. But given an N-bit number \(x\), the number of high-bit zeros is closely related to \(\log_2(x)\) â€“ the more leading zeros, the smaller \(x\). Thus, we can just as well think of think of HyperLogLog as computing a running minimum of \(log_2(H(x))\), as compared to MinHashâ€™s \(H(x)\).</p>
<p>In addition, it turns out that thereâ€™s a sense in which HyperLogLog and MinHash are (somewhat) dual: Given two HyperLogLog structures for two different sets, we can combine them, and estimate the size of their <strong>union</strong>. Given two MinHash structures for those sets, we can compare them and estimate the (relative) size of their <strong>intersection</strong>.</p>
<p>Thus, if you combine both structures, you can produce a sketch that lets you ask questions about both intersections and unions of arbitrary sets! This idea was noticed <a href="https://tech.nextroll.com/media/hllminhash.pdf">at least by 2013</a>, and it turns out thereâ€™s an <a href="https://arxiv.org/abs/1710.08436">ongoing</a> <a href="https://arxiv.org/abs/2101.00314">literature</a> of sketches that combine ideas from the two data structures in interesting ways. I think thatâ€™s neat!</p>

<p>I promised Iâ€™d return to the question of representing documents as sets, so Iâ€™ll also leave a brief note on two common approaches.</p>
<p>First, before applying either of these strategies, we may want to normalize documents in some way. For instance, we likely want to convert to a standard <a href="https://www.unicode.org/reports/tr15/">Unicode normalization form</a>, and we may also wish to case-fold, collapse runs of whitespace, or perform similar transformations.</p>
<h2 id="n-grams-aka-shingles">n-grams aka â€œshinglesâ€Â <a href="#n-grams-aka-shingles"><i>	ğŸ”—ï¸</i></a> </h2>
<p>We can represent a document as a set of all the n-grams that appear in the document, picking some appropriate value of <code>n</code>. In the field of large-scale text processing, often the literature uses the word â€œshingleâ€ instead of â€œn-gram,â€ but I find that needless confusing. We can pick any value of <code>n</code>, with the primary tradeoff that smaller values will tend to compare documents more coarsely (e.g. most English text probably looks fairly similar through the lens of bigrams), and larger ones generating more distinct features and thus larger sets. At some limit I expect you also lose sensitivity, but I suspect performance problems arise earlier.</p>
<p>According to one source Iâ€™ve found<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>, values of n between 5 and 9 appear to be common choices for a range of applications.</p>
<h2 id="word-splitting">Word-splittingÂ <a href="#word-splitting"><i>	ğŸ”—ï¸</i></a> </h2>
<p>We can instead attempt to split the input into â€œwordsâ€ or â€œtokens,â€ and use those as our features. The except from GPT-3 paper above mentions â€œSparkâ€™s standard tokenizer,â€ which I believe refers to <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Tokenizer.html">this class</a>, which simply lowercases the input and then splits on whitespace.</p>
<p>We could use a <a href="https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize">more sophisticated tokenizer</a>, or we could hybridize the approaches by tokenizing and then using n-grams of tokens. In that case we would use a smaller value of <code>n</code>, since individual tokens should much be higher-entropy than bytes or characters.</p>



</div></div>
  </body>
</html>
