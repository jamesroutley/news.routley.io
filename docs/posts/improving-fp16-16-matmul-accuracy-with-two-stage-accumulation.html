<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.spatters.ca/two-stage-fp16-mma">Original</a>
    <h1>Improving FP16/16 matmul accuracy with two-stage accumulation</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article>

  

  <div>
    <p>On Nvidia consumer GPUs such as the RTX 4090, FP16/32 matrix multiplication is limited to run at half the speed of FP16/16, meaning users need to choose between either using tensor core operations that accumulate in FP16 precision or only getting 50% of the GPU’s peak performance.</p>

<p>We can improve the accuracy of FP16/16 matrix multiplication with a two-stage accumulation strategy: use FP16/16 tensor core <code>mma</code> instructions but accumulate the results outside the <code>mma</code> in separate FP32 registers.</p>

<p>This is done by changing the main loop of the matmul kernel from</p>
<div><div><pre><code><span>for</span> <span>(</span><span>int</span> <span>k</span><span>=</span><span>0</span><span>;</span> <span>k</span> <span>&lt;</span> <span>K</span><span>;</span> <span>K</span> <span>+=</span> <span>K_BLOCK</span><span>)</span> <span>{</span>
  <span>// load global-&gt;shared-&gt;reg etc.</span>
  <span>// ...</span>
  <span>mma_m16n8k16</span><span>(</span><span>aReg</span><span>,</span> <span>bReg</span><span>,</span> <span>dReg</span><span>,</span> <span>dReg</span><span>);</span>
  <span>__syncthreads</span><span>();</span>
<span>}</span>
</code></pre></div></div>
<p>to (simplified for clarity)</p>

<div><div><pre><code><span>unsigned</span> <span>cReg</span><span>[</span><span>2</span><span>]</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
<span>for</span> <span>(</span><span>int</span> <span>k</span><span>=</span><span>0</span><span>;</span> <span>k</span> <span>&lt;</span> <span>K</span><span>;</span> <span>K</span> <span>+=</span> <span>K_BLOCK</span><span>)</span> <span>{</span>
  <span>// load global-&gt;shared-&gt;reg etc.</span>
  <span>// ...</span>
  <span>dRegPtr</span> <span>=</span> <span>reinterpret_cast</span><span>&lt;</span><span>half</span> <span>*&gt;</span><span>(</span><span>dReg</span><span>);</span>
  <span>mma_m16n8k16</span><span>(</span><span>aReg</span><span>,</span> <span>bReg</span><span>,</span> <span>cReg</span><span>,</span> <span>dReg</span><span>);</span>
  <span>dRegAcc</span><span>[</span><span>0</span><span>]</span> <span>+=</span> <span>__half2float</span><span>(</span><span>dRegPtr</span><span>[</span><span>0</span><span>]);</span>
  <span>dRegAcc</span><span>[</span><span>1</span><span>]</span> <span>+=</span> <span>__half2float</span><span>(</span><span>dRegPtr</span><span>[</span><span>1</span><span>]);</span>
  <span>dRegAcc</span><span>[</span><span>2</span><span>]</span> <span>+=</span> <span>__half2float</span><span>(</span><span>dRegPtr</span><span>[</span><span>2</span><span>]);</span>
  <span>dRegAcc</span><span>[</span><span>3</span><span>]</span> <span>+=</span> <span>__half2float</span><span>(</span><span>dRegPtr</span><span>[</span><span>3</span><span>]);</span>
  <span>__syncthreads</span><span>();</span>
<span>}</span>
</code></pre></div></div>
<p>The full code is available in Kernel 3.2 <a href="https://github.com/spatters/mma-matmul/blob/5e730a1f931b3caeca3164f3777f7c5593bd9577/kernel_3.cu#L274">here</a>.</p>

<p>An alternative approach to maintaining separate FP32 accumulator registers in the main loop would be to use Split/Stream-K and convert to FP32 when accumulating partial results.</p>

<h2 id="performance">Performance</h2>
<p>We look at the performance impact on one problem shape: M=N=K=4096, using normally distributed inputs <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>. Benchmarking setup as described in my previous post on <a href="https://www.spatters.ca/mma-matmul#benchmarking-setup">Ada matmuls</a>.</p>

<p>On this problem shape, the two-stage accumulation kernel achieves 209.1 TFLOP/s, which is 79% of cuBLAS FP16/16 performance.</p>

<table>
  <thead>
    <tr>
      <th>Kernel</th>
      <th>Execution Time</th>
      <th>TFLOP/s    </th>
      <th>% 4090 peak FP16/16          </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>cublasGemmEx FP16/32</td>
      <td>895 us</td>
      <td>153.6</td>
      <td>47.5%</td>
    </tr>
    <tr>
      <td>Two-stage accumulation</td>
      <td>657 us</td>
      <td>209.1</td>
      <td>63.3%</td>
    </tr>
    <tr>
      <td>cublasGemmEx FP16/16</td>
      <td>520 us</td>
      <td>264.2</td>
      <td>80.0%</td>
    </tr>
  </tbody>
</table>

<h2 id="accuracy">Accuracy</h2>
<p>We compare the results of each kernel to a reference kernel that computes the matmul using FP32 operations on CUDA cores. Percentiles of absolute error of each kernel compared to this reference are shown in the plot below.</p>

<p>Roughly speaking the two-stage accumulation kernel has ~100x larger absolute error than cuBLAS FP16/32, and ~10x smaller absoluter error than cuBLAS FP16/16.</p>

<p>So the two-stage kernel is 36% faster than cuBlAS FP16/32 but with ~100x larger absolute error, as compared to cuBLAS FP16/16 which is 72% faster with ~1000x the absolute error.</p>

<h3 id="references">References</h3>


</div>

</article>

      </div>
    </div></div>
  </body>
</html>
