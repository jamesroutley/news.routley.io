<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openpipe.ai/blog/hn-ai-crypto">Original</a>
    <h1>Is AI the next crypto? Insights from HN comments</h1>
    
    <div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p><a href="https://openpipe.ai/" target="_blank" rel="noopener"><em>OpenPipe</em></a><em> is a platform that makes it easy to convert your existing prompts and completions into a task-specific fine-tuned model. You can get started in a few minutes and the fine-tuned model is often cheaper, faster and often more accurate than the prompt you started with.</em></p><p><a href="https://www.wsj.com/articles/ai-silicon-valley-crypto-boom-blockchain-artificial-intelligence-59622e9c" rel="noopener">Plenty</a> <!-- --><a href="https://www.theatlantic.com/newsletters/archive/2023/01/ai-is-not-the-new-crypto/672746/" rel="noopener">of</a> <!-- --><a href="https://www.reuters.com/breakingviews/biden-ai-plan-is-one-step-avoiding-crypto-trap-2023-10-30/" rel="noopener">observers</a> have noted the similarities between crypto’s frothy history and the current boom in AI. Crypto seems to have busted, at least for the moment — is AI next?<!-- --></p><p>Just for fun, I decided to analyze some data on the question. Both crypto and AI have been heavily debated on <!-- --><a href="https://news.ycombinator.com/" rel="noopener">Hacker News</a>, with discussions going back years. By looking at trends in HN commenter opinions we might find interesting similarities and differences.<!-- --></p><h3>Gathering the data</h3><p><em>Note: if you want to follow along at home, I uploaded the public dataset of all HN posts and comments </em><a href="https://huggingface.co/datasets/OpenPipe/hacker-news" rel="noopener"><em>here</em></a><em>. I also included all the code used in this analysis</em> <!-- --><a href="https://github.com/corbt/hn-analysis" target="_blank" rel="noopener"><em>here</em></a><em>, but be warned, it&#39;s research grade at best!</em></p><p>To get started, I used the <!-- --><a href="https://github.com/HackerNews/API" rel="noopener">HN API</a> to pull all 38 million posts and comments in HN history. Using <!-- --><a href="https://www.pola.rs/" rel="noopener">Polars</a>, we can narrow the dataset to only include front-page stories since 2010:<!-- --></p><div><div><div><div><div><div><pre translate="no"><code><span>stories</span> = <!-- --><span>hn</span>.<!-- --><span>filter</span><span>(</span>
    <!-- --><span>(</span><span>pl</span>.<!-- --><span>col</span><span>(</span><span>&#34;type&#34;</span><span>)</span> == <!-- --><span>&#34;story&#34;</span><span>)</span>
    &amp; <!-- --><span>(</span><span>pl</span>.<!-- --><span>col</span><span>(</span><span>&#34;text&#34;</span><span>)</span>.<!-- --><span>is_null</span><span>(</span><span>)</span><span>)</span>
    &amp; <!-- --><span>(</span><span>pl</span>.<!-- --><span>col</span><span>(</span><span>&#34;url&#34;</span><span>)</span>.<!-- --><span>is_not_null</span><span>(</span><span>)</span><span>)</span>
    &amp; <!-- --><span>(</span><span>pl</span>.<!-- --><span>col</span><span>(</span><span>&#34;score&#34;</span><span>)</span> &gt;= <!-- --><span>30</span><span>)</span>
    &amp; <!-- --><span>(</span><span>pl</span>.<!-- --><span>col</span><span>(</span><span>&#34;descendants&#34;</span><span>)</span> &gt;= <!-- --><span>5</span><span>)</span>
    &amp; <!-- --><span>(</span><span>pl</span>.<!-- --><span>col</span><span>(</span><span>&#34;time&#34;</span><span>)</span> &gt;= <!-- --><span>datetime</span><span>(</span><span>2010</span><span>,</span> <!-- --><span>1</span><span>,</span> <!-- --><span>1</span><span>)</span><span>)</span>
<!-- --><span>)</span>
# -&gt; <!-- --><span>285</span><span>,</span><span>603</span> <!-- --><span>stories</span></code></pre></div></div></div></div></div></div><p>285K stories is still a lot to go through! A simple approach to sort these posts into categories might involve a keyword search for strings like “Bitcoin,” “Blockchain,” “Crypto”, etc. But that would produce a lot of false negatives — stories like <!-- --><a href="https://news.ycombinator.com/item?id=28142437" rel="noopener">Technical Analysis of the Poly Network Hack</a> and <!-- --><a href="https://news.ycombinator.com/item?id=30303903" rel="noopener">Kimchi: The latest update to Mina’s proof system</a> would easily get missed.<!-- --></p><h3>Throwing GPT-3.5 at the problem</h3><p>Instead of naive string matching, let’s use an LLM! I prepared a dataset of all 285K front-page stories along with the top comment on each, since sometimes the story title alone doesn’t provide enough information. Let’s roughly calculate the cost of classifying this dataset using GPT-3.5:</p><div><div><div><div><div><div><pre translate="no"><code><span>avg_chars</span> = <!-- --><span>(</span>
    <!-- --><span>stories</span><span>[</span><span>&#34;top_comment&#34;</span><span>]</span>.<!-- --><span>str</span>.<!-- --><span>len_chars</span><span>(</span><span>)</span>.<!-- --><span>mean</span><span>(</span><span>)</span>
    + <!-- --><span>stories</span><span>[</span><span>&#34;title&#34;</span><span>]</span>.<!-- --><span>str</span>.<!-- --><span>len_chars</span><span>(</span><span>)</span>.<!-- --><span>mean</span><span>(</span><span>)</span>
    + <!-- --><span>stories</span><span>[</span><span>&#34;url&#34;</span><span>]</span>.<!-- --><span>str</span>.<!-- --><span>len_chars</span><span>(</span><span>)</span>.<!-- --><span>mean</span><span>(</span><span>)</span>
<!-- --><span>)</span>

# <!-- --><span>A </span><span>token </span><span>averages </span><span>to </span><span>4</span>-<!-- --><span>5</span> <!-- --><span>chars</span> <!-- --><span>in</span> <!-- --><span>English</span>
<!-- --><span>avg_input_tokens</span> = <!-- --><span>avg_chars</span> * <!-- --><span>0.2</span>

# <!-- --><span>Add </span><span>extra </span><span>tokens </span><span>for</span> <!-- --><span>the </span><span>task </span><span>instructions</span>
<!-- --><span>avg_input_tokens</span> += <!-- --><span>100</span>

<!-- --><span>total_input_tokens</span> = <!-- --><span>avg_input_tokens</span> * <!-- --><span>len</span><span>(</span><span>stories</span><span>)</span>

# <!-- --><span>Simple </span><span>task </span><span>to </span><span>classify </span><span>across </span><span>both </span><span>categories</span><span>,</span> <!-- --><span>assume </span><span>20</span> <!-- --><span>output </span><span>tokens</span>
<!-- --><span>total_output_tokens</span> = <!-- --><span>20</span> * <!-- --><span>len</span><span>(</span><span>stories</span><span>)</span>

<!-- --><span>input_token_price</span> = <!-- --><span>0.001</span> / <!-- --><span>1000</span>
<!-- --><span>output_token_price</span> = <!-- --><span>0.002</span> / <!-- --><span>1000</span>

<!-- --><span>approx_cost</span> = <!-- --><span>(</span>
    <!-- --><span>total_input_tokens</span> * <!-- --><span>input_token_price</span> + <!-- --><span>total_output_tokens</span> * <!-- --><span>output_token_price</span>
<!-- --><span>)</span></code></pre></div></div></div></div></div></div><p>The total cost comes to $86 — not bad to classify every HN front page story for the last 13 years!</p><p>Once we’ve classified these stories, we can use Polars and <!-- --><a href="https://seaborn.pydata.org/" rel="noopener">Seaborn</a> to graph the popularity of AI and blockchain as fractions of all stories posted:<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,ZWJydQFoH8oQBxdaI85imoaw.png" data-framer-height="678" data-framer-width="868" height="339" src="https://framerusercontent.com/images/ZWJydQFoH8oQBxdaI85imoaw.png" width="434"/></p><p>Cool! I noticed a couple of surprises on this graph:</p><ul><li data-preset-tag="p"><p>With the exception of the initial 2013-2014 crypto wave, ML consistently occupied a larger fraction of front-page stories over the last 13 years.</p></li><li data-preset-tag="p"><p>There was a large, long-lived bump in AI/ML stories that peaked in 2018 and tailed off until 2022, when the current wave of LLM hype began.</p></li></ul><h3>Vibes-based commenting</h3><p>Ok, fraction of all posts is an interesting metric, but I’d love to know how HN commenters <!-- --><strong>feel</strong> about the topics… is there a way to answer that question (without breaking the bank)?<!-- --></p><p>First off, I prepared a dataset of all comments on stories that were classified as either crypto or AI-related. That gets me 2.1M comments. Classifying sentiment toward a particular <!-- --><em>topic</em> is probably too complex for a classical sentiment analysis model (someone could write an angry comment defending crypto), so let’s try GPT-3.5 again. To test, I uploaded 4000 comments to a dataset on <!-- --><a href="https://openpipe.ai/" rel="noopener">OpenPipe</a>*, labeled them with both GPT-3.5 and GPT-4 (you can see the prompt I used <!-- --><a href="https://github.com/corbt/hn-analysis/blob/main/generate-story-classification-training-data.ipynb" target="_blank" rel="noopener">here</a>), and used the evals page to compare the outputs side by side:<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,pqiczRAkz0mCo1XVtT48rWOoK4.png" data-framer-height="1050" data-framer-width="2350" height="525" src="https://framerusercontent.com/images/pqiczRAkz0mCo1XVtT48rWOoK4.png" width="1175"/></p><p>You can see at the top right that GPT-3.5 agrees with GPT-4 (labeled as “Original Output”) only 71.5% of the time. That’s not great, since there are only 3 possible classes. Using filters, I can quickly find the outputs that don’t match, and GPT-4’s answers are clearly better most of the time. Ok fine, how much would it cost to just use GPT-4 then? After all, OpenAI just released price cuts!</p><div><div><div><div><div><div><pre translate="no"><code># <!-- --><span>Found </span><span>on </span><span>OpenPipe</span>
<!-- --><span>average_input_tokens</span> = <!-- --><span>446</span>
<!-- --><span>average_output_tokens</span> = <!-- --><span>20</span>

<!-- --><span>input_token_price</span> = <!-- --><span>0.01</span> / <!-- --><span>1000</span>
<!-- --><span>output_token_cost</span> = <!-- --><span>0.03</span> / <!-- --><span>1000</span>

<!-- --><span>total_cost</span> = <!-- --><span>(</span>
    <!-- --><span>input_token_price</span> * <!-- --><span>average_input_tokens</span> + <!-- --><span>output_token_cost</span> * <!-- --><span>average_output_tokens</span>
<!-- --><span>)</span> * <!-- --><span>len</span><span>(</span><span>crypto_ai_comments</span><span>)</span></code></pre></div></div></div></div></div></div><p>Ok, GPT-4 can get the job done for… $10K. Hmm. We could stick with GPT-3.5, but its accuracy isn’t great. And it would still cost $1K, for less trustworthy results.</p><p>We do have another option though, which is to fine-tune our own model! By using fine tuning, we can get better accuracy with a much smaller model than we&#39;d be able to with a general-purpose prompted model. Smaller models mean faster inference, and much lower prices. If you&#39;re interested in learning more or doing fine-tuning on your own, we created a guide that you can find <!-- --><a href="https://news.ycombinator.com/item?id=37484135" target="_blank" rel="noopener">here</a>. The OpenPipe platform also makes creating a fine-tuned model incredibly easy — it’s literally 2 clicks once your dataset is generated.<!-- --></p><p>In this case we&#39;ll fine-tune a <!-- --><a href="https://mistral.ai/news/announcing-mistral-7b/" rel="noopener">Mistral 7B</a> model on the dataset. Mistral 7B is one of the strongest LLMs in its size class right now, beating the similarly-sized Llama 2 variant on most benchmarks.<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,VYvRqfqaORUyzMf7EZHffJQo33Y.png" data-framer-height="636" data-framer-width="1362" height="318" src="https://framerusercontent.com/images/VYvRqfqaORUyzMf7EZHffJQo33Y.png" width="681"/></p><p>I went ahead and did that fine-tune, and added it to the evals page. And since it’s using the same dataset as before, we can compare it head to head with the GPT-3.5 prompt from earlier. We’ve improved our match rate with GPT-4 from 71.5% to 87.8% — not bad!</p><p><img alt="" data-framer-asset="data:framer/asset-reference,byqF3DClm3THkzpjTDCpbSFx0zE.png" data-framer-height="552" data-framer-width="1978" height="276" src="https://framerusercontent.com/images/byqF3DClm3THkzpjTDCpbSFx0zE.png" width="989"/></p><p>Of the 12.2% that our fine tune still got wrong, I quickly reviewed a few dozen. In many cases, there really are multiple “correct” answers — for example, GPT-4 classified <!-- --><a href="https://news.ycombinator.com/item?id=24384570" rel="noopener">this comment</a> as positive towards AI, and the fine-tune classified it as neutral. I think both answers are defensible.<!-- --></p><h3>Just the Facts</h3><p>Ok, let’s plot user sentiment over time! On this graph each negative comment counts for -1 and each positive comment is +1, and I compute a rolling average over a 6-month window.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,cjyaYBc1dlrNPkRp2I9CEW0fIvs.png" data-framer-height="678" data-framer-width="868" height="339" src="https://framerusercontent.com/images/cjyaYBc1dlrNPkRp2I9CEW0fIvs.png" width="434"/></p><p>Interesting! A couple of observations:</p><ul><li data-preset-tag="p"><p>While sentiment towards AI has always been higher than sentiment towards crypto, AI sentiment steadily dropped from 2010-2018 and has been hovering around neutral for the last 5 years.</p></li><li data-preset-tag="p"><p>Crypto sentiment has a much weaker correlation with the crypto hype cycle than I’d expected. Although it seems like right now not many commenters are popping up to defend FTX/SBF. 😂</p></li></ul><p>Ok so admittedly, we haven’t come away with a clean answer to the question in the title (although for the record, I think the answer is “no”). But we’ve found some interesting trends in the stories posted on the front page of HN, as well as the way the average commenter’s opinion has shifted over time. And hopefully we’ve also learned more about how modern LLM-based tools can solve problems in a few hours that only a couple years ago would have taken an ML team weeks!</p><h2>Addendum: HN Sentiment Overall</h2><p>HN user dragonwriter <!-- --><a href="https://news.ycombinator.com/item?id=38194850" target="_blank" rel="noopener">noted that</a> up until recently, sentiment towards crypto and AI was relatively strongly correlated, and wondered if that was an artifact of HN sentiment just getting more negative overall.<!-- --></p><p>I had actually already run the same analysis for Rust and remote work when preparing this post, and removed them in the interest of brevity. Here&#39;s the comment sentiment graph with those added back in:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,wU9wtdqeY6xL4fYqy5MgqB1Ag.png" data-framer-height="678" data-framer-width="868" height="339" src="https://framerusercontent.com/images/wU9wtdqeY6xL4fYqy5MgqB1Ag.png" width="434"/></p><p>Interestingly, there is in fact a noticeable downward slope in average sentiment over time for those topics as well, although they both remain far more popular than either AI or crypto.</p></div></div>
  </body>
</html>
