<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.zompist.com/chance.htm">Original</a>
    <h1>How likely are chance resemblances between languages? (2002)</h1>
    
    <div id="readability-page-1" class="page">
<img src="https://healeycodes.com/verddrop.gif"/> 



<hr/>

<blockquote>
<i>The first rule is, you must not fool yourself.  And you are the easiest person to fool.
</i>
</blockquote>

<p>On sci.lang we are often presented with lists of resemblances between far-flung languages (e.g. Basque and Ainu, Welsh and Mandan, Hebrew and Quechua, Hebrew and every other language, Basque and every other language), along with the claim that such resemblances &#34;couldn&#39;t be due to chance&#34;, or are &#34;too many&#34; to be due to chance.

</p><p>Linguists dismiss these lists, for several reasons.  Often a good deal of work has gone into them, but little linguistic knowledge.  Borrowings and native compounding are not taken into account; the semantic equivalences proffered are quirky; and there is no attempt to find systematic sound correspondences.  And linguists know that chance correspondences do happen.

</p><p>All this is patiently explained, but it doesn&#39;t always convince those with no linguistic training-- especially the last point.  Human beings have been designed by evolution to be good pattern matchers, and to trust the patterns they find; as a corollary their intuition about probability is abysmal.  Lotteries and Las Vegas wouldn&#39;t function if it weren&#39;t so.

</p><p>So, even one resemblance (one of my favorites was <i>gaijin</i> vs. <i>goyim</i>) may be taken as portentous.  More reasonably, we may feel that one resemblance may be due to chance; but some compilers have amassed dozens of resemblances.  Such lists may be criticized on other grounds, but even linguists may not know if the chance argument applies.  <i>Could</i> a few dozen resemblances be due to chance?  If not, what is the approximate cutoff?

</p><p>The same question comes up in evaluating the results of Greenbergian mass comparisons; or proposals relating language families (e.g. Japanese and Tibeto-Burman) based on very small numbers of cognates.   Again, it would be useful to know how many chance resemblances to expect.

</p><p>I will propose a simple but linguistically informed statistical model for estimating the probability of such resemblances, and show how to adjust it to match the specific proposal being evaluated.


</p><h3><SPAN color="#000060">A trivial model: Abstract languages sharing a phonology </SPAN></h3>

<p>Let&#39;s start with a simplified case (we&#39;ll complicate it later).  We will compare two unrelated languages A and B, each of which has 1,000 lexemes of the form CVC, and an identical semantics and phonology.  That is, if there is a lexeme <i>a</i> in A with some meaning M, there will be a lexeme <i>b<SPAN size="2">p</SPAN></i> in B <i>phonetically</i> identical to <i>a</i>, and a lexeme <i>b<SPAN size="2">s</SPAN></i> with the same <i>meaning</i> as <i>a</i>.  

</p><p>What is the probability that <i>b<SPAN size="2">p </SPAN></i>is <i>b<SPAN size="2">s</SPAN></i>?-- that is, that there is a chance resemblance with <i>a</i>?  It can be read off from the phonology of the typical root.  Supposing there are 14 consonants and 5 vowels, it is 1/14 * 1/5 * 1/14, or 1 in 980.  (This assumes that the vowels and consonants are equiprobable, which of course they are not.)   For ease of calculation we&#39;ll round this to 1 in 1000.

</p><p><b>How many</b> <b>chance resemblances</b> are there?  As a first approximation we might note that with a thousand chances at 1 in 1000 odds, there&#39;s a good chance of getting one match.

</p><p>However, this is not enough. How likely is it exactly that we get one match?  What&#39;s the chance of two matches?  Would three be quite surprising?  Fortunately probability theory has solved this problem for us; the chance that you&#39;ll find <i>r</i> matches in <i>n</i> words where the probability of a single match is <i>p</i> is the binomial probability

</p><blockquote> 
(n! / (r! (n-r)!))  p<SPAN size="2"><sup>r</sup></SPAN>  (1 - p)<SPAN size="2"><sup>(n-r)</sup></SPAN>
</blockquote> 

or in this case

<blockquote> 
(1000! / (r! (1000-r)!))  .001<SPAN size="2"><sup>r</sup></SPAN>  .999<SPAN size="2"><sup>(1000-r)</sup></SPAN>
</blockquote> 

For the first few <i>r</i>:

<p>p(1) = .368
</p><p>So the probability of between 1 and 6 matches is .368 + .184 + .0613 + .0153 + .00305 + .000506 = .632, or about 63%.  It would be improbable, in other words, if we found no exact matches in the entire dictionary.  (But not <i>very</i> improbable; p(0), which we can find by subtracting the above <i>p</i>&#39;s from 1.0, is 37%.)

</p><h3><SPAN color="#000060">Semantic and phonetic leeway </SPAN></h3>

<p>Proffered resemblances are rarely exact.  There is always some <b>phonetic</b> <b>and</b> <b>semantic leeway</b>.  Either can be seen as increasing the set of words in B we would consider as a match to a given word <i>a</i> in A.

</p><p>For instance, suppose for each consonant we would accept a match with 3 related consonants, and for each vowel, 3 related vowels.  Since we&#39;re assuming a CVC root structure, this gives 3*3*3 = 27 words in B which might match any given <i>a</i>. 

</p><p>And suppose for each word <i>a</i> we will accept 10 possible meanings for <i>b</i>.  This applies to each of the 27 phonetic matches; so <i>a</i> can now match a pool of 27*10 = 270 lexemes.  The probability that it does so is of course 270 in 1000, or .27.  Every lexeme in A, in other words, has a better than 1 in 4 chance of having a random match in B!

</p><p>How many chance resemblances are there now?  The same formula can be used, with the revised estimate for <i>p</i>:

</p><blockquote>
(1000! / (r! (1000-r!)))  .27<SPAN size="2"><sup>r</sup></SPAN>  .73<SPAN size="2"><sup>(1000-r)</sup></SPAN>.
</blockquote>

There is a significant probability for very high numbers of matches, so we must continue calculating for <i>r</i> well into the hundreds.  The results can be summarized as follows:

<blockquote><table><tbody><tr><td>
p( up to 210 ) - negligible
</td><td>    
</td><td><img src="https://healeycodes.com/chance1.gif"/>
</td></tr></tbody></table></blockquote>

<p>This looks a lot like a normal distribution, and in fact it is one, 
if <i>np</i> and <i>n(1-p)</i> are both over 5.
(For typical lexicon sizes, the distribution will be normal if p &gt; .01.)
The &#39;expected case&#39;-- the number of matches with the highest probability-- will be <i>np</i>;
in the above case, 270, with a probability of .0284.

</p><p>I will suggest refinements to this model below, but the basic features are in place: a probability for a single match; a calculation for number of expected matches; and an adjustment for phonetic and semantic leeway. 


</p><h3><SPAN color="#000060">Vocabulary size and other variables </SPAN></h3>

What happens as various parameters of the model are changed?

<p>In the model above, <b>vocabulary size</b> is not independent of the
sample phonology.  If syllables are all CVC, with 5 vowels and 14
consonants, then there are only 980 phonologically possible words.

</p><p>If we use a larger vocabulary size in the formula, we are 
conceptually re-using some of these possible words.  This might
not be a bad model for a language with many homonyms, or if we are
simply ignoring some of the phonology of the language (e.g. tones,
or final vowels).

</p><p>We can also try different <b>phonologies</b>.  For instance, here are a 
the numbers of expected matches for a few different word types, 
again allowing 3 matches per sound, and 10 semantic matches per word:

</p><blockquote><table>
<tbody><tr><td><i>Form</i>  </td><td>#C </td><td>#V  </td><td><i>Lexicon </i> </td><td><i>Phon matches</i> </td><td><i>Expected </i>
</td></tr><tr><td>CV    </td><td>14 </td><td> 5  </td><td>70   </td><td>9   </td><td>90
</td></tr><tr><td>CVC   </td><td>14 </td><td> 5  </td><td>980  </td><td>27  </td><td>270
</td></tr><tr><td>CVCV  </td><td>14 </td><td> 5  </td><td>4900 </td><td>81  </td><td>810
</td></tr></tbody></table></blockquote>

The number of matches isn&#39;t increasing as fast as the vocabulary
size.  For instance, as we
go from CVC to CVCV, we increase the lexicon fivefold (for each
CVC syllable, there are 5 CVCV syllables, one with each vowel),
while the number of matches only increases threefold (the extra
vowel can be matched only 3 ways). 

<p>(For the first (CV) case, the expected number of matches is larger than the 
lexicon size!  This is because, with the rules as stated, we&#39;re often going to find
more than one match per word.  We&#39;ll come back to this later.)

</p><p>What about <b>exact matches</b>?  Curiously, the number of 
exact matches averages about 1 for any phonology, and therefore
for widely differing vocabulary sizes.  The reason
is not hard to find: as the lexicon size increases, 
the chance of a given match goes down, but we get more chances.

</p><p>(The average of 1 exact match needs to be qualified, because of
homonymy: a word can have several meanings, and thus match several
possible words.  In effect, languages come with a built-in
semantic leeway; a handful of exact matches is therefore no surprise.)

</p><p>What about different <b>phoneme inventories</b>?  What if we have 20
consonants and 10 vowels?  There are now 4000 possible CVC syllables;
yet without varying any other parameters, we would still expect
only 270 matches.

</p><p>But the number of phonetic matches is not really independent of
the phonology we use.  In a five-vowel system, a phonetic leeway
of 3 per sound matches means (e.g.) accepting <i>pat</i> and 
<i>pit</i> as matches for <i>pet</i>.  In a ten-vowel system,
might we not also accept <i>pEt</i> (open e) and <i>pöt</i> (rounded e) as well?

</p><p>Contrariwise, suppose there are only 3 vowels, as in Quechua
or Classical Arabic.  A 3-vowel leeway means that all vowels match
all vowels!

</p><p>Here&#39;s some 20C/10V languages with a few different phonetic leeways (and the same semantic leeway of 10):

</p><blockquote><table>
<tbody><tr><td><i>Form</i>  </td><td>#C </td><td>#V  </td><td><i>Lexicon </i> </td><td><i>Phonetic </i></td><td><i>Expected </i>
</td></tr><tr><td>CV    </td><td>20 </td><td>10   </td><td>200   </td><td>2   </td><td>40
</td></tr><tr><td>CV    </td><td>20 </td><td>10   </td><td>200   </td><td>3   </td><td>90
</td></tr><tr><td>CV    </td><td>20 </td><td>10   </td><td>200   </td><td>5   </td><td>250
</td></tr><tr><td>CVC   </td><td>20 </td><td>10   </td><td>4000  </td><td>2  </td><td>80  
</td></tr><tr><td>CVC   </td><td>20 </td><td>10   </td><td>4000  </td><td>3  </td><td>270  
</td></tr><tr><td>CVC   </td><td>20 </td><td>10   </td><td>4000  </td><td>5  </td><td>1250  
</td></tr><tr><td>CVCV  </td><td>20 </td><td>10   </td><td>40000 </td><td>2  </td><td>160
</td></tr><tr><td>CVCV  </td><td>20 </td><td>10   </td><td>40000 </td><td>3  </td><td>810
</td></tr><tr><td>CVCV  </td><td>20 </td><td>10   </td><td>40000 </td><td>5  </td><td>6250
</td></tr></tbody></table></blockquote>

The reader may well conclude that with the appropriate choice of
parameters, any number of matches is possible!  That would be
more or less accurate; but note:

<ul>

<li>The parameters are not simply selected according to taste;
they must be taken from the particular comparison we are
evaluating.  For instance, if the comparer matches <i>t</i> with
<i>th</i> and <i>ch</i> and <i>d</i>, the phonetic leeway is <i>at least</i> 4. </li><li>If there&#39;s <i>any</i> phonetic and semantic leeway at all-- and
I&#39;ve never seen a relationship proposal with none-- then 
one can well expect dozens or even hundreds of matches, 
rather than the bare handful intuition might suggest.

</li></ul>

<h4><SPAN color="#000060">A computer model </SPAN></h4>

I&#39;ve written a computer program which generates two random
languages and compares them for matches.  Phonology, phoneme
inventory, matching algorithm, phonetic leeway, and semantic
leeway can all be varied.

<p>You can get <a href="https://healeycodes.com/simm.html">the source code for the program here</a>.

</p><p>The program gives several counts, corresponding to slightly
differing <b>match-counting methodologies</b>.  For instance, here&#39;s the output of a sample run simulating one
of the models discussed above: 980 possible words with phonetic leeway of 3 and semantic leeway of 10.

</p><blockquote><pre>Matches in 980 words:
</pre></blockquote>

<p>The program generates two random languages A and B.  Then, for each word in language A, 
the program tests each word in language B.  There is obviously a chance that this will result
in more than one match for each word in A.  The total number of matches is reported in the top
left; the value here (290) is close to our expectation based on <i>np</i> (980 * .275 = 270).
In statistical terms, this is a binomial probabability distribution with replacement.

</p><p>The bottom row counts only one match for each word in A.  So, even if we found three matches
for a given word in A, we count only one.   So in the example, though there were 290 total
matches, there were 256 words in A that had at least one match, and some had more than one.

</p><p>The right-hand column counts matches if we allow each word in B to be used only once.
That is, if any word B is ever counted as a match for a word in A, it&#39;s withdrawn from the pool, 
and no further word in A can match it.  

</p><p>It might seem obvious that we only want one match per word; but again, we must follow 
the practice of the comparison we&#39;re evaluating.  A comparison of Basque and Ainu posted 
to sci.lang, for instance, matched Ainu <i>poro monpeh</i> &#39;big finger (=thumb)&#39; to <i>erpuru</i>,
and <i>poro aynu</i> &#39;big man (=adult)&#39; to <i>porrokatu</i> &#39;tired&#39;.
Yes, the semantic leeway there is formidable; but the point is
that one Ainu word, <i>poro</i>, is being used to match two
entirely different Basque words.

</p><p>It&#39;s not hard to find the expected <b>number of words in A with one or more matches</b> (the number in the 
lower left of the program output).
Let&#39;s start by subtracting out the cases where we found two matches in B.
How many of these will there be?

</p><p>The probability that word A<sub>i</sub> has the same meaning as B<sub>i</sub> is of course 1 in <i>n</i> 
(the lexicon size).  If we allow phonetic leeway, we get additional chances for a match; 
each of these phonetic near-matches is multiplied further by the semantic leeway.  In the example above
we effectively get 270 chances for each word in A.

</p><p>This is really another binomial probability problem: we have 270 trials, each with probability 1 in 980,
and we want to know the chances for 2 successes.  Using the formula given, the chance is 
(270! / (2! (270-2)!))  (1/980)<SPAN size="2"><sup>r</sup></SPAN>  (979/980)<SPAN size="2"><sup>(270-r)</sup></SPAN> = .0288.  
Now since there are 980 words in A, we would expect about .0288 * 980 = 28 two-word matches.  
Since we expected 270 total matches, this leaves about 242 words with one or more matches.
This is not far from the 256 found in the sample run.

</p><p>The same formula will tell us how often a word in A will match <i>three</i> words in B-- the probability
is .00262, leading us to expect 2.57 matches.  Since these would count three times in the total count and
we want them to count only once, we have to subtract another 2 * 2.57 = 5 from that 242, giving us a more accurate
estimate of 237.  A four-word match has a probability of .00018, which doesn&#39;t affect the final number any.
In practice it&#39;s not really worth correcting for anything beyond two-word matches.)

</p><p>(Some readers may worry that the numbers from the sample run (290, 256) are &#39;too far&#39; from the expected
values (270, 237).  But no, these numbers are well within the variance.   If I run the program a hundred times,
I get an average of 270.6 total matches, 240.5 words matched.)


</p><h3><SPAN color="#000060"><a name="better">Better phonological models</a> </SPAN></h3>

We&#39;d like to remove the unrealistic assumptions in this model, 
starting with the absurdly simplified phonologies.  
Fortunately this is not hard to do; it amounts to finding a better<i> p</i>.

<p>The model above assumes that both languages have the same phonology,
which obviously isn&#39;t usually the case.  If you&#39;re evaluating a proposed set of resemblances,
it&#39;s usually fairly evident what the compiler counted as a match.
For instance, <a href="#semitic">the Quechua/Semitic comparison discussed below</a>
normally just (loosely) matches initial and medial consonants.
To estimate <i>p</i>, then, we multiply the probability of a match on initial consonants
with that of a match on medial consonants.

</p><p>To do this, we need to know how many possible consonants there are, and how many of
them are considered a match.  The latter of course must be taken from the proposed
resemblances; if there&#39;s enough of them the number of consonants can be too, otherwise
we need to find out the language&#39;s phonology.

</p><p>Phonemes don&#39;t actually appear with equal frequencies, and this can be important.
For instance, Quechua has just three vowels, a/i/u (though allophonic o/e are found in 
some dictionaries).  The chance that the main vowel of a root is an <b>a</b>, however,
is not 33.3% but 56%.  A match with medial <b>a</b> is therefore correspondingly less
surprising.  In a sidebar, I&#39;ve estimated the chances of <a href="https://healeycodes.com/chanceph.htm">random matches 
between Quechua and Chinese</a> taking <b>phoneme frequencies</b> into account.

</p><p>What if we&#39;re only given <b>a word or two</b>-- say, that cute Japanese <i>gaijin</i> / 
Hebrew <i>goyim</i> pair?
Even with one word, we can estimate phonetic leeway segment by segment:

</p><ul>
<li>first consonant matches; Japanese has about 14 consonant phonemes, so let&#39;s say the chance is 1 in 14
</li><li>first vowel is basically ignored
</li><li>medial consonant is extremely loose-- a dental affricate versus a palatal approximant.
To be as fair as possible, let&#39;s say the chance is 1 in 3 that we get even this close.
</li><li>second vowel matches; Japanese has 5 vowel phonemes, so the chance here is 1 in 5
</li><li>final consonant is nasal in both cases; but Japanese words can <i>only</i> end in 
a vowel or <b>n</b>-- so let&#39;s assign a chance of 1 in 6.
</li></ul>

That gives us (1/14) * (1/3) * (1/5) * (1/6) or 1 in 1260 (= .00079).  This pair doesn&#39;t
of course come with the size of the comparer&#39;s dictionary; but from the probability alone
we can say if the lexicon has at least 1260 words, we&#39;re likely to find at least one 
Japanese/Hebrew match that&#39;s this close.  In a dictionary of 10,000 entries we could expect
about 8 such matches.

<p>Of course it&#39;s worse than that, since there&#39;s semantic leeway as well.  <i>Goyim</i>
means &#39;nations&#39; (from <i>goy</i> &#39;nation&#39;); though it&#39;s metaphorically used for &#39;non-Jews&#39;,
it&#39;s certainly not an exact semantic match for a word meaning &#39;foreigner&#39;. 
(<i>Gaijin</i> is a borrowing from Chinese <i>wairén</i> &#39;outside-person&#39;.)
I can&#39;t emphasize enough that inexact matches greatly multiply the chances for finding 
random resemblances.


</p><h3><SPAN color="#000060">Semantic matches </SPAN></h3>

Unfortunately there&#39;s no rigorous way to improve our estimate of semantic leeway-- the number of meanings that will be considered to match a given word.  Classifying meanings has been a parlor pastime for several centuries, but this has never produced any generally useful way of analyzing meanings, much less determining whether two meanings are close or not.

<p>About all that can be done is to emphasize that most actual attempts to find &#34;cognates&#34; accept very considerable semantic leeway, and that this greatly increases the chance of random matches.

</p><p>It&#39;s helpful to see just what a semantic leeway of 10 or 100 meanings looks like.  For instance, let&#39;s take the word &#34;<b>eat</b>&#34;.  

</p><p>If we allow 10 matches, we&#39;d accept something like:

</p><blockquote>
eat, dine, bite, chew, swallow, feed, hungry, edible, meal, meat
</blockquote>

<p>If we allow 25  matches, we&#39;d accept:

</p><blockquote>
eat, dine, bite, nibble, chew, munch
</blockquote>

<p>If we allow 100 matches, we&#39;d accept:

</p><blockquote>
<p>eat, dine, sup, stuff, gorge, nibble, ruminate, take in, tuck in, gobble, bolt, swill
</p></blockquote>

<p>Ten matches may seem like a lot; but nothing in even the list of 100 matches is a real stretch.  The real question is: looking for matches for &#39;to eat&#39; in A, <b>would our language comparer</b> <b>take a word meaning &#39;nourish&#39; or &#39;delicious&#39; or &#39;gnaw&#39; as a match</b>--indicating that his semantic leeway is more like 100 than like 10?  The temptation is almost irresistible.  

</p><p>It&#39;s worth pointing out that semantic leeway is really <i>multidimensional</i>.  The 100-word list shows several directions we can explore starting from the word &#39;eat&#39;: types of eating; other types of ingestion; types of food; types of feeding or food preparation; qualities or states associated with eating; associated parts of the body; differences in connotation or register; differences in the type of eater; metaphorical extensions.  You may accept only a little variation along each dimension, but because there are many dimensions the total variation is high. 

</p><p>(This may suggest a rough method for estimating semantic leeway: count the dimensions
in which the words differ.  For instance, Greenberg &amp; Ruhlen&#39;s <i>mekeli</i> (&#39;nape&#39;, from Faai)
and <i>amu&#39;ul</i> (&#39;to suck&#39;, from Mixe) seems to follow the semantic links 
<i>nape / (nearby body part) neck or throat / (associated verb) swallow / (another bodily verb) suck</i>;
if there are as few as five alternatives at each of these four steps, the semantic leeway is  
at least 5 x 5 x 5 = 125.  (The reader may enjoy tracing how many steps are required to reach
their proposed Indo-European cognate, &#39;milk&#39;.)


</p><h4><SPAN color="#000060">Effect of semantic leeway </SPAN></h4>

<p>What happens to the number of expected matches as semantic leeway increases?

</p><p>We are traipsing through language A, word by word.  For each word <i>a</i>, the probability <i>p </i>of finding a match in B is, say, .01.
But now suppose the semantic leeway <i>m</i> is 10.  So for word <i>a</i>, we have to check for a match 10<i> </i>times.  

</p><p>What does this mean numerically?  It depends on how we count matches.  </p><ul>

<li> We want to know if <i>a</i> matches <b>any word</b>.  We check for matches for <i>a</i> 10 times.   What are the chances that we&#39;ll find a match?

<p>The easiest way to compute this is backwards: find the probability that we <i>won&#39;t</i> find a match, which is simply (1 - <i>p</i>)<i><SPAN size="2"><sup>m</sup></SPAN></i>.   To see this, think of checking the 3rd word.  To not find a match on words 1, 2, 3, we must find no match on word <i>b</i><SPAN size="2">1</SPAN> (probability .99), no match on word <i>b</i><SPAN size="2">2</SPAN> (prob .99), and no match on word <i>b</i><SPAN size="2">3</SPAN> (prob .99).  Since these events are independent, the cumulative probability of .99 * .99 * .99--that is, (1 - <i>p</i>)<i><SPAN size="2"><sup>m</sup></SPAN></i>.

</p><p> So the probability of finding a match is 1 - ((1 - <i>p</i>)<i><SPAN size="2"><sup>m</sup></SPAN></i>).  In our example, this is 1 - .99<SPAN size="2"><sup>10</sup></SPAN> = .095618. </p></li><li> We want to know the <b>number of words </b><b><i>a</i></b><b> matches</b>.  That is, we might find more than one phonetic match in our set of <i>m</i> semantic matches.  The average number of matches is easy, since it&#39;s <i>p</i> for each word, and thus <i>mp</i> total.  We could also go on to find the probability for one match, two matches, etc.

</li></ul>

<p>Below, I&#39;ll pursue the first of these alternatives, on the 
grounds that it most closely resembles the methodology of the
comparisons we want to evaluate.  As we&#39;ve seen with the 
Basque/Ainu comparison, comparers generally don&#39;t scruple at
using the same word twice.  My impression, however, is 
that this only applies to one of the languages. 

</p><p>In other words, their method is, I take it, to loop 
through the words of language A
and for each one find, if possible, one match in B.  This
methodology avoids multiple matches per word in A, but doesn&#39;t
prevent us from using the same word in B more than once.
The formulas suggested here model this methodology.



</p><h4><SPAN color="#000060">Vocabulary size </SPAN></h4>

<p>Semantic leeway is not really independent of vocabulary size.

</p><p>As the vocabulary grows into the thousands, we can expect
finer and finer distinctions of meaning; but 
the language comparer is likely to ignore them.
A language of 10,000 words may well distinguish &#39;eat (of humans)&#39; vs. 
&#39;eat (of animals)&#39; vs. &#39;devour&#39; vs. &#39;bite&#39; and so on.  Yet the 
language comparer, looking for a match for &#39;eat (of humans)&#39;, is not 
going to skip over  &#39;eat (of animals)&#39;.

</p><p>If the comparer is working only with (say) a 200-item wordlist,
not much semantic variation will be available.  Quite a bit is 
available in a 2000 or 20,000-word lexicon.


</p><h4><SPAN color="#000060">Mismatched lexicons </SPAN></h4>

I&#39;ve assumed so far that the languages being compared share word meanings.  What if they don&#39;t?

<p>Let&#39;s say we&#39;re comparing two lexicons of 1000 words each, and the chance of a single phonetic
match is .03.  If we know that the meaning of a word in language A is also found in language B,
then we can expect about 30 exact semantic matches.

</p><p>Now let&#39;s say that 1/2 of the meanings recorded in A <i>aren&#39;t</i> found in B.  In effect,
as we&#39;re comparing two words, there&#39;s only a 1 in 2 chance that the meaning we&#39;re looking at
is even found in the other lexicon.  So in our example, we&#39;d expect 15 matches.

</p><p>Another complication is that words are polysemous, and the particular set of meanings a 
word has in language A are not likely to be found in language B.  At first glance this might
be expected to lower the chances for a match; but from what I&#39;ve seen of alleged resemblances,
it increases it.  For instance, Quechua <i>simi</i> means &#39;mouth, word, language&#39;; a comparer
is liable to feel free to match on <i>any</i> of these senses.  In effect, polysemy brings with
it its own increased semantic leeway.


</p><h3><SPAN color="#000060">Analyzing a claim </SPAN></h3>

To analyze a claim about language relationship based simply on resemblances (as opposed, of course, to one based on the comparative method), we can apply the principles and formulas developed above.

<p>We will need to know:

</p><ul>
<li> The probability of a phonetic match (implicit in the claimant&#39;s comparisons)
</li><li> The semantic leeway: how many words in B the claimant will allow to match one in A
</li><li> The vocabulary size (we may know this directly, or estimate it from the phonology)
</li></ul>

I analyze two examples below: Greenberg &amp; Ruhlen&#39;s &#34;world etymology&#34; <i>maliq&#39;a</i> &#39;swallow&#39;, and a comparison of Quechua and Hebrew posted to sci.lang.

<h4><SPAN color="#000060">Greenberg &amp; Ruhlen </SPAN></h4>

Greenberg &amp; Ruhlen are comparing multiple languages.  For a truly rigorous judgment we would need phoneme and root structure frequency analyses, as well as vocabulary size estimates, for each language concerned.  This information is not available; but fortunately <a href="https://healeycodes.com/proto.html#maliqa">G&amp;R&#39;s resemblance list</a> alone is enough to deduce most of the parameters required.

<p>To estimate the amount of phonetic leeway they allow, we can simply count the correspondences they allow.  For instance, the vowels seem to be completely ignored.  The middle consonant can be one of l, ly, lh, n, r, or zero-- (at least) 6 possibiliites.  The end consonant can be one of g, j, d, k, q, q&#39;, kh, k&#39;, X, zero-- (at least) 10 possibilities.

</p><p>The initial consonant must, it seems, be <b>m</b>.  There is a reason for that, I think.  Our brains, as psycholinguists have found, respond very strongly to initials.  Find words in A and B that begin with the same letter, and the battle is half-won-- the brain is predisposed to find them very similar.  Indeed, posters to sci.lang sometimes offer &#34;resemblances&#34; that correspond in nothing <i>but</i> the first letter.  (The fact that they find the coincidence remarkable is more evidence for human beings&#39; lousy intuition about probabilities.)  

</p><p>(It&#39;s also worth pointing out that a very high 7% of all words in my Quechua sample text begin with <b>m</b>.  This initial isn&#39;t particularly common in Chinese, but one has to wonder if this choice of initial doesn&#39;t increase the possibility of false cognates, simply by being more common.)

</p><p>Let&#39;s consider (arbitrarily) that G&amp;R&#39;s languages have about 25 phonemes each.  Then the <i>minimum</i> probability of a single, exact-meaning random match is 1/25 * 6/25 * 10/25 or .004.  It must be emphasized that this is a minimum; if we saw more of G&amp;R&#39;s potential cognates we might find that they allow more leeway than this.  And of course it&#39;s only a loose estimate, because it&#39;s an abstract measure intended to cover any two arbitrary languages.

</p><p>As for semantic matches, we can also form a lower bound on the semantic leeway they allow by counting the separate meanings in their glosses.  I count at least 12; but I would consider it highly misleading not to at least double this number, and based on our lists of words related to &#39;eat&#39; I&#39;d consider 100 to be quite reasonable.  If you can accept &#39;breast&#39;, &#39;cheek&#39;, &#39;swallow&#39;, &#39;drink&#39;, &#39;milk&#39;, and &#39;nape of the neck&#39; as cognates, it&#39;s hard to seriously claim that you wouldn&#39;t accept &#39;eat&#39;, &#39;mouth&#39;, &#39;guzzle&#39;, &#39;vomit&#39;, &#39;suckle&#39;, and &#39;stomach&#39;.

</p><p>Let&#39;s say the semantic leeway is 25 words.  Then the probability that we&#39;ll find at least one match for a word is 1 - ((1 - <i>p</i>)<i><SPAN size="2"><sup>m</sup></SPAN></i> = 1 - 0.996<SPAN size="2"><sup>25</sup></SPAN> = .0953.

</p><p>How many random matches can R&amp;G expect to find?

</p><p>Let&#39;s assume the lexicon is 2000 words-- hopefully a good estimate of the size of the lexicons available for many of the obscure languages they work with.  Our formula produces (omitting ranges with minimal probabilities):

</p><p>p( 126 to 150 ) = .0080
</p><p>In other words we should expect close to <b><i>200 random matches</i></b>, or a <b><i>tenth of the lexicon</i></b>.

</p><p>Of course, that&#39;s between two languages.  What&#39;s the likelihood of finding matches <b>across languages</b>?

</p><p>If the languages are related, of course, we would expect many non-random resemblances (though with G&amp;R-style phonetic and semantic laxity we will find many random matches as well).  We should not be cowed by the size of G&amp;R&#39;s cognate list; there are multiple entries per family.  They are not really comparing hundreds of languages, but a much smaller number of language <i>families</i>.

</p><p>There&#39;s no general number of random matches to expect this way, since language families vary in number of languages, and in how similar their lexicons are.  But even closely related languages, like Spanish and French, have significant numbers of non-cognate words (<i>chien</i> vs. <i>perro, chapeau</i> vs. <i>sombrero, manger</i> vs.<i>comer, regarder </i>vs.<i> mirar</i>, etc.).  So if you don&#39;t find enough resemblances in language A<SPAN size="2">1</SPAN>, you can try A<SPAN size="2">2</SPAN>, A<SPAN size="2">3</SPAN>, etc.  Or even search dialects for cognates, as it seems they&#39;ve done with Quechua and Aymara.   The list of random resemblances between <i>families </i>will be larger than the list of random resemblances between <i>languages</i>.

</p><p>Another key question is how many families they&#39;re comparing.  If G&amp;R are right about certain high-level categories, such as Amerind and Eurasiatic, this turns out to be &#34;three or four&#34;.  If families A and B have 500 random resemblances, they&#39;ll have about 125 with family C and 31 with family D-- a quite respectable listing for &#34;proto-World&#34;.

</p><h4><SPAN color="#000060"><a name="semitic">Quechua &amp; Semitic</a> </SPAN></h4>

Now let&#39;s look at a list of Quechua and Semitic resemblances posted to sci.lang (and a level of scholarship which will make us miss Ruhlen &amp; Greenberg).

<table>
<tbody><tr><td><b><i>Quechua</i></b></td>
<td></td>
<td>Semitic</td>
<td></td>
<td></td></tr>

<tr><td><b>llama</b></td>
<td>llama</td>
<td><b>gamal</b></td>
<td>camel (Heb.)</td>
<td></td></tr>

<tr><td><b>qollana</b></td>
<td>leader</td>
<td><b>kohen</b></td>
<td>priest (Heb.)</td>
<td></td></tr>

<tr><td><b>t&#39;eqe</b></td>
<td>rag doll</td>
<td><b>degem</b></td>
<td>model, specimen (Heb.)</td>
<td></td></tr>

<tr><td><b>qhapa</b></td>
<td>rich, powerful</td>
<td><b>gabar</b></td>
<td>become strong (Heb.)</td>
<td></td></tr>

<tr><td><b>qhoyu</b></td>
<td>group</td>
<td><b>goi</b></td>
<td>nation, people (Heb.)</td>
<td></td></tr>

<tr><td><b>qhoruy</b></td>
<td>cut off</td>
<td><b>garaz</b></td>
<td>cut (Heb.)</td>
<td></td></tr>

<tr><td><b>qhasay</b></td>
<td>burp</td>
<td><b>gasah</b></td>
<td>burp (Heb.)</td>
<td></td></tr>

<tr><td><b>q&#39;enti</b></td>
<td>shorten, shrink</td>
<td><b>gamad</b></td>
<td>shrink (Heb.)</td>
<td></td></tr>

<tr><td><b>amaru</b></td>
<td>snake, sage</td>
<td><b>amaru</b></td>
<td>know, teach (Assyr.)</td>
<td></td></tr>

<tr><td><b>anta</b></td>
<td>copper</td>
<td><b>homnt</b></td>
<td>copper (Coptic)</td>
<td></td></tr>

<tr><td><b>atoq</b></td>
<td>fox</td>
<td><b>bachor</b></td>
<td>fox (Coptic)</td>
<td></td></tr>

<tr><td><b>aysana</b></td>
<td>basket</td>
<td><b>tsina</b></td>
<td>basket (Aramaic)</td>
<td></td></tr>

<tr><td><b>ch&#39;olqe</b></td>
<td>wrinkle</td>
<td><b>chorchi</b></td>
<td>wrinkles (Coptic)</td>
<td></td></tr>

<tr><td><b>charki</b></td>
<td>jerky</td>
<td><b>charke</b></td>
<td>drought (Heb.)</td>
<td></td></tr>

<tr><td><b>cholo</b></td>
<td>Andean person</td>
<td><b>chlol</b></td>
<td>folk (Coptic)</td>
<td></td></tr>

<tr><td><b>wanaqo</b></td>
<td>guanaco</td>
<td><b>anaqate</b></td>
<td>she-camel (Assyr.)</td>
<td></td></tr>

<tr><td><b>churi</b></td>
<td>father&#39;s son</td>
<td><b>chere</b></td>
<td>son (Coptic)</td>
<td></td></tr>

<tr><td><b>illa</b></td>
<td>light, jewel</td>
<td><b>ille</b></td>
<td>brightness, light (Coptic)</td>
<td></td></tr>

<tr><td><b>k&#39;ayrapin</b></td>
<td>pancreas</td>
<td><b>kaire</b></td>
<td>gullet, belly (Coptic)</td>
<td></td></tr>

<tr><td><b>kinuwa</b></td>
<td>quinoa</td>
<td><b>knaao</b></td>
<td>sheaf (Coptic)</td>
<td></td></tr>

<tr><td><b>k&#39;ayra</b></td>
<td>frog</td>
<td><b>krur</b></td>
<td>frog (Coptic)</td>
<td></td></tr>

<tr><td><b>kutuna</b></td>
<td>blouse</td>
<td><b>kutunet</b></td>
<td>garment, tunic</td>
<td></td></tr>

<tr><td><b>onqoy</b></td>
<td>sickness, illness</td>
<td><b>thomko</b></td>
<td>ill use, afflict</td>
<td></td></tr>

<tr><td><b>punku</b></td>
<td>door</td>
<td><b>brg</b></td>
<td>be open (Coptic)</td>
<td></td></tr>

<tr><td><b>tarpuy</b></td>
<td>planting</td>
<td><b>sirpad</b></td>
<td>plant (Heb.)</td>
<td></td></tr>

<tr><td><b>hamuna</b></td>
<td>entrance</td>
<td><b>amumuna</b></td>
<td>city gate (Assyr.)</td>
<td></td></tr>

<tr><td><b>hillu</b></td>
<td>sweet tooth</td>
<td><b>akkilu</b></td>
<td>glutton (Assyr.)</td>
<td></td></tr>

<tr><td><b>huku</b></td>
<td>owl</td>
<td><b>akku</b></td>
<td>owl (Assyr.)</td>
<td></td></tr>

<tr><td><b>qoleq</b></td>
<td>silver</td>
<td><b>purku</b></td>
<td>gold (Assyr.)</td>
<td></td></tr>

<tr><td><b>p&#39;uru</b></td>
<td>bladder, gourd</td>
<td><b>buru</b></td>
<td>vessel (Assyr.)</td>
<td></td></tr>

<tr><td><b>ch&#39;enqo</b></td>
<td>small thing</td>
<td><b>enegu</b></td>
<td>suck (Assyr.)</td>
<td></td></tr>

<tr><td><b>watuq</b></td>
<td>diviner</td>
<td><b>baru</b></td>
<td>seer (Assyr.)</td>
<td></td></tr>

<tr><td><b>waliq</b></td>
<td>abundant</td>
<td><b>baru</b></td>
<td>become full (Assyr.)</td>
<td></td></tr>

<tr><td><b>ch&#39;aphra</b></td>
<td>brush, twigs</td>
<td><b>abru</b></td>
<td>brush pile (Assyr.)</td>
<td></td></tr>

<tr><td><b>raphra</b></td>
<td>wing</td>
<td><b>abru</b></td>
<td>wing, fin (Assyr.)</td>
<td></td></tr>

<tr><td><b>apu</b></td>
<td>god, mountain lord</td>
<td><b>abu</b></td>
<td>father (Assyr.)</td>
<td></td></tr>

<tr><td><b>hatarichiy</b></td>
<td>build, incite, disturb</td>
<td><b>adaru</b></td>
<td>worried, disturbed (Assyr.)</td>
<td></td></tr>

<tr><td><b>hayk&#39;api</b></td>
<td>how much?</td>
<td><b>akka&#39;iki</b></td>
<td>how much? (Assyr.)</td>
<td></td></tr>

<tr><td><b>taruka</b></td>
<td>type of deer</td>
<td><b>barih&#39;a</b></td>
<td>antelope (Assyr.)</td>
<td></td></tr>

<tr><td><b>umiqa</b></td>
<td>jewel</td>
<td><b>banu</b></td>
<td>headgear, diadem (Assyr.)</td>
<td></td></tr>

<tr><td><b>wawa</b></td>
<td>baby, child</td>
<td><b>babu</b></td>
<td>child (Assyr.)</td>
<td></td></tr>

<tr><td><b>p&#39;uytu</b></td>
<td>well, puddle</td>
<td><b>buninnu</b></td>
<td>pond (Assyr.)</td>
<td></td></tr>

<tr><td><b>walla</b></td>
<td>domineering person, soldier</td>
<td><b>baxalu</b></td>
<td>ripe, youthful, manly (Assyr.)</td>
<td></td></tr>

<tr><td><b>wayra</b></td>
<td>wind, air</td>
<td><b>billu</b></td>
<td>low wind (Assyr.)</td>
<td></td></tr>

<tr><td><b>wanqara</b></td>
<td>drum</td>
<td><b>balangu</b></td>
<td>kettle-drum (Assyr.)</td>
<td></td></tr>

<tr><td><b>phiri</b></td>
<td>thick; dish made of flour and water</td>
<td><b>buranu</b></td>
<td>meal? (Assyr.)</td>
<td></td></tr>

<tr><td><b>perqa</b></td>
<td>wall</td>
<td><b>birtu</b></td>
<td>fetter; fortress (Assyr.)</td>
<td></td></tr>

<tr><td><b>phasi</b></td>
<td>steamed</td>
<td><b>bashalu</b></td>
<td>boil, roast (Assyr.)</td>
<td></td></tr>

<tr><td><b>maqt&#39;a</b></td>
<td>young man</td>
<td><b>batulu</b></td>
<td>youth (Assyr.)</td>
<td></td></tr>

<tr><td><b>patana</b></td>
<td>stone seat</td>
<td><b>apadana</b></td>
<td>throne room (Assyr.)</td>
<td></td></tr>

<tr><td><b>qhapaq</b></td>
<td>rich, powerful</td>
<td><b>gabshu</b></td>
<td>massive, powerful (Ethiopic)</td>
<td></td></tr>

<tr><td><b>qocha</b></td>
<td>lake, pond</td>
<td><b>gubshu</b></td>
<td>mass of water (Assyr.)</td>
<td></td></tr>

<tr><td><b>llantin</b></td>
<td>type of herb</td>
<td><b>gam-gam</b></td>
<td>a herb (Assyr.)</td>
</tr></tbody></table>

<p>There are a number of obvious problems with this list.
</p><ul>
<li> The compiler (whose name I omit out of charity) knows nothing about the comparative method; no regular correspondences are presented.  
</li><li> Nor does he know much <a href="https://healeycodes.com/quechua.html">about Quechua</a>; he has for instance consistently taken the regular nominalizing suffix -<b>na </b>as part of the root.  (Note what this does to a &#39;resemblance&#39; like <i>aysana / tsina</i>.)  
</li><li> -<b>mu</b>- in <i>hamuy</i> is a movement suffix, leaving the rather unconvincing <i>ha-/amumuna</i>.  Likewise -<b>pi</b> in <i>hayk&#39;api</i> is a locative suffix, leaving <i>hay&#39;ka/akka&#39;iki.</i>
</li><li> Many of the resemblances are based on secondary meanings of Quechua roots.  For instance, &#39;disturb&#39; is a very secondary meaning of the causative <i>hatarichiy</i>; the meaning of the basic root <i>hatariy</i> is &#39;rise up&#39;.  
</li><li> It&#39;s quite naive to compare individual Semitic languages with modern Cuzqueño dialect.  On the Semitic side proto-Semitic or proto-Afro-Asiatic should be used; and on the Quechua side, reconstructed proto-Quechua.  We also know some words in an even earlier form; for instance <i>qocha</i> is related to Aymara <i>qota</i>-- which looks even less like the proposed cognate <i>gubshu</i>.</li></ul>

However, my only concern here is to answer the compiler&#39;s question: &#34;Is it a mere coincidence that there are so many correspondances between these languages?&#34;

<p>The above criticisms cannot answer this question; but the statistical model developed here can.

</p><p>First let&#39;s estimate the degree of <b>phonetic</b> laxness the compiler is allowing.  I&#39;ll use my Quechua frequency table, but I don&#39;t have similar data for the Semitic languages.</p><ul>

<li> He&#39;s fairly strict on initials.  Some have two alternatives (h, q, t, w, ch&#39;), but others match just one consonant (k, m, p, qh, r, ll, ch).  Initial <b>a </b>matches four sounds.  The pool of Semitic consonants matched is small: a, b, ch, d, g, k,  p, s, 0.  So the probability of an initial match is <i>p</i>(h, q, t, w, ch&#39;) * 2/9 + <i>p</i>(k, m, p, qh, r, ll, ch) * 1/9 + <i>p</i>(a) * (4/9) which works out to .14.
</li><li> He&#39;ll allow about any medial vowels to match; but over half the time he does have a close vowel match.  To estimate the probability of this we&#39;d need to know the Semitic vowel probabilities, which we can hardly take as equiprobable-- in Quechua, for instance, 56% of medial vowels are <b>a</b>&#39;s.
</li><li> Medial consonants match between 1 and 6 consonants: e.g. <b>k</b> matches k, g, h; <b>q</b> matches g, ch, q, k, t, 0.  The pool of Semitic consonants matched is larger, with 16 members.  The cumulative probability of a medial consonant match comes out to .524.
</li><li> Almost all the Quechua words end in a vowel, and this can match most anything as well.  Where the Quechua word ends in a consonant, the comparer ignores it, except in the single instance <i>llantin/gam-gam</i>.</li></ul>

37 out of the 54 matches involve just two matches (initial and medial); 17 have two medial consonant matches.  Let&#39;s start by seeing how many <b>two-consonant matches</b> he can expect.  The probability for a single phonetic match should be .17 * .524 = 0.089.  For the resemblances with a vowel match as well, we can estimate a fifth of this or .018.

<p>What&#39;s the level of <b>semantic</b> laxness?  This is hard to say.  Some matches are quite close (<i>burp, copper, basket, son, frog, owl</i>); others are fairly remote (<i>leader/priest; snake/teach, jerky/drought, pancreas/belly; sickness/afflict; door/open; sweet tooth/glutton; small thing/suck; god/father; build/disturbed; domineering/youthful; wall/fortress; stone seat/throne room; two herb names</i>).  I think it would be quite conservative to assume 1 Quechua word can match 20 Semitic words in the compiler&#39;s mind.  

</p><p>Given this semantic leeway, the probability of a match on a single Quechua word is 1 - ((1 - <i>p</i>)<i><SPAN size="2"><sup>m</sup></SPAN></i> = 1 - 0.911<SPAN size="2"><sup>20</sup></SPAN> = .845.  That&#39;s quite telling right there-- it means that, given his phonetic and semantic laxness, the comparer is <i>ordinarily</i> going to find a random match for <i>almost every Quechua word</i>.  

</p><p>My own Quechua dictionary has about 2000 non-Spanish roots.  Our comparer will very likely find more than 1500 chance resemblances.

</p><p>With a vowel match-- which, I emphasize, the comparer bothers with only half the time-- the match probability becomes .302, for roughly 600 chance resemblances.

</p><p>It&#39;s just not that hard to match just two consonants.  How about the <b>three-consonant matches</b>?  Given the initial and medial consonant match probabilities calculated above, the probability of a single 3-consonant exact-semantic match is .17 * .524 * .524 = 0.047.  With the same semantic leeway of 20 words, the probability of a match per word becomes 1 - 0.953<SPAN size="2"><sup>20</sup></SPAN> = .618.

</p><p>Our formula gives:

</p><p>p( 1101 to 1200 ) = .051
</p><p>Is it a mere coincidence that there are so many correspondances between these languages?  No, it isn&#39;t; what would be really surprising would be if there weren&#39;t a thousand more of the same quality.

</p><p>With a vowel match, the match probability becomes .171 and we&#39;d expect over 300 resemblances.  However, there are only eight words in the list that can be described as matching three consonants and a vowel.  Three of those are ruined by including -<i>na</i> as part of the root, and the remaining five include such uninspiring matches as <i>q/t</i> and <i>ll/g</i>. 

</p><h3><SPAN color="#000060">Characteristics of the model </SPAN></h3>

What we have seen offers quantified support for what many linguists would have suspected: <b>the number of chance resemblances soars as phonetic and semantic matches are loosened.

</b><p><b></b>The actual variation is almost <b>linear</b>.  That is, allow word <i>a</i> to match <i>n</i> words phonetically and <i>m</i> words semantically, and you very nearly increase the expected number of matches by <i>n * m</i>.

</p><p>Thus, the <b>results depend almost entirely on the value of </b><b><i>n</i></b><b> and </b><b><i>m</i></b>, and small variations in either <i>n</i> or <i>m</i> become very <b>large variations</b> in the number of matches. 

</p><p>Calculating an expected number of matches, then, it is essential to estimate phonetic and semantic laxness <b>from the claim being evaluated</b>.  There is no such thing as a general &#34;number of random chances expected&#34;.  It depends almost entirely on what you count as a match.

</p><p>Equally, it&#39;s necessary to <b>carefully evaluate any probability calculations</b> offered by the comparer.  Comparers typically present calculations for extremely narrow matches, and then give very loose matches in their word lists.   And they often ignore semantic looseness entirely.  Such errors are not trivial in this game; they can produce numbers that are off by several orders of magnitude.

</p><p><b>Why are we so easy to fool?</b>

</p><p>Why do people fool themselves so easily in this area?  Why is it so hard for even highly intelligent people to convince themselves that random matches will be few?  I think there&#39;s several reasons.</p><ul>

<li> <b>We want to be fooled</b>.  The idea that far-flung languages are related (and with them their speakers) is intriguing.  Proto-World is exciting.  Slogging through the comparative method, by contrast, is dull and too often gives the completely unwanted answer &#34;I don&#39;t know.&#34;
</li><li> <b>The brain is a pattern matchin&#39; machine</b>.  We evolved to quickly find patterns in the world.  It&#39;s not hard to see that this could be a matter of survival (<i>that red striped plant gives you a stomach ache</i>) or give an edge in social competition (<i>I know that the red striped plant is bad for you and Tumba doesn&#39;t</i>; or, <i>Roger&#39;s eyebrows flutter when he has a good hand</i>).  It&#39;s so useful to find patterns that the brain is very tolerant of false patterns.  
</li><li> <b>The brain is no good at probabilities</b>.  We simply have no great intuitive feel for probabilities.  Most people&#39;s eyes glaze over when you start talking about the chances of the chances of <i>r</i> events among <i>n</i> objects over <i>t</i> trials with a single event probability of <i>p</i>.  </li></ul>

When it comes to specific calculations of probability, which so often 
&#34;prove&#34; that the chance of a random match is vanishingly small, more factors come into play. <ul>

<li> <b>We&#39;re skeptical only of numbers we don&#39;t like</b>.  You don&#39;t have to be trying to defraud anyone to fool yourself.  But it&#39;s hard not to avoid our own bias in favor of numbers that go the way we want them to.  If they don&#39;t, we&#39;re displeased, and skeptically examine our calculations, and seize on new assumptions that create better numbers.  If the numbers do behave, we look over our results and our assumptions more carelessly.  (Feynman gives a nice example: an incorrect estimate of a physical constant, which took decades to be corrected, each correction edging toward the correct value.  Since the original estimate was assumed to be correct, scientists were more than usually cautious about new estimates that deviated from it, and easily convinced themselves that they&#39;d made a mistake somewhere.  Estimates near the accepted value were given more slack.) 
</li><li> <b>We don&#39;t double-check our results against reality</b>.  Few who make these calculations have <i>looked</i> for random matches in languages they&#39;re sure are <i>unrelated</i>.  In other words, they have no control case to check their results against.  It&#39;s no wonder they never notice how common random matches really are.  
 
<p>(I <i>have</i> looked for random matches, and found plenty of them; see my <a href="https://healeycodes.com/proto.html#chinesequechua">lists of Chinese/Quechua</a> and <a href="https://healeycodes.com/proto.html#chineseenglish">Chinese/English pseudo-cognates</a>.)

</p></li><li> <b>We haven&#39;t worked the numbers</b>.  Even trained linguists, though they know that random matches will occur, generally can&#39;t say how many.   
</li><li> <b>Statistical or linguistic ignorance</b>.   The calculations are often ruined by elementary statistical errors, or by wildly unrealistic linguistic assumptions, or by disregard of the enormous phonetic and semantic leeway comparers allow themselves.</li></ul>

This document can hopefully help out in this area.  The calculations are here and you can use them to evaluate claims (amateur or professional), or refer others to them.  

<h3><SPAN color="#000060">A computer program for calculating matches </SPAN></h3>

Here&#39;s a very simple computer program for applying the probability 
calculation described above.  It will work for most C compilers, 
but if the probability is very high, you&#39;ll need a very robust 
<tt>double</tt> data type, which I have on the Mac but not Windows.

<p>The important parameters are the  lexicon size <b>n</b>, the probability of a phonetic match <b>p</b>, and the semantic leeway <b>semN </b>(how many meanings count as a match).   

</p><p>The program uses some algebraic tricks to simplify the calculation of
the binomial probability.  For instance, we&#39;re always dividing by r!,
but we always calculate the probability for each r starting at one.
So we can start with rfac = 1.0, and just multiply by r each iteration
to get r!.

</p><p>The rest of the factorial calculation is <tt>n! / (n-k)!</tt>. 
For r = 1, this is <tt>n! / (n - 1)!</tt> which can be rewritten 
<tt>n (n-1)! / (n-1)!</tt>; the <tt>(n-1)!</tt> terms cancel, leaving us with just n.
For r = 2, we have <tt>n! / (n - 2)!</tt>, which is the same as
<tt>n (n-1) (n-2)! / (n-2)!</tt>, which reduces to <tt>n (n-1)</tt>.
That in turn is just (n-1) times the value we got for r = 1.
It works out that factor for each value of r is just (n - r + 1) times the 
value for the previous iteration, and that&#39;s how <tt>nfac_nrfac</tt> is
calculated below.

</p><pre><tt>
#include &lt;stdio.h&gt;
</tt></pre>

Here&#39;s the program output generated when the parameters are set as shown.  Note that stopat and bin are reporting parameters.  <b>stopat</b> = 70 told the computer to calculate propabilities for r = 1 to 70; <b>bin</b> = 10 told it to report these probabilities in groups of 10.  With a little trial and error you can adjust these values for the most useful reporting of results.

<pre>Probability 0.01982
</pre>

<p>The probability reported at the top is the phonetic probability, adjusted to reflect the given semantic leeway. 

</p><p>When the probability (after the semantic adjustment) is very low, .002 or less, you should set the bin size to 1 for best results.

</p><h3><SPAN color="#000060">Acknowledgments </SPAN></h3>

Thanks to the people on sci.lang, particularly Jacques Guy,
Mark Hubey, Ross Clark, Mikael Thompson, and Brian M. Scott, 
for discussions which led to the writing of this paper.

<hr/>

© 1998-99, 2002 by Mark Rosenfelder

<a href="https://healeycodes.com/default.html">[ Home ]</a>



</div>
  </body>
</html>
