<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://freetubeapp.io/">Original</a>
    <h1>FreeTube – A Private YouTube Client</h1>
    
    <div id="readability-page-1" class="page"><div>
    
    <div>
      <p>Sid Shanker - <time datetime="2022-11-26T04:00:38+00:00" itemprop="datePublished">Nov 26, 2022</time></p>
      
    </div>
  </div><div itemprop="articleBody">
      <p>[scroll down if you just want to get to the images]</p>

<p>Stable Diffusion is an amazing new open source machine learning model that can be used
to convert text to images. Given a text prompt like, “Painting of a chicken with colorful feathers”,
you might get something like this:</p>

<p><img src="https://squidarth.com/assets/chicken-colorful-feathers.png" alt="chicken-colorful-feathers" width="400"/></p>

<p>You can give it a shot yourself on this <a href="https://app.baseten.co/apps/VBlnMVP/operator_views/nBrd8zP">page</a> 
(disclosure, I work for <a href="http://baseten.co/">Baseten</a>, which hosts this demo).</p>

<p>As-is, Stable Diffusion can produce some pretty amazing results and is pretty fun to play with.
However, the images that Stable Diffusion can produce are limited to the dataset is trained on.
If you want it to produce images of your dog, for instance, it unfortunately has no way of doing
that out of the box.</p>



<p>While Stable Diffusion can’t do this out of the box, techniques have been developed to easily
“fine-tune” the model on new images. <a href="https://dreambooth.github.io/">Dreambooth</a> out of Google
gives Stable Diffusion users the ability to provide a small number of images (~20), and teach
Stable Diffusion a new concept. The main things that you can do with Dreambooth are:</p>
<ul>
  <li>Teach Stable Diffusion a new style, to allow it to create images in a particular artist style (like <a href="https://huggingface.co/nitrosocke/Ghibli-Diffusion">Studio Ghibli</a>)</li>
  <li>Teach Stable Diffusion a new concept, like your dog, or yourself.</li>
</ul>

<p>I decided to try out the second approach, and teach SD about myself! I used <a href="https://colab.research.google.com/drive/1PsOTCIMONQe2wUWsWSmWXlcvTN_MXHtQ#scrollTo=qEsNHTtVlbkV">this collab notebook</a>, that I borrowed from
<a href="https://github.com/TheLastBen/fast-stable-diffusion">TheLastBen’s Stable Diffusion Repo</a>.</p>

<p>The notebook was very easy to clone and try out. I used around 20 images to fine-tune the model. Here are
some of the results:</p>

<p><img src="https://squidarth.com/assets/sid-favorite%20painting.png" alt="sid-painting" width="400"/></p>

<p><em>Painting of Sid Shanker</em></p>

<p><img src="https://squidarth.com/assets/sid-rembrandt-1.png" alt="sid-rembrandt" width="400"/></p>

<p><em>Painting of Sid Shanker in the style of Rembrandt</em></p>

<p><img src="https://squidarth.com/assets/sid-pixar.png" alt="sid-pixar" width="400"/></p>

<p><em>Sid Shanker in the style of pixar</em></p>

<p><img src="https://squidarth.com/assets/sid-family-guy.png" alt="sid-pixar" width="400"/></p>

<p><em>Sid Shanker in the style of Family Guy</em></p>



<h2 id="fine-tuning-requires-a-lot-of-experimentation">Fine-tuning requires a lot of experimentation</h2>

<p>This didn’t work immediately! It took a couple tries to fiddle around
the input prompts and the images I uploaded to start getting good
results. One of the early mistakes was using too generic a
name for myself (“Sid Shanker”), which resulted in me seeing results like this:</p>

<p><img src="https://squidarth.com/assets/sid-disney-villain-2.png" alt="sid-disney-villain-2" width="400"/></p>

<p>Here, it doesn’t look trained to me at all. After changing the prompt to being something
more unusual (“squidarth”, my Github handle), I started getting the results I wanted.</p>

<p>The other big lever is using the right mix of images (&gt; 20, different angles, high resolution).</p>

<h2 id="you-have-to-know-a-little-bit-about-whats-going-on">You have to know a little bit about what’s going on</h2>

<p>The other interesting thing here I observed is that fine-tuning right now
isn’t a totally clean abstraction – you do have to know a little bit
about what’s going on, and what problems a tool like Dreambooth sets out to solve
to get the results that you want.</p>

<p>A major example here is that it’s really easy to overfit with Dreambooth training.
When I tried running “Sid with other person”, I got this monstrosity:</p>

<p><img src="https://squidarth.com/assets/sid-with-person.png" alt="sid-pixar" width="400"/></p>

<p>Huggingface has a <a href="https://huggingface.co/blog/dreambooth#tldr-recommended-settings">really good guide</a> on how to configure the settings for your project,
but interpreting these understands some amount of knowledge about Stable Diffusion (what’s the text encoder? what’s the unet?)</p>

<p>It’s hard to get away from requiring that level of configurability, since you likely want different
settings for training a style vs. training on a face.</p>

<h2 id="pre-trained-models-are-really-awesome">Pre-trained models are really awesome</h2>

<p>It’s really awesome that someone like me, with a fairly limited amount of ML experience,
was able to fine-tune a model like this to get the results I wanted. The power that
pre-trained models bring to developers is very exciting.</p>

  </div></div>
  </body>
</html>
