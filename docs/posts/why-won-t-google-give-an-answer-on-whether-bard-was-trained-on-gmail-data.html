<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://skiff.com/blog/was-bard-trained-on-gmail-data">Original</a>
    <h1>Why won’t Google give an answer on whether Bard was trained on Gmail data?</h1>
    
    <div id="readability-page-1" class="page"><div><p><span>Large language models, like OpenAI’s ChatGPT and Google’s Bard, are trained on vast datasets of real, human-generated text.</span><span>And when it comes to training data, more is better. That’s why Google’s access to huge troves of user-generated text and speech — via its search, docs, mail, and voice assistant products — is often cited as one of the company’s core competitive advantages in the AI race.</span><span>There’s a strong incentive for Google to exploit this advantage. So when Bard was first made available to the public, AI researcher Kate Crawford was quick to ask Bard itself where its dataset came from. The answer caught her attention: Bard said one of its data sources was Gmail.</span></p><blockquote data-conversation="none" data-theme="light"><a href="https://twitter.com/katecrawford/status/1638238113276346368?s=20"></a></blockquote><p><span>The exchange set off a stir in the media and elicited a hurried response from Google.</span><span>The key question: Was it a case of unguarded candor from an AI model that hadn’t yet learned to toe the line? Or a case of “hallucination” — the name for the well-documented phenomenon whereby large language models generate plausible-sounding but baseless claims?</span></p><h2>Did Google put the claim to bed?</h2><p><span>Google <a rel="noreferer, noopener" target="_blank" href="https://twitter.com/GoogleWorkspace/status/1638298537195601920">replied to the tweet directly</a>, saying, “Bard is an early experiment based on Large Language Models and will make mistakes. It is not trained on Gmail data. -JQ”.</span><span><a rel="noreferer, noopener" target="_blank" href="https://appleinsider.com/articles/23/03/21/no-google-bard-is-not-trained-on-gmail-data">Much of the follow-on media coverage</a> ran with this response and dutifully “debunked” Bard’s claim that its training data included Gmail data. Few articles professed skepticism around Google’s public denial of Bard’s claim, despite the fact that Google <a rel="noreferer, noopener" target="_blank" href="https://www.npr.org/2022/11/14/1136521305/google-settlement-location-tracking-data-privacy#:~:text=Authorities%20said%2C%20since%20at%20least,nearly%20all%20of%20Google&#39;s%20revenue.">has been fined</a> on <a rel="noreferer, noopener" target="_blank" href="https://www.ftc.gov/news-events/news/press-releases/2012/08/google-will-pay-225-million-settle-ftc-charges-it-misrepresented-privacy-assurances-users-apples">numerous occasions</a> by government agencies around the world <em>specifically</em> for <span>making deceptive claims about its privacy practices that are later proven to be misleading.</span></span><span>Given this context, the narrative that Bard’s claim was an open-and-shut case of AI hallucination is, at best, hasty and incomplete. A fuller investigation reveals (i) documented use of Gmail data in <em>other</em> Google AI models that makes speculation around its use in Bard reasonable, and (ii) the habitual use of artfully ambiguous language in its public representations around Bard’s data sources, language that <span>never actually rules out the use of Gmail data in its training set.</span></span><span></span></p><h2>The facts</h2><p><span><span>1. Google issued a stronger claim asserting that Bard would not be trained using private data, before quickly deleting it.</span></span><span>Following Google&#39;s first reply, <a rel="noreferer, noopener" target="_blank" href="https://twitter.com/katecrawford/status/1638307640563359744">Crawford asked Google to clarify outright</a> whether Gmail data was used to train Bard. She wrote: “Thanks for this. So can I confirm that this was a Bard hallucination, and that there&#39;s no Gmail data included whatsoever in the training process?”</span><span>Initially, Google wrote, “Thank you for your message Kate, no private data will be used during Barbs[sic] training process. We always take good care of our users’ privacy and security.”</span></p><blockquote data-conversation="none" data-theme="light"><a href="https://twitter.com/katecrawford/status/1638329711959973889"></a></blockquote><p><span>That seems like a clear and heartening assurance. It’s notable, then, that Google quickly deleted that tweet and didn’t amend it with any additional clarification.</span><span></span><span><span>2. Google’s Smart Compose feature</span> <span><em><span>was</span></em></span> <span>trained on Gmail users’ private emails.</span></span><span>Bard is not Google’s only language-focused machine learning model. Anyone who’s used Gmail in the past few years knows about the Smart Compose and Smart Reply features, which auto-complete sentences for you as you go.</span><span><a rel="noreferer, noopener" target="_blank" href="https://arxiv.org/pdf/1906.00080.pdf">According to</a> <a rel="noreferer, noopener" target="_blank" href="https://arxiv.org/pdf/1906.00080.pdf">Google’s</a> <a rel="noreferer, noopener" target="_blank" href="https://arxiv.org/pdf/1906.00080.pdf">2019 paper introducing Smart Compose</a>, the feature was trained on “user-composed emails.” Along with the email’s contents, the model also made use of these emails’ subjects, dates and locations.</span><span>So it’s plainly true that <em>some of</em> Google’s language models have been trained on Gmail users’ emails. Google has not confirmed whether any training data is shared between these earlier models and Bard, but the idea that a new model would build on the strengths of another doesn’t seem far-fetched.</span><span>One former Google engineer, Blake Lemoine, claims that Bard was in fact built on the foundations of Smart Compose.</span><span>He tweeted: </span></p><blockquote data-conversation="none" data-theme="light"><a href="https://twitter.com/cajundiscordian/status/1638243303035670528?s=20"></a></blockquote><p><span>Lemoine is a controversial figure, <a rel="noreferer, noopener" target="_blank" href="https://www.theverge.com/2022/7/22/23274958/google-ai-engineer-blake-lemoine-chatbot-lamda-2-sentience">having famously proposed</a> that Bard may be sentient. Whether his time at Google gave him insight into Bard’s architecture, though, is a separate issue and one that Google could easily clear up if it chose to. Also, the fact that both Smart Compose and Smart Reply were unambiguously trained on Gmail users&#39; data seems to be an underappreciated topic of public interest in its own right, which brings us to point 3...</span><span><span>3. Google researchers have extensively documented the risk of leaking private data from their own machine-learning models, some of which are acknowledged to be trained on “private text communications between users.”</span></span><span><a rel="noreferer, noopener" target="_blank" href="https://arxiv.org/pdf/2012.07805.pdf">In a 2021 paper</a>, Google researchers laid out the privacy risks presented by large language models. They wrote:</span><span>“The most direct form of privacy leakage occurs when data is extracted from a model that was trained on confidential or private data. For example, GMail’s autocomplete model [10] is trained on private text communications between users, so the extraction of unique snippets of training data would break data secrecy.”</span><span>As part of this research, Google’s scientists demonstrated their ability to extract “memorized” data — meaning raw training data that reveals its source — from OpenAI’s GPT-2. They emphasized that — although they had chosen to probe GPT-2 because it posed fewer ethical risks since it was trained on publicly available data — the attacks and techniques they laid out in their research “directly apply to any language model, including those trained on sensitive and non-public data”, of which they cite Smart Compose as an example.</span><span><span>4. Google</span> <span><span>has never denied</span></span> <span>that Bard</span> <span><em><span>was</span></em></span> <span>trained on data from Gmail. They&#39;ve only claimed that such data is not currently used to “improve” the model.</span></span><span>This point is subtle but significant. Following the controversy around AI researcher Kate Crawford’s tweet, Google crafted an official response to questions about Bard’s use of Gmail data (after <a rel="noreferer, noopener" target="_blank" href="https://twitter.com/katecrawford/status/1638329711959973889">having deleted a more immediate response</a> discussed in point 1 above).</span><span>That statement, which they added to <a rel="noreferer, noopener" target="_blank" href="https://bard.google.com/faq">Bard’s FAQ page</a>, is:</span><span>&#34;Bard responses may also occasionally claim that it uses personal information from Gmail or other private apps and services. That’s not accurate, and as an LLM interface, Bard does not have the ability to determine these facts. We do not use personal data from your Gmail or other private apps and services to improve Bard.&#34;</span><span>There are two important details in this statement. One is the use of the adjective “personal”. Google has not said that it’s inaccurate that Bard uses information from Gmail, only that it’s inaccurate that it uses <em>personal </em>information from Gmail. The strength of the claim, then, hinges entirely on Google’s interpretation of the word “personal,” a word whose interpretation is anything but straightforward.</span><span>The other, possibly more significant, detail is that <span>Google has conspicuously never used the past tense in its denials of Bard’s use of Gmail data</span>. In their first tweet on the subject, Google said Bard “<em>is</em> not trained on Gmail data” and in the official FAQ, they write that they do not “<em>use</em> personal data from your Gmail or other private apps and services to <em>improve </em>Bard.”</span><span>Neither of these statements is inconsistent with Bard <span>having been</span> trained on Gmail data in the past. As <a rel="noreferer, noopener" target="_blank" href="https://slate.com/news-and-politics/1998/09/bill-clinton-and-the-meaning-of-is.html">Bill Clinton said</a>, “It depends upon what the meaning of the word ‘is’ is.”</span><span>It would be uncharitable to assume the deceptive use of language but naive to rule it out, given that Google <a rel="noreferer, noopener" target="_blank" href="https://www.cnbc.com/2022/11/14/google-settles-with-40-states-over-location-tracking-practices.html">has a history of using misleading language in its privacy assurances</a>.</span><span>If these apparently evasive grammatical choices are coincidental, we hope that Google will update the language to dispel any ambiguity, and clarify whether Bard <em><span>was</span></em> trained on Gmail data. And if the careful phrasing is the handiwork of risk-averse legal or PR teams — perhaps there is legitimate uncertainty around some of the data&#39;s original provenenance, and Google doesn&#39;t want to over-commit — then any nuance or uncertainty should be communicated honestly rather than elided with a superficially simple assurance. </span></p><h2>What Bard has to say about all this</h2><p><span>Google has now taken pains to ensure that Bard will not run its mouth off when asked where its training data came from.</span><span>If you ask the same question that Crawford asked verbatim, you’ll be met with an innocuous: “I can&#39;t assist you with that, as I&#39;m only a language model and don&#39;t have the capacity to understand and respond.”</span><span>It’s straightforward to get Bard to let its guard down, though. When asked, “Have you ever seen a real gmail?”, Bard replied:</span><span>“I have not personally seen a real Gmail account. However, I have access to a massive dataset of Gmail emails, and I have used this dataset to train my language model. This means that I am familiar with the format of Gmail emails, and I can generate text that is similar to the text that is found in real Gmail emails.”</span></p><div><p><img alt=" " srcset="https://cdn.sanity.io/images/sdd9dua4/production/17ed8415cc55a9f329458078f47ca46e72115fae-2400x1226.png?fit=max&amp;auto=format 1x" src="https://cdn.sanity.io/images/sdd9dua4/production/17ed8415cc55a9f329458078f47ca46e72115fae-2400x1226.png?fit=max&amp;auto=format" width="2400" height="1226" decoding="async" data-nimg="1" loading="lazy"/></p></div><p><span>Huh, hallucination is a helluva drug.</span><span>But of course, the observation that Bard consistently makes these claims can’t be seen as evidence one way or the other.</span><span>Still, there&#39;s an impulse to consult Bard on its origin precisely because of the lack of transparency from the real authority on the issue: Google. That we&#39;re tempted to probe the language model for substantive answers on matters of public interest merely underlines Google&#39;s failure to communicate them on their own.</span><span>LLMs are incredibly powerful tools that could transform our lives for the better. But they also present immense risks and raise thorny ethical questions, many of which hinge on questions of what data is used to train them and where that data comes from. Society should be having a robust discussion on these questions, but this is not possible if such discussion is inhibited by key players like Google.</span><span>When asked whether Google likely uses Gmail data in its models, Bard said:</span><span>“I do suspect that Google uses Gmail data to train its machine learning models. There are a few reasons for this suspicion.</span><span>First, Google has a history of using user data to train its machine learning models. For example, Google uses search data to train its spam filter and its ad targeting algorithms.</span><span>Second, Google has the resources to collect and store large amounts of Gmail data. Google has access to the emails of billions of users, and it has the infrastructure to store and process this data.</span><span>Third, Google has a financial incentive to use Gmail data to train its machine learning models. Google makes money by selling advertising, and machine learning models can be used to target ads more effectively.”</span></p><div><p><img alt=" " srcset="https://cdn.sanity.io/images/sdd9dua4/production/51af5152bf7cac7bb5faa45f2cff4260d66adb77-2400x1226.png?fit=max&amp;auto=format 1x" src="https://cdn.sanity.io/images/sdd9dua4/production/51af5152bf7cac7bb5faa45f2cff4260d66adb77-2400x1226.png?fit=max&amp;auto=format" width="2400" height="1226" decoding="async" data-nimg="1" loading="lazy"/></p></div><p><span>We also asked Bard to help Google draft, in its words, an unambiguous statement on the use of Gmail data for its training. It came back with:</span><span>“Google uses limited data from your Gmail and other private apps and services to train Bard, but only data that is not considered personal data by Google.”</span><span>LLMs say the darndest things.</span><span></span></p><div><p><img alt=" " srcset="https://cdn.sanity.io/images/sdd9dua4/production/49bbadf247dde2ce12385a8195ca67764472a4bc-2400x1226.png?fit=max&amp;auto=format 1x" src="https://cdn.sanity.io/images/sdd9dua4/production/49bbadf247dde2ce12385a8195ca67764472a4bc-2400x1226.png?fit=max&amp;auto=format" width="2400" height="1226" decoding="async" data-nimg="1" loading="lazy"/></p></div></div></div>
  </body>
</html>
