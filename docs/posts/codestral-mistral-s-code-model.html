<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/codestral/">Original</a>
    <h1>Codestral: Mistral&#39;s Code Model</h1>
    
    <div id="readability-page-1" class="page"><div><p>We introduce Codestral, our first-ever code model. Codestral is an
open-weight generative AI model explicitly designed for code generation
tasks. It helps developers write and interact with code through a shared
instruction and completion API endpoint. As it masters code and English,
it can be used to design advanced AI applications for software
developers.</p><h4 id="a-model-fluent-in-80-programming-languages">A model fluent in 80+ programming languages</h4><p>Codestral is trained on a diverse dataset of 80+ programming languages,
including the most popular ones, such as Python, Java, C, C++,
JavaScript, and Bash. It also performs well on more specific ones like
Swift and Fortran. This broad language base ensures Codestral can assist
developers in various coding environments and projects.</p><p>Codestral saves developers time and effort: it can complete coding
functions, write tests, and complete any partial code using a
fill-in-the-middle mechanism. Interacting with Codestral will help level
up the developer’s coding game and reduce the risk of errors and bugs.</p><h4 id="setting-the-bar-for-code-generation-performance">Setting the Bar for Code Generation Performance</h4><p><strong>Performance.</strong> As a 22B model, Codestral sets a new standard on the
performance/latency space for code generation compared to previous
models used for coding.</p><p><img src="https://mistral.ai/images/news/codestral/General_table.png" alt="Detailed benchmarks" width="100%"/></p><p>Figure 1: With its larger context window of 32k (compared to 4k, 8k or 16k for
competitors), Codestral outperforms all other models in RepoBench, a
long-range eval for code generation..</p><p>We compare Codestral to existing code-specific models with higher
hardware requirements.</p><p><strong>Python.</strong> We use four benchmarks: HumanEval pass@1, MBPP sanitised
pass@1 to evaluate Codestral’s Python code generation ability, CruxEval
to evaluate Python output prediction, and RepoBench EM to evaluate
Codestral’s Long-Range Repository-Level Code Completion.</p><p><strong>SQL.</strong> To evaluate Codestral’s performance in SQL, we used the Spider
benchmark.</p><p><img src="https://mistral.ai/images/news/codestral/HumanEval_table.png" alt="Detailed benchmarks" width="100%"/></p><p>Additional languages. Additionally, we evaluated Codestral&#39;s performance in multiple HumanEval pass@1 across six different languages
in addition to Python: C++, bash, Java, PHP, Typescript, and C#, and
calculated the average of these evaluations.</p><p><img src="https://mistral.ai/images/news/codestral/FIM_table.png" alt="Detailed benchmarks" width="100%"/></p><p>FIM benchmarks. Codestral&#39;s Fill-in-the-middle performance was assessed using HumanEval pass@1 in Python, JavaScript, and Java and
compared to DeepSeek Coder 33B, whose fill-in-the-middle capacity is
immediately usable.</p><h4 id="get-started-with-codestral">Get started with Codestral</h4><h5 id="download-and-test-codestral">Download and test Codestral.</h5><p>Codestral is a 22B open-weight model licensed under the new <em><a href="https://mistral.ai/news/mistral-ai-non-production-license-mnpl/">Mistral AI
Non-Production License</a></em>, which means that you can use it for research
and testing purposes. Codestral can be downloaded on <a href="https://huggingface.co/mistralai/Codestral-22B-v0.1">HuggingFace</a>.</p><h5 id="use-codestral-via-its-dedicated-endpoint">Use Codestral via its dedicated endpoint</h5><p>With this release, comes the addition of a new endpoint: <code>codestral.mistral.ai</code>.
This endpoint should be preferred by users who use our Instruct or Fill-In-the-Middle routes inside their IDE. The API Key for this endpoint is managed at the personal level and isn’t bound by the usual organization rate limits. We’re allowing use of this endpoint for free during a beta period of 8 weeks and are gating it <a href="https://console.mistral.ai/codestral">behind a waitlist</a> to ensure a good quality of service. This endpoint should be preferred by developers implementing IDE plugins or applications where customers are expected to bring their own API keys.</p><h5 id="build-with-codestral-on-la-plateforme">Build with Codestral on La Plateforme</h5><p>Codestral is also immediately available on the usual API endpoint: <code>api.mistral.ai</code> where queries are billed per tokens. This endpoint and integrations are better suited for research, batch queries or third-party application development that exposes results directly to users without them bringing their own API keys.</p><p>You can create your account on <a href="https://console.mistral.ai">La Plateforme</a> and
start building your applications with Codestral by following <a href="https://docs.mistral.ai/capabilities/code_generation/">this guide</a>.
Like all our other models, Codestral is available in our self-deployment
offering starting today: <a href="https://mistral.ai/contact/">contact sales</a>.</p><h5 id="talk-to-codestral-on-le-chat">Talk to Codestral on le Chat</h5><p>We’re exposing an instructed version of Codestral, which is accessible
today through <a href="https://chat.mistral.ai/chat">Le Chat</a>, our
free conversational interface. Developers can interact with Codestral
naturally and intuitively to leverage the model&#39;s capabilities. We see
Codestral as a new stepping stone towards empowering everyone with code
generation and understanding.</p><h5 id="use-codestral-in-your-favourite-coding-and-building-environment">Use Codestral in your favourite coding and building environment.</h5><p>We worked with community partners to expose Codestral to popular tools
for developer productivity and AI application-making.</p><p><strong>Application frameworks.</strong> Codestral is integrated into LlamaIndex and
LangChain starting today, which allows users to build agentic
applications with Codestral easily</p><p><strong>VSCode/JetBrains integration.</strong> <a href="https://docs.mistral.ai/capabilities/code_generation/#integration-with-continuedev">Continue.dev</a> and <a href="https://docs.mistral.ai/capabilities/code_generation/#integration-with-tabnine">Tabnine</a> are empowering developers to use Codestral within the VSCode and JetBrains environments and now enable them to generate and chat with the code using Codestral.</p><p><a href="https://www.youtube.com/watch?v=mjltGOJMJZA">Here</a> is how you can use the Continue.dev VSCode plugin for code generation, interactive conversation, and inline editing with Codestral, and <a href="https://www.youtube.com/watch?v=pFa4NLK9Lbw">here</a> is how users can use the Tabnine VSCode plugin to chat with Codestral.</p><p>For detailed information on how various integrations work with
Codestral, please check <a href="https://docs.mistral.ai/capabilities/code_generation/">our documentation</a> for set-up instructions and examples.</p><p><em>“A public autocomplete model with this combination of speed and quality
hadn’t existed before, and it’s going to be a phase shift for developers
everywhere.”</em></p><p>– Nate Sesti, CTO and co-founder of Continue.dev</p><p><em>“We are excited about the capabilities that Mistral unveils and
delighted to see a strong focus on code and development assistance, an
area that JetBrains cares deeply about.”</em></p><p>– Vladislav Tankov, Head of JetBrains AI</p><p><em>“We used Codestral to run a test on our Kotlin-HumanEval benchmark and
were impressed with the results. For instance, in the case of the pass
rate for T=0.2, Codestral achieved a score of 73.75, surpassing
GPT-4-Turbo’s score of 72.05 and GPT-3.5-Turbo’s score of 54.66.”</em></p><p>– Mikhail Evtikhiev, Researcher at JetBrains</p><p><em>“As a researcher at the company that created the first developer
focused GenAI tool, I&#39;ve had the pleasure of integrating Mistal&#39;s new
code model into our chat product. I am thoroughly impressed by its
performance. Despite its relatively compact size, it delivers results on
par with much larger models we offer to customers. We tested several key
features, including code generation, test generation, documentation,
onboarding processes, and more. In each case, the model exceeded our
expectations. The speed and accuracy of the model will significantly
impact our product&#39;s efficiency vs the previous Mistral model, allowing
us to provide quick and precise assistance to our users. This model
stands out as a powerful tool among the models we support, and I highly
recommend it to others seeking high-quality performance.”</em></p><p>– Meital Zilberstein, R&amp;D Lead @ Tabnine</p><p><em>“Cody speeds up the inner loop of software development, and developers
use features like autocomplete to alleviate some of the day-to-day toil
that comes with writing code. Our internal evaluations show that
Mistral’s new Codestral model significantly reduces the latency of Cody
autocomplete while maintaining the quality of the suggested code. This
makes it an excellent model choice for autocomplete where milliseconds
of latency translate to real value for developers.”</em></p><p><em>–</em> Quinn Slack, CEO and co-founder of Sourcegraph</p><p><em>“I&#39;ve been incredibly impressed with Mistral&#39;s new Codestral model
for AI code generation. In my testing so far, it has consistently
produced highly accurate and functional code, even for complex tasks.
For example, when I asked it to complete a nontrivial function to create
a new LlamaIndex query engine, it generated code that worked seamlessly,
despite being based on an older codebase.”</em></p><p>– Jerry Liu, CEO and co-founder of LlamaIndex</p><p><em>“Code generation is one of the most popular LLM use-cases, so we are
really excited about the Codestral release. From our initial testing,
it&#39;s a great option for code generation workflows because it&#39;s fast,
has favorable context window, and the instruct version supports tool
use. We tested with LangGraph for self-corrective code generation using
the instruct Codestral tool use for output, and it worked really well
out-of-the-box (see our <a href="https://youtu.be/zXFxmI9f06M">video detailing this</a>).”</em></p><p>– Harrison Chase, CEO and co-founder of LangChain</p></div></div>
  </body>
</html>
