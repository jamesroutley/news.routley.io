<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openai.com/dall-e-2/">Original</a>
    <h1>Dall-E 2</h1>
    
    <div id="readability-page-1" class="page"><div id="post-dall-e-2" data-active="">


<div> <div>  <p>DALL·E 2 is a new AI system that can create realistic images and art from a description in natural language.</p>  </div> </div><div id="demos"><div> <div> <p>DALL·E 2 can create original, realistic images and art from a text description. It can combine concepts, attributes, and styles.</p>  </div> <div> <p>DALL·E 2 can make realistic edits to existing images from a natural language caption. It can add and remove elements while taking shadows, reflections, and textures into account.</p>  </div> <div> <p>DALL·E 2 can take an image and create different variations of it inspired by the original.</p>  </div></div></div><div id="research"><div> <p>DALL·E 2 has learned the relationship between images and the text used to describe them. It uses a process called “diffusion,” which starts with a pattern of random dots and gradually alters that pattern towards an image when it recognizes specific aspects of that image.</p> <div> <div> <p><span>play</span> </p> <iframe id="de2-video" src="https://player.vimeo.com/video/692375454" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen=""></iframe> </div> <p>DALL·E 2 Explained 2:47</p> </div> <p>In January 2021, OpenAI introduced <a href="https://openai.com/blog/dall-e/">DALL·E</a>. One year later, our newest system, DALL·E 2, generates more realistic and accurate images with 4x greater resolution.</p> <div> <div> <div> <p>DALL·E 1</p> <div> <p><img data-src="https://cdn.openai.com/dall-e-2/assets/dall-e-1.jpg"/> </p> </div> </div> </div> <div> <div> <p>DALL·E 2</p> <div> <p><img data-src="https://cdn.openai.com/dall-e-2/assets/dall-e-2.jpg"/> </p> </div> </div> </div> <div> <p>a painting of a fox sitting in a field at sunrise in the style of Claude Monet”</p> </div> </div> <p>DALL·E 2 is preferred over DALL·E 1 for its caption matching and photorealism when evaluators were asked to compare 1,000 image generations from each model.</p>  </div></div><div id="safety"><div> <p>DALL·E 2 is a research project which we currently do not make available in our API. As part of our effort to develop and deploy AI responsibly, we are studying DALL·E’s limitations and capabilities with a select group of users. Safety mitigations we have already developed include:</p> <div> <div> <div>  <div> <div> <p>Preventing Harmful Generations</p> <p>We’ve limited the ability for DALL·E 2 to generate violent, hate, or adult images. By removing the most explicit content from the training data, we minimized DALL·E 2’s exposure to these concepts. We also used advanced techniques to prevent photorealistic generations of real individuals’ faces, including those of public figures.</p> </div> </div> </div> </div> <div> <div>  <div> <div> <p>Curbing Misuse</p> <p>Our content policy does not allow users to generate violent, adult, or political content, among other categories. We won’t generate images if our filters identify text prompts and image uploads that may violate our policies. We also have automated and human monitoring systems to guard against misuse.</p> </div> </div> </div> </div> <div> <div>  <div> <div> <p>Phased Deployment Based on Learning</p> <p>We’ve been working with external experts and are previewing DALL·E 2 to a limited number of trusted users who will help us learn about the technology’s capabilities and limitations. We plan to invite more people to preview this research over time as we learn and iteratively improve our safety system.</p> </div> </div> </div> </div> </div> </div></div><div id="why"> <div> <p>Our hope is that DALL·E 2 will empower people to express themselves creatively. DALL·E 2 also helps us understand how advanced AI systems see and understand our world, which is critical to our mission of creating AI that benefits humanity.</p>  <div> <div> <p>Research Advancements</p> <p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen</p> </div> <div> <p>Engineering, Design, and Product</p> <p>Jeff Belgum, Dave Cummings, Chris Hallacy, Joanne Jang, Fraser Kelton, Vishal Kuo, Rachel Lim, Bianca Martin, Evan Morikawa, Rajeev Nayak, Glenn Powell, Krijn Rijshouwer, David Schnurr, Maddie Simens, Felipe Such, Chelsea Voss, Justin Jay Wang</p> </div> <div> <p>Comms, Policy, Legal, Ops, Safety, and Security</p> <p>Steven Adler, Lama Ahmad, Miles Brundage, Kevin Button, Che Chang, Fotis Chantzis, Derek Chen, Frances Choi, Steve Dowling, Elie Georges, Shino Jomoto, Aris Konstantinidis, Gretchen Krueger, Andrew Mayne, Pamela Mishkin, Bob Rotsted, Natalie Summers, Dave Willner, Hannah Wong</p> </div> <div> <p>Acknowledgments</p> <p>Thanks to those who helped with and provided feedback on this release: Sandhini Agarwal, Sam Altman, Chester Cho, Jonathan Gordon, Peter Hoeschele, Jacob Jackson, Shawn Jain, Jong Wook Kim, Matt Knight, Jason Kwon, Joel Lehman, Anna Makanju, Katie Mayer, Bob McGrew, Luke Miller, Mira Murati, Adam Nace, Hyeonwoo Noh, Cullen O’Keefe, Long Ouyang, Michael Petrov, Henrique Ponde de Oliveira Pinto, Alec Radford, Girish Sastry, Pranav Shyam, Aravind Srinivas, Kenneth Stanley, Ilya Sutskever, Preston Tuggle, Arun Vijayvergiya, Peter Welinder</p> </div> </div> </div> </div>


</div></div>
  </body>
</html>
