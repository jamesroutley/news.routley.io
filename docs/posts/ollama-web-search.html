<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ollama.com/blog/web-search">Original</a>
    <h1>Ollama Web Search</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    <h2>September 24, 2025</h2>
    
    
    <section>
      <p><a href="https://ollama.com/signup"><img src="https://files.ollama.com/ollama_web_search.png" alt="Ollama’s web search"/></a></p>

<p>A new web search API is now available in Ollama. Ollama provides a generous free tier of web searches for individuals to use, and higher rate limits are available via <a href="https://ollama.com/cloud">Ollama’s cloud</a>.</p>

<p>This web search capability can augment models with the latest information from the web to reduce hallucinations and improve accuracy.</p>

<p>Web search is provided as a REST API with deeper tool integrations in Ollama’s Python and JavaScript libraries. This also enables models such as OpenAI’s <a href="https://ollama.com/library/gpt-oss"><code>gpt-oss</code></a> models to conduct long-running research tasks.</p>

<h3 id="get-started">Get started</h3>

<p>Create an API key from your <a href="https://ollama.com/settings/keys">Ollama account</a>.</p>

<pre><code>export OLLAMA_API_KEY=&#34;your_api_key&#34;
</code></pre>

<h4 id="curl">cURL</h4>

<pre><code>curl https://ollama.com/api/web_search \
  --header &#34;Authorization: Bearer $OLLAMA_API_KEY&#34; \
  -d &#39;{
    &#34;query&#34;: &#34;what is ollama?&#34;
  }&#39;
</code></pre>

<p><strong>Example output</strong></p>

<pre><code>{
  &#34;results&#34;: [
    {
      &#34;title&#34;: &#34;Ollama&#34;,
      &#34;url&#34;: &#34;https://ollama.com/&#34;,
      &#34;content&#34;: &#34;Cloud models are now available...&#34;
    },
    {
      &#34;title&#34;: &#34;What is Ollama? Introduction to the AI model management tool&#34;,
      &#34;url&#34;: &#34;https://www.hostinger.com/tutorials/what-is-ollama&#34;,
      &#34;content&#34;: &#34;Ariffud M. 6min Read...&#34;
    },
    {
      &#34;title&#34;: &#34;Ollama Explained: Transforming AI Accessibility and Language ...&#34;,
      &#34;url&#34;: &#34;https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/&#34;,
      &#34;content&#34;: &#34;Data Science Data Science Projects Data Analysis...&#34;
    }
  ]
}
</code></pre>

<h4 id="python">Python</h4>

<p>Install and run Ollama’s Python library</p>

<pre><code>pip install &#39;ollama&gt;=0.6.0&#39;
</code></pre>

<p>Then make a request using <code>ollama.web_search</code>:</p>

<pre><code>import ollama
response = ollama.web_search(&#34;What is Ollama?&#34;)
print(response)
</code></pre>

<p><strong>Example output</strong></p>

<pre><code>results = [
    {
        &#34;title&#34;: &#34;Ollama&#34;,
        &#34;url&#34;: &#34;https://ollama.com/&#34;,
        &#34;content&#34;: &#34;Cloud models are now available in Ollama...&#34;
    },
    {
        &#34;title&#34;: &#34;What is Ollama? Features, Pricing, and Use Cases - Walturn&#34;,
        &#34;url&#34;: &#34;https://www.walturn.com/insights/what-is-ollama-features-pricing-and-use-cases&#34;,
        &#34;content&#34;: &#34;Our services...&#34;
    },
    {
        &#34;title&#34;: &#34;Complete Ollama Guide: Installation, Usage &amp; Code Examples&#34;,
        &#34;url&#34;: &#34;https://collabnix.com/complete-ollama-guide-installation-usage-code-examples&#34;,
        &#34;content&#34;: &#34;Join our Discord Server...&#34;
    }
]
</code></pre>

<h4 id="javascript">JavaScript</h4>

<p>Install and run Ollama’s JavaScript library</p>

<pre><code>npm install &#39;ollama@&gt;=0.6.0&#39;
</code></pre>

<pre><code>import { Ollama } from &#34;ollama&#34;;

const client = new Ollama();
const results = await client.webSearch({ query: &#34;what is ollama?&#34; });
console.log(JSON.stringify(results, null, 2));
</code></pre>

<p><strong>Example output</strong></p>

<pre><code>{
  &#34;results&#34;: [
    {
      &#34;title&#34;: &#34;Ollama&#34;,
      &#34;url&#34;: &#34;https://ollama.com/&#34;,
      &#34;content&#34;: &#34;Cloud models are now available...&#34;
    },
    {
      &#34;title&#34;: &#34;What is Ollama? Introduction to the AI model management tool&#34;,
      &#34;url&#34;: &#34;https://www.hostinger.com/tutorials/what-is-ollama&#34;,
      &#34;content&#34;: &#34;Ollama is an open-source tool...&#34;
    },
    {
      &#34;title&#34;: &#34;Ollama Explained: Transforming AI Accessibility and Language Processing&#34;,
      &#34;url&#34;: &#34;https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/&#34;,
      &#34;content&#34;: &#34;Ollama is a groundbreaking...&#34;
    }
  ]
}

</code></pre>

<h3 id="building-a-search-agent">Building a search agent</h3>

<p>Use Ollama’s web search as a tool to build a mini search agent.</p>

<p>The example uses Alibaba’s Qwen 3 model with 4B parameters.</p>

<pre><code>ollama pull qwen3:4b
</code></pre>

<pre><code>from ollama import chat, web_fetch, web_search

available_tools = {&#39;web_search&#39;: web_search, &#39;web_fetch&#39;: web_fetch}

messages = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#34;what is ollama&#39;s new engine&#34;}]

while True:
  response = chat(
    model=&#39;qwen3:4b&#39;,
    messages=messages,
    tools=[web_search, web_fetch],
    think=True
    )
  if response.message.thinking:
    print(&#39;Thinking: &#39;, response.message.thinking)
  if response.message.content:
    print(&#39;Content: &#39;, response.message.content)
  messages.append(response.message)
  if response.message.tool_calls:
    print(&#39;Tool calls: &#39;, response.message.tool_calls)
    for tool_call in response.message.tool_calls:
      function_to_call = available_tools.get(tool_call.function.name)
      if function_to_call:
        args = tool_call.function.arguments
        result = function_to_call(**args)
        print(&#39;Result: &#39;, str(result)[:200]+&#39;...&#39;)
        # Result is truncated for limited context lengths
        messages.append({&#39;role&#39;: &#39;tool&#39;, &#39;content&#39;: str(result)[:2000 * 4], &#39;tool_name&#39;: tool_call.function.name})
      else:
        messages.append({&#39;role&#39;: &#39;tool&#39;, &#39;content&#39;: f&#39;Tool {tool_call.function.name} not found&#39;, &#39;tool_name&#39;: tool_call.function.name})
  else:
    break
</code></pre>

<p><strong>Result</strong></p>

<pre><code>Thinking:  Okay, the user is asking about Ollama&#39;s new engine. I need to figure out what they&#39;re referring to. Ollama is a company that develops large language models, so maybe they&#39;ve released a new model or an updated version of their existing engine...

Tool calls:  [ToolCall(function=Function(name=&#39;web_search&#39;, arguments={&#39;max_results&#39;: 3, &#39;query&#39;: &#39;Ollama new engine&#39;}))]
Result:  results=[WebSearchResult(content=&#39;# New model scheduling\n\n## September 23, 2025\n\nOllama now includes a significantly improved model scheduling system. Ahead of running a model, Ollama’s new engine...

Thinking:  Okay, the user asked about Ollama&#39;s new engine. Let me look at the search results.

First result is from September 23, 2025, talking about new model scheduling. It mentions improved memory management, reduced crashes, better GPU utilization, and multi-GPU performance. Examples show speed improvements and accurate memory reporting. Supported models include gemma3, llama4, qwen3, etc...

Content:  Ollama has introduced two key updates to its engine, both released in 2025:

1. **Enhanced Model Scheduling (September 23, 2025)**
   - **Precision Memory Management**: Exact memory allocation reduces out-of-memory crashes and optimizes GPU utilization.
   - **Performance Gains**: Examples show significant speed improvements (e.g., 85.54 tokens/s vs 52.02 tokens/s) and full GPU layer utilization.
   - **Multi-GPU Support**: Improved efficiency across multiple GPUs, with accurate memory reporting via tools like `nvidia-smi`.
   - **Supported Models**: Includes `gemma3`, `llama4`, `qwen3`, `mistral-small3.2`, and more.

2. **Multimodal Engine (May 15, 2025)**
   - **Vision Support**: First-class support for vision models, including `llama4:scout` (109B parameters), `gemma3`, `qwen2.5vl`, and `mistral-small3.1`.
   - **Multimodal Tasks**: Examples include identifying animals in multiple images, answering location-based questions from videos, and document scanning.

These updates highlight Ollama&#39;s focus on efficiency, performance, and expanded capabilities for both text and vision tasks.
</code></pre>

<p><strong>Recommended models:</strong></p>

<p>These models have great tool-use capabilities and are able to have multi-turn interactions with the user and tools to get to a final result.</p>

<ul>
<li><code>qwen3</code></li>
<li><code>gpt-oss</code></li>
</ul>

<p><strong>Recommended cloud models:</strong></p>

<ul>
<li><code>qwen3:480b-cloud</code></li>
<li><code>gpt-oss:120b-cloud</code></li>
<li><code>deepseek-v3.1-cloud</code></li>
</ul>

<p>The <code>web_search</code> and <code>web_fetch</code> tools can return thousands of tokens. It is recommended to increase the context length of the model to ~32000 tokens for reasonable performance. Search agents work best with full context length.</p>

<h3 id="fetching-page-results">Fetching page results</h3>

<p>To fetch individual pages (e.g. when a user provides a url in the prompt), use the new web fetch API.</p>

<h4 id="python-library">Python library</h4>

<pre><code>from ollama import web_fetch

result = web_fetch(&#39;https://ollama.com&#39;)
print(result)
</code></pre>

<p><strong>Result</strong></p>

<pre><code>WebFetchResponse(
    title=&#39;Ollama&#39;,
    content=&#39;[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama\n\n**Chat &amp; build
with open models**\n\n[Download](https://ollama.com/download) [Explore
models](https://ollama.com/models)\n\nAvailable for macOS, Windows, and Linux&#39;,
    links=[&#39;https://ollama.com/&#39;, &#39;https://ollama.com/models&#39;, &#39;https://github.com/ollama/ollama&#39;]
)
</code></pre>

<p>Example Python code is available on <a href="https://github.com/ollama/ollama-python/blob/main/examples/web-search.py">GitHub</a>.</p>

<h4 id="javascript-library">JavaScript library</h4>

<pre><code>import { Ollama } from &#34;ollama&#34;;

const client = new Ollama();
const fetchResult = await client.webFetch({ url: &#34;https://ollama.com&#34; });
console.log(JSON.stringify(fetchResult, null, 2));
</code></pre>

<p><strong>Result</strong></p>

<pre><code>{
  &#34;title&#34;: &#34;Ollama&#34;,
  &#34;content&#34;: &#34;[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...&#34;,
  &#34;links&#34;: [
    &#34;https://ollama.com/&#34;,
    &#34;https://ollama.com/models&#34;,
    &#34;https://github.com/ollama/ollama&#34;
  ]
}
</code></pre>

<p>Example JavaScript code is available on <a href="https://github.com/ollama/ollama-js/blob/main/examples/websearch/websearch-tools.ts">GitHub</a>.</p>

<h4 id="curl-1">cURL</h4>

<pre><code>curl --request POST \
  --url https://ollama.com/api/web_fetch \
  --header &#34;Authorization: Bearer $OLLAMA_API_KEY&#34; \
  --header &#39;Content-Type: application/json&#39; \
  --data &#39;{
      &#34;url&#34;: &#34;ollama.com&#34;
}&#39;
</code></pre>

<p><strong>Result</strong></p>

<pre><code>{
  &#34;title&#34;: &#34;Ollama&#34;,
  &#34;content&#34;: &#34;[Cloud models](https://ollama.com/blog/cloud-models) are now available in Ollama...&#34;,
  &#34;links&#34;: [
    &#34;http://ollama.com/&#34;,
    &#34;http://ollama.com/models&#34;,
    &#34;https://github.com/ollama/ollama&#34;
  ]
}
</code></pre>

<h3 id="integrations">Integrations</h3>

<h3 id="mcp-server-model-context-protocol-server">MCP Server (Model Context Protocol server)</h3>

<p>You can enable web search in any MCP client through the <a href="https://github.com/ollama/ollama-python/blob/main/examples/web-search-mcp.py">Python MCP server</a>.</p>

<h4 id="cline">Cline</h4>

<p>To integrate with Cline, configure MCP servers in its settings.</p>

<ul>
<li>Manage MCP Servers &gt; Configure MCP Servers &gt; Add the configuration below</li>
</ul>

<pre><code>{
  &#34;mcpServers&#34;: {
    &#34;web_search_and_fetch&#34;: {
      &#34;type&#34;: &#34;stdio&#34;,
      &#34;command&#34;: &#34;uv&#34;,
      &#34;args&#34;: [&#34;run&#34;, &#34;path/to/web-search-mcp.py&#34;],
      &#34;env&#34;: { &#34;OLLAMA_API_KEY&#34;: &#34;your_api_key_here&#34; }
    }
  }
}
</code></pre>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/cline_mcp_web_search.png" alt="Cline"/></a></p>

<h4 id="codex">Codex</h4>

<p>Add the following configuration to <code>~/.codex/config.toml</code></p>

<pre><code>[mcp_servers.web_search]
command = &#34;uv&#34;
args = [&#34;run&#34;, &#34;path/to/web-search-mcp.py&#34;]
env = { &#34;OLLAMA_API_KEY&#34; = &#34;your_api_key_here&#34; }
</code></pre>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/codex_demo.png" alt="Codex"/></a></p>

<h4 id="goose">Goose</h4>

<p>You can integrate with Ollama via Goose’s extensions.</p>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/goose_web1.png" alt="Goose"/></a></p>

<p><a href="https://ollama.com/download"><img src="https://files.ollama.com/goose_web2.png" alt="Goose"/></a></p>

<h3 id="get-started-1">Get started</h3>

<p>Web search is included with a free Ollama account, with much higher rate limits available by <a href="https://ollama.com/cloud">upgrading your Ollama subscription</a>.</p>

<p>To get started, <a href="https://ollama.com/signup">sign up for an Ollama account</a>!</p>

    </section>
  </article></div>
  </body>
</html>
