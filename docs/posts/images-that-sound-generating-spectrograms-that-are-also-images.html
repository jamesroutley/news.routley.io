<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ificl.github.io/images-that-sound/">Original</a>
    <h1>Images that Sound: Generating spectrograms that are also images</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            <h2> arXiv 2024 </h2>
            

            <p><span>University of Michigan</span>
            </p>
            
            <p><span>Correspondence to: <span>ude.hcimu@gnayzc</span></span>
            </p>

            
            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- TEASER + INTRO -->
  <section>
    <div>
      <div>
        <!-- First column in its own row -->
        <p>
          <h3>
            <b>tl;dr:</b> We use diffusion models to generate spectrograms that look like images but can also be played as sound.
          </h3>
        </p>
      </div>
      <!-- Second column in a separate row -->
      <div>
        <div>
          <div>
            <div>
              <video autoplay="" muted="" loop="" playsinline="">
                <source src="./static/videos/teaser.mp4" type="video/mp4"/>
              </video>
              <p>
                Note the above teaser is muted. For examples with sound, please see our <a href="#gallery">gallery</a> below.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- OVERVIEW -->
  <section>
    <div>
      <div>
        <div>
          <h2>Overview</h2>
          <div>
            <p>
              Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And
              natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to
              synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these
              spectrograms <strong>images that sound</strong>. Our approach is simple and zero-shot, and it leverages pre-trained
              text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse
              process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that
              is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method
              successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a
              desired image prompt. 
            </p>
            <p>
              We describe our <a href="#method">method</a> in more detail and show examples in our <a href="#gallery">gallery</a> below.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Gallery -->
  <section>
    <div>
      <!-- Paper video. -->
      <div>
        <div>
          <h2 id="gallery">Gallery</h2>
          <!-- <br> -->
          <p>Play the video to hear the audio!</p>

          <h2 id="gallery">Colorful <em>Images that Sound</em></h2>
          

          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->
          
          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->

          <!-- <div class="content is-centered has-text-centered">
            <p style="margin-top: 30px; font-family: cursive; font-size: 20px;">More videos will be coming soon!</p>
          </div> -->
        </div>
      </div>
    </div>
  </section>


  <!-- METHOD -->
  <section>
    <div>
      <div>
        <div>
          <h2 id="method">Method</h2>
          <div>

              <p><img src="https://ificl.github.io/images-that-sound/static/images/method.jpg"/></p><p>
              We pose the problem of generating <em>images that sound</em> as a multimodal composition problem:
              our goal is to obtain a sample that is likely under both the distribution of images and the distribution of spectrograms.
              To do this, we simultaneously denoise using an image diffusion model and an audio diffusion model.
              Given a noisy latent \(\mathbf{z}_t\), we compute two text-conditioned noise estimates \(\boldsymbol{\epsilon}_{v}^{(t)}\) and
              \(\boldsymbol{\epsilon}_{a}^{(t)}\). One for each modality. We then obtain a multimodal noise estimate
              \(\tilde{\boldsymbol{\epsilon}}^{(t)}\) via weighted averaging, which we then use to denoise. 
              Repeating this process iteratively results in a clean latent \(\mathbf{z}_0\). Finally, we decode this clean
              latent to a spectrogram and convert it into a waveform using a pretrained vocoder.
              As we only change the inference time procedure, our method is zero-shot, requiring no training or fine-tuning.
            </p>

            <div>
              <video autoplay="" muted="" loop="" playsinline="">
                <source src="./static/videos/denoise.mp4" type="video/mp4"/>
              </video>
              <p>
                Iteratively denoising using both a spectrogram diffusion model 
                and an image diffusion model. See <a href="#gallery">above</a> 
                for videos with sound.
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <section>
    <div>
      <!-- Concurrent Work. -->
      <div>
        <div>
          <h2>Related Links and Works</h2>
  
          <div>
            <p>
              <a href="https://mixmag.net/feature/spectrogram-art-music-aphex-twin" target="_blank">Spectrogram Art</a>, 
              by <a href="https://www.instagram.com/beckybuckle/" target="_blank">Becky Buckle</a>: an article about the 
              history of artists concealing images in the spectrogram of their music.
            </p>
            <p>
              <a href="https://github.com/LeviBorodenko/spectrographic" target="_blank">SpectroGraphic</a>, 
              by <a href="https://github.com/LeviBorodenko" target="_blank">Levi Borodenko</a>:
              a tool to turn images into spectrograms and recover the corresponding audio.
            </p>
            <p>
              <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" target="_blank">Composable Diffusion</a>,
              by <a href="https://nanliu.io/" target="_blank">Liu</a> <i>et al.</i>,
              which originally showed how to compose image diffusion models together.
            </p>
            <p>
              <a href="https://dangeng.github.io/visual_anagrams/" target="_blank">Visual Anagrams</a>,
              by <a href="https://dangeng.github.io/" target="_blank">Geng</a> <i>et al.</i>,
              which uses pretrained diffusion models and compositionality to make multi-view optical illusions.
            </p>
            <p>
              <a href="https://dangeng.github.io/factorized_diffusion/" target="_blank">Factorized Diffusion</a>,
              by <a href="https://dangeng.github.io/" target="_blank">Geng</a> <i>et al.</i>,
              which generates various perceptual illusions via decomposition with diffusion models. We use
              their code the colorize the spectrograms.
            </p>
            <p>
              <a href="https://diffusionillusions.com/" target="_blank">Diffusion Illusions</a>,
              by <a href="https://ryanndagreat.github.io/" target="_blank">Burgert</a> <i>et al.</i>,
              which produces multi-view illusions, along with other visual effects, through score distillation sampling.
              We adapt their code to make an SDS style baseline for generating images that sound.
            </p>
            
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->
  
    </div>
  </section>


  <section id="BibTeX">
    <div>
      <div>
        <div>
          <h2>BibTeX</h2>
          <pre><code>@article{chen2024soundify,
  title     = {Images that Sound: Composing Images and Sounds on a Single Canvas},
  author    = {Chen, Ziyang and Geng, Daniel and Owens, Andrew},
  year      = {2024},
  url       = {https://ificl.github.io/images-that-sound/},
}</code></pre>
        </div>
      </div>
    </div>
  </section>


  



</div>
  </body>
</html>
