<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/muxamilian/duralava">Original</a>
    <h1>Show HN: Lava lamp simulated by neural net in infinite loop</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
          <article itemprop="text">
<p dir="auto"><em>duralava</em> is a neural network which can simulate a lava lamp in an infinite loop.</p>
<h2 dir="auto"><a id="user-content-example-videos" aria-hidden="true" href="#example-videos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example videos</h2>
<p dir="auto">This is not a real lava lamp but a &#34;fake&#34; one generated by duralava. (Might take some time to load.)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1943719/152648707-41552c03-5f45-4727-a4ae-6cb5c0aefab6.png"><img src="https://user-images.githubusercontent.com/1943719/152648707-41552c03-5f45-4727-a4ae-6cb5c0aefab6.png" alt="out_180, duralava neural network deep learning lava lamp sample 1"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1943719/152648713-36c848d6-aba0-436d-8630-a5c2f3c332ca.png"><img src="https://user-images.githubusercontent.com/1943719/152648713-36c848d6-aba0-436d-8630-a5c2f3c332ca.png" alt="out_170, duralava neural network deep learning lava lamp sample 2"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1943719/152648716-6c27c629-7fec-44c8-a9b5-5811f070e579.png"><img src="https://user-images.githubusercontent.com/1943719/152648716-6c27c629-7fec-44c8-a9b5-5811f070e579.png" alt="out_160, duralava neural network deep learning lava lamp sample 3"/></a></p>
<h2 dir="auto"><a id="user-content-novelty" aria-hidden="true" href="#novelty"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Novelty</h2>
<p dir="auto">duralava can</p>
<ul dir="auto">
<li>learn a physical process (a lava lamp).</li>
<li>generate an arbitrarily long sequence of output, without diverging even after hours (outputting tens of thousands of frames).</li>
</ul>
<h2 dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How it works</h2>
<p dir="auto">Generative Adversarial Networks (GANs) can learn to generate new samples of data. For example, a GAN can be trained to output images of a lava lamp which look as real as possible. To accomplish this, the GAN gets an input vector with normally distributed noise. For duralava this vector is of length 64. Based on this random noise vector it generates a lava lamp image. The random vector thus encodes the state of the lava lamp.</p>
<p dir="auto">For training, the GAN is presented a real image of a lava lamp and also one of the fake lava lamp and then it learns to make the fake ones look as real as possible.</p>
<p dir="auto">For a lava lamp, a sequence of images has to be created. This sequence should in fact be infinite since a lava lamp can run forever. Thus the GAN should learn to output an arbitrarily long sequence of lava lamp images as a video. This is achieved by using a recurrent neural network (RNN). The RNN gets the 64 element noise vector of time step <em>t</em> and outputs the 64 element noise vector for time stemp <em>t+1</em>.</p>
<p dir="auto">The tricky part is to make sure that the state of the lava lamp (the 64 element random noise vector) remains stable. It could for example happen that over time the distribution of noise in the vector diverges from a normal distribution and that the mean becomes 10 and the standard deviation 52. In this case, the output images of the lava lamps wouldn&#39;t be correct anymore as the GAN was trained to expect the input vector to be normally distributed. To solve this problem, I make sure that in training the output of the RNN stays normally distributed. This is accomplished by adding penalization terms in the training which discourage the noise to diverge from the normal distribution.</p>
<h2 dir="auto"><a id="user-content-learning" aria-hidden="true" href="#learning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Learning</h2>
<p dir="auto">To learn a new model run</p>

<p dir="auto">Around 10 GB of combined CPU and GPU memory are required. I used python 3.9.7 and the pip requirements listed in <code>requirements.txt</code>.</p>
<h2 dir="auto"><a id="user-content-live-mode" aria-hidden="true" href="#live-mode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Live mode</h2>
<p dir="auto">To generate an output video live use some of the trained weights like this:</p>
<div data-snippet-clipboard-copy-content="python learn.py --weights logs/20220104-213105/weights.180 --mode live"><pre><code>python learn.py --weights logs/20220104-213105/weights.180 --mode live
</code></pre></div>
<h2 dir="auto"><a id="user-content-generating-an-output-video" aria-hidden="true" href="#generating-an-output-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generating an output video</h2>
<p dir="auto">To generate an output video as an APNG animation file use some of the trained weights like this:</p>
<div data-snippet-clipboard-copy-content="python learn.py --weights logs/20220104-213105/weights.180 --mode video"><pre><code>python learn.py --weights logs/20220104-213105/weights.180 --mode video
</code></pre></div>
<p dir="auto">An APNG named <code>out.png</code> will be created in the current directory. For creating APNGs from a trained neural network, you need to have <code>ffmpeg</code> installed.</p>
<h2 dir="auto"><a id="user-content-low-hanging-fruit" aria-hidden="true" href="#low-hanging-fruit"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Low-hanging fruit</h2>
<p dir="auto">I trained on a MacBook Air with an M1 SoC with 16 GB of shared memory for CPU and GPU. Thus, memory was the limiting factor in my experiments.</p>
<p dir="auto">With more memory, one could</p>
<ul dir="auto">
<li>Increase the resolution (currently 64x64 pixels)</li>
<li>Increase the training sequence length (currently 20)</li>
<li>Increase the batch size (currently 32)</li>
<li>Increase the size of the recurrent neural networks, which model the evolution of the lava lamp over time</li>
</ul>
<h2 dir="auto"><a id="user-content-dataset" aria-hidden="true" href="#dataset"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dataset</h2>
<p dir="auto"><code>lavalamp.mov</code> contains more than 1 hour of footage of a lava lamp at 30 fps and can be freely used for any purpose. In the <code>frames</code> directory there are the individual frames of the video scaled to 64x64 pixels, which I used for training the model.</p>
</article>
        </div></div>
  </body>
</html>
