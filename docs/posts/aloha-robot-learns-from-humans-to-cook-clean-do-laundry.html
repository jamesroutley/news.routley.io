<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://venturebeat.com/automation/stanfords-mobile-aloha-robot-learns-from-humans-to-cook-clean-do-laundry/">Original</a>
    <h1>ALOHA robot learns from humans to cook, clean, do laundry</h1>
    
    <div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-01-05T18:45:09+00:00" datetime="2024-01-05T18:45:09+00:00">January 5, 2024 10:45 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="419" src="https://venturebeat.com/wp-content/uploads/2024/01/Screen-Shot-2024-01-05-at-1.40.51-PM.png?fit=750%2C419&amp;strip=all" alt="Screengrab of YouTube video showing Stanford&#39;s robot Mobile ALOHA in action."/></p><p><span>Credit: Stanford University</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary">
		<div id="content" role="main">

			<article id="post-2927419">
				<div>
					<p>A new AI system developed by researchers at Stanford University makes impressive breakthroughs in training mobile robots that can perform complex tasks in different environments. </p>



<p>Called <a href="https://mobile-aloha.github.io/">Mobile ALOHA</a> (A Low-cost Open-source Hardware System for Bimanual Teleoperation) the system addresses the high costs and technical challenges of training mobile bimanual robots that require careful guidance from human operators. </p>



<figure><p>
<iframe title="Mobile ALOHA Robot - Teleoperating a 3-Course Cantonese Meal" width="500" height="281" src="https://www.youtube.com/embed/mnLVbwxSdNM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>It costs a fraction of off-the-shelf systems and can learn from as few as 50 human demonstrations. </p>



<p>This new system comes against the backdrop of an acceleration in robotics, enabled partly by the success of generative models.</p>



<h2 id="h-limits-of-current-robotics-systems">Limits of current robotics systems</h2>



<p>Most robotic manipulation tasks focus on table-top manipulation. This includes a recent wave of models that have been built based on transformers and diffusion models, architectures widely used in generative AI.</p>



<p>However, many of these models lack the mobility and dexterity necessary for generally useful tasks. Many tasks in everyday environments require coordinating mobility and dexterous manipulation capabilities.</p>



<p>“With additional degrees of freedom added, the interaction between the arms and base actions can be complex, and a small deviation in base pose can lead to large drifts in the arm’s end-effector pose,” the Stanford researchers write in <a href="http://arxiv.org/abs/2401.02117">their paper</a>, adding that prior works have not delivered “a practical and convincing solution for bimanual mobile manipulation, both from a hardware and a learning standpoint.”</p>



<h2 id="h-mobile-aloha">Mobile ALOHA</h2>



<p>The new system developed by Stanford researchers builds on top of <a href="https://tonyzhaozh.github.io/aloha/">ALOHA</a>, a low-cost and whole-body teleoperation system for collecting bimanual mobile manipulation data. </p>



<p>A human operator demonstrates tasks by manipulating the robot arms through a teleoperated control. The system captures the demonstration data and uses it to train a control system through end-to-end imitation learning.</p>



<p>Mobile ALOHA extends the system by mounting it on a wheeled base. It is designed to provide a cost-effective solution for training robotic systems. The entire setup, which includes webcams and a laptop with a consumer-grade GPU, costs around $32,000, which is much cheaper than off-the-shelf bimanual robots, which can cost up to $200,000.</p>



<figure><img decoding="async" src="https://venturebeat.com/wp-content/uploads/2024/01/image.png?strip=all" alt="" data-recalc-dims="1"/></figure>



<p><em>Mobile ALOHA configuration (source: </em><a href="http://arxiv.org/abs/2401.02117"><em>arxiv</em></a><em>)</em></p>



<p>Mobile ALOHA is designed to teleoperate all degrees of freedom simultaneously. The human operator is tethered to the system by the waist and drives it around the work environment while operating the arms with controllers. This enables the robot control system to simultaneously learn movement and other control commands. Once it gathers enough information, the model can then repeat the sequence of tasks autonomously.</p>



<p>The teleoperation system is capable of multiple hours of consecutive usage. The results are impressive and show that a simple training recipe enables the system to learn complex mobile manipulation tasks. </p>



<p>The demos show the trained robot cooking a three-course meal with delicate tasks such as breaking eggs, mincing garlic, pouring liquid, unpackaging vegetables, and flipping chicken in a frying pan. </p>



<p>Mobile ALOHA can also do a variety of house-keeping tasks, including watering plants, using a vacuum, loading and unloading a dishwasher, getting drinks from the fridge, opening doors, and operating washing machines</p>



<h2 id="h-imitation-learning-and-co-training">Imitation learning and co-training</h2>



<p>Like many recent works in robotics, Mobile ALOHA takes advantage of transformers, the architecture used in large language models. The original ALOHA system used an architecture called Action Chunking with Transformers (ACT), which takes images from multiple viewpoints and joint positions as input and predicts a sequence of actions.</p>



<figure><img decoding="async" fetchpriority="high" width="1600" height="441" src="https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=1600&amp;strip=all 1600w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=300&amp;strip=all 300w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=768&amp;strip=all 768w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=800&amp;strip=all 800w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=1536&amp;strip=all 1536w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=400&amp;strip=all 400w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=750&amp;strip=all 750w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=578&amp;strip=all 578w, https://venturebeat.com/wp-content/uploads/2024/01/image.png?resize=1600%2C441&amp;strip=all?w=930&amp;strip=all 930w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"/></figure>



<p><em>Action Chunking with Transformers (ACT) (source: </em><a href="https://tonyzhaozh.github.io/aloha/"><em>ALOHA webpage</em></a><em>)</em></p>



<p>Mobile ALOHA extends that system by adding movement signals to the input vector. This formulation allows Mobile ALOHA to reuse previous deep imitation learning algorithms with minimal changes.</p>



<p>“We observe that simply concatenating the base and arm actions then training via direct imitation learning can yield strong performance,” the researchers write. “Specifically, we concatenate the 14-DoF joint positions of ALOHA with the linear and angular velocity of the mobile base, forming a 16-dimensional action vector.”</p>



<p>The work also benefits from the success of recent methods that pre-train models on diverse robot datasets from other projects. Of special note is RT-X, a project by <a href="https://venturebeat.com/ai/deepminds-remarkable-new-ai-controls-robots-of-all-kinds/">DeepMind and 33 research institutions</a>, which combined several robotics datasets to create control systems that could generalize well beyond their training data and robot morphologies. </p>



<p>“Despite the differences in tasks and morphology, we observe positive transfer in nearly all mobile manipulation tasks, attaining equivalent or better performance and data efficiency than policies trained using only Mobile ALOHA data,” the researchers write.</p>



<p>Using existing data enabled the researchers to train Mobile ALOHA for complex tasks with very few human demonstrations</p>



<p>“With co-training, we are able to achieve over 80% success on these tasks with only 50 human demonstrations per task, with an average of 34% absolute improvement compared to no co-training,” the researchers write.</p>



<h2 id="h-not-production-ready">Not production-ready</h2>



<p>Despite its impressive results, Mobile ALOHA has drawbacks. For example, its bulkiness and unwieldy form factor do not make it suitable for tight environments. </p>



<p>In the future, the researchers plan to improve the system by adding more degrees of freedom and reducing the robot’s volume.</p>



<p>It is also worth noting that this is not a fully autonomous system that can learn to explore new environments on its own. It still requires full demonstrations by human operators in its environment, though it learns the tasks with fewer examples than previous methods, thanks to its co-training system.</p>



<p>The researchers will explore changes to the AI model that will allow the robot to self-improve and acquire new knowledge. </p>
<p><strong>VentureBeat&#39;s mission</strong> is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=bottomBoilerplate" data-type="URL" data-id="/newsletters/">Discover our Briefings.</a></p><!-- Boilerplate CSS for "after" -->				</div><!-- .article-content -->

									
				
			</article><!-- #post-2927419 .article-wrapper -->


		</div><!-- #content -->
	</div></div>
  </body>
</html>
