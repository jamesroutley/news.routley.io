<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://deepseekcoder.github.io/">Original</a>
    <h1>DeepSeek Coder: Let the Code Write Itself</h1>
    
    <div id="readability-page-1" class="page"><div>
          <p>
            DeepSeek Coder comprises a series of code language models trained from scratch on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens.
            We provide various sizes of the code model, ranging from 1B to 33B versions.
            Each model is pre-trained on repo-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, resulting in foundational models (DeepSeek-Coder-Base).
            We further fine-tune the base model with 2B tokens of instruction data to get instruction-tuned models, namedly DeepSeek-Coder-Instruct.
          </p>
          <!-- <p>
            For coding capabilities, DeepSeek-Coder-Base achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.
            And Compared to GPT35-turbo, DeepSeek-Coder-Instruct demonstrates superior performance in human evaluation while maintaining comparable performance in MBPP.
          </p> -->
          <ul>
            <li>
              Pretrained on <b>2 Trillion</b> tokens over more than 80 programming languages.
            </li>
            <li>
              Various model sizes (<b>1.3B</b>, <b>5.7B</b>, <b>6.7B</b> and <b>33B</b>) to support different requirements.
            </li>
            <li>
              A window size of <b>16K window </b>size, supporting <b>project-level</b> code completion and infilling.
            </li>
            <li>
              <b>State-of-the-Art</b> performance among open code models. 
            </li>
            <li>
              <b>Open source and free for research and commercial use</b>.
            </li>
          </ul>
          
        </div></div>
  </body>
</html>
