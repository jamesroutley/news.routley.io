<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.photoroom.com/inside-photoroom/so-you-want-to-rent-an-nvidia-h100-cluster-2024-consumer-guide?slug=inside-photoroom/so-you-want-to-rent-an-nvidia-h100-cluster-2024-consumer-guide&amp;_storyblok_published=511470179&amp;">Original</a>
    <h1>So you want to rent an NVIDIA H100 cluster? 2024 Consumer Guide</h1>
    
    <div id="readability-page-1" class="page"><div><div data-selector="blogpost"><div data-selector="blogpost-header"><div><p><img alt="Eliot Andres" loading="lazy" decoding="async" data-nimg="fill" src="https://storyblok-cdn.photoroom.com/f/191576/1080x1080/cec5740daa/eliot_author.webp"/></p><p><span data-selector="blogpost-author">Eliot Andres</span><span><svg xmlns="http://www.w3.org/2000/svg" width="5" height="4" viewBox="0 0 5 4" fill="none"><circle cx="2.5" cy="2" r="2" fill="#93949A"></circle></svg></span><span data-selector="blogpost-date">July 8, 2024</span></p></div></div><p><span><p>With the NVIDIA H100 becoming the hottest commodity, you&#39;ve probably considered it as a birthday gift for your partner or a great graduation present for your kid.</p><p>Consumer reports are notoriously lacking on the topic, so we thought we might enlighten you on decision criteria to pick the perfect cluster for your needs.</p><p>After an extensive search for a 256 H100 cluster to provide a reliable playing field for the ML team at Photoroom, we learned what to look for when renting one. Below are our learnings.</p><p><i>This blog post does not contain any private pricing information and does not directly compare any provider against another.</i></p><p>This was our first time renting a large-ish GPU cluster 24/7. Training interruptions and inability to launch trainings can really hinder the ability of the team to build powerful models. Therefore, we prioritized reliability over all other criteria, even price.</p><p>But since ‚Äúhow much does it cost‚Äù is the question on everyone‚Äôs mind, we‚Äôll start with costs. Most people measure it in terms of $ per GPU hour. There are 3 criteria that will impact the price:</p><ul><li><p>commit size (how many GPUs)</p></li><li><p>commit duration</p></li><li><p>upfront payment</p></li></ul><p>Cluster sizes can go up to few tens of thousand of GPUs, dwarfing the 256 GPU cluster we‚Äôre renting. Currently, most clusters can be rented from 1 month to 3 years, after which it becomes more economically viable to buy.</p><p>Upfront payments vary from provider to provider, with most of the independent ones requiring at least 25% upfront to secure the deal. Avoid 100% upfront deals to maintain leverage and ensure the provider&#39;s financial health before transferring large sums to someone you haven&#39;t met in person.</p><p>On top of that price, you need to factor in a few elements: cost of support (a few % at most hyperscalers), cost of storage, egress.</p><p><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/2000x774/82bf7e26d8/h100-hourly-price.webp"/></p><p>The interconnect is the neural spine of your cluster. That‚Äôs where all of the information flows. An unreliable interconnect can crash your training periodically. Crashes lead to wasted training resources (need to restart from last checkpoint) and ML practitioner headaches (need to babysit the training).</p><p>There are two main types of interconnect: Infiniband or Ethernet. Infiniband is from NVIDIA (they acquired Mellanox in 2019). It‚Äôs considered the Rolls Royce of the interconnects: expensive but reliable. Many providers want to avoid relying on NVIDIA too much, therefore they rely on an independent Ethernet solution (EFA at AWS, RoCE at Oracle, TCPX at Google Cloud).</p><p>From our tests, we found that Infiniband was systematically outperforming Ethernet interconnects in terms of speed. When using 16 nodes / 128 GPUs, the difference varied from 3% to 10% in terms of distributed training throughput[1]. The gap was widening as we were adding more nodes: Infiniband was scaling almost linearly, while other interconnects scaled less efficiently.</p><p><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/1373x507/ba2288ba9a/ethernet_vs_infinibnad.png/m/"/><i>Graph comparing throughput between Infiniband vs a purposefully unnamed Ethernet interconnect. Each node contains 8 GPUs</i></p><p>We also found that Ethernet was less stable overall. We did manage to run 48h uninterrupted training on some Ethernet interconnects, but faced some obscure errors on some others.</p><p><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/1910x252/67eb8652b2/interconnect_issue_1.png/m/"/><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/1602x689/f27992ed54/interconnect.png/m/"/><i>A few examples of obscure interconnect issues that can be painful to debug</i></p><p>Our recommendation is to test with your workload. If the cluster is 5% slower but 10% cheaper while remaining stable, then it‚Äôs worth it to go with Ethernet.</p><p>Brand-new GPUs tend to break right after they come out of the factory. When launching a training on a newly-built cluster, it‚Äôs not unexpected to see a few % of the GPUs fail. Even after a burn-in test from the GPU provider. Because it will happen to you, it‚Äôs a good idea to align with your provider on what happens in that case.</p><p>Spare nodes are machines kept on standby in case one of the machines in your cluster encounters an issue. Some providers can swap it instantly (a few minutes) with an API call. Others require you to open a ticket, escalate it and take much longer to swap the node.</p><p>A node with a broken GPU is virtually useless; therefore, it‚Äôs a good idea to provision in the SLA that a node that doesn‚Äôt have 8 healthy GPUs counts as a broken node.</p><p>Node colocation is important. Most providers ensure all nodes are in close proximity to each other. If your packets have to traverse multiple switches and move to another building, the chances of training instability are higher. Ensure node location can be enforced.</p><p>To keep those GPUs busy, you need to keep them fed with data. There are several options. We‚Äôve decided to go with VAST storage: data is stored on dedicated SSD machines in the same data center and streamed. With this technology, we‚Äôre able to stream at a very comfortable ~100 Gbps without any hiccups. Note that we work with images, so when working with text, you may have lower data needs.</p><p>Most H100 nodes come with 27 TB of physical storage attached to them. There are a few techniques to transform those disks into distributed ones, such as <a href="https://docs.ceph.com" rel="noopener nofollow" target="_blank">Ceph</a>. We decided once more to go with the reliable, less-hacky solution and picked the more expensive VAST. That leaves ~800 TB of storage used only for caching, but we expect the reliability to be worth it.</p><p>Not all providers offer all storage solutions, so ask in advance.</p><p>Note that depending on where your data is, you might need to move it in advance as streaming it is unrealistic. Assuming our datacenter had enough bandwidth to stream from an AWS zone at 100 Gbp/s and that we would be silly enough to not cache anything, it would cost ~$0.6 / second or $1.6M /  month just in egress cost  <span data-type="emoji" data-name="exploding_head" emoji="ü§Ø">ü§Ø</span> ¬†[2]</p><p>In our experience, nothing beats the convenience of a shared Slack channel with the engineers managing the cluster. If you‚Äôve lived through tickets opened for weeks with no useful answer or resolution in sight, you know how convenient it is to chat and pair program directly with the engineers in charge</p><p>Some large providers told us very clearly that a direct line of contact with engineers ‚Äúwill never happen‚Äù and that even with the highest support tier we‚Äôd need to go through the ticketing system. It‚Äôs possible we‚Äôre too small to get that privilege. When selecting your provider, ask what kind of support they offer and make sure you have an SLA on response times.</p><p><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/1078x630/3f282d6b92/chat_with_engineer.png/m/"/><i>Example of an interaction that would rarely happen on a ticketing system</i></p><p>This line of support is key as you can expect issues to arise at all times during training. If you look at Mistral blog posts, they often thank their GPU providers ‚Äúfor their 24/7 help in marshaling our cluster‚Äù (Mistral trainings are larger than ours).</p><p>You‚Äôll need to determine how you‚Äôll orchestrate and manage the machines in the cluster. There are a few options:</p><ul><li><p>Bare metal: You‚Äôre provided with access to the machine, and you manage the software running on them from A to Z.</p></li><li><p>Kubernetes (K8s): provider maintains the Kubernetes setup, you select which containers you want to run on each machine</p></li><li><p>SLURM: provider maintains the <a href="https://slurm.schedmd.com/documentation.html" rel="noopener nofollow" target="_blank">SLURM workload</a> manager setup</p></li><li><p>Virtual Machines (VMs): provider maintains the hypervisor, you get access through a Virtual Machine.</p></li></ul><p>Our provider didn‚Äôt offer to manage SLURM for us, therefore we went with VMs and manage our own SLURM setup. We didn‚Äôt go with Bare Metal as it‚Äôs more management and VMs offer more options to our provider to properly monitor the machines. Note there‚Äôs a very small performance loss from the virtualization (less than 1%).</p><p><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/1590x894/644ca4757f/stack_comparison.png/m/"/><i>Breakdown of the stacks offered on </i><a href="http://gpulist.ai" rel="noopener nofollow" target="_blank"><i>gpulist.ai</i></a></p><p>We insisted on getting a 48h uninterrupted run on the cluster before committing. It was also a key requirement to be able to run on the exact same cluster as the one we would be renting. We started with 4 nodes to iron out issues without paying too much, then expanded to 32 nodes. It took a few attempts to get a smooth run, for various reasons (node failing, interconnect issues).</p><p>You can also trust your provider that the cluster you‚Äôll be getting will be ‚Äúalmost the same‚Äù as the one they can offer to test on, but it‚Äôs riskier. Especially if it‚Äôs not in the same datacenter. It might be required if the cluster you‚Äôre renting is not ready yet.</p><p>Discussing with providers, we learned that the average cluster utilization in the industry is between 30 and 50%. Surprising, isn‚Äôt it? I‚Äôve learned that the main reason companies are committing is not to optimize for the price per hour. It is to ensure their ML team has access to GPUs at any time, which is not possible with an on-demand solution.</p><p><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/1322x354/70dab58c84/gpu_utilization.png/m/"/><i>GPU utilization of the 256 GPU cluster used by the ML team at Photoroom. Purple is a one week rolling average. We expect the utilization to go up in the next weeks as we‚Äôre getting more familiar with the cluster.</i></p><p>Our dream setup would be to have an on-demand cluster, where you launch trainings and only pay for utilization. We‚Äôve been looking for years and no one was able to offer an SLA on GPU availability (e.g. ‚Äú95% of the time, you won‚Äôt wait more than 10 minutes to launch a training‚Äù). Which makes sense: in the current GPU craze, keeping GPUs on standby for your customers would be crazy.</p><p>When running in the US, a 256 H100 cluster consumes ~1000 tons of CO2 in a year [3]. That‚Äôs 1000 Paris-NYC trips.</p><p>Some providers compensate with carbon credits, we believe this has limited impact. The GPU boom causes more gas plants or coal plants to be put online [4]. Others build solar and wind plants, injecting green electricity in the grid they consume. A few run GPUs in countries with a grid that runs exclusively on green electricity (hydroelectric, geothermal).</p><p><img alt="" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" src="https://storyblok-cdn.photoroom.com/f/191576/1932x1130/08d02f6c2b/carbon_intensity.png/m/"/><i>Electricity intensity for a few of the countries we‚Äôve considered for our GPU cluster. </i><a href="https://ourworldindata.org/grapher/carbon-intensity-electricity?tab=chart&amp;time=latest..2023&amp;country=FRA~EU-27~OWID_WRL~NLD~USA~IND~DEU~NOR" rel="noopener nofollow" target="_blank"><i>Source</i></a></p><p>While that criteria might seem secondary to some, as the co-founder of a fast growing startup I believe our decisions can have an impact. By prioritizing environmental considerations, I hope to inspire other companies to do the same.</p><p>If you‚Äôre an ML practitioner, you can do your part. Help put this important topic on the map by asking your manager ‚Äúwhat‚Äôs the CO2 footprint of our cluster‚Äù and advocate to move towards a greener one.</p><ul><li><p>Availability date: is your cluster available now or will you have to wait? If so, is the date enforced in the contract? We rented another smaller cluster with a different provider and it is currently one month late.</p></li><li><p>Burn-in: will the cluster be extensively tested before you get access to it or will you be the guinea pig figuring out the last issues?</p></li><li><p>Renewal: will you be able to renew with that provider and how much of an advance notice do you need to give?</p></li><li><p>Note that for training, you‚Äôre not interested in H100 PCIe and require HXM5. Although they share the name, PCIE H100s are also less powerful (less cores, lower power rating)</p></li><li><p>Some providers don‚Äôt own the GPU they rent to you. They manage the support and software stack. If one GPU malfunctions, this adds another layer: you -&gt; provider -&gt; GPU owner -&gt; SuperMicro/Gigabit -&gt; NVIDIA.</p></li><li><p>Ask for the exact name of the datacenter your GPUs will be in, then look at how experienced the company running it is</p></li><li><p>If you get the chance, get some H200s. I‚Äôve been told NVIDIA sells them at the same price as H100. Thanks to the double memory, they end up being faster</p></li></ul><p>Thank you to David from the Photoroom team who thoroughly ran the benchmarks on the GPUs, slayed CUDA issues and ultimately helped us pick the best provider.</p><p>Ultimately, we‚Äôve decided to go with Genesis Cloud, a German provider that offers an Infiniband cluster that runs on CO2-free hydroelectricity. This blog post is not an endorsement or an ad. Nevertheless, we encourage companies who can factor-in CO2 emission in their decision to do it, as the impact on climate is tremendous. As a company, we&#39;re only part of the way there, as our inference workloads are running in the US in majority.</p><p>If you‚Äôre interested in joining a small and efficient ML team who now has the compute to do marvels, have a look at our <a href="https://www.photoroom.com/company">open positions</a>.</p><hr/><p><i>Great articles on the topic of startups running their own training GPU cluster:</i></p><ul><li><p><a href="https://imbue.com/research/70b-infrastructure/" rel="noopener nofollow" target="_blank"><b>From bare metal to a 70B model: infrastructure set-up and scripts</b></a></p></li><li><p><a href="https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness" rel="noopener nofollow" target="_blank"><b>Training great LLMs entirely from ground up in the wilderness as a startup</b></a></p></li></ul><p><i>[1] We‚Äôre not interconnect experts and probably haven‚Äôt spent enough time fine-tuning the configuration.</i></p><p><i>[2] Using the list price of 5 cent / GB (AWS‚Äô cheapest egress tier), assuming no caching (which would be absurd). </i>12.5 [GB/s] * 3600 * 24 * 30 * 0.05 = 1 620 000 </p><p><i>[3] kg_co2_per_year = kW_per_node_per_hour * hours_per_year * num_nodes * PUE * co2_emitted_per_kW_hour = 10 * (24*365) * 32 * 1.2 * 0.369 = ~1241 tons, assuming a 100% cluster utilization. A Paris-NYC flight is roughly </i><a href="https://curb6.com/footprint/flights/paris-cdg/new-york-jfk" rel="noopener nofollow" target="_blank"><i>1 ton of CO2</i></a></p><p><i>[4] </i><a href="https://www.washingtonpost.com/business/2024/06/21/artificial-intelligence-nuclear-fusion-climate/" rel="noopener nofollow" target="_blank"><i>AI is exhausting the power grid, Washington Post</i></a><i> (version without paywall on </i><a href="https://www.msn.com/en-us/money/technology/ai-is-exhausting-the-power-grid-tech-firms-are-seeking-a-miracle-solution/ar-BB1oDl5z" rel="noopener nofollow" target="_blank"><i>msn.com</i></a><i>)</i></p></span></p><div><p><img alt="Eliot Andres" loading="lazy" decoding="async" data-nimg="fill" src="https://storyblok-cdn.photoroom.com/f/191576/1080x1080/cec5740daa/eliot_author.webp"/></p><p><span>Eliot Andres</span><span>Cofounder &amp; CTO @ Photoroom</span></p></div></div></div></div>
  </body>
</html>
