<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/dleemiller/WordLlama">Original</a>
    <h1>Show HN: Wordllama â€“ Things you can do with the token embeddings of an LLM</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><strong>WordLlama</strong> is a fast, lightweight NLP toolkit that handles tasks like fuzzy-deduplication, similarity and ranking with minimal inference-time dependencies and optimized for CPU hardware.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://thewhodidthis.com/dleemiller/WordLlama/blob/main/wordllama.png"><img src="https://thewhodidthis.com/dleemiller/WordLlama/raw/main/wordllama.png" alt="Word Llama" width="50%"/></a>
</p>

<ul dir="auto">
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#what-is-it">What is it?</a></li>
<li><a href="#mteb-results-l2_supercat">MTEB Results</a></li>
<li><a href="#embed-text">Embed Text</a></li>
<li><a href="#training-notes">Training Notes</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#extracting-token-embeddings">Extracting Token Embeddings</a></li>
<li><a href="#citations">Citations</a></li>
<li><a href="#license">License</a></li>
</ul>

<p dir="auto">Install:</p>

<p dir="auto">Load the 256-dim model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from wordllama import WordLlama

# Load the default WordLlama model
wl = WordLlama.load()

# Calculate similarity between two sentences
similarity_score = wl.similarity(&#34;i went to the car&#34;, &#34;i went to the pawn shop&#34;)
print(similarity_score)  # Output: 0.06641249096796882

# Rank documents based on their similarity to a query
query = &#34;i went to the car&#34;
candidates = [&#34;i went to the park&#34;, &#34;i went to the shop&#34;, &#34;i went to the truck&#34;, &#34;i went to the vehicle&#34;]
ranked_docs = wl.rank(query, candidates)
print(ranked_docs)
# Output:
# [
#   (&#39;i went to the vehicle&#39;, 0.7441646856486314),
#   (&#39;i went to the truck&#39;, 0.2832691551894259),
#   (&#39;i went to the shop&#39;, 0.19732814982305436),
#   (&#39;i went to the park&#39;, 0.15101404519322253)
# ]

# additional inference methods
wl.deduplicate(candidates, threshold=0.8) # fuzzy deduplication
wl.cluster(docs, k=5, max_iterations=100, tolerance=1e-4) # labels using kmeans/kmeans++ init
wl.filter(query, candidates, threshold=0.3) # filter candidates based on query
wl.topk(query, candidates, k=3) # return topk strings based on query"><pre><span>from</span> <span>wordllama</span> <span>import</span> <span>WordLlama</span>

<span># Load the default WordLlama model</span>
<span>wl</span> <span>=</span> <span>WordLlama</span>.<span>load</span>()

<span># Calculate similarity between two sentences</span>
<span>similarity_score</span> <span>=</span> <span>wl</span>.<span>similarity</span>(<span>&#34;i went to the car&#34;</span>, <span>&#34;i went to the pawn shop&#34;</span>)
<span>print</span>(<span>similarity_score</span>)  <span># Output: 0.06641249096796882</span>

<span># Rank documents based on their similarity to a query</span>
<span>query</span> <span>=</span> <span>&#34;i went to the car&#34;</span>
<span>candidates</span> <span>=</span> [<span>&#34;i went to the park&#34;</span>, <span>&#34;i went to the shop&#34;</span>, <span>&#34;i went to the truck&#34;</span>, <span>&#34;i went to the vehicle&#34;</span>]
<span>ranked_docs</span> <span>=</span> <span>wl</span>.<span>rank</span>(<span>query</span>, <span>candidates</span>)
<span>print</span>(<span>ranked_docs</span>)
<span># Output:</span>
<span># [</span>
<span>#   (&#39;i went to the vehicle&#39;, 0.7441646856486314),</span>
<span>#   (&#39;i went to the truck&#39;, 0.2832691551894259),</span>
<span>#   (&#39;i went to the shop&#39;, 0.19732814982305436),</span>
<span>#   (&#39;i went to the park&#39;, 0.15101404519322253)</span>
<span># ]</span>

<span># additional inference methods</span>
<span>wl</span>.<span>deduplicate</span>(<span>candidates</span>, <span>threshold</span><span>=</span><span>0.8</span>) <span># fuzzy deduplication</span>
<span>wl</span>.<span>cluster</span>(<span>docs</span>, <span>k</span><span>=</span><span>5</span>, <span>max_iterations</span><span>=</span><span>100</span>, <span>tolerance</span><span>=</span><span>1e-4</span>) <span># labels using kmeans/kmeans++ init</span>
<span>wl</span>.<span>filter</span>(<span>query</span>, <span>candidates</span>, <span>threshold</span><span>=</span><span>0.3</span>) <span># filter candidates based on query</span>
<span>wl</span>.<span>topk</span>(<span>query</span>, <span>candidates</span>, <span>k</span><span>=</span><span>3</span>) <span># return topk strings based on query</span></pre></div>

<p dir="auto">WordLlama is a utility for NLP and word embedding model that recycles components from large language models (LLMs) to create efficient and compact word representations (such as GloVe, Word2Vec or FastText).
WordLlama begins by extracting the token embedding codebook from a state-of-the-art LLM (e.g., LLama3 70B), and training a small context-less model in a general purpose embedding framework.</p>
<p dir="auto">WordLlama improves on all MTEB benchmarks above word models like GloVe 300d, while being substantially smaller in size (<strong>16MB default model</strong> @ 256-dim vs &gt;2GB).</p>
<p dir="auto">Features of WordLlama include:</p>
<ol dir="auto">
<li><strong>Matryoshka Representations</strong>: Truncate embedding dimension as needed.</li>
<li><strong>Low Resource Requirements</strong>: A simple token lookup with average pooling, enables this to operate fast on CPU.</li>
<li><strong>Binarization</strong>: Models trained using the straight through estimator can be packed to small integer arrays for even faster hamming distance calculations. (coming soon)</li>
<li><strong>Numpy-only inference</strong>: Lightweight and simple.</li>
</ol>
<p dir="auto">For flexibility, WordLlama employs the Matryoshka representation learning training technique. The largest model (1024-dim) can be truncated to 64, 128, 256 or 512.
For binary embedding models, we implement straight-through estimators during training. For dense embeddings, 256 dimensions sufficiently captures most of the performance, while for binary embeddings validation accuracy is close to saturation at 512-dimensions (64 bytes packed).</p>
<p dir="auto">The final weights are saved <em>after</em> weighting, projection and truncation of the entire tokenizer vocabulary. Thus, WordLlama becomes a single embedding matrix (nn.Embedding) that is considerably smaller than the gigabyte-sized llm codebooks we start with. The original tokenizer is still used to preprocess the text into tokens, and the reduced size token embeddings are average pooled. There is very little computation required, and the resulting model sizes range from 16mb to 250mb for the 128k llama3 vocabulary.</p>
<p dir="auto">It&#39;s good option for some nlp-lite tasks. You can train sklearn classifiers on it, perform basic semantic matching, fuzzy deduplication, ranking and clustering.
I think it should work well for creating LLM output evaluators, or other preparatory tasks involved in multi-hop or agentic workflows.
You can perform your own llm surgery and train your own model on consumer GPUs in a few hours.
Because of its fast and portable size, it makes a good &#34;Swiss-Army Knife&#34; utility for exploratory analysis and utility applications.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">MTEB Results (l2_supercat)</h2><a id="user-content-mteb-results-l2_supercat" aria-label="Permalink: MTEB Results (l2_supercat)" href="#mteb-results-l2_supercat"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Metric</th>
<th>WL64</th>
<th>WL128</th>
<th>WL256 (X)</th>
<th>WL512</th>
<th>WL1024</th>
<th>GloVe 300d</th>
<th>Komninos</th>
<th>all-MiniLM-L6-v2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Clustering</td>
<td>30.27</td>
<td>32.20</td>
<td>33.25</td>
<td>33.40</td>
<td>33.62</td>
<td>27.73</td>
<td>26.57</td>
<td>42.35</td>
</tr>
<tr>
<td>Reranking</td>
<td>50.38</td>
<td>51.52</td>
<td>52.03</td>
<td>52.32</td>
<td>52.39</td>
<td>43.29</td>
<td>44.75</td>
<td>58.04</td>
</tr>
<tr>
<td>Classification</td>
<td>53.14</td>
<td>56.25</td>
<td>58.21</td>
<td>59.13</td>
<td>59.50</td>
<td>57.29</td>
<td>57.65</td>
<td>63.05</td>
</tr>
<tr>
<td>Pair Classification</td>
<td>75.80</td>
<td>77.59</td>
<td>78.22</td>
<td>78.50</td>
<td>78.60</td>
<td>70.92</td>
<td>72.94</td>
<td>82.37</td>
</tr>
<tr>
<td>STS</td>
<td>66.24</td>
<td>67.53</td>
<td>67.91</td>
<td>68.22</td>
<td>68.27</td>
<td>61.85</td>
<td>62.46</td>
<td>78.90</td>
</tr>
<tr>
<td>CQA DupStack</td>
<td>18.76</td>
<td>22.54</td>
<td>24.12</td>
<td>24.59</td>
<td>24.83</td>
<td>15.47</td>
<td>16.79</td>
<td>41.32</td>
</tr>
<tr>
<td>SummEval</td>
<td>30.79</td>
<td>29.99</td>
<td>30.99</td>
<td>29.56</td>
<td>29.39</td>
<td>28.87</td>
<td>30.49</td>
<td>30.81</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">The <a href="https://huggingface.co/dleemiller/word-llama-l2-supercat" rel="nofollow">l2_supercat</a> is a Llama2-vocabulary model. To train this model, I concatenated codebooks from several models, including Llama2 70B and phi3 medium (after removing additional special tokens).
Because several models have used the Llama2 tokenizer, their codebooks can be concatenated and trained together. Performance of the resulting model is comparable to training the Llama3 70B codebook, while being 4x smaller (32k vs 128k vocabulary).</p>

<p dir="auto"><a href="https://thewhodidthis.com/dleemiller/WordLlama/blob/main/wordllama/RESULTS.md">Results</a></p>
<p dir="auto">Llama3-based: <a href="https://huggingface.co/dleemiller/wordllama-l3-supercat" rel="nofollow">l3_supercat</a></p>

<p dir="auto">Hereâ€™s how you can load pre-trained embeddings and use them to embed text:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from wordllama import WordLlama

# Load pre-trained embeddings
# truncate dimension to 64
wl = WordLlama.load(trunc_dim=64)

# Embed text
embeddings = wl.embed([&#34;the quick brown fox jumps over the lazy dog&#34;, &#34;and all that jazz&#34;])
print(embeddings.shape)  # (2, 64)"><pre><span>from</span> <span>wordllama</span> <span>import</span> <span>WordLlama</span>

<span># Load pre-trained embeddings</span>
<span># truncate dimension to 64</span>
<span>wl</span> <span>=</span> <span>WordLlama</span>.<span>load</span>(<span>trunc_dim</span><span>=</span><span>64</span>)

<span># Embed text</span>
<span>embeddings</span> <span>=</span> <span>wl</span>.<span>embed</span>([<span>&#34;the quick brown fox jumps over the lazy dog&#34;</span>, <span>&#34;and all that jazz&#34;</span>])
<span>print</span>(<span>embeddings</span>.<span>shape</span>)  <span># (2, 64)</span></pre></div>
<p dir="auto">Binary embedding models can be used like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Binary embeddings are packed into uint64
# 64-dims =&gt; array of 1x uint64 
wl = WordLlama.load(trunc_dim=64, binary=True)  # this will download the binary model from huggingface
wl.embed(&#34;I went to the car&#34;) # Output: array([[3029168427562626]], dtype=uint64)

# load binary trained model trained with straight through estimator
wl = WordLlama.load(dim=1024, binary=True)

# Uses the hamming similarity to binarize
similarity_score = wl.similarity(&#34;i went to the car&#34;, &#34;i went to the pawn shop&#34;)
print(similarity_score)  # Output: 0.57421875

ranked_docs = wl.rank(&#34;i went to the car&#34;, [&#34;van&#34;, &#34;truck&#34;])

wl.binary = False # turn off hamming and use cosine

# load a different model class
wl = WordLlama.load(config=&#34;l3_supercat&#34;, dim=1024) # downloads model from HF"><pre><span># Binary embeddings are packed into uint64</span>
<span># 64-dims =&gt; array of 1x uint64 </span>
<span>wl</span> <span>=</span> <span>WordLlama</span>.<span>load</span>(<span>trunc_dim</span><span>=</span><span>64</span>, <span>binary</span><span>=</span><span>True</span>)  <span># this will download the binary model from huggingface</span>
<span>wl</span>.<span>embed</span>(<span>&#34;I went to the car&#34;</span>) <span># Output: array([[3029168427562626]], dtype=uint64)</span>

<span># load binary trained model trained with straight through estimator</span>
<span>wl</span> <span>=</span> <span>WordLlama</span>.<span>load</span>(<span>dim</span><span>=</span><span>1024</span>, <span>binary</span><span>=</span><span>True</span>)

<span># Uses the hamming similarity to binarize</span>
<span>similarity_score</span> <span>=</span> <span>wl</span>.<span>similarity</span>(<span>&#34;i went to the car&#34;</span>, <span>&#34;i went to the pawn shop&#34;</span>)
<span>print</span>(<span>similarity_score</span>)  <span># Output: 0.57421875</span>

<span>ranked_docs</span> <span>=</span> <span>wl</span>.<span>rank</span>(<span>&#34;i went to the car&#34;</span>, [<span>&#34;van&#34;</span>, <span>&#34;truck&#34;</span>])

<span>wl</span>.<span>binary</span> <span>=</span> <span>False</span> <span># turn off hamming and use cosine</span>

<span># load a different model class</span>
<span>wl</span> <span>=</span> <span>WordLlama</span>.<span>load</span>(<span>config</span><span>=</span><span>&#34;l3_supercat&#34;</span>, <span>dim</span><span>=</span><span>1024</span>) <span># downloads model from HF</span></pre></div>

<p dir="auto">Binary embedding models showed more pronounced improvement at higher dimensions, and either 512 or 1024 is recommended for binary embedding.</p>
<p dir="auto">L2 Supercat was trained using a batch size of 512 on a single A100 for 12 hours.</p>

<ul dir="auto">
<li>Working on adding inference features:
<ul dir="auto">
<li>Semantic text splitting</li>
</ul>
</li>
<li>Add example notebooks
<ul dir="auto">
<li>DSPy evaluators</li>
<li>RAG pipelines</li>
</ul>
</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Extracting Token Embeddings</h2><a id="user-content-extracting-token-embeddings" aria-label="Permalink: Extracting Token Embeddings" href="#extracting-token-embeddings"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To extract token embeddings from a model, ensure you have agreed to the user agreement and logged in using the Hugging Face CLI (for llama3 models). You can then use the following snippet:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from wordllama.extract import extract_safetensors

# Extract embeddings for the specified configuration
extract_safetensors(&#34;llama3_70B&#34;, &#34;path/to/saved/model-0001-of-00XX.safetensors&#34;)"><pre><span>from</span> <span>wordllama</span>.<span>extract</span> <span>import</span> <span>extract_safetensors</span>

<span># Extract embeddings for the specified configuration</span>
<span>extract_safetensors</span>(<span>&#34;llama3_70B&#34;</span>, <span>&#34;path/to/saved/model-0001-of-00XX.safetensors&#34;</span>)</pre></div>
<p dir="auto">HINT: Embeddings are usually in the first safetensors file, but not always. Sometimes there is a manifest, sometimes you have to snoop around and figure it out.</p>
<p dir="auto">For training, use the scripts in the github repo. You have to add a configuration file (copy/modify an existing one into the folder).</p>
<div data-snippet-clipboard-copy-content="$ pip install wordllama[train]
$ python train.py train --config your_new_config
(training stuff happens)
$ python train.py save --config your_new_config --checkpoint ... --outdir /path/to/weights/
(saves 1 model per matryoshka dim)"><pre><code>$ pip install wordllama[train]
$ python train.py train --config your_new_config
(training stuff happens)
$ python train.py save --config your_new_config --checkpoint ... --outdir /path/to/weights/
(saves 1 model per matryoshka dim)
</code></pre></div>

<p dir="auto">If you use WordLlama in your research or project, please consider citing it as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@software{miller2024wordllama,
  author = {Miller, D. Lee},
  title = {WordLlama: Recycled Token Embeddings from Large Language Models},
  year = {2024},
  url = {https://github.com/dleemiller/wordllama},
  version = {0.2.5}
}"><pre><span>@software</span>{<span>miller2024wordllama</span>,
  <span>author</span> = <span><span>{</span>Miller, D. Lee<span>}</span></span>,
  <span>title</span> = <span><span>{</span>WordLlama: Recycled Token Embeddings from Large Language Models<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2024<span>}</span></span>,
  <span>url</span> = <span><span>{</span>https://github.com/dleemiller/wordllama<span>}</span></span>,
  <span>version</span> = <span><span>{</span>0.2.5<span>}</span></span>
}</pre></div>

<p dir="auto">This project is licensed under the MIT License.</p>
</article></div></div>
  </body>
</html>
