<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/certik/fastGPT">Original</a>
    <h1>Fast GPT-2 inference written in Fortran</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">The progression of GPT-2 codes from the original to &#34;minimal&#34;, &#34;nano&#34; and
&#34;pico&#34;:</p>
<ul dir="auto">
<li><a href="https://github.com/openai/gpt-2">openai/gpt-2</a></li>
<li><a href="https://github.com/karpathy/mingpt">karpathy/minGPT</a></li>
<li><a href="https://github.com/karpathy/nanogpt">karpathy/nanoGPT</a></li>
<li><a href="https://github.com/jaymody/picoGPT">jaymody/picoGPT</a></li>
</ul>
<p dir="auto"><code>fastGPT</code> is very similar to <code>picoGPT</code> (very small and readable), but it is
also fast (see the Benchmarks section below). The speed and readability is
achieved by using Fortran. I wrote a
<a href="https://ondrejcertik.com/blog/2023/03/fastgpt-faster-than-pytorch-in-300-lines-of-fortran/" rel="nofollow">blog post</a>
introducing fastGPT.</p>
<p dir="auto"><code>fastGPT</code> features:</p>
<ul dir="auto">
<li>Fast? <g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></li>
<li>Training code? <g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji></li>
<li>Batch inference? <g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji></li>
<li>top-p sampling? <g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji> top-k? <g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji> temperature? <g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji> categorical sampling?! <g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji> greedy? <g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></li>
<li>Readable? <g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></li>
<li>Small? <g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></li>
</ul>
<p dir="auto">A quick breakdown of each of the files:</p>
<ul dir="auto">
<li><code>gpt2.f90</code>: the actual GPT-2 model and a decoder</li>
<li><code>main.f90</code>: the main driver</li>
<li><code>create_model.py</code>: downloads the TensorFlow model and converts to our own
format (<code>model.dat</code>)</li>
<li><code>encode_input.py</code>: encodes the text input into tokens (input file for <code>gpt2</code>)</li>
<li>Matmul implementations
<ul dir="auto">
<li><code>linalg_f.f90</code> native Fortran</li>
<li><code>linalg_c.f90</code>, <code>linalg_accelerate.c</code> macOS Accelerate Framework</li>
</ul>
</li>
<li><code>pt.py</code>: a reference script to run PyTorch (returns the same answer)</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-build-and-run" aria-hidden="true" href="#build-and-run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Build and Run</h2>
<p dir="auto">Install prerequisites:</p>
<div data-snippet-clipboard-copy-content="mamba env create -f environment.yml
conda activate fastgpt"><pre><code>mamba env create -f environment.yml
conda activate fastgpt
</code></pre></div>
<p dir="auto">Configure and build:</p>

<p dir="auto">Create the <code>model.dat</code> file from a given GPT-2 model. Supported sizes (and the
corresponding names to be used in <code>pt.py</code>, and the approximate download size):
&#34;124M&#34; (<code>gpt2</code>, 0.5GB), &#34;355M&#34; (<code>gpt-medium</code>, 1.5GB), &#34;774M&#34; (<code>gpt-large</code>,
3GB), &#34;1558M&#34; (<code>gpt-xl</code>, 6GB). This will download the model and cache it for
subsequent runs:</p>
<div data-snippet-clipboard-copy-content="python create_model.py --models_dir &#34;models&#34; --model_size &#34;124M&#34;"><pre><code>python create_model.py --models_dir &#34;models&#34; --model_size &#34;124M&#34;
</code></pre></div>
<p dir="auto">Now you can modify the <code>input</code> file to change the input string and set other
parameters.</p>
<p dir="auto">Run (requires <code>model.dat</code> and <code>input</code> in the current directory):</p>

<h3 tabindex="-1" dir="auto"><a id="user-content-example-output" aria-hidden="true" href="#example-output"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example Output</h3>
<p dir="auto">The above <code>./gpt2</code> command prints on Apple M1 Max:</p>
<div data-snippet-clipboard-copy-content="$ ./gpt2
Loading the model...
    done. Time:   0.111s

Model parameters:
n_vocab = 50257
n_ctx   =  1024
n_embd  =   768
n_layer =    12
n_head  =    12

Input text
Alan Turing theorized that computers would one day become very powerful, but even he could not imagine

Encoding: tokenizing input text into tokens (currently slow)...
    done. Time:   0.074s

Input parameters:
n_seq                =  19
n_tokens_to_generate =  20

Input tokens:
 36235 39141 18765  1143   326  9061   561   530  1110  1716   845  3665    11   475   772   339   714   407  5967

Decoded input as text:
Alan Turing theorized that computers would one day become very powerful, but even he could not imagine

Running model...
 how they would be able to do so.

&#34;I think that the most important thing is
    done. Time:   0.304s (1.01x)

Output tokens:
   703   484   561   307  1498   284   466   523    13   198   198     1    40   892   326   262   749  1593  1517   318

Decoded output as text:
 how they would be able to do so.

&#34;I think that the most important thing is"><pre><code>$ ./gpt2
Loading the model...
    done. Time:   0.111s

Model parameters:
n_vocab = 50257
n_ctx   =  1024
n_embd  =   768
n_layer =    12
n_head  =    12

Input text
Alan Turing theorized that computers would one day become very powerful, but even he could not imagine

Encoding: tokenizing input text into tokens (currently slow)...
    done. Time:   0.074s

Input parameters:
n_seq                =  19
n_tokens_to_generate =  20

Input tokens:
 36235 39141 18765  1143   326  9061   561   530  1110  1716   845  3665    11   475   772   339   714   407  5967

Decoded input as text:
Alan Turing theorized that computers would one day become very powerful, but even he could not imagine

Running model...
 how they would be able to do so.

&#34;I think that the most important thing is
    done. Time:   0.304s (1.01x)

Output tokens:
   703   484   561   307  1498   284   466   523    13   198   198     1    40   892   326   262   749  1593  1517   318

Decoded output as text:
 how they would be able to do so.

&#34;I think that the most important thing is
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-chat-interface" aria-hidden="true" href="#chat-interface"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Chat interface</h3>
<p dir="auto">Here is an example chat using the largest 1558M model:</p>
<div data-snippet-clipboard-copy-content="$ ./chat
Your name is fastGPT and you are an AI bot. The user will ask you questions and you answer in a nice, truthful, short way.
User: What is the capital of Czechia?
fastGPT: Prague.
User: How many legs does a dog have?
fastGPT: Four.
User: What color does the sky have?
fastGPT: Blue.
User: What can you type a document on?
fastGPT: A typewriter.
User: What can you drive in?
fastGPT: A car.
User: What can you fly in?
fastGPT: A plane.
User: What continent is Germany in?
fastGPT: Europe.
User: When did Second World War start?
fastGPT: 1939.
User: When did it end?
fastGPT: 1945.
User: When did the U.S. enter the Second World War?
fastGPT: 1941.
User: When did the First World War start?
fastGPT: 1914.
User: When did it end?
fastGPT: 1918.
User: When did the Mexican-American war start?
fastGPT: 1846.
User: When did it end?
fastGPT: 1848.
User: What color is snow?
fastGPT: White.
User: What color do plants usually have?
fastGPT: Green.
User: What is your name?
fastGPT: fastGPT."><pre><code>$ ./chat
Your name is fastGPT and you are an AI bot. The user will ask you questions and you answer in a nice, truthful, short way.
User: What is the capital of Czechia?
fastGPT: Prague.
User: How many legs does a dog have?
fastGPT: Four.
User: What color does the sky have?
fastGPT: Blue.
User: What can you type a document on?
fastGPT: A typewriter.
User: What can you drive in?
fastGPT: A car.
User: What can you fly in?
fastGPT: A plane.
User: What continent is Germany in?
fastGPT: Europe.
User: When did Second World War start?
fastGPT: 1939.
User: When did it end?
fastGPT: 1945.
User: When did the U.S. enter the Second World War?
fastGPT: 1941.
User: When did the First World War start?
fastGPT: 1914.
User: When did it end?
fastGPT: 1918.
User: When did the Mexican-American war start?
fastGPT: 1846.
User: When did it end?
fastGPT: 1848.
User: What color is snow?
fastGPT: White.
User: What color do plants usually have?
fastGPT: Green.
User: What is your name?
fastGPT: fastGPT.
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-blas-implementation" aria-hidden="true" href="#blas-implementation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BLAS Implementation</h3>
<p dir="auto">You can choose which BLAS implementation to use for <code>matmul</code> using:</p>
<ul dir="auto">
<li><code>-DFASTGPT_BLAS=OpenBLAS</code>: Use OpenBLAS</li>
<li><code>-DFASTGPT_BLAS=Accelerate</code>: Use the macOS Accelerate Framework</li>
<li><code>-DFASTGPT_BLAS=Fortran</code>: Use the default Fortran&#39;s intrinsic <code>matmul</code></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-benchmarks" aria-hidden="true" href="#benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Benchmarks</h2>
<p dir="auto">On Apple M1 Max, inference of the above input file (20 tokens):</p>
<div data-snippet-clipboard-copy-content="                                1 core  2 cores  4 cores  8 cores

fastGPT (Accelerate, fast_tanh) 0.288s

fastGPT (Accelerate)            0.299s
PyTorch (Accelerate)            0.346s

fastGPT (OpenBLAS)              0.837s  0.514s    0.341s   0.339s
PyTorch (OpenBLAS)              0.873s  0.539s    0.386s   0.392s

fastGPT (Accelerate, no cache)  0.717s
picoGPT (Accelerate, no cache)  0.765s
PyTorch (Accelerate, no cache)  0.787s

fastGPT (OpenBLAS, no cache)    2.343s  1.603s    1.209s   1.018s
PyTorch (OpenBLAS, no cache)    2.356s  1.520s    1.104s   0.997s
picoGPT (OpenBLAS, no cache)    2.427s  1.645s    1.272s   1.081s"><pre><code>                                1 core  2 cores  4 cores  8 cores

fastGPT (Accelerate, fast_tanh) 0.288s

fastGPT (Accelerate)            0.299s
PyTorch (Accelerate)            0.346s

fastGPT (OpenBLAS)              0.837s  0.514s    0.341s   0.339s
PyTorch (OpenBLAS)              0.873s  0.539s    0.386s   0.392s

fastGPT (Accelerate, no cache)  0.717s
picoGPT (Accelerate, no cache)  0.765s
PyTorch (Accelerate, no cache)  0.787s

fastGPT (OpenBLAS, no cache)    2.343s  1.603s    1.209s   1.018s
PyTorch (OpenBLAS, no cache)    2.356s  1.520s    1.104s   0.997s
picoGPT (OpenBLAS, no cache)    2.427s  1.645s    1.272s   1.081s
</code></pre></div>
<p dir="auto">Total run (includes loading the model and Python imports):</p>
<div data-snippet-clipboard-copy-content="fastGPT (Accelerate, fast_tanh): 0.401s
picoGPT (8 cores):               3.445s
PyTorch (OpenBLAS, 4 cores):     4.867s"><pre><code>fastGPT (Accelerate, fast_tanh): 0.401s
picoGPT (8 cores):               3.445s
PyTorch (OpenBLAS, 4 cores):     4.867s
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-todo" aria-hidden="true" href="#todo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TODO</h2>
<ul>
<li> Parallelization:
<ul>
<li> Over heads: <a data-error-text="Failed to load title" data-id="1610516399" data-permission-text="Title is private" data-url="https://github.com/certik/fastGPT/issues/2" data-hovercard-type="issue" data-hovercard-url="/certik/fastGPT/issues/2/hovercard" href="https://github.com/certik/fastGPT/issues/2">#2</a></li>
<li> MPI: <a data-error-text="Failed to load title" data-id="1610527333" data-permission-text="Title is private" data-url="https://github.com/certik/fastGPT/issues/5" data-hovercard-type="issue" data-hovercard-url="/certik/fastGPT/issues/5/hovercard" href="https://github.com/certik/fastGPT/issues/5">#5</a></li>
</ul>
</li>
<li> Other sampling methods: <a data-error-text="Failed to load title" data-id="1610536789" data-permission-text="Title is private" data-url="https://github.com/certik/fastGPT/issues/8" data-hovercard-type="issue" data-hovercard-url="/certik/fastGPT/issues/8/hovercard" href="https://github.com/certik/fastGPT/issues/8">#8</a></li>
<li> Batching: <a data-error-text="Failed to load title" data-id="1610533059" data-permission-text="Title is private" data-url="https://github.com/certik/fastGPT/issues/7" data-hovercard-type="issue" data-hovercard-url="/certik/fastGPT/issues/7/hovercard" href="https://github.com/certik/fastGPT/issues/7">#7</a></li>
<li> Improve the UI:
<ul>
<li> Implement the input tokenizer in Fortran: <a data-error-text="Failed to load title" data-id="1610515229" data-permission-text="Title is private" data-url="https://github.com/certik/fastGPT/issues/1" data-hovercard-type="issue" data-hovercard-url="/certik/fastGPT/issues/1/hovercard" href="https://github.com/certik/fastGPT/issues/1">#1</a></li>
<li> Show the words as they are generated: <a data-error-text="Failed to load title" data-id="1610531224" data-permission-text="Title is private" data-url="https://github.com/certik/fastGPT/issues/6" data-hovercard-type="issue" data-hovercard-url="/certik/fastGPT/issues/6/hovercard" href="https://github.com/certik/fastGPT/issues/6">#6</a></li>
</ul>
</li>
</ul>
</article>
          </div></div>
  </body>
</html>
