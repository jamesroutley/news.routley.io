<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/voxtral">Original</a>
    <h1>Voxtral – Frontier open source speech understanding models</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><img src="https://cms.mistral.ai/assets/ec026954-d85f-4b11-94fd-d26fc8e13ae2.png?width=2206&amp;height=1190" alt="Triangle Voxtral Blog"/></p>
<h3 dir="ltr">Voice: the original UI.</h3>
<p dir="ltr">Voice was humanity’s first interface—long before writing or typing, it let us share ideas, coordinate work, and build relationships. As digital systems become more capable, voice is returning as our most natural form of human-computer interaction.</p>
<p dir="ltr">Yet today’s systems remain limited—unreliable, proprietary, and too brittle for real-world use. Closing this gap demands tools with exceptional transcription, deep understanding, multilingual fluency, and open, flexible deployment.</p>
<p dir="ltr">We release the Voxtral models to accelerate this future. These state‑of‑the‑art speech understanding models are  available in two sizes—a 24B variant for production-scale applications and a 3B variant for local and edge deployments. Both versions are released under the Apache 2.0 license. We have also made both models available on our API, and also provided a highly optimized transcription-only <a href="https://console.mistral.ai/">endpoint</a> that delivers unparalleled cost-efficiency.</p>
<h3 dir="ltr">Open, affordable, and production-ready speech understanding for everyone. </h3>
<p dir="ltr">Until recently, gaining truly usable speech intelligence in production meant choosing between two trade-offs:</p>
<ol>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Open-source ASR systems with high word error rates and limited semantic understanding</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Closed, proprietary APIs that combine strong transcription with language understanding, but at significantly higher cost and with less control over deployment</p>
</li>
</ol>
<p dir="ltr">Voxtral bridges this gap. It offers state-of-the-art accuracy and native semantic understanding in the open, at less than half the price of comparable APIs. This makes high-quality speech intelligence accessible and controllable at scale. </p>
<p dir="ltr">Both Voxtral models go beyond transcription with capabilities that include:</p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><em>Long-form context:</em> with a 32k token context length, Voxtral handles audios up to 30 minutes for transcription, or 40 minutes for understanding </p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><em>Built-in Q&amp;A and summarization:</em> Supports asking questions directly about the audio content or generating structured summaries, without the need to chain separate ASR and language models</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><em>Natively multilingual:</em> Automatic language detection and state-of-the-art performance in the world’s most widely used languages (English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian, to name a few), helping teams serve global audiences with a single system</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><em>Function-calling straight from voice:</em> Enables direct triggering of backend functions, workflows, or API calls based on spoken user intents, turning voice interactions into actionable system commands without intermediate parsing steps.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><em>Highly capable at text:</em> Retains the text understanding capabilities of its language model backbone, Mistral Small 3.1</p>
</li>
</ul>
<p dir="ltr">These capabilities make the Voxtral models ideal for real-world interactions and downstream actions, such as summaries, answers, analysis, and insights. For cost-sensitive use-cases, Voxtral Mini Transcribe outperforms OpenAI Whisper for less than half the price. For premium use cases, Voxtral Small matches the performance of ElevenLabs Scribe, also for less than half the price.</p>
<div><div><p><audio src="/audio/chat-fr.m4a">Your browser does not support the audio element.</audio></p><p><span>0:00</span><span>0:15</span></p></div></div>
<h2>Benchmarks</h2>
<h3 dir="ltr">Speech Transcription</h3>
<p dir="ltr">To assess Voxtral’s transcription capabilities, we evaluate it on a range of English and multilingual benchmarks. For each task, we report the macro-average word error rate (lower is better) across languages. For English, we report a short-form (&lt;30-seconds) and long-form (&gt;30-seconds) average.</p>
<p dir="ltr">Voxtral comprehensively outperforms Whisper large-v3, the current leading open-source Speech Transcription model. It beats GPT-4o mini Transcribe and Gemini 2.5 Flash across all tasks, and achieves state-of-the-art results on English short-form and Mozilla Common Voice, surpassing ElevenLabs Scribe and demonstrating its strong multilingual capabilities.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/3f77f74c-f1a9-4a42-af04-3c72c78fd295.png?width=1600&amp;height=995" alt="V Plot 1"/></p>
<p dir="ltr">When evaluated across languages in FLEURS, Voxtral Small outperforms Whisper on every task, achieving state-of-the-art performance in a number of European languages.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/6a3aaf3e-7041-41c6-9cd0-14cde11e3cba.png?width=1600&amp;height=900" alt="V Plot 2"/></p>
<p dir="ltr">Macro-average details:</p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">En short-form: LibriSpeech Clean, LibriSpeech Other, GigaSpeech, VoxPopuli, Switchboard, CHiME-4, SPGISpeech</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">En long-form: Earnings-21 10-m, Earnings-22 10-m</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Mozilla Common Voice 15.1: English, French, German, Spanish, Italian, Portuguese, Dutch, Hindi</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">FLEURS: English, French, German, Spanish, Italian, Portuguese, Dutch, Hindi, Arabic</p>
</li>
</ul>
<h3 dir="ltr">Audio Understanding</h3>
<p dir="ltr">Voxtral Small and Mini are capable of answering questions directly from speech, or by providing an audio and a text-based prompt. To evaluate Audio Understanding capabilities, we create speech-synthesized versions of three common Text Understanding tasks. We also evaluate the models on an in-house Audio Understanding (AU) Benchmark, where the model is tasked with answering challenging questions on 40 long-form audio examples. Finally, we assess Speech Translation capabilities on the FLEURS-Translation benchmark.</p>
<p dir="ltr">Voxtral Small is competitive with GPT-4o-mini and Gemini 2.5 Flash across all tasks, achieving state-of-the-art performance in Speech Translation.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/d77c4d21-84a9-437f-b9c9-3b27725261ff.png?width=2746&amp;height=1482" alt="V Plot 3"/></p>
<h3 dir="ltr"><strong>Text</strong></h3>
<p dir="ltr">Voxtral retains the text capabilities of its Language-Model backbone, enabling it to be used as a drop-in replacement for Ministral and Mistral Small 3.1 respectively.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/977d9b1c-a999-4e6a-9b84-87654cee6ec5.png?width=1600&amp;height=958" alt="V Plot 4"/></p>
<h2 dir="ltr">Try it for free </h2>
<p dir="ltr">Whether you’re prototyping on a laptop, running private workloads on-premises, or scaling to production in the cloud, getting started is straightforward.</p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Download and run locally: Both Voxtral (24B) and Voxtral Mini (3B) are available to <a href="https://huggingface.co/mistralai/">download</a> on Hugging Face</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Try the API: Integrate frontier speech intelligence into your application with a single <a href="https://console.mistral.ai/">API call</a>. Pricing starts at $0.001 per minute, making high-quality transcription and understanding affordable at scale. Check out our documentation <a href="https://docs.mistral.ai/capabilities/audio/">here</a>. </p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Try it on Le Chat: Try Voxtral in <a href="http://chat.mistral.ai">Le Chat</a>’s voice mode (rolling out to all users in the next couple of weeks)—on web or mobile. Record or upload audio, get transcriptions, ask questions, or generate summaries.</p>
</li>
</ul>
<h3 dir="ltr">Advanced enterprise features. </h3>
<p dir="ltr">We also offer capabilities for Voxtral designed for enterprises with higher security, scale, or domain-specific requirements. Please <a href="https://mistral.ai/contact">reach out to us</a> if you are considering: </p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Private deployment at production-scale: Our solutions team can help you set up Voxtral for production-scale inference entirely within your own infrastructure. This is ideal for use cases in regulated industries with strict data privacy requirements. This includes guidance and tooling for deploying Voxtral across multiple GPUs or nodes, with quantized builds optimized for production throughput and cost efficiency.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Domain-specific fine-tuning: Work with our <a href="https://mistral.ai/services">applied AI</a> team to adapt Voxtral to specialized contexts—such as legal, medical, customer support, or internal knowledge bases—improving accuracy for your use case.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Advanced context: We’re inviting design partners to build support for speaker identification, emotion detection, advanced diarization, and even longer context windows to meet a wider variety of needs out of the box.</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Dedicated integration support: Priority access to engineering resources and consulting to help integrate Voxtral cleanly into your existing workflows, products, or data pipelines.</p>
</li>
</ul>
<h3 dir="ltr">Coming up.</h3>
<p dir="ltr">We will be hosting a live webinar with our friends at Inworld (check out their cool speech-to-speech <a href="https://inworld-mistral-demo.inworld.ai/index.html">demo</a> with Voxtral and Inworld TTS!) to showcase how you can build end-to-end voice-powered agents on Wednesday, Aug 6. If you’re interested, please register <a href="https://lu.ma/zzgc68zw">here</a>. </p>
<p dir="ltr">We’re working on making our audio capabilities more feature-rich in the forthcoming months. In addition to speech understanding, will we soon support: </p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Speaker segmentation </p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Audio markups such as age and emotion</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Word-level timestamps</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Non-speech audio recognition</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">And more!</p>
</li>
</ul>
<p dir="ltr">We’re excited to see what you will build with Voxtral.</p>
<div>
<h2 dir="ltr">BTW, we’re hiring!</h2>
<p dir="ltr">The release of our Voxtral models marks a significant step forward, but our journey is far from over. Our ambition is to build the most natural, delightful near-human-like voice interfaces and there&#39;s lot more work to do. We are actively expanding our nascent audio team and looking for talented research scientists and engineers who share our ambition.</p>
<p dir="ltr">If you’re interested in joining us on our mission to democratize artificial intelligenceI, we welcome your applications to <a href="https://mistral.ai/careers">join our team</a>! </p>
</div></div></div></div>
  </body>
</html>
