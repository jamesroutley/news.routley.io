<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/">Original</a>
    <h1>We ran over 600 image generations to compare AI image models</h1>
    
    <div id="readability-page-1" class="page"><div>
		
<p><b>tl:dr; </b>We’ve been making photo apps for iOS for long enough that we have gray hairs now, and using our experience we ran over 600 image generations to compare which AI models work best for which image edits. If you want, you can jump right to the <a href="#filters">image comparisons</a>, or the <a href="#conclusion">conclusion</a>, but promise us you won’t presumptuous comments on Hacker News until you’ve also read the background!</p>



<h2>Background</h2>



<p>Hi! We’re LateNiteSoft, and we’ve been working on photography-related iOS apps for 15 years now. Working on market-leading apps such as Camera+, <a href="https://photon.cam/?from=lnsblog">Photon</a> and <a href="https://recvideoapp.com">REC</a>, we’ve always had our finger on the pulse on what users want out of their mobile photography.</p>



<p>With the ground-breaking release of OpenAI’s <b>gpt-image-1</b> image generation model earlier this year, we started investigating all the interesting use cases we could think of for AI image editing.</p>



<p>But as a company that has never taken any venture capital investment, all our products have to pay for themselves. We’re in it to delight our users, not just capture market share and sell them out. When considering AI projects, one thing has been clear – we can’t take the AI startup road where you have a generous free tier, charge an unreasonably small monthly fee for “unlimited”, and hope you’re going to make it up on scale (code for “someone please acquire us”).</p>



<p>All the AI-focused billing systems we could find out there were based on this. Assuming you want to claim unlimited access, and then sandbag users with “fair use” clauses and prevent them from any actual unlimited usage (which is, obviously, untenable, since you’ll end up with one $20/mo user reselling to everyone else).</p>



<p>Since we want to fairly charge our customers for what they actually use, we’ve built a credit-based “pay per generation”-style billing system (that internally we’ve been calling CreditProxy). We’ve also been planning on providing this as a service, since nobody else seems to be doing it, so if you’re interested in being a trial user, <a href="https://latenitesoft.com/contact/">get in touch!</a></p>



<p>We released our app <a href="https://apps.apple.com/app/apple-store/id6745558534?pt=11365&amp;ct=lnsblog&amp;mt=8">MorphAI</a> as a public proof of concept to give CreditProxy a proper real world-test, and have marketed it to the users of Camera+, which includes traditional photo-editing functionality, including a whole host of popular photo filters, giving us a built-in audience of customers ready for the next step in image editing.</p>



<p>With the release of newer models like nanoBanana and Seedream, we’ve had to consider which models make sense to support. We need to explore the trade-offs between quality, prompt complexity, and pricing.</p>



<p>A couple of hastily-hacked together scripts, and many, many AI generation credits later, we have some results! So that everyone else also doesn’t have to waste their money, we figured we’d share what we found:</p>



<h2>The Tests</h2>



<p>Based on our experience with Camera+ and the kind of edits our users have been making with MorphAI, we picked a host of somewhat naive prompts. Veteran Midjourney users may scoff at these, but in our experience these are the kinds of prompts that our average user is likely to use.</p>



<p>As for test photos, we chose some some representative things people like to take photos of: their pets, their kids, landscapes, their cars, and product photography.</p>



<p>
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/landscape.jpg"/> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/car.jpg"/> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/pets.jpg"/> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/portrait.jpg"/> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/product.jpg"/>
</p>



<p>Image generation times are also relevant. During our test period, the generation time for all models was fairly consistent, and didn’t vary by image or prompt complexity.</p>



<figure><table><tbody><tr><td><strong>OpenAI (High)</strong></td><td><strong>Gemini</strong></td><td><strong>Seedream</strong></td></tr><tr><td>80 seconds</td><td>11 seconds</td><td>9 seconds</td></tr></tbody></table></figure>



<p>OpenAI also has a quality setting, the images included here were all generated on High quality, but we also tested Medium, and those generations averaged 36 seconds. We can include the Medium quality images as well if there is any interest!</p>



<p>There are a ton of photos to compare here, so to make things easier to flip through, here are some <strong>keyboard shortcuts</strong> to help you out: Click on a photo to see it larger. Now you can use the arrow keys to switch between models. Press the tab key to switch between test images. Hit ESC to leave the view.</p>



<h2 id="filters">Classic filters</h2>



<p>These are the types of filters that we used to implement manually, by painstakingly hand-crafting textures and Photoshop layers and then converting those to Objective-C code. Now all you need is a few words into a language model (and to burn down half of a rain forest or so; just the cost of progress).</p>



<p>Our conclusion for this category is that for photo realistic filters like this, Gemini really shines by preserving details from the original and minimizing hallucinations, but often at the expense of the strength and creativity of the effect. Especially with photos of people, Gemini seems to refuse to apply any edits at all, with a strong bias towards photo realism.</p>



<p>OpenAI really likes to mess with the details of the photo, giving a characteristic “AI slop” feel, which can be a deal breaker on things like human faces.</p>



<h3>Grungy vintage photo</h3>






<h3>Use soft, diffused lighting</h3>






<h3>Transform into a kaleidoscopic pattern</h3>



<p>Gemini took some really odd shortcuts in generating some of these!</p>






<h3>Apply a heat map effect</h3>



<p>It’s clear that none of the models actually have a concept of what generates heat here, aside from Seedream knowing that humans generate heat, clearly revealing that without any ground truth the models struggle.</p>






<h3>Make it look like a long exposure photograph</h3>



<p>This is an interesting test since in some of the sample photos a long exposure doesn’t make sense. In the ones where it makes the most sense – the landscape and the car, OpenAI did the best, but on the other hand it completely messed up the cats and the product, and the portrait photo turned into a trippy art piece.</p>



<p>Gemini, maybe logically, did nothing. Seedream liked adding light streaks as if a car drove past, with only the portrait photo seemingly making any sense.</p>






<h3>Pinhole camera</h3>



<p>In this case, it was funny to watch Gemini take a literal approach and generate actual pictures of cameras! For this reason we re-worked this prompt by just adding the word “effect”.</p>






<h3>Pinhole camera effect</h3>



<p>Gemini liked to generate a literal pinhole camera here so we tried modifying the prompt.</p>






<h3>Add a layer of fog or mist</h3>






<h3>Make it look like it’s golden hour</h3>






<h3>Make it look like it’s etched in glass</h3>



<p>With this prompt, there is ambiguity in what “it” is, so we tried a reworded prompt as well. Only OpenAI consistently knew what a traditional etched glass effect looks like. Seedream’s glass item effect looks really cool!</p>






<h3>Make it look like the photo is etched in glass</h3>



<p>Gemini has a really interesting interpretation here! And Seedream had some pretty fantastic results.</p>






<h3>Remove background</h3>



<p>This is a classic job people have spent their lives doing manually in Photoshop since the early 90’s. But what is a “background”, really? Is the ground in front of a car the “background”? We also retried this with a tweaked prompt.</p>



<p>OpenAI’s “sloppification” of the details of objects makes it useless for this purpose.</p>






<h3>Isolate the object</h3>



<p>With the tweaked prompt, Gemini’s API actually returned a followup question: <em>“Which object would you like to isolate? There are two cats in the image.”</em>, which our generation script was not prepared to handle! So it is missing from this comparison.</p>






<h3>Give it a metallic sheen</h3>



<p>Another case where “it” is vague and we can retry with a more specific prompt. The product imagery is another case where Seedream created a really stunning result, even adding a reflection of someone taking the photo with their phone!</p>






<h3>Give the object a metallic sheen</h3>



<p>Modifying the prompt here really only changed OpenAI’s interpretation.</p>






<h2>Lens effects</h2>



<p>One of the filter packs we had worked on for Camera+ using traditional methods was a lens effect filter pack. But unlike traditional edits, with generative AI you can also create wide-angle lens effects that can just make up the portions of the image that the camera couldn’t capture.</p>



<p>This is another category where it’s very visible how OpenAI regenerates and hallucinates all the details in a picture, where Gemini and Seedream’s results are very faithful to the original and look more like actual lens permutations.</p>



<h3>Apply a fish-eye lens effect</h3>






<h3>Strong bokeh blur</h3>



<p>It was pretty surprising how poorly the models did here considering how common this must be among the training data. OpenAI give a strong blur but no bokeh effects. Gemini gives us a bunch of random circles in front of the image, demonstrating an understanding of what people want out of a bokeh filter but not how it works. Seedream does really well here.</p>






<h3>Apply a Dutch angle (canted frame)</h3>



<p>OpenAI really lost it’s mind here on the car photo.</p>






<h3>Change to a bird’s-eye view</h3>






<h2>Style transfer</h2>



<p>Style Transfer is the process of applying an artistic style to a photo. This technique predates the current AI model by quite a few years with popular apps generating Van Gogh paintings out of your photos. We were also early out in attempting style transfer for our apps, shout out to Noel’s Intel iMac which had to run at full blast all night just to generate a 256x256px image, since it was our only machine with a compatible GPU.</p>



<p>While Gemini was good at preserving reality in the more photorealistic effects in the previous section, when it comes to the more artistic styles, OpenAI has them beat, while Gemini keeps things far too conservative, especially with photos of a human in them, where it sometimes seems to just do nothing at all, is this some kind of safety guardrail?</p>



<h3>Draw this in the style of a Studio Ghibli movie</h3>



<p>ChatGPT went viral with this prompt, with Sam Altman even making it his profile on X. And OpenAI keeps the crown – is Google too conservative in order to avoid a lawsuit? Seedream makes an attempt but they just end up looking like “generic Anime”.</p>






<h3>Transform into watercolor painting</h3>






<h3>Make it look like a pastel artwork</h3>






<h3>Transform into Art Nouveau style</h3>






<h3>Apply a ukiyo-e Japanese woodblock print style</h3>



<p>A very stark example of Gemini failing to apply a style on photos with humans. This is a prompt where Seedream knocked it out of the park, perhaps showing a larger portion of their training data being sourced from asian cultures than the western models.</p>






<h3>Transform into low poly art</h3>



<p>Seedream blows everyone else away here.</p>






<h2>Portrait effects</h2>



<p>For prompts about human appearance, we have only applied them to the portrait photo.</p>



<h3>Make it look like a caricature</h3>



<p>Seedream seems to be biased towards asian culture, giving an anime look instead of a western-style cartoon caricature.</p>






<h3>Turn them into an action figure in the blister pack</h3>



<p>OpenAI’s style here went viral a while back, but Gemini is stunningly realistic. Seedream is a weird mix of realistic and hallucinations.</p>






<h2>Generative edits</h2>



<p>The place where generative AI really shines is when it can show off some creativity, and these were some prompts we added as suggestions in MorphAI to showcase that and inspire our users. OpenAI still seems to win here.</p>



<h3>Create a 70’s vinyl record cover</h3>



<p>This is an example of a prompt that has a small viral moment with OpenAI, but the other models can’t even get the aspect ratio right.</p>






<h3>Introduce mythical creatures native to this environment</h3>



<p>This one showcases OpenAI’s creativity. Gemini seems kind of creepy?</p>






<h3>Add a mystical portal or gateway</h3>



<p>Gemini replacing the face with a portal is certainly a choice!</p>






<h3>Incorporate futuristic technology elements</h3>



<p>Another example of OpenAI being far more creative and willing to re-do the whole image.</p>






<h3>Make it look whimsical and enchanting</h3>



<p>This one also shows OpenAI being more artistic, and Gemini being more realistic while still trying to incorporate the prompt.</p>






<h3>Transform the scene to a stormy night</h3>






<h2 id="conclusion">Conclusion</h2>



<p>If you made it all the way down here you probably don’t need a summary, but for our purposes, we’ve at least concluded that there is no one-size-fits all model at this point.</p>



<p>OpenAI is great for fully transformative filters like style transfer or more creative generative applications, whereas Gemini works better for more realistic edits. Seedream lies somewhere in the middle and is a bit of a jack of all trades, and for the price and performance may be a good replacement for OpenAI.</p>



<p>We’ve been experimenting on working on a “prompt classifier” to automatically choose a model – sending artistic prompts to OpenAI and more realistic prompts to Gemini, if there’s any interest we can follow up with how that worked out!</p>















<h4>Methodology</h4>



<p>Tests were performed on October 8 with <code>gpt-image-1</code>, <code>gemini-2.5-flash-image</code> and <code>seedream-4-0-250828</code>.</p>



<p>Timings were measured on a consumer internet connection in Japan (Fiber connection, 10 Gbps nominal bandwidth) during a limited test run in a short time period.</p>
	</div></div>
  </body>
</html>
