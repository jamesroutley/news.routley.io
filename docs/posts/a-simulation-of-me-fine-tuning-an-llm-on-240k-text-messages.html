<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://edwarddonner.com/2024/01/02/fine-tuning-an-llm-on-240k-text-messages/">Original</a>
    <h1>A simulation of me: fine-tuning an LLM on 240k text messages</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>I’m far from the first to think of this. Several people — perhaps inspired by creepy Black Mirror episodes — have tried to fine-tune an LLM on their SMS or WhatsApp history in an effort to create a simulation of themselves.</p>



<p>Generally results have disappointed; conversations tend to be mundane (it seems we don’t share our most profound insights by text), and quickly descend into loops.</p>



<p>I’ve been dying to have a try myself. Over the holiday break, I had a spare few moments and a $100 Xmas gift to blow on google compute, and I got to work between helpings of turkey.</p>



<h4>The strategy</h4>



<p>I’ll write a series of posts about how I approached it, but here’s the gist:</p>



<h5>1. GET THE MESSAGES</h5>



<p>I used a utility called <a href="https://imazing.com/">iMazing</a> to download all my SMS / iMessage and WhatsApp conversations. I filtered out group chats, people not in my contacts, and people I rarely message. This yielded 240,805 messages with 288 people.</p>



<h5>2. PREPARE THE DATASET</h5>



<p>I created my training and test datasets by grouping into chunks of messages with the same person, packing as many messages as possible into each chunk without exceeding 200 tokens. This resulted in 25,087 chunks of data, with each chunk containing a prompt and about 7-10 messages.</p>



<figure><img decoding="async" width="598" height="432" data-attachment-id="257" data-permalink="https://edwarddonner.com/2024/01/02/fine-tuning-an-llm-on-240k-text-messages/image/" data-orig-file="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/image.png?fit=598%2C432&amp;ssl=1" data-orig-size="598,432" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/image.png?fit=300%2C217&amp;ssl=1" data-large-file="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/image.png?fit=598%2C432&amp;ssl=1" src="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/image.png?resize=598%2C432&amp;ssl=1" alt="" srcset="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/image.png?w=598&amp;ssl=1 598w, https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/image.png?resize=300%2C217&amp;ssl=1 300w" sizes="(max-width: 598px) 100vw, 598px" data-recalc-dims="1"/></figure>



<p>Here’s an example chunk with a more administrative interaction between me and my building’s handyman, with some details changed.</p>



<pre><code><mark>&lt;&lt;SYS&gt;&gt;</mark>Write a realistic text message chat. Avoid repetition.<mark>&lt;&lt;/SYS&gt;&gt;</mark></code></pre>



<h5>3. HYPER-PARAMETER SEARCH</h5>



<p>I started with the 7B parameter Llama 2 and fine-tuned using QLoRA on V100 VMs. I found conflicting advice all over the internets on QLoRA hyper-parameters and ended up doing a pretty extensive search. Later I upgraded to 13B parameters with a marked improvement in results.</p>



<figure><img decoding="async" width="4928" height="2528" data-attachment-id="201" data-permalink="https://edwarddonner.com/2024/01/02/fine-tuning-an-llm-on-240k-text-messages/wb-chart-1_1_2024-12_37_50-pm/" data-orig-file="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/wb-chart-1_1_2024-12_37_50-pm.png?fit=4928%2C2528&amp;ssl=1" data-orig-size="4928,2528" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="wb-chart-1_1_2024-12_37_50-pm" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/wb-chart-1_1_2024-12_37_50-pm.png?fit=300%2C154&amp;ssl=1" data-large-file="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/wb-chart-1_1_2024-12_37_50-pm.png?fit=1024%2C525&amp;ssl=1" src="https://i0.wp.com/edwarddonner.com/wp-content/uploads/2024/01/wb-chart-1_1_2024-12_37_50-pm.png?resize=4928%2C2528&amp;ssl=1" alt="" data-recalc-dims="1"/></figure>



<h5>4. BACK TO THE DATA</h5>



<p>Then I returned to step 2. I worked on better ways to format the training data and on improvements to the prompt. I was thrown off at first because the training and eval loss actually increased, but the results were absolutely improving.</p>



<h5>5. GENERATION</h5>



<p>Finally, I used Hugging Face’s <a href="https://huggingface.co/docs/transformers/v4.36.1/main_classes/text_generation">Text Generation</a> to write conversations, either playing the role of me, or one of my contacts, or acting as both sides. I experimented with penalties for repetition, token suppression, beam search and more.</p>



<h4>Initial disappointment</h4>



<p>My early attempts yielded similar results to my predecessors. The LLM would generate conversations that seemed to be surreal parodies of my life. My work colleagues would endlessly reschedule meetings earlier and later; my partner and I would relentlessly debate our lunch options. Efforts to encourage the model to be more diverse only resulted in increasingly unappetizing lunch alternatives.</p>



<h4>But then..</h4>



<p>To my enormous delight, persistence paid off, largely after working on the prompt, the input data format and the generation approach. The LLM has become highly effective at imitating me. To my surprise, it can also convincingly act as many of my friends — really anyone with whom I’ve exchanged at least 1,000 messages.</p>



<p>I’m startled by how real the conversations seem, and how they reflect the nuances of my different relationships. The model isn’t always on point – it still sometimes gets stuck in loops or goes awry – but I would say more than half of the conversations would pass a Turing Test in that I wouldn’t be able to tell you if real or generated. I’ve actually been so surprised that I’ve sometimes searched my text history in case the model is regurgitating real conversations (it’s not).</p>



<p>This is less surprising, but I think it’s really cute – for my contacts where English isn’t their first language, the model cruelly reproduces spelling and grammar mistakes and quirky expressions.</p>



<h4>Examples</h4>



<p>I’m not comfortable sharing generated conversations with actual contacts, so instead for this post I primed the LLM with messages from fictional people (in orange below) and let it imagine how the conversation might proceed. I must admit that the performance was considerably poorer; without more context to go on, the model quickly fell into its comfort zone of meeting logistics. I have ideas of how to improve this by providing more of a backstory in the prompt.</p>



<pre><code><mark>### Edward: Hi David, this is Ed. We met at yesterday&#39;s AI conference. You were interested in hearing more about our matching models and perhaps trying our product.</mark></code></pre>



<pre><code><mark>### Edward: Hey Lucy! This is Edward. It was great to bump into you yesterday - I don&#39;t think we&#39;ve spoken since university! How are you?</mark></code></pre>



<p>Here’s a more intriguing example of the model imitating me; again text in orange has been provided to the model and it supplied the responses:</p>



<pre><code><mark>### Edward: Hey</mark></code></pre>



<p>My deeper secret is nothing too outrageous, but it’s at least of the “I could tell you but then I’d have to kill you” variety, so probably best not to publish here…</p>



<h4>Not going to go there</h4>



<p>This post could take a rather macabre turn at this point. A very close friend of mine died about 10 years ago, and we have a long text message history together. You don’t need to watch Black Mirror to know where I’m heading with this. But you’ll be relieved to hear that I recognized this would be in extremely poor taste and removed the data. Enough on that topic!</p>



<p>EDIT: Hacker News reader wyldfire pointed out that there could be another angle for this kind of investigation, questioning whether “LLMs could advance to a point where it might be therapeutic to have a conversation with a deceased loved one.”</p>



<h4>What’s next</h4>



<p>I’ll be writing a series of posts on my approach so that others can experiment. I’d love to hear how you get on and help with any roadblocks. I learned so much on this journey, and very much enjoyed the experience, and I hope you will too.</p>



<p>I’m excited to try RAG and other techniques to give the model more context on its conversations. I expect another step change in quality of conversations. I look forward to the day that the model can fully replace me in replying to all my text messages..</p>



<p>I’m sure there’s tons more mileage in improving my LLM. I’m going to try other base models, further refinement of the prompt and input data, and more work on the generation. Most importantly, it goes without saying: with every day that goes by, my training dataset grows a little larger!</p>




</div></div>
  </body>
</html>
