<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://praeclarum.org/2023/05/19/webgpu-torch.html">Original</a>
    <h1>PyTorch for WebGPU</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><strong>TL;DR</strong> I’ve been working on a WebGPU optimized inference and autograd library called <a href="https://github.com/praeclarum/webgpu-torch">webgpu-torch</a> with an API that matches PyTorch. The goal is to run neural networks in the browser at speeds comparable to a Linux workstation. Many kernels have been implemented and its design is easily extensible. It’s <a href="https://www.npmjs.com/package/webgpu-torch">available on NPM now</a> and works in both the browser and Node.js!</p>

<h2 id="neural-networks-in-the-browser">Neural Networks in the Browser</h2>

<p><a href="https://github.com/praeclarum/transformers-js">Nine months ago</a>, I got Hugging Face Transformers (Large Language Models like GPT but a wee bit smaller) working in the browsers thanks to the ONNX web runtime and some painfully hand-coded tokenizers.</p>

<p>It’s quite liberating running these nets in the browser since the web is the best software distribution platform ever created. You can just send someone a link and they can run your code. No need to install anything. No need to worry about what OS they’re running. No need to worry about what hardware they have. It’s all just there.</p>

<p>The only problem is that ONNX is a wee bit, shall we say, slow.</p>

<p>Thankfully, WebGPU has arrived in browsers and we can now properly access the GPU to write optimized kernels for neural network operations. This is a huge deal. It means we can now run neural networks in the browser at speeds comparable to NVIDIA/CUDA.</p>

<p>Someone just needs to, you know, do the hard work of implementing all those operations for the GPU.</p>

<p>Well that’s what I’m very pleased to announce I’ve been working on for the past few months. I’ve been re-implementing PyTorch in TypeScript for WebGPU.</p>

<h2 id="what-is-a-pytorch">What is a PyTorch?</h2>

<p>PyTorch is a wrapper over the torch runtime (which I first used with Lua) for performing neural network operations. It’s a very popular library for doing AI work and seems to have won the arms race for now.</p>

<p>The library is broken up into parts:</p>

<ol>
  <li>
    <p>An optimized (for GPU) math library supporting element-wise operations, matrix multiplication, convolutions, reductions, etc. over tensors.</p>
  </li>
  <li>
    <p>An automatic differentiation library (autograd) that is just a lot of bookkeeping to keep track of the operations performed on tensors so that gradients can be calculated.</p>
  </li>
  <li>
    <p>A neural network library that is just a bunch of layers that can be composed together to form a neural network.</p>
  </li>
</ol>

<p>Doesn’t sound so hard to re-implement right? And so I did.</p>

<h2 id="what-is-a-webgpu">What is a WebGPU?</h2>

<p>WebGPU is the new standard for accessing GPUs from the browser. It supports generic compute shaders and is designed to be a low level API that can be used to build higher level libraries. The compute shaders are able to break work up into a 3D grid and, so long as you can reformulate your code to take advantage of that 3D grid, you can benefit from dedicated hardware doing the computations.</p>

<p>This is perfect for the web since JavaScript is single-threaded and not optimized for doing heavy computation. The GPU is a perfect fit for this since it’s designed to do heavy computation in parallel.</p>

<h2 id="writing-optimized-webgpu-kernels">Writing Optimized WebGPU Kernels</h2>

<p>PyTorch is very mature now and supports a huge variety of operations. It’s also very well optimized for CUDA and CUDNN (NVIDIA’s compute libraries). So how do you go about re-implementing all of those for WebGPU?</p>

<p>Well, you start with the basics. You implement the basic operations like element-wise operations, matrix multiplication, convolutions, reductions, etc. But there is a tremendous amount of similarity between these operations.</p>

<p>For example, element-wise multiplication and addition only vary by the operator used in the inner loop. The trick is to optimize the memory layout and kernels of those operations so they are fast. They need to adapt to big and small GPUs and they need to adapt to big and small workloads.</p>

<p>This is a perfect scenario to take advantage of code generation. I wrote a code generator that takes a template and generates the optimized kernels for each operation. The code generator is written in TypeScript and generates WebGPU compute shader code. This means that the generated code can be heavily optimized for the given scenario and those optimizations can be shared between operations.</p>

<p>For example, here is how I define the <code>ReLU</code> operation (from <code>op_table.ts</code>):</p>

<div><div><pre><code><span>{</span>
    <span>name</span><span>:</span> <span>&#34;</span><span>relu</span><span>&#34;</span><span>,</span>
    <span>nnName</span><span>:</span> <span>&#34;</span><span>ReLU</span><span>&#34;</span><span>,</span>
    <span>nnOp</span><span>:</span> <span>true</span><span>,</span>
    <span>type</span><span>:</span> <span>&#34;</span><span>unary</span><span>&#34;</span><span>,</span>
    <span>forward</span><span>:</span> <span>&#34;</span><span>output = max(input, 0.0)</span><span>&#34;</span><span>,</span>
    <span>backward</span><span>:</span> <span>&#34;</span><span>inputGrad = input &gt; 0.0 ? outputGrad : 0.0</span><span>&#34;</span><span>,</span>
<span>}</span>
</code></pre></div></div>

<p>In this template I define both the forward computation <code>max(input, 0.0)</code> and the backward computation <code>input &gt; 0.0 ? outputGrad : 0.0</code>. The code generator then generates the optimized kernels for both the forward and backward passes based on the size of your GPU (the size of compute workgroups) and the shape of tensors (in addition to the memory layouts of the tensors).</p>

<p>Keeping the template short and simple gives me flexibility to optimize the kernels as needed while preserving the core logic. For example, different kernels can be emitted for contiguous memory tensors vs strided memory tensors. For operations like reductions, 1D, 2D, 3D, and xD kernels can be emitted to take advantage of the 3D workgroup grid.</p>

<p>At first I designed the template system to help me save some typing, but I quickly realized its power and now I use it for all operations.</p>

<h2 id="debugging-webgpu-kernels">Debugging WebGPU Kernels</h2>

<p>Another huge benefit came from the fact that I was generating the kernels. I could generate the kernels to not only emit WebGPU code, but also JavaScript code. The core logic gets wrapped in another function that can be called from JavaScript. This means that I can run the same code in JavaScript and WebGPU and compare the results. Even better, I can debug kernels in JavaScript and then execute them on WebGPU.</p>

<p>The JavaScript CPU kernels are terribly slow, but they’re not supposed to be fast. They instead provide a convenient playground for debugging and testing kernels.</p>

<p>This also means that my WebGPU library can also run just fine in Node.js, without WebGPU, whatever. Isn’t it great when architectural decisions keep paying off?</p>

<h2 id="testing">Testing</h2>

<p>The worst part of using a new neural network library is when it doesn’t give the exact same results as previous libraries you’ve used. One of my biggest frustrations with the WebGL ONNX backend is the fact that it gives very inaccurate results compared to PyTorch. I didn’t want that. I want full fidelity. I want to make sure all my WebGPU kernels match the results of PyTorch operations.</p>

<p>To that end, I have built a test harness that first runs code snippets in PyTorch to record results, then runs the same code snippets in my library and compares the results. If they don’t match, it throws an error.</p>

<p>This has produced a silly but fun web page to go visit. If you go to <a href="https://praeclarum.org/webgpu-torch/tests/">https://praeclarum.org/webgpu-torch/tests/</a> you will see a huge set of tests running to verify all the supported operations. It’s a great way to see what operations are supported and what the results are.</p>

<h2 id="goals">Goals</h2>

<p>I like to train imaging networks and to that end my goal is to get Stable Diffusion and similar nets running under this library. Once that’s accomplished I will focus on the many Hugging Face transformer networks. I’m hoping to get all of them running in the browser at CUDA speeds.</p>

<p>I have a set of TODOs in the README of the project. If you’re interested in helping out, please take a look!</p>

  </div>
</article>

      </div>
    </div></div>
  </body>
</html>
