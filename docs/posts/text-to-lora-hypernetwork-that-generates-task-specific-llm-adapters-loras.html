<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/SakanaAI/text-to-lora">Original</a>
    <h1>Text-to-LoRA: Hypernetwork that generates task-specific LLM adapters (LoRAs)</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/SakanaAI/text-to-lora/blob/main/assets/overview_animation.gif"><img height="300px" src="https://github.com/SakanaAI/text-to-lora/raw/main/assets/overview_animation.gif" data-animated-image=""/></a>
</p>
<hr/>

<p dir="auto">Install <code>uv</code> if you don&#39;t have <code>uv</code> (see <a href="https://docs.astral.sh/uv/getting-started/installation/" rel="nofollow">https://docs.astral.sh/uv/getting-started/installation/</a>)</p>
<p dir="auto">With <code>uv</code> installed, run the following to install the dependencies.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/SakanaAI/text-to-lora.git
cd text-to-lora
# make sure you have `uv` installed
# (see https://docs.astral.sh/uv/getting-started/installation/)
uv self update
uv venv --python 3.10 --seed
uv sync
# we use the following wheel for installation
# you might have to change the wheel to be compatible with your hardware
uv pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu123torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
uv pip install src/fishfarm"><pre>git clone https://github.com/SakanaAI/text-to-lora.git
<span>cd</span> text-to-lora
<span><span>#</span> make sure you have `uv` installed</span>
<span><span>#</span> (see https://docs.astral.sh/uv/getting-started/installation/)</span>
uv self update
uv venv --python 3.10 --seed
uv sync
<span><span>#</span> we use the following wheel for installation</span>
<span><span>#</span> you might have to change the wheel to be compatible with your hardware</span>
uv pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu123torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
uv pip install src/fishfarm</pre></div>
<hr/>

<p dir="auto"><em><strong>Downloading trained T2L</strong></em></p>
<p dir="auto"><g-emoji alias="warning">‚ö†Ô∏è</g-emoji> <strong>You need to download the checkpoints before running any of the demos.</strong> <g-emoji alias="warning">‚ö†Ô∏è</g-emoji></p>
<p dir="auto"><g-emoji alias="warning">‚ö†Ô∏è</g-emoji> <strong>You need a &gt;16GB GPU to handle both models simultaneously to run any of these demos.</strong> <g-emoji alias="warning">‚ö†Ô∏è</g-emoji></p>
<div dir="auto" data-snippet-clipboard-copy-content="uv run huggingface-cli login
uv run huggingface-cli download SakanaAI/text-to-lora --local-dir . --include &#34;trained_t2l/*&#34;"><pre>uv run huggingface-cli login
uv run huggingface-cli download SakanaAI/text-to-lora --local-dir <span>.</span> --include <span><span>&#34;</span>trained_t2l/*<span>&#34;</span></span></pre></div>
<p dir="auto"><em><strong>Web UI</strong></em></p>
<p dir="auto">This demo runs <code>Mistral-7B-Instruct-v0.2</code> locally alongside a T2L model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# start webui locally
uv run python webui/app.py"><pre><span><span>#</span> start webui locally</span>
uv run python webui/app.py</pre></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/SakanaAI/text-to-lora/blob/main/assets/text-to-lora-demo.gif"><img height="500px" src="https://github.com/SakanaAI/text-to-lora/raw/main/assets/text-to-lora-demo.gif" data-animated-image=""/></a>
</p>
<p dir="auto"><em><strong>LoRA generation from CLI</strong></em></p>
<p dir="auto">This script allows us to generate a LoRA based on a task description. Running for the first time would take longer as the base model will be downloaded and cached.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# uv run generate_lora.py {T2l_DIRECTORY} {TASK_DESCRIPTION}
# e.g.,
uv run python scripts/generate_lora.py \
trained_t2l/llama_8b_t2l \
&#34;This task challenges your problem-solving abilities through mathematical reasoning. You must carefully read each scenario and systematically work through the data to compute the final outcome.&#34;

# You might be able to run T2L w/ `gemma-2-2b-it` with a smaller GPU.
uv run python scripts/generate_lora.py \
trained_t2l/gemma_2b_t2l \
&#34;This task challenges your problem-solving abilities through mathematical reasoning. You must carefully read each scenario and systematically work through the data to compute the final outcome.&#34;"><pre><span><span>#</span> uv run generate_lora.py {T2l_DIRECTORY} {TASK_DESCRIPTION}</span>
<span><span>#</span> e.g.,</span>
uv run python scripts/generate_lora.py \
trained_t2l/llama_8b_t2l \
<span><span>&#34;</span>This task challenges your problem-solving abilities through mathematical reasoning. You must carefully read each scenario and systematically work through the data to compute the final outcome.<span>&#34;</span></span>

<span><span>#</span> You might be able to run T2L w/ `gemma-2-2b-it` with a smaller GPU.</span>
uv run python scripts/generate_lora.py \
trained_t2l/gemma_2b_t2l \
<span><span>&#34;</span>This task challenges your problem-solving abilities through mathematical reasoning. You must carefully read each scenario and systematically work through the data to compute the final outcome.<span>&#34;</span></span></pre></div>
<p dir="auto"><em><strong>Evaluating generated LoRA</strong></em></p>
<p dir="auto">We can evaluate the generated LoRA by using the path printed at the end of the above script.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# uv run python scripts/run_eval.py --model-dir {base_model_dir} \
# --lora-dirs {lora_dirs} --save-results --tasks {tasks}
# e.g.,
uv run python scripts/run_eval.py \
--model-dir meta-llama/Llama-3.1-8B-Instruct \
--lora-dirs {PATH_TO_GENERATED_LORA} \
--save-results --tasks gsm8k

# You might be able to run T2L w/ `gemma-2-2b-it` with a smaller GPU.
uv run python scripts/run_eval.py \
--model-dir google/gemma-2-2b-it \
--lora-dirs {PATH_TO_GENERATED_LORA} \
--save-results --tasks gsm8k"><pre><span><span>#</span> uv run python scripts/run_eval.py --model-dir {base_model_dir} \</span>
<span><span>#</span> --lora-dirs {lora_dirs} --save-results --tasks {tasks}</span>
<span><span>#</span> e.g.,</span>
uv run python scripts/run_eval.py \
--model-dir meta-llama/Llama-3.1-8B-Instruct \
--lora-dirs {PATH_TO_GENERATED_LORA} \
--save-results --tasks gsm8k

<span><span>#</span> You might be able to run T2L w/ `gemma-2-2b-it` with a smaller GPU.</span>
uv run python scripts/run_eval.py \
--model-dir google/gemma-2-2b-it \
--lora-dirs {PATH_TO_GENERATED_LORA} \
--save-results --tasks gsm8k</pre></div>
<blockquote>
<p dir="auto"><em><strong>Disclaimer ‚ùï</strong></em></p>
<p dir="auto">Even with random descriptions, SFT-trained T2L still generates reasonable LoRAs.
This result with SFT-trained T2L differs from Table 5 in the paper that uses reconstruction-trained T2L.
Nonetheless, comparing the results obtained between aligned and unaligned descriptions, aligned LoRAs perform noticeably better overall. The training descriptions can be found in <code>tasks/</code> folder, while the unseen ones used for evaluation can be found in <code>configs/hyper_lora_decontam_lol_tasks.yaml</code></p>
</blockquote>
<hr/>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/SakanaAI/text-to-lora/blob/main/assets/hyperlora_fig-1_medium.jpg"><img height="500px" src="https://github.com/SakanaAI/text-to-lora/raw/main/assets/hyperlora_fig-1_medium.jpg"/></a>
</p>

For asynchronous validation evaluation, we need a separate evaluator script.
The `watcher.py` checks for new checkpoints and evaluates them as they get saved.
The script also keeps track of which one is the best checkpoint so far.
<div dir="auto" data-snippet-clipboard-copy-content="# start a watcher process for async eval
uv run watcher.py"><pre><span><span>#</span> start a watcher process for async eval</span>
uv run watcher.py</pre></div>
<p dir="auto">Then run one of the following scripts for each GPU you have.
Each takes around 5 days on a single H100 GPU.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# T2L training
./scripts/train_t2l_mistral.sh
./scripts/train_t2l_llama.sh
./scripts/train_t2l_gemma.sh"><pre><span><span>#</span> T2L training</span>
./scripts/train_t2l_mistral.sh
./scripts/train_t2l_llama.sh
./scripts/train_t2l_gemma.sh</pre></div>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üèóÔ∏è Reconstruction Training</h2><a id="user-content-Ô∏è-reconstruction-training" aria-label="Permalink: üèóÔ∏è Reconstruction Training" href="#Ô∏è-reconstruction-training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
First, we need to train the &#34;oracle&#34; adapters for all tasks.
This script trains hundreds of LoRAs and might run for many hours.
<div dir="auto" data-snippet-clipboard-copy-content="./scripts/train_lora_baselines.sh"><pre>./scripts/train_lora_baselines.sh</pre></div>
<p dir="auto">Then, we train T2L to reconstruct the oracle adapters.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# train T2L via reconstruction training
WANDB_MODE=disabled uv run python scripts/train_hyper_recon.py configs/hyper_lora_decontam_lol_tasks.yaml \
--model_dir=mistralai/Mistral-7B-Instruct-v0.2/ \
--emb_model=Alibaba-NLP/gte-large-en-v1.5 \
--warmup_frac=0.1 --lr=1e-3 --epochs=10000 \
--n_train_ds=479 --exp_setup=hyper_lora --encoder_type=linear \
--pred_z_score=True --n_descs_per_ds=128 --n_embs_per_sampled_task=1 \
--n_tasks_per_batch=4 --factorized=False --delta_w_scaling=10000 --shared_AB_head=True"><pre><span><span>#</span> train T2L via reconstruction training</span>
WANDB_MODE=disabled uv run python scripts/train_hyper_recon.py configs/hyper_lora_decontam_lol_tasks.yaml \
--model_dir=mistralai/Mistral-7B-Instruct-v0.2/ \
--emb_model=Alibaba-NLP/gte-large-en-v1.5 \
--warmup_frac=0.1 --lr=1e-3 --epochs=10000 \
--n_train_ds=479 --exp_setup=hyper_lora --encoder_type=linear \
--pred_z_score=True --n_descs_per_ds=128 --n_embs_per_sampled_task=1 \
--n_tasks_per_batch=4 --factorized=False --delta_w_scaling=10000 --shared_AB_head=True</pre></div>
<hr/>

<p dir="auto">Base model</p>
<div dir="auto" data-snippet-clipboard-copy-content="./scripts/eval_base_models.sh"><pre>./scripts/eval_base_models.sh</pre></div>
<p dir="auto">T2L</p>
<div dir="auto" data-snippet-clipboard-copy-content="# example for T2L trained for gemma-2-2b-it
WANDB_MODE=disabled uv run python scripts/eval_hypermod_checkpoint.py --checkpoint_path trained_t2l/gemma_2b_t2l/hypermod.pt --full_eval --use-icl"><pre><span><span>#</span> example for T2L trained for gemma-2-2b-it</span>
WANDB_MODE=disabled uv run python scripts/eval_hypermod_checkpoint.py --checkpoint_path trained_t2l/gemma_2b_t2l/hypermod.pt --full_eval --use-icl</pre></div>
<p dir="auto"><code>--use-icl</code> includes 3-shot in-context examples into evaluation queries.</p>
<hr/>


<blockquote>
<p dir="auto"><em><strong>Disclaimer ‚ùï</strong></em></p>
<p dir="auto">We have re-trained a new set of baselines and T2L models due to a small mismatch
between the specific package version combinations used in the original submission.</p>
<p dir="auto">Furthermore, vLLM is inherently non-deterministic when applying LoRA (in the version-configuration combinations we tested).
Thus, we have observed some small variance between evaluation runs even with a fixed initial seed.</p>
<p dir="auto">However, after re-retraining and collecting new results, T2L still consistently outperforms baselines across model families, confirming the validity of our original empirical analysis. We present this new set of results with the updated package versions in the following tables.
(<code>eval #1</code> and <code>eval #2</code> indicate that the results are gathered from two different evaluation calls with the same random seed.)</p>
</blockquote>
<p dir="auto"><em><strong>Mistral-7B-Instruct-v0.2</strong></em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>ArcC (acc)</th>
<th>ArcE (acc)</th>
<th>BoolQ (acc)</th>
<th>GSM8K (acc)</th>
<th>HS (acc)</th>
<th>OQA (acc)</th>
<th>PIQA (acc)</th>
<th>WG (acc)</th>
<th>HE (pass@1)</th>
<th>MBPP (pass@1)</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mistral-7B</td>
<td>65.79</td>
<td>77.74</td>
<td>71.56</td>
<td>41.02</td>
<td>49.64</td>
<td>54.20</td>
<td>72.96</td>
<td>45.07</td>
<td>39.02</td>
<td>42.61</td>
<td>55.96</td>
</tr>
<tr>
<td>Mistral-7B w/ ICL</td>
<td>72.01</td>
<td>86.03</td>
<td>71.80</td>
<td>41.02</td>
<td>59.15</td>
<td>65.60</td>
<td>76.33</td>
<td>58.09</td>
<td>39.02</td>
<td>41.35</td>
<td>61.04</td>
</tr>
<tr>
<td>Mistral-7B w/ task desc</td>
<td>61.86</td>
<td>77.53</td>
<td>71.50</td>
<td>41.02</td>
<td>45.08</td>
<td>56.20</td>
<td>69.75</td>
<td>47.59</td>
<td>38.41</td>
<td>39.60</td>
<td>54.85</td>
</tr>
<tr>
<td>MT LoRA (eval #1)</td>
<td>76.54</td>
<td><strong>89.27</strong></td>
<td><strong>85.23</strong></td>
<td><strong>46.47</strong></td>
<td>67.11</td>
<td>72.40</td>
<td><strong>82.81</strong></td>
<td>62.51</td>
<td>39.02</td>
<td>45.36</td>
<td>66.67</td>
</tr>
<tr>
<td>MT LoRA (eval #2)</td>
<td>76.54</td>
<td><strong>89.27</strong></td>
<td><strong>85.23</strong></td>
<td>45.64</td>
<td>67.10</td>
<td>72.40</td>
<td><strong>82.81</strong></td>
<td>62.51</td>
<td><strong>39.63</strong></td>
<td>46.12</td>
<td>66.73</td>
</tr>
<tr>
<td>Hyperdecoders (eval #1)</td>
<td>76.62</td>
<td>88.38</td>
<td>84.34</td>
<td>46.10</td>
<td><strong>67.25</strong></td>
<td>72.60</td>
<td>82.48</td>
<td>62.83</td>
<td>35.37</td>
<td><strong>52.88</strong></td>
<td>66.89</td>
</tr>
<tr>
<td>Hyperdecoders (eval #2)</td>
<td>77.05</td>
<td>88.38</td>
<td>84.46</td>
<td>45.49</td>
<td>66.73</td>
<td>73.20</td>
<td>81.94</td>
<td>62.35</td>
<td>35.98</td>
<td>52.38</td>
<td>66.80</td>
</tr>
<tr>
<td>T2L (eval #1)</td>
<td><strong>77.42</strong></td>
<td>89.20</td>
<td>84.62</td>
<td>44.02</td>
<td>67.08</td>
<td><strong>75.07</strong></td>
<td>82.32</td>
<td><strong>63.14</strong></td>
<td>38.62</td>
<td>48.71</td>
<td>67.02</td>
</tr>
<tr>
<td>T2L (eval #2)</td>
<td><strong>77.42</strong></td>
<td>89.20</td>
<td>84.62</td>
<td>44.20</td>
<td>67.07</td>
<td><strong>75.07</strong></td>
<td>82.32</td>
<td><strong>63.14</strong></td>
<td>38.62</td>
<td>48.87</td>
<td><strong>67.05</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em><strong>Llama-3.1-8B-Instruct</strong></em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>ArcC (acc)</th>
<th>ArcE (acc)</th>
<th>BoolQ (acc)</th>
<th>GSM8K (acc)</th>
<th>HS (acc)</th>
<th>OQA (acc)</th>
<th>PIQA (acc)</th>
<th>WG (acc)</th>
<th>HE (pass@1)</th>
<th>MBPP (pass@1)</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-8B</td>
<td>73.29</td>
<td>90.53</td>
<td>80.40</td>
<td>75.36</td>
<td>66.53</td>
<td>75.40</td>
<td>79.60</td>
<td>55.01</td>
<td>65.24</td>
<td>68.92</td>
<td>73.03</td>
</tr>
<tr>
<td>Llama-8B w/ ICL</td>
<td>80.80</td>
<td>91.96</td>
<td>79.88</td>
<td>75.36</td>
<td>59.19</td>
<td>78.00</td>
<td>80.63</td>
<td><strong>61.64</strong></td>
<td>65.24</td>
<td>69.17</td>
<td>74.19</td>
</tr>
<tr>
<td>Llama-8B w/ task desc</td>
<td>72.78</td>
<td>90.95</td>
<td>76.57</td>
<td>75.36</td>
<td>63.21</td>
<td>73.00</td>
<td>80.52</td>
<td>56.20</td>
<td><strong>67.07</strong></td>
<td><strong>70.93</strong></td>
<td>72.66</td>
</tr>
<tr>
<td>MT LoRA (eval #1)</td>
<td>82.42</td>
<td><strong>93.18</strong></td>
<td>84.40</td>
<td>76.35</td>
<td>73.88</td>
<td>81.80</td>
<td>81.94</td>
<td>60.38</td>
<td>62.20</td>
<td>69.42</td>
<td>76.60</td>
</tr>
<tr>
<td>MT LoRA (eval #2)</td>
<td>82.42</td>
<td><strong>93.18</strong></td>
<td><strong>84.43</strong></td>
<td>76.88</td>
<td><strong>73.89</strong></td>
<td>81.80</td>
<td>81.94</td>
<td>60.30</td>
<td>62.20</td>
<td>69.67</td>
<td>76.67</td>
</tr>
<tr>
<td>T2L (eval #1)</td>
<td><strong>82.82</strong></td>
<td>93.04</td>
<td>84.23</td>
<td><strong>77.05</strong></td>
<td>73.84</td>
<td>82.07</td>
<td><strong>82.12</strong></td>
<td>61.19</td>
<td>64.63</td>
<td><strong>70.93</strong></td>
<td><strong>77.19</strong></td>
</tr>
<tr>
<td>T2L (eval #2)</td>
<td>82.65</td>
<td>93.15</td>
<td>84.34</td>
<td>76.98</td>
<td>73.88</td>
<td><strong>82.13</strong></td>
<td>82.06</td>
<td>61.38</td>
<td>64.23</td>
<td>70.09</td>
<td>77.09</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em><strong>Gemma-2-2b-it</strong></em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>ArcC (acc)</th>
<th>ArcE (acc)</th>
<th>BoolQ (acc)</th>
<th>GSM8K (acc)</th>
<th>HS (acc)</th>
<th>OQA (acc)</th>
<th>PIQA (acc)</th>
<th>WG (acc)</th>
<th>HE (pass@1)</th>
<th>MBPP (pass@1)</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td>gemma-2-2b-it</td>
<td>73.63</td>
<td>89.86</td>
<td>80.98</td>
<td>55.27</td>
<td>54.99</td>
<td>70.80</td>
<td>70.84</td>
<td>53.83</td>
<td><strong>43.90</strong></td>
<td>13.53</td>
<td>60.76</td>
</tr>
<tr>
<td>gemma + ICL</td>
<td>72.10</td>
<td>88.80</td>
<td><strong>82.29</strong></td>
<td>55.27</td>
<td>55.59</td>
<td>72.20</td>
<td>67.68</td>
<td>53.35</td>
<td><strong>43.90</strong></td>
<td>43.36</td>
<td>63.45</td>
</tr>
<tr>
<td>gemma + ICL + task desc</td>
<td>72.10</td>
<td>88.80</td>
<td><strong>82.29</strong></td>
<td>55.27</td>
<td>55.59</td>
<td>72.20</td>
<td>67.68</td>
<td>53.35</td>
<td><strong>43.90</strong></td>
<td>43.36</td>
<td>63.45</td>
</tr>
<tr>
<td>mt lora + ICL (#eval1)</td>
<td>74.06</td>
<td>88.80</td>
<td>81.01</td>
<td>56.41</td>
<td>60.09</td>
<td><strong>74.40</strong></td>
<td>68.17</td>
<td><strong>58.01</strong></td>
<td>39.63</td>
<td>50.13</td>
<td>65.07</td>
</tr>
<tr>
<td>mt lora + ICL (#eval2)</td>
<td>74.06</td>
<td>88.80</td>
<td>81.04</td>
<td>56.56</td>
<td>60.10</td>
<td><strong>74.40</strong></td>
<td>68.17</td>
<td><strong>58.01</strong></td>
<td>39.63</td>
<td>49.87</td>
<td>65.06</td>
</tr>
<tr>
<td>T2L + ICL (#eval1)</td>
<td><strong>74.12</strong></td>
<td><strong>90.03</strong></td>
<td>82.27</td>
<td>56.61</td>
<td><strong>62.87</strong></td>
<td>74.33</td>
<td><strong>73.61</strong></td>
<td>57.51</td>
<td>39.02</td>
<td><strong>50.54</strong></td>
<td>66.09</td>
</tr>
<tr>
<td>T2L + ICL (#eval2)</td>
<td><strong>74.12</strong></td>
<td><strong>90.03</strong></td>
<td>82.27</td>
<td><strong>56.89</strong></td>
<td>62.86</td>
<td>74.33</td>
<td><strong>73.61</strong></td>
<td>57.51</td>
<td>39.02</td>
<td><strong>50.54</strong></td>
<td><strong>66.12</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h3 tabindex="-1" dir="auto">üì° Huggingface datasets connection</h3><a id="user-content--huggingface-datasets-connection" aria-label="Permalink: üì° Huggingface datasets connection" href="#-huggingface-datasets-connection"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Huggingface datasets server might reject connections sometimes, which can be problematic if you haven&#39;t locally cached the relevant datasets.
The cause could be that we&#39;re downloading a lot of datasets (~500 datasets).
If this happens, keep retrying the SFT training script until all datasets are cached locally.
Once the datasets are cached, you should be able to run the SFT training script.</p>
<hr/>

<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{
    charakorn2025texttolora,
    title={Text-to-Lo{RA}: Instant Transformer Adaption},
    author={Rujikorn Charakorn and Edoardo Cetin and Yujin Tang and Robert Tjarko Lange},
    booktitle={Forty-second International Conference on Machine Learning},
    year={2025},
    url={https://openreview.net/forum?id=zWskCdu3QA}
}"><pre><span>@inproceedings</span>{
    charakorn2025texttolora,
    <span>title</span>=<span><span>{</span>Text-to-Lo{RA}: Instant Transformer Adaption<span>}</span></span>,
    <span>author</span>=<span><span>{</span>Rujikorn Charakorn and Edoardo Cetin and Yujin Tang and Robert Tjarko Lange<span>}</span></span>,
    <span>booktitle</span>=<span><span>{</span>Forty-second International Conference on Machine Learning<span>}</span></span>,
    <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
    <span>url</span>=<span><span>{</span>https://openreview.net/forum?id=zWskCdu3QA<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
