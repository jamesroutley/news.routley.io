<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/">Original</a>
    <h1>Matrix Multiplication Inches Closer To Mythic Goal</h1>
    
    <div id="readability-page-1" class="page"><div data-reactid="243"><div data-reactid="244"><p><img alt="A graphic showing improvements in the speed of matrix multiplication" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2021/03/Matrix_multiplication_2880x1620_Lede.jpg" data-reactid="246"/></p></div><figcaption data-reactid="248"><section data-reactid="249"><div data-reactid="250"><p>Mathematicians have been getting closer to their goal of reaching exponent two — meaning multiplying a pair of <em>n</em>-by-<em>n</em> matrices in only <em>n</em><sup>2</sup> steps — but will they ever reach it?</p></div></section></figcaption></div><div data-reactid="394"><div data-reactid="395"><p><img alt="A graphic showing some of the steps involved in multiplying matrices" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2021/03/Matrix-graphic-1.svg" data-reactid="397"/></p></div><figcaption data-reactid="399"><section data-reactid="400"><div data-reactid="401"><!-- react-text: 402 --><!-- /react-text --><p>Samuel Velasco/Quanta Magazine</p></div></section></figcaption></div><div data-reactid="404"><section data-reactid="405"><div data-reactid="406"><div data-reactid="407"><div data-reactid="408"><p>This operation is known as taking the “inner product” of a row with a column. To compute the other entries in the product matrix, repeat the procedure with the corresponding rows and columns.</p>
<p>Altogether, the textbook method for multiplying two-by-two matrices requires eight multiplications, plus some additions. Generally, this way of multiplying two <em>n</em>-by-<em>n</em> matrices together requires <em>n</em><sup>3</sup> multiplications along the way.</p>
</div></div></div></section></div><div data-reactid="415"><section data-reactid="416"><div data-reactid="417"><div data-reactid="418"><div data-reactid="419"><p>As matrices grow larger, the number of multiplications needed to find their product increases much faster than the number of additions. While it takes eight intermediate multiplications to find the product of two-by-two matrices, it takes 64 to find the product of four-by-four matrices. However, the number of additions required to add those sets of matrices is much closer together. Generally, the number of additions is equal to the number of entries in the matrix, so four for the two-by-two matrices and 16 for the four-by-four matrices. This difference between addition and multiplication begins to get at why researchers measure the speed of matrix multiplication purely in terms of the number of multiplications required.</p>
<p>“Multiplications are everything,” said Umans. “The exponent on the eventual running time is fully dependent only on the number of multiplications. The additions sort of disappear.”</p>
<p>For centuries people thought <em>n</em><sup>3</sup> was simply the fastest that matrix multiplication could be done. In 1969, Volker Strassen reportedly set out to prove that there was no way to multiply two-by-two matrices using fewer than eight multiplications. Apparently he couldn’t find the proof, and after a while he realized why: There’s actually a way to do it with seven!</p>
<p>Strassen came up with a complicated set of relationships that made it possible to replace one of those eight multiplications with 14 extra additions. That might not seem like much of a difference, but it pays off thanks to the importance of multiplication over addition. And by finding a way to save a single multiplication for small two-by-two matrices, Strassen found an opening that he could exploit when multiplying larger matrices.</p>
<p>“This tiny improvement leads to huge improvements with big matrices,” Vassilevska Williams said.</p>
</div></div></div></section></div><div data-reactid="420"><div data-reactid="421"><p><img alt="A color photo Virginia Williams in a red shirt in front of a whiteboard, and a color photo of Josh Alman in a gray shirt with a gray background" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2021/03/Williams-color-corrected_v2.jpg" data-reactid="423"/></p><p><img alt="A color photo Virginia Williams in a red shirt in front of a whiteboard, and a color photo of Josh Alman in a gray shirt with a gray background" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2021/03/Alman_v1.jpg" data-reactid="426"/></p></div><figcaption data-reactid="428"><section data-reactid="429"><div data-reactid="430"><p>Virginia Vassilevska Williams of the Massachusetts Institute of Technology and Josh Alman of Harvard University discovered the fastest-ever way to multiply two matrices, clocking in at <em>n</em><sup>2.3728596 </sup>steps.</p><p>Jared Charney; Richard TK Hawke</p></div></section></figcaption></div><div data-reactid="433"><section data-reactid="434"><div data-reactid="435"><div data-reactid="436"><div data-reactid="437"><p>Say, for example, you want to multiply a pair of eight-by-eight matrices. One way to do it is to break each big matrix into four four-by-four matrices, so each one has four entries. Because a matrix can also have entries that are themselves matrices, you can thus think of the original matrices as a pair of two-by-two matrices, each of whose four entries is itself a four-by-four matrix. Through some manipulation, these four-by-four matrices can themselves each be broken into four two-by-two matrices.</p>
<p>The point of this reduction — of repeatedly breaking bigger matrices into smaller matrices — is that it’s possible to apply Strassen’s algorithm over and over again to the smaller matrices, reaping the savings of his method at each step. Altogether, Strassen’s algorithm improved the speed of matrix multiplication from <em>n</em><sup>3</sup> to <em>n</em><sup>2.81</sup> multiplicative steps.</p>
<p>The next big improvement took place in the late 1970s, with a fundamentally new way to approach the problem. It involves translating matrix multiplication into a different computational problem in linear algebra involving objects called tensors. The particular tensors used in this problem are three-dimensional arrays of numbers composed of many different parts, each of which looks like a small matrix multiplication problem.</p>
<p>Matrix multiplication and this problem involving tensors are equivalent to each other in a sense, yet researchers already had faster procedures for solving the latter one. This left them with the task of determining the exchange rate between the two: How big are the matrices you can multiply for the same computational cost that it takes to solve the tensor problem?</p>
<p>“This is a very common paradigm in theoretical computer science, of reducing between problems to show they’re as easy or as hard as each other,” Alman said.</p>
<p>In 1981 Arnold Schönhage used this approach to prove that it’s possible to perform matrix multiplication in <em>n</em><sup>2.522</sup> steps. Strassen later called this approach the “laser method.”</p>
<p>Over the last few decades, every improvement in matrix multiplication has come from improvements in the laser method, as researchers have found increasingly efficient ways to translate between the two problems. In their new proof, Alman and Vassilevska Williams reduce the friction between the two problems and show that it’s possible to “buy” more matrix multiplication than previously realized for solving a tensor problem of a given size.</p>
<p>“Josh and Virginia have basically found a way of taking the machinery inside the laser method and tweaking it to get even better results,” said <a href="https://www.microsoft.com/en-us/research/people/cohn/">Henry Cohn</a> of Microsoft Research.</p>
<p>The paper improves the theoretical speed limit on matrix multiplication to <em>n</em><sup>2.3728596</sup>.</p>
<p>It also allows Vassilevska Williams to regain the matrix multiplication crown, which she previously held in 2012 (<em>n</em><sup>2.372873</sup>), then <a href="https://arxiv.org/abs/1401.7714">lost in 2014</a> to François Le Gall (<em>n</em><sup>2.3728639</sup>).</p>

<p>Yet for all the jockeying, it’s also clear this approach is providing diminishing returns. In fact, Alman and Vassilevska Williams’s improvement may have nearly maxed out the laser method — remaining far short of that ultimate theoretical goal.</p>
<p>“There isn’t likely to be a leap to exponent two using this particular family of methods,” Umans said.</p>
<p>Getting there will require discovering new methods and sustaining the belief that it’s even possible.</p>
<p>Vassilevska Williams remembers a conversation she once had with Strassen about this: “I asked him if he thinks you can get [exponent 2] for matrix multiplication and he said, ‘no, no, no, no, no.’”</p>
</div></div></div></section></div></div>
  </body>
</html>
