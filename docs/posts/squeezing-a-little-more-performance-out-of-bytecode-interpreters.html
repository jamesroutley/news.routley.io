<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stefan-marr.de/2023/06/squeezing-a-little-more-performance-out-of-bytecode-interpreters/">Original</a>
    <h1>Squeezing a Little More Performance Out of Bytecode Interpreters</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
  

<p>Earlier this year, Wanhong Huang, <a href="https://tugawa.github.io/index-e.html">Tomoharu Ugawa</a>, and myself published some new experiments
on interpreter performance.
We experimented with a
<a href="https://en.wikipedia.org/wiki/Genetic_algorithm">Genetic Algorithm</a>
to squeeze a little more performance out of bytecode interpreters.
Since I spent much of my research time looking for ways to improve <a href="https://stefan-marr.de/tag/interpreters/">interpreter
performance</a>, I was quite intrigued by the basic question behind Wanhong’s experiments:
<em>which is the best order of bytecode handlers in the interpreter loop?</em></p>

<h2 id="the-basics-bytecode-loops-and-modern-processors">The Basics: Bytecode Loops and Modern Processors</h2>

<p>Let’s start with a bit of background.
Many of today’s widely used interpreters use <a href="https://en.wikipedia.org/wiki/Bytecode">bytecodes</a>,
which represent a program as operations quite similar to processor instructions.
Though, depending on the language we are trying to support in our interpreter,
the <em>bytecodes</em> can be arbitrarily complex, in terms of how they encode arguments,
but also in terms of the behavior they implement.</p>

<p>In the simplest case, we would end up with an interpreter loop that looks
roughly like this:</p>

<figure><pre><code data-lang="c"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td><pre><span>uint8_t</span> <span>bytecode</span> <span>=</span> <span>{</span> <span>push_local</span><span>,</span> <span>push_local</span><span>,</span> <span>add</span><span>,</span> <span>/* ... */</span> <span>}</span>
<span>while</span> <span>(</span><span>true</span><span>)</span> <span>{</span>
  <span>uint8_t</span> <span>bytecode</span> <span>=</span> <span>bytecodes</span><span>[</span><span>index</span><span>];</span>
  <span>index</span> <span>+=</span> <span>/* ... */</span><span>;</span>
  <span>switch</span> <span>(</span><span>bytecode</span><span>)</span> <span>{</span>
    <span>case</span> <span>push_local</span><span>:</span>
      <span>// ...</span>
    <span>case</span> <span>add</span><span>:</span>
      <span>// ...</span>
    <span>case</span> <span>call_method</span><span>:</span>
      <span>// ...</span>
  <span>}</span>
<span>}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<figcaption>Listing 1: <code>switch</code>/<code>case</code> Interpreter Loop.</figcaption>

<p>Here, <code>push_local</code> and <code>add</code> are much simpler than any <code>call_method</code> bytecode.
Depending on the language that we try to implement, <code>push_local</code> is likely just a few processor instructions, while <code>call_method</code> might be significantly more complex, because it may need to lookup the method, ensure that arguments are passed correctly, and ensure that we have memory for local variables for the method that is to be executed.
Since bytecodes can be arbitrarily complex,
<a href="https://repositum.tuwien.at/handle/20.500.12708/11220">S. Brunthaler</a>
distinguished between <em>high abstraction-level</em> interpreters
and <em>low abstraction-level</em> ones.
High abstraction-level interpreters
do not spend a lot of time on the bytecode dispatch, but low abstraction-level
ones do, because their bytecodes are comparably simple, and have often just a handful of processor instructions. Thus, low abstraction-level interpreters
would benefit most from optimizing the bytecode dispatch.</p>

<p>A classic optimization of the bytecode dispatch is <a href="https://dl.acm.org/doi/10.1145/362248.362270">threaded code
interpretation</a>,
in which we represent a program not only using bytecodes,
but with an additional array of jump addresses. This optimization is also often called
<em>direct threaded code</em>. It is particularly beneficial for low abstraction-level
interpreters but applied more widely.</p>

<figure><pre><code data-lang="c"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td><pre><span>uint8_t</span> <span>bytecode</span> <span>=</span> <span>{</span> <span>push_local</span><span>,</span> <span>push_local</span><span>,</span> <span>add</span><span>,</span> <span>/* ... */</span> <span>}</span>
<span>void</span><span>*</span> <span>targets</span> <span>=</span> <span>{</span> <span>&amp;&amp;</span><span>push_local</span><span>,</span> <span>&amp;&amp;</span><span>push_local</span><span>,</span> <span>&amp;</span><span>add</span><span>,</span> <span>/* ... */</span> <span>}</span>

<span>push_local</span><span>:</span>
  <span>// ...</span>
  <span>void</span><span>*</span> <span>target</span> <span>=</span> <span>targets</span><span>[</span><span>index</span><span>];</span>
  <span>index</span> <span>+=</span> <span>/* ... */</span>
  <span>goto</span> <span>*</span><span>target</span><span>;</span>

<span>add</span><span>:</span>
  <span>// ...</span>
  <span>void</span><span>*</span> <span>target</span> <span>=</span> <span>targets</span><span>[</span><span>index</span><span>];</span>
  <span>index</span> <span>+=</span> <span>/* ... */</span>
  <span>goto</span> <span>*</span><span>target</span><span>;</span>

<span>call_method</span><span>:</span>
  <span>// ...</span>
  <span>void</span><span>*</span> <span>target</span> <span>=</span> <span>targets</span><span>[</span><span>index</span><span>];</span>
  <span>index</span> <span>+=</span> <span>/* ... */</span>
  <span>goto</span> <span>*</span><span>target</span><span>;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<figcaption>Listing 2: Directed Threaded Interpreter.</figcaption>

<p>With this interpreter optimization, we do not have the explicit <code>while</code> loop.
Instead, we have <code>goto</code> labels for each bytecode handler
and each handler has a separate copy of the dispatch code, that is the <code>goto</code> jump instruction.</p>

<p>This helps modern processors in at least two ways:</p>

<ol>
  <li>
    <p>it avoids an extra jump to the top of the loop
at the end of each bytecode,</p>
  </li>
  <li>
    <p>and perhaps more importantly,
we have multiple dispatch points instead of a single one.</p>
  </li>
</ol>

<p>This is important for branch prediction.
In our first loop, a processor would not be able to predict
where a jump is going, because the <code>switch</code> normally translates to a single jump
that goes to most, if not all bytecodes.
Though, when we have the jump at the end of a bytecode handler,
we may only see a subset of bytecodes,
which increases the chance that the processor can predict the jump correctly.</p>

<p>Unfortunately, modern processors are rather complex.
They have limits for instance for how many jump targets they can remember.
They may also end up combining the history for different jump instructions,
perhaps because they use an associative cache based on the address of the jump
instruction.
They have various different caches, including the instruction cache,
into which our interpreter loop ideally fits for best performance.
And all these things may interact in unexpected ways.</p>

<p>For me, these things make it pretty hard to predict the performance
of a bytecode loop for a complex language such as JavaScript or Ruby.</p>

<p>And if it’s too hard to understand, why not try some kind of machine learning.
And that’s indeed what Wanhong and Tomoharu came up with.</p>

<h2 id="towards-an-optimal-bytecode-handler-ordering">Towards an Optimal Bytecode Handler Ordering</h2>

<p>With all the complexity of modern processors,
Wanhong and Tomoharu observed that changing the order of bytecode handlers
can make a significant difference for the overall performance of an interpreter.
Of course, this will only make a difference if our interpreter indeed spends
a significant amount of time in the bytecode loop and the handler dispatch itself.</p>

<p>When looking at various interpreters, we will find most of them use a <em>natural order</em>.
With that I mean, the bytecode handlers are in the order of the numbers assigned to each bytecode.
Other possible orders could be a random order,
or perhaps even an order based on the frequency of bytecodes or the frequency of bytecode sequences.
Thus, one might simply first have the most frequently used bytecodes, and then less frequently used
ones, perhaps hoping that this means the most used instructions fit into caches,
or help the branch predictor in some way.</p>

<p>The goal of our experiments was to find out whether we can use a Genetic Algorithm to find a better ordering
so that we can improve interpreter performance.
We use a genetic algorithm to create new orders of bytecode handlers
by producing a crossover from two existing orderings that combine both
with a few handlers being reordered additionally, which adds mutation
into the new order.
The resulting bytecode handler order is then
compile into a new interpreter for which we measure the run time of a benchmark.
With a Genetic Algorithm, one can thus generate variations of handler orders that over multiple
generations of crossover and mutation may evolve to a faster handler order.</p>

<p>I’ll skip the details here, but please check out the <a href="#paper">paper</a> below for the specifics.</p>

<h2 id="results-on-a-javascript-interpreter">Results on a JavaScript Interpreter</h2>

<p>So, how well does this approach work?
To find out, we applied it to the <a href="http://spa.info.kochi-tech.ac.jp/~ugawa/ejs-comlan-preprint.pdf">eJSVM</a>,
a JavaScript interpreter that is designed for resource-constraint devices.</p>

<p>In the context of resource-constraint embedded devices,
it may make sense to tailor an interpreter to a specific applications
to gain best performance.
Thus we started by optimizing the interpreter for a specific benchmark
on a specific machine.
To keep the time needed for the experiments manageable,
we used three Intel machines and one Raspberry Pi with an ARM CPU.
In many ways, optimizing for a specific benchmark is the best-case scenario,
which is only practical if we
can deploy a specific application together with the interpreter.
<a href="#fig:js-results">Figure 1</a> shows the results
on benchmarks from the <a href="https://github.com/smarr/are-we-fast-yet">Are We Fast Yet benchmark suite</a>.
We can see that surprisingly large improvements.
While the results depend very much on the processor architecture,
every single benchmark sees an improvement on all platforms.</p>

<figure id="fig:js-results">
<figure id="fig:js-results:mA">
<img src="https://stefan-marr.de/assets/2023/bytecode-handler-opt/self-mA.png" width="300"/>
<figcaption>(a) Intel Core i9-12900 with GCC 10.3</figcaption>
</figure>
<figure id="fig:results:mB">
<img src="https://stefan-marr.de/assets/2023/bytecode-handler-opt/self-mB.png" width="300"/>
<figcaption>(b) Intel Core i7-11700 with GCC 9.4</figcaption>
</figure>
<figure id="fig:results:mC">
<img src="https://stefan-marr.de/assets/2023/bytecode-handler-opt/self-mC.png" width="300"/>
<figcaption>(c) Intel Xeon W-2235 with GCC 9.4</figcaption>
</figure>
<figure id="fig:results:rasp">
<img src="https://stefan-marr.de/assets/2023/bytecode-handler-opt/self-rasp.png" width="300"/>
<figcaption>(d) ARM Cortex A53 with GCC 10.2.1</figcaption>
</figure>
<figcaption><strong>Fig. 1:</strong> Speedup over the baseline interpreter after optimizing
it for the given benchmark, on the given processor.
The results suggest a strong influence for the processor architecture and benchmark.
On the Intel Xeon, we see a speedup of up to 23% on a specific benchmark and that
every single benchmark can benefit.
</figcaption>
</figure>

<p>Unfortunately, we can’t really know which programs user will
run on our interpreters for all scenarios.
Thus, we also looked at how interpreter speed improves when we train
the interpreter on a single benchmark.
<a href="#fig:cross:mC">Figure 2</a> shows how the performance changes when we train
the interpreter for a specific benchmark.
In the top left corner, we see the results when training for the Bounce
benchmark. While Bounce itself sees a 7.5% speedup, the same interpreter
speeds up the List benchmark by more than 12%.
Training the interpreter on the Permute benchmark gives how ever much less
improvements for the other benchmarks.</p>

<figure id="fig:cross:mC">
<img src="https://stefan-marr.de/assets/2023/bytecode-handler-opt/crossprog_sol-mC.png" width="650"/>
<figcaption><strong>Fig. 2:</strong> Speedup of benchmarks on interpreters
trained for a specific benchmark on the Intel Xeon W-2235 with GCC 9.4.
The gains here depend strongly on the benchmark.
While training with CD or Queens gives an average speedup of more than 10%
across all benchmarks, training on Permute only gives about 3%.
</figcaption>
</figure>

<p>In the <a href="#paper">paper</a>, we look at a few more aspects
including which Genetic Algorithm works best
and how portable performance is between architectures.</p>

<h2 id="optimizing-bytecode-handler-order-for-other-interpreters">Optimizing Bytecode Handler Order for other Interpreters</h2>

<p>Reading this blog post, you may wonder how to best go about experimenting
with your own interpreter.
We also briefly tried optimizing CRuby, however, we unfortunately did not yet
manage to find time to continue,
but we found a few things that one needs to watch out for when doing so.</p>

<p>First, you may have noticed that we used a relatively old versions of GCC.
For eJSVM, these gave good results and did not interfere with our reordering.
However, on CRuby and with newer GCCs, the compiler will start to reorder
basic blocks itself, which makes it harder to get the desired results.
Here flags such as <span><code>-fno-reorder-blocks</code></span>
or <span><code>-fno-reorder-blocks-and-partition</code></span> may be needed.
Clang didn’t seem to reorder basic blocks in the interpreter loop.
As a basic test of how big a performance impact might be,</p>

<p>I simply ran a handful of random bytecode handler orders, which I would normally would
expect to show some performance difference, likely a slowdown.
Though, for CRuby I did not see a notable performance change,
which suggests that bytecode dispatch may not be worth optimizing further.
But it’s a bit early to tell conclusively at this point.
We should give CPython and others a go, but haven’t gotten around to it just yet.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you care about interpreter performance,
maybe it’s worth to take a look at the interpreter loop
and see whether modern processors deliver better performance
when bytecode handlers get reordered.</p>

<p>Our results suggest that it can give large improvements when training for 
a specific benchmark.
There is also still a benefit for other benchmarks that we did not train
for, though, it depends on how similar the training benchmark is to the others.</p>

<p>For more details, please read the paper linked below, or reach out on Twitter <a href="https://twitter.com/smarr">@smarr</a>.</p>



<p><strong>Abstract</strong></p>

<blockquote>
  <p>Interpreter performance remains important today. Interpreters are needed in
resource constrained systems, and even in systems with just-in-time compilers,
they are crucial during warm up. A common form of interpreters is a bytecode
interpreter, where the interpreter executes bytecode instructions one by one.
Each bytecode is executed by the corresponding bytecode handler.</p>

<p>In this paper, we show that the order of the bytecode handlers in the
interpreter source code affects the execution performance of programs on the
interpreter. On the basis of this observation, we propose a genetic algorithm
(GA) approach to find an approximately optimal order. In our GA approach, we
find an order optimized for a specific benchmark program and a specific CPU.</p>

<p>We evaluated the effectiveness of our approach on various models of CPUs
including x86 processors and an ARM processor. The order found using GA
improved the execution speed of the program for which the order was optimized
between 0.8% and 23.0% with 7.7% on average. We also assess the cross-benchmark
and cross-machine performance of the GA-found order. Some orders showed good
generalizability across benchmarks, speeding up all benchmark programs.
However, the solutions do not generalize across different machines, indicating
that they are highly specific to a microarchitecture.</p>

</blockquote>

<ul>
  <li>Optimizing the Order of Bytecode Handlers in Interpreters using a Genetic Algorithm</li>

    <li>
      Paper:
        <a href="https://stefan-marr.de/downloads/acmsac23-huang-et-al-optimizing-the-order-of-bytecode-handlers-in-interpreters-using-a-genetic-algorithm.pdf">
          PDF</a>
    </li>

    <li>
        DOI: <a href="https://doi.org/10.1145/3555776.3577712">10.1145/3555776.3577712</a>
    </li>

    


    <li>
      BibTex:
      <span tabindex="0"><span>bibtex</span>
      <pre>@inproceedings{Huang:2023:GA,
  abstract = {Interpreter performance remains important today. Interpreters are needed in
  resource constrained systems, and even in systems with just-in-time compilers,
  they are crucial during warm up. A common form of interpreters is a bytecode
  interpreter, where the interpreter executes bytecode instructions one by one.
  Each bytecode is executed by the corresponding bytecode handler.
  
  In this paper, we show that the order of the bytecode handlers in the
  interpreter source code affects the execution performance of programs on the
  interpreter. On the basis of this observation, we propose a genetic algorithm
  (GA) approach to find an approximately optimal order. In our GA approach, we
  find an order optimized for a specific benchmark program and a specific CPU.
  
  We evaluated the effectiveness of our approach on various models of CPUs
  including x86 processors and an ARM processor. The order found using GA
  improved the execution speed of the program for which the order was optimized
  between 0.8% and 23.0% with 7.7% on average. We also assess the cross-benchmark
  and cross-machine performance of the GA-found order. Some orders showed good
  generalizability across benchmarks, speeding up all benchmark programs.
  However, the solutions do not generalize across different machines, indicating
  that they are highly specific to a microarchitecture.},
  author = {Huang, Wanhong and Marr, Stefan and Ugawa, Tomoharu},
  blog = {https://stefan-marr.de/2023/06/squeezing-a-little-more-performance-out-of-bytecode-interpreters/},
  booktitle = {The 38th ACM/SIGAPP Symposium on Applied Computing (SAC &#39;23)},
  doi = {10.1145/3555776.3577712},
  isbn = {978-1-4503-9517-5/23/03},
  keywords = {Bytecodes CodeLayout EmbeddedSystems GeneticAlgorithm Interpreter JavaScript MeMyPublication Optimization myown},
  month = mar,
  pages = {10},
  pdf = {https://stefan-marr.de/downloads/acmsac23-huang-et-al-optimizing-the-order-of-bytecode-handlers-in-interpreters-using-a-genetic-algorithm.pdf},
  publisher = {ACM},
  series = {SAC&#39;23},
  title = {{Optimizing the Order of Bytecode Handlers in Interpreters using a Genetic Algorithm}},
  year = {2023},
  month_numeric = {3}
}
</pre>
      </span>
    </li>
</ul>


</div></div>
  </body>
</html>
