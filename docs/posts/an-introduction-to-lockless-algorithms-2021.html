<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/Articles/844224/">Original</a>
    <h1>An introduction to lockless algorithms (2021)</h1>
    
    <div id="readability-page-1" class="page"><div>
<center>
           <div><b>Please consider subscribing to LWN</b><p>Subscriptions are the lifeblood of LWN.net.  If you appreciate this
       content and would like to see more of it, your subscription will
       help to ensure that LWN continues to thrive.  Please visit
       <a href="https://lwn.net/subscribe/">this page</a> to join up and keep LWN on
       the net.</p></div>
           </center>
           
<p>
Lockless algorithms are of interest for the Linux kernel when traditional
locking primitives either cannot be used or are not performant enough.
For this reason they come up every now and then on LWN; one of the last
mentions, which prompted me to write this article series, was <a href="https://lwn.net/Articles/827180/">last July</a>.
Topics that arise even more frequently are  read-copy-update (RCU — <a href="https://lwn.net/Articles/262464/">these
articles from 2007</a> are still highly relevant), <a href="https://lwn.net/Articles/575460/">reference counting</a>, and
ways of wrapping lockless primitives into higher-level,
more easily understood APIs.  These articles will delve into the concepts
behind lockless algorithms and how they are used in the kernel.

</p><p>

 Low-level knowledge of the memory model is universally recognized as
advanced material that can scare even the most seasoned kernel hackers; our
editor wrote (in the July article) that &#34;<q>it takes a special kind of
mind to really understand the 
memory model</q>&#34;.  It&#39;s been <a href="https://lwn.net/Articles/576642/">said</a> that the
Linux kernel memory model (and in particular </p><a href="https://elixir.bootlin.com/linux/latest/source/Documentation/memory-barriers.txt"><tt>Documentation/memory-barriers.txt</tt></a><p>)
can be used to frighten small children, and the same is probably true of
just the words &#34;acquire&#34; and &#34;release&#34;.

</p><p>

At the same time, mechanisms like RCU and <a href="https://lwn.net/Articles/22818/">seqlocks</a> are in such widespread
use in the kernel that almost every developer will sooner or later
encounter fundamentally lockless programming interfaces.  For this reason,
it is a good idea to equip yourself with at least a basic understanding
of lockless primitives.  Throughout this series I will describe
what acquire and release semantics are really about, and present
five relatively simple patterns that alone can cover most uses of the
primitives.

</p><h4>Acquire, release, and &#34;happens before&#34;</h4>

<p>
In order to make the big step from the (relative) comfort of
synchronization primitives to lockless programming, we shall first
take a look at why locks work in the first place.  Usually, this
is taught in terms of mutual exclusion: locks prevent multiple
threads of execution from reading or writing the same data
concurrently.  But what does &#34;concurrently&#34; really mean?  And
what happens when thread T is done with that data and thread U
starts using it?

</p><p>
In order to answer these questions, we can turn to a theoretical
framework that Leslie Lamport established in his 1978 paper
<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/time-clocks.pdf">&#34;Time,
Clocks and the Ordering of Events in a Distributed System&#34;</a>.
According to the paper, the events in a distributed system can
be ordered according to whether an event P <em>happens before</em>
another event Q:

</p><ul>
<li> The ordering of events is <em>total</em> within a single thread of
execution.  In layman&#39;s terms, for any two events from the same thread
you can always say which came first and which came second.

</li><li> If two events do not happen within a single thread of execution,
event P happens before event Q if event P is a message send and
event Q is the corresponding message receive.

</li><li> The relation is <em>transitive</em>.  Therefore, if event P happens
before event Q and event Q happens before event R, then event
P happens before R.
</li></ul>

<p>
The &#34;happens before&#34; relation is a <em>partial</em> ordering: it is possible
to have two events P and Q such that neither happens before the other.
When this happens, the two events are <em>concurrent</em>.  Remember how
a lock prevents concurrent accesses to the same data structure?  That is
because, when you protect a data structure with a lock, all accesses
to the data structure form a total ordering, as if they came
from a single thread.  Lamport&#39;s framework also provides a basic idea
of what happens when a lock is handed off from a thread to another: some
kind of &#34;message passing&#34; ensures that the unlock operation of thread T
&#34;happens before&#34; the lock operation of thread U.

</p><p>
As it turns out, this is not just theory: in order to ensure that
their caches are coherent, CPUs exchange messages over buses such as
Intel&#39;s QPI or AMD&#39;s HyperTransport.  However, this level of detail is
definitely too much for our purposes.  Instead, we will generalize the
&#34;happens before&#34; definition to cover any kind of synchronization
primitive.

</p><p>
Lamport&#39;s fundamental insight was that synchronization happens when two
threads operate with symmetric operations on the same data structure.
In our generalization, we will list pairs of operations (such as sending
and receiving a message through the same queue) that synchronize one
thread with another.  Furthermore, we will classify the operations in
each pair as either <em>release</em> or <em>acquire</em>; equivalently
we can say that they &#34;have release (or acquire) semantics&#34;.

</p><p>
Within a pair, the release operation <em>synchronizes with</em> the
corresponding acquire operation.  &#34;Synchronizing&#34; means that,
whenever one thread performs a release operation and another thread
performs the corresponding acquire operation, the &#34;happens before&#34;
relation grows edges from the releasing thread to the acquiring thread.
&#34;Happens before&#34; remains a partial order in the general case, but
it now spans two threads, or more than two, thanks to transitivity.
More formally:

</p><ul>
<li> The ordering of operations is total within a single thread.

</li><li> If an operation P with release semantics synchronizes with an
     operation Q with acquire semantics, then operation P happens before
     operation Q, even if they happen in different threads.

</li><li> Just like before, the relation is transitive and defines a partial
     ordering of operations.
</li></ul>

<p>
The old definition follows by giving release semantics to message send
and acquire semantics to message receive.  A message send synchronizes
the sending thread with the thread (or threads) that receives the message.
We can also rephrase our previous findings according to this new
definition: for locks to work, unlocking must have release semantics
and must synchronize with locking—which in turn has acquire semantics.
No matter if the lock is contended or not, the resulting &#34;happens before&#34;
edges ensure a smooth hand-off from one thread to the other.

</p><p>
Acquire and release semantics may seem like an abstract concept, but they
truly provide simple explanations of many common multithreaded programming
practices.  For example, consider two user-space threads accessing a global
variable <tt>s</tt>:

</p><pre>    thread 1                              thread 2
    --------------------------------      ------------------------
    s = &#34;hello&#34;;
    pthread_create(&amp;t, NULL, t2, NULL);
                                          puts(s);
                                          s = &#34;world&#34;;
    pthread_join(t, NULL);
    puts(s);
</pre>

<p>
Are the accesses to the variable safe? Can thread 2 assume
that <tt>s</tt> will read as <tt>&#34;hello&#34;</tt>, and likewise
can thread 1 assume that <tt>s</tt> will be <tt>&#34;world&#34;</tt>
after <tt>pthread_join()</tt>?  The answer is affirmative, and we
can explain why in terms of acquire and release semantics:

</p><ul>
<li> <tt>pthread_create()</tt> has release semantics and synchronizes with
     the start of thread 2 (which has acquire semantics).  Therefore,
     anything written before a thread is created can be safely accessed
     from the thread.

</li><li> Exiting thread 2 has release semantics and synchronizes with
     <tt>pthread_join()</tt> (which has acquire semantics).  Therefore,
     anything the thread writes before exiting can be safely accessed after
     <tt>pthread_join()</tt>.
</li></ul>

<p>
Note that data is flowing from one thread to the other with no lock involved:
congratulations, you have made it through the first example of lockless
programming.  To sum up:

</p><ul>
<li> If the programmer wants thread 2 to &#34;see the effects of&#34; everything
  that happened so far in thread 1, the two threads need to synchronize
  with each other: this is done with a release operation in thread 1
  and an acquire operation in thread 2.

</li><li> Knowing which APIs provide acquire/release semantics lets you
  write code that relies on the ordering provided by those APIs.
</li></ul>

<p>
Having understood how release and acquire semantics work for high-level
synchronization primitives, we can now consider them in the context of
individual memory accesses.

</p><h4>The message-passing pattern</h4>

<p>
In the previous paragraph we have seen how the acquire and release
semantics of
<tt>pthread_create()</tt> and <tt>pthread_join()</tt> allow the creator
of a thread to exchange information with that thread and vice
versa.  We will now see how this kind of communication can happen
in a lockless manner while threads run.

</p><p>
If the message is a simple scalar value, for example a boolean, it
could be read and written directly to a memory location.  However, consider
what happens if the message is a pointer, as in the following example:

</p><pre>    thread 1                            thread 2
    --------------------------------    ------------------------
    a.x = 1;
    message = &amp;a;                       datum = message;
                                        if (datum != NULL)
                                          printk(&#34;%d\n&#34;, datum-&gt;x);
</pre>

<p>
If <tt>message</tt> is initially <tt>NULL</tt>, thread 2 will read
either <tt>NULL</tt> or <tt>&amp;a</tt>, we don&#39;t know which.  The
problem is that, even if
</p><pre>    datum = message;
</pre>
<p>were to read <tt>&amp;a</tt>, that assignment is still not synchronized
against the assignment in thread 1:
</p><pre>    message = &amp;a;
</pre><p>
Therefore, there is no edge in the
<i>happens before</i> relation connecting the two threads:

</p><pre>    a.x = 1;                            datum = message;
       |                                    |
       |   happens before                   |
       v                                    v
    message = &amp;a;                       datum-&gt;x
</pre>

<p>
Because the two threads of execution are disconnected,
there is still no guarantee that <tt>datum-&gt;x</tt> will read
as 1; we don&#39;t know that the assignment to <tt>a.x</tt> <i>happens
before</i> that read or not..
For this to happen, the store and load must be endowed with release and
acquire semantics respectively.

</p><p>
To that end, we have the &#34;store-release&#34; and &#34;load-acquire&#34;
operations.  A store-release operation P, in addition to writing to a
memory location, <em>synchronizes with</em> a load-acquire operation Q if Q
reads the value that was written by P.  Here is a fixed version
of the above code, using Linux&#39;s <tt>smp_store_release()</tt> and
<tt>smp_load_acquire()</tt>:

</p><pre>    thread 1                                  thread 2
    --------------------------------          ------------------------
    a.x = 1;
    smp_store_release(&amp;message, &amp;a);          datum = smp_load_acquire(&amp;message);
                                              if (datum != NULL)
                                                printk(&#34;%x\n&#34;, datum-&gt;x);
</pre>

<p>
With this change, if <tt>datum</tt> is <tt>&amp;a</tt> we can affirm
that the store <em>happened before</em> the load.  (I am assuming for
simplicity that only one thread can write <tt>&amp;a</tt> to
<tt>message</tt>.  Saying &#34;thread 2 reads the value written by
thread 1&#34; does not refer to the specific bit pattern that goes in memory, it
really means 
that thread 1&#39;s store is the last whose effect is visible to thread 2).
The relation looks like this now:

</p><pre>    a.x = 1;
       |
       v
    smp_store_release(&amp;message, &amp;a);  -----&gt;  datum = smp_load_acquire(&amp;message);
                                                  |
                                                  v
                                              datum-&gt;x
</pre>

<p>
And everything works.  Because of transitivity, whenever thread 2
reads the value written by thread 1, everything that thread 1 did up
to the store-release will also be visible to thread 2 after the
load-acquire.  Note that, unlike the <tt>pthread_join()</tt> case,
&#34;synchronizes with&#34; does not mean that thread 2 &#34;waits for&#34; thread 1
to do the write.  The above drawing only applies if thread 2 happens to
read the value that thread 1 has written.

</p><p>
In the Linux kernel, the above code will often be written in a slightly
different manner:

</p><pre>    thread 1                              thread 2
    --------------------------------      ------------------------
    a.x = 1;
    smp_wmb();
    WRITE_ONCE(message, &amp;a);              datum = READ_ONCE(message);
                                          smp_rmb();
                                          if (datum != NULL)
                                            printk(&#34;%x\n&#34;, datum-&gt;x);
</pre>

<p>
In this case, the release and acquire semantics are provided by
the <em>memory barriers</em> <tt>smp_wmb()</tt> and <tt>smp_rmb()</tt>.
Memory barriers also have acquire and release semantics, but they are a
bit more complicated to reason about than simple loads and stores.
We will get back to them when we talk about seqlocks.

</p><p>
Regardless of whether one uses load-acquire/store-release or
<tt>smp_rmb()</tt>/<tt>smp_wmb()</tt>, this is an extremely common
pattern, and one that should be understood well.  Among its uses
we find:

</p><ul>
<li> All sorts of ring buffers.  Each entry of the ring buffer often points
  to other data; usually, there are also head/tail locations that contain
  an index in the ring buffer.  The producer side will use store-release
  operations, synchronizing with load-acquire operations in the consumer.

</li><li> RCU.  As far as the compiler is concerned, the familiar
  <tt>rcu_dereference()</tt> and <tt>rcu_assign_pointer()</tt> APIs are similar to
  load-acquire and store-release operations.  Thanks to some assumptions that
  are true for all processors except the Alpha, <tt>rcu_dereference()</tt>
  can be compiled to a regular load; still, <tt>rcu_assign_pointer()</tt>
  synchronizes with <tt>rcu_dereference()</tt> as if it were a load-acquire
  operation.

</li><li> Publishing pointers into an array. In this (modified) KVM snippet, if
  <tt>kvm_get_vcpu()</tt> sees the incremented <tt>kvm-&gt;online_vcpus</tt>,
  the associated entry in the array will be valid:

<pre>    kvm_vm_ioctl_create_vcpu()                     kvm_get_vcpu()
    -----------------------------------------      -----------------------------------------------
    kvm-&gt;vcpus[kvm-&gt;online_vcpus] = vcpu;          if (idx &lt; smp_load_acquire(&amp;kvm-&gt;online_vcpus))
    smp_store_release(&amp;kvm-&gt;online_vcpus,            return kvm-&gt;vcpus[idx];
                      kvm-&gt;online_vcpus + 1);      return NULL;
</pre>
</li></ul>

<p>
Apart from the mechanics of the load-acquire/store-release operations,
there is another aspect of the message-passing pattern that you should
ponder: it is a <em>single producer</em> algorithm.  If there are multiple
writers, they must be protected against each other by other means,
for example with a mutex.  Lockless algorithms do not exist in a void;
they are but one part of the concurrent programming toolbox, and they
work best when combined with other, more traditional tools.

</p><p>
This is just the beginning of an extended series on lockless algorithms.
The next installment will look further at how atomic memory
operations can be ordered, and look into how memory barriers are at
the heart of both the &#34;seqcounts&#34; mechanism and the Linux scheduler.<br clear="all"/></p><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#Lockless_algorithms">Lockless algorithms</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/GuestIndex/">GuestArticles</a></td><td><a href="https://lwn.net/Archives/GuestIndex/#Bonzini_Paolo">Bonzini, Paolo</a></td></tr>
            </tbody></table></div></div>
  </body>
</html>
