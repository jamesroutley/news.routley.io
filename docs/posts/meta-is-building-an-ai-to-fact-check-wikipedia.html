<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://singularityhub.com/2022/08/26/meta-is-building-an-ai-to-fact-check-wikipedia-all-6-5-million-articles/">Original</a>
    <h1>Meta is building an AI to fact-check Wikipedia</h1>
    
    <div id="readability-page-1" class="page"><div>
                <p>Most people older than 30 probably remember doing research with good old-fashioned encyclopedias. You’d pull a heavy volume from the shelf, check the index for your topic of interest, then flip to the appropriate page and start reading. It wasn’t as easy as typing a few words into the Google search bar, but on the plus side, you knew that the information you found in the pages of the <i>Britannica</i> or the <i>World Book</i> was accurate and true.</p>
<p>Not so with internet research today. The overwhelming multitude of sources was confusing enough, but add the proliferation of misinformation and it’s a wonder any of us believe a word we read online.</p>
<p>Wikipedia is a case in point. As of early 2020, the site’s English version was averaging about <a href="https://techcrunch.com/2020/01/23/wikipedia-english-six-million-articles">255 million</a> page views per day, making it the eighth-most-visited website on the internet. As of last month, it had moved up to spot <a href="https://www.similarweb.com/top-websites/">number seven</a>, and the English version currently has over <a href="https://en.wikipedia.org/wiki/List_of_Wikipedias#Number_of_Wikipedias_by_language_families_and_groups">6.5 million</a> articles.</p>
<p>But as high-traffic as this go-to information source may be, its accuracy leaves something to be desired; the <a href="https://en.wikipedia.org/wiki/Reliability_of_Wikipedia">page</a> about the site’s own reliability states, “The online encyclopedia does not consider itself to be reliable as a source and discourages readers from using it in academic or research settings.”</p>
<p>Meta—of the former Facebook—wants to change this. In a <a href="https://tech.fb.com/artificial-intelligence/2022/07/how-ai-could-help-make-wikipedia-entries-more-accurate/">blog post</a> published last month, the company’s employees describe how AI could help make Wikipedia more accurate.</p>
<p>Though tens of thousands of people participate in editing the site, the facts they add aren’t necessarily correct; even when citations are present, they’re not always accurate nor even relevant.</p>
<p>Meta is developing a machine learning model that scans these citations and cross-references their content to Wikipedia articles to verify that not only the topics line up, but specific figures cited are accurate.</p>
<p>This isn’t just a matter of picking out numbers and making sure they match; Meta’s AI will need to “understand” the content of cited sources (though “understand” is a misnomer, as complexity theory researcher Melanie Mitchell <a href="https://singularityhub.com/2021/05/06/to-advance-ai-we-need-to-better-understand-human-intelligence-and-address-these-4-fallacies/">would tell you</a>, because AI is still in the “narrow” phase, meaning it’s a tool for highly sophisticated pattern recognition, while “understanding” is a word used for human cognition, which is still a very different thing).</p>
<p>Meta’s model will “understand” content not by comparing text strings and making sure they contain the same words, but by comparing mathematical representations of blocks of text, which it arrives at using natural language understanding (NLU) techniques.</p>
<p>“What we have done is to build an index of all these web pages by chunking them into passages and providing an accurate representation for each passage,” Fabio Petroni, Meta’s Fundamental AI Research tech lead manager, <a href="https://www.digitaltrends.com/computing/meta-wikipedia-bot-citations/">told <i>Digital Trends</i></a>. “That is not representing word-by-word the passage, but the meaning of the passage. That means that two chunks of text with similar meanings will be represented in a very close position in the resulting n-dimensional space where all these passages are stored.”</p>
<p>The AI is being trained on a set of four million Wikipedia citations, and besides picking out faulty citations on the site, its creators would like it to eventually be able to suggest accurate sources to take their place, pulling from a massive index of data that’s continuously updating.</p>
<p>One big issue left to work out is working in a grading system for sources’ reliability. A paper from a scientific journal, for example, would receive a higher grade than a blog post. The amount of content online is so vast and varied that you can find “sources” to support just about any claim, but parsing the misinformation from the disinformation (the former means incorrect, while the latter means deliberately deceiving), and the peer-reviewed from the non-peer-reviewed, the fact-checked from the hastily-slapped-together, is no small task—but a very important one when it comes to trust.</p>
<p>Meta has open-sourced its model, and those who are curious can see a <a href="https://verifier.sideeditor.com/">demo</a> of the verification tool. Meta’s blog post noted that the company isn’t partnering with Wikimedia on this project, and that it’s still in the research phase and not currently being used to update content on Wikipedia.</p>
<p>If you imagine a not-too-distant future where everything you read on Wikipedia is accurate and reliable, wouldn’t that make doing any sort of research a bit too easy? There’s something valuable about checking and comparing various sources ourselves, is there not? It was a big a leap to go from paging through heavy books to typing a few words into a search engine and hitting “Enter”; do we really want Wikipedia to move from a research jumping-off point to a gets-the-last-word source?</p>
<p>In any case, Meta’s AI research team will continue working toward a tool to improve the online encyclopedia. “I think we were driven by curiosity at the end of the day,” Petroni <a href="https://www.digitaltrends.com/computing/meta-wikipedia-bot-citations/">said</a>. “We wanted to see what was the limit of this technology. We were absolutely not sure if [this AI] could do anything meaningful in this context. No one had ever tried to do something similar.”</p>
<p><em>Image Credit: <a href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1802614">Gerd Altmann</a> from <a href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1802614">Pixabay</a></em></p>
    </div></div>
  </body>
</html>
