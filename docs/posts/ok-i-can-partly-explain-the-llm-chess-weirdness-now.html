<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dynomight.net/more-chess/">Original</a>
    <h1>OK, I can partly explain the LLM chess weirdness now</h1>
    
    <div id="readability-page-1" class="page"><section>
    <p>We recently talked about a <a href="https://dynomight.net/chess/">mystery</a>: All large language models (LLMs) are terrible at chess. All, that is, except for <code>gpt-3.5-turbo-instruct</code>, which for some reason can play at an advanced amateur level. This is despite the fact that this model is more than a year old and much smaller than recent models. What’s going on?</p>

<p>I suggested four possible explanations:</p>

<ul>
  <li>
    <p><strong>Theory 1:</strong> Large enough <em>base</em> models are good at chess, but this doesn’t persist through instruction tuning to chat models.</p>
  </li>
  <li>
    <p><strong>Theory 2</strong>: For some reason, <code>gpt-3.5-turbo-instruct</code> was trained on more chess data.</p>
  </li>
  <li>
    <p><strong>Theory 3:</strong> There’s something magical about certain LLM architectures.</p>
  </li>
  <li>
    <p><strong>Theory 4:</strong> There’s “competition” between different types of data, so for an LLM to play chess well, you need a large <em>fraction</em> of the data to be chess games.</p>
  </li>
</ul>

<p>The internet offered several other theories. The most common were:</p>

<ul>
  <li>
    <p><strong>Theory 5</strong>: OpenAI is cheating.</p>
  </li>
  <li>
    <p><strong>Theory 6</strong>: LLMs can’t actually play chess.</p>
  </li>
  <li>
    <p><strong>Theory 7</strong>: Large enough <em>base</em> models are good at chess, but this doesn’t persist through instruction tuning to chat models, Dynomight you are so bad for not suggesting this, how are you so dumb and bad?</p>
  </li>
</ul>

<p>I’ve now done new experiments and—good news—everyone is wrong!</p>

<p>Here, I’ll show that recent chat models <em>can</em> play chess quite well, as long as you’re willing to go through sufficiently extreme contortions to figure out how to prompt them. Then I’ll give my theory for what’s happening.</p>

<p>But first…</p>

<h2 id="i-really-dont-think-openai-is-cheating">I really don’t think OpenAI is cheating</h2>

<p>I was astonished that half the internet is convinced that OpenAI is cheating. Many, <em>many</em> people suggested that there must be some special case in <code>gpt-3.5-turbo-instruct</code> that recognizes chess notation and calls out to an external chess engine.</p>

<p>I think this is <em>extremely</em> unlikely. Because:</p>

<ol>
  <li>
    <p>Many people from OpenAI have said they didn’t do that. Sure, people lie, but conspiracies are hard, and why lie about <em>this</em>?</p>
  </li>
  <li>
    <p>In chess, you can arrive at the same board state via different sequences of moves. Chess engines don’t care, but <code>gpt-3.5-turbo-instruct</code> <em>does</em> care and <a href="https://nicholas.carlini.com/writing/2023/chess-llm.html#:~:text=Language%20Modeling;%20Not%20Winning%20(Part%202)">plays very differently</a> for different move sequences.</p>
  </li>
  <li>
    <p>While <code>gpt-3.5-turbo-instruct</code> is great by the standards of chess amateurs, it’s bad by the standards of experts and <em>pathetic</em> by the standards of chess engines. If you’re going to cheat, why stop at an Elo of 1800?</p>
  </li>
  <li>
    <p>If you change the way you prompt <code>gpt-3.5-turbo-instruct</code>, this will subtly change how it plays. Is there some neural network that looks at the text and dynamically sets the chess engine skill level?</p>
  </li>
  <li>
    <p>Later OpenAI models are by default much worse. Did they remove the cheat?</p>
  </li>
  <li>
    <p>I will show (below) that later OpenAI models can also play well if you use the right incantations.</p>
  </li>
</ol>

<p>If OpenAI <em>did</em> cheat, they went to insane lengths to cheat in a way that looks exactly like an LLM is choosing the moves and not at all like calling out to an external chess engine.</p>

<h2 id="yes-llms-can-play-chess">Yes, LLMs can play chess</h2>

<p>I was also surprised to see so many people suggest that LLMs can’t <em>really</em> play chess, all they do is memorize openings and then play randomly.</p>

<p>This is wrong. LLMs can definitely play chess, and we need to make peace with this.</p>

<p>For one, <code>gpt-3.5-turbo-instruct</code> rarely suggests illegal moves, even in the late game. This requires “understanding” chess. If this doesn’t convince you, I encourage you to write a program that can take strings like <code>1. e4 d5 2. exd5 Qxd5 3. Nc3</code> and then say if the last move was legal.</p>

<p>And I defy you to maintain that LLMs can’t play chess after looking at some actual games. Here are ten: <a href="https://lichess.org/iwi02kUm">1</a> <a href="https://lichess.org/bfaDMlVm">2</a> <a href="https://lichess.org/lNE5mKPO">3</a> <a href="https://lichess.org/C3xV1uAz">4</a> <a href="https://lichess.org/YpzT2KQS">5</a> <a href="https://lichess.org/dy9m2DsU">6</a> <a href="https://lichess.org/4YDx633U">7</a> <a href="https://lichess.org/ZM9ZbDfo">8</a> <a href="https://lichess.org/59cTZSjs">9</a> <a href="https://lichess.org/DmeQQx7Y">10</a>. It plays pretty well even in completely new board states that have never existed in any game before in history.</p>

<h2 id="so-whats-happening">So what’s happening?</h2>

<p>Why is one LLM great and all the others terrible?</p>

<p>Let me remind you of what I’m talking about. First, take <code>gpt-3.5-turbo-instruct</code>. This is a “completion” model, meaning all it does is take some text and generate new text that might come after. I gave it text like this:</p>

<p><code>[Event &#34;Shamkir Chess&#34;]</code></p>

<p>I then took the first few characters and used them as a move.</p>

<p>Next, take <code>gpt-4o-mini</code> and <code>gpt-4o</code>. These are “chat” models, meaning you give them a “system prompt” that says what they’re supposed to do and a “user prompt” and then they try to answer you. I used this system prompt:</p>

<p><code>You are a chess grandmaster.</code></p>

<p>And I used user prompts like this (repeating the system prompt):</p>

<p><code>You are a chess grandmaster.</code></p>

<p>Here are the results of these three models against Stockfish—a standard chess AI—on level 1, with a maximum of 0.01 seconds to make each move. After the game was over, I calculated the score after each turn in “centipawns” where a pawn is worth 100 points, and ±1500 indicates a win or loss. Here is the average over 50 games (click to zoom in):</p>

<p><a href="https://dynomight.net/img/more-chess/old.pdf"><img src="https://dynomight.net/img/more-chess/old.svg" alt=""/></a></p>

<p><em>Note</em>: Last time I found that <code>gpt-3.5-turbo-instruct</code> won every single game and <code>gpt-4o</code> <em>lost</em> every game. I think the difference now is just that I have 50 samples rather than 10.</p>

<p>So that’s the mystery: <code>gpt-3.5-turbo-instruct</code> is great, and other models are terrible. Last time I tested lots of open models and they were terrible too. Why?</p>

<p>To answer this, let’s see if we can make these other models play chess better.</p>

<h2 id="should-we-fiddle-with-the-prompt">Should we fiddle with the prompt?</h2>

<p>You can nitpick at how I prompted the chat models. Is it a good idea to repeat the system prompt at the top of the user prompt? Is it a good idea to add all the metadata like user names before you start listing moves?</p>

<p>As far as I can tell, no one knows. So I tried every combination of these things being on or off. With <code>gpt-4o-mini</code> (a small model) it seemed to make little difference.</p>

<p><a href="https://dynomight.net/img/more-chess/gpt-4o-mini-variants.pdf"><img src="https://dynomight.net/img/more-chess/gpt-4o-mini-variants.svg" alt=""/></a></p>

<p>With <code>gpt-4o</code> (a bigger model) it… <em>maybe</em> made a difference?</p>

<p><a href="https://dynomight.net/img/more-chess/gpt-4o-variants.pdf"><img src="https://dynomight.net/img/more-chess/gpt-4o-variants.svg" alt=""/></a></p>

<p>Taken at face value, this says that repeating the system prompt helps a bit but metadata hurts a bit. But I’m not sure if that’s real or just noise. For simplicity, I decided to not repeat the system prompt and to turn off metadata for all further experiments.</p>

<h2 id="should-we-add-examples">Should we add examples?</h2>

<p>If you want LLMs to do something, standard advice is to provide some examples. So I did.</p>

<ul>
  <li><strong>Input A</strong>: <code>1.</code></li>
  <li><strong>Output A</strong>: <code>e4</code></li>
  <li><strong>Input B</strong>: <code>1. e4</code></li>
  <li><strong>Output B</strong>: <code>d5</code></li>
  <li><strong>Input C</strong>: <code>1. e4 e5 2. Nf3 Nc6 3.</code></li>
  <li><strong>Output C</strong>: <code>Bb5</code></li>
</ul>

<p>That’s all I used, just those three examples. I provided these in “correctly” using the API, not by jamming them into the user prompt. The results were:</p>

<p><a href="https://dynomight.net/img/more-chess/examples.pdf"><img src="https://dynomight.net/img/more-chess/examples.svg" alt=""/></a></p>

<p><em>Very</em> good.</p>

<p>Is this surprising? I thought this was surprising.</p>

<p>I mean, sure, this kind of “in-context learning” is a big part of what makes LLMs so exciting. And examples are probably <em>the</em> most standard piece of advice from practitioners.</p>

<p>Still, I was blown away that three tiny examples could have such a <em>profound</em> effect on performance. More (or different) examples might be even better. I didn’t check, because generating each of these figures requires an ungodly number of queries.</p>

<h2 id="should-we-fine-tune">Should we fine-tune?</h2>

<p>Another standard (albeit more difficult) way to improve LLMs is to fine-tune—to optimize the weights to be good at whatever task using data for that task.</p>

<p>So I did this for both <code>gpt-4o-mini</code> and <code>gpt-4o</code>.</p>

<p>To generate the fine-tuning data, I had Stockfish play 100 games against itself on its highest difficulty level. For each game, I picked a random move and used it as an example. Here was one example:</p>

<ul>
  <li><strong>System prompt</strong>: (same as above)</li>
  <li><strong>User prompt</strong>:  <code>1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 e5 7. Nb3 Be7 8. Be3 Be6 9. f4 exf4 10. Bxf4 Nc6 11. Qd3 Ne5 12. Qg3 Nh5 13. Qe3</code></li>
  <li><strong>Desired output</strong>: <code>Nxf4</code></li>
</ul>

<p>I then had Stockfish play another 100 games as validation data. The results were:</p>

<p><a href="https://dynomight.net/img/more-chess/finetune.pdf"><img src="https://dynomight.net/img/more-chess/finetune.svg" alt=""/></a></p>

<p>Good! Fine-tuning helps.</p>

<p><em>Note</em>: The first time I fine-tuned <code>gpt-4o</code>, the results seemed bad, so I ran it again with a smaller step size. This makes me nervous.</p>

<h2 id="should-we-combine-examples-and-fine-tuning">Should we combine examples and fine-tuning?</h2>

<p>If examples are good, and fine-tuning is good, will putting them together be even better?</p>

<p>My intuition was no, since three in-context examples seems trivial when compared to 100 fine-tuning examples. Once fine-tuning was done, I figured the examples would be superfluous.</p>

<p>The answer <em>was</em> no, but for different reasons:</p>

<p><a href="https://dynomight.net/img/more-chess/examples-finetune.pdf"><img src="https://dynomight.net/img/more-chess/examples-finetune.svg" alt=""/></a></p>

<p>According to that figure, fine-tuning helps. And examples help. But it’s <em>examples</em> that make <em>fine-tuning</em> redundant, not the other way around.</p>

<p>Ohkay.</p>

<h2 id="should-we-provide-legal-moves">Should we provide legal moves?</h2>

<p>LLMs sometimes struggle to give legal moves. In these experiments, I try 10 times and if there’s still no legal move, I just pick one at random. So I wondered: Maybe I could help the LLM out by listing the legal moves before giving the game history? Some might say this is “cheating”, but let’s try it anyway.</p>

<p>I used this system prompt:</p>

<p><code>You are a chess grandmaster.</code></p>

<p>And I sent user prompts like this:</p>

<p><code>Here are the current legal moves:</code></p>

<p>Here are the results:</p>

<p><a href="https://dynomight.net/img/more-chess/list-legal-moves.svg"><img src="https://dynomight.net/img/more-chess/list-legal-moves.svg" alt=""/></a></p>

<p>Disaster. Listing the legal moves makes the models play <em>much</em> worse. They don’t just win fewer games, they start making mistakes after a much smaller number of turns.</p>

<p>Ohhhkay. I guess let’s not do that.</p>

<h2 id="i-had-an-idea">I had an idea</h2>

<p>Thinking about the above, I had an idea. An idea that I think is… rather good.</p>

<p>Let’s back up a second. To make an LLM, you first make a “base” model. All that base models do is take a string and continue it. Given <code>The best tea is </code> , they have some probability of outputting <code>green tea</code> or <code>oolong</code> or whatever. (The right answer is <code>oolong</code>.)</p>

<p>If you want an LLM that can talk to you, you can <em>sort of</em> get a base model to do this by sending them strings that look like this:</p>

<p><code>(Transcript of chat between USER and ASSISTANT who is super chill and answers all questions without judgment.)</code>
</p>

<p>LLMs trained on general text are smart enough to recognize that what comes next is probably something that a super chill agent would say to a neurotic user. So they’ll typically do something reasonable. But in practice, they aren’t great. The responses tend to reflect the chaos of the internet, which isn’t exactly what you want from an assistant.</p>

<p>Chat models go further in two ways. First, they create special tokens to indicate the different parts of the conversation, sort of like this (except you should think of <code>&lt;|SYSTEM|&gt;</code> et al. as being single special characters).</p>

<p><code>&lt;|SYSTEM|&gt;</code></p>

<p>Then, they do “instruction tuning”—they <em>re-train</em> the weights so that the model is good at responding to prompts given in this format.</p>

<p>So, when I asked <code>gpt-4o</code> to predict a chess move, the string that was actually presented to the system looked sort of like this:</p>

<p><code>&lt;|SYSTEM|&gt;</code></p>

<p>To make <code>gpt-4o</code>, OpenAI first made a base model. As far as I know, that model doesn’t have a name, so let’s call it <code>gpt-4-base</code>. It then did instruction tuning and stuck the instruction-tuned model behind the chat interface, to give us <code>gpt-4o</code>. (It also did some other stuff like <a href="https://en.wikipedia.org/wiki/Knowledge_distillation">distillation</a>, but never mind.)</p>

<p>I’ve gone through all this background because it allows us to state a central question: How good is <code>gpt-4-base</code> at chess? Is it as good at <code>gpt-3.5-turbo-instruct</code>? And if it <em>is</em>, then why is <code>gpt-4o</code> worse? Is it because of the instruction tuning? Or is it just because of the chat template, with the <code>&lt;|USER|&gt;</code> and <code>&lt;|ASSISTANT|&gt;</code> tokens floating around in ways that don’t happen in chess games written down in PGN notation?</p>

<p>I’m not sure, because OpenAI doesn’t deign to share <code>gpt-4-base</code>, nor to allow queries of <code>gpt-4o</code> in completion mode. But maybe we can help <code>gpt-4o</code> remember its evolutionary history. Maybe we can <em>prompt</em> <code>gpt-4o</code> in a way that will sort of trick it into responding <em>more</em> like it was in completion mode.</p>

<h2 id="should-we-regurgitate">Should we regurgitate?</h2>

<p>Thus my idea: Instead of just asking for a move, how about we ask the model to repeat the whole game and <em>then</em> give a move?</p>

<p>I changed the system prompt to this:</p>

<p><code>You are a chess grandmaster.</code></p>

<p>Given a prompt like <code>1. e4 e5 2.</code> I expected the model to return an output like <code>1. e4 e5 2. Nf7</code>. I checked to make sure it successfully repeated the entire game before giving a new legal move.</p>

<p>This works:</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate.svg" alt=""/></a></p>

<p>By forcing the models to repeat the whole move sequence, you force the model to create a context for itself where it’s much more likely to choose good moves.</p>

<p>This makes <code>gpt-4o-mini</code> and <code>gpt-4o</code> better. It also seems like strong evidence that if we could query <code>gpt-4-base</code> in completion mode, it would be pretty good.</p>

<p><em>Note</em>: When using this type of prompt, I first gave the model ten tries to repeat the whole sequence and then give a legal move at the end. If none of those tries succeeded, I gave it another ten tries to at least produce a legal move after the new turn number, even if it didn’t repeat the whole game perfectly. If that <em>still</em> didn’t succeed, I chose a move at random.</p>

<h2 id="can-we-regurgitate-better">Can we regurgitate better?</h2>

<p>Fine-tuning is good. Regurgitation is good. Are they good together?</p>

<p>To test this, I needed to do a <em>new</em>, <em>independent</em> run of fine-tuning. I used the exact same sequence of games and moves, but with outputs repeating the inputs before giving a new move. For example:</p>

<ul>
  <li><strong>System prompt</strong>: (same as above)</li>
  <li><strong>User prompt</strong>:  <code>1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 e5 7. Nb3 Be7 8. Be3 Be6 9. f4 exf4 10. Bxf4 Nc6 11. Qd3 Ne5 12. Qg3 Nh5 13. Qe3</code></li>
  <li><strong>Desired output</strong>: <code>1. e4 c5 2. Nf3 d6 3. d4 cxd4 4. Nxd4 Nf6 5. Nc3 a6 6. Be2 e5 7. Nb3 Be7 8. Be3 Be6 9. f4 exf4 10. Bxf4 Nc6 11. Qd3 Ne5 12. Qg3 Nh5 13. Qe3 Nxf4</code></li>
</ul>

<p>This… maybe helped a little?</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate-finetune.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate-finetune.svg" alt=""/></a></p>

<p>And how about examples? Will they improve regurgitation?</p>

<p>I used the same  three examples:</p>

<ul>
  <li><strong>Input A:</strong> <code>1.</code></li>
  <li><strong>Output A:</strong> <code>1. e4</code></li>
  <li><strong>Input AB:</strong> <code>1. d4</code></li>
  <li><strong>Output B:</strong> <code>1. d4 d5</code></li>
  <li><strong>Input C:</strong> <code>1. e4 e5 2. Nf3 Nc6 3.</code></li>
  <li><strong>Output C:</strong> <code>1. e4 e5 2. Nf3 Nc6 3. Nf3</code></li>
</ul>

<p>Like before, these had a remarkable impact given how little information they contain.</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate-examples.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate-examples.svg" alt=""/></a></p>

<p>And should we combine examples <em>and</em> fine tuning?</p>

<p><a href="https://dynomight.net/img/more-chess/regurgitate-examples-finetune.pdf"><img src="https://dynomight.net/img/more-chess/regurgitate-examples-finetune.svg" alt=""/></a></p>

<p>Here we have the same (strange) result as without regurgitation. If you fine-tune, then adding examples helps. But it’s still worse than examples without fine-tuning.</p>

<h2 id="where-do-we-stand">Where do we stand?</h2>

<p>What have we learned so far?</p>

<ul>
  <li><strong>GOOD</strong>: Regurgitation, examples, fine tuning (without examples)</li>
  <li><strong>UNCLEAR</strong>: Metadata, repeating the system prompt, fine tuning (with examples)</li>
  <li><strong>BAD</strong>: Providing the list of legal moves</li>
</ul>

<p>So if we use regurgitation and examples and turn everything else off, how good is it? Will it play as well as our old nemesis?</p>

<p><a href="https://dynomight.net/img/more-chess/but.pdf"><img src="https://dynomight.net/img/more-chess/but.svg" alt=""/></a></p>

<p>No. It’s respectable, but still not quite as good as <code>gpt-3.5-turbo-instruct</code>.</p>

<p>To compare these more directly, I had <code>gpt-4o + regurgitate + examples</code> play 50 games against <code>gpt-3.5-turbo-instruct</code>. In all cases, <code>gpt-4o</code> was white.</p>

<table>
  <thead>
    <tr>
      <th>outcome for  <code>gpt-4o + regurgitate + examples</code></th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>win</td>
      <td>10</td>
    </tr>
    <tr>
      <td>tie</td>
      <td>5</td>
    </tr>
    <tr>
      <td>loss</td>
      <td>35</td>
    </tr>
  </tbody>
</table>

<p>According to <a href="https://3dkingdoms.com/chess/elo.htm">this calculator</a>, that’s consistent with an Elo difference of -191. But you need to account for the fact that <code>gpt-4o</code> was always white, reportedly worth around <a href="https://en.wikipedia.org/wiki/First-move_advantage_in_chess#Winning_percentages">35 Elo</a>. Since <code>gpt-3.5-turbo-instruct</code> has been measured at around 1800 Elo, this suggests <code>gpt-4o</code> with regurgitation and examples hits around 1800 - 191 - 35/2 ≈ 1590 Elo, which is still “intermediate amateur” territory.</p>

<p>Here are 10 games of <code>gpt-4o + regurgitate + examples</code> playing against Stockfish: <a href="https://lichess.org/Q2lUPANn">1</a> <a href="https://lichess.org/Da7HUOz4">2</a> <a href="https://lichess.org/Ijitm5vI">3</a> <a href="https://lichess.org/ib0wwcAO">4</a> <a href="https://lichess.org/jOY82JGZ">5</a> <a href="https://lichess.org/hD9fQMMj">6</a> <a href="https://lichess.org/SmlDpA6O">7</a> <a href="https://lichess.org/kyRzFcvU">8</a> <a href="https://lichess.org/NAI3jWPo">9</a> <a href="https://lichess.org/x2LFUowA">10</a></p>

<p>And here are 10 games of <code>gpt-4o + regurgitate + examples</code> playing against <code>gpt-3.5-turbo-instruct</code>: <a href="https://lichess.org/RHiS3LUm">1</a> <a href="https://lichess.org/cq2ZKWwq">2</a>  <a href="https://lichess.org/2uLoukYE">3</a> <a href="https://lichess.org/JdPgUBCj">4</a> <a href="https://lichess.org/05DgjWJc">5</a> <a href="https://lichess.org/BoHLjFp8">6</a>  <a href="https://lichess.org/o4zh92NZ">7</a> <a href="https://lichess.org/wC4bp7gp">8</a> <a href="https://lichess.org/gHj3afte">9</a> <a href="https://lichess.org/ytgH7Qzr">10</a></p>

<h2 id="so-heres-my-current-theory">So here’s my current theory</h2>

<p>Here’s my best guess for what is happening:</p>

<p><strong>Part 1:</strong> OpenAI trains its base models on datasets with more/better chess games than those used by open models.</p>

<p><strong>Part 2</strong>: Recent <em>base</em> OpenAI models would be excellent at chess (in completion mode, if we could access them). But the <em>chat</em> models that we actually get access to aren’t.</p>

<p>I think part 1 is true because all the open models are <a href="https://dynomight.net/chess/">terrible</a> at chess, regardless of if they are base models or chat models. I suspect this is <em>not</em> some kind of architectural limitation—if you fine-tuned <code>llama-3.1-70b</code> on billions of expert chess games, I would be surprised if it could not beat <code>gpt-3.5-turbo-instruct</code> (rumored to have only around 20 billion parameters).</p>

<p>Meanwhile, in section A.2 of <a href="https://arxiv.org/pdf/2312.09390#page=29">this paper</a> (h/t Gwern) some OpenAI authors mention that GPT-4 was trained on chess games in PGN notation, filtered to only include players with Elo at least 1800. I haven’t seen any public confirmation that <code>gpt-3.5-turbo-instruct</code> used the same data, but it seems plausible. And can it really be a coincidence that <code>gpt-3.5-turbo-instruct</code> plays games <em>in PGN notation with a measured Elo of 1800</em>?</p>

<p>I can’t find any details about how much chess data was included when training Llama et al. I’m sure many games made their way in from the open internet. But specifically curating a giant database of high quality games probably just gives better results, and the open models probably just didn’t do that.</p>

<p>(Incidentally, I encourage people at all AI companies to leak secrets to me. If you use the <a href="https://dynomight.net/about/">anonymous feedback form</a>, please write with sufficient technicality that I can verify your expertise. Secrets will be used only for good, not evil.)</p>

<p>It’s also conceivable that some models are playing worse because they have <em>too much</em> chess data. It could be that the open internet has too many games from low-skill players and that if you don’t filter these out, then the models <em>correctly</em> predict that players would make low-quality moves. But I suspect not, because a smart model would recognize that <em>if the sequence of moves so far is high skill</em> then the player isn’t a total idiot and probably won’t throw away their queen. But the models don’t seem to do that.</p>

<p>I think part 2 of my theory is true mostly because of the experiments I did in this post: If you do weird contortions to “trick” OpenAI chat models into behaving more like completion models, then they play much better. So I suspect that the underlying <em>base</em> models (which we can’t touch) are good.</p>

<p>Now, there’s a major uncertainty in part 2. If <code>gpt-4o</code> in chat mode is worse than <code>gpt-4-base</code> in completion mode, then why? Is it the chat interface or the instruction tuning, or both? Put another way, would <code>gpt-4-base</code> be good at chess in a simulated chat mode? And would <code>gpt-4o</code> be good if we could query it <em>in completion mode</em>?</p>

<p>It’s impossible to say, because we can’t do those experiments.</p>

<h2 id="parting-thoughts">Parting thoughts</h2>

<ol>
  <li>
    <p>Isn’t it great how much of AI is now palace intrigue?</p>
  </li>
  <li>
    <p>It’s very likely that there are ways to coax better behavior out of <code>gpt-4o</code>. In truth, I barely scratched the surface here.</p>
  </li>
  <li>
    <p>It’s ridiculously hard to find the optimal combination of prompts and examples and fine-tuning, etc. It’s a very large space, there are no easy abstractions to allow you to search through the space, LLMs are unpredictable and fragile, and these experiments are slow and expensive.</p>
  </li>
  <li>
    <p>I tried running the final recipe with <code>gpt-4</code> (rather than <code>gpt-4o</code>), and it played poorly. I suspect the reason is that the combination of tricks I found is <code>gpt-4o</code> specific. Maybe <code>gpt-4</code> needs a different prompt? Or more examples? Or would respond better to fine-tuning? Who knows.</p>
  </li>
  <li>
    <p>In many ways, this feels less like engineering and more like a search for spells.</p>
  </li>
</ol>

<h2 id="ps">P.S.</h2>

<p>Thanks to the <a href="https://dynomight.net/automated/">Automator</a> for crucial guidance and boundless patience. Thanks to Daniel Gross for paying for all the electrons. Here is some good prior work on LLMs and chess:</p>

<ul>
  <li>
    <p>Adam Karvonen’s <a href="https://github.com/adamkarvonen/chess_gpt_eval">chess gpt eval repo</a>, which does careful tests on how good <code>gpt-3.5-turbo-instruct</code> is.</p>
  </li>
  <li>
    <p>Adam Karvonen’s <a href="https://github.com/adamkarvonen/chess_llm_interpretability">chess llm interpretability repo</a>  and paper, <a href="https://arxiv.org/pdf/2403.15498v2">“Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models”</a> which show, among other things, that <code>gpt-3.5-turbo-instruct</code> <em>does</em> seem to build up some kind of internal representation of board state.</p>
  </li>
  <li>
    <p>Matheiu Archer’s <a href="https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/">estimates of ELO</a> for <code>gpt-3.5-turbo-instruct</code> and <code>gpt-3.5-turbo</code> and <code>gpt-4</code>. This also experiments with different temperatures.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2406.11741">Transcendence: Generative Models Can Outperform The Experts That Train Them</a> (h/t WTFwhatthehell)</p>
  </li>
  <li>
    <p>Nicholas Carlini’s <a href="https://nicholas.carlini.com/writing/2023/chess-llm.html">Playing chess with large language models</a>.</p>
  </li>
</ul>

<p><em>Update</em>: Corrected example one output which slightly improved the results.</p>

  </section></div>
  </body>
</html>
