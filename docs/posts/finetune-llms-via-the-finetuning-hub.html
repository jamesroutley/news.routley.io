<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/georgian-io/LLM-Finetuning-Hub">Original</a>
    <h1>Show HN: finetune LLMs via the Finetuning Hub</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<div dir="auto">

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/georgian-io/LLM-Finetuning-Hub/blob/main/assets/repo-main.png"><img src="https://github.com/georgian-io/LLM-Finetuning-Hub/raw/main/assets/repo-main.png" width="512" height="296"/></a></p><p dir="auto">LLM Finetuning Hub contains code and insights to finetune various large language models for your use-case.</p>
<p dir="auto">We stress-test both open-source and close-source LLMs through our Evaluation Framework to check their applicability for real-life business use-cases. Finetuning LLMs has never been easier.</p>
<p dir="auto"><a href="#evaluation-framework">Evaluation Framework</a> •
<a href="#getting-started">Getting Started</a> •
<a href="#llm-roadmap">LLM Roadmap</a> •
<a href="#contributing">Contributing</a></p>
</div>
<h2 tabindex="-1" dir="auto"><a id="user-content-evaluation-framework" aria-hidden="true" tabindex="-1" href="#evaluation-framework"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Evaluation Framework</h2>
<p dir="auto">For a holistic evaluation, we will make use of the <strong>Evaluation Framework</strong> that contains <strong>4 pillars</strong>:</p>

<p dir="auto">For each of the above four pillars, we are sharing our codebase and insights to:</p>
<ul dir="auto">
<li>Assist you to leverage LLMs for your business needs and challenges</li>
<li>Decide which LLM suits your needs from a performance and cost perspective</li>
<li>Boost reproducibility efforts which are becoming increasingly difficult with LLMs</li>
</ul>
<p dir="auto">We are providing scripts that are ready-to-use for:</p>
<ul dir="auto">
<li>Finetuning LLMs on your proprietary dataset via PeFT methodologies such as LoRA and Prefix Tuning</li>
<li>Performing hyperparameter optimization to get the maximum performance out of these models</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<p dir="auto">You can start fine-tuning your choice of LLM in 4 easy steps:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Setup conda environment</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.11.0-Linux-x86_64.sh
bash Miniconda3-py38_4.11.0-Linux-x86_64.sh
source ~/.bashrc
conda create --name llm_finetuning python=3.9
conda activate llm_finetuning"><pre>wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.11.0-Linux-x86_64.sh
bash Miniconda3-py38_4.11.0-Linux-x86_64.sh
<span>source</span> <span>~</span>/.bashrc
conda create --name llm_finetuning python=3.9
conda activate llm_finetuning</pre></div>
</li>
<li>
<p dir="auto"><strong>Install relevant packages</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/georgian-io/LLM-Finetuning-Hub.git
cd LLM-Finetuning-Hub/
pip install -r requirements.txt"><pre>git clone https://github.com/georgian-io/LLM-Finetuning-Hub.git
<span>cd</span> LLM-Finetuning-Hub/
pip install -r requirements.txt</pre></div>
</li>
<li>
<p dir="auto"><strong>Finetune your LLM of choice</strong></p>
<p dir="auto">For instance, to finetune Falcon-7B, do the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd falcon/ # navigate to Falcon folder
python falcon_classification.py --lora_r 8 --epochs 5 --dropout 0.1 # finetune Falcon-7B on newsgroup classification dataset
python falcon_classification_inference.py --experiment &lt;experiment folder&gt; # evaluate finetuned Falcon
python falcon_summarization.py --lora_r 8 --epochs 1 --dropout 0.1 # finetune Falcon-7B on samsum chat dataset
python falcon_summarization_inference.py --experiment &lt;experiment folder&gt; # evaluate finetuned Falcon"><pre><span>cd</span> falcon/ <span><span>#</span> navigate to Falcon folder</span>
python falcon_classification.py --lora_r 8 --epochs 5 --dropout 0.1 <span><span>#</span> finetune Falcon-7B on newsgroup classification dataset</span>
python falcon_classification_inference.py --experiment <span>&lt;</span>experiment folder<span>&gt;</span> <span><span>#</span> evaluate finetuned Falcon</span>
python falcon_summarization.py --lora_r 8 --epochs 1 --dropout 0.1 <span><span>#</span> finetune Falcon-7B on samsum chat dataset</span>
python falcon_summarization_inference.py --experiment <span>&lt;</span>experiment folder<span>&gt;</span> <span><span>#</span> evaluate finetuned Falcon</span></pre></div>
<p dir="auto">For instance, to finetune Flan-T5-Large, do the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd flan-t5/ # navigate to Flan-T5 folder
python flan_classification.py --peft_method prefix --prefix_tokens 20 --epochs 5 # finetune Flan-T5 on newsgroup dataset
python flan_classification_inference.py --experiment &lt;experiment folder&gt; # evaluate finetuned Flan-T5
python flan_summarization.py --peft_method lora --lora_r 8 --epochs 1 # finetune Flan-T5 on samsum chat dataset
python flan_summarization_inference.py --experiment &lt;experiment folder&gt; # evalute finetuned Flan-T5"><pre><span>cd</span> flan-t5/ <span><span>#</span> navigate to Flan-T5 folder</span>
python flan_classification.py --peft_method prefix --prefix_tokens 20 --epochs 5 <span><span>#</span> finetune Flan-T5 on newsgroup dataset</span>
python flan_classification_inference.py --experiment <span>&lt;</span>experiment folder<span>&gt;</span> <span><span>#</span> evaluate finetuned Flan-T5</span>
python flan_summarization.py --peft_method lora --lora_r 8 --epochs 1 <span><span>#</span> finetune Flan-T5 on samsum chat dataset</span>
python flan_summarization_inference.py --experiment <span>&lt;</span>experiment folder<span>&gt;</span> <span><span>#</span> evalute finetuned Flan-T5</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Zero-shot and Few-shot your LLM of choice</strong></p>
<p dir="auto">For instance, to use Falcon-7B on newsgroup classification task, do the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python falcon_baseline_inference.py --task_type classification --prompt_type zero-shot
python falcon_baseline_inference.py --task_type classification --prompt_type few-shot"><pre>python falcon_baseline_inference.py --task_type classification --prompt_type zero-shot
python falcon_baseline_inference.py --task_type classification --prompt_type few-shot</pre></div>
<p dir="auto">To use Falcon-7B on samsum summarization task, do the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python falcon_baseline_inference.py --task_type summarization --prompt_type zero-shot
python falcon_baseline_inference.py --task_type summarization --prompt_type few-shot"><pre>python falcon_baseline_inference.py --task_type summarization --prompt_type zero-shot
python falcon_baseline_inference.py --task_type summarization --prompt_type few-shot</pre></div>
</li>
</ol>
<p dir="auto">NOTE: All of our experiments were conducted on the AWS EC2 instance: g5.2xlarge. It has one 24GB Nvidia GPU, and is sufficient to finetune the LLMs in this repository.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-llm-roadmap" aria-hidden="true" tabindex="-1" href="#llm-roadmap"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LLM Roadmap</h2>
<p dir="auto">Our plan is to perform these experiments on all the LLMs below. To that end, this is a tentative roadmap of the LLMs that we aim to cover, and their corresponding codebase and README links:</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Benchmarked?</th>
<th>Open-Source?</th>
<th>README</th>
<th>Codebase</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flan-T5</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/georgian-io/LLM-Finetuning-Hub/blob/main/flan-t5/README.md">Link</a></td>
<td><a href="https://github.com/georgian-io/LLM-Finetuning-Hub/tree/main/flan-t5">Folder</a></td>
</tr>
<tr>
<td>Falcon</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/georgian-io/LLM-Finetuning-Hub/blob/main/falcon/README.md">Link</a></td>
<td><a href="https://github.com/georgian-io/LLM-Finetuning-Hub/tree/main/falcon">Folder</a></td>
</tr>
<tr>
<td>RedPajama</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/georgian-io/LLM-Finetuning-Hub/blob/main/redPajama/README.md">Link</a></td>
<td><a href="https://github.com/georgian-io/LLM-Finetuning-Hub/tree/main/redPajama">Folder</a></td>
</tr>
<tr>
<td>Llama-2</td>
<td></td>
<td>✅</td>
<td></td>
<td></td>
</tr>
<tr>
<td>OpenLlama</td>
<td></td>
<td>✅</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SalesForce XGen</td>
<td></td>
<td>✅</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mosaic MPT</td>
<td></td>
<td>✅</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Cerebras</td>
<td></td>
<td>✅</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Writer Palmyra</td>
<td></td>
<td>❌</td>
<td></td>
<td></td>
</tr>
<tr>
<td>OpenAI GPT-3.5</td>
<td></td>
<td>❌</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Google PaLM</td>
<td></td>
<td>❌</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Inflection Pi</td>
<td></td>
<td>❌</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto"><a id="user-content-contributing" aria-hidden="true" tabindex="-1" href="#contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h2>
<p dir="auto">If you would like to contribute to this project, we recommend following the &#34;fork-and-pull&#34; Git workflow.</p>
<ol dir="auto">
<li><strong>Fork</strong> the repo on GitHub</li>
<li><strong>Clone</strong> the project to your own machine</li>
<li><strong>Commit</strong> changes to your own branch</li>
<li><strong>Push</strong> your work back up to your fork</li>
<li>Submit a <strong>Pull request</strong> so that we can review your changes</li>
</ol>
<p dir="auto">NOTE: Be sure to merge the latest from &#34;upstream&#34; before making a pull request!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-correspondence" aria-hidden="true" tabindex="-1" href="#correspondence"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Correspondence</h2>
<p dir="auto">If you have any questions, please reach out to:</p>
<ul dir="auto">
<li>Rohit Saha (<a href="mailto:rohit@georgian.io">Email</a> | <a href="https://www.linkedin.com/in/rohit-saha-ai/" rel="nofollow">LinkedIn</a>)</li>
<li>Kyryl Truskovskyi (<a href="mailto:kyryl@georgian.io">Email</a> | <a href="https://www.linkedin.com/in/kyryl-truskovskyi-275b7967/" rel="nofollow">LinkedIn</a>)</li>
<li>Maria Ponomarenko (<a href="mailto:mariia.ponomarenko@georgian.io">Email</a> | <a href="https://www.linkedin.com/in/maria-ponomarenko-71b465179/" rel="nofollow">LinkedIn</a>)</li>
</ul>
</article>
          </div></div>
  </body>
</html>
