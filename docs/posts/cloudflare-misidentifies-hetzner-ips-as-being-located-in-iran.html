<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gitlab.com/gitlab-com/gl-infra/production/-/issues/8121#note_1237201726">Original</a>
    <h1>Cloudflare misidentifies Hetzner IPs as being located in Iran</h1>
    
    <div id="readability-page-1" class="page">




<header data-testid="navbar">
<a href="#content-body">Skip to content</a>

</header>

<div>


<div>





<div>
<main id="content-body" itemscope="" itemtype="http://schema.org/SoftwareSourceCode">





<div>
<div>
<div data-header-actions-data="{&#34;can_create_issue&#34;:&#34;true&#34;,&#34;can_create_incident&#34;:&#34;false&#34;,&#34;can_destroy_issue&#34;:&#34;false&#34;,&#34;can_reopen_issue&#34;:&#34;false&#34;,&#34;can_report_spam&#34;:&#34;&#34;,&#34;can_update_issue&#34;:&#34;false&#34;,&#34;is_issue_author&#34;:&#34;false&#34;,&#34;issue_path&#34;:&#34;/gitlab-com/gl-infra/production/-/issues/8121&#34;,&#34;new_issue_path&#34;:&#34;/gitlab-com/gl-infra/production/-/issues/new?add_related_issue=8121&#34;,&#34;project_path&#34;:&#34;gitlab-com/gl-infra/production&#34;,&#34;report_abuse_path&#34;:&#34;/-/abuse_reports/add_category&#34;,&#34;reported_user_id&#34;:949688,&#34;reported_from_url&#34;:&#34;https://gitlab.com/gitlab-com/gl-infra/production/-/issues/8121&#34;,&#34;submit_as_spam_path&#34;:&#34;/gitlab-com/gl-infra/production/-/issues/8121/mark_as_spam&#34;,&#34;issuable_email_address&#34;:null,&#34;can_promote_to_epic&#34;:&#34;false&#34;}" data-initial="{&#34;endpoint&#34;:&#34;/gitlab-com/gl-infra/production/-/issues/8121&#34;,&#34;updateEndpoint&#34;:&#34;/gitlab-com/gl-infra/production/-/issues/8121.json&#34;,&#34;canUpdate&#34;:false,&#34;canDestroy&#34;:false,&#34;issuableRef&#34;:&#34;#8121&#34;,&#34;imported&#34;:false,&#34;markdownPreviewPath&#34;:&#34;/gitlab-com/gl-infra/production/-/preview_markdown?target_id=8121\u0026target_type=Issue&#34;,&#34;markdownDocsPath&#34;:&#34;/help/user/markdown&#34;,&#34;lockVersion&#34;:1,&#34;issuableTemplateNamesPath&#34;:&#34;/gitlab-com/gl-infra/production/description_templates/names/issue&#34;,&#34;initialTitleHtml&#34;:&#34;intermittent registry.gitlab.com client timeouts from hetzner.de VPSes&#34;,&#34;initialTitleText&#34;:&#34;intermittent registry.gitlab.com client timeouts from hetzner.de VPSes&#34;,&#34;initialDescriptionHtml&#34;:&#34;\u003cp data-sourcepos=\&#34;1:1-1:322\&#34; dir=\&#34;auto\&#34;\u003eWe&#39;ve recently (2-3 weeks?) started experiencing high failure rates in our CI build jobs \u003ca href=\&#34;https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs?statuses=FAILED\&#34;\u003ehttps://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs?statuses=FAILED\u003c/a\u003e and subjectively it seems, that it&#39;s getting worse each week, there is one such example \u003ca href=\&#34;https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs/3418282471\&#34;\u003ehttps://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs/3418282471\u003c/a\u003e.\u003c/p\u003e\u0026#x000A;\u003cdiv class=\&#34;gl-relative markdown-code-block js-markdown-code\&#34;\u003e\u0026#x000A;\u003cpre data-sourcepos=\&#34;3:1-9:3\&#34; class=\&#34;code highlight js-syntax-highlight language-plaintext\&#34; lang=\&#34;plaintext\&#34; v-pre=\&#34;true\&#34;\u003e\u003ccode\u003e\u003cspan id=\&#34;LC1\&#34; class=\&#34;line\&#34; lang=\&#34;plaintext\&#34;\u003eRunning with gitlab-runner 15.6.1 (133d7e76)\u003c/span\u003e\u0026#x000A;\u003cspan id=\&#34;LC2\&#34; class=\&#34;line\&#34; lang=\&#34;plaintext\&#34;\u003e...\u003c/span\u003e\u0026#x000A;\u003cspan id=\&#34;LC3\&#34; class=\&#34;line\&#34; lang=\&#34;plaintext\&#34;\u003ePulling docker image registry.gitlab.com/prpl-foundation/prplos/prplos/prplos/sdk-ipq40xx-generic:latest ...\u003c/span\u003e\u0026#x000A;\u003cspan id=\&#34;LC4\&#34; class=\&#34;line\&#34; lang=\&#34;plaintext\&#34;\u003eWARNING: Failed to pull image with policy \&#34;always\&#34;: Error response from daemon: Get \&#34;https://registry.gitlab.com/v2/\&#34;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) (manager.go:237:15s)\u003c/span\u003e\u0026#x000A;\u003cspan id=\&#34;LC5\&#34; class=\&#34;line\&#34; lang=\&#34;plaintext\&#34;\u003eERROR: Job failed: failed to pull image \&#34;registry.gitlab.com/prpl-foundation/prplos/prplos/prplos/sdk-ipq40xx-generic:latest\&#34; with specified policies [always]: Error response from daemon: Get \&#34;https://registry.gitlab.com/v2/\&#34;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) (manager.go:237:15s)\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u0026#x000A;\u003ccopy-code\u003e\u003c/copy-code\u003e\u0026#x000A;\u003c/div\u003e\u0026#x000A;\u003cp data-sourcepos=\&#34;11:1-11:162\&#34; dir=\&#34;auto\&#34;\u003eIt usually helped to restart the failing pipeline, but for example today it&#39;s not enough to re-run the failed pipeline for 3 times so I&#39;ve decided to report that.\u003c/p\u003e\u0026#x000A;\u003cp data-sourcepos=\&#34;13:1-13:377\&#34; dir=\&#34;auto\&#34;\u003eWe&#39;re using pool of 10 build workers, so I don&#39;t think, that it&#39;s an excessive use of resources, well, at least it does look like a timeout error and not a rate limiting. The issue seems to be related just to registry.gitlab.com service, once the container image is pulled, then there is a bunch of git clones from various gitlab.com repositories and those seems never to fail.\u003c/p\u003e&#34;,&#34;initialDescriptionText&#34;:&#34;We&#39;ve recently (2-3 weeks?) started experiencing high failure rates in our CI build jobs https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs?statuses=FAILED and subjectively it seems, that it&#39;s getting worse each week, there is one such example https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs/3418282471.\n\n```\nRunning with gitlab-runner 15.6.1 (133d7e76)\n...\nPulling docker image registry.gitlab.com/prpl-foundation/prplos/prplos/prplos/sdk-ipq40xx-generic:latest ...\nWARNING: Failed to pull image with policy \&#34;always\&#34;: Error response from daemon: Get \&#34;https://registry.gitlab.com/v2/\&#34;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) (manager.go:237:15s)\nERROR: Job failed: failed to pull image \&#34;registry.gitlab.com/prpl-foundation/prplos/prplos/prplos/sdk-ipq40xx-generic:latest\&#34; with specified policies [always]: Error response from daemon: Get \&#34;https://registry.gitlab.com/v2/\&#34;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) (manager.go:237:15s)\n```\n\nIt usually helped to restart the failing pipeline, but for example today it&#39;s not enough to re-run the failed pipeline for 3 times so I&#39;ve decided to report that.\n\nWe&#39;re using pool of 10 build workers, so I don&#39;t think, that it&#39;s an excessive use of resources, well, at least it does look like a timeout error and not a rate limiting. The issue seems to be related just to registry.gitlab.com service, once the container image is pulled, then there is a bunch of git clones from various gitlab.com repositories and those seems never to fail.&#34;,&#34;initialTaskCompletionStatus&#34;:{&#34;count&#34;:0,&#34;completed_count&#34;:0},&#34;canCreateIncident&#34;:false,&#34;fullPath&#34;:&#34;gitlab-com/gl-infra/production&#34;,&#34;iid&#34;:8121,&#34;issuableId&#34;:119791532,&#34;issueType&#34;:&#34;issue&#34;,&#34;isHidden&#34;:false,&#34;zoomMeetingUrl&#34;:null,&#34;authorId&#34;:949688,&#34;authorName&#34;:&#34;Petr Å tetiar&#34;,&#34;authorUsername&#34;:&#34;ynezz&#34;,&#34;authorWebUrl&#34;:&#34;/ynezz&#34;,&#34;createdAt&#34;:&#34;2022-12-03T10:08:33+00:00&#34;,&#34;isFirstContribution&#34;:false,&#34;serviceDeskReplyTo&#34;:null,&#34;registerPath&#34;:&#34;/users/sign_up?redirect_to_referer=yes&#34;,&#34;signInPath&#34;:&#34;/users/sign_in?redirect_to_referer=yes&#34;,&#34;publishedIncidentUrl&#34;:null,&#34;slaFeatureAvailable&#34;:&#34;false&#34;,&#34;uploadMetricsFeatureAvailable&#34;:&#34;false&#34;,&#34;projectId&#34;:7444821,&#34;projectPath&#34;:&#34;production&#34;,&#34;projectNamespace&#34;:&#34;gitlab-com/gl-infra&#34;,&#34;canAdmin&#34;:false,&#34;hasIssueWeightsFeature&#34;:true,&#34;hasIterationsFeature&#34;:true,&#34;canAdminRelation&#34;:false}" id="js-issuable-app">

<div>
<div><p data-sourcepos="1:1-1:322" dir="auto">We&#39;ve recently (2-3 weeks?) started experiencing high failure rates in our CI build jobs <a href="https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs?statuses=FAILED">https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs?statuses=FAILED</a> and subjectively it seems, that it&#39;s getting worse each week, there is one such example <a href="https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs/3418282471">https://gitlab.com/prpl-foundation/prplos/feed-prpl/-/jobs/3418282471</a>.</p>
<div>
<pre data-sourcepos="3:1-9:3" lang="plaintext" v-pre="true"><code><span id="LC1" lang="plaintext">Running with gitlab-runner 15.6.1 (133d7e76)</span>
<span id="LC2" lang="plaintext">...</span>
<span id="LC3" lang="plaintext">Pulling docker image registry.gitlab.com/prpl-foundation/prplos/prplos/prplos/sdk-ipq40xx-generic:latest ...</span>
<span id="LC4" lang="plaintext">WARNING: Failed to pull image with policy &#34;always&#34;: Error response from daemon: Get &#34;https://registry.gitlab.com/v2/&#34;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) (manager.go:237:15s)</span>
<span id="LC5" lang="plaintext">ERROR: Job failed: failed to pull image &#34;registry.gitlab.com/prpl-foundation/prplos/prplos/prplos/sdk-ipq40xx-generic:latest&#34; with specified policies [always]: Error response from daemon: Get &#34;https://registry.gitlab.com/v2/&#34;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) (manager.go:237:15s)</span></code></pre>
<copy-code></copy-code>
</div>
<p data-sourcepos="11:1-11:162" dir="auto">It usually helped to restart the failing pipeline, but for example today it&#39;s not enough to re-run the failed pipeline for 3 times so I&#39;ve decided to report that.</p>
<p data-sourcepos="13:1-13:377" dir="auto">We&#39;re using pool of 10 build workers, so I don&#39;t think, that it&#39;s an excessive use of resources, well, at least it does look like a timeout error and not a rate limiting. The issue seems to be related just to registry.gitlab.com service, once the container image is pulled, then there is a bunch of git clones from various gitlab.com repositories and those seems never to fail.</p></div>
</div>

</div>

</div>


</div>




</main>
</div>


</div>
</div>








</div>
  </body>
</html>
