<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/bigscience-workshop/petals">Original</a>
    <h1>Petals: Run 100B&#43; language models at home bit-torrent style</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/473dd9f992924d27457650251786464f72e54121ac6e9210add0f483ca849277/68747470733a2f2f692e696d6775722e636f6d2f3765523750616e2e706e67"><img src="https://camo.githubusercontent.com/473dd9f992924d27457650251786464f72e54121ac6e9210add0f483ca849277/68747470733a2f2f692e696d6775722e636f6d2f3765523750616e2e706e67" width="400" data-canonical-src="https://i.imgur.com/7eR7Pan.png"/></a></p>
<p dir="auto">Generate text using distributed BLOOM and fine-tune it for your own tasks:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from petals import DistributedBloomForCausalLM

model = DistributedBloomForCausalLM.from_pretrained(&#34;bigscience/bloom-petals&#34;, tuning_mode=&#34;ptune&#34;, pre_seq_len=16)
# Embeddings &amp; prompts are on your device, BLOOM blocks are distributed across the Internet

inputs = tokenizer(&#34;A cat sat&#34;, return_tensors=&#34;pt&#34;)[&#34;input_ids&#34;]
outputs = model.generate(inputs, max_new_tokens=5)
print(tokenizer.decode(outputs[0]))  # A cat sat on a mat...

# Fine-tuning (updates only prompts or adapters hosted locally)
optimizer = torch.optim.AdamW(model.parameters())
for input_ids, labels in data_loader:
    outputs = model.forward(input_ids)
    loss = cross_entropy(outputs.logits, labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()"><pre><span>from</span> <span>petals</span> <span>import</span> <span>DistributedBloomForCausalLM</span>

<span>model</span> <span>=</span> <span>DistributedBloomForCausalLM</span>.<span>from_pretrained</span>(<span>&#34;bigscience/bloom-petals&#34;</span>, <span>tuning_mode</span><span>=</span><span>&#34;ptune&#34;</span>, <span>pre_seq_len</span><span>=</span><span>16</span>)
<span># Embeddings &amp; prompts are on your device, BLOOM blocks are distributed across the Internet</span>

<span>inputs</span> <span>=</span> <span>tokenizer</span>(<span>&#34;A cat sat&#34;</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>)[<span>&#34;input_ids&#34;</span>]
<span>outputs</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span>, <span>max_new_tokens</span><span>=</span><span>5</span>)
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>outputs</span>[<span>0</span>]))  <span># A cat sat on a mat...</span>

<span># Fine-tuning (updates only prompts or adapters hosted locally)</span>
<span>optimizer</span> <span>=</span> <span>torch</span>.<span>optim</span>.<span>AdamW</span>(<span>model</span>.<span>parameters</span>())
<span>for</span> <span>input_ids</span>, <span>labels</span> <span>in</span> <span>data_loader</span>:
    <span>outputs</span> <span>=</span> <span>model</span>.<span>forward</span>(<span>input_ids</span>)
    <span>loss</span> <span>=</span> <span>cross_entropy</span>(<span>outputs</span>.<span>logits</span>, <span>labels</span>)
    <span>optimizer</span>.<span>zero_grad</span>()
    <span>loss</span>.<span>backward</span>()
    <span>optimizer</span>.<span>step</span>()</pre></div>
<p dir="auto">
    <g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">üöÄ</g-emoji> ¬†<b><a href="https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing" rel="nofollow">Try now in Colab</a></b>
</p>
<p dir="auto">Connect your own GPU and increase Petals capacity:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# In an Anaconda env
conda install pytorch cudatoolkit=11.3 -c pytorch
pip install -U petals
python -m petals.cli.run_server bigscience/bloom-petals

# Or using our GPU-enabled Docker image
sudo docker run --net host --ipc host --gpus all --volume petals-cache:/cache --rm \
    learningathome/petals:main python -m petals.cli.run_server bigscience/bloom-petals"><pre><span><span>#</span> In an Anaconda env</span>
conda install pytorch cudatoolkit=11.3 -c pytorch
pip install -U petals
python -m petals.cli.run_server bigscience/bloom-petals

<span><span>#</span> Or using our GPU-enabled Docker image</span>
sudo docker run --net host --ipc host --gpus all --volume petals-cache:/cache --rm \
    learningathome/petals:main python -m petals.cli.run_server bigscience/bloom-petals</pre></div>
<p dir="auto"><g-emoji alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png">üí¨</g-emoji> If you have any issues or feedback, please join <a href="https://discord.gg/D9MwApKgWa" rel="nofollow">our Discord server</a>!</p>
<p dir="auto">Check out more examples and tutorials:</p>
<ul dir="auto">
<li>Chatbot web app: <a href="http://chat.petals.ml" rel="nofollow">link</a>, <a href="https://github.com/borzunov/petals-chat">source code</a></li>
<li>Training a personified chatbot: <a href="https://github.com/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb">notebook</a></li>
<li>Fine-tuning BLOOM for text semantic classification: <a href="https://github.com/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb">notebook</a></li>
<li>Launching your own swarm: <a href="https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm">tutorial</a></li>
<li>Running a custom foundation model: <a href="https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals">tutorial</a></li>
</ul>
<h2 dir="auto"><a id="user-content-how-does-it-work" aria-hidden="true" href="#how-does-it-work"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How does it work?</h2>
<ul dir="auto">
<li>Petals runs large language models like BLOOM-176B <strong>collaboratively</strong> ‚Äî you load a small part of the model, then team up with people serving the other parts to run inference or fine-tuning.</li>
<li>Inference runs at ‚âà 1 sec per step (token) ‚Äî 10x faster than possible with offloading, enough for chatbots and other interactive apps. Parallel inference reaches hundreds of tokens/sec.</li>
<li>Beyond classic language model APIs ‚Äî you can employ any fine-tuning and sampling methods by executing custom paths through the model or accessing its hidden states. You get the comforts of an API with the flexibility of PyTorch.</li>
</ul>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/58732a64488a9be928e25f3e60e3692b989ffe212ac86cb4902d8df20a042b03/68747470733a2f2f692e696d6775722e636f6d2f525459463379572e706e67"><img src="https://camo.githubusercontent.com/58732a64488a9be928e25f3e60e3692b989ffe212ac86cb4902d8df20a042b03/68747470733a2f2f692e696d6775722e636f6d2f525459463379572e706e67" width="800" data-canonical-src="https://i.imgur.com/RTYF3yW.png"/></a>
</p>
<p dir="auto">
    <g-emoji alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png">üìú</g-emoji> ¬†<b><a href="https://arxiv.org/pdf/2209.01188.pdf" rel="nofollow">Read paper</a></b>
</p>
<h3 dir="auto"><a id="user-content--privacy-and-security" aria-hidden="true" href="#-privacy-and-security"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><g-emoji alias="lock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f512.png">üîí</g-emoji> Privacy and security</h3>
<p dir="auto">The Petals public swarm is designed for research and academic use. <strong>Please do not use the public swarm to process sensitive data.</strong> We ask for that because it is an open network, and it is technically possible for peers serving model layers to recover input data and model outputs or modify them in a malicious way. Instead, you can <a href="https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm">set up a private Petals swarm</a> hosted by people and organization you trust, who are authorized to process your data. We discuss privacy and security in more detail <a href="https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety">here</a>.</p>
<h3 dir="auto"><a id="user-content--models-terms-of-use" aria-hidden="true" href="#-models-terms-of-use"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><g-emoji alias="clipboard" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png">üìã</g-emoji> Model&#39;s terms of use</h3>
<p dir="auto">Before building your own application that runs a language model with Petals, please check out the model&#39;s <strong>terms of use, risks, and limitations</strong>. In case of BLOOM, they are described in its <a href="https://huggingface.co/bigscience/bloom" rel="nofollow">model card</a> and <a href="https://huggingface.co/spaces/bigscience/license" rel="nofollow">license</a>.</p>
<h2 dir="auto"><a id="user-content-faq" aria-hidden="true" href="#faq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>FAQ</h2>
<ol dir="auto">
<li>
<p dir="auto"><strong>What&#39;s the motivation for people to host model layers in the public swarm?</strong></p>
<p dir="auto">People who run inference and fine-tuning themselves get a certain speedup if they host a part of the model locally. Some may be also motivated to &#34;give back&#34; to the community helping them to run the model (similarly to how <a href="https://en.wikipedia.org/wiki/BitTorrent" rel="nofollow">BitTorrent</a> users help others by sharing data they have already downloaded).</p>
<p dir="auto">Since it may be not enough for everyone, we are also working on introducing explicit <strong>incentives</strong> (&#34;bloom points&#34;) for people donating their GPU time to the public swarm. Once this system is ready, people who earned these points will be able to spend them on inference/fine-tuning with higher priority or increased security guarantees, or (maybe) exchange them for other rewards.</p>
</li>
<li>
<p dir="auto"><strong>Why is the platform named &#34;Petals&#34;?</strong></p>
<p dir="auto">&#34;Petals&#34; is a metaphor for people serving different parts of the model. Together, they host the entire language model ‚Äî <a href="https://huggingface.co/bigscience/bloom" rel="nofollow">BLOOM</a>.</p>
<p dir="auto">While our platform focuses on BLOOM now, we aim to support more <a href="https://arxiv.org/abs/2108.07258" rel="nofollow">foundation models</a> in future.</p>
</li>
</ol>
<h2 dir="auto"><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<p dir="auto">Here&#39;s how to install Petals with conda:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda install pytorch cudatoolkit=11.3 -c pytorch
pip install -U petals"><pre>conda install pytorch cudatoolkit=11.3 -c pytorch
pip install -U petals</pre></div>
<p dir="auto">This script uses Anaconda to install CUDA-enabled PyTorch.
If you don&#39;t have anaconda, you can get it from <a href="https://www.anaconda.com/products/distribution" rel="nofollow">here</a>.
If you don&#39;t want anaconda, you can install PyTorch <a href="https://pytorch.org/get-started/locally/" rel="nofollow">any other way</a>.
If you want to run models with 8-bit weights, please install <strong>PyTorch with CUDA 11</strong> or newer for compatility with <a href="https://github.com/timDettmers/bitsandbytes">bitsandbytes</a>.</p>
<p dir="auto"><strong>System requirements:</strong> Petals only supports Linux for now. If you don&#39;t have a Linux machine, consider running Petals in Docker (see our <a href="https://hub.docker.com/r/learningathome/petals" rel="nofollow">image</a>) or, in case of Windows, in WSL2 (<a href="https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl" rel="nofollow">read more</a>). CPU is enough to run a client, but you probably need a GPU to run a server efficiently.</p>
<h2 dir="auto"><a id="user-content-Ô∏è-development" aria-hidden="true" href="#Ô∏è-development"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><g-emoji alias="hammer_and_wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png">üõ†Ô∏è</g-emoji> Development</h2>
<p dir="auto">Petals uses pytest with a few plugins. To install them, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda install pytorch cudatoolkit=11.3 -c pytorch
git clone https://github.com/bigscience-workshop/petals.git &amp;&amp; cd petals
pip install -e .[dev]"><pre>conda install pytorch cudatoolkit=11.3 -c pytorch
git clone https://github.com/bigscience-workshop/petals.git <span>&amp;&amp;</span> <span>cd</span> petals
pip install -e .[dev]</pre></div>
<p dir="auto">To run minimalistic tests, you need to make a local swarm with a small model and some servers. You may find more information about how local swarms work and how to run them in <a href="https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm">this tutorial</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export MODEL_NAME=bloom-testing/test-bloomd-560m-main

python -m petals.cli.run_server $MODEL_NAME --block_indices 0:12 \
  --identity tests/test.id --host_maddrs /ip4/127.0.0.1/tcp/31337 --new_swarm  &amp;&gt; server1.log &amp;
sleep 5  # wait for the first server to initialize DHT

python -m petals.cli.run_server $MODEL_NAME --block_indices 12:24 \
  --initial_peers SEE_THE_OUTPUT_OF_THE_1ST_PEER &amp;&gt; server2.log &amp;

tail -f server1.log server2.log  # view logs for both servers"><pre><span>export</span> MODEL_NAME=bloom-testing/test-bloomd-560m-main

python -m petals.cli.run_server <span>$MODEL_NAME</span> --block_indices 0:12 \
  --identity tests/test.id --host_maddrs /ip4/127.0.0.1/tcp/31337 --new_swarm  <span>&amp;</span><span>&gt;</span> server1.log <span>&amp;</span>
sleep 5  <span><span>#</span> wait for the first server to initialize DHT</span>

python -m petals.cli.run_server <span>$MODEL_NAME</span> --block_indices 12:24 \
  --initial_peers SEE_THE_OUTPUT_OF_THE_1ST_PEER <span>&amp;</span><span>&gt;</span> server2.log <span>&amp;</span>

tail -f server1.log server2.log  <span><span>#</span> view logs for both servers</span></pre></div>
<p dir="auto">Then launch pytest:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export MODEL_NAME=bloom-testing/test-bloomd-560m-main REF_NAME=bigscience/bloom-560m
export INITIAL_PEERS=/ip4/127.0.0.1/tcp/31337/p2p/QmS9KwZptnVdB9FFV7uGgaTq4sEKBwcYeKZDfSpyKDUd1g
PYTHONPATH=. pytest tests --durations=0 --durations-min=1.0 -v"><pre><span>export</span> MODEL_NAME=bloom-testing/test-bloomd-560m-main REF_NAME=bigscience/bloom-560m
<span>export</span> INITIAL_PEERS=/ip4/127.0.0.1/tcp/31337/p2p/QmS9KwZptnVdB9FFV7uGgaTq4sEKBwcYeKZDfSpyKDUd1g
PYTHONPATH=. pytest tests --durations=0 --durations-min=1.0 -v</pre></div>
<p dir="auto">After you&#39;re done, you can terminate the servers and ensure that no zombie processes are left with <code>pkill -f petals.cli.run_server &amp;&amp; pkill -f p2p</code>.</p>
<p dir="auto">The automated tests use a more complex server configuration that can be found <a href="https://github.com/bigscience-workshop/petals/blob/main/.github/workflows/run-tests.yaml">here</a>.</p>
<h3 dir="auto"><a id="user-content-code-style" aria-hidden="true" href="#code-style"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Code style</h3>
<p dir="auto">We use <a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html" rel="nofollow">black</a> and <a href="https://pycqa.github.io/isort/" rel="nofollow">isort</a> for all pull requests.
Before committing your code, simply run <code>black . &amp;&amp; isort .</code> and you will be fine.</p>
<hr/>
<p dir="auto">
    This project is a part of the <a href="https://bigscience.huggingface.co/" rel="nofollow">BigScience</a> research workshop.
</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d7c42a6fe6c109c9cba63d27deb58ffd535ae3a4ac371c459c10a75c99359525/68747470733a2f2f706574616c732e6d6c2f626967736369656e63652e706e67"><img src="https://camo.githubusercontent.com/d7c42a6fe6c109c9cba63d27deb58ffd535ae3a4ac371c459c10a75c99359525/68747470733a2f2f706574616c732e6d6c2f626967736369656e63652e706e67" width="150" data-canonical-src="https://petals.ml/bigscience.png"/></a>
</p>
</article>
          </div></div>
  </body>
</html>
