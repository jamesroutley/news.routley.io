<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html">Original</a>
    <h1>Io_uring, kTLS and Rust for zero syscall HTTPS server</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">


  

  <div itemprop="articleBody">
    <p>Around the turn of the century we started to get a bigger need for high
capacity web servers. For example there was <a href="https://en.wikipedia.org/wiki/C10k_problem">the C10k problem</a> paper.</p>

<p>At the time, the kinds of things done to reduce work done per request was
pre-forking the web server. This means a request could be handled without an
expensive process creation.</p>

<p>Because yes, creating a new process for every request used to be something
perfectly normal.</p>

<p>Things did get better. People learned how to create threads, making things more
light weight. Then they switched to using <code>poll()</code>/<code>select()</code>, in order to not
just spare the process/thread creation, but the whole context switch.</p>

<p>I remember a comment on <a href="https://en.wikipedia.org/wiki/Kuro5hin">Kuro5hin</a> from anakata, the creator of both The
Pirate Bay and the web server that powered it, along the lines of “I am select()
of borg, resistance is futile”, mocking someone for not understanding how to
write a scalable web server.</p>

<p>But <code>select()</code>/<code>poll()</code> also doesn’t scale. If you have ten thousand
connections, that’s an array of ten thousand integers that need to be sent to
the kernel for every single iteration of your request handling loop.</p>

<p>Enter <code>epoll</code> (<code>kqueue</code> on other operating systems, but I’m focusing on Linux
here). Now that’s better. The main loop is now:</p>

<div><pre><code>  set_up_epoll()
  while True:
    new, read, write = epoll()
    epoll_add_connections(new)
    for con in read:
      process(con.read())
      if con.read_all_we_need:
        epoll_remove_read_op(con)
    for con in write:
      con.write_buffer()
      if con.buffer_empty:
        epoll_remove_write_op(con)
</code></pre>
</div>

<p>All the syscalls are pretty cheap. <code>epoll()</code> only deals in deltas, and it
doesn’t have to be re-told the thousands of active connections.</p>

<p>But they’re not without cost. Once we’ve gotten this far, the cost of a syscall
is actually a significant part of the total remaining cost.</p>

<p>We’re here going to ignore improvements like <code>sendfile()</code> and <code>splice()</code>, and
instead jump to…</p>

<h2 id="iouring">io_uring</h2>

<p>Instead of performing a syscall for everything we want to do, commanding the
kernel to do this or that, io_uring lets us just keep writing orders to a
queue, and letting the kernel consume that queue asynchronously.</p>

<p>For example, we can put <code>accept()</code> into the queue. The kernel will pick that
up, wait for an incoming connection, and when it arrives it’ll put a
“completion” into the completion queue.</p>

<p>The web server can then check the completion queue. If there’s a completion
there, it can act on it.</p>

<p>This way the web server can queue up all kinds of operations that were
previously “expensive” syscalls by simply writing them to memory. That’s it.
And then it’ll read the results from another part of memory. That’s it.</p>

<p>In order to avoid busy looping, both the kernel and the web server will only
busy-loop checking the queue for a little bit (configurable, but think
milliseconds), and if there’s nothing new, the web server will do a syscall to
“go to sleep” until something gets added to the queue.</p>

<p>Similarly on the kernel side, the kernel will stop busy-looping if there’s
nothing new, and needs a syscall to start busylooping again.</p>

<p>This sounds like it would be tricky to optimize, but it’s not. In the end the
web server just puts stuff on the queue, and calls a library function that only
does that syscall if the kernel actually has stopped busylooping.</p>

<p>This means that a busy web server can serve all of its queries without even once
(after setup is done) needing to do a syscall. As long as queues keep getting
added to, <code>strace</code> will show <em>nothing</em>.</p>

<h2 id="one-thread-per-core">One thread per core</h2>

<p>Since CPUs today have many cores, ideally you want to run exactly one thread
per core, bind it to that core, and not share any read-write data structure.</p>

<p>For <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> hardware, you also want to make sure that a thread only
accesses memory on the local NUMA node. <a href="https://youtu.be/36qZYL5RlgY">This netflix talk</a> has some
interesting stuff on NUMA and high volume HTTP delivery.</p>

<p>The request load will still not be perfectly balanced between the threads (and
therefore cores), but I guess fixing that would have to be the topic of a
future post.</p>

<h2 id="memory-allocations">Memory allocations</h2>

<p>We will still have memory allocations though, both on the kernel and web server
side. Memory allocations in user space will eventually need syscalls.</p>

<p>For the web server side, you can pre-allocate a fixed chunk for every
connection, and then have everything about that connection live there. That way
new connections don’t need syscalls, memory doesn’t get fragmented, and you
don’t run the risk of running out of memory.</p>

<p>On the kernel side each connection will still need buffers for incoming and
outgoing bytes. This may be somewhat controllable via socket options, but again
it’ll have to be the subject of a future post.</p>

<p>Try to not run out of RAM. Bad things tend to happen.</p>

<h2 id="ktls">kTLS</h2>

<p><a href="https://docs.kernel.org/networking/tls-offload.html">kTLS</a> is a feature of the Linux kernel where an application can hand off
the job of encryption/decryption to the kernel. The application still has to
perform the TLS handshake, but after that it can enable kTLS and pretend that
it’s all sent in plaintext.</p>

<p>You may say that this doesn’t actually speed anything up, it just moves <em>where</em>
encryption was done. But there are gains:</p>

<ol>
  <li>This means that <code>sendfile()</code> can be used, removing the need to copy a bunch
of data between user space and kernel space.</li>
  <li>If the network card has hardware support for it, the crypto operation may
actually be offloaded from the CPU onto the network card, leaving the CPU to
do better things.</li>
</ol>

<h2 id="descriptorless-files">Descriptorless files</h2>

<p>Another optimization is to avoid passing file descriptors back and forth
between user space and kernel space. The mapping between file descriptors and
io_uring apparently has overhead.</p>

<p>So in comes <a href="https://lwn.net/Articles/863071/">descriptorless files</a> via
<a href="https://docs.rs/io-uring/latest/io_uring/struct.Submitter.html#method.register_files"><code>register_files</code></a>.</p>

<p>Now the supposed file descriptor numbers that user space sees are just
integers. They don’t show up in <code>/proc/pid/fd</code>, and can only be used with
io_uring. They’re still capped by the <code>ulimit</code> file descriptor limit, though.</p>

<h2 id="tarweb">tarweb</h2>

<p>In order to learn these technologies better, I built <a href="https://github.com/ThomasHabets/tarweb">a web server
incorporating all these things</a>.</p>

<p>It’s named <code>tarweb</code> because it’s a web server that serves the content of a
single tar file.</p>

<p>Rust, io_uring, and kTLS. Not exactly the most common combination. I found that
io_uring and kTLS didn’t play super well together. Enabling kTLS requires three
<code>setsockopt()</code> calls, and io_uring doesn’t support <code>setsockopt</code> (until they
merge <a href="https://github.com/tokio-rs/io-uring/pull/320">my PR</a>, that is).</p>

<p>And the <code>ktls</code> crate, part of <code>rustls</code>, only allows you to call the synchronous
<code>setsockopt()</code>, not export the needed struct for me to pass to my new
io_uring <code>setsockopt</code>. <a href="https://github.com/rustls/ktls/pull/54">Another pr sent</a>.</p>

<p>So with those two PRs merged, it’s working great.</p>

<p>tarweb is far from perfect. The code needs a lot of work, and there’s no
guarantee that the TLS library (rustls) doesn’t do memory allocations during
handshakes. But it does serve https without even one syscall on a per request
basis. And that’s pretty cool.</p>

<h2 id="benchmarks">Benchmarks</h2>

<p>I have not done any benchmarks yet. I want to clean the code up first.</p>

<h2 id="io-uring-and-safety">io-uring and safety</h2>

<p>One thing making io_uring more complex than synchronous syscalls is that any
buffer needs to stay in memory until the operation is marked completed by
showing up in the completion queue.</p>

<p>For example when submitting a <code>write</code> operation, the memory location of those
bytes must not be deallocated or overwritten.</p>

<p>The <code>io-uring</code> crate doesn’t help much with this. The API doesn’t allow the
borrow checker to protect you at compile time, and I don’t see it doing any
runtime checks either.</p>

<p>I feel like I’m back in C++, where any mistake can blow your whole leg off.
It’s a miracle that I’ve not seen a segfault.</p>

<p>Someone should make a <code>safer-ring</code> crate or similar, using the powers of
<a href="https://blog.cloudflare.com/pin-and-unpin-in-rust/">pinning</a> and/or borrows or something, to achieve Rust’s normal “if it
compiles, then it’s correct”.</p>


  </div>

  
  
  
  
  
  
</article>

      </div>
    </div></div>
  </body>
</html>
