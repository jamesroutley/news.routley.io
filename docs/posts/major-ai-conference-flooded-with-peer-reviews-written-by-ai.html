<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/d41586-025-03506-6">Original</a>
    <h1>Major AI conference flooded with peer reviews written by AI</h1>
    
    <div id="readability-page-1" class="page"><div data-test="access-teaser"> <figure><picture><source type="image/webp" srcset="//media.nature.com/lw767/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg?as=webp 767w, //media.nature.com/lw319/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"/><img alt="Close up view of a red toy robot sat amongst a stack of books." loading="lazy" src="http://media.nature.com/lw767/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg"/><figcaption><p><span>An AI-detection tool developed by Pangram labs found that peer reviewers are increasingly using chatbots to draft responses to authors.</span><span>Credit: breakermaximus/iStock via Getty</span></p></figcaption></picture></figure><p>What can researchers do if they suspect that their manuscripts have been peer reviewed using artificial intelligence (AI)? Dozens of academics have raised concerns on social media about manuscripts and peer reviews submitted to the organizers of next year’s International Conference on Learning Representations (ICLR), an annual gathering of specialists in machine learning. Among other things, they flagged <a href="https://www.nature.com/articles/d41586-025-02853-8" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02853-8" data-track-category="body text link">hallucinated citations</a> and suspiciously long and vague feedback on their work.</p><p>Graham Neubig, an AI researcher at Carnegie Mellon University in Pittsburgh, Pennsylvania, was one of those who received peer reviews that seemed to have been<a href="https://www.nature.com/articles/d41586-025-03390-0" data-track="click" data-label="https://www.nature.com/articles/d41586-025-03390-0" data-track-category="body text link"> produced using large language models (LLMs)</a>. The reports, he says, were “very verbose with lots of bullet points” and requested analyses that were not “the standard statistical analyses that reviewers ask for in typical AI or machine-learning papers.”</p><p>But Neubig needed help proving that the reports were AI-generated. So, he posted on X (formerly Twitter) and offered a reward for anyone who could scan all the conference submissions and their peer reviews for AI-generated text. The next day, he got a response from Max Spero, chief executive of Pangram Labs in New York City, which develops tools to detect AI-generated text. Pangram screened all 19,490 studies and 75,800 peer reviews submitted for ICLR 2026, which will take place in Rio de Janeiro, Brazil, in April. Neubig and more than 11,000 other AI researchers will be attending.</p><p>Pangram’s analysis revealed that around 21% of the ICLR peer reviews were fully AI-generated, and more than half contained signs of AI use. The findings were <a href="https://www.pangram.com/blog/pangram-predicts-21-of-iclr-reviews-are-ai-generated" data-track="click" data-label="https://www.pangram.com/blog/pangram-predicts-21-of-iclr-reviews-are-ai-generated" data-track-category="body text link">posted online by Pangram Labs</a>. “People were suspicious, but they didn’t have any concrete proof,” says Spero. “Over the course of 12 hours, we wrote some code to parse out all of the text content from these paper submissions,” he adds. </p><p>The conference organizers say they will now use automated tools to assess whether submissions and peer reviews breached policies on using <a href="https://www.nature.com/articles/d41586-025-01839-w" data-track="click" data-label="https://www.nature.com/articles/d41586-025-01839-w" data-track-category="body text link">AI in submissions and peer reviews</a>. This is the first time that the conference has faced this issue at scale, says Bharath Hariharan, a computer scientist at Cornell University in Ithaca, New York, and senior programme chair for ICLR 2026. “After we go through all this process … that will give us a better notion of trust.” </p><h2>AI-written peer review</h2><p>The Pangram team used one of its own tools, which <a href="https://www.nature.com/articles/d41586-025-02936-6" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02936-6" data-track-category="body text link">predicts whether text is generated or edited by LLMs</a>. Pangram’s analysis flagged 15,899 peer reviews that were fully AI-generated. But it also identified many manuscripts that had been submitted to the conference with suspected cases of AI-generated text: 199 manuscripts (1%) were found to be fully AI-generated; 61% of submissions were mostly human-written; but 9% contained more than 50% AI-generated text. </p><p>Pangram described the model in a preprint<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>, which it submitted to ICLR 2026. Of the four peer reviews received for the manuscript, one was flagged as fully AI-generated and another as lightly AI-edited, the team’s analysis found.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-025-00894-7" data-track="click" data-track-label="recommended article"><img alt="" src="http://media.nature.com/w400/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_50959900.jpg"/><p>AI is transforming peer review — and many scientists are worried</p></a></article><p>For many researchers who received peer reviews for their submissions to ICLR, the Pangram analysis confirmed what they had suspected. Desmond Elliott, a computer scientist at the University of Copenhagen, says that one of three reviews he received seemed to have missed “the point of the paper”. His PhD student who led the work suspected that the review was generated by LLMs, because it mentioned numerical results from the manuscript that were incorrect and contained odd expressions. </p><p>When Pangram released its findings, Elliott adds, “the first thing I did was I typed in the title of our paper because I wanted to know whether my student’s gut instinct was correct”. The suspect peer review, which Pangram’s analysis flagged as fully AI-generated, gave the manuscript the lowest rating, leaving it “on the borderline between accept and reject”, says Elliott. “It&#39;s deeply frustrating”.</p><h2>Repercussions</h2></div></div>
  </body>
</html>
