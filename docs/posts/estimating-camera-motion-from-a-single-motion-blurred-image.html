<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jerredchen.github.io/image-as-imu/">Original</a>
    <h1>Estimating Camera Motion from a Single Motion-Blurred Image</h1>
    
    <div id="readability-page-1" class="page">

  <section>
    
</section>


<!-- Teaser video-->
<section>
  <div>
    <p>
      <video poster="" preload="auto" id="tree" playsinline="" autoplay="" muted="" loop="" width="100%">
        <!-- Your video here -->
        <source src="static/videos/teaser_web.mp4" type="video/mp4"/>
      </video>
      <h2>
        Given a single motion-blurred image, we exploit the motion blur cues to predict the camera velocity at that instant <i> without performing any deblurring</i>.
      </h2>
    </p>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Motivation -->
<section>
  </section>
<!-- End of motivation -->

<!-- Method -->
<section>
  <div>
    <div>
      <h2>Method Overview</h2>
      <div>
        <div>
          <p><img src="https://jerredchen.github.io/image-as-imu/static/images/blur_method.jpg" alt="Method Overview"/>
          </p>
          <p> 
              Given a single image, our method predicts a dense motion flow field and a monocular depth map. 
              We then recover the instantaneous camera velocity with linear least squares using the known exposure time and intrinsics. 
              To disambiguate the velocity direction in a video, we use the predicted motion to compute the photometric error between the current frame and the previous/next frames and flip the direction if necessary.
            </p>
        </div>
      </div>
    </div>
  </div>
  <br/>
</section>
<!-- End of method -->

<!-- Evaluation Setup -->
<section>
  </section>
<!-- End of evaluation setup -->


<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Experimental Results</h2>
      <div>
        <div>
          <p>
              We evaluate our method on real-world motion-blurred videos. While the baseline methods must use multiple frames to compute the velocity, our network only takes a <i>single frame</i> as input. 
              Because the true direction of a single motion-blurred image is ambiguous, we flip the velocity direction as necessary based on the photometric error between frames. 
              We directly treat the gyroscope readings as the angular velocity ground truth, and we approximate the translational velocity ground truth using the ARKit poses and framerate. 
              Note that the angular velocity axes are x-up, y-left, z-backwards (using the IMU convention) whereas the the translational velocity axes are x-right, y-down, z-forward (using OpenCV convention).
            </p>
        </div>
      </div>
      
      
    </div>
  </div>
  
</section>
<!-- End video carousel -->


<!-- Quantitative Results -->
<section>
  <div>
    <div>
      <h2>Quantitative Results</h2>
      <div>
        <div>
          <p><img src="https://jerredchen.github.io/image-as-imu/static/images/tables.png" alt="Table Results"/>
          </p>
        </div>
      </div>
    </div>
  </div>
  <br/>
</section>
<!-- End of quantitative results -->

<!-- Animation. -->
<div>
  <div>
    <h3>Frame-by-Frame Inference</h3>
    <p>
        Our network uses a single frame as input <i>in complete isolation from the rest of the video</i>. 
        Try the slider to see how our method recovers the camera motion at a particular frame!
      </p>
    
    <br/>

  </div>
</div>
<!--/ Animation. -->

<!-- Runtime -->
<section>
  <div>
    <h2>Runtime Comparison</h2>
      <video poster="" id="tree" playsinline="" autoplay="" muted="" loop="" width="100%">
        <!-- Your video here -->
        <source src="static/videos/runtime_web.mp4" type="video/mp4"/>
      </video>
      <p> 
          We show the runtimes of our method against the baselines on one of the real-world videos. All methods are run on an Nvidia RTX 3090. 
          Even including the direction disambiguation, our method is significantly faster and runs in real-time at 30 Hz. 
        </p>
  </div>
</section>
<!-- End of runtime -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@article{chen2025_imageimu,
  title     = {{Image as an IMU}: Estimating Camera Motion from a Single Motion-Blurred Image},
  author    = {Chen, Jerred and Clark, Ronald},
  journal   = {arXiv preprint},
  year      = {2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
