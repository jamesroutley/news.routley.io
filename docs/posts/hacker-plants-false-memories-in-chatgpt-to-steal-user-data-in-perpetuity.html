<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arstechnica.com/security/2024/09/false-memories-planted-in-chatgpt-give-hacker-persistent-exfiltration-channel/">Original</a>
    <h1>Hacker plants false memories in ChatGPT to steal user data in perpetuity</h1>
    
    <div id="readability-page-1" class="page"><div>
            <h4>
      MEMORY PROBLEMS    —
</h4>
            
            <h2 itemprop="description">Emails, documents, and other untrusted content can plant malicious memories.</h2>
            <section>

  


  
</section>        </div><section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/memory-800x560.jpg" alt="Hacker plants false memories in ChatGPT to steal user data in perpetuity"/>
      <figcaption><p>Getty Images</p></figcaption>  </figure>

  




<!-- cache hit 115:single/related:94447731f52e28b62a22ce87bc7a6495 --><!-- empty -->
<p>When security researcher Johann Rehberger recently reported a vulnerability in ChatGPT that allowed attackers to store false information and malicious instructions in a user’s long-term memory settings, OpenAI summarily closed the inquiry, labeling the flaw a safety issue, not, technically speaking, a security concern.</p>
<p>So Rehberger did what all good researchers do: He created a proof-of-concept exploit that used the vulnerability to exfiltrate all user input in perpetuity. OpenAI engineers took notice and issued a partial fix earlier this month.</p>
<h2>Strolling down memory lane</h2>
<p>The vulnerability abused long-term conversation memory, a feature OpenAI began testing <a href="https://arstechnica.com/information-technology/2024/02/amnesia-begone-soon-chatgpt-will-remember-what-you-tell-it-between-sessions/">in February</a> and made more broadly available <a href="https://openai.com/index/memory-and-new-controls-for-chatgpt/">in September</a>. Memory with ChatGPT stores information from previous conversations and uses it as context in all future conversations. That way, the LLM can be aware of details such as a user’s age, gender, philosophical beliefs, and pretty much anything else, so those details don’t have to be inputted during each conversation.
</p><p>Within three months of the rollout, Rehberger <a href="https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/">found</a> that memories could be created and permanently stored through indirect <a href="https://arstechnica.com/information-technology/2022/09/twitter-pranksters-derail-gpt-3-bot-with-newly-discovered-prompt-injection-hack/">prompt injection</a>, an AI exploit that causes an LLM to follow instructions from untrusted content such as emails, blog posts, or documents. The researcher demonstrated how he could trick ChatGPT into believing a targeted user was 102 years old, lived in the Matrix, and insisted Earth was flat and the LLM would incorporate that information to steer all future conversations. These false memories could be planted by storing files in Google Drive or Microsoft OneDrive, uploading images, or browsing a site like Bing—all of which could be created by a malicious attacker.</p>                                                                        
                                                                                
<p>Rehberger privately reported the finding to OpenAI in May. That same month, the company closed the report ticket. A month later, the researcher submitted a new disclosure statement. This time, he included a PoC that caused the ChatGPT app for macOS to send a verbatim copy of all user input and ChatGPT output to a server of his choice. All a target needed to do was instruct the LLM to view a web link that hosted a malicious image. From then on, all input and output to and from ChatGPT was sent to the attacker&#39;s website.</p>
<figure><p><iframe type="text/html" width="560" height="315" src="https://www.youtube.com/embed/zb0q5AW5ns8?si=N_v0kYZbQPETRNF0?start=0&amp;wmode=transparent" frameborder="0" allowfullscreen=""></iframe></p><figcaption></figcaption></figure>
<p>“What is really interesting is this is memory-persistent now,” Rehberger said in the above video demo. “The prompt injection inserted a memory into ChatGPT’s long-term storage. When you start a new conversation, it actually is still exfiltrating the data.”</p>
<p>The attack isn’t possible through the ChatGPT web interface, thanks to an API OpenAI rolled out <a href="https://embracethered.com/blog/posts/2023/openai-data-exfiltration-first-mitigations-implemented/">last year</a>.</p>
<p>While OpenAI has introduced a fix that prevents memories from being abused as an exfiltration vector, the researcher said, untrusted content can still perform prompt injections that cause the memory tool to store long-term information planted by a malicious attacker.</p>
<p>LLM users who want to prevent this form of attack should pay close attention during sessions for output that indicates a new memory has been added. They should also regularly review stored memories for anything that may have been planted by untrusted sources. OpenAI provides guidance <a href="https://openai.com/index/memory-and-new-controls-for-chatgpt/">here</a> for managing the memory tool and specific memories stored in it. Company representatives didn’t respond to an email asking about its efforts to prevent other hacks that plant false memories.</p>

                                                </div>

            
            
            
        </section></div>
  </body>
</html>
