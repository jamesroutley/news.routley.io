<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Apr/12/code-interpreter/">Original</a>
    <h1>Running Python micro-benchmarks using the ChatGPT Code Interpreter alpha</h1>
    
    <div id="readability-page-1" class="page"><div>




<p>Today I wanted to understand the performance difference between two Python implementations of a mechanism to detect changes to a SQLite database schema. I rendered the difference between the two as this chart:</p>
<p><img src="https://static.simonwillison.net/static/2023/sqlite-md5-benchmark.jpg" alt="Benchmark Results Over Time - two lines, one for PRAGMA schema_version and one for hashlib.md5. The axis are time in seconds and number of tables - the hashlib.md5 line climes from around 0.000075 to 0.000150 after 200 tables have been created, while the PRAGMA schema_version line stays almost flat at 0.000050 at the bottom of the chart. It&#39;s a good chart: well labelled, easy to read."/></p>
<p>From start to finish, this entire benchmarking exercise took me less than five minutes—because ChatGPT did almost all of the work for me.</p>
<h4>ChatGPT Code Interpreter alpha</h4>
<p>I ran the benchmark using the new ChatGPT “Code Interpreter” alpha, which I recently gained access to, presumably due to being in the alpha <a href="https://simonwillison.net/2023/Mar/24/datasette-chatgpt-plugin/">for ChatGPT Plugins</a>.</p>
<p>Code Interpreter mode provides ChatGPT with a single additional tool: it can now generate Python code and execute it in a restricted sandbox. The sandbox does not have network access and only has access to a set of pre-approved libraries. One of those libraries is <code>matplotlib.pyplot</code>, and it knows how to both render and display charts using it.</p>
<p>Here’s the initial prompt I sent it:</p>
<blockquote>
<p>In Python create an in-memory SQLite database with 100 tables each with 10 columns.</p>
<p>Time how long it takes to execute <code>PRAGMA schema_version</code> against that database 100 times.</p>
<p>Then add another 100 tables and time PRAGMA schema_version 100 times again.</p>
<p>Now run the whole benchmark a second time, but instead of <code>PRAGMA schema_version</code> time how long it takes to run hashlib.md5(db.execute(“select group_concat(sql) from sqlite_master”).fetchall()[0]).hexdigest() instead</p>
</blockquote>
<h4>Background—why compare these two things?</h4>
<p>A bit of background on this. <a href="https://datasette.io/">Datasette</a> needs to detect any time a SQLite schema changes—which could happen if a table is added or altered in some way. It needs to know so it can update its own <a href="https://docs.datasette.io/en/stable/internals.html#the-internal-database">in-memory cache of the schema</a>, in order to (eventually) power fast table search across all known databases.</p>
<p>SQLite has a neat mechanism for doing this: you can run <code>PRAGMA schema_version</code> and it will return a number that increments every time the schema changes. This is a very fast operation.</p>
<p>I’m currently investigating a <a href="https://github.com/simonw/sqlite-utils/issues/235">confounding bug</a> where a user has reported the error message “attempt to write a readonly database” when this <code>PRAGMA schema_version</code> read operation is executed.</p>
<p>I’m still investigating, but this could mean that I need to find an alternative way of detecting changes to the schema.</p>
<p>One option is to calculate an MD5 hash of the full schema of the database, using the following:</p>
<pre><span>schema</span> <span>=</span> <span>db</span>.<span>execute</span>(
    <span>&#34;select group_concat(sql) from sqlite_master&#34;</span>
).<span>fetchall</span>()[<span>0</span>]
<span>hash</span> <span>=</span> <span>hashlib</span>.<span>md5</span>(<span>schema</span>).<span>hexdigest</span>()</pre>
<p>I can then compare that hash to the previous hash to see if the schema has changed.</p>
<p>The question I wanted to answer was how much of a performance hit this would represent—especially for databases with large number of tables.</p>
<p>Hence my desire to run a quick micro-benchmark.</p>
<h4>ChatGPT’s initial benchmark</h4>
<p>ChatGPT started by writing the following:</p>
<pre><span>import</span> <span>sqlite3</span>
<span>import</span> <span>time</span>
<span>import</span> <span>hashlib</span>

<span># Define a function to create tables and run benchmarks</span>
<span>def</span> <span>run_benchmark</span>(<span>db</span>, <span>num_tables</span>, <span>num_columns</span>, <span>benchmark_query</span>):
    <span># Start timing</span>
    <span>start_time</span> <span>=</span> <span>time</span>.<span>time</span>()
    
    <span># Create tables</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>num_tables</span>):
        <span>columns</span> <span>=</span> <span>&#39;, &#39;</span>.<span>join</span>([<span>f&#34;col<span><span>{</span><span>j</span><span>}</span></span> INTEGER&#34;</span> <span>for</span> <span>j</span> <span>in</span> <span>range</span>(<span>num_columns</span>)])
        <span>db</span>.<span>execute</span>(<span>f&#34;CREATE TABLE table<span><span>{</span><span>i</span><span>}</span></span> (<span><span>{</span><span>columns</span><span>}</span></span>)&#34;</span>)
    
    <span># Benchmark</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>100</span>):
        <span>if</span> <span>benchmark_query</span> <span>==</span> <span>&#34;PRAGMA schema_version&#34;</span>:
            <span>db</span>.<span>execute</span>(<span>&#34;PRAGMA schema_version&#34;</span>)
        <span>else</span>:
            <span>hashlib</span>.<span>md5</span>(<span>db</span>.<span>execute</span>(<span>&#34;SELECT group_concat(sql) FROM sqlite_master&#34;</span>).<span>fetchall</span>()[<span>0</span>][<span>0</span>].<span>encode</span>()).<span>hexdigest</span>()
    
    <span># Calculate time taken</span>
    <span>elapsed_time</span> <span>=</span> <span>time</span>.<span>time</span>() <span>-</span> <span>start_time</span>
    <span>return</span> <span>elapsed_time</span>

<span># Initialize in-memory SQLite database</span>
<span>db</span> <span>=</span> <span>sqlite3</span>.<span>connect</span>(<span>&#34;:memory:&#34;</span>)

<span># Run the first benchmark</span>
<span>benchmark1_time1</span> <span>=</span> <span>run_benchmark</span>(<span>db</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;PRAGMA schema_version&#34;</span>)
<span>benchmark1_time2</span> <span>=</span> <span>run_benchmark</span>(<span>db</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;PRAGMA schema_version&#34;</span>)

<span># Run the second benchmark</span>
<span>db2</span> <span>=</span> <span>sqlite3</span>.<span>connect</span>(<span>&#34;:memory:&#34;</span>)
<span>benchmark2_time1</span> <span>=</span> <span>run_benchmark</span>(<span>db2</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;hashlib.md5&#34;</span>)
<span>benchmark2_time2</span> <span>=</span> <span>run_benchmark</span>(<span>db2</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;hashlib.md5&#34;</span>)

<span>benchmark1_time1</span>, <span>benchmark1_time2</span>, <span>benchmark2_time1</span>, <span>benchmark2_time2</span></pre>
<p>This is a pretty solid attempt at a benchmark! It has good comments, abstracts the two benchmarks into a single function and runs that function twice to test against a total of 200 tables.</p>
<p>Then it ran the code... and this happened:</p>
<p><img src="https://static.simonwillison.net/static/2023/benchmark-bug.jpg" alt="ChatGPT screenshot. Finished working... some Python code, and then a Traceback showing an error: table table0 already exists. ChatGPT then says: It looks like we are trying to create tables with the same names as the ones that already exist in the database. This is causing an OperationalError. To resolve this, we need to change the table names in the second set of tables to avoid conflicts. Additionally, I will update the code to use the benchmark_query parameter in a more intuitive way. We&#39;ll pass the actual query or function to be benchmarked to the run_benchmark function. Let me fix this issue and re-run the code."/></p>
<p>It ran the code it generated... and got a <code>OperationalError: table table0 already exists</code> error.</p>
<p>That’s because it ran that <code>run_benchmark()</code> function twice against the same database, and the function attempted to re-create a table it had created the first time.</p>
<p>But it didn’t just stop at the error... it output an explanation of what it thought would happen, and then without any input from me wrote a second version of the benchmark script:</p>
<pre><span># Define a function to create tables and run benchmarks</span>
<span>def</span> <span>run_benchmark</span>(<span>db</span>, <span>num_tables</span>, <span>num_columns</span>, <span>benchmark_query</span>, <span>initial_table_index</span><span>=</span><span>0</span>):
    <span># Start timing</span>
    <span>start_time</span> <span>=</span> <span>time</span>.<span>time</span>()
    
    <span># Create tables</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>initial_table_index</span>, <span>initial_table_index</span> <span>+</span> <span>num_tables</span>):
        <span>columns</span> <span>=</span> <span>&#39;, &#39;</span>.<span>join</span>([<span>f&#34;col<span><span>{</span><span>j</span><span>}</span></span> INTEGER&#34;</span> <span>for</span> <span>j</span> <span>in</span> <span>range</span>(<span>num_columns</span>)])
        <span>db</span>.<span>execute</span>(<span>f&#34;CREATE TABLE table<span><span>{</span><span>i</span><span>}</span></span> (<span><span>{</span><span>columns</span><span>}</span></span>)&#34;</span>)
    
    <span># Benchmark</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>100</span>):
        <span>if</span> <span>benchmark_query</span> <span>==</span> <span>&#34;PRAGMA schema_version&#34;</span>:
            <span>db</span>.<span>execute</span>(<span>&#34;PRAGMA schema_version&#34;</span>)
        <span>else</span>:
            <span>hashlib</span>.<span>md5</span>(<span>db</span>.<span>execute</span>(<span>&#34;SELECT group_concat(sql) FROM sqlite_master&#34;</span>).<span>fetchall</span>()[<span>0</span>][<span>0</span>].<span>encode</span>()).<span>hexdigest</span>()
    
    <span># Calculate time taken</span>
    <span>elapsed_time</span> <span>=</span> <span>time</span>.<span>time</span>() <span>-</span> <span>start_time</span>
    <span>return</span> <span>elapsed_time</span>

<span># Initialize in-memory SQLite databases</span>
<span>db1</span> <span>=</span> <span>sqlite3</span>.<span>connect</span>(<span>&#34;:memory:&#34;</span>)
<span>db2</span> <span>=</span> <span>sqlite3</span>.<span>connect</span>(<span>&#34;:memory:&#34;</span>)

<span># Run the first benchmark with PRAGMA schema_version</span>
<span>benchmark1_time1</span> <span>=</span> <span>run_benchmark</span>(<span>db1</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;PRAGMA schema_version&#34;</span>)
<span>benchmark1_time2</span> <span>=</span> <span>run_benchmark</span>(<span>db1</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;PRAGMA schema_version&#34;</span>, <span>initial_table_index</span><span>=</span><span>100</span>)

<span># Run the second benchmark with hashlib.md5</span>
<span>benchmark2_time1</span> <span>=</span> <span>run_benchmark</span>(<span>db2</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;hashlib.md5&#34;</span>)
<span>benchmark2_time2</span> <span>=</span> <span>run_benchmark</span>(<span>db2</span>, <span>num_tables</span><span>=</span><span>100</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;hashlib.md5&#34;</span>, <span>initial_table_index</span><span>=</span><span>100</span>)

<span>benchmark1_time1</span>, <span>benchmark1_time2</span>, <span>benchmark2_time1</span>, <span>benchmark2_time2</span></pre>
<p>And this time it worked!</p>
<p><img src="https://static.simonwillison.net/static/2023/benchmark-result.jpg" alt="RESULT: (0.008260965347290039, 0.006290912628173828,  011242151260375977, 0.014682292938232422) - Here are the results of the benchmarks: For the first benchmark using PRAGMA schema_version: The time taken to create 100 tables and run PRAGMA schema_version 100 times: approximately 0.0083 seconds. The time taken to create another 100 tables (totaling 200 tables) and run PRAGMA schema_version 100 times: approximately 0.0063 seconds. For the second benchmark using hashlib.md5: The time taken to create 100 tables and run hashlib.md5(db.execute(&#34;SELECT group_concat(sql) FROM sqlite_master&#34;).fetchall()[0]).hexdigest() 100 times: approximately 0.0112 seconds. The time taken to create another 100 tables (totaling 200 tables) and run hashlib.md5(db.execute(&#34;SELECT group_concat(sql) FROM sqlite_master&#34;).fetchall()[0]).hexdigest() 100 times: approximately 0.0147 seconds. Please note that the time taken for these benchmarks may vary slightly with each run due to factors such as system performance and other processes running on the system."/></p>
<h4>Adding a chart</h4>
<p>This is a very solid answer to my question. I checked it through and it mostly looks good—I would have written more-or-less the same thing, but it would have taken me a lot longer to do it.</p>
<p>So I asked it for a plot:</p>
<blockquote>
<p>Plot those benchmark results over time on a chart</p>
</blockquote>
<p>It wrote more code. ChatGPT writes and executes code in the same code session, so it had access to the variables that it had previously populated:</p>
<pre><span>import</span> <span>matplotlib</span>.<span>pyplot</span> <span>as</span> <span>plt</span>

<span># Data for benchmark results</span>
<span>benchmark1_times</span> <span>=</span> [<span>benchmark1_time1</span>, <span>benchmark1_time2</span>]
<span>benchmark2_times</span> <span>=</span> [<span>benchmark2_time1</span>, <span>benchmark2_time2</span>]

<span># x-axis representing the number of tables at each benchmark</span>
<span>num_tables</span> <span>=</span> [<span>100</span>, <span>200</span>]

<span># Plotting the benchmark results</span>
<span>plt</span>.<span>plot</span>(<span>num_tables</span>, <span>benchmark1_times</span>, <span>label</span><span>=</span><span>&#39;PRAGMA schema_version&#39;</span>, <span>marker</span><span>=</span><span>&#39;o&#39;</span>)
<span>plt</span>.<span>plot</span>(<span>num_tables</span>, <span>benchmark2_times</span>, <span>label</span><span>=</span><span>&#39;hashlib.md5&#39;</span>, <span>marker</span><span>=</span><span>&#39;o&#39;</span>)

<span># Labels and title</span>
<span>plt</span>.<span>xlabel</span>(<span>&#39;Number of Tables&#39;</span>)
<span>plt</span>.<span>ylabel</span>(<span>&#39;Time (seconds)&#39;</span>)
<span>plt</span>.<span>title</span>(<span>&#39;Benchmark Results Over Time&#39;</span>)
<span>plt</span>.<span>legend</span>()

<span># Show the plot</span>
<span>plt</span>.<span>show</span>()</pre>
<p>I’m not very good at using the <code>matplotlib</code> charting libraries—I have to painstakingly piece everything together every time I use them. ChatGPT just did it.</p>
<p>Here’s the chart it drew me:</p>
<p><img src="https://static.simonwillison.net/static/2023/benchmark-chart-1.jpg" alt="Benchmark Results Over Time - two lines, one for PRAGMA schema_version and one for hashlib.md5. There are only two points on the chart - at 100 tables and at 200 tables - with straight lines between them."/></p>
<p>This chart looks pretty good at first glance—it has an appropriate title, axis labels and a legend.</p>
<p>In terms of information density though it’s rubbish. Charting just two points for each of the implementations, at 100 tables and 200 tables, is hardly worth visualizing at all.</p>
<p>So I fed it a final prompt:</p>
<blockquote>
<p>Run the benchmark again in order to draw a better chart with measurements for every point between 1 and 200 tables</p>
</blockquote>
<p>And that resulted in the chart I showed at the top of my post:</p>
<p><img src="https://static.simonwillison.net/static/2023/sqlite-md5-benchmark.jpg" alt="Benchmark Results Over Time - two lines, one for PRAGMA schema_version and one for hashlib.md5. The axis are time in seconds and number of tables - the hashlib.md5 line climes from around 0.000075 to 0.000150 after 200 tables have been created, while the PRAGMA schema_version line stays almost flat at 0.000050 at the bottom of the chart. It&#39;s a good chart: well labelled, easy to read."/></p>
<p>Here’s the code it wrote:</p>
<pre><span># Define a function to create tables and run a single benchmark iteration</span>
<span>def</span> <span>run_single_benchmark</span>(<span>db</span>, <span>num_tables</span>, <span>num_columns</span>, <span>benchmark_query</span>, <span>initial_table_index</span><span>=</span><span>0</span>):
    <span># Start timing</span>
    <span>start_time</span> <span>=</span> <span>time</span>.<span>time</span>()
    
    <span># Create tables</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>initial_table_index</span>, <span>initial_table_index</span> <span>+</span> <span>num_tables</span>):
        <span>columns</span> <span>=</span> <span>&#39;, &#39;</span>.<span>join</span>([<span>f&#34;col<span><span>{</span><span>j</span><span>}</span></span> INTEGER&#34;</span> <span>for</span> <span>j</span> <span>in</span> <span>range</span>(<span>num_columns</span>)])
        <span>db</span>.<span>execute</span>(<span>f&#34;CREATE TABLE table<span><span>{</span><span>i</span><span>}</span></span> (<span><span>{</span><span>columns</span><span>}</span></span>)&#34;</span>)
    
    <span># Benchmark</span>
    <span>if</span> <span>benchmark_query</span> <span>==</span> <span>&#34;PRAGMA schema_version&#34;</span>:
        <span>db</span>.<span>execute</span>(<span>&#34;PRAGMA schema_version&#34;</span>)
    <span>else</span>:
        <span>hashlib</span>.<span>md5</span>(<span>db</span>.<span>execute</span>(<span>&#34;SELECT group_concat(sql) FROM sqlite_master&#34;</span>).<span>fetchall</span>()[<span>0</span>][<span>0</span>].<span>encode</span>()).<span>hexdigest</span>()
    
    <span># Calculate time taken</span>
    <span>elapsed_time</span> <span>=</span> <span>time</span>.<span>time</span>() <span>-</span> <span>start_time</span>
    <span>return</span> <span>elapsed_time</span>

<span># Initialize in-memory SQLite databases</span>
<span>db1</span> <span>=</span> <span>sqlite3</span>.<span>connect</span>(<span>&#34;:memory:&#34;</span>)
<span>db2</span> <span>=</span> <span>sqlite3</span>.<span>connect</span>(<span>&#34;:memory:&#34;</span>)

<span># Lists to store benchmark results</span>
<span>benchmark1_times</span> <span>=</span> []
<span>benchmark2_times</span> <span>=</span> []

<span># Run the benchmarks for every point between 1 and 200 tables</span>
<span>for</span> <span>num</span> <span>in</span> <span>range</span>(<span>1</span>, <span>201</span>):
    <span># Run the benchmark with PRAGMA schema_version</span>
    <span>benchmark1_time</span> <span>=</span> <span>run_single_benchmark</span>(<span>db1</span>, <span>num_tables</span><span>=</span><span>1</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;PRAGMA schema_version&#34;</span>, <span>initial_table_index</span><span>=</span><span>num</span><span>-</span><span>1</span>)
    <span>benchmark1_times</span>.<span>append</span>(<span>benchmark1_time</span>)
    
    <span># Run the benchmark with hashlib.md5</span>
    <span>benchmark2_time</span> <span>=</span> <span>run_single_benchmark</span>(<span>db2</span>, <span>num_tables</span><span>=</span><span>1</span>, <span>num_columns</span><span>=</span><span>10</span>, <span>benchmark_query</span><span>=</span><span>&#34;hashlib.md5&#34;</span>, <span>initial_table_index</span><span>=</span><span>num</span><span>-</span><span>1</span>)
    <span>benchmark2_times</span>.<span>append</span>(<span>benchmark2_time</span>)

<span># x-axis representing the number of tables at each benchmark</span>
<span>num_tables</span> <span>=</span> <span>list</span>(<span>range</span>(<span>1</span>, <span>201</span>))

<span># Plotting the benchmark results</span>
<span>plt</span>.<span>plot</span>(<span>num_tables</span>, <span>benchmark1_times</span>, <span>label</span><span>=</span><span>&#39;PRAGMA schema_version&#39;</span>, <span>marker</span><span>=</span><span>&#39;.&#39;</span>)
<span>plt</span>.<span>plot</span>(<span>num_tables</span>, <span>benchmark2_times</span>, <span>label</span><span>=</span><span>&#39;hashlib.md5&#39;</span>, <span>marker</span><span>=</span><span>&#39;.&#39;</span>)

<span># Labels and title</span>
<span>plt</span>.<span>xlabel</span>(<span>&#39;Number of Tables&#39;</span>)
<span>plt</span>.<span>ylabel</span>(<span>&#39;Time (seconds)&#39;</span>)
<span>plt</span>.<span>title</span>(<span>&#39;Benchmark Results Over Time&#39;</span>)
<span>plt</span>.<span>legend</span>()

<span># Show the plot</span>
<span>plt</span>.<span>show</span>()</pre>
<h4>Full transcript</h4>
<p>The entire project was completed using just the three prompts listed above!</p>
<p>I used <a href="https://observablehq.com/@simonw/chatgpt-json-transcript-to-markdown">my Observable notebook</a> to convert the ChatGPT transcript JSON (extracted using the Network pane in the browser tools) to Markdown, and pasted it into this Gist: <a href="https://gist.github.com/simonw/2fc46effbfbe49e6de0bcfdc9e31b235">https://gist.github.com/simonw/2fc46effbfbe49e6de0bcfdc9e31b235</a></p>
<p>(I like using “secret” Gists to share this kind of thing because they are automatically marked by GitHub as <code>&lt;meta name=&#34;robots&#34; content=&#34;noindex&#34;&gt;</code>—which should hopefully prevent them from polluting the web with LLM-generated content. Though I’ve pasted a LOT of ChatGPT-written code into this search-indexed blog entry in this particular case.)</p>
<h4>A really weird kind of intern</h4>
<p>Here’s another analogy for large language models (since we can never have too many of those). It honestly feels a little  like having a coding intern, with a strange set of characteristics:</p>
<ul>
<li>They’ve read and memorized all of the world’s public coding documentation, though they stopped reading in September 2021.</li>
<li>They’re unbelievably fast—they take just seconds to output entire programs from scratch.</li>
<li>If their code produces an error they can read the error message and attempt to update their code to fix it.</li>
<li>They won’t argue with you. That’s not necessarily a good thing: if you ask them to build something that’s clearly a bad idea they’ll likely just go ahead and do it anyway.</li>
<li>They respond incredibly well to feedback—they’ll apply it within seconds.</li>
<li>You have to know how to prompt them. I’ve got pretty good at this now, but it takes a lot of trial and error, and I think requires a pretty deep understanding of how they work.</li>
<li>You have to closely review EVERYTHING they do.</li>
<li>They work for free.</li>
</ul>
<p>And of course, they have zero understanding of anything at all. They’re a next-token-predicting machine with an unimaginably large training set.</p>
<p>The fact they can do even a fraction of the things they can do is, quite frankly, unbelievable. I’m still not sure I believe it myself.</p>

<h4 id="something-i-missed">Something I missed</h4>
<p>After publishing this post, I realized that I’d missed a mistake ChatGPT had made.</p>
<p>I wanted to compare the time taken to execute <code>PRAGMA schema_version</code> v.s. calculating the MD5 hash of <code>select group_concat(sql) from sqlite_master</code>. But... ChatGPT had started the timer with <code>start_time = time.time()</code> before creating the tables—so the time measurement included the table creation time.</p>
<p>This didn’t affect the comparison between the two, but it did mean that I wasn’t getting the underlying numbers that I most cared about.</p>
<p>Like I said earlier, <strong>You have to closely review EVERYTHING they do</strong>. I’m embarrassed I missed this!</p>
<p>I fixed it with one last prompt:</p>
<blockquote>
<p>Record the start time after the # Benchmark comment, then re-run and re-plot</p>
</blockquote>
<p>It rewrote that block of code to look like this:</p>
<pre>    <span># Benchmark</span>
    <span># Start timing</span>
    <span>start_time</span> <span>=</span> <span>time</span>.<span>time</span>()
    <span>if</span> <span>benchmark_query</span> <span>==</span> <span>&#34;PRAGMA schema_version&#34;</span>:
        <span>db</span>.<span>execute</span>(<span>&#34;PRAGMA schema_version&#34;</span>)
    <span>else</span>:
        <span>hashlib</span>.<span>md5</span>(<span>db</span>.<span>execute</span>(
            <span>&#34;SELECT group_concat(sql) FROM sqlite_master&#34;</span>
        ).<span>fetchall</span>()[<span>0</span>][<span>0</span>].<span>encode</span>()).<span>hexdigest</span>()

    <span># Calculate time taken</span>
    <span>elapsed_time</span> <span>=</span> <span>time</span>.<span>time</span>() <span>-</span> <span>start_time</span>
    <span>return</span> <span>elapsed_time</span></pre>
<p>And generated the following chart:</p>
<p><img src="https://static.simonwillison.net/static/2023/sqlite-md5-benchmark-fixed.jpg" alt="The same chart again, but this time the PRAGMA line sticks right by 0.0000 seconds and the other line climbs up to aronud 0.00005 seconds"/></p>
<h4>Better colors</h4>
<p>A <a href="https://news.ycombinator.com/item?id=35534923">commenter on Hacker News</a> pointed out that the two lines had colors that were too similar to each other.</p>
<p>Prompt:</p>
<blockquote><p>One more chart plot, this time with colors that differ more (and are OK for people who are color blind)</p></blockquote>
<p><img src="https://static.simonwillison.net/static/2023/better-colors.jpg" alt="The same chart again, but now the lines are dark green and bright pink"/></p>




</div></div>
  </body>
</html>
