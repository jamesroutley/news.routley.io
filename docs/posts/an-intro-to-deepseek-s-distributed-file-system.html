<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://maknee.github.io/blog/2025/3FS-Performance-Journal-1/">Original</a>
    <h1>An Intro to DeepSeek&#39;s Distributed File System</h1>
    
    <div id="readability-page-1" class="page"><article> <div id="markdown-content">  <ul> <li><a href="https://maknee.github.io/blog/2025/3FS-Performance-Journal-1/">An Intro to DeepSeek’s Distributed File System</a></li> </ul>  <p>3FS (<a href="https://github.com/deepseek-ai/3FS" rel="external nofollow noopener" target="_blank">Fire-Flyer File System</a><span></span><span>Geez, what a tongue twister</span>) is a distributed filesystem released by <a href="https://www.deepseek.com/" rel="external nofollow noopener" target="_blank">DeepSeek</a> during their <a href="https://github.com/deepseek-ai/open-infra-index" rel="external nofollow noopener" target="_blank">open source release week</a>. This blog post will dive into what distributed file systems are and how 3FS operates, starting with some background.</p> <h2 id="what-is-a-distributed-filesystem">What is a distributed filesystem?</h2> <p>Distributed filesystems trick applications into thinking they’re talking to a regular local filesystem. This abstraction is incredibly powerful: a file that’s actually fragmented across 10 different machines appears as a simple file path like <code>/3fs/stage/notes.txt</code></p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/part1/local_distributed_fs.svg" alt=""/></p><p><em>Using the distributed filesystem is no different than local filesystem </em> </p> </div> <p>In the image above, I create the same folder and file on a local and distributed filesystem by running <code>mkdir</code> and <code>cat</code>. The commands are exactly the same. With a distributed filesystem, all of those details are abstracted away from the user, who can simply work with the files without worrying about how many machines, network calls or disks are involved behind the scene.</p> <h2 id="why-use-a-distributed-filesystem">Why use a distributed filesystem?</h2> <p>Distributed filesystems provide two main advantages over local storage – they can serve massive amounts of data (up to petabytes) and provide high throughput that exceed the capabilities of a single machine. They offer fault tolerance (the system keeps running if one machine goes down) and redundancy (if data gets corrupted on one node, other nodes have original copies).</p> <p>Distributed filesystems are used in many practical applications:</p> <ul> <li>Parallel processing frameworks (<a href="https://hadoop.apache.org/" rel="external nofollow noopener" target="_blank">HDFS</a> supporting <a href="https://www.databricks.com/blog/2014/01/21/spark-and-hadoop.html" rel="external nofollow noopener" target="_blank">Spark</a>)</li> <li>ML training pipelines with <a href="https://github.com/stas00/ml-engineering/blob/master/storage/README.md" rel="external nofollow noopener" target="_blank">Dataloaders and checkpointing</a> </li> <li>Internal large-scale code/data repositories supported by <a href="https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system" rel="external nofollow noopener" target="_blank">Google’s Colossus</a> </li> <li>Industry applications like <a href="https://juicefs.com/en/blog/user-stories/juicefs-vs-cephfs-distributed-file-system-artificial-intelligence-storage" rel="external nofollow noopener" target="_blank">Traveling</a> </li> <li>Photo storage is served by <a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf" rel="external nofollow noopener" target="_blank">Meta’s Haystack</a> </li> </ul>  <p>So, how does 3FS work?</p> <p>At its core, 3FS consists of four primary node types:</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/explain/overview.svg" alt=""/></p><p><em>Components involved in 3FS </em> </p> </div> <p>The components serve distinct purposes:</p> <ol> <li> <strong>Meta</strong> – manage the metadata: file locations, properties, paths, etc.</li> <li> <strong>Mgmtd</strong> – management server controls the cluster configuration: where are other nodes, which nodes are alive, and replication factor <ul> <li>Think of it as a router that knows every node’s address and can help nodes find each other<span></span><span>A similar analogy is the centralized server used in <a href="https://en.wikipedia.org/wiki/Hole_punching_(networking)" rel="external nofollow noopener" target="_blank">NAT hole punching</a></span>.</li> </ul> </li> <li> <strong>Storage</strong> – nodes that hold the actual file data on physical disks.</li> <li> <strong>Client</strong> – communicates with all other nodes to view and modify the filesystem: <ul> <li>ask Mgmtd to discover other nodes</li> <li>ask Meta servers to perform file operations (open, stat, close, symlink)</li> <li>transfer data with storage nodes</li> </ul> </li> </ol> <p>Now let’s look at each component in greater detail.</p> <h2 id="mgmtd">Mgmtd</h2> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/explain/mgmtd_register.svg" alt=""/></p><p><em>Mgmtd Registering </em> </p> </div> <p>Mgmtd tracks what nodes are running in the cluster. Storage and meta nodes register when they boot up, sending periodic heartbeats to confirm they’re still alive. This gives a central view of the system – one can immediately identify which nodes are down.</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/explain/mgmtd_request.svg" alt=""/></p><p><em>Mgmtd Requests </em> </p> </div> <p>Nodes don’t need to maintain connections with every other node in the network. Instead, they can discover nodes by querying the mgmtd node. While this adds an extra round trip when locating nodes, it can reduce complexity since node discovery isn’t static.</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/explain/mgmtd_chain.svg" alt=""/></p><p><em>Mgmtd Chains </em> </p> </div> <p>Also, Mgmtd maintains the configuration for different nodes operating within a distributed algorithm. In particular, replicated chains (<a href="https://www.usenix.org/legacy/event/usenix09/tech/full_papers/terrace/terrace.pdf" rel="external nofollow noopener" target="_blank">CRAQ</a><span></span><span>CRAQ is a pretty neat algorithm that achieves strong consistency with fault tolerance by treating nodes as a chain. I’ll explain this in depth in another section.</span>) are established and its nodes are stored as configuration in mgmtd.</p>  <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/explain/meta_overview.svg" alt=""/></p><p><em>Meta overview </em> </p> </div> <p>The meta node is a bit more complex than mgmtd. Clients communicate with it via RPC calls. The meta server performs typical filesystem operations (open, create, stat, unlink) on the metastore. File metadata resides in inodes, storing properties like size, permissions, owner, and timestamps. DirEntry objects map paths to inodes, with multiple DirEntries possible for a single file (similar to symlinks). Both inodes and DirEntries are stored in FoundationDB<span></span><span>One might wonder what the keys to founationdb look like? Inode: “INOD + inode id, dir entry: “DENT” + nodeid + path</span> using transactions for idempotent operations. A session manager tracks open files, storing file sessions in FoundationDB. If clients disconnect without closing files, the session manager initiates file syncs. File deletion requests queue to a garbage collector, which removes data from storage nodes before deleting directory entries and inodes.</p> <h2 id="storage">Storage</h2> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/explain/storage_overview.svg" alt=""/></p><p><em>Storage overview </em> </p> </div> <p>The storage node’s main function is manage data on physical storage by breaking it up into chunks:</p> <ul> <li>The Rust ChunkEngine<span></span><span>Why Rust? Well, there’s a legacy chunk manager named <code>ChunkStore</code> that’s written in C++. I don’t see really why in rust, probably because it’s interesting to work in and provides more safety guarantees</span> keeps track of blocks of disk storage. <ul> <li>Chunks represent a piece of physical disk and keeps track of its metadata (id, size, offset on disk, physical disk, checksums, versions, …). This is the most primitive data structure that all other structures use to keep track of blocks of data.</li> <li>The chunk engine doesn’t allow users to interact with chunks directly since it would add complexity to using engine. The interface to the engine has operations which gives users a rigid and clear way to interact with the engine (lookup, allocation, commit, metadata…)</li> <li>By default, all of this is stored in <a href="https://github.com/google/leveldb" rel="external nofollow noopener" target="_blank">LevelDB</a> with a prefix byte repesenting the type of operation (querying the metadata) and the chunk id as the key.</li> </ul> </li> <li>There are different workers that uses the chunk engine to maintain the physical storage <ul> <li>The AllocateWorker allocates new chunks in the chunk engine</li> <li>The PunchHoleWorker reclaims chunks if they’re no longer used</li> <li>The AioReadWorker processes reads requests to the chunks and queues reads in <a href="https://en.wikipedia.org/wiki/Io_uring" rel="external nofollow noopener" target="_blank">io_uring</a> queue, submits and waits for completion<span></span><span>Initially, I was surprised. The chunk engine doesn’t perform operations on the actual physical disk, it really only manages the metadata. One reason for this might be to keep the ChunkEngine implementation rather lean by having it only try to manage metadata.</span>.</li> </ul> </li> <li>The storage node needs to know how to forward a write to the next target in a CRAQ chain<span></span><span>For now, just know that writes need to be forwarded to other nodes</span> <ul> <li>Targets consist of chunks (think of this as logical store containing different chunks)</li> <li>A chain consists of multiple targets (typically spanning multiple nodes)</li> <li>The storage node queries the mgmtd server for other nodes’ chains and the corresponding targets (nodes) in that chain that a write needs to forward to.</li> </ul> </li> </ul> <h2 id="craq">CRAQ</h2> <p>CRAQ (<a href="https://www.usenix.org/legacy/event/usenix09/tech/full_papers/terrace/terrace.pdf" rel="external nofollow noopener" target="_blank">Chain Replication with Apportioned Queries</a>) is a protocol for achieving strong consistency with linearizability. It serves as the core mechanism to keep data chunks fault-tolerant. I’ll explain how CRAQ works and then, show its implementation in 3FS.</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/craq/craq_write_dirty.svg" alt=""/></p><p><em>Craq write propagation </em> </p> </div> <p>Writes begin at the head. In our example, we write <code>name=henry</code> to the system. As the write moves down the chain, each entry is marked as “dirty” with a version number. Dirty entries aren’t safe to read. Once the write reaches the tail, it’s committed and marked as “clean”.</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/craq/craq_write_clean.svg" alt=""/></p><p><em>Craq write commit </em> </p> </div> <p>Writes become clean as commit messages propagates backward from tail to head. Each node commits the entry and marks it clean.</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/craq/craq_read_clean.svg" alt=""/></p><p><em>Craq clean read </em> </p> </div> <p>For reads, the process is straightforward: if an object is clean, it’s immediately returned to the client.</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/craq/craq_read_dirty.svg" alt=""/></p><p><em>Craq dirty read </em> </p> </div> <p>The challenge occurs with dirty objects. Each chain tracks both dirty and clean versions. Since the tail always contains the latest committed data, the replica queries the tail for the most recent committed object, ensuring strong consistency.</p> <h3 id="craq-performance">CRAQ performance</h3> <p>CRAQ read and write performance varies by workload. Write throughput and latency are limited by the slowest node in the chain, as writes must process through each node sequentially. For example, in <a href="https://en.wikipedia.org/wiki/Zipf%27s_law" rel="external nofollow noopener" target="_blank">zipfian</a> workloads (where frequently accessed data dominates), read performance suffers because objects may be dirty, forcing queries to the tail node. This creates a bottleneck as the tail must serve most of the read requests.</p> <h3 id="how-is-craq-used-in-3fs">How is CRAQ used in 3FS</h3> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/craq/craq_stripe.svg" alt=""/></p><p><em>Storage is striped and CRAQ runs ontop </em> </p> </div> <p>In this example, 5 nodes with 5 SSDs each form the cluster. Storage targets replicate to 3 nodes, designed to avoid overlap so that node failures don’t affect overall throughput significantly<span></span><span>Consider an extreme scenario where all the chains are placed on nodes 1, 2, 3. If node 1 fails, the distributed system would serve lose 1/3 of the total throughput instead of 1/5 of total throughput shown in the above image. <a href="https://github.com/deepseek-ai/3FS/blob/ee9a5cee0a85c64f4797bf380257350ca1becd36/docs/design_notes.md" rel="external nofollow noopener" target="_blank">3FS design notes</a> shows an example with a deeper explanation.</span>. CRAQ operates on top, managing head, middle, and tail nodes.</p> <p>3FS defaults to strongly consistent reads. Writes flow from head to tail and back, with throughput limited by the slowest node and latency determined by the combined latency across all chain nodes.</p> <div> <p><img loading="lazy" src="https://maknee.github.io/assets/images/posts/2025-03-13/papers/ionia-table-1.svg" width="100%" alt=""/></p> </div> <p>As shown in the comparison table, in the common case, CRAQ delivers scalable, low-latency reads at the cost of high write latency compared to other protocols and systems.</p>  <p>One might ask – is this architecture different from other distributed filesystems? At a high level, the components are familiar – some notion of client, metadata, storage, and management nodes appear in virtually every distributed system.</p> <p>The difference lies in its real-world applicability and practical implementation:</p> <ul> <li>which workloads it excels at handling</li> <li>its tuning flexibility</li> <li>deployment simplicity</li> <li>throughput scaling capabilities</li> <li>maintaining latency within SLOs</li> <li>reliability</li> </ul> <p>and its finer technical details that determines its usability:</p> <ul> <li>what bottlenecks are there</li> <li>how it manages bottlenecks</li> <li>its approach to locking (or absence thereof)</li> <li>the specific data structures employed</li> <li>the hardware the software was designed for</li> <li>what fault tolerant algorithm or erasure coding is used</li> </ul>  <p>With that in mind, I want to dive deep into analyzing the performance of this relatively new open-source distributed filesystem<span></span><span>Distributed filesystems come once in blue moon, taking <a href="https://dl.acm.org/doi/10.1145/3341301.3359656" rel="external nofollow noopener" target="_blank">several years to develop</a></span>. Current benchmarks are rather limited. There’s no comparisons with single-node systems and other distributed filesystems, so it’s difficult to gauge how well 3FS performs.</p> <p>Some questions I want to explore:</p> <ul> <li>Do some of the DeepSeek’s claims hold up, especially regarding <a href="https://github.com/deepseek-ai/3FS/blob/ee9a5cee0a85c64f4797bf380257350ca1becd36/docs/design_notes.md#limitations-of-fuse" rel="external nofollow noopener" target="_blank">FUSE bottlenecks</a>?</li> <li>Can I reproduce their performance graphs in some way?</li> <li>In what scenario does the performance degrade?</li> <li>What are the system’s bottlenecks (CPU/memory/disk/network)?</li> <li>In what types of workloads does the fileysystem excel at?</li> <li>How does it compare with other distributed filesystems?</li> <li>How does it address problems that existing systems face?</li> <li>Am I able to make any improvements to the system?</li> </ul> <p>Throughout the rest of the series, I will be going through the process of making initial assumptions, testing them, and learning from discrepancies to develop a deeper understanding of how 3FS actually performs.</p>  <p>Implementation details are documented in the <a href="https://github.com/deepseek-ai/3FS/blob/ee9a5cee0a85c64f4797bf380257350ca1becd36/docs/design_notes.md" rel="external nofollow noopener" target="_blank">design notes</a>.</p> <p>Additional technical documentation regarding early implementation phases is available (in Chinese):</p> <ul> <li><a href="https://www.high-flyer.cn/blog/3fs/" rel="external nofollow noopener" target="_blank">Intro</a></li> <li><a href="https://www.high-flyer.cn/blog/3fs-1/" rel="external nofollow noopener" target="_blank">Async IO</a></li> <li><a href="https://www.high-flyer.cn/blog/3fs-3/" rel="external nofollow noopener" target="_blank">RDMA Read</a></li> <li><a href="https://www.high-flyer.cn/blog/3fs-3/" rel="external nofollow noopener" target="_blank">Network routing</a></li> <li><a href="https://www.high-flyer.cn/blog/3fs-4/" rel="external nofollow noopener" target="_blank">Load balancing reads</a></li> </ul> <p>The system architecture is partially documented in <a href="https://arxiv.org/abs/2408.14158" rel="external nofollow noopener" target="_blank">the Fire-Flyer AI-HPC paper</a>.</p>  <p>Thanks to <a href="https://vimarsh.me/" rel="external nofollow noopener" target="_blank">Vimarsh Sathia</a> for reviewing this post.</p> </div> </article></div>
  </body>
</html>
