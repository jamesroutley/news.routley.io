<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://svraster.github.io/">Original</a>
    <h1>Sparse Voxels Rasterization: Real-Time High-Fidelity Radiance Field Rendering</h1>
    
    <div id="readability-page-1" class="page">

  <section>
    <div>
      <div>
        <div>
          <div>
            
            

          <!-- affiliations -->
          <p><sup>1</sup> Nvidia  
            <sup>2</sup> Cornell University  
            <sup>3</sup> National Taiwan University
          </p>

          <!-- TODO: update paper and video link once upload -->
          
        </div>

      </div>
    </div>
    </div>
    
    
  </section>


  <section>
    <div>
      <div>
        
        <p>We optimize adaptive sparse voxels radiance field from multi-view images without SfM points. The fly-through videos are rendered by our SVRaster in &gt;100 FPS.</p>
      </div>
    </div>
  </section>


  <section>
    <div>
      <!-- Abstract. -->
      <div>
        <div>
          <h2>Overview</h2>
          <p>
              We propose an efficient radiance field rendering algorithm that incorporates a rasterization process on adaptive sparse voxels without neural networks or 3D Gaussians. There are two key contributions coupled with the proposed system. The first is to adaptively and explicitly allocate sparse voxels to different levels of detail within scenes, faithfully reproducing scene details with 65536<sup>3</sup> grid resolution while achieving high rendering frame rates. Second, we customize a rasterizer for efficient adaptive sparse voxels rendering. We render voxels in the correct depth order by using ray direction-dependent Morton ordering, which avoids the well-known popping artifact found in Gaussian splatting. Our method improves the previous neural-free voxel model by over 4db PSNR and more than 10x FPS speedup, achieving state-of-the-art comparable novel-view synthesis results. Additionally, our voxel representation is seamlessly compatible with grid-based 3D processing techniques such as Volume Fusion, Voxel Pooling, and Marching Cubes, enabling a wide range of future extensions and applications.
            </p>
        </div>
      </div>
      <!--/ Abstract. -->
      
      <p><img src="https://svraster.github.io/images/teaser.jpg"/>
      </p>
    </div>
    
  </section>


  <section>
    <div>
      <div>
        <div>
          <h2>Adaptive Sparse Voxel Representation and Rendering</h2>
          <div>
            <p>
              Our scene representation is a hybrid of primitive and volumetric model.
              <strong>(a) Primitive component.</strong> We explicitly allocate voxels primitives to cover different scene level-of-details under an Octree layout. Note that we do not replicate a traditional Octree data structure with parent-child pointers or linear Octree. We only keep voxels at the Octree leaf nodes without any ancestor nodes.
              <strong>(b) Volumetric component.</strong> Inside a voxel is a volumetric (trilinear) density field and a (constant) spherical harmonic field. We sample K points on the ray-voxel intersection segment to compute the intensity contribution from the voxel to the pixel with numerical integration.
            </p>
            <p><img src="https://svraster.github.io/images/representations.jpg"/>
            </p>
            </div>
        </div>
      </div>
    </div>
  </section>


  <section>
    <div>
      <div>
        <div>
          <h2>Novel-view Synthesis Results</h2>
          <div>
            <p><img src="https://svraster.github.io/images/quantitative_nvs.jpg"/>
            </p>
          </div>
        </div>
      </div>

      

      

      

    </div>
  </section>


  <section>
    <div>
      <div>
        <div>
          <h2>Adaptive Sparse Voxel Fusion</h2>
          <div>
            <p>
              Fusing 2D modalities into the trained sparse voxels is efficient. The grid points simply take the weighted from the 2D views following classical volume fusing method. We show several examples in the following.
            </p>
            <p>
              <strong>Rendered depths → Sparse-voxel SDF → Mesh</strong>
            </p>
            <p><img src="https://svraster.github.io/images/meshing.jpg"/>
            </p>
            </div>
        </div>
      </div>
    </div>
  </section>

  

  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@article{Sun2024SVR,
  title={Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering},
  author={Cheng Sun and Jaesung Choe and Charles Loop and Wei-Chiu Ma and Yu-Chiang Frank Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.04459},
}</code>
      </pre>
    </div>
  </section>


  



</div>
  </body>
</html>
