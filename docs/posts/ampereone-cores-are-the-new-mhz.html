<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jeffgeerling.com/blog/2024/ampereone-cores-are-new-mhz">Original</a>
    <h1>AmpereOne: Cores Are the New MHz</h1>
    
    <div id="readability-page-1" class="page"><div><p><em>Cores</em> are the new megahertz, at least for enterprise servers. We&#39;ve gone quickly from 32, to 64, to 80, to 128, and now to 192-cores on a single CPU socket!</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampereone-hero-open.jpeg" alt="AmpereOne A192-32X open"/></p>

<p>Amazon built <a href="https://www.aboutamazon.com/news/aws/graviton4-aws-cloud-computing-chip">Graviton 4</a>, Google built <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu">Axiom</a>, but if you want <em>your own</em> massive Arm server, Ampere&#39;s the only game in town. And fastest Arm CPU in the <em>world</em> is inside the box pictured above.</p>

<p>It has 192 custom Arm cores running at 3.2 Gigahertz, and in <em>some</em> benchmarks, it stays in the ring with AMD&#39;s fastest EPYC chip, the 9965 &#34;Turin Dense&#34;, which <em>also</em> has 192 cores.</p>

<p>High-core-count servers are the cutting edge in datacenters, and they&#39;re so insane, most software <em>doesn&#39;t even know how to handle it</em>. <code>btop</code> has to go full screen on the CPU graph just to fit all the cores:</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampereone-btop-192-cores.jpeg" alt="AmpereOne btop 192 cores"/></p>

<p>To support all those cores, this system has 8 channels of DDR5 ECC RAM, and 128 lanes of PCIe Gen 5.</p>

<p>This particular system is a <a href="https://www.supermicro.com/en/products/system/megadc/2u/ars-211me-fnr">Supermicro ARS-211ME-FNR</a>. It&#39;s targeted at Telco Edge deployments, and this particular unit was sent by Supermicro and Ampere for testing—I&#39;ve been doing that for a month now.</p>

<p>This blog post is a lightly-edited transcript of this video (though you can continue past it to read the rest, if you like reading more!):</p>

<div>
<p><iframe src="https://www.youtube.com/embed/t05OZAruyYY" frameborder="0" allowfullscreen=""></iframe></p>
</div>

<h2>A Strange New Era</h2>

<p>This <em>isn&#39;t</em> the world&#39;s fastest single-socket server. It&#39;s not even the most <em>efficient</em>! AMD&#39;s latest EPYC &#39;Turin Dense&#39; CPU takes both of those titles. But this <em>is</em> the best value, in terms of <em>performance per dollar</em>.</p>

<p>This is a strange era:</p>

<ul>
<li>AMD reigns for performance <em>and</em> efficiency.</li>
<li>Arm reigns for price and specific workloads.</li>
<li>Intel, well... <a href="https://www.investopedia.com/intel-stock-one-disaster-after-another-chip-stocks-wall-street-8710894">they&#39;re really having a year, aren&#39;t they?</a></li>
</ul>

<p>But Ampere <a href="https://amperecomputing.com/blogs/introducing-ampereone-aurora">already announced a 256 and <em>512</em>-core variant</a>, which will also bump the memory channels from 8 to 12. That could be a <em>monster</em>, but... it&#39;s not here yet.</p>

<p>This server <em>isn&#39;t</em> targeted at HPC, or High Performance Computing.</p>

<p>Cores are great, but it&#39;s all about how you slice them. Don&#39;t think of this as a single 192-core server. Think of it more like <em>48</em> dedicated 4-core servers in one box. And each of those servers has 10 gigs of high-speed RAM and consistent performance.</p>

<p>Before we test that, let&#39;s dive into the hardware.</p>

<h2>Hardware - Exterior</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampereone-remove-pcie-bracket.jpeg" alt="AmpereOne server removing PCIe bracket"/></p>

<p>And right away, you might be confused. Usually servers have their ports on the <em>back</em>, not the front. But this server is meant for Telco use for 5G, so it&#39;s short-depth, and all the ports are on the front.</p>

<p>Across the front, there are 6 U.2 NVMe drive bays, populated in my system with 2 TB Samsung NVMe drives. Then there&#39;s a micro USB serial port for console access, two USB 3 ports, and VGA for monitoring. Then there&#39;s a 1 Gbps IPMI port connected to the ASPEED BMC chip, which is running OpenBMC for remote administration. Finishing out the built-in IO, there are two 25 Gbps SFP28 ports, for high-speed networking.</p>

<p>Then there are cutouts where you can install more PCI Express cards. There&#39;s an OCP 3 slot (usually populated by more networking, like dual 100 Gbps Mellanox cards), and up to four single-height, or two dual-height PCIe cards. Two slots are PCIe Gen 5 x16, and two are PCIe Gen 5 x8, and in total this system has 4 <em>Teratransfers</em> per second of PCI Express bandwidth, but who&#39;s counting?</p>

<p>The sides have mounting points for rails, and the back... doesn&#39;t have any ports at all! Well, except for power plugs for dual redundant 1600W Titanium PSUs.</p>

<p>The rest of the back is covered by four giant hot-swap fans with louvres on the back side. This was the first time I&#39;d see louvres on server fans, and I couldn&#39;t find any good info about this design online, so I got an answer from Roger Chen, Supermicro&#39;s Senior Director of System Design Engineering.</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampereone-fan-spinning.jpeg" alt="AmpereOne hot swap fan spinning"/></p>

<p>He said it&#39;s so air only comes in through the front. <em>Makes sense!</em> If a fan fails, its doors will close and no air can come in from the hotter back side of the server. And as illustrated in the picture above, the fans have a lot of momentum, so don&#39;t stick your finger in the blades if you just yanked it out!</p>

<p>The system&#39;s designed with N+1 fan redundancy, a term more familiar to <a href="https://mainstream-corp.com/n-plus-1-vs-n-minus-1-redundancy/">those in the HVAC industry</a>. Basically, fans <em>will</em> fail, so like every other part of this server, it&#39;s designed to keep the CPU running full blast even with a dead fan.</p>

<p>Anyway, enough geeking out over a fan. Let&#39;s get inside.</p>

<h2>Hardware - Interior</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampereone-cpu-ram.jpeg" alt="AmpereOne CPU and RAM"/></p>

<p>Besides two screws retaining the lid, most parts on this system are toolless, which makes maintenance a breeze. Two PCIe risers are lifted out of the way to expose the motherboard, the OCP 3 card slot, and a plastic shroud guiding airflow around the CPU and RAM.</p>

<p>Pop that up, and you reveal a 22110 M.2 NVMe slot, two <em>additional</em> unused PCIe Gen 5 x8 connectors, and the CPU and 16 RAM slots. Since this system was configured for <em>speed</em> over raw capacity, my system only had one DIMM per channel, allowing the RAM to clock in at DDR5 5200 MT/sec.</p>

<p>The CPU itself uses direct-die cooling—at least for the massive CPU core tile in the middle—so I didn&#39;t want to risk removing the heatsink and damaging anything. Luckily, Patrick over on ServeTheHome did just that, and you can go look at his <a href="https://www.servethehome.com/this-is-ampere-ampereone-a192-32x-a-192-core-arm-server-cpu-arm/">pictures of the A192-32X on STH</a>.</p>

<p>The IO and memory dies surround the central CPU cores, but they are covered by a rectangular heat spreader. The heatsink provides the necessary pressure to contact the <em>nearly 6,000 pins</em> in the LGA5964 socket.</p>

<p>Elsewhere, there are extra power sockets for internal PCIe cards (e.g. workstation or enterprise GPUs), tidy cabling for NVMe slots, redundant BMC firmware, and... it&#39;s fairly well crammed in, being a short-depth server!</p>

<h2>Telco Edge Word Soup</h2>

<p>This isn&#39;t an HPC (High Performance Computing) server; it probably wouldn&#39;t be used to build a Top500 supercomputer—though you could if you tried.</p>

<p>This unit is built for Telco Edge, or more specifically, 5G Open RAN.</p>

<p>Edge is just a fancy way of saying &#34;we put all our servers in the cloud, but then people started noticing latency was worse, so now we&#39;re putting some servers back closer to where people use them.&#34;</p>

<p>There are <em>thousands</em> of remote cell sites and regional mini-datacenters. If you can <a href="https://stlpartners.com/articles/edge-computing/what-is-edge-computing/">cache the most popular stuff you run on your cell network</a> on a server running at the &#39;edge&#39;, like in a regional facility or at individual cell sites, that saves a lot over cloud solutions like AWS.</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampereone-nvme-bays.jpeg" alt="AmpereOne 6 U.2 NVMe SSD bays"/></p>

<p>Telco companies run a <em>lot</em> of services. So having a machine like this running 48 4-core VMs with great performance for each service makes sense. Using the 6 U.2 slots, you could cache a hundred <em>terabytes</em> of data at the edge, all at ridiculously high speeds.</p>

<p>This Supermicro system is certified at <a href="https://en.wikipedia.org/wiki/Network_Equipment-Building_System">NEBS level 3</a>, which just means all the parts have been tested for deployment in Telco exchanges. There are standards for fire suppression, vibration resistance, airflow, redundancy, and efficiency limits.</p>

<p>ORAN, or Open RAN, stands for <a href="https://www.keysight.com/us/en/assets/7121-1103/ebooks/The-Essential-Guide-for-Understanding-O-RAN.pdf">Open Radio Access Networks</a>. Vendors like Ampere and Supermicro are standards-based, meaning they don&#39;t throw in proprietary connections or software. You drop in one of these servers use it for 5G right away.</p>

<h2>Other uses</h2>

<p>You can configure these servers for other things too, like for GPU workloads. That could accelerate machine learning or LLMs even better.</p>

<p>You can also build them as CI servers, if you develop software for cars, Macs, or other Arm platforms.</p>

<p>Finally, <em>webservers</em> and web apps are a sweet spot for the custom Arm cores. You get a similar efficiency advantage as <a href="https://aws.amazon.com/ec2/graviton/">AWS Graviton</a> or <a href="https://cloud.google.com/blog/products/compute/introducing-googles-new-arm-based-cpu">Google Axion</a>, except... you can actually <em>buy one of these</em> and run it in your own datacenter.</p>

<h2>Getting it running</h2>

<p>The short depth made it easy to install in my rack. I put in the rails, and could haul it over to the rack without assistance.</p>

<p>Right after I installed it, though, I found one of the RAM sticks was spitting out &#39;correctable ECC&#39; errors during benchmarks. Ampere quickly shipped a replacement, and it was easy pulling the server and replacing the stick of RAM right in the rack.</p>

<p>My initial testing uncovered a few quirks. Ampere shipped this with a 64 kilobyte page size for the Linux kernel. This is actually <a href="https://www.phoronix.com/review/aarch64-64k-kernel-perf/3">way more efficient than the 4K default</a> most people use. And that&#39;s why Apple and even Raspberry Pi switched to 16K now.</p>

<p>But if some software bails out on 16K, even <em>more</em> programs don&#39;t know what to do when you try running &#39;em on 64K! And <a href="http://support.primatelabs.com/discussions/geekbench/86728-cant-run-geekbench-6-arm-preview-on-ampereone-192-core-system">Geekbench 6 is one of those programs</a>.</p>

<p>But then after discussing this with Patrick from STH, I realized something he concluded much earlier: <a href="https://www.servethehome.com/a-reminder-that-geekbench-6-is-not-for-big-cpus/">Geekbench 6 is a <em>really</em> bad benchmark for servers with big CPUs</a>!</p>

<p>I switched to a 4K kernel and ran Geekbench 5 instead, and I got a score <a href="https://browser.geekbench.com/v5/cpu/23018587">over <em>80,000</em> multi-core</a>. That&#39;s <a href="https://browser.geekbench.com/v5/cpu/multicore?page=10">not the world record</a>, but it&#39;s up there with much more expensive AMD EPYC chips, at least.</p>

<h2>Performance</h2>

<p>All my benchmark results and testing notes are documented in my sbc-reviews repo: <a href="https://github.com/geerlingguy/sbc-reviews/issues/52">AmpereOne A192-32X (Supermicro)</a>. Yes, this is a bit more than an SBC, but that&#39;s where I put my results, so deal with it :)</p>

<p>Unsurprisingly, on my 25 Gbps network, I got 20 gigabits per second between my <a href="https://github.com/geerlingguy/arm-nas">ZFS Arm NAS</a> and this server.</p>

<p>With Samba, I could copy files over the network between between 8 to 15 Gigabits, which makes sense with all the overhead of Samba and Ethernet.</p>

<p>Even though it&#39;s a little silly, I kicked things off with an HPC benchmark: HPL, or High Performance Linpack.</p>

<p>Letting the thing rip, the 192 core CPU gets <a href="https://github.com/geerlingguy/top500-benchmark/issues/43"><em>three teraflops</em> of FP64 compute</a>. That&#39;s almost <a href="https://www.pugetsystems.com/labs/hpc/NVIDIA-RTX4090-ML-AI-and-Scientific-Computing-Performance-Preliminary-2382/#HPL_Linpack">3x as fast as an Nvidia 4090</a>. Granted, that card isn&#39;t targeted at the same kind of workloads.</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampere-apple-m4-top500-efficiency.jpg" alt="AmpereOne vs Apple M4 Efficiency"/></p>

<p>During that run, the system used almost 700W, making it <em>nearly</em> the most efficient Arm computer I&#39;ve ever tested. That is, until Apple released the M4! But over 4 Gflops/W is still pretty awesome!</p>

<p>If I bump the clock speed down to 2.6 Gigahertz, and unplug the power-hungry NVMe drives, I can knock out 4.82 Gflops/W, which is frankly amazing.</p>

<p>I don&#39;t have an AMD Turin system to test, but I&#39;m in awe that we can build 2U servers with this much power and efficiency in a single socket.</p>

<p>The Turin CPU may be more efficient overall, but one thing the AmpereOne excels at is native arm64 software. To that end, Ampere actually built a specialty benchmark: <a href="https://github.com/AmpereComputing/qemu-coremark"><code>qemu-coremark</code></a>.</p>

<p>It sets up as many 4-core Arm VMs as possible, and runs coremark inside each. <em>Obviously</em> this is slanted in Ampere&#39;s favor, but if you <em>do</em> have Arm native software, like if you&#39;re running web services or developing for cars which mostly run on Arm, you might be interested in the results.</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/qemu-coremark-benchmark-result-ampereone.jpg" alt="QEMU Coremark arm64 VM benchmark results"/></p>

<p>On this server, I get an aggregate score of about 4.7 million. On a <em>256-core</em> Intel Granite Rapids system with two sockets—one that Wendell from Level1Techs tested—it only gets 1.25 million. That&#39;s with 64 more physical CPU cores on a hyperthreaded machine—but of course it&#39;s emulating Arm.</p>

<p>Now I have to be completely transparent: Ampere is probably using coremark here because it&#39;s one of the few benchmarks where <a href="https://www.phoronix.com/review/amd-epyc-9965-ampereone/3">Phoronix found the AmpereOne system still beats AMD</a>. So not only will it perform worse core-for-core, x86 gets slaughtered if it has to run it emulating the arm64 instruction set!</p>

<p>But if you run Arm-native software, this is a very real picture of the speedup you get running it on Arm-native servers.</p>

<p>I ran through a gauntlet of other tests, too. Linux compiles in under a minute. 64 megabyte memory reads have &lt; 50ns latency. And the PCIe Gen 4 NVMe drives it shipped with hit 8 GB/sec in RAID 0.</p>

<p>Also, with 512 gigs of RAM and a massive CPU, it can run a 405 <em>billion</em> parameter Large Language Model. It&#39;s not <em>fast</em>, but it did run, giving me just under a token per second.</p>

<p>Ampere has an <a href="https://github.com/AmpereComputingAI/llama.cpp">optimized version of llama.cpp</a> that can run models even faster.</p>

<p>I&#39;m much more comfortable benchmarking SBCs than servers, though. For raw numbers and more thorough analysis, I&#39;d suggest you read through other articles:</p>

<ul>
<li>Phoronix: <a href="https://www.phoronix.com/review/ampereone-a192-32x">192 Core ARM Server Performance &amp; Power Efficiency</a></li>
<li>Serve The Home: <a href="https://www.servethehome.com/ampere-ampereone-a192-32x-review-a-192-arm-core-supermicro-nvidia-broadcom-kioxia-server-cpu/">AmpereOne A192-32X Review: A 192 Arm Core Server CPU</a></li>
<li>Chips and Cheese: <a href="https://chipsandcheese.com/p/ampereone-at-hot-chips-2024-maximizing-density">AmpereOne at Hot Chips 2024: Maximizing Density</a></li>
</ul>

<h2>Quirks and Growth</h2>

<p>One thing I don&#39;t like about AmpereOne, at least in its current state: idle power consumption.</p>

<p>Doing nothing at all, this machine is burning 200W of power. Removing most of the RAM and the NVMe drives lowers that a little, but it&#39;s still a <em>lot</em> of power if you don&#39;t run heavy workloads all day.</p>

<p>Having 128 lanes of PCIe is a factor, but even Intel and AMD have better idle states, to the point some EPYC systems idle well under 100W.</p>

<p>AmpereOne is also late. There was a ton of buzz back when it was announced, <a href="https://amperecomputing.com/press/ampere-unveils-processor-ampereone-192-cores">back in May—of <em>2023</em></a>.</p>

<p>If they had shipped by the end of 2023, Ampere could&#39;ve reigned supreme at the top of the server CPU market for performance <em>and</em> efficiency, if only for a little while.</p>

<p>But for whatever reason, the actual shipping CPUs didn&#39;t start rolling out until late this year, after AMD stole their thunder with &#39;Turin Dense&#39;.</p>

<p>The other problem with the year-plus delay is how it sets expectations moving forward.</p>

<p>Ampere announced the <a href="https://amperecomputing.com/blogs/introducing-ampereone-aurora">Aurora</a>, a 512-core monster CPU with 12 memory channels, back in June. Will it ship in 2025? It&#39;s anyone&#39;s guess. But I do hope so.</p>

<p>The other theme this year is <em>Arm is not niche</em> anymore.</p>

<p>Arm&#39;s grown up. I&#39;ve been running a ZFS NAS with hundreds of gigs of ECC RAM, 25 gig networking, U.2 NVMe storage, and a hundred terabytes of hard drives, for almost a year now. Every single video I&#39;ve uploaded in the last 6 months was edited on its disks.</p>

<p>The idea that Arm is experimental, or that you can&#39;t run something on Arm, is outdated. Even high-end Arm SBCs running the RK3588 are fast enough to handle edge server use cases.</p>

<p>Some server software, like TrueNAS, <a href="https://forums.truenas.com/t/truenas-scale-on-arm-2024-thread/2706">still doesn&#39;t support Arm</a>, but the amount of software that <em>doesn&#39;t</em> run on Arm is shrinking by the day.</p>

<p>Only Windows and certain niche server apps seem to be lagging. I already showed off how Windows on Arm runs <a href="https://www.youtube.com/watch?v=thz5S_uciHk">better on the Ampere Altra than on Microsoft&#39;s own Arm PCs</a>.</p>

<p>Of course, Linux is still better if you want: GPU support, better performance, and drivers for practically anything. But it&#39;s nice to see Microsoft slowly joining the Arm party.</p>

<p>Microsoft&#39;s been running Ampere in their own cloud service <a href="https://azure.microsoft.com/en-us/blog/azure-virtual-machines-with-ampere-altra-arm-based-processors-generally-available/">since 2022</a>! The obvious question is why doesn&#39;t Microsoft support Windows on Arm on the only CPUs that can run it to its full potential?</p>

<h2>Conclusion</h2>

<p>But let&#39;s not get sidetracked. My main takeaway today is we&#39;re seeing the death of certain computing myths.</p>

<ul>
<li><strong>&#34;x86 has faster single core performance&#34;</strong>: Apple&#39;s ruled that out with their M4</li>
<li><strong>&#34;Arm is more efficient&#34;</strong>: that&#39;s not always true—AMD just built the most efficient 192 core server this year, beating Ampere!</li>
</ul>

<p>The big difference is the AmpereOne A192-32X <a href="https://www.phoronix.com/review/ampereone-a192-32x">is $5,555</a>, while the EPYC 9965 is <a href="https://www.amd.com/en/newsroom/press-releases/2024-10-10-amd-launches-5th-gen-amd-epyc-cpus-maintaining-le.html">almost <em>$15,000</em></a>!</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/ampereone-s-tui-stable.jpeg" alt="AmpereOne s-tui running stable at full speed"/></p>

<p>If you want the fastest, most efficient server CPU, that&#39;s the EPYC. But if you want the best <em>value</em>, the best performance per dollar, it&#39;s the AmpereOne—at least assuming the list prices are anywhere near reality. And I think that&#39;s why Amazon, Google, Microsoft—all the cloud providers, really—can charge less for more, when they run on Arm.</p>

<p>We&#39;re in a strange place. Arm and x86 are both valid options now, depending on what you want, whether that&#39;s the best performance, or the best value.</p>

<p>The 192-core AmpereOne arrived a year too late to be the smash hit I originally expected, but it still could win out for value this generation.</p>

<p>I&#39;m also happy to see systems like this that run without exotic water cooling and 240V power. Some high-end enterprise servers will be impossible to repurpose for a homelab in a few years, but this one isn&#39;t.</p>

<hr/>

<p><em>Thanks to Supermicro and Ampere for providing the server. Thanks to Wendell from Level1Techs, Patrick from ServeTheHome, and Jeff from Craft Computing for their assistance in running some benchmarks.</em></p></div></div>
  </body>
</html>
