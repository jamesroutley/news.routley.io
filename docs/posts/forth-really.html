<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rescrv.net/w/2026/02/06/associative">Original</a>
    <h1>FORTH? Really!?</h1>
    
    <div id="readability-page-1" class="page"><div>
<article>
  <section id="content">
    
    <div>
      <p>Imagine you have to generate the word that succeeds this colon: <strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong></p>
<p>What would you put in that blank space?</p>
<p>It’s easier when the question comes first.</p>
<p>But what if we structured things such that the blank had to be generated before its constituent
parts.  LLMs are wonderful, but I see too many people try to break down recursively to solve
problems like top-down humans do.  Instead, I posit that <span>FORTH</span> and associative/applicative languages
may be better for transformer architectures.  Concatenate, not integrate.  Agree on the stack state.</p>
<p>I set out to question if this could be true.</p>
<h2>Sideways Passing Join.</h2>
<p>Imagine you had this program:</p>
<div><pre><span></span><code>A SCAN [foo &gt; 5] FILTER
B SCAN [foo &lt; 5] FILTER
BUILD
PROBE
</code></pre></div>

<p>that performs a natural join on A and B’s shared identifiers.</p>
<p>Because of the properties of associative languages you can always make local edits.  For example,
if you made a sed-like transformation, you could replace <code>BUILD PROBE</code> with the following anywhere
there’s a <code>BUILD PROBE</code> sequence to do a sideways-information-passing join:</p>
<div><pre><span></span><code><span>DUP</span><span> </span><span>STATS</span><span> </span><span>SWAP</span><span> </span><span>BUILD</span>
<span>[</span><span>PUSHDOWN</span><span>]</span><span> </span><span>DIP</span><span> </span><span>PROBE</span>
</code></pre></div>

<p>This same associative property allows us to divide a program into, “What’s been generated
in-context,” and, “What remains to be generated.”  We shuffle one token at a time to extend the
context and consume our desire to generate tokens.</p>
<p>I have a hunch that transformations of finite automatons over subsequences of the text can be used
to write optimization passes for the database layer.</p>
<p>A phrase from Manfred von Thun goes, “syntactic concatenation is semantic composition.”</p>
<h2>A Benchmark</h2>
<p>I set out to benchmark what models can do in this regard.  Would the order of terms matter to an
attention transformer?  The experiment is simple:  I want to construct a tree over numbers and
measure when the tree conforms to instructions.  In my experiment I used parity to assess whether
the sum of a sub-tree’s children were even or odd.  Thus, prefix notation needs to know the overall
answer before it generates the sub-answers.  Postfix notation generates bottom-up, generating
sub-answers before answering further.</p>
<p>If you think about how you answer, “What is the next token,” you’ll see where I’m going.</p>
<h3>Setup</h3>
<p>Given:  A sequence of numbers.
Construct:  A prefix or postfix parity tree.</p>
<p>What is a parity tree?  An unbalanced, left- or right-skewed binary tree whose leaves are numbers
and whose interior nodes represent the parity of their transitive children.</p>
<h3>Results</h3>
<p>I ran four trials across Opus and Haiku (sonnet gave results I need to better understand before I’ll
publish).  Thinking consistently outperforms non-thinking.  Opus consistently outperforms Haiku.
And postfix consistently outperforms prefix.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Thinking</th>
<th>Postfix Acc</th>
<th>Prefix Acc</th>
<th>Both Correct</th>
<th>Postfix Only</th>
<th>Prefix Only</th>
<th>Both Wrong</th>
</tr>
</thead>
<tbody>
<tr>
<td>Haiku</td>
<td>Yes</td>
<td>88.3%</td>
<td>36.7%</td>
<td>110</td>
<td>155</td>
<td>0</td>
<td>35</td>
</tr>
<tr>
<td>Haiku</td>
<td>No</td>
<td>6.7%</td>
<td>4.3%</td>
<td>9</td>
<td>11</td>
<td>4</td>
<td>276</td>
</tr>
<tr>
<td>Opus</td>
<td>Yes</td>
<td>98.3%</td>
<td>81.3%</td>
<td>243</td>
<td>52</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>Opus</td>
<td>No</td>
<td>50.0%</td>
<td>9.7%</td>
<td>28</td>
<td>122</td>
<td>1</td>
<td>149</td>
</tr>
</tbody>
</table>
<p>All makes sense in the world.</p>
    </div>
  </section>
</article><!-- /.card -->
        </div></div>
  </body>
</html>
