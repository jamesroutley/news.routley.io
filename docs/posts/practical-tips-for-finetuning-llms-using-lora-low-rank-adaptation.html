<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">Original</a>
    <h1>Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><a href="https://arxiv.org/abs/2106.09685" rel="">Low-rank adaptation</a><span> (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it&#39;s an essential technique worth familiarizing oneself with.</span></p><p><a href="https://lightning.ai/pages/community/lora-insights/" rel="">Last month, I shared an article with several LoRA experiments</a><span>, based on the open-source </span><a href="https://github.com/Lightning-AI/lit-gpt" rel="">Lit-GPT repository</a><span> that I co-maintain with my colleagues at Lightning AI. This Ahead of AI article aims to discuss the primary lessons I derived from my experiments. Additionally, I&#39;ll address some of the frequently asked questions related to the topic.  If you are interested in finetuning custom LLMs, I hope these insights will save you some time in &#34;the long run&#34; (no pun intended).</span></p><p>In brief, the main takeaways I am discussing in this article are the following:</p><ol><li><p>Despite the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.</p></li><li><p>QLoRA presents a trade-off that might be worthwhile if you&#39;re constrained by GPU memory. It offers 33% memory savings at the cost of a 39% increase in runtime.</p></li><li><p>When finetuning LLMs, the choice of optimizer shouldn&#39;t be a major concern. While SGD on its own is suboptimal, there&#39;s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.</p></li><li><p>While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn&#39;t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.</p></li><li><p>For static datasets, iterating multiple times, as done in multi-epoch training, might not be beneficial. It often deteriorates the results, probably due to overfitting.</p></li><li><p>If you&#39;re incorporating LoRA, ensure it&#39;s applied across all layers, not just to the Key and Value matrices, to maximize model performance.</p></li><li><p>Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank&#39;s value.</p></li><li><p>7 billion parameter models can be finetuned efficiently within a few hours on a single GPU possessing 14 GB of RAM. With a static dataset, optimizing an LLM to excel across all benchmark tasks is unattainable. Addressing this requires diverse data sources, or perhaps LoRA might not be the ideal tool.</p></li></ol><p>In addition, I will answer ten common questions around LoRA:</p><ul><li><p>Q1: How Important is the Dataset?</p></li><li><p>Q2: Does LoRA Work for Domain Adaptation?</p></li><li><p>Q3: How Do You Select the Best Rank?</p></li><li><p>Q4: Does LoRA Need to Be Enabled for All Layers?</p></li><li><p>Q5: How To Avoid Overfitting?</p></li><li><p>Q6: What about Other Optimizers?</p></li><li><p>Q7: What Other Factors Influence Memory Usage?</p></li><li><p>Q8: How Does it Compare to Full Finetuning and RLHF?</p></li><li><p>Q9: Can LoRA Weights be Combined?</p></li><li><p>Q10: What about Layer-wise Optimal Rank Adaptation?</p></li></ul><p>(In the previous issue of AI, I mentioned that I wanted to write a more general introduction with a from-scratch code implementation of LoRA sometime if there&#39;s interest. According to your feedback, there&#39;s a lot of interest, and I plan to share another article on LoRA in the future. For now, this article is focused on the broader ideas and takeaways from working with LoRA—a top-down view.)</p><p>Large language models are large, and it can be expensive to update all model weights during training due to GPU memory limitations. </p><p><span>For example, suppose we have an LLM with 7B parameters represented in a weight matrix </span><em>W</em><span>. (In reality, the model parameters are, of course, distributed across different matrices in many layers, but for simplicity, we refer to a single weight matrix here). During backpropagation, we learn a </span><em>ΔW</em><span> matrix, which contains information on how much we want to update the original weights to minimize the loss function during training.</span></p><p>The weight update is then as follows:</p><p><em>W</em><sub>updated</sub><span> = </span><em>W</em><span> + </span><em>ΔW</em></p><p><span>If the weight matrix </span><em>W</em><span> contains 7B parameters, then the weight update matrix </span><em>ΔW</em><span> also contains 7B parameters, and computing the matrix </span><em>ΔW</em><span> can be very compute and memory intensive.</span></p><p><span>The LoRA method proposed by </span><a href="https://arxiv.org/abs/2106.09685" rel="">Hu </a><em><a href="https://arxiv.org/abs/2106.09685" rel="">et al.</a></em><span> replaces to decompose the weight changes, </span><em>ΔW</em><span>, into a lower-rank representation. To be precise, it does not require to explicitly compute </span><em>ΔW</em><span>. Instead, LoRA learns the decomposed representation of </span><em>ΔW</em><span> directly during training which is where the savings are coming from, as shown in the figure below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png" width="1456" height="612" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:612,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>As illustrated above, the decomposition of </span><em>ΔW</em><span> means that we represent the large matrix </span><em>ΔW</em><span> with two smaller LoRA matrices, </span><em>A</em><span> and </span><em>B</em><span>. If </span><em>A</em><span> has the same number of rows as </span><em>ΔW</em><span> and </span><em>B</em><span> has the same number of columns as </span><em>B</em><span>, we can write the decomposition as </span><em>ΔW = AB</em><span>. (</span><em>AB </em><span>is the matrix multiplication result between matrices </span><em>A</em><span> and </span><em>B</em><span>.) </span></p><p><span>How much memory does this save? It depends on the rank </span><em>r</em><span>, which is a hyperparameter. For example, if </span><em>ΔW</em><span> has 10,000 rows and 20,000 columns, it stores 200,000,000 parameters. If we choose </span><em>A</em><span> and </span><em>B</em><span> with </span><em>r=8</em><span>, then </span><em>A</em><span> has 10,000 rows and 8 columns, and </span><em>B</em><span> has 8 rows and 20,000 columns, that&#39;s 10,000×8 + 8×20,000 = 240,000 parameters, which is about 830× less than 200,000,000.</span></p><p><span>Of course, </span><em>A</em><span> and </span><em>B</em><span> can&#39;t capture all the information that </span><em>ΔW</em><span> could capture, but this is by design. When using LoRA, we hypothesize that the model requires </span><em>W</em><span> to be a large matrix with full rank to capture all the knowledge in the pretraining dataset. However, when we finetune an LLM, we don&#39;t need to update all the weights and capture the core information for the adaptation in a smaller number of weights than </span><em>ΔW</em><span> would; hence, we have the low-rank updates via </span><em>AB</em><span>.</span></p><p>Running multiple experiments with LoRA, I found that the benchmark results are surprisingly consistent across the different runs despite the inherent randomness of LLM training or when training models on GPUs in general. This is a good basis for additional comparison studies.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png" width="1244" height="278" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:278,&#34;width&#34;:1244,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>(Note that the results were obtained with default settings using a small </span><em>r=8</em><span>. The experimental details can be found in my other article </span><a href="https://lightning.ai/pages/community/lora-insights/" rel="">here</a><span>.)</span></p><p><a href="https://arxiv.org/abs/2305.14314" rel="">QLoRA by Dettmers</a><em><a href="https://arxiv.org/abs/2305.14314" rel=""> et al.</a></em><span>, short for quantized LoRA, is a technique that further reduces memory usage during finetuning. During backpropagation, QLoRA quantizes the pretrained weights to 4-bit precision and uses paged optimizers to handle memory spikes.</span></p><p>Indeed, I found that one can save 33% of GPU memory when using LoRA. However, this comes at a 39% increased training runtime caused by the additional quantization and dequantization of the pretrained model weights in QLoRA.</p><p>Default LoRA with 16-bit brain floating point precision:</p><ul><li><p>Training time: 1.85 h</p></li><li><p>Memory used: 21.33 GB</p></li></ul><p><span>QLoRA with 4-bit </span><em>Normal Floats:</em></p><ul><li><p>Training time: 2.79 h</p></li><li><p>Memory used: 14.18 GB</p></li></ul><p>Moreover, I found that the modeling performance was barely affected, which makes QLoRA a feasible alternative to regular LoRA training to work around the common GPU memory bottleneck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png" width="1240" height="216" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:216,&#34;width&#34;:1240,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>Learning rate schedulers lower the learning rate throughout the training to optimize convergence and avoid overshooting the loss minima. </p><p>Cosine annealing is a learning rate scheduler that adjusts the learning rate following a cosine curve. It starts with a high learning rate, which then decreases smoothly, approaching zero in a cosine-like manner. A commonly used variant is the half-cycle variant, where only a half-cosine cycle is completed over the course of training, as shown in the figure below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png" width="441" height="330.75" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ebde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1092,&#34;width&#34;:1456,&#34;resizeWidth&#34;:441,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>As part of my experiments, I added a cosine annealing scheduler to the LoRA finetuning scripts and observed that it improved the SGD performance noticeably. However, it has less impact on Adam and AdamW optimizers and makes almost no difference.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png" width="1368" height="286" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:286,&#34;width&#34;:1368,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>The potential advantages of SGD over Adam are discussed in the next section.</p><p>Adam and AdamW optimizers remain popular choices in deep learning even though they are very memory-intensive when we are working with large models. The reason is that Adam optimizers maintain two moving averages for each model parameter: the first moment (mean) of the gradients and the second moment (uncentered variance) of the gradients. In other words, Adam optimizers store two additional values for each single model parameter in memory. If we are working with a 7B parameter model, that&#39;s an extra 14B parameters to track during training.</p><p>SGD optimizers don&#39;t need to track any additional parameters during training, so a question is: what advantage does swapping Adam by SGD have on the peak memory requirements when training LLMs? </p><p><span>In my experiments, training a 7B parameter Llama 2 model trained with AdamW and LoRA defaults (</span><em>r=8</em><span>) required 14.18 GB of GPU memory. Training the same model with SGD instead required 14.15 GB of GPU memory. In other words, the savings (0.03 GB) were minimal. </span></p><p><span>Why are the memory savings so small? That&#39;s because with LoRA, we only have a small number of trainable parameters. For instance, if </span><em>r=8</em><span>, we have 4,194,304 trainable LoRA parameters out of all 6,738,415,616 parameters in a 7B Llama 2 model. </span></p><p>If we just look at the bare numbers, 4,194,304 trainable parameters still sound like a lot, but if we do the math, we only have 4,194,304 × 2 × 16 bit = 134.22 megabits = 16.78 megabytes. (We observed a 0.03 Gb = 30 Mb difference since there is an additional overhead in storing and copying optimizer states.) The 2 represents the number of extra parameters that Adam stores, and the 16-bit refers to the default precision for the model weights.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png" width="409" height="289.6756238003839" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/b9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:738,&#34;width&#34;:1042,&#34;resizeWidth&#34;:409,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>However, if we increase the LoRA </span><em>r</em><span> to 256, something I&#39;ve done in later experiments, the difference between Adam and SGD optimizers becomes more noticeable:</span></p><ul><li><p>17.86 GB with AdamW</p></li><li><p>14.46 GB with SGD</p></li></ul><p><span>As a takeaway, swapping Adam optimizers with SGD may not be worthwhile when LoRA&#39;s </span><em>r</em><span> is small. However, it may be worthwhile when we are increasing </span><em>r</em><span>. </span><br/></p><p>In conventional deep learning, we often iterate over a training set multiple times -- an iteration over the training set is called an epoch. It&#39;s common to run hundreds of training epochs when training convolutional neural networks, for example. Is multi-epoch training useful for instruction finetuning as well?</p><p><span>When I increased the number of iterations for the </span><a href="https://github.com/tatsu-lab/stanford_alpaca" rel="">50k-example Alpaca</a><span> instruction finetuning dataset by a factor of two (analogous to 2 training epochs), I noticed a decline in model performance.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png" width="1358" height="186" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:186,&#34;width&#34;:1358,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>The takeaway is that multi-epoch training might not benefit instruction finetuning since it can deteriorate the results. I observed the same with the 1k-example LIMA dataset. This performance decline is likely due to increased overfitting, which warrants additional investigation.</p><div><div><p>Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.</p><div data-component-name="SubscribeWidget"><div><div><form action="/api/v1/free?nojs=true" method="post" novalidate=""><div></div></form></div></div></div></div></div><p>The tables above showed experiments where LoRA was only enabled for select weight matrices, i.e., the Key and Value weight matrices in each transformer layer. In addition, we can also enable LoRA for the Query weight matrices, the projection layers, the other linear layers between the multihead attention blocks, and the linear output layer. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png" width="309" height="257.11567164179104" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:892,&#34;width&#34;:1072,&#34;resizeWidth&#34;:309,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>If we enable LoRA for all these additional layers, we increase the number of trainable parameters by a factor of 5, from 4,194,304 to 20,277,248, for a 7B Llama 2 model. This also comes with a larger memory requirement (16.62 GB instead of 14.18 GB) but can increase the modeling performance noticeably.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png" width="1364" height="168" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/f34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:168,&#34;width&#34;:1364,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:62148,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>However, a limitation of my experiment is that I only explored two settings: (1) LoRA for only the query and value weight matrices enabled, and (2) LoRA for all layers enabled. It might be worthwhile exploring the other combinations in future experiments. For example, it would be interesting to know whether activating LoRA for the projection layer is actually beneficial.</p><p><span>As the </span><a href="https://arxiv.org/abs/2106.09685" rel="">original LoRA paper</a><span> outlines, LoRA introduces an additional scaling coefficient for applying the LoRA weights to the pretrained weights during the forward pass. The scaling involves the rank parameter r, which we discussed earlier, as well as another hyperparameter α (alpha) that is applied as follows:</span></p><pre><code>scaling = alpha / r
weight += (lora_B @ lora_A) * scaling </code></pre><p>As we can see in the code formula above, the larger the influence of the LoRA weights.</p><p><span>Previous experiments used </span><em>r=8</em><span> and </span><em>alpha=16</em><span>, which resulted in a 2-fold scaling. Choosing alpha as two times r is a common rule of thumb when using LoRA for LLMs, but I was curious if this still holds for larger r values. In other words, “alpha = 2×rank” really seems to be a sweet spot.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png" width="1366" height="528" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/c6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:528,&#34;width&#34;:1366,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:188324,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6bc4d04-7b49-45d1-a198-817232e98ece_1366x528.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>(I experimented with </span><em>r=32</em><span>, </span><em>r=64</em><span>, </span><em>r=128</em><span>, and </span><em>r=512</em><span> but omitted the results for clarity as </span><em>r=256</em><span> resulted in the best performance.)</span></p><p><span>Indeed, the choosing alpha as two times as large as </span><em>r</em><span> resulted in the best outcomes.</span></p><p><span>One of the main takeaways is that LoRA allows us to finetune 7B parameter LLMs on a single GPU. In this particular case, using QLoRA with the best setting (</span><em>r=256</em><span> and </span><em>alpha=512</em><span>), this 17.86 GB with AdamW takes about 3 hours (on an A100) for 50k training examples (here, the Alpaca dataset).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png" width="1370" height="174" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d309a2b3-6087-4094-a280-bc60feca149a_1370x174.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:174,&#34;width&#34;:1370,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>In the remaining sections of this article, I am answering additional questions you might have.</span><br/></p><h2><strong>Q1: How Important is the Dataset?</strong><div id="§q-how-important-is-the-dataset"><div></div></div></h2><p>The dataset can be critical. I used the Alpaca dataset, which contains 50k training examples, for my experiments. I chose this dataset because it&#39;s quite popular, and experimenting with different datasets was out of scope due to the already extensive length of the article.</p><p>However, it&#39;s worth noting that Alpaca is a synthetic dataset that was generated by querying an old version of ChatGPT and is probably not the best by today&#39;s standards. </p><p><span>Data quality can be very important. For example, in June, I discussed the LIMA dataset (</span><a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset" rel="">Ahead of AI #9: LLM Tuning &amp; Dataset Perspectives</a><span>), a curated dataset consisting of only 1k examples.</span></p><p><span>According to the </span><a href="https://arxiv.org/abs/2305.11206" rel="">LIMA: Less Is More for Alignment</a><span> paper, a 65B Llama model finetuned on LIMA noticeably outperforms a 65B Llama model finetuned on Alpaca.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png" width="675" height="250.3434065934066" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:540,&#34;width&#34;:1456,&#34;resizeWidth&#34;:675,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>Using the best configuration (</span><em>r=256,</em><span> </span><em>alpha=512</em><span>) on LIMA, I got similar, if not better, performance than the 50x larger Alpaca dataset.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png" width="1388" height="270" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/bc33e126-6803-4a18-837a-6525226d068b_1388x270.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:270,&#34;width&#34;:1388,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><h2><strong>Q2: Does LoRA Work for Domain Adaptation?</strong><div id="§q-does-lora-work-for-domain-adaptation"><div></div></div></h2><p>Unfortunately, I don&#39;t have a good answer to this question. As a rule of thumb, knowledge is usually absorbed from the pretraining dataset. Instruction finetuning is generally more about helping or guiding the LLM towards following instructions. </p><p>However, it&#39;s worth noting that if memory is a concern, LoRA can also be used for further pretraining existing pretrained LLMs on domain-specific datasets. </p><p><span>Note that my experiments also included two arithmetic benchmarks (they are included in </span><a href="https://lightning.ai/pages/community/lora-insights/" rel="">my other more technical write-up</a><span>), on which LoRA-finetuned models performed significantly worse than the pretrained base models. My hypothesis is that the model unlearned arithmetic because the Alpaca dataset did not contain corresponding examples. Whether the model completely lost the knowledge or whether it&#39;s because the model can&#39;t handle the instructions anymore would require further investigation. However, a takeaway here is that it&#39;s probably a good idea to include examples of each task you care about when finetuning LLMs.</span><br/></p><h2><strong>Q3: How Do You Select the Best Rank?</strong><div id="§q-how-do-you-select-the-best-rank"><div></div></div></h2><p><span>Unfortunately, I don&#39;t have any good heuristic for selecting a good </span><em>r </em><span>and think that it&#39;s a hyperparameter that needs to be explored for each LLM and each dataset. I suspect that choosing an </span><em>r </em><span>that is too large could result in more overfitting. On the other hand, a small r may not be able to capture diverse tasks in a dataset. In other words, I suspect that the more diverse the tasks in the dataset, the larger the </span><em>r</em><span> should be. For example, if I only want a model that carries out basic 2-digit arithmetic, then a tiny </span><em>r </em><span>might already be sufficient. However, this is only a hypothesis and would require additional investigation.</span></p><h2><strong>Q4: Does LoRA Need to Be Enabled for All Layers?</strong><div id="§q-does-lora-need-to-be-enabled-for-all-layers"><div></div></div></h2><p>I only explored two settings: (1) LoRA for only the query and value weight matrices enabled, and (2) LoRA for all layers enabled. It might be worthwhile exploring the other combinations in future experiments. For example, it would be interesting to know whether activating LoRA for the projection layer is actually beneficial.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg" width="271" height="229.9794921875" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:869,&#34;width&#34;:1024,&#34;resizeWidth&#34;:271,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>For instance, if we consider the various settings (</span><code>lora_query</code><span>, </span><code>lora_key</code><span>, </span><code>lora_value</code><span>, </span><code>lora_projection</code><span>, </span><code>lora_mlp</code><span>, and </span><code>lora_head</code><span>), that&#39;s </span><em>2^6 = 64</em><span> combinations to explore. This exploration would be an interesting topic for future studies.</span></p><h2><strong>Q5: How To Avoid Overfitting?</strong><div id="§q-how-to-avoid-overfitting"><div></div></div></h2><p><span>Generally, a larger </span><em>r</em><span> can lead to more overfitting because it determines the number of trainable parameters. If a model suffers from overfitting, decreasing r or increasing the dataset size are the first candidates to explore. Moreover, you could try to increase the weight decay rate in AdamW or SGD optimizers, and you can consider increasing the dropout value for LoRA layers. </span></p><p><span>The LoRA dropout parameter that I haven&#39;t explored in my experiments (I used a fixed dropout rate of 0.05), is an interesting topic for future investigations.</span><br/></p><h2><strong>Q6: What about Other Optimizers?</strong><div id="§q-what-about-other-optimizers"><div></div></div></h2><p><span>Other interesting optimizers for LLMs are worth exploring in the future. One such optimizer is </span><a href="https://arxiv.org/abs/2305.14342" rel="">Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</a><span>, which was published in May.</span></p><p><span>Sophia is a second-order optimization algorithm that promises to be particularly attractive for LLMs where Adam and AdamW are usually the dominant ones. Compared to Adam, Sophia is 2× faster, and models trained with Sophia can achieve better modeling performance, according to the paper. In a nutshell, Sophia normalizes the gradients by gradient curvature instead of gradient variance, as in Adam.</span><br/></p><h2><strong>Q7: What Other Factors Influence Memory Usage?</strong><div id="§q-what-other-factors-influence-memory-usage"><div></div></div></h2><p>Besides precision and quantization settings, the model size, the batch size, and the number of trainable LoRA parameters, the dataset can also influence memory usage.</p><p>Note that Llama 2 has a block size of 4048. For instance, if an LLM has a block size of 4048 tokens, it can process sequences of up to 4048 tokens at once. However, shorter training sequences can result in substantial memory savings due to the masking of future tokens.</p><p>For example, the Alpaca dataset is relatively small, with a maximum length of 1304 tokens.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg" width="539" height="404.25" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:960,&#34;width&#34;:1280,&#34;resizeWidth&#34;:539,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>When I experimented with other datasets that had lengths of up to 2048 tokens, I noticed that the memory usage went up from 17.86 GB to 26.96 GB. </span></p><h2><strong>Q8: How Does it Compare to Full Finetuning and RLHF?</strong><div id="§q-how-does-it-compare-to-full-finetuning-and-rlhf"><div></div></div></h2><p><span>I did not run any RLHF experiments (for those who are curious, I covered RLHF </span><a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" rel="">here</a><span>), but I did consider full finetuning. Full finetuning required at least 2 GPUs and was completed in 3.5 h using 36.66 GB on each GPU. However, the benchmark results were not very good, likely due to overfitting or suboptimal hyperparameters.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png" width="1358" height="218" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:218,&#34;width&#34;:1358,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption></figcaption></figure></div><h2><strong>Q9: Can LoRA Weights be Combined?</strong><div id="§q-can-lora-weights-be-combined"><div></div></div></h2><p>Yes, it&#39;s possible to combine multiple sets of LoRA weights. During training, we keep the LoRA weights separate from the pretrained weights and add them during each forward pass. </p><p>However, If you have a real-world application with many sets of LoRA weights, for example, one set for each application customer, it makes sense to store these weights separately to save disk space. However, it&#39;s possible to merge the pretrained weights with the LoRA weights after training to create a single model. This way, we don&#39;t have to apply the LoRA weights in each forward pass:</p><pre><code>weight += (lora_B @ lora_A) * scaling</code></pre><p>Instead, we apply the weight update as shown above and save the merged (added) weights.</p><p>Similarly, we can keep adding multiple LoRA weight sets:</p><pre><code>weight += (lora_B_set1 @ lora_A_set1) * scaling_set1
weight += (lora_B_set2 @ lora_A_set2) * scaling_set2
weight += (lora_B_set3 @ lora_A_set3) * scaling_set3
...</code></pre><p><span>I have yet to do experiments to evaluate the performance of such an approach, but this is technically already possible via the </span><a href="https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/merge_lora.py" rel="">scripts/merge_lora.py</a><span> script provided in Lit-GPT.</span><br/></p><h2><strong>Q10: What about Layer-wise Optimal Rank Adaptation?</strong><div id="§q-what-about-layer-wise-optimal-rank-adaptation"><div></div></div></h2><p><span>For simplicity, we usually train deep neural networks with the same learning rate for each layer, and the learning rate is a hyperparameter that we need to optimize. To take it further, we can also choose a different learning rate for each layer (</span><a href="https://kozodoi.me/blog/20220329/discriminative-lr#:~:text=The%20implementation%20of%20layer%2Dwise,with%20the%20corresponding%20learning%20rates." rel="">in PyTorch, this is not too complicated</a><span>). However, it&#39;s rarely done in practice because it adds additional overhead, and there are usually already so many knobs to tune when training deep neural networks.</span></p><p><span>Analogous to choosing different learning rates for different layers, we can also choose different LoRA ranks for different layers. I haven&#39;t found any experiments on this, but a document that details this approach is </span><a href="https://medium.com/@tom_21755/llm-optimization-layer-wise-optimal-rank-adaptation-lora-1444dfbc8e6a" rel="">Layer-wise Optimal Rank Adaptation</a><span> (also abbreviated LORA). In theory, this sounds like a good idea in practice. However, it also adds an extensive number of choices when optimizing hyperparameters.</span></p><p><span>If you&#39;re familiar with the fundamentals of machine learning and deep learning but are looking to bridge some knowledge gaps, the 30 chapters in my new book </span><em><strong>Machine Learning and AI Beyond the Basics</strong></em><span> answer critical questions in the field. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png" width="195" height="257.54716981132077" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/a6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:630,&#34;width&#34;:477,&#34;resizeWidth&#34;:195,&#34;bytes&#34;:307764,&#34;alt&#34;:&#34;&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6793a57-8bc7-4f43-9065-faed87cc701b_477x630.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><a href="https://www.amazon.com/Machine-Learning-AI-Beyond-Basics/dp/1718503768" rel="">Machine Learning and AI Beyond the Basics</a><span> book cover</span></figcaption></figure></div><p><em><strong>Machine Learning and AI Beyond the Basics</strong></em><span> is a fully revised and edited version of </span><em>Machine Learning Q and AI</em><span> and is now available for </span><a href="https://nostarch.com/machine-learning-and-ai-beyond-basics" rel="">pre-order on the No Starch Press website</a><span> and </span><a href="https://www.amazon.com/Machine-Learning-AI-Beyond-Basics/dp/1718503768/ref=sr_1_12?crid=21OZ206J24U3G&amp;keywords=raschka&amp;qid=1699032392&amp;sprefix=raschka%2Caps%2C150&amp;sr=8-12" rel="">Amazon</a><span>.</span></p></div></div></div>
  </body>
</html>
