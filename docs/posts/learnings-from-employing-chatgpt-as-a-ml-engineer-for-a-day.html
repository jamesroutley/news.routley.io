<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://encord.com/blog/we-employed-chatgpt-as-an-ml-engineer-this-is-what-we-learned/">Original</a>
    <h1>Learnings from employing ChatGPT as a ML Engineer for a day</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>TLDR</strong>; <em>Among the proliferation of recent use cases using the AI application ChatGPT, we ask whether it can be used to make improvements in other AI systems. <a href="https://encord.com/" rel="noopener noreferrer">We</a> test it on a practical problem in a modality of AI in which it was not trained, computer vision, and report the results. The average over ChatGPT suggestions, achieves on average 10.1% improvement in precision and 34.4% improvement in recall over our random sample, using a purely data-centric metric driven approach. Code for the post is <a href="https://colab.research.google.com/drive/1uC4vW1nPxidP_RA6bkgQK5PJggn1aYxR?usp=sharing" rel="noopener noreferrer">here</a>.</em></p><p><strong>Introduction</strong> </p><p>Few technological developments have captured the public imagination as quickly and as starkly as the release of ChatGPT by OpenAI. </p><p>Within two months of launch, it had garnered over 100 million users, the <a href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/" rel="noopener noreferrer">fastest public application</a> to do so in history. While many see ChatGPT as a leap forward technologically, its true spark <em>wasn’t</em> based on a dramatic technological step change (<a href="https://arxiv.org/abs/2005.14165" rel="noopener noreferrer">GPT3</a>, the model it is based on has been around for almost 3 years), but instead on the fact that it was an AI application perfectly calibrated towards individual human interactions.</p><p>It was its <em>ostentatiousness</em> as an AI system that could demonstrate real-world value that so thoroughly awed the public. Forms of AI are present in various aspects of modern life but are mostly hidden away (Google searches, Youtube recommendations, spam filters, identity recognition) in the background. </p><p>ChatGPT is one of the few that is <em>blatantly</em> artificial, but also intelligent. AI being in the limelight has spawned a deluge of thought pieces, articles, videos, blog posts, and podcasts. </p><p>Amid this content rush have been renewed questions and concerns around the more provocative implications of AI advancement, progression towards<a href="https://www.rmit.edu.au/news/acumen/ChatGPT_sentientorsingularity" rel="noopener noreferrer"> AI consciousness</a>, <a href="https://www.independent.co.uk/tech/chatgpt-openai-agi-ai-chat-b2252002.html" rel="noopener noreferrer">artificial general intelligence(AGI)</a>, and a <a href="https://www.hfsresearch.com/research/is-the-singularity-nigh-deconstructing-the-chatgpt-hype/" rel="noopener noreferrer">technological singularity</a>. In the most speculative scenarios, the fear (or hope depending on who you ask) is that the sophistication, power, and complexity of our models will eventually breach an event horizon of breakaway intelligence, where the system develops the capability to iteratively self-improve both it’s core functionality and it’s own ability to self-improve. This can create an exponentially growing positive reinforcement cycle spurring an unknowable and irreversible transition of society and human existence as we know it.</p><p><img src="https://images.prismic.io/encord/66fff815-78af-4102-9001-3e7b3e1b007d_image.png?auto=compress,format" alt="Not yet, ChatGPT"/></p><p>Not yet, ChatGPT</p><p>To be clear…that’s not where we are.</p><p>ChatGPT is not the dawn of the Terminator. Prognosticating on the direction of a technology still in rapid development is often a folly, and on its broader implications even more so. However, in the presence of a large looming and unanswerable question, we can still develop insights and intuition by asking smaller, more palatable ones. </p><p>In that vein, we look for a simpler question we can pose and subsequently test on the topic of AI self-improvement:</p><p><strong>Can we find an avenue by which we can examine if one AI system can iteratively improve another AI system?</strong></p><p> We observe that the main agents at the moment for AI progression are people working in machine learning as engineers and researchers. A sensible proxy sub-question might then be:</p><p><strong>Can ChatGPT function as a competent machine learning engineer?</strong></p><p><strong>The Set Up</strong></p><p>If ChatGPT is to function as an ML engineer, it is best to run an inventory of the tasks that the role entails. The daily life of an ML engineer includes among others:</p><ul><li>Manual inspection and exploration of data</li><li>Training models and evaluating model results</li><li>Managing model deployments and model monitoring processes.</li><li>Writing custom algorithms and scripts.</li></ul><p>The thread tying together the role is the fact that machine learning engineers have to be versatile technical problem solvers. Thus, rather than running through the full gamut of ML tasks for ChatGPT, we can focus on the more abstract and creative problem-solving elements of the role. </p><p>We will narrow the scope in a few ways by having ChatGPT: </p><ul><li><strong>Work specifically in the modality of computer vision</strong>: We chose computer vision both as it is our expertise and because as a large language model, ChatGPT, did not (as far as we know) have direct access to any visual media in its training process. It thus approaches this field from a purely conceptual point of view.<br/></li><li><strong>Reason over one concrete toy problem</strong>: In honour of both the Python library we all know and love and the endangered animal we also all know and love, we pick our toy problem to be building a robust object detector of <strong>pandas</strong>. We will use data from the open source <a href="https://youtube-vos.org/dataset/" rel="noopener noreferrer">YouTube-VOS dataset</a> which we have relabelled independently and with deliberate mistakes.<br/></li><li><strong>Take an explicitly data-centric approach: </strong>We choose a <a href="https://medium.com/p/9b187a1bd412" rel="noopener noreferrer">data-centric</a> methodology as it is often what we find has the highest leverage for practical model development. We can strip out much of the complication of model and parameter selection so that we can focus more on improving the data and labels being input into the model for training. Taking a more model-centric approach of running through hyper parameters and model architectures, while important, will push less on testing abstract reasoning abilities from ChatGPT.<br/></li><li><strong>Use existing tools: </strong>To further simplify the task, we remove any dependence on internal tools ML engineers often build for themselves. ChatGPT (un)fortunately can’t spend time in a Jupyter Notebook. We will leverage the Encord platform to simplify the model training/inference by using Encord’s <a href="https://encord.com/blog/introduction-to-micro-models-or-how-i-learned-to-stop-worrying-and-love-overfitting/" rel="noopener noreferrer">micro-models</a> and running data, label, and model evaluation through the open source tool<a href="https://github.com/encord-team/encord-active" rel="noopener noreferrer"> Encord Active</a>. Code for running the model training is presented below.</li></ul><p><img src="https://images.prismic.io/encord/4d7c3b36-42a6-4205-a31a-ff125f7f179b_code.png?auto=compress,format" alt="code"/></p><p>With this narrowed scope in mind, our approach will be to use ChatGPT to write custom <strong><a href="https://encord.com/blog/closing-the-ai-production-gap-encord-active-a-new#h9" rel="noopener noreferrer">quality metrics</a></strong> through Encord Active that we can run over the data, labels, and model predictions to filter and clean data in our panda problem. </p><p><strong>Quality metrics</strong> are additional parametrizations over your data, labels, and models; they are methods of indexing training data and predictions in semantically interesting and relevant ways. Examples can include everything from more general attributes like the blurriness of an image to arbitrarily specific ones like the number of average distance between pedestrians in an image.</p><p>ChatGPT’s job as our ML engineer will be to come up with ideas to break down and curate our labels and data for the purpose of improving the downstream model performance, mediated through tangible quality metrics that it can program on its own accord. </p><p>The code for an example quality metric is below:</p><p><img src="https://images.prismic.io/encord/b971bd6e-e9c3-4dda-9a0c-0ce49f003d4d_code+python.png?auto=compress,format" alt="python code"/></p><p>This simple metric calculates the <strong>aspect ratio of bounding boxes</strong> within the data set. Using this metric to traverse the dataset we see one way to break down the data:</p><p><img src="https://images.prismic.io/encord/a743ad8d-7651-440f-9e68-b9fc8df7e386_encord_gif.gif?auto=compress,format" alt="gif"/></p><p>ChatGPT’s goal will be to find other relevant metrics which we can use to improve the data. </p><p>One final point we must also consider is that ChatGPT is at a great disadvantage as an ML engineer. It can’t directly see the data it’s dealing with. Its implementation capabilities are also limited. Finally, ChatGPT, as a victim of its own success, has frequent bouts of outages and non-responses. ChatGPT is an ML engineer at your company that lives on a desert island with very bad wifi where the only software they can run is Slack. We thus require a human-in-the-loop approach for practical reasons. We will serve as the eyes, ears, and fingers of ChatGPT as well as its probing colleagues in this project.</p><p><strong>The Process</strong></p><p>With the constraints set above, our strategy will be as follows:</p><ol><li>Run an initial benchmark model on randomly sampled data</li><li>Ask ChatGPT for ideas to improve the data and label selection process</li><li>Have it write custom quality metrics to instantiate those ideas </li><li>Train new models on improved data</li><li>Evaluate the models and compare</li></ol><p>With a data-centric approach, we will limit the variables with which ChatGPT has to experiment to just quality metrics. All models and hyper parameters will remain fixed. To summarise:</p><p><img src="https://images.prismic.io/encord/324d0e2a-2ad3-4876-9d12-70f637ea80b8_table.png?auto=compress,format" alt="table"/></p><p>One of the luxuries we have as humans versus a neural network trapped on OpenAI servers (for now), is we can directly look at the data. Through some light visual inspection, it is not too difficult to quickly notice label errors (that we deliberately introduced). Through the process, we will do our best to communicate as much as what we see to ChatGPT to give it sufficient context for its solutions. Some (incontrovertibly cute) data examples include:</p><p><img src="https://images.prismic.io/encord/63ac0b6a-31ac-49d7-9138-287244db93db_panda.png?auto=compress,format" alt="panda"/></p><p><img src="https://images.prismic.io/encord/62f2afdf-6a00-48f0-b842-3127863ba2d8_panda-detect.png?auto=compress,format" alt="panda-detect"/></p><p><img src="https://images.prismic.io/encord/e3aff99e-cd81-4bc0-a899-f5ff84e03d14_panda-with-woman.png?auto=compress,format" alt="panda-with-woman"/></p><p>With some clear labeling errors:</p><p><img src="https://images.prismic.io/encord/0e6a5dde-72a7-4cd5-b6d0-e60f32debda3_man.png?auto=compress,format" alt="man"/></p><p><img src="https://images.prismic.io/encord/1afb5dec-b52d-4e9a-aa0a-2b0fe2572932_bycycle.png?auto=compress,format" alt="bycycle"/></p><p><strong>Not exactly pandas</strong></p><p><strong>Benchmarking</strong></p><p>With all this established, we can start by running an initial model to benchmark ChatGPT’s efforts. For our benchmark we take a random sample of 25% of the training data set and run it through the model training loop described above. Our initial results come to a mean average precision (mAP) of 0.595 and mean average recall of 0.629.</p><p><img src="https://images.prismic.io/encord/40850b90-15d9-4a30-b6b7-b30f81c47e24_chart.png?auto=compress,format" alt="chart"/></p><p>We can now try out ChatGPT for where to go next.</p><p><strong>ChatGPT Does Machine Learning</strong></p><p>With our benchmark set, we engage with ChatGPT on ideas for ways to improve our dataset with quality metrics. Our first approach is to set up a high-level description of the problem and elicit initial suggestions.</p><p><img src="https://images.prismic.io/encord/f59567de-b85c-4195-9b27-f4012cd36df0_q1.png?auto=compress,format" alt="q1"/></p><p><img src="https://images.prismic.io/encord/1325335c-7686-4905-9e04-efa3eccea687_a1.png?auto=compress,format" alt="a1"/></p><p>A series of back and forths yields a rather productive brainstorming session:</p><p><img src="https://images.prismic.io/encord/dc462108-cf69-43b3-b2b5-2bbd538a6034_qa.png?auto=compress,format" alt="qa"/></p><p>Already, we have some interesting ideas for metrics. To continue to pull ideas, we chat over a series of multiple long conversations, including an ill fated attempt to play Wordle during a break.</p><p><img src="https://images.prismic.io/encord/cf501f48-d892-4193-a143-fb6da8ab42b8_qa1.png?auto=compress,format" alt="qa1"/></p><p><img src="https://images.prismic.io/encord/815ba476-1aaa-4b1c-b44c-1ab590e22b03_qa2.png?auto=compress,format" alt="qa2"/></p><p>After our many interactions(and a solid rapport having been built) we can go over some of the sample metric suggestions ChatGPT came up with:</p><ul><li><strong>Object Proximity</strong>: parametrizing the average distance between panda bounding boxes in the frame. This can be useful if pandas are very close to each other, which might confuse an object detection model.<br/></li><li><strong>Object Confidence Score: </strong>this is a standard approach of using an existing model to rank the confidence level of a label with the initial model itself.<br/></li><li><strong>Object Count</strong>: a simple metric, relevant for the cases where the number of pandas in the frame affects the model accuracy.<br/></li><li><strong>Bounding Box Tightness</strong>: calculates the deviation of the aspect ratio of contours within the bounding box from the aspect ratio of the box itself to gauge whether the box is tightly fitted around the object of interest</li></ul><p>Besides ideas for metrics, we also want its suggestions for <em>how</em> to use these metrics in practice.</p><p><strong>Data selection:</strong></p><p>Let’s ask ChatGPT what it thinks about using metrics to improve data quality.</p><p><img src="https://images.prismic.io/encord/a04199fb-5de1-4a4c-98d3-48b63559d4d7_qa3.png?auto=compress,format" alt="qa3"/></p><p><strong>Label errors</strong></p><p>We can also inquire about what to do when metrics find potential label errors.</p><p><img src="https://images.prismic.io/encord/70a652be-9823-42b4-9049-debce57c3ad9_qa4.png?auto=compress,format" alt="qa4"/></p><p>To summarise, a few concrete strategies for improving our model according to ChatGPT are:</p><ul><li><strong>Data Stratification</strong>: using metrics to stratify our dataset for balance</li><li><strong>Error Removal</strong>: filtering potential errors indicated by our metrics from the training data</li><li><strong>Relabeling</strong>: sending potential label errors back to get re-annotated </li></ul><p>We have now compiled a number of actionable metrics and strategies through ChatGPT ideation. The next question is:</p><p><strong>Can ChatGPT actually implement these solutions?</strong> </p><p>The next step will be for it to instantiate its ideas as its own custom metrics. Let’s try it out by inputting some sample metric code and instructions from Encord Active’s <a href="https://encord-active-docs.web.app/" rel="noopener noreferrer">documentation</a> on an example metric:</p><p><img src="https://images.prismic.io/encord/4bb074c3-c696-4fcc-8057-724e4ae7823d_qa5.png?auto=compress,format" alt="qa5"/></p><p><img src="https://images.prismic.io/encord/6e323a17-b0fa-4bf6-91eb-24881e9053f3_qa-code.png?auto=compress,format" alt="qa-code"/></p><p>From the surface, it looks good. We run through various other metrics ChatGPT suggested and find similar plausible looking code snippets we can try via copy/paste. Plugging these metric into Encord Active to see how well it runs we get:</p><p><img src="https://images.prismic.io/encord/d0e9ceb9-34de-4658-bccc-3e3ee016b3a2_console.png?auto=compress,format" alt="console"/></p><p>Looks like some improvement is needed here. Let’s ask ChatGPT what it can do about this.</p><p><img src="https://images.prismic.io/encord/e9261253-9355-4ffc-b818-ffb72c99bde6_qa6.png?auto=compress,format" alt="qa6"/></p><p>Uh-oh. Despite multiple refreshes, we find a stonewalled response. Like an unreliable colleague, ChatGPT has written buggy code and then logged out for the day, unresponsive.</p><p>Seems like human input is still required to get things running. After a healthy amount of manual debugging, we do finally get ChatGPT’s metrics to run. We can now go back and implement its suggestions.</p><p>As with any data science initiative, the key is trying many iterations over candidate ideas. We will run through multiple ChatGPT metrics and average the results, but for purposes of brevity we will only try the filtering strategy it suggested. We spare ChatGPT from running these experiments, letting it rest after the work it has done so far. We share sample code for some of our experiments and ChatGPT metrics in the Colab notebook <a href="https://colab.research.google.com/drive/1uC4vW1nPxidP_RA6bkgQK5PJggn1aYxR?usp=sharing" rel="noopener noreferrer">here</a>.</p><p><strong>The Results</strong></p><p>We run multiple experiments using ChatGPT’s metrics and strategies. Looking over metrics we can see ChatGPT’s model performance below.</p><p><img src="https://images.prismic.io/encord/40850b90-15d9-4a30-b6b7-b30f81c47e24_chart.png?auto=compress,format" alt="chart"/></p><p><img src="https://images.prismic.io/encord/42b57de3-b091-412b-89e0-b8ebba0bd1eb_chart1.png?auto=compress,format" alt="chart1"/></p><p>The average over ChatGPT metric suggestions, achieves on average 10.1% improvement in precision and 34.4% improvement in recall over our random sample, using a purely data-centric metric driven approach. </p><p>While not a perfect colleague, ChatGPT did a commendable job in ideating plausible metrics. Its ideas made a significant improvement in our panda detector. So is ChatGPT the next Geofrey Hinton?</p><p>Not exactly. For one, its code required heavy human intervention to run. We also would be remiss to omit the observation that in the background of the entire conversation process, there was a healthy dose of human judgement implicitly injected. Our most successful metric in fact, the one that had the most significant impact in reducing label error, and improving model performance was heavily influenced by human suggestion:</p><p><img src="https://images.prismic.io/encord/21006009-0b8a-43c0-8a0c-f2e96ab11a61_qa7.png?auto=compress,format" alt="qa7"/></p><p>This “Bounding Box Tightness” metric, defined early, was arrived at over a series of prompts and responses. The path to the metric was not carved by ChatGPT, but instead by human hand. </p><p>ChatGPT also missed the opportunity for some simple problem-specific metrics. An obvious human observation about pandas is their black and white colour. A straightforward metric to have tried for label error would have been taking the number of white and black bounding box pixels divided by the total number of pixels in the box. A low number for this metric would indicate that the bounding box potentially did not tightly encapsulate a panda. While obvious to us, we could not coax an idea like this despite our best efforts.</p><p><strong>Conclusion</strong></p><p>Overall, it does seem ChatGPT has a formidable understanding of computer vision. It was able to generate useful suggestions and initial template code snippets for a problem and framework for which it was unfamiliar with. With its help, we were able to measurably improve our panda detector.The ability to create code templates rather than having to write everything from scratch can also significantly speed up the iteration cycle of a (human) ML engineer. </p><p>Where it lacked, however, was in its ability to build off its own previous ideas and conclusions without guidance. It never delved deeper into the specificity of the problem through focused reasoning.</p><p>For humans, important insights are not easily won, they come from building on top of experience and other previously hard-fought insights. ChatGPT doesn’t seem to have yet developed this capability, still relying on the direction of a “prompt engineer.” While it can be a strong starting point for machine learning ideas and strategies, it does not yet have the depth of cognitive capacity for independent ML engineering. </p><p>Can ChatGPT be used to improve an AI system? Yes.</p><p>Would we hire it as our next standalone ML engineer? No. </p><p>Let’s wait until GPT4.</p><p>In the meantime, if you’d like to try out Encord Active yourself you can find the open source repository on <a href="https://github.com/encord-team/encord-active" rel="noopener noreferrer">GitHub</a> and if you have questions on how to use ChatGPT to improve your own datasets ask us on our <a href="https://join.slack.com/t/encordactive/shared_invite/zt-1hc2vqur9-Fzj1EEAHoqu91sZ0CX0A7Q" rel="noopener noreferrer">Slack channel</a>.</p><p><img src="https://images.prismic.io/encord/4a4075ab-8590-412a-917d-78ce0975c4db_conclusion.png?auto=compress,format" alt="child"/></p></div></div>
  </body>
</html>
