<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule">Original</a>
    <h1>68–95–99.7 Rule</h1>
    
    <div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div id="mw-content-text" lang="en" dir="ltr"><div>
<div><div><p><a href="https://en.wikipedia.org/wiki/File:Empirical_rule_histogram.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Empirical_rule_histogram.svg/450px-Empirical_rule_histogram.svg.png" decoding="async" width="450" height="446" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/22/Empirical_rule_histogram.svg/675px-Empirical_rule_histogram.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/22/Empirical_rule_histogram.svg/900px-Empirical_rule_histogram.svg.png 2x" data-file-width="553" data-file-height="548"/></a></p><div><p>For an approximately <a href="https://en.wikipedia.org/wiki/Normal_distribution" title="Normal distribution">normal data set</a>, the values within one standard deviation of the mean account for about 68% of the set; while within two standard deviations account for about 95%; and within three standard deviations account for about 99.7%. Shown percentages are rounded theoretical probabilities intended only to approximate the empirical data derived from a normal population.</p></div></div></div>
<div><div><p><a href="https://en.wikipedia.org/wiki/File:Standard_score_and_prediction_interval.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Standard_score_and_prediction_interval.svg/250px-Standard_score_and_prediction_interval.svg.png" decoding="async" width="250" height="250" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Standard_score_and_prediction_interval.svg/375px-Standard_score_and_prediction_interval.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Standard_score_and_prediction_interval.svg/500px-Standard_score_and_prediction_interval.svg.png 2x" data-file-width="1225" data-file-height="1225"/></a></p><div><p>Prediction interval (on the <a href="https://en.wikipedia.org/wiki/Y-axis" title="Y-axis">y-axis</a>) given from the <a href="https://en.wikipedia.org/wiki/Standard_score" title="Standard score">standard score</a> (on the <a href="https://en.wikipedia.org/wiki/X-axis" title="X-axis">x-axis</a>). The y-axis is logarithmically scaled (but the values on it are not modified).</p></div></div></div>
<p>In <a href="https://en.wikipedia.org/wiki/Statistics" title="Statistics">statistics</a>, the <b>68–95–99.7 rule</b>, also known as the <b>empirical rule</b>, is a shorthand used to remember the percentage of values that lie within
an <a href="https://en.wikipedia.org/wiki/Interval_estimate" title="Interval estimate">interval estimate</a> in a <a href="https://en.wikipedia.org/wiki/Normal_distribution" title="Normal distribution">normal distribution</a>: 68%, 95%, and 99.7% of the values lie within one, two, and three <a href="https://en.wikipedia.org/wiki/Standard_deviation" title="Standard deviation">standard deviations</a> of the <a href="https://en.wikipedia.org/wiki/Arithmetic_mean" title="Arithmetic mean">mean</a>, respectively.
</p><p>In mathematical notation, these facts can be expressed as follows, where Pr() is the <a href="https://en.wikipedia.org/wiki/Probability_function" title="Probability function">probability function</a>,<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> <span><a href="https://en.wikipedia.org/wiki/%CE%A7" title="Χ">Χ</a></span> is an observation from a normally distributed <a href="https://en.wikipedia.org/wiki/Random_variable" title="Random variable">random variable</a>, <span><i><a href="https://en.wikipedia.org/wiki/%CE%9C" title="Μ">μ</a></i></span> (mu) is the mean of the distribution, and <span><i><a href="https://en.wikipedia.org/wiki/%CE%A3" title="Σ">σ</a></i></span> (sigma) is its standard deviation:
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1ddc342f4e2c499595f53c446d0b6883fabd4db3" aria-hidden="true" alt="{\displaystyle {\begin{aligned}\Pr(\mu -1\sigma \leq X\leq \mu +1\sigma )&amp;\approx 68.27\%\\\Pr(\mu -2\sigma \leq X\leq \mu +2\sigma )&amp;\approx 95.45\%\\\Pr(\mu -3\sigma \leq X\leq \mu +3\sigma )&amp;\approx 99.73\%\end{aligned}}}"/></span></dd></dl>
<p>The usefulness of this heuristic especially depends on the question under consideration. 
</p><p>In the <a href="https://en.wikipedia.org/wiki/Empirical_science" title="Empirical science">empirical sciences</a>, the so-called  <b>three-sigma rule of thumb</b> (or <b>3σ rule</b>) expresses a conventional <a href="https://en.wikipedia.org/wiki/Heuristic" title="Heuristic">heuristic</a> that nearly all values are taken to lie within three standard deviations of the mean, and thus it is empirically useful to treat 99.7% <a href="https://en.wikipedia.org/wiki/Probability" title="Probability">probability</a> as near certainty.<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>
</p><p>In the <a href="https://en.wikipedia.org/wiki/Social_sciences" title="Social sciences">social sciences</a>, a result may be considered &#34;<a href="https://en.wikipedia.org/wiki/Statistical_significance" title="Statistical significance">significant</a>&#34; if its <a href="https://en.wikipedia.org/wiki/Confidence_level" title="Confidence level">confidence level</a> is of the order of a two-sigma effect (95%), while in <a href="https://en.wikipedia.org/wiki/Particle_physics" title="Particle physics">particle physics</a>, there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a <a href="https://en.wikipedia.org/wiki/Discovery_(observation)" title="Discovery (observation)">discovery</a>.
</p><p>A weaker three-sigma rule can be derived from <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality" title="Chebyshev&#39;s inequality">Chebyshev&#39;s inequality</a>, stating that even for non-normally distributed variables, at least 88.8% of cases should fall within properly calculated three-sigma intervals. For <a href="https://en.wikipedia.org/wiki/Unimodality#Unimodal_probability_distribution" title="Unimodality">unimodal distributions</a>, the probability of being within the interval is at least 95% by the <a href="https://en.wikipedia.org/wiki/Vysochanskij%E2%80%93Petunin_inequality" title="Vysochanskij–Petunin inequality">Vysochanskij–Petunin inequality</a>. There may be certain assumptions for a distribution that force this probability to be at least 98%.<sup id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>
</p>
<meta property="mw:PageProp/toc"/>
<h2><span id="Cumulative_distribution_function">Cumulative distribution function</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule&amp;action=edit&amp;section=1" title="Edit section: Cumulative distribution function">edit</a><span>]</span></span></h2>
<div><div><p><a href="https://en.wikipedia.org/wiki/File:Cumulative_distribution_function_for_normal_distribution,_mean_0_and_sd_1.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Cumulative_distribution_function_for_normal_distribution%2C_mean_0_and_sd_1.png/270px-Cumulative_distribution_function_for_normal_distribution%2C_mean_0_and_sd_1.png" decoding="async" width="270" height="189" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Cumulative_distribution_function_for_normal_distribution%2C_mean_0_and_sd_1.png/405px-Cumulative_distribution_function_for_normal_distribution%2C_mean_0_and_sd_1.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Cumulative_distribution_function_for_normal_distribution%2C_mean_0_and_sd_1.png/540px-Cumulative_distribution_function_for_normal_distribution%2C_mean_0_and_sd_1.png 2x" data-file-width="3700" data-file-height="2595"/></a></p></div></div>
<p>These numerical values &#34;68%, 95%, 99.7%&#34; come from the <a href="https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_functions" title="Normal distribution">cumulative distribution function of the normal distribution</a>.
</p><p>The prediction interval for any <a href="https://en.wikipedia.org/wiki/Standard_score" title="Standard score">standard score</a> <i>z</i> corresponds numerically to (1−(1−<span>Φ</span><sub><i>μ</i>,<i>σ</i><sup>2</sup></sub>(z))·2).
</p><p>For example, <span>Φ(2) ≈ 0.9772</span>, or <span>Pr(<i>X</i> ≤ <i>μ</i> + 2<i>σ</i>) ≈ 0.9772</span>, corresponding to a prediction interval of (1 − (1 − 0.97725)·2) = 0.9545 = 95.45%.
This is not a symmetrical interval – this is merely the probability that an observation is less than <span><i>μ</i> + 2<i>σ</i></span>. To compute the probability that an observation is within two standard deviations of the mean (small differences due to rounding):
</p>
<dl><dd><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5b4df2eb34623910a7dbe855eb97ee17142f1cf2" aria-hidden="true" alt="{\displaystyle \Pr(\mu -2\sigma \leq X\leq \mu +2\sigma )=\Phi (2)-\Phi (-2)\approx 0.9772-(1-0.9772)\approx 0.9545}"/></span></dd></dl>
<p>This is related to <a href="https://en.wikipedia.org/wiki/Confidence_interval" title="Confidence interval">confidence interval</a> as used in statistics: <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/402cf58bbebd49726fdd3b772d17638fe9efb56f" aria-hidden="true" alt="{\displaystyle {\bar {X}}\pm 2{\frac {\sigma }{\sqrt {n}}}}"/></span> is approximately a 95% confidence interval when <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/90b968141b314f4de17f5e63f18dcdc126352bac" aria-hidden="true" alt="{\bar {X}}"/></span> is the average of a sample of size <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" aria-hidden="true" alt="n"/></span>.
</p>
<h2><span id="Normality_tests">Normality tests</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule&amp;action=edit&amp;section=2" title="Edit section: Normality tests">edit</a><span>]</span></span></h2>

<p>The &#34;68–95–99.7 rule&#34; is often used to quickly get a rough probability estimate of something, given its standard deviation, if the population is assumed to be normal.  It is also used as a simple test for <a href="https://en.wikipedia.org/wiki/Outliers" title="Outliers">outliers</a> if the population is assumed normal, and as a <a href="https://en.wikipedia.org/wiki/Normality_test" title="Normality test">normality test</a> if the population is potentially not normal.
</p><p>To pass from a sample to a number of standard deviations, one first computes the <a href="https://en.wikipedia.org/wiki/Deviation_(statistics)" title="Deviation (statistics)">deviation</a>, either the <a href="https://en.wikipedia.org/wiki/Errors_and_residuals_in_statistics" title="Errors and residuals in statistics">error or residual</a> depending on whether one knows the population mean or only estimates it.  The next step is <a href="https://en.wikipedia.org/wiki/Standardizing" title="Standardizing">standardizing</a> (dividing by the population standard deviation), if the population parameters are known, or <a href="https://en.wikipedia.org/wiki/Studentizing" title="Studentizing">studentizing</a> (dividing by an estimate of the standard deviation), if the parameters are unknown and only estimated.
</p><p>To use as a test for outliers or a normality test, one computes the size of deviations in terms of standard deviations, and compares this to expected frequency. Given a sample set, one can compute the <a href="https://en.wikipedia.org/wiki/Studentized_residual" title="Studentized residual">studentized residuals</a> and compare these to the expected frequency: points that fall more than 3 standard deviations from the norm are likely outliers (unless the <a href="https://en.wikipedia.org/wiki/Sample_size" title="Sample size">sample size</a> is significantly large, by which point one expects a sample this extreme), and if there are many points more than 3 standard deviations from the norm, one likely has reason to question the assumed normality of the distribution. This holds ever more strongly for moves of 4 or more standard deviations.
</p><p>One can compute more precisely, approximating the number of extreme moves of a given magnitude or greater by a <a href="https://en.wikipedia.org/wiki/Poisson_distribution" title="Poisson distribution">Poisson distribution</a>, but simply, if one has multiple 4 standard deviation moves in a sample of size 1,000, one has strong reason to consider these outliers or question the assumed normality of the distribution.
</p><p>For example, a 6<i>σ</i> event corresponds to a chance of about two <a href="https://en.wikipedia.org/wiki/Parts_per_billion" title="Parts per billion">parts per billion</a>. For illustration, if events are taken to occur daily, this would correspond to an event expected every 1.4 million years.  This gives a <a href="https://en.wikipedia.org/wiki/Normality_test#Back_of_the_envelope_test" title="Normality test">simple normality test</a>: if one witnesses a 6<i>σ</i> in daily data and significantly fewer than 1 million years have passed, then a normal distribution most likely does not provide a good model for the magnitude or frequency of large deviations in this respect.
</p><p>In <i><a href="https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable" title="The Black Swan: The Impact of the Highly Improbable">The Black Swan</a></i>, <a href="https://en.wikipedia.org/wiki/Nassim_Nicholas_Taleb" title="Nassim Nicholas Taleb">Nassim Nicholas Taleb</a> gives the example of risk models according to which the <a href="https://en.wikipedia.org/wiki/Black_Monday_(1987)" title="Black Monday (1987)">Black Monday</a> crash would correspond to a 36-<i>σ</i> event:
the occurrence of such an event should instantly suggest that the model is flawed, i.e. that the process under consideration is not satisfactorily modeled by a normal distribution. Refined models should then be considered, e.g. by the introduction of <a href="https://en.wikipedia.org/wiki/Stochastic_volatility" title="Stochastic volatility">stochastic volatility</a>.  In such discussions it is important to be aware of the problem of the <a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy" title="Gambler&#39;s fallacy">gambler&#39;s fallacy</a>, which states that a single observation of a rare event does not contradict that the event is in fact rare.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="It is not obvious that this is also part of the gambler&#39;s fallacy (November 2016)">citation needed</span></a></i>]</sup> It is the observation of a plurality of purportedly rare events that increasingly <a href="https://en.wikipedia.org/wiki/Belief_revision" title="Belief revision">undermines the hypothesis</a> that they are rare, i.e. the validity of the assumed model. A proper modelling of this process of gradual loss of confidence in a hypothesis would involve the designation of <a href="https://en.wikipedia.org/wiki/Prior_probability" title="Prior probability">prior probability</a> not just to the hypothesis itself but to all possible alternative hypotheses. For this reason, <a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing" title="Statistical hypothesis testing">statistical hypothesis testing</a> works not so much by confirming a hypothesis considered to be likely, but by <a href="https://en.wikipedia.org/wiki/Null_hypothesis" title="Null hypothesis">refuting hypotheses considered unlikely</a>.
</p>
<h2><span id="Table_of_numerical_values">Table of numerical values</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule&amp;action=edit&amp;section=3" title="Edit section: Table of numerical values">edit</a><span>]</span></span></h2>

<p>Because of the exponentially decreasing tails of the normal distribution, odds of higher deviations decrease very quickly. From the <a href="https://en.wikipedia.org/wiki/Standard_deviation#Rules_for_normally_distributed_data" title="Standard deviation">rules for normally distributed data</a> for a daily event:
</p>
<table>
<tbody><tr>
<th>Range
</th>
<th>Expected fraction of
<p>population inside range
</p>
</th>
<th>Expected fraction of
<p>population outside range
</p>
</th>
<th colspan="2">Approx. expected</th>
<th>Approx. frequency for daily event
</th></tr>
<tr>
<td><span> <i>μ</i> ± 0.5<i>σ</i></span>
</td>
<td><span>0.382<span>924</span><span>922</span><span>548</span><span>026</span></span>
</td>
<td>6.171E-01 = 61.71 %
</td>
<td>3 in 
</td>
<td>5
</td>
<td>Four or five times a week
</td></tr>
<tr>
<td><i>μ</i> ± <i>σ</i>
</td>
<td><span>0.682<span>689</span><span>492</span><span>137</span><span>086</span></span><sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>
</td>
<td>3.173E-01 = 31.73 %
</td>
<td>1 in 
</td>
<td>3
</td>
<td>Twice or thrice a week
</td></tr>
<tr>
<td><i>μ</i> ± 1.5<i>σ</i>
</td>
<td><span>0.866<span>385</span><span>597</span><span>462</span><span>284</span></span>
</td>
<td>1.336E-01 = 13.36 %
</td>
<td>1 in 
</td>
<td>7
</td>
<td>Weekly
</td></tr>
<tr>
<td><i>μ</i> ± 2<i>σ</i>
</td>
<td><span>0.954<span>499</span><span>736</span><span>103</span><span>642</span></span><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>
</td>
<td>4.550E-02 = 4.550 %
</td>
<td>1 in 
</td>
<td>22
</td>
<td>Every three weeks
</td></tr>
<tr>
<td><i>μ</i> ± 2.5<i>σ</i>
</td>
<td><span>0.987<span>580</span><span>669</span><span>348</span><span>448</span></span>
</td>
<td>1.242E-02 = 1.242 %
</td>
<td>1 in 
</td>
<td>81
</td>
<td>Quarterly
</td></tr>
<tr>
<td><i>μ</i> ± 3<i>σ</i>
</td>
<td><span>0.997<span>300</span><span>203</span><span>936</span><span>740</span></span><sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>
</td>
<td>2.700E-03 = 0.270 % = 2.700 ‰
</td>
<td>1 in 
</td>
<td>370
</td>
<td>Yearly
</td></tr>
<tr>
<td><i>μ</i> ± 3.5<i>σ</i>
</td>
<td><span>0.999<span>534</span><span>741</span><span>841</span><span>929</span></span>
</td>
<td>4.653E-04 = 0.04653 % = 465.3 ppm
</td>
<td>1 in 
</td>
<td>2149
</td>
<td>Every 6 years
</td></tr>
<tr>
<td><i>μ</i> ± 4<i>σ</i>
</td>
<td><span>0.999<span>936</span><span>657</span><span>516</span><span>334</span></span>
</td>
<td>6.334E-05 = 63.34 ppm
</td>
<td>1 in 
</td>
<td><span><span data-sort-value="7004157870000000000♠"></span>15<span>787</span></span>
</td>
<td>Every 43 years (twice in a lifetime)
</td></tr>
<tr>
<td><i>μ</i> ± 4.5<i>σ</i>
</td>
<td><span>0.999<span>993</span><span>204</span><span>653</span><span>751</span></span>
</td>
<td>6.795E-06 = 6.795 ppm
</td>
<td>1 in 
</td>
<td><span><span data-sort-value="7005147160000000000♠"></span>147<span>160</span></span>
</td>
<td>Every 403 years (once in the modern era)
</td></tr>
<tr>
<td><i>μ</i> ± 5<i>σ</i>
</td>
<td><span>0.999<span>999</span><span>426</span><span>696</span><span>856</span></span>
</td>
<td>5.733E-07 = 0.5733 ppm = 573.3 ppb
</td>
<td>1 in 
</td>
<td><span><span data-sort-value="7006174427800000000♠"></span>1<span>744</span><span>278</span></span>
</td>
<td>Every <span><span data-sort-value="7003477600000000000♠"></span>4776</span> years (once in recorded history)
</td></tr>
<tr>
<td><i>μ</i> ± 5.5<i>σ</i>
</td>
<td><span>0.999<span>999</span><span>962</span><span>020</span><span>875</span></span>
</td>
<td>3.798E-08 = 37.98 ppb
</td>
<td>1 in 
</td>
<td><span><span data-sort-value="7007263302540000000♠"></span>26<span>330</span><span>254</span></span>
</td>
<td>Every <span><span data-sort-value="7004720900000000000♠"></span>72<span>090</span></span> years (thrice in history of <a href="https://en.wikipedia.org/wiki/Homo_sapiens" title="Homo sapiens">modern humankind</a>)
</td></tr>
<tr>
<td><i>μ</i> ± 6<i>σ</i>
</td>
<td><span>0.999<span>999</span><span>998</span><span>026</span><span>825</span></span>
</td>
<td>1.973E-09 = 1.973 ppb
</td>
<td>1 in 
</td>
<td><span><span data-sort-value="7008506797346000000♠"></span>506<span>797</span><span>346</span></span>
</td>
<td>Every 1.38 million years (twice in history of <a href="https://en.wikipedia.org/wiki/Homo_(genus)" title="Homo (genus)">humankind</a>)
</td></tr>
<tr>
<td><i>μ</i> ± 6.5<i>σ</i>
</td>
<td><span>0.999<span>999</span><span>999</span><span>919</span><span>680</span></span>
</td>
<td>8.032E-11 = 0.08032 ppb = 80.32 ppt
</td>
<td>1 in 
</td>
<td><span><span data-sort-value="7010124501973930000♠"></span>12<span>450</span><span>197</span><span>393</span></span>
</td>
<td>Every 34 million years (twice since the <a href="https://en.wikipedia.org/wiki/Extinction_of_dinosaurs" title="Extinction of dinosaurs">extinction of dinosaurs</a>)
</td></tr>
<tr>
<td><i>μ</i> ± 7<i>σ</i>
</td>
<td><span>0.999<span>999</span><span>999</span><span>997</span><span>440</span></span>
</td>
<td>2.560E-12 = 2.560 ppt
</td>
<td>1 in 
</td>
<td><span><span data-sort-value="7011390682215445000♠"></span>390<span>682</span><span>215</span><span>445</span></span>
</td>
<td>Every 1.07 billion years (four occurrences in <a href="https://en.wikipedia.org/wiki/History_of_Earth" title="History of Earth">history of Earth</a>)
</td></tr>
<tr>
<td><i>μ</i> ± <span><i>x</i></span><i>σ</i>
</td>
<td><a href="https://en.wikipedia.org/wiki/Error_function" title="Error function"><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d9307019c6a39fed28fbfc2e8d1785efd2216f3e" aria-hidden="true" alt="{\displaystyle \operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)}"/></span></a>
</td>
<td><a href="https://en.wikipedia.org/wiki/Error_function" title="Error function"><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/db5877c2456714471dbfa376bdb6a4d5076fcac7" aria-hidden="true" alt="{\displaystyle 1-\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)}"/></span></a>
</td>
<td>1 in 
</td>
<td><span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/560b87d8432f6f72b29b1bac875fde25e3507bf0" aria-hidden="true" alt="{\displaystyle {\tfrac {1}{1-\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)}}}"/></span>
</td>
<td>Every <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/560b87d8432f6f72b29b1bac875fde25e3507bf0" aria-hidden="true" alt="{\displaystyle {\tfrac {1}{1-\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)}}}"/></span> days
</td></tr></tbody></table>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule&amp;action=edit&amp;section=4" title="Edit section: See also">edit</a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/P-value" title="P-value"><i>p</i>-value</a></li>
<li><a href="https://en.wikipedia.org/wiki/Six_Sigma#Sigma_levels" title="Six Sigma">Six Sigma#Sigma levels</a></li>
<li><a href="https://en.wikipedia.org/wiki/Standard_score" title="Standard score">Standard score</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-statistic" title="T-statistic"><i>t</i>-statistic</a></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule&amp;action=edit&amp;section=5" title="Edit section: References">edit</a><span>]</span></span></h2>
<div>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFHuber2018">Huber, Franz (2018). <a rel="nofollow" href="https://books.google.com/books?id=Z2J7DwAAQBAJ&amp;pg=PA80"><i>A Logical Introduction to Probability and Induction</i></a>. New York, N.Y.: <a href="https://en.wikipedia.org/wiki/Oxford_University_Press" title="Oxford University Press">Oxford University Press</a>. p. 80. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/9780190845414" title="Special:BookSources/9780190845414"><bdi>9780190845414</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+Logical+Introduction+to+Probability+and+Induction&amp;rft.place=New+York%2C+N.Y.&amp;rft.pages=80&amp;rft.pub=Oxford+University+Press&amp;rft.date=2018&amp;rft.isbn=9780190845414&amp;rft.aulast=Huber&amp;rft.aufirst=Franz&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DZ2J7DwAAQBAJ%26pg%3DPA80&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span>this usage of &#34;three-sigma rule&#34; entered common usage in the 2000s, e.g. cited in  <cite><span title="Free registration required"><a rel="nofollow" href="https://archive.org/details/businessstatisti0000unse"><i>Schaum&#39;s Outline of Business Statistics</i></a></span>. McGraw Hill Professional. 2003. p. 359. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/9780071398763" title="Special:BookSources/9780071398763"><bdi>9780071398763</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Schaum%27s+Outline+of+Business+Statistics&amp;rft.pages=359&amp;rft.pub=McGraw+Hill+Professional&amp;rft.date=2003&amp;rft.isbn=9780071398763&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fbusinessstatisti0000unse&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span>, and in <cite id="CITEREFGrafarend2006">Grafarend, Erik W. (2006). <span title="Free access subject to limited trial, subscription normally required"><a rel="nofollow" href="https://archive.org/details/linearnonlinearm00wgra"><i>Linear and Nonlinear Models: Fixed Effects, Random Effects, and Mixed Models</i></a></span>. Walter de Gruyter. p. <a rel="nofollow" href="https://archive.org/details/linearnonlinearm00wgra/page/n573">553</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/9783110162165" title="Special:BookSources/9783110162165"><bdi>9783110162165</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Linear+and+Nonlinear+Models%3A+Fixed+Effects%2C+Random+Effects%2C+and+Mixed+Models&amp;rft.pages=553&amp;rft.pub=Walter+de+Gruyter&amp;rft.date=2006&amp;rft.isbn=9783110162165&amp;rft.aulast=Grafarend&amp;rft.aufirst=Erik+W.&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Flinearnonlinearm00wgra&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span>See:
<ul><li><cite id="CITEREFWheelerChambers1992">Wheeler, D. J.; Chambers, D. S. (1992). <a rel="nofollow" href="https://books.google.com/books?id=XvMJAQAAMAAJ"><i>Understanding Statistical Process Control</i></a>. SPC Press. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/9780945320135" title="Special:BookSources/9780945320135"><bdi>9780945320135</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Understanding+Statistical+Process+Control&amp;rft.pub=SPC+Press&amp;rft.date=1992&amp;rft.isbn=9780945320135&amp;rft.aulast=Wheeler&amp;rft.aufirst=D.+J.&amp;rft.au=Chambers%2C+D.+S.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DXvMJAQAAMAAJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></li>
<li><cite id="CITEREFCzitromSpagon1997"><a href="https://en.wikipedia.org/wiki/Veronica_Czitrom" title="Veronica Czitrom">Czitrom, Veronica</a>; Spagon, Patrick D. (1997). <a rel="nofollow" href="https://books.google.com/books?id=gEpkYxwDbvgC&amp;pg=PA342"><i>Statistical Case Studies for Industrial Process Improvement</i></a>. SIAM. p. 342. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a> <a href="https://en.wikipedia.org/wiki/Special:BookSources/9780898713947" title="Special:BookSources/9780898713947"><bdi>9780898713947</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Statistical+Case+Studies+for+Industrial+Process+Improvement&amp;rft.pages=342&amp;rft.pub=SIAM&amp;rft.date=1997&amp;rft.isbn=9780898713947&amp;rft.aulast=Czitrom&amp;rft.aufirst=Veronica&amp;rft.au=Spagon%2C+Patrick+D.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DgEpkYxwDbvgC%26pg%3DPA342&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></li>
<li><cite id="CITEREFPukelsheim1994">Pukelsheim, F. (1994). &#34;The Three Sigma Rule&#34;. <i>American Statistician</i>. <b>48</b> (2): 88–91. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2684253">10.2307/2684253</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a> <a rel="nofollow" href="https://www.jstor.org/stable/2684253">2684253</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Statistician&amp;rft.atitle=The+Three+Sigma+Rule&amp;rft.volume=48&amp;rft.issue=2&amp;rft.pages=88-91&amp;rft.date=1994&amp;rft_id=info%3Adoi%2F10.2307%2F2684253&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2684253%23id-name%3DJSTOR&amp;rft.aulast=Pukelsheim&amp;rft.aufirst=F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></li></ul>
</span></li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFSloane_&#34;A178647&#34;"><a href="https://en.wikipedia.org/wiki/Neil_Sloane" title="Neil Sloane">Sloane, N. J. A.</a> (ed.). <a rel="nofollow" href="https://oeis.org/A178647">&#34;Sequence A178647&#34;</a>. <i>The <a href="https://en.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences" title="On-Line Encyclopedia of Integer Sequences">On-Line Encyclopedia of Integer Sequences</a></i>. OEIS Foundation.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+On-Line+Encyclopedia+of+Integer+Sequences&amp;rft.atitle=Sequence%26%23x20%3BA178647&amp;rft_id=https%3A%2F%2Foeis.org%2FA178647&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite id="CITEREFSloane_&#34;A110894&#34;"><a href="https://en.wikipedia.org/wiki/Neil_Sloane" title="Neil Sloane">Sloane, N. J. A.</a> (ed.). <a rel="nofollow" href="https://oeis.org/A110894">&#34;Sequence A110894&#34;</a>. <i>The <a href="https://en.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences" title="On-Line Encyclopedia of Integer Sequences">On-Line Encyclopedia of Integer Sequences</a></i>. OEIS Foundation.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+On-Line+Encyclopedia+of+Integer+Sequences&amp;rft.atitle=Sequence%26%23x20%3BA110894&amp;rft_id=https%3A%2F%2Foeis.org%2FA110894&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><cite id="CITEREFSloane_&#34;A270712&#34;"><a href="https://en.wikipedia.org/wiki/Neil_Sloane" title="Neil Sloane">Sloane, N. J. A.</a> (ed.). <a rel="nofollow" href="https://oeis.org/A270712">&#34;Sequence A270712&#34;</a>. <i>The <a href="https://en.wikipedia.org/wiki/On-Line_Encyclopedia_of_Integer_Sequences" title="On-Line Encyclopedia of Integer Sequences">On-Line Encyclopedia of Integer Sequences</a></i>. OEIS Foundation.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+On-Line+Encyclopedia+of+Integer+Sequences&amp;rft.atitle=Sequence%26%23x20%3BA270712&amp;rft_id=https%3A%2F%2Foeis.org%2FA270712&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A68%E2%80%9395%E2%80%9399.7+rule"></span></span>
</li>
</ol></div></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule&amp;action=edit&amp;section=6" title="Edit section: External links">edit</a><span>]</span></span></h2>
<ul><li>&#34;<a rel="nofollow" href="http://www-stat.stanford.edu/~naras/jsm/NormalDensity/NormalDensity.html">The Normal Distribution</a>&#34; by Balasubramanian Narasimhan</li>
<li>&#34;<a rel="nofollow" href="http://www.wolframalpha.com/input/?i=erf%28x%2Fsqrt%282%29%29">Calculate percentage proportion within <i>x</i> sigmas</a> at WolframAlpha</li></ul>

<!-- 
NewPP limit report
Parsed by mw2425
Cached time: 20230402021136
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.331 seconds
Real time usage: 0.544 seconds
Preprocessor visited node count: 1817/1000000
Post‐expand include size: 101299/2097152 bytes
Template argument size: 2193/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 36547/5000000 bytes
Lua time usage: 0.194/10.000 seconds
Lua memory usage: 8799034/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  354.884      1 -total
 28.82%  102.285      1 Template:Reflist
 20.60%   73.122      4 Template:Navbox
 20.52%   72.827      1 Template:ProbDistributions
 18.57%   65.905      5 Template:Cite_book
 11.84%   42.008      1 Template:Short_description
  9.25%   32.844      1 Template:Citation_needed
  7.94%   28.185      1 Template:Fix
  7.87%   27.941      9 Template:Val
  7.25%   25.730      1 Template:Or-section
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:9606881-0!canonical and timestamp 20230402021135 and revision id 1145360219. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> -->
</div></div>
  </body>
</html>
