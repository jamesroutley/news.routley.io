<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://healeycodes.com/optimizing-my-disk-usage-program">Original</a>
    <h1>Optimizing My Disk Usage Program</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>In my previous post, <a href="https://healeycodes.com/maybe-the-fastest-disk-usage-program-on-macos">Maybe the Fastest Disk Usage Program on macOS</a>, I wrote about <a href="https://github.com/healeycodes/dumac">dumac</a>. A very fast alternative to <code>du -sh</code> that uses a macOS-specific syscall <a href="https://man.freebsd.org/cgi/man.cgi?query=getattrlistbulk&amp;sektion=2&amp;manpath=macOS+13.6.5">getattrlistbulk</a> to be much faster than the next leading disk usage program.</p><p>I received some great technical feedback in the <a href="https://lobste.rs/s/ddphh5/maybe_fastest_disk_usage_program_on_macos">Lobsters thread</a>. After implementing some of the suggestions, I was able to increase performance by ~28% on my <a href="https://github.com/healeycodes/dumac/blob/a2901c6867f194be73f92486826f33d3cf7658cb/setup_benchmark.py#L90">large benchmark</a>.</p><pre><div><div><p><span>hyperfine --warmup 3 --min-runs 5 &#39;./before temp/deep&#39; &#39;./after temp/deep&#39;</span></p><p><span>Benchmark 1: ./before temp/deep</span></p><p><span>  Time (mean ± σ):     910.4 ms ±  10.1 ms    [User: 133.4 ms, System: 3888.5 ms]</span></p><p><span>  Range (min … max):   894.5 ms … 920.0 ms    5 runs</span></p><p><span>Benchmark 2: ./after temp/deep</span></p><p><span>  Time (mean ± σ):     711.9 ms ±  10.5 ms    [User: 73.9 ms, System: 2705.6 ms]</span></p><p><span>  Range (min … max):   700.1 ms … 725.0 ms    5 runs</span></p><p><span>Summary</span></p><p><span>  ./after temp/deep ran</span></p><p><span>    1.28 ± 0.02 times faster than ./before temp/deep</span></p></div></div></pre><p>The main performance gain came from reducing thread scheduling overhead, while minor gains were from optimizing access to the inode hash-set shards.</p><h2 id="better-parallelism">Better Parallelism</h2><p>The previous version of <code>dumac</code> used <a href="https://crates.io/crates/tokio">Tokio</a> to spawn a task for each directory.</p><pre><div><div><p><span>// Process subdirectories concurrently</span><span></span></p><p><span></span><span>if</span><span> </span><span>!</span><span>dir_info</span><span>.</span><span>subdirs</span><span>.</span><span>is_empty</span><span>(</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>let</span><span> futures</span><span>:</span><span> </span><span>Vec</span><span>&lt;</span><span>_</span><span>&gt;</span><span> </span><span>=</span><span> dir_info</span><span>.</span><span>subdirs</span><span>.</span><span>into_iter</span><span>(</span><span>)</span><span></span></p><p><span>        </span><span>.</span><span>map</span><span>(</span><span>|</span><span>subdir</span><span>|</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>let</span><span> subdir_path </span><span>=</span><span> </span><span>Path</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>root_dir</span><span>)</span><span></span></p><p><span>                </span><span>.</span><span>join</span><span>(</span><span>&amp;</span><span>subdir</span><span>)</span><span></span></p><p><span>                  </span><span>.</span><span>to_string_lossy</span><span>(</span><span>)</span><span></span></p><p><span>                  </span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>;</span><span></span></p><p><span>            </span><span>tokio</span><span>::</span><span>spawn</span><span>(</span><span>async</span><span> </span><span>move</span><span> </span><span>{</span><span></span></p><p><span>                </span><span>calculate_size</span><span>(</span><span>subdir_path</span><span>)</span><span>.</span><span>await</span><span></span></p><p><span>            </span><span>}</span><span>)</span><span></span></p><p><span>        </span><span>}</span><span>)</span><span></span></p><p><span>        </span><span>.</span><span>collect</span><span>(</span><span>)</span><span>;</span><span></span></p><p><span>    </span><span>// Collect all results</span><span></span></p><p><span>    </span><span>for</span><span> future </span><span>in</span><span> futures </span><span>{</span><span></span></p><p><span>        </span><span>match</span><span> future</span><span>.</span><span>await</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>Ok</span><span>(</span><span>Ok</span><span>(</span><span>size</span><span>)</span><span>)</span><span> </span><span>=&gt;</span><span> total_size </span><span>+=</span><span> size</span><span>,</span><span></span></p><p><span>            </span><span>// ..</span><span></span></p><p><span>        </span><span>}</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span></span><span>}</span></p></div></div></pre><p>And then in the middle of this task, calling <code>getattrlistbulk</code> required a blocking call:</p><pre><div><div><p><span>// In the middle of the async task</span><span></span></p><p><span></span><span>// ..</span><span></span></p><p><span></span><span>let</span><span> dir_info </span><span>=</span><span> </span><span>tokio</span><span>::</span><span>task</span><span>::</span><span>spawn_blocking</span><span>(</span><span>{</span><span></span></p><p><span>    </span><span>let</span><span> root_dir </span><span>=</span><span> root_dir</span><span>.</span><span>clone</span><span>(</span><span>)</span><span>;</span><span></span></p><p><span>    </span><span>move</span><span> </span><span>|</span><span>|</span><span> </span><span>get_dir_info</span><span>(</span><span>&amp;</span><span>root_dir</span><span>)</span><span> </span><span>// Calls getattrlistbulk</span><span></span></p><p><span></span><span>}</span><span>)</span><span>.</span><span>await</span><span>.</span><span>map_err</span><span>(</span><span>|</span><span>_</span><span>|</span><span> </span><span>&#34;task join error&#34;</span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>)</span><span>?</span><span>?</span><span>;</span></p></div></div></pre><p>Tokio runs many tasks on a few threads by swapping the running task at each <code>.await</code>. Blocking threads are spawned on demand to avoid blocking the core threads which are handling tasks. I learned this <em>after</em> shipping the first version when I read <a href="https://docs.rs/tokio/latest/tokio/index.html#cpu-bound-tasks-and-blocking-code">CPU-bound tasks and blocking code</a>.</p><p>Spawning a new thread for each syscall is unnecessary overhead. Additionally, there aren&#39;t many opportunities to use non-blocking I/O since <code>getattrlistbulk</code> is blocking. Opening the file descriptor on the directory could be made async with something like <a href="https://docs.rs/tokio/latest/tokio/io/unix/struct.AsyncFd.html">AsyncFd</a> but it&#39;s very quick and isn&#39;t the bottleneck.</p><p>To reiterate the problem: I&#39;m using Tokio as a thread pool even though I&#39;m not directly benefiting from the task scheduling overhead, and worse, I&#39;m creating lots of new threads (one per directory).</p><p>YogurtGuy saw this issue and suggested using <a href="https://crates.io/crates/rayon">Rayon</a> instead of Tokio:</p><blockquote><p>As it stands, each thread must operate on a semaphore resource to limit concurrency. If this was limited directly via the number of threads, each thread could perform operations without trampling on each other. […] In addition, each call to <code>spawn_blocking</code> appears to involve inter-thread communication. Work-stealing would allow each thread to create and consume work without communication.</p></blockquote><p>By using Rayon, I can re-use threads from a thread pool, and avoid creating a new thread for each <code>getattrlistbulk</code> call. Using a work-stealing design, I can recursively call <code>calculate_size</code>:</p><pre><div><div><p><span>// Calculate total size recursively using rayon work-stealing</span><span></span></p><p><span></span><span>pub</span><span> </span><span>fn</span><span> </span><span>calculate_size</span><span>(</span><span>root_dir</span><span>:</span><span> </span><span>String</span><span>)</span><span> </span><span>-&gt;</span><span> </span><span>Result</span><span>&lt;</span><span>i64</span><span>,</span><span> </span><span>String</span><span>&gt;</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>// ..</span><span></span></p><p><span>    </span><span>// Process subdirectories in parallel</span><span></span></p><p><span>    </span><span>let</span><span> subdir_size </span><span>=</span><span> </span><span>if</span><span> </span><span>!</span><span>dir_info</span><span>.</span><span>subdirs</span><span>.</span><span>is_empty</span><span>(</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>        dir_info</span></p><p><span>            </span><span>.</span><span>subdirs</span></p><p><span>            </span><span>// Parallel iterator.</span><span></span></p><p><span>            </span><span>// The recursive calculate_size calls are scheduled with</span><span></span></p><p><span>            </span><span>// very little overhead!</span><span></span></p><p><span>            </span><span>.</span><span>into_par_iter</span><span>(</span><span>)</span><span></span></p><p><span>            </span><span>.</span><span>map</span><span>(</span><span>|</span><span>subdir</span><span>|</span><span> </span><span>{</span><span></span></p><p><span>                </span><span>let</span><span> subdir_path </span><span>=</span><span> </span><span>Path</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>root_dir</span><span>)</span><span></span></p><p><span>                    </span><span>.</span><span>join</span><span>(</span><span>&amp;</span><span>subdir</span><span>)</span><span></span></p><p><span>                    </span><span>.</span><span>to_string_lossy</span><span>(</span><span>)</span><span></span></p><p><span>                    </span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>;</span><span></span></p><p><span>                </span><span>calculate_size</span><span>(</span><span>subdir_path</span><span>)</span><span></span></p><p><span>            </span><span>}</span><span>)</span><span></span></p><p><span>            </span><span>.</span><span>map</span><span>(</span><span>|</span><span>result</span><span>|</span><span> </span><span>match</span><span> result </span><span>{</span><span></span></p><p><span>                </span><span>Ok</span><span>(</span><span>size</span><span>)</span><span> </span><span>=&gt;</span><span> size</span><span>,</span><span></span></p><p><span>                </span><span>Err</span><span>(</span><span>e</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>                    </span><span>eprintln!</span><span>(</span><span>&#34;dumac: {}&#34;</span><span>,</span><span> e</span><span>)</span><span>;</span><span></span></p><p><span>                    </span><span>0</span><span></span></p><p><span>                </span><span>}</span><span></span></p><p><span>            </span><span>}</span><span>)</span><span></span></p><p><span>            </span><span>.</span><span>sum</span><span>(</span><span>)</span><span></span></p><p><span>    </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>0</span><span></span></p><p><span>    </span><span>}</span><span>;</span><span></span></p><p><span>    </span><span>// ..</span></p></div></div></pre><p>I benchmarked and profiled these two approaches to see what changes I could observe. Tokio tasks with many blocking calls, and Rayon work-stealing.</p><p>Using macOS&#39;s Instruments, I checked that <code>dumac</code> was now using a fixed amount of threads:</p><p><span><img alt="Comparing the system call trace view by thread" fetchpriority="high" width="725" height="326.1966126656848" decoding="async" data-nimg="1" srcset="/_next/image?url=%2Fposts%2Foptimizing-my-disk-usage-program%2Fthreads.png&amp;w=750&amp;q=100 1x, /_next/image?url=%2Fposts%2Foptimizing-my-disk-usage-program%2Fthreads.png&amp;w=1920&amp;q=100 2x" src="https://www.ghacks.net/_next/image?url=%2Fposts%2Foptimizing-my-disk-usage-program%2Fthreads.png&amp;w=1920&amp;q=100"/></span></p><p>Additionally, the number of syscalls has been halved. Although the <em>count</em> of syscalls is not a perfect proxy for performance, in this case, it suggests we&#39;ve achieved the simplicity we&#39;re after.</p><p>The macOS syscalls related to Tokio&#39;s thread scheduling are greatly reduced. Also, not pictured, the number of context switches was reduced by ~80% (1.2M -&gt; 235k).</p><p><span><img alt="Comparing the system call trace view by syscall" fetchpriority="high" width="725" height="238.46790205162145" decoding="async" data-nimg="1" srcset="/_next/image?url=%2Fposts%2Foptimizing-my-disk-usage-program%2Fsyscalls.png&amp;w=750&amp;q=100 1x, /_next/image?url=%2Fposts%2Foptimizing-my-disk-usage-program%2Fsyscalls.png&amp;w=1920&amp;q=100 2x" src="https://www.ghacks.net/_next/image?url=%2Fposts%2Foptimizing-my-disk-usage-program%2Fsyscalls.png&amp;w=1920&amp;q=100"/></span></p><p>And finally, the most important result is the large benchmark.</p><pre><div><div><p><span>hyperfine --warmup 3 --min-runs 5 &#39;./before temp/deep&#39; &#39;./after temp/deep&#39;</span></p><p><span>Benchmark 1: ./before temp/deep</span></p><p><span>  Time (mean ± σ):     901.3 ms ±  47.1 ms    [User: 125.5 ms, System: 3394.6 ms]</span></p><p><span>  Range (min … max):   821.6 ms … 942.8 ms    5 runs</span></p><p><span>Benchmark 2: ./after temp/deep</span></p><p><span>  Time (mean ± σ):     731.6 ms ±  20.6 ms    [User: 76.5 ms, System: 2681.7 ms]</span></p><p><span>  Range (min … max):   717.4 ms … 767.1 ms    5 runs</span></p><p><span>Summary</span></p><p><span>  ./after temp/deep ran</span></p><p><span>    1.23 ± 0.07 times faster than ./before temp/deep</span></p></div></div></pre><p>Why is it faster? Because we&#39;re creating and managing fewer threads, and we&#39;re waiting on less syscalls that are unrelated to the core work we&#39;re doing.</p><h2 id="reducing-inode-lock-contention">Reducing Inode Lock Contention</h2><p>YogurtGuy had <a href="https://lobste.rs/s/ddphh5/maybe_fastest_disk_usage_program_on_macos#c_fkix5i">another point</a> on how <code>dumac</code> deduplicates inodes. In order to accurately report disk usage, hard links are deduplicated by their underlying inode. This means that, while our highly concurrent program is running, we need to read and write a data structure from all running threads.</p><p>I chose a sharded hash-set to reduce lock contention. Rather than a single hash-set with a single mutex, there are <code>128</code> hash-sets with <code>128</code> mutexes. The inode (a <code>u64</code>) is moduloed by <code>128</code> to find the hash-set that needs to be locked and accessed:</p><pre><div><div><p><span>// Global sharded inode set for hardlink deduplication</span><span></span></p><p><span></span><span>static</span><span> </span><span>SEEN_INODES</span><span>:</span><span> </span><span>LazyLock</span><span>&lt;</span><span>[</span><span>Mutex</span><span>&lt;</span><span>HashSet</span><span>&lt;</span><span>u64</span><span>&gt;&gt;</span><span>;</span><span> </span><span>SHARD_COUNT</span><span>]</span><span>&gt;</span><span> </span><span>=</span><span></span></p><p><span>    </span><span>LazyLock</span><span>::</span><span>new</span><span>(</span><span>|</span><span>|</span><span> </span><span>std</span><span>::</span><span>array</span><span>::</span><span>from_fn</span><span>(</span><span>|</span><span>_</span><span>|</span><span> </span><span>Mutex</span><span>::</span><span>new</span><span>(</span><span>HashSet</span><span>::</span><span>new</span><span>(</span><span>)</span><span>)</span><span>)</span><span>)</span><span>;</span><span></span></p><p><span></span><span>fn</span><span> </span><span>shard_for_inode</span><span>(</span><span>inode</span><span>:</span><span> </span><span>u64</span><span>)</span><span> </span><span>-&gt;</span><span> </span><span>usize</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>(</span><span>inode </span><span>%</span><span> </span><span>SHARD_COUNT</span><span> </span><span>as</span><span> </span><span>u64</span><span>)</span><span> </span><span>as</span><span> </span><span>usize</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>// Returns the blocks to add (blocks if newly seen, 0 if already seen)</span><span></span></p><p><span></span><span>fn</span><span> </span><span>check_and_add_inode</span><span>(</span><span>inode</span><span>:</span><span> </span><span>u64</span><span>,</span><span> blocks</span><span>:</span><span> </span><span>i64</span><span>)</span><span> </span><span>-&gt;</span><span> </span><span>i64</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>let</span><span> shard_idx </span><span>=</span><span> </span><span>shard_for_inode</span><span>(</span><span>inode</span><span>)</span><span>;</span><span></span></p><p><span>    </span><span>let</span><span> shard </span><span>=</span><span> </span><span>&amp;</span><span>SEEN_INODES</span><span>[</span><span>shard_idx</span><span>]</span><span>;</span><span></span></p><p><span>    </span><span>let</span><span> </span><span>mut</span><span> seen </span><span>=</span><span> shard</span><span>.</span><span>lock</span><span>(</span><span>)</span><span>;</span><span></span></p><p><span>    </span><span>if</span><span> seen</span><span>.</span><span>insert</span><span>(</span><span>inode</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>        blocks </span><span>// Inode was newly added, count the blocks</span><span></span></p><p><span>    </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>0</span><span> </span><span>// Inode already seen, don&#39;t count</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span></span><span>}</span></p></div></div></pre><p>This is the correct solution if inodes are distributed randomly across the <code>u64</code> space but, as <a href="https://lobste.rs/s/ddphh5/maybe_fastest_disk_usage_program_on_macos#c_fkix5i">YogurtGuy points out</a>, they are not:</p><blockquote><p>I like the sharded hash-set approach, but I think the implementation may have some inefficiencies. First, the <a href="https://github.com/healeycodes/dumac/blob/152dad272ae3e1c73ecaead23341fb32392729ee/src/main.rs#L42">shard is chosen by <code>inode % SHARD_COUNT</code></a>. Inodes tend to be sequential, so while this choice of shard will distribute requests across multiple hash sets, it will also increase contention, since a single directory will have its inodes distributed across every hash set. I wonder if <code>(inode &gt;&gt; K) % SHARD_COUNT</code> might therefore lead to better performance for some values of <code>K</code>. Especially if each thread batched its requests to each hash set.</p></blockquote><p>I looked at the data, and saw that they were right.</p><pre><div><div><p><span>// Inside calculate_size</span><span></span></p><p><span></span><span>// ..</span><span></span></p><p><span>all_inodes</span><span>.</span><span>sort</span><span>(</span><span>)</span><span>;</span><span></span></p><p><span></span><span>println!</span><span>(</span><span>&#34;dir_info: {:?}, all_inodes: {:?}&#34;</span><span>,</span><span> dir_info</span><span>,</span><span> all_inodes</span><span>)</span><span>;</span><span></span></p><p><span></span><span>// They&#39;re sequential!</span><span></span></p><p><span></span><span>// subdirs: [] }, all_inodes: [50075095, 50075096, 50075097, 50075098, 50075099, 50075100, 50075101, 50075102, 50075103, 50075104, 50075105, 50075106, 50075107, 50075108, 50075109, 50075110, 50075111, 50075112, 50075113, 50075114, 50075115, 50075116, 50075117, 50075118, 50075119, 50075120, 50075121, 50075122, 50075123, 50075124, 50075125, 50075126, 50075127, 50075128, 50075129, 50075130, 50075131, 50075132, 50075133, 50075134, 50075135, 50075136, 50075137, 50075138, 50075139, 50075140, 50075141, 50075142, 50075143, 50075144, 50075145, 50075146, 50075147, 50075148, 50075149, 50075150, 50075151, 50075152, 50075153, 50075154, 50075155, 50075156, 50075157, 50075158, 50075159, 50075160, 50075161, 50075162, 50075163, 50075164, 50075165, 50075166, 50075167, 50075168, 50075169, 50075170, 50075171, 50075172, 50075173, 50075174, 50075175, 50075176, 50075177, 50075178, 50075179, 50075180, 50075181, 50075182, 50075183, 50075184, 50075185, 50075186, 50075187, 50075188, 50075189, 50075190, 50075191, 50075192, 50075193, 50075194]</span></p></div></div></pre><p>Each directory I spot-checked seemed to have sequential inodes. Inodes are created at the filesystem level (not at the directory level) and since my benchmark files were created in quick succession each directory&#39;s files are roughly sequential.</p><p>This is true for many real-life cases. The contents of a directory are often written at the same time (e.g. <code>npm i</code>).</p><p>The reason this is a problem is that when we modulo by 128, we don&#39;t necessarily reduce the chance of lock collisions. Recall that we&#39;re trying to take our inodes and shard them across many hash-sets. This is what that looks like:</p><pre><div><div><p><span># dir1 handled by thread1</span></p><p><span>50075095 % 128 = 87</span></p><p><span>50075096 % 128 = 88</span></p><p><span>50075097 % 128 = 89</span></p><p><span>50075098 % 128 = 90</span></p><p><span>50075099 % 128 = 91</span></p></div></div></pre><p>But if there&#39;s a separate directory with inodes starting at an entirely different point, like, say <code>65081175</code>, they can modulo to the same hash-set:</p><pre><div><div><p><span># dir2 handled by thread2</span></p><p><span>65081175 % 128 = 87 # same values as above</span></p><p><span>65081176 % 128 = 88</span></p><p><span>65081177 % 128 = 89</span></p><p><span>65081178 % 128 = 90</span></p><p><span>65081179 % 128 = 91</span></p></div></div></pre><p>In the worst case, if thread1 and thread2 run at the same time, they could fight for the lock on each of the entries they are handling! You can imagine how many threads iterating over directories could hit this case.</p><p>I tested it while running my benchmark:</p><pre><div><div><p><span>if</span><span> shard</span><span>.</span><span>is_locked</span><span>(</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>println!</span><span>(</span><span>&#34;shard is locked&#34;</span><span>)</span><span>;</span><span></span></p><p><span></span><span>}</span></p></div></div></pre><p>And found the average like so:</p><pre><div><div><p><span>avg</span><span>=</span><span>$(</span><span>sum</span><span>=</span><span>0</span><span>;</span><span> </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>{</span><span>1</span><span>..</span><span>15</span><span>}</span><span>;</span><span> </span><span>do</span><span> </span><span>c</span><span>=</span><span>$(</span><span>./target/release/dumac temp/deep </span><span>|</span><span> </span><span>grep</span><span> -c </span><span>&#34;shard is locked&#34;</span><span>)</span><span>;</span><span> </span><span>sum</span><span>=</span><span>$((</span><span>sum + c</span><span>)</span><span>)</span><span>;</span><span> </span><span>done</span><span>;</span><span> </span><span>echo</span><span> </span><span>&#34;scale=2; </span><span>$sum</span><span> / 15&#34;</span><span> </span><span>|</span><span> </span><span>bc</span><span>)</span><span>;</span><span> </span><span>echo</span><span> </span><span>&#34;Average: </span><span>$avg</span><span>&#34;</span><span></span></p><p><span></span><span># Average: 176.66</span></p></div></div></pre><p>Since directories are handled one at a time (for each thread) it would be ideal if the shard was per directory — but this isn&#39;t possible. We can get close by removing some of the least significant bits, say, <code>8</code>.</p><pre><div><div><p><span># dir1 handled by thread1</span></p><p><span>50075095 &gt;&gt; 8 = 195605 # this block of 256 entries share a key</span></p><p><span>50075096 &gt;&gt; 8 = 195605</span></p><p><span>50075097 &gt;&gt; 8 = 195605</span></p><p><span>50075098 &gt;&gt; 8 = 195605</span></p><p><span>50075099 &gt;&gt; 8 = 195605</span></p><p><span># dir2 handled by thread2</span></p><p><span>65081175 &gt;&gt; 8 = 254223 # this separate block of 256 entries share a different key</span></p><p><span>65081176 &gt;&gt; 8 = 254223</span></p><p><span>65081177 &gt;&gt; 8 = 254223</span></p><p><span>65081178 &gt;&gt; 8 = 254223</span></p><p><span>65081179 &gt;&gt; 8 = 254223</span></p></div></div></pre><p>I tested this idea using my benchmark:</p><pre><div><div><p><span>fn</span><span> </span><span>shard_for_inode</span><span>(</span><span>inode</span><span>:</span><span> </span><span>u64</span><span>)</span><span> </span><span>-&gt;</span><span> </span><span>usize</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>(</span><span>(</span><span>inode </span><span>&gt;&gt;</span><span> </span><span>8</span><span>)</span><span> </span><span>%</span><span> </span><span>SHARD_COUNT</span><span> </span><span>as</span><span> </span><span>u64</span><span>)</span><span> </span><span>as</span><span> </span><span>usize</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>// avg=$(sum=0; for i in {1..15}; do c=$(./target/release/dumac temp/deep | grep -c &#34;shard is locked&#34;); sum=$((sum + c)); done; echo &#34;scale=2; $sum / 15&#34; | bc); echo &#34;Average: $avg&#34;</span><span></span></p><p><span></span><span>// Average: 4.66</span></p></div></div></pre><p>The average lock collision dropped dramatically from <code>176.66</code> to <code>4.66</code>. I was surprised at this result. I didn&#39;t expect the decrease to be so great.</p><p>I also tested with some hash functions that avalanche sequential integers but the results were comparable to my original idea with just the modulo.</p><p>So, what is so special about <code>inode &gt;&gt; 8</code>?</p><p><a href="https://developer.apple.com/support/downloads/Apple-File-System-Reference.pdf">APFS</a> hands out IDs sequentially so the bottom bits toggle fastest, causing threads that are scanning directory trees in creation order to pile into the same shard when using <code>inode % 128</code>.</p><p>Shifting by <code>8</code> groups <code>256</code> consecutive inodes together. This improves temporal locality and cuts down inter-shard contention in our multithreaded crawl even though the statistical distribution stays perfectly flat across the whole disk.</p><p>The ideal number of inodes to group together depends on the number of inodes that were created at the same time per directory. Since that&#39;s not possible to work out ahead of time, I will pick <code>256</code> (my gut says that most directories have fewer than <code>256</code> files) and will keep shifting by <code>8</code> bits.</p><p>As a result, the reduced lock contention improved the performance of the large benchmark by ~5% or so.</p><p>I just pushed up these performance improvements to <code>dumac</code>&#39;s <a href="https://github.com/healeycodes/dumac">repository</a>. Let me know if you have any technical feedback!</p></div></div></div>
  </body>
</html>
