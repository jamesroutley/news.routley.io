<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://capnfabs.net/posts/fuji-raf-compression-algorithm/">Original</a>
    <h1>How Does Lossless Compression in Fuji RAF Files Work? (2020)</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody" id="content"><p>Oh boy, this is going to be a doozy of a blog post.</p><p>I‚Äôve spent the last three months in New York<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> at the <a href="https://www.recurse.com/scout/click?t=2d223bf1d085781da7bd13f08d9cc914">Recurse Center</a>, developing an understanding of the fundamentals of digital photography.</p><p>Something that‚Äôs been really, really rewarding about this project is that I‚Äôve been able to take my own images, start with the actual bits in a <a href="https://ridiculousfish.com/hexfiend/">hex editor</a>, and interpret them into images that progressively become more faithful and more beautiful, the more code I write.</p><p>Well, almost. There was one minor hiccup ‚Äì all of the RAW files I‚Äôve shot over the last 3 years have been encoded with Fuji‚Äôs lossless image compression algorithm. This is a fantastic technology as a photographer ‚Äì 50 MB files are swiftly reduced to 25 MB, with literally no reduction in quality ‚Äì but if you‚Äôre writing your own processor, then you need to decompress the data, and compression algorithms are usually <em>complex</em>.</p><h2 id="when-should-you-implement-something-yourself">When should you implement something yourself?</h2><p>I <em>knew</em> that this was going to be a rabbit hole before I started. I could <em>hear</em> the voice of my former boss saying ‚ÄúFabian. Do you really need to be doing this?‚Äù.</p><p>I‚Äôve written about <a href="https://capnfabs.net/posts/when-do-you-stop-writing/">Yak Shaves before</a>, and in theory I should know better ‚Äì but, part of what attracted me to the Recurse Center in the first place was it seemed like a place where going down rabbit holes was permitted. Maybe not advised, but definitely permitted. So, in the first week of February, I found myself trying to figure out how Fuji‚Äôs lossless image compression technology worked.</p><p>The thing was, I already had the ability to load compressed Fuji RAF files in my software ‚Äì the excellent, albeit minimally documented, <a href="https://www.libraw.org/">LibRaw</a> project has had support for this since 2016, and my code was initially loading files by binding to LibRaw. At some point, though, I got frustrated with not knowing exactly what was <em>in</em> my RAW file. Libraw is functionally very good, but it doesn‚Äôt do a good job of surfacing what its sources are for any given piece of data<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>. One thing led to another, and at some point I‚Äôd decided to replace LibRaw with my own code. Which meant I‚Äôd need to re-implement Fuji‚Äôs lossless compression algorithm, at least for the photos from my specific camera model.</p><h2 id="how-does-it-work">How does it work?</h2><p>Fuji RAF lossless compression can be characterised as an adaptive, differential compression algorithm. Let‚Äôs break that down:</p><ul><li><em>Adaptive</em>: the algorithm changes with the data it has processed in the past</li><li><em>Differential</em>: the algorithm is predicated on storing the <em>difference</em> between an ‚Äôexpected‚Äô value and the actual value.</li></ul><h2 id="step-1-split-into-stripes">Step 1: Split into stripes</h2><p>First, split the image into a set of vertical stripes. My camera (the Fuji X-T2) uses 8. Each of these stripes is encoded independently (i.e. with separate input, output, and state), which means it‚Äôs <em>possible to parallelise the encoding and decoding</em>. This turns out to be super important for performance on modern CPUs ‚Äì parallelising my decoder resulted in a 6x speedup in user time (on a 4-core hyperthreaded machine).</p><figure><figcaption><p>Splitting the image into vertical stripes</p></figcaption></figure><h2 id="step-2-sensor-data--colored-vector-collation">Step 2: Sensor data ‚Üí Colored vector collation</h2><p>Now that we‚Äôve split the image into stripes, we can break each stripe down further into <em>lines</em> of nx6 pixels. We‚Äôll take each of those lines, and map their pixels into color vectors. It‚Äôs probably easiest to explain this with a diagram:</p><figure><figcaption><p>Mapping Sensor Pixels to Colored Blocks</p></figcaption></figure><p>If you‚Äôve never worked with digital imaging before, you might be surprised that each pixel in a camera sensor <em>only represents one color</em>. The pixels themselves aren‚Äôt capable of differentiating between different colors of light, just of counting approximate numbers of photons, so manufacturers usually slap a <a href="https://en.wikipedia.org/wiki/Color_filter_array">Color Filter Array</a> on top to narrow the color range that each pixel responds to.</p><p>We collate the pixels by color because later, we‚Äôll apply a transform that encodes the differences between neighbouring pixels. If the neighbouring pixels all represent the same component color, those differences are smaller, which allows for better compression.</p><p>Now that we‚Äôve got the data collated into solid color blocks, we‚Äôre ready to start processing.</p><h2 id="step-3-color-vectors--bits">Step 3: Color vectors ‚Üí bits</h2><p>Here‚Äôs where it gets real interesting. Now that we‚Äôve collected the data into color vectors, we interleave two color vectors and compress them together.</p><p>Let‚Äôs use R0 and G0 as our first pair of color-lines. We now need to iterate through each item of R and G, but the order is kinda special:</p><blockquote><p>R[0], G[0], R[2], G[2], R[4], G[4], R[6], G[6], R[1], G[1], R[8], G[8], R[3], G[3], ‚Ä¶</p></blockquote><p>We start by iterating through the <em>even</em> slots in R and G. After we‚Äôve done the first 4 of each, we can <em>also</em> start iterating through the odd slots, which will always be 5 positions behind the even slots.</p><p>Here it is again in diagram format:</p><figure><figcaption><p>Iteration Order. Notice that we start jumping back to the odd pixels after we‚Äôve started with the even pixels.</p></figcaption></figure><p>This feels weird, but hold with me, we‚Äôll be able to explain it very soon! I promise.</p><h3 id="making-a-sample-for-a-single-value">Making a sample for a single value</h3><p>Now, the idea for a single value is:</p><ul><li>Figure out an <em>expected value</em> for the cell, by (approximately) taking a weighted average of the <em>already processed values</em> around it</li><li>Compute the difference between the <em>actual</em> value and the expected value</li><li>Encode the difference and send to output</li><li>Adapt the encoding process so that it gives better compression ratios in future.</li></ul><p>Let‚Äôs examine each of those steps individually:</p><h3 id="computing-an-expected-value">Computing an expected value</h3><p>Loosely, the expected value of a cell is computed as a weighted average of values around it. What‚Äôs vital about this, however, is that these are values that we‚Äôve <em>already processed</em>, because when we‚Äôre decoding, we‚Äôll use <em>expected value</em> + <em>difference</em> to compute the output value. The decoder will need to be able to compute the <em>expected value</em>, which means that the encoder can only use values it‚Äôs already decoded to compute an expected value.</p><p>The details of this are probably better represented in code, but here‚Äôs a visual indication of the cells we‚Äôre selecting for the weighted average. For even cells, we select these:</p><figure><figcaption><p>Reference pixels for computing Weighted Average, for <strong>even</strong>-indexed pixels</p></figcaption></figure><p>And for odd cells, it‚Äôs these:</p><figure><figcaption><p>Reference pixels for computing Weighted Average, for <strong>odd</strong>-indexed pixels</p></figcaption></figure><p>This reveals a couple of important ordering dependencies:</p><ol><li>Part of the reason why we don‚Äôt start iterating on odd cells until we‚Äôve done a bunch of even cells is because we need the even cells to be processed in order to process the odd cells<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</li><li>To compute the weighed average for each row, we need the values of the previous two rows.</li></ol><p>The specifics of the weighted average formulas feel <em>pretty</em> esoteric to me. I‚Äôm inclined to suggest that engineers within Fuji were trying stuff out to see what gave the best compression ratios across a bunch of different sample images, found something that worked and was low-cost, and baked that in to the format<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.</p><h3 id="compute-the-difference">Compute the difference</h3><p>This is straightforward! The difference is simply <code>actual_value - expected_value</code>. üéâ</p><h3 id="encode-the-difference">Encode the difference</h3><p>Because photographic images tend to have big blocks of color, and to change slowly across the image, it‚Äôs typically possible to represent the difference from the weighted average in less bits than are required to store the value directly. For an image where every pixel is encoded with 14 bits, we can typically represent the difference between neighbouring pixels with maybe 5 or 6 bits.</p><p>Unfortunately, we can‚Äôt simply store the differences and call it ‚Äòdone‚Äô ‚Äì just because the differences are <em>typically</em> low, it doesn‚Äôt mean they‚Äôre <em>always</em> low, and in order to capture <em>every possible</em> difference from the weighted average, we require the same number of bits as we had to start with. We‚Äôre going to need a clever encoding scheme to handle this discrepancy.</p><p>Let‚Äôs say that we‚Äôve got a difference of 27 from the expected value. We‚Äôll represent that in binary as:</p><pre tabindex="0"><code>11011
</code></pre><p>Now, let‚Äôs say that most of the time, it only requires 6 bits to encode the difference between neighbouring pixels. We‚Äôd pad the binary number out to 6 bits:</p><pre tabindex="0"><code>011011
</code></pre><p>‚Ä¶ but we still need to encode whether we should add or subtract from the expected value. So let‚Äôs use <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two‚Äôs complement</a>. We now need to encode 2x the range of numbers (positive and negative), so we need to add another bit. For our example of adding to the expected value, this just means we need to add another zero at the front:</p><pre tabindex="0"><code>0011011
</code></pre><p>If we needed to instead <em>subtract</em> 27, we‚Äôd use:</p><pre tabindex="0"><code>1100101
</code></pre><p>With this 7-bit scheme, we could encode differences between -128 and +127 from the expected value.</p><h3 id="variable-length-codes">Variable Length Codes</h3><p>So, what should we do if we need to encode a difference <em>outside</em> of this range?</p><p>Let‚Äôs say that we need to encode a difference of 300 in our 7-bit scheme. We‚Äôll start by converting to binary, which requires 10 bits as a signed two‚Äôs complement integer:</p><pre tabindex="0"><code>+300 = 0100101100
</code></pre><p>Now, let‚Äôs split on the 7-bit boundary. We know we can store the last 7 bits using the scheme we already have:</p><pre tabindex="0"><code>???
|   7-bit fixed width
|   |
010 0101100
</code></pre><p>So we‚Äôve got <code>0b010</code> remaining at the front. That translates to 2 in decimal, which is a pretty small number. It‚Äôs worth noting that we‚Äôd expect it to be less likely for this overflow to be a 3, and even less likely again for it to be a 4, or 5, or a 6, because in general, we‚Äôre expecting the differences to be small.</p><p>This sounds like the perfect candidate for a <a href="https://en.wikipedia.org/wiki/Variable-length_code"><em>variable length code</em></a>. In fact, this is exactly what the algorithm does, converting the ‚Äò2‚Äô prefix into two 0s, followed by a 1:</p><pre tabindex="0"><code>     2 zeros
     |  terminating one
     |  |
2 =&gt; 00 1
</code></pre><p>Similarly:</p><pre tabindex="0"><code>5 =&gt; 000001
4 =&gt; 00001
3 =&gt; 0001
2 =&gt; 001
1 =&gt; 01
0 =&gt; 1
</code></pre><p>Simple, right? And now, we can encode our difference of 300 as:</p><pre tabindex="0"><code>        [variable-length]
        |
        |   [7-bit fixed-length]
        |   |
+300 =&gt; 001 0101100
</code></pre><p>For a difference of +300, we‚Äôre still only paying 3 + 7 bits = 10 bits, instead of the usual 14 to encode a full sample.</p><h3 id="when-twos-complement-isn-the-right-tool-for-the-job">When two‚Äôs complement <em>isn‚Äôt</em> the right tool for the job</h3><p>We‚Äôve made a problem for ourselves now: notice how we effectively ‚Äòdropped‚Äô the leading sign bit when we converted to variable length?</p><p>Consider the encoding of the following two numbers using our scheme thus far:</p><pre tabindex="0"><code>        [two&#39;s complement]      [encoded]
+300 =&gt; 0100101100 (10 bits) =&gt; 001 0101100
-212 =&gt;  100101100 ( 9 bits) =&gt; 001 0101100
</code></pre><p>Oh, they‚Äôre identical! That‚Äôs bad! We‚Äôre not sufficiently able to distinguish between smaller negative numbers and larger positive ones. It turns out that two‚Äôs complement encoding implicitly relies upon the size of the memory in which it is encoded, and our variable coding system drops that information, because it only encodes a single number.</p><p>So, let‚Äôs throw away our earlier choice of using two‚Äôs complement, and use a <a href="https://en.wikipedia.org/wiki/Signed_number_representations#Signed_magnitude_representation_(SMR)">signed magnitude representation</a>, storing the sign bit at the end, as part of the ‚Äòfixed-length code‚Äô section<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>. Let‚Äôs try encoding +300 and -212 again.</p><pre tabindex="0"><code>+300 =&gt; 100101100     [unsigned binary, 9 bits]
     =&gt; 1001011000    [add sign bit to end]
     =&gt; 100 1011000   [split on 7-bit boundary]
     =&gt; 00001 1011000 [encode using variable length code]

-212 =&gt; 11010100      [unsigned binary, 8 bits]
     =&gt; 110101001     [add sign bit to end]
     =&gt; 11 0101001    [split on 7-bit boundary]
     =&gt; 0001 0101001  [encode using variable length code]
</code></pre><p>Much better! They have different representations now.</p><h3 id="failsafes-for-large-differences">Failsafes for large differences</h3><p>You might have noticed that we‚Äôre getting pretty close to our original 14-bit sample size by this point. Indeed, for 14-bit samples, the worst-case difference will be that we‚Äôre off by 2^13 - 1 = 8192<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>, and if we try to represent that using our variable length coding scheme, we‚Äôll get:</p><pre tabindex="0"><code>+8191 =&gt; 1111111111111   [unsigned binary, 9 bits]
      =&gt; 11111111111110  [add sign bit to end]
      =&gt; 1111111 1111110 [split on 7 bit boundary]
      =&gt; 00000000 00000000 00000000 00000000
      .. 00000000 00000000 00000000 00000000
      .. 00000000 00000000 00000000 00000000
      .. 00000000 00000000 00000000 00000001
      .. 1111110 ü§î      [encode using variable length code]
</code></pre><p>At some point much earlier, it probably would have just made sense to somehow write that we wanted to directly encode the 14-bit difference instead of using the variable length system for this number.</p><p>Fuji‚Äôs encoding scheme has a failsafe mechanism for this ‚Äì if you end up writing 41 or more consecutive zeros, then stop appending zeros and encode the 14-bit delta directly. But‚Ä¶ 41 still seems like an awful lot, right?</p><p>The problem is, you need a way of encoding the failsafe into your scheme, too. You could, for example, add a ‚Äôtype‚Äô bit to every value to indicate whether it was absolute or relative. Then, your format would become something like:</p><pre tabindex="0"><code>         use variable code
         |
         | variable length part
         | |
         | |    fixed size part
         | |    |
 -212 =&gt; 1 0001 0101001
+8191 =&gt; 0 01111111111111
         | |
         | 14-bit two&#39;s complement difference
         |
         use 14-bit direct-encoded difference
</code></pre><p>But then you have a new problem: you‚Äôre effectively paying a <em>whole extra bit on every sample</em> for something you need extremely rarely. A single bit doesn‚Äôt seem like a lot, but on a 24 Megapixel image, it adds up to 3 MB per photo (!!).</p><p>In general, this is the line of thinking that compression schemes follow. It‚Äôs a tradeoff between ‚Äúhow much is this going to cost‚Äù vs ‚Äúhow often am I going to need to pay this cost?‚Äù I guess ‚Äò41 bits‚Äô is the tradeoff that Fuji picked, probably experimentally, to get the best performance out of the image<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.</p><p>Now that we‚Äôre talking about cost vs frequency, it‚Äôs probably a great time to talk about choosing a threshold for splitting between the Variable-Length and Direct-Binary coding schemes.</p><h3 id="how-big-should-your-direct-binary-coding-section-be">How Big Should your Direct Binary Coding Section be?</h3><p>Earlier, we stated:</p><blockquote><p>let‚Äôs say that most of the time, it only requires 6 bits to encode the difference between neighbouring pixels.</p></blockquote><p>That assumption‚Äôs served us well, but it‚Äôs time to admit that it‚Äôs not correct enough of the time. The optimal number of bits changes from image to image. Consider ‚Äì for an image that‚Äôs entirely a single colour, you could encode the difference between each pixel as 0, which requires exactly 0 bits to store, whereas for an image where pixel values alternate between 0 and 8191, every delta will be 14 bits (13 bits for 8191, +1 including the sign).</p><p>Further, assuming a fixed number of bits works best when photos are consistently variable across the whole frame, and they‚Äôre usually not. How many photos have huge swathes of blue sky in them and then details in the foreground?</p><figure><img srcset="/posts/fuji-raf-compression-algorithm/ROFL3122_hu_8d5e22ec412c40c7.jpg,
/posts/fuji-raf-compression-algorithm/ROFL3122_hu_a6bce204bf7f33c9.jpg 2x" src="https://capnfabs.net/posts/fuji-raf-compression-algorithm/ROFL3122_hu_8d5e22ec412c40c7.jpg" alt="Somewhere near Bach, Austria, September 2018"/><figcaption><p>Somewhere near Bach, Austria, September 2018</p></figcaption></figure><p>And like, what about photos with lots of background noise, because they were shot at night at ISO 12500?</p><figure><img srcset="/posts/fuji-raf-compression-algorithm/ROFL3506_hu_920fcc042576151d.jpg,
/posts/fuji-raf-compression-algorithm/ROFL3506_hu_7c3bdb0aa1fae143.jpg 2x" src="https://capnfabs.net/posts/fuji-raf-compression-algorithm/ROFL3506_hu_920fcc042576151d.jpg" alt="I guess there was a full moon that night."/><figcaption><p>I guess there was a full moon that night.</p></figcaption></figure><h3 id="lets-make-it-adaptive">Let‚Äôs make it <em>ADAPTIVE</em></h3><p>Rather than choose a constant number of bits for the ‚Äúdirect-encoded‚Äù part of each value, we can set things up so that the number of bits changes in response to the input. Then:</p><ul><li>Areas with higher local contrast get more bits to encode differences</li><li>Areas that are all the same color can be more efficiently compressed.</li></ul><p>I know very little about these schemes in general, but I <em>do</em> know what I read in the LibRaw source code. Here‚Äôs approximately how it works:</p><ul><li>Figure out ‚Äúhow much the image is changing in a horizontal direction‚Äù by subtracting two known horizontal neighbours</li><li>Figure out ‚Äúhow much the image is changing in a vertical direction‚Äù by subtracting two known vertical neighbours</li><li>Quantize those two values into one of 81 ‚Äúbuckets‚Äù, which represents the approximate direction and magnitude of change in the image.</li></ul><p>Let‚Äôs illustrate the steps so far with an example:</p><figure><figcaption><p>Choosing a bucket from neighbouring pixel. The bucket at position (-3, +2) is chosen.</p></figcaption></figure><p>Now, each bucket has been sneakily summing up all the <em>Differences From Expected Value</em> as we‚Äôve worked our way across the image, and counting the number of values that have fallen into that bucket. From these two values, we can:</p><ul><li>Compute the <em>Average Difference</em> corresponding to that bucket,</li><li>Use the number of bits that it would take to encode the Average Difference for the ‚Äúdirect binary coded‚Äù part of the output format, and</li><li>Update the Average Difference with the Difference we just encoded.</li></ul><p>The end result of this is that the algorithm is able to ‚Äúlearn‚Äù how many bits it will probably need to encode the difference for various patterns in the photo. All going well, this should mean that very few bits are required to encode areas without a lot of variation, and areas with a lot of variation are encoded maximally efficiently, i.e. using as much direct encoding, and as little of the expensive variable encoding, as possible. ‚ú®</p><h2 id="step-4-ship-the-bits-and-were-done">Step 4: Ship the bits, and we‚Äôre done!</h2><p>Now that we‚Äôve encoded the value, we send it to output. Save it in a file, or send it across the network, it doesn‚Äôt matter! It‚Äôs not our problem anymore üéâ</p><p>Congratulations, you now know the fundamentals behind a very specific photographic compression algorithm.</p><h2 id="how-effective-is-this-algorithm">How effective is this algorithm?</h2><p>Yeah, that‚Äôs a great question! I really don‚Äôt know how to quantify that, but I <em>can</em> say that it consistently offers smaller files than the default compression in the <a href="https://en.wikipedia.org/wiki/Digital_Negative">Digital Negative</a> format (which is why I‚Äôm still using compressed RAF).</p><p>On average, my Fuji X-T2 camera will produce a 50.5 MB uncompressed RAF file, of which 49.7 MB is actual RAW data. The values in this file are 14-bit, padded to 16-bit, so if we simply packed all the bits in, we‚Äôd be down to about 43.5 MB. Of the photos I‚Äôve shot over the last few years, they typically contain between 21 and 27 MB of compressed RAW data, so I guess we‚Äôre seeing a ~35-50% reduction. Not bad, I guess!</p><p>It‚Äôs worth noting briefly that there are <em>lots of other considerations</em> when evaluating a compression algorithm, like compress / inflate performance and ease of implementation. But if I‚Äôm going to talk about that, I‚Äôd rather save it for a future blog post.</p><h2 id="what-about-decompression">What about Decompression?</h2><p>To decompress the image, follow the same steps but in reverse üòâ</p><p>Ok, not quite. I initially set out to <em>only</em> write decompression code, but I personally found it valuable to implement compression first, in order to better understand <em>why</em> some of the code I was writing needed to be there. Compression and Decompression are two sides of the same puzzle, I guess.</p><p>The critical point at which I finally felt like I understood what was going on in this algorithm was when I‚Äôd figured out that the Adaptive Coding / Average Difference / Bucketing Subsystem operates in the same direction for both compression and decompression. Think of it like a checksum or a hash: you only ever calculate them in one direction, regardless of whether you‚Äôre reading or writing the data.</p><h2 id="was-all-of-this-a-good-use-of-time">Was all of this a good use of time?</h2><p>This project took me around 4 weeks, all told:</p><ul><li>About a week to figure out what the hell was going on</li><li>About two weeks writing my own implementation</li><li>About a week writing this blog post<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>.</li></ul><figure><img srcset="/posts/fuji-raf-compression-algorithm/the-abyss_hu_86615111d0de9bed.jpg,
/posts/fuji-raf-compression-algorithm/the-abyss_hu_f736748015cb406b.jpg 2x" src="https://capnfabs.net/posts/fuji-raf-compression-algorithm/the-abyss_hu_86615111d0de9bed.jpg" alt="‚ÄúAbout a week to figure out what the hell was going on‚Äù. These are just the notes I kept üòê"/><figcaption><p>‚ÄúAbout a week to figure out what the hell was going on‚Äù. These are just the notes I kept üòê</p></figcaption></figure><p>I <em>think</em> it was worth it.</p><p>There‚Äôs a couple of reasons why I‚Äôm still excited about this work:</p><ul><li>Some team of engineers at Fuji spent months designing a compression algorithm, thinking through lots of fiddly, complex pieces along the way. There‚Äôs something really nice about taking the care and time to observe all the little decisions that were made, and to wonder about what circumstances led to the specifics. I‚Äôve also learned <em>so many cool little tricks</em> from this project.</li><li>At the flea market in Berlin, you can buy other people‚Äôs old reels of film negative. It‚Äôs kinda weird and creepy, but I really like the idea of history being available if you look hard enough for it, and it‚Äôd be a real shame for every digital photo compressed with this algorithm to turn to noise one day. If you‚Äôre a future Digital Archeologist reading through this page, hi! I hope you found this helpful<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>.</li></ul><h2 id="resources">Resources</h2><p>Hopefully you learned something from this piece! If you‚Äôre after more detail, then please accept my sympathies, and these links:</p><ul><li>The <a href="https://github.com/LibRaw/LibRaw/">libraw</a> source, which has a C++ implementation of the decompression algorithm, and is how I figured all of this out.</li><li>My own Rust implementation in the <a href="https://github.com/capnfabs/blitz">capnfabs/blitz</a> repo.</li></ul><p>If you have questions or feedback, or want to know when I publish more stuff about how digital photography works, feel free to <a href="https://capnfabs.net/contact/">get in touch</a>.</p></div></div>
  </body>
</html>
