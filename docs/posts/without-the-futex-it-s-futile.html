<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://h4x0r.org/futex/">Original</a>
    <h1>Without the futex, it&#39;s futile</h1>
    
    <div id="readability-page-1" class="page"><div><main><article><section><div><p><a href="https://eatonphil.com/">Phil Eaton‚Äôs</a> <a href="https://eatonphil.com/2025-art-of-multiprocessor-programming.html">book club</a> is starting
<u>The Art of Multiprocessor Programming, 2nd Edition</u>
, which is a <em>very</em> well regarded textbook, and pretty recently updated (2021). I‚Äôve even heard of a couple of authors.</p><p>I‚Äôve done a lot of concurrent programming, and have always felt like I‚Äôve still got plenty to learn, so I was excited for the topic. So far, what I‚Äôve learned is that I would never recommend this book, despite any merits.</p><p>Academia certainly struggles to find the right balance between teaching foundational principles and practical information. Being this book is explicitly targeting fourth-year undergraduates and grad students, it should definitely cover the fundamentals, right?</p><p>So how the heck could it not cover the <em>futex</em>??</p><p>The name sure sounds like ‚Äúmutex‚Äù, and that <strong>is</strong> where the name comes from: ‚Äúfast, user space mutex‚Äù. But, it isn‚Äôt really, it‚Äôs a building block for concurrency primitives that ushered in a modern world of concurrent performance that makes System V (sysv) feel so old and busted, it feels wrong to even call it a dinosaur ü¶ñ, as if it were once mighty?</p><p>Way back in the last millennium, locking primitives were pretty much all based on the System V IPC (inter-process communication) code, specifically their semaphore code. All common concurrency primitives were over-complicated under the hood, and just didn‚Äôt scale well to large numbers of threads.</p><p>Until Linux added the futex.</p><p>Going back to the <a href="https://www.kernel.org/doc/ols/2002/ols2002-pages-479-495.pdf">original futuex paper</a> in 2002, it was immediately clear that the futex was a huge improvement in highly concurrent environments. Just in that original paper, their tests with 1000 parallel tasks ran <em>20-120 times faster</em> than sysv locks..ü§Ø</p><p>Needless to say, other common operating systems followed suit, including Windows in 2012 and macOS by 2016.</p><p>These days, any <strong>good</strong> locking primitive is going to be based on a futex. You should expect system libraries like <code>pthreads</code> will use the futex extensively.</p><p>When there‚Äôs too much contention for a lock, actively trying to acquire it as fast as possible will just eat CPU that other threads could be using for work, to just sit there and wait.</p><p>It‚Äôd be nice to be able to clear up that CPU and put the thread to sleep until the lock is ready.</p><p>Unless you‚Äôre essentially going to build your own thread scheduling implementation in user-space (which is basically what async implementations do at their core), you either need to poll, which is inefficient, or you need operating system support.</p><p>In sysv IPC, the core tool for getting OS-supported blocking when building higher-level concurrency program was the <em>semaphore</em>, which <strong>intertwined locking and waiting</strong>.</p><p>The <em>futex</em> essentially separates the <em>locking</em> from <em>waiting (and waking)</em> tasks.</p><p>The flexibility you get from separating those two concerns is key to good lock performance. It becomes much easier to avoid unnecessary delays (like sleeps with exponential backoffs) and bottlenecks, particularly <strong>system calls themselves</strong>, which are quite expensive compared to most of the code involved in locking.</p><p>For instance, if you‚Äôre building a mutex, your unlock operation can skip calling into the kernel for the unlock operation if you can be confident in userland that no threads are waiting. In sysv land, an unlock was always a system call.</p><p>Essentially, the futex <code>wait()</code> call allows a task to block, queuing that task inside the kernel, on a list specifically associated with a particular memory address, and allows an optional timeout.</p><p>The <code>wake()</code> operation will dequeue threads from the internal list, running them again. You can choose how many to wake, but generally, the code will wake either 1 or all of them, never anything in between.</p><p>People often describe the futex as, <strong>‚ÄúWait on memory address‚Äù</strong>. That overlooks the notification side, but it‚Äôs a much more apt name, and why Windows‚Äô name for this API (<code>WaitOnAddress</code>) is superior API naming (to be fair, they did have a decade to think about the name).</p><p>The memory address you‚Äôre waiting on tends to be a 32-bit integer, sometimes a 64-bit integer. The OS doesn‚Äôt care much about the semantics of the value inside that integer. However, and very importantly, when calling the <code>wait()</code> operation, the caller <em>must</em> present what it thinks the value of the futex is. If the value has changed, the operation fails.</p><p>Providing the value protects from waiting for a wake that has already happened. Whenever you need to wait, you‚Äôre waiting for some particular state to occur. The OS doesn‚Äôt care about the specifics of how your state is encoded in the futex, since it‚Äôs responsible for the order of operations on the futex; it won‚Äôt enqueue a waiter with an incorrect view of the current state.</p><p>For example, let‚Äôs say you‚Äôre a thread who wants to wait for state <code>Y</code> (which might be availability of a lock), and you have checked, and you think the state is <code>X</code> (say‚Ä¶ locked). In the time from when you checked to the time of the wait, the state could actually be <code>Y</code>. And, if you‚Äôre using a more complicated concurrency primitive like a reader-writer lock, the state could have changed to <code>Z</code>.</p><p>The OS doesn‚Äôt need to care; it just knows when a task needs to take a long, hard look at its decisions. üëÄü™û</p><p>We‚Äôve established it‚Äôs important, so let‚Äôs supplement the book with some of the content it should have had.</p><p>Being a low-level primitive, futexes are certainly hard to get right. The OS will make sure all operations on it from its perspective are well ordered, but when you modify state, not only to do you have to have confidence in your algorithm, you have to worry about the compiler or hardware performing relevent operations either out-of-order or in an overlapping way.</p><p>Still, it‚Äôs not <strong>too</strong> hard to build a basic mutex on top of a futex, and it‚Äôs a good exercise to start to show how we can often avoid unnecessary system calls.</p><p>Let‚Äôs start by building ourselves a little wrapper for <code>futex</code> that gives us access to the core functionality, and works across Linux and Mac. Other OSes are cool and all, yet still left as an exercise to the reader.</p><p>Our futex will be a 32-bit integer (I believe this is the only size on Linux still, so it‚Äôs the most portable). But when we use the thing for state management, we will want to play life safe and ensure that any work we do on the contents are guaranteed to be <em>sequentially consistent</em>, where there‚Äôs a linear order to the operations, and no thread would have seen anything inconsistent with that order.</p><table><tbody><tr><td>üèÉüèø‚Äç‚ôÇÔ∏è</td><td>While many algorithms can work with much weaker consistency guarantees, we will generally avoid memory ordering optimization because it&#39;s so error prone, and often doesn&#39;t really make a significant difference for real-world programs. Just say no to premature optimization.</td></tr></tbody></table><p>To make it easier to get the full sequentially consistent experience, we will explicitly declare our futex to be atomic, even though the APIs we call probably will not explicitly declare them as such in formal parameter declarations. We‚Äôll generally use an explicit atomic call for operations, but declaring variables atomic ensures that, when you don‚Äôt explicitly use an atomic operation to access, you get one anyway.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#include</span> <span>&lt;stdint.h&gt; // for uint32_t</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;stdatomic.h&gt;</span><span>
</span></span></span><span><span><span></span><span>typedef</span> <span>_Atomic</span>(<span>uint32_t</span>) <span>h4x0r_futex_t</span>;
</span></span></code></pre></div><p>On Linux, the main system call for the futex is a Swiss Army knife with many commands and options, both. Some of the features of the API turned out to be bad ideas, but endure because nobody wants to break code dependent on them. While the futex isn‚Äôt too hard in concept, the complexity of the Linux API is pretty staggering.</p><p>We‚Äôll skip over all the complexity and just write a wrapper for the basic functionality under the assumption that you‚Äôre going to use futexes to build stuff within the context of a single process with threads (e.g., we won‚Äôt worry about locks shared between processes in this article).</p><p>Here‚Äôs the core functionality we need:</p><ol><li>The calling thread will wait <em>if the state is correct</em>.</li><li>The calling thread can specify an optional timeout.</li><li>We get back a result indicating success (<code>0</code>) or error (depends on why we didn‚Äôt wait, but both the race condition and timeout are valid reasons, as are interrupts).</li></ol><table><tbody><tr><td>üßµüò¥</td><td>Being explicit, if the return value is <code>0</code>, that means your thread slept, and welcome back from dreamland!</td></tr></tbody></table><p>In Linux, glibc doesn‚Äôt wrap the <code>futex</code> system call, so we‚Äôll have to use their generic system call wrapper, <code>syscall</code>:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#include</span> <span>&lt;time.h&gt; // For struct timespec.</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>#if defined(__linux__)
</span></span></span><span><span><span>#include</span> <span>&lt;linux/futex.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;sys/syscall.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;unistd.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;stdint.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>static</span> <span>inline</span> <span>int</span>
</span></span><span><span><span>h4x0r_futex_wait_timespec</span>(<span>h4x0r_futex_t</span>  <span>*</span>futex,
</span></span><span><span>                         <span>uint32_t</span>         expected,
</span></span><span><span>                         <span>struct</span> timespec <span>*</span>timeout_ptr)
</span></span><span><span>{
</span></span><span><span>    <span>int</span> err <span>=</span> <span>syscall</span>(SYS_futex,
</span></span><span><span>                      futex,
</span></span><span><span>                      FUTEX_WAIT_PRIVATE,
</span></span><span><span>                      expected,
</span></span><span><span>                      timeout_ptr,
</span></span><span><span>                      NULL,
</span></span><span><span>                      <span>0</span>);
</span></span><span><span>    <span>if</span> (err <span>==</span> <span>-</span><span>1</span>) {
</span></span><span><span>        <span>return</span> errno;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>0</span>;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>#endif
</span></span></span></code></pre></div><p>The <code>syscall()</code> wrapper doesn‚Äôt return the full error, just <code>-1</code>, which is why we have to check <code>errno</code>.</p><p>There, we return the error code (if any) from our wrapper, even though the system call returns -1 on an error, and then passes the specific error via <code>errno</code>.</p><p>Waking a thread up involves calling the same system call, but we just pass the corresponding wake operation:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#include</span> <span>&lt;stdbool.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>#if defined(__linux__)
</span></span></span><span><span><span></span><span>static</span> <span>inline</span> <span>int</span>
</span></span><span><span><span>h4x0r_futex_wake</span>(<span>h4x0r_futex_t</span> <span>*</span>futex, <span>bool</span> all)
</span></span><span><span>{
</span></span><span><span>    <span>uint32_t</span> n <span>=</span> all <span>?</span> INT_MAX : <span>1</span>;
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>syscall</span>(SYS_futex,
</span></span><span><span>                   futex,
</span></span><span><span>                   FUTEX_WAKE_PRIVATE,
</span></span><span><span>                   n,
</span></span><span><span>                   NULL,
</span></span><span><span>                   NULL,
</span></span><span><span>                   <span>0</span>);
</span></span><span><span>}
</span></span><span><span><span>#endif
</span></span></span></code></pre></div><p>When waking, we <strong>do not</strong> pass an expected value for the futex (it‚Äôs irrelevant), nor do we pass a timeout (this operation can‚Äôt block), and the error gets returned directly.</p><p>Instead, we do need to pass the number of threads to wake, but in our API, we‚Äôve simplified that down to a flag to toggle between one waiter and all waiters.</p><p>My MacOS wrapper apparently is a bit out of date; I use the somewhat undocumented <code>__ulock</code> interface (its documentation is just some comments in the Darwin source code), but TIL that they added a new, simpler interface, just last year (<code>os_sync_wait_on_address</code>). Still, for now, we‚Äôll go old school for better compatibility and build the same two operations:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#if defined(__APPLE__)
</span></span></span><span><span><span>#include</span> <span>&lt;stdint.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>// These were never exposed in any headers.
</span></span></span><span><span><span></span><span>extern</span> <span>int</span> <span>__ulock_wait2</span>(<span>uint32_t</span>, <span>void</span> <span>*</span>, <span>uint64_t</span>, <span>uint64_t</span>, <span>uint64_t</span>);
</span></span><span><span><span>extern</span> <span>int</span> <span>__ulock_wake</span>(<span>uint32_t</span>, <span>void</span> <span>*</span>, <span>uint64_t</span>);
</span></span><span><span>
</span></span><span><span><span>#define H4X0R_NSEC_PER_SEC                   1000000000
</span></span></span><span><span><span>#define H4X0R_LOCK_COMPARE_AND_WAIT          1
</span></span></span><span><span><span>#define H4X0R_LOCK_WAKE_ALL                  0x00000100
</span></span></span><span><span><span>#define H4X0R_LOCK_WAKE_THREAD               0x00000200
</span></span></span><span><span><span></span>
</span></span><span><span><span>#define H4X0R_WAKE_ALL    (H4X0R_LOCK_COMPARE_AND_WAIT | H4X0R_LOCK_WAKE_ALL)
</span></span></span><span><span><span>#define H4X0R_WAKE_THREAD (H4X0R_LOCK_COMPARE_AND_WAIT | H4X0R_LOCK_WAKE_THREAD)
</span></span></span><span><span><span></span>
</span></span><span><span><span>static</span> <span>inline</span> <span>int</span>
</span></span><span><span><span>h4x0r_futex_wait_timespec</span>(<span>h4x0r_futex_t</span>   <span>*</span>futex,
</span></span><span><span>                         <span>uint32_t</span>         expected,
</span></span><span><span>                         <span>struct</span> timespec <span>*</span>timeout)
</span></span><span><span>{
</span></span><span><span>    <span>uint64_t</span> timeout_ns <span>=</span> <span>0</span>;
</span></span><span><span>
</span></span><span><span>    <span>if</span> (timeout) {
</span></span><span><span>        timeout_ns <span>=</span> timeout<span>-&gt;</span>tv_nsec <span>+</span> timeout<span>-&gt;</span>tv_sec <span>*</span> H4X0R_NSEC_PER_SEC;
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> <span>__ulock_wait2</span>(H4X0R_LOCK_COMPARE_AND_WAIT,
</span></span><span><span>                         futex,
</span></span><span><span>                         (<span>uint64_t</span>)expected,
</span></span><span><span>                         timeout_ns,
</span></span><span><span>                         <span>0</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>int</span>
</span></span><span><span><span>h4x0r_futex_wake</span>(<span>h4x0r_futex_t</span> <span>*</span>futex, <span>bool</span> all)
</span></span><span><span>{
</span></span><span><span>    <span>return</span> <span>__ulock_wake</span>(all <span>?</span> H4X0R_WAKE_ALL : H4X0R_WAKE_THREAD,
</span></span><span><span>                        futex,
</span></span><span><span>                        <span>0ULL</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>#endif
</span></span></span></code></pre></div><p>The book doesn‚Äôt really cover the mutex well, focusing more on the term ‚Äúmutual exclusion‚Äù, which gets its own chapter early days, and is thrown around liberally in a chapter about spin locks. Sure, locks don‚Äôt need to involve waiting, and the core of a mutex doesn‚Äôt require waiting. So a spin lock is definitely a form of mutex. The book does show spin locks, and they‚Äôre not hard to build:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#include</span> <span>&lt;stdatomic.h&gt;</span><span>
</span></span></span><span><span><span>#include</span> <span>&lt;stdint.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>typedef</span> _Atomic <span>uint32_t</span> <span>h4x0r_spin_lock_t</span>;
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_spin_lock_init</span>(<span>h4x0r_spin_lock_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>atomic_store</span>(lock, <span>0</span>);        
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_spin_lock_acquire</span>(<span>h4x0r_spin_lock_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>while</span> (<span>atomic_fetch_or</span>(lock, <span>1</span>)) <span>/* do nothing */</span> ;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_spin_lock_release</span>(<span>h4x0r_spin_lock_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>   <span>atomic_store</span>(lock, <span>0</span>);
</span></span><span><span>}
</span></span></code></pre></div><p>It might not be intuitive why the above lock works. The <code>atomic_fetch_or()</code> operation performs a bit-wise OR, but returns the value from before the operation began. As implied by the ‚Äúfunction‚Äù name, the entire operation happens atomically, and in reality, won‚Äôt be a proper function; it‚Äôll most likely inline to a tiny bit of assembly.</p><p>Let‚Äôs say 100 threads are contending for the lock, and let‚Äôs just imagine no thread gives up the lock, either. We use the version of this API that guarantees sequential consistency. As a result, at runtime, all threads will essentially perform the OR, but only one will see <code>0</code> as a return value. That‚Äôs the thread that gets the lock.</p><p>Many people are surprised that any lock can be built with only single bit, but there you go.</p><p>Still, our simple spin lock has some potential problems:</p><ol><li>It doesn‚Äôt deal with heavy contention (i.e., it doesn‚Äôt provide a way to offload CPU via waiting).</li><li>If a thread mistakenly calls our <code>unlock</code> operation on a lock it doesn‚Äôt own, it‚Äôll unlock the lock üò±</li><li>If a thread recursively tries to acquire this lock, it‚Äôll end up blocked, waiting for a lock it already holds‚Äì deadlock!</li></ol><p>Some people wouldn‚Äôt consider the last one a real problem, as we‚Äôll discuss later. But the other two are definitely worth addressing.</p><p>Anyway, let‚Äôs show how we can deal with all of these issues.</p><h2 id="when-and-how-to-block">When and how to block</h2><p>Most libraries will have separate APIs for spin locks and mutexes.</p><p>Yet, good mutex implementations <em>do</em> tend to start with a spin lock, and if they try some number of times and fail, then there‚Äôs too much contention, so they wait.</p><p>If the lock is uncontended, the spin lock will be successful on the first try, and will be pretty fast ‚Äì no system call. With light contention and a short critical section, we may still get the lock without having to make the system call to wait.</p><p>One thing I wanted to see from the book was guidance on how long mutexes should spin for. Surely, there‚Äôs not a one-size-fits-all answer, especially as hardware platforms evolve. Clearly, the overhead of a system call, the number of tasks, and the duration of the critical section could all come into play, but does anyone have any metrics here?</p><p>Personally, I just tend to use a hard-coded <code>16</code> iterations, but with no strong reason. I suppose I probably saw someone else use it once, but also with no explanation.</p><p>When it comes to dealing with contention, the book covers exponential back-off ‚Äì when you don‚Äôt get the lock due to contention, you exponentially increase the time you wait, in an attempt to try to spread out contention (usually, exponential backup stops at some maximum duration, which usually comes after 5-8 failed attempts).</p><p>But what do they tell you to do?? Call <code>sleep()</code> for the backoff period. What if contention clears right as you put yourself to sleep? You wait anyway, and while perhaps you had the opportunity to take the lock uncontested had you kept spinning, and you might get really unlucky, and the contention could come back right as you‚Äôre waking up.</p><p>By the way, blocking is <strong>so</strong> overlooked in this book; they don‚Äôt even <em>mention</em> the word polling (I bought the e-book and searched extensively). This issue with using <code>sleep()</code> to block certainly isn‚Äôt mentioned, even when it‚Äôs casually tossed into their code.</p><p>You probably have an inkling already‚Äì the futex is how modern locks avoid these problems.</p><p>If a futex is available, exponential backoff (or any polling-based approach) makes little sense. With polling, wakes are essentially arbitrary guesses. With the futex, wakes are explicitly tied to the mutex becoming available.</p><p>When there‚Äôs significant contention with a polling-based approach, if we don‚Äôt use exponential back-off, we could end up with many threads continuing to wake up all at the same time, just to contend with each other again.</p><p>With a futex, we can just wake up a single thread (which might have to contend with new threads coming in).</p><p>Let‚Äôs build it, even though we still won‚Äôt be dealing with recursion or accidental unlocks.</p><p>Given the risk of accidental unlock, we‚Äôll call this our ‚Äúunsafe‚Äù mutex.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#define H4X0R_SPIN_COUNT 16
</span></span></span><span><span><span></span>
</span></span><span><span><span>typedef</span> <span>h4x0r_futex_t</span> <span>h4x0r_mutex_unsafe_t</span>;
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_unsafe_init</span>(<span>h4x0r_mutex_unsafe_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>atomic_store</span>(lock, <span>0</span>);        
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_unsafe_acquire</span>(<span>h4x0r_mutex_unsafe_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>for</span> (<span>uint32_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> H4X0R_SPIN_COUNT; i<span>++</span>) {
</span></span><span><span>        <span>if</span> (<span>!</span><span>atomic_fetch_or</span>(lock, <span>1</span>)) {
</span></span><span><span>            <span>return</span>;
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>    <span>while</span> (true) {
</span></span><span><span>        <span>h4x0r_futex_wait_timespec</span>((<span>h4x0r_futex_t</span> <span>*</span>)lock, <span>1</span>, NULL);
</span></span><span><span>        <span>if</span> (<span>!</span><span>atomic_fetch_or</span>(lock, <span>1</span>)) {
</span></span><span><span>            <span>return</span>;
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_unsafe_release</span>(<span>h4x0r_mutex_unsafe_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>   <span>atomic_store</span>(lock, <span>0</span>);
</span></span><span><span>   <span>h4x0r_futex_wake</span>(lock, false);
</span></span><span><span>}
</span></span></code></pre></div><p>Here, we spin for a bit, and if things aren‚Äôt contested, then we wait on the futex. Whoever has the lock will notify up to one thread on wake. For our simple use case, it doesn‚Äôt really matter why we fail if we don‚Äôt wait on the futex, so we just loop.</p><table><tbody><tr><td>‚ÄºÔ∏è</td><td>You are generally not guaranteed that threads will be awoken in any specific order. Still, generally, that&#39;s probably not worth worrying about.</td></tr></tbody></table><h2 id="minimizing-system-calls">Minimizing system calls</h2><p>The futex can help us avoid a bunch of unnecessary wake-ups, and thus a lot of system calls. However, in many cases, mutex access will be totally uncontested, and we‚Äôll be making a system call to wake up‚Ä¶ nobody.</p><p>There are ways to address this pretty simply, allowing us to avoid <em>most</em> spurious wakeups. We‚Äôre only using one bit of our futex, and it doesn‚Äôt actually matter which bit. So we‚Äôre going to move the bit up to the top, and use the rest as a wait-counter.</p><p>We‚Äôll have threads add to it before they first try to go to sleep, and subtract when they wake up. When the thread holding the lock unlocks the mutex, we‚Äôll use <code>atomic_fetch_and()</code> to remove the mutex‚Äôs <strong>lock</strong> flag, but leave all other bits intact, and we‚Äôll then look to see if the wait-count is zero. If it is, we‚Äôll skip the wake-up.</p><p>This can still lead to <em>some</em> extra system calls:</p><ol><li><p>The mutex could get released after the thread added its value to wait count, but before it actually puts itself to sleep.</p></li><li><p>Other threads may end up adding to the count and going to sleep once we‚Äôve released, and after another thread grabbed the lock.</p></li></ol><p>The first case seems scary; it won‚Äôt risk a deadlock, because if the unlock happens before the wait, the waiting thread will have the wrong expected value, and the futex call will fail.</p><p>The second case leads to a spurious wake-up. The woken thread checks the futex again, sees it‚Äôs locked, and goes back to sleep, but that‚Äôs two extra system calls.</p><p>Still, this is going to avoid plenty of unnecessary wakes and system calls in practice.</p><p>Using this idea, let‚Äôs build our second mutex. This version is also unsafe, because we‚Äôre not yet dealing with ownership, so we‚Äôll modify calls with <code>unsafe2</code>. But we‚Äôll reuse the <code>h4x0r_mutex_unsafe_t</code> type, and calls that do not change from the previous implementation.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// We&#39;ll re-use these constants in our last mutex.
</span></span></span><span><span><span></span><span>#define H4X0R_MUTEX_LOCK_ON (1 &lt;&lt; 31)
</span></span></span><span><span><span>#define H4X0R_MUTEX_LOCK_OFF ~(H4X0R_MUTEX_LOCK_ON)
</span></span></span><span><span><span></span>
</span></span><span><span><span>// This function will get reused too.
</span></span></span><span><span><span></span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_value_is_unlocked</span>(<span>uint32_t</span> value)
</span></span><span><span>{
</span></span><span><span>    <span>return</span> <span>!</span>(value <span>&amp;</span> H4X0R_MUTEX_LOCK_ON);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_unsafe2_try_lock</span>(<span>h4x0r_mutex_unsafe_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>uint32_t</span> value <span>=</span> <span>atomic_fetch_or</span>(lock, H4X0R_MUTEX_LOCK_ON);
</span></span><span><span>    <span>return</span> <span>h4x0r_mutex_value_is_unlocked</span>(value);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>uint32_t</span>
</span></span><span><span><span>h4x0r_mutex_unsafe2_add_waiter</span>(<span>h4x0r_mutex_unsafe_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>return</span> <span>1</span> <span>+</span> <span>atomic_fetch_add</span>(lock, <span>1</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_unsafe2_acquire</span>(<span>h4x0r_mutex_unsafe_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>uint32_t</span> expected;
</span></span><span><span>
</span></span><span><span>    <span>for</span> (<span>uint32_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> H4X0R_SPIN_COUNT; i<span>++</span>) {
</span></span><span><span>        <span>if</span> (<span>h4x0r_mutex_unsafe2_try_lock</span>(lock)) {
</span></span><span><span>            <span>return</span>;
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    expected <span>=</span> <span>h4x0r_mutex_unsafe2_add_waiter</span>(lock);
</span></span><span><span>
</span></span><span><span>    <span>while</span> (true) {
</span></span><span><span>        <span>if</span> (<span>h4x0r_mutex_value_is_unlocked</span>(expected)
</span></span><span><span>	    <span>&amp;&amp;</span> <span>h4x0r_mutex_unsafe2_try_lock</span>(lock)) {
</span></span><span><span>            <span>atomic_fetch_add</span>(lock, <span>-</span><span>1</span>);
</span></span><span><span>            <span>return</span>;
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>h4x0r_futex_wait_timespec</span>((<span>h4x0r_futex_t</span> <span>*</span>)lock, expected, NULL);
</span></span><span><span>        expected <span>=</span> <span>atomic_load</span>(lock);
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_unsafe2_release</span>(<span>h4x0r_mutex_unsafe_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>   <span>uint32_t</span> waiters <span>=</span> <span>atomic_fetch_and</span>(lock, H4X0R_MUTEX_LOCK_OFF);
</span></span><span><span>
</span></span><span><span>   <span>if</span> (waiters <span>!=</span> H4X0R_MUTEX_LOCK_ON) {
</span></span><span><span>       <span>h4x0r_futex_wake</span>(lock, false);
</span></span><span><span>   }
</span></span><span><span>}
</span></span></code></pre></div><p>We broke out the lock test and the attempt to lock into separate inline functions for clarity.</p><p>Also, we did the same for the function that registers ourselves as a waiter, as it‚Äôs a little more complicated than just atomically bumping the wait count. Much like <code>atomic_fetch_or()</code>, the function <code>atomic_fetch_add()</code> will return the value there <strong>BEFORE</strong> our modification.</p><p>But, we‚Äôre going to have to tell the futex routine what value we think is there when we go to wait, so we <em>also</em> need to add to the wait count on the value that gets returned.</p><p>Notice that, when we do add to the wait count, it doesn‚Äôt matter how many times we wake from the futex; we only add ourselves one time, and remove ourselves only once we <em>acquire</em> the lock. We definitely don‚Äôt want ourselves to be counted when we test to see if we should make the system call to wake a waiter.</p><p>Here too, <code>atomic_fetch_and()</code> will return a value before our operation is applied. So in this mutex, if there are no waiters at the point of our atomic operation, the value we get back will actually be <code>H4X0R_MUTEX1_LOCK_ON</code>, even though we will have just set the mutex‚Äôs value to <code>0</code>.</p><p>Here, we don‚Äôt redo the operation; we just make the proper comparison.</p><h2 id="asserting-ownership">Asserting Ownership</h2><p>Dealing with ownership isn‚Äôt too big a deal; we just need to keep track of who owns the lock, if anyone, and check it when it‚Äôs safe to do so.</p><p>We‚Äôll use <code>pthread_t</code> for identity, which we can directly store and compare, even though it has a slight issue‚Äì the underlying implementation of the data structure is implementation-dependent.</p><p>This means we can‚Äôt 100% reliably keep track of the thread with just the <code>pthread_t</code> value, because we don‚Äôt know a portable value that says ‚Äúnot a thread.‚Äù The easiest thing for us to do is just take up some more space, and keep a flag to keep track of whether it‚Äôs owned or not, for when we are checking ownership.</p><p>Once we check ownership, our mutex is safe, so this will be our default mutex type (we‚Äôll modify this code to make a recursive mutex later).</p><p>Here, then, is our new mutex definition and initializer:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>struct</span> {
</span></span><span><span>    <span>h4x0r_futex_t</span>      futex;
</span></span><span><span>    <span>_Atomic</span>(<span>pthread_t</span>) owner;
</span></span><span><span>    <span>_Atomic</span>(<span>bool</span>)      owned;
</span></span><span><span>} <span>h4x0r_mutex_t</span>;
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_init</span>(<span>h4x0r_mutex_t</span> <span>*</span>mutex)
</span></span><span><span>{
</span></span><span><span>    <span>*</span>mutex <span>=</span> (<span>h4x0r_mutex_t</span>) {
</span></span><span><span>	.futex <span>=</span> <span>0</span>,
</span></span><span><span>	.owned <span>=</span> false,
</span></span><span><span>    };
</span></span><span><span>}
</span></span></code></pre></div><p>We‚Äôve declared all of the fields atomic. Yes, we‚Äôre only going to change them when we own the mutex, but we can‚Äôt always count on sequential consistency on the fields if they aren‚Äôt atomic, even if they‚Äôre in the same struct as threads that are. There are platforms (like ARM) where you‚Äôre particularly at risk of issues if you‚Äôre not really careful. When in doubt, go for full sequential consistency.</p><p>The atomics do force sequential consistency when we update those fields. Without it, we might end up with, for instance, thread A yielding the futex, but their ownership flag still being set when thread B quickly grabs the lock.</p><p>In the rest of this implementation, we‚Äôre going to take the same basic approach we did with the previous mutex, with the significant changes (besides moving from a uint32_t only to a data structure) being:</p><ol><li>When we unlock a mutex, we‚Äôll double-check that we own it first. If we do <strong>NOT</strong>, that‚Äôs a fatal error, and we‚Äôll print a message and abort.</li><li>We‚Äôll check for ownership <em>before</em> we acquire a lock.</li><li>With this check, if there‚Äôs an owner, and we‚Äôre <em>not</em> it, that‚Äôs also a fatal error.</li></ol><p>We‚Äôll also add a timeout field to our lock, which we‚Äôll pass down to the futex if we block (the time we spend spinning will be irrelevant).</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_try_lock</span>(<span>h4x0r_mutex_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>
</span></span><span><span>    <span>uint32_t</span> value <span>=</span> <span>atomic_fetch_or</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, H4X0R_MUTEX_LOCK_ON);
</span></span><span><span>    <span>pthread_t</span> self;
</span></span><span><span>
</span></span><span><span>    <span>if</span> (<span>!</span><span>h4x0r_mutex_value_is_unlocked</span>(value)) {
</span></span><span><span>	<span>return</span> false;
</span></span><span><span>    }
</span></span><span><span>    <span>// If what we read when we wrote says &#34;unlocked&#34;, then we
</span></span></span><span><span><span></span>    <span>// successfully acquired the lock.
</span></span></span><span><span><span></span>    self <span>=</span> <span>pthread_self</span>();
</span></span><span><span>
</span></span><span><span>    <span>if</span> (<span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owned)) {
</span></span><span><span>	<span>if</span> (<span>pthread_equal</span>(self, <span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owner))) {
</span></span><span><span>	    <span>fprintf</span>(stderr, <span>&#34;Mutex was used recursively.</span><span>\n</span><span>&#34;</span>);
</span></span><span><span>	}
</span></span><span><span>	<span>else</span> {
</span></span><span><span>	    <span>fprintf</span>(stderr, <span>&#34;Acquired a lock owner didn&#39;t properly yield.</span><span>\n</span><span>&#34;</span>);
</span></span><span><span>	}
</span></span><span><span>	<span>abort</span>();
</span></span><span><span>    }
</span></span><span><span>	
</span></span><span><span>    <span>// We have the lock, so we make it known.
</span></span></span><span><span><span></span>    <span>atomic_store</span>(<span>&amp;</span>lock<span>-&gt;</span>owned, true);
</span></span><span><span>    <span>atomic_store</span>(<span>&amp;</span>lock<span>-&gt;</span>owner, self);
</span></span><span><span>    
</span></span><span><span>    <span>return</span> true;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>uint32_t</span>
</span></span><span><span><span>h4x0r_mutex_add_waiter</span>(<span>h4x0r_mutex_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>return</span> <span>1</span> <span>+</span> <span>atomic_fetch_add</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, <span>1</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_acquire</span>(<span>h4x0r_mutex_t</span> <span>*</span>lock, <span>struct</span> timespec <span>*</span>timeout)
</span></span><span><span>{
</span></span><span><span>    <span>uint32_t</span>  expected;
</span></span><span><span>
</span></span><span><span>    <span>for</span> (<span>uint32_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> H4X0R_SPIN_COUNT; i<span>++</span>) {
</span></span><span><span>        <span>if</span> (<span>h4x0r_mutex_try_lock</span>(lock)) {
</span></span><span><span>            <span>return</span> true;
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    expected <span>=</span> <span>h4x0r_mutex_add_waiter</span>(lock);
</span></span><span><span>
</span></span><span><span>    <span>while</span> (true) {
</span></span><span><span>        <span>if</span> (<span>h4x0r_mutex_value_is_unlocked</span>(expected) <span>&amp;&amp;</span>
</span></span><span><span>	    <span>h4x0r_mutex_try_lock</span>(lock)) {
</span></span><span><span>            <span>atomic_fetch_add</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, <span>-</span><span>1</span>);
</span></span><span><span>            <span>return</span> true;
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>int</span> err <span>=</span> <span>h4x0r_futex_wait_timespec</span>((<span>h4x0r_futex_t</span> <span>*</span>)<span>&amp;</span>lock<span>-&gt;</span>futex,
</span></span><span><span>					    expected,
</span></span><span><span>					    timeout);
</span></span><span><span>
</span></span><span><span>	<span>if</span> (err <span>==</span> ETIMEDOUT) {
</span></span><span><span>	    <span>return</span> false;
</span></span><span><span>	}
</span></span><span><span>        expected <span>=</span> <span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>futex);
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_release</span>(<span>h4x0r_mutex_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (<span>!</span><span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owned)
</span></span><span><span>        <span>||</span> <span>!</span><span>pthread_equal</span>(<span>pthread_self</span>(), <span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owner))) {
</span></span><span><span>	<span>fprintf</span>(stderr, <span>&#34;Thread unlocked a mutex it doesn&#39;t own.</span><span>\n</span><span>&#34;</span>);
</span></span><span><span>	<span>abort</span>();
</span></span><span><span>    }
</span></span><span><span>        
</span></span><span><span>    <span>atomic_store</span>(<span>&amp;</span>lock<span>-&gt;</span>owned, false);
</span></span><span><span>    
</span></span><span><span>    <span>uint32_t</span> waiters <span>=</span> <span>atomic_fetch_and</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, H4X0R_MUTEX_LOCK_OFF);
</span></span><span><span>
</span></span><span><span>    <span>if</span> (waiters <span>!=</span> H4X0R_MUTEX_LOCK_ON) {
</span></span><span><span>	<span>h4x0r_futex_wake</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, false);
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>We implemented our locking capability on top of <code>h4x0r_mutex_try_lock()</code>, which makes a single attempt to claim a lock. If it succeeds, it then performs its validity checks, and if THOSE succeed, it sets the ownership info.</p><p>When we‚Äôre releasing the lock, we test the boolean to see if there‚Äôs an owner. If that‚Äôs set, we then ensure that we‚Äôre the right owner.</p><p>The <code>try_lock</code> is a common API feature for mutexes, as an alternative to a timeout.</p><p>There‚Äôs a very good question the book didn‚Äôt seem to opine on: whether you should ever be using recursive mutexes at all. For mutexes, I tend to lean towards <strong>no</strong>, as it encourages sloppy programming.</p><p>But, I will admit to being on the fence, and you‚Äôre mature enough to make up your own minds. So let‚Äôs look at what we‚Äôd have to do.</p><p>To make our previous lock recursive, we‚Äôll keep a field that keeps track of the levels of nesting. To avoid deadlocking with ourselves, our lock functions will need to check to ensure they don‚Äôt own the lock before their first lock attempt.</p><p>For that reason, the core <code>try_lock</code> operation is moved to a call marked <code>internal</code>, specifically <code>h4x0r_mutex_recursive_internal_try_lock()</code>. The call <code>h4x0r_mutex_recursive_try_lock()</code> only needs to perform the ownership check, then can make the internal call.</p><p>Here‚Äôs what initialization looks like now:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>struct</span> {
</span></span><span><span>    <span>h4x0r_futex_t</span>      futex;
</span></span><span><span>    <span>_Atomic</span>(<span>uint32_t</span>)  depth;
</span></span><span><span>    <span>_Atomic</span>(<span>pthread_t</span>) owner;
</span></span><span><span>    <span>_Atomic</span>(<span>bool</span>)      owned;
</span></span><span><span>} <span>h4x0r_mutex_recursive_t</span>;
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_recursive_init</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>mutex)
</span></span><span><span>{
</span></span><span><span>    <span>*</span>mutex <span>=</span> (<span>h4x0r_mutex_recursive_t</span>) {
</span></span><span><span>	.futex <span>=</span> <span>0</span>,
</span></span><span><span>	.depth <span>=</span> <span>0</span>,
</span></span><span><span>	.owned <span>=</span> false,
</span></span><span><span>    };
</span></span><span><span>}
</span></span></code></pre></div><p>That‚Äôs no big deal. Since two different lock calls now need to check ownership, we‚Äôll break that out into its own helper.</p><p>We‚Äôre going to need to check ownership before we try to lock the first time. We‚Äôll have our function return <code>true</code> if the calling thread already owns the lock, and false if not.</p><p>So it will return false:</p><ol><li>If the mutex is not owned.</li><li>If it‚Äôs locked, but the current thread doesn‚Äôt own it.</li></ol><p>If it‚Äôs not one of these two cases, then it‚Äôs a recursive call by the owner. This call will bump up the depth counter before returning <code>true</code>; the caller can immediately bail.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_recursive_check_ownership</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock,
</span></span><span><span>				      <span>pthread_t</span>                self)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (<span>!</span><span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owned)) {
</span></span><span><span>	<span>return</span> false;
</span></span><span><span>    }
</span></span><span><span>    <span>if</span> (<span>pthread_equal</span>(self, <span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owner))) {
</span></span><span><span>	<span>atomic_fetch_add</span>(<span>&amp;</span>lock<span>-&gt;</span>depth, <span>1</span>);
</span></span><span><span>	<span>return</span> true;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> false;
</span></span><span><span>}
</span></span></code></pre></div><p>If we didn‚Äôt own the lock, once we acquire the lock, we need to assert our ownership. At this point, we can double-check that the lock is not marked as owned. If it were, that‚Äôd indicate a bug in our implementation, so it could make some sense to skip that check.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// Called from our internal try-lock call, once it knows we definitely
</span></span></span><span><span><span>// just acquired ownership.
</span></span></span><span><span><span></span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_recursive_new_ownership</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock,
</span></span><span><span>				    <span>pthread_t</span>                self)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (<span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owned)) {
</span></span><span><span>	<span>fprintf</span>(stderr, <span>&#34;Acquired a lock owner didn&#39;t properly yield.</span><span>\n</span><span>&#34;</span>);
</span></span><span><span>	<span>abort</span>();
</span></span><span><span>    }
</span></span><span><span>    <span>atomic_store</span>(<span>&amp;</span>lock<span>-&gt;</span>owned, true);
</span></span><span><span>    <span>atomic_store</span>(<span>&amp;</span>lock<span>-&gt;</span>owner, self);
</span></span><span><span>    <span>atomic_store</span>(<span>&amp;</span>lock<span>-&gt;</span>depth, <span>1</span>);
</span></span><span><span>}
</span></span></code></pre></div><p>When the user unlocks a mutex, we will also need to check ownership. We‚Äôll also need to distinguish between the case where we‚Äôre done nesting, and should actually unlock, and when we shouldn‚Äôt:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// Called when definitely holding the lock.
</span></span></span><span><span><span>// Returns true if we are nested, and false if we aren&#39;t (proper unlock).
</span></span></span><span><span><span></span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_recursive_nesting_check</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>
</span></span><span><span>    <span>if</span> (<span>!</span><span>pthread_equal</span>(<span>pthread_self</span>(), <span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>owner))) {
</span></span><span><span>	<span>fprintf</span>(stderr, <span>&#34;Thread is trying to unlock a lock it doesn&#39;t own.</span><span>\n</span><span>&#34;</span>);
</span></span><span><span>	<span>abort</span>();
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>if</span> (<span>atomic_fetch_add</span>(<span>&amp;</span>lock<span>-&gt;</span>depth, <span>-</span><span>1</span>) <span>&gt;</span> <span>1</span>) {
</span></span><span><span>	<span>return</span> true;
</span></span><span><span>    }
</span></span><span><span>	<span>atomic_store</span>(<span>&amp;</span>lock<span>-&gt;</span>owned, false);
</span></span><span><span>	<span>return</span> false;
</span></span><span><span>}
</span></span></code></pre></div><p>The rest of the recursive lock implementation is then straightforward, given our previous locks:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_recursive_internal_try_lock</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock, <span>pthread_t</span> self)
</span></span><span><span>{
</span></span><span><span>    <span>uint32_t</span> value <span>=</span> <span>atomic_fetch_or</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, H4X0R_MUTEX_LOCK_ON);
</span></span><span><span>    <span>bool</span> result    <span>=</span>  <span>h4x0r_mutex_value_is_unlocked</span>(value);
</span></span><span><span>
</span></span><span><span>    <span>if</span> (result) {
</span></span><span><span>	<span>h4x0r_mutex_recursive_new_ownership</span>(lock, self);
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> result;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>uint32_t</span>
</span></span><span><span><span>h4x0r_mutex_recursive_add_waiter</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>return</span> <span>1</span> <span>+</span> <span>atomic_fetch_add</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, <span>1</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_recursive_acquire</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock,
</span></span><span><span>			      <span>struct</span> timespec         <span>*</span>timeout)
</span></span><span><span>{
</span></span><span><span>    <span>uint32_t</span>  expected;
</span></span><span><span>    <span>pthread_t</span> self <span>=</span> <span>pthread_self</span>();
</span></span><span><span>
</span></span><span><span>    <span>if</span> (<span>h4x0r_mutex_recursive_check_ownership</span>(lock, self)) {
</span></span><span><span>	<span>// We already owned the lock, and incremented the nesting count.
</span></span></span><span><span><span></span>	<span>return</span> true;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>for</span> (<span>uint32_t</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> H4X0R_SPIN_COUNT; i<span>++</span>) {
</span></span><span><span>        <span>if</span> (<span>h4x0r_mutex_recursive_internal_try_lock</span>(lock, self)) {
</span></span><span><span>	    <span>// internal_try_lock will set up ownership.
</span></span></span><span><span><span></span>            <span>return</span> true;
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    expected <span>=</span> <span>h4x0r_mutex_recursive_add_waiter</span>(lock);
</span></span><span><span>
</span></span><span><span>    <span>while</span> (true) {
</span></span><span><span>        <span>if</span> (<span>h4x0r_mutex_value_is_unlocked</span>(expected)
</span></span><span><span>            <span>&amp;&amp;</span> <span>h4x0r_mutex_recursive_internal_try_lock</span>(lock, self)) {
</span></span><span><span>            <span>atomic_fetch_add</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, <span>-</span><span>1</span>);
</span></span><span><span>            <span>return</span> true;
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>int</span> err <span>=</span> <span>h4x0r_futex_wait_timespec</span>((<span>h4x0r_futex_t</span> <span>*</span>)<span>&amp;</span>lock<span>-&gt;</span>futex,
</span></span><span><span>					    expected,
</span></span><span><span>					    timeout);
</span></span><span><span>	<span>if</span> (err <span>==</span> ETIMEDOUT) {
</span></span><span><span>	    <span>return</span> false;
</span></span><span><span>	}
</span></span><span><span>        expected <span>=</span> <span>atomic_load</span>(<span>&amp;</span>lock<span>-&gt;</span>futex);
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_mutex_recursive_try_lock</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>pthread_t</span> self <span>=</span> <span>pthread_self</span>();
</span></span><span><span>
</span></span><span><span>    <span>if</span> (<span>h4x0r_mutex_recursive_check_ownership</span>(lock, self)) {
</span></span><span><span>	<span>// We already owned the lock, and incremented the nesting count.
</span></span></span><span><span><span></span>	<span>return</span> true;
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> <span>h4x0r_mutex_recursive_internal_try_lock</span>(lock, self);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_mutex_recursive_release</span>(<span>h4x0r_mutex_recursive_t</span> <span>*</span>lock)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (<span>h4x0r_mutex_recursive_nesting_check</span>(lock)) {
</span></span><span><span>	<span>// We were nested, and the decrement happened, so we&#39;re done.
</span></span></span><span><span><span></span>	<span>return</span>;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>uint32_t</span> waiters <span>=</span> <span>atomic_fetch_and</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, H4X0R_MUTEX_LOCK_OFF);
</span></span><span><span>
</span></span><span><span>    <span>if</span> (waiters <span>!=</span> H4X0R_MUTEX_LOCK_ON) {
</span></span><span><span>	<span>h4x0r_futex_wake</span>(<span>&amp;</span>lock<span>-&gt;</span>futex, false);
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><h2 id="remaining-issues">Remaining issues</h2><p>The core issue we have left is, what happens when a thread exits or dies when a mutex is locked??</p><p>It‚Äôs a bit much for us to cover today, but here‚Äôs a sketch of what we‚Äôd need to do:</p><ol><li>Each thread will need a private list of locks it is actively holding, which we would modify in the calls above.</li><li>In most thread APIs, we can register a callback when a thread exits (or is canceled). We‚Äôd need to register a handler for this, probably once per thread, when it‚Äôs launched.</li><li>That callback will need to go through the list and break any locks still held.</li><li>Many people won‚Äôt worry about crashed threads, as they often will crash the whole program. However, you can catch the signal a crash generates and keep the overall process from terminating. On Linux, you can compare your view of what threads are live with the OS‚Äôs view in <code>proc</code>. On other platforms, you might not be able to get the exact thread that crashed. However, if you‚Äôre managing threads that launch, you will probably have a way to give yourself visibility into what threads do not check in over a very short time.</li></ol><p>Another related issue comes up once you allow mutexes to span processes via memory shared across processes. The same basic code all works fine, modulo some changes to how to use a futex. Our problem is, what happens when a process holding a lock forks??</p><p>That‚Äôs also an issue you can handle if you‚Äôre managing all the locks. But if it‚Äôs a problem mentioned in the book‚Äì I can‚Äôt find it, after skimming over <em>31 uses</em> of the word <code>fork</code> (outside the context of cutleryüç¥, which gets one use). Those uses are spread across a mere <em>9 pages</em>. Almost all of those uses are talking specifically about algorithms leveraging Java‚Äôs <code>ForkJoin</code> class, which is a thread pooling scheme within a process, not a proper posix <code>fork()</code>.</p><h2 id="reading--writing-_the-art--engineering-of-multiprocessor-programming_">Reading / Writing <em>The Art / Engineering of Multiprocessor Programming</em></h2><p>The book being named with the word ‚ÄúArt‚Äù is apt, because it‚Äôs not really engineering best practices.</p><p>To be fair, the book does mention that some locking primitives are lock-aware and some aren‚Äôt. It also shows how to build a recursive lock using a non-recursive lock (in Java, of course).</p><p>But it does so using a condition variable, which is horrible. If you just care about engineering best practices, you should have easy access to a reentrant mutex already (pthreads provides one). I‚Äôd expect that to be more performant than a condition variable, which already needs a mutex‚Äì it over-complicates the implementation.</p><p>If you‚Äôre trying to learn how things work under the hood, the condition variable isn‚Äôt giving you what you need to know. What you need to know is the <code>futex</code>, full stop.</p><p>Also, the book doesn‚Äôt seem to opine on recursive locks. It just shows them. I think the conventional wisdom on using them in mutexes is important to understand. And, I also think it becomes a different discussion if you‚Äôre talking about other concurrency primitives.</p><p>For instance, when using reader-writer locks (RW locks), I think recursive locks are really appropriate, just incredibly hard to get right.</p><p>If you‚Äôve never used an RW lock, they allow any number of readers to lock for reading, all at the same time. If a writer is waiting to lock, it waits for all readers to clear (and in sane implementations, new readers cannot be added with a waiting writer).</p><p>But writers, once they do get the lock, get exclusive access, so they can edit without fear of readers seeing an inconsistent state.</p><p>In concept, that‚Äôs a great primitive to use for operations like dynamic lists. If there‚Äôs no mutation happening, but there could be lots of parallel reading, reads can end up pretty cheap, because they only have to stop for writes.</p><p>Consider an API call like:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>void</span> <span>h4x0r_list_append_all_contents</span>(<span>h4x0r_list_t</span> <span>*</span>dst, <span>h4x0r_list_t</span> <span>*</span>items);
</span></span></code></pre></div><p>The idea being, we‚Äôre going to combine the two lists by copying the contents of the <code>items</code> list onto the end of the <code>dst</code> list.</p><p>Now, we clearly want to grab a write lock on <code>dst</code>. And for <code>items</code>, we definitely want a read lock on that one.</p><p>But, what if we were given that API, and wanted to double a list, by doing:</p><p>I‚Äôve seen APIs like this, where the same item is routinely passed through a nested set of construction operations that can mix reading and mutation. So you might need reads to nest, you might need writes to nest, all for the same item, in the same chain of calls.</p><p>We‚Äôd have no problems building an API that could handle such things in a program without concurrency, as long as we anticipate the need and fix the number of items to copy in before we start.</p><p>But in a multi-threaded app, if we don‚Äôt allow nested locks, we now also have to anticipate <em>every</em> situation where it might make sense for a list to be passed multiple times, and then add code to explicitly test for it.</p><p>So it‚Äôd be great to have solid semantics for RW lock nesting. But unfortunately, the POSIX standard leaves recursive locking as undefined behavior, which means that even if a conformant library provides such recursion, you definitely should not use it.</p><p>And in practice, behavior across common implementations is not remotely consistent. There‚Äôs a good reason why this was left undefined ‚Äì it‚Äôs kind of hard. Since multiple threads can hold a lock, each thread must have a separate read nesting level.</p><p>And then how do you deal with writes that might be intermingled in there? How do you keep the accounting correct, so that you don‚Äôt drop write access too early, for instance?</p><p>These problems are solvable, though (and at some point soon, I‚Äôll share my RW lock).</p><p>But in <em>The Art Of Multiprocessor Programming</em>, I can‚Äôt find any mention of such issues, neither warning you about the semantic challenges that can surprise you, nor showing how such things might be dealt with.</p><p>It‚Äôs not a surprise, since they don‚Äôt even mention the futex. Nor do they seem to cover async runtimes, despite their popularity (perhaps a bit more excusable since they‚Äôre generally multiplexing a single thread, but it still feels too important not to cover well).</p><p>The full source code is available <a href="https://codeberg.org/crashoverride/mutex">on codeberg</a>.</p><p>I have plenty of other problems with this book, but they‚Äôd each probably need their own rant, and I‚Äôm not going to spend the time. But in short, this isn‚Äôt a Computer Science textbook, so much as it is a <em>history book</em>.</p><p>To other CS academics writing textbooks today, whether you‚Äôre focused on theory or practice, please make sure you at least <em>acknowledge</em> the important concepts that were around when you were <strong>writing the book!</strong></p><p>And ideally, make sure you cover some content from the current millennium.</p><p>If the only significant thing indicating the book was written (or edited) recently is the copyright date, consider that you might be doing a disservice with your book.</p><p>kthxbai,</p><p>Lee T, expert waiter üíÅüèª‚Äç‚ôÇÔ∏è (with the finest threads üßµ)</p></div></section></article></main></div></div>
  </body>
</html>
