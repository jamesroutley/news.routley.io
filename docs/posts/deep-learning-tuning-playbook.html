<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/google-research/tuning_playbook">Original</a>
    <h1>Deep Learning Tuning Playbook</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><em>This is not an officially supported Google product.</em></p>
<p dir="auto"><strong>Varun Godbole<sup>†</sup>, George E. Dahl<sup>†</sup>, Justin Gilmer<sup>†</sup>, Christopher J. Shallue<sup>‡</sup>, Zachary Nado<sup>†</sup></strong></p>
<p dir="auto">† Google Research, Brain Team</p>
<p dir="auto">‡ Harvard University</p>
<h2 dir="auto"><a id="user-content-table-of-contents" aria-hidden="true" href="#table-of-contents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Table of Contents</h2>
<ul dir="auto">
<li><a href="#who-is-this-document-for">Who is this document for?</a></li>
<li><a href="#why-a-tuning-playbook">Why a tuning playbook?</a></li>
<li><a href="#guide-for-starting-a-new-project">Guide for starting a new project</a>
<ul dir="auto">
<li><a href="#choosing-a-model-architecture">Choosing the model architecture</a></li>
<li><a href="#choosing-the-optimizer">Choosing the optimizer</a></li>
<li><a href="#choosing-the-batch-size">Choosing the batch size</a></li>
<li><a href="#choosing-the-initial-configuration">Choosing the initial configuration</a></li>
</ul>
</li>
<li><a href="#a-scientific-approach-to-improving-model-performance">A scientific approach to improving model performance</a>
<ul dir="auto">
<li><a href="#the-incremental-tuning-strategy">The incremental tuning strategy</a></li>
<li><a href="#exploration-vs-exploitation">Exploration vs exploitation</a></li>
<li><a href="#choosing-the-goal-for-the-next-round-of-experiments">Choosing the goal for the next round of experiments</a></li>
<li><a href="#Designing-the-next-round-of-experiments">Designing the next round of experiments</a></li>
<li><a href="#Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration">Determining whether to adopt a training pipeline change or
hyperparameter
configuration</a></li>
<li><a href="#After-exploration-concludes">After exploration concludes</a></li>
</ul>
</li>
<li><a href="#Determining-the-number-of-steps-for-each-training-run">Determining the number of steps for each training run</a>
<ul dir="auto">
<li><a href="#Deciding-how-long-to-train-when-training-is-not-compute-bound">Deciding how long to train when training is not compute-bound</a></li>
<li><a href="#Deciding-how-long-to-train-when-training-is-compute-bound">Deciding how long to train when training is compute-bound</a></li>
</ul>
</li>
<li><a href="#Additional-guidance-for-the-training-pipeline">Additional guidance for the training pipeline</a>
<ul dir="auto">
<li><a href="#Optimizing-the-input-pipeline">Optimizing the input pipeline</a></li>
<li><a href="https://github.com/google-research/tuning_playbook/blob/main/Evaluating-model-performance">Evaluating model performance</a></li>
<li><a href="#Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint">Saving checkpoints and retrospectively selecting the best checkpoint</a></li>
<li><a href="#Setting-up-experiment-tracking">Setting up experiment tracking</a></li>
<li><a href="#Batch-normalization-implementation-details">Batch normalization implementation details</a></li>
<li><a href="#Considerations-for-multi-host-pipelines">Considerations for multi-host pipelines</a></li>
</ul>
</li>
<li><a href="#faqs">FAQs</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#citing">Citing</a></li>
<li><a href="#contributing">Contributing</a></li>
</ul>
<h2 dir="auto"><a id="user-content-who-is-this-document-for" aria-hidden="true" href="#who-is-this-document-for"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Who is this document for?</h2>
<p dir="auto">This document is for engineers and researchers (both individuals and teams)
interested in <strong>maximizing the performance of deep learning models</strong>. We assume
basic knowledge of machine learning and deep learning concepts.</p>
<p dir="auto">Our emphasis is on the <strong>process of hyperparameter tuning</strong>. We touch on other
aspects of deep learning training, such as pipeline implementation and
optimization, but our treatment of those aspects is not intended to be complete.</p>
<p dir="auto">We assume the machine learning problem is a supervised learning problem or
something that looks a lot like one (e.g. self-supervised). That said, some of
the prescriptions in this document may also apply to other types of problems.</p>
<h2 dir="auto"><a id="user-content-why-a-tuning-playbook" aria-hidden="true" href="#why-a-tuning-playbook"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why a tuning playbook?</h2>
<p dir="auto">Currently, there is an astonishing amount of toil and guesswork involved in
actually getting deep neural networks to work well in practice. Even worse, the
actual recipes people use to get good results with deep learning are rarely
documented. Papers gloss over the process that led to their final results in
order to present a cleaner story, and machine learning engineers working on
commercial problems rarely have time to take a step back and generalize their
process. Textbooks tend to eschew practical guidance and prioritize fundamental
principles, even if their authors have the necessary experience in applied work
to provide useful advice. When preparing to create this document, we couldn&#39;t
find any comprehensive attempt to actually explain <em>how to get good results with
deep learning</em>. Instead, we found snippets of advice in blog posts and on social
media, tricks peeking out of the appendix of research papers, occasional case
studies about one particular project or pipeline, and a lot of confusion. There
is a vast gulf between the results achieved by deep learning experts and less
skilled practitioners using superficially similar methods. At the same time,
these very experts readily admit some of what they do might not be
well-justified. As deep learning matures and has a larger impact on the world,
the community needs more resources covering useful recipes, including all the
practical details that can be so critical for obtaining good results.</p>
<p dir="auto">We are a team of five researchers and engineers who have worked in deep learning
for many years, some of us since as early as 2006. We have applied deep learning
to problems in everything from speech recognition to astronomy, and learned a
lot along the way. This document grew out of our own experience training neural
networks, teaching new machine learning engineers, and advising our colleagues
on the practice of deep learning. Although it has been gratifying to see deep
learning go from a machine learning approach practiced by a handful of academic
labs to a technology powering products used by billions of people, deep learning
is still in its infancy as an engineering discipline and we hope this document
encourages others to help systematize the field&#39;s experimental protocols.</p>
<p dir="auto">This document came about as we tried to crystalize our own approach to deep
learning and thus it represents the opinions of the authors at the time of
writing, not any sort of objective truth. Our own struggles with hyperparameter
tuning made it a particular focus of our guidance, but we also cover other
important issues we have encountered in our work (or seen go wrong). Our
intention is for this work to be a living document that grows and evolves as our
beliefs change. For example, the material on debugging and mitigating training
failures would not have been possible for us to write two years ago since it is
based on recent results and ongoing investigations. Inevitably, some of our
advice will need to be updated to account for new results and improved
workflows. We do not know the <em>optimal</em> deep learning recipe, but until the
community starts writing down and debating different procedures, we cannot hope
to find it. To that end, we would encourage readers who find issues with our
advice to produce alternative recommendations, along with convincing evidence,
so we can update the playbook. We would also love to see alternative guides and
playbooks that might have different recommendations so we can work towards best
practices as a community. Finally, any sections marked with a <g-emoji alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png">🤖</g-emoji> emoji are places
we would like to do more research. Only after trying to write this playbook did
it become completely clear how many interesting and neglected research questions
can be found in the deep learning practitioner&#39;s workflow.</p>
<h2 dir="auto"><a id="user-content-guide-for-starting-a-new-project" aria-hidden="true" href="#guide-for-starting-a-new-project"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Guide for starting a new project</h2>
<p dir="auto">Many of the decisions we make over the course of tuning can be made once at the
beginning of a project and only occasionally revisited when circumstances
change.</p>
<p dir="auto">Our guidance below makes the following assumptions:</p>
<ul dir="auto">
<li>Enough of the essential work of problem formulation, data cleaning, etc. has
already been done that spending time on the model architecture and training
configuration makes sense.</li>
<li>There is already a pipeline set up that does training and evaluation, and it
is easy to execute training and prediction jobs for various models of
interest.</li>
<li>The appropriate metrics have been selected and implemented. These should be
as representative as possible of what would be measured in the deployed
environment.</li>
</ul>
<h3 dir="auto"><a id="user-content-choosing-the-model-architecture" aria-hidden="true" href="#choosing-the-model-architecture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing the model architecture</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>When starting a new project, try to reuse a model that already
works.</em></p>
<ul dir="auto">
<li>Choose a well established, commonly used model architecture to get working
first. It is always possible to build a custom model later.</li>
<li>Model architectures typically have various hyperparameters that determine
the model&#39;s size and other details (e.g. number of layers, layer width, type
of activation function).
<ul dir="auto">
<li>Thus, choosing the architecture really means choosing a family of
different models (one for each setting of the model hyperparameters).</li>
<li>We will consider the problem of choosing the model hyperparameters in
<a href="#choosing-the-initial-configuration">Choosing the initial configuration</a>
and
<a href="#a-scientific-approach-to-improving-model-performance">A scientific approach to improving model performance</a>.</li>
</ul>
</li>
<li>When possible, try to find a paper that tackles something as close as
possible to the problem at hand and reproduce that model as a starting
point.</li>
</ul>
<h3 dir="auto"><a id="user-content-choosing-the-optimizer" aria-hidden="true" href="#choosing-the-optimizer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing the optimizer</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Start with the most popular optimizer for the type of problem at
hand.</em></p>
<ul dir="auto">
<li>No optimizer is the &#34;best&#34; across all types of machine learning problems and
model architectures. Even just
<a href="https://arxiv.org/abs/1910.05446" rel="nofollow">comparing the performance of optimizers is a difficult task</a>.
<g-emoji alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png">🤖</g-emoji></li>
<li>We recommend sticking with well-established, popular optimizers, especially
when starting a new project.
<ul dir="auto">
<li>Ideally, choose the most popular optimizer used for the same type of
problem.</li>
</ul>
</li>
<li>Be prepared to give attention to <strong>*<strong><strong>all</strong></strong>*</strong> hyperparameters of the
chosen optimizer.
<ul dir="auto">
<li>Optimizers with more hyperparameters may require more tuning effort to
find the best configuration.</li>
<li>This is particularly relevant in the beginning stages of a project when
we are trying to find the best values of various other hyperparameters
(e.g. architecture hyperparameters) while treating optimizer
hyperparameters as
<a href="#identifying-scientific-nuisance-and-fixed-hyperparameters">nuisance parameters</a>.</li>
<li>It may be preferable to start with a simpler optimizer (e.g. SGD with
fixed momentum or Adam with fixed <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9853a2357fd18b27e5cfd5bb3779f74d">$\epsilon$</math-renderer>, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9853a2357fd18b27e5cfd5bb3779f74d">$\beta_{1}$</math-renderer>, and
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9853a2357fd18b27e5cfd5bb3779f74d">$\beta_{2}$</math-renderer>) in the initial stages of the project and switch to a more
general optimizer later.</li>
</ul>
</li>
<li>Well-established optimizers that we like include (but are not limited to):
<ul dir="auto">
<li>
<a href="#what-are-the-update-rules-for-all-the-popular-optimization-algorithms">SGD with momentum</a>
(we like the Nesterov variant)</li>
<li>
<a href="#what-are-the-update-rules-for-all-the-popular-optimization-algorithms">Adam and NAdam</a>,
which are more general than SGD with momentum. Note that Adam has 4
tunable hyperparameters
<a href="https://arxiv.org/abs/1910.05446" rel="nofollow">and they can all matter</a>!
<ul dir="auto">
<li>See
<a href="#how-should-adams-hyperparameters-be-tuned">How should Adam&#39;s hyperparameters be tuned?</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 dir="auto"><a id="user-content-choosing-the-batch-size" aria-hidden="true" href="#choosing-the-batch-size"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing the batch size</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>The batch size governs the training speed and shouldn&#39;t be used
to directly tune the validation set performance. Often, the ideal batch size
will be the largest batch size supported by the available hardware.</em></p>
<ul dir="auto">
<li>The batch size is a key factor in determining the <em>training time</em> and
<em>computing resource consumption</em>.</li>
<li>Increasing the batch size will often reduce the training time. This can be
highly beneficial because it, e.g.:
<ul dir="auto">
<li>Allows hyperparameters to be tuned more thoroughly within a fixed time
interval, potentially resulting in a better final model.</li>
<li>Reduces the latency of the development cycle, allowing new ideas to be
tested more frequently.</li>
</ul>
</li>
<li>Increasing the batch size may either decrease, increase, or not change the
resource consumption.</li>
<li>The batch size should <em>not be</em> treated as a tunable hyperparameter for
validation set performance.
<ul dir="auto">
<li>As long as all hyperparameters are well-tuned (especially the learning
rate and regularization hyperparameters) and the number of training
steps is sufficient, the same final performance should be attainable
using any batch size (see
<a href="https://arxiv.org/abs/1811.03600" rel="nofollow">Shallue et al. 2018</a>).</li>
<li>Please see <a href="#why-shouldnt-the-batch-size-be-tuned-to-directly-improve-validation-set-performance">Why shouldn&#39;t the batch size be tuned to directly improve
validation set
performance?</a></li>
</ul>
</li>
</ul>
<h4 dir="auto"><a id="user-content-determining-the-feasible-batch-sizes-and-estimating-training-throughput" aria-hidden="true" href="#determining-the-feasible-batch-sizes-and-estimating-training-throughput"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Determining the feasible batch sizes and estimating training throughput</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-choosing-the-batch-size-to-minimize-training-time" aria-hidden="true" href="#choosing-the-batch-size-to-minimize-training-time"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing the batch size to minimize training time</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-choosing-the-batch-size-to-minimize-resource-consumption" aria-hidden="true" href="#choosing-the-batch-size-to-minimize-resource-consumption"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing the batch size to minimize resource consumption</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-changing-the-batch-size-requires-re-tuning-most-hyperparameters" aria-hidden="true" href="#changing-the-batch-size-requires-re-tuning-most-hyperparameters"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Changing the batch size requires re-tuning most hyperparameters</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-how-batch-norm-interacts-with-the-batch-size" aria-hidden="true" href="#how-batch-norm-interacts-with-the-batch-size"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How batch norm interacts with the batch size</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-choosing-the-initial-configuration" aria-hidden="true" href="#choosing-the-initial-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing the initial configuration</h3>
<ul dir="auto">
<li>Before beginning hyperparameter tuning we must determine the starting point.
This includes specifying (1) the model configuration (e.g. number of
layers), (2) the optimizer hyperparameters (e.g. learning rate), and (3) the
number of training steps.</li>
<li>Determining this initial configuration will require some manually configured
training runs and trial-and-error.</li>
<li>Our guiding principle is to find a simple, relatively fast, relatively
low-resource-consumption configuration that obtains a &#34;reasonable&#34; result.
<ul dir="auto">
<li>&#34;Simple&#34; means avoiding bells and whistles wherever possible; these can
always be added later. Even if bells and whistles prove helpful down the
road, adding them in the initial configuration risks wasting time tuning
unhelpful features and/or baking in unnecessary complications.
<ul dir="auto">
<li>For example, start with a constant learning rate before adding fancy
decay schedules.</li>
</ul>
</li>
<li>Choosing an initial configuration that is fast and consumes minimal
resources will make hyperparameter tuning much more efficient.
<ul dir="auto">
<li>For example, start with a smaller model.</li>
</ul>
</li>
<li>&#34;Reasonable&#34; performance depends on the problem, but at minimum means
that the trained model performs much better than random chance on the
validation set (although it might be bad enough to not be worth
deploying).</li>
</ul>
</li>
<li>Choosing the number of training steps involves balancing the following
tension:
<ul dir="auto">
<li>On the one hand, training for more steps can improve performance and
makes hyperparameter tuning easier (see
<a href="https://arxiv.org/abs/1811.03600" rel="nofollow">Shallue et al. 2018</a>).</li>
<li>On the other hand, training for fewer steps means that each training run
is faster and uses fewer resources, boosting tuning efficiency by
reducing the time between cycles and allowing more experiments to be run
in parallel. Moreover, if an unnecessarily large step budget is chosen
initially, it might be hard to change it down the road, e.g. once the
learning rate schedule is tuned for that number of steps.</li>
</ul>
</li>
</ul>
<h2 dir="auto"><a id="user-content-a-scientific-approach-to-improving-model-performance" aria-hidden="true" href="#a-scientific-approach-to-improving-model-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>A scientific approach to improving model performance</h2>
<p dir="auto">For the purposes of this document, the ultimate goal of machine learning
development is to maximize the utility of the deployed model. Even though many
aspects of the development process differ between applications (e.g. length of
time, available computing resources, type of model), we can typically use the
same basic steps and principles on any problem.</p>
<p dir="auto">Our guidance below makes the following assumptions:</p>
<ul dir="auto">
<li>There is already a fully-running training pipeline along with a
configuration that obtains a reasonable result.</li>
<li>There are enough computational resources available to conduct meaningful
tuning experiments and run at least several training jobs in parallel.</li>
</ul>
<h3 dir="auto"><a id="user-content-the-incremental-tuning-strategy" aria-hidden="true" href="#the-incremental-tuning-strategy"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The incremental tuning strategy</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Start with a simple configuration and incrementally make
improvements while building up insight into the problem. Make sure that any
improvement is based on strong evidence to avoid adding unnecessary complexity.</em></p>
<ul dir="auto">
<li>Our ultimate goal is to find a configuration that maximizes the performance
of our model.
<ul dir="auto">
<li>In some cases, our goal will be to maximize how much we can improve the
model by a fixed deadline (e.g. submitting to a competition).</li>
<li>In other cases, we want to keep improving the model indefinitely (e.g.
continually improving a model used in production).</li>
</ul>
</li>
<li>In principle, we could maximize performance by using an algorithm to
automatically search the entire space of possible configurations, but this
is not a practical option.
<ul dir="auto">
<li>The space of possible configurations is extremely large and there are
not yet any algorithms sophisticated enough to efficiently search this
space without human guidance.</li>
</ul>
</li>
<li>Most automated search algorithms rely on a hand-designed <em>search space</em> that
defines the set of configurations to search in, and these search spaces can
matter quite a bit.</li>
<li>The most effective way to maximize performance is to start with a simple
configuration and incrementally add features and make improvements while
building up insight into the problem.
<ul dir="auto">
<li>We use automated search algorithms in each round of tuning and
continually update our search spaces as our understanding grows.</li>
</ul>
</li>
<li>As we explore, we will naturally find better and better configurations and
therefore our &#34;best&#34; model will continually improve.
<ul dir="auto">
<li>We call it a <em>launch</em> when we update our best configuration (which may
or may not correspond to an actual launch of a production model).</li>
<li>For each launch, we must make sure that the change is based on strong
evidence – not just random chance based on a lucky configuration – so
that we don&#39;t add unnecessary complexity to the training pipeline.</li>
</ul>
</li>
</ul>
<p dir="auto">At a high level, our incremental tuning strategy involves repeating the
following four steps:</p>
<ol dir="auto">
<li>Identify an appropriately-scoped goal for the next round of experiments.</li>
<li>Design and run a set of experiments that makes progress towards this goal.</li>
<li>Learn what we can from the results.</li>
<li>Consider whether to launch the new best configuration.</li>
</ol>
<p dir="auto">The remainder of this section will consider this strategy in much greater
detail.</p>
<h3 dir="auto"><a id="user-content-exploration-vs-exploitation" aria-hidden="true" href="#exploration-vs-exploitation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Exploration vs exploitation</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Most of the time, our primary goal is to gain insight into the
problem.</em></p>
<ul dir="auto">
<li>Although one might think we would spend most of our time trying to maximize
performance on the validation set, in practice we spend the majority of our
time trying to gain insight into the problem, and comparatively little time
greedily focused on the validation error.
<ul dir="auto">
<li>In other words, we spend most of our time on &#34;exploration&#34; and only a
small amount on &#34;exploitation&#34;.</li>
</ul>
</li>
<li>In the long run, understanding the problem is critical if we want to
maximize our final performance. Prioritizing insight over short term gains
can help us:
<ul dir="auto">
<li>Avoid launching unnecessary changes that happened to be present in
well-performing runs merely through historical accident.</li>
<li>Identify which hyperparameters the validation error is most sensitive
to, which hyperparameters interact the most and therefore need to be
re-tuned together, and which hyperparameters are relatively insensitive
to other changes and can therefore be fixed in future experiments.</li>
<li>Suggest potential new features to try, such as new regularizers if
overfitting is an issue.</li>
<li>Identify features that don&#39;t help and therefore can be removed, reducing
the complexity of future experiments.</li>
<li>Recognize when improvements from hyperparameter tuning have likely
saturated.</li>
<li>Narrow our search spaces around the optimal value to improve tuning
efficiency.</li>
</ul>
</li>
<li>When we are eventually ready to be greedy, we can focus purely on the
validation error even if the experiments aren&#39;t maximally informative about
the structure of the tuning problem.</li>
</ul>
<h3 dir="auto"><a id="user-content-choosing-the-goal-for-the-next-round-of-experiments" aria-hidden="true" href="#choosing-the-goal-for-the-next-round-of-experiments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing the goal for the next round of experiments</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Each round of experiments should have a clear goal and be
sufficiently narrow in scope that the experiments can actually make progress
towards the goal.</em></p>
<ul dir="auto">
<li>Each round of experiments should have a clear goal and be sufficiently
narrow in scope that the experiments can actually make progress towards the
goal: if we try to add multiple features or answer multiple questions at
once, we may not be able to disentangle the separate effects on the results.</li>
<li>Example goals include:
<ul dir="auto">
<li>Try a potential improvement to the pipeline (e.g. a new regularizer,
preprocessing choice, etc.).</li>
<li>Understand the impact of a particular model hyperparameter (e.g. the
activation function)</li>
<li>Greedily maximize validation error.</li>
</ul>
</li>
</ul>
<h3 dir="auto"><a id="user-content-designing-the-next-round-of-experiments" aria-hidden="true" href="#designing-the-next-round-of-experiments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Designing the next round of experiments</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Identify which hyperparameters are scientific, nuisance, and
fixed hyperparameters for the experimental goal. Create a sequence of studies to
compare different values of the scientific hyperparameters while optimizing over
the nuisance hyperparameters. Choose the search space of nuisance
hyperparameters to balance resource costs with scientific value.</em></p>
<h4 dir="auto"><a id="user-content-identifying-scientific-nuisance-and-fixed-hyperparameters" aria-hidden="true" href="#identifying-scientific-nuisance-and-fixed-hyperparameters"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Identifying scientific, nuisance, and fixed hyperparameters</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-creating-a-set-of-studies" aria-hidden="true" href="#creating-a-set-of-studies"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Creating a set of studies</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-striking-a-balance-between-informative-and-affordable-experiments" aria-hidden="true" href="#striking-a-balance-between-informative-and-affordable-experiments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Striking a balance between informative and affordable experiments</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-extracting-insight-from-experimental-results" aria-hidden="true" href="#extracting-insight-from-experimental-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Extracting insight from experimental results</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>In addition to trying to achieve the original scientific goal of
each group of experiments, go through a checklist of additional questions and,
if issues are discovered, revise the experiments and rerun them.</em></p>
<ul dir="auto">
<li>Ultimately, each group of experiments has a specific goal and we want to
evaluate the evidence the experiments provide toward that goal.
<ul dir="auto">
<li>However, if we ask the right questions, we will often find issues that
need to be corrected before a given set of experiments can make much
progress towards their original goal.
<ul dir="auto">
<li>If we don’t ask these questions, we may draw incorrect conclusions.</li>
</ul>
</li>
<li>Since running experiments can be expensive, we also want to take the
opportunity to extract other useful insights from each group of
experiments, even if these insights are not immediately relevant to the
current goal.</li>
</ul>
</li>
<li>Before analyzing a given set of experiments to make progress toward their
original goal, we should ask ourselves the following additional questions:
<ul dir="auto">
<li><a href="#identifying-bad-search-space-boundaries">Is the search space large enough?</a>
<ul dir="auto">
<li>If the optimal point from a study is near the boundary of the search
space in one or more dimensions, the search is probably not wide
enough. In this case, we should run another study with an expanded
search space.</li>
</ul>
</li>
<li><a href="#not-sampling-enough-points-in-the-search-space">Have we sampled enough points from the search space?</a>
<ul dir="auto">
<li>If not, run more points or be less ambitious in the tuning goals.</li>
</ul>
</li>
<li>What fraction of the trials in each study are <strong>infeasible</strong> (i.e.
trials that diverge, get really bad loss values, or fail to run at all
because they violate some implicit constraint)?
<ul dir="auto">
<li>When a very large fraction of points in a study are <strong>infeasible</strong>
we should try to adjust the search space to avoid sampling such
points, which sometimes requires reparameterizing the search space.</li>
<li>In some cases, a large number of infeasible points can indicate a
bug in the training code.</li>
</ul>
</li>
<li><a href="#how-can-optimization-failures-be-debugged-and-mitigated">Does the model exhibit optimization issues?</a></li>
<li><a href="#examining-the-training-curves">What can we learn from the training curves of the best trials?</a>
<ul dir="auto">
<li>For example, do the best trials have training curves consistent with
problematic overfitting?</li>
</ul>
</li>
</ul>
</li>
<li>If necessary, based on the answers to the questions above, refine the most
recent study (or group of studies) to improve the search space and/or sample
more trials, or take some other corrective action.</li>
<li>Once we have answered the above questions, we can move on to evaluating the
evidence the experiments provide towards our original goal (for example,
<a href="#detecting-whether-a-change-is-useful-with-isolation-plots">evaluating whether a change is useful</a>).</li>
</ul>
<h4 dir="auto"><a id="user-content-identifying-bad-search-space-boundaries" aria-hidden="true" href="#identifying-bad-search-space-boundaries"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Identifying bad search space boundaries</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-not-sampling-enough-points-in-the-search-space" aria-hidden="true" href="#not-sampling-enough-points-in-the-search-space"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Not sampling enough points in the search space</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-examining-the-training-curves" aria-hidden="true" href="#examining-the-training-curves"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Examining the training curves</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-detecting-whether-a-change-is-useful-with-isolation-plots" aria-hidden="true" href="#detecting-whether-a-change-is-useful-with-isolation-plots"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Detecting whether a change is useful with isolation plots</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-automate-generically-useful-plots" aria-hidden="true" href="#automate-generically-useful-plots"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Automate generically useful plots</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration" aria-hidden="true" href="#determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Determining whether to adopt a training pipeline change or hyperparameter configuration</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>When deciding whether to make a change to our model or training
procedure or adopt a new hyperparameter configuration going forward, we need to
be aware of the different sources of variation in our results.</em></p>
<ul dir="auto">
<li>When we are trying to improve our model, we might observe that a particular
candidate change initially achieves a better validation error compared to
our incumbent configuration, but find that after repeating the experiment
there is no consistent advantage. Informally, we can group the most
important sources of variation that might cause such an inconsistent result
into the following broad categories:
<ul dir="auto">
<li><strong>Training procedure variance</strong>, <strong>retrain variance</strong>, or <strong>trial
variance</strong>: the variation we see between training runs that use the same
hyperparameters, but different random seeds.
<ul dir="auto">
<li>For example, different random initializations, training data
shuffles, dropout masks, patterns of data augmentation operations,
and orderings of parallel arithmetic operations, are all potential
sources of trial variance.</li>
</ul>
</li>
<li><strong>Hyperparameter search variance</strong>, or <strong>study variance</strong>: the variation
in results caused by our procedure to select the hyperparameters.
<ul dir="auto">
<li>For example, we might run the same experiment with a particular
search space, but with two different seeds for quasi-random search
and end up selecting different hyperparameter values.</li>
</ul>
</li>
<li><strong>Data collection and sampling variance</strong>: the variance from any sort of
random split into training, validation, and test data or variance due to
the training data generation process more generally.</li>
</ul>
</li>
<li>It is all well and good to make comparisons of validation error rates
estimated on a finite validation set using fastidious statistical tests, but
often the trial variance alone can produce statistically significant
differences between two different trained models that use the same
hyperparameter settings.</li>
<li>We are most concerned about study variance when trying to make conclusions
that go beyond the level of an individual point in hyperparameters space.
<ul dir="auto">
<li>The study variance depends on the number of trials and the search space
and we have seen cases where it is larger than the trial variance as
well as cases where it is much smaller.</li>
</ul>
</li>
<li>Therefore, before adopting a candidate change, consider running the best
trial N times to characterize the run-to-run trial variance.
<ul dir="auto">
<li>Usually, we can get away with only recharacterizing the trial variance
after major changes to the pipeline, but in some applications we might
need fresher estimates.</li>
<li>In other applications, characterizing the trial variance is too costly
to be worth it.</li>
</ul>
</li>
<li>At the end of the day, although we only want to adopt changes (including new
hyperparameter configurations) that produce real improvements, demanding
complete certainty that something helps isn&#39;t the right answer either.</li>
<li>Therefore, if a new hyperparameter point (or other change) gets a better
result than the baseline (taking into account the retrain variance of both
the new point and the baseline as best we can), then we probably should
adopt it as the new baseline for future comparisons.
<ul dir="auto">
<li>However, we should only adopt changes that produce improvements that
outweigh any complexity they add.</li>
</ul>
</li>
</ul>
<h3 dir="auto"><a id="user-content-after-exploration-concludes" aria-hidden="true" href="#after-exploration-concludes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>After exploration concludes</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Bayesian optimization tools are a compelling option once we’re
done exploring for good search spaces and have decided what hyperparameters even
should be tuned at all.</em></p>
<ul dir="auto">
<li>At some point, our priorities will shift from learning more about the tuning
problem to producing a single best configuration to launch or otherwise use.</li>
<li>At this point, there should be a refined search space that comfortably
contains the local region around the best observed trial and has been
adequately sampled.</li>
<li>Our exploration work should have revealed the most essential hyperparameters
to tune (as well as sensible ranges for them) that we can use to construct a
search space for a final automated tuning study using as large a tuning
budget as possible.</li>
<li>Since we no longer care about maximizing our insight into the tuning
problem, many of
<a href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning">the advantages of quasi-random search</a>
no longer apply and Bayesian optimization tools should be used to
automatically find the best hyperparameter configuration.
<ul dir="auto">
<li>If the search space contains a non-trivial volume of divergent points
(points that get NaN training loss or even training loss many standard
deviations worse than the mean), it is important to use black box
optimization tools that properly handle trials that diverge (see
<a href="https://arxiv.org/abs/1403.5607" rel="nofollow">Bayesian Optimization with Unknown Constraints</a>
for an excellent way to deal with this issue).</li>
</ul>
</li>
<li>At this point, we should also consider checking the performance on the test
set.
<ul dir="auto">
<li>In principle, we could even fold the validation set into the training
set and retraining the best configuration found with Bayesian
optimization. However, this is only appropriate if there won&#39;t be future
launches with this specific workload (e.g. a one-time Kaggle
competition).</li>
</ul>
</li>
</ul>
<h2 dir="auto"><a id="user-content-determining-the-number-of-steps-for-each-training-run" aria-hidden="true" href="#determining-the-number-of-steps-for-each-training-run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Determining the number of steps for each training run</h2>
<ul dir="auto">
<li>There are two types of workloads: those that are compute-bound and those
that are not.</li>
<li>When training is <strong>compute-bound</strong>, training is limited by how long we are
willing to wait and not by how much training data we have or some other
factor.
<ul dir="auto">
<li>In this case, if we can somehow train longer or more efficiently, we
should see a lower training loss and, with proper tuning, an improved
validation loss.</li>
<li>In other words, <em>speeding up</em> training is equivalent to <em>improving</em>
training and the &#34;optimal&#34; training time is always &#34;as long as we can
afford.&#34;</li>
<li>That said, just because a workload is compute-limited doesn&#39;t mean
training longer/faster is the only way to improve results.</li>
</ul>
</li>
<li>When training is <strong>not compute-bound</strong>, we can afford to train as long as we
would like to, and, at some point, training longer doesn&#39;t help much (or
even causes problematic overfitting).
<ul dir="auto">
<li>In this case, we should expect to be able to train to very low training
loss, to the point where training longer might slightly reduce the
training loss, but will not meaningfully reduce the validation loss.</li>
<li>Particularly when training is not compute-bound, a more generous
training time budget can make tuning easier, especially when tuning
learning rate decay schedules, since they have a particularly strong
interaction with the training budget.
<ul dir="auto">
<li>In other words, very stingy training time budgets might require a
learning rate decay schedule tuned to perfection in order to achieve
a good error rate.</li>
</ul>
</li>
</ul>
</li>
<li>Regardless of whether a given workload is compute-bound or not, methods that
increase the variance of the gradients (across batches) will usually result
in slower training progress, and thus may increase the number of training
steps required to reach a particular validation loss. High gradient variance
can be caused by:
<ul dir="auto">
<li>Using a smaller batch size</li>
<li>Adding data augmentation</li>
<li>Adding some types of regularization (e.g. dropout)</li>
</ul>
</li>
</ul>
<h3 dir="auto"><a id="user-content-deciding-how-long-to-train-when-training-is-not-compute-bound" aria-hidden="true" href="#deciding-how-long-to-train-when-training-is-not-compute-bound"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deciding how long to train when training is <em>not</em> compute-bound</h3>
<ul dir="auto">
<li>Our main goal is to ensure we are training long enough for the model to
reach the best possible result, while avoiding being overly wasteful in the
number of training steps.</li>
<li>When in doubt, err on the side of training longer. Performance should never
degrade when training longer, assuming retrospective (optimal) checkpoint
selection is used properly and checkpoints are frequent enough.</li>
<li>Never tune the <code>max_train_steps</code> number in a study. Pick a value and use it
for all trials. From these trials, plot the training step that retrospective
checkpoint selection finds in order to refine the choice of
<code>max_train_steps</code>.
<ul dir="auto">
<li>For example, if the best step is always during the first 10% of
training, then the maximum number of steps is way too high.</li>
<li>Alternatively, if the best step is consistently in the last 25% of
training we might benefit from training longer and re-tuning the decay
schedule.</li>
</ul>
</li>
<li>The ideal number of training steps can change when the architecture or data
changes (e.g. adding data augmentation).</li>
<li>Below we describe how to pick an initial candidate value for
<code>max_train_steps</code> based on the number of steps necessary to &#34;perfectly fit&#34;
the training set using a constant learning rate.
<ul dir="auto">
<li>Note, we are not using the phrase &#34;perfectly fit the training set&#34; in a
precise or mathematically well-defined way. It is merely meant as an
informal descriptor to indicate a very low training loss.
<ul dir="auto">
<li>For example, when training with the log loss, absent regularization
terms, we might see the training loss keep slowly improving until we
reach floating point limits as the network weights grow without
bound and the predictions of the model on the training set become
increasingly confident. In this case, we might say the model
&#34;perfectly fit&#34; the training set around the time the
misclassification error reached zero on the training set.</li>
</ul>
</li>
<li>The starting value for <code>max_train_steps</code> we find may need to be
increased if the amount of gradient noise in the training procedure
increases.
<ul dir="auto">
<li>For example, if data augmentation or regularizers like dropout are
introduced to the model.</li>
</ul>
</li>
<li>It may be possible to decrease <code>max_train_steps</code> if the training process
improves somehow.
<ul dir="auto">
<li>For example, with a better tuned optimizer or a better tuned
learning rate schedule.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 dir="auto"><a id="user-content-algorithm-for-picking-an-initial-candidate-for-max_train_steps-using-a-learning-rate-sweep" aria-hidden="true" href="#algorithm-for-picking-an-initial-candidate-for-max_train_steps-using-a-learning-rate-sweep"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Algorithm for picking an initial candidate for max_train_steps using a learning rate sweep</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-deciding-how-long-to-train-when-training-is-compute-bound" aria-hidden="true" href="#deciding-how-long-to-train-when-training-is-compute-bound"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deciding how long to train when training is compute-bound</h3>
<ul dir="auto">
<li>In some cases, training loss keeps improving indefinitely and our patience
and computational resources become the limiting factors.</li>
<li>If training loss (or even validation loss) keeps improving indefinitely,
should we always train as long as we can afford? Not necessarily.
<ul dir="auto">
<li>We might be able to tune more effectively by running a larger number of
shorter experiments and reserving the longest &#34;production length&#34; runs
for the models we hope to launch.</li>
<li>As the training time for trials approaches our patience limit, tuning
experiments become more relevant for our potential launch candidates,
but we can complete fewer of them.</li>
<li>There are probably many questions we can answer while only training for
~10% of the production length, but there is always a risk that our
conclusions at this time limit will not apply to experiments at 20% of
the production length, let alone 100%.</li>
</ul>
</li>
<li>Tuning in multiple rounds with increasing, per-trial training step limits is
a sensible approach.
<ul dir="auto">
<li>We can do as many rounds as we want, but usually 1-3 are the most
practical.</li>
<li>Essentially, try to obtain as much understanding of the problem as
possible using trials with a very quick turnaround time, trading off
tuning thoroughness with relevance to the final, longest runs.</li>
<li>Once a given per-trial time limit has generated useful insights, we can
increase the training time and continue tuning, double-checking our
conclusions from the shorter runs as needed.</li>
</ul>
</li>
<li>As a starting point, we recommend two rounds of tuning:
<ul dir="auto">
<li>Round 1: Shorter runs to find good model and optimizer hyperparameters.</li>
<li>Round 2: Very few long runs on good hyperparameter points to get the
final model.</li>
</ul>
</li>
<li>The biggest question going from <code>Round i</code> → <code>Round i+1</code> is how to
adjust learning rate decay schedules.
<ul dir="auto">
<li>One common pitfall when adjusting learning rate schedules between rounds
is using all the extra training steps with too small of a learning rate.</li>
</ul>
</li>
</ul>
<h4 dir="auto"><a id="user-content-round-1" aria-hidden="true" href="#round-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Round 1</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-round-2" aria-hidden="true" href="#round-2"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Round 2</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h2 dir="auto"><a id="user-content-additional-guidance-for-the-training-pipeline" aria-hidden="true" href="#additional-guidance-for-the-training-pipeline"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Additional guidance for the training pipeline</h2>
<h3 dir="auto"><a id="user-content-optimizing-the-input-pipeline" aria-hidden="true" href="#optimizing-the-input-pipeline"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Optimizing the input pipeline</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>The causes and interventions of input-bound pipelines are highly
task-dependent; use a profiler and look out for common issues.</em></p>
<ul dir="auto">
<li>Use an appropriate profiler to diagnose input-bound pipelines. For example,
<a href="https://jax.readthedocs.io/en/latest/profiling.html" rel="nofollow">Perfetto</a> for JAX or
<a href="https://www.tensorflow.org/guide/profiler" rel="nofollow">TensorFlow profiler</a> for
TensorFlow.</li>
<li>Ultimately, the specific causes and interventions will be highly
task-dependent. Broader engineering considerations (e.g. minimizing disk
footprint) may warrant worse input pipeline performance.</li>
<li>Common causes:
<ul dir="auto">
<li>Data are not colocated with the training process, causing I/O latency
(this might happen when reading training data over a network).</li>
<li>Expensive online data preprocessing (consider doing this once offline
and saving).</li>
<li>Unintentional synchronization barriers that interfere with data pipeline
prefetching. For example, when synchronizing metrics between the device
and host in CommonLoopUtils
(<a href="https://github.com/google/CommonLoopUtils/blob/fea2518ada8814a78e1492023fd9f00edb0b0568/clu/metrics.py#L291">link</a>).</li>
</ul>
</li>
<li>Common tips:
<ul dir="auto">
<li>Instrument input pipeline to prefetch examples (e.g.
<a href="https://www.tensorflow.org/guide/data_performance#prefetching" rel="nofollow">tf.data.Dataset.prefetch</a>)</li>
<li>Remove unused features/metadata from each as early in the pipeline as
possible.</li>
<li>Increase the replication of the number of jobs generating examples for
the input pipeline. For example, by using the
<a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service" rel="nofollow">tf.data service</a>.</li>
</ul>
</li>
</ul>
<h3 dir="auto"><a id="user-content-evaluating-model-performance" aria-hidden="true" href="#evaluating-model-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evaluating model performance</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Run evaluation at larger batch sizes than training. Run
evaluations at regular step intervals, not regular time intervals.</em></p>
<h4 dir="auto"><a id="user-content-evaluation-settings" aria-hidden="true" href="#evaluation-settings"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evaluation settings</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-setting-up-periodic-evaluations" aria-hidden="true" href="#setting-up-periodic-evaluations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Setting up periodic evaluations</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h4 dir="auto"><a id="user-content-choosing-a-sample-for-periodic-evaluation" aria-hidden="true" href="#choosing-a-sample-for-periodic-evaluation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Choosing a sample for periodic evaluation</h4>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint" aria-hidden="true" href="#saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Saving checkpoints and retrospectively selecting the best checkpoint</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Run training for a fixed number of steps and retrospectively
choose the best checkpoint from the run.</em></p>
<ul dir="auto">
<li>Most deep learning frameworks support
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.training.html" rel="nofollow">model checkpointing</a>.
That is, the current state of the model is periodically preserved on disk.
This allows the training job to be resilient to compute instance
interruptions.</li>
<li>The best checkpoint is often not the last checkpoint, particularly when the
validation set performance does not continue to increase over time but
rather fluctuates about a particular value.</li>
<li>Set up the pipeline to keep track of the N best checkpoints seen so far
during training. At the end of training, model selection is then a matter of
choosing the best checkpoint seen during training. We call this
<strong>retrospective optimal checkpoint selection</strong>.</li>
<li>Supporting prospective early stopping is usually not necessary, since we’re
pre-specifying a trial budget and are preserving the N best checkpoints seen
so far.</li>
</ul>
<h3 dir="auto"><a id="user-content-setting-up-experiment-tracking" aria-hidden="true" href="#setting-up-experiment-tracking"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Setting up experiment tracking</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>When tracking different experiments, make sure to note a number
of essentials like the best performance of a checkpoint in the study, and a
short description of the study.</em></p>
<ul dir="auto">
<li>We&#39;ve found that keeping track of experiment results in a spreadsheet has
been helpful for the sorts of modeling problems we&#39;ve worked on. It often
has the following columns:
<ul dir="auto">
<li>Study name</li>
<li>A link to wherever the config for the study is stored.</li>
<li>Notes or a short description of the study.</li>
<li>Number of trials run</li>
<li>Performance on the validation set of the best checkpoint in the study.</li>
<li>Specific reproduction commands or notes on what unsubmitted changes were
necessary to launch training.</li>
</ul>
</li>
<li>Find a tracking system that captures at least the information listed above
and is convenient for the people doing it. Untracked experiments might as
well not exist.</li>
</ul>
<h3 dir="auto"><a id="user-content-batch-normalization-implementation-details" aria-hidden="true" href="#batch-normalization-implementation-details"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Batch normalization implementation details</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>Nowadays batch norm can often be replaced with LayerNorm, but in
cases where it cannot, there are tricky details when changing the batch size or
number of hosts.</em></p>
<ul dir="auto">
<li>Batch norm normalizes activations using their mean and variance over the
current batch, but in the multi-device setting these statistics are
different on each device unless explicitly synchronized.</li>
<li>Anecdotal reports (mostly on ImageNet) say calculating these normalizing
statistics using only ~64 examples actually works better in practice (see
Ghost Batch Norm from <a href="https://arxiv.org/abs/1705.08741" rel="nofollow">this paper</a>).</li>
<li>Decoupling the total batch size and the number of examples used to calculate
batch norm statistics is particularly useful for batch size comparisons.</li>
<li>Ghost batch norm implementations do not always correctly handle the case
where the per-device batch size &gt; virtual batch size. In this case we&#39;d
actually need to subsample the batch on each device in order to get the
proper number of batch norm statistic examples.</li>
<li>Exponential moving averages used in test mode batch norm are just a linear
combination of training statistics, so these EMAs only need to be
synchronized before saving them in checkpoints. However, some common
implementations of batch norm do not synchronize these EMAs and only save
the EMA from the first device.</li>
</ul>
<h3 dir="auto"><a id="user-content-considerations-for-multi-host-pipelines" aria-hidden="true" href="#considerations-for-multi-host-pipelines"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Considerations for multi-host pipelines</h3>
<p dir="auto"><em><strong>Summary:</strong></em> <em>for logging, evals, RNGs, checkpointing, and data sharding,
multi-host training can make it very easy to introduce bugs!</em></p>
<ul dir="auto">
<li>Ensure the pipeline is only logging and checkpointing on one host.</li>
<li>Make sure before evaluation or checkpointing is run, the batch norm
statistics are synchronized across hosts.</li>
<li>It is critical to have RNG seeds that are the same across hosts (for model
initialization), and seeds that are different across hosts (for data
shuffling/preprocessing), so make sure to mark them appropriately.</li>
<li>Sharding data files across hosts is usually recommended for improved
performance.</li>
</ul>
<h2 dir="auto"><a id="user-content-faqs" aria-hidden="true" href="#faqs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>FAQs</h2>
<h3 dir="auto"><a id="user-content-what-is-the-best-learning-rate-decay-schedule-family" aria-hidden="true" href="#what-is-the-best-learning-rate-decay-schedule-family"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is the best learning rate decay schedule family?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-which-learning-rate-decay-should-i-use-as-a-default" aria-hidden="true" href="#which-learning-rate-decay-should-i-use-as-a-default"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Which learning rate decay should I use as a default?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-why-do-some-papers-have-complicated-learning-rate-schedules" aria-hidden="true" href="#why-do-some-papers-have-complicated-learning-rate-schedules"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why do some papers have complicated learning rate schedules?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-how-should-adams-hyperparameters-be-tuned" aria-hidden="true" href="#how-should-adams-hyperparameters-be-tuned"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How should Adam’s hyperparameters be tuned?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning" aria-hidden="true" href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why use quasi-random search instead of more sophisticated black box optimization algorithms during the exploration phase of tuning?</h3>
<details><summary><em>[Click to expand]</em></summary>
<ul dir="auto">
<li>Quasi-random search (based on
<a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence" rel="nofollow">low-discrepancy sequences</a>)
is our preference over fancier black box optimization tools when used as
part of an iterative tuning process intended to maximize insight into the
tuning problem (what we refer to as the &#34;exploration phase&#34;). Bayesian
optimization and similar tools are more appropriate for the exploitation
phase.</li>
<li>Quasi-random search based on randomly shifted low-discrepancy sequences can
be thought of as &#34;jittered, shuffled grid search&#34;, since it uniformly, but
randomly, explores a given search space and spreads out the search points
more than random search.</li>
<li>The advantages of quasi-random search over more sophisticated black box
optimization tools (e.g. Bayesian optimization, evolutionary algorithms)
include:
<ol dir="auto">
<li>Sampling the search space non-adaptively makes it possible to change the
tuning objective in post hoc analysis without rerunning experiments.
<ul dir="auto">
<li>For example, we usually want to find the best trial in terms of
validation error achieved at any point in training. But the
non-adaptive nature of quasi-random search makes it possible to find
the best trial based on final validation error, training error, or
some alternative evaluation metric without rerunning any
experiments.</li>
</ul>
</li>
<li>Quasi-random search behaves in a consistent and statistically
reproducible way.
<ul dir="auto">
<li>It should be possible to reproduce a study from six months ago even
if the implementation of the search algorithm changes, as long as it
maintains the same uniformity properties. If using sophisticated
Bayesian optimization software, the implementation might change in
an important way between versions, making it much harder to
reproduce an old search. It isn’t always possible to roll back to an
old implementation (e.g. if the optimization tool is run as a
service).</li>
</ul>
</li>
<li>Its uniform exploration of the search space makes it easier to reason
about the results and what they might suggest about the search space.
<ul dir="auto">
<li>For example, if the best point in the traversal of quasi-random
search is at the boundary of the search space, this is a good (but
not foolproof) signal that the search space bounds should be
changed. <a href="#identifying-bad-search-space-boundaries">This section</a>
goes into more depth. However, an adaptive black box optimization
algorithm might have neglected the middle of the search space
because of some unlucky early trials even if it happens to contain
equally good points, since it is this exact sort of non-uniformity
that a good optimization algorithm needs to employ to speed up the
search.</li>
</ul>
</li>
<li>Running different numbers of trials in parallel versus sequentially will
not produce statistically different results when using quasi-random
search (or other non-adaptive search algorithms), unlike with adaptive
algorithms.</li>
<li>More sophisticated search algorithms may not always handle infeasible
points correctly, especially if they aren&#39;t designed with neural network
hyperparameter tuning in mind.</li>
<li>Quasi-random search is simple and works especially well when many tuning
trials will be running in parallel.
<ul dir="auto">
<li>Anecdotally<sup><a href="#user-content-fn-3-9853a2357fd18b27e5cfd5bb3779f74d" id="user-content-fnref-3-9853a2357fd18b27e5cfd5bb3779f74d" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>, it is very hard for an adaptive algorithm to beat a
quasi-random search that has 2X its budget, especially when many
trials need to be run in parallel (and thus there are very few
chances to make use of previous trial results when launching new
trials).</li>
<li>Without expertise in Bayesian optimization and other advanced black
box optimization methods, we might not achieve the benefits they
are, in principle, capable of providing. It is hard to benchmark
advanced black box optimization algorithms in realistic deep
learning tuning conditions. They are a very active area of current
research, and the more sophisticated algorithms come with their own
pitfalls for inexperienced users. Experts in these methods are able
to get good results, but in high-parallelism conditions the search
space and budget tend to matter a lot more.</li>
</ul>
</li>
</ol>
</li>
<li>That said, if our computational resources only allow a small number of
trials to run in parallel and we can afford to run many trials in sequence,
Bayesian optimization becomes much more attractive despite making our tuning
results harder to interpret.</li>
</ul>
</details>
<h3 dir="auto"><a id="user-content-where-can-i-find-an-implementation-of-quasi-random-search" aria-hidden="true" href="#where-can-i-find-an-implementation-of-quasi-random-search"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Where can I find an implementation of quasi-random search?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-how-many-trials-are-needed-to-get-good-results-with-quasi-random-search" aria-hidden="true" href="#how-many-trials-are-needed-to-get-good-results-with-quasi-random-search"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How many trials are needed to get good results with quasi-random search?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-how-can-optimization-failures-be-debugged-and-mitigated" aria-hidden="true" href="#how-can-optimization-failures-be-debugged-and-mitigated"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How can optimization failures be debugged and mitigated?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-why-do-you-call-the-learning-rate-and-other-optimization-parameters-hyperparameters-they-are-not-parameters-of-any-prior-distribution" aria-hidden="true" href="#why-do-you-call-the-learning-rate-and-other-optimization-parameters-hyperparameters-they-are-not-parameters-of-any-prior-distribution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why do you call the learning rate and other optimization parameters hyperparameters? They are not parameters of any prior distribution.</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-why-shouldnt-the-batch-size-be-tuned-to-directly-improve-validation-set-performance" aria-hidden="true" href="#why-shouldnt-the-batch-size-be-tuned-to-directly-improve-validation-set-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why shouldn&#39;t the batch size be tuned to directly improve validation set performance?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h3 dir="auto"><a id="user-content-what-are-the-update-rules-for-all-the-popular-optimization-algorithms" aria-hidden="true" href="#what-are-the-update-rules-for-all-the-popular-optimization-algorithms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What are the update rules for all the popular optimization algorithms?</h3>
<details><summary><em>[Click to expand]</em></summary>
</details>
<h2 dir="auto"><a id="user-content-acknowledgments" aria-hidden="true" href="#acknowledgments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgments</h2>
<ul dir="auto">
<li>We owe a debt of gratitude to Max Bileschi, Roy Frostig, Zelda Mariet, Stan
Bileschi, Mohammad Norouzi, Chris DuBois and Charles Sutton for reading the
manuscript and providing valuable feedback.</li>
<li>We reused some experimental data for several plots that were originally
produced by Naman Agarwal for other joint research.</li>
<li>We would like to thank Will Chen for invaluable advice on the presentation of the document.</li>
<li>We would also like to thank Rohan Anil for useful discussions.</li>
</ul>
<h2 dir="auto"><a id="user-content-citing" aria-hidden="true" href="#citing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Citing</h2>
<div data-snippet-clipboard-copy-content="@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}"><pre><code>@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}
</code></pre></div>
<h2 dir="auto"><a id="user-content-contributing" aria-hidden="true" href="#contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributing</h2>
<ul dir="auto">
<li>
<p dir="auto">This is not an officially supported Google product.</p>
</li>
<li>
<p dir="auto">We&#39;d love to hear your feedback!</p>
<ul dir="auto">
<li>If you like the playbook, please <a href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars#starring-a-repository">leave a star</a>! Or email
deep-learning-tuning-playbook [at] googlegroups.com. Testimonials help
us justify creating more resources like this.</li>
<li>If anything seems incorrect, please file an issue to start a discussion.
For questions or other messages where an issue isn&#39;t appropriate, please
open a new discussion topic on GitHub.</li>
</ul>
</li>
<li>
<p dir="auto">As discussed in the preamble, this is a living document. We anticipate
making periodic improvements, both small and large. If you’d like to be
notified, please watch our repository (see <a href="https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository">instructions</a>).</p>
</li>
<li>
<p dir="auto">Please don&#39;t file a pull request without first coordinating with the authors
via the issue tracking system.</p>
</li>
</ul>
<h3 dir="auto"><a id="user-content-contributor-license-agreement" aria-hidden="true" href="#contributor-license-agreement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contributor License Agreement</h3>
<p dir="auto">Contributions to this project must be accompanied by a Contributor License
Agreement (CLA). You (or your employer) retain the copyright to your
contribution; this simply gives us permission to use and redistribute your
contributions as part of the project. Head over to
<a href="https://cla.developers.google.com/" rel="nofollow">https://cla.developers.google.com/</a> to see your current agreements on file or
to sign a new one.</p>
<p dir="auto">You generally only need to submit a CLA once, so if you&#39;ve already submitted one
(even if it was for a different project), you probably don&#39;t need to do it
again.</p>
<h3 dir="auto"><a id="user-content-code-reviews" aria-hidden="true" href="#code-reviews"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Code Reviews</h3>
<p dir="auto">All submissions, including submissions by project members, require review. We
use GitHub pull requests for this purpose. Consult
<a href="https://help.github.com/articles/about-pull-requests/">GitHub Help</a> for more
information on using pull requests.</p>
<h3 dir="auto"><a id="user-content-community-guidelines" aria-hidden="true" href="#community-guidelines"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Community Guidelines</h3>
<p dir="auto">This project follows
<a href="https://opensource.google/conduct/" rel="nofollow">Google&#39;s Open Source Community Guidelines</a>.</p>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-3-9853a2357fd18b27e5cfd5bb3779f74d">
<p dir="auto">Ben Recht and Kevin Jamieson
<a href="http://www.argmin.net/2016/06/20/hypertuning/">pointed out</a> how strong
2X-budget random search is as a baseline (the
<a href="https://jmlr.org/papers/volume18/16-558/16-558.pdf">Hyperband paper</a>
makes similar arguments), but it is certainly possible to find search
spaces and problems where state-of-the-art Bayesian optimization
techniques crush random search that has 2X the budget. However, in our
experience beating 2X-budget random search gets much harder in the
high-parallelism regime since Bayesian optimization has no opportunity to
observe the results of previous trials. <a href="#user-content-fnref-3-9853a2357fd18b27e5cfd5bb3779f74d" data-footnote-backref="" aria-label="Back to content"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
</li>
</ol>
</section>
</article>
          </div></div>
  </body>
</html>
