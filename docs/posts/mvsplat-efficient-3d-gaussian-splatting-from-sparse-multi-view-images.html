<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://donydchen.github.io/mvsplat/">Original</a>
    <h1>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</h1>
    
    <div id="readability-page-1" class="page">


<nav role="navigation" aria-label="main navigation">
  
  
</nav>


<section>
  <div>
    <div>
      <div>
        <div>
          

          
          
          
          

          <p>
            <h2>ECCV 2024</h2>
          </p>

          <p><span><sup>1</sup>Monash University, </span>
            <span><sup>2</sup>ETH Zurich, </span>
            <span><sup>3</sup>University of Tübingen, Tübingen AI Center, </span>
          </p>
          <p><span><sup>4</sup>VGG, University of Oxford, </span>
            <span><sup>5</sup>Microsoft, </span>
            <span><sup>6</sup>Nanyang Technological University</span>
          </p>

          

        </div>
        </div>
      </div>
    </div>
  
</section>



<section>
  <div>
    <p>
      <video id="replay-video" autoplay="" controls="" muted="" preload="" playsinline="" loop="" width="100%">
            <source src="static/videos/teaser.mp4" type="video/mp4"/>
      </video>
      <h2>
        <b>TL;DR:</b> MVSplat builds a cost volume representation to efficiently predict 3D Gaussians from sparse
        multi-view images in a single forward pass.
      </h2>
    </p>
  </div>
</section>


<section>
  <div>
    <!-- Highlights -->
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
        We introduce MVSplat, an efficient model that, given sparse multi-view images as input, predicts clean feed-forward 3D Gaussians. To accurately localize the Gaussian centers, we build a cost volume representation via plane sweeping, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We also learn other Gaussian primitives&#39; parameters jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussians via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, MVSplat achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). <b>More impressively, compared to the latest state-of-the-art method pixelSplat, MVSplat uses 10× fewer parameters and infers more than 2× faster while providing higher appearance and geometry quality as well as better cross-dataset generalization.</b>
        </p>
      </div>
    </div>
    
  </div>
</section>


<section>
  <div>

    <div>
      <div>
        <h2>Overview</h2>

        <p><img src="https://donydchen.github.io/mvsplat/static/images/architecture.png"/></p><figcaption><b>Overview of MVSplat.</b> Given multiple posed images as input, we first extract
        multi-view image features with a multi-view Transformer, which contains self- and
        cross-attention layers to exchange information across views. Next, we construct per-view 
        cost volumes using plane sweeping. The Transformer features and cost volumes
        are concatenated together as input to a 2D U-Net (with cross-view attention) for cost
        volume refinement and predicting per-view depth maps. The per-view depth maps are
        unprojected to 3D and combined using a simple deterministic union operation as the 3D
        Gaussian centers. The opacity, covariance and color Gaussian parameters are predicted
        jointly with the depth maps. Finally, novel views are rendered from the predicted 3D
        Gaussians with the splatting operation.</figcaption>

      </div>
    </div>

  </div>
</section>


<section>
  <div>

    <div>
      <div>
        <h2>Comparisons with the State-of-the-art</h2>

        <div>
          <p>We present qualitative comparisons with the following state-of-the-art models:</p>
          <ul>
            <li><a href="https://davidcharatan.com/pixelsplat/">pixelSplat</a>: The latest feed-forward 3D Gaussians model
              that utilies data-driven regression architecture to predict Gaussian centers, leading to 
              poor geometry reconstruction and limited ability of cross-dataset generalization.</li>
            <li><a href="https://haofeixu.github.io/murf/">MuRF</a>: The latest feed-forward NeRF model that leverages 
              3D volume and (2+1)D CNN, which is expensive to train and renders comparably slowly.</li>
          </ul>
        </div>

        <p><img src="https://donydchen.github.io/mvsplat/static/images/sota_comparisons.png" alt="comparison on Real Estate 10k and ACID dataset"/></p>

      </div>
    </div>

  </div>
</section>


<section>
  <div>

    <div>
      <div>
        <h2>Comparisons of Geometry Reconstruction</h2>

        <p>Our MVSplat produces significantly higher-quality 3D Gaussian primitives than the latest state-of-the-art
        pixelSplat. The readers are invited to view the corresponding &#34;.ply&#34; files of the 3D Gaussians exported from
        both models provided at <b><a href="https://drive.google.com/drive/folders/1nBpUQnBvAIL7oLODElhIiLkW9x5gAuWs" target="_blank">HERE</a></b>.
        We recommend viewing them with online viewers, <i>e.g.</i>,
        <a href="https://projects.markkellogg.org/threejs/demo_gaussian_splats_3d.php?art=1&amp;cu=0,0,1&amp;cp=0,1,0&amp;cla=1,0,0" target="_blank">3D Gaussian Splatting with Three.js</a>
        (camera up should be set to &#34;0,0,1&#34;).</p>

        <p><img src="https://donydchen.github.io/mvsplat/static/images/point_clouds.png" alt="point clouds and depth maps"/>
      </p></div>
    </div>

  </div>
</section>



<section>
  <div>

    <div>
      <div>
        <h2>Comparisons of Cross-dataset Generalization</h2>

        <p>Our MVSplat is inherently superior in generalizing to <em>out-of-distribution</em> novel scenes, primarily due to the fact that the
      cost volume captures the <em>relative similarity</em> between features, which remains <em>invariant</em> compared to the absolute scale of
      features. Here, we present cross-dataset generalization by training models solely on RealEstate10K (indoor scenes), and directly 
      test them on DTU (object-centric scenes) and ACID (outdoor scenes).</p>

        <p><img src="https://donydchen.github.io/mvsplat/static/images/re10k_generalization.png" alt="trained on RealEstate10K, and tested on DTU and ACID"/></p>

      </div>
    </div>

  </div>
</section>



<section id="Acknowledgements">
  <div>
    <h2>Acknowledgements</h2><p>
    This research is supported by the Monash FIT Start-up Grant. Dr. Chuanxia Zheng is supported by EPSRC SYN3D EP/Z001811/1.
  </p></div>
</section>


<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@article{chen2024mvsplat,
    title   = {MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images},
    author  = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
    journal = {arXiv preprint arXiv:2403.14627},
    year    = {2024},
}</code></pre>

  </div>
</section>







</div>
  </body>
</html>
