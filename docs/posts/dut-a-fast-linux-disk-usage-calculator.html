<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://codeberg.org/201984/dut">Original</a>
    <h1>Show HN: Dut – a fast Linux disk usage calculator</h1>
    
    <div id="readability-page-1" class="page"><div id="readme">

	

	
	<div>
		
		<div>
			
				
<h2 id="user-content-features" dir="auto">Features</h2>
<ul dir="auto">
<li>Accurate counting of hard links with an output inspired by NCDU.</li>
<li>Pure ASCII output which is fully compatible with the plain Linux tty.</li>
<li>Configurable output format. Changing the maximum depth of files shown is a simple command-line
argument.</li>
</ul>
<h2 id="user-content-examples" dir="auto">Examples</h2>
<p dir="auto">By default, <code>dut</code> will output a tree of the biggest directories it finds under your current directory.</p>
<pre><code>$ dut -n 10
 2.4G    0B       /- Local
 2.4G    0B     /- AppData
 2.4G    0B   /- NetworkService
 2.4G    0B |- ServiceProfiles
 2.5G   63M |- servicing
 5.2G  423M |   /- FileRepository
 5.2G  426M | /- DriverStore
 9.6G  2.5G |- System32
  12G  7.2G /- WinSxS
  29G  225M .
</code></pre><p dir="auto">The <code>-n 10</code> option limits it to 10 rows. To limit the depth shown, use <code>-d &lt;n&gt;</code>.</p>
<pre><code>$ dut -n 10 -d 1
 964M    0B |- MEMORY.DMP
1010M    0B |- SoftwareDistribution
 1.2G  1.0G |- SysWOW64
 1.3G  208M |- assembly
 1.8G  1.8G |- SystemApps
 2.4G    0B |- ServiceProfiles
 2.5G   63M |- servicing
 9.6G  2.5G |- System32
  12G  7.2G /- WinSxS
  29G  225M .
</code></pre><p dir="auto">The first column in the output tells you how much space a given entry takes up on your disk.
This can be an overcount, however, because of hard links (identical files that are only stored
once on the disk). Hard links under a directory are deduplicated in the first column&#39;s number,
but hard links that go outside of a directory to somewhere else will still be counted here.</p>
<p dir="auto">That&#39;s where the second column comes in. It tells you how much of an entry&#39;s size is shared
with other entries outside of it because of hard links. In the output above, we can see that
most of the entries have a lot of data shared with other entries, but the root directory only
has 225M shared with others. This tells us that there&#39;s a lot of hard links going between all
of the entries shown above.</p>
<p dir="auto">If you want to see how much of an entry&#39;s size is unique to just it, you can subtract the second
column from the first one.</p>
<p dir="auto">The full list of options can be seen with <code>dut -h</code>.</p>
<h2 id="user-content-how-to-build" dir="auto">How to build</h2>
<p dir="auto"><code>dut</code> is a single source file, so all you need is a C11 compiler:</p>
<pre><code>gcc -O3 -flto main.c -o dut
</code></pre><p dir="auto">To install, copy the <code>dut</code> executable to a directory on your PATH, e.g. <code>/usr/local/bin</code>.</p>
<p dir="auto">If get linker errors about missing symbols, you may need to add the <code>-pthread</code> option to gcc.</p>
<h2 id="user-content-benchmarks" dir="auto">Benchmarks</h2>
<p dir="auto"><code>dut</code> is remarkably fast, but it doesn&#39;t win in all cases. It loses to a couple programs when
Linux&#39;s disk caches aren&#39;t populated yet, which is usually the first time you run it on a certain
directory. On subsequent runs, <code>dut</code> beats everything else by a significant margin.</p>
<p dir="auto">Benchmarked programs:</p>
<ul dir="auto">
<li><code>du</code> from <a href="https://www.gnu.org/software/coreutils/coreutils.html" rel="nofollow">coreutils</a> (C)</li>
<li><a href="https://github.com/byron/dua-cli" rel="nofollow"><code>dua</code></a> (Rust)</li>
<li><a href="https://github.com/KSXGitHub/parallel-disk-usage" rel="nofollow"><code>pdu</code></a> (Rust)</li>
<li><a href="https://github.com/bootandy/dust" rel="nofollow"><code>dust</code></a> (Rust)</li>
<li><a href="https://github.com/dundee/gdu" rel="nofollow"><code>gdu</code></a> (Go)</li>
</ul>
<p dir="auto">If you know of a faster program, let me know and I&#39;ll add it to these benchmarks.</p>
<h3 id="user-content-benchmark-1-measuring-performance-from-linux-s-disk-cache" dir="auto">Benchmark 1: Measuring performance from Linux&#39;s disk cache</h3>
<p dir="auto">The first benchmark is calculating the total disk usage of both of the SSDs in my laptop. I did
warm-up runs beforehand to make sure everything is cached, so this benchmark doesn&#39;t touch the disk
at all.</p>
<h4 id="user-content-specs" dir="auto">Specs</h4>
<ul dir="auto">
<li>CPU: i5-10500h</li>
<li>RAM: 16 GB</li>
<li>OS: Arch Linux, kernel 6.8.4</li>
</ul>
<p dir="auto">In order to make things fair, I forced <code>dut</code> and <code>dust</code> to output in color and show 60 rows. I also
added a 10 second sleep between each program&#39;s run to limit the effects of thermal throttling.</p>
<pre><code>$ hyperfine &#39;dut -Cn 60 /&#39; &#39;du -sh /&#39; &#39;pdu /&#39; &#39;dust -n 60 /&#39; &#39;gdu --non-interactive /&#39; &#39;dua /&#39; -s &#39;sleep 10&#39; -i
Benchmark 1: dut -Cn 60 /
  Time (mean ± σ):     467.4 ms ±  11.7 ms    [User: 410.3 ms, System: 4595.4 ms]
  Range (min … max):   442.5 ms … 485.4 ms    10 runs

Benchmark 2: du -sh /
  Time (mean ± σ):      3.566 s ±  0.049 s    [User: 0.775 s, System: 2.743 s]
  Range (min … max):    3.486 s …  3.615 s    10 runs

  Warning: Ignoring non-zero exit code.

Benchmark 3: pdu /
  Time (mean ± σ):     732.1 ms ±  13.8 ms    [User: 1887.3 ms, System: 6123.5 ms]
  Range (min … max):   717.6 ms … 755.8 ms    10 runs

Benchmark 4: dust -n 60 /
  Time (mean ± σ):      1.438 s ±  0.031 s    [User: 3.068 s, System: 6.962 s]
  Range (min … max):    1.397 s …  1.481 s    10 runs

Benchmark 5: gdu --non-interactive /
  Time (mean ± σ):      1.361 s ±  0.103 s    [User: 7.556 s, System: 7.034 s]
  Range (min … max):    1.298 s …  1.569 s    10 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet system without any interferences from other programs. It might help to use the &#39;--warmup&#39; or &#39;--prepare&#39; options.

Benchmark 6: dua /
  Time (mean ± σ):      1.459 s ±  0.133 s    [User: 4.054 s, System: 9.640 s]
  Range (min … max):    1.346 s …  1.659 s    10 runs

Summary
  dut -Cn 60 / ran
    1.57 ± 0.05 times faster than pdu /
    2.91 ± 0.23 times faster than gdu --non-interactive /
    3.08 ± 0.10 times faster than dust -n 60 /
    3.12 ± 0.30 times faster than dua /
    7.63 ± 0.22 times faster than du -sh /
</code></pre><p dir="auto">The warning about a non-zero exit code was due to <code>du</code> reporting an error for not being able
to access directories in /proc and /root.</p>
<h3 id="user-content-benchmark-2-ssd-performance" dir="auto">Benchmark 2: SSD Performance</h3>
<p dir="auto">This bechmark is operating on the same filesystem as above, except I&#39;m flushing the disk caches
in-between runs. This results in having to read all the data from the SSD each time instead of
getting it from RAM.</p>
<p dir="auto">This is a more niche use-case since most of the time <code>dut</code> will be running from the cache. It
only has to read from the disk on its first run in a particular directory.</p>
<h4 id="user-content-drives" dir="auto">Drives:</h4>
<ul dir="auto">
<li>Intel 660p 512G</li>
<li>SX8200PNP-512GT-S</li>
</ul>
<pre><code>$ sudo hyperfine &#39;dut -Cn 60 /&#39; &#39;du -sh /&#39; &#39;pdu /&#39; &#39;dust -n 60 /&#39; &#39;gdu --non-interactive /&#39; &#39;dua /&#39; -s &#39;sleep 10&#39; -i -M 3 -p &#39;echo 1 &gt; /proc/sys/vm/drop_caches&#39;
Benchmark 1: dut -Cn 60 /
  Time (mean ± σ):      5.773 s ±  0.184 s    [User: 0.406 s, System: 4.694 s]
  Range (min … max):    5.561 s …  5.881 s    3 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet system without any interferences from other programs. It might help to use the &#39;--warmup&#39; or &#39;--prepare&#39; options.

Benchmark 2: du -sh /
  Time (mean ± σ):     20.779 s ±  0.058 s    [User: 0.767 s, System: 3.709 s]
  Range (min … max):   20.712 s … 20.819 s    3 runs

  Warning: Ignoring non-zero exit code.

Benchmark 3: pdu /
  Time (mean ± σ):      4.279 s ±  0.292 s    [User: 1.701 s, System: 5.543 s]
  Range (min … max):    4.072 s …  4.613 s    3 runs

Benchmark 4: dust -n 60 /
  Time (mean ± σ):      5.009 s ±  0.348 s    [User: 2.608 s, System: 6.211 s]
  Range (min … max):    4.726 s …  5.397 s    3 runs

Benchmark 5: gdu --non-interactive /
  Time (mean ± σ):      4.090 s ±  0.081 s    [User: 7.027 s, System: 6.989 s]
  Range (min … max):    4.040 s …  4.183 s    3 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet system without any interferences from other programs. It might help to use the &#39;--warmup&#39; or &#39;--prepare&#39; options.

Benchmark 6: dua /
  Time (mean ± σ):      6.269 s ±  0.133 s    [User: 4.541 s, System: 12.786 s]
  Range (min … max):    6.162 s …  6.418 s    3 runs

Summary
  gdu --non-interactive / ran
    1.05 ± 0.07 times faster than pdu /
    1.22 ± 0.09 times faster than dust -n 60 /
    1.41 ± 0.05 times faster than dut -Cn 60 /
    1.53 ± 0.04 times faster than dua /
    5.08 ± 0.10 times faster than du -sh /
</code></pre><h3 id="user-content-benchmark-3-hdd-performance" dir="auto">Benchmark 3: HDD Performance</h3>
<p dir="auto">For this benchmark, I did the same benchmark as the last except I did it on an HDD instead. Some of
the Rust programs perform quite badly in this scenario, but <code>dua</code> still beats <code>dut</code> narrowly.</p>
<p dir="auto">The test location is my home directory on an old Linux installation. There are approximately 26k
subdirectories.</p>
<p dir="auto">The drive being measured is a 2 terabyte 5400rpm Western Digital WD20EFRX connected to my laptop
with a USB enclosure.</p>
<pre><code>$ sudo hyperfine &#39;dut -Cn 60&#39; &#39;du -sh&#39; &#39;pdu&#39; &#39;dust -n 60&#39; &#39;gdu --non-interactive&#39; &#39;dua&#39; -s &#39;sleep 10&#39; -i -M 3 -p &#39;echo 1 &gt; /proc/sys/vm/drop_caches&#39;
Benchmark 1: dut -Cn 60
  Time (mean ± σ):     36.720 s ±  0.350 s    [User: 0.078 s, System: 0.740 s]
  Range (min … max):   36.411 s … 37.100 s    3 runs

Benchmark 2: du -sh
  Time (mean ± σ):     44.810 s ±  0.043 s    [User: 0.108 s, System: 0.657 s]
  Range (min … max):   44.767 s … 44.854 s    3 runs

Benchmark 3: pdu
  Time (mean ± σ):     81.361 s ±  0.954 s    [User: 0.320 s, System: 0.935 s]
  Range (min … max):   80.675 s … 82.451 s    3 runs

Benchmark 4: dust -n 60
  Time (mean ± σ):     86.991 s ±  2.449 s    [User: 0.337 s, System: 1.042 s]
  Range (min … max):   84.411 s … 89.286 s    3 runs

Benchmark 5: gdu --non-interactive
  Time (mean ± σ):     41.096 s ±  0.229 s    [User: 1.086 s, System: 1.165 s]
  Range (min … max):   40.837 s … 41.273 s    3 runs

Benchmark 6: dua
  Time (mean ± σ):     34.472 s ±  0.965 s    [User: 9.107 s, System: 29.192 s]
  Range (min … max):   33.733 s … 35.564 s    3 runs

Summary
  dua ran
    1.07 ± 0.03 times faster than dut -Cn 60
    1.19 ± 0.03 times faster than gdu --non-interactive
    1.30 ± 0.04 times faster than du -sh
    2.36 ± 0.07 times faster than pdu
    2.52 ± 0.10 times faster than dust -n 60
</code></pre><h4 id="user-content-why-are-pdu-and-dust-so-bad-on-hdd" dir="auto">Why are <code>pdu</code> and <code>dust</code> so bad on HDD?</h4>
<p dir="auto">It&#39;s hard to say. My best guess is they have a really HDD-unfriendly access pattern, since they
both use Rayon for multithreading which uses FIFO ordering for tasks. This results in them doing
a breadth-first search of the filesystem, whereas <code>dut</code> and <code>du</code> both use depth-first search. I
don&#39;t know why one ordering is better than the other, but the difference is pretty drastic.</p>
<p dir="auto">I also think that ordering is the reason <code>dut</code> doesn&#39;t do so well on SSD either, but I&#39;m not so
sure of that.</p>

			
		</div>
	</div>
</div></div>
  </body>
</html>
