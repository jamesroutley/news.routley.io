<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lock.cmpxchg8b.com/zenbleed.html">Original</a>
    <h1>Zenbleed</h1>
    
    <div id="readability-page-1" class="page">

<nav id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#vulnerability">Vulnerability</a></li>
<li><a href="#exploitation">Exploitation</a></li>
<li><a href="#discovery">Discovery</a></li>
<li><a href="#solution">Solution</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</nav>
<p>If you remove the first word from the string <code>&#34;hello world&#34;</code>, what should the result be? This is the story of how we discovered that the answer could be your root password!</p>
<section id="introduction">
<h2>Introduction</h2>
<p>All x86-64 CPUs have a set of 128-bit vector registers called the <code>XMM</code> registers. You can never have enough bits, so recent CPUs have extended the width of those registers up to 256-bit and even 512-bits.</p>
<p>The 256-bit extended registers are called <code>YMM</code>, and the 512-bit registers are <code>ZMM</code>.</p>
<p>These big registers are useful in lots of situations, not just number crunching! They‚Äôre even used by standard C library functions, like <code>strcmp</code>, <code>memcpy</code>, <code>strlen</code> and so on.</p>
<p>Let‚Äôs take a look at an example. Here are the first few instructions of glibc‚Äôs AVX2 optimized <code>strlen</code>:</p>
<pre><code>(gdb) x/20i __strlen_avx2
...
   &lt;__strlen_avx2+9&gt;:   vpxor  xmm0,xmm0,xmm0
...
   &lt;__strlen_avx2+29&gt;:  vpcmpeqb ymm1,ymm0,YMMWORD PTR [rdi]
   &lt;__strlen_avx2+33&gt;:  vpmovmskb eax,ymm1
...
   &lt;__strlen_avx2+41&gt;:  tzcnt  eax,eax
   &lt;__strlen_avx2+45&gt;:  vzeroupper
   &lt;__strlen_avx2+48&gt;:  ret</code></pre>
<p>The full routine is complicated and handles lots of cases, but let‚Äôs step through this simple case. Bear with me, I promise there‚Äôs a point!</p>
<p>The first step is to initialize <code>ymm0</code> to zero, which is done by just <code>xor</code>ing <code>xmm0</code> with itself<a href="#fn1" id="fnref1"><sup>1</sup></a>.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/zb-vpxor.svg" alt="VPXOR xmm0, xmm0, xmm0"/><figcaption>VPXOR xmm0, xmm0, xmm0</figcaption>
</figure>
<pre><code>    &gt; vpxor xmm0, xmm0, xmm0
      vpcmpeqb ymm1, ymm0, [rdi]
      vpmovmskb eax, ymm1
      tzcnt eax, eax
      vzeroupper</code></pre>
</blockquote>
<p>Here <code>rdi</code> contains a pointer to our string, so <code>vpcmpeqb</code> will check which bytes in <code>ymm0</code> match our string, and stores the result in <code>ymm1</code>.</p>
<p>As we‚Äôve already set <code>ymm0</code> to all zero bytes, only nul bytes will match.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/zb-vpcmpeqb.svg" alt="vpcmpeqb ymm1, ymm0, rdi"/><figcaption>vpcmpeqb ymm1, ymm0, rdi</figcaption>
</figure>
<pre><code>      vpxor xmm0, xmm0, xmm0
    &gt; vpcmpeqb ymm1, ymm0, [rdi]
      vpmovmskb eax, ymm1
      tzcnt eax, eax
      vzeroupper</code></pre>
</blockquote>
<p>Now we can extract the result into a general purpose register like <code>eax</code> with <code>vpmovmskb</code>.</p>
<p>Any nul byte will create a 1 bit, and any other value will create a 0 bit.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/zb-vpmovmskb.svg" alt="vpmovmskb eax, ymm1"/><figcaption>vpmovmskb eax, ymm1</figcaption>
</figure>
<pre><code>      vpxor xmm0, xmm0, xmm0
      vpcmpeqb ymm1, ymm0, [rdi]
    &gt; vpmovmskb eax, ymm1
      tzcnt eax, eax
      vzeroupper</code></pre>
</blockquote>
<p>Finding the first zero byte is now just a case of counting the number of trailing zero bits.</p>
<p>That‚Äôs a common enough operation that there‚Äôs an instruction for it - <code>tzcnt</code> (Trailing Zero Count).</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/zb-tzcnt.svg" alt="tzcnt eax, eax"/><figcaption>tzcnt eax, eax</figcaption>
</figure>
<pre><code>      vpxor xmm0, xmm0, xmm0
      vpcmpeqb ymm1, ymm0, [rdi]
      vpmovmskb eax, ymm1
    &gt; tzcnt eax, eax
      vzeroupper</code></pre>
</blockquote>
<p>Now we have the position of the first nul byte, in just four machine instructions!</p>
<p>You can probably imagine just how often <code>strlen</code> is running on your system right now, but suffice to say, bits and bytes are flowing into these vector registers from all over your system constantly.</p>
<section id="zeroing-registers">
<h4>Zeroing Registers</h4>
<p>You might have noticed that I missed one instruction, and that‚Äôs <code>vzeroupper</code>.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/zb-vzeroupper.svg" alt="vzeroupper"/><figcaption>vzeroupper</figcaption>
</figure>
<pre><code>      vpxor xmm0, xmm0, xmm0
      vpcmpeqb ymm1, ymm0, [rdi]
      vpmovmskb eax, ymm1
      tzcnt eax, eax
    &gt; vzeroupper</code></pre>
</blockquote>
<p>You guessed it, <code>vzeroupper</code> will zero the upper bits of the vector registers.</p>
<p>The reason we do this is because if you mix <code>XMM</code> and <code>YMM</code> registers, the <code>XMM</code> registers automatically get promoted to full width. It‚Äôs a bit like integer promotion in C.</p>
<p>This works fine, but <a href="https://en.wikipedia.org/wiki/Superscalar">superscalar</a> processors need to track dependencies so that they know which operations can be parallelized. This promotion adds a dependency on those upper bits, and that causes unnecessary stalls while the processor waits for results it didn‚Äôt really need.</p>
<p>These stalls are what glibc is trying to avoid with <code>vzeroupper</code>. Now any future results won‚Äôt depend on what those bits are, so we safely avoid that bottleneck!</p>
</section>
<section id="the-vector-register-file">
<h4>The Vector Register File</h4>
<p>Now that we know <em>what</em> <code>vzeroupper</code> does, <em>how</em> does it do it?</p>
<p>Your processor doesn‚Äôt have a single physical location where each register lives, it has what‚Äôs called a <em><a href="https://en.wikipedia.org/wiki/Register_file">Register File</a></em> and a <em>Register Allocation Table</em>. This is a bit like managing the heap with <code>malloc</code> and <code>free</code>, if you think of each register as a pointer. The RAT keeps track of what space in the register file is assigned to which register.</p>
<p>In fact, when you zero an <code>XMM</code> register, the processor doesn‚Äôt store those bits anywhere at all - it just sets a flag called the <a href="https://en.wikichip.org/wiki/amd/microarchitectures/zen_2#Floating_Point_Unit">z-bit</a> in the RAT. This flag can be applied to the upper and lower parts of <code>YMM</code> registers independently, so <code>vzeroupper</code> can simply set the z-bit and then release any resources assigned to it in the register file.</p>
<figure>
<blockquote>
<img src="https://lock.cmpxchg8b.com/img/zb-zbit.svg" alt="Z-Bit"/>
<figcaption>
A register allocation table (left) and a physical register file (right).
</figcaption>
</blockquote>
</figure>
</section>
<section id="speculation">
<h4>Speculation</h4>
<p>Hold on, there‚Äôs another complication! Modern processors use <a href="https://en.wikipedia.org/wiki/Speculative_execution">speculative execution</a>, so sometimes operations have to be rolled back.</p>
<p>What should happen if the processor <em>speculatively</em> executed a <code>vzeroupper</code>, but then discovers that there was a branch misprediction? Well, we will have to revert that operation and put things back the way they were‚Ä¶ maybe we can just unset that z-bit?</p>
<p>If we return to the analogy of <code>malloc</code> and <code>free</code>, you can see that it can‚Äôt be that simple - that would be like calling <code>free()</code> on a pointer, and then changing your mind!</p>
<p>That would be a use-after-free vulnerability, but there is no such thing as a use-after-free in a CPU‚Ä¶ or is there?</p>
<p>Spoiler: <strong><em>yes there is</em></strong> üôÇ</p>
<figure>
<blockquote>
<img src="https://lock.cmpxchg8b.com/img/zb-leaking.gif" alt="Zenbleed Demo"/>
<figcaption>
This animation shows why resetting the z-bit is not sufficient.
</figcaption>
</blockquote>
</figure>
</section>
</section>
<section id="vulnerability">
<h2>Vulnerability</h2>
<p>It turns out that with precise scheduling, you <em>can</em> cause some processors to recover from a mispredicted <code>vzeroupper</code> incorrectly!</p>
<p>This technique is CVE-2023-20593 and it works on all Zen 2 class processors, which includes at least the following products:</p>
<ul>
<li>AMD Ryzen 3000 Series Processors</li>
<li>AMD Ryzen PRO 3000 Series Processors</li>
<li>AMD Ryzen Threadripper 3000 Series Processors</li>
<li>AMD Ryzen 4000 Series Processors with Radeon Graphics</li>
<li>AMD Ryzen PRO 4000 Series Processors</li>
<li>AMD Ryzen 5000 Series Processors with Radeon Graphics</li>
<li>AMD Ryzen 7020 Series Processors with Radeon Graphics</li>
<li>AMD EPYC ‚ÄúRome‚Äù Processors</li>
</ul>
<p>The bug works like this, first of all you need to trigger something called the <em>XMM Register Merge Optimization</em><a href="#fn2" id="fnref2"><sup>2</sup></a>, followed by a <a href="https://en.wikipedia.org/wiki/Register_renaming">register rename</a> and a mispredicted <code>vzeroupper</code>. This all has to happen within a precise window to work.</p>
<p>We now know that basic operations like <code>strlen</code>, <code>memcpy</code> and <code>strcmp</code> will use the vector registers - so we can effectively spy on those operations happening anywhere on the system! It doesn‚Äôt matter if they‚Äôre happening in other virtual machines, sandboxes, containers, processes, whatever!</p>
<p>This works because the register file is shared by everything on the same physical core. In fact, two hyperthreads even share the same physical register file.</p>
<p>Don‚Äôt believe me? Let‚Äôs write an exploit üôÇ</p>
</section>
<section id="exploitation">
<h2>Exploitation</h2>
<p>There are quite a few ways to trigger this, but let‚Äôs examine a very simple example.</p>
<pre><code>    vcvtsi2s{s,d}   xmm, xmm, r64
    vmovdqa         ymm, ymm
    jcc             overzero
    vzeroupper
overzero:
    nop</code></pre>
<p>Here <code>cvtsi2sd</code> is used to trigger the merge optimization. It‚Äôs not important what <code>cvtsi2sd</code> is <em>supposed</em> to do, I‚Äôm just using it because it‚Äôs one of the instructions the manual says use that optimization<a href="#fn3" id="fnref3"><sup>3</sup></a>.</p>
<p>Then we need to trigger a <a href="https://en.wikipedia.org/wiki/Register_renaming">register rename</a>, <code>vmovdqa</code> will work. If the conditional branch<a href="#fn4" id="fnref4"><sup>4</sup></a> is taken but the CPU predicts the not-taken path, the <code>vzeroupper</code> will be mispredicted and the bug occurs!</p>
<section id="optimization">
<h4>Optimization</h4>
<figure>
<img src="https://lock.cmpxchg8b.com/img/zb-exploit.gif" alt="Exploit Running"/><figcaption>Exploit Running</figcaption>
</figure>
<p>It turns out that mispredicting on purpose is difficult to optimize! It took a bit of work, but I found a variant that can leak about 30 kb per core, per second.</p>
<p>This is fast enough to monitor encryption keys and passwords as users login!</p>
<p>We‚Äôre releasing our full technical advisory, along with all the associated code today. Full details will be available in our <a href="https://github.com/google/security-research/tree/master/pocs/cpus">security research repository</a>.</p>
<p>If you want to test the exploit, the code is available <a href="https://lock.cmpxchg8b.com/files/zenbleed-v5.tar.gz">here</a>.</p>
<p>Note that the code is for Linux, but the bug is not dependent on any particular operating system - all operating systems are affected!</p>
</section>
</section>
<section id="discovery">
<h2>Discovery</h2>
<p>I found this bug by fuzzing, big surprise üôÇ I‚Äôm not the first person to apply fuzzing techniques to finding hardware flaws. In fact, vendors fuzz their own products extensively - the industry term for it is <em>Post-Silicon Validation</em>.</p>
<p>So how come this bug wasn‚Äôt found earlier? I think I did a couple of things differently, perhaps with a new perspective as I don‚Äôt have an EE background!</p>
<section id="feedback">
<h4>Feedback</h4>
<p>The best performing fuzzers are guided by coverage <a href="https://google.github.io/clusterfuzz/reference/coverage-guided-vs-blackbox/">feedback</a>. The problem is that there is nothing really analogous to code coverage in CPUs‚Ä¶ However, we do have <a href="https://en.wikipedia.org/wiki/Hardware_performance_counter">performance counters</a>!</p>
<p>These will let us know when all kinds of <a href="https://perfmon-events.intel.com/skylake.html">interesting architectural events</a> happen.</p>
<p>Feeding this data to the fuzzer lets us gently guide it towards exploring interesting features that we wouldn‚Äôt have been able to find by chance alone!</p>
<p>It was challenging to get the details right, but I used this to teach my fuzzer to find interesting instruction sequences. This allowed me to discover features like merge optimization automatically, without any input from me!</p>
</section>
<section id="oracle">
<h4>Oracle</h4>
<p>When we fuzz software, we‚Äôre usually looking for crashes. Software isn‚Äôt supposed to crash, so we know something must have gone wrong if it does.</p>
<p>How can we know if a a CPU is executing a randomly generated program correctly? It might be completely correct for it to crash!</p>
<p>Well, a few solutions have been proposed to this problem. One approach is called <a href="https://ieeexplore.ieee.org/abstract/document/4751878">reversi</a>. The general idea is that for every random instruction you generate, you also generate the inverse (e.g.¬†<code>ADD r1, r2</code> ‚Üí <code>SUB r1, r2</code>). Any deviation from the initial state at the end of execution must have been an error, neat!</p>
<p>The reversi approach is clever, but it makes generating testcases very complicated for a CISC architecture like x86.</p>
<p>A simpler solution is to use an <em>oracle</em>. An oracle is just another CPU or a simulator that we can use to check the result. If we compare the results from our test CPU to our oracle CPU, any mismatch would suggest that something went wrong.</p>
<p>I developed a new approach with a combination of these two ideas, I call it <em>Oracle Serialization</em>.</p>
</section>
<section id="oracle-serialization">
<h4>Oracle Serialization</h4>
<p>As developers we monitor the <em>macro-architectural state</em>, that‚Äôs just things like register values. There is also the <em>micro-architectural state</em> which is mostly invisible to us, like the branch predictor, <a href="https://en.wikipedia.org/wiki/Out-of-order_execution">out-of-order execution</a> state and the instruction <a href="https://en.wikipedia.org/wiki/Instruction_pipelining">pipeline</a>.</p>
<p>Serialization lets us have <em>some</em> control over that, by instructing the CPU to reset <a href="https://en.wikipedia.org/wiki/Instruction-level_parallelism">instruction-level parallelism</a>. This includes things like store/load barriers, speculation fences, cache line flushes, and so on.</p>
<p>The idea of a Serialized Oracle is to generate a random program, then automatically transform it into a <a href="https://www.felixcloutier.com/x86/serialize#description">serialized</a> form.</p>
<hr/>
<div>
<table>
<caption>A randomly generated sequence of instructions, and the same sequence but with randomized alignment, serialization and speculation fences added.</caption>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><code>movnti [rbp+0x0],ebx</code></td>
<td><code>movnti [rbp+0x0],ebx</code></td>
</tr>
<tr>
<td></td>
<td><code color="green">sfence</code></td>
</tr>
<tr>
<td><code>rcr dh,1</code></td>
<td><code>rcr dh,1</code></td>
</tr>
<tr>
<td></td>
<td><code color="green">lfence</code></td>
</tr>
<tr>
<td><code>sub r10, rax</code></td>
<td><code>sub r10, rax</code></td>
</tr>
<tr>
<td></td>
<td><code color="green">mfence</code></td>
</tr>
<tr>
<td><code>rol rbx, cl</code></td>
<td><code>rol rbx, cl</code></td>
</tr>
<tr>
<td></td>
<td><code color="green">nop</code></td>
</tr>
<tr>
<td><code>xor edi,[rbp-0x57]</code></td>
<td><code>xor edi,[rbp-0x57]</code></td>
</tr>
</tbody>
</table>
</div>
<hr/>
<p>These two program might have very different performance characteristics, but they should produce identical output. The serialized form can now be my oracle!</p>
<p>If the final states don‚Äôt match, then there must have been some error in how they were executed micro-architecturally - that could indicate a bug.</p>
<p>This is exactly how we first discovered this vulnerability, the output of the serialized oracle didn‚Äôt match!</p>
</section>
</section>
<section id="solution">
<h2>Solution</h2>
<p>We reported this vulnerability to AMD on the 15th May 2023.</p>
<p>AMD have <a href="https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/commit/?id=0bc3126c9cfa0b8c761483215c25382f831a7c6f">released</a> an microcode update for affected processors. Your BIOS or Operating System vendor may already have an update available that includes it.</p>
<section id="workaround">
<h4>Workaround</h4>
<p>It is highly recommended to use the microcode update.</p>
<p>If you can‚Äôt apply the update for some reason, there is a software workaround: you can set the <a href="https://en.wiktionary.org/wiki/chicken_bit">chicken bit</a> <code>DE_CFG[9]</code>.</p>
<p>This may have some performance cost.</p>
<section id="linux">
<h5>Linux</h5>
<p>You can use <code>msr-tools</code> to set the chicken bit on all cores, like this:</p>
<pre><code># wrmsr -a 0xc0011029 $(($(rdmsr -c 0xc0011029) | (1&lt;&lt;9)))</code></pre>
</section>
<section id="freebsd">
<h5>FreeBSD</h5>
<p>On FreeBSD you would use <code>cpucontrol(8)</code>.</p>
</section>
<section id="others">
<h5>Others</h5>
<p>If you‚Äôre using some other operating system and don‚Äôt know how to set <abbr title="Model Specific Registers">MSRs</abbr>, ask your vendor for assistance.</p>
<p>Note that it is <em>not</em> sufficient to disable <abbr title="Simultaneous
Multithreading">SMT</abbr>.</p>
</section>
</section>
<section id="detection">
<h4>Detection</h4>
<p>I am not aware of any reliable techniques to detect exploitation. This is because no special system calls or privileges are required.</p>
<p>It is definitely <em>not</em> possible to detect improper usage of <code>vzeroupper</code> statically, please don‚Äôt try!</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion</h2>
<p>It turns out that memory management is hard, even in silicon üôÇ</p>
<section id="acknowledgements">
<h4>Acknowledgements</h4>
<p>This bug was discovered by me, Tavis Ormandy from Google Information Security!</p>
<p>I couldn‚Äôt have found it without help from my colleagues, in particular Eduardo Vela Nava and Alexandra Sandulescu. I also had help analyzing the bug from Josh Eads.</p>
</section>
</section>




</div>
  </body>
</html>
