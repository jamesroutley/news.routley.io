<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/">Original</a>
    <h1>Raspberry Pi&#39;s New AI Hat Adds 8GB of RAM for Local LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><div><section><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/raspberry-pi-ai-hat-2.jpg" alt="Raspberry Pi AI HAT+ 2" width="700" height="auto"/></figure><p>Today Raspberry Pi launched their new <a href="https://www.raspberrypi.com/products/ai-hat-plus-2/">$130 AI HAT+ 2</a> which includes a Hailo 10H and 8 GB of <a href="https://www.micron.com/products/memory/dram-components/lpddr4/part-catalog/part-detail/mt53e2g32d4de-046-wt-c">LPDDR4X RAM</a>.</p><p>With that, the Hailo 10H is capable of running LLMs entirely standalone, freeing the Pi&#39;s CPU and system RAM for other tasks. The chip runs at a maximum of 3W, with 40 TOPS of INT8 NPU inference performance in addition to the equivalent 26 TOPS INT4 machine vision performance on the earlier AI HAT with Hailo 8.</p><p>In practice, it&#39;s not as amazing as it sounds.</p><p>You still can&#39;t upgrade the RAM on the Pi, but at least this way if you <em>do</em> have a need for an AI coprocessor, you don&#39;t have to eat up the Pi&#39;s memory to run things on it.</p><p>And it&#39;s a lot cheaper and more compact than <a href="https://www.jeffgeerling.com/blog/2025/big-gpus-dont-need-big-pcs/">running an eGPU on a Pi</a>. In that sense, it&#39;s more useful than the silly NPUs Microsoft forces into their &#39;AI PCs&#39;.</p><p>But it&#39;s still a solution in search of a problem, in all but the most niche of use cases.</p><p>Besides feeling like I&#39;m living in the world of the <a href="https://www.youtube.com/watch?v=Ac7G7xOG2Ag">Turbo Encabulator</a> every time I&#39;m testing AI hardware, I find the marketing of these things to be very vague, and the applications not very broad.</p><p>For example, the Hailo 10H is advertised as being used for a <a href="https://www.youtube.com/watch?v=flD-WfJ4pUg">Fujitsu demo of automatic shrink detection for a self-checkout</a>.</p><p>That&#39;s certainly not a worthless use case, but it&#39;s not something I&#39;ve ever needed to do. I have a feeling this board is meant more for development, for people who want to deploy the 10H in other devices, rather than as a total solution to problems individual Pi owners need to solve.</p><p>Especially when it comes to the headline feature: running inference, like with LLMs.</p><h2 id="video">Video</h2><p>I also published a video with all the information in this blog post, but if you enjoy text more than video, scroll on past—it doesn&#39;t offend me!</p><div><p><iframe src="https://www.youtube.com/embed/jRQaur0LdLE" frameborder="0" allowfullscreen=""></iframe></p></div><h2 id="llm-performance-on-the-ai-hat-2">LLM performance on the AI HAT+ 2</h2><p>I ran everything on an 8 gig Pi 5, so I could get an apples-to-apples comparison, running the same models on the Pi&#39;s CPU as I did on the AI HAT&#39;s NPU.</p><p>They both have the same 8GB LPDDR4X RAM configuration, so <em>ideally</em>, they&#39;d have similar performance.</p><p>I tested every model Hailo put out so far, and compared them, Pi 5 versus Hailo 10H:</p><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/pi-ai-hat-2-llm-compare-inference.jpg" alt="Raspberry Pi AI HAT+ 2 - Inference performance NPU vs CPU" width="700" height="auto"/></figure><p>The Pi&#39;s built-in CPU trounces the Hailo 10H.</p><p>The Hailo is only close, really, on Qwen2.5 Coder 1.5B.</p><p>It <em>is</em> slightly more efficient in most cases:</p><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/pi-ai-hat-2-llm-compare-efficiency.jpg" alt="Raspberry Pi AI HAT+ 2 - Inference efficiency NPU vs CPU" width="700" height="auto"/></figure><p>But looking more closely at power draw, we can see why the Hailo doesn&#39;t keep up:</p><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/pi-ai-hat-2-power-draw-compare-llm.jpg" alt="Raspberry Pi AI HAT+ 2 - Power draw NPU vs CPU" width="700" height="auto"/></figure><p>The Pi&#39;s CPU is allowed to max out it&#39;s power limits (10W on the SoC), which are a lot higher than the Hailo&#39;s (3W).</p><h2 id="qwen-30b-on-a-pi">Qwen 30B on a Pi</h2><p>So power holds it back, but the 8 gigs of RAM holds back the LLM use case (vs just running on the Pi&#39;s CPU) the most. The Pi 5 can be bought in up to a <em>16 GB</em> configuration. That&#39;s as much as you get in decent consumer graphics cards<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p><p>Because of that, many quantized medium-size models target 10-12 GB of RAM usage (leaving space for context, which eats up another 2+ GB of RAM).</p><p>A couple weeks ago, <a href="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/">ByteShape got Qwen3 30B A3B Instruct to fit on a 16GB Pi 5</a>. Now this post isn&#39;t about LLMs, but the short of it is they found a novel way to compress the model to fit in 10 GB of RAM.</p><p>A little bit of quality is lost, but like a JPEG, it&#39;s still good enough to ace all the contrived tests (like building a TODO list app, or sorting a complex list) that the tiny models I ran on the Hailo 10H didn&#39;t complete well (see the video earlier in this post for details).</p><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/llama-cpp-pi-5-qwen3-30b-a3b-instruct.jpg" alt="Raspberry Pi 16GB running Qwen3 30B model" width="700" height="auto"/></figure><p>To test the 30B model, I <a href="https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5/">installed llama.cpp following this guide from my blog</a>, and <a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/">downloaded the compressed model</a>.</p><p>I asked it to generate a single page TODO list app, and it&#39;s still not a speed demon (this is a Pi CPU with LPDDR4x RAM we&#39;re talking about), but after a little while, it gave me this:</p><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/pi-ai-16gb-qwen3-30b-todo-list-app.jpg" alt="Raspberry Pi 16GB Qwen3 Generated TODO list app" width="700" height="auto"/></figure><p>It met all my requirements:</p><ul><li>I can type in as many items as I want</li><li>I can drag them around to rearrange them</li><li>I can check off items and they go to the bottom of the list...</li></ul><p>It&#39;s honestly crazy how many small tasks you can do even with free local models... even on a Pi. <a href="https://en.wikipedia.org/wiki/Natural_language_programming">Natural Language Programming</a> was just a dream back when I started my career.</p><p>Besides being angry Google, OpenAI, Anthropic and all these other companies are <a href="https://am.jpmorgan.com/us/en/asset-management/adv/insights/market-insights/market-updates/on-the-minds-of-investors/is-ai-already-driving-us-growth/">consuming all the world&#39;s money and resources</a> doing this stuff—not to mention <a href="https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html">destroying the careers of thousands of junior developers</a>—it is kinda neat to see NLP work for very tightly defined examples.</p><h2 id="benchmarking-computer-vision">Benchmarking computer vision</h2><p>But I don&#39;t think this HAT is the best choice to run local, private LLMs (at least not as a primary goal).</p><p>What it <em>is</em> good for, is vision processing. But the original AI HAT was good for that too!</p><p>In my testing, Hailo&#39;s <a href="https://github.com/hailo-ai/hailo-rpi5-examples">hailo-rpi5-examples</a> were not yet updated for this new HAT, and even if I specified the Hailo 10H manually, model files would not load, or I ran into errors once the board was detected.</p><p>But Raspberry Pi&#39;s models ran, so I tested them with a Camera Module 3:</p><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/pi-ai-hat-vision-30fps-yolo.jpg" alt="Raspberry Pi AI HAT+ 2 running YOLO vision model at 30fps" width="700" height="auto"/></figure><p>I pointed it over at my desk, and it was able to pick out things like my keyboard, my monitor (which it thought was a TV), my phone, and even the mouse tucked away in the back.</p><p>It all ran quite fast—and 10x faster than on the Pi&#39;s CPU—but the problem is I can do the same thing with the <em>original</em> AI HAT ($110)—or the <a href="https://www.raspberrypi.com/products/ai-camera/">AI Camera</a> ($70).</p><p>If you <em>just</em> need vision processing, I would stick with one of those.</p><p>The headline feature of the AI HAT+ 2 is the ability to run in a &#39;mixed&#39; mode, where it can process machine vision (frames from a camera or video feed), while also running inference (like an LLM or text-to-speech).</p><figure><img src="https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/pi-ai-hat-mixed-vision-llm-no-work.jpg" alt="Raspberry Pi AI HAT+ 2 mixed inference and vision not working" width="700" height="auto"/></figure><p>Unfortunately, when I tried running two models simultaneously, I ran into segmentation faults or &#39;device not ready&#39;, and lacking any working examples from Hailo, I had to give up on getting that working in time for this post.</p><p>Just like the original AI HAT, there&#39;s some growing pains.</p><p>It seems like with most hardware with &#34;AI&#34; in the name, it&#39;s hardware-first, then software comes later—if it comes at all. At least with Raspberry Pi&#39;s track record, the software <em>does</em> come, it&#39;s just... often the solutions are only useful in tiny niche use cases.</p><h2 id="conclusion">Conclusion</h2><p>8 GB of RAM is useful, but it&#39;s not quite enough to give this HAT an advantage over just paying for the bigger 16GB Pi with more RAM, which will be more flexible and run models faster.</p><p>The main use case for this HAT might be in power-constrained applications where you need both vision processing <em>and</em> inferencing. But even there... it&#39;s hard to say &#34;yes, buy this thing&#34;, because for just a few more watts, the Pi could achieve better performance for inference in tandem with the $70 <a href="https://www.raspberrypi.com/products/ai-camera/">AI Camera</a> or the $110 <a href="https://www.microcenter.com/product/687346/product?src=raspberrypi">AI HAT+</a> for the vision processing.</p><p>Outside of running tiny LLMs in less than 10 watts, maybe the idea is you use the AI HAT+ 2 as a development kit for designing devices using the 10H like self-checkout scanners (which might not even run on a Pi)? I&#39;m not sure.</p></section></div></div></article></div></div>
  </body>
</html>
