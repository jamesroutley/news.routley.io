<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/potahtml/mpa-archive">Original</a>
    <h1>Show HN: Crawl a modern website to a zip, serve the website from the zip</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Crawls a Multi-Page Application into a zip file. Serve the Multi-Page
Application from the zip file. A MPA archiver. Could be used as a Site
Generator.</p>

<p dir="auto"><code>npm install -g mpa-archive</code></p>


<p dir="auto"><code>mpa http://example.net</code></p>
<p dir="auto">Will crawl the url recursively and save it in <code>example.net.zip</code>. Once
done, it will display a report and can serve the files from the zip.</p>

<p dir="auto"><code>mpa</code></p>
<p dir="auto">Will create a server for each zip file on the current directory. Host
is <code>localhost</code> with a <code>port</code> seeded to the zip file path.</p>

<ul dir="auto">
<li>It uses headless puppeteer</li>
<li>Crawls <code>http://example.net</code> with <code>cpu count / 2</code> threads</li>
<li>Progress is displayed in the console</li>
<li>Fetches <code>sitemap.txt</code> and <code>sitemap.xml</code> as a seed point</li>
<li>Reports HTTP status codes different than 200, 304, 204, 206</li>
<li>Crawls on site urls only but will <code>fetch</code> external resources</li>
<li>Intercepts site resources and saves that too</li>
<li>Generates <code>mpa/sitemap.txt</code> and <code>mpa/sitemap.xml</code></li>
<li>Saves site sourcemaps</li>
<li>Can resume if process exit, save checkpoint every 250 urls</li>
</ul>

<ul dir="auto">
<li>save it in an incremental compression format, that doesnt require
re-compressing the whole file when it changes, maybe already does
that?</li>
<li>urls to externals resources are not re-written to be local
resources, if this is done then stuff loaded from the root will
break</li>
<li>it should crawl the site by clicking the links instead of opening a
full tab</li>
</ul>
</article></div></div>
  </body>
</html>
