<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd">Original</a>
    <h1>Noisy neighbor detection with eBPF</h1>
    
    <div id="readability-page-1" class="page"><article><div><div><section><div><div><div><div><div><div><div><div><div><div><div><a href="https://netflixtechblog.medium.com" rel="noopener follow"><div><div aria-hidden="false"><div><div><p><img alt="Netflix Technology Blog" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"/></p></div></div></div></div></a><a href="https://netflixtechblog.com" rel="noopener  ugc nofollow"><div><div><div aria-hidden="false"><div><div><p><img alt="Netflix TechBlog" src="https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"/></p></div></div></div></div></div></a></div></div></div></div></div></div></div><p id="3108"><em>By </em><a href="https://www.linkedin.com/in/josefernandezmn/" rel="noopener ugc nofollow" target="_blank"><em>Jose Fernandez</em></a><em>, </em><a href="https://www.linkedin.com/in/sebastien-dabdoub-2a5a0958/" rel="noopener ugc nofollow" target="_blank"><em>Sebastien Dabdoub</em></a><em>, </em><a href="https://www.linkedin.com/in/jason-koch-5692172/" rel="noopener ugc nofollow" target="_blank"><em>Jason Koch</em></a><em>, </em><a href="https://www.linkedin.com/in/artemtkachuk/" rel="noopener ugc nofollow" target="_blank"><em>Artem Tkachuk</em></a></p><p id="8709">The Compute and Performance Engineering teams at Netflix regularly investigate performance issues in our multi-tenant environment. The first step is determining whether the problem originates from the application or the underlying infrastructure. One issue that often complicates this process is the &#34;noisy neighbor&#34; problem. On <a rel="noopener ugc nofollow" target="_blank" href="https://blog.jacobvosmaer.nl/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>, our multi-tenant compute platform, a &#34;noisy neighbor&#34; refers to a container or system service that heavily utilizes the server&#39;s resources, causing performance degradation in adjacent containers. We usually focus on CPU utilization because it is our workloads’ most frequent source of noisy neighbor issues.</p><p id="f191">Detecting the effects of noisy neighbors is complex. Traditional performance analysis tools such as <a href="https://www.brendangregg.com/perf.html" rel="noopener ugc nofollow" target="_blank">perf</a> can introduce significant overhead, risking further performance degradation. Additionally, these tools are typically deployed after the fact, which is too late for effective investigation.<em> </em>Another challenge is that debugging noisy neighbor issues requires significant low-level expertise and specialized tooling<em>. </em>In this blog post, we&#39;ll reveal how we leveraged <a href="https://ebpf.io/" rel="noopener ugc nofollow" target="_blank">eBPF</a> to achieve continuous, low-overhead instrumentation of the Linux scheduler, enabling effective self-serve monitoring of noisy neighbor issues. You’ll learn how Linux kernel instrumentation can improve your infrastructure observability with deeper insights and enhanced monitoring.</p><p id="792f">To ensure the reliability of our workloads that depend on low latency responses, we instrumented the <a href="https://en.wikipedia.org/wiki/Run_queue" rel="noopener ugc nofollow" target="_blank">run queue</a> latency for each container, which measures the time processes spend in the scheduling queue before being dispatched to the CPU. Extended waiting in this queue can be a telltale of performance issues, especially when containers are not utilizing their total CPU allocation. Continuous instrumentation is critical to catching such matters as they emerge, and eBPF, with its hooks into the Linux scheduler with minimal overhead, enabled us to monitor run queue latency efficiently.</p><p id="048b">To emit a run queue latency metric, we leveraged three eBPF hooks: <code>sched_wakeup</code><strong>, </strong><code>sched_wakeup_new</code><strong>,</strong> and <code>sched_switch</code>.</p></div></div><div><div><div><figure><div role="button" tabindex="0"><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6bapyclfXZPsUIaXFM-xaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*6bapyclfXZPsUIaXFM-xaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*6bapyclfXZPsUIaXFM-xaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*6bapyclfXZPsUIaXFM-xaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*6bapyclfXZPsUIaXFM-xaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6bapyclfXZPsUIaXFM-xaQ.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6bapyclfXZPsUIaXFM-xaQ.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*6bapyclfXZPsUIaXFM-xaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*6bapyclfXZPsUIaXFM-xaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*6bapyclfXZPsUIaXFM-xaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*6bapyclfXZPsUIaXFM-xaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*6bapyclfXZPsUIaXFM-xaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*6bapyclfXZPsUIaXFM-xaQ.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*6bapyclfXZPsUIaXFM-xaQ.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" width="1000" height="563" loading="eager" role="presentation"/></picture></div></div><figcaption>Diagram of how run queue latency is measured and instrumented</figcaption></figure></div></div></div><div><div><p id="184b">The <code>sched_wakeup</code><strong> </strong>and <code>sched_wakeup_new</code> hooks are invoked when a process changes state from &#39;sleeping&#39; to &#39;runnable.&#39; They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.</p><pre><span id="07d3">struct {</span></pre><p id="5852">Conversely, the <code>sched_switch</code> hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task&#39;s process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.</p><pre><span id="012a">SEC(&#34;tp_btf/sched_switch&#34;)</span></pre><p id="58f0">One of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process&#39;s cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an<a href="https://elixir.bootlin.com/linux/v6.6.16/source/include/linux/sched.h#L1225" rel="noopener ugc nofollow" target="_blank"> RCU (Read Copy Update) lock</a>.</p><p id="0a15">To safely access this RCU-protected information, we can leverage <a href="https://docs.kernel.org/bpf/kfuncs.html" rel="noopener ugc nofollow" target="_blank">kfuncs</a> in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.</p><pre><span id="769f">void bpf_rcu_read_lock(void) __ksym;</span></pre><p id="ba4a">Once the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF <a href="https://nakryiko.com/posts/bpf-ringbuf/" rel="noopener ugc nofollow" target="_blank">ring buffer</a>. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.</p><pre><span id="5a42">struct {</span></pre><p id="cd03">Our userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, <a rel="noopener ugc nofollow" target="_blank" href="https://blog.jacobvosmaer.nl/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a">Atlas</a>. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric (<code>runq.latency</code>) for that container. We also increment a counter metric (<code>sched.switch.out</code>) to monitor preemptions occurring for the container&#39;s processes. Access to the <code>prev_cgroup_id</code> of the preempted process allows us to tag the metric with the cause of the preemption, whether it&#39;s due to a process within the same container (or cgroup), a process in another container, or a system service.</p><p id="1bb4">It&#39;s important to highlight that both the <code>runq.latency</code> metric and the <code>sched.switch.out</code> metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve — relying solely on the <code>runq.latency </code>metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it&#39;s actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.</p><p id="8be9">Below is the <code>runq.latency</code> metric for a server running a single container with ample CPU capacity. The 99th percentile averages 83.4µs (microseconds), serving as our baseline. Although there are some spikes reaching 400µs, the latency remains within acceptable parameters.</p></div></div><div><div><div><figure><div role="button" tabindex="0"><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_DcYxRgeDwX5i07IrdTZyA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_DcYxRgeDwX5i07IrdTZyA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_DcYxRgeDwX5i07IrdTZyA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_DcYxRgeDwX5i07IrdTZyA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_DcYxRgeDwX5i07IrdTZyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_DcYxRgeDwX5i07IrdTZyA.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*_DcYxRgeDwX5i07IrdTZyA.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*_DcYxRgeDwX5i07IrdTZyA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_DcYxRgeDwX5i07IrdTZyA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_DcYxRgeDwX5i07IrdTZyA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_DcYxRgeDwX5i07IrdTZyA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_DcYxRgeDwX5i07IrdTZyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_DcYxRgeDwX5i07IrdTZyA.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*_DcYxRgeDwX5i07IrdTZyA.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" width="1000" height="420" loading="eager" role="presentation"/></picture></div></div><figcaption>container1’s 99th percentile runq.latency averages 83µs (microseconds), with spikes up to 400µs, without adjacent containers. This serves as our baseline for a container not contending for CPU on a host.</figcaption></figure></div></div></div><div><p id="d6c6">At 10:35, launching <code>container2</code>, which fully utilized all CPUs on the host, caused a significant 131-millisecond spike (131,000 microseconds) in <code>container1</code>&#39;s P99 run queue latency. This spike would be noticeable in the userspace application if it were serving HTTP traffic. If userspace app owners reported an unexplained latency spike, we could quickly identify the noisy neighbor issue through run queue latency metrics.</p></div><div><div><div><figure><div role="button" tabindex="0"><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*DJrwEbrWPOxVMS0JP7uE9A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*DJrwEbrWPOxVMS0JP7uE9A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*DJrwEbrWPOxVMS0JP7uE9A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*DJrwEbrWPOxVMS0JP7uE9A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*DJrwEbrWPOxVMS0JP7uE9A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DJrwEbrWPOxVMS0JP7uE9A.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*DJrwEbrWPOxVMS0JP7uE9A.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*DJrwEbrWPOxVMS0JP7uE9A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*DJrwEbrWPOxVMS0JP7uE9A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*DJrwEbrWPOxVMS0JP7uE9A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*DJrwEbrWPOxVMS0JP7uE9A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*DJrwEbrWPOxVMS0JP7uE9A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*DJrwEbrWPOxVMS0JP7uE9A.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*DJrwEbrWPOxVMS0JP7uE9A.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" width="1000" height="459" loading="eager" role="presentation"/></picture></div></div><figcaption>Launching container2 at 10:35, which maxes out all CPUs on the host, <strong>caused a 131-millisecond spike in container1’s P99 run queue latency</strong> due to increased preemptions by system processes. This indicates a noisy neighbor issue, where system services compete for CPU time with containers.</figcaption></figure></div></div></div><div><div><p id="1ce4">The <code>sched.switch.out</code> metric indicates that the spike was due to increased preemptions by system processes, highlighting a noisy neighbor issue where system services compete with containers for CPU time. Our metrics show that the noisy neighbors were actually system processes, likely triggered by <code>container2</code> consuming all available CPU capacity.</p><p id="5964">We developed an open-source eBPF process monitoring tool called <a rel="noopener ugc nofollow" target="_blank" href="https://blog.jacobvosmaer.nl/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5">bpftop</a> to measure the overhead of eBPF code in this kernel hot path. Our profiling with <code>bpftop</code> shows that the instrumentation adds less than 600 nanoseconds to each <code>sched_*</code> hook. We conducted a performance analysis on a Java service running in a container, and the instrumentation did not introduce significant overhead. The performance variance with the run queue profiling code active versus inactive was not measurable in milliseconds.</p><p id="2b6f">During our research on how eBPF statistics are measured in the kernel, we identified an opportunity to improve the calculation. We submitted this <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ce09cbdd988887662546a1175bcfdfc6c8fdd150" rel="noopener ugc nofollow" target="_blank">patch</a>, which was included in the Linux kernel 6.10 release.</p></div></div><div><div><div><figure><div role="button" tabindex="0"><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*YD6hkXce9a70AgvSHstgWA.gif 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*YD6hkXce9a70AgvSHstgWA.gif 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*YD6hkXce9a70AgvSHstgWA.gif 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*YD6hkXce9a70AgvSHstgWA.gif 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*YD6hkXce9a70AgvSHstgWA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YD6hkXce9a70AgvSHstgWA.gif 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*YD6hkXce9a70AgvSHstgWA.gif 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*YD6hkXce9a70AgvSHstgWA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*YD6hkXce9a70AgvSHstgWA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*YD6hkXce9a70AgvSHstgWA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*YD6hkXce9a70AgvSHstgWA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*YD6hkXce9a70AgvSHstgWA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*YD6hkXce9a70AgvSHstgWA.gif 1100w, https://miro.medium.com/v2/resize:fit:2000/1*YD6hkXce9a70AgvSHstgWA.gif 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="" width="1000" height="208" loading="eager" role="presentation"/></picture></div></div></figure></div></div></div><div><div><p id="3e54">Through trial and error and using <code>bpftop</code>, we identified several optimizations that helped maintain low overhead for our eBPF code:</p><ul><li id="bdd6">We found that <code>BPF_MAP_TYPE_HASH</code> was the most performant for storing enqueued timestamps. Using <code>BPF_MAP_TYPE_TASK_STORAGE</code> resulted in nearly a twofold performance decline. <code>BPF_MAP_TYPE_PERCPU_HASH</code> was slightly less performant than <code>BPF_MAP_TYPE_HASH</code>, which was unexpected and requires further investigation.</li><li id="6517"><code>BPF_MAP_TYPE_LRU_HASH</code> maps are 40–50 nanoseconds slower per operation than regular hash maps. Due to space concerns from PID churn, we initially used them for enqueued timestamps. Ultimately, we settled on <code>BPF_MAP_TYPE_HASH</code> with an increased size to mitigate this risk.</li><li id="0f38">The <code>BPF_CORE_READ</code> helper adds 20–30 nanoseconds per invocation. In the case of raw tracepoints, specifically those that are &#34;BTF-enabled&#34; (<code>tp_btf/*</code>), it is safe and more efficient to access the task struct members directly. Andrii Nakryiko recommends this approach in this <a href="https://nakryiko.com/posts/bpf-core-reference-guide/#btf-enabled-bpf-program-types-with-direct-memory-reads" rel="noopener ugc nofollow" target="_blank">blog post</a>.</li><li id="c6da">The <code>sched_switch</code>, <code>sched_wakeup</code>, and <code>sched_wakeup_new</code> are all triggered for kernel tasks, which are identifiable by their PID of 0. We found monitoring these tasks unnecessary, so we implemented several early exit conditions and conditional logic to prevent executing costly operations, such as accessing BPF maps, when dealing with a kernel task. Notably, kernel tasks operate through the scheduler queue like any regular process.</li></ul><p id="6194">Our findings highlight the value of low-overhead continuous instrumentation of the Linux kernel with eBPF. We have integrated these metrics into customer dashboards, enabling actionable insights and guiding multitenancy performance discussions. We can also now use these metrics to refine CPU isolation strategies to minimize the impact of noisy neighbors. Additionally, thanks to these metrics, we&#39;ve gained deeper insights into the Linux scheduler.</p><p id="4565">This work has also deepened our understanding of eBPF technology and underscored the importance of tools like <code>bpftop</code> for optimizing eBPF code. As eBPF adoption increases, we foresee more infrastructure observability and business logic shifting to it. One promising project in this space is <a href="https://github.com/sched-ext/scx" rel="noopener ugc nofollow" target="_blank">sched_ext</a>, which has the potential to revolutionize how scheduling decisions are made and tailored to specific workload needs.</p></div></div></div></div></section></div></div></article></div>
  </body>
</html>
