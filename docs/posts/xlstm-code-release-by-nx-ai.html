<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/NX-AI/xlstm">Original</a>
    <h1>xLSTM code release by NX-AI</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/NX-AI/xlstm/blob/main/res/desc_xlstm_overview.svg"><img src="https://github.com/NX-AI/xlstm/raw/main/res/desc_xlstm_overview.svg" alt="xLSTM Figure"/></a></p>
<blockquote>
<p dir="auto">Paper: <a href="https://arxiv.org/abs/2405.04517" rel="nofollow">https://arxiv.org/abs/2405.04517</a></p>
</blockquote>

<p dir="auto">xLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM.
Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM
and shows promising performance on Language Modeling when compared to Transformers or State Space Models.</p>

<p dir="auto">Create a conda environment from the file <code>environment_pt220cu121.yaml</code>.
Install the model code only (i.e. the module <code>xlstm</code>) as package:</p>
<p dir="auto">Instally via pip:</p>

<p dir="auto">Clone from github:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/NX-AI/xlstm.git
cd xlstm
pip install -e ."><pre>git clone https://github.com/NX-AI/xlstm.git
<span>cd</span> xlstm
pip install -e <span>.</span></pre></div>

<p dir="auto">This package is based on PyTorch and was tested for versions <code>&gt;=1.8</code>. For the CUDA version of sLSTM, you need Compute Capability &gt;= 8.0, see <a href="https://developer.nvidia.com/cuda-gpus" rel="nofollow">https://developer.nvidia.com/cuda-gpus</a>. For a well-tested environment, install the <code>environment_pt220cu121.yaml</code> as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda env create -n xlstm -f environment_pt220cu121.yaml
conda activate xlstm"><pre>conda env create -n xlstm -f environment_pt220cu121.yaml
conda activate xlstm</pre></div>

<p dir="auto">For non language applications or for integrating in other architectures you can use the <code>xLSTMBlockStack</code> and for language modeling or other token-based applications you can use the <code>xLSTMLMModel</code>.</p>

<p dir="auto">The <code>xLSTMBLockStack</code> is meant for use as alternative backbone in existing projects. It is similar to a stack of Transformer blocks, but uses xLSTM blocks:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch

from xlstm import (
    xLSTMBlockStack,
    xLSTMBlockStackConfig,
    mLSTMBlockConfig,
    mLSTMLayerConfig,
    sLSTMBlockConfig,
    sLSTMLayerConfig,
    FeedForwardConfig,
)

cfg = xLSTMBlockStackConfig(
    mlstm_block=mLSTMBlockConfig(
        mlstm=mLSTMLayerConfig(
            conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4
        )
    ),
    slstm_block=sLSTMBlockConfig(
        slstm=sLSTMLayerConfig(
            backend=&#34;cuda&#34;,
            num_heads=4,
            conv1d_kernel_size=4,
            bias_init=&#34;powerlaw_blockdependent&#34;,
        ),
        feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=&#34;gelu&#34;),
    ),
    context_length=256,
    num_blocks=7,
    embedding_dim=128,
    slstm_at=[1],

)

xlstm_stack = xLSTMBlockStack(cfg)

x = torch.randn(4, 256, 128).to(&#34;cuda&#34;)
xlstm_stack = xlstm_stack.to(&#34;cuda&#34;)
y = xlstm_stack(x)
y.shape == x.shape"><pre><span>import</span> <span>torch</span>

<span>from</span> <span>xlstm</span> <span>import</span> (
    <span>xLSTMBlockStack</span>,
    <span>xLSTMBlockStackConfig</span>,
    <span>mLSTMBlockConfig</span>,
    <span>mLSTMLayerConfig</span>,
    <span>sLSTMBlockConfig</span>,
    <span>sLSTMLayerConfig</span>,
    <span>FeedForwardConfig</span>,
)

<span>cfg</span> <span>=</span> <span>xLSTMBlockStackConfig</span>(
    <span>mlstm_block</span><span>=</span><span>mLSTMBlockConfig</span>(
        <span>mlstm</span><span>=</span><span>mLSTMLayerConfig</span>(
            <span>conv1d_kernel_size</span><span>=</span><span>4</span>, <span>qkv_proj_blocksize</span><span>=</span><span>4</span>, <span>num_heads</span><span>=</span><span>4</span>
        )
    ),
    <span>slstm_block</span><span>=</span><span>sLSTMBlockConfig</span>(
        <span>slstm</span><span>=</span><span>sLSTMLayerConfig</span>(
            <span>backend</span><span>=</span><span>&#34;cuda&#34;</span>,
            <span>num_heads</span><span>=</span><span>4</span>,
            <span>conv1d_kernel_size</span><span>=</span><span>4</span>,
            <span>bias_init</span><span>=</span><span>&#34;powerlaw_blockdependent&#34;</span>,
        ),
        <span>feedforward</span><span>=</span><span>FeedForwardConfig</span>(<span>proj_factor</span><span>=</span><span>1.3</span>, <span>act_fn</span><span>=</span><span>&#34;gelu&#34;</span>),
    ),
    <span>context_length</span><span>=</span><span>256</span>,
    <span>num_blocks</span><span>=</span><span>7</span>,
    <span>embedding_dim</span><span>=</span><span>128</span>,
    <span>slstm_at</span><span>=</span>[<span>1</span>],

)

<span>xlstm_stack</span> <span>=</span> <span>xLSTMBlockStack</span>(<span>cfg</span>)

<span>x</span> <span>=</span> <span>torch</span>.<span>randn</span>(<span>4</span>, <span>256</span>, <span>128</span>).<span>to</span>(<span>&#34;cuda&#34;</span>)
<span>xlstm_stack</span> <span>=</span> <span>xlstm_stack</span>.<span>to</span>(<span>&#34;cuda&#34;</span>)
<span>y</span> <span>=</span> <span>xlstm_stack</span>(<span>x</span>)
<span>y</span>.<span>shape</span> <span>==</span> <span>x</span>.<span>shape</span></pre></div>
<p dir="auto">If you are working with yaml strings / files for configuration you can also use dacite to create the config dataclasses. This is the same as the snippet above:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from omegaconf import OmegaConf
from dacite import from_dict
from dacite import Config as DaciteConfig
from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig

xlstm_cfg = &#34;&#34;&#34; 
mlstm_block:
  mlstm:
    conv1d_kernel_size: 4
    qkv_proj_blocksize: 4
    num_heads: 4
slstm_block:
  slstm:
    backend: cuda
    num_heads: 4
    conv1d_kernel_size: 4
    bias_init: powerlaw_blockdependent
  feedforward:
    proj_factor: 1.3
    act_fn: gelu
context_length: 256
num_blocks: 7
embedding_dim: 128
slstm_at: [1]
&#34;&#34;&#34;
cfg = OmegaConf.create(xlstm_cfg)
cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))
xlstm_stack = xLSTMBlockStack(cfg)

x = torch.randn(4, 256, 128).to(&#34;cuda&#34;)
xlstm_stack = xlstm_stack.to(&#34;cuda&#34;)
y = xlstm_stack(x)
y.shape == x.shape
"><pre><span>from</span> <span>omegaconf</span> <span>import</span> <span>OmegaConf</span>
<span>from</span> <span>dacite</span> <span>import</span> <span>from_dict</span>
<span>from</span> <span>dacite</span> <span>import</span> <span>Config</span> <span>as</span> <span>DaciteConfig</span>
<span>from</span> <span>xlstm</span> <span>import</span> <span>xLSTMBlockStack</span>, <span>xLSTMBlockStackConfig</span>

<span>xlstm_cfg</span> <span>=</span> <span>&#34;&#34;&#34; </span>
<span>mlstm_block:</span>
<span>  mlstm:</span>
<span>    conv1d_kernel_size: 4</span>
<span>    qkv_proj_blocksize: 4</span>
<span>    num_heads: 4</span>
<span>slstm_block:</span>
<span>  slstm:</span>
<span>    backend: cuda</span>
<span>    num_heads: 4</span>
<span>    conv1d_kernel_size: 4</span>
<span>    bias_init: powerlaw_blockdependent</span>
<span>  feedforward:</span>
<span>    proj_factor: 1.3</span>
<span>    act_fn: gelu</span>
<span>context_length: 256</span>
<span>num_blocks: 7</span>
<span>embedding_dim: 128</span>
<span>slstm_at: [1]</span>
<span>&#34;&#34;&#34;</span>
<span>cfg</span> <span>=</span> <span>OmegaConf</span>.<span>create</span>(<span>xlstm_cfg</span>)
<span>cfg</span> <span>=</span> <span>from_dict</span>(<span>data_class</span><span>=</span><span>xLSTMBlockStackConfig</span>, <span>data</span><span>=</span><span>OmegaConf</span>.<span>to_container</span>(<span>cfg</span>), <span>config</span><span>=</span><span>DaciteConfig</span>(<span>strict</span><span>=</span><span>True</span>))
<span>xlstm_stack</span> <span>=</span> <span>xLSTMBlockStack</span>(<span>cfg</span>)

<span>x</span> <span>=</span> <span>torch</span>.<span>randn</span>(<span>4</span>, <span>256</span>, <span>128</span>).<span>to</span>(<span>&#34;cuda&#34;</span>)
<span>xlstm_stack</span> <span>=</span> <span>xlstm_stack</span>.<span>to</span>(<span>&#34;cuda&#34;</span>)
<span>y</span> <span>=</span> <span>xlstm_stack</span>(<span>x</span>)
<span>y</span>.<span>shape</span> <span>==</span> <span>x</span>.<span>shape</span></pre></div>

<p dir="auto">The <code>xLSTMLMModel</code> is a wrapper around the <code>xLSTMBlockStack</code> that adds the token embedding and lm head.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from omegaconf import OmegaConf
from dacite import from_dict
from dacite import Config as DaciteConfig
from xlstm import xLSTMLMModel, xLSTMLMModelConfig

xlstm_cfg = &#34;&#34;&#34; 
vocab_size: 50304
mlstm_block:
  mlstm:
    conv1d_kernel_size: 4
    qkv_proj_blocksize: 4
    num_heads: 4
slstm_block:
  slstm:
    backend: cuda
    num_heads: 4
    conv1d_kernel_size: 4
    bias_init: powerlaw_blockdependent
  feedforward:
    proj_factor: 1.3
    act_fn: gelu
context_length: 256
num_blocks: 7
embedding_dim: 128
slstm_at: [1]
&#34;&#34;&#34;
cfg = OmegaConf.create(xlstm_cfg)
cfg = from_dict(data_class=xLSTMLMModelConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))
xlstm_stack = xLSTMLMModel(cfg)

x = torch.randint(0, 50304, size=(4, 256)).to(&#34;cuda&#34;)
xlstm_stack = xlstm_stack.to(&#34;cuda&#34;)
y = xlstm_stack(x)
y.shape[1:] == (256, 50304)"><pre><span>from</span> <span>omegaconf</span> <span>import</span> <span>OmegaConf</span>
<span>from</span> <span>dacite</span> <span>import</span> <span>from_dict</span>
<span>from</span> <span>dacite</span> <span>import</span> <span>Config</span> <span>as</span> <span>DaciteConfig</span>
<span>from</span> <span>xlstm</span> <span>import</span> <span>xLSTMLMModel</span>, <span>xLSTMLMModelConfig</span>

<span>xlstm_cfg</span> <span>=</span> <span>&#34;&#34;&#34; </span>
<span>vocab_size: 50304</span>
<span>mlstm_block:</span>
<span>  mlstm:</span>
<span>    conv1d_kernel_size: 4</span>
<span>    qkv_proj_blocksize: 4</span>
<span>    num_heads: 4</span>
<span>slstm_block:</span>
<span>  slstm:</span>
<span>    backend: cuda</span>
<span>    num_heads: 4</span>
<span>    conv1d_kernel_size: 4</span>
<span>    bias_init: powerlaw_blockdependent</span>
<span>  feedforward:</span>
<span>    proj_factor: 1.3</span>
<span>    act_fn: gelu</span>
<span>context_length: 256</span>
<span>num_blocks: 7</span>
<span>embedding_dim: 128</span>
<span>slstm_at: [1]</span>
<span>&#34;&#34;&#34;</span>
<span>cfg</span> <span>=</span> <span>OmegaConf</span>.<span>create</span>(<span>xlstm_cfg</span>)
<span>cfg</span> <span>=</span> <span>from_dict</span>(<span>data_class</span><span>=</span><span>xLSTMLMModelConfig</span>, <span>data</span><span>=</span><span>OmegaConf</span>.<span>to_container</span>(<span>cfg</span>), <span>config</span><span>=</span><span>DaciteConfig</span>(<span>strict</span><span>=</span><span>True</span>))
<span>xlstm_stack</span> <span>=</span> <span>xLSTMLMModel</span>(<span>cfg</span>)

<span>x</span> <span>=</span> <span>torch</span>.<span>randint</span>(<span>0</span>, <span>50304</span>, <span>size</span><span>=</span>(<span>4</span>, <span>256</span>)).<span>to</span>(<span>&#34;cuda&#34;</span>)
<span>xlstm_stack</span> <span>=</span> <span>xlstm_stack</span>.<span>to</span>(<span>&#34;cuda&#34;</span>)
<span>y</span> <span>=</span> <span>xlstm_stack</span>(<span>x</span>)
<span>y</span>.<span>shape</span>[<span>1</span>:] <span>==</span> (<span>256</span>, <span>50304</span>)</pre></div>

<p dir="auto">The synthetic experiments show-casing the benefits of sLSTM over mLSTM and vice versa best are the Parity task and the Multi-Query Associative Recall task. The Parity task can only be solved with state-tracking capabilities provided by the memory-mixing of sLSTM. The Multi-Query Associative Recall task measures memorization capabilities, where the matrix-memory and state expansion of mLSTM is very beneficial.
In combination they do well on both tasks.</p>
<p dir="auto">To run each, run the <code>main.py</code> in the experiments folder like:</p>
<div data-snippet-clipboard-copy-content="python experiments/main.py --config parity_xLSTM01.yaml   # xLSTM[0:1], sLSTM only
python experiments/main.py --config parity_xLSTM10.yaml   # xLSTM[1:0], mLSTM only
python experiments/main.py --config parity_xLSTM11.yaml   # xLSTM[1:1], mLSTM and sLSTM"><pre><code>python experiments/main.py --config parity_xLSTM01.yaml   # xLSTM[0:1], sLSTM only
python experiments/main.py --config parity_xLSTM10.yaml   # xLSTM[1:0], mLSTM only
python experiments/main.py --config parity_xLSTM11.yaml   # xLSTM[1:1], mLSTM and sLSTM
</code></pre></div>
<p dir="auto">Note that the training loop does not contain early stopping or test evaluation.</p>

<p dir="auto">If you use this codebase, or otherwise find our work valuable, pleace cite the xLSTM paper:</p>
<div data-snippet-clipboard-copy-content="@article{xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\&#34;o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\&#34;u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}
"><pre><code>@article{xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\&#34;o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\&#34;u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}

</code></pre></div>
</article></div></div>
  </body>
</html>
