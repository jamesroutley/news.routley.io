<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://transactional.blog/blog/2024-erasure-coding">Original</a>
    <h1>Erasure Coding for Distributed Systems</h1>
    
    <div id="readability-page-1" class="page"><div>
    
    <hr/>
    
    
    
    
<div id="preamble">
  <p>Suppose one has \(N\) servers across which to store a file.  One extreme is to give each of the \(N\) servers a full copy of the file.  Any server can supply a full copy of the file, so even if \(N-1\) servers are destroyed, then the file hasn’t been lost.  This provides the best durability and fault tolerance but is the most expensive in terms of storage space used.  The other extreme is to carve the data up into \(N\) equal-sized chunks, and give each server one chunk.  Reading the file will require reading all \(N\) chunks and reassembling the file.  This will provide the best cost efficiency, as each server can contribute to the file read request while using the minimum amount of storage space.</p>
<p>Erasure codes are the way to more generally describe the space of trade-offs between storage efficiency and fault tolerance.  One can say &#34;I’d like this file carved into \(N\) chunks, such that it can still be reconstructed with any \(M\) chunks destroyed&#34;, and there’s an erasure code with those parameters which will provide the minimum-sized chunks necessary to meet that goal.</p>
<p>The simplest intuition for there being middle points in this tradeoff is to consider a file replicated across three servers such that reading from any two should be able to yield the whole contents.  We can divide the file into two pieces, the first half of the file forms the first chunk (\(A\)) and the second half of the file forms the second chunk (\(B\)).  We can then produce a third equal-sized chunk (\(C\)) that’s the exclusive or of the first two (\(A \oplus B = C\)).  By reading any two of the three chunks, we can reconstruct the whole file:</p>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<th><p>Chunks Read</p></th>
<th><p>Reconstruct Via</p></th>
</tr>
<tr>
<td><p>\(\{A, B\}\)</p></td>
<td><p>\(A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{A, C\}\)</p></td>
<td><p>\(A :: A \oplus C =&gt; (A \oplus (A \oplus B)) =&gt; A :: B\)</p></td>
</tr>
<tr>
<td><p>\(\{B, C\}\)</p></td>
<td><p>\(B \oplus C :: B =&gt; (B \oplus (A \oplus B)) :: B =&gt; A :: B\)</p></td>
</tr>
</tbody>
</table>
<p>And all erasure codes follow this same pattern of having separate data and parity chunks.</p>
</div>
<h2 id="_erasure_coding_basics">
Erasure Coding Basics
</h2> 
<p>Configuring an erasure code revolves around one formula:</p>

<div>
<table>
<tbody><tr>
<td>
\(k\)
</td>
<td>
<p>The number of pieces the data is split into.  One must read at least this many chunks in total to be able to reconstruct the value.  Each chunk in the resulting erasure code will be \(1/k\) of the size of the original file.</p>
</td>
</tr>
<tr>
<td>
\(m\)
</td>
<td>
<p>The number of parity chunks to generate.  This is the fault tolerance of the code, or the number of reads which can fail to complete.</p>
</td>
</tr>
<tr>
<td>
\(n\)
</td>
<td>
<p>The total number of chunks that are generated.</p>
</td>
</tr>
</tbody></table>
</div>
<p>Erasure codes are frequently referred to by their \(k+m\) tuple.  It is important to note that the variable names are not consistent across all literature.  The only constant is that an erasure code written as \(x+y\) means \(x\) data chunks and \(y\) parity chunks.</p>
<p>Please enjoy a little calculator to show the effects of different \(k\) and \(m\) settings:</p>
<div x-data="{k: 3, m: 2}">
<p>
Each chunk is \(1/k = \)<kbd x-text="(100/k).toFixed(2)"></kbd>% of the size of the original data.  There are \(k + m =\)<kbd x-text="k+m"></kbd> chunks total, and together they are equivalent to \((m + k) / k =\)<kbd x-text="((m+k)/k).toFixed(2)"></kbd> full copies of the data.
</p></div>
<p>Erasure codes are incredibly attractive to storage providers, as they offer a way to fault tolerance at minimal storage overhead.
Backblaze B2 runs with \(17+3\), allowing it to tolerate 3 failures using 1.18x the storage space.  OVH Cloud uses an \(8+4\) code, allowing it to tolerate 4 failures using 1.5x the storage space.  Scaleway uses a \(6+3\) code, tolerating three failures using 1.5x the storage space.  &#34;Cloud storage reliability for Big Data applications&#34;<a id="_sideref_1"></a><sup>[1]</sup> pays significant attention to the subject of erasure coding due to the fundamental role it plays in increasing durability for storage providers at a minimal cost of additional storage space.
<span><a id="_sidedef_1"></a>[1]: Rekha Nachiappan, Bahman Javadi, Rodrigo N. Calheiros, and Kenan M. Matawie. 2017. Cloud storage reliability for Big Data applications. <em>J. Netw. Comput. Appl.</em> 97, C (November 2017), 35–47. <a href="https://scholar.google.com/scholar?cluster=12723199345811969350">[scholar]</a></span></p>
<p>The main trade-off in erasure coding is a reduction in storage space used at the cost of an increase in requests issued to read data.  Rather than issuing one request to read a file-sized chunk from one disk, requests are issued to \(k+m\) disks.  Storage systems meant for infrequently accessed data, form ideal targets for erasure coding.  Infrequent access means issuing more IO operations per second won’t be a problematic tax, and the storage savings are significant when compared to storing multiple full copies of every file.</p>
<p>&#34;Erasure coding&#34; describes a general class of algorithms and not any one algorithm in particular.  In general, Reed-Solomon codes can be used to implement any \(k+m\) configuration of erasure codes.  Due to the prevalence of <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels">RAID</a>, special attention in erasure coding research has been paid to developing more efficient algorithms specialized for implementing these specific subsets of erasure coding. RAID-0 is \(k+0\) erasure coding.  RAID-1 is \(1+m\) erasure coding.  RAID-4 and RAID-5 are slightly different variations of \(k+1\) erasure coding.  RAID-6 is \(k+2\) erasure coding.  Algorithms specifically designed for these cases are mentioned in the implementation section below, but it’s also perfectly fine to not be aware of what exact algorithm is being used to implement the choice of a specific \(k+m\) configuration.</p>
<p>Everything described in this post is about <em>Minimum Distance Separable</em> (MDS) erasure codes, which are only one of many erasure code families.  MDS codes provide the quorum-like property that any \(m\) chunks can be used to reconstruct the full value.  Other erasure codes take other tradeoffs, where some combinations of less than \(m\) chunks can be used to reconstruct the full value, but other combinations require more than \(m\) chunks.  &#34;Erasure Coding in Windows Azure Storage&#34;<a id="_sideref_2"></a><sup>[2]</sup> nicely explains the motivation of why Azure devised Local Reconstruction Codes for their deployment.  &#34;SD Codes: Erasure Codes Designed for How Storage Systems Really Fail&#34;<a id="_sideref_3"></a><sup>[3]</sup> pitches specializing an erasure code towards recovering from sector failures, as the most common failure type.  Overall, if one has knowledge about the expected pattern of failures, then a coding scheme that allow recovering from expected failures with less than \(m\) chunks, and unexpected failures with more than \(m\) chunks would have a positive expected value.
<span><a id="_sidedef_2"></a>[2]: Cheng Huang, Huseyin Simitci, Yikang Xu, Aaron Ogus, Brad Calder, Parikshit Gopalan, Jin Li, and Sergey Yekhanin. 2012. Erasure Coding in Windows Azure Storage. In <em>2012 USENIX Annual Technical Conference (USENIX ATC 12)</em>, USENIX Association, Boston, MA, 15–26. <a href="https://scholar.google.com/scholar?cluster=7930684733311413322">[scholar]</a></span></p>
<h2 id="_applications_in_distributed_systems">
Applications in Distributed Systems
</h2> 
<h3 id="_space_and_tail_latency_improvements">
Space and Tail Latency Improvements
</h3> 
<p>The most direct application is in reducing the storage cost and increasing the durability of data in systems with a known, fixed set of replicas.
Think of blob/object storage or NFS storage.  A metadata service maps a file path to a server that stores the file.  Instead of having 3 replicas storing the full file each, have 15 replicas store the chunks of the (10+5) erasure coded file.  Such a coding yields half the total amount of data to store, and more than double the fault tolerance.</p>
<p>More generally, this pattern translates to &#34;instead of storing data across \(X\) servers, consider storing it across \(X+m\) replicas with an \(X+m\) erasure code&#34;.  Over on Marc Brooker’s blog, this is illustrated <a href="https://brooker.co.za/blog/2023/01/06/erasure.html">using a caching system</a>.  Instead of using consistent hashing to identify one of \(k\) cache servers to query, one can use a \(k+m\) erasure code with \(k+m\) cache servers and not have to wait for the \(m\) slowest responses.  This provides both a storage space and tail latency improvement.</p>
<p>Again, the space and latency savings do come at a cost, which is an increase in IOPS/QPS, or effectively CPU.  In both cases, we’re betting that the limiting resource which determines how many machines or disks we need to buy is storage capacity, and that we can increase our CPU usage to decrease the amount of data that needs to be stored.  If the system is already pushing its CPU limits, then erasure coding might not be a cost-saving idea.</p>
<h3 id="_quorum_systems">
Quorum Systems
</h3> 
<p>Consider a quorum system with 5 replicas, where one must read from and write to at least 3 of them, a simple majority.  Erasure codes are well matched on the read side, where a \(3+2\) erasure code equally represents that a read may be completed using the results from any 3 of the 5 replicas.  Unfortunately, the rule is that writes are allowed to complete as long as they’re received by any 3 replicas, so one could only use a \(1+2\) code, which is exactly the same as writing three copies of the file.  Thus, there are no trivial savings to be had by applying erasure coding.</p>
<p>RS-Paxos<a id="_sideref_4"></a><sup>[4]</sup> examined the applicability of erasure codes to Paxos, and similarly concluded that the only advantage is when there’s an overlap between two quorums of more than one replica.  A quorum system of 7 replicas, where one must read and write to at least 5 of them would have the same 2 replica fault tolerance, but would be able to apply a \(3+2\) erasure code.  In general, with \(N\) replicas and a desired fault tolerance of \(f\), the best one can do with a fixed erasure coding scheme is \((N-2f)+f\).
<span><a id="_sidedef_4"></a>[4]: Shuai Mu, Kang Chen, Yongwei Wu, and Weimin Zheng. 2014. When paxos meets erasure code: reduce network and storage cost in state machine replication. In <em>Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing</em> (HPDC &#39;14), Association for Computing Machinery, New York, NY, USA, 61–72. <a href="https://scholar.google.com/scholar?cluster=16520033292975033789">[scholar]</a></span></p>
<p>HRaft<a id="_sideref_5"></a><sup>[5]</sup> explores that there is a way to get the desired improvement from a simple majority quorum, but adapting the coding to match the number of available replicas.  When all 5 replicas are available then we may use a \(3+2\) encoding, when 4 are available then use a \(2+2\) encoding, and when only 3 are available then use a \(1+2\) encoding<a id="_sideref_6"></a><sup>[6]</sup>.  Adapting the erasure code to the current replica availability yields our optimal improvement, but comes with a number of drawbacks.  Each write is optimistic in guessing the number of replicas that are currently available, and writes must be re-coded and resent to all replicas if one replica unexpectedly doesn’t acknowledge the write.  Additionally, one must still provision the system such that a replica storing the full value of every write is possible, so that after two failures, the system running in a \(1+2\) configuration won’t cause unavailability due to lacking disk space or throughput.  However, if failures are expected to be rare and will be recovered from quickly, then HRaft’s adaptive encoding scheme will yield significant improvements.
<span><a id="_sidedef_5"></a>[5]: Yulei Jia, Guangping Xu, Chi Wan Sung, Salwa Mostafa, and Yulei Wu. 2022. HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks. In <em>2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</em>, 1316–1326. <a href="https://scholar.google.com/scholar?cluster=15724086733201598850">[scholar]</a></span>
<span><a id="_sidedef_6"></a>[6]: And just to emphasize again, a \(1+2\) erasure encoding is just 3 full copies of the data.  It’s the same as not applying any erasure encoding.  The only difference is that it’s promised that only three full copies of the data are generated and sent to replicas.</span></p>
<h2 id="_usage_basics">
Usage Basics
</h2> 
<p>For computing erasure codings, there is a mature and standard <a href="https://jerasure.org/">Jerasure</a>.  If on a modern Intel processor, the Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/isa-l/overview.html">Intelligent Storage Acceleration Library</a> is a SIMD-optimized library consistently towards the top of the benchmarks.</p>
<p>As an example, we can use <a href="https://pypi.org/project/pyeclib/">pyeclib</a> as a way to get easy access to an erasure coding implementation from python, and apply it to specifically to HRaft’s proposed adaptive erasure coding scheme:</p>
<details>
<summary>Python source code</summary>
<div>
<div>
<div>
<pre><code data-lang="python"><span>#!/usr/bin/env python
# Usage: ./ec.py &lt;K&gt; &lt;M&gt;
</span><span>import</span> <span>sys</span>
<span>K</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>1</span><span>])</span>
<span>M</span> <span>=</span> <span>int</span><span>(</span><span>sys</span><span>.</span><span>argv</span><span>[</span><span>2</span><span>])</span>

<span># Requires running the following to install dependencies:
# $ pip install --user pyeclib
# $ sudo dnf install liberasurecode-devel
</span><span>import</span> <span>pyeclib.ec_iface</span> <span>as</span> <span>ec</span>

<span># liberasurecode_rs_vand is built into liberasurecode, so this
# shouldn&#39;t have any other dependencies.
</span><span>driver</span> <span>=</span> <span>ec</span><span>.</span><span>ECDriver</span><span>(</span><span>ec_type</span><span>=</span><span>&#39;liberasurecode_rs_vand&#39;</span><span>,</span>
                     <span>k</span><span>=</span><span>K</span><span>,</span> <span>m</span><span>=</span><span>M</span><span>,</span> <span>chksum_type</span><span>=</span><span>&#39;none&#39;</span><span>)</span>
<span>data</span> <span>=</span> <span>bytes</span><span>([</span><span>i</span> <span>%</span> <span>100</span> <span>+</span> <span>32</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>10000</span><span>)])</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Erasure Code(K data chunks = </span><span>{</span><span>K</span><span>}</span><span>, M parity chunks = </span><span>{</span><span>M</span><span>}</span><span>)&#34;</span>
      <span>f</span><span>&#34; of </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes&#34;</span><span>)</span>

<span># Produce the coded chunks.
</span><span>chunks</span> <span>=</span> <span>driver</span><span>.</span><span>encode</span><span>(</span><span>data</span><span>)</span>

<span># There&#39;s some metdata that&#39;s prefixed onto each chunk to identify
# its position.  This isn&#39;t technically required, but there isn&#39;t
# an easy way to disable it.  There&#39;s also some additional bytes
# which I can&#39;t account for.
</span><span>metadata_size</span> <span>=</span> <span>len</span><span>(</span><span>driver</span><span>.</span><span>get_metadata</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>]))</span>
<span>chunk_size</span> <span>=</span> <span>len</span><span>(</span><span>chunks</span><span>[</span><span>0</span><span>])</span> <span>-</span> <span>metadata_size</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Encoded into </span><span>{</span><span>len</span><span>(</span><span>chunks</span><span>)</span><span>}</span><span> chunks of </span><span>{</span><span>chunk_size</span><span>}</span><span> bytes&#34;</span><span>)</span>
<span>print</span><span>(</span><span>&#34;&#34;</span><span>)</span>

<span># This replication scheme is X% less efficient than writing 1 copy
</span><span>no_ec_size</span> <span>=</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;No EC: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>*</span><span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes, </span><span>{</span><span>1</span><span>/</span><span>(</span><span>K</span><span>+</span><span>M</span><span>)</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Expected: </span><span>{</span><span>(</span><span>M</span><span>+</span><span>K</span><span>)</span><span>/</span><span>K</span> <span>*</span> <span>len</span><span>(</span><span>data</span><span>)</span><span>}</span><span> bytes,&#34;</span>
      <span>f</span><span>&#34; </span><span>{</span><span>1</span><span>/</span> <span>(</span><span>1</span><span>/</span><span>K</span> <span>*</span> <span>(</span><span>K</span><span>+</span><span>M</span><span>))</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency&#34;</span><span>)</span>
<span>total_ec_size</span> <span>=</span> <span>chunk_size</span> <span>*</span> <span>len</span><span>(</span><span>chunks</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;Actual: </span><span>{</span><span>total_ec_size</span><span>}</span><span> bytes,&#34;</span>
      <span>f</span><span>&#34; </span><span>{</span><span>len</span><span>(</span><span>data</span><span>)</span> <span>/</span> <span>total_ec_size</span> <span>*</span> <span>100</span><span>}</span><span>% efficiency&#34;</span><span>)</span>

<span># Validate that our encoded data decodes using minimal chunks
</span><span>import</span> <span>random</span>
<span>indexes</span> <span>=</span> <span>random</span><span>.</span><span>sample</span><span>(</span><span>range</span><span>(</span><span>K</span><span>+</span><span>M</span><span>),</span> <span>K</span><span>)</span>
<span># Prepended metadata is used to determine the chunk part number
# from the data itself.  Other libraries require this to be
# passed in as part of the decode call.
</span><span>decoded_data</span> <span>=</span> <span>driver</span><span>.</span><span>decode</span><span>([</span><span>chunks</span><span>[</span><span>idx</span><span>]</span> <span>for</span> <span>idx</span> <span>in</span> <span>indexes</span><span>])</span>
<span>assert</span> <span>decoded_data</span> <span>==</span> <span>data</span></code></pre>
</div>
</div>
</div>
</details>
<p>When there are 5/5 replicas available, HRaft would use a \(3+2\) erasure code:</p>
<div>
<div>
<pre>$ ./ec.py 3 2
Erasure Code(K data chunks = 3, M parity chunks = 2) of 10000 bytes
Encoded into 5 chunks of 3355 bytes

No EC: 50000 bytes, 20% efficiency
Expected: 16666.666666666668 bytes, 60.00000000000001% efficiency
Actual: 16775 bytes, 59.61251862891207% efficiency</pre>
</div>
</div>
<p>When there are 4/5 replicas available, HRaft would use a \(2+2\) erasure code:</p>
<div>
<div>
<pre>$ ./ec.py 2 2
Erasure Code(K data chunks = 2, M parity chunks = 2) of 10000 bytes
Encoded into 4 chunks of 5021 bytes

No EC: 40000 bytes, 25% efficiency
Expected: 20000.0 bytes, 50% efficiency
Actual: 20084 bytes, 49.790878311093406% efficiency</pre>
</div>
</div>
<p>When there are 3/5 replicas available, HRaft would use a \(1+2\) erasure code:</p>
<div>
<div>
<pre>$ ./ec.py 1 2
Erasure Code(K data chunks = 1, M parity chunks = 2) of 10000 bytes
Encoded into 3 chunks of 10021 bytes

No EC: 30000 bytes, 33.33333333333333% efficiency
Expected: 30000.0 bytes, 33.33333333333333% efficiency
Actual: 30063 bytes, 33.263480025280245% efficiency</pre>
</div>
</div>
<h2 id="_usage_not_so_basics">
Usage Not So Basics
</h2> 
<p>As always, things aren’t quite perfectly simple.</p>
<h3 id="_decoding_cost_variability">
Decoding Cost Variability
</h3> 
<p>Decoding performance varies with the number of data chunks that need to be recovered.  Decoding a \(3+2\) code from the three data chunks is computationally trivial.  Decoding the same file from two data chunks and one parity chunk involves solving a system of linear equations via Gaussian elimination, and the computational increases as the number of required parity chunks involved increases.  Thus, if using an erasure code as part of a quorum system, be aware that the CPU cost of decoding will vary depending on exactly which replicas reply.</p>
<p>There are a few different papers comparing different erasure code implementations and their performance across varying block size and number of data chunks to reconstruct.  I’ll suggest &#34;Practical Performance Evaluation of Space Optimal Erasure Codes for High Speed Data Storage Systems&#34;<a id="_sideref_7"></a><sup>[7]</sup> as the one I liked the most, from which the following figure was taken:
<span><a id="_sidedef_7"></a>[7]: Rui Chen and Lihao Xu. 2019. Practical Performance Evaluation of Space Optimal Erasure Codes for High-Speed Data Storage Systems. <em>SN Comput. Sci.</em> 1, 1 (December 2019). <a href="https://scholar.google.com/scholar?cluster=9222594581704961566">[scholar]</a></span></p>

<div>

<p><img src="https://elijer.github.io/images/blog/2024-erasure-coding/decoding_performance-45d237e9.png" alt="decoding performance"/>
</p>
</div>
<h3 id="_library_differences">
Library Differences
</h3> 
<p>Liberasurecode abstracts over most common erasure coding implementation libraries, but be aware that does not mean that the implementations are equivalent.  Just because two erasure codes are both \(3+2\) codes doesn’t mean the same math was used to construct them.</p>
<p>Correspondingly, liberasurecode doesn’t <em>just</em> do the linear algebra work, it &#34;helpfully&#34; adds metadata necessary to configure which decoder to use and how, which you can’t disable or modify:</p>
<div>
<p>liberasurecode / erasurecode.h</p>
<div>
<pre><code data-lang="c"><span>struct</span> <span>__attribute__</span><span>((</span><span>__packed__</span><span>))</span>
<span>fragment_metadata</span>
<span>{</span>
    <span>uint32_t</span>    <span>idx</span><span>;</span>                <span>/* 4 */</span>
    <span>uint32_t</span>    <span>size</span><span>;</span>               <span>/* 4 */</span>
    <span>uint32_t</span>    <span>frag_backend_metadata_size</span><span>;</span>    <span>/* 4 */</span>
    <span>uint64_t</span>    <span>orig_data_size</span><span>;</span>     <span>/* 8 */</span>
    <span>uint8_t</span>     <span>chksum_type</span><span>;</span>        <span>/* 1 */</span>
    <span>uint32_t</span>    <span>chksum</span><span>[</span><span>LIBERASURECODE_MAX_CHECKSUM_LEN</span><span>];</span> <span>/* 32 */</span>
    <span>uint8_t</span>     <span>chksum_mismatch</span><span>;</span>    <span>/* 1 */</span>
    <span>uint8_t</span>     <span>backend_id</span><span>;</span>         <span>/* 1 */</span>
    <span>uint32_t</span>    <span>backend_version</span><span>;</span>    <span>/* 4 */</span>
<span>}</span> <span>fragment_metadata_t</span><span>;</span></code></pre>
</div>
</div>
<p>This is just a liberasurecode thing.  Using either Jerasure or ISA-L directly allows access to only the erasure coded data.  It <em>is</em> required as part of the APIs that each chunk must be provided along with if it was the Nth data or parity chunk, so the index must be maintained somehow as part of metadata.</p>
<p>As was noted in the <a href="https://www.youtube.com/watch?v=URAm-bbst-o">YDB talk at HydraConf</a>, Jerasure does a permutation of the output from what one would expect from just the linear algebra.  This means that it’s up to the specific implementation details of a library as to if reads must be aligned with writes — Jerasure cannot read a subset or superset of what was encoded.  ISA-L applies no permutation, so reads may decode unaligned subsets or supersets of encoded data.</p>
<p>Jerasure and ISA-L are, by far, the most popular libraries for erasure coding, but they’re not the only ones.  <a href="https://github.com/tahoe-lafs/zfec">tahoe-lafs/zfec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a> is also a reasonably well-known implementation.  Christopher Taylor has written at least three MDS erasure coding implementations taking different tradeoffs (<a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a>, <a href="https://github.com/catid/longhair">catid/longhair<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a>, <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a>), and a comparison and discussion of the differences can be found on <a href="https://github.com/catid/leopard/blob/master/Benchmarks.md">leopard’s benchmarking results page</a>.  If erasure coding becomes a bottleneck, a library more optimized for your specific use case can likely be found somewhere, but ISA-L is generally good enough.</p>
<h2 id="_implementing_erasure_codes">
Implementing Erasure Codes
</h2> 
<p>It is entirely acceptable and workable to treat erasure codes as a magic function that turns 1 file into \(n\) chunks and back.  You can stop reading here, and not knowing the details of what math is being performed will not hinder your ability to leverage erasure codes to great effect in distributed systems or databases.  (And if you continue, take what follows with a large grain of salt, as efficient erasure coding is a subject folk have spent years on, and the below is what I’ve collected from a couple of days of reading through papers I only half understand.)</p>
<p>The construction of the \(n\) chunks is some linear algebra generally involving a Galois Field, none of which is important to understand to be able to productively <em>use</em> erasure codes.  Backblaze published <a href="https://www.backblaze.com/blog/reed-solomon/">a very basic introduction</a>.  The best introduction to the linear algebra of erasure coding that I’ve seen is Fred Akalin’s <a href="https://www.akalin.com/intro-erasure-codes">&#34;A Gentle Introduction to Erasure Codes&#34;</a>.  <a href="https://tomverbeure.github.io/2022/08/07/Reed-Solomon.html">Reed-Solomon Error Correcting Codes from the Bottom Up</a> covers Reed-Solomon codes and Galois Field polynomials specifically.  There’s also a plethora of erasure coding-related questions on the Stack Overflow family of sites, so any question over the math that one might have has already likely been asked and answered there.</p>
<p>With the basics in place, there are two main dimensions to investigate: what is the exact MDS encoding and decoding algorithm to implement, and how can one implement that algorithm most efficiently?</p>
<h3 id="_algorithmic_efficiency">
Algorithmic Efficiency
</h3> 
<p>In general, most MDS codes are calculated as a matrix multiplication, where addition is replaced with XOR, and multiply is replaced with a more expensive multiplication over GF(256).  For the special cases of 1-3 parity chunks (\(m \in \{1,2,3\}\)), there are algorithms not derived from Reed-Solomon and which use only XORs:</p>
<div>
<ul>
<li>
<p>\(m=1\) is a trivial case of a single parity chunk, which is just the XOR of all data chunks.</p>
</li>
<li>
<p>\(m=2\) is also known as RAID-6, for which I would recommend Liberation codes<a id="_sideref_8"></a><sup>[8]</sup><a id="_sideref_9"></a><sup>[9]</sup> as <em>nearly</em> optimal with an implementation available as part of <a href="https://jerasure.org/">Jerasure</a>, and HDP codes<a id="_sideref_10"></a><sup>[10]</sup> and EVENODD<a id="_sideref_11"></a><sup>[11]</sup> as notable but patented.  If \(k+m+2\) is prime, then X-Codes<a id="_sideref_12"></a><sup>[12]</sup> are also optimal.</p>
</li>
<li>
<p>\(m=3\) can be done via STAR coding<a id="_sideref_13"></a><sup>[13]</sup>.</p>
</li>
</ul>
</div>

<p>Otherwise and more generally, a form of Reed-Solomon coding is used.  The encoding/decoding matrix is either a \(k \times n\) Vandermonde<a id="_sideref_14"></a><sup>[14]</sup> matrix with the upper \(k \times k\) of it Gaussian eliminated to form an identity matrix, or an \(k \times k\) identity matrix with a \(k \times m\) Cauchy<a id="_sideref_15"></a><sup>[15]</sup> matrix glued onto the bottom.  In both cases, the goal is to form a matrix where the top \(k \times k\) is an identity matrix (so that each data chunk is preserved), and any deletion of \(m\) rows yields an invertible matrix.  Encoding is multiplying by this matrix, and decoding deletes the rows corresponding to erased chunks, and then solves the matrix as a system of linear equations for the missing data.</p>
<p>Gaussian elimination, as used in ISA-L, is the simplest method of decoding, but also the slowest.  For Cauchy matrixes, this can be improved<a id="_sideref_16"></a><sup>[16]</sup>, as done in <a href="https://github.com/catid/cm256">catid/cm256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a>.  The current fastest methods appear to be implemented in <a href="https://github.com/catid/leopard">catid/leopard<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a>, which uses Fast Fourier Transforms<a id="_sideref_17"></a><sup>[17]</sup><a id="_sideref_18"></a><sup>[18]</sup> for encoding and decoding.</p>

<h3 id="_implementation_efficiency">
Implementation Efficiency
</h3> 
<p>There are levels of implementation efficiency for erasure codes that function over any \(k+m\) configuration:</p>
<div>
<ol>
<li>
<p>Implement the algorithm in C, and rely on the compiler for auto-vectorization.</p>
<p>This provides the most straightforward and most portable implementation, at acceptable performance.  Usage of <code>restrict</code> and ensuring the appropriate architecture-specific compilation flags have been specified (e.g. <code>-march=native</code>).</p>
</li>
<li>
<p>Rely on a vectorization library or compiler intrinsics to abstract the platform specifics.</p>
<p><a href="https://github.com/google/highway">google/highway<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a> and <a href="https://github.com/xtensor-stack/xsimd">xtensor-stack/xsimd<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a> appear to be reasonably commonly used libraries that try to use the best available SIMD instructions to accomplish general tasks.  There is also the upcoming <a href="https://en.cppreference.com/w/cpp/experimental/simd/simd"><code>std::experimental::simd</code></a>.  C/C++ compilers also offer <a href="https://gcc.gnu.org/onlinedocs/gcc/Vector-Extensions.html">builtins</a> for vectorization support.</p>
<p>The core of encoding and decoding is Galois field multiply and addition.  Optimized libraries for this can be found at <a href="https://github.com/catid/gf256">catid/gf256<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a> and <a href="https://web.eecs.utk.edu/~jplank/plank/papers/CS-07-593/">James Plank’s Fast Galois Field Arithmetic Library</a>.</p>
</li>
<li>
<p>Handwrite a vectorized implementation of the core encoding and decoding functions.</p>
<p>Further discussion of fast GF(256) operations can be found in the PARPAR project: <a href="https://github.com/animetosho/ParPar/blob/master/fast-gf-multiplication.md">fast-gf-multiplication</a> and the <a href="https://github.com/animetosho/ParPar/blob/master/xor_depends/info.md">xor_depends work</a>.  The consensus appears to be that a XOR-only GF multiply should be faster than a table-driven multiply.</p>

</li>
</ol>
</div>
<p>Optimizing further involves specializing the code to one specific \(k+m\) configuration by transforming the matrix multiplication with a constant into a linear series of instructions, and then:</p>
<div>
<ol start="4">
<li>
<p>Find an optimal coding matrix and XOR schedule for the specific GF polynomial and encoding matrix.</p>

</li>
<li>
<p>Apply further operation, memory, and cache optimizations.</p>

<p>The code is publicly available at <a href="https://github.com/yuezato/xorslp_ec">yuezato/xorslp_ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a>.</p>
</li>
<li>
<p>Programmatically explore an optimized instruction schedule for a specific architecture.</p>

<p>The code is publicly available at <a href="https://github.com/Thesys-lab/tvm-ec">Thesys-lab/tvm-ec<span><img src="https://github.com/favicon.ico" alt="GitHub" width="14" height="14"/></span></a>.</p>
</li>
</ol>
</div>
<p>For a more fully explored treatment of this topic, please see <a href="https://www.usenix.org/conference/fast19/presentation/zhou">&#34;Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques&#34;</a><a id="_sideref_19"></a><sup>[19]</sup>, which also has a video of the presenter if that’s your preferred medium.
<span><a id="_sidedef_19"></a>[19]: Tianli Zhou and Chao Tian. 2020. Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques. <em>ACM Trans. Storage</em> 16, 1 (March 2020). <a href="https://scholar.google.com/scholar?cluster=15189943361362749273">[scholar]</a></span></p>
<h2 id="_references">
References
</h2> 
<p><a href="https://elijer.github.io/garden/devnotes/LeetCode-Journal/2024-erasure-coding.bib">References as BibTeX</a></p>
<p>And if you’re looking to broadly dive deeper, I’d suggest starting with reviewing <a href="https://dblp.org/pid/07/3005.html">James S. Plank’s publications</a>.</p>
    <!-- TODO: consider https://utteranc.es/ for in-page comments. -->
      <hr/>
      
      <p><a href="https://discu.eu/?q=https://transactional.blog/blog/2024-erasure-coding.html&amp;submit_title=Erasure Coding for Distributed Systems">See discussion of this page on Reddit, HN, and lobsters.</a></p>
    </div></div>
  </body>
</html>
