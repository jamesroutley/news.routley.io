<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.brendangregg.com/blog//2024-10-29/ai-flame-graphs.html">Original</a>
    <h1>AI Flame Graphs</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Imagine halving the resource costs of AI and what that could mean for the planet and the industry -- based on extreme estimates such savings could reduce the total US power usage by over 10% by 2030<SPAN size="-2"><sup>1</sup></SPAN>. At Intel we&#39;ve been creating a new analyzer tool to help reduce AI costs called <em>AI Flame Graphs</em>: a visualization that shows an AI accelerator or GPU hardware profile along with the full software stack, based on my <strong><a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">CPU flame graphs</a></strong>. Our first version is available to customers in the <strong><a href="https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html">Intel Tiber AI Cloud</a></strong> as a preview for the Intel Data Center GPU Max Series (previously called Ponte Vecchio). Here is an example:</p>

<center><a href="https://www.brendangregg.com/blog/images/2024/matrixAIflamegraph.svg"><img src="https://www.brendangregg.com/blog/images/2024/matrixAIflamegraph.png" width="700"/></a></center>

<p>(Click for interactive <a href="https://www.brendangregg.com/blog/images/2024/matrixAIflamegraph.svg">SVG</a>.) The <SPAN color="#00bb00">green</SPAN> frames are the actual instructions running on the AI or GPU accelerator, <SPAN color="#008888">aqua</SPAN> shows the source code for these functions, and <SPAN color="#bb0000">red</SPAN> (C), <SPAN color="#888800">yellow</SPAN> (C++), and <SPAN color="#a04000">orange</SPAN> (kernel) show the CPU code paths that initiated these AI/GPU programs. The <SPAN color="#808080">gray</SPAN> &#34;-&#34; frames just help highlight the boundary between CPU and AI/GPU code. The x-axis is proportional to cost, so you look for the widest things and find ways to reduce them.</p>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/AIflamegraph-legend.png" width="150"/></center></div>

<p>This flame graph shows a simple program for SYCL (a high-level C++ language for accelerators) that tests three implementations of matrix multiply, running them with the same input workload. The flame graph is dominated by the slowest implementation, multiply_basic(), which doesn&#39;t use any optimizations and consumes at 72% of stall samples and is shown as the widest tower. On the right are two thin towers for multiply_local_access() at 21% which replaces the accessor with a local variable, and multiply_local_access_and_tiling() at 6% which also adds matrix tiling. The towers are getting smaller as optimizations are added.</p>

<p>This flame graph profiler is a prototype based on Intel EU stall profiling for hardware profiling and <a href="https://ebpf.io/">eBPF</a> for software instrumentation. It&#39;s designed to be <strong>easy and low-overhead</strong>, just like a CPU profiler. You should be able to generate a flame graph of an existing AI workload whenever you want, without having to restart anything or launch additional code via an interposer.</p>

<h2>Instruction-offset Profiling</h2>

<p>This is not the first project to build an AI profiler or even something called an AI Flame Graph, however, others I&#39;ve seen focus on tracing CPU stacks and timing accelerator execution, but don&#39;t profile the instruction offsets running on the accelerator; or do profile them but via expensive binary instrumentation. I wanted to build AI flame graphs that work like CPU flame graphs: Easy to use, negligible cost, production safe, and shows everything. A daily tool for developers, with most of the visualization <em>in the language of the developer</em>: source code functions.</p>

<p>This has been an internal AI project at Intel for the past year. Intel was already investing in this space, building the EU stall profiler capability for the Intel Data Center GPU Max Series that provides an approximation of HW instruction sampling. I was lucky to have <strong>Dr. Matthew (Ben) Olson</strong>, an Intel AI engineer who has also worked on eBPF performance tooling (<a href="https://github.com/intel/processwatch">processwatch</a>) as well as memory management research, join my team and do most of the development work. His background has helped us power through difficulties that seemed insurmountable. We&#39;ve also recently been joined by <strong>Dr. Brandon Kammerdiener</strong> (coincidentally another graduate of the University of Tennessee, like Ben), who also has eBPF and memory internals experience, and has been helping us take on harder and harder workloads. And <strong>Gabriel Mu√±oz</strong> just joined today to help with releases. Now that our small team has shown that this is possible, we&#39;ll be joined by other teams at Intel to develop this further.</p>

<p>We could have built a harder-to-use and higher-overhead version months ago using Intel <a href="https://www.brendangregg.com/blog//2024-10-29/binary%20instrumentation">GTPin</a> but for widespread adoption it needs minimal overhead and ease of use so that developers don&#39;t hesitate to use this daily and to add it to deployment pipelines.</p>

<h2>What&#39;s a Flame Graph?</h2>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/flamegraph-cost.png" width="300"/></center></div>

<p>A <a href="https://www.brendangregg.com/flamegraphs.html">flame graph</a> is a visualization I invented in 2011 for showing sampled code stack traces. It has become the standard for CPU profiling and analysis, helping developers quickly find performance improvements and eliminate regressions. A CPU flame graph shows the &#34;big picture&#34; of running software, with x-axis proportional to CPU cost. The example picture on the right summarizes how easy it can be to go from compute costs to responsible code paths. Prior to flame graphs, it could take hours to understand a complex profile by reading through <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#Problem">hundreds of pages of output</a>. Now it takes seconds: all you have to do is look for the widest rectangles.</p>

<p>Flame graphs have had worldwide adoption. They have been the basis for five startups so far, have been adopted in over thirty performance analysis products, and have had <a href="https://www.brendangregg.com/Slides/YOW2022_flame_graphs/#8">over eighty implementations</a>.</p>

<p>My first implementation of flame graphs took a few hours on a Wednesday night after work. The real effort has been in the decade since, where I worked with different profilers, runtimes, libraries, kernels, compilers, and hypervisors to get flame graphs working properly in different environments, including fixing stack walking and symbolization. Earlier this year I posted about the final missing piece: Helping distros <a href="https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html">enable frame pointers</a> so that profiling works across standard system libraries.</p>

<p>Similar work is necessary for AI workloads: fixing stacks and symbols and getting profiling to work for different hardware, kernel drivers, user-mode drivers, frameworks, runtimes, languages, and models. A lot more work, too, as AI analysis has less maturity than CPU analysis.</p>

<h2>Searching Samples</h2>

<p>If you are new to flame graphs, it&#39;s worth mentioning the built-in search capability. In the earlier example, most of the stall samples are caused by sbid: software scoreboard dependency. As that may be a unique search term, you can run search (Ctrl-F, or click &#34;Search&#34;) on &#34;sbid&#34; and it will highlight it in magenta:</p>

<center><img src="https://www.brendangregg.com/blog/images/2024/AIflamegraph-search.png" width="530"/></center>

<p>Search also shows the total number of stack samples that contained sbid in the bottom right: 78.4%. You can search for any term in the flame graph: accelerator instructions, source paths, function names, etc., to quickly calculate the percentage of stacks where it is present (excluding vertical overlap) helping you prioritise performance work.</p>

<p>Note that the samples are EU stall-based, which means theoretical performance wins can take the percentages down to zero. This is different to timer-based samples as are typically used in CPU profiling. Stalls mean you better focus on the pain, the parts of the code that aren&#39;t making forward progress, but you aren&#39;t seeing resource usage by unstalled instructions. I&#39;d like to supuport timer-based samples in the future as well, so we can have both views.</p>

<h2>Who will use this?</h2>

<p>At a recent golang conference, I asked the audience of 200+ to raise their hands if they were using CPU flame graphs. Almost every hand went up. I know of companies where flame graphs are a daily tool that developers use to understand and tune their code, reducing compute costs. This will become a daily tool for AI developers.</p>

<p>My employer will use this as well for evaluation analysis, to find areas to tune to beat competitors, as well as to better understand workload performance to aid design.</p>

<h2>Why is AI profiling hard?</h2>

<p>Consider CPU instruction profiling: This is easy when the program and symbol table are both in the file system and in a standardized file format (such as ELF) as is the case with native compiled code (C). CPU profiling gets hard for JIT-complied code, like Java, as instructions and symbols are dynamically generated and placed in main memory (the process heap) without following a universal standard. For such JITted code we use runtime-specific methods and agents to retrieve snapshots of the heap information, which is different for each runtime.</p>

<p>AI workloads also have different runtimes (and frameworks, languages, user-mode drivers, compilers, etc.) any of which can require special tinkering to get their CPU stacks and symbols to work. These CPU stacks are shown as the red, orange, and yellow frames in the AI Flame Graph. Some AI workloads are easy to get these frames working, some (like PyTorch) are a lot more work. </p>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/AIsourcezoom.png" width="450"/></center></div>

<p>But the real challenge is instruction profiling of actual GPU and AI accelerator programs -- shown as the aqua and green frames -- and correctly associating them with the CPU stacks beneath them. Not only may these GPU and AI programs not exist in the file system, but they may not even exist in main memory! Even for running programs. Once execution begins, they may be deallocated from main memory and only exist in special accelerator memory, beyond the direct reach of OS profilers and debuggers. Or within reach, but only through a prohibitively high-overhead HW-specific debugger interface.</p>

<p>There&#39;s also no /proc representation for these programs either (I&#39;ve been proposing building an equivalent) so there&#39;s no direct way to even tell what is running and what isn&#39;t, and all the other /proc details. Forget instruction profiling, even ps(1) and all the other process tools do not work.</p>

<p>It&#39;s been a mind-bending experience, revealing what gets taken for granted because it has existed in CPU land for decades: A process table. Process tools. Standard file formats. Programs that exist in the file system. Programs running from main memory. Debuggers. Profiliers. Core dumping. Disassembling. Single stepping. Static and dynamic instrumentation. Etc. For GPUs and AI, this is all far less mature. It can make the work exciting at times, when you think something is impossible and then find or devise a way.</p>

<p>Fortunately we have a head start as some things do exist. Depending on the runtime and kernel driver, there are debug interfaces where you can list running accelerator programs and other statistics, as used by tools like intel_gpu_top(1). You can kill -9 a GPU workload using intel_gpu_abrt(1). Some interfaces can even generate basic ELF files for the running accelerator programs that you can try to load in a debugger like gdb(1). And there is support for GPU/AI program disassembly, if you can get your hands on the binary. It feels to me like GPU/AI debugging, OS style, is about two years old. Better than zero, but still early on, and lots more ahead of us. A decade, at least.</p>

<h2>What do AI developers think of this?</h2>

<p>We&#39;ve shown AI Flame Graphs to other AI developers at Intel and a common reaction is to be a bit puzzled, wondering what to do with it. AI developers think about their bit of code, but with AI Flame Graphs they can now see the entire stack for the first time, including the HW, and many layers they don&#39;t usually think about or don&#39;t know about. It basically looks like a pile of gibberish with their code only a small part of the flame graph.</p>

<div><center><a href="https://www.brendangregg.com/Slides/YOW2022_flame_graphs/#8"><img src="https://www.brendangregg.com/blog/images/2024/flamegraph-montage.png" width="190"/></a></center></div>

<p>This reaction is similar to people&#39;s first experiences with CPU flame graphs, which show parts of the system that developers and engineers typically don&#39;t work on, such as runtime internals, system libraries, and kernel internals. Flame graphs are great at highlighting the dozen or so functions that matter the most, so it becomes a problem of learning what those functions do across a few different code bases, which are typically open source. Understanding a dozen such functions can take a few hours or even a few days -- but if this leads to a 10% or 2x cost win, it is time well spent. And the next time the user looks at a flame graph, they start saying &#34;I&#39;ve seen that function before&#34; and so on. You can get to the point where understanding the bulk of a CPU flame graph takes less than a minute: look for the widest tower, click to zoom, read the frames, done.</p>

<p>I&#39;m encouraged by the success of CPU flame graphs, with over 80 implementations and countless real world case studies. Sometimes I&#39;m browsing a performance issue I care about on github and hit page down and there&#39;s a CPU flame graph. They are everywhere.</p>

<p>I expect AI developers will also be able to understand AI Flame Graphs in less than a minute, but to start with people will be spending a day or more browsing code bases they didn&#39;t know were involved. Publishing case studies of found wins will also help people learn how to interpret them, and also help explain the value.</p>

<h2>What about PyTorch?</h2>

<p>Another common reaction we&#39;ve had is that AI developers are using PyTorch, and initially we didn&#39;t support it as it meant walking Python stacks, which isn&#39;t trivial. But prior work has been done there (to support CPU profiling) and after a lot of tinkering we now have the first PyTorch AI Flame Graph:</p>

<center><a href="https://www.brendangregg.com/blog/images/2024/PyTorchFlamegraph.svg"><img src="https://www.brendangregg.com/blog/images/2024/PyTorchFlamegraph.png" width="700"/></a></center>

<p>(Click for interactive <a href="https://www.brendangregg.com/blog/images/2024/PyTorchFlamegraph.svg">SVG</a>.) The PyTorch functions are at the bottom and are colored pink. This example runs oneDNN kernels that are JIT-generated, and don&#39;t have a source path so that layer just reads &#34;jit&#34;. Getting all other the layers included was a real pain to get going, but an important milestone. We think if we can do PyTorch we can do anything.</p>

<p>In this flame graph, we show PyTorch running the Llama 2 7B model using the Intel Extensions for PyTorch (IPEX). This flame graph shows the origin of the GPU kernel execution all the way back to the Python source code shown in pink. Most samples are from a stack leading up to a gemm_kernel (matrix multiply) shown in aqua, which like the previous example has many stalls due to software scoreboarding.</p>

<p>There are two instructions (0xa30 and 0xa90) that combined are 27% of the entire profile. I expect someone will ask: Can&#39;t we just click on instructions and have it bring up a dissassembly view with full source? Yes, that should be possible, but I can&#39;t answer how we&#39;re going to provide this yet. Another expected question I can&#39;t yet answer: Since there are now multiple products providing AI auto-tuning of CPU workloads using CPU flame graphs (including <a href="https://granulate.io/">Intel Granulate</a>) can&#39;t we have AI auto-tuning of <em>AI</em> workloads using AI Flame Graphs?</p>

<h2>First Release: Sometimes hard and with moderate overhead</h2>

<p>Getting AI Flame Graphs to work with some workloads is easy, but others are currently hard and cost moderate overhead. It&#39;s similar to CPU profiling, where some workloads and languages are easy to profile, whereas others need various things fixed. Some AI workloads use many software dependencies that need various tweaks and recompilation (e.g., enabling frame pointers so that stack walking works) making setup time consuming. PyTorch is especially difficult and can take over a week of OS work to be ready for AI Flame Graphs. We will work on getting these tweaks changed upstream in their respective repositories, something involving teams inside and outside of Intel, and is a process I&#39;d expect to take at least a year. During that time AI workloads will gradually become easier to flame graph, and with lower-overhead as well.</p>

<p>I&#39;m reminded of eBPF in the early days: You had to patch and recompile the kernel and LLVM and Clang, which could take multiple days if you hit errors. Since then all the eBPF dependency patches have been merged, and default settings changed, so that eBPF &#34;just works.&#34; We&#39;ll get there with AI Flame Graphs too, but right now it&#39;s still those early days.</p>

<p>The changes necessary for AI Flame Graphs are really about improving debugging in general, and are a requirement for <a href="https://www.brendangregg.com/Slides/eBPFSummit2023_FastByFriday/">Fast by Friday</a>: A vision where we can root-cause analyze anything in five days or less.</p>

<h2>Availability</h2>

<p>AI Flame Graphs will first become available on the <a href="https://www.brendangregg.com/blog//2024-10-29/yes,%20Intel%20has%20a%20public%20cloud">Intel Tiber AI Cloud</a> as a preview feature for the Intel Data Center GPU Max Series. If you are currently deployed there you can ask through the Intel service channel for early access. As for if or when it will support other hardware types, be in other Intel products, be officially launched, be open source, etc., these involve various other teams at Intel and they need to make their own announcements before I can discuss them here.</p>

<h2>Conclusions</h2>

<p>Finding performance improvements for AI data centers of just fractions of a percent can add up to planetary savings in electricity, water, and money. If AI flame graphs have the success that CPU flame graphs have had, I&#39;d expect finding improvements of over 10% will be common, and 50% and higher will eventually be found*. But it won&#39;t be easy in these early days as there are still many software components to tweak and recompile, and software layers to learn about that are revealed in the AI flame graph.</p>

<p>In the years ahead I imagine others will build their own AI flame graphs that look the same as this one, and there may even be startups selling them, but if they use more difficult-to-use and higher-overhead technologies I fear they could turn companies off the idea of AI flame graphs altogether and prevent them from finding sorely needed wins. This is too important to do badly. AI flame graphs should be easy to use, cost negligible overhead, be production safe, and show everything. Intel has proven it&#39;s possible.</p>

<h2>Disclaimer</h2>

<p><SPAN size="-1">
* This is a personal blog post that makes personal predictions but not guarantees of possible performance improvements. Feel free to take any claim with a grain of salt, and feel free to wait for an official publication and public launch by Intel on this technology.</SPAN></p><SPAN size="-1">

</SPAN><p><SPAN size="-1"><sup>1</sup> Based on halving the Arm CEO Rene Haas&#39; estimate of 20-25% quoted in <a href="https://arstechnica.com/ai/2024/06/is-generative-ai-really-going-to-wreak-havoc-on-the-power-grid/">Taking a closer look at AI&#39;s supposed energy apocalypse</a> by Kyle Orland of ArsTechnica.
</SPAN></p>

<h2>Thanks</h2>

<p><i>Thanks to everyone at Intel who have helped us make this happen. Markus Flierl has driven this project and made it a top priority, and Greg Lavender has expressed his support. Special thanks to Michael Cole, Matthew Roper, Luis Strano, Rodrigo Vivi, Joonas Lahtinen, Stanley Gambarin, Timothy Bauer, Brandon Yates, Maria Kraynyuk, Denis Samoylov, Krzysztof Raszknowski, Sanchit Jain, Po-Yu Chen, Felix Degrood, Piotr Rozenfeld, Andi Kleen, and all of the other coworkers that helped clear things up for us, and thanks in advance for everyone else who will be helping us in the months ahead.</i></p><p><i>My final thanks is to the companies and developers who do the actual hands-on work with flame graphs, collecting them, examining them, finding performance wins, and applying them.</i></p>

</div></div>
  </body>
</html>
