<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/openlm-research/open_llama">Original</a>
    <h1>OpenLLaMA: An Open Reproduction of LLaMA</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">In this repo, we release a permissively licensed open source reproduction of Meta AI&#39;s <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" rel="nofollow">LLaMA</a> large language model. In this release, we&#39;re releasing a public preview of the 7B OpenLLaMA model that has been trained with 200 billion tokens. We provide PyTorch and Jax weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Stay tuned for our updates.</p>
<ul dir="auto">
<li><a href="https://huggingface.co/openlm-research/open_llama_7b_preview_200bt" rel="nofollow">JAX and PyTorch weights on Huggingface Hub</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-dataset-and-training" aria-hidden="true" href="#dataset-and-training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dataset and Training</h2>
<p dir="auto">We train our models on the <a href="https://www.together.xyz/blog/redpajama" rel="nofollow">RedPajama</a> dataset released by <a href="https://www.together.xyz/" rel="nofollow">Together</a>, which is a reproduction of the LLaMA training dataset containing over 1.2 trillion tokens. We follow the exactly same preprocessing steps and training hyperparameters as the original LLaMA paper, including model architecture, context length, training steps, learning rate schedule, and optimizer.  The only difference between our setting and the original one is the dataset used: OpenLLaMA employs the RedPajama dataset rather than the one utilized by the original LLaMA.</p>
<p dir="auto">We train the models on cloud TPU-v4s using <a href="https://github.com/young-geng/EasyLM">EasyLM</a>, a JAX based training pipeline we developed for training and fine-tuning language model. We employ a combination of normal data parallelism and <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/" rel="nofollow">fully sharded data parallelism (also know as ZeRO stage 3)</a> to balance the training throughput and memory usage. Overall we reach a throughput of over 1900 tokens / second / TPU-v4 chip in our training run. The training loss can be seen in the figure below.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/openlm-research/open_llama/blob/main/media/loss_200bt.png"><img src="https://github.com/openlm-research/open_llama/raw/main/media/loss_200bt.png" alt=""/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-evaluation" aria-hidden="true" href="#evaluation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Evaluation</h2>
<p dir="auto">We evaluated OpenLLaMA on a wide range of tasks using <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>.  The LLaMA results are generated by running the original LLaMA model on the same evaluation metrics. We note that our results for the LLaMA model differ slightly from the original LLaMA paper, which we believe is a result of different evaluation protocols. Similar differences have been reported in <a href="https://github.com/EleutherAI/lm-evaluation-harness/issues/443" data-hovercard-type="issue" data-hovercard-url="/EleutherAI/lm-evaluation-harness/issues/443/hovercard">this issue of lm-evaluation-harness</a>. Additionally, we present the results of GPT-J, a 6B parameter model trained on the <a href="https://pile.eleuther.ai/" rel="nofollow">Pile</a> dataset by <a href="https://www.eleuther.ai/" rel="nofollow">EleutherAI</a>.</p>
<p dir="auto">The original LLaMA model was trained for 1 trillion tokens and GPT-J was trained for 500 billion tokens, whereas OpenLLaMA was trained on 200 billion tokens.  We present the results in the table below. OpenLLaMA exhibits comparable performance to the original LLaMA and GPT-J across a majority of tasks, and outperforms them in some tasks. We expect that the performance of OpenLLaMA, after completing its training on 1 trillion tokens, will be enhanced even further.</p>
<table>
<thead>
<tr>
<th><strong>Task/Metric</strong></th>
<th><strong>GPT-J 6B</strong></th>
<th><strong>LLaMA 7B</strong></th>
<th><strong>Open LLaMA 7B Preview 200B Tokens</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>anli_r1/acc</td>
<td>0.32</td>
<td>0.35</td>
<td>0.34</td>
</tr>
<tr>
<td>anli_r2/acc</td>
<td>0.34</td>
<td>0.34</td>
<td>0.35</td>
</tr>
<tr>
<td>anli_r3/acc</td>
<td>0.35</td>
<td>0.37</td>
<td>0.34</td>
</tr>
<tr>
<td>arc_challenge/acc</td>
<td>0.34</td>
<td>0.39</td>
<td>0.31</td>
</tr>
<tr>
<td>arc_challenge/acc_norm</td>
<td>0.37</td>
<td>0.41</td>
<td>0.34</td>
</tr>
<tr>
<td>arc_easy/acc</td>
<td>0.67</td>
<td>0.68</td>
<td>0.66</td>
</tr>
<tr>
<td>arc_easy/acc_norm</td>
<td>0.62</td>
<td>0.52</td>
<td>0.59</td>
</tr>
<tr>
<td>boolq/acc</td>
<td>0.66</td>
<td>0.75</td>
<td>0.67</td>
</tr>
<tr>
<td>cb/acc</td>
<td>0.36</td>
<td>0.36</td>
<td>0.38</td>
</tr>
<tr>
<td>cb/f1</td>
<td>0.26</td>
<td>0.24</td>
<td>0.29</td>
</tr>
<tr>
<td>hellaswag/acc</td>
<td>0.50</td>
<td>0.56</td>
<td>0.47</td>
</tr>
<tr>
<td>hellaswag/acc_norm</td>
<td>0.66</td>
<td>0.73</td>
<td>0.63</td>
</tr>
<tr>
<td>openbookqa/acc</td>
<td>0.29</td>
<td>0.29</td>
<td>0.26</td>
</tr>
<tr>
<td>openbookqa/acc_norm</td>
<td>0.38</td>
<td>0.41</td>
<td>0.37</td>
</tr>
<tr>
<td>piqa/acc</td>
<td>0.75</td>
<td>0.78</td>
<td>0.74</td>
</tr>
<tr>
<td>piqa/acc_norm</td>
<td>0.76</td>
<td>0.78</td>
<td>0.74</td>
</tr>
<tr>
<td>record/em</td>
<td>0.88</td>
<td>0.91</td>
<td>0.87</td>
</tr>
<tr>
<td>record/f1</td>
<td>0.89</td>
<td>0.91</td>
<td>0.88</td>
</tr>
<tr>
<td>rte/acc</td>
<td>0.54</td>
<td>0.56</td>
<td>0.53</td>
</tr>
<tr>
<td>truthfulqa_mc/mc1</td>
<td>0.20</td>
<td>0.21</td>
<td>0.21</td>
</tr>
<tr>
<td>truthfulqa_mc/mc2</td>
<td>0.36</td>
<td>0.34</td>
<td>0.34</td>
</tr>
<tr>
<td>wic/acc</td>
<td>0.50</td>
<td>0.50</td>
<td>0.50</td>
</tr>
<tr>
<td>winogrande/acc</td>
<td>0.64</td>
<td>0.68</td>
<td>0.62</td>
</tr>
<tr>
<td>wsc/acc</td>
<td>0.37</td>
<td>0.35</td>
<td>0.57</td>
</tr>
<tr>
<td>Average</td>
<td>0.50</td>
<td>0.52</td>
<td>0.50</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto"><a id="user-content-preview-weights-release-and-usage" aria-hidden="true" href="#preview-weights-release-and-usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Preview Weights Release and Usage</h2>
<p dir="auto">To encourage the feedback from the community, we release a preview checkpoint of our weights. The checkpoint can be downloaded from <a href="https://huggingface.co/openlm-research/open_llama_7b_preview_200bt" rel="nofollow">HuggingFace Hub</a>. We release the weights in two formats: an EasyLM format to be use with our <a href="https://github.com/young-geng/EasyLM">EasyLM framework</a>, and a PyTorch format to be used with the <a href="https://huggingface.co/docs/transformers/index" rel="nofollow">Huggingface Transformers</a> library.</p>
<p dir="auto">For using the weights in our EasyLM framework, please refer to the <a href="https://github.com/young-geng/EasyLM/blob/main/docs/llama.md">LLaMA documentation of EasyLM</a>. Note that unlike the original LLaMA model, our OpenLLaMA tokenizer and weights are trained completely from scratch so it is no longer needed to obtain the original LLaMA tokenizer and weights. For using the weights in the transformers library, please follow the <a href="https://huggingface.co/docs/transformers/main/model_doc/llama" rel="nofollow">transformers LLaMA documentation</a>. Note that we use BOS (beginning of sentence) token (id=1) during training, so it is important to prepend this token for best performance during few-shot evaluation.</p>
<p dir="auto">Both our training framework EasyLM and the preview checkpoint weights are licensed permissively under the Apache 2.0 license.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-future-plans" aria-hidden="true" href="#future-plans"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Future Plans</h2>
<p dir="auto">The current release is only a preview of what the complete OpenLLaMA release will offer. We are currently focused on completing the training process on the entire RedPajama dataset. This can gives us a good apple-to-apple comparison between the original LLaMA and our OpenLLaMA. Other than the 7B model, we are also training a smaller 3B model in hope of facilitating language model usage in low resource use cases.  Please stay tuned for our upcoming releases.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contact" aria-hidden="true" href="#contact"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contact</h2>
<p dir="auto">We would love to get feedback from the community. If you have any questions, please open an issue or contact us.</p>
<p dir="auto">OpenLLaMA is developed by:
<a href="https://young-geng.xyz/" rel="nofollow">Xinyang Geng</a>* and <a href="https://www.haoliu.site/" rel="nofollow">Hao Liu</a>* from Berkeley AI Research.
*Equal Contribution</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-reference" aria-hidden="true" href="#reference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reference</h2>
<p dir="auto">If you found OpenLLaMA useful in your research or applications, please cite using the following BibTeX:</p>
<div data-snippet-clipboard-copy-content="@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}"><pre><code>@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}
</code></pre></div>
<div data-snippet-clipboard-copy-content="@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}"><pre><code>@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}
</code></pre></div>
<div data-snippet-clipboard-copy-content="@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\&#39;e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}"><pre><code>@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\&#39;e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
</code></pre></div>
</article>
          </div></div>
  </body>
</html>
