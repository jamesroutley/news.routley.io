<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vondra.me/posts/tuning-aio-in-postgresql-18/">Original</a>
    <h1>Tuning async IO in PostgreSQL 18</h1>
    
    <div id="readability-page-1" class="page"><div><p>PostgreSQL 18 was <a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=3d6a828938a5fa0444275d3d2f67b64ec3199eb7">stamped</a>
earlier this week, and as usual there’s a <a href="https://www.postgresql.org/docs/release/18.0/">lot of improvements</a>.
One of the big architectural changes is asynchronous I/O (AIO), allowing
asynchronous scheduling of I/O, giving the database more control and
better utilizing the storage.</p><p>I’m not going to explain how AIO works, or present detailed benchmark
results. There have been multiple <a href="https://pganalyze.com/blog/postgres-18-async-io">really</a>
<a href="https://www.dbi-services.com/blog/postgresql-18-support-for-asynchronous-i-o/">good</a>
<a href="https://www.pgedge.com/blog/highlights-of-postgresql-18">blog</a>
<a href="https://neon.com/postgresql/postgresql-18/asynchronous-io">posts</a> about
<a href="https://www.cybertec-postgresql.com/en/postgresql-18-better-i-o-performance-with-aio/">that</a>.
There’s also a great <a href="https://www.youtube.com/watch?v=GR5v9DHiS8w">talk from pgconf.dev 2025</a>
about AIO, and a recent <a href="https://talkingpostgres.com/episodes/what-went-wrong-what-went-right-with-aio-with-andres-freund">“Talking Postgres” podcast episode</a>
with Andres, discussing various aspects of the whole project. I highly
suggest reading / watching those.</p><p>I want to share a couple suggestions on how to tune the AIO in Postgres
18, and explain some inherent (but not immediately obvious) trade-offs
and limitations.</p><p>Ideally, this tuning advice would be included in the <a href="https://www.postgresql.org/docs/18/index.html">docs</a>.
But that requires a clear consensus on the suggestions, usually based
on experience from the field. And because AIO is a brand new feature,
it’s too early for that. We have done a fair amount of benchmarking
during development, and we used that to pick the defaults. But that
can’t substitute experience from running actual production systems.</p><p>So here’s a blog post with my personal opinions on how to (maybe) tweak
the defaults, and what trade offs you’ll have to consider.</p><h2 id="io_method--io_workers">io_method / io_workers</h2><p>There’s a handful of <a href="https://www.postgresql.org/docs/18/runtime-config-resource.html#GUC-IO-METHOD">parameters</a>
relevant to AIO (or I/O in general). But you probably need to worry
about just these two, introduced in Postgres 18:</p><ul><li><code>io_method = worker</code> (options: <code>sync</code>, <code>io_uring</code>)</li><li><code>io_workers = 3</code></li></ul><p>The other parameters (like <code>io_combine_limit</code>) have reasonable defaults.
I don’t have great suggestions on how to tune them, so just leave those
alone. In this post I’ll focus on the two important ones.</p><h3 id="io_method">io_method</h3><p>The <code>io_method</code> determines AIO actually handles requests - what process
performs the I/O, and how is the I/O scheduled. It has three possible
values:</p><ul><li><p><code>sync</code> - This is a “backwards compatibility” option, doing synchronous
I/O with <code>posix_fadvice</code> where supported. This prefetches data into
page cache, not into shared buffers.</p></li><li><p><code>worker</code> - Creates a pool of “IO workers”, doing the actual I/O. When
a backend needs to read a block from a data file, it inserts a
request into a queue in shared memory. An I/O worker wakes up, does
the <code>pread</code>, puts it into shared buffers and notifies the backend.</p></li><li><p><code>io_uring</code> - Each backend has a <code>io_uring</code> instance (a pair of queues)
and uses it to perform the I/O. Except that instead of doing <code>pread</code>
it submits the requests through <code>io_uring</code>.</p></li></ul><p>The default is <code>io_method = worker</code>. We did consider defaulting both to
<code>sync</code> or <code>io_uring</code>, but I think <code>worker</code> is the right choice. It’s
actually “asynchronous”, and it’s available everywhere (because it’s
our implementation).</p><p><code>sync</code> was seen as a “fallback” choice, in case we run into issues
during beta/RC. But we did not, and it’s not certain using <code>sync</code> would
actually help, because it still goes through the AIO infrastructure.
You can still use <code>sync</code> if you prefer to mimic older releases.</p><p><code>io_uring</code> is a popular way to do async I/O (and not just disk I/O!).
And it’s great, very efficient and lightweight. But it’s specific to
Linux, while we support a lot of platforms. We could have used
platform-specific defaults (similarly to <code>wal_sync_method</code>). But it
seemed like unnecessary complexity.</p><p><strong>Note:</strong> Even on Linux it’s hard to verify <code>io_uring</code>. Some container
runtimes (e.g. <a href="https://github.com/containerd/containerd/pull/9320">containerd</a>)
disabled <code>io_uring</code> support a while back, because of security risks.</p><p>None of the <code>io_method</code> options is “universally superior.” There’ll
always be workloads where A outperforms B and vice versa. In the end,
we wanted most systems to use AIO and get the benefits, and we wanted
to keep things simple, so we kept <code>worker</code>.</p><p><strong>Advice</strong>: My advice is to stick to <code>io_method = worker</code>, and to adjust
the <code>io_workers</code> value (as explained in the following section).</p><h3 id="io_workers">io_workers</h3><p>The Postgres defaults are very conservative. It will start even on a
tiny machine like Raspberry Pi. Which is great! The flip side is it’s
terrible for typical database servers which tend to have much more
RAM/CPU. To get good performance on such larger machines, you need to
adjust a couple parameters (<code>shared_buffers</code>, <code>max_wal_size</code>, …).</p><p>I wish we had an automated way to pick “good” initial values for these
basic parameters, but it’s way harder than it looks. It depends a lot
on the context (e.g. other stuff might be running on the same system).
At least there are tools like <a href="https://pgtune.leopard.in.ua/">PGTune</a>
that will recommend sensible values …</p><p>This certainly applies to the <code>io_workers = 3</code> default, which creates
just 3 I/O workers. That may be fine on a small machine with 8 cores,
but it’s definitely not enough for 128 cores.</p><p>I can actually demonstrate this using results from a benchmark I did
as input for picking the <code>io_method</code> default. The benchmark generates
a synthetic data set, and then runs queries matching parts of the data
(while forcing a particular scan type).</p><p><em><strong>Note</strong>: The benchmark (along with scripts, a lot of results and a
much more detailed explanation) was originally shared in the
<a href="https://www.postgresql.org/message-id/e6db33f3-50de-43d3-9d9f-747c3b376e80%40vondra.me">pgsql-hackers</a>
thread about the <code>io_method</code> default. Look at that thread for more
details and feedback from various other people. The presented results
are from a small workstation with Ryzen 9900X (12 cores/24 threads),
and 4 NVMe SSDs (in RAID0).</em></p><p>Here’s a chart comparing query timing for different <code>io_method</code> options
[<a href="https://vondra.me/posts/tuning-aio-in-postgresql-18/iomethod.pdf">PDF</a>]:</p><p><a href="https://vondra.me/posts/tuning-aio-in-postgresql-18/iomethod.png"><img src="https://vondra.me/posts/tuning-aio-in-postgresql-18/iomethod.png" title="comparison of query timing by io_methods" alt="comparison of query timing by io_methods"/></a></p><p>Each color is a different <code>io_method</code> value (17 is “Postgres 17”).
There are two data data series for “worker”, with different numbers
of workers (3 and 12). This is for two data sets:</p><ul><li><p>uniform - uniform distribution (so the I/O is entirely random)</p></li><li><p>linear_10 - sequential with a bit of randomness (imperfect correlation)</p></li></ul><p>The charts show a couple very interesting things:</p><ul><li><p><strong>index scans</strong> - <code>io_method</code> has no effect, which makes perfect sense
because index scans do not use AIO yet (all the I/O is synchronous).</p></li><li><p><strong>bitmap scans</strong> - The behavior is a lot messier. The <code>worker</code> method
performs best, but only with 12 workers. With the default 3 workers
it actually performs poorly for low selectivity queries.</p></li><li><p><strong>sequential scans</strong> - There’s a clear difference between the methods.
<code>worker</code> is the fastest, about twice as faster than <code>sync</code> (and PG17).
<code>io_uring</code> is somewhere in between.</p></li></ul><p>The poor performance of <code>worker</code> with 3 I/O workers for bitmap scans is
even more visible with log-scale y-axis [<a href="https://vondra.me/posts/tuning-aio-in-postgresql-18/iomethod-log.pdf">PDF</a>]:</p><p><a href="https://vondra.me/posts/tuning-aio-in-postgresql-18/iomethod-log.png"><img src="https://vondra.me/posts/tuning-aio-in-postgresql-18/iomethod-log.png" title="comparison of query timing by io_methods (log-scale)" alt="comparison of query timing by io_methods (log-scale)"/></a></p><p>The <code>io_workers=3</code> configuration is consistently the slowest (in the
linear chart this was almost impossible to notice).</p><p>The good thing is that while I/O workers are not free, they are not too
expensive either. So if you have extra workers, that’s probably better
than having too few.</p><p>In the future, we’ll probably make this “adaptive” by starting/stopping
workers based on demand. So we’d always have just the right number.
There’s even a <a href="https://commitfest.postgresql.org/patch/5913/">WIP patch</a>,
but it didn’t make it into Postgres 18. (This would be a good time to
take a look and review it!)</p><p><strong>Advice</strong>: Consider increasing <code>io_workers</code>. I don’t have a great value
or formula to use, maybe something like 1/4 of cores would work?</p><h2 id="trade-offs">Trade offs</h2><p>There’s no universally optimal configuration. I saw suggestions to “use
io_uring for maximum efficiency”, but the earlier benchmark clearly
shows <code>io_uring</code> being significantly slower than <code>worker</code> for sequential
scans.</p><p>Don’t get me wrong. I love <code>io_uring</code>, it’s a great interface. And the
advice is not “wrong” either. Any tuning advice is a simplification,
and there will be cases contradicting it. The world is never as simple
as the advice makes it seem. It hides the grotty complexity behind a
much simpler rule, that’s the whole point of having such advice.</p><p>So what are the trade offs and differences between the AIO methods?</p><h3 id="bandwidth">bandwidth</h3><p>One big difference between <code>io_uring</code> and <code>worker</code> is where the work
happens. With <code>io_uring</code>, all the work happens in the backend itself,
while with <code>worker</code> this happens in a separate process.</p><p>This may have some interesting consequences on bandwidth, depending on
how expensive it’s to handle the I/O. And it can be fairly expensive,
because it involves:</p><ul><li>the actual I/O</li><li>verifying checksums (which are enabled by default in Postgres 18)</li><li>copying the data into shared buffers</li></ul><p>With <code>io_uring</code>, all of this happens in the backend itself. The I/O part
may be more efficient, but the checksums / <code>memcpy</code> can be a bottleneck.
With <code>worker</code>, this work is effectively divided between the workers. If
you have one backend and 3 workers, the limits are 3x higher.</p><p>Of course, this goes the other way too. If you have 16 connections, then
with <code>io_uring</code> this is 16 processes that can verify checksums, etc.
With <code>worker</code>, the limit is whatever <code>io_workers</code> is set to.</p><p>This is where my advice to set <code>io_workers</code> to ~25% of the cores comes
from. I can imagine going higher, possibly up to one IO worker per core.
In any case, 3 seems clearly too low.</p><p><strong>Note:</strong> I believe the ability to spread costs over multiple processes
is why <code>worker</code> outperforms <code>io_uring</code> for sequential scans. The ~20%
difference seems about right for checksums and memcpy in this benchmark.</p><h3 id="signals">signals</h3><p>Another important detail is the cost of inter-process communication
between the backend and the IO worker(s), which is based on UNIX
signals. Performing an I/O looks like this:</p><ol><li>backend adds a read request to a queue in shared memory</li><li>backend sends a <code>signal</code> to a IO worker, to wake it up</li><li>IO worker performs the I/O requested by the backend, and copies the
data into shared buffers</li><li>IO worker sends a <code>signal</code> the backend, notifying it about the I/O
completion</li></ol><p>In the worst case, this means a round trip with 2 signals per 8K block.
The trouble is, signals are not free - a process can only do a finite
number of those per second.</p><p>I wrote a <a href="https://vondra.me/posts/tuning-aio-in-postgresql-18/signal-echo.c">simple benchmark</a>, sending signals between
two processes. On my machines, this reports 250k-500k round trips per
second. If each 8K block needs a round trip, this means 2-4GB/s. That’s
not a lot, especially considering the data may already be in page cache,
not just for cold data read from storage. According to a <a href="https://vondra.me/posts/tuning-aio-in-postgresql-18/page-cache-test.c">test copying
data from page cache</a>, a process can do 10-20GB/s,
so about 4x more. Clearly, signals may be a bottleneck.</p><p><em><strong>Note</strong>: The exact limits are hardware-specific, and may be much lower
on older machines. But the general observation holds on all machines I
have access to.</em></p><p>The good thing is this only affects a “worst case” workload, reading
8KB pages one by one. Most regular workloads don’t look like this.
Backends usually find a lot of buffers in shared memory already (and
then no I/O is needed). Or the I/O happens in larger chunks thanks to
look-ahead, which amortizes the signal cost over many blocks. I don’t
expect this to be a serious problem.</p><p>There’s a longer discussion about the AIO overheads (not just due to
signals) in the <a href="https://www.postgresql.org/message-id/1c9302da-c834-4773-a527-1c1a7029c5a3@vondra.me">index prefetching thread</a>.</p><h3 id="file-limit">file limit</h3><p>The <code>io_uring</code> doesn’t need any IPC, so it’s not affected by the signal
overhead, or anything like that. But <code>io_uring</code> has limits too, just in
a different place.</p><p>For example, each process is subject to per-process bandwidth limits
(e.g. how much memcpy can a single process do). But judging by the
page-cache test, those limits are fairly high - 10-20GB/s, or so.</p><p>Another thing to consider is that <code>io_uring</code> may need a fair number of
file descriptors. As explained in <a href="https://www.postgresql.org/message-id/xvd2cyrtd4wk42ugweydxfcy3bwtaymu4gqmky5fpfcu6xia4m%40qbgeq23yncch">this pgsql-hackers</a>
thread:</p><blockquote><p>The issue is that, with io_uring, we need to create one FD for each
possible child process, so that one backend can wait for completions
for IO issued by another backend [1]. Those io_uring instances need
to be created in postmaster, so they’re visible to each backend.
Obviously that helps to much more quickly run into an unadjusted soft
RLIMIT_NOFILE, particularly if max_connections is set to a higher
value.</p></blockquote><p>So if you decide to use <code>io_uring</code>, you may need to adjust <code>ulimit -n</code>
too.</p><p><em><strong>Note</strong>: This is not the only place in Postgres code where you may run
into the limit on file descriptors. About a year ago I posted a
<a href="https://vondra.me/posts/patch-idea-statistics-for-file-descriptor-cache/">patch idea related to file descriptor cache</a>.
Each backend keeps up to <code>max_files_per_process</code> open file descriptors,
and by default that GUC is set to 1000. That used to be enough, but with
partitioning (or schema per tenant) it’s fairly easy to trigger a storm
of expensive open/close calls. That’s a separate (but similar) issue.</em></p><h2 id="summary">Summary</h2><p>AIO is a massive architectural change, and in Postgres 18 it has various
limitations. It only supports reads, and some operations still use the
old synchronous I/O. Those limitations are not permanent, and should
be addressed in future releases.</p><p>Based on the discussion in this blog post, my tuning advice is to:</p><ul><li><p><strong>Keep the <code>io_method = worker</code> default</strong>, unless you can demonstrate
the <code>io_uring</code> actually works better for your workload. Use <code>sync</code>
only if you need a behavior as close to Postgres 17 as possible (even
if it means being slower in some cases).</p></li><li><p><strong>Increase <code>io_workers</code></strong> to a value considering the total number of
cores. Something like 25% cores seems reasonable, possibly even 100%
in extreme cases.</p></li></ul><p>If you come up with some interesting observations, please report them
either to me or (even better) to the <a href="https://www.postgresql.org/list/pgsql-hackers/">pgsql-hackers</a>,
so that we can consider that when adding tuning advice to the docs.</p></div><p>Do you have feedback on this post? Please reach out by e-mail to <a href="mailto:tomas@vondra.me">tomas@vondra.me</a>.</p></div>
  </body>
</html>
