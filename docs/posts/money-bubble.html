<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.tbray.org/ongoing/When/202x/2024/02/25/Money-AI-Bubble">Original</a>
    <h1>Money bubble</h1>
    
    <div id="readability-page-1" class="page"><div id="centercontent">
<p itemprop="description">I think I’m probably going to lose quite a lot of money in the next year or two. It’s partly AI’s fault, but not
    mostly. Nonetheless I’m mostly going to write about AI, because it intersects the technosphere, where I’ve lived for
    decades.</p>

<p>I’ve given up having a regular job. The family still has income but mostly we’re harvesting our
    savings, built up over decades in a well-paid profession. Which means that we are, willy-nilly, investors. And thus aware of the
    fever-dream finance landscape that is InvestorWorld.</p>

<p id="p-1"><span>The Larger Bubble</span> · 
Put in the simplest way: Things have been too good for too long in InvestorWorld: low interest, high profits, the unending rocket
    rise of the Big-Tech sector, now with AI afterburners. Wile E. Coyote hasn’t actually run off the edge of the cliff yet, but
    there are just way more ways for things to go wrong than right in the immediate future.</p>

<p>If you want to dive a little deeper, <cite>The Economist</cite> has a sharp (but
    paywalled) take in
    <a href="https://www.economist.com/finance-and-economics/2024/02/25/stockmarkets-are-booming-but-the-good-times-are-unlikely-to-last">Stockmarkets
    are booming. But the good times are unlikely to last</a>. Their argument is that profits are overvalued by investors because, in
    recent years, they’ve always gone up. Mr Market ignores the fact that that at least some of those gleaming profits are artifacts of
    tax-slashing by right-wing governments.</p>

<p>That piece considers the observation that “Many investors hope that AI will ride to the rescue” and is politely
    skeptical.</p>

<p id="p-2"><span>Popping the bubble</span> · 
My own feelings aren’t polite; closer to
    <a href="https://finance.yahoo.com/news/yep-you-are-living-in-a-nvidia-led-tech-bubble-110014738.html">Yep, you are living in a
    Nvidia-led tech bubble</a> by Brian Sozzi over at Yahoo! Finance.</p>

<p>Sozzi is fair, pointing out that this bubble feels different from the cannabis and crypto crazes; among other things,
    chipmakers and cloud providers are reporting big high-margin revenues for real actual products. But he hammers the central point:
    What we’re seeing is FOMO-driven dumb money thrown at technology by people who have no hope of
    understanding it. Just because everybody else is and because the GPTs and image generators have cool demos.
    Sozzi has the numbers, looking at valuations through standard old-as-dirt filters and shaking his head at what he sees.</p>

<p>What’s going to happen, I’m pretty sure, is that AI/ML will, inevitably, disappoint; in the financial sense I mean, probably
    doing some useful things, maybe even a lot, but not generating the kind of profit explosions that you’d need to justify
    the bubble. So it’ll pop, and my bet it is takes a bunch of the finance world with it. As bad as 2008? Nobody knows, but it
    wouldn’t surprise me.</p>

<p>The rest of this piece considers the issues facing AI/ML,  with the goal of showing why I see it as
    a bubble-inflator and eventual bubble-popper.</p>

<p>First, a disclosure: I speak as an educated amateur. I’ve never gone much below the surface of the technology, never
    constructed a model or built model-processing software, or looked closely at the math.  But I think the discussion below still
    works.</p>

<p id="p-3"><span>What’s good about AI/ML</span> · 
Spoiler: I’m not the kind of burn-it-with-fire skeptic that I became around anything blockchain-flavored. It is clear
    that generative models manage to embed significant parts of the structure of language, of code, of pictures, of
    many things where that has previously not been the case. The understanding is sufficient to reliably accomplish the objective:
    <i>Produce plausible output</i>.</p>

<p>I’ve read enough Chomsky to believe that facility with language is a defining characteristic of intelligence. More than that, a
    necessary but not sufficient ingredient.  I dunno if anyone will build an AGI in my lifetime, but I am confident that the task
    would remain beyond reach without the functions offered by today’s generative models.</p>

<p>Furthermore, I’m super impressed by something nobody else seems to talk about: Prompt parsing. Obviously, prompts are
    processed into a representation that reliably sends the model-traversal logic down substantially the right
    paths. The LLMbots of this world may regularly be crazy and/or just wrong, but they do consistently if not correctly address the
    substance of the prompt.
    There is seriously good natural-language engineering going on here that AI’s critics aren’t paying enough attention
    to.</p>

<p>So I have no patience with those who scoff at today’s technology, accusing it being a glorified Markov chain. Like the
    song says:  Something’s
    happening here! (What it is ain’t exactly clear.)</p>

<p>It helps that in the late teens I saw neural-net pattern-matching at work on real-world problems from close up and
    developed serious respect for what that technology can do; An example is EC2’s
    <a href="https://aws.amazon.com/blogs/compute/evaluating-predictive-scaling-for-amazon-ec2-capacity-optimization/">Predictive Auto
    Scaling</a> (and gosh, it looks like
    <a href="https://www.google.com/search?rls=en&amp;q=predictive+auto+scaling&amp;ie=UTF-8&amp;oe=UTF-8">the competition has it
    too</a>).</p>

<p>And recently, Adobe Lightroom has shipped a pretty awesome “Select Sky” feature. It makes my M2 MacBook
    Pro think hard for a second or two, but I rarely see it miss even an isolated scrap of sky off in the corner of the frame.  It
    allows me, in a picture like this, to make the sky’s brightness echo the water’s.</p>

<p><a href="https://www.tbray.org/ongoing/When/202x/2024/02/25/-big/PXL_20240111_213727870.jpg.html"><img alt="Brightly-lit boats on dark water under a dark sky" title="Brightly-lit boats on dark water under a dark sky" src="https://www.tbray.org/ongoing/When/202x/2024/02/25/PXL_20240111_213727870.png"/></a></p>
<p>And of course I’ve heard about success stories in radiology and other disciplines.</p>

<p>Thus, please don’t call me an “AI skeptic” or some such. There is a there there.</p>

<p id="p-4"><span>But…</span> · 
Given that, why do I still think that the flood of money being thrown at this tech is dumb, and that most of it will be lost?
    Partly just because of that flood. When financial decision makers throw loads of money at things they don’t
    understand, lots of it is <em>always</em> lost.</p>

<p>In the Venture-Capital business, that’s an understood part of the business
    cycle; they’re looking to balance that out with a small number of 10x startup wins.
    But when big old insurance companies and airlines and so on are piling in and releasing effusive statements about building
    the company around some new tech voodoo, the outcome, in my experience, is very rarely good.</p>

<p>But let’s be specific.</p>

<p id="p-5"><span>Meaning</span> · 
As I said above, I think the human mind has a large and important language-processing system.  But that’s not all. It’s also
    a (slow, poorly-understood) computer, with access to a medium-large database of facts and recollections, an ultra-slow numeric
    processor, and a facilities for estimation, prediction, speculation, and invention. Let’s group all this stuff together and call
    it “meaning”.</p>

<p>Have a look at <a href="https://aclanthology.org/2020.acl-main.463.pdf">Climbing towards NLU:
    On Meaning, Form, and Understanding in the Age of Data</a> by Emily Bender and Alexander Koller (July 2000). I don’t agree with
    all of it, and it addresses an earlier generation of generative models, but it’s very thought-provoking. It postulates the
    “Octopus Test”, a good variation on the bad old Chinese-Room analogy. It talks usefully about how human language acquisition
    works. A couple of quotes: “It is instructive to look at the past to appreciate this question. Computational linguistics has
    gone through many fashion cycles over the course of its history” and “In this paper, we have argued that in contrast to some
    current hype, meaning cannot be learned from form alone.”</p>

<p>I’m not saying these problems can’t be solved. Software systems can be equipped with databases of facts, and who knows,
    perhaps some day estimation, prediction, speculation, and invention. But it’s not going to be easy.</p>

<p id="p-7"><span>Difficulty</span> · 
I think there’s a useful analogy between the stories AI and of self-driving cars. As I write this, 
    Apple has apparently decided that 
    <a href="https://arstechnica.com/gadgets/2024/02/after-a-decade-of-stops-and-starts-apple-kills-its-electric-car-project">generative 
    AI is easier than shipping an autonomous car</a>. I’m particularly sensitive to this analogy because back around 2010, as the
    first self-driving prototypes were coming into view, I predicted, loudly and in public, that this technology was about to become
    ubiquitous and turn the economy inside out. Ouch.</p>

<p>There’s a pattern: The technologies that really do change the world tend to have strings of successes, producing obvious
    benefits even in their earliest forms, to the extent that geeks load them in the back floor of organizations just to get shit
    done. As they say, “The CIO is the last to know.”</p>

<p>Contrast cryptocurrencies and blockchains, which limped along from year to year, always promising a brilliant future, never
    doing anything useful.  As to the usefulness of self-driving technology, I still think it’s gonna get there, but it’s surrounded
    by a cloud of litigation.</p>

<p>Anyhow, anybody who thinks that it’ll be easy to teach “meaning” (as I described it above) to today’s generative AI is a fool,
    and you shouldn’t give them your money.</p>

<p id="p-6"><span>Money and carbon</span> · 
Another big problem we’re not talking about enough is the cost of generative AI.
    <cite>Nature</cite> offers    
    <a href="https://www.nature.com/articles/d41586-024-00478-x">Generative AI’s environmental costs are soaring — and mostly
    secret</a>. In a Mastodon thread,
    <a href="https://phanpy.social/#/social.v.st/a/109360452395342558">@Quixoticgeek@social.v.st</a> says 
    <a href="https://phanpy.social/#/social.v.st/s/111991430750212364">We need to talk about data centres</a>, and includes a few
    hard and sobering numbers.</p>

<p>Short form: This shit is <em>expensive</em>, in dollars and in carbon load. Nvidia pulled in
    <a href="https://investor.nvidia.com/news/press-release-details/2024/NVIDIA-Announces-Financial-Results-for-Fourth-Quarter-and-Fiscal-2024/">$60.9
    billion in 2023, up 126% from the previous year</a>, and is heading for a $100B/year run rate, while reporting a 75% margin.</p>

<p>Another thing these articles <em>don’t</em> mention is that building, deploying, and running generative-AI systems requires significant
    effort from a small group of people who now apparently constitute the world’s highest-paid cadre of engineers. And good luck
    trying to hire one if you’re a mainstream company where IT is a cost center.</p>

<p>All this means that for the technology to succeed, it not only has to do something useful, but people and businesses will have to
    be ready to pay a significantly high price for that something.</p>

<p>I’m not saying that there’s nothing that qualifies, but I am betting that it’s not in ad-supported territory.</p>

<p>Also, it’s going to have to deal with pushback from unreasonable climate-change resisters like, for example, me.</p>

<p id="p-8"><span>Anyhow…</span> · 
I kind of flipped out, and was motivated to finish this blog piece, when I saw
    <a href="https://www.engadget.com/uk-government-wants-to-use-ai-to-cut-civil-service-jobs-140031159.html">this</a>: “UK
    government wants to use AI to cut civil service jobs: Yes, you read that right.” The idea<span> —</span> to have
    citizen input processed and responded to by an LLM<span> —</span> is hideously toxic and broken; and usefully
    reveals the kind of thinking that makes morally crippled leaders all across our system love this technology.</p>

<p>The road ahead looks bumpy from where I sit. And when the business community wakes up and realizes that replacing
    people with shitty technology doesn’t show up as a positive on the financials after you factor in the consequences of customer
    rage, that’s when the hot air gushes out of the bubble.</p>

<p>It might not take big chunks of InvestorWorld with it. But I’m betting it does.</p>

<hr/>


<hr/>

</div></div>
  </body>
</html>
