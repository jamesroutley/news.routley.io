<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/Articles/1027814/">Original</a>
    <h1>Python audio processing with pedalboard</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>
The
<a href="https://pypi.org/project/pedalboard/"><tt>pedalboard</tt></a>
library for Python is aimed at audio processing of various sorts, from
converting between formats to adding audio effects.  The maintainer of
<tt>pedalboard</tt>, Peter Sobot, gave a talk about audio in Python at
<a href="https://us.pycon.org/2025/">PyCon US 2025</a>, which was held in Pittsburgh,
Pennsylvania in May.  He started from the basics of digital audio and then
moved into working with <tt>pedalboard</tt>. There were, as might be guessed, audio examples
in the talk, along with some visual information; interested readers may want to view the <a href="https://www.youtube.com/watch?v=kpKrsFqJgXc">YouTube video</a> of the
presentation.
</p>

<p>
Sobot works for Spotify as a machine-learning engineer in its audio
intelligence lab, so he works on a team using machine-learning models to
analyze music.  The company has various <a href="https://developer.spotify.com/documentation/web-api">open APIs</a>
that can be used to analyze audio tracks, but it has also released code as
open-source software; <tt>pedalboard</tt> is available under the GPLv3.  It
has also released open models, such as <a href="https://basicpitch.spotify.com/">Basic Pitch</a>, which turns audio
data into <a href="https://en.wikipedia.org/wiki/MIDI">Musical Instrument
Digital Interface</a> (MIDI) files; the model can be used on the web or
incorporated into other tools, he said.
</p>

<p>
He noted that he had created a <a href="https://spotify.github.io/pedalboard/demo/">demo</a> for the talk
that attendees could try out during his presentation; it used Python and <tt>pedalboard</tt>
in the browser (by way of <a href="https://pyodide.org/en/stable/">Pyodide</a> and <a href="https://webassembly.org/">WebAssembly</a>) to allow users to create
audio. &#34;<q>If somebody were to make sound during my talk because they&#39;re
playing with this REPL, I would be overjoyed.</q>&#34;  He got his wish a
little over halfway into his 30-minute talk.
</p>

<h4>Digital audio</h4>

<p>
To understand how digital audio works, you need to understand how sound
works, Sobot said.  &#34;<q>Sound is really just pressure moving through the
air</q>&#34;; his voice is moving the air in front of his mouth, which the
microphone picks up and sends to the speakers, which create pressure waves
by moving the air there.  As with anything physical that can be stored
into a computer, the way to get sound into the machine is to measure it.
</p>

<p><a href="https://lwn.net/Articles/1028170/">
<img src="https://static.lwn.net/images/2025/pycon-sobot-sm.png" alt="[Peter Sobot]" title="Peter Sobot" width="280" height="219"/>
</a></p><p>
He showed a simple graph with time as the x-axis and pressure on the
y-axis; he then played a short saxophone clip, with each note showing up as
a dot on the graph.  The ten or so dots obviously corresponded to the notes in some
fashion, but it was not entirely clear how, in part because each measurement
was only taken every 0.4 seconds.  A sample rate of one sample per 0.4
seconds is 2.5Hz.  Sample rate is important for digital audio; &#34;<q>the
faster that you sample, the higher quality  audio that you get</q>&#34;.  It is
not just quality, though, higher sampling rate means that more details,
especially high-frequency details, are captured.
</p>

<p>
Looking a little more closely at the graph, he pointed out that silence
(0.0) is not at the bottom of the y-axis as might be inferred, but is in
the middle; some of the points measured are above that line, some below,
because the pressure is a wave that is moving back and forth, he said.  The
maximum loudness (+/- 1.0) is both at the bottom and the top of the y-axis;
our ears do not distinguish between waves moving toward or away from us, he
said, but the measurements do.  Each of the points on the graph has a
pressure value that can be turned into a number, such as 60% amplitude (or
0.60) or 90% amplitude in a negative direction (-0.90).
</p>

<p>
If you wanted to reproduce that audio clip, though, you would need much
higher-fidelity sampling, so he showed the graph that results from sampling
at a standard <a href="https://en.wikipedia.org/wiki/44,100_Hz">digital-audio rate of 44,100Hz</a>.  That graph (or waveform) has
so many points that you cannot distinguish individual measurements anymore,
though you can still see where the original points came from.
</p>

<p>
He zoomed the graph in to see it at millisecond scale, which consisted of
around ten close-to-identical waves.  Measuring the waves from peak to peak
showed that the period was 5.7142ms—the reciprocal of that is 174.61Hz,
which can be looked up in a <a href="https://muted.io/note-frequencies/">table of note frequencies</a> to show that it is an
<a href="https://en.wikipedia.org/wiki/F_(musical_note)">F</a> (specifically F<sub>3</sub>). 
</p>

<p>
Looking at the microsecond scale allows seeing the individual measurements
that were made by the analog-to-digital converter when the audio was
digitized. That can be looked at as a string of numbers, such as -0.003, 0.018,
0.128, and so on.  That is all that digital audio is, Sobot said; it is
just a stream of numbers coupled with a sample rate, which is enough
information to reproduce the original sound.
</p>

<p>
In order to store that information on disk, it could written out as, say,
64-bit floating-point numbers, but that takes a lot of space: roughly 21MB
per minute per channel.  That is not really workable for streaming audio
over the network or for storing a music collection.  &#34;<q>So, instead, very
smart people came up with compression algorithms</q>&#34;, such as MP3 or Ogg
Vorbis, which can reduce that size for a channel to around 1MB per minute.
The algorithms do that &#34;<q>by throwing away parts of the audio that we
can&#39;t really hear</q>&#34; because the frequency is outside of our hearing or
the sound is covered up by other parts of the audio.
</p>

<h4>Audio in Python</h4>

<p>
But these compressed floating-point representations no longer correspond to
the original data; in order to work with them, there will be a need to
convert them using a library of some sort.  Since Python is a &#34;batteries
included&#34; language, there should be something in the standard library to
work with audio data, he said.  The <a href="https://docs.python.org/3/library/wave.html"><tt>wave</tt> module</a>
is available, but it provides a pretty low-level API.  He showed how it can
be used to retrieve the floating-point values, &#34;<q>but it&#39;s kind of a
pain</q>&#34;, so he would not recommend that approach.
</p>

<p>
Instead, he showed the <tt>pedalboard</tt> interface, which is much more natural to use.
From his slides (which do not appear to be online):
</p><pre>    from pedalboard.io import AudioFile

    with AudioFile(&#34;my_favourite_song.mp3&#34;) as f:
        f.samplerate    # =&gt; 44100
        f.num_channels  # =&gt; 2
        f.read(3)       # =&gt; array of shape (2, 3)
        # array([
        #    [ 0.01089478, 0.00302124, 0.00738525],
        #    [ ... ]
        # ])
</pre><p>
The </p><tt>read()</tt><p> function returns a <a href="https://numpy.org/">NumPy</a> N-dimensional array (</p><a href="https://numpy.org/doc/stable/reference/arrays.ndarray.html"><tt>ndarray</tt></a><p>)
with two rows (one for each channel) each having three sample values (as
requested by the </p><tt>read(3)</tt><p>).  The first row in the array is the left
channel and the other is the right.  He then showed ways to use <a href="https://numpy.org/doc/stable/user/basics.indexing.html#slicing-and-striding">array
slices</a> to select various portions of the audio:
</p><pre>    with AudioFile(&#34;my_favourite_song.mp3&#34;) as f:
        audio = f.read(f.frames)  # =&gt; shape (2, 1_323_000)
        audio[1]  # right channel ...
        audio[:, :100]  # first 100 samples, stereo
        audio[:, :f.samplerate * 10]  # first 10 seconds
        audio[:, -(f.samplerate * 10):]  # last 10 seconds
</pre><p>
In addition, the file does not have to all be read into memory like it is
above, the </p><a href="https://spotify.github.io/pedalboard/reference/pedalboard.io.html#pedalboard.io.ReadableAudioFile.seek"><tt>seek()</tt></a><p>
function can be used with </p><tt>read()</tt><p> to extract arbitrary sections of the
audio data.
</p>

<p>
He displayed a small program to add a delay (or echo) effect to the
audio, by simply doing math on the sample values and plugging them back
into the array:
</p><pre>    ...
    mono = audio[0]  # one channel
    delay_seconds = 0.2  # 1/5 of a second
    delay_samples = int(f.samplerate * delay_seconds)
    volume = 0.75  # 75% of original volume
    for i in range(len(mono) - delay_samples):
        mono[i + delay_samples] += mono[i] * volume
</pre>


<p>
He played a short clip of a major scale on the piano, both before and after
the effect, which was easily noticeable both audibly and in the graphs of
the waveforms that he also displayed.  That showed the kinds of effects that a
few lines of Python code can create.  His next example added some
distortion (using the <a href="https://docs.python.org/3/library/math.html#math.tanh"><tt>math.tanh()</tt></a>
hyperbolic tangent function) to a short guitar riff—once again with just a
few lines of Python.
</p>

<h4>Audio problems</h4>

<p>
There are some pitfalls to working with audio and working with it in Python
that he wanted to talk about.  He gave an example of a program to increase
the volume of an audio file, perhaps as a web service, by reading in all of
the samples, then multiplying them by two (&#34;<q>don&#39;t do this</q>&#34;). The
resulting file can be written out using <tt>pedalboard</tt> in much the same way:
</p><pre>    with AudioFile(&#34;out.mp3&#34;, &#34;w&#34;, f.samplerate) as o:
        o.write(louder_audio)
</pre><p>
That program might work for a while, making users happy, until one day when
it takes the whole server down by crashing the Python process.  The
problem, as might be guessed, comes from reading the entire audio file into memory.
</p>

<p>
He tested his favorite song, which is 3m22s in length, takes up 5MB on
disk, which corresponds to 68MB once it is uncompressed into memory.  That
is around 14x compression, which is pretty good, he said, but maybe another
user uploads a podcast MP3.  That file is 60m in length, but podcasts
compress well, so it is only 28.8MB on disk—uncompressed, however, it is
1.2GB, for 42x compression.  That still would probably not be enough to
crash the machine, however.  When someone uploads &#34;whoops_all_silence.ogg&#34;,
which is 12 hours of silence (&#34;<q>just zeroes</q>&#34;) that only takes up 3.9MB
on disk because Ogg has good compression, it turns into 14.2GB in memory
(7,772x compression) and crashes the system.
</p>

<p>
The important thing to remember, Sobot said, is that the input size does not
necessarily predict the output size, so you should always treat the files
as if they could have any length.  Luckily, that&#39;s easy to do in Python.
He showed some code to open both the input and output file at the same time
and to process the input file in bounded chunks of frames.  That way, the
program never reads more than a fixed amount at a time, so the code scales
to any file size.  If there is only one takeaway from his talk, he
suggested that attendees should always &#34;<q>think of audio as a stream</q>&#34;
that can (and sometimes does) go on forever.
</p>

<p>
Another thing to keep in mind is the processing speed of Python, he said.
For example, the loop in his distortion example takes eight seconds per
minute of audio, which is not bad; that is a 7.5x speedup so it could still
easily be done in realtime.  But, switching the loop to use the <a href="https://numpy.org/doc/stable/reference/generated/numpy.tanh.html">NumPy
version of <tt>tanh()</tt></a> makes it run in 23ms per minute of audio
(a 2541x speedup).  That means using NumPy, which does the looping and
calculating in C, is 338x faster than using a
regular Python loop; &#34;<q>for audio, pure Python is slow</q>&#34;.
</p>

<h4><tt>pedalboard</tt></h4>

<p>
He asked how many of the attendees played electric guitar or bass and was
pleased to see a lot of them; those folks already know what a <a href="https://en.wikipedia.org/wiki/Guitar_pedalboard">pedalboard</a>
(in the real world) is.  For the rest, pedalboards provide various kinds of
effects on the output of a guitar, with configuration settings to change
the type and intensity of the effect.  <tt>pedalboard</tt> the library is meant to be a
pedalboard in Python.
</p>

<p>
He showed some effects from <tt>pedalboard</tt>, including using the <a href="https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.Reverb"><tt>pedalboard.Reverb</tt>
class</a> for easily adding &#34;reverb&#34; (i.e. reverberations) to an audio
file.  It is rare that only a single effect is desired, he said, so
<tt>pedalboard</tt> makes it easy to put them together:
</p><pre>    board = Pedalboard([
        Distortion(gain_db=25),
        Delay(delay_seconds=0.6, feedback=0.5, mix=0.5),
        Reverb(room_size=0.75),
    ])
    effected = board(audio, f.samplerate)
</pre><p>
Checking out the 
<a href="https://youtu.be/kpKrsFqJgXc?t=1429">video at 23:49</a> will
demonstrate these effects nicely.
</p>

<p>
<tt>pedalboard</tt> &#34;<q>ships with a <a href="https://spotify.github.io/pedalboard/reference/pedalboard.html#">ton
of plugins</a></q>&#34; for effects, Sobot said.  But users often have plugins
from third parties that they want to use; <tt>pedalboard</tt> can do that too.  The <a href="https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.load_plugin"><tt>pedalboard.load_plugin()</tt>
function</a> can be used to access <a href="https://en.wikipedia.org/wiki/Virtual_Studio_Technology">Virtual
Studio Technology</a> (VST3) plugins (as a <a href="https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.VST3Plugin"><tt>pedalboard.VST3Plugin</tt>
class</a>).  Those plugins can be configured in Python or by using the user
interface of the plugin itself with the <a href="https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.VST3Plugin.show_editor"><tt>show_editor()</tt>
method</a>.  VST3 instruments are supported as well.
</p>

<h4>Advanced <tt>pedalboard</tt></h4>

<p>
In the waning minutes of the talk, Sobot did a bit of a whirlwind tour through
some advanced <tt>pedalboard</tt> features.  For example, audio files can be <a href="https://spotify.github.io/pedalboard/reference/pedalboard.io.html#pedalboard.io.ReadableAudioFile.resampled_to">resampled</a>
&#34;<q>in constant time, with constant memory</q>&#34;, easily, and on the fly:
</p><pre>    with AudioFile(&#34;some_file.flac&#34;).resampled_to(22050) as f:
        ...
</pre><p>
The </p><a href="https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.time_stretch"><tt>pedalboard.time_stretch()</tt>
function</a><p> can change the length of an audio file.  The </p><a href="https://spotify.github.io/pedalboard/reference/pedalboard.io.html#pedalboard.io.AudioFile.encode"><tt>AudioFile.encode()</tt></a><p>
function will convert to different formats, such as MP3, FLAC, and Ogg; the
</p><tt>AudioFile</tt><p> interface can be used to write a file in a
different format as well.  The </p><a href="https://spotify.github.io/pedalboard/reference/pedalboard.io.html#pedalboard.io.AudioStream"><tt>AudioStream</tt>
class</a><p> can be used to send or receive live audio streams (e.g. to
speakers or from a microphone)—or both while doing some kind of effect in
realtime. But, wait, there&#39;s more ...
</p>

<p>
In the Q&amp;A, an attendee asked about hardware requirements for working
with audio using <tt>pedalboard</tt>.  Sobot said that the requirements
were modest, since the code is mostly running in C and C++ that has been
optimized over the last 30 years or so; people are using Raspberry Pi
devices successfully, for example.  Another wondered about seeking into the
middle of a frame in an MP3 file, but Sobot said that <tt>pedalboard</tt>
would ensure that any seek would land on a sample boundary, just as if the
file had already been decoded in memory.
</p>

<p>
[Thanks to the Linux Foundation for its travel sponsorship that allowed me to travel to Pittsburgh for PyCon US.]
</p></div></div>
  </body>
</html>
