<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/marv1nnnnn/llm-min.txt">Original</a>
    <h1>Show HN: Min.js style compression of tech docs for LLM context</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"/></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/afa6acb8b16dba41da2aaa15991ecc9e600d28c7f905f66c32b0030df20ce925/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e31302532422d626c7565" alt="Python Version" data-canonical-src="https://img.shields.io/badge/Python-3.10%2B-blue"/></a>
<a href="https://console.cloud.google.com/apis/api/gemini.googleapis.com/overview?project=llm-min" rel="nofollow"><img src="https://camo.githubusercontent.com/8e6e26ccbea21fab3467d671fb3229f2104e8e0ba589f73a07e40804f3063bce/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47656d696e692d4150492d677265656e" alt="Gemini API" data-canonical-src="https://img.shields.io/badge/Gemini-API-green"/></a></p>

<ul dir="auto">
<li><a href="#llm-mintxt-minjs-style-compression-of-tech-docs-for-llm-context-">llm-min.txt: Min.js Style Compression of Tech Docs for LLM Context 🤖</a>
<ul dir="auto">
<li><a href="#-table-of-contents">📜 Table of Contents</a></li>
<li><a href="#what-is-llm-mintxt-and-why-is-it-important">What is <code>llm-min.txt</code> and Why is it Important?</a></li>
<li><a href="#understanding-llm-mintxt-a-machine-optimized-format-">Understanding <code>llm-min.txt</code>: A Machine-Optimized Format 🧩</a></li>
<li><a href="#does-it-really-work-visualizing-the-impact">Does it Really Work? Visualizing the Impact</a></li>
<li><a href="#its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation">It&#39;s necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using <code>llm-min</code> will largely improve the success rate of code generation.</a></li>
<li><a href="#quick-start-">Quick Start 🚀</a></li>
<li><a href="#output-directory-structure-">Output Directory Structure 📂</a></li>
<li><a href="#choosing-the-right-ai-model-why-gemini-">Choosing the Right AI Model (Why Gemini) 🧠</a></li>
<li><a href="#how-it-works-a-look-inside-srcllm_min-%EF%B8%8F">How it Works: A Look Inside (src/llm_min) ⚙️</a></li>
<li><a href="#whats-next-future-plans-">What&#39;s Next? Future Plans 🔮</a></li>
<li><a href="#common-questions-faq-">Common Questions (FAQ) ❓</a></li>
<li><a href="#want-to-help-contributing-">Want to Help? Contributing 🤝</a></li>
<li><a href="#license-">License 📜</a></li>
</ul>
</li>
</ul>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">What is <code>llm-min.txt</code> and Why is it Important?</h2><a id="user-content-what-is-llm-mintxt-and-why-is-it-important" aria-label="Permalink: What is llm-min.txt and Why is it Important?" href="#what-is-llm-mintxt-and-why-is-it-important"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you&#39;ve ever used an AI coding assistant (like GitHub Copilot, Cursor, or others powered by Large Language Models - LLMs), you&#39;ve likely encountered situations where they don&#39;t know about the latest updates to programming libraries. This knowledge gap exists because AI models have a &#34;knowledge cutoff&#34; – a point beyond which they haven&#39;t learned new information. Since software evolves rapidly, this limitation can lead to outdated recommendations and broken code.</p>
<p dir="auto">Several innovative approaches have emerged to address this challenge:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://llmstxt.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/cee972bc70b97903b0c5bfb4a5c07d2cf76521237ffae1166df120505060f09a/68747470733a2f2f6c6c6d737478742e6f72672f6c6f676f2e706e67" alt="llms.txt logo" width="60" data-canonical-src="https://llmstxt.org/logo.png"/></a> <a href="https://llmstxt.org/" rel="nofollow">llms.txt</a>
A community-driven initiative where contributors create reference files (<code>llms.txt</code>) containing up-to-date library information specifically formatted for AI consumption.</p>
</li>
<li>
<p dir="auto"><a href="https://context7.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/48f83838d8c895d18f477e9d5fb7bb10f90cd920a763d5101a1e13e73aff4070/68747470733a2f2f656e637279707465642d74626e302e677374617469632e636f6d2f696d616765733f713d74626e3a414e6439476352625075774b4e647545414242443567415a4f5f4153397a30467955416d6c37326a33672673" alt="Context7 logo" width="60" data-canonical-src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRbPuwKNduEABBD5gAZO_AS9z0FyUAml72j3g&amp;s"/></a> <a href="https://context7.com/" rel="nofollow">Context7</a>
A service that dynamically provides contextual information to AIs, often by intelligently summarizing documentation.</p>
</li>
</ul>
<p dir="auto">While these solutions are valuable, they face certain limitations:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>llms.txt</code> files can become extraordinarily large – some exceeding <strong>800,000</strong> tokens (word fragments). This size can overwhelm many AI systems&#39; context windows.</p>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/token.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/token.png" alt="Token comparison for llms.txt" width="500"/></a>
<p dir="auto">Many shorter <code>llms.txt</code> variants simply contain links to official documentation, requiring the AI to fetch and process those documents separately. Even the comprehensive versions (<code>llms-full.txt</code>) often exceed what most AI assistants can process at once. Additionally, these files may not always reflect the absolute latest documentation.</p>
</li>
<li>
<p dir="auto"><code>Context7</code> operates somewhat as a &#34;black box&#34; – while useful, its precise information selection methodology isn&#39;t fully transparent to users. It primarily works with GitHub code repositories or existing <code>llms.txt</code> files, rather than any arbitrary software package.</p>
</li>
</ul>
<p dir="auto"><strong><code>llm-min.txt</code> offers a fresh approach:</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/icon.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/icon.png" alt="llm-min.txt icon" width="300"/></a></p>
<p dir="auto">Inspired by <code>min.js</code> files in web development (JavaScript with unnecessary elements removed), <code>llm-min.txt</code> adopts a similar philosophy for technical documentation. Instead of feeding an AI a massive, verbose manual, we leverage another AI to distill that documentation into a super-condensed, highly structured summary. The resulting <code>llm-min.txt</code> file captures only the most essential information needed to understand a library&#39;s usage, packaged in a format optimized for AI assistants rather than human readers.</p>
<p dir="auto">Modern AI reasoning capabilities excel at this distillation process, creating remarkably efficient knowledge representations that deliver maximum value with minimal token consumption.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Understanding <code>llm-min.txt</code>: A Machine-Optimized Format 🧩</h2><a id="user-content-understanding-llm-mintxt-a-machine-optimized-format-" aria-label="Permalink: Understanding llm-min.txt: A Machine-Optimized Format 🧩" href="#understanding-llm-mintxt-a-machine-optimized-format-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <code>llm-min.txt</code> file utilizes the <strong>Structured Knowledge Format (SKF)</strong> – a compact, machine-optimized format designed for efficient AI parsing rather than human readability. This format organizes technical information into distinct, highly structured sections with precise relationships.</p>
<p dir="auto"><strong>Key Elements of the SKF Format:</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Header Metadata:</strong> Every file begins with essential contextual information:</p>
<ul dir="auto">
<li><code># IntegratedKnowledgeManifest_SKF</code>: Format identifier and version</li>
<li><code># SourceDocs: [...]</code>: Original documentation sources</li>
<li><code># GenerationTimestamp: ...</code>: Creation timestamp</li>
<li><code># PrimaryNamespace: ...</code>: Top-level package/namespace, critical for understanding import paths</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Three Core Structured Sections:</strong> The content is organized into distinct functional categories:</p>
<ul dir="auto">
<li>
<p dir="auto"><code># SECTION: DEFINITIONS (Prefix: D)</code>: Describes the static aspects of the library:</p>
<ul dir="auto">
<li>Canonical component definitions with unique global IDs (e.g., <code>D001:G001_MyClass</code>)</li>
<li>Namespace paths relative to <code>PrimaryNamespace</code></li>
<li>Method signatures with parameters and return types</li>
<li>Properties/fields with types and access modifiers</li>
<li>Static relationships like inheritance or interface implementation</li>
<li><strong>Important:</strong> This section effectively serves as the glossary for the file, as the traditional glossary (<code>G</code> section) is used during generation but deliberately omitted from the final output to save space.</li>
</ul>
</li>
<li>
<p dir="auto"><code># SECTION: INTERACTIONS (Prefix: I)</code>: Captures dynamic behaviors within the library:</p>
<ul dir="auto">
<li>Method invocations (<code>INVOKES</code>)</li>
<li>Component usage patterns (<code>USES_COMPONENT</code>)</li>
<li>Event production/consumption</li>
<li>Error raising and handling logic, with references to specific error types</li>
</ul>
</li>
<li>
<p dir="auto"><code># SECTION: USAGE_PATTERNS (Prefix: U)</code>: Provides concrete usage examples:</p>
<ul dir="auto">
<li>Common workflows for core functionality</li>
<li>Step-by-step sequences involving object creation, configuration, method invocation, and error handling</li>
<li>Each pattern has a descriptive name (e.g., <code>U_BasicCrawl</code>) with numbered steps (<code>U_BasicCrawl.1</code>, <code>U_BasicCrawl.2</code>)</li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Line-Based Structure:</strong> Each item appears on its own line following precise formatting conventions that enable reliable machine parsing.</p>
</li>
</ol>
<p dir="auto"><strong>Example SKF Format (Simplified):</strong></p>
<div data-snippet-clipboard-copy-content="# IntegratedKnowledgeManifest_SKF/1.4 LA
# SourceDocs: [example-lib-docs]
# GenerationTimestamp: 2024-05-28T12:00:00Z
# PrimaryNamespace: example_lib

# SECTION: DEFINITIONS (Prefix: D)
# Format_PrimaryDef: Dxxx:Gxxx_Entity [DEF_TYP] [NAMESPACE &#34;relative.path&#34;] [OPERATIONS {op1:RetT(p1N:p1T)}] [ATTRIBUTES {attr1:AttrT1}] (&#34;Note&#34;)
# ---
D001:G001_Greeter [CompDef] [NAMESPACE &#34;.&#34;] [OPERATIONS {greet:Str(name:Str)}] (&#34;A simple greeter class&#34;)
D002:G002_AppConfig [CompDef] [NAMESPACE &#34;config&#34;] [ATTRIBUTES {debug_mode:Bool(&#34;RO&#34;)}] (&#34;Application configuration&#34;)
# ---

# SECTION: INTERACTIONS (Prefix: I)
# Format: Ixxx:Source_Ref INT_VERB Target_Ref_Or_Literal (&#34;Note_Conditions_Error(Gxxx_ErrorType)&#34;)
# ---
I001:G001_Greeter.greet INVOKES G003_Logger.log (&#34;Logs greeting activity&#34;)
# ---

# SECTION: USAGE_PATTERNS (Prefix: U)
# Format: U_Name:PatternTitleKeyword
#         U_Name.N:[Actor_Or_Ref] ACTION_KEYWORD (Target_Or_Data_Involving_Ref) -&gt; [Result_Or_State_Change_Involving_Ref]
# ---
U_BasicGreeting:Basic User Greeting
U_BasicGreeting.1:[User] CREATE (G001_Greeter) -&gt; [greeter_instance]
U_BasicGreeting.2:[greeter_instance] INVOKE (greet name=&#39;Alice&#39;) -&gt; [greeting_message]
# ---
# END_OF_MANIFEST"><pre lang="text"><code># IntegratedKnowledgeManifest_SKF/1.4 LA
# SourceDocs: [example-lib-docs]
# GenerationTimestamp: 2024-05-28T12:00:00Z
# PrimaryNamespace: example_lib

# SECTION: DEFINITIONS (Prefix: D)
# Format_PrimaryDef: Dxxx:Gxxx_Entity [DEF_TYP] [NAMESPACE &#34;relative.path&#34;] [OPERATIONS {op1:RetT(p1N:p1T)}] [ATTRIBUTES {attr1:AttrT1}] (&#34;Note&#34;)
# ---
D001:G001_Greeter [CompDef] [NAMESPACE &#34;.&#34;] [OPERATIONS {greet:Str(name:Str)}] (&#34;A simple greeter class&#34;)
D002:G002_AppConfig [CompDef] [NAMESPACE &#34;config&#34;] [ATTRIBUTES {debug_mode:Bool(&#34;RO&#34;)}] (&#34;Application configuration&#34;)
# ---

# SECTION: INTERACTIONS (Prefix: I)
# Format: Ixxx:Source_Ref INT_VERB Target_Ref_Or_Literal (&#34;Note_Conditions_Error(Gxxx_ErrorType)&#34;)
# ---
I001:G001_Greeter.greet INVOKES G003_Logger.log (&#34;Logs greeting activity&#34;)
# ---

# SECTION: USAGE_PATTERNS (Prefix: U)
# Format: U_Name:PatternTitleKeyword
#         U_Name.N:[Actor_Or_Ref] ACTION_KEYWORD (Target_Or_Data_Involving_Ref) -&gt; [Result_Or_State_Change_Involving_Ref]
# ---
U_BasicGreeting:Basic User Greeting
U_BasicGreeting.1:[User] CREATE (G001_Greeter) -&gt; [greeter_instance]
U_BasicGreeting.2:[greeter_instance] INVOKE (greet name=&#39;Alice&#39;) -&gt; [greeting_message]
# ---
# END_OF_MANIFEST
</code></pre></div>
<p dir="auto">The <code>llm-min-guideline.md</code> file (generated alongside <code>llm-min.txt</code>) provides detailed decoding instructions and schema definitions that enable an AI to correctly interpret the SKF format. It serves as the essential companion document explaining the notation, field meanings, and relationship types used throughout the file.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Does it Really Work? Visualizing the Impact</h2><a id="user-content-does-it-really-work-visualizing-the-impact" aria-label="Permalink: Does it Really Work? Visualizing the Impact" href="#does-it-really-work-visualizing-the-impact"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>llm-min.txt</code> achieves dramatic token reduction while preserving the essential knowledge needed by AI assistants. The chart below compares token counts between original library documentation (<code>llm-full.txt</code>) and the compressed <code>llm-min.txt</code> versions:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/comparison.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/comparison.png" alt="Token Compression Comparison"/></a></p>
<p dir="auto">These results demonstrate token reductions typically ranging from 90-95%, with some cases exceeding 97%. This extreme compression, combined with the highly structured SKF format, enables AI tools to ingest and process library documentation far more efficiently than with raw text.</p>
<p dir="auto">In our samples directory, you can examine these impressive results firsthand:</p>
<ul dir="auto">
<li><code>sample/crawl4ai/llm-full.txt</code>: Original documentation (uncompressed)</li>
<li><code>sample/crawl4ai/llm-min.txt</code>: The compressed SKF representation</li>
<li><code>sample/crawl4ai/llm-min-guideline.md</code>: The format decoder companion file, also seen in <a href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/llm-min-guideline.md">llm-min-guideline.md</a></li>
</ul>
<p dir="auto">Most compressed files contain around 10,000 tokens – well within the processing capacity of modern AI assistants.</p>
<p dir="auto"><strong>How to use it?</strong></p>
<p dir="auto">Simply reference the files in your AI-powered IDE&#39;s conversation, and watch your assistant immediately gain detailed knowledge of the library:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/demo.gif"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/demo.gif" alt="Demo" data-animated-image=""/></a></p>
<p dir="auto"><strong>How does it perform?</strong></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">It&#39;s necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using <code>llm-min</code> will largely improve the success rate of code generation.</h2><a id="user-content-its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation" aria-label="Permalink: It&#39;s necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using llm-min will largely improve the success rate of code generation." href="#its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto">Getting started with <code>llm-min</code> is straightforward:</p>
<p dir="auto"><strong>1. Installation:</strong></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>For regular users (recommended):</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install llm-min

# Install required browser automation tools
playwright install"><pre>pip install llm-min

<span><span>#</span> Install required browser automation tools</span>
playwright install</pre></div>
</li>
<li>
<p dir="auto"><strong>For contributors and developers:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository (if not already done)
# git clone https://github.com/your-repo/llm-min.git
# cd llm-min

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies with UV (faster than pip)
uv sync
uv pip install -e .

# Optional: Set up pre-commit hooks for code quality
# uv pip install pre-commit
# pre-commit install"><pre><span><span>#</span> Clone the repository (if not already done)</span>
<span><span>#</span> git clone https://github.com/your-repo/llm-min.git</span>
<span><span>#</span> cd llm-min</span>

<span><span>#</span> Create and activate a virtual environment</span>
python -m venv .venv
<span>source</span> .venv/bin/activate  <span><span>#</span> On Windows: .venv\Scripts\activate</span>

<span><span>#</span> Install dependencies with UV (faster than pip)</span>
uv sync
uv pip install -e <span>.</span>

<span><span>#</span> Optional: Set up pre-commit hooks for code quality</span>
<span><span>#</span> uv pip install pre-commit</span>
<span><span>#</span> pre-commit install</span></pre></div>
</li>
</ul>
<p dir="auto"><strong>2. Set Up Your Gemini API Key:</strong> 🔑</p>
<p dir="auto"><code>llm-min</code> uses Google&#39;s Gemini AI to generate compressed documentation. You&#39;ll need a Gemini API key to proceed:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Best practice:</strong> Set an environment variable named <code>GEMINI_API_KEY</code> with your key value:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Linux/macOS
export GEMINI_API_KEY=your_api_key_here

# Windows (Command Prompt)
set GEMINI_API_KEY=your_api_key_here

# Windows (PowerShell)
$env:GEMINI_API_KEY=&#34;your_api_key_here&#34;"><pre><span><span>#</span> Linux/macOS</span>
<span>export</span> GEMINI_API_KEY=your_api_key_here

<span><span>#</span> Windows (Command Prompt)</span>
<span>set</span> GEMINI_API_KEY=your_api_key_here

<span><span>#</span> Windows (PowerShell)</span>
<span>$env</span>:GEMINI_API_KEY=<span><span>&#34;</span>your_api_key_here<span>&#34;</span></span></pre></div>
</li>
<li>
<p dir="auto"><strong>Alternative:</strong> Supply your key directly via the <code>--gemini-api-key</code> command-line option.</p>
</li>
</ul>
<p dir="auto">You can obtain a Gemini API key from the <a href="https://aistudio.google.com/app/apikey" rel="nofollow">Google AI Studio</a> or Google Cloud Console.</p>
<p dir="auto"><strong>3. Generate Your First <code>llm-min.txt</code> File:</strong> 💻</p>
<p dir="auto">Choose one of the following input sources:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Short</th>
<th>Type</th>
<th>What it does</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--output-dir</code></td>
<td><code>-o</code></td>
<td><code>DIRECTORY</code></td>
<td>Where to save the generated files (default is a folder named <code>llm_min_docs</code>).</td>
</tr>
<tr>
<td><code>--output-name</code></td>
<td><code>-n</code></td>
<td><code>TEXT</code></td>
<td>Give a custom name for the subfolder inside <code>output-dir</code>.</td>
</tr>
<tr>
<td><code>--max-crawl-pages</code></td>
<td><code>-p</code></td>
<td><code>INTEGER</code></td>
<td>Max web pages to read (default: 200; 0 means no limit).</td>
</tr>
<tr>
<td><code>--max-crawl-depth</code></td>
<td><code>-D</code></td>
<td><code>INTEGER</code></td>
<td>How many links deep to follow on a website (default: 3).</td>
</tr>
<tr>
<td><code>--chunk-size</code></td>
<td><code>-c</code></td>
<td><code>INTEGER</code></td>
<td>How much text to give the AI at once (default: 600,000 characters).</td>
</tr>
<tr>
<td><code>--gemini-api-key</code></td>
<td><code>-k</code></td>
<td><code>TEXT</code></td>
<td>Your Gemini API Key (if not set as an environment variable).</td>
</tr>
<tr>
<td><code>--gemini-model</code></td>
<td><code>-m</code></td>
<td><code>TEXT</code></td>
<td>Which Gemini model to use (default: <code>gemini-2.5-flash-preview-04-17</code>).</td>
</tr>
<tr>
<td><code>--verbose</code></td>
<td><code>-v</code></td>
<td></td>
<td>Show more detailed messages while it&#39;s working.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Key Command-Line Options:</strong></p>
<ul dir="auto">
<li>Process the Python package <code>typer</code>, read up to 50 web pages, and save to a folder called <code>my_docs</code>:
<div dir="auto" data-snippet-clipboard-copy-content="llm-min -pkg &#34;typer&#34; -o my_docs -p 50 --gemini-api-key YOUR_API_KEY_HERE"><pre>llm-min -pkg <span><span>&#34;</span>typer<span>&#34;</span></span> -o my_docs -p 50 --gemini-api-key YOUR_API_KEY_HERE</pre></div>
</li>
</ul>
<p dir="auto"><strong>Example Commands:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Process the &#34;typer&#34; package, save to &#34;my_docs&#34; folder
llm-min -pkg &#34;typer&#34; -o my_docs -p 50

# Process the FastAPI documentation website
llm-min -u &#34;https://fastapi.tiangolo.com/&#34; -o my_docs -p 50

# Process documentation files in a local folder
llm-min -i &#34;./docs&#34; -o my_docs"><pre><span><span>#</span> Process the &#34;typer&#34; package, save to &#34;my_docs&#34; folder</span>
llm-min -pkg <span><span>&#34;</span>typer<span>&#34;</span></span> -o my_docs -p 50

<span><span>#</span> Process the FastAPI documentation website</span>
llm-min -u <span><span>&#34;</span>https://fastapi.tiangolo.com/<span>&#34;</span></span> -o my_docs -p 50

<span><span>#</span> Process documentation files in a local folder</span>
llm-min -i <span><span>&#34;</span>./docs<span>&#34;</span></span> -o my_docs</pre></div>
<p dir="auto"><strong>4. Programmatic Usage in Python:</strong> 🐍</p>
<p dir="auto">You can also integrate <code>llm-min</code> directly into your Python applications:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llm_min import LLMMinGenerator
import os

# Configuration for the AI processing
llm_config = {
    &#34;api_key&#34;: os.environ.get(&#34;GEMINI_API_KEY&#34;),  # Use environment variable
    &#34;model_name&#34;: &#34;gemini-2.5-flash-preview-04-17&#34;,  # Recommended model
    &#34;chunk_size&#34;: 600000,  # Characters per AI processing batch
    &#34;max_crawl_pages&#34;: 200,  # Maximum pages to crawl
    &#34;max_crawl_depth&#34;: 3,  # Link following depth
}

# Initialize the generator (output files will go to ./my_output_docs/[package_name]/)
generator = LLMMinGenerator(output_dir=&#34;./my_output_docs&#34;, llm_config=llm_config)

# Generate llm-min.txt for the &#39;requests&#39; package
try:
    generator.generate_from_package(&#34;requests&#34;)
    print(&#34;✅ Successfully created documentation for &#39;requests&#39;!&#34;)
except Exception as e:
    print(f&#34;❌ Error processing &#39;requests&#39;: {e}&#34;)

# Generate llm-min.txt from a documentation URL
try:
    generator.generate_from_url(&#34;https://bun.sh/llms-full.txt&#34;)
    print(&#34;✅ Successfully processed &#39;https://bun.sh/llms-full.txt&#39;!&#34;)
except Exception as e:
    print(f&#34;❌ Error processing URL: {e}&#34;)"><pre><span>from</span> <span>llm_min</span> <span>import</span> <span>LLMMinGenerator</span>
<span>import</span> <span>os</span>

<span># Configuration for the AI processing</span>
<span>llm_config</span> <span>=</span> {
    <span>&#34;api_key&#34;</span>: <span>os</span>.<span>environ</span>.<span>get</span>(<span>&#34;GEMINI_API_KEY&#34;</span>),  <span># Use environment variable</span>
    <span>&#34;model_name&#34;</span>: <span>&#34;gemini-2.5-flash-preview-04-17&#34;</span>,  <span># Recommended model</span>
    <span>&#34;chunk_size&#34;</span>: <span>600000</span>,  <span># Characters per AI processing batch</span>
    <span>&#34;max_crawl_pages&#34;</span>: <span>200</span>,  <span># Maximum pages to crawl</span>
    <span>&#34;max_crawl_depth&#34;</span>: <span>3</span>,  <span># Link following depth</span>
}

<span># Initialize the generator (output files will go to ./my_output_docs/[package_name]/)</span>
<span>generator</span> <span>=</span> <span>LLMMinGenerator</span>(<span>output_dir</span><span>=</span><span>&#34;./my_output_docs&#34;</span>, <span>llm_config</span><span>=</span><span>llm_config</span>)

<span># Generate llm-min.txt for the &#39;requests&#39; package</span>
<span>try</span>:
    <span>generator</span>.<span>generate_from_package</span>(<span>&#34;requests&#34;</span>)
    <span>print</span>(<span>&#34;✅ Successfully created documentation for &#39;requests&#39;!&#34;</span>)
<span>except</span> <span>Exception</span> <span>as</span> <span>e</span>:
    <span>print</span>(<span>f&#34;❌ Error processing &#39;requests&#39;: <span><span>{</span><span>e</span><span>}</span></span>&#34;</span>)

<span># Generate llm-min.txt from a documentation URL</span>
<span>try</span>:
    <span>generator</span>.<span>generate_from_url</span>(<span>&#34;https://bun.sh/llms-full.txt&#34;</span>)
    <span>print</span>(<span>&#34;✅ Successfully processed &#39;https://bun.sh/llms-full.txt&#39;!&#34;</span>)
<span>except</span> <span>Exception</span> <span>as</span> <span>e</span>:
    <span>print</span>(<span>f&#34;❌ Error processing URL: <span><span>{</span><span>e</span><span>}</span></span>&#34;</span>)</pre></div>
<p dir="auto">For a complete list of command-line options, run:</p>

<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Output Directory Structure 📂</h2><a id="user-content-output-directory-structure-" aria-label="Permalink: Output Directory Structure 📂" href="#output-directory-structure-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">When <code>llm-min</code> completes its processing, it creates the following organized directory structure:</p>
<div data-snippet-clipboard-copy-content="your_chosen_output_dir/
└── name_of_package_or_website/
    ├── llm-full.txt             # Complete documentation text (original content)
    ├── llm-min.txt              # Compressed SKF/1.4 LA structured summary
    └── llm-min-guideline.md     # Essential format decoder for AI interpretation"><pre lang="text"><code>your_chosen_output_dir/
└── name_of_package_or_website/
    ├── llm-full.txt             # Complete documentation text (original content)
    ├── llm-min.txt              # Compressed SKF/1.4 LA structured summary
    └── llm-min-guideline.md     # Essential format decoder for AI interpretation
</code></pre></div>
<p dir="auto">For example, running <code>llm-min -pkg &#34;requests&#34; -o my_llm_docs</code> produces:</p>
<div data-snippet-clipboard-copy-content="my_llm_docs/
└── requests/
    ├── llm-full.txt             # Original documentation
    ├── llm-min.txt              # Compressed SKF format (D, I, U sections)
    └── llm-min-guideline.md     # Format decoding instructions"><pre lang="text"><code>my_llm_docs/
└── requests/
    ├── llm-full.txt             # Original documentation
    ├── llm-min.txt              # Compressed SKF format (D, I, U sections)
    └── llm-min-guideline.md     # Format decoding instructions
</code></pre></div>
<p dir="auto"><strong>Important:</strong> The <code>llm-min-guideline.md</code> file is a critical companion to <code>llm-min.txt</code>. It provides the detailed schema definitions and format explanations that an AI needs to correctly interpret the structured data. When using <code>llm-min.txt</code> with an AI assistant, always include this guideline file as well.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Choosing the Right AI Model (Why Gemini) 🧠</h2><a id="user-content-choosing-the-right-ai-model-why-gemini-" aria-label="Permalink: Choosing the Right AI Model (Why Gemini) 🧠" href="#choosing-the-right-ai-model-why-gemini-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>llm-min</code> utilizes Google&#39;s Gemini family of AI models for document processing. While you can select a specific Gemini model via the <code>--gemini-model</code> option, we strongly recommend using the default: <code>gemini-2.5-flash-preview-04-17</code>.</p>
<p dir="auto">This particular model offers an optimal combination of capabilities for documentation compression:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Advanced Reasoning:</strong> Excels at understanding complex technical documentation and extracting the essential structural relationships needed for the SKF format.</p>
</li>
<li>
<p dir="auto"><strong>Exceptional Context Window:</strong> With a 1-million token input capacity, it can process large documentation chunks at once, enabling more coherent and comprehensive analysis.</p>
</li>
<li>
<p dir="auto"><strong>Cost Efficiency:</strong> Provides an excellent balance of capability and affordability compared to other large-context models.</p>
</li>
</ol>
<p dir="auto">The default model has been carefully selected to deliver the best results for the <code>llm-min</code> compression process across a wide range of documentation styles and technical domains.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">How it Works: A Look Inside (src/llm_min) ⚙️</h2><a id="user-content-how-it-works-a-look-inside-srcllm_min-️" aria-label="Permalink: How it Works: A Look Inside (src/llm_min) ⚙️" href="#how-it-works-a-look-inside-srcllm_min-️"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <code>llm-min</code> tool employs a sophisticated multi-stage process to transform verbose documentation into a compact, machine-optimized SKF manifest:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Input Processing:</strong> Based on your command-line options (e.g., <code>--package &#34;requests&#34;</code>), <code>llm-min</code> gathers documentation from the appropriate source (PyPI, web crawling, or local files).</p>
</li>
<li>
<p dir="auto"><strong>Text Preparation:</strong> The collected documentation is cleaned and segmented into manageable chunks for processing. The original text is preserved as <code>llm-full.txt</code>.</p>
</li>
<li>
<p dir="auto"><strong>Three-Step AI Analysis Pipeline (Gemini):</strong> This is the heart of the SKF manifest generation, orchestrated by the <code>compact_content_to_structured_text</code> function in <code>compacter.py</code>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Step 1: Global Glossary Generation (Internal Only):</strong></p>
<ul dir="auto">
<li>Each document chunk is analyzed using the <code>SKF_PROMPT_CALL1_GLOSSARY_TEMPLATE</code> prompt to identify key technical entities and generate a <em>chunk-local</em> glossary fragment with temporary <code>Gxxx</code> IDs.</li>
<li>These fragments are consolidated via the <code>SKF_PROMPT_CALL1_5_MERGE_GLOSSARY_TEMPLATE</code> prompt, which resolves duplicates and creates a unified entity list.</li>
<li>The <code>re_id_glossary_items</code> function then assigns globally sequential <code>Gxxx</code> IDs (G001, G002, etc.) to these consolidated entities.</li>
<li>This global glossary is maintained in memory throughout the process but is <strong>not included in the final <code>llm-min.txt</code> output</strong> to conserve space.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Step 2: Definitions &amp; Interactions (D &amp; I) Generation:</strong></p>
<ul dir="auto">
<li>For the first document chunk (or if there&#39;s only one chunk), the AI uses the <code>SKF_PROMPT_CALL2_DETAILS_SINGLE_CHUNK_TEMPLATE</code> with the global glossary to generate initial D and I items.</li>
<li>For subsequent chunks, the <code>SKF_PROMPT_CALL2_DETAILS_ITERATIVE_TEMPLATE</code> is used, providing both the global glossary and previously generated D&amp;I items as context to avoid duplication.</li>
<li>As each chunk is processed, newly identified D and I items are accumulated and assigned sequential global IDs (D001, D002, etc. and I001, I002, etc.).</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Step 3: Usage Patterns (U) Generation:</strong></p>
<ul dir="auto">
<li>Similar to Step 2, the first chunk uses <code>SKF_PROMPT_CALL3_USAGE_SINGLE_CHUNK_TEMPLATE</code>, receiving the global glossary, all accumulated D&amp;I items, and the current chunk text.</li>
<li>Subsequent chunks use <code>SKF_PROMPT_CALL3_USAGE_ITERATIVE_TEMPLATE</code>, which additionally receives previously generated U-items to enable pattern continuation and avoid duplication.</li>
<li>Usage patterns are identified with descriptive names (e.g., <code>U_BasicNetworkFetch</code>) and contain numbered steps (e.g., <code>U_BasicNetworkFetch.1</code>, <code>U_BasicNetworkFetch.2</code>).</li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Final Assembly:</strong> The complete <code>llm-min.txt</code> file is created by combining:</p>
<ul dir="auto">
<li>The SKF manifest header (protocol version, source docs, timestamp, primary namespace)</li>
<li>The accumulated <code>DEFINITIONS</code> section</li>
<li>The accumulated <code>INTERACTIONS</code> section</li>
<li>The accumulated <code>USAGE_PATTERNS</code> section</li>
<li>A final <code># END_OF_MANIFEST</code> marker</li>
</ul>
</li>
</ol>
<p dir="auto"><strong>Conceptual Pipeline Overview:</strong></p>
<div data-snippet-clipboard-copy-content="User Input      →  Doc Gathering   →  Text Processing   →  AI Step 1: Glossary   →  In-Memory Global    →  AI Step 2: D&amp;I     →  Accumulated D&amp;I
(CLI/Python)       (Package/URL)      (Chunking)           (Extract + Merge)        Glossary (Gxxx)        (Per chunk)          (Dxxx, Ixxx)
                                                                                                                                     ↓
           ┌─────────────────────────────────────────────────────────────────────────────────────────────────┐                      ↓
           ↓                                                                                                 ↑                      ↓
Final SKF Manifest   ←   Assembly   ←   Accumulated Usage   ←   AI Step 3: Usage   ←   Global Glossary + Accumulated D&amp;I
(llm-min.txt)            (D,I,U)        Patterns (U_Name.N)      (Per chunk)           (Required context for generating valid U-items)"><pre><code>User Input      →  Doc Gathering   →  Text Processing   →  AI Step 1: Glossary   →  In-Memory Global    →  AI Step 2: D&amp;I     →  Accumulated D&amp;I
(CLI/Python)       (Package/URL)      (Chunking)           (Extract + Merge)        Glossary (Gxxx)        (Per chunk)          (Dxxx, Ixxx)
                                                                                                                                     ↓
           ┌─────────────────────────────────────────────────────────────────────────────────────────────────┐                      ↓
           ↓                                                                                                 ↑                      ↓
Final SKF Manifest   ←   Assembly   ←   Accumulated Usage   ←   AI Step 3: Usage   ←   Global Glossary + Accumulated D&amp;I
(llm-min.txt)            (D,I,U)        Patterns (U_Name.N)      (Per chunk)           (Required context for generating valid U-items)
</code></pre></div>
<p dir="auto">This multi-stage approach ensures that the SKF manifest is comprehensive, avoids duplication across chunks, and maintains consistent cross-references between entities, definitions, interactions, and usage patterns.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">What&#39;s Next? Future Plans 🔮</h2><a id="user-content-whats-next-future-plans-" aria-label="Permalink: What&#39;s Next? Future Plans 🔮" href="#whats-next-future-plans-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We&#39;re exploring several exciting directions to evolve <code>llm-min</code>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Public Repository for Pre-Generated Files</strong> 🌐
A central hub where the community could share and discover <code>llm-min.txt</code> files for popular libraries would be valuable. This would eliminate the need for individual users to generate these files repeatedly and ensure consistent, high-quality information. Key challenges include quality control, version management, and hosting infrastructure costs.</p>
</li>
<li>
<p dir="auto"><strong>Code-Based Documentation Inference</strong> 💻
An intriguing possibility is using source code analysis (via Abstract Syntax Trees) to automatically generate or augment documentation summaries. While initial experiments have shown this to be technically challenging, particularly for complex libraries with dynamic behaviors, it remains a promising research direction that could enable even more accurate documentation.</p>
</li>
<li>
<p dir="auto"><strong>Model Control Protocol Integration</strong> 🤔
While technically feasible, implementing <code>llm-min</code> as an MCP server doesn&#39;t fully align with our current design philosophy. The strength of <code>llm-min.txt</code> lies in providing reliable, static context – a deterministic reference that reduces the uncertainty sometimes associated with dynamic AI integrations. We&#39;re monitoring user needs to determine if a server-based approach might deliver value in the future.</p>
</li>
</ul>
<p dir="auto">We welcome community input on these potential directions!</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Common Questions (FAQ) ❓</h2><a id="user-content-common-questions-faq-" aria-label="Permalink: Common Questions (FAQ) ❓" href="#common-questions-faq-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Q: Do I need a reasoning-capable model to generate an <code>llm-min.txt</code> file?</strong> 🧠</p>
<p dir="auto">A: Yes, generating an <code>llm-min.txt</code> file requires a model with strong reasoning capabilities like Gemini. The process involves complex information extraction, entity relationship mapping, and structured knowledge representation. However, once generated, the <code>llm-min.txt</code> file can be effectively used by any competent coding model (e.g., Claude 3.5 Sonnet) to answer library-specific questions.</p>
<p dir="auto"><strong>Q: Does <code>llm-min.txt</code> preserve all information from the original documentation?</strong> 📚</p>
<p dir="auto">A: No, <code>llm-min.txt</code> is explicitly designed as a lossy compression format. It prioritizes programmatically relevant details (classes, methods, parameters, return types, core usage patterns) while deliberately omitting explanatory prose, conceptual discussions, and peripheral information. This selective preservation is what enables the dramatic token reduction while maintaining the essential technical reference information an AI assistant needs.</p>
<p dir="auto"><strong>Q: Why does generating an <code>llm-min.txt</code> file take time?</strong> ⏱️</p>
<p dir="auto">A: Creating an <code>llm-min.txt</code> file involves a sophisticated multi-stage AI pipeline:</p>
<ol dir="auto">
<li>Gathering and preprocessing documentation</li>
<li>Analyzing each chunk to identify entities (glossary generation)</li>
<li>Consolidating entities across chunks</li>
<li>Extracting detailed definitions and interactions from each chunk</li>
<li>Generating representative usage patterns</li>
</ol>
<p dir="auto">This intensive process can take several minutes, particularly for large libraries. However, once created, the resulting <code>llm-min.txt</code> file can be reused indefinitely, providing much faster reference information for AI assistants.</p>
<p dir="auto"><strong>Q: I received a &#34;Gemini generation stopped due to MAX_TOKENS limit&#34; error. What should I do?</strong> 🛑</p>
<p dir="auto">A: This error indicates that the Gemini model reached its output limit while processing a particularly dense or complex documentation chunk. Try reducing the <code>--chunk-size</code> option (e.g., from 600,000 to 300,000 characters) to give the model smaller batches to process. While this might slightly increase API costs due to more separate calls, it often resolves token limit errors.</p>
<p dir="auto"><strong>Q: What&#39;s the typical cost for generating one <code>llm-min.txt</code> file?</strong> 💰</p>
<p dir="auto">A: Processing costs vary based on documentation size and complexity, but for a moderate-sized library, expect to spend between <strong>$0.01 and $1.00 USD</strong> in Gemini API charges. Key factors affecting cost include:</p>
<ul dir="auto">
<li>Total documentation size</li>
<li>Number of chunks processed</li>
<li>Complexity of the library&#39;s structure</li>
<li>Selected Gemini model</li>
</ul>
<p dir="auto">For current pricing details, refer to the <a href="https://cloud.google.com/vertex-ai/pricing#gemini" rel="nofollow">Google Cloud AI pricing page</a>.</p>
<p dir="auto"><strong>Q: Did you vibe code this project</strong> 🤖</p>
<p dir="auto">A: Yes, definitely. This project was developed using <a href="https://roocode.com/" rel="nofollow">Roocode</a> with a custom configuration called <a href="https://github.com/marv1nnnnn/rooroo">Rooroo</a>.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Want to Help? Contributing 🤝</h2><a id="user-content-want-to-help-contributing-" aria-label="Permalink: Want to Help? Contributing 🤝" href="#want-to-help-contributing-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We welcome contributions to make <code>llm-min</code> even better! 🎉</p>
<p dir="auto">Whether you&#39;re reporting bugs, suggesting features, or submitting code changes via pull requests, your involvement helps improve this tool for everyone. Check our GitHub repository for contribution guidelines and open issues.</p>
<hr/>

<p dir="auto">This project is licensed under the MIT License. See the <code>LICENSE</code> file for complete details.</p>
</article></div></div>
  </body>
</html>
