<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rockorager.dev/log/lsr-ls-but-with-io-uring/">Original</a>
    <h1>lsr: ls with io_uring</h1>
    
    <div id="readability-page-1" class="page"><div id="main">
      
  <article>
    <header>
      
      <a href="https://rockorager.dev/log/lsr-ls-but-with-io-uring/">
        <time datetime="2025-05-06T16:15:50Z">May 06, 2025</time>
      </a>
    </header>
    <section><p>As an excercise in syscall golf, I wrote an implementation of <code>ls(1)</code> which uses my IO library, <a href="https://github.com/rockorager/ourio" target="_blank">ourio</a> to perform as much of the IO as possible. What I ended up with is something that is faster than any version or alternative to <code>ls</code> I tested, and also performs an <strong>order of magnitude fewer syscalls</strong>. I’m calling it <a href="https://tangled.sh/@rockorager.dev/lsr" target="_blank">lsr</a>. Let’s start with the benchmarks, then we’ll see how we got there.</p><p><img src="https://rockorager.dev/log/lsr-ls-but-with-io-uring/screenshot.webp"/></p><div id="time"><h3><a href="#time">Time</a></h3><p>Data gathered with <code>hyperfine</code> on a directory of <code>n</code> plain files.</p><table><tbody><tr><th>Program</th><th>n=10</th><th>n=100</th><th>n=1,000</th><th>n=10,000</th></tr><tr><td>lsr -al</td><td>372.6 µs</td><td>634.3 µs</td><td>2.7 ms</td><td>22.1 ms</td></tr><tr><td>ls -al</td><td>1.4 ms</td><td>1.7 ms</td><td>4.7 ms</td><td>38.0 ms</td></tr><tr><td>eza -al</td><td>2.9 ms</td><td>3.3 ms</td><td>6.6 ms</td><td>40.2 ms</td></tr><tr><td>lsd -al</td><td>2.1 ms</td><td>3.5 ms</td><td>17.0 ms</td><td>153.4 ms</td></tr><tr><td>uutils ls -al</td><td>2.9 ms</td><td>3.6 ms</td><td>11.3 ms</td><td>89.6 ms</td></tr></tbody></table></div><div id="syscalls"><h3><a href="#syscalls">Syscalls</a></h3><p>Data gathered with <code>strace -c</code> on a directory of <code>n</code> plain files. (Lower is better)</p><table><tbody><tr><th>Program</th><th>n=10</th><th>n=100</th><th>n=1,000</th><th>n=10,000</th></tr><tr><td>lsr -al</td><td>20</td><td>28</td><td>105</td><td>848</td></tr><tr><td>ls -al</td><td>405</td><td>675</td><td>3,377</td><td>30,396</td></tr><tr><td>eza -al</td><td>319</td><td>411</td><td>1,320</td><td>10,364</td></tr><tr><td>lsd -al</td><td>508</td><td>1,408</td><td>10,423</td><td>100,512</td></tr><tr><td>uutils ls -al</td><td>445</td><td>986</td><td>6,397</td><td>10,005</td></tr></tbody></table></div><div id="how-we-got-there"><h2><a href="#how-we-got-there">How we got there</a></h2><p>Let’s start with how <code>lsr</code> works. To list directory contents, we basically have 3 stages to the program:</p><ol><li>Parse args</li><li>Gather data</li><li>Print data</li></ol><p>All of the IO involved happens in the second step. Wherever possible, <code>lsr</code> utilizes io_uring to pull in the data it needs. To get to that point, it means that we open the target directory with io_uring, if we need local time, user data, or group data, we open (and read) those files with io_uring. We do all <code>stat</code> calls via io_uring, and as needed we do the equivalent of an <code>lstat</code> via io_uring. In practice, this means that the number of syscalls we have should be drastically smaller than equivalent programs because we are able to batch the <code>stat</code> syscall. The results clearly show this…<code>lsr</code> has at least an order of magnitude fewer syscalls than it’s closest equivalent, being <code>uutils ls</code>.</p><p>We also use the zig stdlib StackFallbackAllocator. This let’s <code>lsr</code> allocate memory it needs up front, but fallback to a different allocator when it’s exhausted the fixed allocation. We allocate 1MB up front, which is more than enough for typical usage. This further reduces syscalls by reducing mmap usage.</p><p>As a result of working directly with io_uring, we also bypass several libc related pitfalls. Namely, we have no dynamic linking - <code>ls</code> has some considerable overhead in loading libc and related libraries…but it also has the benefit of having locale support. <code>lsr</code> does not boast such a feature. Despite being statically linked, <code>lsr</code> is still smaller than GNU <code>ls</code>: 138.7KB vs 79.3KB when built with ReleaseSmall.</p><h2>Anomolies and Thoughts</h2><p>I have no idea what <code>lsd</code> is doing. I haven’t read the source code, but from viewing it’s <code>strace</code>, it is calling <code>clock_gettime</code> around 5 times <strong>per file</strong>. Why? I don’t know. Maybe it’s doing internal timing of steps along the way?</p><p>Sorting ends up being a massive part of the workload. I suspect this is where <code>uutils ls</code> is getting slowed down, since it is doing pretty good on a syscall basis. <code>lsr</code> spends about 30% of it’s runtime sorting, the rest is the IO loop.</p><p>This ended up being a pretty fun project to write, and didn’t take too much time either. I am shocked at how much io_uring can be used to reduce syscalls…<code>ls</code> is a pretty basic example but you can only imagine how much of an effect this would have on something like a server.</p><p>Also - I’m using <a href="https://tangled.sh" target="_blank">tangled.sh</a> for this project. They have a really cool idea, and I want to see how the PR workflow is so…if you have any bugs or changes, please visit the <a href="https://tangled.sh/@rockorager.dev/lsr" target="_blank">repo</a>. All you need is an atproto account + app password. I suspect more icons will be needed, feel free to make an issue for icon requests!</p></div></section>
    
  </article>

    </div></div>
  </body>
</html>
