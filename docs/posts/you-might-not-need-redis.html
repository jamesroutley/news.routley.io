<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.viblo.se/posts/no-need-redis/">Original</a>
    <h1>You might not need Redis</h1>
    
    <div id="readability-page-1" class="page"><p>
  <a href="https://redis.io/">Redis</a> is arguably the most well regarded tech
  of all. My impression is that not even PostgreSQL has the same kind of
  positive aura around it. And of all tech available Redis is at the top of
  things you &#34;need&#34; to have in your tech stack. Maybe this will change with
  their latest
  <a href="https://redis.com/blog/redis-adopts-dual-source-available-licensing/">licencing change</a>, but that remains to be seen.
</p>
<p>
  It&#39;s not difficult to see why, Redis is after all a very well architected and
  impressive piece of technology and this post is not arguing against any
  technical merits of Redis. What I will argue is that despite no technical flaw
  in Redis itself you still might not need it!
</p>

<h2>My Experience</h2>
<p>
  In the last 3 places I worked (10+ years) I have seen the same pattern. A
  problem comes up, Redis is seen as a great fit and here we go.
</p>
<p>
  However, when looking closer at the actual use case it turns out that Redis
  didn&#39;t improve things, or did not fix the underlying problem. It just added
  complexity for complexity&#39;s sake.
</p>

<h2>First time, at Tantan</h2>
<p>
  First time Redis was proposed was at
  <a href="https://tantanapp.com/">Tantan</a>, the second largest dating app in
  China and a sort of Chinese Tinder. At the time we had 50-100 very powerful
  database servers running PostgreSQL.
</p>
<p>
  To not make this post too long I will simplify the architecture a bit: Each
  server stored a subset of the users &#39;swipes&#39;, sharded by the UserId so that a
  specific user only had data on one server. To add new swipes, the code would
  locate the server belonging to the user, and then run an insert SQL statement.
  If you are curious I have held a couple of talk of Tantan&#39;s DB architecture,
  see <a href="https://www.viblo.se/talks/">here</a> for a more in depth overview.
</p>
<p>
  The need arose to keep track of a little piece of additional data, namely the
  count of swipes. Basically a single integer value for each user, with two
  important properties: The value would be updated very often since &#39;swipe&#39;
  where the most common action of a user, and the value would often be read, to
  always show the correct value.
</p>
<p>
  Our first thought (mine as well), where to put this data in Redis. A small
  amount of data, frequently updated and often requested felt like a perfect
  match for Redis. A single beefy Redis (many cores, lots of RAM type of
  machine) should be able to handle the load, so we would only need a couple
  (for redundancy). We bought (no cloud!) and installed the machines and started
  to configure Redis. While doing this a question rose in my mind - &#39;Why can&#39;t
  we just store this data on the shards in PostgreSQL, next to the swipes?&#39;. The
  data itself would be microscopic in comparison and the additional load would
  also be microscopic in comparison to what these servers were already doing.
  After discussions in the team, they agreed! It would just add complexity to
  our relatively simple stack to add Redis!
</p>
<p>
  Guess what, after deployment the added load couldn&#39;t barely be seen on the db
  servers. Success!
</p>
<p>
  This is one of my favorite architecture moments. Something I think back to
  whenever someone proposes adding new tech, but the reasoning behind is not
  thought through all the way.
</p>

<h2>Second time, at Bannerflow</h2>
<p>
  After I left Tantan I joined a ad tech company called
  <a href="https://www.bannerflow.com/">Bannerflow</a>. There we built a &#34;CMP&#34;,
  a web-platform for our enterprise users to create and publish ads online.
</p>
<p>
  At Bannerflow, one team were building a new set of microservices to configure
  and publish ads to social networks like Facebook. While there are certainly
  complexities in regard to how format the ads to fit the various social
  network&#39;s API contracts, get access etc., this was not a high load scenario
  like at Tantan.
</p>
<p>
  The team decided to add a Redis instance, for some cache of sorts. Note that
  this was for a system with a load not even 0.1% of Tantan&#39;s.
</p>
<p>
  Unfortunately, I will not be able to explain why Redis was (thought to be)
  needed in this post, because I don&#39;t know the reasoning either. After the
  initial developer left, no-one in the team could explain it to me when we
  discussed it! Looking at the code, or the number of calls or any other metric
  I could not see any reason, and we could agree that given a bit of spare time
  the best would be to remove it. (Now, running a managed Redis in Azure is not
  a lot of work, so I could not justify rearchitect it just for simplicity, but
  the long term plan was clear)
</p>
<p>
  I have to take this on me for not staying close enough to the team while they
  were designing and building the new feature, and despite skilled developers we
  had a (in this case) useless Redis sneaked in there. Lesson learned!
</p>

<h2>Third time, at MAJORITY</h2>
<p>
  And finally, as of 2024 I now work at the fintech company
  <a href="https://www.majority.com">MAJORITY</a>. And surprise, surprise, they
  had just introduced Redis in the months before I joined!
</p>
<p>
  The first usage was to cache the result of an external API call to lookup
  geolocation of users, so that the service in question could process location
  requests quicker, and also run cheaper. What I think is interesting here is
  that it is very reasonable to want to cache this data, no objection here.
</p>
<p>
  What is less clear is what Redis added. By chance this specific service did
  two things for the lookup, one DB call (to a Azure SQL Database), and one
  Redis call. This made it very easy to compare and evaluate.
</p>
<p>
  The service in question had its own DB, sharing a DB cluster with other
  services. This specific DB had so low amount of load that it did not even show
  up when looking at the cluster load in Azure. Moving the Redis usage over to
  the DB would result in ~2x that load, which is a big
  <em>relative</em> increase, but in <em>absolute</em> numbers a very minor
  increased load on the DB when the original load was more or less 0!
</p>
<p>
  Of course, when a new tech is introduced into a tech stack, more and more
  parts will start using it. Same here. Soon the need arose to have locks shared
  between several instances of the same microservice. Since Redis was already in
  use, it was natural to use it for this as well.
</p>
<p>
  But, looking closer it was easy to see that these locks could just as well use
  the locking mechanism inside the main DB (Azure SQL). Some mentioned
  performance and to not put more load on the DB, but just as before it was not
  a high performance use case, actually not anywhere near even 1 lock per
  second.
</p>
<p>
  Just like in Bannerflow, the implementation were already done by the time I
  really understood it. And same as in Bannerflow, we decided to try and move
  away from Redis.
</p>

<h2>Ending</h2>
<p>
  Each of the three cases were unique, with different tech stacks, different
  domains and different load. But somehow they were still unified by the desire
  to use Redis!
</p>
<p>
  If you nod in agreement, or are at least open for more on the same topic,
  there&#39;s this quite well known talk by Dan McKinley I recommend to check out:
  <a href="https://boringtechnology.club/">Choose Boring Technology</a> By pure
  coincidence it&#39;s also about Redis...
</p>

<i>Victor 2024-03-12</i>
</div>
  </body>
</html>
