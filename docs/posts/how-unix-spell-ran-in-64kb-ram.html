<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram">Original</a>
    <h1>How Unix spell ran in 64kb RAM</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p>How do you fit a 250kB dictionary in 64kB of RAM and still perform fast lookups? For reference, even with modern compression techniques like gzip -9, you can&#39;t compress this file below 85kB.</p><p><span>In the 1970s, Douglas McIlroy faced this exact challenge while implementing the spell checker for Unix at AT&amp;T. The constraints of the PDP-11 computer meant the entire dictionary needed to fit in just 64kB of RAM. A seemingly impossible task.</span></p><p>The story of Unix spell is more than just historical curiosity. It&#39;s a masterclass in engineering under constraints: how to analyze a problem from first principles, leverage mathematical insights, and design elegant solutions that work within strict resource limits.</p><p>If you&#39;re short on time, here&#39;s the key engineering story:</p><ul><li><p>The Unix spell started in the 1970s as an afternoon prototype by Steve Johnson at AT&amp;T, before Douglas McIlroy rewrote it to improve its performance and accuracy.</p></li><li><p>McIlroy&#39;s first innovation was a clever linguistics-based stemming algorithm that reduced the dictionary to just 25,000 words while improving accuracy.</p></li><li><p>For fast lookups, he initially used a Bloom filter—perhaps one of its first production uses. Interestingly, Dennis Ritchie provided the implementation. They tuned it to have such a low false positive rate that they could skip actual dictionary lookups.</p></li><li><p>When the dictionary grew to 30,000 words, the Bloom filter approach became impractical, leading to innovative hash compression techniques.</p></li><li><p>They computed that 27-bit hash codes would keep collision probability acceptably low, but needed compression.</p></li><li><p>McIlroy&#39;s solution was to store differences between sorted hash codes, after discovering these differences followed a geometric distribution.</p></li><li><p>Using Golomb&#39;s code, a compression scheme designed for geometric distributions, he achieved 13.60 bits per word—remarkably close to the theoretical minimum of 13.57 bits.</p></li><li><p>Finally, he partitioned the compressed data to speed up lookups, trading a small memory increase (final size ~14 bits per word) for significantly faster performance.</p></li></ul><p>The rest of the article expands each of these points and gives a detailed explanation with all the math and logic behind them.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg" width="800" height="529" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:529,&#34;width&#34;:800,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;A PDP-11 machine, source: Wikipedia&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="A PDP-11 machine, source: Wikipedia" title="A PDP-11 machine, source: Wikipedia" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a><figcaption>A PDP-11 machine, source: Wikipedia</figcaption></figure></div><p>In order to secure funding for Unix, Ken Thompson and Dennis Ritchie pitched Unix as a text processing system for the patents department to AT&amp;T. Naturally, a text processing system needed a spell checker as well. </p><p>The first version of Unix spell was written by Steve Johnson in 1975 which was  a prototype. Jon Bentley mentions that Steve wrote it in one afternoon. Even though it worked, it was not very accurate. </p><p>It was pretty simple. It would split the input file into a stream of words, do some light preprocessing such as remove numbers and special characters, convert to lower case, then sort, unique, and finally pass the list to the spell program which would simply check for the existence of those words in a dictionary on the disk.</p><p>Because of its simplistic implementation, it was not very accurate, and also slow because of dictionary lookups on the disk.</p><p>After seeing the adoption of the initial version, Douglas McIlroy took up the project to rewrite it with the goal of improving the accuracy and performance of the tool. He worked on two separate fronts both involving some very clever engineering:</p><ul><li><p>Building an affix removal algorithm for reducing words to their stems, and a compact dictionary consisting of the stem words</p></li><li><p>A compact data structure for loading the dictionary into memory for doing fast lookups</p></li></ul><p>This article is going to be focused on the data structure design part, but let’s spend a section to get an overview on the affix removal algorithm to see how it worked.</p><p>Using a full fledged dictionary for doing lookups was slow because the computers those days had only a few kilobytes of main memory and using disk based lookups was even more slower. </p><p>Douglas McIlroy came up with the idea of an algorithm which would iteratively remove common prefixes and suffixes from a word and look up a dictionary to see if the reduced word is present in it or not. The algorithm would follow the affix removal process until there were no affixes left to remove and if even after this the word was not present in the dictionary, then it would be flagged as a misspelling.</p><p>For instance, the algorithm would reduce the word “misrepresented” to “present” by removing the prefixes “mis”, “re”, and the suffix “ed”. And because “present” is a valid word in the dictionary, it would not flag it as a misspelling.</p><p>This affix removal technique was not 100% accurate and would sometimes let misspelled words pass through. But, such occurrences were deemed acceptable at that time. He also implemented a bunch of exceptions to these rules to avoid some of the common errors. </p><p>Overall, this algorithm resulted in a very compact dictionary. The final dictionary consisted of 25,000 words, which seemed possible to load into memory with a well engineered data structure.</p><p>Let’s move on to discussing how he managed to implement in-memory dictionary lookups with just 64 kB of memory.</p><div><p>Bloom published his work on Bloom filter in 1970 while the Unix spell was developed in the mid-1970s. At this time, Bloom filter was not even called Bloom filter. In his paper, Douglas calls it a “superimposed code scheme”. </p><p>Interestingly, the Bloom filter implementation he used was given to him by Dennis Ritchie.</p></div><p>Even though the dictionary size was 25,000 words, it was still not possible to load it as it is in just 64kB of RAM. Besides, it also needed fast lookups. </p><p><span>The first data structure that Douglas used was a Bloom filter. In the paper he doesn’t call it Bloom filter, instead he refers to it as a “superimposed coding scheme”, attributed to </span><a href="https://dl.acm.org/doi/pdf/10.1145/362686.362692" rel="">Bloom’s paper from 1970</a><span>. Interestingly, he gives the credit for the implementation of the Bloom filter he used to Dennis Ritchie.</span></p><p>A Bloom filter consists of a bit table initialized to all zeros. To add an item to the Bloom filter, you apply multiple hash functions to the item. Each hash function generates an index in the table, and that bit index is set to 1. If k hash functions are used, then, k different bit indices are turned on in the table.</p><blockquote><h5><em><span>For a more detailed explanation of Bloom filter, please check out </span><a href="https://blog.codingconfessions.com/p/bloom-filters-and-beyond" rel="">my article on Bloom filters</a><span>.</span></em></h5></blockquote><p><span>Looking up an item, whether it exists in the table or not, requires the same procedure. You need to apply the k hash functions, and for each of them check if the corresponding bit is set to 1 in the table or not. If even one of the bits is not on, then it means that the item is not present in the dataset. However, if all the bits are set, then it indicates that the item </span><em>might</em><span> be present, but this may also be a false positive. </span></p><p>False positives can occur because of hash collisions. When querying for an item, we cannot be 100% sure if a bit is on in the table because of the query item, or because of a hash collision with another item. </p><p>When using Bloom filter, you need to implement a strategy to handle false positives. For instance, in this case it could mean doing a full dictionary search. But that would defeat the whole purpose of using a Bloom filter, which was to save memory and do fast dictionary lookups. In the case of a spell checker, most of the words exist in the dictionary and only a fraction of words are misspelled, so we would be checking the full dictionary quite a lot.</p><p><span>However, a Bloom filter can be tuned to achieve a desired false positive rate. The following formula computes the false positive probability for a Bloom filter with a given size </span><code>n</code><span>, number of inserted items </span><code>m</code><span>, and number of hash functions </span><code>k</code><span>. </span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{q} \approx \left( 1 - e^{-\frac{kn}{m}} \right) \\
&amp;\text{where:} &amp; \\
&amp;\text{q}:\ \text{Probability that a bit is 1} \\
&amp;m: \ \text{Number of bits in the Bloom filter} \\
&amp;k: \ \text{Number of hash functions used} \\
&amp;n: \ \text{Number of elements inserted}
\end{align*}
\)</span></p></div><p>In his paper, Douglas mentions that a false positive probability of 1 in 2000 was acceptable to them, which meant that for such a low false positive rate, they did not need to consult the dictionary. </p><p>As they had a dictionary of 25,000 items, the number of items was fixed. They fixed the bit table size at 400,000 bits because of the limited amount of memory. Based on these factors, using 11 hash functions allowed them to keep the false positive rate at around 1/2000.</p><p>They used the Bloom filter based spell implementation for a while. In the paper, Douglas mentions that even though the false positive rate was acceptable, in the wild, they were encountering a lot of new words that needed to be added to the dictionary. This led to the dictionary size going up from 25,000 to 30,000.</p><p>However, for a dictionary of this size, their Bloom filter required a bigger bit table size which was not possible for them. As a result, Douglas looked for alternate data structure designs to be able to fit a dictionary of 30,000 words in memory with similar lookup performance and accuracy.</p><p>As the dictionary size exploded from 25,000 to 30,000, Douglas needed a more memory efficient data structure to hold the dictionary in memory. </p><p>A hash table was an attractive solution, but it would have consumed much more memory than a Bloom filter because it requires storing the hash, as well as, the actual words to handle collisions.</p><p>Instead of a full hash table, Douglas decided to store just the hashes of the words. The lookup required computing the hash of the input word, and then checking for its existence in the hashes using a scan. </p><p>One intuition for doing so might have been that the individual words can be of varying lengths, but a hash function will naturally compress them down to a fixed number of bits, and that may possibly allow them to fit the hashes in memory.</p><p>But hashes can collide, so they needed a large enough hash code to have an acceptably low probability of collisions.</p><p><span>If each word in the dictionary is hashed to a hash code of size </span><code>b</code><span> bits, then there are </span><code>2^b</code><span> total possible hash codes in that space. If the size of the dictionary is </span><code>v</code><span> words, then the probability of a hash collision can be computed as:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
\text{P(hash collision)} = \frac{v}{2^b}
\end{align*}\)</span></p></div><p>They had a dictionary of 30,000 words, which is ~2^15 words. Moreover, he mentions that a collision probability of 1 in 2^12 was acceptable to them. This gave a hash code size of 27 bits.</p><p>But 27-bit hash codes were too big: with 2^15 words, they needed 2^15 * 27 bits of memory, while the PDP-11 had only 2^15 * 16 bits (64kB) of RAM—compression was essential. </p><p>Before implementing any compression algorithm, we need to know what is the theoretical minimum number of bits we can achieve to compress this piece of data. It acts as a benchmark to tell us how well we are able to compress the data.</p><p><span>This theoretical minimum is computed using the information content of the event which generated the data we are trying to compress. This concept comes from </span><a href="https://en.wikipedia.org/wiki/Information_theory" rel="">information theory</a><span> which is the underpinning foundation for all of the data compression techniques. </span></p><p><span>The basic idea behind </span><a href="https://en.wikipedia.org/wiki/Information_content" rel="">information content</a><span> is to use the probability of an event to determine how many bits are needed to encode it without loss of information. </span></p><p>A highly likely event carries less information (for instance a 100% probable event has no information and needs 0 bits to encode), while a less probable event contains much more information and needs more bits. There is an inverse relationship between the probability of an event and its information content, which leads to the following formula:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{I(E)} = \log_2\left(\frac{1}{P(E)}\right) \\
&amp;\text{Or, I(E) = } -\log_2{P(E)} \\
&amp;\text{where P(E) is } \text{probability of event E}
\end{align*}\)</span></p></div><p>Now, to compute the information content of a set of hash codes, we need to figure out the probability of generating them.</p><p><span>If the size of a hash code is </span><code>b</code><span> bits, then there are a total of </span><code>2^b</code><span> possible hash codes in that space. Out of that, we are selecting a set of </span><code>v</code><span> unique hash codes. </span></p><div data-component-name="Latex"><p><span>\(

\begin{align*}


\text{The total number of ways to select such sets = }\binom{2^b}{v}
\end{align*}
\)</span></p></div><p>Therefore, the probability of any one of these sets of being generated is:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
\text{P} = 
\frac{1}{\binom{2^b}{v}}
\end{align*}
\)</span></p></div><p>Thus, the information content of these hash codes is:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;I(E) = - \log_2\left(\frac{1}{\binom{2^b}{v}}\right) \\
&amp;= \log_2\left(\binom{2^b}{v}\right) \\
&amp;= \log_2\left(\frac{(2^b)!}{v! \, (2^b - v)!}\right)
\end{align*}\)</span></p></div><p><span>To simplify things, we can use </span><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation" rel="">Stirling’s approximation</a><span>:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{Stirling&#39;s approximation: } \quad \log_2(n!) \approx n \log_2(n) - n \log_2(e), \\
\end{align*}
\)</span></p></div><p><span>The paper makes another simplifying assumption that the number of words in the dictionary (30,000) is much smaller than the total number of hash codes (2^27), i.e., </span><code>v « 2^b</code><span>, this allows them to simplify </span><code>(2^b - v)</code><span> as  </span><code>2^b</code><span> in the above computation. </span></p><p>Using these two approximations leads to the following formula for the information content:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;I(E) \approx v\left[b - \log_2\left(\frac{v}{e}\right)\right]
\end{align*}\)</span></p></div><p><span>Plugging in </span><code>v=30,000</code><span> and </span><code>b=27</code><span>, the minimum number of bits needed to encode a single hash code turns out to be 13.57, which was ~50% shorter than the original hash codes, and within the capacity of the PDP-11’s memory.</span></p><p><span>At this point they knew how much compression they could achieve but the bigger question was how to get there. Instead of compressing the raw hash codes, what if they computed and stored the differences between successive hash codes (in their sorted order)? This is similar to how </span><a href="https://en.wikipedia.org/wiki/Delta_encoding" rel="">delta encoding</a><span> works, but not quite the same.</span></p><p>There were a couple of advantages of working with hash differences.</p><ul><li><p>By definition the differences were smaller than the raw hash codes</p></li><li><p>And many of the difference value would repeat because the difference of several hash codes might be the same. </p></li></ul><p>This implied that it was easier to compress these differences than the hash codes. </p><p>Hash differences were computed by sorting the hash codes and taking differences between consecutive values.</p><p>For instance:</p><pre><code>sorted hash codes: 5, 14, 21, 32, 55, 67
hash differences: 5, 9, 7, 11, 23, 12</code></pre><p>Let’s also see how the lookup of a word worked when they stored hash differences instead of the actual value.</p><p>To check if a word exists in the dictionary or not, they would compute the hash of the word and check for its existence in the dictionary via a simple algorithm. </p><pre><code>lookup(input_hashcode) -&gt; bool:
  sum = hash_differences[0]
  i = 1
  while True:
    sum += hash_differences[i]
    if sum == input_hashcode:
      return True
    if sum &gt; input_hashcode:
      return False
    i += 1</code></pre><p>Now, let’s discuss how they came up with a compression scheme for this data.</p><p>The basic principle behind lossless compression is to assign shorter codes to symbols with higher probabilities, and longer codes to symbols with lower probabilities. This makes sense because symbols with higher probabilities tend to occur more frequently in the data and assigning them shorter codes means higher compression rate.</p><p>But this requires computing the probability distribution of all the symbols in the data, and then using it to generate compressed codes. The probability distribution table is needed at decompression time as well to perform the decoding. </p><p>This had two problems for Douglas:</p><ul><li><p>Holding a probability distribution table for ~30,000 symbols in memory would have taken away any compression advantage he was getting from compression itself. So he needed a scheme which was free of this requirement.</p></li><li><p>Computing the probabilities of the hash differences would have been time expensive. All the 30,000 possible hash difference values, their sums and counts would not have been possible to keep in memory for computing their probabilities. So, it would have required an expensive disk based data structure to compute these probabilities.</p></li></ul><p>But McIlroy came up with an elegant solution by recognizing that the hash differences followed a geometric distribution, enabling an efficient compression scheme. Let’s first understand how these hash difference values are geometrically distributed.</p><p><span>The </span><a href="https://en.wikipedia.org/wiki/Geometric_distribution" rel="">geometric distribution</a><span> is a discrete probability distribution which is used to model scenarios where we conduct an experiment until we get a success. For instance, rolling a six-faced die until we get a “1” forms a geometric distribution, with the probability of success being 1/6. A simpler example is tossing a coin until we get a head. </span></p><p><span>If the probability of failure is </span><code>p</code><span>, the probability of success is </span><code>q</code><span>, and if success occurs in the </span><code>kth</code><span> trial, then the </span><a href="https://en.wikipedia.org/wiki/Probability_mass_function" rel="">probability mass function</a><span> of the geometric distribution is given by the following formula:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;P = p^kq
\end{align*}\)</span></p></div><p>Now, let&#39;s understand how the hash difference values map to this distribution.</p><p><span>As each hash code is </span><code>b</code><span> bits wide, we have a space of </span><code>2^b</code><span> points. And we have </span><code>v</code><span> hash codes spread out in this space. The probability of any point in this space containing a hash code is </span><code>q=v/2^b</code><span>, and the probability of a point being empty is </span><code>p=1-(v/2^b)</code><span>.</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{P(a point containing a hash code)} = q = \frac{v}{2^b} \\
&amp;\text{P(an empty point)} = p = 1 - \left(\frac{v}{2^b}\right)
\end{align*}\)</span></p></div><p><span>But we are interested in modelling the distribution of hash difference values, rather than the hash codes themselves. A hash difference </span><code>k</code><span> occurs when two consecutive hash values in the sorted sequence are </span><code>k</code><span> positions apart. For instance, if we have two successive hash code values 20 and 25, then the hash difference is 5.</span></p><p><span>What&#39;s the probability of seeing a hash difference of </span><code>k</code><span>? Given any hash value </span><code>h</code><span> in our space:</span></p><ul><li><p><span>We need the next </span><code>k-1</code><span> positions after </span><code>h</code><span> to be empty</span></p></li><li><p><span>And then we need a hash value at position </span><code>h+k</code></p></li><li><p><span>The probability of </span><code>k-1</code><span> empty positions is </span><code>p^(k-1)</code></p></li><li><p><span>The probability of a hash value at position </span><code>h+k</code><span> is </span><code>q</code></p></li></ul><p><span>Therefore, the probability of a hash difference of </span><code>k</code><span> is:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{P(difference=k) =} p^\left(k-1\right)q
\end{align*}\)</span></p></div><p>This follows exactly the form of a geometric distribution!</p><blockquote><h5><em>If you read the spell paper, you will find that the author takes a different route to arrive at this conclusion. He models the generation of the hash codes as a Poisson process and proceeds from there.</em></h5></blockquote><p><span>But, what is the point of modelling this as a geometric distribution? It turns out, there is a very simple and efficient </span><a href="https://en.wikipedia.org/wiki/Run-length_encoding" rel="">run-length encoding</a><span> scheme for geometrically distributed integers given by Golomb in his 1965 </span><a href="https://web.stanford.edu/class/ee398a/handouts/papers/Golomb%20-%20Run-Length%20Codes%20-%20IT66.pdf" rel="">paper</a><span>. Let’s see how it works.</span></p><p>Golomb’s code is a simple run-length encoding scheme for geometrically distributed integers which was used by Douglas to compress the hash differences. It takes advantage of the fact that geometrically distributed values have an exponentially decaying probability of success, which can be leveraged for performing compression.</p><p><span>What do we mean by that? Recall that in the case of geometric distribution, the probability of success after </span><code>k</code><span> trials is given as:</span></p><p><span>Let’s say, we find an integer </span><code>m</code><span>, such that </span><code>p^m = 1/2</code><span>. Then, it means that the probability of success after </span><code>k + m</code><span> trials is:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{P(success after k + m trials)} = p^\left(k + m\right)q = \frac{1}{2}p^kq
\end{align*}\)</span></p></div><p><span>In other words, the probability of getting success in </span><code>k + m</code><span> trials is half of that of  the probability of getting a success in </span><code>m</code><span> trials. And this probability continues to half every </span><code>m</code><span> successive trials. This is an exponentially decaying distribution of probabilities.</span></p><p><span>Let&#39;s consider a fair coin toss example where we toss the coin until a head occurs. For a fair coin, </span><code>p = q = 1/2</code><span>.</span></p><p><span>Here, we have </span><code>m = 1</code><span>, because </span><code>p^1 = 1/2</code><span>. It means that the probabilities decay by half every trial:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;P(k=1) = \frac{1}{2} \\
&amp;P(k=2) = \frac{1}{4} \\
&amp;P(k=3) = \frac{1}{8} \\
&amp;P(k=4) = \frac{1}{16}


\end{align*}\)</span></p></div><p><span>Let’s take another example of a biased coin, such that </span><code>p=1/sqrt(2) = 0.707</code><span>, and </span><code>q=0.293</code><span>.</span></p><p><span>Here, we have </span><code>m=2</code><span>, because </span><code>p^2 = 1/2</code><span>. In this case the probabilities decay after blocks of size 2.</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;P(k=1) = 0.293 \\
&amp;P(k=2) = 0.207 \\
&amp;P(k=3) = 0.116 \\
&amp;P(k=4) = 0.104 \\
&amp;P(k=5) = 0.073 \\
&amp;P(k=6) = 0.051


\end{align*}\)</span></p></div><p><span>You can see that the probabilities decay by half for every even value of </span><code>k</code><span>. For instance:</span></p><div data-component-name="Latex"><p><span>\(\frac{P(k=2)}{P(k=4)} = \frac{P(K=4)}{P(k=6)} \approx 0.5\)</span></p></div><p><span>This pattern of exponential decay allows us to group the hash difference values in blocks of size </span><code>m</code><span>. Each value within a block gets a code of size </span><code>k</code><span> bits, while the next block gets codes of size </span><code>k + 1</code><span> bits. The reasoning behind it is rooted in information theory.</span></p><p><span>The minimum number of bits required to encode the outcome of an event is given by its information content which is </span><code>-log₂(p)</code><span>, where </span><code>p</code><span> is the probability of that event. </span></p><p>It means that if an event has probability 1/2, it needs 1 bit code, an event with probability 1/4 needs 2 bits, an event with probability 1/8 needs 3 bit codes and so on.</p><p><span>We can leverage this for geometrically distributed values by arranging them in blocks of size </span><code>m</code><span>. The values within a given block can be assigned codes of equal length, and the values in the subsequent block gets codes one bit wider for its </span><code>m</code><span> values.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png" width="680" height="165" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:165,&#34;width&#34;:680,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:20471,&#34;alt&#34;:&#34;Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes.&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes." title="Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes.</figcaption></figure></div><p>It turns out, formation of blocks with increasingly larger codes also leads to a beautiful self-similar bit pattern with properties that make it very easy to generate such codes. Let’s see how this self-similar pattern forms.</p><p>Self-similar pattern essentially means that the codes at a specific index within a block repeats itself at the same index in the next block, with one padding bit added on its left.</p><p>For instance, if the code at the 2nd position in the first block is 0001, then the 2nd code in the 2nd block will be 10001. Similarly, the 2nd position code in the 3rd block will be 110001, and so on. </p><p>This self-similar pattern naturally emerges because of the enforcement of 1 bit longer codes in each successive block. Let us see a concrete example.</p><p><span>Let’s say, our block size is </span><code>m=5</code><span> and the codes in the first block are </span><code>k=4</code><span> bits wide. </span></p><p><span>If the first code in the first block is </span><code>0110</code><span>, then we can generate the codes for the rest of the block by adding 1 to the previous code value. The codes for the first block will look like this:</span></p><pre><code>block-1 codes: 0110 0111 1000 1001 1010</code></pre><p><span>As you can see, the first block ends at the code </span><code>1010</code><span>. The natural value for the next code should be </span><code>1011</code><span>, but because this code lies in the next block, its code has to be 1 bit larger. As a result we need to left shift it by one bit, which makes it </span><code>10110</code><span>. And if you notice the least 4 bits of this code are the same as the first code in the first block. The following diagram highlights this visually.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png" width="581" height="511" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/db717247-7d87-4196-8c38-be44f82fe747_581x511.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:511,&#34;width&#34;:581,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:37780,&#34;alt&#34;:&#34;An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block" title="An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption>An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block</figcaption></figure></div><pre><code>Another thing worth Noticing here is that we are obtaining the first value of the 2nd block by left shifting it by 1 bit, which means its least significant bit (LSB) is always 0. 

For the self-similar patterns to form, the first code of the first block should also always have its LSB set to 0. This implies that this first code is always an even number of the form 2x. </code></pre><p><span>The self-similar code has a couple of additional properties which provide an intuitive way to figure out the minimum bit width </span><code>k</code><span> for the codes in the first block. Let’s see how.</span></p><p><span>To figure out the minimum number of bits needed to encode the first block, let’s assume the first encoded value in the first block is </span><code>2x</code><span>. </span></p><p><span>Then the first code of the 2nd block </span><em>should</em><span> be </span><code>2x + m</code><span>. However, because the codes in the next block need to be 1 bit wider, this value gets shifted to the left by 1 bit, which makes it </span><code>2(2x + m)</code><span>.</span></p><blockquote><h6><em>Left shifting a value by 1 bit doubles it.</em></h6></blockquote><p><span>The self-similarity pattern gives rise to another way to think about these codes. If the first code in the first block is </span><code>2x</code><span>, then in the next block:</span></p><ul><li><p><span>We want the same pattern (</span><code>2x</code><span>)</span></p></li><li><p>But with an extra bit on the left</p></li><li><p><span>Adding a bit on the left is equivalent to adding </span><code>2^k</code></p></li><li><p><span>So the code becomes </span><code>2^k + 2x</code></p></li></ul><p>Combining these two relations gives us the following equation</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;2^k + 2x = 2(2x + m) \\
&amp;\text{Or, } 2^k = 2m + 2x \\
&amp;\text{Here, x is a nonnegative integer, so we can simplify the above to:} \\
&amp;2^k \ge 2m
\end{align*}\)</span></p></div><p><span>By solving for the smallest integer value of </span><code>k</code><span> we can get the code width of the first block.</span></p><p><span>The same equation also gives us the value of </span><code>x</code><span>:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
x = 2^\left(k-1\right) - m = 2^{\log_2(m)} - m
\end{align*}\)</span></p></div><p>Knowing the values of m, k, and x gives way for a simple encoding algorithm.</p><p>The paper by Golomb gave a different algorithm for encoding and decoding, but the Unix spell code used a slightly complicated but more efficient algorithm. I describe the algorithm as implemented in the Unix spell.</p><p>Let’s understand how to encode a value using Golomb’s code. Recall that we have:</p><ul><li><p><span>Initial bit width </span><code>k</code></p></li><li><p><span>Block size </span><code>m</code></p></li><li><p><span>First code in first block is </span><code>2x</code><span>, where </span><code>x = 2^(k-1) - m</code></p></li></ul><p>Here’s the encoding algorithm:</p><pre><code><code>def encode(value):
    # Case 1: Values less than x
    # These get shorter codes of length k-1
    # Because we have unused bit patterns available
    if value &lt; x:
        return value, k-1  # return (code, length)
    
    # Case 2: Values &gt;= x
    # Need to find which block they belong to
    value = value - x     # adjust relative to first code
    y = 1                 # tracks block number through bit shifts
    length = k           # start with k bits
    
    # Find block by repeatedly subtracting block size
    while value &gt;= m:    # m is block size
        value -= m       # move to next block
        y = y &lt;&lt; 1      # add padding bit for next block
        length += 1     # each block needs one more bit
    
    # Generate final code:
    # (y-1) &lt;&lt; k creates padding bits based on block number
    # x*2 adds offset for the first code
    # value adds position within current block
    code = ((y-1) &lt;&lt; k) + (x*2) + value
    return code, length
</code></code></pre><p>The following figure shows two examples to illustrate how it works in practice:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png" width="939" height="1121" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1121,&#34;width&#34;:939,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:105065,&#34;alt&#34;:&#34;Examples of how the encoding algorithm works for values 2 and 8&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="Examples of how the encoding algorithm works for values 2 and 8" title="Examples of how the encoding algorithm works for values 2 and 8" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption>Examples of how the encoding algorithm works for values 2 and 8</figcaption></figure></div><p><span>You can find the original Unix svr4 implementation of this algorithm </span><a href="https://github.com/calmsacibis995/svr4-src/blob/main/cmd/spell/huff.c#L105" rel="">here</a><span>.</span></p><p>The decoding process is also not that complicated. Given a code, we need to:</p><ol><li><p><span>First look at its top </span><code>k-1</code><span> bits (call this w):</span></p><ul><li><p><span>If </span><code>w</code><span> is less than </span><code>x</code><span>, then this is a shorter code</span></p></li><li><p><span>The decoded value is simply </span><code>w</code><span> itself</span></p></li></ul></li><li><p><span>If </span><code>w ≥ x</code><span>, then we need to </span></p><ul><li><p>Include one more bit into w</p></li><li><p><span>Look at the least significant </span><code>k</code><span> bits of </span><code>w</code><span>: call it </span><code>u</code></p></li><li><p><span>if </span><code>u &lt; 2x + m</code><span>:</span></p><ul><li><p><code>value = x + u + (s - 1)m</code></p></li><li><p><span>where </span><code>s</code><span> is the number of extra bits included into w</span></p></li></ul></li><li><p>else:</p><ul><li><p><span>keep including more bits into </span><code>w</code><span> until </span><code>u &lt; 2x + m</code></p></li></ul></li></ul></li></ol><p><span>You can find the original Unix svr4 implementation of the decode algorithm </span><a href="https://github.com/calmsacibis995/svr4-src/blob/main/cmd/spell/huff.c#L83" rel="">here</a><span>.</span></p><p>So how well this technique was able to compress the hash differences?</p><p>Recall that the theoretical limit of compression was 13.57 bits per word. Golomb codes managed to achieve an expected code length of 13.60, remarkably close to this theoretical minimum.</p><p>However, looking up a value in this compressed dictionary was quite slow. It required starting from the beginning, decoding and summing values until finding or exceeding the desired hash code.</p><p><span>To speed this up, the final Unix spell implementation partitioned the table of differences into </span><code>M</code><span> bins. This allowed them to first locate the correct bin, and then only scan within that bin, speeding up the search by a factor of </span><code>M</code><span>.</span></p><p><span>This partitioning scheme required storing additional pointers to the bins, adding </span><code>log₂M</code><span> bits per word to the storage requirement. The total storage increased to about 14 bits per word, but this was an acceptable trade-off: it was still within their memory budget while providing significantly faster lookups.</span></p><p data-attrs="{&#34;url&#34;:&#34;https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;,&#34;action&#34;:null,&#34;class&#34;:null}" data-component-name="ButtonCreateButton"><a href="https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>The Unix spell command is a fascinating piece of engineering history that emerged from the severe memory constraints of the PDP-11. What makes it particularly interesting is how it evolved from a simple disk-based dictionary lookup to an elegant solution combining multiple computer science concepts:</p><ul><li><p>Probabilistic data structures (Bloom filters)</p></li><li><p>Information theory (optimal bit width calculations)</p></li><li><p>Probability theory (geometric distribution)</p></li><li><p>Compression algorithms (Golomb coding)</p></li></ul><p>The engineering journey is particularly instructive:</p><ol><li><p>Started with Bloom filters achieving acceptable false positive rates</p></li><li><p>When dictionary size grew, switched to compressed hashing where:</p><ul><li><p>They computed theoretical minimum bits needed</p></li><li><p>Recognized patterns in hash differences</p></li><li><p>Used Golomb&#39;s code to achieve near-optimal compression</p></li><li><p>Added binning for faster lookups with minimal space overhead</p></li></ul></li></ol><p>Even though modern spell checkers use different techniques like edit distance and language models, the engineering insights from Unix spell remain valuable. It shows how deep understanding of theoretical concepts combined with practical constraints can lead to efficient and elegant solutions.</p><p>Most importantly, it demonstrates that some of the best innovations happen when we are resource constrained, forcing us to think deeper about our problems rather than throwing more hardware at them.</p><ul><li><p><a href="https://ia800601.us.archive.org/11/items/development-of-spelling-list/Image092317125441_text.pdf" rel="">Development of a Spelling List by Douglas McIlroy, 1982</a></p></li><li><p><a href="https://web.stanford.edu/class/ee398a/handouts/papers/Golomb%20-%20Run-Length%20Codes%20-%20IT66.pdf" rel="">Run-length Encodings by Golomb, 1965</a></p></li><li><p><a href="https://dl.acm.org/doi/pdf/10.1145/362686.362692" rel="">Space/Time Trade-offs in Hash Coding with Allowable Errors by Bloom, 1970</a></p></li><li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3532.315102" rel="">A Spelling Checker by Jon Bentley, 1985</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/History_of_Unix#1970s" rel="">History of Unix</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/Geometric_distribution" rel="">Geometric Distribution</a></p></li><li><p><a href="https://blog.codingconfessions.com/p/bloom-filters-and-beyond" rel="">Bloom filter</a></p></li><li><p><a href="https://github.com/calmsacibis995/svr4-src/tree/main/cmd/spell" rel="">Unix Spell Source</a><span> </span></p></li></ul><p><em>If you find my work interesting and valuable, you can support me by opting for a paid subscription (it’s $6.40 monthly/$58 annual). As a bonus you get access to monthly live sessions, and all the past recordings.</em></p><p><strong>Subscribed</strong></p><p><em><span>Many people report failed payments, or don’t want a recurring subscription. For that I also have a </span><a href="https://buymeacoffee.com/codeconfessions" rel="">buymeacoffee page</a><span>. Where you can buy me a coffee or become a member. I will upgrade you to a paid subscription for the equivalent duration here.</span></em></p><p data-attrs="{&#34;url&#34;:&#34;https://buymeacoffee.com/codeconfessions&#34;,&#34;text&#34;:&#34;Buy me a coffee&#34;,&#34;action&#34;:null,&#34;class&#34;:&#34;button-wrapper&#34;}" data-component-name="ButtonCreateButton"><a href="https://buymeacoffee.com/codeconfessions" rel=""><span>Buy me a coffee</span></a></p><p><em>I also have a GitHub Sponsor page. You will get a sponsorship badge, and also a complementary paid subscription here.</em></p><p data-attrs="{&#34;url&#34;:&#34;https://github.com/sponsors/abhinav-upadhyay&#34;,&#34;text&#34;:&#34;Sponsor me on GitHub&#34;,&#34;action&#34;:null,&#34;class&#34;:&#34;button-wrapper&#34;}" data-component-name="ButtonCreateButton"><a href="https://github.com/sponsors/abhinav-upadhyay" rel=""><span>Sponsor me on GitHub</span></a></p></div></div></div>
  </body>
</html>
