<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.owlposting.com/p/a-primer-on-molecular-dynamics">Original</a>
    <h1>A Primer on Molecular Dynamics</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><article><div><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png" width="1456" height="817" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/de29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:817,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:2785248,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde29a064-9fa5-4ddc-96d8-6f5ce5f3a72a_2040x1144.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a></figure></div><ol><li><p><a href="https://www.abhishaike.com/i/144423480/introduction" rel="">Introduction</a></p></li><li><p><a href="https://www.abhishaike.com/i/144423480/how-does-md-work-in-practice" rel="">How does MD work in practice?</a></p><ol><li><p><a href="https://www.abhishaike.com/i/144423480/system-definition" rel="">System Definition</a></p></li><li><p><a href="https://www.abhishaike.com/i/144423480/force-fields" rel="">Force Fields </a></p><ol><li><p><a href="https://www.abhishaike.com/i/144423480/the-basic-definition" rel="">The basic definition</a></p></li><li><p><a href="https://www.abhishaike.com/i/144423480/the-details" rel="">The details</a></p></li></ol></li><li><p><a href="https://www.abhishaike.com/i/144423480/energy-minimization-equilibration" rel="">Energy minimization + equilibration </a></p></li><li><p><a href="https://www.abhishaike.com/i/144423480/production-simulation" rel="">Production simulation</a></p></li></ol></li><li><p><a href="https://www.abhishaike.com/i/144423480/interesting-miscellaneous-things" rel="">Interesting miscellaneous things</a></p><ol><li><p><a href="https://www.abhishaike.com/i/144423480/bypassing-small-timescales" rel="">Bypassing small timescales</a></p></li><li><p><a href="https://www.abhishaike.com/i/144423480/quantum-effects" rel="">Quantum effects </a></p></li><li><p><a href="https://www.abhishaike.com/i/144423480/free-energy-calculations" rel="">Free energy calculations</a></p></li></ol></li><li><p><a href="https://www.abhishaike.com/i/144423480/case-studies" rel="">Case studies </a></p><ol><li><p><a href="https://www.abhishaike.com/i/144423480/discovery-of-lirafugratinib-rly-a-highly-selective-irreversible-small-molecule-inhibitor-of-fgfr" rel="">Discovery of lirafugratinib (RLY-4008), a highly selective irreversible small-molecule inhibitor of FGFR2</a></p></li><li><p><a href="https://www.abhishaike.com/i/144423480/characterizing-receptor-flexibility-to-predict-mutations-that-lead-to-human-adaptation-of-influenza-hemagglutinin" rel="">Characterizing Receptor Flexibility to Predict Mutations That Lead to Human Adaptation of Influenza Hemagglutinin</a></p></li></ol></li><li><p><a href="https://www.abhishaike.com/i/144423480/conclusion" rel="">Conclusion</a></p></li></ol><p>I’ve recently become very interested in molecular dynamics, or MD. </p><p><span>Why would you want to do MD at all? MD allow you to watch molecules dance in full atomic detail. While experimental techniques give you static molecular mugshots, MD lets you observe the dynamic behavior of molecules over time. And understanding molecular motion is key for everything in biology, </span><strong>everything</strong><span> in biology is vibrating molecules underneath the surface! Beyond just watching, you can use MD to predict molecular behavior. Want to know how tightly a drug will bind its target? Run an MD simulation and calculate the binding free energy. Trying to design a new enzyme? Simulate different designs to see which folds and functions best. Obviously, there are limitations — as we’ll get into — but the possibilities the field presents are fascinating. </span></p><p><span>Unfortunately, MD also happens to be incredibly hard to learn on your own. Unlike my other posts, where I can go from nearly zero knowledge to blog-post-capable in a week, MD has taken me the better part of a month and I’m </span><em>still </em><span>unsure of a lot of things. I’m much more used to the general fuzziness of ML; extremely overparameterized statistical models that can actively learn means that their exact theoretical underpinnings are, at best, interesting, but rarely useful outside of a few cases. So, hand-waving is practically fine! This isn’t the case in MD, all of it feels important to know and understand. You’re dealing with physics after all, and even if you hand-wave away certain aspects of the physics, your simplified model will be that much less capable of understanding certain phenomena. </span></p><p>In this post, I’ll try to equip you with enough knowledge to run basic simulations on your own, while also having enough theoretical backing to understand what’s actually going on under the surface. Obviously, we’ll be skipping a lot of things, but there should be enough here to vaguely understand some MD papers! To test this out, we’ll also go over two MD papers at the end of the post. </p><p><span>Consider a protein that we dreamed up. Completely unfolded, just a straight line of amino-acids linked together, Glycine-Lysine-Alanine, and so on. What happens if we drop this protein in a bucket of water and wait a few hundred nanoseconds? It’ll slowly turn from a straight line into a knotted mess of tangles and curves. Or, in other words, it will </span><strong>fold. </strong></p><p>Most people will be familiar with the concept of a protein folding, the Alphafold2 news-cycle hammered that into most of us. But it&#39;s worth considering what it means for a protein to &#39;want&#39; to fold at all. In many ways, a protein &#39;wants&#39; to fold in the same way a dropped apple &#39;wants&#39; to fall to the ground — physics simply pulls it in that direction. In the case of the apple, gravity is making the biggest call. In the case of the protein, the forces at play are more at the molecular level. Here are some:</p><ul><li><p><strong>Bond forces</strong><span>: Chemical attachments between neighboring atoms that can be stretched, twisted, or turned to different angles.</span></p></li><li><p><strong>Electrostatic interactions</strong><span>: Attractions or repulsion between charged atoms. </span></p></li><li><p><strong>Van der Waals forces</strong><span>: Weak electromagnetic attractions between all atoms in close proximity</span></p></li><li><p><strong>Solvent interactions</strong><span>: Some parts of a protein may be hydrophobic/hydrophilic and will desire to hide its hydrophobic elements away from water + expose its hydrophilic elements to water. </span></p></li></ul><p><span>There’s a few more of course, but these are the major ones. These forces will push and pull on every atom contained in the protein, ultimately forcing it to a certain configuration. As mentioned, this final configuration is the folded state of the protein, but we could interpret this final state in a more physics-grounded way: </span><strong>low thermodynamic free energy.</strong><span> It’s not worth pondering too much </span><em>how</em><span> to quantitatively assess this, but rest assured there are, we’ll discuss it a bit more later. Low thermodynamic free energy of a folded structure means it’s a stable structure (hard to move from), and high means its an unstable structure (easy to move from). </span></p><p>To note, the above pushing-pulling on our protein isn’t just the case for protein folding, it’s for all problems! Docking with small molecules, chemical reactions, everything moves towards the direction of low thermodynamic free energy. </p><p>Let’s return to our protein though, how do we figure out the final folded state of our protein with MD? There’s some nuance here in that some proteins will have multiple states with low thermodynamic free energy states, so the equilibrium state for a protein could be flip-flopping between multiple states. Let’s ignore that for now and assume there is a singular final state for this specific protein. </p><p>First, we dunk the protein in a big cube of water. </p><p><span>Computationally that is. This isn’t an oversimplification, you literally ‘define’ a 5x5x5 (or however large) nanometer cube filled with H20 molecules around your protein, or the </span><strong>solvent</strong><span>. Why water? Most proteins exist in aqueous environments, so this is just an attempt to match the ‘natural’ environment of a protein. This is important because the environment often defines what final states a protein can reach — there are many possible end states of a protein, but not all of them are reachable in all environments. Lots of simulations also add in various salts and ions in the solution to bring the overall charge of the cube of water + protein to neutral, as that’s also (mostly) biologically realistic. To note, water isn’t the only possible solvent, one can really use anything, like ethanol, it just depends on whether the MD software package you’re using supports that. </span></p><p><span>Two last things. One, you may often hear the phrase ‘</span><strong>periodic boundary condition</strong><span>’ when it comes to the definition of this box of water, this just means that the box will loop any molecule that touches a side to the other side of the box. This isn’t always done but is common enough that I thought I should mention it. Two, the solvent can be represented </span><strong>implicitly</strong><span> or </span><strong>explicitly </strong><span>— if implicit, the solvent is represented as a continuous medium, if explicit, represented as individual molecules. This will change how the potential energy (discussed later) is calculated, but it’s a minor-add on that I won’t discuss too heavily during the rest of this post. For clarity&#39;s sake, the equations as present in the next sections will be assuming an implicit solvent. </span></p><p><span>This box filled with salts and ions + water + maybe some other things + our protein is often referred to as the ‘</span><strong>system</strong><span>’. Let’s not get </span><em>too</em><span> ahead of ourselves with pretending we’re being actually realistic though, we’re missing the millions of other proteomic, ionic, and otherwise interactions that our protein has while it actually floats around in-vivo. But assuming a box of water with some salts isn’t a bad starting point! </span></p><p><span>After we have our system, we need to get around to defining the laws of physics of that system. As in, given the charge, size, and so on for every atom in our system, what does real-world physics say should happen to the position of those atoms second to second? This is often deemed the ‘</span><strong>force field</strong><span>’ of the system, and there’s lots of different options for that. </span><em>That’s strange</em><span>, you may ask, </span><em>isn’t there only one set of laws of physic</em><span>s? Well, yes. But we don’t really know how to accurately capture those laws of physics in a computer in a way that’s even slightly tractable to solve, so a bunch of very smart people have all built very different ways of approximating those laws. The grander strokes of the force themselves are captured; all well-known forcefields have </span><em>some</em><span> conception of the major forces (discussed below). They differ in the nuances; some very minor things are ignored in some (e.g. flexibility in the water molecules), some hyperparameters are shifted away from theoretically correct values to empirically derived ones, and so on. A few names of force fields you may come across are CHARMM, AMBER, and GROMOS. </span></p><p><em>How do we choose which force field to use for our problem?</em><span> Some are explicitly tuned for some problems over others (e.g. modeling biomolecules, like GLYCAM), some are generally useful for any atomic problem, some are meant to be less accurate but fast, some are more accurate but slow, and so on. It’s a very empirical exercise to find which one will work best, and whether it needs to be tuned further. </span></p><p><em>How accurate are we being here with these force fields?</em><span> Surely more than the system, right?  Most force fields used in practice do include an awful lot of effects and model things quite well but are missing one thing: </span><strong>quantum effects</strong><span>. This includes stuff like electron tunneling, electron delocalization, and electron correlation; all of these are entirely ignored in the calculations. How does this practically change the results of the simulations? It depends </span><em>entirely</em><span> on the problem. For our problem of finding the fold of a protein, quantum effects (as far as I can tell) play a relatively minor role. The main cases in which it does make a difference is when the simulation involves breaking/forming of chemical bonds or transition metals, as those are the two primary cases (amongst a long tail of others) where quantum effects become extremely important. We’ll ignore these for now, and discuss it bit more in a future section. </span></p><p><span>A final note on understanding force fields: a helpful way to look at our proteins is as a bunch of atoms, all connected by </span><strong>springs</strong><span>. The exact composition, tension, push/pulling force, and flexibility of these springs can be thought of as the force field, which defines all of these values in extreme detail. This analogy eventually falls apart, as we’ll see in the next section, but as far as mental models for MD go, springs aren’t a bad one!</span></p><p><span>A lot of MD ‘basics’ tutorials are really fuzzy on what the force field is </span><strong>actually</strong><span> defining, instead relying on analogies or hand-wavy explanations. And, on the other hand, actual MD papers feel extremely complicated. Is there a middle ground here? I’m not sure, but I’ll try to get there. </span></p><p><span>What </span><em>actually</em><span> is a force field? Mathematically, it is a way to establish what the total </span><strong>potential energy</strong><span> of a system is. This is the value that we ultimately want to decrease as much as possible during our MD simulation. </span></p><p>Here is the potential energy equation used by many MD force fields. First, the equations for finding potential energy: </p><div data-component-name="Latex"><p><span>\(\begin{equation}
E_{\text{total}} = E(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N)
\end{equation}\)</span></p></div><div data-component-name="Latex"><p><span>\(\begin{equation}
E_{\text{total}} = \sum_{\text{bonds}} k_b (r - r_0)^2 + \sum_{\text{angles}} k_{\theta} (\theta - \theta_0)^2 + \sum_{\text{dihedrals}} V_n [1 + \cos(n\phi - \gamma)] + \sum_{i=1}^{N-1} \sum_{j=i+1}^{N} \left[ \frac{A_{ij}}{R_{ij}^{12}} - \frac{B_{ij}}{R_{ij}^{6}} + \frac{q_i q_j}{\epsilon R_{ij}} \right]
\end{equation}
\)</span></p></div><p>Or, represented in picture form:</p><p><span>But, remember, we actually want the derivative of each term here, not potential energy itself! This is also referred to as </span><strong>force</strong><span>. The following equation for it is a bit ugly and doesn’t make much intuitive sense, but it’s provided here for clarity&#39;s sake. </span><strong>We won’t be referring back to it!</strong><span> </span></p><div data-component-name="Latex"><p><span>\(\frac{dE_{\text{total}}}{dr} = \sum_{\text{bonds}} 2k_b (r - r_0) + \sum_{\text{angles}} 2k_{\theta} (\theta - \theta_0) \frac{d\theta}{dr} + \sum_{\text{dihedrals}} V_n n \sin(n\phi - \gamma) \frac{d\phi}{dr} + \sum_{i=1}^{N-1} \sum_{j=i+1}^{N} \left[ -\frac{12 A_{ij}}{R_{ij}^{13}} + \frac{6 B_{ij}}{R_{ij}^{7}} - \frac{q_i q_j}{\epsilon R_{ij}^2} \right]
\)</span></p></div><p>If you’ve committed to reading this section, relax! The potential energy equation is genuinely quite simple, we’ll get through this. Let’s break it apart into pieces and explain the concept behind each one. </p><p>First up is quantifying the energy created by the bonds between atoms. </p><div data-component-name="Latex"><p><span>\(\begin{equation}
\sum_{\text{bonds}} k_b (r - r_0)^2 
\end{equation}\)</span></p></div><p><span>We’re going to iterate through every chemical bond in our system. For each bond, we’ll take the difference between that bond length and 𝑟_0, which refers to the bond length at which energy is minimized. 𝑟_0​ is static across bonds of the same type, like double bonds or bonds between carbon-carbons. So, in essence, we’re checking how much a bond has deviated from its ‘ideal’, low-energy form. Then we square that, because we’re treating bonds as springs, and </span><a href="https://en.wikipedia.org/wiki/Hooke%27s_law" rel="">we do that for springs</a><span> (other reasons too obviously, but kind of unimportant). Then, we multiply it all by 𝑘_𝑏​, which is a static value of how much that particular bond ‘resists’ deformation. Higher 𝑘_𝑏 means stiffer, so more potential energy is created by deviations. As with 𝑟_0, 𝑘_𝑏 is static across all bonds of the same type.</span></p><p>Next is quantifying energy created by the angles between atoms. </p><div data-component-name="Latex"><p><span>\(\sum_{\text{angles}} k_{\theta} (\theta - \theta_0)^2 \)</span></p></div><p>Basically the exact same as the above equation, just slightly modified. We’re going to iterate through every angle formed by three connected atoms in our system. For each angle, we’ll take the difference between the current angle 𝜃 and 𝜃_0, which is the angle at which the energy is minimized. 𝜃_0​ is fixed for angles of the same type, like the typical 109.5 degrees for sp3-hybridized carbon atoms. Again, we’re checking how much an angle has deviated from its ‘ideal’, low-energy form. Then we square that deviation, just like we do with bonds, because angles also behave like springs in this model. After that, we multiply it by 𝑘_𝜃​, a static value that indicates how much the angle ‘resists’ deformation. A higher 𝑘𝜃​ means the angle is stiffer and thus more potential energy is created by deviations. Like 𝜃_0, 𝑘𝜃​ is the same for all angles of the same type.</p><p>First up, the energy created by the torsion of the bonds between atoms:</p><div data-component-name="Latex"><p><span>\(\sum_{\text{dihedrals}} V_n [1 + \cos(n\phi - \gamma)] \)</span></p></div><p>This one requires some definitions. Mainly what ‘dihedral’ angles are. Imagine you have four contiguous atoms connected in a chain: A-B-C-D. The dihedral angle is the angle between the plane formed by atoms A-B-C and the plane formed by atoms B-C-D. Knowing the dihedral angle allows you to understand how ‘twisted’ the configuration of atom bonds are. </p><p><span>We’re going to iterate every dihedral 𝜙 angle in our system. For each 𝜙</span><em>, </em><span>we’ll plug it into a cosine function that’ll tell us how much it deviates from it’s ideal, low-energy position. Within the cosine function, we’ll multiply it by n to account for 𝜙 wrapping around being identical in meaning (e.g, 360 𝜙 is equivalent to 720 𝜙) and subtract that from the ideal dihedral angle 𝛾. As with all other static values, 𝛾 is constant across known sets of 4 atoms. To ensure all values are positive, we add 1 to the cosine result. </span></p><p>Finally, we multiply by 𝑉𝑛​, a constant that represents the energy cost of deviating from the ideal angle. A higher 𝑉𝑛 means a higher energy penalty for deviations. This constant is again set for all dihedrals of the same type.</p><p>Finally, we have a term to define energy created by not-bond-related forces. </p><div data-component-name="Latex"><p><span>\(\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} \left[ \frac{A_{ij}}{R_{ij}^{12}} - \frac{B_{ij}}{R_{ij}^{6}} + \frac{q_i q_j}{\epsilon R_{ij}} \right]
\)</span></p></div><p>Because this isn’t bond-related, it’ll be calculated between every single pair of atoms in our system, hence the double sigma. On that note, not-bond-related is a bit general, isn’t it? This equation actually refers to two forces: Lennard-Jones potential (van der Waals forces) represent the first two, and electrostatic interactions represent the third one. Let’s go through them.</p><p><span>The first term is the repulsive energy between atoms 𝑖_</span><em>i</em><span> and 𝑗_</span><em>j </em><span>(van der Waals repulsion). Here, 𝑅_𝑖𝑗​ is the distance between the atoms, and 𝐴_𝑖𝑗​ is a constant that controls the strength of the repulsive force. The distance is raised to the 12th power, meaning the repulsive energy increases very rapidly as the atoms get closer together, preventing them from overlapping.</span></p><p><span>The second term is the attractive energy between atoms 𝑖_</span><em>i</em><span> and 𝑗_</span><em>j </em><span>(van der Waals attraction). Again, 𝑅_𝑖𝑗​ is the distance between the atoms, and 𝐵_𝑖𝑗​ is a constant that controls the strength of the attractive force. The distance is raised to the 6th power, so the attractive energy decreases as the atoms move apart, but not as sharply as the repulsive energy.</span></p><p><span>The third term is the electrostatic energy between atoms 𝑖_</span><em>i</em><span> and 𝑗_</span><em>j</em><span>. 𝑞_𝑖​ and 𝑞_𝑗 are the charges on the atoms, and 𝜖 is the dielectric constant, which reduces the effective strength of the electrostatic interaction based on the surrounding medium. Remember, in our case, this is water! The distance 𝑅𝑖​ again appears in the denominator, but with no exponents, so the electrostatic energy linearly decreases as the atoms move apart.</span></p><p>That wasn’t too bad! All we need are bond stretches, bond angles, bond dihedral angles, van der Waals forces, and electrostatic potentials to make up most of the force field math, easy! </p><p>Are we missing anything? </p><p><span>There’s a few more forces, specifically explicit solvent effects (if we are representing the solvent explicitly), improper torsions, and the exact impact of temperature/pressure on the system. These are relatively minor compared to the four we have discussed — but still important! If you’d like to get a much more in-depth read into what’s going on in the background of force fields, I’d recommend checking out the </span><a href="https://ambermd.org/doc12/Amber23.pdf#page=276" rel="">AMBER docs</a><span> and </span><a href="http://docs.openmm.org/7.0.0/userguide/theory.html" rel="">OpenMM docs</a><span>; the MD software package documentation ecosystem contains </span><strong>much</strong><span> more useful information compared to any individual paper. </span></p><p>Finally, we can move on from force fields.</p><p><span>There are two steps we do before simulation: </span><strong>energy minimization</strong><span> and </span><strong>equilibration</strong><span>. </span></p><p>After dropping our protein into a bowl of water and setting up the force field, we may find two problems when attempting simulation. One, there may be accidental overlaps of molecules in our system, leading to massive electrostatic force values that’ll break our system. Two, attempting to immediately perform simulation will result in extremely high force values, as pressure + temperature goes from zero to their set values (lets say 1 psi + 300 Kelvin) over the span of a picosecond. This, understandably, means the simulation loses a bit of realism. </p><p><span>For the overlap problem, we perform energy minimization. Here, we just iteratively calculate the potential energy of the system and use gradient descent algorithms to iterative adjust </span><strong>only</strong><span> the position of the molecules, not the velocities. The end goal is a configuration that is a local minimum in the potential energy landscape.  </span></p><p><span>For the temperature problem, we perform equilibration. During this phase, the same selected force field is used, but the pressure and temperature of the system start from a very low value and are slowly brought to the desired values over a longer time span (10-100’s of picoseconds). This allows the system to ‘settle’, both the water atoms and the protein. Exactly </span><em>how</em><span> the pressure and temperature are raised + maintained are sets of equations that are often referred to as, respectively, </span><strong>thermostats</strong><span> and </span><strong>barostats</strong><span>. </span></p><p>There are a few questions we may have over this.</p><p><em>Why do we refer to this separately?</em><span> </span><em>Isn’t this technically simulation since we’re using our chosen force field in our system?</em><span> Yes — with pressure + temperature variation if we’re being pedantic — but, because researchers don’t typically consider this as part of the MD trajectory when doing post-run analysis, it’s also not considered part of the actual simulation. We’ll defer explanation of simulation details to the next section. </span></p><p><em>How do we decide how fast to raise the temperature/pressure?</em><span> </span><a href="https://ambermd.org/tutorials/advanced/tutorial8/loop7.php" rel="">The whole subject is somewhat of an art, as the AMBER documentation notes, but basic things work fine:</a></p><blockquote><p>Equilibration protocols are still largely a matter of personal preference.  Some protocols call for very elaborate procedures involving gradually increasing temperature in a step-wise fashion while other more aggressive approach simply use a linear temperature gradient and heat the system up to the desired temperature. </p></blockquote><p><em>Neither temperature nor pressure was included in the force field equations, where do they come in? </em><span>Because, technically speaking, neither temperature nor pressure modifications change the potential energy of the system! They </span><em>do</em><span> indirectly modify it, but only via altering the positions + velocities of particles in the system. We won’t discuss the temperature/pressure math here too much, because it doesn’t feel super valuable. For more details, I once again recommend checking out the AMBER or OpenMM docs! </span></p><p><em>Now</em><span> we’re finally ready to start our simulation. </span></p><p>Let’s start at the zeroth second of our simulation, T=0. </p><p><span>At this moment, the positions and velocities of every atom in our system are known (either set to static values or derived as a result of the energy minimization step), and we’d like to know how it changed during some established time-step. Typically, MD simulations at the scale of protein folding operate at the femtosecond level, or 10^-15 seconds. Let’s operate with a similar mindset, our simulation with go from 0, 1, 2…and so on femtoseconds. Why can’t we go faster? </span><a href="https://web.stanford.edu/class/archive/cs/cs279/cs279.1222/lectures/lecture4_annot.pdf" rel="">Numerical instability, this lecture covers it a bit more</a><span>.</span></p><p>What happens at T=1?</p><p>First, let’s calculate the force of our system at T=0 needed to push the i’th particle in the direction of the lowest potential energy. This is the negative derivative of the potential energy of the system, as discussed above. We’ll do this for every particle in our system:</p><div data-component-name="Latex"><p><span>\(\begin{equation}
\vec{F}_i = -\nabla_{\vec{r}_i} E(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N)
\end{equation}
\)</span></p></div><p>From here, we need to find some way to tie this force into what the new velocity + position for each particle at T=1. We can defer to Newton’s second law for this! Because we know force and the mass of every particle (provided by the force field), we can use the law to find the acceleration for each particle.</p><div data-component-name="Latex"><p><span>\(\vec{F}_i = m_i  \vec{a}_i\)</span></p></div><div data-component-name="Latex"><p><span>\(m_i \vec{a}_i = -\nabla_{\vec{r}_i} E(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N)
\)</span></p></div><div data-component-name="Latex"><p><span>\(\vec{a}_i = -\frac{1}{m_i} \nabla_{\vec{r}_i} E(\mathbf{r}_1, \mathbf{r}_2, \ldots, \mathbf{r}_N)
\)</span></p></div><p>But we aren’t actually specifically interested in the acceleration of each particle, but rather the velocity and position changes over a time interval! For that, we need to rely on methods that can integrate over Newtons laws as, if you recall, acceleration is the derivative of velocity and second derivative of position. </p><p><span>Because my differential equations background is pretty horrible, we’ll do something </span><strong>very</strong><span> basic and apply Euler’s method here to grab out the velocity + position update. </span></p><p>We can approximate a velocity update with this:</p><div data-component-name="Latex"><p><span>\(\vec{v}_i(t + \Delta t) \approx \vec{v}_i(t) + \vec{a}_i(t) \Delta t
\)</span></p></div><p>Reasonably intuitive, we just need to substitute the acceleration value here with the re-arranged Newton’s second law.</p><p>The position update is even easier.</p><div data-component-name="Latex"><p><span>\(\vec{r}_i(t + \Delta t) = \vec{r}_i(t) + \vec{v}_i(t) \Delta t
\)</span></p></div><p><span>Keep in mind though, I’m using Euler’s method here is because it’s simple! In practice, more sophisticated methods are used, such as </span><a href="https://en.wikipedia.org/wiki/Verlet_integration" rel="">Verlet Integration</a><span> or </span><a href="https://en.wikipedia.org/wiki/Langevin_equation" rel="">Langevin Integration</a><span>, because Euler’s method is extremely inaccurate and slow for any reasonably sized time-step. </span></p><p><span>And that’s it! We now know the updated velocity and position at the next time step, we just need to do this a few million times and our simulation is complete! Over very many hours of compute time, we’ll end up a massive stack of data known as a </span><strong>trajectory</strong><span>, which will encode the position and velocities for every particle in our system from timestep-to-timestep, showing the molecular dance that occurs. The video below is an accurate representation of how our folded protein&#39;s trajectory may look (start at 0:15). Lots of vibrating around, slowly poking its way towards a stable structure, and then fully stabilizing — all over 6 million femtoseconds (6 microseconds). </span></p><p>Gorgeous! With that, we’ve completed a very basic simulation workflow. There’s a lot of post-simulation analysis to be done, but as that is usually custom to the problem one is solving, we’ll leave example analyses to the case study section below. </p><p>Here is a collection of things that didn’t come up in the above section but may still pop up in MD papers + are cool. </p><p><span>Because MD time-steps are limited to femtoseconds — due to increasing instability and inaccuracy upon integrating Newton’s second law if pushed beyond that — a huge tail of biological phenomena are extremely difficult to model reasonably well. Our initial problem, protein folding, would be a hard sell for us to </span><em>actually</em><span> do — many protein folding events typically take milliseconds to </span><strong>seconds</strong><span>. Ligand binding and unbinding events often have microsecond to millisecond lifetimes. Allosteric transitions, ion channel gating, and major conformational changes associated with protein function can also take microseconds or longer. </span></p><p>This means that in a typical MD simulation, we&#39;re only directly observing a tiny fraction of the molecule’s full dynamic behavior. We might see fast, small-scale motions like sidechain rotations, loop fluctuations, and transient hydrogen bonds, but we likely won&#39;t capture large-scale slow motions or rare conformations. And, unfortunately, biology is filled with large, slow things and rare events. </p><p>There are ways around this!</p><p><span>One could simply scale things up. Anton, a supercomputer built by </span><a href="https://www.deshawresearch.com/" rel="">DESRES</a><span>, was custom-built from the ground up with optimized hardware to tackle exactly this through pure computational power. The extreme modeling speeds they were able to achieve yielded a humorous paper title that I could only describe as a flex: </span><strong><a href="https://dl.acm.org/doi/abs/10.1145/3458817.3487397" rel="">Anton 3: twenty microseconds of molecular dynamics simulation before lunch</a></strong><a href="https://dl.acm.org/doi/abs/10.1145/3458817.3487397" rel="">.</a><span> Of course, while academics are able to access fragments of the full Anton system with permission, using it regularly isn’t feasible for any non-employee of DESRES. </span></p><p><span>But there’s a subtle point here. The issue with small timescales has nothing to do with </span><strong>time</strong><span> specifically, but rather that molecules are traversing an energy landscape, may end up roaming around in local minima of potential energy states, and only eventually find their way out after lengthy stretches of microseconds. Stretching out the time of the simulation is one way to solve the problem, as Anton was built to do. But another option is to </span><strong>change the simulation itself</strong><span>, such that it’s able to quickly traverse this energy landscape without getting stuck — allowing us to get that much more information per femtosecond of simulation. </span></p><p><span>An increasingly important tactic used here is </span><strong>enhanced sampling</strong><span>, which is far more in reach of the average scientist. This is an extremely broad category of methods that seek to massively increase the diversity of molecular dynamics trajectories by changing the ‘rules’ of the simulation away from physical reality. There are </span><strong>tons</strong><span> of examples. Here are a few: adding </span><a href="https://en.wikipedia.org/wiki/Metadynamics" rel="">potential energy costs to visiting previous states</a><span> in order to encourage faster stabilization (metadynamics), </span><a href="https://pubmed.ncbi.nlm.nih.gov/29744830/" rel="">running multiple simulations at different temperatures in parallel and allow swapping of temperatures to encourage conformational space exploration</a><span> (replica exchange molecular dynamics), and </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3438784/" rel="">altering the potential energy equations to reduce energy barriers between conformational states</a><span> (accelerated molecular dynamics). An entire blog post could be written on this subject alone, the diversity of ideas here is immense. </span><a href="https://arxiv.org/pdf/2202.04164" rel="">If interested, a very useful — though clearly meant for experts — review paper on the subject can be found here.</a></p><p>As we mentioned earlier, most classical force fields used in MD simulations don&#39;t account for quantum mechanical effects like electron delocalization or tunneling. For many biological systems, like the protein folding example we walked through, this is a reasonable approximation. However, there are certain cases where quantum effects become critical to model accurately, such as when simulating chemical reactions or systems involving transition metals. </p><p><span>We’re being vague here. What do we actually </span><strong>mean </strong><span>when we say ‘quantum effects’? What we really mean is we go from pretending electrons don’t exist (beyond the charge they provide) to modeling them directly. Here are a few of the major forces in a quantum system:</span></p><ol><li><p><strong>Electronic kinetic energy. </strong><span>Mostly self-explanatory, the force that each electron in the system has. </span></p></li><li><p><strong>Electron-nucleus electrostatic attractions</strong><span>. This is the attractive electrostatic interaction between electrons and nucleuses.</span></p></li><li><p><strong>Electron-electron electrostatic interactions.</strong><span> This is the repulsive electrostatic interaction between electrons.</span></p></li><li><p><strong>Exchange energy.</strong><span> This is a quantum mechanical effect that arises from the Pauli exclusion principle, which states that no two electrons can have the same quantum state. The exchange energy lowers the energy of the system by keeping electrons with the same spin apart.</span></p></li></ol><p><span>There’s a few more as well. Unlike how we treated force fields and simulation, I am going to stay away far away from the math on this one — I have talked about quantum stuff enough times with physicist friends to understand that attempting short-form explanation of it is rarely worth it. What I </span><em>will</em><span> touch on is some terminology. </span></p><p><span>Attempting to model quantum effects forces us to deviate from Newtons second law when deriving acceleration updates (and thus velocity and position updates as well). Remember, electrons are not a particle, they are a wave, so Newton’s second law doesn’t apply in this case! Because of this, we must rely on the</span><a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation" rel=""> </a><strong><a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation" rel="">Schrödinger equation</a></strong><span>, which could be seen as the quantum counterpart of Newton’s second law to allow incorporation of waves. However, the Schrödinger equation is computationally intractable to solve, so, </span><a href="https://pubs.acs.org/doi/10.1021/acs.accounts.1c00514" rel="">in practice</a><span>, simplifications like the </span><a href="https://en.wikipedia.org/wiki/Born%E2%80%93Oppenheimer_approximation" rel="">Born–Oppenheimer (BO) approximation</a><span> are used to replace it. We could also directly relax the complexity of the quantum ‘rules’, as </span><a href="https://en.wikipedia.org/wiki/Density_functional_theory" rel="">Density Functional Theory (DFT)</a><span>, or </span><a href="https://en.wikipedia.org/wiki/Hartree%E2%80%93Fock_method" rel="">Hartree–Fock methods</a><span> do. These are often combined, e.g, BO is typically used in DFT and Hartee-Fock. </span></p><p>But even with the approximations, modeling quantum mechanics is still incredibly hard.</p><p><span>One approach to making quantum effects tractable is to use a method called </span><strong>quantum mechanics/molecular mechanics (QM/MM)</strong><span>. In QM/MM, a small region of the system that requires quantum mechanical treatment (like the active site of an enzyme) is modeled using quantum mechanics, while the rest of the system is treated classically with the usual force field we discussed earlier. This does add some extra headaches in </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9105939/" rel="">working out how two segments of a molecule having two different laws of physics will interact with one another</a><span>, but it does seem to work!</span></p><p><span>We could also do something a little more clever — there are some quantum effects that could be reframed in a classical mechanics manner, allowing us to simply tack it alongside the usual potential energy equations. Electron charge distribution — the ability for electrons to be shared between atoms or to be off-center from its parent atom — is one of those things. This is a phenomenon that is only noticeable with a quantum lens but </span><em>could</em><span> be modeled directly using Newtonian laws by treating electrons as a particle. </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6520134/" rel="">This was the hope behind polarizable force fields</a><span>, which ignore all quantum effects other than electron charge distribution, </span><a href="https://pubs.acs.org/doi/10.1021/jp910674d" rel="">such as the AMOEBA force field.</a><span> This sort of hack </span><a href="https://cs.stanford.edu/people/ihaque/papers/amoeba.pdf" rel="">does seem to yield better performance compared to fixed charge models and compares well to typical pure QM modeling methods</a><span>, but it does have an added computational cost, as you are dramatically upping the number of particles in the system. </span></p><p>For what it’s worth, this is my favorite section of this post. </p><p>We could imagine our protein being able to fold itself into a vast landscape of different conformations, each requiring different amounts of energy. Amongst these conformations are a variety that are able to nicely pack all of the hydrophobic residues into the core of the protein, fold itself such that residues are nicely tied up with hydrogen bonds, minimize the steric clashes/stretches/torques of its bonds, and so on — these states are energetically favorable. </p><p>But is energetically favorable all a protein ‘cares’ about? Potential energy, as determined by the interactions within the protein (like bond stretching, van der Waals interactions, etc.), does indeed favor the folded state. If we were only considering potential energy, the protein would always fold into the state with the lowest potential energy. But it doesn’t.</p><p><span>What also enters into the picture is </span><strong>entropy</strong><span>. It’s much easier to think of entropy in a statistical manner rather than via its actual definition. Conformations that achieve these nice ‘neat’ properties with low potential energies are relatively rare compared to the broad space of all possible conformations. Let&#39;s say a protein stumbles into a low-potential energy state. There are very few conformations in this state that maintain this low potential energy - maybe a slight rotation of an amino acid residue or a minor shift in a bond angle would kick it right out of this favorable zone. On the other hand, if a protein is in a high-potential energy state, it can largely do whatever it ‘wants’ and it will continue to retain high potential energy. </span><strong>The former case has low entropy, the latter has high entropy.</strong><span> </span></p><p><strong>Systems tend to favor states that are energetically favorable (low potential energy) and states that are statistically favorable (high entropy). </strong><span>This is a really beautiful idea, our protein is caught being two opposing forces, and must balance them. And an important one too, </span><strong>because it determines how statistically likely a possible end-state is</strong><span>. This is extremely important for understanding the interactions between drugs and proteins!</span></p><p>The interplay between these two values is often referred to as the ‘free energy’ of the system and is encapsulated by the Gibbs free energy equation:</p><div data-component-name="Latex"><p><span>\(\begin{equation}
G =  H - T S
\end{equation}\)</span></p></div><ul><li><p>𝐺 is the free energy, which we’d like to keep as low as possible. </p></li><li><p>𝐻 is the enthalpy, which is largely equivalent to potential energy. </p></li><li><p>𝑆 is the entropy.</p></li><li><p>𝑇 is the temperature in Kelvin. As the temperature rises, the entropy of the system dominates in importance, since atoms that move faster also desire to retain that much more entropy. </p></li></ul><p><strong>Interesting and somewhat unrelated note</strong><span>, free energy itself could be viewed as ‘weighted’ state probability. The Boltzmann distribution provides a way to connect the free energy of a state to the probability of the system being in that state. It states that the probability of a system being in a particular state (𝑃𝑖) is proportional to the exponential of the negative free energy of that state (𝐺𝑖) divided by the product of the Boltzmann constant (𝑘) and the temperature (𝑇), which is in turn divided by the sum of all possible states, which serves as a normalization factor to ensure that the probabilities of all states sum to 1.</span></p><div data-component-name="Latex"><p><span>\(P_i = \frac{e^{-E_i/kT}}{\sum_{j} e^{-E_j/kT}}
\)</span></p></div><p><span>But, for complex systems like proteins, naively applying the Gibbs energy equation or the Boltzmann distribution (as given) is </span><strong>computationally intractable</strong><span>. There are just too many possible conformations to enumerate and evaluate.</span></p><p>How do we get around this? </p><p><span>Well, let’s ground the problem. Let’s say we’re interested we’re developing a drug and want to assess how stable the binding of the drug is to a protein of interest — or, in other words, the free energy of the complex. The potential energy here is easy to evaluate, but the entropy is, again, challenging. One way we could simplify the problem is realizing that we perhaps don’t </span><em>need</em><span> to care about the absolute free energy, </span><strong>but rather the difference in free energy between the ligand bound to the protein, and the ligand unbound to the protein.</strong><span> To note, we absolutely do lose some information by phrasing the problem this way! If we focus on deriving the </span><em>change</em><span> in free energy, we cannot understand the global ranking of this ligand-protein complex in reference to </span><em>every</em><span> ligand-protein pair to ever exist. </span><strong>But we don’t care about that!</strong><span> Instead, what we&#39;re usually interested in is the difference in free energy between two states, such as the folded and unfolded states of a protein, or the bound and unbound states of a drug-protein complex.</span></p><p><span>This makes things </span><em>so</em><span> much easier. The realization that differences in free energy is what we actually desire led to the development of </span><strong>alchemical free energy calculations </strong><span>— called ‘alchemical’ because it involves unphysical changes to the simulation. In this class of methods, we needn’t enumerate through every possible state, but rather just the states ‘between’ two given states. One common method for doing this is </span><strong>free energy peturbations</strong><span>.</span></p><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4420631/" rel="">Here’s how its done.</a></p><p>Let’s consider state A as the protein by itself and state B as the protein with the docked ligand. In these methods, we need to define a pathway to gradually transform the system from state A (protein without ligand) to state B (protein with ligand bound). This is done by introducing a coupling parameter, often denoted as λ, which takes values from 0 to 1. When λ=0, the system is in state A, and when λ=1, the system is in state B. At intermediate values of λ, the system is in a hybrid state between A and B. In practice, this transformation is often done by gradually turning off the interactions between the ligand and the protein (for the binding process) or gradually turning them on (for the unbinding process). At each intermediate λ value, we run molecular dynamics simulations to sample the configurations of the system at that particular point along the transformation path.</p><p><span>For each configuration sampled at a given λ value, we calculate the potential energy of the system in both the current state and the neighboring states (i.e., at λ and λ+Δλ). The free energy difference between these states can then be calculated using the </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4420631/" rel="">Zwanzig relationship</a><span>.</span></p><div data-component-name="Latex"><p><span>\(\Delta G = -kT \ln \langle \exp(-\Delta U / kT) \rangle
\)</span></p></div><p>Here, ΔG is the free energy difference between adjacent states, kT is the Boltzmann constant, T is the temperature, -ΔU is delta of potential energy between the states, and the angle brackets denote an ensemble average over the configurations sampled at each λ value. By summing these free energy differences over the entire transformation path, we can obtain the total free energy difference between states A and B.</p><p>Importantly, these alchemical methods implicitly capture the entropic contributions to the free energy difference through the sampling of configurations at each intermediate state. The Boltzmann factor exp(-ΔU/kT) weights the contribution of each configuration based on its potential energy difference, effectively accounting for the relative probabilities of different configurations, which is related to entropy.</p><p><a href="https://www.researchgate.net/publication/280557809_Free-energy_calculations_in_structure-based_drug_design" rel="">There are </a><strong><a href="https://www.researchgate.net/publication/280557809_Free-energy_calculations_in_structure-based_drug_design" rel="">tons</a></strong><a href="https://www.researchgate.net/publication/280557809_Free-energy_calculations_in_structure-based_drug_design" rel=""> of alchemical free energy methods.</a><span> I’ve missed an enormous amount of nuance in this section, but it should equip you with enough terminology to dig into the details. How do you split up states? Where do methods like this go wrong? How do you ensure entropic contributions by the state split-up are actually captured? Questions for the reader to ponder…</span></p><p><span>There is no better way to realize you understand a subject than to read a full-fledged paper about it and to actually </span><em>get it</em><span>, when previously it would’ve all been gibberish. Let’s go over one. </span></p><p><span>Released by </span><a href="https://relaytx.com/" rel="">Relay Therapeutics</a><span> just a few months ago, this paper is one of the few cases I’ve seen where it is described, in extreme detail, how MD helped lead to the development of a drug. </span></p><p>Here’s some context. Existing FGFR inhibitors are pan-FGFR, hitting all isoforms (FGFR1-4) and causing dose-limiting toxicities as a result. Hyperphosphatemia from FGFR1 inhibition and diarrhea from FGFR4 inhibition often lead to dose reductions or treatment interruptions, capping the efficacy of these drugs. A selective FGFR2 inhibitor could potentially avoid these issues and allow more effective treatment. However, the high structural similarity between FGFR isoforms has thwarted conventional structure-based drug design approaches to find selectivity handles.</p><p>To tackle this, the authors turned to long-timescale molecular dynamics (MD) simulations. They ran simulations up to 25 μs to thoroughly sample the conformational landscapes of FGFR1 and FGFR2, hunting for differences in protein dynamics that could be exploited for selective targeting.</p><p>The starting structures for the simulations were based on X-ray crystal structures of the FGFR1 and FGFR2 domains (PDB IDs 4RWI and 1GJO, respectively). The domains were placed in a cubic simulation box with periodic boundary conditions, again with water and ions as a solvent. The protein was modeled using the Amber99SB*-ILDN force field, and the small molecule ligands were modeled using the general Amber force field. The equilibration process was more typical, using an off the shelf thermostat (Nosé–Hoover thermostat) and barostat (Martyna-Tobias-Klein barostat) to modify, respectively, the temperature and pressure. </p><p>Fascinatingly, the MD simulations revealed a key difference in the behavior of a region called the P-loop between FGFR1 and FGFR2: </p><p>In the simulations of FGFR1, the P-loop quickly contracted from the extended conformation and became disordered. In the simulations of FGFR2, however, a somewhat extended conformation persisted, and the P-loop was far less flexible than that of FGFR1. This result suggested that the P-loop might be a suitable region for selective targeting of FGFR2.</p><p>The visualizations of these trajectories are quite beautiful in how much information they convey. Here is the flexible FGFR1 P-loop:</p><p>And the less flexible FGFR2 P-loop:</p><p>They also use distance deviation plots between the two P-loops to show how different they are:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png" width="676" height="309" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:309,&#34;width&#34;:676,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:95525,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ec718a5-2861-4278-938e-c2f3ebfd3bb9_676x309.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>There’s something quite special here, MD revealed something — a conformational difference — that would’ve required much more expensive methods (like cryo-EM) to find! The authors continued down this rabbit-hole, hypothesizing how targeting the P-loop affected FGFR1 and FGFR2:</p><p><span>Our initial design efforts aimed to covalently engage FGFR2 Cys491, a residue that lies at the tip of the extended P-loop and is also targeted by the covalent pan-FGFRi futibatinib (</span><a href="https://www.pnas.org/doi/full/10.1073/pnas.2317756121#core-r10" rel="">10</a><span>, </span><a href="https://www.pnas.org/doi/full/10.1073/pnas.2317756121#core-r20" rel="">20</a><span>, </span><a href="https://www.pnas.org/doi/full/10.1073/pnas.2317756121#core-r21" rel="">21</a><span>). Simulations of FGFR1 binding one of our early selective compounds suggested that its selectivity arose because the compound stabilized the FGFR1 P-loop in an extended conformation with such a low degree of flexibility that covalent engagement of Cys488 (homologous to FGFR2 Cys491) was discouraged. </span></p><p>In other words, targeting the P-loop may be a way to inhibit FGFR2 (bind to) but not FGFR1! </p><p>The paper goes on to use MD for docking purposes between potential molecules and binding to the P-loops of FGFR1 and FGFR2. This is a cool idea! But it’s also a bit hard to tell how useful MD was here, eyeballing the results don’t show huge differences in how ligands interacted with the P-loop, and they don’t attempt to calculate free energies anywhere. It feels likely that the key contributions of MD here were in identifying the P-loop as a selectivity handle and in providing post-hoc rationalization of the binding modes, rather than in directly guiding the docking itself. </p><p>Still though, a drug was made!</p><p>Using this approach as part of an iterative process of optimization, our efforts culminated in the identification of lirafugratinib (RLY-4008), a highly selective, orally available small-molecule FGFR2 inhibitor to enter clinical development.</p><p>Much more of an exploratory paper than an absolutely clinically useful one, but a great demonstration of the diversity of questions that MD can help answer. </p><p>Here’s the context: influenza pandemics occur when an (typically) avian influenza strain acquires mutations that allow it to infect and transmit between humans. A key step in this process is when the viral surface protein Hemagglutinin (or HA) switches its binding preference from avian to human sialic acid (SA) receptors via a mutation. However, predicting which specific mutations will enable this switch is challenging! One reason for this difficulty is that human SA receptors are quite flexible and can adopt many different conformations when bound to HA. This diversity makes it hard to determine from crystal structures alone which HA mutations will facilitate human receptor binding. An excellent use case for MD!</p><p>The simulations focused on a combination of SA’s and HA’s. </p><p>For SA’s (the cell surface receptor), 3-SLN, representing the avian sialic acid receptor, 6-SLN and 506-SLN, representing the human sialic acid receptors. </p><p><span>For HA’s (the viral protein), DK76, IN05, and SH13 were used. Along these, there were also several mutated forms of these HA’s used:DK76</span><sup>Q226L,G228S,A227S</sup><span> , DK76</span><sup>Q226L,G228S,P186N</sup><span>, DK76</span><sup>Q226L,G228S,P186N,A227S</sup><span>, DK76</span><sup>Q226L,G228S</sup><span>, DK76</span><sup>E190D,G225D</sup><span> IN05</span><sup>Q226L,G228S,S227A</sup><span>, IN05</span><sup>Q226L,G228S</sup><span>. Should these mutations mean anything to you? Some of these are just mutations that are known to ‘switch’ affinity of the viral protein between the human SA and the avian SA, and others are hypothetical gain-of-function mutations. </span></p><p>These were placed in cubic water boxes with periodic boundary conditions. The proteins were modeled using the Amber99SB-ILDN force field, while the GLYCAM06 force field was used for the sialic acid sugars — an example of multiple specialized force fields working alongside one-another. For each HA and SA combination, they ran simulations for up to 25 μs. The resulting set of trajectories (as in, every frame) were clustered together, yielding 7 clusters of identifiable conformations amongst all SA-HA pair frames (alongside an 8th ‘other’ category). </p><p><span>The rest of the analysis of this is…confusing. Probably not to someone who studies MD! But given what we’ve learned so far, hard to grasp. They come up with a way to measure </span><a href="https://pubs.acs.org/doi/suppl/10.1021/acs.jctc.1c01044/suppl_file/ct1c01044_si_001.pdf" rel="">binding affinity without relying on free energy difference</a><span>s — likely assisted by how long-running the simulation is, as they are able to directly observe </span><strong>many</strong><span> binding-unbinding events — and assert that their MD-derived numbers match up quite well with experimentally determined binding affinity values. They confirm this by suggesting a mutation to an HA to increase affinity and seeing that both MD and experimental validation both show an increase in binding affinity. </span></p><p><span>But I took the liberty of plotting their Kd values (binding affinity) derived from MD (KD_MD) and experimentally-determined (KD_MST) across multiple HA-SA pairs and…I’m not really seeing a strong correlation? There are a few cases in which it clearly works, but overall it feels quite random, outside of one outlier case. </span><a href="https://gist.github.com/Abhishaike/a04c81f5400329dde291980c6fbc9592" rel="">I’ve added a Github gist here of the (very basic) plotting work</a><span>. It feels like the one-off de-novo mutation showing increased binding affinity in both MD and experimental techniques was either spurious or something that is real, but only works in some situations. I could very much be wrong about this though, please let me know if there’s something I’m missing!</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin" width="1456" height="798" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/f8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:798,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;Output image&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null}" alt="Output image" title="Output image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8fcdb45-57c7-4783-8e44-550a444c54d1_2008x1101.bin 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>Overall, very interesting hypothesis and cool how large conformational diversity was observed in both experimental determined structures AND in MD! But the binding affinity part of the paper was a bit off, unsure on whether I’m misunderstanding something or the results in this particular area are quite weak. </p><p><span>We covered the key components of an MD workflow - defining the system, choosing a force field, energy minimization, running the production simulations. We also looked at a few miscellaneous items, like quantum effects, enhanced sampling, and free energy calculations. Finally, we looked at a few papers, and saw cases of the jargon we’ve been learning this whole time being used to build </span><strong>useful</strong><span> atomic simulations. </span></p><p>But seven-thousand words later, I&#39;ve only scratched the surface here! There are endless rabbit holes to dive down - coarse-grained simulations, more information on enhanced sampling, modifying the pH in simulations, and so on. But hopefully this post has given you a solid foundation to build on.</p><p>MD isn’t perfect and will likely remain imperfect for years to come. The timescales are still severely limited compared to biological reality. Quantum effects are largely ignored, even using approximate QM methods is hard. Force fields are approximations and there&#39;s no clear &#34;best&#34; choice. Setting up and running simulations requires a ton of expert knowledge. </p><p>But the future is interesting! </p><p><span>Things like </span><a href="https://arxiv.org/abs/2308.11155" rel="">neural force fields</a><span>, </span><a href="https://github.com/bjing2016/alphaflow" rel="">trajectory sampling using Alphafold2-esque models</a><span>, and </span><a href="https://pubs.acs.org/doi/10.1021/acs.jpclett.3c01723" rel="">trajectory interpolation using neural nets</a><span> are all on the horizon and could lead to a revolution in the way we work with MD. Hopefully I get a chance to write about some of these directions someday! For now, it’s early days with these methods, and nothing has popped out as immediately, groundbreakingly useful. But, as with everything in biology-ML, things could shift overnight. </span></p></div></div></div></article></div></div></div><div><div id="discussion"><div><h4>Discussion about this post</h4></div></div></div></div>
  </body>
</html>
