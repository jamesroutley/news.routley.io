<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite">Original</a>
    <h1>AGI Existential Risk Persists Despite Controls That Buy Time</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><em>This post is early this week to meet a deadline for an essay contest. </em></p><p>Given advances in artificial intelligence over the past few years, it seems likely that artificial general intelligence (AGI) could emerge by 2070. And the rapid advance of AI capabilities raises urgent questions about current and future risks of this technology.</p><p><span>Put simply, whenever capabilities outpace controls, risk increases. Innovation throughout human history has featured this dynamic, and we are still here. But with AI, a different approach will become necessary as capabilities advance toward human-level AGI: </span><strong>controls will need to catch up to, keep pace with, and eventually outpace capabilities</strong><span>.</span></p><p><span>A few rudimentary graphs can illustrate why. If capabilities improve faster than controls, risk also increases, likely at a greater-than-linear rate</span></p><div><p><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-1-125167098" rel="">1</a></p></div><p><span>. And in a so-called fast-takeoff AI scenario, where AGI exceeds human intelligence and begins improving itself at rapid speed, the curves are likely to diverge sharply, leaving a large and ever-widening gap between sophistication of controls and capabilities. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png" width="485" height="420" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:420,&#34;width&#34;:485,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:60702,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14dcd90d-c11d-4818-b4f7-041874d4223c_485x420.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a></figure></div><p>Those curves are likely not compatible with thriving human societies. A growing gap between controls and capabilities would represent increasing potential for existential disasters or catastrophes over time.</p><p>That is why controls must catch up to, then keep pace with, and eventually exceed capabilities. Yes, that will mean periodic slowdowns and eventually a stopping point. It will not mean all risk is eliminated; instead, it will mean that controls are commensurate with the level of risk and sufficient to reduce risk to an acceptable level. </p><p>If capabilities and controls evolve at a similar pace, that might look something like this (a more traditional scenario on the left, and a better scenario on the right): </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png" width="1002" height="453" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/c00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:453,&#34;width&#34;:1002,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:114131,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00c039c-5a73-4ad5-b145-c93820a911c0_1002x453.png 1456w" sizes="100vw"/></picture></div></a></figure></div><p>The scenario on the right buys time, manages risk, and reduces the likelihood of any sudden and un-manageable takeoff in capabilities. It opens the door for humans to benefit from AI capabilities while keeping risks reasonably in check.  </p><p>But such an approach would likely work only as long as humans (and the controls they design) are smarter than AI. Almost by definition, AGI that reaches or exceeds human-level intelligence would learn to outmaneuver human controls and increase its own capabilities faster than human controls could catch up (bringing us back to that first graph with the exponential capabilities curve far outpacing controls). </p><p><span>Therefore, if we approach (note: I did not say </span><em>reach</em><span>, because </span><em>reaching </em><span>that point would be too late) a point where risk can no longer be reduced to an acceptable level, the only adequate control will be to stop. </span></p><p><span>One possible way to think about this would be to visualize human-level AGI as a threshold and then aim to leave a margin of safety between controls and capabilities and that perhaps-irreversible threshold. The margin should be ample to allow for uncertainty. Visually, it might look something like this</span></p><div><p><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-2-125167098" rel="">2</a></p></div><p><span>: </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png" width="909" height="376" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/b72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:376,&#34;width&#34;:909,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:78225,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb72cd7c0-5ccc-4c7c-bd50-8b27a0c386d6_909x376.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>Admittedly, this seems like a tall order, and threading the needle will be tricky. Yet, the world has (imperfectly but acceptably) managed the potentially existential risk of nuclear weapons for about 80 years via stringent risk controls and international regulatory coordination through the International Atomic Energy Agency (IAEA). With AI, another factor that’s not present with nuclear weapons is the need to align AI systems with human interests and priorities.</p><p>We’ll cover each of these three areas—operational risk controls, alignment, and regulation—in turn, and then consider how the combination of these balancing feedback loops may buy time and ultimately set boundaries to prevent humanity from crossing a possibly-irreversible threshold beyond which AGI could spiral out of our control.</p><p><strong>Operational Risk Controls - A Brief Overview</strong><span> </span></p><p><span>As I wrote in my essay </span><a href="https://riskmusings.substack.com/p/possible-paths-for-ai-regulation" rel="">Possible Paths for AI Regulation</a></p><div><p><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-3-125167098" rel="">3</a></p></div><p><span>, companies and organizations developing or using AI need to implement controls commensurate with the risk posed by the technology. That doesn’t mean—and shouldn’t mean— implementing layers of process or paperwork without reducing risk. Instead, </span><strong>some characteristics of effective controls are</strong><span>: </span></p><ul><li><p>They are supported rather than undermined at the highest levels of the organization and are well-communicated throughout the organization.</p></li><li><p>They are well-placed at high-leverage points within processes and systems.</p></li><li><p>They target the root causes of risk and prioritize prevention when possible.</p></li><li><p>They cost less than the risks they protect against and avoid unnecessary overhead. </p></li><li><p>They allow sufficient flexibility for buy-in and strategic implementation across different departments’ workflows while minimizing exceptions and workarounds.</p></li><li><p>They are forward-looking, with consideration of how processes and systems are evolving and how the controls might apply to their anticipated future states.</p></li><li><p>Their second-order effects are manageable and do not undermine the intent of the controls (e.g., by driving activity “underground” without actually reducing risk) </p></li></ul><p>More specifically, some key principles for operational risk controls include: </p><ul><li><p><strong>Defense in depth:</strong><span> Failure of one control should never lead to catastrophic failure of the entire control system. Single points of failure are out; redundancy and diversity of controls are in. Redundancy of controls means multiple controls are used to address the same risk. Diversity of controls means different </span><em>types </em><span>of controls are used to address the same risk. The diversity of controls can be broad, and the redundancy of controls can be deep, depending on the risk posed by a system. </span></p><p><span>However, this </span><em>doesn’t</em><span> mean controls should be implemented haphazardly, stacked all together in one place, or at every point in a process or system. Instead, </span><em><strong>controls should be implemented at high-leverage points in processes and systems</strong></em><span>. For example, when dealing with high-volume data, if a set of chokepoints can be identified through which all or most data must eventually flow, those are optimal points for control application. </span></p></li><li><p><strong>Prioritizing controls over expediency:</strong><span> As a general principle, activating a control should be easier than overriding it. </span></p></li><li><p><strong>Prioritizing preventive controls:</strong><span> Because AI is faster than humans, preventive controls take on more importance than detective or corrective controls. Key examples of preventive controls include alignment approaches (discussed later), separation of duties, and change management. </span></p><ul><li><p><strong>Separation of duties:</strong><span> No single person should be able to take drastic action to increase or alter AI’s capabilities. The more drastic the escalation of capabilities, the more stakeholders and sign-offs should be required. Lively debate among stakeholders should be encouraged, not squashed, since the safety increase from additional sign-offs could be compromised if stakeholders feel they can’t speak up without negative consequences. </span></p><p>Similarly, no single person should be able to override an AI rule or a control. If the rule or control is critical enough, high-level approval should be required to override it. In the most critical situations, no single person should know about all the controls or have the power to override all the controls they know about, so even if an AI convinces humans to override controls, not all controls may be overridden. </p></li><li><p><strong>Change management:</strong><span> No matter how good or trusted a developer is, they can always benefit from code review and a formal change management process. No developer should be able to unilaterally test their own code with no additional review or push their own code into production outside of a formal change management process. </span></p></li></ul></li><li><p><strong>Detective and corrective controls:</strong><span> True AGI would likely be able to outpace or undermine detective and corrective controls. But in the years </span><em>before </em><span>AGI’s creation, there is still time to identify when things go wrong and then correct the deficiencies that allowed incidents to occur. </span></p><ul><li><p><span>For example, </span><strong>automated detective controls</strong><span> might identify a massive and unexpected volume of requests flowing from an AI system and shut it down until humans can review the traffic and determine what happened. </span></p></li><li><p><strong>Manual kill switches</strong><span> can serve as corrective controls activated by humans in response to AI malfunction or misaligned action, although designers of kill switches need to balance speed (is there time to hit the switch and stop AI?) with assessment of second-order effects (will doing this cause an even bigger problem?). </span></p></li><li><p><strong>Immutable logging</strong><span> can help detect anomalies and malfunctions, such as when patterns of use or operation shift in unexpected ways (e.g., an AI logs into a system at an unusual time or tries to access a new resource, or a developer deletes records to cover their tracks). </span></p></li><li><p><strong>Near-miss reporting and remediation</strong><span> can help AI developers and users address root causes and avoid future errors before risk fully manifests. Judiciously sharing risks, near-misses, and mitigation strategies across companies and organizations, possibly through an independent information-sharing organization, without naming names, placing blame, or identifying specific firms’ weaknesses, can strengthen the resilience of the entire ecosystem. </span></p></li></ul></li></ul><p><span>Overall, controls are not keeping pace with AI capabilities right now. It’s not a catastrophe right now, either, because current AI systems don’t pose existential risk. But </span><em><strong>as AI capabilities advance further and existential risks move into the realm of the plausible, controls will need to catch up, start keeping pace with, and ultimately outpace capabilities.</strong></em><span> </span></p><p><strong>AI Alignment</strong><span> </span><strong>- An Important Piece of the Puzzle</strong></p><p><span>One aspect of AI risk control that bears special mention is alignment of AI systems with human interests and goals. Think of Isaac Asimov’s canonical </span><a href="https://en.wikipedia.org/wiki/Three_Laws_of_Robotics" rel="">Three Laws of Robotics</a><span>: essentially, prioritize human well-being and don’t injure humans; obey human orders unless those orders would injure humans; and protect oneself unless doing so would conflict with human orders or injure humans. </span></p><p>In principle, it sounds good. But in practice, there are lots of ways this type of guidance can go wrong. Think of those fables where hapless humans find genies in a bottle and make sub-optimal wishes. Precisely defining our wishes is extremely hard. </p><p>But just because this problem is hard—and perhaps impossible to make bulletproof—doesn’t mean it can’t be a component of AI risk control. Avoiding reliance on a single control is redundancy and diversity of controls in action! And alongside other AI risk controls, alignment can play an important role. The current situation of rapidly advancing AI capabilities without commensurate breakthroughs in alignment increases risk.</p><p>Some under-explored avenues for alignment research may prove fruitful. As just one example, on the question of corrigibility (developing AI systems that are amenable to shutdowns and other interventions), it might be possible to explore approaches based on biological systems principles such as homeostasis and apoptosis (which is essentially corrigibility at the cellular level in biological organisms, since it destroys components of the organism to benefit the whole). What if corrigibility can be built into AI systems not at the whole-system level but at lower levels (layer, weight/parameter, etc.), where whole-system incentives are less likely to oppose or may even favor shutdown of components that diverge from homeostasis (the pre-trained state)?</p><p><span>In addition, on the broader question of alignment, it seems possible that AIs given goals that operate as balancing feedback loops (such as homeostasis aka “maintain the pre-trained state”) could prove easier to align than AIs given goals that operate as reinforcing feedback loops (e.g., “win this game”). Alignment research has explored goal definition before, but a </span><a href="https://en.wikipedia.org/wiki/System_dynamics" rel="">system dynamics</a><span> lens may offer fresh insights.</span></p><p>Overall, much research remains to be done on alignment, but it’s an important piece of the puzzle. </p><p><strong>Good Regulation - Hard But Feasible</strong></p><p><span>Like effective controls, there are several characteristics of effective regulation. As I wrote in my essay </span><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel="">Emerging Risks and Smart Regulation</a></p><div><p><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-4-125167098" rel="">4</a></p></div><p><span>, effective regulation: </span></p><ul><li><p>Is targeted. </p></li><li><p>Engages the governed, listening to their concerns and gaining their trust as a governed party to ensure buy-in at a strategic level instead of tactical compliance in a check-the-box exercise. </p></li><li><p>Is not make-work and is reasonably time-efficient, allowing sufficient flexibility in implementation for different types of businesses and organizations. </p></li><li><p><span>Ideally makes required </span><em>something industry participants mostly wanted to do anyway but couldn’t</em><span> since doing so would have put them at a perceived or actual market disadvantage. In essence, regulation in this situation is a key for overcoming a prisoner’s dilemma.</span></p></li><li><p>Helps ensure safety for stakeholders that have insufficient leverage to negotiate with more powerful stakeholders on their own (consumer protection regulation is an example). </p></li><li><p>Is forward-looking, with consideration of how processes and systems are evolving and how the regulation might apply to their anticipated future states.  </p></li><li><p>Costs businesses and societies less than the risk it protects against. </p></li><li><p>Has second-order effects that are manageable and do not undermine the intent of the regulation (e.g., by driving activity outside of the regulatory jurisdiction en masse without actually reducing risk in the system as a whole).</p></li></ul><p><span>Although good regulation is hard, it’s not impossible. One example of a good regulation (in my opinion) is an SEC rule called </span><a href="https://www.sec.gov/rules/final/2010/34-63241.pdf" rel="">15c3-5</a><span>. Prior to this regulation, broker-dealers could give clients their market participant identifier (MPID) and let the clients send orders directly to the markets themselves. This posed potentially significant risks for markets </span><em>and </em><span>for the broker-dealer if a client (e.g., a hedge fund) did not have sufficient controls to catch erroneous orders sent by their trading algorithms. An order might be too large (a fat-finger order), or an algorithm might send millions of orders in quick succession, causing market instability and risk exposure. </span></p><p>After the regulation, orders had to flow through a layer of broker-dealer risk controls. Broker-dealers have sufficient money and resources to implement controls in a way that their small clients might not. (Broker-dealers also can develop metrics around near-misses, events, incidents, trends, etc.) </p><p><span>This example might have applicability for AI regulations and metrics, not least because training clusters might serve as bottleneck points where controls could be applied. And take a look at what the SEC </span><a href="https://www.sec.gov/rules/final/2010/34-63241.pdf" rel="">wrote about industry feedback</a><span> after this regulation was proposed: </span></p><p><span>“Nearly all of the commenters supported the overarching goal of the proposed rulemaking—to assure that broker-dealers with market access have effective controls and procedures reasonably designed to manage the financial, regulatory, and other risks of that activity.”</span></p><div><p><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-5-125167098" rel="">5</a></p></div><p><span>Of course, “… several commenters recommended that the proposal be amended or clarified in certain respects.”</span></p><div><p><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-6-125167098" rel="">6</a></p></div><p><span> Every organization also considers its self-interest and has incentives to sway regulation in its own favor. But the broad, high-level agreement with the proposed rule indicated it was on the right track—and the regulators ultimately implemented it “substantially as proposed.” </span></p><p>I’ll delve more into this and other regulations and what makes them (in my view) effective in a future post, but suffice to say that effective regulation is hard but not impossible. (It’s almost certainly easier than getting perfectly aligned AI, especially across an industry.)</p><p><strong>Summing It Up and Shifting the Risk Curve</strong><span> </span></p><p>So, given the need for strong operational risk controls, better AI alignment approaches, and effective regulation, what is the chance that losing control of AGI will bring about existential catastrophe for humanity? </p><p>Over the long haul, if risk controls, alignment, and regulation work in tandem, it’s highly possible that they will buy time to catch up to, keep pace with, and eventually exceed capabilities innovation. Instead of a capabilities-controls relationship like the one on the left, we could get the one on the right: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png" width="980" height="449" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:449,&#34;width&#34;:980,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:106515,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d3a7d29-481d-400d-81ad-4e070105454f_980x449.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>The curve on the right we can deal with; we can evolve alongside it and adjust to oscillations around it. The capabilities curve on the left is probably too steep for us to handle. So, the combination of controls, alignment, and regulation could help us enjoy benefits of AI (e.g., new medicines, innovative energy solutions) while reducing</span></p><div><p><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-7-125167098" rel="">7</a></p></div><p><span> the risks. </span></p><p><span>But over the long haul, the risk of existential catastrophe still seems high if true AGI </span><em>that exceeds human-level intelligence </em><span>is developed. Even effective risk controls and regulations extend only to the boundaries of human intelligence. Beyond those boundaries, getting outmaneuvered eventually becomes almost certain, just as humans outmaneuver ants when we set out sugary baits that the ants happily tote back to their nest. </span></p><p><span>One of the best things controls, alignment approaches, and regulations can do for us, therefore, is to help us identify when we approach (remember: not </span><em>reach</em><span>, since </span><em>reaching </em><span>that point would be too late) a point where risk can no longer be reduced to an acceptable level. </span></p><p><span>This raises a thorny question: </span><em><strong>what is the acceptable level of risk? Whether humans can agree on a reasonable answer, and can make the choice to stop when risk exceeds that level, will determine the outcome of our foray into artificial intelligence.</strong></em><span> The stakes are high, yet we have successfully threaded this needle before with nuclear weapons. (After all, we don’t have individual nuclear reactors on top of each home’s roof, and we don’t sell nuclear chemistry kits for kids. We stopped before we reached that point!) Whether we can thread it now with an even more complex technology, ensuring controls catch up to, keep pace with, and eventually outpace capabilities—and making the decision to stop if or when that’s no longer possible—will determine our future. </span></p><div data-component-name="FootnoteToDOM"><p><a id="footnote-1-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-anchor-1-125167098" contenteditable="false" rel="">1</a></p><p>Every capabilities increase has second-, third-, and nth-order effects that increase system complexity. </p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-2-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-anchor-2-125167098" contenteditable="false" rel="">2</a></p><p><span>Note that controls that reach the level of AGI </span><em>also </em><span>would be dangerous, so if controls eventually outpace capabilities, then the margin of safety would be the difference between sophistication of controls and the likely AGI threshold.</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-3-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-anchor-3-125167098" contenteditable="false" rel="">3</a></p><div><p><span>Losi, Stephanie. “</span><a href="https://riskmusings.substack.com/p/possible-paths-for-ai-regulation" rel="">Possible Paths for AI Regulation</a><span>.” October 27, 2022.</span></p></div></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-4-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-anchor-4-125167098" contenteditable="false" rel="">4</a></p><div><p><span>Losi, Stephanie. “</span><a href="https://riskmusings.substack.com/p/emerging-risks-and-smart-regulation" rel="">Emerging Risks and Smart Regulation</a><span>.” October 20, 2022. </span></p></div></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-5-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-anchor-5-125167098" contenteditable="false" rel="">5</a></p><div><p><span>“Risk Management Controls for Brokers or Dealers with Market Access”, page 4, </span><a href="http://“Risk Management Controls for Brokers or Dealers with Market Access”, page 4 of https://www.sec.gov/rules/final/2010/34-63241.pdf" rel="">https://www.sec.gov/rules/final/2010/34-63241.pdf</a></p></div></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-6-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-anchor-6-125167098" contenteditable="false" rel="">6</a></p><div><p><span>“Risk Management Controls for Brokers or Dealers with Market Access”, page 4, </span><a href="http://“Risk Management Controls for Brokers or Dealers with Market Access”, page 4 of https://www.sec.gov/rules/final/2010/34-63241.pdf" rel="">https://www.sec.gov/rules/final/2010/34-63241.pdf</a></p></div></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-7-125167098" href="https://riskmusings.substack.com/p/agi-existential-risk-persists-despite#footnote-anchor-7-125167098" contenteditable="false" rel="">7</a></p><p>As any information security specialist will tell you, it’s impossible to eliminate all risk, and there’s no such thing as perfection.</p></div></div></div></div>
  </body>
</html>
