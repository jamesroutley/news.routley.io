<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.greptile.com/state-of-ai-coding-2025">Original</a>
    <h1>The State of AI Coding Report 2025</h1>
    
    <div id="readability-page-1" class="page"><div><div><div id="section-table of contents"><div><div><div><p>Table of Contents</p><h2>Navigate the Report</h2></div></div></div></div><section></section><section><div><div><div><p>Chart 1.1</p><div><div><p>Median PR size increased 33% from March to November 2025, rising from 57 to 76 lines changed per PR.</p><p>Captured from Greptile internal data engineering team velocity</p></div></div></div><div><p>Chart 1.2</p><div><div><p>Lines of code per developer grew from 4,450 to 7,839 as AI coding tools act as a force multiplier.</p><p>Captured from Greptile internal data engineering team velocity</p></div></div></div><div><p>Chart 1.3</p><div><div><p>Medium teams (6-15 devs) increased output from 7,005 to 13,227 lines per developer.</p><p>Captured from Greptile internal data engineering team velocity</p></div></div></div><div><p>Chart 1.4</p><div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M181.66,146.34a8,8,0,0,1,0,11.32l-24,24a8,8,0,0,1-11.32-11.32L164.69,152l-18.35-18.34a8,8,0,0,1,11.32-11.32Zm-72-24a8,8,0,0,0-11.32,0l-24,24a8,8,0,0,0,0,11.32l24,24a8,8,0,0,0,11.32-11.32L91.31,152l18.35-18.34A8,8,0,0,0,109.66,122.34ZM216,88V216a16,16,0,0,1-16,16H56a16,16,0,0,1-16-16V40A16,16,0,0,1,56,24h96a8,8,0,0,1,5.66,2.34l56,56A8,8,0,0,1,216,88Zm-56-8h28.69L160,51.31Zm40,136V96H152a8,8,0,0,1-8-8V40H56V216H200Z"></path></svg><p><span>Lines Changed Per File Up 20%</span></p></div></div><div><p>Median lines changed per file grew from 18 to 22 as PRs become denser.</p><p>Captured from Greptile internal data engineering team velocity</p></div></div></div></div></div></section><hr/><section><div><div><div><p>Chart 2.1</p><div><div><p>mem0 dominates with 59% market share. The clear leader in AI memory infrastructure.</p><p>PyPI + npm monthly downloads, Nov 2025</p></div></div></div><div><p>Chart 2.2</p><div><div><p>No clear winner. Weaviate leads at 25%, but 6 players sit between 10-25% share.</p><p>PyPI + npm monthly downloads, Nov 2025</p></div></div></div><div><p>Chart 2.3</p><div><div><p>CLAUDE.md leads adoption at 67%. Most teams use multiple formats.</p><p>17% of repos use all three formats</p></div></div></div></div><div><div><p>Chart 2.4</p><div><div><p>Anthropic SDK leads at 43M (8x growth). Pydantic AI explodes 3.7x to 6M.</p><p>PyPI + npm monthly downloads, Apr–Nov 2025</p></div></div></div><div><p>Chart 2.5</p><div><div><p>LiteLLM grows 4× to 41M monthly downloads.</p><p>PyPI + npm monthly downloads, Jun–Nov 2025</p><p>LangSmith is bundled with LangChain installs</p></div></div></div></div></div></section><hr/><section><div><div><div><p>Chart 3.1</p><div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M240,136v64a16,16,0,0,1-16,16H32a16,16,0,0,1-16-16V136a16,16,0,0,1,16-16H72a8,8,0,0,1,0,16H32v64H224V136H184a8,8,0,0,1,0-16h40A16,16,0,0,1,240,136Zm-117.66-2.34a8,8,0,0,0,11.32,0l48-48a8,8,0,0,0-11.32-11.32L136,108.69V24a8,8,0,0,0-16,0v84.69L85.66,74.34A8,8,0,0,0,74.34,85.66ZM200,168a12,12,0,1,0-12,12A12,12,0,0,0,200,168Z"></path></svg><p><span>LLM Provider SDK Downloads</span></p></div></div><div><p>OpenAI leads at 130M. Anthropic grew 1,547x since Apr 2023. Google trails at 13.6M.</p><p>PyPI monthly downloads, Jan 2022–Nov 2025</p></div></div></div><div><p>Chart 3.2</p><div><div><p>OpenAI-to-Anthropic ratio dropped from 47:1 (Jan 2024) to 4.2:1 (Nov 2025).</p><div><div><canvas role="img" height="150" width="300"></canvas><p>Peak: 47:1 (Jan 2024)</p><p>Now: 4.2:1</p></div></div><p>PyPI monthly downloads ratio, Jul 2023–Nov 2025</p></div></div></div></div></div></section><hr/><div><div><p>Each model ran through the same six test suites with identical parameters:</p></div><div><div><p>Shared parameters</p><div><div><p><span>temperature = 0.2, top_p = 1.0, max_tokens = 1024</span></p></div><div><p><span>Exponential backoff on retryable errors (429, 5xx) with delays of 0.2s, 0.4s, and 0.8s</span></p></div><div><p><span>All models saw the same prompt set under the same protocol</span></p></div></div></div></div><div><div><div><p>01</p><h5>TTFT suite</h5><p>Measured time-to-first-token (TTFT) distribution across requests, reporting p25/p50/p75 percentiles. Three warmup requests preceded measurement.</p></div><div><p>02</p><h5>Throughput suite</h5><p>Measured aggregate tokens per second, reporting p25/p50/p75 percentiles across test runs.</p></div></div></div></div><div><div><p>A comprehensive comparison of all models across key performance metrics.</p></div><div><div><table><thead><tr><th>Model</th><th>Provider</th><th>TTFT p25</th><th>TTFT p50</th><th>TTFT p75</th><th>Throughput p25</th><th>Throughput p50</th><th>Throughput p75</th></tr></thead><tbody><tr><td>GPT-5-Codex</td><td>OpenAI</td><td>3.7 s</td><td>5.0 s</td><td>6.6 s</td><td>53 tok/s</td><td>62 tok/s</td><td>73 tok/s</td></tr><tr><td>GPT-5.1</td><td>OpenAI</td><td>3.9 s</td><td>5.5 s</td><td>7.6 s</td><td>55 tok/s</td><td>62 tok/s</td><td>68 tok/s</td></tr><tr><td>Sonnet 4.5</td><td>Anthropic</td><td>1.8 s</td><td>2.0 s</td><td>2.2 s</td><td>17 tok/s</td><td>19 tok/s</td><td>21 tok/s</td></tr><tr><td>Opus 4.5</td><td>Anthropic</td><td>1.9 s</td><td>2.2 s</td><td>3.0 s</td><td>14 tok/s</td><td>18 tok/s</td><td>20 tok/s</td></tr><tr><td>Gemini 3 Pro</td><td>Google</td><td>11.8 s</td><td>13.1 s</td><td>14.5 s</td><td>4 tok/s</td><td>4 tok/s</td><td>5 tok/s</td></tr></tbody></table></div></div></div><section><div><div><div><div><h3>Time to First Token (TTFT)</h3></div><p>Anthropic&#39;s Opus 4.5 and Sonnet 4.5 return the first token in under 2.5 seconds at p50; the other three models take more than twice as long. In interactive coding sessions, that time-to-first-token gap is often the difference between staying in flow and context-switching while you wait.</p></div><div><div><div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M128,40a96,96,0,1,0,96,96A96.11,96.11,0,0,0,128,40Zm0,176a80,80,0,1,1,80-80A80.09,80.09,0,0,1,128,216ZM173.66,90.34a8,8,0,0,1,0,11.32l-40,40a8,8,0,0,1-11.32-11.32l40-40A8,8,0,0,1,173.66,90.34ZM96,16a8,8,0,0,1,8-8h48a8,8,0,0,1,0,16H104A8,8,0,0,1,96,16Z"></path></svg><p><span>TTFT Percentiles (p25 / p50 / p75)</span></p></div></div></div></div></div></div></div></section><section><div><div><div><p>GPT-5 Codex and GPT-5.1 deliver the highest sustained throughput across the distribution. In practice, that means long generations finish sooner and you can keep more coding agents or CI jobs moving in parallel, with Anthropic in the middle tier and Gemini 3 Pro at the back.</p></div><div><div><div><div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M207.06,72.67A111.24,111.24,0,0,0,128,40h-.4C66.07,40.21,16,91,16,153.13V176a16,16,0,0,0,16,16H224a16,16,0,0,0,16-16V152A111.25,111.25,0,0,0,207.06,72.67ZM224,176H119.71l54.76-75.3a8,8,0,0,0-12.94-9.42L99.92,176H32V153.13c0-3.08.15-6.12.43-9.13H56a8,8,0,0,0,0-16H35.27c10.32-38.86,44-68.24,84.73-71.66V80a8,8,0,0,0,16,0V56.33A96.14,96.14,0,0,1,221,128H200a8,8,0,0,0,0,16h23.67c.21,2.65.33,5.31.33,8Z"></path></svg><p><span>Tokens per Second (p25 / p50 / p75)</span></p></div></div></div></div></div></div></div></section><div><div><p>The key patterns are the multipliers, not the absolute prices. Calculated using public list pricing as of December 15, 2025 for an 8k input / 1k output workload, normalized to GPT-5 Codex = 1× (no caching/batch discounts).</p></div></div><hr/><section><div><h3>Foundational Model Advances</h3><div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/2412.19437"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>DeepSeek-V3 Technical Report</h4><p>DeepSeek-V3 is a 671B-parameter Mixture-of-Experts model that activates only 37B parameters per token, focusing on efficiency rather than raw size. The report shows how architectural choices can narrow the gap with much larger dense models.</p><div><div><p><span>Multi-Head Latent Attention compresses key/value representations into small latent vectors, shrinking KV caches and easing memory pressure.</span></p></div><div><p><span>Sparse MoE routing activates only a few experts per token and limits cross-node communication to keep GPUs fully utilized.</span></p></div><div><p><span>Multi-Token Prediction adds auxiliary targets per token, increasing learning signal density during training.</span></p></div><div><p><span>Overall, the model treats scale as a data-flow and memory-management problem, not just a parameter-count problem.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.20215"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>Qwen2.5-Omni Technical Report</h4><p>Qwen2.5-Omni is a multimodal model that separates perception (audio/vision encoders) from sequence modeling (a shared language model), with an emphasis on stable, real-time text–audio–video reasoning.</p><div><div><p><span>Time-aligned Multimodal RoPE (TMRoPE) synchronizes audio and video via consistent temporal position embeddings.</span></p></div><div><p><span>Encoders process inputs in blocks, while a central language model handles long-range reasoning and context.</span></p></div><div><p><span>A Thinker–Talker architecture splits responsibilities: Thinker does text reasoning, Talker turns internal representations into streaming speech.</span></p></div><div><p><span>The design highlights that decoupling perception, reasoning, and generation can make multimodal systems easier to scale and debug.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2501.01880"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>Long Context vs. RAG for LLMs: An Evaluation and Revisits</h4><p>This paper systematically compares long-context (LC) models and RAG across 12 QA datasets and ~19k questions, focusing on how each approach handles external information.</p><div><div><p><span>LC tends to outperform RAG on continuous, well-structured sources (books, wiki articles) and precise fact-based questions.</span></p></div><div><p><span>RAG tends to win on fragmented, multi-source, and dialogue-heavy data, especially under loose F1-style scoring.</span></p></div><div><p><span>Summarization-based retrieval performs similarly to LC, while simple chunk-based retrieval falls behind.</span></p></div><div><p><span>The core claim: LC and RAG succeed under different structural assumptions about where relevant information lives.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2502.11444"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>Does RAG Really Perform Bad for Long Context?</h4><p>RetroLM introduces KV-level retrieval for long-context tasks, treating the KV cache as the retrieval surface instead of raw text.</p><div><div><p><span>Inputs are split into fixed-size KV &#34;pages&#34; with bookmark tokens summarizing each page.</span></p></div><div><p><span>A trained page retriever selects important KV pages per layer; offloaded pages live off-device and are pulled back on demand.</span></p></div><div><p><span>Across LongBench, InfiniteBench, and RULER, RetroLM beats standard RAG pipelines and other efficient long-context methods.</span></p></div><div><p><span>The framework reframes retrieval as selecting which cached representations to keep, rather than which raw tokens to stuff into the prompt.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2502.00674"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>Rethinking Mixture-of-Agents</h4><p>Self-MoA examines whether diverse model ensembles are actually necessary for strong Mixture-of-Agents performance.</p><div><div><p><span>Standard MoA queries multiple different models and aggregates their answers; Self-MoA instead repeatedly samples a single strong model.</span></p></div><div><p><span>An aggregator LLM combines multiple responses from that one model, trading cross-model diversity for in-model diversity.</span></p></div><div><p><span>Experiments on AlpacaEval 2.0 and other benchmarks show Self-MoA outperforming traditional MoA when proposer quality is high.</span></p></div><div><p><span>A sequential variant, Self-MoA-Seq, aggregates in sliding windows to stay within context limits while scaling the number of samples.</span></p></div></div></div></div></div></div></section><section><div><h3>Application-Layer Innovations</h3><div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/2507.19457"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>GEPA: Reflective Prompt Evolution Can Outperform RL</h4><p>GEPA (Genetic-Pareto) is a reflective prompt-evolution method that optimizes instructions using execution traces instead of updating model weights.</p><div><div><p><span>The system samples rollouts, has the model analyze its own traces in natural language, and proposes new prompts.</span></p></div><div><p><span>A Pareto front keeps multiple candidate prompts that perform well on different subsets of data.</span></p></div><div><p><span>Across four tasks, GEPA matches or beats GRPO-style RL with up to 35× fewer rollouts.</span></p></div><div><p><span>The work treats prompts as an external optimization layer, using natural-language reflection rather than heavyweight RL.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/2509.06283"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>SFR-DeepResearch: Single-Agent RL for Deep Web Research</h4><p>SFR-DeepResearch (SFR-DR) is a reinforcement-learning framework for training a single web-research agent that decides when to search, browse, or execute code.</p><div><div><p><span>The agent uses three minimal tools—search_internet, browse_page, stateless code_interpreter—designed to force explicit reasoning.</span></p></div><div><p><span>A self-managed memory tool (clean_memory) lets the agent control long-horizon context instead of passively appending everything.</span></p></div><div><p><span>Length-normalized RL stabilizes multi-step optimization and prevents degenerate, repetitive tool use.</span></p></div><div><p><span>Results on Humanity&#39;s Last Exam and related benchmarks highlight that context management and planning are the core bottlenecks, not just model size.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2509.21865"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>Beyond RAG vs Long-Context</h4><p>LDAR (Learning Distraction-Aware Retrieval) targets the performance drop that occurs when relevant passages are mixed with noisy context.</p><div><div><p><span>A small Transformer operates purely on similarity-score distributions of candidate passages, predicting a lower and upper similarity bound.</span></p></div><div><p><span>Retrieval becomes selecting a continuous &#34;band&#34; of passages, instead of top-k or independent Bernoulli decisions.</span></p></div><div><p><span>LDAR uses 25–63% of the tokens of long-context baselines while maintaining or improving performance.</span></p></div><div><p><span>The central claim is that context quality and distraction-awareness matter more than raw context size, especially on noisy corpora.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/html/2506.15841v1"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>MEM1: Constant-Memory Long-Horizon Agents</h4><p>MEM1 is an RL framework that trains LLM agents to operate over long multi-turn tasks while keeping memory usage nearly constant.</p><div><div><p><span>At each step, previous memory and new observations are merged into a compact internal state token (IS), and older context is discarded.</span></p></div><div><p><span>A masked-trajectory RL scheme reconstructs valid trajectories for PPO without feeding the entire history.</span></p></div><div><p><span>MEM1-7B matches or beats much larger baselines on tasks with up to 16 sequential objectives while reducing memory use by ~3.7×.</span></p></div><div><p><span>The work shows that long-horizon behavior can come from learned internal state handling rather than expanding context windows or bolting on external memory.</span></p></div></div></div></div><div><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2503.09516"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a><div><h4>Search-R1: Training LLMs to Reason and Search with RL</h4><p>Search-R1 trains models to interleave step-by-step reasoning with live search-engine queries.</p><div><div><p><span>The framework uses a structured template: think for internal reasoning, search for queries, information for retrieved context, and answer for final output.</span></p></div><div><p><span>PPO or GRPO updates apply only to model-generated segments, treating the search engine as part of the environment.</span></p></div><div><p><span>Evaluated across seven QA datasets, Search-R1 delivers large gains over strong RAG baselines, including on multi-hop tasks like HotpotQA and 2WikiMultiHopQA.</span></p></div><div><p><span>The paper positions targeted, RL-trained search behavior as an alternative to static top-k retrieval and hand-crafted tool chains.</span></p></div></div></div></div></div></div></section><section><div><div><p><img alt="Greptile" loading="lazy" width="80" height="80" decoding="async" data-nimg="1" src="https://amontalenti.com/logo-only.svg"/></p><h2>Automatically review PRs with your team&#39;s standards</h2></div></div></section><hr/></div></div></div>
  </body>
</html>
