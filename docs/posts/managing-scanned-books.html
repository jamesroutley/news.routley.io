<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rgoswami.me/posts/managing-scanned-books/">Original</a>
    <h1>Managing Scanned Books</h1>
    
    <div id="readability-page-1" class="page"><div><blockquote><p>Where I consider digitizing colored content.</p></blockquote><h2 id="background">Background</h2><p>It so happened that I recently acquired a rather unique book which (somewhat
oddly) had no corresponding digital variant <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. I’ve discussed how I modify books
and papers <a href="https://www.mironov.com/posts/my-life-in-eink/">for consumption on my Kobo Aura HD in the past</a>, but this comes before
that. Much of this is essentially a plug for <a href="https://github.com/4lex4/scantailor-advanced">Scantailor Advanced</a> <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p><h2 id="goals">Goals</h2><p>Starting with the initial output <code>pdf</code> files from a scanner’s “scan to me” function <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> the goal is to have all the bells and whistles of a well-done digital book:</p><dl><dt>Fixing orientations</dt><dd>This is meant to deal with rotations mostly, skew is later</dd><dt>Splitting pages</dt><dd>Since most scanners with books take two pages at a time</dd><dt>Handling skew</dt><dd>Deskewing works to reorient pages for visibility</dd><dt>Reworking content selections</dt><dd>The automated selection worked will for this, and is especially useful if there are artifacts</dd><dt>Adding back margins</dt><dd>For padding post selection</dd><dt>Output determination</dt><dd>600 DPI is generally enough, and “mixed” mode works great for images, the one thing to add is that “Higher search sensitivity” for picture shapes is pretty useful</dd></dl><p>These six steps can be carried out with Scantailor Advanced, which has a very
intuitive GUI for the same but there are but two more things:</p><ul><li>Optical Character Recognition (OCR)</li><li>A proper Table of Contents</li></ul><p>We will also have to first get high resolution images out of the <code>pdf</code> files.
For starters our project might look like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>❯ ls
</span></span><span><span>2</span><span>appendix.pdf	chap6.pdf		     images
</span></span><span><span>3</span><span>chap2_chap3.pdf  chap7.pdf		     toc_chap1_intro.pdf
</span></span><span><span>4</span><span>chap4.pdf	chap8.pdf
</span></span><span><span>5</span><span>chap5.pdf
</span></span></code></pre></div><h2 id="from-pdf-to-tiff">From <code>pdf</code> to <code>tiff</code></h2><p>Without any bells and whistles, a simple bash script will suffice to extract
high resolution <code>.tiff</code> files from <code>.pdf</code> inputs. The only caveat is the need to
carefully format the page names so they can be joined together easily. This lead to:</p><div><pre tabindex="0"><code data-lang="bash"><span><span> 1</span><span><span># The first page number (output file name) for the current PDF</span>
</span></span><span><span> 2</span><span><span>export</span> <span>INP_SCAN_NAME</span><span>=</span>toc_chap1_intro
</span></span><span><span> 3</span><span><span>output_index</span><span>=</span><span>1</span>
</span></span><span><span> 4</span><span>pdftoppm -tiff <span>&#34;</span><span>${</span><span>INP_SCAN_NAME</span><span>}</span><span>.pdf&#34;</span>  temp
</span></span><span><span> 5</span><span><span>i</span><span>=</span><span>0</span>
</span></span><span><span> 6</span><span><span>for</span> file in temp*.tif<span>;</span> <span>do</span>
</span></span><span><span> 7</span><span>  <span>i</span><span>=</span><span>$((</span>i+1<span>))</span>
</span></span><span><span> 8</span><span>  <span>new_index</span><span>=</span><span>$(</span><span>printf</span> <span>&#34;%03d&#34;</span> <span>$((</span>output_index <span>+</span> i <span>-</span> <span>1</span><span>)))</span>
</span></span><span><span> 9</span><span>  <span>new_file</span><span>=</span><span>&#34;page_</span><span>${</span><span>new_index</span><span>}</span><span>.tiff&#34;</span>
</span></span><span><span>10</span><span>  mv <span>&#34;</span><span>$file</span><span>&#34;</span> <span>&#34;images/</span><span>$new_file</span><span>&#34;</span>
</span></span><span><span>11</span><span><span>done</span>
</span></span></code></pre></div><p>At this point we can plug the images directly into ScanTailor Advanced and work
through the GUI.</p><h2 id="ocr-setup">OCR Setup</h2><p>Working with the TIFF images which form the output at ScanTailor Advanced is a
good bet. System packages needed are:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>trizen -S tesseract tesseract-data-eng <span>\
</span></span></span><span><span>2</span><span><span></span>    tesseract-data-isl <span>\
</span></span></span><span><span>3</span><span><span></span>    ghostscript unpaper pngquant <span>\
</span></span></span><span><span>4</span><span><span></span>    img2pdf jbig2enc pdftk
</span></span></code></pre></div><p>Subsequently I ended up with a throwaway <code>conda</code> environment for using <a href="https://github.com/ocrmypdf/OCRmyPDF">OCRmyPDF</a>
(like with all <code>python</code> tools).</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>micromamba create -p <span>$(</span><span>pwd</span><span>)</span>/.ocrtmp pip ocrmypdf
</span></span><span><span>2</span><span>micromamba activate <span>$(</span><span>pwd</span><span>)</span>/.ocrtmp
</span></span></code></pre></div><p>Since <code>ocrmypdf</code> works on single PDF files or alternatively single images, one
brute-force approach is simply:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span><span>cd</span> out <span># where scantailor puts things</span>
</span></span><span><span>2</span><span>mkdir temp
</span></span><span><span>3</span><span><span>for</span> file in page*.tif<span>;</span> <span>do</span>
</span></span><span><span>4</span><span>  <span>base</span><span>=</span><span>$(</span>basename <span>&#34;</span><span>$file</span><span>&#34;</span> .tif<span>)</span>
</span></span><span><span>5</span><span>  ocrmypdf -l eng --output-type pdf <span>&#34;</span><span>$file</span><span>&#34;</span> <span>&#34;temp/</span><span>${</span><span>base</span><span>}</span><span>.pdf&#34;</span>
</span></span><span><span>6</span><span><span>done</span>
</span></span></code></pre></div><p>Which leads to a series of <code>.pdf</code> files.</p><h2 id="concatenation-and-compression">Concatenation and Compression</h2><p><a href="https://www.ghostscript.com/">Ghostscript</a> is the best way to get a single reasonably <strong>compressed</strong> output from
these files<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span> 1</span><span><span>export</span> <span>OUTPUT_PDF_DPI</span><span>=</span><span>500</span> <span># good enough for me</span>
</span></span><span><span> 2</span><span><span># Or 600 for the Scantailor default</span>
</span></span><span><span> 3</span><span>gs -q -dNOPAUSE -dBATCH -sDEVICE<span>=</span>pdfwrite <span>\
</span></span></span><span><span> 4</span><span><span></span>    -dCompatibilityLevel<span>=</span>1.7 -dPDFSETTINGS<span>=</span>/ebook <span>\
</span></span></span><span><span> 5</span><span><span></span>    -dEmbedAllFonts<span>=</span><span>true</span> -dSubsetFonts<span>=</span><span>true</span> <span>\
</span></span></span><span><span> 6</span><span><span></span>    -dColorImageDownsampleType<span>=</span>/Bicubic <span>\
</span></span></span><span><span> 7</span><span><span></span>    -dColorImageResolution<span>=</span><span>$OUTPUT_PDF_DPI</span> <span>\
</span></span></span><span><span> 8</span><span><span></span>    -dGrayImageDownsampleType<span>=</span>/Bicubic <span>\
</span></span></span><span><span> 9</span><span><span></span>    -dGrayImageResolution<span>=</span><span>$OUTPUT_PDF_DPI</span> <span>\
</span></span></span><span><span>10</span><span><span></span>    -dMonoImageDownsampleType<span>=</span>/Bicubic <span>\
</span></span></span><span><span>11</span><span><span></span>    -dMonoImageResolution<span>=</span><span>$OUTPUT_PDF_DPI</span> <span>\
</span></span></span><span><span>12</span><span><span></span>    -sOutputFile<span>=</span><span>&#34;output_</span><span>${</span><span>OUTPUT_PDF_DPI</span><span>}</span><span>.pdf&#34;</span> <span>\
</span></span></span><span><span>13</span><span><span></span>    temp/*.pdf
</span></span></code></pre></div><p>Where the compatibility level is to <a href="https://stackoverflow.com/questions/29417217/how-are-pdf-files-able-to-be-partially-displayed-while-downloading">allow for linearization</a> <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> (needs <a href="http://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf">PDF 1.7</a>).
Sometimes this leads to files which are still too large (Table ref:tbl:sizes).</p><table><thead><tr><th>DPI</th><th>Size (in MB)</th></tr></thead><tbody><tr><td>300</td><td>139.2</td></tr><tr><td>600</td><td>303.5</td></tr></tbody></table><p>In such cases <code>pdfsizeopt</code> (from <a href="https://github.com/pts/pdfsizeopt">here</a>) can be setup in the environment with:</p><div><pre tabindex="0"><code data-lang="bash"><span><span> 1</span><span>mkdir <span>$CONDA_PREFIX</span>/tmp/pdfsizeopt
</span></span><span><span> 2</span><span><span>cd</span> <span>$CONDA_PREFIX</span>/tmp/pdfsizeopt
</span></span><span><span> 3</span><span>wget -O pdfsizeopt_libexec_linux.tar.gz https://github.com/pts/pdfsizeopt/releases/download/2023-04-18/pdfsizeopt_libexec_linux-v9.tar.gz
</span></span><span><span> 4</span><span>tar xzvf pdfsizeopt_libexec_linux.tar.gz
</span></span><span><span> 5</span><span>rm -f    pdfsizeopt_libexec_linux.tar.gz
</span></span><span><span> 6</span><span>wget -O pdfsizeopt.single https://raw.githubusercontent.com/pts/pdfsizeopt/master/pdfsizeopt.single
</span></span><span><span> 7</span><span>chmod +x pdfsizeopt.single
</span></span><span><span> 8</span><span>mv * <span>$CONDA_PREFIX</span>/bin <span>&amp;&amp;</span> <span>cd</span> <span>$CONDA_PREFIX</span>/bin
</span></span><span><span> 9</span><span>ln -s pdfsizeopt.single <span>$CONDA_PREFIX</span>/bin/pdfsizeopt
</span></span><span><span>10</span><span>rm -rf <span>$CONDA_PREFIX</span>/tmp/*
</span></span></code></pre></div><p>Which can then be used as:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>pdfsizeopt output_600.pdf
</span></span></code></pre></div><p>The compression takes a while <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>, but the results are completely
lossless and the reduction was pretty neat, from <code>390 MB</code> to <code>250 MB</code>, though it
took forever, and <a href="https://acrobat.adobe.com/link/acrobat/compress-pdf">Adobe Acrobat</a> reduced a <code>390 MB</code> file to <code>108 MB</code> in minutes.</p><figure><img src="https://d33wubrfki0l68.cloudfront.net/ed0da7dcb349dc4c417805a50a78db9f9725757a/8f9ec/ox-hugo/2023-06-26_21-31-14_screenshot.png" alt="Figure 1: Lossless compression (250 MB) on the left, lossy Acrobat compression (108 MB) on the right"/><figcaption><p><span>Figure 1: </span>Lossless compression (<code>250 MB</code>) on the left, lossy Acrobat compression (<code>108 MB</code>) on the right</p></figcaption></figure><p>For handling the cover image I ended up with:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span><span># from the ScanTailor 600 DPI image</span>
</span></span><span><span>2</span><span><span># 39.3 MB</span>
</span></span><span><span>3</span><span>tiffcp -c zip:p9  cpage-1.tif cpage.tif <span>#  19.7 MB</span>
</span></span><span><span>4</span><span>convert cpage.tif -compress JPEG -quality 90% cpage.jpg <span># 2.0 MB</span>
</span></span></code></pre></div><p>Then I dumped it through a <a href="https://compressjpg.com/">web tool</a> <sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>(around <code>560 KB</code>) before finishing up
with <code>ocrmypdf</code> for the final <code>pdf</code> of <code>535 KB</code>.</p><h2 id="toc-generation">TOC Generation</h2><p>Sometimes I feel too lazy and end up doing my table of contents editing in
Master PDF editor <sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>. That being said my most favored way for
a fully fledged TOC with chapters and sub-headings is with the venerable <a href="https://www.pdflabs.com/tools/pdftk-the-pdf-toolkit/">PDFtk</a>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span> 1</span><span>cat toc.txt
</span></span><span><span> 2</span><span>BookmarkBegin
</span></span><span><span> 3</span><span>BookmarkTitle: Chapter <span>1</span>
</span></span><span><span> 4</span><span>BookmarkLevel: <span>1</span>
</span></span><span><span> 5</span><span>BookmarkPageNumber: <span>5</span>
</span></span><span><span> 6</span><span>
</span></span><span><span> 7</span><span>BookmarkBegin
</span></span><span><span> 8</span><span>BookmarkTitle: Section 1.1
</span></span><span><span> 9</span><span>BookmarkLevel: <span>2</span>
</span></span><span><span>10</span><span>BookmarkPageNumber: <span>6</span>
</span></span><span><span>11</span><span>
</span></span><span><span>12</span><span>BookmarkBegin
</span></span><span><span>13</span><span>BookmarkTitle: Section 1.2
</span></span><span><span>14</span><span>BookmarkLevel: <span>2</span>
</span></span><span><span>15</span><span>BookmarkPageNumber: <span>10</span>
</span></span><span><span>16</span><span>BookmarkBegin
</span></span><span><span>17</span><span>
</span></span><span><span>18</span><span>
</span></span><span><span>19</span><span>BookmarkTitle: Chapter <span>2</span>
</span></span><span><span>20</span><span>BookmarkLevel: <span>1</span>
</span></span><span><span>21</span><span>BookmarkPageNumber: <span>15</span>
</span></span><span><span>22</span><span>...
</span></span></code></pre></div><p>This can be applied with:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>pdftk input.pdf update_info toc.txt output output_with_toc.pdf
</span></span></code></pre></div><p>Sometimes I end up padding the <code>pdf</code> with a few extra pages to ensure the page
numbers (printed) match the ones in the TOC.</p><h2 id="linearization">Linearization</h2><p>Since we targeted PDF 1.7, with linearization support <sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>. We need <a href="https://qpdf.readthedocs.io/">qpdf</a>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>micromamba install qpdf
</span></span><span><span>2</span><span>qpdf --linearize output_300.pdf olin300.pdf
</span></span></code></pre></div><p>This has seemingly no effect on size so it can be the last step.</p><h2 id="conclusions">Conclusions</h2><p>This isn’t my first time working with a scanned book. Felt like a lot more steps
than before though. Some more bash magic could have automated extracting the
images from scanner generated <code>pdf</code> files, and perhaps some other minor
tweaks <sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> or other tools <sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>, though for my purposes, this
worked well enough. I was a little disappointed to not be able to hit optimal
file sizes without web tools, but given the amount of time expended, it was
probably to be expected.</p></div></div>
  </body>
</html>
