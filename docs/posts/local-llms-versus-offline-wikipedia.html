<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://evanhahn.com/local-llms-versus-offline-wikipedia/">Original</a>
    <h1>Local LLMs versus offline Wikipedia</h1>
    
    <div id="readability-page-1" class="page"><div><p>Two days ago, MIT Technology review published <a href="https://www.technologyreview.com/2025/07/17/1120391/how-to-run-an-llm-on-your-laptop/">“How to run an LLM on your laptop”</a>. It opens with an anecdote about using offline LLMs in an apocalypse scenario. “‘It’s like having a weird, condensed, faulty version of Wikipedia, so I can help reboot society with the help of my little USB stick,’ [Simon Willison] says.”</p><p>This made me wonder: how do the sizes of local LLMs compare to the size of offline Wikipedia downloads?</p><p>I compared some models from the <a href="https://ollama.com/library">Ollama library</a> to various downloads on <a href="https://kiwix.org/">Kiwix</a>. I chose models that could be run on some consumer-grade hardware, and Wikipedia bundles that didn’t have images for a better comparison. Here’s what I found, ordered by size:</p><table><thead><tr><th>Name</th><th>Download size</th></tr></thead><tbody><tr><td>Best of Wikipedia (best 50K articles, no details)</td><td>356.9MB</td></tr><tr><td>Simple English Wikipedia (no details)</td><td>417.5MB</td></tr><tr><td>Qwen 3 0.6B</td><td>523MB</td></tr><tr><td>Simple English Wikipedia</td><td>915.1MB</td></tr><tr><td>Deepseek-R1 1.5B</td><td>1.1GB</td></tr><tr><td>Llama 3.2 1B</td><td>1.3GB</td></tr><tr><td>Qwen 3 1.7B</td><td>1.4GB</td></tr><tr><td>Best of Wikipedia (best 50K articles)</td><td>1.93GB</td></tr><tr><td>Llama 3.2 3B</td><td>2.0GB</td></tr><tr><td>Qwen 3 4B</td><td>2.6GB</td></tr><tr><td>Deepseek-R1 8B</td><td>5.2GB</td></tr><tr><td>Qwen 3 8B</td><td>5.2GB</td></tr><tr><td>Gemma3n e2B</td><td>5.6GB</td></tr><tr><td>Gemma3n e4B</td><td>7.5GB</td></tr><tr><td>Deepseek-R1 14B</td><td>9GB</td></tr><tr><td>Qwen 3 14B</td><td>9.3GB</td></tr><tr><td>Wikipedia (no details)</td><td>13.82GB</td></tr><tr><td>Mistral Small 3.2 24B</td><td>15GB</td></tr><tr><td>Qwen 3 30B</td><td>19GB</td></tr><tr><td>Deepseek-R1 32B</td><td>20GB</td></tr><tr><td>Qwen 3 32B</td><td>20GB</td></tr><tr><td>Wikipedia: top 1 million articles</td><td>48.64GB</td></tr><tr><td>Wikipedia</td><td>57.18GB</td></tr></tbody></table><p>This comparison has many caveats:</p><ul><li><p>This is an apples-to-oranges comparison. Encyclopedias and LLMs have different purposes, strengths, and weaknesses. They are fundamentally different technologies!</p></li><li><p>File size is not the only important detail. LLMs, even local ones, can use lots of memory and processor power. Offline Wikipedia will work better on my ancient, low-power laptop.</p></li><li><p>Other entries might be more useful for a specific purpose. For example, you can download a selection of Wikipedia articles about chemistry, or an LLM that’s better tuned for your hardware. (And Kiwix has lots of other things you can download, like all of Stack Overflow.)</p></li><li><p>I picked these entries based on vibes. Nothing rigorous about this comparison!</p></li></ul><p>Despite those caveats, I thought it was interesting that Wikipedia’s best 50,000 articles are, very roughly, equivalent to Llama 3.2 3B. Or that Wikipedia can be smaller than the smallest LLM, and larger than the largest ones—at least in an offline scenario on consumer hardware.</p><p>Maybe I’ll download both, just in case.</p></div></div>
  </body>
</html>
