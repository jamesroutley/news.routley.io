<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nullprogram.com/blog/2018/05/27/">Original</a>
    <h1>When FFI function calls beat native C</h1>
    
    <div id="readability-page-1" class="page"><div>
<article>
  
  <time datetime="2018-05-27">
    May 27, 2018
  </time>
  <p>
    nullprogram.com/blog/2018/05/27/
  </p>

  <p><em>Update: There’s a good discussion on <a href="https://news.ycombinator.com/item?id=17171252">Hacker News</a>.</em></p>

<p>Over on GitHub, David Yu has an interesting performance benchmark for
function calls of various Foreign Function Interfaces (<a href="https://en.wikipedia.org/wiki/Foreign_function_interface">FFI</a>):</p>

<p><a href="https://github.com/dyu/ffi-overhead">https://github.com/dyu/ffi-overhead</a></p>

<p>He created a shared object (<code>.so</code>) file containing a single, simple C
function. Then for each FFI he wrote a bit of code to call this function
many times, measuring how long it took.</p>

<p>For the C “FFI” he used standard dynamic linking, not <code>dlopen()</code>. This
distinction is important, since it really makes a difference in the
benchmark. There’s a potential argument about whether or not this is a
fair comparison to an actual FFI, but, regardless, it’s still
interesting to measure.</p>

<p>The most surprising result of the benchmark is that
<strong><a href="http://luajit.org/">LuaJIT’s</a> FFI is substantially faster than C</strong>. It’s about
25% faster than a native C function call to a shared object function.
How could a weakly and dynamically typed scripting language come out
ahead on a benchmark? Is this accurate?</p>

<p>It’s actually quite reasonable. The benchmark was run on Linux, so the
performance penalty we’re seeing comes the <em>Procedure Linkage Table</em>
(PLT). I’ve put together a really simple experiment to demonstrate the
same effect in plain old C:</p>

<p><a href="https://github.com/skeeto/dynamic-function-benchmark">https://github.com/skeeto/dynamic-function-benchmark</a></p>

<p>Here are the results on an Intel i7-6700 (Skylake):</p>

<div><div><pre><code>plt: 1.759799 ns/call
ind: 1.257125 ns/call
jit: 1.008108 ns/call
</code></pre></div></div>

<p>These are three different types of function calls:</p>

<ol>
  <li>Through the PLT</li>
  <li>An indirect function call (via <code>dlsym(3)</code>)</li>
  <li>A direct function call (via a JIT-compiled function)</li>
</ol>

<p>As shown, the last one is the fastest. It’s typically not an option
for C programs, but it’s natural in the presence of a JIT compiler,
including, apparently, LuaJIT.</p>

<p>In my benchmark, the function being called is named <code>empty()</code>:</p>



<p>And to compile it into a shared object:</p>

<div><div><pre><code>$ cc -shared -fPIC -Os -o empty.so empty.c
</code></pre></div></div>

<p>Just as in my <a href="https://nullprogram.com/blog/2017/09/21/">PRNG shootout</a>, the benchmark calls this function
repeatedly as many times as possible before an alarm goes off.</p>

<h3 id="procedure-linkage-tables">Procedure Linkage Tables</h3>

<p>When a program or library calls a function in another shared object,
the compiler cannot know where that function will be located in
memory. That information isn’t known until run time, after the program
and its dependencies are loaded into memory. These are usually at
randomized locations — e.g. <em>Address Space Layout Randomization</em>
(ASLR).</p>

<p>How is this resolved? Well, there are a couple of options.</p>

<p>One option is to make a note about each such call in the binary’s
metadata. The run-time dynamic linker can then <em>patch</em> in the correct
address at each call site. How exactly this would work depends on the
particular <a href="https://eli.thegreenplace.net/2012/01/03/understanding-the-x64-code-models">code model</a> used when compiling the binary.</p>

<p>The downside to this approach is slower loading, larger binaries, and
less <a href="https://nullprogram.com/blog/2016/04/10/">sharing of code pages</a> between different processes. It’s
slower loading because every dynamic call site needs to be patched
before the program can begin execution. The binary is larger because
each of these call sites needs an entry in the relocation table. And the
lack of sharing is due to the code pages being modified.</p>

<p>On the other hand, the overhead for dynamic function calls would be
eliminated, giving JIT-like performance as seen in the benchmark.</p>

<p>The second option is to route all dynamic calls through a table. The
original call site calls into a stub in this table, which jumps to the
actual dynamic function. With this approach the code does not need to
be patched, meaning it’s <a href="https://nullprogram.com/blog/2016/12/23/">trivially shared</a> between processes.
Only one place needs to be patched per dynamic function: the entries
in the table. Even more, these patches can be performed <em>lazily</em>, on
the first function call, making the load time even faster.</p>

<p>On systems using ELF binaries, this table is called the Procedure
Linkage Table (PLT). The PLT itself doesn’t actually get patched —
it’s mapped read-only along with the rest of the code. Instead the
<em>Global Offset Table</em> (GOT) gets patched. The PLT stub fetches the
dynamic function address from the GOT and <em>indirectly</em> jumps to that
address. To lazily load function addresses, these GOT entries are
initialized with an address of a function that locates the target
symbol, updates the GOT with that address, and then jumps to that
function. Subsequent calls use the lazily discovered address.</p>

<p><img src="https://nullprogram.com/img/diagram/plt.svg" alt=""/></p>

<p>The downside of a PLT is extra overhead per dynamic function call,
which is what shows up in the benchmark. Since the benchmark <em>only</em>
measures function calls, this appears to be pretty significant, but in
practice it’s usually drowned out in noise.</p>

<p>Here’s the benchmark:</p>

<div><div><pre><code><span>/* Cleared by an alarm signal. */</span>
<span>volatile</span> <span>sig_atomic_t</span> <span>running</span><span>;</span>

<span>static</span> <span>long</span>
<span>plt_benchmark</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
    <span>long</span> <span>count</span><span>;</span>
    <span>for</span> <span>(</span><span>count</span> <span>=</span> <span>0</span><span>;</span> <span>running</span><span>;</span> <span>count</span><span>++</span><span>)</span>
        <span>empty</span><span>();</span>
    <span>return</span> <span>count</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>Since <code>empty()</code> is in the shared object, that call goes through the PLT.</p>

<h3 id="indirect-dynamic-calls">Indirect dynamic calls</h3>

<p>Another way to dynamically call functions is to bypass the PLT and
fetch the target function address within the program, e.g. via
<code>dlsym(3)</code>.</p>

<div><div><pre><code><span>void</span> <span>*</span><span>h</span> <span>=</span> <span>dlopen</span><span>(</span><span>&#34;path/to/lib.so&#34;</span><span>,</span> <span>RTLD_NOW</span><span>);</span>
<span>void</span> <span>(</span><span>*</span><span>f</span><span>)(</span><span>void</span><span>)</span> <span>=</span> <span>dlsym</span><span>(</span><span>&#34;f&#34;</span><span>);</span>
<span>f</span><span>();</span>
</code></pre></div></div>

<p>Once the function address is obtained, the overhead is smaller than
function calls routed through the PLT. There’s no intermediate stub
function and no GOT access. (Caveat: If the program has a PLT entry for
the given function then <code>dlsym(3)</code> may actually return the address of
the PLT stub.)</p>

<p>However, this is still an <em>indirect</em> function call. On conventional
architectures, <em>direct</em> function calls have an immediate relative
address. That is, the target of the call is some hard-coded offset from
the call site. The CPU can see well ahead of time where the call is
going.</p>

<p>An indirect function call has more overhead. First, the address has to
be stored somewhere. Even if that somewhere is just a register, it
increases register pressure by using up a register. Second, it
provokes the CPU’s branch predictor since the call target isn’t
static, making for extra bookkeeping in the CPU. In the worst case the
function call may even cause a pipeline stall.</p>

<p>Here’s the benchmark:</p>

<div><div><pre><code><span>volatile</span> <span>sig_atomic_t</span> <span>running</span><span>;</span>

<span>static</span> <span>long</span>
<span>indirect_benchmark</span><span>(</span><span>void</span> <span>(</span><span>*</span><span>f</span><span>)(</span><span>void</span><span>))</span>
<span>{</span>
    <span>long</span> <span>count</span><span>;</span>
    <span>for</span> <span>(</span><span>count</span> <span>=</span> <span>0</span><span>;</span> <span>running</span><span>;</span> <span>count</span><span>++</span><span>)</span>
        <span>f</span><span>();</span>
    <span>return</span> <span>count</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>The function passed to this benchmark is fetched with <code>dlsym(3)</code> so the
compiler can’t <a href="https://nullprogram.com/blog/2018/05/01/">do something tricky</a> like convert that indirect
call back into a direct call.</p>

<p>If the body of the loop was complicated enough that there was register
pressure, thereby requiring the address to be spilled onto the stack,
this benchmark might not fare as well against the PLT benchmark.</p>

<h3 id="direct-function-calls">Direct function calls</h3>

<p>The first two types of dynamic function calls are simple and easy to
use. <em>Direct</em> calls to dynamic functions is trickier business since it
requires modifying code at run time. In my benchmark I put together a
<a href="https://nullprogram.com/blog/2015/03/19/">little JIT compiler</a> to generate the direct call.</p>

<p>There’s a gotcha to this: on x86-64 direct jumps are limited to a 2GB
range due to a signed 32-bit immediate. This means the JIT code has to
be placed virtually nearby the target function, <code>empty()</code>. If the JIT
code needed to call two different dynamic functions separated by more
than 2GB, then it’s not possible for both to be direct.</p>

<p>To keep things simple, my benchmark isn’t precise or very careful
about picking the JIT code address. After being given the target
function address, it blindly subtracts 4MB, rounds down to the nearest
page, allocates some memory, and writes code into it. To do this
correctly would mean inspecting the program’s own memory mappings to
find space, and there’s no clean, portable way to do this. On Linux
this <a href="https://nullprogram.com/blog/2016/09/03/">requires parsing virtual files under <code>/proc</code></a>.</p>

<p>Here’s what my JIT’s memory allocation looks like. It assumes
<a href="https://nullprogram.com/blog/2016/05/30/">reasonable behavior for <code>uintptr_t</code> casts</a>:</p>

<div><div><pre><code><span>static</span> <span>void</span>
<span>jit_compile</span><span>(</span><span>struct</span> <span>jit_func</span> <span>*</span><span>f</span><span>,</span> <span>void</span> <span>(</span><span>*</span><span>empty</span><span>)(</span><span>void</span><span>))</span>
<span>{</span>
    <span>uintptr_t</span> <span>addr</span> <span>=</span> <span>(</span><span>uintptr_t</span><span>)</span><span>empty</span><span>;</span>
    <span>void</span> <span>*</span><span>desired</span> <span>=</span> <span>(</span><span>void</span> <span>*</span><span>)((</span><span>addr</span> <span>-</span> <span>SAFETY_MARGIN</span><span>)</span> <span>&amp;</span> <span>PAGEMASK</span><span>);</span>
    <span>/* ... */</span>
    <span>unsigned</span> <span>char</span> <span>*</span><span>p</span> <span>=</span> <span>mmap</span><span>(</span><span>desired</span><span>,</span> <span>len</span><span>,</span> <span>prot</span><span>,</span> <span>flags</span><span>,</span> <span>fd</span><span>,</span> <span>0</span><span>);</span>
    <span>/* ... */</span>
<span>}</span>
</code></pre></div></div>

<p>It allocates two pages, one writable and the other containing
non-writable code. Similar to <a href="https://nullprogram.com/blog/2017/01/08/">my closure library</a>, the lower
page is writable and holds the <code>running</code> variable that gets cleared by
the alarm. It needed to be nearby the JIT code in order to be an
efficient RIP-relative access, just like the other two benchmark
functions. The upper page contains this assembly:</p>

<div><div><pre><code><span>jit_benchmark:</span>
        <span>push</span>  <span>rbx</span>
        <span>xor</span>   <span>ebx</span><span>,</span> <span>ebx</span>
<span>.loop:</span>  <span>mov</span>   <span>eax</span><span>,</span> <span>[</span><span>rel</span> <span>running</span><span>]</span>
        <span>test</span>  <span>eax</span><span>,</span> <span>eax</span>
        <span>je</span>    <span>.done</span>
        <span>call</span>  <span>empty</span>
        <span>inc</span>   <span>ebx</span>
        <span>jmp</span>   <span>.loop</span>
<span>.done:</span>  <span>mov</span>   <span>eax</span><span>,</span> <span>ebx</span>
        <span>pop</span>   <span>rbx</span>
        <span>ret</span>
</code></pre></div></div>

<p>The <code>call empty</code> is the only instruction that is dynamically generated
— necessary to fill out the relative address appropriately (the minus
5 is because it’s relative to the <em>end</em> of the instruction):</p>

<div><div><pre><code>    <span>// call empty</span>
    <span>uintptr_t</span> <span>rel</span> <span>=</span> <span>(</span><span>uintptr_t</span><span>)</span><span>empty</span> <span>-</span> <span>(</span><span>uintptr_t</span><span>)</span><span>p</span> <span>-</span> <span>5</span><span>;</span>
    <span>*</span><span>p</span><span>++</span> <span>=</span> <span>0xe8</span><span>;</span>
    <span>*</span><span>p</span><span>++</span> <span>=</span> <span>rel</span> <span>&gt;&gt;</span>  <span>0</span><span>;</span>
    <span>*</span><span>p</span><span>++</span> <span>=</span> <span>rel</span> <span>&gt;&gt;</span>  <span>8</span><span>;</span>
    <span>*</span><span>p</span><span>++</span> <span>=</span> <span>rel</span> <span>&gt;&gt;</span> <span>16</span><span>;</span>
    <span>*</span><span>p</span><span>++</span> <span>=</span> <span>rel</span> <span>&gt;&gt;</span> <span>24</span><span>;</span>
</code></pre></div></div>

<p>If <code>empty()</code> wasn’t in a shared object and instead located in the same
binary, this is essentially the direct call that the compiler would have
generated for <code>plt_benchmark()</code>, assuming somehow it didn’t inline
<code>empty()</code>.</p>

<p>Ironically, calling the JIT-compiled code requires an indirect call
(e.g. via a function pointer), and there’s no way around this. What
are you going to do, JIT compile another function that makes the
direct call? Fortunately this doesn’t matter since the part being
measured in the loop is only a direct call.</p>

<h3 id="its-no-mystery">It’s no mystery</h3>

<p>Given these results, it’s really no mystery that LuaJIT can generate
more efficient dynamic function calls than a PLT, <em>even if they still
end up being indirect calls</em>. In my benchmark, the non-PLT indirect
calls were 28% faster than the PLT, and the direct calls 43% faster
than the PLT. That’s a small edge that JIT-enabled programs have over
plain old native programs, though it comes at the cost of absolutely
no code sharing between processes.</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
    
  
  </nav>
</article>

</div></div>
  </body>
</html>
