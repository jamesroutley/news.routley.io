<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://alexshtf.github.io/2024/01/21/Bernstein.html">Original</a>
    <h1>Are polynomial features the root of all evil? (2024)</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    

<p>When fitting a non-linear model using linear regression, we typically generate new features using non-linear functions. We also know that any function, in theory, can be approximated by a sufficiently high degree polynomial. This result is known as <a href="https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem">Weierstrass approximation theorem</a>. But many blogs, papers, and even books tell us that high polynomials should be avoided. They tend to oscilate and overfit, and regularization doesn’t help! They even scare us with images, such as the one below, when the polynomial fit using the data points (in red) is far away from the true function (in blue):
<img src="https://alexshtf.github.io/assets/poly_overfit.png" alt="Polynomial overfitting"/></p>

<p>It turns out that it’s just a MYTH. There’s nothing inherently wrong with high degree polynomials, and in contrast to what is typically taught, high degree polynomials are easily controlled using standard ML tools, like regularization. The source of the myth stems mainly from two misconceptions about polynomials that we will explore here. In fact, not only they are great non-linear features, certain representations also provide us with powerful control over the shape of the function we wish to learn.</p>

<p>A colab notebook with the code for reproducing the above results is available <a href="https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/polyfeatures.ipynb">here</a>.</p>



<p>Vladimir Vapnik, in his famous book “The Nature of Statistical Learning Theory” which is cited more than 100,000 times as of today, coined the approximation vs. estimation balance. The approximation power of a model is its ability to represent the “reality” we would like to learn. Typically, approximation power increases with the complexity of the model - more parameters mean more power to represent any function to arbitrary precision. Polynomials are no different - higher degree polynomials can represent functions to higher accuracy. However, more parameters make it difficult to <em>estimate these parameters from the data</em>.</p>

<p>Indeed, higher degree polynomials have a higher capacity to approximate arbitrary functions. And since they have more coefficients, these coefficients are harder to estimate from data. But how does it differ from other non-linear features, such as the well-known <a href="https://en.wikipedia.org/wiki/Radial_basis_function">radial basis functions</a>? Why do polynomials have such a bad reputation? Are they truly hard to estimate from data?</p>

<p>It turns out that the primary source is the standard polynomial basis for n-degree polynomials \(\mathbb{E}_n = {1, x, x^2, ..., x^n}\). Indeed, any degree \(n\)  polynomial can be written as a linear combination of these functions:</p><p>

\[\alpha_0 \cdot 1 + \alpha_1 \cdot x + \alpha_2 \cdot x^2 + \cdots + \alpha_n x^n\]

</p><p>But the standard basis \(\mathbb{B}_n\) is <em>awful</em> for estimating polynomials from data. In this post we will explore other ways to represent polynomials that are appropriate for machine learning, and are readily available in standard Python packages. We note, that one advantage of polynomials over other non-linear feature bases is that the only hyperparameter is their <em>degree</em>. There is no “kernel width”, like in radial basis functions<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>The second source of their bad reputation is misunderstanding of Weierstrass’ approximation theorem. It’s usually cited as “polynomials can approximate arbitrary continuous functions”. But that’s not entrely true. They can approximate arbitrary continuous functions <strong>in an interval</strong>. This means that when using polynomial features, the data must be normalized to lie in an interval. It can be done using min-max scaling, computing empirical quantiles, or passing the feature through a sigmoid. But we should avoid the use of polynomials on raw un-normalized features.</p>



<p>In this post we will demonstrate fitting the function</p><p>

\[f(x)=\sin(8 \pi x) / \exp(x)+x\]

</p><p>on the interval \([0, 1]\) by fitting to \(m=30\) samples corrupted by Gaussian noise. The following code implements the function and generates samples:</p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>def</span> <span>true_func</span><span>(</span><span>x</span><span>):</span>
  <span>return</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>8</span> <span>*</span> <span>np</span><span>.</span><span>pi</span> <span>*</span> <span>x</span><span>)</span> <span>/</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>x</span><span>)</span> <span>+</span> <span>x</span>

<span>m</span> <span>=</span> <span>30</span>
<span>sigma</span> <span>=</span> <span>0.1</span>

<span># generate features
</span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>42</span><span>)</span>
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>rand</span><span>(</span><span>m</span><span>)</span>
<span>y</span> <span>=</span> <span>true_func</span><span>(</span><span>X</span><span>)</span> <span>+</span> <span>sigma</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>m</span><span>)</span>
</code></pre></div></div>

<p>For function plotting, we will use uniformly-spaced points in \([0, 1]\). The following code plots the true function and the sample points:</p>

<div><div><pre><code><span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span>plt_xs</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>1000</span><span>)</span>
<span>plt</span><span>.</span><span>scatter</span><span>(</span><span>X</span><span>.</span><span>ravel</span><span>(),</span> <span>y</span><span>.</span><span>ravel</span><span>())</span>
<span>plt</span><span>.</span><span>plot</span><span>(</span><span>plt_xs</span><span>,</span> <span>true_func</span><span>(</span><span>plt_xs</span><span>),</span> <span>&#39;blue&#39;</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div></div>

<p><img src="https://alexshtf.github.io/assets/polyfit_func.png" alt="polyfit_func"/></p>

<p>Now let’s fit a polynomial to the sampled points using the standard basis. Namely, we’re given the set of noisy points \(\{ (x_i, y_i) \}_{i=1}^m\), and we need to find the coefficients \(\alpha_0, \dots, \alpha_n\) that minimize:</p><p>

\[\sum_{i=1}^m (\alpha_0 + \alpha_1 x_i + \dots + \alpha_n x_i^n - y_i)^2\]

</p><p>As expected, this is readily accomplished by transforming each sample \(x_i\) to a vector of features \(1, x_i, \dots, x_i^n\), and fitting a linear regression model to the resulting features. Fortunately, NumPy has the <code>numpy.polynomial.polynomial.polyvander</code>function. It takes a vector containing \(x_1, \dots, x_m\) and produces the matrix</p><p>

\[\begin{pmatrix}
1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^n \\
1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^n \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_m &amp; x_m^2 &amp; \dots &amp; x_m^n \\
\end{pmatrix}\]

</p><p>The name of the function comes from the name of the matrix - the Vandermonde matrix. Let’s use it to fit a polynomial of degree \(n=50\).</p>

<div><div><pre><code><span>from</span> <span>sklearn.linear_model</span> <span>import</span> <span>LinearRegression</span>
<span>import</span> <span>numpy.polynomial.polynomial</span> <span>as</span> <span>poly</span>

<span>n</span> <span>=</span> <span>50</span>
<span>model</span> <span>=</span> <span>LinearRegression</span><span>(</span><span>fit_intercept</span><span>=</span><span>False</span><span>)</span>
<span>model</span><span>.</span><span>fit</span><span>(</span><span>poly</span><span>.</span><span>polyvander</span><span>(</span><span>X</span><span>,</span> <span>deg</span><span>=</span><span>n</span><span>),</span> <span>y</span><span>)</span>
</code></pre></div></div>

<p>The reason we use <code>fit_intercept=False</code> is because the ‘intercept’ is provided by the first column of the Vandermonde matrix. Now we can plot the function we just fit:</p>

<div><div><pre><code><span>plt</span><span>.</span><span>scatter</span><span>(</span><span>X</span><span>.</span><span>ravel</span><span>(),</span> <span>y</span><span>.</span><span>ravel</span><span>())</span>                                    <span># plot the samples
</span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>plt_xs</span><span>,</span> <span>true_func</span><span>(</span><span>plt_xs</span><span>),</span> <span>&#39;blue&#39;</span><span>)</span>                          <span># plot the true function
</span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>plt_xs</span><span>,</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>poly</span><span>.</span><span>polyvander</span><span>(</span><span>plt_xs</span><span>,</span> <span>deg</span><span>=</span><span>n</span><span>)),</span> <span>&#39;r&#39;</span><span>)</span> <span># plot the fit model
</span><span>plt</span><span>.</span><span>ylim</span><span>([</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>])</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div></div>

<p>As expected, we got the “scary” image from the beginning of this post. Indeed, the standard basis is awful for model fitting! We hope that regularization provides a remedy, but it does not. Maybe adding some L2 regularization helps? Let’s use the <code>Ridge</code> class from the <code>sklearn.linear_model</code>  package to fit an L2 regularized model:</p>

<div><div><pre><code><span>from</span> <span>sklearn.linear_model</span> <span>import</span> <span>Ridge</span>

<span>reg_coef</span> <span>=</span> <span>1e-7</span>
<span>model</span> <span>=</span> <span>Ridge</span><span>(</span><span>fit_intercept</span><span>=</span><span>False</span><span>,</span> <span>alpha</span><span>=</span><span>reg_coef</span><span>)</span>
<span>model</span><span>.</span><span>fit</span><span>(</span><span>poly</span><span>.</span><span>polyvander</span><span>(</span><span>X</span><span>,</span> <span>deg</span><span>=</span><span>n</span><span>),</span> <span>y</span><span>)</span>

<span>plt</span><span>.</span><span>scatter</span><span>(</span><span>X</span><span>.</span><span>ravel</span><span>(),</span> <span>y</span><span>.</span><span>ravel</span><span>())</span>                                    <span># plot the samples
</span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>plt_xs</span><span>,</span> <span>true_func</span><span>(</span><span>plt_xs</span><span>),</span> <span>&#39;blue&#39;</span><span>)</span>                          <span># plot the true function
</span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>plt_xs</span><span>,</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>poly</span><span>.</span><span>polyvander</span><span>(</span><span>plt_xs</span><span>,</span> <span>deg</span><span>=</span><span>n</span><span>)),</span> <span>&#39;r&#39;</span><span>)</span> <span># plot the fit model
</span><span>plt</span><span>.</span><span>ylim</span><span>([</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>])</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div></div>

<p>We get the following result:</p>

<p><img src="https://alexshtf.github.io/assets/polyfit_standard_ridge.png" alt="polyfit_standard_ridge"/></p>

<p>The regularization coefficient coefficient of \(\alpha=10^{-7}\) is large enough to break the model in \([0,0.8]\) but not large enough to avoid over-fitting in \([0.8, 1]\). Increasing the coefficient clearly won’t help - the model will be broken even further in \([0, 0.8]\).</p>

<p>Since we will be trying several polynomial bases, it makes sense to write a more generic function for our experiments that will accept various “Vandermonde” matrix functions of the basis of our choice, fit the polynomial using the <code>Ridge</code> class, and plot it with the original function and the sample points.</p>

<div><div><pre><code><span>def</span> <span>fit_and_plot</span><span>(</span><span>vander</span><span>,</span> <span>n</span><span>,</span> <span>alpha</span><span>):</span>
  <span>model</span> <span>=</span> <span>Ridge</span><span>(</span><span>fit_intercept</span><span>=</span><span>False</span><span>,</span> <span>alpha</span><span>=</span><span>alpha</span><span>)</span>
  <span>model</span><span>.</span><span>fit</span><span>(</span><span>vander</span><span>(</span><span>X</span><span>,</span> <span>deg</span><span>=</span><span>n</span><span>),</span> <span>y</span><span>)</span>

  <span>plt</span><span>.</span><span>scatter</span><span>(</span><span>X</span><span>.</span><span>ravel</span><span>(),</span> <span>y</span><span>.</span><span>ravel</span><span>())</span>                           <span># plot the samples
</span>  <span>plt</span><span>.</span><span>plot</span><span>(</span><span>plt_xs</span><span>,</span> <span>true_func</span><span>(</span><span>plt_xs</span><span>),</span> <span>&#39;blue&#39;</span><span>)</span>                 <span># plot the true function
</span>  <span>plt</span><span>.</span><span>plot</span><span>(</span><span>plt_xs</span><span>,</span> <span>model</span><span>.</span><span>predict</span><span>(</span><span>vander</span><span>(</span><span>plt_xs</span><span>,</span> <span>deg</span><span>=</span><span>n</span><span>)),</span> <span>&#39;r&#39;</span><span>)</span> <span># plot the fit model
</span>  <span>plt</span><span>.</span><span>ylim</span><span>([</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>])</span>
  <span>plt</span><span>.</span><span>show</span><span>()</span>  
</code></pre></div></div>

<p>Now we can reproduce our latest experiment by invoking:</p>

<div><div><pre><code><span>fit_and_plot</span><span>(</span><span>poly</span><span>.</span><span>polyvander</span><span>,</span> <span>n</span><span>=</span><span>50</span><span>,</span> <span>alpha</span><span>=</span><span>1e-7</span><span>)</span>
</code></pre></div></div>



<p>It turns out that in our sister discipline, approximation theory, reseachers also encountered similar difficulties with the standard basis \(\mathbb{E}_n\), and developed a thoery for approximating functions by polynomials from different bases. Two prominent examples of bases of \(n\)-degree polynomials include, and their:</p>

<ol>
  <li>The <a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials">Chebyshev polynomials</a> \(\mathbb{T}_n = \{ T_0, T_1, \dots, T_n \}\), implemented in the <code>numpy.polynomial.chebyshev</code> module.</li>
  <li>The <a href="https://en.wikipedia.org/wiki/Legendre_polynomials">Legendre polynomials</a> \(\mathbb{P}_n = \{ P_0, P_1, \dots, P_n \}\), implemented in the <code>numpy.polynomial.legendre</code> module.</li>
</ol>

<p>They are the computational workhorse of a large variety of numerical algorithms that are enabled by approximating a function using a polynomial, and are well-known for their advantages in approximating functions in the \([-1, 1]\) interval<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">2</a></sup>. In particular, the corresponding “Vandermonde” matrices are provided by the <code>chebvander</code> and <code>legvander</code> functions in corresponding modules above. Each row in these matrices contains the value of the basis functions at each point, just like the standard Vandermonde matrix of the standard basis. For example, the Chebyshev Vandermonde matrix is:</p><p>

\[\begin{pmatrix}
T_0(x_1) &amp; T_1(x_1) &amp; \dots &amp; T_n(x_1) \\
T_0(x_2) &amp; T_1(x_2) &amp; \dots &amp; T_n(x_2) \\
\vdots &amp; \vdots  &amp; \ddots&amp; \vdots  \\
T_0(x_m) &amp; T_1(x_m) &amp; \dots &amp; T_n(x_m) \\
\end{pmatrix}\]

</p><p>I will not elaborate their formulas and properties here for a reason that will immediately be revealed. However, I highly recomment Prof. Nick Trefethen’s “Approximation theory and approximation practice” <a href="https://people.maths.ox.ac.uk/trefethen/atapvideos.html">online video course</a> to get familiar with their advantages. His book with the same name is an excellent introduction to the subject.</p>

<p>It might be tempting to try fitting a Chebyshev polynomial using our <code>fit_and_plot</code> method above directly:</p>

<div><div><pre><code><span>import</span> <span>numpy.polynomial.chebyshev</span> <span>as</span> <span>cheb</span>

<span>fit_and_plot</span><span>(</span><span>cheb</span><span>.</span><span>chebvander</span><span>,</span> <span>n</span><span>=</span><span>50</span><span>,</span> <span>alpha</span><span>=</span><span>1e-7</span><span>)</span>
</code></pre></div></div>

<p>However, that’s not the best thing to do. We aim to fit a function sampled from \([0, 1]\), but the Chebyshev basis “lives” in \([-1, 1]\). Therefore, we will add the transformation \(x \to 2x-1\) before invoking the <code>chebvander</code> function:</p>

<div><div><pre><code><span>def</span> <span>scaled_chebvander</span><span>(</span><span>x</span><span>,</span> <span>deg</span><span>):</span>
  <span>return</span> <span>cheb</span><span>.</span><span>chebvander</span><span>(</span><span>2</span> <span>*</span> <span>x</span> <span>-</span> <span>1</span><span>,</span> <span>deg</span><span>=</span><span>deg</span><span>)</span>

<span>fit_and_plot</span><span>(</span><span>scaled_chebvander</span><span>,</span> <span>n</span><span>=</span><span>50</span><span>,</span> <span>alpha</span><span>=</span><span>1</span><span>)</span>
</code></pre></div></div>

<p>Note that a different basis requires a different regularization coefficient. We get the following result:</p>

<p><img src="https://alexshtf.github.io/assets/polyfit_cheb_reg1.png" alt="polyfit_cheb_reg1"/></p>

<p>Whoa! Seems even worse than the standard basis!. Maybe more regularization helps?</p>

<div><div><pre><code><span>fit_and_plot</span><span>(</span><span>scaled_chebvander</span><span>,</span> <span>n</span><span>=</span><span>50</span><span>,</span> <span>alpha</span><span>=</span><span>10</span><span>)</span>
</code></pre></div></div>

<p><img src="https://alexshtf.github.io/assets/polyfit_cheb_reg10.png" alt="polyfit_cheb_reg10"/></p>

<p>Appears that our polynomial is both a bad fit for the function, and extremely oscilatory. Even worse when the standard basis! Interested readers can repeat the experiment with Legendre polynomials and see a slightly better, but similar result. So what’s wrong? Is everything that approximation theory tries to teach us about polynomials wrong?</p>

<p>The answer stems from the fundamental difference between two tasks:</p>

<ul>
  <li><strong>Interpolation</strong> - finding a polynomial that agrees with the approximated function \(f(x)\) <em>exactly</em> at a set of <em>carefully chosen</em> points</li>
  <li><strong>Fitting</strong> - finding a polynomial that agrees <em>approximately</em> with a given <em>noisy</em> set of points, which are <em>out of our control</em>.</li>
</ul>

<p>The Chebyshev and Legendre bases perform extremely well at the the interpolation task, but not at the fitting task. It turns out that the polynomial \(T_k\) in the Chebyshev basis, and the polynomial \(P_k\) in the Legendre basis, are both \(k\)-degree polynomials. For example, \(T_1\) is a linear function, whereas \(T_{50}\) is a polynomial of degree 50. These two functions are radically different. Thus, the coefficient of \(T_1\) and \(T_{50}\) have “different units”. This property is shared with the standard basis as well. Thus, we have two issues:</p>

<ol>
  <li>A small change of the coefficient of a high degree basis function, say the coefficient \(\alpha_{50}\), has a huge effect on the shape of the polynomial. Thus, a small perturbation in the input data, be it from noise or a slighly different data point \(x_i\), has a <em>huge</em> effect of the fit model.</li>
  <li>L2 regularization makes no sense! For reasonable functions, the coefficient \(\alpha_{50}\) should be much smaller than the coefficient \(\alpha_1\). This is regardless of the choice of the basis!</li>
</ol>

<p>Both properties show that for the fitting, rather the interpolation tasks we need something else.</p>



<p>A remedy is provided by the <a href="https://en.wikipedia.org/wiki/Bernstein_polynomial">Bernstein basis</a> \(\mathbb{B}_n = \{  b_{0,n}, \dots, b_{n, n} \}\). These are \(n\)-degree polynomials defined by on \([0, 1]\) by:</p><p>

\[b_{i,n}(x) = \binom{n}{i} x^i (1-x)^{n-i}\]

</p><p>These polynomials are widely used in computer graphics to approximate curves and surfaces, but it appears that they’re less known in the machine learning community. In fact, all the text you see on the screen when reading this post is rendered using Bernstein polynomials<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">3</a></sup>. We will study them more in depth in the next posts, but at this stage I would like to point out two simple properties that give an intuitive explanation of why they’re useful in machine learning.</p>

<p>First, note that each \(b_{i,n}\) is an \(n\)-degree polynomial. Thus, when representing a polynomial using</p><p>

\[p_n(x) = \alpha_0 b_{0,n}(x) + \alpha_1 b_{1,n}(x) + \dots + \alpha_n b_{n,n}(x),\]

</p><p>all the coefficients have the same “units”.</p>

<p>If the formula of \(b_{i,n}(x)\) seems familiar - you are correct. It is exactly the probability mass function of the binomial distribution for obtaining \(i\) successes in a sequence of trials whose success probability is \(x\). Therefore, \(b_{i,n}(x) \geq 0\),  and \(\sum_{i=0}^n b_{i,n}(x) = 1\) for any \(x \in [0, 1]\). Consequently, the polynomial \(p_n(x)\) is just a weighted average of the coefficients \(\alpha_0, \dots, \alpha_n\). So not only the coefficients have the same “units”, their “units” are also the same as the model’s labels. Thus, they’re much easier to regularize - they’re all on the same “scale”.</p>

<p>Finally, due to the equivalence with the binomial distribution p.m.f, we can implement a “Vandermonde” matrix in Python using the <code>scipy.stats.binom.pmf</code> function.</p>

<div><div><pre><code><span>from</span> <span>scipy.stats</span> <span>import</span> <span>binom</span>

<span>def</span> <span>bernvander</span><span>(</span><span>x</span><span>,</span> <span>deg</span><span>):</span>
	<span>return</span> <span>binom</span><span>.</span><span>pmf</span><span>(</span><span>np</span><span>.</span><span>arange</span><span>(</span><span>1</span> <span>+</span> <span>deg</span><span>),</span> <span>deg</span><span>,</span> <span>x</span><span>.</span><span>reshape</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>))</span>
</code></pre></div></div>

<p>Let’s try and fit without regularization at all</p>

<div><div><pre><code><span>fit_and_plot</span><span>(</span><span>bernvander</span><span>,</span> <span>n</span><span>=</span><span>50</span><span>,</span> <span>alpha</span><span>=</span><span>0</span><span>)</span>
</code></pre></div></div>

<p><img src="https://alexshtf.github.io/assets/polyfit_bern_reg0.png" alt="polyfit_bern_reg0"/></p>

<p>We see our regular over-fitting. Now let’s see that they’re indeed easy to regularize. After trying several regularization coefficients, I came up with this:</p>

<div><div><pre><code><span>fit_and_plot</span><span>(</span><span>bernvander</span><span>,</span> <span>n</span><span>=</span><span>50</span><span>,</span> <span>alpha</span><span>=</span><span>5e-7</span><span>)</span>
</code></pre></div></div>

<p><img src="https://alexshtf.github.io/assets/polyfit_bern_reg5em4.png" alt="polyfit_bern_reg5em4"/></p>

<p>Beautiful! This is a polynomial of degree 50! The fit is great, no oscillations, and the misfit near the right endpoint stems from the noise - I don’t believe there’s enough information in the data to convey the fact that it should “curve up” rather than “curve down”.</p>

<p>Let’s see what happens when we crank-up the degree. Can we produce a nice non-oscilating polynomial?</p>

<div><div><pre><code><span>fit_and_plot</span><span>(</span><span>bernvander</span><span>,</span> <span>n</span><span>=</span><span>100</span><span>,</span> <span>alpha</span><span>=</span><span>5e-4</span><span>)</span>
</code></pre></div></div>

<p><img src="https://alexshtf.github.io/assets/polyfit_bern_100_reg5em4.png" alt="polyfit_bern_100_reg5em4"/></p>

<p>This is a polynomial of degree 100, that does not overfit!</p>



<p>The notorious reputation of high-degree polynomials in the machine learning community is primarily a myth. Despite it, papers, books, and blog posts are based on this premise as if it was an axiom. Bernstein polynomials are little known in the machine learning community, but there are a few papers<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup> using them to represent polynomial features. Their main advantage is ease of use - we can use high degree polynomials to exploit their approximation power, and easily control model complexity with just one hyperparameter - the regularization coefficient.</p>

<p>In the following posts we will explore the Bernstein basis in more detail. We will use it to create polynomial features for real-world datasets and test it versus the standard basis. Moreover, we will see how to regularize the coefficients to control the shape of the function we aim to represent.. For example, what if we know that the function we’re aiming to fit is increasing? Stay tuned!</p>

<hr/>



  </div></div>
  </body>
</html>
