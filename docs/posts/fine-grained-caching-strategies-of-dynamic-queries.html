<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jensrantil.github.io/posts/fast-aggregate-queries-on-dynamic-data/">Original</a>
    <h1>Fine-grained caching strategies of dynamic queries</h1>
    
    <div id="readability-page-1" class="page"><div><p>Today I would like to talk about caching strategies for aggregate queries over
time-based data which is updated often. This is something I spent significant
brain-cycles on my previous job and I would love to share some of my findings.</p><h2 id="example-data--use-case">Example data &amp; use case
<a href="#example-data--use-case"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>For the sake of the rest of this post, let’s say we have a relational database table containing financial transactions:</p><table><thead><tr><th><strong>date (ASC)</strong></th><th><strong>id</strong></th><th><strong>userId</strong></th><th><strong>description</strong></th><th><strong>amount</strong></th><th><strong>tags</strong></th></tr></thead><tbody><tr><td>2023-05-22</td><td>1</td><td>1</td><td>BestBuy</td><td>$42</td><td>[“tools”]</td></tr><tr><td>2023-05-29</td><td>2</td><td>2</td><td>Netflix</td><td>$9.9</td><td>[“entertainment”]</td></tr><tr><td>2023-05-29</td><td>3</td><td>2</td><td>Lowe’s</td><td>$42</td><td>[“tools”]</td></tr><tr><td>2023-06-03</td><td>4</td><td>2</td><td>Amazon</td><td>$22</td><td>[“tools”, “entertainment”]</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><p>The table has a secondary ordered index on the <code>date</code> column in such a way that
one can quickly query a slice of the dates (ie <code>... WHERE date BETWEEN &#39;2023-05-24&#39; AND &#39;2023-05-31&#39;</code>). Let’s assume that the table consists of enough
rows in such a way that <code>SELECT SUM(amount) FROM transactions</code> is slow.</p><p>The access pattern <strong>requirements</strong> for this table are as follows:</p><ol><li>Mutations (ie. insertions, updates, and deletes) can happen to any of the
rows. That is, the table is not append-only.</li><li>Mutations happen more frequently than querying.</li><li>Customers are interested in summing up the amounts of a subset of
transactions using custom filters dynamically generated from a web
interface. Things they can filter on is<ol><li>tags, amount, and description.</li><li>on date ranges.</li></ol></li><li>Customers expect consecutive queries to return quickly.</li></ol><h2 id="implementation">Implementation
<a href="#implementation"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>It turns out that the above requirement is a surprisingly hard problem to
solve! I like to relax engineering problems a bit to understand what is
actually hard, so let’s build up a solution from scratch where we relax some of
the problems:</p><h3 id="immutable-data">Immutable data
<a href="#immutable-data"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>For now, let’s assume that the table is immutable (ie. mutations are not
allowed). So, what is wrong with simply constructing a <code>SELECT</code> query against
immutable data? Well, it turns out that querying it is too slow (requirement
4). The classic way to solve this is to add a caching layer* in front of the
database. We use the SQL query as our cache key, and return the cached value if
it exists - otherwise, run the expensive query against the database.</p><p>(Memcache and Redis are two excellent distributed caches that could be used for
this - and can be scaled horizontally. For certain applications, you might even
be fine with an in-memory cache in your client.)</p><h3 id="in-place-mutations--no-partitioning">In-place mutations &amp; no partitioning
<a href="#in-place-mutations--no-partitioning"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>As pointed out in our original requirements, our data is <em>not</em> immutable. So,
let’s now assume that our data can be mutated. That is, added, removed, or
deleted. This means that we need cache invalidation to avoid returning stale
data. Since the list of all SQL query cache keys isn’t known beforehand, we
need to invalidate <em>all</em> keys. Most caches support this.</p><p>The problem with invalidating the full cache is fairly obvious; Every write
will make <em>every</em> following query slow since it needs to hit the database.
That’s no good.</p><p>It’s worth pointing out that if the transactions would be tied to a <code>userId</code>,
at least we could invalidate only that user’s cache keys. Certain caches
(looking at you Redis) support iterating over the cache keys, but most don’t.
Either way that would be an expensive operation. A workaround for this would be
to start working with <em>hashing</em>. If we introduce a new table, called
<code>cache_invalidation_token</code> mapping a <code>userId</code> to some random <a href="https://en.wikipedia.org/wiki/Cryptographic_nonce" target="_blank" rel="noopener">nonce</a>
that gets updated every time we modify a user’s financial transaction (within
the same <em>database</em> transaction), we could then use <code>HASH(sql) XOR NONCE(userId)</code> as our cache lookup key. By updating the nonce on every write,
we would implicitly invalidate all the SQL results. Neat!</p><p>As a side note <code>cache_invalidation_tokens</code> mapping could be stored in a cache
itself. Whether to store it next to the <code>transactions</code> table is a matter of how
certain you want to be that the cache invalidation happens on every write if
there is a network partition. You can of course also automatically add TTLs to
the <code>cache_invalidation_tokens</code> cache to handle that case, occasionally risking
intermittent stale data from time to time. Trade-offs, trade-offs…</p><h3 id="date-based-partitioning">Date-based partitioning
<a href="#date-based-partitioning"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>The problem with the above approach is that every cache invalidation requires a
full pass over all the user’s data again. Can we do better? Usually, yes, and
this is where things get interesting; We can do more fine-grained cache
invalidation by date. By partitioning our cached SQL results by date, for
example, month, we can invalidate only certain parts of our data. Let me
explain:</p><p>For simplicity, let’s just ignore the <code>userId</code> field and assume we always
filter by it and take it into consideration when doing a lookup from the cache.
If we instead define our <code>cache_invalidation_tokens</code> mapping as <code>(year, month) =&gt; nonce</code>, the query</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-01-01&#39;</span> <span>AND</span> <span>&#39;2023-06-01&#39;</span>
</span></span></code></pre></div><p>would trigger five cache lookups and potentially five SQL query executions:</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-01-01&#39;</span> <span>AND</span> <span>&#39;2023-02-01&#39;</span>;
</span></span><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-02-01&#39;</span> <span>AND</span> <span>&#39;2023-03-01&#39;</span>;
</span></span><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-03-01&#39;</span> <span>AND</span> <span>&#39;2023-04-01&#39;</span>;
</span></span><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-04-01&#39;</span> <span>AND</span> <span>&#39;2023-05-01&#39;</span>;
</span></span><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-05-01&#39;</span> <span>AND</span> <span>&#39;2023-06-01&#39;</span>;
</span></span></code></pre></div><p>Each SQL query would first check if the cache key <code>HASH(sql) XOR NONCE(year, month)</code> exists, followed by an optional query against the primary table on
cache miss. Finally, all the results would be summed up to a final
<code>SUM(amount)</code>. Further, every mutation would then need to update with a new
random nonce for the <code>(year, month)</code> (as before, either in a database
transaction or in a cache).</p><p>The above-described approach is a trade-off between shorter scans on average
when data has been mutated, at the cost of more queries against the database.
The size of the time buckets (months etc.) really depends on the tradeoffs
between</p><ul><li>the number of queries hitting the database and the cache.</li><li>whether mutations usually update certain date ranges (ie. close to today).</li><li>how often reads happen (to keep the cache updated).</li></ul><h3 id="advanced-prepopulating-the-cache-hot">Advanced: Prepopulating the cache hot
<a href="#advanced-prepopulating-the-cache-hot"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>If low latency is needed for certain known SQL queries, there is nothing
stopping a database writer from asynchronously populating the cache afterward.
For example, maybe summing the amount without any custom filtering is so common
that populating that in the cache is worth it.</p><p>The two popular caches <a href="https://memcached.org/" target="_blank" rel="noopener">Memcached</a> and <a href="https://redis.com/" target="_blank" rel="noopener">Redis</a> both support
<a href="https://github.com/memcached/memcached/blob/efee763c93249358ea5b3b42c7fd4e57e2599c30/doc/protocol.txt#L354" target="_blank" rel="noopener">atomic incrementation of integers</a> which also could be done at
write instead of a full recalculation and storing cache invalidation tokens.</p><h3 id="advanced-2-phase-lookups--hierarchical-date-based-partitioning">Advanced: 2-phase lookups &amp; hierarchical date-based partitioning
<a href="#advanced-2-phase-lookups--hierarchical-date-based-partitioning"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>The careful reader might have noticed my example above was slightly contrived;
the date range for my example query was covering full even months. What if
someone would query</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> date <span>BETWEEN</span> <span>&#39;2023-01-05&#39;</span> <span>AND</span> <span>&#39;2023-04-15&#39;</span>
</span></span></code></pre></div><p>? Ie.</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-01-05&#39;</span> <span>AND</span> <span>&#39;2023-02-01&#39;</span>;
</span></span><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-02-01&#39;</span> <span>AND</span> <span>&#39;2023-03-01&#39;</span>;
</span></span><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-03-01&#39;</span> <span>AND</span> <span>&#39;2023-04-01&#39;</span>;
</span></span><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> description<span>=</span><span>&#39;Netflix&#39;</span> <span>AND</span> date <span>BETWEEN</span> <span>&#39;2023-04-01&#39;</span> <span>AND</span> <span>&#39;2023-04-15&#39;</span>;
</span></span></code></pre></div><p>The likelihood for the first and last query to be found in the cache would be
rather small, as the SQL query would be fairly unique.</p><p>Another problem would be the query:</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> date <span>BETWEEN</span> <span>&#39;2000-01-01&#39;</span> <span>AND</span> <span>&#39;2023-01-01&#39;</span>
</span></span></code></pre></div><p>In the worst-case scenario, if nothing is found in the cache, this would
trigger <code>23 years * 12 months = 276 query</code> executions against the database!</p><p>The above two described problems could be solved by two different approaches:</p><p>The first workaround would be to do <strong>two lookup phases</strong>: First you would do a
pass of all cache lookups, wait for them to be done, and then execute <em>a
single</em> SQL query based on the ranges not within the cache, ie. something like:</p><div><pre tabindex="0"><code data-lang="sql"><span><span><span>SELECT</span> <span>SUM</span>(amount) <span>FROM</span> transactions <span>WHERE</span> (date <span>BETWEEN</span> <span>&#39;2000-01-01&#39;</span> <span>AND</span> <span>&#39;2015-01-01&#39;</span>) <span>OR</span> (date <span>BETWEEN</span> <span>&#39;2017-01-01&#39;</span> <span>AND</span> <span>&#39;2023-01-01&#39;</span>)
</span></span></code></pre></div><p>This would definitely reduce the number of queries against the database, but
not the <code>cache_invalidation_tokens</code> cache!</p><p>To hit the cache less, one could instead use <strong>hierarchical date-based
partitioning</strong> where nonces are introduced for different date partition
granularity. For example, <code>NONCE(userId, year)</code>, <code>NONCE(userId, month)</code>, and
<code>NONCE(userId, day)</code>. A mutation of a financial transaction with the date
<code>2013-08-03</code> for user X, would then invalidate the cache for the keys <code>(X, 2013)</code>, <code>(X, 2013-08)</code>, and <code>(X, 2013-08-03)</code>. The query logic above would
become more complex, but would prefer querying in the following priority if
possible:</p><ol><li>year partition from cache.</li><li>month partition from cache.</li><li>day partition from cache.</li><li>SQL query against the relational primary data.</li></ol><p>The hierarchical approach would have the benefit of reducing the hits to the
relational database while taking a cost in amplifying the writes needed to
<code>cache_invalidation_tokens</code> as well as the storage needed for it.</p><h2 id="conclusion">Conclusion
<a href="#conclusion"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>Introducing finer-grained partitioned caching is a useful tool to not have to
invalidate all caches on every mutation.</p><p>One important aspect this article did <em>not</em> cover too much is the importance of
finding the right abstraction such that you can easily iterate on caching
strategies like this. You need a single place that can control how data is
written to the relational database, as well as how that data is accessed. If
you have many different clients accessing your database, you can’t do this kind
of work easily.</p><h2 id="addendum-i-on-readwrite-ratio--caching">Addendum I: On read/write ratio &amp; caching
<a href="#addendum-i-on-readwrite-ratio--caching"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>A common way to categorize computer systems is whether they have a high
read/ratio or a low read/write ratio. The ratio is high if there are more reads
than the writes. It is low if there are more writes than reads.</p><p>An example of a <em>high read/write</em> ratio could be an address book; you look
things up very often, but you rarely update your contacts. <em>Low read/write
ratio</em> could be when you have a lot of data being received but you rarely look
at it. A good example of that is a logging system; Your application will write
lots of log lines, but you will most likely rarely look at every log.</p><p>I once heard someone say something of the like</p><blockquote><p>Solving high read-write ratio problems is fairly easy. Solving low read-write
ratio problems is fairly easy. The hard problem is when you have closer to a
1:1 ratio between reads and &gt; writes.</p></blockquote><p>It’s so true! If I recall correctly, the quote came from someone when they were
talking about <a href="https://martinfowler.com/bliki/CQRS.html" target="_blank" rel="noopener">the Command-Query Responsibility Segregation(CQRS)</a>
pattern. It’s a pattern where you explicitly split your system into one part
that takes care of writes (validation &amp; data consistency) and another part that
takes care of serving reads.</p><p>The reason why this is a tricky engineering problem to solve is that we are
bordering the land of a 1:1 ratio.</p><p>The nice thing about the hierarchical date-based approach is that it allows for
some flexibility in how much you would like to optimize for reads vs. writes
without turning into an either-or decision.</p><h2 id="addendum-ii-on-general-ranged-data">Addendum II: On general ranged data
<a href="#addendum-ii-on-general-ranged-data"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h2><p>This article was written with date-based table records. There is nothing
stopping someone from taking a more general approach to partitioning other
types of columns!</p></div></div>
  </body>
</html>
