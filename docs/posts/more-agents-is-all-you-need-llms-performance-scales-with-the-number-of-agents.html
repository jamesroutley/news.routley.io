<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2402.05120">Original</a>
    <h1>More Agents Is All You Need: LLMs performance scales with the number of agents</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2402.05120">View PDF</a>
    <a href="https://arxiv.org/html/2402.05120v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Deheng Ye [<a href="https://arxiv.org/show-email/6d8d604a/2402.05120">view email</a>]      </p></div></div>
  </body>
</html>
