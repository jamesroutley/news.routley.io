<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://accelerated-computing.com/blog/notes-on-neural-networks-01-basic-concepts/">Original</a>
    <h1>Notes on neural networks: basic concepts</h1>
    
    <div id="readability-page-1" class="page"><div>
<h2>Notes on neural networks: basic concepts</h2>

<h3>2 October 2023</h3>

<h3 id="overview">Overview</h3>
<p>A neural network is (broadly speaking) a collection of transformations arranged in a sequence of layers of neurons. A neuron is made up of some weights $W$, a bias $b$, and (frequently) a non-linear function $g$. Examples of non-linear functions typically used are ReLU, tanh, softmax, and sigmoid. A neuron in layer $i$ takes as inputs the outputs of the neurons in layer $i - 1$, applies the linear transformation encoded by its weights, adds its bias, then finally outputs the value of $g$ applied to the sum. The neurons, together with their inputs and outputs, form a computation graph, where an edge from neuron $s$ to neuron $t$ indicates that the output of $s$ is used in the computation of $t$. In practice, networks are often created with random weights and biases equal to zero.</p>
<p>Neural networks are trained by minimising a loss function $L$. Given some training examples $X$ and labels $Y$, if $\hat{Y}$ denotes the output of a network on $X$, then $L = L(\hat{Y}, Y)$ should be chosen such that it takes non-negative values, and such that the closer $\hat{Y}$ is to $Y$, the smaller $L$ becomes. Examples of loss functions include mean squared error and binary cross entropy. Minimising the loss is achieved through adjusting all the weights and biases in the network in a way that minimises the distance between $\hat{Y}$ and $Y$. This is done via the process of <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> – iterating over the computation graph backwards from $L$, computing the derivatives of all weights and biases with respect to $L$, and using those to update the network parameters (the set of all weights and biases) in such a way that results in a reduced loss. For example, during gradient descent, a network parameter is updated by subtracting from it some small multiple – known as the learning rate – of its gradient.</p>
<p>Backpropagation is a topic worthy of its own post, but the general idea is that by successively applying the chain rule to nodes in our computation graph – in a carefully chosen order – we can compute the gradient of our network parameters with respect to some loss, no matter how deep the network architecture.</p>
<h3 id="example">Example</h3>
<p>Let’s use <a href="https://pytorch.org/">PyTorch</a><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> to try to learn the function which takes a point on the plane and maps it to $1$ if it lies within the unit circle, and zero otherwise:</p>
<p>$$
(x_1, x_2) \mapsto \begin{cases}
1 &amp;\text{if } x_1^2 + x_2^2 &lt; 1 \\
0 &amp;\text{otherwise}
\end{cases}
$$</p>
<p>We’ll construct a network with three layers: one hidden layer with ten neurons, one hidden layer with five neurons, and a final output layer with a single neuron. For the non-linear functions, we’ll use ReLU in the hidden layers, and sigmoid for the output layer.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> We create a synthetic dataset of $10,000$ points, partitioned (in a roughly $80$:$20$ split) into $n_{\text{train}}$ training examples, and $n_{\text{dev}}$ examples to be held back, only to be used in evaluating our model’s performance after training has complete. To keep things simple, we’ll use a fixed epoch count (the number of iterations of the training loop performed) of $1,000$ and learning rate of $0.5$.</p>
<p>Before we dive into the code, let’s have a look at the dimensions of our inputs, outputs, and network parameters:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Dimension</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>X_train</td>
<td>($n_{\text{train}}$, $2$)</td>
<td>Inputs to network, arranged in $n_{\text{train}}$ rows of coordinate pairs</td>
</tr>
<tr>
<td>Y_train</td>
<td>($n_{\text{train}}$, $1$)</td>
<td>Training labels of ones and zeros, arranged in $n_{\text{train}}$ rows</td>
</tr>
<tr>
<td>X_dev</td>
<td>($n_{\text{dev}}$, $2$)</td>
<td>Input examples held back from training, arranged in $n_{\text{dev}}$ rows</td>
</tr>
<tr>
<td>Y_dev</td>
<td>($n_{\text{dev}}$, $1$)</td>
<td>Training labels held back from training, arranged in $n_{\text{train}}$ rows</td>
</tr>
<tr>
<td>W1</td>
<td>($2$, $10$)</td>
<td>Weights for the ten neurons in the first layer <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
</tr>
<tr>
<td>b1</td>
<td>($10$)</td>
<td>Biases for the ten neurons in the first layer <sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></td>
</tr>
<tr>
<td>W2</td>
<td>($10$, $5$)</td>
<td>Weights for the five neurons in the second layer</td>
</tr>
<tr>
<td>b2</td>
<td>($5$)</td>
<td>Biases for the five neurons in the second layer</td>
</tr>
<tr>
<td>W3</td>
<td>($5$, $1$)</td>
<td>Weights for the single neuron in the output layer</td>
</tr>
<tr>
<td>b3</td>
<td>($1$)</td>
<td>Bias for the single neurons in the output layer</td>
</tr>
</tbody>
</table>
<p>We begin by creating our synthetic dataset (and acknowledging how contrived this example is – we’re using the very function that we’re trying to learn to generate them!) and initialising the parameters of our network. We then train the model, by performing the following steps in a loop:</p>
<ol>
<li>Perform a forward pass of our inputs through the network.</li>
<li>Compute the training loss at the current iteration.</li>
<li>Backpropagate through that loss.</li>
<li>Update each param by subtracting a small fraction of its gradient (with respect to the loss), and then resetting its gradient for the next iteration.</li>
</ol>
<p>After training is complete, we evaluate our model against our held back data, and print out some statistics, as well as some examples of false positive and negative classifications.</p>
<p>The complete code listing follows:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>typing</span>
</span></span><span><span>
</span></span><span><span><span>import</span> <span>torch</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>Dataset</span> <span>=</span> <span>tuple</span><span>[</span><span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span><span>]</span>
</span></span><span><span>
</span></span><span><span><span>NeuralNetwork</span> <span>=</span> <span>tuple</span><span>[</span>
</span></span><span><span>    <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>torch</span><span>.</span><span>Tensor</span>
</span></span><span><span><span>]</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>generate_datasets</span><span>(</span><span>generator</span><span>:</span> <span>torch</span><span>.</span><span>Generator</span><span>)</span> <span>-&gt;</span> <span>Dataset</span><span>:</span>
</span></span><span><span>    <span>X</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>(</span><span>10000</span><span>,</span> <span>2</span><span>,</span> <span>generator</span><span>=</span><span>generator</span><span>)</span> <span>*</span> <span>10.0</span> <span>-</span> <span>5.0</span>
</span></span><span><span>    <span>Y</span> <span>=</span> <span>((</span><span>X</span><span>**</span><span>2</span><span>)</span><span>.</span><span>sum</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span> <span>&lt;</span> <span>1.0</span><span>)</span><span>.</span><span>float</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>P</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>(</span><span>10000</span><span>,</span> <span>generator</span><span>=</span><span>generator</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>X_train</span> <span>=</span> <span>X</span><span>[</span><span>P</span> <span>&lt;</span> <span>0.8</span><span>]</span>
</span></span><span><span>    <span>Y_train</span> <span>=</span> <span>Y</span><span>[</span><span>P</span> <span>&lt;</span> <span>0.8</span><span>]</span>
</span></span><span><span>
</span></span><span><span>    <span>X_dev</span> <span>=</span> <span>X</span><span>[</span><span>P</span> <span>&gt;=</span> <span>0.8</span><span>]</span>
</span></span><span><span>    <span>Y_dev</span> <span>=</span> <span>Y</span><span>[</span><span>P</span> <span>&gt;=</span> <span>0.8</span><span>]</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>X_train</span><span>,</span> <span>Y_train</span><span>,</span> <span>X_dev</span><span>,</span> <span>Y_dev</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>init_network</span><span>(</span><span>generator</span><span>)</span> <span>-&gt;</span> <span>NeuralNetwork</span><span>:</span>
</span></span><span><span>    <span>W1</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>2</span><span>,</span> <span>10</span><span>,</span> <span>generator</span><span>=</span><span>generator</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>    <span>b1</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>10</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>W2</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>10</span><span>,</span> <span>5</span><span>,</span> <span>generator</span><span>=</span><span>generator</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>    <span>b2</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>5</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>W3</span> <span>=</span> <span>torch</span><span>.</span><span>randn</span><span>(</span><span>5</span><span>,</span> <span>1</span><span>,</span> <span>generator</span><span>=</span><span>generator</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>    <span>b3</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>1</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>W1</span><span>,</span> <span>b1</span><span>,</span> <span>W2</span><span>,</span> <span>b2</span><span>,</span> <span>W3</span><span>,</span> <span>b3</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>forward_pass</span><span>(</span><span>params</span><span>:</span> <span>NeuralNetwork</span><span>,</span> <span>X</span><span>:</span> <span>torch</span><span>.</span><span>Tensor</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>Tensor</span><span>:</span>
</span></span><span><span>    <span>W1</span><span>,</span> <span>b1</span><span>,</span> <span>W2</span><span>,</span> <span>b2</span><span>,</span> <span>W3</span><span>,</span> <span>b3</span> <span>=</span> <span>params</span>
</span></span><span><span>    <span>Z1</span> <span>=</span> <span>(</span><span>X</span> <span>@</span> <span>W1</span> <span>+</span> <span>b1</span><span>)</span><span>.</span><span>relu</span><span>()</span>
</span></span><span><span>    <span>Z2</span> <span>=</span> <span>(</span><span>Z1</span> <span>@</span> <span>W2</span> <span>+</span> <span>b2</span><span>)</span><span>.</span><span>relu</span><span>()</span>
</span></span><span><span>    <span>Z3</span> <span>=</span> <span>(</span><span>Z2</span> <span>@</span> <span>W3</span> <span>+</span> <span>b3</span><span>)</span><span>.</span><span>sigmoid</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>Z3</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>mean_squared_error</span><span>(</span><span>A</span><span>,</span> <span>Y</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>((</span><span>A</span> <span>-</span> <span>Y</span><span>)</span> <span>**</span> <span>2</span><span>)</span><span>.</span><span>squeeze</span><span>()</span><span>.</span><span>mean</span><span>()</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>fit</span><span>(</span><span>params</span><span>:</span> <span>NeuralNetwork</span><span>,</span> <span>X</span><span>:</span> <span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>Y</span><span>:</span> <span>torch</span><span>.</span><span>Tensor</span><span>):</span>
</span></span><span><span>    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>1000</span><span>):</span>
</span></span><span><span>        <span>Z</span> <span>=</span> <span>forward_pass</span><span>(</span><span>params</span><span>,</span> <span>X</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>L</span> <span>=</span> <span>mean_squared_error</span><span>(</span><span>Z</span><span>,</span> <span>Y</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>L</span><span>.</span><span>backward</span><span>()</span>
</span></span><span><span>
</span></span><span><span>        <span>for</span> <span>param</span> <span>in</span> <span>params</span><span>:</span>
</span></span><span><span>            <span>param</span><span>.</span><span>data</span> <span>-=</span> <span>0.5</span> <span>*</span> <span>typing</span><span>.</span><span>cast</span><span>(</span><span>torch</span><span>.</span><span>Tensor</span><span>,</span> <span>param</span><span>.</span><span>grad</span><span>)</span>
</span></span><span><span>            <span>param</span><span>.</span><span>grad</span> <span>=</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span><span>:</span>
</span></span><span><span>    <span>generator</span> <span>=</span> <span>torch</span><span>.</span><span>Generator</span><span>()</span>
</span></span><span><span>    <span>generator</span><span>.</span><span>manual_seed</span><span>(</span><span>1337</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>X_train</span><span>,</span> <span>Y_train</span><span>,</span> <span>X_dev</span><span>,</span> <span>Y_dev</span> <span>=</span> <span>generate_datasets</span><span>(</span><span>generator</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>params</span> <span>=</span> <span>init_network</span><span>(</span><span>generator</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>fit</span><span>(</span><span>params</span><span>,</span> <span>X_train</span><span>,</span> <span>Y_train</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>with</span> <span>torch</span><span>.</span><span>no_grad</span><span>():</span>
</span></span><span><span>        <span>Y_dev_hat</span> <span>=</span> <span>forward_pass</span><span>(</span><span>params</span><span>,</span> <span>X_dev</span><span>)</span>
</span></span><span><span>        <span>dev_loss</span> <span>=</span> <span>mean_squared_error</span><span>(</span><span>Y_dev_hat</span><span>,</span> <span>Y_dev</span><span>)</span>
</span></span><span><span>        <span>Y_dev_hat</span> <span>=</span> <span>(</span><span>Y_dev_hat</span> <span>&gt;</span> <span>0.5</span><span>)</span><span>.</span><span>float</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>tp</span> <span>=</span> <span>(</span><span>Y_dev</span> <span>==</span> <span>1.0</span><span>)</span> <span>&amp;</span> <span>(</span><span>Y_dev_hat</span> <span>==</span> <span>1.0</span><span>)</span>
</span></span><span><span>    <span>fp</span> <span>=</span> <span>(</span><span>Y_dev</span> <span>==</span> <span>0.0</span><span>)</span> <span>&amp;</span> <span>(</span><span>Y_dev_hat</span> <span>==</span> <span>1.0</span><span>)</span>
</span></span><span><span>    <span>tn</span> <span>=</span> <span>(</span><span>Y_dev</span> <span>==</span> <span>0.0</span><span>)</span> <span>&amp;</span> <span>(</span><span>Y_dev_hat</span> <span>==</span> <span>0.0</span><span>)</span>
</span></span><span><span>    <span>fn</span> <span>=</span> <span>(</span><span>Y_dev</span> <span>==</span> <span>1.0</span><span>)</span> <span>&amp;</span> <span>(</span><span>Y_dev_hat</span> <span>==</span> <span>0.0</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>true_positives</span> <span>=</span> <span>tp</span><span>.</span><span>float</span><span>()</span><span>.</span><span>sum</span><span>()</span><span>.</span><span>item</span><span>()</span>
</span></span><span><span>    <span>false_positives</span> <span>=</span> <span>fp</span><span>.</span><span>float</span><span>()</span><span>.</span><span>sum</span><span>()</span><span>.</span><span>item</span><span>()</span>
</span></span><span><span>    <span>true_negatives</span> <span>=</span> <span>tn</span><span>.</span><span>float</span><span>()</span><span>.</span><span>sum</span><span>()</span><span>.</span><span>item</span><span>()</span>
</span></span><span><span>    <span>false_negatives</span> <span>=</span> <span>fn</span><span>.</span><span>float</span><span>()</span><span>.</span><span>sum</span><span>()</span><span>.</span><span>item</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span>false_positive_samples</span> <span>=</span> <span>X_dev</span><span>[</span><span>fp</span><span>.</span><span>squeeze</span><span>(),</span> <span>:]</span>
</span></span><span><span>    <span>false_negative_samples</span> <span>=</span> <span>X_dev</span><span>[</span><span>fn</span><span>.</span><span>squeeze</span><span>(),</span> <span>:]</span>
</span></span><span><span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>dev_loss</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>true_positives</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>false_positives</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>true_negatives</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>false_negatives</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>false_positive_samples</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>false_negative_samples</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span></code></pre></div><p>When running, we see the following output:</p>
<pre tabindex="0"><code>dev_loss=tensor(0.0027)
true_positives=45.0
false_positives=1.0
true_negatives=1965.0
false_negatives=4.0
false_positive_samples=tensor([[-0.5229, -0.9005]])
false_negative_samples=tensor([[-0.8656,  0.3219],
        [ 0.8683,  0.4416],
        [ 0.6218,  0.7571],
        [-0.9056, -0.2783]])
</code></pre><p>The loss is pleasingly low, we have few false positives and negatives, and the ones we do have are close enough to the unit circle that we can forgive our model these minor transgressions.</p>
<h3 id="conclusion">Conclusion</h3>
<p>It’s worth repeating that the example given above is very much contrived – it solves a simple problem (and likely overfits!), on a tiny dataset, that runs in a few seconds on contemporary hardware. Nevertheless, I found putting it together helped solidify some key concepts, particularly around the dimensions involved in a network’s weights and biases.</p>




            </div></div>
  </body>
</html>
