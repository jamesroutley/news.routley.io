<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jeffgeerling.com/blog/2025/4x-faster-network-file-sync-rclone-vs-rsync/">Original</a>
    <h1>4x faster network file sync with rclone (vs rsync) (2025)</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><div><section><p>For the past couple years, I have transported my &#39;working set&#39; of video and project data to and from work on an external Thunderbolt NVMe SSD.</p><p>But it&#39;s always been <em>slow</em> when I do the sync. In a typical day, I may generate a new project folder with 500-1000 individual files, and dozens of them may be 1-10 GB in size.</p><p>The Thunderbolt drive I had was capable of well over 5 GB/sec, and my 10 Gbps network connection is capable of 1 GB/sec. I even <a href="https://www.youtube.com/watch?v=gaV-O6NPWrI">upgraded my Thunderbolt drive to Thunderbolt 5 lately</a>... though that was not the bottleneck.</p><p>I used the following rsync command to copy files from a network share mounted on my Mac to the drive (which I call &#34;Shuttle&#34;):</p><div><pre tabindex="0"><code data-lang="fallback"><span><span>rsync -au --progress --stats /Volumes/mercury/* /Volumes/Shuttle/Video_Projects
</span></span></code></pre></div><p><code>mercury</code> is so named because it&#39;s a fast NVMe-backed NAS volume on my <a href="https://www.jeffgeerling.com/blog/2024/building-efficient-server-grade-arm-nas">Arm NAS</a> (all my network volumes are named after celestial bodies).</p><p>As a test, I deleted one of the dozen or so active projects off my &#39;Shuttle&#39; drive, and ran my <code>rsync</code> copy:</p><div><pre tabindex="0"><code data-lang="fallback"><span><span>$ time rsync -au --progress --stats /Volumes/mercury/* /Volumes/Shuttle/Video_Projects
</span></span><span><span>Radxa Orion O6/
</span></span><span><span>Radxa Orion O6/.DS_Store
</span></span><span><span>           6148 100%    4.80MB/s   00:00:00 (xfer#1, to-check=1582/3564)
</span></span><span><span>Radxa Orion O6/Micro Center Visit Details.pages
</span></span><span><span>         141560 100%    9.83MB/s   00:00:00 (xfer#2, to-check=1583/3564)
</span></span><span><span>Radxa Orion O6/Radxa Orion O6.md
</span></span><span><span>          19817 100%    1.89MB/s   00:00:00 (xfer#3, to-check=1584/3564)
</span></span><span><span>Radxa Orion O6/BIOS and Images/
</span></span><span><span>Radxa Orion O6/BIOS and Images/orion-o6-bios-0.2.2-1.zip
</span></span><span><span>        3916964 100%   83.32MB/s   00:00:00 (xfer#4, to-check=1586/3564)
</span></span><span><span>Radxa Orion O6/BIOS and Images/orion-o6-bios-9.0.0-apr-11.7z
</span></span><span><span>        4341505 100%  112.62MB/s   00:00:00 (xfer#5, to-check=1587/3564)
</span></span><span><span>Radxa Orion O6/Scratch/
</span></span><span><span>Radxa Orion O6/Scratch/bios 9.0.0 - screen acpi and debian 12 attempt.mp4
</span></span><span><span>      254240026 100%  114.11MB/s   00:00:02 (xfer#6, to-check=1589/3564)
</span></span><span><span>...
</span></span><span><span>Number of files: 3564
</span></span><span><span>Number of files transferred: 122
</span></span><span><span>Total file size: 244284287846 B
</span></span><span><span>Total transferred file size: 62947785101 B
</span></span><span><span>Unmatched data: 62947785101 B
</span></span><span><span>Matched data: 0 B
</span></span><span><span>File list size: 444318 B
</span></span><span><span>File list generation time: 9.155 seconds
</span></span><span><span>File list transfer time: 0.078 seconds
</span></span><span><span>Total sent: 62955918871 B
</span></span><span><span>Total received: 2728 B
</span></span><span><span>
</span></span><span><span>sent 62955918871 bytes  received 2728 bytes  128990035 bytes/sec
</span></span><span><span>total size is 244284287846  speedup is 3.88
</span></span><span><span>
</span></span><span><span>real	8:17.57
</span></span><span><span>user	3:13.14
</span></span><span><span>sys	2:45.45
</span></span></code></pre></div><p>The full copy took over 8 minutes, for a total of about 59 GiB of files copied. There are two problems:</p><ol><li><code>rsync</code> performs copies single-threaded, <em>serially</em>, meaning only one file is copied at a time</li><li>Even for very large files, <code>rsync</code> seems to max out on this network share around 350 MB/sec</li></ol><p>I had been playing with different compression algorithms, trying to <code>tar</code> then pipe that to <code>rsync</code>, even experimenting with running the <code>rsync</code> daemon instead of SSH... but never could I get a <em>significant</em> speedup! In fact, some compression modes would actually slow things down as my energy-efficient NAS is running on some slower Arm cores, and they bog things down a bit single-threaded...</p><h2 id="rclone-to-the-rescue"><code>rclone</code> to the rescue</h2><p>I&#39;ve been using <code>rclone</code> as part of my <a href="https://www.jeffgeerling.com/blog/2021/my-backup-plan">3-2-1 backup plan</a> for years. It&#39;s amazing at copying, moving, and syncing files from and to almost any place (including Cloud storage, local storage, NAS volumes, etc.), but I had somehow pigeonholed it as &#34;for cloud to local or vice-versa&#34;, and never considered it for <em>local</em> transfer, like over my own LAN.</p><p>But it has an option that allows transfers in parallel, <code>--multi-thread-streams</code>, which <a href="https://stackoverflow.com/a/62460707">Stack Overflow user <code>dantebarba</code> suggested</a> someone use in the same scenario.</p><p>So I gave it a try.</p><p>After fiddling a bit with the exact parameters to match rsync&#39;s <code>-a</code>, and handling the weird symlinks like <code>.fcpcache</code> directories Final Cut Pro spits out inside project files, I came up with:</p><div><pre tabindex="0"><code data-lang="fallback"><span><span>rclone sync \
</span></span><span><span>  --exclude=&#39;**/._*&#39; \
</span></span><span><span>  --exclude=&#39;.fcpcache/**&#39; \
</span></span><span><span>  --multi-thread-streams=32 \
</span></span><span><span>  -P -L --metadata \
</span></span><span><span>  /Volumes/mercury/ /Volumes/Shuttle/Video_Projects
</span></span></code></pre></div><p>Using this method, I could see my Mac&#39;s network connection quickly max out around 1 GB/sec, completing the same directory copy in 2 minutes:</p><div><pre tabindex="0"><code data-lang="fallback"><span><span>$ rclone sync \                                                                       
</span></span><span><span>  --exclude=&#39;**/._*&#39; \
</span></span><span><span>  --exclude=&#39;.fcpcache/**&#39; \
</span></span><span><span>  --multi-thread-streams=32 \
</span></span><span><span>  --progress --links --metadata \
</span></span><span><span>  /Volumes/mercury/ /Volumes/Shuttle/Video_Projects
</span></span><span><span>2025/05/06 12:03:57 NOTICE: Config file &#34;/Users/jgeerling/.config/rclone/rclone.conf&#34; not found - using defaults
</span></span><span><span>Transferred:   	   58.625 GiB / 58.625 GiB, 100%, 0 B/s, ETA -
</span></span><span><span>Checks:              2503 / 2503, 100%
</span></span><span><span>Transferred:          122 / 122, 100%
</span></span><span><span>Server Side Copies:   122 @ 58.625 GiB
</span></span><span><span>Elapsed time:      2m15.3s
</span></span></code></pre></div><p><s>I&#39;m not 100% sure why <code>rclone</code> says 59 GB were copied, versus <code>rsync</code>&#39;s 63 GB. Probably the exclusion of the <code>.fcpcache</code> directory?</s> lol units... GiB vs GB ;)</p><p>But the conclusion—especially after seeing my 10 Gbps connection <em>finally</em> being fully utilized—is that <code>rclone</code> is about 4x faster working in parallel.</p><p>I also ran comparisons just changing out a couple files, and <code>rclone</code> and <code>rsync</code> were almost identical, as the full scan of the directory tree for metadata changes takes about the same time on both (about 18 seconds). It&#39;s just the parallel file transfers that help <code>rclone</code> pull ahead.</p></section></div></div></article></div></div>
  </body>
</html>
