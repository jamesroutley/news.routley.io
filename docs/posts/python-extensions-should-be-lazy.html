<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gauge.sh/blog/python-extensions-should-be-lazy">Original</a>
    <h1>Python extensions should be lazy</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><div data-w-id="5b8e0128-dfc2-fd07-77df-30819f5d5004"><p>I recently had to optimize a Python tool which made a lot of calls to <code>ast.parse</code>. On a codebase with about 500k LoC, these calls alone took around 8 seconds! This was pretty shocking - the <code>ast</code> module is written in C, and 500k LoC is not particularly large.</p><p>So why was the C implementation so slow? The short answer is that Python is forcing it to be slow. For the long answer, we’ll have to dig into the source.</p><h2><strong>Why is C so slow?</strong></h2><p>The first thing to recognize is that the <code>ast</code> module fundamentally has more work to do than just a C program parsing Python ASTs. Not only does it need to tokenize the Python source and then build the AST, <strong>it must also return that AST to the Python interpreter.</strong></p><p>Let’s look at some of the relevant source code from <a href="https://github.com/python/cpython/blob/967a4f1d180d4cd669d5c6e3ac5ba99af4e72d4e/Python/pythonrun.c#L1425-L1453">CPython</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/665a5f120c4c63df1944d627/66b2fd89e9065aa8843c5762_666b78b0949d1a81e01cda5c_Py_CompileStringObject.png" loading="lazy" alt=""/></p><figcaption>This function turns Python source code into an ast.AST object</figcaption></figure><p>If you haven’t read much C (like me), it’s probably not clear what you’re looking at. The important points to see here are:<code>‍</code></p><p><code>- _PyParser_ASTFromString</code> turns source code into a native <code>mod_ty</code> struct <strong>[C -&gt; C]</strong> </p><p><code>- PyAST_mod2obj</code> turns the <code>mod_ty</code> struct into a <code>PyObject</code> <strong>[C -&gt; Python]</strong> </p><p>- We can ignore the final three lines since we will have the <code>PyCF_ONLY_AST</code> flag set when calling <code>ast.parse</code></p><p>It’s critical to note here that <code>PyAST_mod2obj</code> translates the <strong>entire AST immediately</strong>. This means every node in the source becomes a <code>PyObject</code>, along with every value that each node holds.</p><p>When I run this through Valgrind with the 500k LoC as input, the results show a massive number of memory allocations (<code>malloc</code>) and a significant amount of time spent on garbage collection. This is when ‘<a href="https://docs.python.org/3/reference/datamodel.html#objects-values-and-types">everything is an object</a>’ really bites you.</p><figure><p><img src="https://cdn.prod.website-files.com/665a5f120c4c63df1944d627/66b2fd89e9065aa8843c576b_666b7903836f04ab2b2f0304_python_callgrind_old_get_project_imports.png" loading="lazy" alt=""/></p><figcaption>Valgrind output showing ~3k GCs, ~59M mallocs</figcaption></figure><h2><strong>The Solution</strong></h2><p>Given the findings above, I moved all of the AST-dependent operations into a Rust extension. This avoided the need to build a Python-compatible AST entirely, and instead kept all the data in compact Rust structs. In other words, this Rust extension was processing the ASTs in Rust-land and <strong>only returning the final results to Python</strong>.</p><p>After the rewrite, the cumulative runtime of the relevant code went from <strong>8.7s</strong> to <strong>530ms</strong> (a <strong>16x speedup</strong>).</p><p>With Valgrind we can also see that this got rid of the massive GC pauses, and dramatically reduced the memory pressure. While the original version made almost 60M malloc calls and spent 35% of its cycles on garbage collection, the new implementation shows no significant GC activity and makes only ~7M malloc calls.</p><figure><p><img src="https://cdn.prod.website-files.com/665a5f120c4c63df1944d627/66b2fd89e9065aa8843c5766_666b7a3b3855f4decaa412eb_rust_callgrind_cycle_estimation.png" loading="lazy" alt=""/></p><figcaption>~7M mallocs, no GC pauses</figcaption></figure><h2>Laziness</h2><p>So what does this have to do with laziness?</p><p>Pushing data into Python’s memory model is a performance bottleneck. It follows that an approach like <a href="https://numpy.org/doc/1.21/reference/internals.html">numpy’s</a> is likely worth exploring for more Python extensions. In numpy, the underlying data behind objects like <code>np.array</code> is stored in contiguous blocks of memory owned by the compiled C extension. Python lists, dicts, and even numbers are created only when explicitly built by the end-user.</p><p>In the case of ASTs, one could imagine a kind of ‘query language’ API for Python that operates on data that is owned by the extension - analogous to SQL over the highly specialized binary representations that a database would use. This would let the extension own the memory, and would lazily create Python objects when necessary.</p><figure><p><img src="https://cdn.prod.website-files.com/665a5f120c4c63df1944d627/66b39b97e074c5cc060603f6_66b39b80b4fb395218cad63a_Screenshot%2520from%25202024-08-07%252009-05-54.png" loading="lazy" alt=""/></p><figcaption>AST query engine diagram</figcaption></figure><p>A compiled extension for Python is much faster when it can lazily build <code>PyObjects</code> only when necessary - keeping as much of the data compactly within its own language as possible.</p><p>‍</p></div></div></div></section></div>
  </body>
</html>
