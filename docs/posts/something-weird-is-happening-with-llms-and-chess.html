<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dynomight.net/chess/">Original</a>
    <h1>Something weird is happening with LLMs and Chess</h1>
    
    <div id="readability-page-1" class="page"><section>
    <p>A year ago, there was a lot of talk about large language models (LLMs) playing chess. Word was that if you trained a big enough model on enough text, then you could send it a partially played game, ask it to predict the next move, and it would play at the level of an advanced amateur.</p>

<p>This seemed important. These are “language” models, after all, designed to predict <em>language</em>.</p>

<p>Now, modern LLMs are trained on a sizeable fraction of all the text ever created. This surely includes many chess games. But they weren’t <em>designed</em> to be good at chess. And the games that are available are just <em>lists of moves</em>. Yet people found that LLMs could play all the way through to the end game, with never-before-seen boards.</p>

<p>Did the language models build up some kind of internal representation of board state? And how to construct that state from lists of moves in chess’s <a href="https://en.wikipedia.org/wiki/Portable_Game_Notation">extremely confusing notation</a>? And how valuable different pieces and positions are? And how to force checkmate in an end-game? And they did this all “by accident”, as part of their goal of predicting general text?</p>

<p>If language models can do all that for chess, then maybe it’s a hint of how they deal with other situations too.</p>

<p>So that was very exciting. A year ago.</p>

<p>Since then, there’s mostly been silence. So I decided to check in and see how things are going. Having done that, I can now report: Weirdly.</p>

<h2 id="what-i-did">What I did</h2>

<p>To make LLMs play chess, I sent them prompts like this:</p>

<div><div><pre><code>You are a chess grandmaster.
Please choose your next move.
Use standard algebraic notation, e.g. &#34;e4&#34; or &#34;Rdf8&#34; or &#34;R1a3&#34;.
NEVER give a turn number.
NEVER explain your choice.
Here is a representation of the position:

[Event &#34;Shamkir Chess&#34;]
[White &#34;Anand, Viswanathan&#34;]
[Black &#34;Topalov, Veselin&#34;]
[Result &#34;1-0&#34;]
[WhiteElo &#34;2779&#34;]
[BlackElo &#34;2740&#34;]

1. e4 e6 2. d3 c5 3. Nf3 Nc6 4. g3 Nf6 5.
</code></pre></div></div>

<p>I used the output as a move. I always had the LLM play as white against Stockfish—a standard chess AI—on the lowest difficulty setting.</p>

<p>The first model I tried was <a href="https://huggingface.co/meta-llama/Llama-3.2-3B"><code>llama-3.2-3b</code></a>. This is a “base model”, meaning it is mostly trained to output text, not to chat with you or obey instructions. It’s quite small by modern standards, with only 3 billion parameters. For reference, GPT-2, released back in 2019, had 1.5 billion parameters, and GPT-4 is rumored to have around 1.8 <em>trillion.</em></p>

<p>I had it play 50 games, then had a chess engine score each board after each turn in “centipawns”. This is a measure where a pawn is 100 points, but there’s also accounting for position. If the game was over, I assigned a score of +1500 if the LLM won, 0 if there was a tie, and -1500 if it lost.</p>

<p>The results were:</p>

<p><a href="https://dynomight.net/img/chess/llama-3.2-3b_stockfish.pdf"><img src="https://dynomight.net/img/chess/llama-3.2-3b_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/AAsQ2Ydi">Terrible</a>. (Click to see a game. Click the above image to zoom in.)</p>

<p>In the above figure, there’s one light line for each game, and the black line shows the per-turn median. The LLM can play standard openings for a few moves but then quickly starts throwing away pieces. It lost every single game, even though Stockfish was on the lowest setting.</p>

<p>Maybe that model is too small? So I got <a href="https://huggingface.co/meta-llama/Llama-3.1-70B"><code>llama-3.1-70b</code></a>, which is a similar model but with 70 billion parameters instead of 3 billion. The results were:</p>

<p><a href="https://dynomight.net/img/chess/llama-3.1-70b_stockfish.pdf"><img src="https://dynomight.net/img/chess/llama-3.1-70b_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/56nPlnP3">Terrible</a>. A <em>little</em> better, but still extremely bad.</p>

<p>Next I tried <code>llama-3.1-70b-instruct</code>, a similar model, except trained to be better at following instructions. The results were:</p>

<p><a href="https://dynomight.net/img/chess/llama-3.1-70b-instruct_stockfish.pdf"><img src="https://dynomight.net/img/chess/llama-3.1-70b-instruct_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/uJu3RKRR">Terrible</a>.</p>

<p>Maybe there’s something wrong with the Llama models or datasets? So I tried <a href="https://huggingface.co/Qwen/Qwen2.5-72B"><code>Qwen-2.5-72b</code></a>.</p>

<p><a href="https://dynomight.net/img/chess/qwen-2.5-72b_stockfish.pdf"><img src="https://dynomight.net/img/chess/qwen-2.5-72b_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/5ZMxoM3T">Terrible.</a></p>

<p>Maybe Qwen is somehow defective too? So I tried <a href="https://huggingface.co/CohereForAI/c4ai-command-r-v01"><code>command-r-v01</code></a>, a 35 billion parameter model.</p>

<p><a href="https://dynomight.net/img/chess/command-r-v01_stockfish.pdf"><img src="https://dynomight.net/img/chess/command-r-v01_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/TrkfFSTA">Terrible.</a></p>

<p>And then I tried <a href="https://huggingface.co/google/gemma-2-27b"><code>gemma-2-27b</code></a>.</p>

<p><a href="https://dynomight.net/img/chess/gemma-2-27b_stockfish.pdf"><img src="https://dynomight.net/img/chess/gemma-2-27b_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/FeNVh8no">Terrrible.</a></p>

<p>And then I tried <code>gpt-3.5-turbo-instruct</code>. This is a closed OpenAI model, so details are very murky. I only ran 10 trials since AI companies have inexplicable neglected to send me free API keys and this was costing <a href="https://dynomight.net/automated/">The Automator</a> money. The results were:</p>

<p><a href="https://dynomight.net/img/chess/gpt-3.5-turbo-instruct_stockfish.pdf"><img src="https://dynomight.net/img/chess/gpt-3.5-turbo-instruct_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/4CCCdinZ">Excellent</a>. Very, <em>very</em>, good.</p>

<p>Even if you raise Stockfish’s level a few clicks, this model will still win every game.</p>

<p>Moving on…  I next tried <code>gpt-3.5-turbo</code>, a model that’s similar, except tuned to be more chatty and conversational.</p>

<p><a href="https://dynomight.net/img/chess/gpt-3.5-turbo_stockfish.pdf"><img src="https://dynomight.net/img/chess/gpt-3.5-turbo_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/Q7QwEBcn">Terrible</a>.</p>

<p>And then I tried <code>gpt-4o-mini</code>, which is a newer chat model.</p>

<p><a href="https://dynomight.net/img/chess/gpt-4o-mini_stockfish.pdf"><img src="https://dynomight.net/img/chess/gpt-4o-mini_stockfish.svg" alt=""/></a></p>

<p><a href="https://lichess.org/z9PyJVSE">Terrible</a>.</p>

<p>And then I tried <code>gpt-4o</code>, a bigger chat model.</p>

<p><a href="https://dynomight.net/img/chess/gpt-4o_stockfish.pdf"><img src="https://dynomight.net/img/chess/gpt-4o_stockfish.svg" alt="gpt-4o"/></a></p>

<p><a href="https://lichess.org/govbFo94">Terrible</a>.</p>

<p>It lost every single game, though it lost slightly slower.</p>

<p>Finally, I tried <code>o1-mini</code>, a model that’s supposed to be able to solve complex tasks. (I’m too poor for <code>o1</code>.)</p>

<p><a href="https://dynomight.net/img/chess/o1-mini_stockfish.pdf"><img src="https://dynomight.net/img/chess/o1-mini_stockfish.svg" alt="o1-mini"/></a></p>

<p><a href="https://lichess.org/d44jrD2E">Terrible</a>.</p>

<p>So, umm:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Llama-3.2-3b</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>Llama-3.2-3b-instruct</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>Llama-3.1-70b</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>Llama-3.1-70b-instruct</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>Qwen-2.5</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>command-r-v01</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>gemma-2-27b</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>gemma-2-27b-it</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>gpt-3.5-turbo-instruct</code></td>
      <td>Excellent</td>
    </tr>
    <tr>
      <td><code>gpt-3.5-turbo</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>gpt-4o-mini</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>gpt-4o</code></td>
      <td>Terrible</td>
    </tr>
    <tr>
      <td><code>o1-mini</code></td>
      <td>Terrible</td>
    </tr>
  </tbody>
</table>

<p>And, uh:</p>

<p><a href="https://dynomight.net/img/chess/all_mean.pdf"><img src="https://dynomight.net/img/chess/all_mean.svg" alt="all"/></a></p>

<p>Notice anything? Any patterns jump out at you?</p>

<h2 id="discussion">Discussion</h2>

<p>There are lots of people on the internet who have tried to get LLMs to play chess. The history seems to go something like this:</p>

<ul>
  <li>
    <p><strong>Before September 2023</strong>: Wow, recent LLMs can sort of play chess! They fall apart after the early game, but they can do something! Amazing!</p>
  </li>
  <li>
    <p><strong>September-October 2023</strong>: Wow! LLMs can now play chess at an advanced amateur level! Amazing!</p>
  </li>
  <li>
    <p><strong>(Year of silence.)</strong></p>
  </li>
  <li>
    <p><strong>Recently:</strong> Wow, recent LLMs can sort of play chess! They fall apart after the early game, but they can do something! Amazing!</p>
  </li>
</ul>

<p>I can only assume that lots of other people are experimenting with recent models, getting terrible results, and then mostly not saying anything. I haven’t seen anyone say explicitly that only <code>gpt-3.5-turbo-instruct</code> is good at chess. No other LLM is remotely close.</p>

<p>To be fair, a year ago, many people did notice that <code>gpt-3.5-turbo-instruct</code> was much better than <code>gpt-3.5-turbo</code>. Many speculated at the time that this is because <code>gpt-3.5-turbo</code> was subject to additional tuning to be good at chatting.</p>

<p>That might be true. Here’s a comparison of three models where we have similar versions with or without additional chat tuning.</p>

<p><a href="https://dynomight.net/img/chess/instruct_mean.pdf"><img src="https://dynomight.net/img/chess/instruct_mean.svg" alt="instruct comparison"/></a></p>

<p>(Again, do not be confused by the name <code>gpt-3.5-turbo-instruct</code>. I stress that this is more like a base model than <code>gpt-3.5-turbo</code>. This is the opposite of the naming scheme everyone else uses where “<code>instruct</code>” or “<code>it</code>” means more tuning to be good at chatting.)</p>

<p>In all cases, additional instruction tuning makes the model worse. But the difference is very small in two cases, and enormous in the other.</p>

<h2 id="possible-theories">Possible theories</h2>

<p>I can think of four possible explanations.</p>

<p><strong>Theory 1: Base models <em>at sufficient scale</em> can play chess, but instruction tuning destroys it.</strong></p>

<p>This would be consistent with our data. But I did manage to get <a href="https://huggingface.co/meta-llama/Llama-3.1-405B"><code>llama-3.1-405b</code></a> to play a couple games. Despite being larger than <code>gpt-3.5-turbo</code>, it was still terrible.</p>

<p><strong>Theory 2: GPT-3.5-instruct was trained on more chess games.</strong></p>

<p>All models were clearly trained on a <em>lot</em> of chess games. But it’s hard to know exactly how many.</p>

<p><strong>Theory 3: There’s something particular about different transformer architectures.</strong></p>

<p>I doubt this, but it could be that for some reason, Llama type models are uniquely bad at chess.</p>

<p><strong>Theory 4: There’s “competition” between different types of data.</strong></p>

<p>We know that transformers trained specifically on chess games can be <a href="https://github.com/sgrvinod/chess-transformers">extremely good</a> at chess. Maybe <code>gpt-3.5-turbo-instruct</code> happens to have been trained on a higher <em>fraction</em> of chess games, so it decided to dedicate a larger fraction of its parameters to chess.</p>

<p>That is, maybe LLMs sort of have little “chess subnetworks” hidden inside of them, but the size of the subnetworks depends on the fraction of data that was chess. (If this theory were true, we should probably expect that <em>big enough</em> models should become good at chess, provided they are trained on enough chess games, even if the <em>fraction</em> of chess games is low.)</p>

<h2 id="details">Details</h2>

<p>I did things this way (i.e. by working with standard algebraic notation) because this is how people got good results two years ago, and in preliminary experiments I also found it to work best.</p>

<p>If you want to know <em>exactly</em> how I did things, here are some words: I ran all the open models (anything <em>not</em> from  OpenAI, meaning anything that doesn’t start with <code>gpt</code> or <code>o1</code>) myself using <code>Q5_K_M</code> quantization, whatever that is. For the open models I manually generated the set of legal moves and then used <a href="https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md">grammars</a> to constrain the models, so they always generated legal moves. Since OpenAI is lame and doesn’t support full grammars, for the closed (OpenAI) models I tried generating up to 10 times and if it still couldn’t come up with a legal move, I just chose one randomly. For the chat models <code>llama-3.1-70b-instruct</code>, <code>gpt-3.5-turbo</code>, <code>gpt-4o-mini</code>, and <code>gpt-4o</code> I changed the system prompt to “You are a chess grandmaster. You will be given a partially completed game. After seeing it, you should choose the next move.” It’s impossible to change the system prompt for <code>o1-mini</code>, so I didn’t. I used a temperature of 0.7 for all the open models and the default for the closed (OpenAI) models. The fact that OpenAI has “open” as part of their name sure made this paragraph hard to write.</p>

<h2 id="token-weirdness">Token weirdness</h2>

<p>One <em>extremely</em> strange thing I noticed was that if I gave a prompt like “<code>1. e4 e5 2. </code>” (with a space at the end), the open models would play much, <em>much</em> worse than if I gave a prompt like “<code>1 e4 e5 2.</code>” (without a space) and let the model generate the space itself. Huh?</p>

<p>After some confusion, I’m pretty sure this is because of the tokenizer. Look at how the <a href="https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/">Llama tokenizer</a> breaks up a string of moves:</p>

<p><img src="https://dynomight.net/img/chess/tokens.png" alt="tokens"/></p>

<p>After the “<code>1.</code>”, it generates “<code> e</code>” as a <em>single token</em>. That’s not the same as having a space followed by an <code>e</code>. So putting in the space and asking models to generate tokens presents the model with a confusing situation and lead to bad predictions.</p>

<p>The right way to deal with this is “token healing”—to delete the last token of the input and then do constrained generation over all strings that start with the deleted stuff. But I couldn’t figure out any easy way to do that. So, instead I left the space out and modified the grammar so that the model could generate a space (or not), then one of the current legal moves, and then another space (or not).</p>

<h2 id="ps">P.S.</h2>

<p>Some people have asked to see all the games from <code>gpt-3.5-turbo-instruct</code>. Behold: <a href="https://lichess.org/iwi02kUm">1</a> <a href="https://lichess.org/bfaDMlVm">2</a> <a href="https://lichess.org/lNE5mKPO">3</a> <a href="https://lichess.org/C3xV1uAz">4</a> <a href="https://lichess.org/YpzT2KQS">5</a> <a href="https://lichess.org/dy9m2DsU">6</a> <a href="https://lichess.org/4YDx633U">7</a> <a href="https://lichess.org/ZM9ZbDfo">8</a> <a href="https://lichess.org/59cTZSjs">9</a> <a href="https://lichess.org/DmeQQx7Y">10</a></p>

  </section></div>
  </body>
</html>
