<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.metafilter.com/mefi/193406">Original</a>
    <h1>“A bold and brassy trumpet melody?”</h1>
    
    <div id="readability-page-1" class="page"><div><p>Last year I spent a bit of time <a href="https://github.com/geerlingguy/turing-pi-cluster">building a Kubernetes cluster with the original Turing Pi</a>. It was fun, and interesting, but ultimately the performance of the Compute Module 3+ it was designed around led me to running my homelab off some newer Pi 4 model B computers, which are at least twice as fast for almost everything I run on them.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/turing-pi-2-board.jpeg" alt="Turing Pi 2" width="700" height="496"/></p>

<p>So this year, I was excited when the folks at Turing Pi sent me a <a href="https://turingpi.com">Turing Pi 2</a> to test drive. <em>And</em> the board arrived just in time for Patrick Kennedy from ServeTheHome to challenge me to a cluster build-off at Supercomputing &#39;21! Check out <a href="https://www.servethehome.com/building-the-ultimate-x86-and-arm-cluster-in-a-box/">his ARM cluster build here</a>.</p>

<p>The Turing Pi 2 is a mini ITX motherboard capable of holding up to four Raspberry Pi Compute Module 4s <em>or</em> NVIDIA Jetson Nanos, and it integrates a board management backplane, power management, and gigabit Ethernet switch, alongside various PCI Express breakouts, so you can build a 4-node SBC cluster.</p>

<p>And it performs—as I hoped—much better than the older version. Not only do the CM4, Jetson Nano, and even pin-compatible replacements like Radxa&#39;s CM3 and Pine64&#39;s SOQuartz all have full gigabit Ethernet, their CPUs are noticeably faster than the CM3+ they replace.</p>

<p>The board I have is a prototype, and as such is still running a very early version of the firmware—RTC PWM fan control aren&#39;t even implemented yet! But the folks at Turing Pi seem set on having all the features ironed out for a January 2022 launch. The board will cost about $200, with CM4 adapter cards (which connect a CM4 to the vertical SO-DIMM slot) adding on $10 each.</p>

<h2>PCIe Expansion support</h2>

<p>Since each CM4 has one PCI Express Gen 2 lane available, the board exposes each one in a different way.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/turing-pi-2-overhead.jpeg" alt="Turing Pi 2 overhead top down shot" width="650" height="434"/></p>

<p>For node 1 (at the top of the board), there&#39;s a mini PCIe slot with a SIM tray underneath—useful for things like 4G or 5G modems. For node 2, there&#39;s another mini PCIe slot (with no SIM tray).</p>

<p>Node 3 is connected to an ASMedia 2-port SATA controller, so you can plug in up to two SATA III drives directly into the Turing Pi 2, and they&#39;ll be controlled by the computer in slot 3.</p>

<p>Node 4 is connected to a VL805 USB 3.0 controller, and that exposes a USB 3 front panel header and two USB 3.0 ports on the rear to the computer in slot 4.</p>

<p>The idea is you can use node 3 as a storage controller (e.g. run two drives in RAID 1 and have an NFS share for your cluster running on that node), use node 4 for USB devices (and share them, if needed, through that node), and use nodes 1 and 2 for whatever specific connectivity you want (e.g. a wireless 4G gateway for redundant Internet access).</p>

<p>The back of the board has two bridged 1 Gbps Ethernet adapters—though the Realtek RTL8370MB Ethernet switch is managed, so assuming updated firmware, the board&#39;s network configuration should be malleable as well.</p>

<h2>Power</h2>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/atx-power-header-turing-pi-2.jpeg" alt="ATX Power header on Turing Pi 2" width="650" height="434"/></p>

<p>In a first for a Raspberry Pi board, the Turing Pi 2 gets its power via a standard ATX 24-pin power header. You can use any standard PC PSU, or even a <a href="https://amzn.to/3d2MCXS">Pico PSU</a> and 12V adapter to power the Turing Pi 2.</p>

<p>With four CM4 8GB Lite nodes with WiFi and Bluetooth, plus a connected 2TB Crucial SSD, power consumption was around 15W at idle, and 25W when running under full load on all four nodes (as measured at the wall by a Kill-a-Watt). I&#39;ll talk a little more about efficiency in the performance section.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/reset-power-button-turing-pi-2.jpeg" alt="Reset and Power button detail on Turing Pi 2" width="650" height="434"/></p>

<p>The board also has a reset and power button built in, as well as headers for front panel power and reset functionality. And these buttons work well, providing a sometimes-necessary full halt/reboot to the cluster.</p>

<p>If you press &#39;reset&#39;, the entire cluster will be powered down immediately, then booted back up, one slot at a time. If you press &#39;power&#39; the cluster will power down gracefully one slot at a time, and remain off. Pressing &#39;power&#39; again boots the cluster back up, one slot at a time.</p>

<h2>Other IO</h2>

<p>Around the edge of the board, there are various other IO options as well. There are UART headers for each Pi, along with a full 40-pin GPIO header for slot 1. There&#39;s also a full-size HDMI 2.0 port attached to slot 1.</p>

<p>There are micro USB ports for flashing both the MCU and eMMC modules on the CM4s themselves (you can see them in the picture in the power section above), and unlike the Turing Pi v1, all nodes can be hot-plugged and eMMC can be flashed using software control, courtesy of a new board management backplane, managed by an STM32 chip.</p>

<p>The user interface and CLI for that chip is still a bit rough, so I haven&#39;t had a chance to really dig in—hopefully I&#39;ll have more time for that soon, since I&#39;m planning on racking up this unit!</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/front-panel-header-usb-3-dsi-turing-pi-2.jpeg" alt="DSI front panel header and USB 3 connector on Turing Pi 2" width="650" height="434"/></p>

<p>There&#39;s also a front panel header for LEDs and a power switch, as well as a DSI display connector attached to slot 1.</p>

<p>There are a few other connections, jumpers, and dip switches too, but I&#39;ll defer to the official site and documentation for more detail.</p>

<h2>Blinkenlights</h2>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/turing-pi-blinkenlights.gif" alt="Turing Pi 2 Blinkenlights" width="380" height="240"/></p>

<p>Besides performance, one thing I didn&#39;t like about the original board was how few status LEDs it had. This new board is much better in that regard, with status LEDs for almost every important feature.</p>

<p>Both of the Ethernet ports on the back have functional link and activity lights. Each slot <em>also</em> has appropriately-colored link and activity lights. Each slot also has a power indicator LED, plus there&#39;s an overall board power LED, and an LED to indicate the MCU is &#39;on&#39; (helpful when deciding to safely shut down the cluster).</p>

<p>And finally, each of the CM4 adapter cards has a green power and activity LED on the back, so you can visually confirm they&#39;re powered on and doing something.</p>

<p>All in all, a good show of <a href="https://en.wikipedia.org/wiki/Blinkenlights">blinkenlights</a> if I&#39;ve ever seen one.</p>

<h2>Performance</h2>

<p>No Raspberry Pi-based cluster will seriously compete in the <a href="https://www.top500.org">Top500</a> list, but since the list was just updated at Supercomputing &#39;21—which was held a few weeks ago in St. Louis, my hometown—I thought it&#39;d be a fun experiment to see how it fares running HPL (High Performance Linpack), the standard benchmark for the Top500 clusters.</p>

<p>Initially, I had some trouble getting HPL to run. I built an automated playbook to build MPI, ATLAS, and HPL on the cluster (it&#39;s in my <a href="https://github.com/geerlingguy/turing-pi-2-cluster">turing-pi-2-cluster</a> repo), and everything would work, but when I ran the test on more than two nodes, it would just hang.</p>

<p>As it turns out, the problem was DNS (of course <a href="https://redshirtjeff.com/listing/it-was-dns-shirt?product=211">it was DNS</a>), and once I added all the IP addresses to the cluster hosts files, I was able to get it to run. I benchmarked the cluster at the default clock (1.5 GHz) and overclocked (to 2.0 GHz), and here are the results:</p>

<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Result</th>
<th>Wattage</th>
<th>Gflops/W</th>
</tr>
</thead>
<tbody>
<tr>
<td>HPL (1.5 GHz base clock)</td>
<td>44.942 Gflops</td>
<td>24.5W</td>
<td>1.83 Gflops/W</td>
</tr>
<tr>
<td>HPL (2.0 GHz overclock)</td>
<td>51.327 Gflops</td>
<td>33W</td>
<td>1.54 Gflops/W</td>
</tr>
</tbody>
</table>

<p>The cluster would rank somewhere in the middle of the Top500—at least <a href="https://www.top500.org/lists/top500/list/1999/11/?page=3">in November 1999</a>! And by my calculation, to make the cutoff for <em>this</em> year&#39;s list, I&#39;d only need around 146,772 more Compute Module 4&#39;s :)</p>

<p>But what&#39;s more pertinent to a little cluster-on-a-board like the Turing Pi 2 is <em>energy efficiency</em>. And looking at the Gflops/Watt measurement, it actually ranks favorably in <em>this</em> year&#39;s <a href="https://www.top500.org/lists/green500/list/2021/11/?page=2">Green500</a>—with 1.83 Gflops/W, it would rank in the 150s. Of course, the other servers in that range are throwing out <em>thousands of teraflops</em>... so not exactly an equal comparison.</p>

<p>But the cluster does run at 15W idle, and about 24W full tilt, with all four Pis running Linpack. Not too shabby if you compare this little ~$500 cluster to a four node setup of old X86 laptops or mini desktops.</p>

<p>If you want to dig deeper into the test methodology and results, check out my <a href="https://github.com/geerlingguy/turing-pi-2-cluster/issues/1">HPL benchmarking issue on GitHub</a>.</p>

<h2>Video</h2>

<p>In addition to this blog post, I have a video that goes more in-depth on the cluster build and board features—<a href="https://www.youtube.com/watch?v=IUPYpZBfsMU">check it out on YouTube</a>:</p>

<div>
<p><iframe src="https://www.youtube.com/embed/IUPYpZBfsMU" frameborder="0" allowfullscreen=""></iframe></p>
</div>

<h2>Conclusion</h2>

<p>The Turing Pi 2 should cost about $200, and the Compute Module 4 adapter cards $10 each. It should be available in January 2022, and hopefully in greater quantities than the Turing Pi 1, which was almost always out of stock!</p>

<p>I think this board is a great platform for learning and low-end ARM cluster builds, and could also be useful for edge environments or other places where power and budget are primary constraints, but you still need multiple nodes (for whatever reason).</p>

<p>It has improved on the first Turing Pi board in every way except quantity of Pis, but that&#39;s to be expected since there&#39;s only so much space in the mini ITX footprint, and it also means the board can support many new features, like Nvidia Jetson Nano boards and multiple PCI express expansion cards!</p>

<p>Check it out at <a href="https://turingpi.com">TuringPi.com</a>.</p>
</div></div>
  </body>
</html>
