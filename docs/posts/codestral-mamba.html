<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/codestral-mamba/">Original</a>
    <h1>Codestral Mamba</h1>
    
    <div id="readability-page-1" class="page"><div><p>Following the publishing of the Mixtral family, Codestral Mamba is another step in our effort to study and provide new architectures. It is available for free use, modification, and distribution, and we hope it will open new perspectives in architecture research. Codestral Mamba was designed with help from Albert Gu and Tri Dao.</p><p>Unlike Transformer models, <a href="https://arxiv.org/abs/2312.00752">Mamba models</a> offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length. It allows users to engage with the model extensively with quick responses, irrespective of the input length. This efficiency is especially relevant for code productivity use cases—this is why we trained this model with advanced code and reasoning capabilities, enabling it to perform on par with SOTA transformer-based models.</p><p><img src="https://mistral.ai/images/news/codestral-mamba/codestral-mamba-benchmarks.png" alt="Detailed Codestral Mamba benchmarks" width="100%"/></p><p>We have tested Codestral Mamba on in-context retrieval capabilities up to 256k tokens. We expect it to be a great local code assistant!</p><p>You can deploy Codestral Mamba using the <a href="https://github.com/mistralai/mistral-inference/releases/tag/v1.2.0">mistral-inference</a> SDK, which relies on the reference implementations from Mamba’s GitHub repository. The model can also be deployed through <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba">TensorRT-LLM</a>. For local inference, keep an eye out for support in llama.cpp. You may download the raw weights from <a href="https://huggingface.co/mistralai/mamba-codestral-7B-v0.1">HuggingFace</a>.</p><p>For easy testing, we made Codestral Mamba available on <a href="https://console.mistral.ai/">la Plateforme</a> (<code>codestral-mamba-2407</code>), alongside its big sister, Codestral 22B. While Codestral Mamba is available under the Apache 2.0 license, Codestral 22B is available under a <a href="https://mistral.ai/contact/">commercial license</a> for self-deployment or a community license for testing purposes.</p><p><strong>Important:</strong> This is an instructed model, with 7,285,403,648 parameters.</p></div></div>
  </body>
</html>
