<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2206.15378">Original</a>
    <h1>Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Perolat%2C+J">Julien Perolat</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=de+Vylder%2C+B">Bart de Vylder</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hennes%2C+D">Daniel Hennes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tarassov%2C+E">Eugene Tarassov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Strub%2C+F">Florian Strub</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=de+Boer%2C+V">Vincent de Boer</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Muller%2C+P">Paul Muller</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Connor%2C+J+T">Jerome T. Connor</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Burch%2C+N">Neil Burch</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anthony%2C+T">Thomas Anthony</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=McAleer%2C+S">Stephen McAleer</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Elie%2C+R">Romuald Elie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cen%2C+S+H">Sarah H. Cen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhe Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gruslys%2C+A">Audrunas Gruslys</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Malysheva%2C+A">Aleksandra Malysheva</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khan%2C+M">Mina Khan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ozair%2C+S">Sherjil Ozair</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Timbers%2C+F">Finbarr Timbers</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pohlen%2C+T">Toby Pohlen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Eccles%2C+T">Tom Eccles</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rowland%2C+M">Mark Rowland</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lanctot%2C+M">Marc Lanctot</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lespiau%2C+J">Jean-Baptiste Lespiau</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Piot%2C+B">Bilal Piot</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Omidshafiei%2C+S">Shayegan Omidshafiei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lockhart%2C+E">Edward Lockhart</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sifre%2C+L">Laurent Sifre</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Beauguerlange%2C+N">Nathalie Beauguerlange</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Munos%2C+R">Remi Munos</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Silver%2C+D">David Silver</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh%2C+S">Satinder Singh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hassabis%2C+D">Demis Hassabis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tuyls%2C+K">Karl Tuyls</a></p></div>
      
    
  
    <p><a href="http://harihareswara.net/pdf/2206.15378">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  We introduce DeepNash, an autonomous agent capable of learning to play the
imperfect information game Stratego from scratch, up to a human expert level.
Stratego is one of the few iconic board games that Artificial Intelligence (AI)
has not yet mastered. This popular game has an enormous game tree on the order
of $10^{535}$ nodes, i.e., $10^{175}$ times larger than that of Go. It has the
additional complexity of requiring decision-making under imperfect information,
similar to Texas hold&#39;em poker, which has a significantly smaller game tree (on
the order of $10^{164}$ nodes). Decisions in Stratego are made over a large
number of discrete actions with no obvious link between action and outcome.
Episodes are long, with often hundreds of moves before a player wins, and
situations in Stratego can not easily be broken down into manageably-sized
sub-problems as in poker. For these reasons, Stratego has been a grand
challenge for the field of AI for decades, and existing AI methods barely reach
an amateur level of play. DeepNash uses a game-theoretic, model-free deep
reinforcement learning method, without search, that learns to master Stratego
via self-play. The Regularised Nash Dynamics (R-NaD) algorithm, a key component
of DeepNash, converges to an approximate Nash equilibrium, instead of &#39;cycling&#39;
around it, by directly modifying the underlying multi-agent learning dynamics.
DeepNash beats existing state-of-the-art AI methods in Stratego and achieved a
yearly (2022) and all-time top-3 rank on the Gravon games platform, competing
with human expert players.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Julien Perolat [<a href="http://harihareswara.net/show-email/c7d856ea/2206.15378">view email</a>]
      </p></div></div>
  </body>
</html>
