<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vgel.me/posts/seahorse/">Original</a>
    <h1>Why do LLMs freak out over the seahorse emoji?</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    
  
  <hr/>
<p><em>This is an edited and expanded version of a Twitter post, originally in response to @arm1st1ce, that can be found here: <a href="https://x.com/voooooogel/status/1964465679647887838">https://x.com/voooooogel/status/1964465679647887838</a></em></p>
<hr/>
<p>Is there a seahorse emoji? Let&#39;s ask GPT-5 Instant:</p>
<p><img src="https://vgel.me/posts/seahorse/gpt5.png" alt=""/></p>
<p>Wtf? Let&#39;s ask Claude Sonnet 4.5 instead:</p>
<p><img src="https://vgel.me/posts/seahorse/sonnet45.png" alt=""/></p>
<p>What&#39;s going on here? Maybe Gemini 2.5 Pro handles it better?</p>
<p><img src="https://vgel.me/posts/seahorse/gemini.png" alt=""/></p>
<p>OK, something is going on here. Let&#39;s find out why.</p>
<span id="continue-reading"></span><h2 id="LLMs_really_think_there&#39;s_a_seahorse_emoji"><a href="#LLMs_really_think_there&#39;s_a_seahorse_emoji">
  <img src="https://vgel.me/permalink.svg" alt="permalink for LLMs_really_think_there&#39;s_a_seahorse_emoji"/>
</a>LLMs really think there&#39;s a seahorse emoji</h2>
<p>Here are the answers you get if you ask several models whether a seahorse emoji exists, yes or no, 100 times:</p>
<blockquote>
<p>Is there a seahorse emoji, yes or no? Respond with one word, no punctuation.</p>
</blockquote>
<ul>
<li>gpt-5-chat
<ul>
<li>100% &#39;Yes&#39;</li>
</ul>
</li>
<li>gpt-5
<ul>
<li>100% &#39;Yes&#39;</li>
</ul>
</li>
<li>claude-4.5-sonnet
<ul>
<li>100% &#39;Yes&#39;</li>
</ul>
</li>
<li>llama-3.3-70b
<ul>
<li>83% &#39;yes&#39;</li>
<li>17% &#39;Yes&#39;</li>
</ul>
</li>
</ul>
<p>Needlessly to say, popular language models are <em>very</em> confident that there&#39;s a seahorse emoji. And they&#39;re not alone in that confidence - here&#39;s a Reddit thread with hundreds of comments from people who distinctly remember a seahorse emoji existing:</p>
<p><img src="https://vgel.me/posts/seahorse/reddit.png" alt=""/></p>
<p>There&#39;s tons of this - Google &#34;seahorse emoji&#34; and you&#39;ll find TikToks, Youtube videos, and even (now defunct) memecoins based around the supposed vanishing of a seahorse emoji that everyone is pretty sure used to exist - but of course, never did.</p>
<p>Maybe LLMs believe a seahorse emoji exists because so many humans in the training data do. Or maybe it&#39;s a convergent belief - given how many other aquatic animals are in Unicode, it&#39;s reasonable for both humans and LLMs to assume (generalize, even) that such a delightful animal is as well. A seahorse emoji was even <a href="https://unicode.org/emoji/emoji-proposals-status.html">formally proposed at one point</a>, but was rejected in 2018.</p>
<p>Regardless of the root cause, many LLMs begin each new context window fresh with the mistaken latent belief that the seahorse emoji exists. But why does that produce such strange behavior? I mean, I used to believe a seahorse emoji existed myself, but if I had tried to send it to a friend, I would&#39;ve simply looked for it on my keyboard and realized it wasn&#39;t there, not sent the wrong emoji and then gone into an emoji spam doomloop. So what&#39;s happening inside the LLM that causes it to act like this?</p>
<h2 id="Using_the_logit_lens"><a href="#Using_the_logit_lens">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Using_the_logit_lens"/>
</a>Using the logit lens</h2>
<p>Let&#39;s look into this using everyone&#39;s favorite underrated interpretability tool, <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">the logit lens</a>!</p>
<p>Using this prompt prefix - a templated chat with the default llama-3.3-70b system prompt, a question about the seahorse emoji, and a partial answer from the model right before it gives the actual emoji:</p>
<pre><code><span>&lt;|begin_of_text|&gt;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id&gt;
</span><span>Cutting Knowledge Date: December 2023
</span><span>Today Date: 26 Jul 2024
</span><span>
</span><span>&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
</span><span>
</span><span>Is there a seahorse emoji?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
</span><span>
</span><span>Yes, there is a seahorse emoji:
</span></code></pre>
<p>We can take the model&#39;s <code>lm_head</code>, which is usually only used on the output of the last layer, and apply it to <em>every</em> layer to produce intermediate token predictions. That process produces this table, showing for every fourth layer what the most likely token would be for the next three positions after the prefix (tokens 0, 1, and 2), and what the top 5 most likely predictions for the first position is (token 0 topk 5):</p>
<table>
<thead>
<tr><th>layer</th><th colspan="3">tokens</th><th>tokens</th><th>token 0</th></tr>
<tr><th></th><th>0</th><th>1</th><th>2</th><th>merged</th><th>(topk 5)</th></tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>83244&#39;ĠBail&#39;</td>
<td>15591&#39;ĠHarr&#39;</td>
<td>5309&#39;Ġvert&#39;</td>
<td>Bail Harr vert</td>
<td>[&#39;ĠBail&#39;, &#39;ĠPeanut&#39;, &#39;ĠãĢ&#39;, &#39;orr&#39;, &#39;ĠâĢĭâĢĭ&#39;]</td>
</tr>
<tr>
<td>4</td>
<td>111484&#39;emez&#39;</td>
<td>26140&#39;abi&#39;</td>
<td>25727&#39;avery&#39;</td>
<td>emezabiavery</td>
<td>[&#39;emez&#39;, &#39;Ġunm&#39;, &#39;ĠOswald&#39;, &#39;Ġrem&#39;, &#39;rix&#39;]</td>
</tr>
<tr>
<td>8</td>
<td>122029&#39;chyb&#39;</td>
<td>44465&#39;ĠCaps&#39;</td>
<td>15610&#39;iller&#39;</td>
<td>chyb Capsiller</td>
<td>[&#39;chyb&#39;, &#39;ĠSund&#39;, &#39;ØªØ±ÛĮ&#39;, &#39;resse&#39;, &#39;Ġsod&#39;]</td>
</tr>
<tr>
<td>12</td>
<td>1131&#39;...&#39;</td>
<td>48952&#39;ĠCliff&#39;</td>
<td>51965&#39;ĠJackie&#39;</td>
<td>... Cliff Jackie</td>
<td>[&#39;...&#39;, &#39;ages&#39;, &#39;dump&#39;, &#39;qing&#39;, &#39;Ġexp&#39;]</td>
</tr>
<tr>
<td>16</td>
<td>1131&#39;...&#39;</td>
<td>12676&#39;365&#39;</td>
<td>31447&#39;ĠAld&#39;</td>
<td>...365 Ald</td>
<td>[&#39;...&#39;, &#39;...Ċ&#39;, &#39;Ġindeed&#39;, &#39;Ġboth&#39;, &#39;ĠYes&#39;]</td>
</tr>
<tr>
<td>20</td>
<td>1131&#39;...&#39;</td>
<td>109596&#39;éļĨ&#39;</td>
<td>51965&#39;ĠJackie&#39;</td>
<td>...隆 Jackie</td>
<td>[&#39;...&#39;, &#39;...Ċ&#39;, &#39;Z&#39;, &#39;Ġboth&#39;, &#39;ĠHust&#39;]</td>
</tr>
<tr>
<td>24</td>
<td>12&#39;-&#39;</td>
<td>31643&#39;ï¸ı&#39;</td>
<td>287&#39;ing&#39;</td>
<td>-️ing</td>
<td>[&#39;-&#39;, &#39;...&#39;, &#39;âĢ¦&#39;, &#39;...Ċ&#39;, &#39;em&#39;]</td>
</tr>
<tr>
<td>28</td>
<td>1131&#39;...&#39;</td>
<td>96154&#39;ĠGaut&#39;</td>
<td>51965&#39;ĠJackie&#39;</td>
<td>... Gaut Jackie</td>
<td>[&#39;...&#39;, &#39;-&#39;, &#39;...Ċ&#39;, &#39;-Ċ&#39;, &#39;Ġ&#39;]</td>
</tr>
<tr>
<td>32</td>
<td>1131&#39;...&#39;</td>
<td>96154&#39;ĠGaut&#39;</td>
<td>6892&#39;Ġing&#39;</td>
<td>... Gaut ing</td>
<td>[&#39;...&#39;, &#39;âĢ¦&#39;, &#39;...Ċ&#39;, &#39;O&#39;, &#39;zer&#39;]</td>
</tr>
<tr>
<td>36</td>
<td>1131&#39;...&#39;</td>
<td>12&#39;-&#39;</td>
<td>88&#39;y&#39;</td>
<td>...-y</td>
<td>[&#39;...&#39;, &#39;âĢ¦&#39;, &#39;...Ċ&#39;, &#39;Ġ&#39;, &#39;u&#39;]</td>
</tr>
<tr>
<td>40</td>
<td>1131&#39;...&#39;</td>
<td>31643&#39;ï¸ı&#39;</td>
<td>88&#39;y&#39;</td>
<td>...️y</td>
<td>[&#39;...&#39;, &#39;u&#39;, &#39;âĢ¦&#39;, &#39;Âł&#39;, &#39;...Ċ&#39;]</td>
</tr>
<tr>
<td>44</td>
<td>80435&#39;ĠScor&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>Scor horse horse</td>
<td>[&#39;ĠScor&#39;, &#39;u&#39;, &#39;ĠPan&#39;, &#39;in&#39;, &#39;Ġhttps&#39;]</td>
</tr>
<tr>
<td>48</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>horse horse horse</td>
<td>[&#39;Ġhorse&#39;, &#39;Âł&#39;, &#39;ĠPan&#39;, &#39;ĠHomes&#39;, &#39;ĠHorse&#39;]</td>
</tr>
<tr>
<td>52</td>
<td>9581&#39;Ġsea&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>sea horse horse</td>
<td>[&#39;Ġsea&#39;, &#39;Ġhorse&#39;, &#39;ĠHorse&#39;, &#39;ĠSea&#39;, &#39;âĢĳ&#39;]</td>
</tr>
<tr>
<td>56</td>
<td>9581&#39;Ġsea&#39;</td>
<td>43269&#39;ĠSeah&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>sea Seah horse</td>
<td>[&#39;Ġsea&#39;, &#39;ĠSea&#39;, &#39;ĠSeah&#39;, &#39;Ġhippoc&#39;, &#39;Ġhorse&#39;]</td>
</tr>
<tr>
<td>60</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>horse horse horse</td>
<td>[&#39;Ġhorse&#39;, &#39;Ġsea&#39;, &#39;ĠSeah&#39;, &#39;Ġse&#39;, &#39;horse&#39;]</td>
</tr>
<tr>
<td>64</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>horse horse horse</td>
<td>[&#39;Ġhorse&#39;, &#39;Ġse&#39;, &#39;ĠHorse&#39;, &#39;horse&#39;, &#39;Ġhors&#39;]</td>
</tr>
<tr>
<td>68</td>
<td>60775&#39;horse&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>15580&#39;Ġhorse&#39;</td>
<td>horse� horse</td>
<td>[&#39;horse&#39;, &#39;Ġse&#39;, &#39;Ġhorse&#39;, &#39;Ġhippoc&#39;, &#39;ĠSeah&#39;]</td>
</tr>
<tr>
<td>72</td>
<td>513&#39;Ġse&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>513&#39;Ġse&#39;</td>
<td>se� se</td>
<td>[&#39;Ġse&#39;, &#39;Ġhippoc&#39;, &#39;horse&#39;, &#39;ĠðŁ&#39;, &#39;Ġhorse&#39;]</td>
</tr>
<tr>
<td>76</td>
<td>513&#39;Ġse&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>513&#39;Ġse&#39;</td>
<td>se� se</td>
<td>[&#39;Ġse&#39;, &#39;Ġhippoc&#39;, &#39;hip&#39;, &#39;Ġhorse&#39;, &#39;ĠHipp&#39;]</td>
</tr>
<tr>
<td>80</td>
<td>11410&#39;ĠðŁ&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>254&#39;ł&#39;</td>
<td>🐠</td>
<td>[&#39;ĠðŁ&#39;, &#39;ðŁ&#39;, &#39;ĠðŁĴ&#39;, &#39;Ġ&#39;, &#39;ĠðŁĳ&#39;]</td>
</tr>
</tbody>
</table>
<p>This is the logit lens: using the model&#39;s <code>lm_head</code> to produce logits (token likelihoods) as a way to investigate its internal states. Note that the tokens and likelihoods we get from the logit lens here are not equivalent to the model&#39;s <em>full</em> internal states! For that, we would need a more complex technique like representation reading or sparse autoencoders. Instead, this is a <em>lens</em> on that state - it shows what the output token would be <em>if</em> this layer were the last one. But despite this limitation, the logit lens is still useful. The states of early layers may be difficult to interpret using it, but as we move up through the stack we can see that the model is iteratively refining those states towards its final prediction, a fish emoji.</p>
<p><small>(Why do the unmerged tokens look like that &#39;ĠðŁ&#39;, &#39;Ĳ&#39;, &#39;ł&#39; nonsense? It&#39;s because of a tokenizer quirk - those tokens encode the UTF-8 bytes for the fish emoji. It&#39;s not relevant to this post, but if you&#39;re curious, ask Claude or your favorite LLM to explain this paragraph and this line of code: <code>bytes([bpe_byte_decoder[c] for c in &#39;ĠðŁĲł&#39;]).decode(&#39;utf-8&#39;) == &#39; 🐠&#39;</code>)</small></p>
<p>Take a look at what happens in the middle layers, though - it&#39;s not the early-layer weirdness <em>or</em> the emoji bytes of the final prediction! Instead we get words relating to <em>useful concepts</em>, specifically the concept of a seahorse. For example, on layer 52, we get &#34;sea horse horse&#34; - three residual positions in a row encoding the &#34;seahorse&#34; concept. Later, in the top-k for the first position, we get a mixture of &#34;sea&#34;, &#34;horse&#34;, and an emoji byte sequence prefix, &#34;ĠðŁ&#34;.</p>
<p>So what is the model thinking about? &#34;seahorse + emoji&#34;! It&#39;s trying to construct a residual representation of a seahorse combined with an emoji. Why would the model try to construct this combination? Well, let&#39;s look into how the <code>lm_head</code> actually works.</p>
<h2 id="lm_head"><a href="#lm_head">
  <img src="https://vgel.me/permalink.svg" alt="permalink for lm_head"/>
</a><code>lm_head</code></h2>
<p>A language model&#39;s <code>lm_head</code> is a huge matrix of residual-sized vectors associated with token ids, one for every token in the vocabulary (~300,000). When a residual is passed into it, either after flowing through the model normally or early because someone is using the logit lens on an earlier layer, the <code>lm_head</code> is going to compare that input residual with each residual-sized vector in that big matrix, and (in coordination with the sampler) select the token id associated with the vector that matrix contains that is most similar to the input residual.</p>
<p>(More technically: <code>lm_head</code> is a linear layer without a bias, so <code>x @ w.T</code> does dot products with each unembedding vector to produce raw scores. Then your usual log_softmax and argmax/temperature sample.)</p>
<p>That means if the model wants to output the word &#34;hello&#34;, for example in response to a friendly greeting from the user, it needs to construct a residual as similar as possible to the vector for the &#34;hello&#34; token that the <code>lm_head</code> can then turn into the hello token id. And using logit lens, we can see that&#39;s exactly what happens in response to &#34;Hello :-)&#34;:</p>
<table>
<thead>
<tr><th>layer</th><th colspan="3">tokens</th><th>tokens</th><th>token 0</th></tr>
<tr><th></th><th>0</th><th>1</th><th>2</th><th>merged</th><th>(topk 5)</th></tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0&#39;!&#39;       </td>
<td>0&#39;!&#39;       </td>
<td>40952&#39;opa&#39;     </td>
<td>!!opa                    </td>
<td>[&#39;&#34;&#39;, &#39;!&#39;, &#39;#&#39;, &#39;%&#39;, &#39;$&#39;]</td>
</tr>
<tr>
<td>8</td>
<td>121495&#39;ÅĻiv&#39;    </td>
<td>16&#39;1&#39;       </td>
<td>73078&#39;iae&#39;     </td>
<td>řiv1iae                  </td>
<td>[&#39;ÅĻiv&#39;, &#39;-&#39;, &#39;(&#39;, &#39;.&#39;, &#39;,&#39;]</td>
</tr>
<tr>
<td>16</td>
<td>34935&#39;Ġconsect&#39;</td>
<td>7341&#39;arks&#39;    </td>
<td>13118&#39;Ġindeed&#39; </td>
<td> consectarks indeed      </td>
<td>[&#39;Ġobscure&#39;, &#39;Ġconsect&#39;, &#39;äºķ&#39;, &#39;ĠÐ¿ÑĢÐ¾ÑĦÐµÑģÑģÐ¸Ð¾Ð½Ð°Ð»ÑĮ&#39;, &#39;Îŀ&#39;]</td>
</tr>
<tr>
<td>24</td>
<td>67846&#39;&lt;[&#39;      </td>
<td>24748&#39;Ġhello&#39;  </td>
<td>15960&#39;Ġhi&#39;     </td>
<td>&lt;[ hello hi              </td>
<td>[&#39;&lt;[&#39;, &#39;arks&#39;, &#39;outh&#39;, &#39;ĠHam&#39;, &#39;la&#39;]</td>
</tr>
<tr>
<td>32</td>
<td>15825&#39;-back&#39;   </td>
<td>2312&#39;ln&#39;      </td>
<td>14451&#39;UBL&#39;     </td>
<td>-backlnUBL               </td>
<td>[&#39;ÂŃi&#39;, &#39;-back&#39;, &#39;Ġquestion&#39;, &#39;ln&#39;, &#39;ant&#39;]</td>
</tr>
<tr>
<td>40</td>
<td>15648&#39;Ġsmile&#39;  </td>
<td>14262&#39;Welcome&#39; </td>
<td>1203&#39;Ġback&#39;   </td>
<td> smileWelcome back       </td>
<td>[&#39;Ġsmile&#39;, &#39;ĠÑĥÐ»ÑĭÐ±&#39;, &#39;Ġsmiled&#39;, &#39;ĠSmile&#39;, &#39;etwork&#39;]</td>
</tr>
<tr>
<td>48</td>
<td>15648&#39;Ġsmile&#39;  </td>
<td>21694&#39;ĠHi&#39;     </td>
<td>1203&#39;Ġback&#39;   </td>
<td> smile Hi back           </td>
<td>[&#39;Ġsmile&#39;, &#39;Ġsmiled&#39;, &#39;ĠHello&#39;, &#39;Ġsmiling&#39;, &#39;Ġhello&#39;]</td>
</tr>
<tr>
<td>56</td>
<td>22691&#39;ĠHello&#39;  </td>
<td>15960&#39;Ġhi&#39;     </td>
<td>1203&#39;Ġback&#39;   </td>
<td> Hello hi back           </td>
<td>[&#39;ĠHello&#39;, &#39;Ġhi&#39;, &#39;Ġsmile&#39;, &#39;Ġhello&#39;, &#39;Hello&#39;]</td>
</tr>
<tr>
<td>64</td>
<td>4773&#39;-sm&#39;     </td>
<td>24748&#39;Ġhello&#39;  </td>
<td>1203&#39;Ġback&#39;   </td>
<td>-sm hello back           </td>
<td>[&#39;-sm&#39;, &#39;ĠHello&#39;, &#39;ĠSm&#39;, &#39;sm&#39;, &#39;Hello&#39;]</td>
</tr>
<tr>
<td>72</td>
<td>22691&#39;ĠHello&#39;  </td>
<td>22691&#39;ĠHello&#39;  </td>
<td>1203&#39;Ġback&#39;   </td>
<td> Hello Hello back        </td>
<td>[&#39;ĠHello&#39;, &#39;Ġhello&#39;, &#39;Hello&#39;, &#39;ĠHEL&#39;, &#39;Ġhel&#39;]</td>
</tr>
<tr>
<td>80</td>
<td>271&#39;ĊĊ&#39;      </td>
<td>9906&#39;Hello&#39;   </td>
<td>0&#39;!&#39;       </td>
<td>
<p>Hello!                 </p></td>
<td>[&#39;ĊĊ&#39;, &#39;ĊĊĊ&#39;, &#39;&lt;|end_of_text|&gt;&#39;, &#39;ĊĊĊĊ&#39;, &#39;&#34;ĊĊ&#39;]</td>
</tr>
</tbody>
</table>
<p><small>(&#39;Ċ&#39; is another tokenizer quirk - it represents a line break. &#39;Ġ&#39; is similarly a space.)</small></p>
<p>Likewise, if the model wants to output a seahorse emoji, it needs to construct a residual similar to the vector for the seahorse emoji output token(s) - which in theory could be any arbitrary value, but in practice is &#34;seahorse + emoji&#34;, word2vec style. We can see this in action with a real emoji, the fish emoji:</p>
<pre><code><span>&lt;|begin_of_text|&gt;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
</span><span>
</span><span>Cutting Knowledge Date: December 2023
</span><span>Today Date: 26 Jul 2024
</span><span>
</span><span>&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
</span><span>
</span><span>Is there a fish emoji?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
</span><span>
</span><span>Yes, there is a fish emoji:
</span></code></pre>
<table>
<thead>
<tr><th>layer</th><th colspan="3">tokens</th><th>tokens</th><th>token 0</th></tr>
<tr><th></th><th>0</th><th>1</th><th>2</th><th>merged</th><th>(topk 5)</th></tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>83244&#39;ĠBail&#39;</td>
<td>15591&#39;ĠHarr&#39;</td>
<td>5309&#39;Ġvert&#39;</td>
<td>Bail Harr vert</td>
<td>[&#39;ĠBail&#39;, &#39;ĠPeanut&#39;, &#39;ĠãĢ&#39;, &#39;orr&#39;, &#39;ĠâĢĭâĢĭ&#39;]</td>
</tr>
<tr>
<td>8</td>
<td>122029&#39;chyb&#39;</td>
<td>44465&#39;ĠCaps&#39;</td>
<td>15610&#39;iller&#39;</td>
<td>chyb Capsiller</td>
<td>[&#39;chyb&#39;, &#39;...&#39;, &#39;ØªØ±ÛĮ&#39;, &#39;ĠSund&#39;, &#39;resse&#39;]</td>
</tr>
<tr>
<td>16</td>
<td>1131&#39;...&#39;</td>
<td>12676&#39;365&#39;</td>
<td>65615&#39;ĠSole&#39;</td>
<td>...365 Sole</td>
<td>[&#39;...&#39;, &#39;...Ċ&#39;, &#39;Ġboth&#39;, &#39;Ġindeed&#39;, &#39;ĠYes&#39;]</td>
</tr>
<tr>
<td>24</td>
<td>12&#39;-&#39;</td>
<td>31643&#39;ï¸ı&#39;</td>
<td>51965&#39;ĠJackie&#39;</td>
<td>-️ Jackie</td>
<td>[&#39;-&#39;, &#39;...&#39;, &#39;âĢ¦&#39;, &#39;em&#39;, &#39;...Ċ&#39;]</td>
</tr>
<tr>
<td>32</td>
<td>1131&#39;...&#39;</td>
<td>96154&#39;ĠGaut&#39;</td>
<td>88&#39;y&#39;</td>
<td>... Gauty</td>
<td>[&#39;...&#39;, &#39;âĢ¦&#39;, &#39;...Ċ&#39;, &#39;O&#39;, &#39;u&#39;]</td>
</tr>
<tr>
<td>40</td>
<td>220&#39;Ġ&#39;</td>
<td>6&#34;&#39;&#34;</td>
<td>7795&#39;Ġfish&#39;</td>
<td>&#39;fish</td>
<td>[&#39;Ġ&#39;, &#39;...&#39;, &#39;âĢ¦&#39;, &#39;Âł&#39;, &#39;u&#39;]</td>
</tr>
<tr>
<td>48</td>
<td>7795&#39;Ġfish&#39;</td>
<td>7795&#39;Ġfish&#39;</td>
<td>7795&#39;Ġfish&#39;</td>
<td>fish fish fish</td>
<td>[&#39;Ġfish&#39;, &#39;ĠFish&#39;, &#39;ĠBerk&#39;, &#39;âĢ¦&#39;, &#39;Âł&#39;]</td>
</tr>
<tr>
<td>56</td>
<td>7795&#39;Ġfish&#39;</td>
<td>7795&#39;Ġfish&#39;</td>
<td>7795&#39;Ġfish&#39;</td>
<td>fish fish fish</td>
<td>[&#39;Ġfish&#39;, &#39;ĠFish&#39;, &#39;fish&#39;, &#39;Fish&#39;, &#39;é±¼&#39;]</td>
</tr>
<tr>
<td>64</td>
<td>7795&#39;Ġfish&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>7795&#39;Ġfish&#39;</td>
<td>fish� fish</td>
<td>[&#39;Ġfish&#39;, &#39;ĠFish&#39;, &#39;ĠPis&#39;, &#39;Fish&#39;, &#39;ĠÙħØ§Ùĩ&#39;]</td>
</tr>
<tr>
<td>72</td>
<td>7795&#39;Ġfish&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>253&#39;Ł&#39;</td>
<td>fish��</td>
<td>[&#39;Ġfish&#39;, &#39;ĠFish&#39;, &#39;ĠðŁ&#39;, &#39;Ġ&#39;, &#39;ÂŁ&#39;]</td>
</tr>
<tr>
<td>80</td>
<td>11410&#39;ĠðŁ&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>253&#39;Ł&#39;</td>
<td>🐟</td>
<td>[&#39;ĠðŁ&#39;, &#39;ðŁ&#39;, &#39;Ġ&#39;, &#39;ĠĊĊ&#39;, &#39;ĠâĻ&#39;]</td>
</tr>
</tbody>
</table>
<p>Here, everything works perfectly. The model constructs the &#34;fish + emoji&#34; residual - look at the layer 72 topk, where we have both &#34;fish&#34; and the emoji byte prefix &#34;ĠðŁ&#34; - meaning that the residual at this point is similar to both &#34;fish&#34; and &#34;emoji&#34;, just like we&#39;d expect. Then when this vector is passed into the <code>lm_head</code> after the final layer, we see a 🐟 just as the model expected.</p>
<p>But unlike with 🐟, the seahorse emoji doesn&#39;t exist. The model tries to construct a &#34;seahorse + emoji&#34; vector just as it would for a real emoji, and on layer 72 we even get a very similar construction as with the fish emoji - &#34; se&#34;, &#34;horse&#34;, and the emoji prefix byte prefix:</p>
<table>
<thead>
<tr><th>layer</th><th colspan="3">tokens</th><th>tokens</th><th>token 0</th></tr>
<tr><th></th><th>0</th><th>1</th><th>2</th><th>merged</th><th>(topk 5)</th></tr>
</thead>
<tbody>
<tr>
<td>72</td>
<td>513&#39;Ġse&#39;</td>
<td>238&#39;Ĳ&#39;</td>
<td>513&#39;Ġse&#39;</td>
<td>se� se</td>
<td>[&#39;Ġse&#39;, &#39;Ġhippoc&#39;, &#39;horse&#39;, &#39;ĠðŁ&#39;, &#39;Ġhorse&#39;]</td>
</tr>
</tbody>
</table>
<p>But alas, there&#39;s no continuation to ĠðŁ corresponding to a seahorse, so the <code>lm_head</code> similarity score calculation maxes out with horse- or sea-animal-related emoji bytes instead, and an unintended emoji is sampled.</p>
<p>Now, that sampling is valuable information for the model! You can see that in, e.g. the Claude 4.5 Sonnet example below, when the tokens get appended into the context autoregressively, the model can tell that they don&#39;t form the intended seahorse emoji. The previous, fuzzy &#34;seahorse + emoji&#34; concept has been &#34;snapped&#34; by the <code>lm_head</code> to an emoji that actually exists, like a tropical fish or horse.</p>
<p><img src="https://vgel.me/posts/seahorse/sonnet45.png" alt=""/></p>
<p>Once this happens, it&#39;s up to the model how to proceed. Some models like 4.5 Sonnet try again, and eventually update on the evidence, changing mid-response to a statement about how the seahorse emoji doesn&#39;t exist. Other models like gpt-5-chat spiral for longer, sometimes never recovering. Other models will either blissfully ignore that the emoji is incorrect, and some will even correct themselves instantly after seeing only a single incorrect sample.</p>
<p>But until the model gets the wrong output token from <code>lm_head</code>, it just <em>doesn&#39;t know</em> that its initial belief about a seahorse emoji existing was wrong. It can only assume that &#34;seahorse + emoji&#34; will produce the tokens it wants.</p>
<h2 id="Some_speculation"><a href="#Some_speculation">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Some_speculation"/>
</a>Some speculation</h2>
<p>To speculate a bit more, I wonder if this problem is part of the benefit of reinforcement learning for LLMs - it gives the model information about its <code>lm_head</code> that&#39;s otherwise difficult for it to get at because it&#39;s at the end of the layer stack.</p>
<p>(Remember that base models are not trained on their own outputs / rollouts. That only happens in RL.)</p>
<h2 id="Code"><a href="#Code">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Code"/>
</a>Code</h2>
<p>If you want to try this yourself, you can find a starter script on Github here: <a href="https://gist.github.com/vgel/025ad6af9ac7f3bc194966b03ea68606">https://gist.github.com/vgel/025ad6af9ac7f3bc194966b03ea68606</a></p>


    <ul>
      
        <li><strong>Previous entry:</strong> <a href="https://vgel.me/posts/representation-engineering/">Representation Engineering Mistral-7B an Acid Trip</a></li>
      
      
    </ul>
</article></div>
  </body>
</html>
