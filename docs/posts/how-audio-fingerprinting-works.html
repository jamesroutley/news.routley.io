<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://emysound.com/blog/open-source/2020/06/12/how-audio-fingerprinting-works.html">Original</a>
    <h1>How Audio Fingerprinting Works</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
      <div>

        

<p>I have been developing the <em>SoundFingerprinting</em> open source project for the last ten years.
One of the questions I often receive is “how does music recognition works?”
For the library users, it is somewhat similar to a one-way hash function.</p>

<p>You provide a file at the input, and after a certain number of conversions, you get “audio fingerprints” at the output.
Looking at the actual values of these fingerprints, they are entirely opaque.</p>

<table>
<tbody><tr>
<td>
287121152, 1275791411, 539499396, 185209916, 319820588,
1244475492, 1751526233, 1862426472, 306596106, 204033582,
1242574602, 50341460, 773194334, 339627067, 2014447110,
738722113, 587801446, 320160293, 540288008, 1146191520,
461079818, 959409810, 889623606, 16778759, 285488401
</td>
</tr>
<tr>
<td>
<b>Figure 1</b>. <i>1.8</i> seconds of fingerprinted content
</td>
</tr>
</tbody></table>

<p>The question begs: What <strong>information</strong> do these integers contain?
My goal in this article is to dissect the algorithm’s two main steps: compression and hashing.
Additionally, I will explain why audio and image search are so intertwined.</p>

<h2 id="compression-and-hashing">Compression and Hashing</h2>
<blockquote>
  <p>Compression - from Latin comprimere <strong>press together</strong></p>
</blockquote>

<p>From a higher level, all audio fingerprinting algorithms go through two transformation steps: lossy compression and hashing.
We want to preserve as much <strong>relevant</strong> information as possible with the smallest footprint.</p>

<p>Look at the following array:</p>

<div><div><pre><code><span>// ten entries of 1</span>
<span>int</span> <span>[]</span> <span>data</span> <span>=</span> <span>{</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>}</span>
</code></pre></div></div>
<p>We need at least 40 bytes to store it <code>10*sizeof(int)</code>.
In practice, a data point $P$ has two fundamental properties that influence how fast you can find it in a database of similar points.
That is the size and data format.</p><p>

\[SearchTime(P) \sim Size(P) + DataFormat(P)\]

</p><p>Let’s try to compress the array.
The first step we can apply is a simple <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman Encoding</a> that allows representing the same amount of information more compactly:</p>

<div><div><pre><code><span>// the value of one repeats ten times.</span>
<span>int</span> <span>val</span> <span>=</span> <span>1</span><span>,</span> <span>repeat</span> <span>=</span> <span>10</span><span>;</span>
</code></pre></div></div>
<p>Now it uses 8 bytes: <code>2*sizeof(int)</code>, a 5x reduction.
Good step forward on the size side, but what about the format?
The format is still clunky, so let’s <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">encode</a> it into one long:</p>

<div><div><pre><code><span>// tuple with 2 integer values (1, 10) gets encoded into one long</span>
<span>long</span> <span>hash</span> <span>=</span> <span>(</span><span>1L</span> <span>&lt;&lt;</span> <span>32</span><span>)</span> <span>+</span> <span>10</span><span>;</span> <span>// value of hash is 4,294,967,306</span>
</code></pre></div></div>
<p>After these steps, we have one variable that holds exactly the same amount information in a much more <em>convenient</em> format. Why is it convenient?</p>
<ul>
  <li>it is compact, storing 1000 of similar sequences requires 5x less space</li>
  <li>it is <strong>sortable</strong>, you can search an entry within \(log(N)\) steps, where N is the number of stored entries.</li>
</ul>

<p>On a higher level, Audio Fingerprinting algorithms try to achieve a similar goal: <em>Convert an audio signal to the most compact and convenient format for storing and searching</em>.</p>

<p>Notice that the above conversion is an instance of <strong>lossless</strong> compression.
You can completely restore the initial sequence from the final value:</p>

<div><div><pre><code><span>int</span> <span>repeat</span> <span>=</span> <span>hash</span> <span>-</span> <span>(</span><span>1L</span> <span>&lt;&lt;</span> <span>32</span><span>);</span> <span>// 10, how many times to repeat</span>
<span>int</span> <span>val</span> <span>=</span> <span>(</span><span>hash</span> <span>-</span> <span>repeat</span><span>)</span> <span>/</span> <span>(</span><span>1L</span> <span>&lt;&lt;</span> <span>32</span><span>);</span> <span>// 1, the value to repeat</span>
<span>int</span> <span>[]</span> <span>data</span> <span>=</span> <span>Enumerable</span><span>.</span><span>Repeat</span><span>(</span><span>val</span><span>,</span> <span>repeat</span><span>).</span><span>ToArray</span><span>();</span>  <span>// initial data</span>
</code></pre></div></div>

<p>Audio Fingerprints do not require lossless encoding.
You don’t need to reconstruct the original signal knowing the fingerprints.
The only requirement is fingerprints with the same value should come from the same or similar sound.</p>

<p>Since we don’t need to reconstruct initial data, we can apply a different last transformation step on the tuple <code>(1, 10)</code>.</p>

<div><div><pre><code><span>// tuple (1, 10) getting hashed</span>
<span>string</span> <span>hash</span> <span>=</span> <span>(</span><span>repeat</span> <span>+</span> <span>val</span><span>).</span><span>ToString</span><span>();</span> <span>// &#34;11&#34;</span>
</code></pre></div></div>

<p>The hash value “11” ticks both boxes:</p>
<ul>
  <li>it is even more compact = 2 bytes, 20x reduction from the original size.</li>
  <li>it is convenient to store as a simple string.</li>
</ul>

<p>Hashing has its own cost.
You can’t reconstruct original tuple since you don’t know if <code>&#34;11&#34;</code> is 1 that repeats ten times, or is it 9 that repeats 2 times.
Even more so, we have 10 other tuples that hash into the same value of “11”.
These are called <strong>hash collisions</strong>.</p>

<p>Audio Fingerprints are similar.
Some of them are equal because they come from the same source, others because of a random hash collision.</p>

<h2 id="how-to-compress-an-audio-signal">How to compress an audio signal</h2>
<p>There is a wide variety of file formats <strong>mp3</strong>, <strong>ogg</strong>, <strong>flac</strong>, all of them generating different byte representations.
All fingerprinting algorithms decode the input file into a raw wav format first.
<em>SoundFingerprinting</em> is no exception.
Once we decode the audio file, we can focus on the following properties of the audio track:</p>

<ul>
  <li>number of channels - mono, stereo or other</li>
  <li>sample rate - typically 44,1KHz</li>
  <li>bit depth - 8, 16 or 32 bits per sample</li>
</ul>

<p>If we have a one-minute audio file, sampled at 44,1 KHz with 32 bits per sample, we need <code>21,168,000</code> bytes to store it: <code>2*60*44100*32/8</code>.
It amounts to 20MB of storage for just 1 minute of audio content.
The following compression steps minimize the size of the data 32x fold:</p>

<ul>
  <li>stereo to mono - 2x reduction</li>
  <li>44KHz to 5,5Khz - 8x reduction</li>
  <li>32 to 16-bit depth - 2x reduction</li>
</ul>

<p>The same 1-minute file now has 661,500 bytes ~0.63MB.
It is an example of <strong>lossy</strong> compression.
We are essentially getting rid of the rich quality in favor of a more compact format.
Perceptually, we can still hear the sounds and differentiate them.
It is just not as sharp and clean for regular use.
Below are some examples of the compressed formats<sub>1</sub>:</p>

<div>
	<div>
		<div>
			<table>
				<tbody><tr>
					<th>
						Sound
					</th>
					<th>
						Channels
					</th>
					<th>
						Sample Rate
					</th>
					<th>
						Bit Depth
					</th>
					<th>
						Size Reduction
					</th>
				</tr>
				<tr>
					<td>
						<audio id="orig-sweetheart" controls="controls">
							<source type="audio/mp3" src="/audio/how-it-works/sweetheart-s16.mp3"/>
							<p>Your browser does not support the audio element.</p>
						</audio>
					</td>
					<td>
						2
					</td>
					<td>
						44.1KHz
					</td>
					<td>
						16
					</td>
					<td></td>
				</tr>
				<tr>
					<td>
						<audio id="mono-sweetheart" controls="controls">
							<source type="audio/mp3" src="/audio/how-it-works/sweetheart-s16-mono.mp3"/>
							<p>Your browser does not support the audio element.</p>
						</audio>
					</td>
					<td>
						1
					</td>
					<td>
						44.1KHz
					</td>
					<td>
						16
					</td>
					<td>2x</td>
				</tr>
				<tr>
					<td>
						<audio id="mono-11khz-sweetheart" controls="controls">
							<source type="audio/wav" src="/audio/how-it-works/sweetheart-s16-mono-11KHz.wav"/>
							<p>Your browser does not support the audio element.</p>
						</audio>
					</td>
					<td>
						1
					</td>
					<td>
						11KHz
					</td>
					<td>
						16
					</td>
					<td>8x</td>
				</tr>
				<tr>
					<td>
						<audio id="mono-u8-11khz-sweetheart" controls="controls">
							<source type="audio/wav" src="/audio/how-it-works/sweetheart-u8-mono-11KHz.wav"/>
							<p>Your browser does not support the audio element.</p>
						</audio>
					</td>
					<td>
						1
					</td>
					<td>
						11KHz
					</td>
					<td>
						8
					</td>
					<td>16x</td>
				</tr>
				<tr>
					<td>
						<audio id="mono-u8-5khz-sweetheart" controls="controls">
							<source type="audio/wav" src="/audio/how-it-works/sweetheart-u8-mono-5KHz.wav"/>
							<p>Your browser does not support the audio element.</p>
						</audio>
					</td>
					<td>
						1
					</td>
					<td>
						5.5KHz
					</td>
					<td>
						8
					</td>
					<td>32x</td>
				</tr>
			</tbody></table>
		</div>
	</div>
	<div>
		<p><b>Table 1. </b><i>Let Me Call You Sweetheart</i> in different audio formats</p>
	</div>
</div>

<h2 id="what-acoustic-information-we-want-to-preserve">What acoustic information we want to preserve?</h2>
<p>Even after all these compression steps, the audio signal does not show any improvement in format convenience for storing and searching goals.</p>

<div><div><pre><code><span>// one minute mono file encoded in 8 bit depth sampled at 5512Hz</span>
<span>// is just an array of bytes of 165,360 length</span>
<span>byte</span><span>[]</span> <span>audio</span> <span>=</span> <span>{</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>,</span><span>5</span><span>,</span><span>6.</span><span>..}</span>
</code></pre></div></div>
<p>It is still unclear how you can search for a 10-second snippet picked at any location from this audio signal.</p>
<div><div><pre><code><span>// search query</span>
<span>byte</span><span>[]</span> <span>query</span> <span>=</span> <span>{</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>,</span><span>5</span><span>,</span><span>6.</span><span>..}</span> <span>// length of the array 10 * 5512 = 55,120 elements.</span>
</code></pre></div></div>
<p>Even if you correctly align the query snippet in the original file, element by element comparison will not work.
It is enough to increase the “loudness” in the query snippet by any factor, say 1.25x, and all your array values in the query will increase by the same factor - 1.25x.
As expected, Audio Fingerprinting algorithms should be resilient to sound intensity increase or decrease, as well as noise.</p>

<p>All audio fingerprinting algorithms handle this challenge by treating the sound as a spectrum of frequencies.
Sound is a pressure wave conveyed through some medium like air or water<sub>2</sub>.
Two tones are similar if the wave that describes them is similar.
The content of an audio signal array is just a discrete version of that wave.
If we are sampling at 5512Hz, we describe each second of the sound with 5512 samples.</p>

<p>To better understand how discretization works, see how it works for yourself.</p>

<div>
	
	<div>
		<p><b>Figure 2.</b> Sample a 1Hz wave at 10Hz rate</p>
	</div>
</div>

<p>The graph shows how 1 second of an audio signal is sampled in 10 points.
Technically, we sampled the signal at 10Hz rate.
The signal itself is just a pure tone since it is composed only of one function: \(sin(x)\).
Notice how the sinusoid completed 1 full cycle in 1 second on the <em>y-axis</em>.
It means that the sound wave vibrates with a frequency of 1Hz.</p>

<p>The file that you would like to fingerprint can be sampled at any rate, <em>SoundFingerprinting</em> will downsample it to 5512Hz.
We didn’t pick 5512Hz at random.
Within a discretization rate of 5512Hz, according to <a href="https://en.wikipedia.org/wiki/Nyquist–Shannon_sampling_theorem">Nyquist–Shannon</a> theorem, we will be able to capture frequencies from 0 to 2756Hz.
The theorem states that for a given sample rate \(f_{s}\) perfect reconstruction is guaranteed possible for a band limit \(B&lt;f_{s}/2\).</p>

<p>A healthy auditory system can capture frequencies between 20 to 22KHz. Why do we disregard such a big range [2756Hz, 22000Hz]?
Mostly because we trade high frequencies for a smaller footprint, moreover, out of the available frequency range [0Hz, 2756Hz], <em>SoundFingerprinting</em> takes into account only the domain between [300Hz, 2000Hz] filtering the rest.
Arguably, this range proved to work best for the task<sub>3</sub>.</p>

<p>You can play with the pure tone generator to get an idea of which frequency components are filtered and which ones are left.</p>

<div>
	
	<div>
		<p><b>Figure 3.</b> Pure tone generator</p>
	</div>
</div>

<p>In case you are familiar with the exact nature of the fingerprinted sound, you can change this range to include more frequency components.</p>

<p>One more reason to fingerprint only a fixed frequency range relates to musicology. Musicians often use A4 as a central note when tuning their instruments.
The modern concert pitch for this note is 440Hz, which aligns well with the fingerprinted range.</p>

<div>
	<div>
		<div>
			<table>
				<tbody><tr>
					<th>Note    </th>
					<th>Frequency Hz</th>
				</tr>
				<tr>
					<td>A4</td>
					<td>440</td>
				</tr>
				<tr>
					<td>B4</td>
					<td>493</td>
				</tr>
				<tr>
					<td>C5</td>
					<td>523</td>
				</tr>
				<tr>
					<td>D5</td>
					<td>587</td>
				</tr>
				<tr>
					<td>E5</td>
					<td>659</td>
				</tr>
				<tr>
					<td>F5</td>
					<td>698</td>
				</tr>
				<tr>
					<td>G5</td>
					<td>783</td>
				</tr>
				<tr>
					<td>A5</td>
					<td>880</td>
				</tr>
			</tbody></table>
		</div>
	</div>
	<div>
		<p><b>Table 2. </b>Notes tuned around A4 at 440Hz frequency</p>
	</div>
</div>

<p>From the auditory system perspective, different musical instruments can sound different even when playing the same note.
It happens because of the harmonics that they emit alongside the pure tone.
Harmonics are additional frequencies that lie at multiples of the fundamental frequency. An instrument that plays A4 generates 440Hz pure tone and harmonics at 880 Hz, 1320 Hz, 1760 Hz.</p>

<p>Click to play an example of A4 that plays as pure tone (0-5 seconds), then with odd harmonics (5-10 seconds), then will all harmonics (10-15 seconds).</p>

<div>
	<div>
		<p><audio id="pure_tone" controls="controls">
				<source type="audio/ogg" src="/audio/how-it-works/440HZ_tone.ogg"/>&lt;/source&gt;
				<p>Your browser does not support the audio element.</p>
			</audio>
			
		</p>
	</div>
	<div>
		<div>
			<picture>
				<source srcset="/images/how-it-works/pure-tone_small.webp 800w,
				                /images/how-it-works/pure-tone.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
				<source srcset="/images/how-it-works/pure-tone_small.png 800w,
				                /images/how-it-works/pure-tone.jpg 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
				<img src="https://blog.rowan.earth/images/how-it-works/pure-tone_small.png" width="100%" alt="Spectrogram of a pure tone with odd and all harmonics."/>
			</picture>
		</div>
	</div>
	<div>
		<p><b>Figure 4. </b>440Hz sound as pure tone 0 to 5th second, with odd 5th to 10th second and all harmonics 10 to 15 second</p>
	</div>
</div>

<p>From the fingerprinting perspective, we are not very interested in high-end harmonics.
They are undoubtedly important for our perception but not critical for pattern recognition tasks.
That’s why we filter them.</p>

<p>Similarly, low-end bass frequencies are usually capturing environmental noise.
Check a typical airplane noise.</p>

<div>
	<div>
		<p><audio id="pure_tone" controls="controls">
				<source type="audio/mp3" src="/audio/how-it-works/airplane.mp3"/>&lt;/source&gt;
				<p>Your browser does not support the audio element.</p>
			</audio>
		</p>
	</div>
	<div>
		<div>
			<picture>
				<source srcset="/images/how-it-works/airplane-noise-3_small.webp 800w,
				                /images/how-it-works/airplane-noise-3.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
				<source srcset="/images/how-it-works/airplane-noise-3_small.png 800w,
				                /images/how-it-works/airplane-noise-3.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
				<img src="https://blog.rowan.earth/images/how-it-works/airplane-noise-3_small.png" width="100%" alt="Spectrogram of an airplane noise that doesn&#39;t enter fingerprinted range."/>
			</picture>
		</div>
	</div>
	<div>
		<p><b>Figure 5. </b>Airplane noise doesn&#39;t enter fingerprinted range</p>
	</div>
</div>

<p>Since we start fingerprinting above 300Hz, whatever is lower is filtered, not affecting generated fingerprints.</p>

<p>Let’s take a look at a typical 15 seconds conversion between two people.
White lines outline the frequency band, which is used by default by <em>SoundFingerprinting</em> library to build audio fingerprints.</p>

<div>
    <div>
        <picture>
            <source srcset="/images/how-it-works/frequency-band_small.webp 800w,
                            /images/how-it-works/frequency-band.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
            <source srcset="/images/how-it-works/frequency-band_small.png 800w,
                            /images/how-it-works/frequency-band.jpg 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
            <img src="https://blog.rowan.earth/images/how-it-works/frequency-band_small.png" width="100%" alt="Spectrogram of a conversation. White horizontal lines outline the range used for fingerprinting [300Hz, 2000Hz]"/>
        </picture>
    </div>
</div>
<div>
    <p><b>Figure 6.</b> Spectrogram of a conversation. White horizontal lines outline the range used for fingerprinting [300Hz, 2000Hz].</p>
</div>

<p>The brighter is the area, the more <em>acoustic information</em> is present in the frequency ranges shown in the image.</p>

<p>In other words, we are mostly interested in <em>frequency peaks</em> that contain the most relevant <em>information</em> we would like to preserve (shown in white).</p>

<div>
    <div>
        <picture>
            <source srcset="/images/how-it-works/frequency-peaks_small.webp 800w,
                            /images/how-it-works/frequency-peaks.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
            <source srcset="/images/how-it-works/frequency-peaks_small.png 800w,
                            /images/how-it-works/frequency-peaks.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
            <img src="https://blog.rowan.earth/images/how-it-works/frequency-peaks_small.png" width="100%" alt="White areas contain most valuable acoustic information we want to preserve"/>
        </picture>
    </div>
</div>
<div>
    <p><b>Figure 7.</b> White areas contain most valuable acoustic information we want to preserve.</p>
</div>

<p>All audio fingerprinting algorithms are hunting these peaks, with various techniques and approaches.</p>

<h2 id="overview-of-image-search">Overview of image search</h2>

<p>Once we’ve identified what kind of information we would like to preserve, the main challenge is encoding it in an efficient format for storing and searching.
If we look at the previous spectrogram examples, they can be treated as three-dimensional objects:</p>
<ul>
  <li><em>x-axis</em> - time in seconds</li>
  <li><em>y-axis</em> - frequency in Hertz</li>
  <li>the amplitude of the frequency - the intensity of the color</li>
</ul>

<p>The most intuitive way to encode this information is simply in an <strong>image</strong>.</p>

<p>At this stage, many people are getting confused.
We are transitioning from an audio domain to a visual domain simply because there is no better way to encode a spectrogram than in an image.
Similar spectrogram images bear similar sounds.</p>

<p>Notice how image search algorithms try to solve a similar problem:</p>

<ul>
  <li><em>identify</em> features which best describe the image</li>
  <li><em>encode identified features</em> in an easy to use format for storing and searching</li>
</ul>

<div>
    <p>
Most sophisticated image search algorithms are constrained by the requirement of being scale and rotation invariant.
Let&#39;s take <abbr>ORB</abbr><sup>1</sup> as one of the feature extraction algorithms.
ORB builds a set of keypoints from the image, which are preserved even if the image is scaled or rotated.
    </p>
    <p><b>1.</b> ORB - Oriented FAST and Rotated BRIEF, alternative to SIFT and SURF in computation cost, matching performance and mainly the patents.
    </p>
</div>
<div>
	<div>
        <picture>
            <source srcset="/images/how-it-works/orb_points_small.webp 800w,
                            /images/how-it-works/orb_points.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
            <source srcset="/images/how-it-works/orb_points_small.png 800w,
                            /images/how-it-works/orb_points.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
            <img src="https://blog.rowan.earth/images/how-it-works/orb_points_small.png" width="100%" alt="ORB keypoints preserved during rotation and scale"/>
        </picture>
	</div>
</div>
<div>
	<p><b>Figure 8.</b> ORB keypoints preserved during rotation and scale</p>
</div>

<p>This property is incredibly useful for various use-cases but, at no surprise, comes with computational costs.
Fortunately, we do not need features that are immune to scaling or rotation.</p>

<p>Spectrogram images have an advantageous attribute: <strong>their scale and orientation is stable</strong>.
Extracting features from such images is much easier.</p>

<p>The <strong>height</strong> of a spectrogram image equals to the frequency resolution.
<em>Y-axis</em> is encoded in 32 log-spaced bins, which yield 32 pixels.</p>

<p>The <strong>width</strong> is chosen empirically.</p>
<div>
	<div>
        <picture>
            <source srcset="/images/how-it-works/cut-spectrums_small.webp 800w,
                            /images/how-it-works/cut-spectrums.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
            <source srcset="/images/how-it-works/cut-spectrums_small.png 800w,
                            /images/how-it-works/cut-spectrums.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
            <img src="https://blog.rowan.earth/images/how-it-works/cut-spectrums_small.png" width="100%" alt="19 spectrograms cut from an audio file each 128x32 pixels in size"/>
        </picture>
	</div>
</div>
<div>
	<p><b>Figure 9.</b> 19 spectrograms cut from an audio file each 128x32 pixels in size</p>
</div>
<p>Since <em>x-axis</em> in a spectrogram corresponds to the time domain, the width of an image is equal to the length of the cut in seconds.
<em>SoundFingerprinting</em> uses 8192 audio samples per fingerprint, which corresponds to 1.48s.
The spectrogram uses 64 samples overlap, meaning the intensity of one pixel corresponds to the frequency resolution of 11.6ms.
It yields an image that is 128x32 pixels, where every pixel is between 0-255 range.</p>

<p>These parameters may be quite confusing if you are reading them for the first time.
Don’t worry, just keep in mind that the spectrogram image is essentially an interpretation of how sound frequencies change over time.</p>

<p>We could have easily chosen a shorter timespan, say 4096 samples.
In this case, the width of the image would have been 64 pixels.
Choosing these values is a matter of parameter tuning.</p>

<p>To summarize, the images we are trying to detect are of the same size and orientation.
<em>Features</em> that make them unique are spectrogram peaks. So how do we encode them?</p>

<h2 id="encoding-image-features">Encoding Image Features</h2>
<p>Even though we have compressed relevant acoustic information of 1.48 seconds into the 128x32 image, it is still not an easy format to search.
A vector of size 4096 is just too big for the task.</p>

<div><div><pre><code><span>// every image is 4096 bytes</span>
<span>byte</span><span>[][]</span> <span>images</span> <span>=</span> <span>GetSpectrogramImages</span><span>(</span><span>audioFile</span><span>);</span>
</code></pre></div></div>

<p>To give a perspective, one descriptor in the ORB algorithm is a byte vector of length 32.
It makes them quite easy to search in a vast database of descriptors.</p>

<p>Since the focus is on frequency peaks, one easy way to reduce the dimensions of our image but keep the peaks is <em>thresholding</em>.
The thresholding methods replace each pixel in an image with either:</p>

<ul>
  <li>a black pixel if the image intensity \(I_{i,j}\) is less than some fixed constant T</li>
  <li>or a white pixel if the image intensity is greater than T.</li>
</ul>

<p>Thresholding gives the advantage of getting a very sparse representation of the same byte array, with intensity either 0 or 255, which can be easily encoded as booleans 0 or 1.</p>

<div>
	<p><img src="https://blog.rowan.earth/images/how-it-works/spectrums-thresholded.png" alt="Thresholded spectrogram images. Threshold T equals to 165, resulting intensity values encoded as 0 or 1" width="100%"/>
	</p>
</div>
<div>
	<p><b>Figure 10.</b> Thresholded spectrogram images. Threshold T equals to 165, resulting intensity values encoded as 0 or 1</p>
</div>

<p>Query images that are having <em>white contours</em> in the same image <em>coordinates</em> can be treated as similar images, thus similar sound.
One other approach is instead of thresholding the image, we can use <em>Haar Wavelets</em> to achieve a similar goal: identify frequency peaks.
Haar Wavelets have been successfully used in image search for quite some time<sub>4</sub>.</p>

<p>One naive approach when searching a spectrogram image in a database of stored fingerprints is to iterate over all them comparing one by one.
It will take \(N^2\) comparisons, and soon will become unfeasible even for a database that contains few images.
To give a perspective, 1000 hours of audio content will generate around 38 million fingerprint images.</p>

<p>Thankfully, there has been significant progress in the Approximate Nearest Neighbor Search in the last years<sub>5</sub>.
Let us analyze <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">locality-sensitive hashing</a>.</p>

<h2 id="hashing-with-locality-sensitive-hashing">Hashing with Locality Sensitive Hashing</h2>
<blockquote>
  <p>The origin of the word “locality” is “localis” -  from French localité or late Latin localitas, from localis ‘relating to a place’</p>
</blockquote>

<p>Locality Sensitive Hashing is an algorithmic technique that tries to build a <em>forgiving hash</em> data structure.
Its primary purpose is to ignore small differences between close points and hash them into the same bucket.
All the points that are “related to the same place” should have an equal hash.</p>

<div>
	<p><img src="https://blog.rowan.earth/images/how-it-works/images-to-points.png" alt="Two images hashed in two points within distance R" width="100%"/>
	</p>
</div>
<div>
	<p><b>Figure 11.</b> Two images hashed in two points within distance R</p>
</div>

<p>One of the most important choices we need to make when designing the <em>LSH</em> data structure is defining the <em>distance metric</em>.
In other words, how sensitive our hash function should be to the proximity of the data points?
In the <em>SoundFingerprinting</em> case we can take advantage of the two things:</p>
<ul>
  <li>the <code>0,1</code> domain, our feature vector is a bit-vector</li>
  <li>the sparsity of the data</li>
</ul>

<p>Knowing these properties allows us to treat our images as <em>sets</em>.
And for sets, we can use the <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard Coefficient</a> as our distance metric.</p>

<div>
    <p>
I&#39;ve always found it difficult to explain how sets can be partitioned using locality sensitive hashing.
Let me explain what kind of LSH techniques are used for geometric spaces first.
Geometric vectors are much easier to visualize since we can use the Euclidian distance<sup>2</sup> function to denote proximity between points.
    </p>
    <p><b>2.</b> Euclidean distance or Euclidean metric is the &#34;ordinary&#34; straight-line distance between two points.
    </p>
</div>

<p>Ideally, we want to partition our points into different hash buckets such that only those points <em>related</em> to each other, hash into the same bin.</p>

<div>
	<div>
        <picture>
            <source srcset="/images/how-it-works/perfect-hashing_small.webp 800w,
                            /images/how-it-works/perfect-hashing.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
            <source srcset="/images/how-it-works/perfect-hashing_small.png 800w,
                            /images/how-it-works/perfect-hashing.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
            <img src="https://blog.rowan.earth/images/how-it-works/perfect-hashing_small.png" width="100%" alt="Point P&#39; and Q&#39; hash into the same bucket"/>
        </picture>
	</div>
</div>
<div>
	<p><b>Figure 12.</b> <code>p&#39;</code> and <code>q&#39;</code> hash into the same bucket</p>
</div>

<div>
    <p>
Unfortunately, this type of perfect partition is not attainable.
Designing an ideal partition function will take longer than doing a simple element-by-element search through the entire dataset<sup>3</sup>.
    </p>
    <p><b>3. </b> Similarly how <i>k-means</i> clustering is a NP-hard problem.
    </p>
</div>

<p>To solve this problem, <a href="https://www.cs.princeton.edu/courses/archive/spr04/cos598B/bib/IndykM-curse.pdf">researchers</a> have introduced <em>randomized partitioning</em>.
For each different randomized partition, the points will hash into different hash buckets, creating different hash indexes.</p>

<div>
	<p><img src="https://blog.rowan.earth/images/how-it-works/partitions.gif" alt="Three different partitions create different indexes" width="100%"/>
	</p>
</div>
<div>
	<p><b>Figure 13</b>. Three different partitions create different indexes</p>
</div>

<p>The number of randomized partitions governs the probability of getting a successful match \(p\) from a similar query point \(q\).
Since we can’t have a perfect partitioning, LSH data structure will give us a probability of a successful match, with respect to how many partitions we take.</p>

<p>The size of the dataset and the points’ dimensions govern the number of random projections.
Generally speaking, the more partitions you take, the higher the probability of a successful match between points that are within close distance.</p>

<div>
    <p>
A curious reader might ask why we haven&#39;t chosen <i>Hamming</i> distance<sup>4</sup> as we are already operating with binary vectors?
The reason is that the resulting images are very sparse<sub>6</sub>.
Consider two completely zeroed spectrogram images <code>(p,q)</code> (that is, full or partial <b>silence</b>).
The <i>Hamming</i> distance between the two will be zero, <i>reflecting</i> complete similarity.
    </p>
    <p><b>4.</b> <i>Hamming</i> distance between two bit-vectors measures number of positions with different bit values
    </p>
</div>

<p>We should not treat silence as a <em>similarity signal</em>.
It can produce too many false positives, as any recorded discussion is full of small pauses.
That’s why choosing the <em>Jaccard Coefficient</em> is a better choice.
Comparing to a <em>Hamming</em> distance, two spectrograms with complete <em>silence</em> will have a <em>Jaccard Coefficient</em> equal to <em>1</em>, signaling total dissimilarity.</p>

<h3 id="jaccard-coefficient-as-a-distance-metric">Jaccard Coefficient as a distance metric</h3>
<p><em>Jaccard Index</em> is a statistic used for indicating how common two sets are.
It is defined as <em>Intersection over Union</em> of measured sets.</p><p>

\[J(Q,P)  = \frac{|Q \cap P|}{|Q \cup P|}\]

</p><p>As an analogy, think about how a book store can treat a profile of their customer.
A profile may contain a set of past purchased books out of all available titles: <code>bool[] purchasedBooks</code>.
Every index of that bit-vector would map to a particular title.
For example, <code>purchasedBooks[2]</code> would tell us if the customer bought the third title in the bookstore.</p>

<p>To compare two different customer profiles, we could easily count how many values of <code>1</code> are in common between these profiles and divide it by the total number of purchased books.</p>
<div><div><pre><code><span>public</span> <span>double</span> <span>CalculateJaccardIndex</span><span>(</span><span>string</span> <span>customerProfileP</span><span>,</span> <span>string</span> <span>customerProfileQ</span><span>)</span>
<span>{</span>
    <span>bool</span><span>[]</span> <span>q</span> <span>=</span> <span>GetProfile</span><span>(</span><span>customerProfileP</span><span>);</span> <span>// get customer profile</span>
    <span>bool</span><span>[]</span> <span>p</span> <span>=</span> <span>GetProfile</span><span>(</span><span>customerProfileQ</span><span>);</span> <span>// get customer profile</span>

    <span>var</span> <span>zipped</span> <span>=</span> <span>q</span><span>.</span><span>Zip</span><span>(</span><span>p</span><span>);</span>
    <span>int</span> <span>intersection</span> <span>=</span> <span>zipped</span><span>.</span><span>Count</span><span>(</span><span>tuple</span> <span>=&gt;</span> <span>tuple</span><span>.</span><span>First</span> <span>&amp;&amp;</span> <span>tuple</span><span>.</span><span>Second</span><span>);</span>
    <span>int</span> <span>union</span> <span>=</span> <span>zipped</span><span>.</span><span>Count</span><span>(</span><span>tuple</span> <span>=&gt;</span> <span>tuple</span><span>.</span><span>First</span> <span>||</span> <span>tuple</span><span>.</span><span>Second</span><span>);</span>
    <span>if</span><span>(</span><span>union</span> <span>==</span> <span>0</span><span>)</span>
        <span>return</span> <span>0d</span><span>;</span>
    <span>return</span> <span>(</span><span>double</span><span>)</span><span>intersection</span> <span>/</span> <span>union</span><span>;</span>
<span>}</span>
</code></pre></div></div>
<p>The more values of <code>1</code> are equal, the more likely these customers have the same book preferences.
Not surprisingly, this is how many recommendation engines work.
Google used this metric to personalize visitor’s <a href="https://www2007.org/papers/paper570.pdf">news feed</a>.</p>

<p>To turn the <em>Jaccard Index</em> into a distance metric we can use its complimentary.
It is known as the <em>Jaccard Coefficient</em>.</p>
<div><div><pre><code><span>public</span> <span>double</span> <span>CalculateJaccardCoefficient</span><span>(</span><span>string</span> <span>customerProfileP</span><span>,</span> <span>string</span> <span>customerProfileQ</span><span>)</span>
<span>{</span>
    <span>return</span> <span>1</span> <span>-</span> <span>CalculateJaccardIndex</span><span>(</span><span>customerProfileP</span><span>,</span> <span>customerProfileQ</span><span>);</span>
<span>}</span>
</code></pre></div></div>
<p>So for two customer profiles with the same list of purchases, the <em>Jaccard Coefficient</em> will be <code>0</code>, meaning they bought the exact same titles.
A value of <code>1</code> means customers share zero common books.</p>

<p>Now that we know how the <em>Jaccard Coefficient</em> can measure the similarity between sets let’s get back to spectrogram images.
We’ve compressed our image into a 4096 bit-vector: <code>bool[] frequencyPeaks</code>.
At any position in this vector, the value of <code>1</code> tells us if the original sound contained a frequency peak at that location.
We are not interested in the actual <em>amplitude</em> of that particular peak.
The mere presence of the frequency peak is enough for us to differentiate between the sounds.</p>

<p>Not surprisingly, treating our images are sets, gives us the power to use the <em>Jaccard Coefficient</em> as a measure of similarity.
The smaller the distance’s value, the more frequency peaks are shared between the query and the database sounds.</p>

<p>Below images outline why the <em>Jaccard</em> metric is a better choice for identifying dissimilarities.
<em>Hamming</em> considers unset bits as similar, which is not the case for thresholded images.</p>
<div>
	<div>
        <picture>
            <source srcset="/images/how-it-works/hamming-cs-jaccard_small.webp 800w,
                            /images/how-it-works/hamming-cs-jaccard.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
            <source srcset="/images/how-it-works/hamming-cs-jaccard_small.png 800w,
                            /images/how-it-works/hamming-cs-jaccard.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
            <img src="https://blog.rowan.earth/images/how-it-works/hamming-cs-jaccard_small.png" width="100%" alt="Jaccard vs Hamming distance metric. When bit-vectors are sparse Jaccard Coefficient outlines dissimilarities better."/>
        </picture>
	</div>
</div>
<div>
	<p><b>Figure 14.</b> Jaccard vs Hamming distance metric. When bit-vectors are sparse Jaccard Coefficient outlines dissimilarities better.</p>
</div>

<p>Since we are not operating in geometric space, instead of making random projections we will apply random set permutations known as <a href="https://en.wikipedia.org/wiki/MinHash">Min-Hashing</a> as our LSH schema.</p>

<h2 id="random-permutations-with-min-hashing">Random permutations with Min Hashing</h2>

<p>To calculate the min hash of a bit-vector, we permute the vector and find the index of the first 1.
The index itself is our hash value: \(h_i(P) = \textbf{index of first 1}\).</p>

<p>Since one permutation is not enough to use as an effective partition schema, final hash function \(g(P)\) will be a concatenation of $k$ min-hashes<sub>7</sub>.</p><p>

\[g(P)=\langle\ h_1(P),h_2(P),h_3(P),...,h_k(P)\rangle\]

</p><p>Given the following bit-vector, click on the <em>Hash</em> button to find three min-hash values and final \(g(P)\), used in one of \(L\) hash tables.
Every time you click on <em>Permute</em> a new \(g(P)\) hash-function will be generated.</p>





<p>How is this helpful?
Actually, it can be shown that the probability of two min-hash functions to be equal is equal to <em>Jaccard Coefficient</em><sub>8</sub>.</p><p>

\[Pr[h_i(P) = h_i(Q)] = J(P,Q)\]

</p><p>It is an incredibly powerful property.
We can now estimate how many times points \(P\) and \(Q\) will partition into the same bin according to how similar those points are.
Our distance function drives the probability of collision.
Exactly what we need for locality-sensitive partition.</p>

<p>Let’s see in practice how min-hash functions behave for two similar points.
Points \(P\) and \(Q\) have a <em>Jaccard Index</em> equal to 2/3 (two out of three bits are equal).</p>

<p>Click on the <em>Simulate</em> button, to hash these points 1000 times and count how many times hash values are equal.</p>






<p>After achieving locality-sensitive hashing for our images, we are almost done.
The last step is using \(L\) hash functions, one for every hash table: \(g_L(P)\\)</p>

<p>How many hash tables do we need?
Finding optimal values is a matter of experimentation.
<em>SoundFingerprinting</em> library uses 100 permutations to create 25 indexes.
It means every hash value is a concatenation of 4 min hash functions.
Every index is stored in a conventional hash-table.</p>

<h2 id="what-prevents-successful-matches">What prevents successful matches?</h2>
<blockquote>
  <p>“Why the songs do not match?”</p>
</blockquote>

<p>One of the most common questions I receive is, “Why the songs don’t match?”
<em>SoundFingerprinting</em> hunts frequency peaks at certain locations in time.
When a sound does not produce any matches, it means no frequency peaks matched with what you have in the database.
The following four reasons cover the vast majority of use-cases why the query didn’t produce any matches.</p>

<h4 id="aliasing">Aliasing</h4>
<blockquote>
  <p>Latin origin is <strong>alias</strong> - ‘at another time’</p>
</blockquote>

<p>A common problem that frequently occurs with low-quality recording equipment is <em>aliasing</em>.
When a high-frequency sound is recorded at a specific sample rate, all frequencies that are bigger than <em>Nyquist Frequency</em> have to be suppressed or filtered.
Otherwise, high-frequency components will appear in the spectrogram as their low-frequency counterparts.</p>

<p>For an intuitive explanation, let’s visualize this phenomenon in videos.
It is called the <em>wagon wheel effect</em>, an illusion that the wheel rotates backward.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/jHS9JGkEOmA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>When a video capturing device records at N frames per second, any object in the video which rotates with a frequency &gt;= N/2, will exhibit this behavior.</p>

<p>The exact problem is present when recording sound.
If you don’t need to capture with high fidelity, your recording device still has to be able to filter high frequencies.</p>

<div>
    <p>
    How can we spot <i>aliasing</i>?
    The most common way to spot issues with your downsampler or recording device is with a chirp<sup>5</sup> signal.
    </p>
    <p><b>5.</b> A chirp is a signal in which the frequency increases with time
    </p>
</div>

<p>That’s how a spectrogram looks for a chirp signal.</p>

<div>
	<div>
		<p><audio id="chirp-200-11Khz" controls="controls">
				<source type="audio/ogg" src="/audio/how-it-works/chirp.ogg"/>&lt;/source&gt;
				<p>Your browser does not support the audio element.</p>
			</audio>
			
		</p>
	</div>
	<div>
		<div>
			<picture>
				<source srcset="/images/how-it-works/chirp-spectrogram-small_small.webp 800w,
								/images/how-it-works/chirp-spectrogram-small.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
				<source srcset="/images/how-it-works/chirp-spectrogram-small_small.png 800w,
								/images/how-it-works/chirp-spectrogram-small.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
				<img src="https://blog.rowan.earth/images/how-it-works/chirp-spectrogram-small_small.png" width="100%" alt="Chirp Spectrogram"/>
			</picture>
		</div>
	</div>
	<div>
		<p><b>Figure 14.</b> Spectrogram of a chirp signal, 200-11025Hz</p>
	</div>
</div>

<p>Notice how the frequency of the sound increases with time.</p>

<p>Now let’s downsample this signal to 5512Hz.
In an ideal case, there shouldn’t be any signal after ~5th second, since it contains only frequencies bigger than 2756Hz after that.
We will downsample without suppressing high-frequencies.</p>

<div>
	<div>
		<p><audio id="chirp-downsampled" controls="controls">
				<source type="audio/wav" src="/audio/how-it-works/chirp-5512Hz.wav"/>&lt;/source&gt;
				<p>Your browser does not support the audio element.</p>
			</audio>
		</p>
	</div>
	<div>
		<div>
			<picture>
				<source srcset="/images/how-it-works/chirp-downsampled_small.webp 800w,
								/images/how-it-works/chirp-downsampled.webp 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/webp"/>
				<source srcset="/images/how-it-works/chirp-downsampled_small.png 800w,
								/images/how-it-works/chirp-downsampled.png 1024w" sizes="(max-width: 600px) 50vw, 100vw" type="image/png"/>
				<img src="https://blog.rowan.earth/images/how-it-works/chirp-downsampled_small.png" width="100%" alt="Chirp Downsampled Spectrogram"/>
			</picture>
		</div>
	</div>
	<div>
		<p><b>Figure 15.</b> Chirp signal downsampled to 5512Hz. </p>
	</div>
</div>

<p>Since we are using a naive downsampler, notice how after 5th second, high frequencies are seen as their complementary.
That’s how aliasing works for the sound.
High frequencies suddenly appear “at another time.”</p>

<p>Before trying to match sounds from the microphone, make sure your equipment does not alias the signal.</p>

<h4 id="clipping">Clipping</h4>
<p>Clipping is cutting short the amplitude of the signal at its maximum.</p>

<p>Say your bit depth is a signed short 16 bit.
It means that the largest value you can represent with this encoding is 32767.
After you record your sound, the digital file gets through a set of processing steps before it is saved on the disk.
If any of these transformations increase the original sound’s amplitude, you may end up clipping your signal.</p>

<p>As an example, if you double the amplitude of a signal, you may overdrive some of the values beyond the limit (i.e. 32767).
It will saturate the signal, and it will get clipped at the maximum value.</p>

<p>How to detect clipping?
It is not so difficult. Clipping creates an audio effect known as <em>distortion</em>.
Listen to the next audio and notice how certain instruments (specifically those generating high-pitched sounds) create a distorted sound.</p>

<div>
    <p><audio id="sweetheart-clipped" controls="controls">
            <source type="audio/mp3" src="/audio/how-it-works/sweetheart-s16-mono-clipped.mp3"/>&lt;/source&gt;
            <p>Your browser does not support the audio element.</p>
        </audio>
    </p>
</div>

<p>This type of distortion introduces frequency peaks in locations not present in the original signal, affecting the recognition rate.</p>

<h4 id="tempo-change">Tempo Change</h4>
<p>The tempo of the music is the speed of the underlying beat.
It is measured in beats per minute.
Remixed songs quite often change the beat to adhere to a particular style of music.
Notice in the following examples how remixed version increased the tempo by 30 bpm.</p>

<p>The initial tempo of the song is roughly 90 bpms.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/cxjvTXo9WWM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Listen to the remixed version, which runs at 120 bpm. To notice the change wait for the chorus.</p>



<p>Tempo change shortens the distance between frequency peaks.
The initial song will not be recognized when querying with a remixed version.
The easiest solution is to fingerprint the same song with changed tempo.</p>

<h4 id="frequency-range">Frequency Range</h4>
<p><em>SoundFingerprinting</em> uses a 300-2000Hz range for creating audio fingerprints.
If your sound contains specific differences that are outside of this range, it will not be recognized.</p>

<p>I received multiple times question about fingerprinting “birds singing.”
I didn’t dive deep into its intricacies, though the first thing that I would look at is their frequency range.
To make it work, you would need to make sure you are fingerprinting the right range</p>

<p>It also applies to more specific use-cases when the initial sound contains high-pitched audio or deep bass.</p>

<h2 id="final-thoughts">Final Thoughts</h2>
<p>Hope you enjoyed reading the explanation of Audio Fingerprinting.</p>

<p>To support my efforts in developing <em>SoundFingerprinting</em> further, please star the project on GitHub.</p>

<p>The core fingerprinting algorithm is MIT licensed, so you can use, change, and distribute it freely.</p>

<p>If you are a business that needs scalable storage for the fingerprints, contact me at sergiu [at] emysound (dot) com.
Subscribe to this blog, where I post articles about algorithms and more.</p>

<h5 id="references">References</h5>

<div>
    <div>
        <p>1 - <a href="https://mtlsites.mit.edu/Courses/6.050/2008/notes/mp3.html" target="_blank">6.050J/2.110J – Information, Entropy and Computation – Spring 2008</a></p>
        <p>2 - <a href="https://jackschaedler.github.io/circles-sines-signals/sound.html" target="_blank">Seeing circles, sines, and signals</a></p>
        <p>3 - J. Haitsma, T. Kalker (2002). A Highly Robust Audio Fingerprinting System. Proc. International Conf. Music Information Retrieval.</p>
        <p>4 - Jacobs, Finkelstein, A., Salesin, D. Fast Multiresolution Image Querying</p>
        <p>5 - Piotr Indyk, Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.</p>
        <p>6 - Shumeet Baluja, Michele Covell. Content Fingerprinting Using Wavelets.</p>
        <p>7 - Mining of Massive Datasets, LSH for Minhash Signatures, p.88</p>
        <p>8 - Mining of Massive Datasets, Minhashing and Jaccard Similarity, p.83</p>
    </div>
</div>



        <hr/>

        

      </div>
    </div>
  </div></div>
  </body>
</html>
