<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/">Original</a>
    <h1>Purple Llama by Meta AI</h1>
    
    <div id="readability-page-1" class="page"><div><p>Cybersecurity and LLM prompt safety are important areas for generative AI safety today. We have prioritized these considerations in our first party products and highlighted them as best practice in the Llama 2 <a href="https://ai.meta.com/llama/responsible-use-guide/" target="_blank" data-lnfb-mode="ie">Responsible Use Guide</a>.</p><p>Cybersecurity</p><div><p>We are sharing what we believe is the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&amp;CK) and built in collaboration with our security subject matter experts. With this initial release, we aim to provide tools that will help address some risks outlined in the <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/" target="_blank" data-lnfb-mode="ie">White House commitments on developing responsible AI</a>, including:</p><ul><li>Metrics for quantifying LLM cybersecurity risks.</li><li>Tools to evaluate the frequency of insecure code suggestions.</li><li>Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks.</li></ul><p>We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our <a href="https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/" target="_blank" data-lnfb-mode="ie">Cybersec Eval paper</a> for more details.</p><br/></div><p>Input/Output Safeguards</p><div><p>As we outlined in Llama 2’s <a href="https://ai.meta.com/llama/responsible-use-guide/" target="_blank" data-lnfb-mode="ie">Responsible Use Guide</a>, we recommend that all inputs and outputs to the LLM be checked and filtered in accordance with content guidelines appropriate to the application.</p><p>To support this, and empower the community, we are releasing <a href="https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/" target="_blank" data-lnfb-mode="ie">Llama Guard</a>, an openly-available model that performs competitively on common open benchmarks and provides developers with a pretrained model to help defend against generating potentially risky outputs.</p><p>As part of our ongoing commitment to open and transparent science, we are releasing our methodology and an extended discussion of model performance in our <a href="https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/" target="_blank" data-lnfb-mode="ie">Llama Guard paper</a>. This model has been trained on a mix of publicly-available datasets to enable detection of common types of potentially risky or violating content that may be relevant to a number of developer use cases. Ultimately, our vision is to enable developers to customize this model to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem.</p><br/></div><p><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.2365-6/407630966_890117059136270_2844720574287423651_n.png?_nc_cat=1&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=-UAAGZ41x0oAX9Va8B4&amp;_nc_ht=scontent-iad3-1.xx&amp;oh=00_AfDQ6qb_arS56jXH_833blE2hRMjCk3ybxfSXaj4sn9yiQ&amp;oe=658C4733" alt="" id="u_0_a_TO"/></p><p>Why purple?</p><p>We believe that to truly mitigate the challenges that generative AI presents we need to take both attack (red team) and defensive (blue team) postures. <a href="https://danielmiessler.com/p/red-blue-purple-teams/" target="_blank" data-lnfb-mode="ie">Purple teaming</a>, composed of both red and blue team responsibilities, is a collaborative approach to evaluating and mitigating potential risks. The same ethos applies to generative AI. Hence, our investment in Purple Llama will be comprehensive.</p><p>An open ecosystem</p><div><p>Taking an open approach to AI is not new for Meta. Exploratory research, open science, and cross-collaboration are foundational to our AI efforts, and we believe there’s an important opportunity to create an open ecosystem. This collaborative mindset was at the forefront when <a href="https://ai.meta.com/blog/llama-2/" target="_blank" data-lnfb-mode="ie">Llama 2 launched in July</a> with over 100 partners, and we’re excited to share that many of those same partners will be partnering with us on open trust and safety, including: AI Alliance, AMD, Anyscale, AWS, Bain, CloudFlare, Databricks, Dropbox, Google Cloud, Hugging Face, IBM, Intel, Microsoft, MLCommons, Nvidia, Oracle, Orange, Scale AI, Together.AI, and many more to come.</p><p>We’ve also worked with our partners at <a href="https://paperswithcode.com/" target="_blank" data-lnfb-mode="ie">Papers With Code</a> and <a href="https://crfm.stanford.edu/helm/latest/" target="_blank" data-lnfb-mode="ie">HELM</a> to incorporate these evals into their benchmarks, alongside our collaborators within the <a href="https://mlcommons.org/2023/10/mlcommons-announces-the-formation-of-ai-safety-working-group/" target="_blank" data-lnfb-mode="ie">MLCommons AI Safety Working Group</a>.</p><p>We’re excited to collaborate with each and every one of our partners as well as others who share the same vision of an open ecosystem of responsibly-developed generative AI.</p></div><p>The path forward</p><p>We are hosting a <a href="https://nips.cc/" target="_blank" data-lnfb-mode="ie">workshop at NeurIPs 2023</a>, where we plan to share these tools and provide a technical deep dive to help people get started. We hope you’ll join us. We expect safety guidelines and best practices to be an ongoing conversation in the field, and we want your input. We are excited to continue the conversation, find ways to partner, and learn more about what areas matter to you.</p><p>Dive deeper and learn more</p></div></div>
  </body>
</html>
