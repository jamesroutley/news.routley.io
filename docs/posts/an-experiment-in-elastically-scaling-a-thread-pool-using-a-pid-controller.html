<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/stevana/elastically-scalable-thread-pools">Original</a>
    <h1>An experiment in elastically scaling a thread pool using a PID controller</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">An experiment in controlling the size of a thread pool using a PID controller.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-motivation" aria-hidden="true" href="#motivation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Motivation</h2>
<p dir="auto">A tried and tested way to achieve parallelism is to use pipelining. It&#39;s used
extensively in manufacturing and in computer hardware.</p>
<p dir="auto">For example, Airbus <a href="https://youtu.be/oxjT7veKi9c?t=2682" rel="nofollow">apparently</a> outputs
two airplanes per day on average, even though it takes two months to build a
single airplane from start to finish. It&#39;s also used inside CPUs to <a href="https://en.wikipedia.org/wiki/Instruction_pipelining" rel="nofollow">pipeline
instructions</a>.</p>
<p dir="auto">Let&#39;s imagine we want to take advantage of pipelining in some software system.
To make things more concrete, let&#39;s say we have a system where some kind of
requests come on over the network and we want to process them in some way. The
first stage of the pipeline is to parse the incoming requests from raw
bytestrings into some more structured data, the second stage is to apply some
validation logic to the parsed data and the third stage is to process the valid
data and produce some outputs that are then sent back to the client or stored
somewhere.</p>
<section data-identity="b3715eb1-f8e0-40f8-93c5-aa7e2c69ad5e" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid" data-type="mermaid" aria-label="mermaid rendered output container">
  <div data-json="{&#34;data&#34;:&#34;  flowchart LR\n    Q1(Queue of bytestrings) --&amp;gt; S1((Parse)) --&amp;gt; Q2(Queue of data)\n    Q2--&amp;gt; S2((Validate)) --&amp;gt; Q3(Queue of valid data)\n    Q3--&amp;gt; S3((Process)) --&amp;gt; Q4(Queue of outputs)\n&#34;}" data-plain="  flowchart LR
    Q1(Queue of bytestrings) --&gt; S1((Parse)) --&gt; Q2(Queue of data)
    Q2--&gt; S2((Validate)) --&gt; Q3(Queue of valid data)
    Q3--&gt; S3((Process)) --&gt; Q4(Queue of outputs)
" dir="auto">
    <div dir="auto">
      <pre lang="mermaid" aria-label="Raw mermaid code">  flowchart LR
    Q1(Queue of bytestrings) --&gt; S1((Parse)) --&gt; Q2(Queue of data)
    Q2--&gt; S2((Validate)) --&gt; Q3(Queue of valid data)
    Q3--&gt; S3((Process)) --&gt; Q4(Queue of outputs)
</pre>
    </div>
  </div>
  <span role="presentation">
    <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" data-view-component="true">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke"></circle>
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>
  </span>
</section>

<p dir="auto">The service time of an item can differ from stage to stage, for example parsing
might be slower than validation, which can create bottlenecks. Luckily it&#39;s
quite easy to spot bottlenecks by merely observing the queue lengths and once a
slow stage is found we can often fix it by merely adding an additional parallel
processor to that stage. For example we could spin up two or more threads that
take bytestrings from the first queue and turn them into structured data and
thereby compensate for parsing being slow.</p>
<p dir="auto">By spinning up more threads we can decrease latency (waiting time in the queue)
and increase throughput (process more items), but we are also on the other hand
using more energy and potentially hogging CPU resources that might be better
used elsewhere in the pipeline or system at large.</p>
<p dir="auto">So here&#39;s the question that the rest of this post is concerned about: can we
dynamically spin up and spin down threads at a stage in response to the input
queue length for that stage?</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-plan" aria-hidden="true" href="#plan"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Plan</h2>
<p dir="auto">Let&#39;s focus on a single stage of the pipeline to make things easier for
ourselves.</p>
<section data-identity="854e5e16-452f-4683-a482-44d1b92ccf0f" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid" data-type="mermaid" aria-label="mermaid rendered output container">
  <div data-json="{&#34;data&#34;:&#34;flowchart LR\n    I(Input queue) --&amp;gt; P((Processor)) --&amp;gt; O(Output queue)\n&#34;}" data-plain="flowchart LR
    I(Input queue) --&gt; P((Processor)) --&gt; O(Output queue)
" dir="auto">
    <div dir="auto">
      <pre lang="mermaid" aria-label="Raw mermaid code">flowchart LR
    I(Input queue) --&gt; P((Processor)) --&gt; O(Output queue)
</pre>
    </div>
  </div>
  <span role="presentation">
    <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" data-view-component="true">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke"></circle>
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>
  </span>
</section>

<p dir="auto">We&#39;d like to increase the parallelism of the processors if the input queue
grows, and decrease it when the queue shrinks. One simple strategy might be to
establish thresholds, i.e. if there&#39;s over <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$100$</math-renderer> items in the input queue then
allocate more processors and if there&#39;s no items in the queue then deallocate
them.</p>
<p dir="auto">Since allocating and deallocating processors can be an expense in itself, we&#39;d
like to avoid changing them processor count unnecessarily.</p>
<p dir="auto">The threshold based approach is sensitive to unnecessarily changing the count if
the arrival rate of work fluctuates. The reason for this is because it only
takes the <em>present</em> queue length into account.</p>
<p dir="auto">We can do better by also incorporating the <em>past</em> and trying to predict the
<em>future</em>, this is the basic idea of <a href="https://en.wikipedia.org/wiki/PID_controller" rel="nofollow">PID
controllers</a> from <a href="https://en.wikipedia.org/wiki/Control_theory" rel="nofollow">control
theory</a>.</p>
<p dir="auto">Here&#39;s what the picture looks like with a PID controller in the loop:</p>
<div data-snippet-clipboard-copy-content="                                            +----------------------------------+
                                            |                                  |
    -------------------------------------------&gt;[Input queue]--&gt;[Worker pool]-----&gt;[Output queue]--&gt;
                                            |                                  |
     r(t)   e(t)                    u(t)    |                                  |
    -----&gt;+------&gt;[PID controller]--------&gt; |                                  |
          ^                                 |                                  |
          |                                 +----------------------------------+
          |                                                 | y(t)
          +-------------------------------------------------+
"><pre><code>                                            +----------------------------------+
                                            |                                  |
    -------------------------------------------&gt;[Input queue]--&gt;[Worker pool]-----&gt;[Output queue]--&gt;
                                            |                                  |
     r(t)   e(t)                    u(t)    |                                  |
    -----&gt;+------&gt;[PID controller]--------&gt; |                                  |
          ^                                 |                                  |
          |                                 +----------------------------------+
          |                                                 | y(t)
          +-------------------------------------------------+

</code></pre></div>
<p dir="auto">The PID controller monitors the queue length <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$y(t)$</math-renderer>, compares it to some desired
queue length <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$r(t)$</math-renderer> (also known as the setpoint) and calculates the error <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$e(t)$</math-renderer>.
The error determines the control variable <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$u(t)$</math-renderer> which is used to grow or shrink
the processor pool.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-pseudo-code" aria-hidden="true" href="#pseudo-code"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Pseudo-code</h2>
<p dir="auto">Let&#39;s start top-down with the <code>main</code> function which drives our whole experiment.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-main" aria-hidden="true" href="#main"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Main</h3>
<div data-snippet-clipboard-copy-content="main =

  // Create the in- and out-queues.
  inQueue  := newQueue()
  outQueue := newQueue()


  // The workers don&#39;t do anything interesting, they merely sleep for a bit to
  // pretend to be doing some work.
  worker := sleep 0.025s

  // Create an empty worker pool.
  pool := newPool(worker, inQueue, outQueue)

  // Start the PID controller in a background thread. The parameters provided
  // here allow us to tune the PID controller, we&#39;ll come back to them later.
  kp := 1
  ki := 0.05
  kd := 0.05
  dt := 0.01s
  fork(pidController(kp, ki, kd, dt, pool))


  // Create a workload for our workers. We use the sine function to create
  // between 0 and 40 work items every 0.1s for 60s. The idea being that because
  // the workload varies over time the PID controller will have some work to do
  // figuring out how many workers are needed.
  sineLoadGenerator(inQueue, 40, 0.1s, 60s)"><pre><code>main =

  // Create the in- and out-queues.
  inQueue  := newQueue()
  outQueue := newQueue()


  // The workers don&#39;t do anything interesting, they merely sleep for a bit to
  // pretend to be doing some work.
  worker := sleep 0.025s

  // Create an empty worker pool.
  pool := newPool(worker, inQueue, outQueue)

  // Start the PID controller in a background thread. The parameters provided
  // here allow us to tune the PID controller, we&#39;ll come back to them later.
  kp := 1
  ki := 0.05
  kd := 0.05
  dt := 0.01s
  fork(pidController(kp, ki, kd, dt, pool))


  // Create a workload for our workers. We use the sine function to create
  // between 0 and 40 work items every 0.1s for 60s. The idea being that because
  // the workload varies over time the PID controller will have some work to do
  // figuring out how many workers are needed.
  sineLoadGenerator(inQueue, 40, 0.1s, 60s)
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-worker-pool" aria-hidden="true" href="#worker-pool"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Worker pool</h3>
<p dir="auto">The worker pool itself is merely a struct which packs up the necessary data we
need to be able to scale it up and down.</p>
<div data-snippet-clipboard-copy-content="struct Pool =
  { inQueue:  Queue&lt;Input&gt;
  , outQueue: Queue&lt;Output&gt;
  , worker:   Function&lt;Input, Output&gt;
  , pids:     List&lt;ProcessId&gt;
  }"><pre><code>struct Pool =
  { inQueue:  Queue&lt;Input&gt;
  , outQueue: Queue&lt;Output&gt;
  , worker:   Function&lt;Input, Output&gt;
  , pids:     List&lt;ProcessId&gt;
  }
</code></pre></div>
<p dir="auto">Creating a <code>newPool</code> creates the struct with an empty list of process ids.</p>
<div data-snippet-clipboard-copy-content="newPool worker inQueue outQueue = Pool { ..., pids: emptyList }"><pre><code>newPool worker inQueue outQueue = Pool { ..., pids: emptyList }
</code></pre></div>
<p dir="auto">Scaling up and down are functions that take and return a <code>Pool</code>.</p>
<div data-snippet-clipboard-copy-content="scaleUp pool =
  work := forever
            x := readQueue(pool.inQueue)
            y := pool.worker(x)
            writeQueue(pool.outQueue, y)
  pid   := fork(work)
  pool&#39; := pool.pids = append(pid, pool.pids)
  return pool&#39;"><pre><code>scaleUp pool =
  work := forever
            x := readQueue(pool.inQueue)
            y := pool.worker(x)
            writeQueue(pool.outQueue, y)
  pid   := fork(work)
  pool&#39; := pool.pids = append(pid, pool.pids)
  return pool&#39;
</code></pre></div>
<p dir="auto">The function <code>scaleDown</code> does the inverse, i.e. kills and removes the last
process id from <code>pool.pids</code>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-load-generator" aria-hidden="true" href="#load-generator"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Load generator</h3>
<p dir="auto">In order to create work load that varies over time we use the sine function. The
sine function oscillates between <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$-1$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$1$</math-renderer>:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/stevana/elastically-scalable-thread-pools/blob/main/img/sine.svg"><img src="https://github.com/stevana/elastically-scalable-thread-pools/raw/main/img/sine.svg" alt=""/></a></p>
<p dir="auto">We would like to have it oscillate between <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$0$</math-renderer> and some max value <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$m$</math-renderer>. By
multiplying the output of the sine function by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$m/2$</math-renderer> we get an oscillation
between <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$-m/2$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$m/2$</math-renderer>, we can then add <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$m/2$</math-renderer> to make it oscillate between <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$0$</math-renderer>
and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$m$</math-renderer>.</p>
<p dir="auto">We&#39;ll sample the resulting function once every <code>timesStep</code> seconds, this gives
us the amount of work items (<code>n</code>) to create we then spread those out evenly in
time, rinse and repeat until we reach some <code>endTime</code>.</p>
<div data-snippet-clipboard-copy-content="sineLoadGenerator inQueue workItem maxItems timeStep endTime =
  for t := 0; t &lt; endtime; t += timeStep
    n := sin(t) * maxItems / 2 + maxItems / 2
    for i := 0; i &lt; n; i++
      writeQueue(inQueue, workItem)
      sleep(timeStep / n)"><pre><code>sineLoadGenerator inQueue workItem maxItems timeStep endTime =
  for t := 0; t &lt; endtime; t += timeStep
    n := sin(t) * maxItems / 2 + maxItems / 2
    for i := 0; i &lt; n; i++
      writeQueue(inQueue, workItem)
      sleep(timeStep / n)
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-pid-controller" aria-hidden="true" href="#pid-controller"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>PID controller</h3>
<p dir="auto">The PID controller implementation follows the pseudo-code given at
<a href="https://en.wikipedia.org/wiki/PID_controller#Pseudocode" rel="nofollow">Wikipedia</a>:</p>
<div data-snippet-clipboard-copy-content="previous_error := 0
integral := 0
loop:
   error := setpoint − measured_value
   proportional := error;
   integral := integral + error × dt
   derivative := (error − previous_error) / dt
   output := Kp × proportional + Ki × integral + Kd × derivative
   previous_error := error
   wait(dt)
   goto loop"><pre><code>previous_error := 0
integral := 0
loop:
   error := setpoint − measured_value
   proportional := error;
   integral := integral + error × dt
   derivative := (error − previous_error) / dt
   output := Kp × proportional + Ki × integral + Kd × derivative
   previous_error := error
   wait(dt)
   goto loop
</code></pre></div>
<p dir="auto">Where <code>Kp</code>, <code>Ki</code> and <code>Kd</code> is respectively the proportional, integral and
derivative gain and <code>dt</code> is the loop interval time. The proportional part acts
on the <em>present</em> error value, the integral acts on the <em>past</em> and the derivative
tries to predict the <em>future</em>. The measured value is the input queue length and
the setpoint, i.e. desired queue length, is set to zero. If the <code>output</code> of the
PID controller is less than <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$-100$</math-renderer> (i.e. the queue length is over <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$100$</math-renderer> taking
the present, past and possible future into account) then we scale up and if it&#39;s
more than <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$-20$</math-renderer> (i.e. the queue length is less than <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$20$</math-renderer>) then we scale down the
worker pool.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How it works</h2>
<p dir="auto">We start off by only setting the proportional part and keeping the integral and
derivative part zero, this is called a P-controller. We see below that it will
scale the worker count up and down proportionally to the sine wave shaped load:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/stevana/elastically-scalable-thread-pools/blob/main/img/elastically-scalable-thread-pools-1.0-0.0-0.0.svg"><img src="https://github.com/stevana/elastically-scalable-thread-pools/raw/main/img/elastically-scalable-thread-pools-1.0-0.0-0.0.svg" alt=""/></a></p>
<p dir="auto">A P-controller only focuses on the <em>present</em>, and we see that it allocates and
deallocates workers unnecessarily. In order to smooth things out we introduce
the integral part, i.e. a PI-controller. The integral part takes the <em>past</em> into
account. We see now that the worker count stabilises at <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$28$</math-renderer>:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/stevana/elastically-scalable-thread-pools/blob/main/img/elastically-scalable-thread-pools-1.0-5.0e-2-0.0.svg"><img src="https://github.com/stevana/elastically-scalable-thread-pools/raw/main/img/elastically-scalable-thread-pools-1.0-5.0e-2-0.0.svg" alt=""/></a></p>
<p dir="auto">We can improve on this by adding the derivative part which takes the <em>future</em>
into account. We then see that it stabilises at <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$26$</math-renderer> workers:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/stevana/elastically-scalable-thread-pools/blob/main/img/elastically-scalable-thread-pools-1.0-5.0e-2-5.0e-2.svg"><img src="https://github.com/stevana/elastically-scalable-thread-pools/raw/main/img/elastically-scalable-thread-pools-1.0-5.0e-2-5.0e-2.svg" alt=""/></a></p>
<p dir="auto">With the full PID controller, which stabilises using less workers than the
PI-controller, we see that the queue length spikes up to <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$20$</math-renderer> or so each time
the work load generator hits one of the sine function&#39;s peaks. Recall that we
started scaling down once the queue length was less than <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$20$</math-renderer>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">The above graphs were generated by running: <code>cabal run app -- kp ki kd</code>, where
the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$K_p$</math-renderer>, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$K_i$</math-renderer>, and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="fc5ff3d58ebf7ce50739cabafdc31298">$K_d$</math-renderer> parameters are the tuning parameters for the
PID controller.</p>
<p dir="auto">If you don&#39;t have the GHC Haskell compiler and the <code>cabal</code> build tool already
installed, then the easiest way to get it is via
<a href="https://www.haskell.org/ghcup/" rel="nofollow"><code>ghcup</code></a>. Alternatively if you got <code>nix</code> then
<code>nix-shell</code> should give give you access to all the dependencies you need.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contributing" aria-hidden="true" href="#contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h2>
<p dir="auto">There are many ways we can build upon this experiment, here are a few ideas:</p>
<ul>
<li> We probably want to limit the max number of threads in a pool;</li>
<li> If two or more threads take items from some input queue and put them on
some output queue then there&#39;s no guarantee that the order of the output
items will be the same as the input items. We could solve this, and regain
determinism, by using array based queues and shard on the index, i.e. even
indices goes to one processor and odd to an other or more generally
modulus N can be used to shard between N processors. This is essentially
what the <a href="https://en.wikipedia.org/wiki/Disruptor_(software)" rel="nofollow">LMAX
Disruptor</a> does;</li>
<li> We&#39;ve only looked at one stage in a pipeline, what happens if we have
multiple stages? is it enough to control each individual stage separately
or do we need more global control?</li>
<li> Can we come up with other things to control? E.g. batch sizes?</li>
<li> We&#39;ve only monitored the current queue length, could we combine this with
other data? E.g. time series of the queue length from the previous day?</li>
<li> Is it robust to wildly changing usage patterns? E.g. bursty traffic or the
<a href="https://en.wikipedia.org/wiki/Slashdot_effect" rel="nofollow">Slashdot effect</a>?</li>
<li> We&#39;ve looked at scaling up and down on a single machine (vertical
scaling), what about scaling out and in across multiple machines
(horizontal scaling)?</li>
<li> We generated and processed real work items (by sleeping), could we do a
discrete-event simulation instead to avoid having to wait for the sleeps?</li>
<li> I just picked random values for the PID controller parameters, there are
more principled
<a href="https://en.wikipedia.org/wiki/PID_controller#Overview_of_tuning_methods" rel="nofollow">ways</a>
of tuning the PID controller;</li>
<li> The PID controller we implemented merely followed the pseudo-code from
Wikipedia, there&#39;s probably better ways of implementing it?</li>
</ul>
<p dir="auto">If any of this sounds interesting, feel free to get in touch!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-see-also" aria-hidden="true" href="#see-also"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>See also</h2>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://www.researchgate.net/publication/265611546_A_Review_of_Auto-scaling_Techniques_for_Elastic_Applications_in_Cloud_Environments" rel="nofollow"><em>A Review of Auto-scaling Techniques for Elastic Applications in Cloud
Environments</em></a>
(2014) is a survey paper which talks about both threshold and PID controllers;</p>
</li>
<li>
<p dir="auto"><a href="https://people.eecs.berkeley.edu/~brewer/papers/SEDA-sosp.pdf" rel="nofollow"><em>SEDA: An Architecture for Well-Conditioned Scalable Internet
Services</em></a>
(2001), this is paper that I got the idea for elastic scalable thread pools.
They use a threshold approach rather than a PID controller, saying:</p>
<blockquote>
<p dir="auto">The controller periodically samples the input queue (once per second by
default) and adds a thread when the queue length exceeds some threshold (100
events by default). Threads are removed from a stage when they are idle for a
specified period of time (5 seconds by default).</p>
</blockquote>
<p dir="auto">But also:</p>
<blockquote>
<p dir="auto">Under SEDA, the body of work on control systems can be brought to bear on
service resource management, and we have only scratched the surface of the
potential for this technique.</p>
</blockquote>
<p dir="auto">A bit more explanation is provided by Matt Welsh, who is one of the author, in
his PhD
<a href="https://cs.uwaterloo.ca/~brecht/servers/readings-new/mdw-phdthesis.pdf" rel="nofollow">thesis</a>
(2002):</p>
<blockquote>
<p dir="auto">A benefit to ad hoc controller design is that it does not rely on complex
models and parameters that a system designer may be unable to understand or to
tune. A common complaint of classic PID controller design is that it is often
difficult to understand the effect of gain settings.</p>
</blockquote>
</li>
<li>
<p dir="auto">It could very well be that the way we&#39;ve applied classic PID controllers isn&#39;t
suitable for unpredictable internet traffic loads. There are branches of
control theory might be better suited for this, see, for example,
<a href="https://en.wikipedia.org/wiki/Robust_control" rel="nofollow">robust</a> and
<a href="https://en.wikipedia.org/wiki/Adaptive_control" rel="nofollow">adaptive</a> control theory;</p>
</li>
<li>
<p dir="auto">My previous post: <a href="https://github.com/stevana/pipelined-state-machines#pipelined-state-machines"><em>An experiment in declaratively programming parallel
pipelines of state
machines</em></a>.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-discussion" aria-hidden="true" href="#discussion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Discussion</h2>
<ul dir="auto">
<li><a href="https://news.ycombinator.com/item?id=35148068" rel="nofollow">Hacker News</a>;</li>
<li><a href="https://lobste.rs/s/ybtxic/experiment_elastically_scaling_thread" rel="nofollow">lobste.rs</a>;</li>
<li><a href="https://old.reddit.com/r/haskell/comments/11qyfw7/an_experiment_in_elastically_scaling_a_thread/" rel="nofollow">r/haskell</a>;</li>
<li>Also see Glyn Normington&#39;s
<a href="https://github.com/stevana/elastically-scalable-thread-pools/issues/1" data-hovercard-type="issue" data-hovercard-url="/stevana/elastically-scalable-thread-pools/issues/1/hovercard">comment</a>
in the issue tracker.</li>
</ul>
</article>
          </div></div>
  </body>
</html>
