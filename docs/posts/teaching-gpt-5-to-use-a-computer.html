<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://prava.co/archon/">Original</a>
    <h1>Teaching GPT-5 to Use a Computer</h1>
    
    <div id="readability-page-1" class="page"><div><section><div><hr/><div><p><img src="https://prava.co/archongif.gif" alt="Archon demo animation"/></p></div><p>Over the weekend, <a href="https://x.com/OpenAIDevs/status/1955774997735333936" target="_blank" rel="noopener noreferrer">I won #3 at OpenAI&#39;s GPT-5 Hackathon</a> with Archon - a copilot for your computer. It comes with a mini vision model for speed, and GPT-5 for variable reasoning to plan. I took some time to write about how it works, and our approach to building a self-driving computer with inference math, and the tradeoffs we made.</p><p>Archon is a small bar that sits at the bottom of your Mac/Windows screen where you can type what you want your computer to do in natural language. It takes screenshots to see what&#39;s on screen, uses GPT-5&#39;s reasoning to plan, then a custom fine-tuned model executes clicks and keystrokes. In a racing game demo with a single instruction to &#39;start playing&#39; it recognized the view, used WASD, and navigated the track. Although it didn&#39;t win this time due to latency, its instruction-following ability was clearly superior to prior models. The goal is to make a copilot that makes computers self-driving. Archon is a lightweight client demonstrating that GPT-5&#39;s powerful reasoning combined with tiny fine-tuned models can control any interface through natural language.</p><div><p><video autoplay="" loop="" muted="" playsinline="" controls=""><source src="/ArchonDemoGPT5HackathonLowRes.mp4" type="video/mp4"/>Your browser does not support the video tag.</video></p><p>Full demo video sped up 2x</p></div><h2>GPT-5: Why it worked for us</h2><p><strong>Archon was built entirely using GPT-5&#39;s advanced reasoning capabilities.</strong> We leveraged probably every aspect of GPT-5 from initial development to debugging to training. Codex CLI with GPT-5 with High Thinking enabled us to build the entire app, and GPT-5 with Vision enabled us to see and perceive the screen. GPT-5&#39;s reasoning ability was crucial for instruction following, and planning. These all in one model quite simply wasn&#39;t possible with any other model.</p><p>What makes GPT-5 particularly suited for computer control is its ability to <em>reason through complex multi-step processes</em> while maintaining context across long interactions. Unlike previous models that might hallucinate or lose track of the current state, GPT-5&#39;s chain-of-thought reasoning allows it to break down &#34;start playing this game&#34; into discrete, executable steps while adapting to unexpected UI changes.</p><p><strong>We calibrated how much compute to use strategically to trade off accuracy and latency.</strong> For complex workflows, <code>high</code> reasoning effort mapped out interaction sequences with error handling. GPT-5-mini with function calling <code>preambles</code> enabled us to show the user what we were thinking while simultaneously calling our grounding model. This adaptive approach keeps the user in mind. Whether they are a user, enterprise, need to go through complex, changing UIs, or just need to get something done, we can trade off reasoning for latency and vice versa.</p><h2>How it actually works</h2><pre>+-----------------+      +----------------------+      +------------------+      +----------------+
|  User intent    | ---&gt; |  Planner (GPT-5)     | ---&gt; |  Archon-Mini     | ---&gt; |  Executor      |
|  (&#34;book a ...&#34;) |      |  plan: {click &#34;...&#34;} |      |  ground -&gt; (x,y) |      |  mouse/keyboard|
+-----------------+      +----------------------+      +------------------+      +----------------+
                                                                 ^        |
                                                                 | verify |  (screen diff predicate)
                                                                 +--------+</pre><p><strong>Archon uses a hierarchical approach: a large reasoning model (GPT-5/o-series) decides what to do, and <strong>archon‑mini</strong> figures out exactly where to click.</strong> <!-- -->This split matters because reasoning and grounding are fundamentally different problems with different computational requirements.</p><p>The reasoning model sees the screen and your request, then outputs a semantic action: &#34;click the blue Submit button at the bottom.&#34; Descriptions enable reasoning to be done in natural language. <strong>archon‑mini</strong> takes that description plus the screenshot and outputs exact pixel coordinates: (523, 412). One model for the &#34;what,&#34; another for the &#34;where.&#34;</p><pre># The loop:
screen = capture_screen()
instruction = &#34;book a flight for tomorrow&#34;

# Step 1: Reasoning (200-500ms)
plan = gpt5.reason(screen, instruction)
# → {&#34;action&#34;: &#34;click&#34;, &#34;target&#34;: &#34;Flights tab&#34;}

# Step 2: Grounding (≈50ms)
coords = archon_mini.ground(screen, plan.target)
# → (567, 234)

# Step 3: Execute
mouse.click(coords.x, coords.y)

# Repeat until done</pre><p><strong>archon‑mini</strong> is a 7B Qwen‑2.5‑VL–based executor (dynamic‑res ViT) fine‑tuned with <strong>GRPO</strong> for GUI grounding. In the future, it will be trained with a combination of trajectory‑boosted human demos and synthetic teacher rollouts. It outputs direct <code>(x, y)</code> screen coordinates and structured tool calls.</p><h2>Why vision tokens are expensive (and how we optimize them)</h2><p>For multi-modal models like GPT-5, when you send an image, OpenAI charges based on image &#34;tiles&#34;. A tile is basically a 512px by 512px square, so a 1080p screen gets split into 6 tiles (3×2) since it&#39;s 1920px by 1080px; there&#39;s a base cost of 65 + 129 tokens per tile.</p><p>So in practice a full-frame costs:</p><div><p>65 + (129 × 6) = <strong>839 tokens/frame</strong></p><p>Over 20 steps: 839 × 20 = <strong>16,780 tokens</strong></p><p>At $0.15/1M tokens: ~$0.0025 per task</p></div><p>The neat thing is that OpenAI discounts unchanged prefixes after 1024 tokens, so as we progress through the task, prior screenshots get cached for faster/cheaper inference. But even after GPT-5 tells us &#34;click the blue Submit button&#34;, we still need archon-mini to figure out <em>exactly where</em> that button is. This happens for every single action, so grounding needs to be blazing fast.</p><p>We run a 3MB saliency scorer to identify interactive regions (buttons, text fields, links) and grab ~20 patches from those areas while throwing away dead space. The Submit button at (834, 672) stays cached across clicks since UI is mostly the same—70%+ cache hits means much less GPU time which helps at scale. We also found that if we reduced resolution dynamically where instead of throwing away dead space, we could just downsample irrelevant regions instead. This would keep grounding at ~10–50ms. The reason we use patches is because a 512×512 OpenAI tile covers 262K pixels and 32×32 patches cover 1K pixels... 262x more precise than we need.</p><pre>[ 1920×1080 frame ]  ──►  16×16 saliency heatmap (3 MB model, ~5 ms)
                         │
                         ├─ top-K patches (~20) → crop &amp; upscale to 32×32 → encode → ~640 tokens raw
                         │                             ▲
                         │                             └─ with 70% cache hits → ~192 new tokens/frame
                         │
                         └─ low-saliency regions → skip or cheap downsample
        + cache unchanged patches across frames</pre><div><p><img src="https://prava.co/heatmapscreencap.png" alt="Saliency heatmap showing attention regions on a screenshot"/></p><p>Illustration of the saliency heatmap.</p></div><blockquote>If you enjoy this kind of systems math, pricing, selective encoding, cache design, and test‑time routing — we&#39;re hiring. <a href="https://prava.co/careers">prava.co/careers</a></blockquote><h2>Training: GRPO and synthetic data generation</h2><p><strong>To continue on why we use patches, we trained <strong>archon‑mini</strong> with GRPO, where rewards look like &#34;anywhere inside the element&#34; and the reward is 1 if the click is inside the element and 0 otherwise. Patches are small enough such that if you click anywhere inside the element, you get a reward. To further improve the grounding model, we found trajectory augmentation would be a good idea. Using human demonstrations, you can make a bunch of related trajectories, &#34;boosting&#34; the grounding model.</strong></p><pre>GRPO (Group Relative Policy Optimization)

for each (screenshot, target):
  sample N=8 clicks →  •  •  ○  •  ○  •  ○  •
                       hit=•  miss=○   (inside element = success)
  z-normalize rewards → advantages
  update policy: ↑prob(hits), ↓prob(misses)

It&#39;s fine if you don&#39;t click the center of the element, any pixel inside the boundary is valid.</pre><pre>Trajectory boosting

30 human demos
   │
   ├─ Thought completion (per-step rationale)
   ├─ Action diversification (Enter, Tab+Space, alternate pixels)
   ▼
~1k enriched steps → train with GRPO (8 samples/step) → robust grounding</pre><p>While testing, archon-mini was really bad at clicking bright red buttons, compared to tiny blue buttons it was clicking. We suspect this is because the bright red buttons are more likely to be at the center of the element, and the tiny blue buttons are more likely to be at the edge of the element. More work is needed to make the model more robust and for us to interpret all its capabilities.</p><h2>Speed: adaptive compute that feels instant</h2><p>Test-time compute is getting extremely hyped these days, particularly off of the success of the o-series models. In my experience, I personally get much usage from GPT-5 Pro and previously o3-pro. The reason is because a lot of my day-to-day work revolves around &#34;knowledge work&#34;. Good thing for archon-mini is that it&#39;s a lot of &#34;grounding work&#34; and not a lot of &#34;knowledge work&#34;. You can get a lot of mileage out of a 7B model if you instead vary the reasoning and determine how to properly pipeline the tasks. In the future I intend to use archon-mini with aggressive simplicity and caching with a simple policy:<strong>observe → ground() → execute() → verify → repeat</strong></p><pre>observe → ground() → execute() → verify → repeat</pre><p>On this path, <strong>archon‑mini</strong> runs alone (no planner call), hitting ~50 ms per action on a A100. The router only escalates when signals are uncertain: high saliency entropy, too many candidate targets, recent misclicks, or ambiguous copy (e.g., multiple “Submit” buttons). When that trips, we pipeline one step ahead: <em>Step N</em> (simple) executes now while the reasoner prepares a short plan for <em>Step N+1</em>. The router is a simple policy that looks at the signals and decides whether to escalate or not.</p><pre>Pipelined control (perceived continuity)

time ►  [ ground ] [ ground ] [ ground ] …
          ▲ while planner prepares the next complex step ▲</pre><pre>Routing policy

+----------------------------+
| signals: H(saliency), #cands,
| ocr_density, recent_fails  |
+----------------------------+
   /         |               
 simple   ambiguous        complex
   |         |               |
 ground()  quick_reason     deep_plan
  ~50ms      ~200ms          500–1000ms</pre><p>Consumer workloads are bursty (batch=1). Enterprise is steady (batch 8–64, optimize for throughput). We want the router to be different for each.</p><p>For the typical consumer, we think it&#39;s better to bias toward the fast path (planner stays cold unless ambiguity is detected). In enterprise, we enable continuous batching for planner calls, short aggregation windows, and aggressive prefix caching; <strong>archon‑mini</strong> stays on‑GPU so grounding still feels immediate.</p><p>After ~1 hour of use we typically see a pretty high patch‑cache hit‑rate where similar patches (imagine a screenshot of a button) are cached and reused. Verifying is cheap (single screenshot + state predicate), so we keep iterating quickly without silent drift.</p><p><strong>The encompassing effect is that compared to computer-use models today, many steps can finish in &lt; 100 ms end‑to‑end; a 20‑step flow can land in a few seconds without the “stop‑and‑think” feel.</strong></p><h2>What&#39;s next: streaming control and unifying the stack</h2><pre>Screenshot loop:  capture ─ process ─ act ─ capture ─ process ─ act
Streaming input:  █████ continuous frames █████ → act → act → act</pre><p>In the future we hope to run a streaming capture pipeline similar to Gemma 3.  Consuming frames at 20–30 fps, emitting actions at 5–10 Hz, and verifying state on each commit. This closes the perception→action loop for drag/hover/scroll and makes motion feel natural. The planner would hook into the same stream, but only for escalations.</p><p>We also plan to <em>compile</em> solved steps into micro‑policies. If you&#39;re running something like a RPA task or similar workflow as before, you can simply run the execution locally (with archon-mini running locally) and not have to worry about the planning. Over time, the planner is a background teacher, not a crutch. <strong>We also found that recording screens on computers is a great way to get enough data to do RL training which materially boosts the performance of the model for each specific use case(s) in each vertical/profession.</strong></p><p>We will distill those plans into the local model so more steps stay on the fast path. The path forward is to adopt an end-to-end approach to the problem. For Tesla that&#39;s camera, steering, acceleration. For us it&#39;s screen, mouse, keyboard.</p><pre>Current (hierarchical, streaming):
pixels → planner(plan) → archon‑mini(ground) → execute → verify

Evolving (more unified):
pixels → archon‑mini(policy) → execute → verify
           ↑ distilled from planner traces</pre><p>Eventually we&#39;ll get rid of all the brittle policies and controls and have a model that can think on a second-order to understand how much compute it requires to do a task. Today we want to keep a planner in the loop for rare edge cases and safety; as the executor absorbs those patterns (via streaming, macros, distillation), the system becomes simpler and end-to-end.</p><p>With the release of Tesla Vision V12 on FSD, Tesla showed that they could replace 300K lines of driving code with end-to-end neural nets. I think we&#39;ll see a similar thing happen with the self-driving computer in the next few years.</p><div><h3>Related Work</h3><div><p>He, Y., Jin, J., &amp; Liu, P. (2025). <a href="https://gair-nlp.github.io/PC-Agent-E/" target="_blank" rel="noopener noreferrer">Efficient Agent Training for Computer Use</a>. <em>arXiv preprint arXiv:2505.13909</em>.</p><p>Yang, Y., Li, D., Dai, Y., Yang, Y., Luo, Z., Zhao, Z., Hu, Z., Huang, J., Saha, A., Chen, Z., Xu, R., Pan, L., Xiong, C., &amp; Li, J. (2025). <a href="https://arxiv.org/abs/2507.05791" target="_blank" rel="noopener noreferrer">GTA1: GUI Test-time Scaling Agent</a>. <em>arXiv preprint arXiv:2507.05791</em>.</p></div></div><div><h3>We&#39;re hiring</h3><p>Our mission is to diffuse AGI across the economy. If you&#39;re excited about training models or applying AI to real-world problems, reach out.</p></div></div></section></div></div>
  </body>
</html>
