<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://zuru.tech/blog/the-dangers-behind-image-resizing">Original</a>
    <h1>The dangers behind image resizing (2021)</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><strong>Image resizing</strong> is a very common geometrical transformation of an image, and it simply consists of a scaling operation.</p>
<p>Since it is one of the most common image processing operations, you can find its implementation in all image processing libraries. Because it is so common, you can expect that the behavior is well defined and will be the same among the libraries.</p>
<p>A tricky scenario that could happen, and we as the ML team experienced it, could come from the pre-processing step of a machine learning model.</p>
<p>The workflow of the development of an ML model starts from a training phase, typically in Python. Then, if your metrics on the test set satisfy your requirements, you may want to deploy your algorithm.</p>
<p>Fortunately, the main deep learning frameworks, i.e., <strong>Tensorflow</strong> and <strong>PyTorch</strong>, give you the possibility to export the whole execution graph into a &#34;program,&#34; called <code>SavedModel</code> or <code>TorchScript</code>, respectively. We used the term program because these formats include both the architecture, trained parameters, and computation.</p>
<p>If you are developing a new model from scratch, you can design your application to export the entire pipeline, but this is not always possible if you are using a third-party library. So, for example, you can export only the inference but not the pre-processing.
Here come the resizing problems because you probably need to use a different library to resize your input, maybe because you don&#39;t know how the rescaling is done, or there isn&#39;t the implementation of the Python library in your deploying language.</p>
<h2 id="but-why-the-behavior-of-resizing-is-different?">But why the behavior of resizing is different?</h2>
<p>The definition of scaling function is mathematical and should never be a function of the library being used. Unfortunately, implementations differ across commonly-used libraries and mainly come from how it is done the <strong>interpolation</strong>.</p>
<p>Image transformations are typically done in reverse order (from destination to source) to avoid sampling artifacts.
In practice, for each pixel <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x,y)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span></span></span></span></span> of the destination image, you need to compute the coordinates of the corresponding pixel in the input image and copy the pixel value:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>d</mi><mi>s</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>I</mi><mrow><mi>s</mi><mi>r</mi><mi>c</mi></mrow></msub><mrow><mo fence="true">(</mo><msub><mi>f</mi><mi>x</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>f</mi><mi>y</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">I_{dst}(x,y) = I_{src}\left(f_x(x,y), f_y(x,y)\right)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>I</span><span><span><span><span><span><span></span><span><span><span>d</span><span>s</span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>I</span><span><span><span><span><span><span></span><span><span><span>src</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span><span>f</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span><span>,</span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span><span>)</span></span></span></span></span></span></p>
<p>where <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>f</mi><mi>x</mi></msub><mo separator="true">,</mo><msub><mi>f</mi><mi>y</mi></msub><mo stretchy="false">⟩</mo><mo>:</mo><mi>d</mi><mi>s</mi><mi>t</mi><mo>→</mo><mi>s</mi><mi>r</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">\langle f_x,f_y \rangle : dst \to src</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>⟨</span><span><span>f</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⟩</span><span></span><span>:</span><span></span></span><span><span></span><span>d</span><span>s</span><span>t</span><span></span><span>→</span><span></span></span><span><span></span><span>src</span></span></span></span></span> is the inverse mapping.</p>
<p>Usually, when you compute source coordinates, you get <strong>floating-point</strong> numbers, so you need to decide how to choose which source pixel to copy into the destination.</p>
<p>The problem is that different library could have some little differences in how they implement the interpolation filters but above all, if they introduce the <strong>anti-aliasing filter</strong>. In fact, if we interpret the image scaling as a form of image resampling from the view of the <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist sampling theorem</a>, downsampling to a smaller image from a higher-resolution original can only be carried out after applying a suitable 2D anti-aliasing filter to prevent <a href="https://en.wikipedia.org/wiki/Aliasing">aliasing artifacts</a>.</p>
<p><code>OpenCV</code> that could be considered the standard de-facto in image processing does not use an anti-aliasing filter. On the contrary, <code>Pillow</code>, probably the most known and used image processing library in Python, introduces the anti-aliasing filter.</p>
<h2 id="comparison-of-libraries">Comparison of libraries</h2>
<p>To have an idea of how the different implementations affect the resized output image, we compared four libraries, the ones we considered the most used, in particular in the ML field. Moreover, we focused on libraries that could be used in Python.</p>
<p>We tested the following libraries and methods:</p>
<ol>
<li>
<p><strong>OpenCV</strong> v4.5.3: <a href="https://docs.opencv.org/4.5.2/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d"><code>cv2.resize</code></a></p>
</li>
<li>
<p><strong>Pillow Image Library</strong> (PIL) v8.2.0: <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.resize"><code>Image.resize</code></a></p>
</li>
<li>
<p><strong>TensorFlow</strong> v2.5.0: <a href="https://www.tensorflow.org/api_docs/python/tf/image/resize"><code>tf.image.resize</code></a></p>
<p>This method has a flag <code>anti-alias</code> to enable the anti-aliasing filter (default is false). We tested the method in both cases, either flag disabled and enabled.</p>
</li>
<li>
<p><strong>PyTorch</strong> v1.9.0: <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html"><code>torch.nn.functional.interpolate</code></a></p>
<p>PyTorch has another method for resizing, that is <code>torchvision.transforms.Resize</code>. We decided not to use it for the tests because it is a wrapper around the PIL library, so that the results will be the same compared to <code>Pillow</code>.</p>
</li>
</ol>
<p>Each method supports a set of filters. Therefore, we chose a common subset present in almost all libraries and tested the functions with them. In particular, we chose:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation"><strong><em>nearest</em></strong></a>: the most naive approach that often leads to aliasing. The high-frequency information in the original image becomes misrepresented in the downsampled image.</li>
<li><a href="https://en.wikipedia.org/wiki/Bilinear_interpolation"><strong><em>bilinear</em></strong></a>: a linear filter that works by interpolating pixel values, introducing a continuous transition into the output even where the original image has discrete transitions. It is an extension of linear interpolation to a rectangular grid.</li>
<li><a href="https://en.wikipedia.org/wiki/Bicubic_interpolation"><strong><em>bicubic</em></strong></a>: similar to <em>bilinear</em> filter but in contrast to it, which only takes 4 pixels (2×2) into account, bicubic interpolation considers 16 pixels (4×4). Images resampled with bicubic interpolation should be smoother and have fewer interpolation artifacts.</li>
<li><a href="https://en.wikipedia.org/wiki/Lanczos_resampling"><strong><em>lanczos</em></strong></a>: calculate the output pixel value using a high-quality <em>Lanczos</em> filter (a truncated sinc) on all pixels that may contribute to the output value. Typically it is used on an 8x8 neighborhood.</li>
<li><a href="https://en.wikipedia.org/wiki/Image_scaling#Box_sampling"><strong><em>box</em></strong></a> (aka <strong><em>area</em></strong>): is the simplest linear filter; while better than naive subsampling above, it is typically not used. Each pixel of the source image contributes to one pixel of the destination image with identical weights.</li>
</ul>
<h3 id="qualitative-results">Qualitative results</h3>
<p>To better see the different results obtained with the presented functions, we follow the test made in <a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>. We create a synthetic 128 x 128 image with a circle of thickness 1 px, and we resized it with a downscaling factor of 4 (destination image will be 32 x 32).</p>
<p>Here you can see the qualitative results of our investigation:</p>
<p><a name="figure1" href=""></a>
<img src="https://storage.googleapis.com/strapi-test-bucket/circle_comparison_979eef92de/circle_comparison_979eef92de.png" alt="circle_comparison.png"/></p>
<figcaption>Figure 1. Comparison of methods on the synthetic image of a circle.</figcaption>
<p>In <a href="#figure1">Figure 1</a>, we added an icon to highlight if the downsampling method antialiases and remove high-frequencies in the correct way. We used a green icon to show good results, yellow if the filter does not provide sufficient antialiasing, red if artifacts are introduced in the low-resolution image.</p>
<p>As you can see, even downsampling a simple image of a circle provides wildly inconsistent results across different libraries.</p>
<p><code>OpenCV</code> and <code>PyTorch</code> achieve very similar results for all the filters, and both do not use antialiasing filters.</p>
<p><code>TensorFlow</code> has a hybrid behavior. If you don&#39;t enable the antialiasing flag (first row), the results are the same as <code>OpenCV</code>. Instead, if you enable it, you get the same results as <code>Pillow</code>.</p>
<p>I find it interesting that there are differences even for a filter like the <em>nearest neighbor</em> that is quite straightforward to implement. At the same time, the box filter seems to give the same results among the libraries.</p>
<p>We further evaluate the libraries with other synthetic images with different patterns.
The first one (Figure 2) is a grid of white square 50x50 with 5 pixels of black border.</p>
<p><img src="https://storage.googleapis.com/strapi-test-bucket/grid_comparison_e813663cbb/grid_comparison_e813663cbb.png" alt="grid_comparison.png"/></p>
<figcaption>Figure 2. Comparison of methods on the synthetic image of a grid.</figcaption>
<p>The previous considerations are valid also in this case, and it is clear how without the antialiasing the resulting images do not represent correctly the input.
If you look at <code>OpenCV</code> results, it is curious that they are, in practice, the same results given by the <em>nearest neighbor</em> filter of <code>Pillow</code>. It seems like that incorrectly implemented filters inadvertently resemble the naive approach.</p>
<p>Other interesting results can be achieved using a pattern similar to the previous grid but with a border enlarged to 22 pixels and 20x20 white squares (note that we use a slightly different value for the border and the squares).</p>
<p><img src="https://storage.googleapis.com/strapi-test-bucket/grid2_comparison_58fcd80fb5/grid2_comparison_58fcd80fb5.png" alt="grid2_comparison.png"/>
</p><figcaption>
Figure 3. Comparison of Pillow and OpenCV on a grid image.
From left to right, raw image (652x652), Pillow resized image with the nearest filter, OpenCV resized image with the nearest filter, Pillow resized image with the bilinear filter and OpenCV resized image with the bilinear filter.</figcaption>
<p>With this input the number of squares in the resulting images is unchanged, but in some cases, some rows or columns have a rectangular shape.</p>
<h3 id="natural-images">Natural images</h3>
<p>We chose to use synthetic images because we want to evidence the creation of artifacts and see the differences between libraries. These differences are more difficult to see on natural images, i.e., the ones acquired with a camera. In this case, as you see in the figures below, the results with the <em>nearest neighbor</em> filter are quite the same while, using the <em>bilinear</em> one, the effect of the antialiasing filter becomes more visible.</p>
<p><img src="https://storage.googleapis.com/strapi-test-bucket/dog_nearest_89a646cda2/dog_nearest_89a646cda2.png" alt="dog_nearest.png"/></p>
<figcaption>Figure 4. Nearest neighbor interpolation on a natural image.</figcaption>
<p><img src="https://storage.googleapis.com/strapi-test-bucket/dog_bilinear_7e0fb53021/dog_bilinear_7e0fb53021.png" alt="dog_bilinear.png"/></p>
<figcaption>Figure 5. Bilinear interpolation on a natural image.</figcaption>
    

</div></div></div>
  </body>
</html>
