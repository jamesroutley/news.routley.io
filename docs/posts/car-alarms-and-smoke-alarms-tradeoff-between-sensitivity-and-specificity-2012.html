<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.danslimmon.com/2012/11/02/car-alarms-and-smoke-alarms-the-tradeoff-between-sensitivity-and-specificity/">Original</a>
    <h1>Car alarms and smoke alarms: tradeoff between sensitivity and specificity (2012)</h1>
    
    <div id="readability-page-1" class="page"><article id="post-74">
	<div>
		<!-- .entry-header -->

		<div>
			<p>Wouldn’t you like to live in a world where your monitoring systems only alerted when things were actually broken? And wouldn’t it be great if, in that world, your alerts would <i>always</i> fire if things were broken?</p>
<p>Well so would everybody else. But we don’t live in that world. When we choose a threshold for alerting, we usually have to make a tradeoff between the chance of getting a false positive (an alert that fires when nothing is wrong) and the chance of getting a false negative (an alert that <i>doesn’t</i> fire when something <i>is</i> wrong).</p>
<p>Take the load average on an app server for example: if it’s above 100, then your service is probably broken. But there’s still a chance that the waiting processes aren’t blocking your mission-critical code paths. If you page somebody on this threshold, there’s always a chance that you’ll be waking that person up in the middle of the night for no good reason. However, if you raise the threshold to 200 to get rid of such spurious alerts, you’re making it more likely that a pathologically high load average will go unnoticed.</p>
<p>When presented with this tradeoff, the path of least resistance is to say “Let’s just keep the threshold lower. We’d rather get woken up when there’s nothing broken than sleep through a real problem.” And I can sympathize with that attitude. Undetected outages are embarrassing and harmful to your reputation. Surely it’s preferable to deal with a few late-night fire drills.</p>
<p>It’s a trap.</p>
<p>In the long run, false positives can — and will often — hurt you more than false negatives. Let’s learn about the base rate fallacy.</p>
<h2>The base rate fallacy</h2>
<p>Suppose you have a service that works fine most of the time, but breaks occasionally. It’s not trivial to determine whether the service is working, but you can write a probe that’ll detect its state correctly 99% of the time:</p>
<ul>
<li>If the service is working, there’s a 1% chance that your probe will say it’s broken</li>
<li>If the service is broken, there’s a 1% chance that your probe will say it’s working</li>
</ul>
<p>Naïvely, you might expect this probe to be a decent check of the service’s health. If it goes off, you’ve got a pretty good chance that the service is broken, right?</p>
<p>No. Bad. Wrong. This is what logicians and statisticians call the “base rate fallacy.” Your expectation hinges on the assumption that the service is only working half the time. In reality, if the service is any good, it works almost all the time. Let’s say the service is functional 99.9% of the time. If we assume the service just fails randomly the other 0.1% of the time, we can calculate the true-positive rate:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BTPR%7D+%26+%3D+%26+%5Ctext%7B%28prob.+of+service+failure%29%7D%2A%5Ctext%7B%28prob.+of+detecting+a+failure%29%7D+%5C%5C+%26+%3D+%26+%280.001%29+%2A+%280.99%29+%5C%5C+%26+%3D+%26+0.00099+%5C%5C+%26+%3D+%26+0.099%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BTPR%7D+%26+%3D+%26+%5Ctext%7B%28prob.+of+service+failure%29%7D%2A%5Ctext%7B%28prob.+of+detecting+a+failure%29%7D+%5C%5C+%26+%3D+%26+%280.001%29+%2A+%280.99%29+%5C%5C+%26+%3D+%26+0.00099+%5C%5C+%26+%3D+%26+0.099%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BTPR%7D+%26+%3D+%26+%5Ctext%7B%28prob.+of+service+failure%29%7D%2A%5Ctext%7B%28prob.+of+detecting+a+failure%29%7D+%5C%5C+%26+%3D+%26+%280.001%29+%2A+%280.99%29+%5C%5C+%26+%3D+%26+0.00099+%5C%5C+%26+%3D+%26+0.099%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\begin{array}{rcl} \text{TPR} &amp; = &amp; \text{(prob. of service failure)}*\text{(prob. of detecting a failure)} \\ &amp; = &amp; (0.001) * (0.99) \\ &amp; = &amp; 0.00099 \\ &amp; = &amp; 0.099\% \end{array} "/></p>
<p>That is to say, about 1 in 1000 of all tests will run during a failure and detect that failure correctly. We can also calculate the false-positive rate:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BFPR%7D+%26+%3D+%26+%5Ctext%7B%28prob.+of+service+non-failure%29%7D%2A%5Ctext%7B%28prob.+of+detecting+failure+anyway%29%7D+%5C%5C+%26+%3D+%26+%281-0.001%29%2A%281-0.99%29+%5C%5C+%26+%3D+%26+0.0099+%5C%5C+%26+%3D+%26+0.99%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BFPR%7D+%26+%3D+%26+%5Ctext%7B%28prob.+of+service+non-failure%29%7D%2A%5Ctext%7B%28prob.+of+detecting+failure+anyway%29%7D+%5C%5C+%26+%3D+%26+%281-0.001%29%2A%281-0.99%29+%5C%5C+%26+%3D+%26+0.0099+%5C%5C+%26+%3D+%26+0.99%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BFPR%7D+%26+%3D+%26+%5Ctext%7B%28prob.+of+service+non-failure%29%7D%2A%5Ctext%7B%28prob.+of+detecting+failure+anyway%29%7D+%5C%5C+%26+%3D+%26+%281-0.001%29%2A%281-0.99%29+%5C%5C+%26+%3D+%26+0.0099+%5C%5C+%26+%3D+%26+0.99%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\begin{array}{rcl} \text{FPR} &amp; = &amp; \text{(prob. of service non-failure)}*\text{(prob. of detecting failure anyway)} \\ &amp; = &amp; (1-0.001)*(1-0.99) \\ &amp; = &amp; 0.0099 \\ &amp; = &amp; 0.99\% \end{array} "/></p>
<p>So almost 1 test in 100 will run when the service is not broken, but will report that it’s broken anyway.</p>
<p>You should already be feeling anxious.</p>
<p>With these numbers, we can calculate what the medical field calls the probe’s <b>positive predictive value</b>: the probability that, if a given test produces a positive result, it’s a <i>true positive</i>. For our purposes this is the probability that, if we just got paged, something’s actually broken.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7B%28Positive+predictive+value%29%7D+%26+%3D+%26+%5Cfrac%7B%5Ctext%7BTPR%7D%7D%7B%5Ctext%7BTPR%7D+%2B+%5Ctext%7BFPR%7D%7D+%5C%5C+%26+%3D+%26+%5Cfrac%7B0.00099%7D%7B0.00099+%2B+0.0099%7D+%5C%5C+%26+%3D+%26+0.091+%5C%5C+%26+%3D+%26+9.1%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7B%28Positive+predictive+value%29%7D+%26+%3D+%26+%5Cfrac%7B%5Ctext%7BTPR%7D%7D%7B%5Ctext%7BTPR%7D+%2B+%5Ctext%7BFPR%7D%7D+%5C%5C+%26+%3D+%26+%5Cfrac%7B0.00099%7D%7B0.00099+%2B+0.0099%7D+%5C%5C+%26+%3D+%26+0.091+%5C%5C+%26+%3D+%26+9.1%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7B%28Positive+predictive+value%29%7D+%26+%3D+%26+%5Cfrac%7B%5Ctext%7BTPR%7D%7D%7B%5Ctext%7BTPR%7D+%2B+%5Ctext%7BFPR%7D%7D+%5C%5C+%26+%3D+%26+%5Cfrac%7B0.00099%7D%7B0.00099+%2B+0.0099%7D+%5C%5C+%26+%3D+%26+0.091+%5C%5C+%26+%3D+%26+9.1%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\begin{array}{rcl} \text{(Positive predictive value)} &amp; = &amp; \frac{\text{TPR}}{\text{TPR} + \text{FPR}} \\ &amp; = &amp; \frac{0.00099}{0.00099 + 0.0099} \\ &amp; = &amp; 0.091 \\ &amp; = &amp; 9.1\% \end{array} "/></p>
<p>Bad news. There’s no hand-waving here. If you get alerted by this probe, there’s only a 9.1% chance that something’s actually wrong.</p>
<h2>Car alarms and smoke alarms</h2>
<p>When you hear a car alarm going off, do you run to the window and start looking for car thieves? Do you call 9-1-1? Do you even <i>notice</i> car alarms anymore?</p>
<p>Car alarms have a very low positive predictive value. They go off for so many spurious reasons: glitchy electronics, drunk people leaning on the hood, accidental pressing of the panic button. And as a result of this low PPV, car alarms are much less useful as theft deterrents than they could be.</p>
<p>Now think about smoke alarms. People trust smoke alarms. When a smoke alarm goes off in a school or an office building, everybody stops what they’re doing and walks outside in an orderly fashion. Why? Because when smoke alarms go off (and there’s no drill scheduled), it frequently means there’s actual smoke somewhere.</p>
<p>This is not to say that smoke alarms have a perfect PPV, of course, as anybody who’s lost half an hour of their time to a false positive will tell you. But their PPV is high enough that people still pay attention to them.</p>
<p>We should strive to make our alerts more like smoke alarms than car alarms.</p>
<h2>Sensitivity and specificity</h2>
<p>Let’s go back to our example: probing a service that works 99.9% of the time. There’s some jargon for the tradeoff we’re looking at. It’s the tradeoff between the <b>sensitivity</b> of our test (the probability of detecting a real problem if there is one) and its <b>specificity</b> (the probability that we <i>won’t</i> detect a problem if there <i>isn’t</i> one).</p>
<p>Every time we set a monitoring threshold, we have to balance sensitivity and specificity. And one of the first questions we should ask ourselves is: “How high does our specificity have to be in order to get a decent positive predictive value?” It just takes some simple algebra to figure this out. We start with the PPV formula we used before, enjargoned below:</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BPPV%7D+%26+%3D+%26+%5Cfrac%7B%5Ctext%7BTPR%7D%7D%7B%5Ctext%7BTPR%7D%2B%5Ctext%7BFPR%7D%7D+%5C%5C+%26+%3D+%26+%5Cfrac%7B%5Ctext%7B%28prob.+of+failure%29%7D%5Ccdot%5Ctext%7B%28sensitivity%29%7D%7D%7B%5Ctext%7B%28prob.+of+failure%29%7D%5Ccdot%5Ctext%7B%28sensitivity%29%7D+%2B+%281+-+%5Ctext%7B%28prob.+of+failure%29%7D%29%5Ccdot%281+-+%5Ctext%7B%28specificity%29%7D%29%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BPPV%7D+%26+%3D+%26+%5Cfrac%7B%5Ctext%7BTPR%7D%7D%7B%5Ctext%7BTPR%7D%2B%5Ctext%7BFPR%7D%7D+%5C%5C+%26+%3D+%26+%5Cfrac%7B%5Ctext%7B%28prob.+of+failure%29%7D%5Ccdot%5Ctext%7B%28sensitivity%29%7D%7D%7B%5Ctext%7B%28prob.+of+failure%29%7D%5Ccdot%5Ctext%7B%28sensitivity%29%7D+%2B+%281+-+%5Ctext%7B%28prob.+of+failure%29%7D%29%5Ccdot%281+-+%5Ctext%7B%28specificity%29%7D%29%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BPPV%7D+%26+%3D+%26+%5Cfrac%7B%5Ctext%7BTPR%7D%7D%7B%5Ctext%7BTPR%7D%2B%5Ctext%7BFPR%7D%7D+%5C%5C+%26+%3D+%26+%5Cfrac%7B%5Ctext%7B%28prob.+of+failure%29%7D%5Ccdot%5Ctext%7B%28sensitivity%29%7D%7D%7B%5Ctext%7B%28prob.+of+failure%29%7D%5Ccdot%5Ctext%7B%28sensitivity%29%7D+%2B+%281+-+%5Ctext%7B%28prob.+of+failure%29%7D%29%5Ccdot%281+-+%5Ctext%7B%28specificity%29%7D%29%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\begin{array}{rcl} \text{PPV} &amp; = &amp; \frac{\text{TPR}}{\text{TPR}+\text{FPR}} \\ &amp; = &amp; \frac{\text{(prob. of failure)}\cdot\text{(sensitivity)}}{\text{(prob. of failure)}\cdot\text{(sensitivity)} + (1 - \text{(prob. of failure)})\cdot(1 - \text{(specificity)})} \end{array} "/></p>
<p>To make this math a little more readable, let’s let <i>p</i> = PPV, <i>f</i> = the probability of service failure, <i>a</i> = sensitivity, and <i>b</i> = specificity. And let’s solve for <i>b</i>.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+p+%26+%3D+%26+%5Cfrac%7Bfa%7D%7Bfa+%2B+%281-f%29%2A%281-b%29%7D+%5C%5C+fa+%2B+%281-f%29%281-b%29+%26+%3D+%26+%5Cfrac%7Bfa%7D%7Bp%7D+%5C%5C+1-b+%26+%3D+%26+%5Cfrac%7B%5Cfrac%7Bfa%7D%7Bp%7D+-+fa%7D%7B1-f%7D+%5C%5C+b+%26+%3D+%26+1+-+%5Cfrac%7B%5Cfrac%7Bfa%7D%7Bp%7D+-+fa%7D%7B1-f%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+p+%26+%3D+%26+%5Cfrac%7Bfa%7D%7Bfa+%2B+%281-f%29%2A%281-b%29%7D+%5C%5C+fa+%2B+%281-f%29%281-b%29+%26+%3D+%26+%5Cfrac%7Bfa%7D%7Bp%7D+%5C%5C+1-b+%26+%3D+%26+%5Cfrac%7B%5Cfrac%7Bfa%7D%7Bp%7D+-+fa%7D%7B1-f%7D+%5C%5C+b+%26+%3D+%26+1+-+%5Cfrac%7B%5Cfrac%7Bfa%7D%7Bp%7D+-+fa%7D%7B1-f%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+p+%26+%3D+%26+%5Cfrac%7Bfa%7D%7Bfa+%2B+%281-f%29%2A%281-b%29%7D+%5C%5C+fa+%2B+%281-f%29%281-b%29+%26+%3D+%26+%5Cfrac%7Bfa%7D%7Bp%7D+%5C%5C+1-b+%26+%3D+%26+%5Cfrac%7B%5Cfrac%7Bfa%7D%7Bp%7D+-+fa%7D%7B1-f%7D+%5C%5C+b+%26+%3D+%26+1+-+%5Cfrac%7B%5Cfrac%7Bfa%7D%7Bp%7D+-+fa%7D%7B1-f%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\begin{array}{rcl} p &amp; = &amp; \frac{fa}{fa + (1-f)*(1-b)} \\ fa + (1-f)(1-b) &amp; = &amp; \frac{fa}{p} \\ 1-b &amp; = &amp; \frac{\frac{fa}{p} - fa}{1-f} \\ b &amp; = &amp; 1 - \frac{\frac{fa}{p} - fa}{1-f} \end{array} "/></p>
<p>So, sticking with the parameters of our initial example (0.1% probability of service failure, 99% sensitivity) and deciding that we want a positive predictive value of at least 90% (so that 9 out of 10 alerts will mean something’s actually broken), we end up with</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BSpecificity%7D+%26+%3D+%26+1+-+%5Cfrac%7B%5Cfrac%7B0.001%2A0.99%7D%7B0.9%7D+-+%280.001+%2A+0.99%29%7D%7B%281+-+0.001%29%7D+%5C%5C+%26+%3D+%26+0.9999+%5C%5C+%26+%3D+%26+99.99%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BSpecificity%7D+%26+%3D+%26+1+-+%5Cfrac%7B%5Cfrac%7B0.001%2A0.99%7D%7B0.9%7D+-+%280.001+%2A+0.99%29%7D%7B%281+-+0.001%29%7D+%5C%5C+%26+%3D+%26+0.9999+%5C%5C+%26+%3D+%26+99.99%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cbegin%7Barray%7D%7Brcl%7D+%5Ctext%7BSpecificity%7D+%26+%3D+%26+1+-+%5Cfrac%7B%5Cfrac%7B0.001%2A0.99%7D%7B0.9%7D+-+%280.001+%2A+0.99%29%7D%7B%281+-+0.001%29%7D+%5C%5C+%26+%3D+%26+0.9999+%5C%5C+%26+%3D+%26+99.99%5C%25+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=111111&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\begin{array}{rcl} \text{Specificity} &amp; = &amp; 1 - \frac{\frac{0.001*0.99}{0.9} - (0.001 * 0.99)}{(1 - 0.001)} \\ &amp; = &amp; 0.9999 \\ &amp; = &amp; 99.99\% \end{array} "/></p>
<p>The necessary specificity is about 99.99% — that’s <i>way</i> higher than the sensitivity of 99%! In order to get a probe that detects failures in this service with sufficient reliability, you need to be 100 times less likely to falsely detect a failure than you are to miss a positive!</p>
<h2>So listen.</h2>
<p>You’ll often be tempted to favor high sensitivity at the cost of specificity, and sometimes that’s the right choice. Just be careful: avoid the base rate fallacy by remembering that your false-positive rate needs to be <i>much</i> smaller than your failure rate if you want your test to have a decent positive predictive value.</p>
					</div><!-- .entry-content -->

		<!-- .entry-footer -->
	</div>
</article></div>
  </body>
</html>
