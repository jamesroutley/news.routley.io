<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://justine.lol/mmap/">Original</a>
    <h1>Using mmap to make LLaMA load faster</h1>
    
    <div id="readability-page-1" class="page">

<p>
Apr 5<sup>th</sup>, 2023 @ <a href="https://justine.lol/index.html">justine&#39;s web page</a>
</p>

<img src="https://justine.lol/mmap/llama.png" alt="[Drawing of a LLaMA]" width="334" height="316"/>
<p>
When Meta released
<a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA</a>
back in February, many of us were excited to see a
high-quality <a href="https://en.wikipedia.org/wiki/Large_language_model">Large
Language Model</a> (LLM) become available for public access. Many of us
who signed up however, had difficulties getting LLaMA to run on our edge
and personal computer devices. One month ago,
<a href="https://github.com/ggerganov">Georgi Gerganov</a> started
the <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>
project to provide a solution to this, and since then his project has
been one of the hottest things on GitHub,
having <a href="https://justine.lol/mmap/llama.cpp-starchart.png">earned itself 19k stars</a>. I
spent the last few weeks volunteering for this project, and I&#39;ve got
some great news to share about its recent progress.

</p><p>
We modified llama.cpp to load weights using
<a href="https://en.wikipedia.org/wiki/Mmap">mmap()</a> instead of C++
standard I/O. That enabled us to load LLaMA 100x faster using half as
much memory. Our changes have just been made available in the latest
release. The benefits are as follows:

</p><dl>
<dt>More Processes
</dt><dd>You can now run multiple LLaMA processes simultaneously on your
computer. Here&#39;s a
<a href="https://twitter.com/ggerganov/status/1642115206544343040">video
of Georgi having a conversation with four chatbots</a> powered by four
independent llama.cpp processes running on the same Mac. So llama.cpp is
not only going to be a better friend to you, it can also serve as your
artificial circle of friends too. The trick that makes it possible
is <code>mmap()</code> lets us map the read-only weights
using <code>MAP_SHARED</code>, which is the same technique that&#39;s
traditionally been used for loading executable software. So we figured,
why aren&#39;t we using it to load neural network software too? Now we can.
</dd><dt>Bigger Models
</dt><dd>It&#39;s now <em>safe</em> to load models that are 2x larger without
compromising system stability. Meta gave us the LLaMA models 7B, 13B,
30B, and 65B where bigger numbers usually means better artificial
intelligence that&#39;s hungrier for RAM. If you needed 40GB of RAM before
to safely load a 20GB model, then now you need 20GB (please note your
computer still needs another 8GB or so on top of that for memory that
isn&#39;t weights). The reason why our changes make an improvement is
because <code>mmap()</code> avoids the need to copy pages. Copying pages
is bad, because you don&#39;t want copied memory to compete with the kernel
file cache. When too much copied memory gets created, the kernel reacts
by evicting cache entries, which means LLaMA will load slowly from disk
each time. Since reducing memory requirements, users have been telling
wonderful stories, like running
<a href="https://news.ycombinator.com/item?id=35407026">
LLaMA-13B on an old Android phone</a>. For PCs with 32GB of RAM, you
should be able to comfortably run LLaMA-30B, since it&#39;s 20GB with 4-bit
quantized weights.
</dd><dt>Faster Loading
</dt><dd>Remember that progress bar which made you wait for weights to load
each time you ran the command? We got rid of that. Linux users should
expect a 100x improvement in load time. Windows and MacOS users should
expect a 10x improvement. What this means is that tokens will start
being produced effectively instantaneously when you run LLaMA, almost
providing a similar UX to ChatGPT on the shell. It&#39;s important to note
these improvements are due to an amortized cost. The first time you load
a model after rebooting your computer, it&#39;s still going to go slow,
because it has to load the weights from disk. However each time it&#39;s
loaded afterwards, it should be fast (at least until memory pressure
causes your file cache to be evicted). This is great news for anyone
wanting to use an LLM to generate text from a shell script, similar to
the <code>cat</code> command. However, if your use case requires
frequently restarting inference for reasons of context or quality, then
you&#39;ll now have a quicker road to recovery. There is however a catch:
after your weights file instantly loads, you still need to wait for your
prompt to load. That&#39;s something you can expect to see addressed soon.
</dd></dl>

<p>
One of the reasons llama.cpp attracted so much attention is because it
lowers the barriers of entry for running large language models. That&#39;s
great for helping the benefits of these models be more widely accessible
to the public. It&#39;s also helping businesses save on costs. Thanks to
<code>mmap()</code> we&#39;re much closer to both these goals than we were before.
Furthermore, the reduction of user-visible latency has made the tool
more pleasant to use.

</p><p>
The new <code>mmap()</code> based loader is now available in the
llama.cpp project, which is released under the MIT license on GitHub in
both source and binary forms:

</p><p>
<em><a href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></em>
</p>

<p>
Existing users will need to convert their GGML weights to the new file
format:

</p><pre>less <a href="https://github.com/ggerganov/llama.cpp/blob/53dbba7/migrate-ggml-2023-03-30-pr613.py">migrate-ggml-2023-03-30-pr613.py</a>            <span># view manual</span>
python migrate-ggml-2023-03-30-pr613.py SRC DST  <span># run tool</span>
</pre>

<p>
New users should
<a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">request
access from Meta</a> and read
<a href="https://til.simonwillison.net/llms/llama-7b-m2">Simon
Willison&#39;s blog post</a> for an explanation of how to get started.
Please note that, with our recent changes, some of the steps in his 13B
tutorial relating to multiple .1, etc. files can now be skipped. That&#39;s
because our conversion tools now turn multi-part weights into a single
file.

</p><h2 class="page" id="how">
  <a href="#how">
    How We Did It
  </a>
</h2>

<p>
When the llama.cpp project received feedback that we should be using
<code>mmap()</code> the first idea that came to mind was to find a way to make it
work within the confines of our C++ library abstractions.
<a href="https://github.com/apaz-cli">@apaz-cli</a> was the person who
got the ball rolling on this. The basic idea we tried was to see how
much better <code>mmap()</code> could make the loading of weights, if we wrote a new
implementation of <code>std::ifstream</code>. This meant that, rather
than having the underlying I/O implementation call <code>read()</code>,
it would instead use <code>mmap()</code> from the constructor, and then
the <code>our_ifstream::read()</code> function would just do a
<code>memcpy()</code> under the hood.

</p><p>
We determined that this would improve load latency by 18%. This was a
big deal, since it&#39;s user-visible latency. However it turned out we were
measuring the wrong thing. Please note that I say &#34;wrong&#34; in the best
possible way; being wrong makes an important contribution to knowing
what&#39;s right. I don&#39;t think I&#39;ve ever seen a high-level library that&#39;s
able to do what <code>mmap()</code> does, because it defies attempts at
abstraction. After comparing our solution to dynamic linker
implementations, it became obvious that the true value
of <code>mmap()</code> was in not needing to copy the memory at all. The
weights are just a bunch of floating point numbers on disk. At runtime,
they&#39;re just a bunch of floats in memory. So what <code>mmap()</code>
does is it simply makes the weights on disk available at whatever memory
address we want. We simply must ensure that the layout on disk is the
same as the layout in memory.

</p><h3 class="page" id="proto">
  <a href="#proto">
    Prototyping
  </a>
</h3>

<p>
After going back to the drawing board, the tricky thing here was that
the C++ loading process appeared to reshape the tensors after reading
them. If we add printf statements to the old loading code, we&#39;d get
results like:

</p><pre>moving 0x640 bytes from offset 0x4a607 to offset 0 (n_dims=2 n_parts=2)
moving 0x640 bytes from offset 0x4ac47 to offset 0xc80 (n_dims=2 n_parts=2)
moving 0x640 bytes from offset 0x4b287 to offset 0x1900 (n_dims=2 n_parts=2)
moving 0x640 bytes from offset 0x4b8c7 to offset 0x2580 (n_dims=2 n_parts=2)
moving 0x640 bytes from offset 0x4bf07 to offset 0x3200 (n_dims=2 n_parts=2)
moving 0x640 bytes from offset 0x4c547 to offset 0x3e80 (n_dims=2 n_parts=2)
moving 0x640 bytes from offset 0x4cb87 to offset 0x4b00 (n_dims=2 n_parts=2)
moving 0x640 bytes from offset 0x4d1c7 to offset 0x5780 (n_dims=2 n_parts=2)
<em>... and so forth, for another 200k+ lines</em>
</pre>

<p>
There were also a number of C++ STL containers that got populated with
information during the loading process. It became clear that, in order
to have a mappable file whose memory layout was the same as what
evaluation wanted at runtime, we&#39;d need to not only create a new file,
but also serialize those STL data structures too. The only way around it
would have been to redesign the file format, rewrite all our conversion
tools, and ask our users to migrate their model files. We&#39;d already
earned an 18% gain, so why give that up to go so much further, when we
didn&#39;t even know for certain the new file format would work?

</p><p>
I ended up writing a quick and dirty hack to show that it would work. I
used a C library override trick where I started with code like this:

</p><pre><span>int</span> main(<span>int</span> argc, <span>char</span> **argv) {
    <span>gpt_vocab</span> vocab;
    <span>llama_model</span> model;
    llama_model_load(model, vocab);
    <span>for</span> (;;) {
        llama_eval(model, vocab);
    }
}
</pre>

<p>
Then I modified the code above to avoid using the stack or static
memory, and instead rely on the heap. On platforms like Linux, I was
able to easily override the libc allocators by doing something like
this:

</p><table>
<tbody><tr><td>
<pre><span>struct</span> <span>magic</span> *mag;

<span>int</span> main(<span>int</span> argc, <span>char</span> **argv) {
    <span>gpt_vocab</span> *vocab;
    <span>llama_model</span> *model;
    <span>long</span> len = 100<span>l</span>*1024*1024*1024
    <span>int</span> fd = open(<span>&#34;magic.dat&#34;</span>, O_RDWR|O_CREAT);
    ftruncate(fd, len);
    mag = mmap(0x330000000000, len,
               PROT_READ|PROT_WRITE,
               MAP_SHARED|MAP_FIXED, fd, 0);
    <span>if</span> (!mag-&gt;vocab) {
        vocab = <span>new</span> <span>gpt_vocab</span>;
        model = <span>new</span> <span>llama_model</span>;
        <strong>llama_model_load</strong>(*model, *vocab);
        msync(0x330000000000, len);
        mag-&gt;model = model;
        mag-&gt;vocab = vocab;
    } <span>else</span> {
        vocab = mag-&gt;vocab;
        model = mag-&gt;model;
    }
    <span>for</span> (;;) {
        <strong>llama_eval</strong>(*model, *vocab);
    }
}
</pre>
</td><td>
<pre><span>void</span> *memalign(<span>size_t</span> a, <span>size_t</span> n) {
    <span>if</span> (n &lt; 1) n = 1;
    <span>if</span> (a &lt; 16) a = 16;
    <span>while</span> (a &amp; (a - 1)) ++a;
    <span>// set <em>p</em> to next chunk in *mag on a</span>
    ((<span>size_t</span> *)p)[-1] = n;
    <span>return</span> p;
}

<span>void</span> *malloc(<span>size_t</span> n) {
    <span>return</span> memalign(16, n);
}

<span>void</span> *calloc(<span>size_t</span> n, <span>size_t</span> z) {
    <span>void</span> *p;
    <span>if</span> ((p = malloc((n *= z))))
        memset(p, 0, n);
    <span>return</span> p;
}

<span>void</span> *realloc(<span>void</span> *p, <span>size_t</span> n) {
    <span>void</span> *q;
    <span>if</span> (!p) <span>return</span> malloc(n);
    <span>if</span> (!n) { free(p); <span>return</span> 0; }
    <span>if</span> ((q = malloc(n)))
        memcpy(q, p, ((<span>size_t</span> *)p)[-1]);
    <span>return</span> q;
}

<span>void</span> free(<span>void</span> *p) {}
</pre>
</td></tr></tbody></table>

<p>
<em>Pseudo-C++ adapted
from <a href="https://github.com/ggerganov/llama.cpp/commit/5b8023d935401072b73b63ea995aaae040d57b87">5b8023d935401072b73b63ea995aaae040d57b87</a></em>

</p><p>
The cool thing about the C library, is just about everything depends on
it. If you override functions like <code>malloc()</code> on platforms
like Linux, then all the languages and tools downstream of C (e.g. C++)
will use it too. So the code above not only captures the GGML library
use of <code>malloc()</code>, but also the STL vectors and maps that
were being created too. The only thing I had to do, was make sure the
stack-allocated memory got placed on the heap, which was basically just
the model and vocab objects. The pointers to those of course needed to
be stored in the magically mapped region, so that upon the process
loading a second time, it&#39;d have access to the root of the object graph.

</p><p>
This hack is how I made the case that loading could in fact be
instantaneous. I didn&#39;t need to know much about the implementation
details of the loader. I just redefined the heap so that it was a memory
mapped file rather than the anonymous mapping it would use normally.
Please note the above code <em>does not</em> follow any best practices.
I think my code even deserves the honor of being called an abomination,
which makes it the very best kind of experimental code. The correct and
proper way of doing things is obviously to change the file format. But
that would take 10x more effort. Now we knew for sure that it was worth
doing. So the code you see above was eventually tossed away, so we could
focus on the file format.

</p><h3 class="page" id="mapping">
  <a href="#mapping">
    Mapping Memory
  </a>
</h3>

<p>
About a week later, the first code we ended up putting in the main
branch that calls the <code>mmap()</code> function was
<a href="https://github.com/ggerganov/llama.cpp/commit/c03ae8dca1d7c451054754979e60a6de1f64c3cd">Slaren&#39;s
change</a>. This might surprise some of the people who&#39;ve been following
my work. Managers and celebrities are usually the ones who get all the
kudos. The tech industry isn&#39;t used to having its key collaborators on
landmark technical achievements be anonymous people from 4chan, but
that&#39;s exactly what happened here. While bringing the benefits
of <code>mmap()</code> was a team effort, you could say
that <a href="https://github.com/slaren">@Slaren</a> was the person who
added <code>mmap()</code> support. He did that by pointing out something
very smart, which is that the 7B model only had 1-dimensional tensors,
and as a result, didn&#39;t need to be unsharded, and therefore required no
file format change. So he wrote the code and updated the project to map
the file. Then he changed the loader so that it simply assigns a pointer
to
<code>tensor-&gt;data</code> instead of calling <code>read()</code>,
whenever the tensor is 1-d. In doing this, Slaren showed us that it was
possible to bring the benefits of instant load times to LLaMA 7B users
immediately.

</p><p>
The hardest thing about introducing support for a function like <code>mmap()</code>
though, is figuring out how to get it to work on Windows. I wouldn&#39;t be
surprised if many of the people who had the same idea in the past, about
using <code>mmap()</code> to load machine learning models, ended up not doing it
because they were discouraged by Windows not having it. It turns out
that Windows has a set of nearly, but not quite identical functions,
called <code>CreateFileMapping()</code>
and <code>MapViewOfFile()</code>. <a href="https://github.com/oKatanaaa">@oKatanaaa</a>
is the person most responsible for helping us figure out how to use them
to create a wrapper function. Thanks to him, we were able to delete all
of the old standard i/o loader code at the end of the project, because
every platform in our support vector was able to be supported by <code>mmap()</code>.
That meant we actually had a <em>net negative</em> impact on the number
lines of C++ code! I think coordinated efforts like this are rare, yet
really important for maintaining the attractiveness of a project like
llama.cpp, which is surprisingly able to do LLM inference using only a
few thousand lines of code and zero dependencies. We also had some help
from <a href="https://github.com/CoderRC">@CoderRC</a> who had
previously designed his own set of
<a href="https://github.com/CoderRC/libmingw32_extended">POSIX functions
for Mingw32</a> and knew the best technique for mmap feature detection.

</p><h3 class="page" id="file">
  <a href="#file">
    Changing the File Format
  </a>
</h3>

<p>
So far, we&#39;ve nailed down <code>mmap()</code> support for 7B. However
we&#39;re still using the old C++ standard I/O code for the larger models.
So the only thing left to do at this point was to change the file
format, so that <code>mmap()</code> generalized to all the models we
were using. That was the part I was responsible for doing.

</p><p>
In order to do inference, we need to load a few hundred tensors out of
.pth files using torch, inside our conversion script. With the 7B model
this was relatively simple. We only needed to iterate over the tensors
in a single file, and produce a single file of output. The tensors in 7B
were perfect already, and fully contiguous.

</p><pre>$ ls -hal models/7B/
-rw-r--r--   1 jart  staff   3.9G Mar 29 17:45 ggml-model-q4_0.bin
</pre>

<p>
The issue was that, for models larger than 7B, the tensors were sharded
into multiple files. Under our old way of doing things, we were simply
doing a 1:1 copy when converting from .pth to GGML. As a result, the
ugliness of loading from multiple files was preserved. Here&#39;s what it
looked like on disk, for instance, with the LLaMA-65B model:

</p><pre>$ ls -hal models/65B/
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:42 ggml-model-q4_0.bin
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:43 ggml-model-q4_0.bin.1
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:43 ggml-model-q4_0.bin.2
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:44 ggml-model-q4_0.bin.3
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:45 ggml-model-q4_0.bin.4
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:45 ggml-model-q4_0.bin.5
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:46 ggml-model-q4_0.bin.6
-rw-r--r--   1 jart  staff   4.8G Mar 16 13:46 ggml-model-q4_0.bin.7
</pre>

<p>
Each file had the same structure, except the tensor data itself was like
interlaced movie frames.

</p><p>
<a href="https://justine.lol/mmap/interlaced.gif"><img width="375" height="288" alt="[interlaced.gif]" src="https://justine.lol/mmap/interlaced.gif"/></a>

</p><p>
To make matters more challenging, different tensors are split apart in
different ways, depending on the name. Some were split across columns,
and some were split across rows. <code>mmap()</code> is a powerful system call, but
it doesn&#39;t let you create overlapping mappings that interleave tensors
appropriately. Even if we were willing to use hundreds of thousands of
<code>mmap()</code> calls to reassemble the read/write operations in a
copyless manner, <code>mmap()</code> has a 4096-byte alignment
requirement that is too coarse for the tensors in this format. We had to
rewrite the converter tool to put them back together by hand, into a
much larger unified file, as an upfront one-time cost.

</p><pre>$ ls -hal models/65B/
-rw-r--r--   1 jart  staff    38G Mar 16 13:42 ggml-model-q4_0.bin2
</pre>

<p>
The C++ loader was already doing the necessary conversion. All I had to
do was simply move that code into the Python conversion script instead.
That ensured the same commands people used before would automatically
use the new format. Once I patched that, all which remained was writing
a migration script. That was important since many people deleted Meta&#39;s
original .pth files to save hard disk space, and they needed a tool to
convert from the old format to the new format. This tool is the script
that was recommended above, called
<a href="https://github.com/ggerganov/llama.cpp/blob/53dbba7/migrate-ggml-2023-03-30-pr613.py">migrate-ggml-2023-03-30-pr613.py</a>.
It was relatively straightforward to make, since it follows a similar
logic as the conversion tool. Except in this case, I didn&#39;t need Torch,
pickle, or anything like that. All that was needed, was plain old Numpy
combined with seek, read, and write system calls. That&#39;s nice, since my
favorite distro Alpine can&#39;t even run Torch!

</p><p>
The interesting thing about the <code>seek()</code> function is that
operating systems let us seek past the end of a file. So it creates a
convenient framework for unsharding tensors from multi-part files, since
the i/o can be performed by writing tensors to disk, in such a way that
the tensors have holes. We can then fill those in multiple passes once
the remaining shards are processed. Doing that raises interesting
questions of course, about how the file system might allocate blocks in
the underlying physical medium. It&#39;s something that&#39;s not necessarily
within our control, but I&#39;d still love to learn more about it. For
example, on some file systems I&#39;ve noticed that, after converting a
file, it might load from disk faster if <code>cp</code> is used
afterwards to produce a copy.

</p><p>
There&#39;s one last important benefit to the new file format. It ensures
tensors are aligned on a 32-byte boundary. The old file format didn&#39;t
perform a roundup after writing the model vocabulary to disk. As a
result, floats were being <code>mmap()</code>&#39;d to odd addresses half
the time, which would trigger UBSAN errors. It also potentially left
some meat on the table when it comes to SIMD instructions. Alignment
generally isn&#39;t a problem on modern microarchitectures of the two major
architectures. In practice, the only time misalignment is completely
forbidden is with semaphores on ARM. However just because it seems to
work doesn&#39;t mean doesn&#39;t mean misalignment won&#39;t consume additional
resources under the hood, or cause other problems in sneaky ways. One
example would be x86, where misaligned semaphores will seem to work
until you have the unlucky chance of your <code>unsigned int</code>
overlapping a 64-byte cacheline boundary. For that reason, the new file
format takes a more conservative approach, and it may potentially open
some doors in the future for certain kinds of optimizations.

</p><p>
For further details, please
see <a href="https://github.com/ggerganov/llama.cpp/commit/78ca9838ee36660a776e97e3391b6fb5dcaacf7f">78ca9838ee36660a776e97e3391b6fb5dcaacf7f</a>
and <a href="https://github.com/ggerganov/llama.cpp/commit/ee0c40dd6de8c3c658ae43199939ef40bb1cf408">ee0c40dd6de8c3c658ae43199939ef40bb1cf408</a>.

</p><h2 class="page" id="notes">
  <a href="#notes">
    Notes
  </a>
</h2>

<p>
Many sources of information on the world wide web that explain how to
use <code>mmap()</code> will also insist upon the use
of <code>madvise()</code> as though its benefits were established fact.
I couldn&#39;t measure any evidence that it&#39;d be helpful in our case, since
transformer models like LLaMA need to immediately fault every single
memory page as soon as the weights are loaded.
The <code>madvise()</code> system call is probably only helpful in
situations where only a subset of pages are needed for a nontrivial
amount of time, during which the disk would otherwise become
underutilized.

</p><p>
<code>posix_fadvise(POSIX_FADV_SEQUENTIAL)</code> would be an example of
a kind of advice that&#39;d be potentially more helpful to users of LLMs.
One of the downsides of the Linux <code>cp</code> command, is copying a
file larger than RAM will destroy every existing entry in the file
cache. Under normal circumstances this is a good thing, since a least
recently used strategy usually works. However it can be problematic if
you&#39;re just organizing your files on a production system where you don&#39;t
want to disrupt performance. As far as I know, no standard command line
utility offers a way to exploit this functionality. So we may provide an
example of how to replace the <code>cp</code> command that could address
this use case too. Another feature such a command could offer, would be
the use of <code>copy_file_range()</code>, which enables files to be
copied within the same partition 2x faster than the
<code>sendfile()</code> technique that&#39;s used by standard utilities.

</p>
<img src="https://ipv4.games/claim?name=jart"/>
</div>
  </body>
</html>
