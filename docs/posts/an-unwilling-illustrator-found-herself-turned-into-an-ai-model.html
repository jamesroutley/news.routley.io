<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://waxy.org/2022/11/invasive-diffusion-how-one-unwilling-illustrator-found-herself-turned-into-an-ai-model/">Original</a>
    <h1>An unwilling illustrator found herself turned into an AI model</h1>
    
    <div id="readability-page-1" class="page"><article id="post-45095">

<div>
<p>Last weekend, <a href="https://holliemengert.com/">Hollie Mengert</a> woke up to an email pointing her to a <a href="https://www.reddit.com/r/StableDiffusion/comments/yaquby/2d_illustration_styles_are_scarce_on_stable/">Reddit thread</a>, the first of several messages from friends and fans, informing the Los Angeles-based illustrator and character designer that she was now an AI model.</p>
<p>The day before, a Redditor named MysteryInc152 <a href="https://www.reddit.com/r/StableDiffusion/comments/yaquby/2d_illustration_styles_are_scarce_on_stable/">posted</a> on the Stable Diffusion subreddit, ‚Äú2D illustration Styles are scarce on Stable Diffusion, so I created a DreamBooth model inspired by Hollie Mengert‚Äôs work.‚Äù</p>
<p>Using <a href="https://imgur.com/a/8YRCGsW">32 of her illustrations</a>, MysteryInc152 fine-tuned Stable Diffusion to recreate Hollie Mengert‚Äôs style. He then <a href="https://huggingface.co/ogkalu/Illustration-Diffusion">released</a> the checkpoint under an open license for anyone to use. The model uses her name as the identifier for prompts: ‚Äúillustration of a princess in the forest, holliemengert artstyle,‚Äù for example.</p>
<figure><a href="https://waxy.org/wp-content/uploads/2022/10/comparison2.jpg"><img width="1024" height="499" src="https://waxy.org/wp-content/uploads/2022/10/comparison2-1024x499.jpg" alt="" srcset="https://waxy.org/wp-content/uploads/2022/10/comparison2-1024x499.jpg 1024w, https://waxy.org/wp-content/uploads/2022/10/comparison2-300x146.jpg 300w, https://waxy.org/wp-content/uploads/2022/10/comparison2-768x374.jpg 768w, https://waxy.org/wp-content/uploads/2022/10/comparison2-1536x749.jpg 1536w, https://waxy.org/wp-content/uploads/2022/10/comparison2-800x390.jpg 800w, https://waxy.org/wp-content/uploads/2022/10/comparison2.jpg 1970w" sizes="(max-width: 1024px) 100vw, 1024px"/></a><figcaption>Artwork by Hollie Mengert (left) vs. images generated with Stable Diffusion DreamBooth in her style (right)</figcaption></figure>
<p>The post sparked a <a href="https://www.reddit.com/r/StableDiffusion/comments/yaquby/2d_illustration_styles_are_scarce_on_stable/">debate in the comments</a> about the ethics of fine-tuning an AI on the work of a specific living artist, even as new fine-tuned models are posted daily. The most-upvoted comment asked, ‚ÄúWhether it‚Äôs legal or not, how do you think this artist feels now that thousands of people can now copy her style of works almost exactly?‚Äù</p>
<p>Great question! How <em>did</em> Hollie Mengert feel about her art being used in this way, and what did MysteryInc152 think about the explosive reaction to it? I spoke to both of them to find out ‚Äî but first, I wanted to understand more about how DreamBooth is changing generative image AI.</p>
<hr/>
<p>Since its release in late August, I‚Äôve written about the <a href="https://waxy.org/2022/08/opening-the-pandoras-box-of-ai-art/">explosive creativity and complex ethical and legal debates</a> unleashed by the open-source release of Stable Diffusion, explored the <a href="https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/">billions of images it was trained on</a>, and talked about the <a href="https://waxy.org/2022/09/ai-data-laundering-how-academic-and-nonprofit-researchers-shield-tech-companies-from-accountability/">data laundering</a> that shields corporations like Stability AI from accountability. </p>
<p>By now, we‚Äôve all heard stories of artists who have <a href="https://kotaku.com/ai-art-dall-e-midjourney-stable-diffusion-copyright-1849388060">unwillingly found their work</a> used to train generative AI models, the frustration of being turned into a <a href="https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/">popular prompt</a> for people to mimic you, or how Stable Diffusion was being used to <a href="https://www.theverge.com/2022/9/15/23340673/ai-image-generation-stable-diffusion-explained-ethics-copyright-data">generate pornographic images</a> of celebrities.</p>
<p>But since its release, Stable Diffusion could really only depict the artists, celebrities, and other notable people who were popular enough to be well-represented in the model training data. Simply put, a diffusion model can‚Äôt generate images with subjects and styles that it hasn‚Äôt seen very much.</p>
<hr/>
<p>When Stable Diffusion was first released, I tried to generate images of myself, but even though there are a bunch of photos of me online, there weren‚Äôt enough for the model to understand what I looked like.</p>
<figure><a href="https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai.jpg"><img loading="lazy" width="1024" height="506" src="https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai-1024x506.jpg" alt="" srcset="https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai-1024x506.jpg 1024w, https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai-300x148.jpg 300w, https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai-768x380.jpg 768w, https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai-1536x759.jpg 1536w, https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai-2048x1013.jpg 2048w, https://waxy.org/wp-content/uploads/2022/10/me_vs_untrained_ai-800x396.jpg 800w" sizes="(max-width: 1024px) 100vw, 1024px"/></a><figcaption>Real photos of me (left) vs. Stable Diffusion output for the prompt ‚Äúportrait of andy baio‚Äù (right)</figcaption></figure>
<p>That‚Äôs true of even some famous actors and characters: while it can make a spot-on Mickey Mouse or Charlize Theron, it really struggles with <a href="https://lexica.art/?q=garfield">Garfield</a> and <a href="https://lexica.art/?q=danny+devito">Danny DeVito</a>. It knows that Garfield‚Äôs an orange cartoon cat and Danny DeVito‚Äôs general features and body shape, but not well enough to recognizably render either of them.</p>
<p>On August 26, Google AI announced <a href="https://dreambooth.github.io/">DreamBooth</a>, a technique for introducing new subjects to a pretrained text-to-image diffusion model, training it with as little 3-5 images of a person, object, or style.</p>
<figure><div>
<blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">Today, along with my collaborators at <a href="https://twitter.com/GoogleAI?ref_src=twsrc%5Etfw">@GoogleAI</a>, we announce DreamBooth! It allows a user to generate a subject of choice (pet, object, etc.) in myriad contexts and with text-guided semantic variations! The options are endless. (Thread üëá)</p>‚Äî Nataniel Ruiz (@natanielruizg) <a href="https://twitter.com/natanielruizg/status/1563166568195821569?ref_src=twsrc%5Etfw">August 26, 2022</a></blockquote>
</div></figure>
<p>Google‚Äôs researchers didn‚Äôt release any code, citing the potential ‚Äúsocietal impact‚Äù risk that ‚Äúmalicious parties might try to use such images to mislead viewers.‚Äù</p>
<p>Nonetheless, 11 days later, an AWS AI engineer released the <a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion">first public implementation</a> of DreamBooth using Stable Diffusion, open-source and available to everyone. Since then, there have been several dramatic optimizations in speed, usability, and memory requirements, making it extremely accessible to fine-tune it on multiple subjects quickly and easily.</p>
<hr/>
<p>Yesterday, I used a <a href="https://www.youtube.com/watch?v=ravETUa84P8">simple YouTube tutorial</a> and a popular <a href="https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb">Google Colab notebook</a> to fine-tune Stable Diffusion on 30 cropped 512√ó512 photos of me. The entire process, start to finish, took about 20 minutes and cost me about $0.40. (You can do it for free but it takes 2-3 times as long, so I paid for a faster Colab Pro GPU.)</p>
<p>The result felt like I opened a door to the multiverse, like <a href="https://www.youtube.com/watch?v=a2v7JK8c2fk">remaking that scene</a> from <em>Everything Everywhere All at Once</em>, but with me instead of Michelle Yeoh.</p>
<figure><a href="https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations.jpg"><img loading="lazy" width="1024" height="1024" src="https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations-1024x1024.jpg" alt="" srcset="https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations-1024x1024.jpg 1024w, https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations-300x300.jpg 300w, https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations-150x150.jpg 150w, https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations-768x768.jpg 768w, https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations-800x800.jpg 800w, https://waxy.org/wp-content/uploads/2022/10/me_dreambooth_generations.jpg 1535w" sizes="(max-width: 1024px) 100vw, 1024px"/></a><figcaption>Sample generations of me as a viking, anime, stained glass, vaporwave, Pixar character, Dali/Magritte painting, Greek statue, muppet, and Captain America</figcaption></figure>
<p>Frankly, it was shocking how little effort it took, how cheap it was, and how immediately fun the results were to play with. Unsurprisingly, a bunch of startups have popped up to make it even easier to DreamBooth yourself, including <a href="https://www.astria.ai/">Astria</a>, <a href="https://avatarai.me/">Avatar AI</a>, and <a href="https://www.profilepicture.ai/">ProfilePicture.ai</a>.</p>
<p>But, of course, there‚Äôs nothing stopping you from using DreamBooth on someone, or something, else.</p>
<hr/>
<p>I talked to Hollie Mengert about her experience last week. ‚ÄúMy initial reaction was that it felt invasive that my name was on this tool, I didn‚Äôt know anything about it and wasn‚Äôt asked about it,‚Äù she said. ‚ÄúIf I had been asked if they could do this, I wouldn‚Äôt have said yes.‚Äù</p>
<p>She couldn‚Äôt have granted permission to use all the images, even if she wanted to. ‚ÄúI noticed a lot of images that were fed to the AI were things that I did for clients like Disney and Penguin Random House. They paid me to make those images for them and they now own those images. I never post those images without their permission, and nobody else should be able to use them without their permission either. So even if he had asked me and said, can I use these? I couldn‚Äôt have told him yes to those.‚Äù</p>
<p>She had concerns that the fine-tuned model was associated with her name, in part because it didn‚Äôt really represent what makes her work unique. </p>
<p>‚ÄúWhat I pride myself on as an artist are authentic expressions, appealing design, and relatable characters. And I feel like that is something that I see AI, in general, struggle with most of all,‚Äù Hollie said. </p>
<figure><a href="https://waxy.org/wp-content/uploads/2022/10/comparison.jpg"><img loading="lazy" width="1024" height="499" src="https://waxy.org/wp-content/uploads/2022/10/comparison-1024x499.jpg" alt="" srcset="https://waxy.org/wp-content/uploads/2022/10/comparison-1024x499.jpg 1024w, https://waxy.org/wp-content/uploads/2022/10/comparison-300x146.jpg 300w, https://waxy.org/wp-content/uploads/2022/10/comparison-768x374.jpg 768w, https://waxy.org/wp-content/uploads/2022/10/comparison-1536x749.jpg 1536w, https://waxy.org/wp-content/uploads/2022/10/comparison-800x390.jpg 800w, https://waxy.org/wp-content/uploads/2022/10/comparison.jpg 1970w" sizes="(max-width: 1024px) 100vw, 1024px"/></a><figcaption>Four of Hollie‚Äôs illustrations used to train the AI model (left) and sample AI output (right)</figcaption></figure>
<p>‚ÄúI feel like AI can kind of mimic brush textures and rendering, and pick up on some colors and shapes, but that‚Äôs not necessarily what makes you really hireable as an illustrator or designer. If you think about it, the rendering, brushstrokes, and colors are the most surface-level area of art. I think what people will ultimately connect to in art is a lovable, relatable character. And I‚Äôm seeing AI struggling with that.‚Äù </p>
<p>‚ÄúAs far as the characters, I didn‚Äôt see myself in it. I didn‚Äôt personally see the AI making decisions that that I would make, so I did feel distance from the results. Some of that frustrated me because it feels like it isn‚Äôt actually mimicking my style, and yet my name is still part of the tool.‚Äù</p>
<p>She wondered if the model‚Äôs creator simply didn‚Äôt think of her as a person. ‚ÄúI kind of feel like when they created the tool, they were thinking of me as more of a brand or something, rather than a person who worked on their art and tried to hone things, and that certain things that I illustrate are a reflection of my life and experiences that I‚Äôve had. Because I don‚Äôt think if a person was thinking about it that way that they would have done it. I think it‚Äôs much easier to just convince yourself that you‚Äôre training it to be like an art style, but there‚Äôs like a person behind that art style.‚Äù</p>
<p>‚ÄúFor me, personally, it feels like someone‚Äôs taking work that I‚Äôve done, you know, things that I‚Äôve learned ‚Äî I‚Äôve been a working artist since I graduated art school in 2011 ‚Äî and is using it to create art that that I didn‚Äôt consent to and didn‚Äôt give permission for,‚Äù she said. ‚ÄúI think the biggest thing for me is just that my name is attached to it. Because it‚Äôs one thing to be like, this is a stylized image creator. Then if people make something weird with it, something that doesn‚Äôt look like me, then I have some distance from it. But to have my name on it is ultimately very uncomfortable and invasive for me.‚Äù</p>
<hr/>
<p>I reached out to MysteryInc152 on Reddit to see if they‚Äôd be willing to talk about their work, and we set up a call.</p>
<p>MysteryInc152 is Ogbogu Kalu, a young Nigerian engineer living and working in Halifax, Canada. Ogbogu is a fan of fantasy novels and football, comics and animation, and now, generative AI.</p>
<p>His initial hope was to make a series of comic books, but knew that doing it on his own would take years, even if he had the writing and drawing skills. When he first discovered Midjourney, he got excited and realized that it could work well for his project, and then Stable Diffusion dropped. </p>
<p>Unlike Midjourney, Stable Diffusion was entirely free, open-source, and supported powerful creative tools like <a href="https://huggingface.co/spaces/fffiloni/stable-diffusion-img2img">img2img</a>, <a href="https://huggingface.co/spaces/multimodalart/stable-diffusion-inpainting">inpainting</a>, and <a href="https://github.com/lkwq007/stablediffusion-infinity">outpainting</a>. It was nearly perfect, but achieving a consistent 2D comic book style was still a struggle. He first tried <a href="https://www.youtube.com/watch?v=1mEggRgRgfg">hypernetwork</a> style training, without much success, but DreamBooth finally gave him the results he was looking for.</p>
<p>Before publishing his model, Ogbogu wasn‚Äôt familiar with Hollie Mengert‚Äôs work at all. He was helping another Stable Diffusion user on Reddit who was <a href="https://www.reddit.com/r/StableDiffusion/comments/y7z9oa/hello_i_saw_that_you_can_train_dreambooth_for_a/">struggling to fine-tune a model</a> on Hollie‚Äôs work and getting lackluster results. He refined the image training set, got to work, and published the results the following day. He told me the training process took about 2.5 hours on a GPU at <a href="https://vast.ai/">Vast.ai</a>, and cost less than $2.</p>
<p>Reading the Reddit thread, his stance on the ethics seemed to border on fatalism: the technology is inevitable, everyone using it is equally culpable, and any moral line is completely arbitrary. In the Reddit thread, he <a href="https://www.reddit.com/r/StableDiffusion/comments/yaquby/comment/itfs7q6/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3">debated</a> with those pointing out a difference between using Stable Diffusion as-is and fine-tuning an AI on a single living artist:</p>
<blockquote><p>There is no argument based on morality. That‚Äôs just an arbitrary line drawn on the sand. I don‚Äôt really care if you think this is right or wrong. You either use Stable Diffusion and contribute to the destruction of the current industry or you don‚Äôt. People who think they can use [Stable Diffusion] but are the ‚Äògood guys‚Äô because of some funny imaginary line they‚Äôve drawn are deceiving themselves. There is no functional difference.</p></blockquote>
<p>On our call, I asked him what he thought about the debate. His take was very practical: he thinks it‚Äôs legal to train and use, likely to be determined fair use in court, and you can‚Äôt copyright a style. Even though you can recreate subjects and styles with high fidelity, the original images themselves aren‚Äôt stored in the Stable Diffusion model, with over 100 terabytes of images used to create a tiny 4 GB model. He also thinks it‚Äôs inevitable: Adobe is adding generative AI tools to Photoshop, Microsoft is adding an image generator to their design suite. ‚ÄúThe technology is here, like we‚Äôve seen countless times throughout history.‚Äù</p>
<p>Toward the end of our conversation, I asked, ‚ÄúIf it‚Äôs fair use, it doesn‚Äôt really matter in the eye of the law what the artist thinks. But do you think, having done this yourself and released a model, if they don‚Äôt find flattering, should the artist have any say in how their work is used?‚Äù</p>
<p>He paused for a few seconds. ‚ÄúYeah, that‚Äôs‚Ä¶ that‚Äôs a different‚Ä¶ I guess it all depends. This case is rather different in the sense that it directly uses the work of the artists themselves to replace them.‚Äù Ultimately, he thinks many of the objections to it are a misunderstanding of how it works: it‚Äôs not a form of collage, it‚Äôs creating new images and clearly transformative, more like ‚Äútrying to recall a vivid memory from your past.‚Äù</p>
<p>‚ÄúI personally think it‚Äôs transformative,‚Äù he concluded. ‚ÄúIf it is, then I guess artists won‚Äôt really have a say in how these models get written or not.‚Äù</p>
<hr/>
<p>As I was playing around with the model trained on myself, I started thinking about how cheap and easy it was to make. In the short term, we‚Äôre going to see fine-tuned for anything you can imagine: there are over 700 models in the <a href="https://huggingface.co/sd-concepts-library">Concepts Library</a> on HuggingFace so far, and trending in the last week alone on Reddit, models based on <a href="https://huggingface.co/nitrosocke/classic-anim-diffusion">classic Disney animated films</a>, <a href="https://www.reddit.com/r/StableDiffusion/comments/yf3ltx/new_dreambooth_model_modern_disney_now_available/">modern Disney animated films</a>, <a href="https://huggingface.co/dallinmackay/Tron-Legacy-diffusion">Tron: Legacy</a>, <a href="https://www.reddit.com/r/StableDiffusion/comments/yf38uy/cyberpunk_anime_diffusion_generate_anime_cyborgs/">Cyberpunk: Edgerunners</a>, <a href="https://www.reddit.com/r/StableDiffusion/comments/ygr39w/aespa_karina_dreambooth_stable_diffusion/">K-pop singers</a>, and <a href="https://www.reddit.com/r/StableDiffusion/comments/yf4jxf/finetuned_the_model_on_kurzgesagt_videos_with/">Kurzgesagt videos</a>.</p>
<figure><a href="https://waxy.org/wp-content/uploads/2022/10/image-1.png"><img loading="lazy" width="1024" height="704" src="https://waxy.org/wp-content/uploads/2022/10/image-1-1024x704.png" alt="" srcset="https://waxy.org/wp-content/uploads/2022/10/image-1-1024x704.png 1024w, https://waxy.org/wp-content/uploads/2022/10/image-1-300x206.png 300w, https://waxy.org/wp-content/uploads/2022/10/image-1-768x528.png 768w, https://waxy.org/wp-content/uploads/2022/10/image-1-800x550.png 800w, https://waxy.org/wp-content/uploads/2022/10/image-1.png 1080w" sizes="(max-width: 1024px) 100vw, 1024px"/></a><figcaption>Images generated using the ‚Äú<a href="https://huggingface.co/nitrosocke/classic-anim-diffusion">Classic Animation</a>‚Äù DreamBooth model trained on Disney animated films</figcaption></figure>
<p>Aside from the IP issues, it‚Äôs absolutely going to be used by bad actors: models fine-tuned on images of exes, co-workers, and, of course, popular targets of online harassment campaigns. Combining those with any of the emerging NSFW models trained on large corpuses of porn is a disturbing inevitability.</p>
<p>DreamBooth, like most generative AI, has incredible creative potential, as well as incredible potential for harm. Missing in most of these conversations is any discussion of consent: are you treating people the way you would want to be treated?</p>
<hr/>
<p>The day after we spoke, Ogbogu Kalu reached out to me through Reddit to see how things went with Hollie. I said she wasn‚Äôt happy about it, that it felt invasive and she had concerns about it being associated with her name. If asked for permission, she would have said no, but she also didn‚Äôt own the rights to several of the images and couldn‚Äôt have given permission even if she wanted to.</p>
<p>‚ÄúI figured. That‚Äôs fair enough,‚Äù he responded. ‚ÄúI did think about using her name as a token or not, but I figured since it was a single artist, that would be best. Didn‚Äôt want it to seem like I was training on an artist and obscuring their influence, if that makes sense. Can‚Äôt change that now unfortunately but I can make it clear she‚Äôs not involved.‚Äù</p>
<p>Two minutes later, he renamed the Huggingface model from <code>hollie-mengert-artstyle</code> to the more generic <code>Illustration-Diffusion</code>, and added a line to the README, ‚ÄúHollie is not affiliated with this.‚Äù</p>
<p>Two days later, he released a <a href="https://www.reddit.com/r/StableDiffusion/comments/yfsdx0/introducing_comicdiffusion_consistent_2d_styles/">new model</a> trained on <a href="https://imgur.com/a/qGbMg44">40 images</a> by concept and comic book artist <a href="https://jamesdaly.artstation.com/">James Daly III</a>.</p>
<figure><a href="https://waxy.org/wp-content/uploads/2022/10/comparison_daly.jpg"><img loading="lazy" width="1024" height="505" src="https://waxy.org/wp-content/uploads/2022/10/comparison_daly-1024x505.jpg" alt="" srcset="https://waxy.org/wp-content/uploads/2022/10/comparison_daly-1024x505.jpg 1024w, https://waxy.org/wp-content/uploads/2022/10/comparison_daly-300x148.jpg 300w, https://waxy.org/wp-content/uploads/2022/10/comparison_daly-768x379.jpg 768w, https://waxy.org/wp-content/uploads/2022/10/comparison_daly-1536x758.jpg 1536w, https://waxy.org/wp-content/uploads/2022/10/comparison_daly-800x395.jpg 800w, https://waxy.org/wp-content/uploads/2022/10/comparison_daly.jpg 1946w" sizes="(max-width: 1024px) 100vw, 1024px"/></a><figcaption>Art by James Daly III (left) vs. images generated by Stable Diffusion fine-tuned on his work</figcaption></figure>
</div>
</article></div>
  </body>
</html>
