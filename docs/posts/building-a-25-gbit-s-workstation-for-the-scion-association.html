<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/scionassociation/blog-25gbit-workstation">Original</a>
    <h1>Building a 25 Gbit/s workstation for the SCION Association</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/lga4677_socket.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/lga4677_socket.webp" alt="Empty LGA4677 CPU socket"/></a></p>
<p dir="auto">This is an LGA4677 socket and it&#39;s about to be fitted with a 12-core Intel Xeon
CPU to power the 64 PCIe Gen5 lanes for 3x Mellanox NVIDIA BlueField-2 Dual-25G
smart NICs, which will ultimately power the SCION Association&#39;s new 25 Gbit/s
testbench workstation!</p>
<p dir="auto">I built it to develop and test a new AF_XDP underlay for the
<a href="https://github.com/scionproto/scion">SCION OSS</a> has received significant data plane
performance improvements over the past years, but still requires further work.</p>
<p dir="auto">In this article, I&#39;ll walk you through the entire planning, building, and
configuration process in almost full detail.</p>
<p dir="auto">It‚Äôs hard to say how many hours went into it, but it was clearly a multi-week endeavor.
In total it cost us CHF ~3,741.34 (around ‚âà$4,700 USD) in materials.
See the <a href="#complete-components-list">complete list of components</a> at the end.</p>
<p dir="auto">Disclaimer: I spent many hours writing this article by hand, but I must confess,
LLMs did help me formulate and polish parts of it.</p>


<p dir="auto">SCION
(<strong>S</strong>calability, <strong>C</strong>ontrol, and <strong>I</strong>solation <strong>O</strong>n Next-Generation <strong>N</strong>etworks),
is, in a nutshell, an IETF draft-stage technology of a growing alternative to the
<a href="https://en.wikipedia.org/wiki/Border_Gateway_Protocol" rel="nofollow">Border Gateway Protocol (BGP)</a>.
It&#39;s an innovative inter-<a href="https://en.wikipedia.org/wiki/Autonomous_system_(Internet)" rel="nofollow">AS</a>
routing architecture designed to address BGP&#39;s fundamental flaws and security
vulnerabilities.</p>
<p dir="auto">Maybe at some point in the future the Internet will run SCION, although a more likely
scenario is that SCION and BGP will run alongside each other.
What is clear, though, is that critical infrastructure should run on SCION, where:</p>
<ul dir="auto">
<li>path authenticity</li>
<li>explicit path control (e.g. geofencing)</li>
<li>more consistent latency characteristics</li>
<li>deterministic failover</li>
</ul>
<p dir="auto">are required and best-effort BGP routing is an unacceptable risk.</p>
<p dir="auto">The national bank of Switzerland realized this and since 2024
Switzerland&#39;s banking infrastructure now runs on the SCION-powered
<a href="https://www.six-group.com/en/products-services/banking-services/ssfn.html" rel="nofollow">SSFN</a>,
which relies on the commercial implementation from
<a href="https://www.anapaya.net/" rel="nofollow">Anapaya Systems AG</a> that currently provides up to 100 Gbit/s
border router solutions. The free open source implementation
<a href="https://github.com/scionproto/scion">github.com/scionproto/scion</a> received numerous
data plane performance improvements over the past years but is still lagging behind.</p>
<p dir="auto">If we want to do video calls (and similar high-bandwidth use cases) over
SCION OSS en masse - the data plane performance needs to improve.
Thanks to funding by the <a href="https://nlnet.nl/" rel="nofollow">NLnet Foundation</a> we&#39;ve been working on a new
faster <a href="https://docs.kernel.org/networking/af_xdp.html" rel="nofollow">AF_XDP</a> border router underlay.</p>
<p dir="auto">If you want to learn more about SCION, check out <a href="https://www.scion.org" rel="nofollow">scion.org</a>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Border Router Performance</h3><a id="user-content-border-router-performance" aria-label="Permalink: Border Router Performance" href="#border-router-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">As of the time of writing, the SCION OSS border router performance reached a ceiling of
around 400k-500k packets per second, which is roughly equivalent to 5-6 Gbit/s
at a 1500-byte <a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit" rel="nofollow">MTU</a>.</p>
<p dir="auto">5 Gbit/s per stream* of data is too little, in fact, way too little.
By contrast, today&#39;s Internet carries traffic on the order of
<strong>hundreds of terabits per second</strong> across BGP border routers worldwide.
On the higher end, take the Juniper &#34;MX10008 Universal Routing Platforms&#34; with up to
<strong>76.8 Tbps</strong> (yes, terabits with twelve zeroes) of total bandwidth capacity for example</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/juniper_mx1000x.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/juniper_mx1000x.webp" alt="Juniper MX1000x routing platform specs"/></a></p>
<p dir="auto">These kinds of systems support per-port line rates of 400-500 Gbit/s, depending on the
interface configuration. Individual packet streams carried over such ports can therefore
be forwarded at or near full line rate without being bottlenecked by software overhead,
while massive parallelization across ports enables aggregate bandwidths in the
terabits per second.</p>
<p dir="auto">Such routers usually sit at
<a href="https://en.wikipedia.org/wiki/Internet_exchange_point" rel="nofollow">Internet exchange points</a>
and interconnect
<a href="https://en.wikipedia.org/wiki/Internet_service_provider" rel="nofollow">Internet service providers</a>.
They can easily handle from tens of millions to even billions of packets per second.</p>
<p dir="auto">Even my local ISP now provides 25 Gbit/s
<a href="https://en.wikipedia.org/wiki/Fiber_to_the_x" rel="nofollow">FTTH</a> connections at my small town
near Zurich city.</p>
<p dir="auto">SCION OSS needs to do better; a lot better.</p>
<p dir="auto"><em>* A &#34;stream&#34; is defined here as a flow of packets from a specific source address to
a specific destination address that cannot be parallelized across multiple threads as
parallelization would cause unacceptable levels of packet reordering.</em></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">The Linux Networking Stack</h3><a id="user-content-the-linux-networking-stack" aria-label="Permalink: The Linux Networking Stack" href="#the-linux-networking-stack"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The SCION OSS border router is a Linux user-space program.
It relies solely on the Linux networking stack.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/linux_networking_stack.svg"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/linux_networking_stack.svg" alt="Schematic for the packet flow paths through Linux networking and *tables by Jan Engelhardt"/></a></p>
<p dir="auto"><em>Schematic for the packet flow paths through Linux networking and tables by Jan Engelhardt</em></p>
<p dir="auto">You can think of a packet traversing the networking stack as a person traveling by airplane:</p>
<ul dir="auto">
<li>you enter the airport (Network Interface Controller receiving queue)</li>
<li>you check in and drop off your baggage (buffer allocations)</li>
<li>you pass security screening (packet filtering)</li>
<li>you clear passport control (routing and policy checks)</li>
<li>you wait at the gate (queueing and scheduling)</li>
<li>you board the aircraft (copy to user space)</li>
<li>you find your seat (user-space buffer) and you&#39;re finally ready for takeoff!</li>
</ul>
<p dir="auto">And when transiting - like a packet passing through a router - the same sequence of steps
occurs again in reverse on the transmit path.</p>
<p dir="auto">As you can probably tell, there&#39;s a lot of work between a packet entering one NIC and
exiting on another.</p>
<p dir="auto">We could improve the throughput by adding more router threads, but this would introduce
substantial packet reordering, with packets leaving the system in a different and largely
unpredictable order. High frequency of reordering degrades performance and increases
system resource usage, making this approach non-viable.</p>
<p dir="auto">The only real way to further improve the OSS data plane performance is to bypass the Linux
kernel&#39;s networking stack and fly by private jet:</p>
<ul dir="auto">
<li>you enter the airport (Network Interface Controller)</li>
<li>you&#39;re escorted directly to your private jet (zero-copy fast path to XSK)</li>
<li>you board the plane and you&#39;re done (user-space frame buffer)</li>
</ul>
<p dir="auto">There are several ways in which you can bypass the kernel, namely:</p>
<ul dir="auto">
<li><a href="https://www.dpdk.org/" rel="nofollow">DPDK</a>: a user-space networking framework that bypasses the
Linux networking stack entirely, typically requiring exclusive control over the NIC and
removing it from the kernel&#39;s standard networking stack.</li>
<li><a href="https://docs.kernel.org/networking/af_xdp.html" rel="nofollow">AF_XDP</a>: a Linux kernel mechanism that
allows high-performance packet I/O via a shared memory ring between the NIC driver and
user space.</li>
<li><a href="https://s3-docs.fd.io/vpp/26.02/" rel="nofollow">VPP</a>: a high-performance user-space packet processing
framework that uses vectorized, batch-based processing and is commonly backed by DPDK
or AF_XDP for packet I/O.</li>
</ul>
<p dir="auto">Even though DPDK is the current de facto industry standard for kernel bypass and can
likely achieve higher peak performance, we opted for Linux&#39;s native AF_XDP.
Given that our open-source border router is written in <a href="https://go.dev" rel="nofollow">Go</a>,
and that usability, maintainability, and operational simplicity are among our highest
priorities, AF_XDP provides a better set of trade-offs.</p>

<p dir="auto">AF_XDP is based on
<a href="https://en.wikipedia.org/wiki/Express_Data_Path" rel="nofollow">Express Data Path (XDP)</a>,
which in turn is based on
<a href="https://en.wikipedia.org/wiki/EBPF" rel="nofollow">Extended Berkeley Packet Filter (eBPF)</a>.</p>
<p dir="auto">In a nutshell, you write a small program in restricted C, subject to strict limits on
instruction count (up to ~1 million), loops, and memory access, which is compiled into
eBPF bytecode, verified by the kernel&#39;s eBPF verifier and loaded into the kernel
at runtime, and executed for <strong>every</strong> incoming packet to decide how it is handled.</p>
<p dir="auto">In the SCION border router, we therefore need to:</p>
<ol dir="auto">
<li>mmap a sufficiently large region of memory (ideally using hugepages) referred to as
<a href="https://docs.kernel.org/networking/af_xdp.html#umem" rel="nofollow">UMEM</a>.</li>
<li>initialize the fill, completion, tx and rx rings.</li>
<li>bind an AF_XDP socket to a NIC queue (ideally, in <code>XDP_ZEROCOPY</code> mode,
if the hardware and driver support it).</li>
<li>load a small eBPF/XDP program into the kernel that redirects packets
into frames in the mmapped user-space memory.</li>
</ol>
<p dir="auto">This allows raw packet frames to be delivered directly into the border router
software with minimal overhead for further processing
completely bypassing the network stack.</p>
<p dir="auto">So far so good - but there are problems:</p>
<ul dir="auto">
<li>Typical VM offerings don&#39;t expose the access needed for XDP/AF_XDP
(especially in zero-copy mode). In practice, you usually need bare metal.
You cannot open an AF_XDP socket to send raw packets on, for example,
an AWS EC2 instance.</li>
<li>None of the Linux machines currently at our disposal are equipped with NICs
that support AF_XDP in <code>XDP_ZEROCOPY</code> mode.</li>
</ul>
<p dir="auto">As a result, the only way to properly test and develop a zero-copy-capable AF_XDP
underlay is to obtain hardware that supports it, which is not commonly available in
off-the-shelf consumer-grade systems.</p>

<p dir="auto">Our new goal was to achieve 25 Gbit/s on a single thread of the border router in our
benchmark topology on a relatively small budget, both in terms of time and money.
The only place we could deploy the machine is our not-so-noisy office.
This makes a low noise profile a hard requirement, further narrowing our options.</p>
<p dir="auto">I identified 3 main options:</p>
<ul dir="auto">
<li>Find a suitable used rack server and build
<a href="https://wiki.eth0.nl/index.php/LackRack" rel="nofollow">The LackRack</a>
<ul dir="auto">
<li>relatively cheap üëç</li>
<li>abundantly available üëç</li>
<li>often <strong>very</strong> noisy üëé</li>
</ul>
</li>
<li>Find a suitable used tower server with a lower noise profile
<ul dir="auto">
<li>cheap üëç</li>
<li>typically limited in NIC options and available PCIe lanes üëé</li>
</ul>
</li>
<li>Build a new system from scratch
<ul dir="auto">
<li>exactly the configuration we need üëç</li>
<li>likely expensive üëé</li>
<li>likely requires significant effort and manhours üëé</li>
</ul>
</li>
</ul>
<p dir="auto">Finding a suitable used machine under $5,000 that would also be quiet enough to operate
inside our office proved challenging. After many hours spent browsing various
marketplaces, it became clear that building the system ourselves was the more
practical option at the time.</p>
<p dir="auto">I therefore started by evaluating suitable NICs. After some research, the
following candidates emerged:</p>
<ul dir="auto">
<li><strong>Intel Ethernet 800 Series</strong> (E810 family) (<code>ice</code> driver)</li>
<li><strong>NVIDIA/Mellanox ConnectX-5,6,7</strong> (<code>mlx5</code> driver)</li>
<li><strong>Broadcom NetXtreme-E</strong> (BCM57xxx / BCM588xx series) (<code>bnxt_en</code> driver)</li>
<li><strong>FastLinQ</strong> (QL41xxx / QL45xxx series) (<code>qede</code> driver)</li>
</ul>
<p dir="auto">The NICs would later determine the requirements for the rest of the system.</p>
<p dir="auto">I was quite lucky to have a friend who works on networking at a large US tech company help
me plan and build this system. He asked not to be named, so we&#39;ll refer to him as <em>Frank</em>
throughout the article.</p>

<p dir="auto">Eventually, I found the Mellanox NVIDIA BlueField-2 DPUs
(Data Processing Units, a.k.a. &#34;smart NICs&#34;) with dual 25 Gbit/s ports to be a good deal
on <a href="https://www.piospartslap.de/" rel="nofollow">piospartslap.de</a> and ordered 3 cards for just 289,92‚Ç¨
plus 49,99‚Ç¨ for the express delivery (a total of CHF 318.09).
After reaching out, they kindly agreed to a discounted price of
115‚Ç¨ per card (excluding VAT).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/mellanox_nics_order.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/mellanox_nics_order.webp" alt="Mellanox NVidia BlueField-2 NICs order"/></a></p>
<p dir="auto">Now it was time to plan out the rest of the system.</p>

<p dir="auto">Before choosing the mainboard, it was necessary to decide between going
team Red üî¥ (AMD) and going team Blue üîµ (Intel).</p>
<p dir="auto">I chose to build an Intel-based system because Intel still tends to have a slight edge
in the networking space, particularly due to platform features such as
<a href="https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology.html" rel="nofollow">Intel Data Direct I/O (DDIO)</a>,
which allows NICs to DMA packet data directly into the CPU&#39;s L3 cache instead of main memory.</p>
<p dir="auto">In our benchmark topology we ideally need 5-6 NICs. Three BF-2 cards, each with 2 ports,
consume one PCIe slot and 8 PCIe Gen4 lanes (16 GT/s), which means the system needs a
proper workstation-grade mainboard that can not only accommodate this configuration but
also leave room for future expansion.
Once we reach 25 Gbit/s, it would be desirable to have the option to upgrade the NICs and
move toward the 100 Gbit/s range without having to replace the entire platform.</p>
<p dir="auto">There aren&#39;t that many mainboards that fit these requirements, and these two stood out:</p>
<ul dir="auto">
<li><a href="https://www.gigabyte.com/Enterprise/Server-Motherboard/MS03-CE0-rev-1x-3x" rel="nofollow">Gigabyte MS03-CE0</a></li>
<li><a href="https://www.asus.com/motherboards-components/motherboards/workstation/pro-ws-w790e-sage-se/helpdesk_manual?model2Name=Pro-WS-W790E-SAGE-SE" rel="nofollow">ASUS Pro WS W790E-SAGE SE</a></li>
</ul>
<p dir="auto">Both are within budget, both offer remote management capabilities, and both provide seven
full 16√ó PCIe Gen5 slots. However, the MS03-CE0 was not readily available and would have
required waiting several weeks for delivery from China. Naturally, I went for the ASUS
SAGE SE.</p>

<p dir="auto">ASUS lists a wide range of supported LGA4677 CPUs for the W790E-SAGE SE:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/asus_sage_se_cpusupport1.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/asus_sage_se_cpusupport1.webp" alt="ASUS mainboard CPU support list page 1"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/asus_sage_se_cpusupport2.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/asus_sage_se_cpusupport2.webp" alt="ASUS mainboard CPU support list page 2"/></a></p>
<p dir="auto"><em>Frank</em> happened to have a Sapphire Rapids Q03J
<a href="https://www.intel.com/content/www/us/en/support/articles/000056190/processors.html" rel="nofollow">engineering/qualification sample CPU</a>
with 60 (!) cores for the LGA4677 socket that he wanted to sell anyway,
and he also had access to an ASUS Pro WS W790E-SAGE SE to test it on.</p>
<p dir="auto">Unfortunately, the Q03J and SAGE SE combination didn&#39;t work. The system didn&#39;t boot,
so that option was off the table.
I couldn&#39;t find any decent second-hand offers either, which left me with no real choice
but to buy a brand-new CPU.</p>
<p dir="auto">I would have preferred to use an Intel Xeon W-3400 workstation CPU, but prices on
<a href="https://galaxus.ch" rel="nofollow">galaxus.ch</a> and <a href="https://digitec.ch" rel="nofollow">digitec.ch</a>
started at around CHF ~1600, which sadly exceeded our budget.
So I had to opt for the cheaper 2000 series.
The best CPU I could find available in swiss stocks was the
<a href="https://www.intel.com/content/www/us/en/products/sku/233420/intel-xeon-w52455x-processor-30m-cache-3-20-ghz/specifications.html" rel="nofollow">Intel Xeon W5-2455X</a>,
a 12-core CPU with a 3.2 GHz base clock and 64√ó PCIe Gen5 lanes,
providing enough headroom for potential future expansion at a rather bearable cost of
CHF 1105,-.</p>

<p dir="auto">There were a couple of CPU cooler options and <em>Frank</em> suggested either of:</p>
<ul dir="auto">
<li><a href="https://www.arctic.de/en/Freezer-4U-M/ACFRE00133A" rel="nofollow">Arctic Freezer 4U-M</a></li>
<li><a href="https://www.noctua.at/en/products/nh-u14s-dx-4677" rel="nofollow">Noctua NH-U14S DX-4677</a></li>
</ul>
<p dir="auto">Being a happy Noctua client, I just went with the NH-U14S.</p>
<p dir="auto">There were also liquid cooling options but having had to maintain a liquid cooled CPU
in the past, I didn&#39;t feel it was a good fit for this setup and the Noctua NH-U14S
would probably be silent enough anyway.</p>

<p dir="auto">It quickly became apparent that the ongoing RAM shortage had already affected the market,
making it difficult to find suitable DDR5 ECC memory that was both in stock and
reasonably priced.</p>
<p dir="auto">Fortunately, a
<a href="https://www.corsair.com/ww/de/p/memory/cma64gx5m4b5600c40/ws-ddr5-ecc-rdimm-64gb-4-x-16gb-ddr5-dram-5600mt-s-cl40-memory-kit-cma64gx5m4b5600c40?srsltid=AfmBOoqui_s5tszCYUdL0V9LaVm9yMa2InibYmad2i-q8kKxW6dvNyz_" rel="nofollow">Corsair DDR5 RDIMM kit (64 GB, 4√ó16 GB, 5600 MT/s)</a>
was available on Galaxus, so I picked it right away.</p>

<p dir="auto">The M.2 SSD was the easiest component to decide on. I simply went with a 1TB
<a href="https://semiconductor.samsung.com/consumer-storage/internal-ssd/990-pro-with-heatsink/" rel="nofollow">Samsung 990 Pro with a heatsink</a>
being absolutely sure there won&#39;t be any problems.
It&#39;s widely available, and Digitec even promised next-day delivery.</p>

<p dir="auto">The workstation was supposed to work 24/7 as a server so picking a reliable PSU is quite important.</p>
<p dir="auto">After careful considering, I went with the
<a href="https://www.corsair.com/ww/en/p/psu/cp-9020249-ww/rme-series-rm850e-fully-modular-low-noise-atx-power-supply-cp-9020249-ww" rel="nofollow">Corsair RM850e</a></p>

<p dir="auto">To be honest - I heavily underestimated the task of finding the right SSI-EEB case.
I mean, a case is just a piece of sheet metal, how hard can it be to find the right
piece of metal, right?!</p>
<p dir="auto">First, consider the BlueField-2 NIC shown below.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/bf2_card_in_origbox.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/bf2_card_in_origbox.webp" alt="A BlueField-2 network card in original packaging"/></a></p>
<p dir="auto">These cards do not have active cooling. They are equipped only with a small, thin
heatsink and are designed to be installed in server racks with <strong>very noisy</strong> high-RPM,
high-pressure airflow. As a result, they run extremely hot under normal operation.</p>
<p dir="auto">Server-grade hardware like this is typically expected to operate continuously at high
loads, often without dynamic power management. This becomes problematic in a workstation
tower office setup, as it requires a case that allows fans to be mounted in very close
proximity to the cards, to provide sufficient airflow and static pressure to keep them
within safe and sustainable operating temperatures, at least at around 50-60¬∞C
(122-140¬∞F).</p>
<p dir="auto">I did find the
<a href="http://silverstonetek.com/en/product/info/server-nas/rm52/" rel="nofollow">Silverstone RM52</a>
to be an option worth considering:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/silverstone_rm52.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/silverstone_rm52.webp" alt="The Silverstone RM52 system case with markings for potential fan mounting"/></a></p>
<p dir="auto">Though at a price of CHF ~450 it was, to put it mildly, too expensive.</p>
<hr/>
<p dir="auto">After nearly losing my sanity browsing both new and second-hand listings,
I finally came across the
<a href="https://phanteks.com/product/enthoo-pro-2-server-edition-tg/" rel="nofollow">Phanteks Enthoo Pro II</a>
tower case with a very convenient fan mount:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/enthoo_pro2_niccooling_concept.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/enthoo_pro2_niccooling_concept.webp" alt="Enthoo Pro II NIC cooling concept sketch"/></a></p>
<p dir="auto">Finally, everything except the case coolers was decided upon and ordered.</p>

<p dir="auto">The shopping list was finally complete:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/order1.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/order1.webp" alt="Order of RAM, CPU, PSU and Mainboard"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/order2.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/order2.webp" alt="Order of RAM, CPU, PSU and Mainboard"/></a></p>
<p dir="auto">That, plus CHF ~170,- for the Phanteks case, makes a total of CHF 3210,-.
It could probably have been cheaper, but I really wanted to get most of the work done
before heading off on my three-week vacation.</p>
<p dir="auto">However, at the time of writing, just months later, the price of the RAM kit went up by 232%!
Memory supply seems to have gotten a lot worse.</p>
<p dir="auto">The first components to arrive were the BlueField-2 NICs, followed shortly by the ASUS mainboard, as expected. A bit later, the CPU cooler and the SSD arrived as well.</p>
<hr/>
<p dir="auto">After a significant delay, the CPU, PSU, and RAM finally showed up too. The RAM delay was unsurprising.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/ram_order_delay.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/ram_order_delay.webp" alt="RAM order delay message"/></a></p>
<hr/>
<p dir="auto">The delivery of the Phanteks case was delayed, and no updated or approximate delivery
date was available at the time, so I had to start assembling the system without the case.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/phanteks_case_delivery_date_being_clarified.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/phanteks_case_delivery_date_being_clarified.webp" alt="Phanteks case delivery delay"/></a></p>
<hr/>
<p dir="auto">Last but not least, I needed three
<a href="https://en.wikipedia.org/wiki/Small_Form-factor_Pluggable" rel="nofollow">SFP28 DAC</a> cables to interconnect the NICs between each other.
I was honestly surprised by how expensive these cables are. Domestic Swiss offers ranged from CHF 50,- per cable all the way up to CHF 150,-! Of course, there&#39;s always the option to order them from China for a fraction of the price, but shipping typically takes 2-4 weeks.</p>
<p dir="auto">Luckily, I managed to find a few at a much lower price on <a href="https://ricardo.ch" rel="nofollow">Ricardo</a>!</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/ricardo_sfp_cables.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/ricardo_sfp_cables.webp" alt="25G SFP28 passive DAC cables on Ricardo.ch"/></a></p>

<div dir="auto"><h3 tabindex="-1" dir="auto">NIC Firmware Upgrade: Installation</h3><a id="user-content-nic-firmware-upgrade-installation" aria-label="Permalink: NIC Firmware Upgrade: Installation" href="#nic-firmware-upgrade-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Since the cards arrived early, the first thing I had to do was upgrade their firmware.
These cards often ship with outdated firmware, so updating it is essential, otherwise
you may face all sorts of problems and lower performance.</p>
<p dir="auto">However, since the new workstation system wasn&#39;t ready yet, I decided to try installing
them on my ancient gaming rig back from around the 2010s.
The <a href="https://www.msi.com/Motherboard/Big-Bang-XPower-II/Specification" rel="nofollow">BigBang X-Power II</a>
PCIe Gen3 should have been enough to run the BF-2 NICs.</p>
<p dir="auto">Sadly, when I powered it on, it zapped ‚ö°‚ö°‚ö° and the unmistakable smell of electronic death
quickly filled the room. After more than 11 years of service, my old machine&#39;s VRM had
finally given up.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/old_pc_vrm_dead.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/old_pc_vrm_dead.webp" alt="Old machine VRM dead"/></a></p>
<p dir="auto">Rest in peace, comrade ü™¶.</p>
<p dir="auto">Luckily, I have a new gaming PC with Linux on it. I just didn&#39;t want to disassemble it,
but now I had no choice. The fans and the fat RTX 3090 with the bracket that prevents
it from bending the PCIe slot under its immense weight needed to be removed and make
space for the tiny BlueField-2.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/new_gaming_pc_made_room.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/new_gaming_pc_made_room.webp" alt="My new gaming PC made room for the BF-2 NIC"/></a></p>
<p dir="auto">I also installed the new SSD and installed an Ubuntu system on it.</p>
<hr/>
<p dir="auto">When I first booted the gaming PC with the BF-2 NIC installed,
I was a bit scared when I saw UEFI report:</p>
<div data-snippet-clipboard-copy-content="mlx5_core - over 120000 MS in pre-initializing state, aborting
mlx5_core: mlx5_init_one failed with error code -110"><pre><code>mlx5_core - over 120000 MS in pre-initializing state, aborting
mlx5_core: mlx5_init_one failed with error code -110
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/uefi_error.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/uefi_error.webp" alt="mlx5_core - over 120000 MS in pre-initializing state, aborting"/></a></p>
<p dir="auto">Apparently, this is normal, especially for the very first cold boot.
These BlueField-2 cards are not your regular network card. They&#39;re
<a href="https://en.wikipedia.org/wiki/Data_processing_unit" rel="nofollow">DPUs</a>, which is essentially
an entire PCIe based computer with its own Linux subsystem running on their own
8-core ARM Cortex-A72 CPUs with 16 GB of DDR4 ECC RAM.
No wonder these cards get so hot, they&#39;re insane!
On the first boot, this system has to boot and initialize itself,
which apparently makes my UEFI believe that it&#39;s dead.
After letting it sit for a while and rebooting again - everything was fine
and I managed to boot into Ubuntu and proceed with the firmware upgrade.</p>
<p dir="auto">I also checked temperatures, just to be on the safe side, and they appeared to be fine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="~$ sensors
...
mlx5-pci-0601
Adapter: PCI adapter
asic:         +57.0¬∞C  (crit = +105.0¬∞C, highest = +58.0¬∞C)
..."><pre><span>~</span>$ sensors
...
mlx5-pci-0601
Adapter: PCI adapter
asic:         +57.0¬∞C  (crit = +105.0¬∞C, highest = +58.0¬∞C)
...</pre></div>
<p dir="auto">Previously, I removed one of the fans from my old, dead computer and placed it right next to the NIC&#39;s heatsink to get some airflow through it. That seems to have been sufficient.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">NIC Firmware Upgrade: NVIDIA DOCA</h3><a id="user-content-nic-firmware-upgrade-nvidia-doca" aria-label="Permalink: NIC Firmware Upgrade: NVIDIA DOCA" href="#nic-firmware-upgrade-nvidia-doca"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Before I could do anything with the cards, I needed to install the
<a href="https://developer.nvidia.com/doca-3-0-0-download-archive?deployment_platform=Host-Server&amp;deployment_package=DOCA-Host&amp;target_os=Linux&amp;Architecture=x86_64&amp;Profile=doca-all&amp;Distribution=Ubuntu&amp;version=24.04&amp;installer_type=deb_online" rel="nofollow">NVIDIA DOCA Host-Server package</a>
on the freshly installed Ubuntu system, which is fairly easy to do by following
the documentation.</p>
<p dir="auto">Then I needed to confirm the card is visible on PCIe, and it was:</p>
<div dir="auto" data-snippet-clipboard-copy-content="lspci | grep Mellanox
06:00.0 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
06:00.1 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
06:00.2 Ethernet controller: Mellanox Technologies Device c2d1 (rev 01)
06:00.3 DMA controller: Mellanox Technologies MT42822 BlueField-2 SoC Management Interface (rev 01)"><pre>lspci <span>|</span> grep Mellanox
06:00.0 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
06:00.1 Ethernet controller: Mellanox Technologies MT42822 BlueField-2 integrated ConnectX-6 Dx network controller (rev 01)
06:00.2 Ethernet controller: Mellanox Technologies Device c2d1 (rev 01)
06:00.3 DMA controller: Mellanox Technologies MT42822 BlueField-2 SoC Management Interface (rev 01)</pre></div>
<p dir="auto">I started the Mellanox software tools driver set:</p>

<p dir="auto">And now, I could query the current firmware:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo mlxfwmanager --online
Querying Mellanox devices firmware ...

Device #1:
----------

  Device Type:      BlueField2
  Part Number:      0JNDCM_Dx
  Description:      NVIDIA Bluefield-2 Dual Port 25 GbE SFP Crypto DPU
  PSID:             DEL0000000033
  PCI Device Name:  /dev/mst/mt41686_pciconf0
  Base GUID:        58a2e1030004a9da
  Base MAC:         58a2e104a9da
  Versions:         Current        Available     
     FW             24.36.7506     N/A           
     PXE            3.7.0200       N/A           
     UEFI           14.31.0010     N/A           
     UEFI Virtio blk   22.4.0010      N/A           
     UEFI Virtio net   21.4.0010      N/A           

  Status:           No matching image found
"><pre>sudo mlxfwmanager --online
Querying Mellanox devices firmware ...

Device <span><span>#</span>1:</span>
----------

  Device Type:      BlueField2
  Part Number:      0JNDCM_Dx
  Description:      NVIDIA Bluefield-2 Dual Port 25 GbE SFP Crypto DPU
  PSID:             DEL0000000033
  PCI Device Name:  /dev/mst/mt41686_pciconf0
  Base GUID:        58a2e1030004a9da
  Base MAC:         58a2e104a9da
  Versions:         Current        Available     
     FW             24.36.7506     N/A           
     PXE            3.7.0200       N/A           
     UEFI           14.31.0010     N/A           
     UEFI Virtio blk   22.4.0010      N/A           
     UEFI Virtio net   21.4.0010      N/A           

  Status:           No matching image found
</pre></div>
<p dir="auto">The installed firmware <code>24.36.7506</code> was clearly not up to date. What I expected to see
there is <code>24.46.3048</code>.</p>
<p dir="auto">So I proceeded to <a href="https://developer.nvidia.com/doca-3-0-0-download-archive?deployment_platform=BlueField&amp;deployment_package=BF-FW-Bundle&amp;installer_type=BFB" rel="nofollow">bfb-install</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo bfb-install --bfb ~/bf-fwbundle-3.1.0-82_25.07-prod.bfb --rshim rshim0"><pre>sudo bfb-install --bfb <span>~</span>/bf-fwbundle-3.1.0-82_25.07-prod.bfb --rshim rshim0</pre></div>
<p dir="auto">After a while, the upgrade completed successfully, and I had to power down the system
completely for a few seconds so that the new firmware would be used after the reboot:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th>old version</th>
<th>new version</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>FW</code></td>
<td><code>24.36.7506</code></td>
<td><code>24.46.3048</code></td>
</tr>
<tr>
<td><code>PXE</code></td>
<td><code>3.7.0200</code></td>
<td><code>3.8.0100</code></td>
</tr>
<tr>
<td><code>UEFI</code></td>
<td><code>14.31.0010</code></td>
<td><code>14.39.0014</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">I repeated this process for all three cards. It took a while, but it was simpler than I
had expected.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Switching From DPU to NIC Mode</h2><a id="user-content-switching-from-dpu-to-nic-mode" aria-label="Permalink: Switching From DPU to NIC Mode" href="#switching-from-dpu-to-nic-mode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">As I mentioned earlier, the BlueField-2s are DPUs, not simple NICs. But they can very
well function just like simple 25 Gbit/s NICs if you want them to.
And <a href="https://docs.nvidia.com/doca/archive/2-9-0-cx8/bluefield+modes+of+operation/index.html?utm_source=chatgpt.com#src-3453016816_id-.BlueFieldModesofOperationv2.9.1-NICModeforBlueField-2" rel="nofollow">this part of the DOCA documentation</a>
explains how, so I won&#39;t repeat the steps here.</p>
<p dir="auto">This switch is necessary because, in DPU mode, packet processing is routed through the
onboard ARM cores and the embedded switch, which adds unnecessary complexity and latency
for our use case. NIC mode effectively turns the BlueField-2 into a conventional
high-performance NIC and removes the DPU from the data path.</p>

<p dir="auto">Finally, it was time to start assembling the new SCION workstation. To be honest,
I got a bit nervous. The last time I assembled a system was years ago. I had help from
my younger brother and it wasn&#39;t nearly as expensive as this shiny new piece of
workstation hardware.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/mainboard_ready_for_assembly.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/mainboard_ready_for_assembly.webp" alt="The ASUS Mainboard laying on its original cardboard packaging"/></a></p>
<p dir="auto">Before touching any component, I plugged the PSU into the wall and grounded myself on
it to prevent static electricity from damaging the delicate circuitry. This I regularly
repeated throughout the whole process just to be on the safe side.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/grounding_through_psu.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/grounding_through_psu.webp" alt="grounding myself by touching the PSU that&#39;s plugged into the wall"/></a></p>
<p dir="auto">For the first time in my life, I held a server-grade CPU in my hand - and I have to say, it&#39;s massive! Easily two to three times the size of a typical consumer CPU.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/cpu_top.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/cpu_top.webp" width="45%"/></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/cpu_bottom.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/cpu_bottom.webp" width="45%"/></a>
</p>
<p dir="auto">The installation process does differ from your regular consumer CPU, and it&#39;s
explained in full detail <a href="https://www.asus.com/me-en/support/faq/1050029/" rel="nofollow">here</a>.</p>
<p dir="auto">I installed the CPU + cooler, RAM, M.2 SSD, and connected the power wiring:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/cpu_installed.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/cpu_installed.webp" alt="CPU+cooler, RAM and SSD installed on the mainboard"/></a></p>
<p dir="auto">By the way, the RAM was installed incorrectly, as I later found out and fixed
according to the manual:</p>
<div data-snippet-clipboard-copy-content="Intel¬Æ Xeon¬Æ W-2400 Series Processors do not support
DIMM_C1, DIMM_D1, DIMM_G1, and DIMM_H1"><pre><code>Intel¬Æ Xeon¬Æ W-2400 Series Processors do not support
DIMM_C1, DIMM_D1, DIMM_G1, and DIMM_H1
</code></pre></div>
<hr/>
<p dir="auto">I clicked the power button, and... it <em>almost</em> booted. But something was wrong.
I couldn&#39;t SSH into the Ubuntu system I had installed on this M.2 SSD.</p>
<p dir="auto"><em>Frank</em> suggested reseating the CPU, as uneven screw pressure can cause poor contact in
the LGA4677 socket. In that case, the system may fail to boot or lose access to some PCIe
lanes if the pins don&#39;t line up perfectly. The Q-code LED displayed <code>64</code>, which, according
to the manual, means <code>CPU DXE initialization is started</code>. At that point, I assumed
something had to be wrong with either the CPU or RAM. I reseated both multiple times, wasting several hours in the process, but without any success.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/q_code_table.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/q_code_table.webp" alt="Q-code table from the manual"/></a></p>
<p dir="auto">I even installed the old GTX TITAN GPU, which I previously confirmed to have
worked on another system:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/gtx_titan_on_sage_se.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/gtx_titan_on_sage_se.webp" alt="GTX TITAN GPU installed on the ASUS mainboard"/></a></p>
<p dir="auto">and even though its LEDs lit up I was left with a frustrating &#34;no signal&#34; message on
the HDMI connected display. At this point, I started suspecting the worst.
As you might have guessed - I didn&#39;t get a good night&#39;s sleep.</p>
<p dir="auto">The next day, however, I had to confront the sheer size of my own stupidity. If you look
again at the photo of the installed GPU above, you&#39;ll notice that - for reasons unknown even to myself - I had installed it in PCIe slot 2, which is not supported by the Intel Xeon W5-2455X.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/sage_se_schematic.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/sage_se_schematic.webp" alt="ASUS mainboard schematic stating that PCIe slots 2, 4 and 6 are disabled for W-2400"/></a></p>
<p dir="auto">And it turns out, Q-code <code>64</code> was just the <em>last code</em> displayed and didn&#39;t actually indicate any fault at all. In fact, everything was okay.</p>
<p dir="auto">So I moved the graphics card to PCIe slot 1 and was finally able to enter the UEFI menu, only to run into the next surprise: the M.2 SSD wasn&#39;t recognized.</p>
<p dir="auto">Some time later, I once again realized that I needed to pay closer attention
to the manual:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/disabled_m2slots.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/disabled_m2slots.webp" alt="Manual stating that M.2_1 and M.2_2 slots are disabled on W-2400 series"/></a></p>
<div data-snippet-clipboard-copy-content="M.2_1 and M.2_2 slots will be disabled once an Intel¬Æ Xeon¬Æ W-2400 Series
Processor is installed."><pre><code>M.2_1 and M.2_2 slots will be disabled once an Intel¬Æ Xeon¬Æ W-2400 Series
Processor is installed.
</code></pre></div>
<p dir="auto">After moving the M.2 SSD to slot 3, everything worked just fine.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">BMC - The Remote Management System</h2><a id="user-content-bmc---the-remote-management-system" aria-label="Permalink: BMC - The Remote Management System" href="#bmc---the-remote-management-system"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">As a proper server-/workstation-grade mainboard, this ASUS SAGE SE has the
<a href="https://www.aspeedtech.com/server_ast2600/" rel="nofollow">ASPEED AST2600</a> remote management system
on it. It allows you to access its web UI over a dedicated ethernet port,
independent of the actual host system and control almost everything remotely,
even if the host system is down or corrupted.
It even gives you screen-sharing remote control.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/aspeed_kvm.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/aspeed_kvm.webp" alt="ASPEED AST2600 KVM"/></a></p>

<p dir="auto">The first time I entered the BMC UI, it asked me to change the admin password,
which I did.</p>
<p dir="auto">Later, when I tried to sign in again, it refused to let me in, claiming I had entered
incorrect credentials - which was simply not true. Long story short: the AST2600 firmware
has a bug. It allows you to set a password that exceeds the maximum supported length
without showing any error. Internally, it appears to hash the overlong string as-is, so
even trimming the password to the maximum length during login doesn&#39;t work.
The result is a self-inflicted lockout.</p>
<p dir="auto">I found it out by using <a href="https://linux.die.net/man/1/ipmitool" rel="nofollow">ipmitool</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="~ % ipmitool -I lanplus -H 192.168.1.152 -U admin -P $PASS
chassis status lanplus: password is longer than 20 bytes."><pre><span>~</span> % ipmitool -I lanplus -H 192.168.1.152 -U admin -P <span>$PASS</span>
chassis status lanplus: password is longer than 20 bytes.</pre></div>
<p dir="auto">The only way to recover from this is to log into the host system and reset the BMC password using <code>ipmitool</code>.</p>

<p dir="auto">Now that the Phanteks Enthoo Pro II case had finally arrived, it was time to move the system into its new home.</p>
<p dir="auto">During installation, I accidentally left a standoff in the wrong place.
The motherboard&#39;s backplate drew first blood - and I almost cried.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/case_standoff.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/case_standoff.webp" width="45%"/></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/mainboard_backplate_scratch.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/mainboard_backplate_scratch.webp" width="45%"/></a>
</p>

<p dir="auto">The system was finally mounted in the chassis, and the Enthoo Pro II&#39;s brackets allowed
me to mount the NIC fan quite elegantly, even if it couldn&#39;t be placed perfectly close.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/system_complete_but_without_fans.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/system_complete_but_without_fans.webp" alt="System fully assembled but without the case fans"/></a></p>
<p dir="auto">But one last piece of the puzzle was still missing: air circulation.
So I came up with a concept:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/airflow_concept.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/airflow_concept.webp" alt="Airflow and fan placement concept"/></a></p>
<p dir="auto">and a new shopping list:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/noctua_fans_shopping_list.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/noctua_fans_shopping_list.webp" alt="List of Noctua fans to buy"/></a></p>
<p dir="auto">I decided to go with the more expensive Noctua fans again to keep the noise profile as
low as possible. The NF-A12x25 G2 PWM fans are optimized for static pressure, and one of
them would be responsible for providing sufficient airflow through the NIC heatsinks.</p>
<p dir="auto">A short while later, they arrived:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/fans_arrived.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/fans_arrived.webp" alt="Fans arrived"/></a></p>
<p dir="auto">And I had <em>lots</em> of fun installing them. Just kidding - it was tedious work,
and my back hurt so much afterward that not even a good hot bath helped.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/installing_fans.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/installing_fans.webp" alt="The mess of installing the fans"/></a></p>
<p dir="auto">Once again, I grossly underestimated the effort required for cable management,
planning, and execution. I even had to reroute and rewire parts of the setup after
realizing that one connector sat far too close to a very hot heatsink -
clearly asking for trouble.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/too_close_for_comfort.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/too_close_for_comfort.webp" alt="A connector sitting too close to a hot heatsink"/></a></p>
<p dir="auto">But in the end, I was really happy with how it turned out:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/complete_system.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/complete_system.webp" alt="System completely assembled"/></a></p>
<p dir="auto">Cable management was decent. Temperatures were great and sustainable,
and even at 30-50% fan speed the system was barely audible, even in complete silence.</p>
<div dir="auto" data-snippet-clipboard-copy-content="~$ sensors
coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +23.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 0:        +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 1:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 2:        +20.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 3:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 4:        +23.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 5:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 6:        +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 7:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 8:        +20.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 9:        +20.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 10:       +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 11:       +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)

mlx5-pci-bc01
Adapter: PCI adapter
asic:         +62.0¬∞C  (crit = +105.0¬∞C, highest = +74.0¬∞C)

mlx5-pci-8501
Adapter: PCI adapter
asic:         +57.0¬∞C  (crit = +105.0¬∞C, highest = +66.0¬∞C)

mlx5-pci-4e01
Adapter: PCI adapter
asic:         +59.0¬∞C  (crit = +105.0¬∞C, highest = +70.0¬∞C)

nvme-pci-0100
Adapter: PCI adapter
Composite:    +27.9¬∞C  (low  = -273.1¬∞C, high = +81.8¬∞C)
                       (crit = +84.8¬∞C)
Sensor 1:     +27.9¬∞C  (low  = -273.1¬∞C, high = +65261.8¬∞C)
Sensor 2:     +31.9¬∞C  (low  = -273.1¬∞C, high = +65261.8¬∞C)

mlx5-pci-bc00
Adapter: PCI adapter
asic:         +62.0¬∞C  (crit = +105.0¬∞C, highest = +74.0¬∞C)

mlx5-pci-8500
Adapter: PCI adapter
asic:         +57.0¬∞C  (crit = +105.0¬∞C, highest = +66.0¬∞C)

mlx5-pci-4e00
Adapter: PCI adapter
asic:         +59.0¬∞C  (crit = +105.0¬∞C, highest = +70.0¬∞C)"><pre><span>~</span>$ sensors
coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +23.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 0:        +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 1:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 2:        +20.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 3:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 4:        +23.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 5:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 6:        +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 7:        +22.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 8:        +20.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 9:        +20.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 10:       +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)
Core 11:       +21.0¬∞C  (high = +92.0¬∞C, crit = +100.0¬∞C)

mlx5-pci-bc01
Adapter: PCI adapter
asic:         +62.0¬∞C  (crit = +105.0¬∞C, highest = +74.0¬∞C)

mlx5-pci-8501
Adapter: PCI adapter
asic:         +57.0¬∞C  (crit = +105.0¬∞C, highest = +66.0¬∞C)

mlx5-pci-4e01
Adapter: PCI adapter
asic:         +59.0¬∞C  (crit = +105.0¬∞C, highest = +70.0¬∞C)

nvme-pci-0100
Adapter: PCI adapter
Composite:    +27.9¬∞C  (low  = -273.1¬∞C, high = +81.8¬∞C)
                       (crit = +84.8¬∞C)
Sensor 1:     +27.9¬∞C  (low  = -273.1¬∞C, high = +65261.8¬∞C)
Sensor 2:     +31.9¬∞C  (low  = -273.1¬∞C, high = +65261.8¬∞C)

mlx5-pci-bc00
Adapter: PCI adapter
asic:         +62.0¬∞C  (crit = +105.0¬∞C, highest = +74.0¬∞C)

mlx5-pci-8500
Adapter: PCI adapter
asic:         +57.0¬∞C  (crit = +105.0¬∞C, highest = +66.0¬∞C)

mlx5-pci-4e00
Adapter: PCI adapter
asic:         +59.0¬∞C  (crit = +105.0¬∞C, highest = +70.0¬∞C)</pre></div>

<p dir="auto">One more thing left to do was to configure this workstation for upcoming work.
I needed to figure out which NIC is which in Linux and interconnect them.</p>
<p dir="auto">The easiest way to do so was to connect a card to itself (port 1 -&gt; port 2),
then check <code>sudo mst status</code> and write down a map of <code>/dev/mst/mt*</code>
to their respective <code>domain:bus:dev.fn</code>.</p>
<p dir="auto">Then check all mst devices <code>sudo mlxlink -d /dev/mst/mt41686_pciconf1 -p 1</code> status.
The one where the &#34;Troubleshooting Info - Recommendation&#34; says &#34;No issue was observed&#34;
is the one currently wired one.</p>
<p dir="auto">Under <strong>/etc/udev/rules.d/10-bf2-names.rules</strong> I placed a new config file
mapping names to their respective MACs (MAC addresses are redacted here):</p>
<div dir="auto" data-snippet-clipboard-copy-content="### TOP CARD ‚Äî PCIEX16(G5)_3 ‚Äî PCI domain:bus:dev.fn=0000:85:00.0
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;top_1&#34;
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;top_2&#34;

### CENTER CARD ‚Äî PCIEX16(G5)_5 ‚Äî PCI domain:bus:dev.fn=0000:4e:00.0
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;center_1&#34;
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;center_2&#34;

### BOTTOM CARD ‚Äî PCIEX16(G5)_7 ‚Äî PCI domain:bus:dev.fn=0000:bc:00.0
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;bottom_1&#34;
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;bottom_2&#34;"><pre><span>##</span><span># TOP CARD ‚Äî PCIEX16(G5)_3 ‚Äî PCI domain:bus:dev.fn=0000:85:00.0</span>
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;top_1&#34;
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;top_2&#34;

<span>##</span><span># CENTER CARD ‚Äî PCIEX16(G5)_5 ‚Äî PCI domain:bus:dev.fn=0000:4e:00.0</span>
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;center_1&#34;
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;center_2&#34;

<span>##</span><span># BOTTOM CARD ‚Äî PCIEX16(G5)_7 ‚Äî PCI domain:bus:dev.fn=0000:bc:00.0</span>
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;bottom_1&#34;
SUBSYSTEM==&#34;net&#34;, ACTION==&#34;add&#34;, ATTR{address}==&#34;00:00:00:00:00:00&#34;, NAME=&#34;bottom_2&#34;</pre></div>
<p dir="auto">After reloading I could now easily identify the individual ports by their name.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo udevadm control --reload
sudo udevadm trigger"><pre>sudo udevadm control --reload
sudo udevadm trigger</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="~$ ip -br link
lo               UNKNOWN        00:00:00:00:00:00 &lt;LOOPBACK,UP,LOWER_UP&gt; 
eno3np0          UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
eno2np1          UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
enx96cd268bdb7b  UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
center_1         UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
center_2         UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
top_1            UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
top_2            UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
bottom_1         UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
bottom_2         UP             00:00:00:00:00:00 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 
docker0          DOWN           00:00:00:00:00:00 &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; "><pre><span>~</span>$ ip -br link
lo               UNKNOWN        00:00:00:00:00:00 <span>&lt;</span>LOOPBACK,UP,LOWER_UP<span>&gt;</span> 
eno3np0          UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
eno2np1          UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
enx96cd268bdb7b  UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
center_1         UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
center_2         UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
top_1            UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
top_2            UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
bottom_1         UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
bottom_2         UP             00:00:00:00:00:00 <span>&lt;</span>BROADCAST,MULTICAST,UP,LOWER_UP<span>&gt;</span> 
docker0          DOWN           00:00:00:00:00:00 <span>&lt;</span>NO-CARRIER,BROADCAST,MULTICAST,UP<span>&gt;</span> </pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/cards_interconnections.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/cards_interconnections.webp" alt="NICs Interconnected via SFP28 cables"/></a></p>

<p dir="auto">The biggest challenge with our office is that it&#39;s located inside a banking
infrastructure company&#39;s building. While they do provide a way for us to connect to
the outside, we can&#39;t directly access anything on the inside. Unless, of course,
we use Tailscale.</p>
<p dir="auto">To access the BMC web interface from outside the office, I configured a small Linux
machine in the office, connected to the same switch as the workstation,
and tunneled the connection through Tailscale.</p>
<p dir="auto">By the way, finding the BMC&#39;s LAN IP and MAC address turned into yet another multi-hour
adventure, as they weren&#39;t written down anywhere - not in the documentation and
not on the board itself. First, I tried to scan the local network, but that approach
proved futile. Later, I connected my MacBook to the management port directly via
ethernet cable and used <code>tcpdump</code> to inspect the traffic and determine the MAC.
Unfortunately, it seems like the ASPEED AST2600 BMC doesn&#39;t speak proper
<a href="https://en.wikipedia.org/wiki/Address_Resolution_Protocol" rel="nofollow">ARP</a>.
There is however a short window of time right after a cold boot of the system
when it sends ICMPv6 packets, which reveal its MAC.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scionassociation/blog-25gbit-workstation/blob/main/img/server_is_running.webp"><img src="https://github.com/scionassociation/blog-25gbit-workstation/raw/main/img/server_is_running.webp" alt="Server is running, do not turn off"/></a></p>

<p dir="auto">I started writing AF_XDP experiments, and you can find the code at:
<a href="https://github.com/romshark/afxdp-bench-go">github.com/romshark/afxdp-bench-go</a></p>
<p dir="auto">Eventually, I managed to send 1.5 TB of data in just over eight minutes from one card to
another at 24.6 Gbit/s (practically line rate), using an MTU of 1500 bytes per packet:</p>
<div dir="auto" data-snippet-clipboard-copy-content="FINAL REPORT
 Elapsed:           487.657 s
 TX:                1,000,000,000 packets
 RX:                1,000,000,000 packets
 TX Avg PPS:        2,050,621
 RX Avg PPS:        2,050,621
 TX Avg rate:       24,607.5 Mbps
 RX Avg rate:       24,607.5 Mbps
 Dropped:           0 (0.0000%)

real    8m10.217s
user    0m0.009s
sys     0m0.017s
Requested TX:       1000000000
Egress:
  tx_packets_phy delta: 1000000126
  tx_bytes_phy   delta: 1504000017713
Ingress:
  rx_packets_phy delta: 1000000126
  rx_bytes_phy   delta: 1504000017713"><pre>FINAL REPORT
 Elapsed:           487.657 s
 TX:                1,000,000,000 packets
 RX:                1,000,000,000 packets
 TX Avg PPS:        2,050,621
 RX Avg PPS:        2,050,621
 TX Avg rate:       24,607.5 Mbps
 RX Avg rate:       24,607.5 Mbps
 Dropped:           0 (0.0000%)

real    8m10.217s
user    0m0.009s
sys     0m0.017s
Requested TX:       1000000000
Egress:
  tx_packets_phy delta: 1000000126
  tx_bytes_phy   delta: 1504000017713
Ingress:
  rx_packets_phy delta: 1000000126
  rx_bytes_phy   delta: 1504000017713</pre></div>

<p dir="auto">In the next post, I want to share implementation details and more benchmark results
of the SCION OSS AF_XDP underlay, so stay tuned!</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Component</th>
<th>Quantity</th>
<th>Retailer</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASUS Pro WS W790E-SAGE SE (LGA4677, Intel W790, SSI EEB)</td>
<td>1</td>
<td>digitec.ch</td>
<td>CHF 962.90</td>
</tr>
<tr>
<td>Intel Xeon W5-2455X (3.2 GHz, 12-core)</td>
<td>1</td>
<td>galaxus.ch</td>
<td>CHF 1106.00</td>
</tr>
<tr>
<td>Corsair DDR5 RDIMM 64 GB (4√ó16 GB, 5600 MT/s, ECC)</td>
<td>1</td>
<td>galaxus.ch</td>
<td>CHF 536.00</td>
</tr>
<tr>
<td>Corsair RM850e (850 W)</td>
<td>1</td>
<td>galaxus.ch</td>
<td>CHF 113.00</td>
</tr>
<tr>
<td>Samsung 990 Pro w/ Heatsink (1 TB, M.2 2280)</td>
<td>1</td>
<td>digitec.ch</td>
<td>CHF 101.00</td>
</tr>
<tr>
<td>Noctua NH-U14S DX-4677 CPU cooler</td>
<td>1</td>
<td>digitec.ch</td>
<td>CHF 132.00</td>
</tr>
<tr>
<td>Phanteks Enthoo Pro II Server Edition TG (SSI-EEB)</td>
<td>1</td>
<td>galaxus.ch</td>
<td>~CHF 170.00</td>
</tr>
<tr>
<td>Noctua NF-A14 PWM (140 mm)</td>
<td>6</td>
<td>digitec.ch</td>
<td>CHF 149.40</td>
</tr>
<tr>
<td>Noctua NF-A12x25 G2 PWM Sx2-PP (120 mm, 2-pack)</td>
<td>1 (2 fans)</td>
<td>digitec.ch</td>
<td>CHF 64.90</td>
</tr>
<tr>
<td>Noctua NF-A12x25 G2 PWM (120 mm)</td>
<td>1</td>
<td>digitec.ch</td>
<td>CHF 34.90</td>
</tr>
<tr>
<td>Mellanox/NVIDIA BlueField-2 BF2H532C dual-port 25G (PCIe 4.0 x8)</td>
<td>3</td>
<td>piospartslap.de</td>
<td>289,92 ‚Ç¨</td>
</tr>
<tr>
<td>25G SFP28 passive DAC cable 0.5 m</td>
<td>3</td>
<td>ricardo.ch</td>
<td>CHF 51.15</td>
</tr>
<tr>
<td><strong>Grand Total</strong></td>
<td></td>
<td></td>
<td><strong>CHF 3,741.34</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</article></div></div>
  </body>
</html>
