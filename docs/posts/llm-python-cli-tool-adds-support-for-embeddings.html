<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Sep/4/llm-embeddings/">Original</a>
    <h1>LLM Python/CLI tool adds support for embeddings</h1>
    
    <div id="readability-page-1" class="page"><div>



<p>4th September 2023</p>

<p><a href="https://llm.datasette.io/">LLM</a> is my Python library and command-line tool for working with language models. I just released <a href="https://llm.datasette.io/en/stable/changelog.html#v0-9">LLM 0.9</a> with a new set of features that extend LLM to provide tools for working with <em>embeddings</em>.</p>
<p>This is a long post with a lot of theory and background. If you already know what embeddings are, here’s a TLDR you can try out straight away:</p>
<div><pre><span><span>#</span> Install LLM</span>
pip install llm

<span><span>#</span> If you already installed via Homebrew/pipx you can upgrade like this:</span>
llm install -U llm

<span><span>#</span> Install the llm-sentence-transformers plugin</span>
llm install llm-sentence-transformers

<span><span>#</span> Install the all-MiniLM-L6-v2 embedding model</span>
llm sentence-transformers register all-MiniLM-L6-v2

<span><span>#</span> Generate and store embeddings for every README.md in your home directory, recursively</span>
llm embed-multi readmes \
  --model sentence-transformers/all-MiniLM-L6-v2 \
  --files <span>~</span>/ <span><span>&#39;</span>**/README.md<span>&#39;</span></span>
  <span><span>#</span> Add --store to store the text content as well</span>

<span><span>#</span> Run a similarity search for &#34;sqlite&#34; against those embeddings</span>
llm similar readmes -c sqlite</pre></div>
<p>For everyone else, read on and the above example should hopefully all make sense.</p>
<h4>Embeddings</h4>
<p>Embeddings are a fascinating concept within the larger world of language models.</p>
<p>I explained embeddings in my recent talk, <a href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/">Making Large Language Models work for you</a>. The relevant section of the slides and transcript <a href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/#embeddings">is here</a>, or you can <a href="https://www.youtube.com/watch?v=aC7UQcZN6y8&amp;t=2189s">jump to that section on YouTube</a>.</p>
<p>An embedding model lets you take a string of text—a word, sentence, paragraph or even a whole document—and turn that into an array of floating point numbers called an <em>embedding vector</em>.</p>
<p><img src="https://static.simonwillison.net/static/2023/wordcamp-llms/llm-work-for-you.055.jpeg" alt="On the left is a text post from one of my sites: Storing and serving related documents with openai-to-sqlite and embeddings. An arrow points to a huge JSON array on the right, with the label 1536 floating point numbers."/></p>
<p>A model will always produce the same length of array—1,536 numbers for the <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI embedding model</a>, 384 for <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2</a>—but the array itself is inscrutable. What are you meant to do with it?</p>
<p>The answer is that you can compare them. I like to think of an embedding vector as a location in 1,536-dimensional space. The distance between two vectors is a measure of how semantically similar they are in meaning, at least according to the model that produced them.</p>
<p><img src="https://static.simonwillison.net/static/2023/wordcamp-llms/llm-work-for-you.056.jpeg" alt="A location in 1,536 dimension space  There&#39;s a 3D plot with 400 red dots arranged randomly across 3 axis."/></p>
<p>“One happy dog” and “A playful hound” will end up close together, even though they don’t share any keywords. The embedding vector represents the language model’s interpretation of the meaning of the text.</p>
<p>Things you can do with embeddings include:</p>
<ol>
<li>Find <strong>related items</strong>. I use this on <a href="https://til.simonwillison.net/">my TIL site</a> to display related articles, as described in <a href="https://til.simonwillison.net/llms/openai-embeddings-related-content">Storing and serving related documents with openai-to-sqlite and embeddings</a>.</li>
<li>Build <strong>semantic search</strong>. As shown above, an embeddings-based search engine can find content relevant to the user’s search term even if none of the keywords match.</li>
<li>Implement <strong>retrieval augmented generation</strong>—the trick where you take a user’s question, find relevant documentation in your own corpus and use that to get an LLM to spit out an answer. More on that <a href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/#retrieval-augmented-generation">here</a>.</li>
<li>
<strong>Clustering</strong>: you can find clusters of nearby items and identify patterns in a corpus of documents.</li>
<li><strong>Classification</strong>: calculate the embedding of a piece of text and compare it to pre-calculated “average” embeddings for different categories.</li>
</ol>
<h4>LLM’s new embedding features</h4>
<p>My goal with LLM is to provide a plugin-driven abstraction around a growing collection of language models. I want to make installing, using and comparing these models as easy as possible.</p>
<p>The new release adds several command-line tools for working with embeddings, plus a new Python API for working with embeddings in your own code.</p>
<p>It also adds support for installing additional embedding models via plugins. I’ve released one plugin for this so far: <a href="https://github.com/simonw/llm-sentence-transformers">llm-sentence-transformers</a>, which adds support for new models based on the <a href="https://www.sbert.net/">sentence-transformers</a> library.</p>
<p>The example above shows how to use <code>sentence-transformers</code>. LLM also supports API-driven access to the OpenAI <code>ada-002</code> model.</p>
<p>Here’s how to embed some text using <code>ada-002</code>, assuming you have <a href="https://llm.datasette.io/en/stable/setup.html">installed LLM already</a>:</p>
<div><pre><span><span>#</span> Set your OpenAI API key</span>
llm keys <span>set</span> openai
<span><span>#</span> &lt;paste key here&gt;</span>

<span><span>#</span> Embed some text</span>
llm embed -m ada-002 -c <span><span>&#34;</span>Hello world<span>&#34;</span></span></pre></div>
<p>This will output a huge JSON list of floating point numbers to your terminal. You can add <code>-f base64</code> (or <code>-f hex</code>) to get that back in a different format, though none of these outputs are instantly useful.</p>
<p>Embeddings are much more interesting when you store them.</p>
<p>LLM already uses SQLite to <a href="https://llm.datasette.io/en/stable/logging.html">store prompts and responses</a>. It was a natural fit to use SQLite to store embeddings as well.</p>
<h4>Embedding collections</h4>
<p>LLM 0.9 introduces the concept of a <strong>collection</strong> of embeddings. A collection has a name—like <code>readmes</code>—and contains a set of embeddings, each of which has an ID and an embedding vector.</p>
<p>All of the embeddings in a collection are generated by the same model, to ensure they can be compared with each others.</p>
<p>The <code>llm embed</code> command can store the vector in the database instead of returning it to the console. Pass it the name of an existing (or to-be-created) collection and the ID to use to store the embedding.</p>
<p>Here we’ll store the embedding for the phrase “Hello world” in a collection called <code>phrases</code> with the ID <code>hello</code>, using that <code>ada-002</code> embedding model:</p>
<div><pre>llm embed phrases hello -m ada-002 -c <span><span>&#34;</span>Hello world<span>&#34;</span></span></pre></div>
<p>Future phrases can be added without needing to specify the model again, since it is remembered by the collection:</p>
<div><pre>llm embed phrases goodbye -c <span><span>&#34;</span>Goodbye world<span>&#34;</span></span></pre></div>
<p>The <code>llm embed-db collections</code> shows a list of collections:</p>
<div><pre>phrases: ada-002
  2 embeddings
readmes: sentence-transformers/all-MiniLM-L6-v2
  16796 embeddings</pre></div>
<p>The data is stored in a SQLite <code>embeddings</code> table with the following schema:</p>
<div><pre>CREATE TABLE [collections] (
   [id] <span>INTEGER</span> <span>PRIMARY KEY</span>,
   [name] <span>TEXT</span>,
   [model] <span>TEXT</span>
);
<span>CREATE</span> <span>TABLE</span> &#34;<span>embeddings</span>&#34; (
   [collection_id] <span>INTEGER</span> <span>REFERENCES</span> [collections]([id]),
   [id] <span>TEXT</span>,
   [embedding] BLOB,
   [content] <span>TEXT</span>,
   [content_hash] BLOB,
   [metadata] <span>TEXT</span>,
   [updated] <span>INTEGER</span>,
   <span>PRIMARY KEY</span> ([collection_id], [id])
);

CREATE UNIQUE INDEX [idx_collections_name]
    <span>ON</span> [collections] ([name]);
CREATE INDEX [idx_embeddings_content_hash]
    <span>ON</span> [embeddings] ([content_hash]);</pre></div>
<p>By default this is the SQLite database at the location revealed by <a href="">llm embed-db path</a>, but you can pass <code>--database my-embeddings.db</code> to various LLM commands to use a different database.</p>
<p>Each embedding vector is stored as a binary BLOB in the <code>embedding</code> column, consisting of those floating point numbers packed together as 32 bit floats.</p>
<p>The <code>content_hash</code> column contains a MD5 hash of the content. This helps avoid re-calculating the embedding (which can cost actual money for API-based embedding models like <code>ada-002</code>) unless the content has changed.</p>
<p>The <code>content</code> column is usually <code>null</code>, but can contain a copy of the original text content if you pass the <code>--store</code> option to the <code>llm embed</code> command.</p>
<p><code>metadata</code> can contain a JSON object with metadata, if you pass <code>--metadata &#39;{&#34;json&#34;: &#34;goes here&#34;}</code>.</p>
<p>You don’t have to pass content using <code>-c</code>—you can instead pass a file path using the <code>-i/--input</code> option:</p>
<div><pre>llm embed docs llm-setup -m ada-002 -i llm/docs/setup.md</pre></div>
<p>Or pipe things to standard input like this:</p>
<div><pre>cat llm/docs/setup.md <span>|</span> llm embed docs llm-setup -m ada-002 -i -</pre></div>
<h4>Embedding similarity search</h4>
<p>Once you’ve built a collection, you can search for similar embeddings using the <code>llm similar</code> command.</p>
<p>The <code>-c &#34;term&#34;</code> option will embed the text you pass in using the embedding model for the collection and use that as the comparison vector:</p>
<div><pre>llm similar readmes -c sqlite</pre></div>
<p>You can also pass the ID of an object in that collection to use that embedding instead. This gets you related documents, for example:</p>
<div><pre>llm similar readmes sqlite-utils/README.md</pre></div>
<p>The output from this command is currently newline-delimited JSON.</p>
<h4>Embedding in bulk</h4>
<p>The <code>llm embed</code> command embeds a single string at a time. <code>llm embed-multi</code> is much more powerful: you can feed a CSV or JSON file, a SQLite database or even have it read from a directory of files in order to embed multiple items at once.</p>
<p>Many embeddings models are optimized for batch operations, so embedding multiple items at a time can provide a significant speed boost.</p>
<p>The <code>embed-multi</code> command is described <a href="https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed-multi">in detail in the documentation</a>. Here are a couple of fun things you can do with it.</p>
<p>First, I’m going to create embeddings for every single one of my Apple Notes.</p>
<p>My <a href="https://datasette.io/tools/apple-notes-to-sqlite">apple-notes-to-sqlite</a> tool can export Apple Notes to a SQLite database. I’ll run that first:</p>
<div><pre>apple-notes-to-sqlite notes.db</pre></div>
<p>This took quite a while to run on my machine and generated a 828M SQLite database containing 6,462 records!</p>
<p>Next, I’m going to embed the content of all of those notes using the <code>sentence-transformers/all-MiniLM-L6-v2</code> model:</p>
<div><pre>llm embed-multi notes \
  -d notes.db \
  --sql <span><span>&#39;</span>select id, title, body from notes<span>&#39;</span></span> \
  -m sentence-transformers/all-MiniLM-L6-v2</pre></div>
<p>This took around 15 minutes to run, and increased the size of my database by 13MB.</p>
<p>The <code>--sql</code> option here specifies a SQL query. The first column must be an <code>id</code>, then any subsequent columns will be concatenated together to form the content to embed.</p>
<p>In this case the embeddings are written back to the same <code>notes.db</code> database that the content came from.</p>
<p>And now I can run embedding similarity operations against all of my Apple notes!</p>
<div><pre>llm similar notes -d notes.db -c <span><span>&#39;</span>ideas for blog posts<span>&#39;</span></span></pre></div>
<h4>Embedding files in a directory</h4>
<p>Let’s revisit the example from the top of this post. In this case, I’m using the <code>--files</code> option to search for files on disk and embed each of them:</p>
<div><pre>llm embed-multi readmes \
  --model sentence-transformers/all-MiniLM-L6-v2 \
  --files <span>~</span>/ <span><span>&#39;</span>**/README.md<span>&#39;</span></span></pre></div>
<p>The <code>--files</code> option takes two arguments: a path to a directory and a pattern to match against filenames. In this case I’m searching my home directory recursively for any files named <code>README.md</code>.</p>
<p>Running this command gives me embeddings for all of my README.md files, which I can then search against like this:</p>
<div><pre>llm similar readmes -c sqlite</pre></div>
<h4>Embeddings in Python</h4>
<p>So far I’ve only covered the command-line tools. LLM 0.9 also introduces a new Python API for working with embeddings.</p>
<p>There are two aspects to this. If you just want to embed content and handle the resulting vectors yourself, you can use <code>llm.get_embedding_model()</code>:</p>
<pre><span>import</span> <span>llm</span>

<span># This takes model IDs and aliases defined by plugins:</span>
<span>model</span> <span>=</span> <span>llm</span>.<span>get_embedding_model</span>(<span>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>)
<span>vector</span> <span>=</span> <span>model</span>.<span>embed</span>(<span>&#34;This is text to embed&#34;</span>)</pre>
<p><code>vector</code> will then be a Python list of floating point numbers.</p>
<p>You can serialize that to the same binary format that LLM uses like this:</p>
<pre><span>binary_vector</span> <span>=</span> <span>llm</span>.<span>encode</span>(<span>vector</span>)
<span># And to deserialize:</span>
<span>vector</span> <span>=</span> <span>llm</span>.<span>decode</span>(<span>binary_vector</span>)</pre>
<p>The second aspect of the Python API is the <code>llm.Collection</code> class, for working with collections of embeddings. This example code is quoted <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#working-with-collections">from the documentation</a>:</p>
<pre><span>import</span> <span>sqlite_utils</span>
<span>import</span> <span>llm</span>

<span># This collection will use an in-memory database that will be</span>
<span># discarded when the Python process exits</span>
<span>collection</span> <span>=</span> <span>llm</span>.<span>Collection</span>(<span>&#34;entries&#34;</span>, <span>model_id</span><span>=</span><span>&#34;ada-002&#34;</span>)

<span># Or you can persist the database to disk like this:</span>
<span>db</span> <span>=</span> <span>sqlite_utils</span>.<span>Database</span>(<span>&#34;my-embeddings.db&#34;</span>)
<span>collection</span> <span>=</span> <span>llm</span>.<span>Collection</span>(<span>&#34;entries&#34;</span>, <span>db</span>, <span>model_id</span><span>=</span><span>&#34;ada-002&#34;</span>)

<span># You can pass a model directly using model= instead of model_id=</span>
<span>embedding_model</span> <span>=</span> <span>llm</span>.<span>get_embedding_model</span>(<span>&#34;ada-002&#34;</span>)
<span>collection</span> <span>=</span> <span>llm</span>.<span>Collection</span>(<span>&#34;entries&#34;</span>, <span>db</span>, <span>model</span><span>=</span><span>embedding_model</span>)

<span># Store a string in the collection with an ID:</span>
<span>collection</span>.<span>embed</span>(<span>&#34;hound&#34;</span>, <span>&#34;my happy hound&#34;</span>)

<span># Or to store content and extra metadata:</span>
<span>collection</span>.<span>embed</span>(
    <span>&#34;hound&#34;</span>,
    <span>&#34;my happy hound&#34;</span>,
    <span>metadata</span><span>=</span>{<span>&#34;name&#34;</span>: <span>&#34;Hound&#34;</span>},
    <span>store</span><span>=</span><span>True</span>
)

<span># Or embed things in bulk:</span>
<span>collection</span>.<span>embed_multi</span>(
    [
        (<span>&#34;hound&#34;</span>, <span>&#34;my happy hound&#34;</span>),
        (<span>&#34;cat&#34;</span>, <span>&#34;my dissatisfied cat&#34;</span>),
    ],
    <span># Add this to store the strings in the content column:</span>
    <span>store</span><span>=</span><span>True</span>,
)</pre>
<p>As with everything else in LLM, the goal is that anything you can do with the CLI can be done with the Python API, and vice-versa.</p>
<h4 id="llm-cluster">Clustering with llm-cluster</h4>
<p>Another interesting application of embeddings is that you can use them to cluster content—identifying patterns in a corpus of documents.</p>
<p>I’ve started exploring this area with a new plugin, called <strong><a href="https://github.com/simonw/llm-cluster">llm-cluster</a>.</strong></p>
<p>You can install it like this:</p>

<p>Let’s create a new collection using data pulled from GitHub. I’m going to import all of the <a href="https://github.com/simonw/llm/issues">LLM issues</a> from the GitHub API, using my <a href="https://github.com/simonw/paginate-json">paginate-json</a> tool:</p>
<div><pre>paginate-json <span><span>&#39;</span>https://api.github.com/repos/simonw/llm/issues?state=all&amp;filter=all<span>&#39;</span></span> \
  <span>|</span> jq <span><span>&#39;</span>[.[] | {id: .id, title: .title}]<span>&#39;</span></span> \
  <span>|</span> llm embed-multi llm-issues - \
    --database issues.db \
    --model sentence-transformers/all-MiniLM-L6-v2 \
    --store</pre></div>
<p>Running this gives me a <code>issues.db</code> SQLite database with 218 embeddings contained in a collection called <code>llm-issues</code>.</p>
<p>Now let’s try out the <code>llm-cluster</code> command, requesting ten clusters from that collection:</p>
<div><pre>llm cluster llm-issues --database issues.db 10</pre></div>
<p>The output from this command, truncated, looks like this:</p>
<div><pre>[
  {
    <span>&#34;id&#34;</span>: <span><span>&#34;</span>0<span>&#34;</span></span>,
    <span>&#34;items&#34;</span>: [
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1784149135<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Tests fail with pydantic 2<span>&#34;</span></span>
      },
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1837084995<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Allow for use of Pydantic v1 as well as v2.<span>&#34;</span></span>
      },
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1857942721<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Get tests passing against Pydantic 1<span>&#34;</span></span>
      }
    ]
  },
  {
    <span>&#34;id&#34;</span>: <span><span>&#34;</span>1<span>&#34;</span></span>,
    <span>&#34;items&#34;</span>: [
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1724577618<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Better ways of storing and accessing API keys<span>&#34;</span></span>
      },
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1772024726<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Support for `-o key value` options such as `temperature`<span>&#34;</span></span>
      },
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1784111239<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>`--key` should be used in place of the environment variable<span>&#34;</span></span>
      }
    ]
  },
  {
    <span>&#34;id&#34;</span>: <span><span>&#34;</span>8<span>&#34;</span></span>,
    <span>&#34;items&#34;</span>: [
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1835739724<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Bump the python-packages group with 1 update<span>&#34;</span></span>
      },
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1848143453<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Python library support for adding aliases<span>&#34;</span></span>
      },
      {
        <span>&#34;id&#34;</span>: <span><span>&#34;</span>1857268563<span>&#34;</span></span>,
        <span>&#34;content&#34;</span>: <span><span>&#34;</span>Bump the python-packages group with 1 update<span>&#34;</span></span>
      }
    ]
  }
]</pre></div>
<p>These look pretty good! But wouldn’t it be neat if we had a snappy title for each one?</p>
<p>The <code>--summary</code> option can provide exactly that, by piping the members of each cluster through a call to another LLM in order to generate a useful summary.</p>
<div><pre>llm cluster llm-issues --database issues.db 10 --summary</pre></div>
<p>This uses <code>gpt-3.5-turbo</code> to generate a summary for each cluster, with this default prompt:</p>
<blockquote>
<p>Short, concise title for this cluster of related documents.</p>
</blockquote>
<p>The results I got back are pretty good, including:</p>
<ul>
<li>Template Storage and Management Improvements</li>
<li>Package and Dependency Updates and Improvements</li>
<li>Adding Conversation Mechanism and Tools</li>
</ul>
<p>I tried the same thing using a Llama 2 model <a href="https://simonwillison.net/2023/Aug/1/llama-2-mac/">running on my own laptop</a>, with a custom prompt:</p>
<pre><code>llm cluster llm-issues --database issues.db 10 \
  --summary --model mlc-chat-Llama-2-13b-chat-hf-q4f16_1 \
  --prompt &#39;Concise title for this cluster of related documents, just return the title&#39;
</code></pre>
<p>I didn’t quite get what I wanted! Llama 2 is proving a lot harder to prompt, so each cluster came back with something that looked like this:</p>
<blockquote>
<p>Sure! Here’s a concise title for this cluster of related documents:</p>
<p>“Design Improvements for the Neat Prompt System”</p>
<p>This title captures the main theme of the documents, which is to improve the design of the Neat prompt system. It also highlights the focus on improving the system’s functionality and usability</p>
</blockquote>
<p><a href="https://github.com/simonw/llm-cluster">llm-cluster</a> only took a few hours to throw together, which I’m seeing as a positive indicator that the LLM library is developing in the right direction.</p>
<h4>Future plans</h4>
<p>The two future features I’m most excited about are indexing and chunking.</p>
<h5>Indexing</h5>
<p>The <a href="https://llm.datasette.io/en/stable/embeddings/cli.html#llm-similar">llm similar</a> command and <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#retrieving-similar-items">collection.similar()</a> Python method currently use effectively the slowest brute force approach possible: calculate a cosine difference between input vector and every other embedding in the collection, then sort the results.</p>
<p>This works fine for collections with a few hundred items, but will start to suffer for collections of 100,000 or more.</p>
<p>There are plenty of potential ways of speeding this up: you can run a vector index like <a href="https://github.com/facebookresearch/faiss">FAISS</a> or <a href="https://github.com/nmslib/hnswlib">hnswlib</a>, use a database extension like <a href="https://github.com/asg017/sqlite-vss">sqlite-vss</a> or <a href="https://github.com/pgvector/pgvector">pgvector</a>, or turn to a hosted vector database like <a href="https://www.pinecone.io/">Pinecone</a> or <a href="https://milvus.io/">Milvus</a>.</p>
<p>With this many potential solutions, the obvious answer for LLM is to address this with plugins.</p>
<p>I’m still thinking through the details, but the core idea is that users should be able to define an index against one or more collections, and LLM will then coordinate updates to that index. These may not happen in real-time—some indexes can be expensive to rebuild, so there are benefits to applying updates in batches.</p>
<p>I experimented with FAISS earlier this year in <a href="https://datasette.io/plugins/datasette-faiss">datasette-faiss</a>. That’s likely to be the base for my first implementation.</p>
<p>The <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#sql-schema">embeddings table</a> has an <code>updated</code> timestamp column to support this use-case—so indexers can run against just the items that have changed since the last indexing run.</p>
<p>Follow <a href="https://github.com/simonw/llm/issues/216">issue #216</a> for updates on this feature.</p>
<h5>Chunking</h5>
<p>When building an embeddings-based search engine, the hardest challenge is deciding how best to “chunk” the documents.</p>
<p>Users will type in short phrases or questions. The embedding for a four word question might not necessarily map closely to the embedding of a thousand word article, even if the article itself should be a good match for that query.</p>
<p>To maximize the chance of returning the most relevant content, we need to be smarter about what we embed.</p>
<p>I’m still trying to get a good feeling for the strategies that make sense here. Some that I’ve seen include:</p>
<ul>
<li>Split a document up into fixed length shorter segments.</li>
<li>Split into segments but including a ~10% overlap with the previous and next segments, to reduce problems caused by words and sentences being split in a way that disrupts their semantic meaning.</li>
<li>Splitting by sentence, using NLP techniques.</li>
<li>Splitting into higher level sections, based on things like document headings.</li>
</ul>
<p>Then there are more exciting, LLM-driven approaches:</p>
<ul>
<li>Generate an LLM summary of a document and embed that.</li>
<li>Ask an LLM “What questions are answered by the following text?” and then embed each of the resulting questions!</li>
</ul>
<p>It’s possible to try out these different techniques using LLM already: write code that does the splitting, then feed the results to <a href="https://llm.datasette.io/en/stable/embeddings/python-api.html#storing-embeddings-in-bulk">Collection.embed_multi()</a> or <a href="https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed-multi">llm embed-multi</a>.</p>
<p>But... it would be really cool if LLM could split documents for you—with the splitting techniques themselves defined by plugins, to make it easy to try out new approaches.</p>
<h4>Get involved</h4>
<p>It should be clear by now that the potential scope of the LLM project is enormous. I’m trying to use plugins to tie together an enormous and rapidly growing ecosystem of models and techniques into something that’s as easy for people to work with and build on as possible.</p>
<p>There are plenty of ways you can help!</p>
<ul>
<li>
<a href="https://datasette.io/discord-llm">Join the #llm Discord</a> to talk about the project.</li>
<li>Try out plugins and run different models with them. There are <a href="https://llm.datasette.io/en/stable/plugins/directory.html">12 plugins already</a>, and several of those can be used to run dozens if not hundreds of models (<a href="https://github.com/simonw/llm-mlc">llm-mlc</a>, <a href="https://github.com/simonw/llm-gpt4all">llm-gpt4all</a> and <a href="https://github.com/simonw/llm-llama-cpp">llm-llama-cpp</a> in particular). I’ve hardly scratched the surface of these myself, and I’m testing exclusively on Apple Silicon. I’m really keen to learn more about which models work well, which models don’t and which perform the best on different hardware.</li>
<li>Try <a href="https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html">building a plugin</a> for a new model. My dream here is that every significant Large Language Model will have an LLM plugin that makes it easy to install and use.</li>
<li>Build stuff using LLM and let me know what you’ve built. Nothing fuels an open source project more than stories of cool things people have built with it.</li>
</ul>




</div></div>
  </body>
</html>
