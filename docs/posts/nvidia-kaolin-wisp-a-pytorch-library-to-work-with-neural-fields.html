<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/NVIDIAGameWorks/kaolin-wisp">Original</a>
    <h1>Nvidia Kaolin Wisp: a PyTorch library to work with neural fields</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIAGameWorks/kaolin-wisp/blob/main/media/demo.jpg"><img src="https://github.com/NVIDIAGameWorks/kaolin-wisp/raw/main/media/demo.jpg" alt="drawing" width="1000"/></a></p>
<p dir="auto">NVIDIA Kaolin Wisp is a PyTorch library powered by <a href="https://github.com/NVIDIAGameWorks/kaolin">NVIDIA Kaolin Core</a> to work with
neural fields (including NeRFs, <a href="https://nv-tlabs.github.io/nglod" rel="nofollow">NGLOD</a>, <a href="https://nvlabs.github.io/instant-ngp/" rel="nofollow">instant-ngp</a> and <a href="https://nv-tlabs.github.io/vqad" rel="nofollow">VQAD</a>).</p>
<p dir="auto">NVIDIA Kaolin Wisp aims to provide a set of common utility functions for performing research on neural fields.
This includes datasets, image I/O, mesh processing, and ray utility functions.
Wisp also comes with building blocks like differentiable renderers and differentiable data structures
(like octrees, hash grids, triplanar features) which are useful to build complex neural fields.
It also includes debugging visualization tools, interactive rendering and training, logging, and trainer classes.</p>
<p dir="auto">For an overview on neural fields, we recommend you checkout the EG STAR report:
<a href="https://arxiv.org/abs/2111.11426" rel="nofollow">Neural Fields for Visual Computing and Beyond</a>.</p>
<h2 dir="auto"><a id="user-content-license-and-citation" aria-hidden="true" href="#license-and-citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License and Citation</h2>
<p dir="auto">This codebase is licensed under the NVIDIA Source Code License.
Commercial licenses are also available, free of charge.
Please apply using this link (use &#34;Other&#34; and specify Kaolin Wisp): <a href="https://www.nvidia.com/en-us/research/inquiries/" rel="nofollow">https://www.nvidia.com/en-us/research/inquiries/</a></p>
<p dir="auto">If you find the NVIDIA Kaolin Wisp library useful for your research, please cite:</p>
<div data-snippet-clipboard-copy-content="@misc{KaolinWispLibrary,
      author = {Towaki Takikawa and Or Perel and Clement Fuji Tsang and Charles Loop and Joey Litalien and Jonathan Tremblay and Sanja Fidler and Maria Shugrina},
      title = {Kaolin Wisp: A PyTorch Library and Engine for Neural Fields Research},
      year = {2022},
      howpublished={\url{https://github.com/NVIDIAGameWorks/kaolin-wisp}}
}"><pre><code>@misc{KaolinWispLibrary,
      author = {Towaki Takikawa and Or Perel and Clement Fuji Tsang and Charles Loop and Joey Litalien and Jonathan Tremblay and Sanja Fidler and Maria Shugrina},
      title = {Kaolin Wisp: A PyTorch Library and Engine for Neural Fields Research},
      year = {2022},
      howpublished={\url{https://github.com/NVIDIAGameWorks/kaolin-wisp}}
}
</code></pre></div>
<h2 dir="auto"><a id="user-content-key-features" aria-hidden="true" href="#key-features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Key Features</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIAGameWorks/kaolin-wisp/blob/main/media/blocks.jpg"><img src="https://github.com/NVIDIAGameWorks/kaolin-wisp/raw/main/media/blocks.jpg" alt="drawing" width="750"/></a></p>
<ul dir="auto">
<li>Differentiable feature grids
<ul dir="auto">
<li>Octree grids (from NGLOD)</li>
<li>Hash grids (from Instant-NGP)</li>
<li>Triplanar texture grids (from ConvOccNet, EG3D)</li>
<li>Codebook grids (from VQAD)</li>
</ul>
</li>
<li>Acceleration structures for fast raytracing
<ul dir="auto">
<li>Octree acceleration structures based on Kaolin Core SPC</li>
</ul>
</li>
<li>Tracers to trace rays against neural fields
<ul dir="auto">
<li>PackedSDFTracer for SDFs</li>
<li>PackedRFTracer for radiance fields (NeRFs)</li>
</ul>
</li>
<li>Various datasets for common neural fields
<ul dir="auto">
<li>Standard Instant-NGP compatible datasets</li>
<li>RTMV dataset</li>
<li>SDF sampled from meshes</li>
</ul>
</li>
<li>An interactive renderer where you can train and visualize neural fields</li>
<li>A set of core framework features (<code>wisp.core</code>) for convenience</li>
<li>A set of utility functions (<code>wisp.ops</code>)</li>
</ul>
<p dir="auto">Have a feature request? Leave a GitHub issue!</p>
<h2 dir="auto"><a id="user-content-getting-started" aria-hidden="true" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Getting started</h2>
<h3 dir="auto"><a id="user-content-1-create-an-anaconda-environment" aria-hidden="true" href="#1-create-an-anaconda-environment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1. Create an anaconda environment</h3>
<p dir="auto">The easiest way to get started is to create a virtual Python 3.8 Anaconda environment:</p>
<div data-snippet-clipboard-copy-content="sudo apt-get update
sudo apt-get install libopenexr-dev 
conda create -n wisp python=3.8
conda activate wisp
pip install --upgrade pip"><pre><code>sudo apt-get update
sudo apt-get install libopenexr-dev 
conda create -n wisp python=3.8
conda activate wisp
pip install --upgrade pip
</code></pre></div>
<h3 dir="auto"><a id="user-content-2-install-pytorch" aria-hidden="true" href="#2-install-pytorch"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2. Install PyTorch</h3>
<p dir="auto">You should first install PyTorch by following the <a href="https://pytorch.org/" rel="nofollow">official instructions</a>. The code has been tested with <code>1.9.1</code> to <code>1.12.0</code> on Ubuntu 20.04.</p>
<h3 dir="auto"><a id="user-content-3-install-kaolin" aria-hidden="true" href="#3-install-kaolin"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3. Install Kaolin</h3>
<p dir="auto">You should also install Kaolin, following the <a href="https://kaolin.readthedocs.io/en/latest/notes/installation.html" rel="nofollow">instructions here</a>. <strong>WARNING:</strong> The minimum required version of Kaolin is <code>1.12.0</code>. If you have any issues specifically with Camera classes not existing, make sure you have an up-to-date version of Kaolin.</p>
<h3 dir="auto"><a id="user-content-4-install-the-rest-of-the-dependencies" aria-hidden="true" href="#4-install-the-rest-of-the-dependencies"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>4. Install the rest of the dependencies</h3>
<p dir="auto">Install the rest of the dependencies from <a href="https://github.com/NVIDIAGameWorks/kaolin-wisp/blob/main/requirements.txt">requirements</a>:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
<h3 dir="auto"><a id="user-content-5-installing-the-interactive-renderer-optional" aria-hidden="true" href="#5-installing-the-interactive-renderer-optional"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>5. Installing the interactive renderer (optional)</h3>
<p dir="auto">If you wish to use the interactive renderer and training visualizer, you will need additional dependencies.
Note that you need to have OpenGL available on your system.</p>
<p dir="auto">To install (<strong>make sure you have the CUDA_HOME environment variable set!</strong>):</p>
<div data-snippet-clipboard-copy-content="git clone --recursive https://github.com/inducer/pycuda
cd pycuda
python configure.py --cuda-root=$CUDA_HOME --cuda-enable-gl
python setup.py develop
cd ..
pip install -r requirements_app.txt"><pre><code>git clone --recursive https://github.com/inducer/pycuda
cd pycuda
python configure.py --cuda-root=$CUDA_HOME --cuda-enable-gl
python setup.py develop
cd ..
pip install -r requirements_app.txt
</code></pre></div>
<h3 dir="auto"><a id="user-content-6-installing-wisp" aria-hidden="true" href="#6-installing-wisp"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>6. Installing Wisp</h3>
<p dir="auto">To install wisp, simply execute:</p>

<p dir="auto">in the main wisp directory. You should now be able to run some examples!</p>
<h2 dir="auto"><a id="user-content-using-docker" aria-hidden="true" href="#using-docker"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using Docker</h2>
<h3 dir="auto"><a id="user-content-1-using-our-dockerfile-linux-only" aria-hidden="true" href="#1-using-our-dockerfile-linux-only"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1. Using our Dockerfile (Linux Only)</h3>
<p dir="auto">An easy way to use Wisp is to use our Dockerfile.</p>
<p dir="auto">You first need to have a base image with <a href="https://github.com/NVIDIAGameWorks/kaolin">Kaolin Core</a> installed,
we suggested using the <a href="https://github.com/NVIDIAGameWorks/kaolin/blob/master/tools/linux/Dockerfile.install">Dockerfile</a> of Kaolin Core to build it,
this Dockerfile also takes a Base Image with PyTorch preinstalled, you can either build it with this <a href="https://github.com/NVIDIAGameWorks/kaolin/blob/master/tools/linux/Dockerfile.base">Dockerfile</a>
or use one available on <a href="https://hub.docker.com/r/pytorch/pytorch" rel="nofollow">dockerhub</a> or <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch" rel="nofollow">NGC</a></p>
<div data-snippet-clipboard-copy-content="# Clone Kaolin Core
git clone --recursive https://github.com/NVIDIAGameWorks/kaolin/ path/to/kaolin
cd path/to/kaolin

# (Optional) Build the Core pytorch docker image
docker build -f tools/linux/Dockerfile.base -t kaolinbase --network=host \
    --build-arg CUDA_VERSION=11.3.1 \
    --build-arg CUDNN_VERSION=8 \
    --build-arg PYTHON_VERSION=3.9 \
    --build-arg PYTORCH_VERSION=1.11.0 \
    .

# Build the Kaolin Core docker image
# (replace kaolinbase by any image with pytorch preinstalled)
docker build -f tools/linux/Dockerfile.install -t kaolin --network=host \
    --build-arg BASE_IMAGE=kaolinbase \
    .

# Build the Wisp docker image
cd path/to/wisp
docker build -f tools/linux/Dockerfile -t wisp --network=host \
    --build-arg BASE_IMAGE=kaolin \
    --build-arg INSTALL_RENDERER \
    ."><pre><code># Clone Kaolin Core
git clone --recursive https://github.com/NVIDIAGameWorks/kaolin/ path/to/kaolin
cd path/to/kaolin

# (Optional) Build the Core pytorch docker image
docker build -f tools/linux/Dockerfile.base -t kaolinbase --network=host \
    --build-arg CUDA_VERSION=11.3.1 \
    --build-arg CUDNN_VERSION=8 \
    --build-arg PYTHON_VERSION=3.9 \
    --build-arg PYTORCH_VERSION=1.11.0 \
    .

# Build the Kaolin Core docker image
# (replace kaolinbase by any image with pytorch preinstalled)
docker build -f tools/linux/Dockerfile.install -t kaolin --network=host \
    --build-arg BASE_IMAGE=kaolinbase \
    .

# Build the Wisp docker image
cd path/to/wisp
docker build -f tools/linux/Dockerfile -t wisp --network=host \
    --build-arg BASE_IMAGE=kaolin \
    --build-arg INSTALL_RENDERER \
    .
</code></pre></div>
<h3 dir="auto"><a id="user-content-2-running-the-docker-container" aria-hidden="true" href="#2-running-the-docker-container"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2. Running the Docker container</h3>
<p dir="auto">In order to run the interactive renderer you need to forward the <code>DISPLAY</code> environment variable
and expose the X server on the host.</p>
<div data-snippet-clipboard-copy-content="# expose the X server on the host.
sudo xhost +local:root

# Run the container
docker run --rm -it --gpus=all --net=host --ipc=host -e DISPLAY=$DISPLAY wisp"><pre><code># expose the X server on the host.
sudo xhost +local:root

# Run the container
docker run --rm -it --gpus=all --net=host --ipc=host -e DISPLAY=$DISPLAY wisp
</code></pre></div>
<h2 dir="auto"><a id="user-content-training--rendering-with-wisp" aria-hidden="true" href="#training--rendering-with-wisp"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training &amp; Rendering with Wisp</h2>
<h3 dir="auto"><a id="user-content-training-nglod-nerf-from-multiview-rgb-d-data" aria-hidden="true" href="#training-nglod-nerf-from-multiview-rgb-d-data"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training NGLOD-NeRF from multiview RGB-D data</h3>
<p dir="auto">You will first need to download some sample data to run NGLOD-NeRF.
Go to this <a href="https://drive.google.com/file/d/18hY0DpX2bK-q9iY_cog5Q0ZI7YEjephE/view?usp=sharing" rel="nofollow">Google Drive link</a>
to download a cool Lego V8 engine from the <a href="http://www.cs.umd.edu/~mmeshry/projects/rtmv/" rel="nofollow">RTMV dataset</a>.</p>
<p dir="auto">Once you have downloaded and extracted the data somewhere, you can train a NeRF using <a href="https://nv-tlabs.github.io/nglod/" rel="nofollow">NGLOD</a> with:</p>
<div data-snippet-clipboard-copy-content="python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --dataset-num-workers 4"><pre><code>python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --dataset-num-workers 4
</code></pre></div>
<p dir="auto">This will generate logs inside <code>_results/logs/runs/test-nglod-nerf</code> in which you can find the trained
checkpoint, and <code>EXR</code> images of validation outputs. We highly recommend that you install
<a href="https://github.com/Tom94/tev">tev</a> as the default application to open EXRs.
Note that the <code>--dataset-num-workers</code> argument is used here to control the multiprocessing used to load
ground truth images. To disable the multiprocessing, you can pass in <code>--dataset-num-workers -1</code>.</p>
<p dir="auto">To view the logs with TensorBoard:</p>
<div data-snippet-clipboard-copy-content="tensorboard --logdir _results/logs/runs"><pre><code>tensorboard --logdir _results/logs/runs
</code></pre></div>
<p dir="auto">Want to run the code with different options? Our configuration system makes this very easy.
If you want to run with a different number of levels of details:</p>
<div data-snippet-clipboard-copy-content="python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --num-lods 1"><pre><code>python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --num-lods 1
</code></pre></div>
<p dir="auto">Take a look at <code>wisp/config_parser.py</code> for the list of different options you can pass in, and <code>configs/nglod_nerf.yaml</code>
for the options that are already passed in.</p>
<h3 dir="auto"><a id="user-content-interactive-training" aria-hidden="true" href="#interactive-training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Interactive training</h3>
<p dir="auto">To run the training task interactively using the renderer engine, run:</p>
<div data-snippet-clipboard-copy-content="WISP_HEADLESS=0 python3 app/main_interactive.py --config configs/nglod_nerf_interactive.yaml --dataset-path /path/to/V8 --dataset-num-workers 4"><pre><code>WISP_HEADLESS=0 python3 app/main_interactive.py --config configs/nglod_nerf_interactive.yaml --dataset-path /path/to/V8 --dataset-num-workers 4
</code></pre></div>
<p dir="auto">Every config file that we ship has a <code>*_interactive.yaml</code> counterpart that can be used for better settings
(in terms of user experience)
for the interactive training app. The later examples we show can all be run interactively with
<code>WISP_HEADLESS=1 python3 app/main_interactive.py</code> and the corresponding configs.</p>
<h3 dir="auto"><a id="user-content-using-wisp-in-headless-mode" aria-hidden="true" href="#using-wisp-in-headless-mode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Using <code>wisp</code> in headless mode</h3>
<p dir="auto">To disable interactive mode, and run wisp <em>without</em> loading the graphics API, set the env variable:</p>

<p dir="auto">Toggling this flag is useful for debugging on machines without a display.
This is also needed if you opt to avoid installing the interactive renderer requirements.</p>
<h3 dir="auto"><a id="user-content-training-nglod-sdf-from-meshes" aria-hidden="true" href="#training-nglod-sdf-from-meshes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training NGLOD-SDF from meshes</h3>
<p dir="auto">We also support training neural SDFs from meshes.
You will first need to download a mesh.
Go to this <a href="https://github.com/alecjacobson/common-3d-test-models/blob/master/data/spot.obj">link</a> to download
an OBJ file of the Spot cow.</p>
<p dir="auto">Then, run the SDF training with:</p>
<div data-snippet-clipboard-copy-content="python3 app/main.py --config configs/nglod_sdf.yaml --dataset-path /path/to/spot.obj"><pre><code>python3 app/main.py --config configs/nglod_sdf.yaml --dataset-path /path/to/spot.obj
</code></pre></div>
<p dir="auto">Currently the SDF sampler we have shipped with our code can be quite slow for larger meshes. We plan to
release a more optimized version of the SDF sampler soon.</p>
<h3 dir="auto"><a id="user-content-training-ngp-for-forward-facing-scenes" aria-hidden="true" href="#training-ngp-for-forward-facing-scenes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training NGP for forward facing scenes</h3>
<p dir="auto">Lastly, we also show an example of training a forward-facing scene: the <code>fox</code> scene from <code>instant-ngp</code>.
To train a version of the <a href="https://nvlabs.github.io/instant-ngp/" rel="nofollow">Instant-NGP</a>, first download the <code>fox</code>
dataset from the <code>instant-ngp</code> repository somewhere. Then, run the training with:</p>
<div data-snippet-clipboard-copy-content="python3 app/main.py --config configs/ngp_nerf.yaml --multiview-dataset-format standard --mip 0 --dataset-path /path/to/fox"><pre><code>python3 app/main.py --config configs/ngp_nerf.yaml --multiview-dataset-format standard --mip 0 --dataset-path /path/to/fox
</code></pre></div>
<p dir="auto">Our code supports any &#34;standard&#34; NGP-format datasets that has been converted with the scripts from the
<code>instant-ngp</code> library. We pass in the <code>--multiview-dataset-format</code> argument to specify the dataset type, which
in this case is different from the RTMV dataset type used for the other examples.</p>
<p dir="auto">The <code>--mip</code> argument controls the amount of downscaling that happens on the images when they get loaded. This is useful
for datasets with very high resolution images to prevent overload on system memory, but is usually not necessary for the
fox dataset.</p>
<p dir="auto">Note that our architecture, training, and implementation details still have slight differences from the
published Instant-NGP.</p>
<h3 dir="auto"><a id="user-content-configuration-system" aria-hidden="true" href="#configuration-system"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Configuration System</h3>
<p dir="auto">Wisp accepts configuration from both the command line interface (CLI) and a <code>yaml</code> config file
(examples in <code>configs</code>). Whatever config file you pass in through the <code>--config</code> option will be checked
against the options in <code>wisp/options.py</code> and serve as the <em>default arguments</em>. This means any CLI argument
you additionally pass in will overwrite the options you pass in through the <code>--config</code>.
The order of arguments does not matter.</p>
<p dir="auto">Wisp also supports hierarchical configs, by using the <code>parent</code> argument in the config to set a parent
config file path in relative path from the config location or with an absolute path. Note however that
only a single level of hierarchy is allowed to keep the indirection manageable.</p>
<p dir="auto">If you get any errors from loading in config files, you likely made a typo in your field names. Check
against <code>wisp/options.py</code> as your source of truth. (Or pass in <code>-h</code> for help).</p>
<h2 dir="auto"><a id="user-content-what-is-wisp" aria-hidden="true" href="#what-is-wisp"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>What is &#34;wisp&#34;?</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIAGameWorks/kaolin-wisp/blob/main/media/wisp.jpg"><img src="https://github.com/NVIDIAGameWorks/kaolin-wisp/raw/main/media/wisp.jpg" alt="drawing" height="300"/></a></p>
<p dir="auto">Our library is named after the atmospheric ghost light, will-o&#39;-the-wisp,
which are volumetric ghosts that are harder to model with common standard
geometry representations like meshes. We provide a <a href="https://drive.google.com/file/d/1jKIkqm4XhdeEQwXTqbKlZw-9dO7kJfsZ/view" rel="nofollow">multiview dataset</a> of the
wisp as a reference dataset for a volumetric object.
We also provide the <a href="https://drive.google.com/drive/folders/1Via1TOsnG-3mUkkGteEoRJdEYJEx3wgf?usp=sharing" rel="nofollow">blender file and rendering scripts</a> if you want to generate specific data with this scene, please refer to the <a href="https://drive.google.com/file/d/1IrWKjxxrJOlD3C5lDYvejaSXiPtm_XI_/view?usp=sharing" rel="nofollow">readme.md</a> for greater details on how to generate the data.</p>
<h2 dir="auto"><a id="user-content-thanks" aria-hidden="true" href="#thanks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Thanks</h2>
<p dir="auto">We thank James Lucas, Jonathan Tremblay, Valts Blukis, Anita Hu, and Nishkrit Desai for giving us early feedback
and testing out the code at various stages throughout development.
We thank Rogelio Olguin and Jonathan Tremblay for the Wisp reference data.</p>
</article>
          </div></div>
  </body>
</html>
