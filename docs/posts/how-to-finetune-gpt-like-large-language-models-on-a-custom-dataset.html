<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/">Original</a>
    <h1>How to Finetune GPT-Like Large Language Models on a Custom Dataset</h1>
    
    <div id="readability-page-1" class="page"><section itemprop="text"><div><div><div><h3>Takeaways</h3><p> Learn how to fine-tune large language models (LLMs) on a custom dataset. We will be using <a href="https://github.com/Lightning-AI/Lit-Parrot">Lit-Parrot</a>, a nanoGPT based implementation of the GPT-NeoX</p></div><p>The AI communityâ€™s effort has led to the development of many high-quality open-source LLMs, including but not limited to Open LLaMA, StableLM, and Pythia. You can fine-tune these models on a custom instruction dataset to adapt to your specific task, such as training a chatbot to answer financial questions.</p><p>Lightning AI recently launched Lit-Parrot, the second LLM implementation in the Lit-* series. The goal of these Lit-* series is to provide the AI/ML community with a clean, solid, and optimized implementation of large language models with pretraining and fine-tuning support using <a href="https://lightning.ai/pages/community/tutorial/lora-llm/">LoRA</a> and <a href="https://lightning.ai/pages/community/article/understanding-llama-adapters/">Adapter</a>.</p><p>We will guide you through the process step by step, from installation to model download and data preparation to fine-tuning. If you have already completed a step or are confident about it, feel free to skip it.</p><h2>Installing Lit-Parrot ðŸ¦œ</h2><p>The Lit-Parrot repository is available in the Lightning AI Github organization <a href="https://github.com/Lightning-AI/Lit-Parrot">here</a>. To get started, clone the repository and install its dependencies.</p><pre><code></code></pre><p>We are using <a href="https://github.com/HazyResearch/flash-attention">FlashAttention</a>, a fast and memory-efficient implementation of attention, which is only available in PyTorch Nightly 2.1 at the moment of writing this article.</p><pre><code></code></pre><p>Finally, install the dependencies using <code>pip install -r requirements.txt</code> .</p><h2>Downloading the model weights</h2><p>In order to use the model or fine-tune it we need a pre-trained weight. Thanks to the effort of open source teams, we have a bunch of open source weights that we can use for commercial purposes. Lit-Parrot being a GPT NeoX implementation</p><pre><code></code></pre><p><img decoding="async" src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/05/Untitled.png" alt="" width="780" height="273" srcset="https://lightningaidev.wpengine.com/wp-content/uploads/2023/05/Untitled.png 1866w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/05/Untitled-300x105.png 300w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/05/Untitled-1024x359.png 1024w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/05/Untitled-1536x538.png 1536w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/05/Untitled-300x105@2x.png 600w" sizes="(max-width: 780px) 100vw, 780px"/></p><p>You will see, <code>gpt_neox</code> layers being mapped to the Lit-Parrot layers in the terminal. After this step, you can find the downloaded weights in the <code>checkpoints/togethercomputer/RedPajama-INCITE-Base-3B-v1</code> folder.</p><h2>Prepare the dataset</h2><p>In this tutorial, we will use the <a href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm">Dolly 2.0 instruction dataset</a> by Databricks for fine-tuning. Finetuning involves two main steps- first, we process the dataset in the Lit-Parrot format and then we run the fine-tuning script on the processed dataset.</p><p>Instruction datasets typically have three keys: instruction, input (optional context for the given instruction), and the expected response from the LLM. Below is a sample example of instruction data:<br/></p><pre><code></code></pre><p>The <a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl">dolly 2.0 dataset</a> comes in <a href="https://jsonlines.org/">JSON Lines</a> format, which is plainly speaking a text file with rows of JSON data. It is a convenient format when processing one record at a time. The Dolly dataset contains the following keys â€“<br/></p><pre><code></code></pre></div></div></section></div>
  </body>
</html>
