<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://alanza.xyz/log/just-write-the-function/">Original</a>
    <h1>Just write the function!</h1>
    
    <div id="readability-page-1" class="page"><div><div id="remainder"><p>A pretty frequent Advent of Code metaphor involves having a little rectangular map that you are navigating or manipulating. The map is usually represented in the puzzle input as a rectangular grid of ASCII characters, with newlines, <code>&#39;\n&#39;</code>, delimiting each row of the grid.</p><h2>The “obvious” approach</h2><p>In my previous runs of Advent of Code, I dutifully parsed the map into some kind of 2D array structure, so that if I wanted to access the value at <code>(x,y)</code>, with (0,0) as the upper left-hand corner, I could do <code>const value = map[y][x];</code></p><p>There’s nothing wrong with this pattern, it’s just slow. Many people have made variants of this point in my hearing, but I think the two that stuck the best were <a href="https://www.youtube.com/watch?v=YQs6IC-vgmo" target="_blank">Bjarne Stroustrup</a> and <a href="https://www.youtube.com/watch?v=IroPQ150F6c" target="_blank">Andrew Kelley</a>. (The Stroustrup clip is shorter, and the Kelley talk really shows you how he was able to lean into this learning.) The short of it is that faithfully replicating in memory a multidimensional or nested structure that some data is trying to convey <strong>is often a mistake</strong> because it introduces indirection.</p><p>My understanding (which I want to underline is not expertise) is that your machine has some number of “registers”, each of which holds some small amount of data that can be accessed and manipulated on the level of or at the speed of single assembly instructions—very quickly. After that, computers can hold a good amount of <em>contiguous</em> data in various <em>caches</em> at a time. I’m told that there are various sizes of caches, each larger than the last and correspondingly slower to access. Slower than that is memory that is neither currently in a register nor in cache but somewhere in RAM, and slowest of all (while still on your physical machine) is memory on disk. When you access a bit of data, for example by executing the line <code>const value = map[y][x];</code> above, assuming that the data is not already in a register, the computer will attempt to “load” it from cache or if that operation fails (you have a “cache miss”), from RAM.</p><p>The point is that in the worst case, <code>map[y][x]</code> has to first load <code>map</code> in order to index into it as <code>map[y]</code> and then load <code>map[y]</code> to index into <em>that.</em></p><h2>Just write the function!</h2><p>But what if I do need to operate on the <code>(x,y)</code> coordinate of a point on the map? Well, why not determine those coordinates yourself? After all, they’re not hard to compute: if you store the map as a flat array of bytes, <code>[]const u8</code> in Zig, since each ASCII character takes up one byte, the <code>y</code>-coordinate is the number of newline characters before the byte-offset of the point you care about, and the <code>x</code>-coordinate is either the byte-offset, if <code>y</code> is zero, or the byte-offset minus one more than the byte-offset of the previous newline character.</p><p>If we rely on the assumption that the grid is rectangular, though, we don’t even need to actually count! In code:</p><pre><code><span>fn</span> <span>indexToCoordinates</span><span>(</span>
    <span>offset</span><span>:</span> <span>usize</span><span>,</span>
    <span>len</span><span>:</span> <span>usize</span><span>,</span>
    <span>line_length</span><span>:</span> <span>usize</span><span>,</span>
<span>)</span> <span>error</span><span>{</span> <span>Overflow</span><span>,</span> <span>Delimiter</span> <span>}</span><span>!</span><span>struct</span> <span>{</span> <span>usize</span><span>,</span> <span>usize</span> <span>}</span> <span>{</span>
    <span>if</span> <span>(</span><span>offset</span> <span>&gt;=</span> <span>len</span><span>)</span> <span>return</span> <span>error</span><span>.</span><span>Overflow</span><span>;</span>
    
    
    
    
    <span>const</span> <span>one_indexed</span> = <span>offset</span> <span>+</span> <span>1</span><span>;</span>
    
    
    <span>const</span> <span>one_indexed_x</span> = <span>one_indexed</span> <span>%</span> <span>line_length</span><span>;</span>
    <span>if</span> <span>(</span><span>one_indexed_x</span> <span>==</span> <span>0</span><span>)</span> <span>return</span> <span>error</span><span>.</span><span>Delimiter</span><span>;</span>
    
    <span>const</span> <span>y</span> = <span>@divFloor</span><span>(</span><span>one_indexed</span><span>,</span> <span>line_length</span><span>)</span><span>;</span>
    <span>return</span> <span>.</span><span>{</span> <span>one_indexed_x</span> <span>-</span> <span>1</span><span>,</span> <span>y</span> <span>}</span><span>;</span>
<span>}</span>

<span>const</span> <span>std</span> = <span>@import</span><span>(</span><span>&#34;std&#34;</span><span>)</span><span>;</span>
</code></pre>
<p>Of course, no need to take my word for it! Here’s a gist:</p>
<p>The functions “segmented” and “contiguous” perform exactly the same computation: We randomly generate many valid indices into input, which in the case of “segmented” is represented in memory as a 2D array, and in the case of “contiguous” is instead a flat array. Then we check whether the value at the input is the ASCII character <code>.</code> and if so add the x coordinate multiplied by the y coordinate. In “segmented” this is given to us, while in “contiguous” we must compute it.</p><p>The binary should be run with a name either containing the word “contiguous” or not. If so, calling it as, e.g. <code>./contiguous 08.txt 3</code> will use the input file <code>08.txt</code> as the map and run for <code>10 ^ 3 = 1000</code> iterations.</p><p>Using <a href="https://github.com/sharkdp/hyperfine" target="_blank">hyperfine</a>, and compiling the Zig file in <code>ReleaseFast</code> mode, I benchmarked the performance of these two functions for values of the iterations argument between 1 and 9. (Initially I intended to also run it for 10, but that seemed like it would take a long time.)</p><p>Here is what I found:</p><pre><code>hyperfine --parameter-scan repeats 1 9 &#34;./contiguous 08.txt {repeats}&#34; &#34;./segmented 08.txt {repeats}&#34; -N --warmup 10

Benchmark 1: ./contiguous 08.txt 1
  Time (mean ± σ):       2.5 ms ±   0.2 ms    [User: 0.9 ms, System: 1.4 ms]
  Range (min … max):     2.3 ms …   3.5 ms    1105 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet system without any interferences from other programs. It might help to use the &#39;--warmup&#39; or &#39;--prepare&#39; options.

Benchmark 2: ./segmented 08.txt 1
  Time (mean ± σ):       3.0 ms ±   0.2 ms    [User: 1.0 ms, System: 1.8 ms]
  Range (min … max):     2.7 ms …   4.2 ms    1000 runs

Benchmark 3: ./contiguous 08.txt 2
  Time (mean ± σ):       2.5 ms ±   0.2 ms    [User: 0.9 ms, System: 1.4 ms]
  Range (min … max):     2.3 ms …   3.7 ms    1222 runs

Benchmark 4: ./segmented 08.txt 2
  Time (mean ± σ):       3.0 ms ±   0.2 ms    [User: 1.0 ms, System: 1.8 ms]
  Range (min … max):     2.7 ms …   4.2 ms    967 runs

Benchmark 5: ./contiguous 08.txt 3
  Time (mean ± σ):       2.5 ms ±   0.2 ms    [User: 0.9 ms, System: 1.4 ms]
  Range (min … max):     2.3 ms …   4.0 ms    1217 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet system without any interferences from other programs. It might help to use the &#39;--warmup&#39; or &#39;--prepare&#39; options.

Benchmark 6: ./segmented 08.txt 3
  Time (mean ± σ):       3.0 ms ±   0.2 ms    [User: 1.0 ms, System: 1.8 ms]
  Range (min … max):     2.7 ms …   4.0 ms    992 runs

Benchmark 7: ./contiguous 08.txt 4
  Time (mean ± σ):       2.5 ms ±   0.2 ms    [User: 0.9 ms, System: 1.4 ms]
  Range (min … max):     2.3 ms …   3.6 ms    1080 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet system without any interferences from other programs. It might help to use the &#39;--warmup&#39; or &#39;--prepare&#39; options.

Benchmark 8: ./segmented 08.txt 4
  Time (mean ± σ):       3.1 ms ±   0.2 ms    [User: 1.0 ms, System: 1.8 ms]
  Range (min … max):     2.8 ms …   4.1 ms    1039 runs

Benchmark 9: ./contiguous 08.txt 5
  Time (mean ± σ):       2.8 ms ±   0.2 ms    [User: 1.2 ms, System: 1.4 ms]
  Range (min … max):     2.6 ms …   3.8 ms    1038 runs

Benchmark 10: ./segmented 08.txt 5
  Time (mean ± σ):       3.3 ms ±   0.2 ms    [User: 1.3 ms, System: 1.7 ms]
  Range (min … max):     3.1 ms …   4.5 ms    862 runs

Benchmark 11: ./contiguous 08.txt 6
  Time (mean ± σ):       5.2 ms ±   0.2 ms    [User: 3.5 ms, System: 1.4 ms]
  Range (min … max):     4.9 ms …   6.4 ms    585 runs

Benchmark 12: ./segmented 08.txt 6
  Time (mean ± σ):       6.4 ms ±   0.3 ms    [User: 4.3 ms, System: 1.8 ms]
  Range (min … max):     6.1 ms …   7.5 ms    420 runs

Benchmark 13: ./contiguous 08.txt 7
  Time (mean ± σ):      28.6 ms ±   0.3 ms    [User: 26.3 ms, System: 1.9 ms]
  Range (min … max):    27.8 ms …  29.7 ms    106 runs

Benchmark 14: ./segmented 08.txt 7
  Time (mean ± σ):      36.5 ms ±   0.4 ms    [User: 33.9 ms, System: 2.2 ms]
  Range (min … max):    35.8 ms …  37.6 ms    80 runs

Benchmark 15: ./contiguous 08.txt 8
  Time (mean ± σ):     259.2 ms ±   0.6 ms    [User: 254.6 ms, System: 4.0 ms]
  Range (min … max):   258.4 ms … 260.0 ms    11 runs

Benchmark 16: ./segmented 08.txt 8
  Time (mean ± σ):     335.7 ms ±   0.8 ms    [User: 329.9 ms, System: 5.1 ms]
  Range (min … max):   334.6 ms … 337.3 ms    10 runs

Benchmark 17: ./contiguous 08.txt 9
  Time (mean ± σ):      2.556 s ±  0.003 s    [User: 2.538 s, System: 0.016 s]
  Range (min … max):    2.552 s …  2.562 s    10 runs

Benchmark 18: ./segmented 08.txt 9
  Time (mean ± σ):      3.314 s ±  0.002 s    [User: 3.290 s, System: 0.021 s]
  Range (min … max):    3.312 s …  3.320 s    10 runs
</code></pre><p>As you can see, in all cases <code>./contiguous</code> runs faster—not by a ton, but measurably faster. For example, on the final iteration, the mean time for a run of <code>./segmented</code> took 1.296 times as long to complete as <code>./contiguous</code>. At 1,000,000,000 iterations, (and thanks to the <code>--warmup</code> call), you can be fairly sure that this difference is coming from the hot loop, rather than the different allocation pattern or costs from interacting with the file.</p><p>Initially, (and in my solution for today’s Advent of Code), I had an <code>indexToCoordinates</code> function which actually <em>counted</em> the newline characters on each call. Of course, this transforms an <code>O(1)</code> lookup call into an <code>O(n)</code> iteration calculation! Now, <code>n</code> is fixed and not super large, but each count has to index into the buffer and do a comparison, so there’s no reason to expect that this version of <code>indexToCoordinates</code> should be speedy. It took actually running <code>hyperfine</code> for me to realize that I could get a speedup by trusting the structure of the data.</p></div></div></div>
  </body>
</html>
