<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.brimdata.io/blog/super-structured-data/">Original</a>
    <h1>Super-Structured Data: Rethinking the Schema</h1>
    
    <div id="readability-page-1" class="page"><div>
  <article>
    
    <p>We all know why dealing with real-world data is so hard.
It‚Äôs a big, hairy mess.</p>
<p>While <em>cliche</em> nowadays, you‚Äôre no doubt familiar with the ‚Äú80/20 rule‚Äù
in data analytics, and probably even experienced it yourself:</p>
<blockquote>
<p>80% of your time is spent gathering, cleansing, and storing data,
while 20% of your time is spent actually analyzing it and getting real work done.</p>
</blockquote>
<p>You often end up stuck between the document model of JSON
and the relational model of SQL databases.  Going back and forth between the
two worlds is such a big headache.</p>
<p>Thank goodness there‚Äôs a new and better way.
Let‚Äôs get schemas and messy JSON out of our way.</p>
<p>It‚Äôs called <a href="https://zed.brimdata.io/docs/formats/#2-zed-a-super-structured-pattern"><em>super-structured data</em></a>.</p>
<p>Hold onto your hats.</p>

<p>The gold standard for data analytics is to ‚Äúcleanse‚Äù your messy JSON data and
organize it all in a data warehouse, where data must conform to relational
schemas so everything fits neatly into tables.</p>
<p>In this world, data must conform to the ‚Äúone true way‚Äù of the data warehouse.</p>
<p>Unanticipated data forms must be discarded or stored elsewhere until
changes can be made to the ‚Äúingest pipeline‚Äù and to the warehouse schemas to
accommodate any new shape of messy data.</p>
<p>A common trick is to make super wide tables with lots of ‚Äúnulls‚Äù
that can hold all of the different shapes
of data that might show up ‚Äî only to be foiled by a different form
of messy data that eventually doesn‚Äôt fit.</p>
<p>Somehow this approach to cleaning data feels a bit too forced.</p>
<p>Metaphorically speaking,
the relational model feels a lot like <em>authoritarianism</em>.</p>
<h2 id="the-anarchists-way">The Anarchist‚Äôs Way</h2>
<p>Around 2010, the NoSQL movement arose in reaction to this
schema-rigid authoritarianism.</p>
<p>In this approach, the database is ‚Äúschema-less‚Äù and
data of any shape can be stored anywhere in the database, typically structured
around the document model of JSON.</p>
<p>This <em>anything-goes</em> approach, however, often leads to quite a mess in real-world
deployments.  It is easy and tempting to allow any data in the system as requirements
evolve, leading to a mishmash of JSON data shapes that have to be teased apart
through ever more complex application logic.</p>
<p>Extending our metaphor,
the document model feels a lot like <em>anarchy</em>.</p>
<h2 id="how-did-we-get-here">How Did We Get Here?</h2>
<p>The authoritarians like to call the mishmash of anarchist‚Äôs JSON data a ‚Äúdata swamp‚Äù,
while the anarchists insist that it‚Äôs so much easier to get up and running with
a document database that it‚Äôs well worth coping with the potential mess.</p>
<p>Anarchy or authoritarianism?  Pick your poison.</p>
<p>You all know the history.</p>
<p>Back in the 1980s, the database wars came to an end
when SQL and the relational model emerged as the undeniable champions.</p>
<p>From there, SQL-based data warehouses appeared in the 1990s enabling the
new concept of business intelligence, while in the late 1990s,
the Internet and Web took off like a rocket.</p>
<p>Then, by the early 2000s, the predominance of Web-scale companies with tech stacks
built entirely from scratch led to <em>a massive proliferation of messy data</em>.
Unfortunately, the best warehouses
of that day simply couldn‚Äôt scale to the data demands of the Googles and the Yahoos.</p>
<p>Necessity is the mother of invention and those big Web companies soon developed
custom solutions for doing warehouse-style analytics across massive clusters
of commodity servers.  In 2004, Google published their
<a href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">influential paper on <em>MapReduce</em></a>, and Yahoo later released open-source software called
<a href="https://hadoop.apache.org/"><em>Hadoop</em></a> based on Google‚Äôs MapReduce programming model.</p>
<p>A bit later, researchers at UC Berkeley improved upon the Hadoop design
with <a href="https://spark.apache.org/">Spark</a>.</p>
<p>It was the dawn of Big Data. ü§Æ</p>
<h2 id="the-authoritarian-backlash">The Authoritarian Backlash</h2>
<p>No good deed goes unpunished, and rest assured in 2008,
DeWitt and Stonebraker <a href="https://homes.cs.washington.edu/~billhowe/mapreduce_a_major_step_backwards.html">famously ranted</a>
that MapReduce was</p>
<ul>
<li>‚Äúa giant step backwards‚Äù,</li>
<li>‚Äúa poor implementation‚Äù,</li>
<li>‚Äúnot novel at all‚Äù, and</li>
<li>‚Äúoverlooked the lessons of 40 years of database technology‚Äù.</li>
</ul>
<p>Of course, they were right.</p>
<p>But back then, the Web-scale anarchists couldn‚Äôt just go out
and purchase a sufficiently large authoritarian warehouse license to solve their ever-growing
problems with messy data.  Those data warehouses didn‚Äôt mesh with the
fast-moving anarchy of the day and weren‚Äôt economically viable at
the massive scale required.</p>
<p>It would be another decade before the worlds of big data and relational warehouses
truly began to converge.</p>
<h2 id="nosql-the-anarchists-database">NoSQL: The Anarchist‚Äôs Database</h2>
<p>In the meantime,
many application developers came to loathe the
<a href="https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping">object-relational mapping (ORM)</a>
pattern that required a complex layer of moving parts between their
dynamic and often messy application data and the authoritarian relational model.</p>
<p>Why couldn‚Äôt apps just write JSON data straight into a database?  That would be
so much easier.</p>
<p>So, along came the document-model database to the rescue.</p>
<p>Some of these systems like MongoDB
embraced a pure <a href="https://en.wikipedia.org/wiki/NoSQL">‚ÄúNoSQL‚Äù</a>
approach while others like CouchDB eventually
added a SQL-like query language based on
<a href="https://asterixdb.apache.org/docs/0.9.3/sqlpp/manual.html">SQL++</a>,
which extends SQL to operate over the document data model of JSON.</p>
<h2 id="a-cautious-reaction">A Cautious Reaction</h2>
<p>This NoSQL stuff got popular and the authoritarians spoke again.</p>
<p>A good eight years after the MapReduce rant,
Stonebraker softened his critique of the anarchists,
stating <a href="http://www.redbook.io/ch4-newdbms.html">in the Red Book</a>
that</p>
<blockquote>
<p>[NoSQL systems] are easy for a programmer to get going and do something productive.
RDBMSs, in contrast, are very heavyweight, requiring a schema up front.</p>
</blockquote>
<p>He concluded:</p>
<blockquote>
<p>This is a wake-up call to the commercial vendors to make systems
that are easier to use.</p>
</blockquote>
<p>So what‚Äôs happened in the half dozen years since these insights from the Red Book?</p>
<p>Well, <em>both</em> the document and relational models have continued to thrive and have been
firmly cemented into enterprise data stacks.
Just look at the market capitalizations
of all the companies involved (even after the bursting of Tech Bubble v2.0 in spring 2022).</p>
<p>While the anarchist NoSQL databases have managed to hold their own against the
authoritarian databases as the storage tier for many application deployments,
they‚Äôre not so hot at high-speed analytics and complex multi-dimensional
warehouse problems.</p>
<p>You may have seen a project or two moving data out of these systems and into
ClickHouse or a cloud warehouse when insurmountable scaling problems were hit.
Clearly, the schema-rigid relational model has won the battle of analytics,
and serves as the foundation for the modern cloud warehouse.</p>
<h2 id="schemas-go-viral">Schemas Go Viral</h2>
<p>Given these trends, the schema concept has made a big move out of the database, and
has become a fundamental design element that shows up everywhere these days.</p>
<p>If your data is going to land in a SQL warehouse, why not push schema enforcement
as far upstream as possible?  This way, the authoritarian data teams who
own the model definitions can impose constraints on the anarchist engineering teams to
prevent them from haphazardly creating messy data.</p>
<p>After all, those pesky engineers don‚Äôt understand the business value of data, right?
So best to put some handcuffs on them.  Isn‚Äôt authoritarian control so sweet?</p>
<p>To this end, schemas
lie at the heart of popular data formats like
<a href="https://avro.apache.org/docs/current/spec.html">Avro</a>
and
<a href="https://github.com/apache/parquet-format">Parquet</a>.
And, in the client-server realm,
<a href="https://developers.google.com/protocol-buffers">Protocol Buffers</a>
and <a href="https://thrift.apache.org/">Thrift</a> configure the schema directly into
the compiled implementations of the communicating end points.</p>
<p>But there‚Äôs a cost to pushing the authoritarian model upstream:
a schema-rigid architecture leads to fragile and brittle interdependencies
and implementing change can be difficult and time consuming.</p>
<p>You want to make a change?
Okay, update all your schema definitions, recompile everything, and redeploy.
Someone makes a seemingly innocuous change to a client data structure used on
your mobile app and your mission-critical data pipeline comes to
a screeching halt.  You know the fire drill.</p>
<p>While great for data modeling, schemas can really get in the way
when you‚Äôre just trying to move and store data.</p>
<p>It turns out central control of everything can make things hard.</p>
<h2 id="just-add-thrust">Just Add Thrust</h2>
<p>With enough thrust, pigs can fly, or so goes the saying.
So why not just throw more engineering at the schema problem?</p>
<p>And sure enough, a whole sub-industry has emerged to take your data from JSON cloud APIs
and put it into schemas.</p>
<p>The idea here is that
instead of manually creating schemas, what if the schemas were automatically
created for you?  When something doesn‚Äôt fit in a table, how about automatically
adding columns for the missing fields?</p>
<p>This schema-oriented way of thinking has led to a world where
schemas are a given and any impedance mismatch between
real-world, messy data and tabular schemas shall be solved with ever
more layers of software complexity and engineering.</p>
<h2 id="super-structured-data">Super-structured Data</h2>
<p>We asked ourselves a crazy question: could it be that we‚Äôve built everything
upon the wrong foundational primitives?</p>
<p>Maybe Stonebraker was right?  Maybe it‚Äôs the schemas that are getting in our way?</p>
<p>We realized this schemas-are-everywhere way of thinking is
like putting a square peg (JSON) in a round hole (relational tables).
Yes you can do it, but there‚Äôs nothing natural about it and having two distinct ways of
doing things creates friction and complexity that leads
to wasted time and increased cost.</p>
<p>Could mixing a little controlled anarchy into our authoritarianism perhaps be helpful?</p>
<p>After working on this problem for a couple years, we arrived upon the concept
of <em>super-structured data</em> guided by the following principle:</p>
<blockquote>
<p>Instead of pre-defining schemas to which all values must conform,
data should instead be self-describing and organized around a deep type system,
allowing each value to freely express its structure through its explicit type.</p>
</blockquote>
<p>With super-structured data, the mishmash of relational tables and semi-structured data
embedded in tables all turns into a well-defined set of values that all conform
to precisely defined <em>super-structured types</em>.  Both <em>JSON anarchy</em> and
<em>schema-rigid authoritarianism</em> are just special cases of the super-structured model.</p>
<p>In other words, super-structured data is a superset of both JSON and relational tables.
All JSON documents are super-structured values and any relational table can be represented
with a super-structured type.</p>
<p>For example, the JSON value</p>
<pre tabindex="0"><code>{&#34;s&#34;:&#34;foo&#34;,&#34;a&#34;:[1,&#34;bar&#34;]}
</code></pre><p>would traditionally be called ‚Äúschema-less‚Äù and in fact is said to have the vague type
‚Äúobject‚Äù in the world of JavaScript or ‚Äúdict‚Äù in the world of Python.
However, the super-structured interpretation of this value‚Äôs type is instead:</p>
<blockquote>
<p><em>type record with field <code>s</code> of type string and field <code>a</code>
of type array of type union of types integer and string</em></p>
</blockquote>
<p>We call the former style of typing a ‚Äúshallow‚Äù type system and the latter
style of typing a ‚Äúdeep‚Äù type system.  The hierarchy of a shallow-typed value
must be traversed to determine its structure whereas the structure of a
deeply-typed value is determined directly from its type.</p>
<p>So given a deep type system,
when a sequence of values in fact conforms to, say, a uniform ‚Äúrecord type‚Äù,
then such a collection of record values looks precisely like a relational table.
For example, the sequence of JSON values</p>
<pre tabindex="0"><code>{&#34;id&#34;:1,&#34;name&#34;:&#34;Alice&#34;}
{&#34;id&#34;:2,&#34;name&#34;:&#34;Bob&#34;}
{&#34;id&#34;:3,&#34;name&#34;:&#34;Carlos&#34;}
</code></pre><p>has a natural correspondence with a SQL table created by</p>
<pre tabindex="0"><code>CREATE TABLE contacts (
	id INTEGER,
	name TEXT
);
</code></pre><p>In this case, the rows of this table are typed as <em>type record
with field <code>id</code> of type integer and field <code>name</code> of type string</em>.</p>
<p>If we, in turn, employ named types as part of the super-structured
type system, we can instead create a type called ‚Äúcontacts‚Äù that looks just
like the SQL table:</p>
<pre tabindex="0"><code>type contacts {id:int64,name:string}
</code></pre><p>In the super-structured model, data is self-describing and we can employ
decorators to bind the name to the type as in</p>
<pre tabindex="0"><code>{id:1,name:&#34;Alice&#34;}(=contacts)
{id:2,name:&#34;Bob&#34;}(=contacts)
{id:3,name:&#34;Carlos&#34;}(=contacts)
</code></pre><p>Since the underlying type of <code>contacts</code> is implied by the value,
there is actually no need for an explicit type declaration.</p>
<p>Now the SQL statement</p>
<pre tabindex="0"><code>SELECT name FROM contacts WHERE id=2
</code></pre><p>could be interpreted either traditionally as a query for a row of a relational table
named <code>contacts</code>, or in terms of super-structured data,
as a query over a set of super-structured data where the FROM clause refers
to a first projection by type <code>contacts</code> and the SELECT clause refers to a second
projection of the column <code>name</code>.</p>
<p>In this way, anarchy and authoritarianism can live side by side with a single
data model and authoritarian tables can be projected from a pool of super-structured
data as a simple type query.</p>
<h2 id="hasnt-this-been-done-before">Hasn‚Äôt This Been Done Before?</h2>
<p>Surely, this concept of super-structured data isn‚Äôt rocket science.
Why don‚Äôt things already work this way?!
From a 10,000 foot view, these ideas feel familiar.</p>
<p>The <a href="https://github.com/edgedb/edgedb">EdgeDB</a> project advocates for
<a href="https://github.com/edgedb/edgedb#-types-not-tables-">‚Äútypes not tables‚Äù</a>,
which certainly rhymes with the
super-structured goal of using types instead of schemas to organize data.
And while EdgeDB‚Äôs type system is deeply typed, its storage layer
is just a traditional relational database.  While
this approach masterfully solves some important and thorny problems
(all while strategically reusing mature relational database technology),
it does not solve the
underlying data representation problem.  Instead, EdgeDB is essentially a new data silo
whose type system cannot be used to serialize data external to the system.</p>
<p>Okay, but can‚Äôt we get super-structured properties with other existing data formats?</p>
<p>Let‚Äôs have a look.</p>
<p>Even though JSON isn‚Äôt a candidate,
<a href="https://bsonspec.org/">BSON</a> and
<a href="https://amzn.github.io/ion-docs/">Ion</a> are efficient, binary cousins of JSON
and were created to provide a type-rich elaboration of the semi-structured model.
Unfortunately, both approaches have shallow type systems so they
are not a candidate for super-structured data.</p>
<p>But what about Parquet, Avro, or the hugely popular <a href="https://arrow.apache.org/">Arrow format</a>?</p>
<p>Indeed, these formats all have deep typing but are schema rigid:
an encoded sequence of values requires an up-front schema definition
and all of the values in the sequence must conform to that one schema.
Also, Parquet does not have union types, so mixed-type arrays and dictionaries
aren‚Äôt expressible, though this <a href="https://github.com/apache/parquet-format/pull/44">could be addressed</a>
in a future version of the format.</p>
<p>In a nutshell,</p>
<ul>
<li>JSON, BSON, and Ion are schema-less but have shallow typing, while</li>
<li>Parquet, Avro, and Arrow have deep typing but are schema rigid.</li>
</ul>
<p>Super-structured data, on the other hand, provides the best of both worlds:</p>
<blockquote>
<p>Super-structured data has deep types without schema rigidity.</p>
</blockquote>
<h2 id="schema-registries-to-the-rescue">Schema Registries to the Rescue</h2>
<p>Wait a minute. Can‚Äôt you solve the schema rigidity problem with a schema registry?</p>
<p>Indeed, a number of years ago, developers wanting to transmit
diversely typed sequences of data over a Kafka queue
clearly tripped over the problems of schema-rigid formats.</p>
<p>They needed a solution: why not just use a
<a href="https://docs.confluent.io/platform/current/schema-registry/index.html">schema registry</a>
to persist all the possible schemas in use?</p>
<p>In this approach, each transmitted value is tagged with a small-integer ‚Äúschema ID‚Äù
and the schema registry provides a centralized service for mapping these IDs
to the intended schema.
Consequently, a heterogeneous sequence of Avro or Protocol Buffers values
can be transmitted over a Kafka topic by prepending the schema ID to each
encoded value.  The receiver can then look up and cache each schema using the
ID and the schema registry.</p>
<p>When deployed with Avro, this schema-registry pattern begins to resemble
our model for super-structured data.
In particular, not only does Avro have a deep type system but it also
includes <em>union types</em>, which accommodates
multi-typed arrays and tuples.  And it has a <em>null type</em>,
which when combined with a union type can represent optional values in
a record (or JSON object) just like optional values in a relational column.</p>
<p>Given all this, Avro with a schema registry comes closest to
our concept of super-structured data.
However, the schema-registry service not only creates operational overhead,
but makes the approach entirely unsuitable for a self-contained format
for data serialization.
Without live, online access to the schema registry, a client of this approach
cannot decode any Avro-encoded payload.</p>
<p>In short, a schema registry creates a parallel universe problem: everything
is organized around schemas so data in flight and data at rest must both
conform to the same set of schemas.  When data at rest resides in a
relational database, we now have to keep the tables in the database
consistent with the schemas in the separate registry service.</p>
<p>The schemas are getting in the way again.  What a mess!</p>
<h2 id="the-zed-project">The Zed Project</h2>
<p>To tackle the myriad of challenges with schema-rigid authoritarianism
juxtaposed with JSON anarchy,
our small team at <a href="https://www.brimdata.io/">Brim Data</a>
has been developing, iterating, and refining the ideas for super-structured
data under the umbrella of <a href="https://zed.brimdata.io/docs/">The Zed Project</a>.</p>
<p>At the foundation of Zed, we‚Äôve developed a family of super-structured formats
that all adhere to a common <a href="https://zed.brimdata.io/docs/formats/zed/">Zed data model</a>.
The super-structured formats include</p>
<ul>
<li><a href="https://zed.brimdata.io/docs/formats/zson/">ZSON</a> - a human-readable format based on Zed as a superset of JSON</li>
<li><a href="https://zed.brimdata.io/docs/formats/zng/">ZNG</a> - an efficient binary, format based on Zed and analogous to Avro</li>
<li><a href="https://zed.brimdata.io/docs/formats/zst/">ZST</a> - an efficient columnar format based on Zed and analogous to Parquet</li>
</ul>
<p>A novel advantage to this design is that one cohesive data model supports the
three important variations of serialized data:</p>
<ul>
<li>a human readable form for easy interpretation,</li>
<li>an efficient sequence form for search, and</li>
<li>an efficient columnar form for vectorized analytics.</li>
</ul>
<p>Zed is the first system to unite these three models with a unified set of
formats where converting between the various forms incurs <em>no loss of information</em>.</p>
<p>To crack the problem of efficiently representing super-structured types
across a sequence of values,
the ZNG and ZST formats utilize a concept called a
<a href="https://zed.brimdata.io/docs/formats/zng/#1-introduction"><em>type context</em></a>.
A type context allows us to replace a <em>globally scoped</em> schema registry
with <em>locally scoped</em> type definitions that are embedded within the data sequence itself.
Types need only be
<a href="https://zed.brimdata.io/docs/formats/zng/#21-types-frame">defined once</a>
and can then be reused.  And the type context can always be
<a href="https://zed.brimdata.io/docs/formats/zng/#24-end-of-stream">‚Äúreset‚Äù</a>
within large files or data streams so they can be seekable or
fragmented into independently decodable chunks.  Moreover, values can be moved
from one context to another with a fast and simple
<a href="https://github.com/brimdata/zed/blob/e79bc75889174d56e2a06771c14183d550f368c8/mapper.go#L32">table lookup</a>.</p>
<p>At this point,
you might wonder why create Zed and these formats
in the first place?  Rest assured, we didn‚Äôt just set out to work on
super-structured data for its own sake.</p>
<p>Necessity is the mother of invention and our journey to super-structured data
started when we found it hard to
retain the rich and deeply typed event information from <a href="https://zeek.org/">Zeek logs</a>
without force fitting heterogenous log data into warehouse tables
or dumbing down Zeek events into JSON for storage in document-oriented search systems.
We also realized that in order to do both search and analytics well, you had to
stand up two systems: search systems aren‚Äôt very good at analytics and warehouse
systems aren‚Äôt very good at search.</p>
<p>To this end, we began prototyping these ideas in a command-line tool called
<a href="https://zed.brimdata.io/docs/commands/zq/"><code>zq</code></a>,
which is <a href="https://www.brimdata.io/blog/introducing-zq/">like jq</a>,
but of course operates upon super-structured Zed
data instead of just JSON and has easy-to-use search built in.
Also, since Zed is a superset of other data models,
we‚Äôve included support in <code>zq</code> for reading and writing data in other formats
like JSON, CSV, and Parquet.</p>
<p>Our vision is that super-structured data should make it really easy to
scale down search and analytics
to your laptop, or scale up to a large-scale cloud deployment of a
data lake based on Zed, i.e., ‚ÄúZed lake‚Äù.  Thus, we‚Äôve been developing a
<a href="https://zed.brimdata.io/docs/lake/format/">lake format</a> based on Zed,
which is managed and served by another command-line tool
simply called <a href="https://zed.brimdata.io/docs/commands/zed/"><code>zed</code></a>.</p>
<p>A Zed lake is sort of like a lakehouse but is based on super-structured data,
requires no schema definitions, and has a user-friendly, history-navigable
commit model like <a href="https://git-scm.com/">Git</a>.
Our work on Zed lakes is less mature than <code>zq</code> and the Zed formats,
but the lake implementation has already proven robust enough to run in production
at a non-trivial scale by many of our community users.</p>
<p>To take advantage of the Zed data model, we have also developed a new search,
query and data-transformation language
that we simply call the <a href="https://zed.brimdata.io/docs/language/">‚ÄúZed Language‚Äù</a>.
The Zed language is the primary means to interact with <code>zq</code> and the
<a href="https://zed.brimdata.io/docs/commands/zed/#211-query"><code>zed query</code></a> commands.</p>
<p>To be honest,
we struggled a bit as to whether we should just embrace SQL as the query interface.
Does the world really need yet another query language?</p>
<p>Yet the problem with SQL for our use case is that it‚Äôs simply an awful
user experience for search.
Many of our community users use Zed in a lean-forward style of interactive keyword search
with a certain amount of lightweight analytics.  Forcing these users to switch
to SQL would be a major step back for them.</p>
<p>In the end, we decided to continue to develop the Zed language and explore
the audacious goal of blending keyword search, warehouse-style analytics,
data exploration primitives, and data transformation logic all in one
unified language.  This might sound a bit crazy, but we think we‚Äôre onto something here.</p>
<p>In the long run, we‚Äôll no doubt support
a dedicated SQL query engine that can operate on virtualized SQL tables
projected from Zed types, but for now, our team is small and we‚Äôre exploring
how far we can go with the Zed language.</p>
<p>Finally, we‚Äôve built a desktop application called
<a href="https://github.com/brimdata/brim">‚ÄúBrim‚Äù</a>
that provides an interactive search, analytics, and exploration experience
for Zed data.  Through its integrations with Zeek and <a href="https://suricata.io/">Suricata</a>,
many of our community users rely upon Brim for threat hunting and
incident response.  Other users have implemented ETL pipelines
in Zed and monitor and debug their pipelines using Brim.  Some of our
other users leverage Brim for
<a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">exploratory data analysis</a>
when trying to decipher large, complex JSON objects that were produced elsewhere
in their organization.</p>
<p>The Brim app utilizes the
<a href="https://github.com/brimdata/brim/tree/main/packages/zealot">Zealot library</a>
to bring super-structured data and the Zed data model
to the JavaScript world.
We don‚Äôt aspire for Brim to be a notebook, but rather have leveraged Zealot
to explore some initial integrations with notebook systems like
<a href="https://observablehq.com/">Observable</a>.
In a future article, we‚Äôll write about our Observable integration.</p>
<h2 id="try-it-out">Try it out</h2>
<p>If you‚Äôd like to try Zed and Brim, it‚Äôs all pretty easy.  You can:</p>
<ul>
<li><a href="https://zed.brimdata.io/docs/">check out our docs</a>,</li>
<li><a href="https://zed.brimdata.io/docs/install/">install the Zed software</a>,</li>
<li><a href="https://github.com/brimdata/brim/wiki/Installation">install Brim</a>, and</li>
<li><a href="https://www.brimdata.io/join-slack/">join our public Slack team</a>.</li>
</ul>
<p>We love working with all our users to help guide us to the best ways of solving
your real, everyday problems. Give us a holler and we look forward to chatting.</p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>It‚Äôs hard to make data easy and the jury is certainly out on Zed, but let‚Äôs
see how far we can get.</p>
<p>Let‚Äôs see if we can use the Zed type system to get schemas out of our way.</p>
<p>Let‚Äôs see if we can do better than shaving the hard edges off
JSON‚Äôs square peg to fit in the round hole of relational schemas and dataframes.</p>
<p>Maybe, just maybe,
by mixing a bit of controlled anarchy into the world of schema-rigid
authoritarianism, Zed can make data engineering much, much easier after all.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p><a href="https://www.linkedin.com/in/noahtreuhaft/">Noah Treuhaft</a>
coined the awesomely perfect term <em>super-structured data</em> to describe
what we‚Äôve been working on.</p>
<p><a href="https://www.linkedin.com/in/garrisonhess/">Garrison Hess</a>
came up with the clever metaphor
of ‚Äúanarchy vs. authoritarianism‚Äù as a reaction to the design motivation of Zed.</p>

  </article>
</div></div>
  </body>
</html>
