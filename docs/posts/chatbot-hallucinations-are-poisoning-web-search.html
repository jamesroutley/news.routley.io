<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.wired.com/story/fast-forward-chatbot-hallucinations-are-poisoning-web-search/">Original</a>
    <h1>Chatbot Hallucinations Are Poisoning Web Search</h1>
    
    <div id="readability-page-1" class="page"><div><div data-journey-hook="client-content" data-testid="BodyWrapper"><div><p><span>Web search is</span> such a routine part of daily life that it’s easy to forget how marvelous it is. Type into a little text box and a complex array of technologies—vast data centers, ravenous web crawlers, and stacks of algorithms that poke and parse a query—spring into action to serve you a simple set of relevant results.</p><p>At least, that’s the idea. The age of <a href="https://www.wired.com/tag/artificial-intelligence/">generative AI</a> threatens to sprinkle epistemological sand into the gears of web search by fooling algorithms designed for a time when the web was mostly written by humans.</p><p>Take what I learned this week about Claude Shannon, the brilliant mathematician and engineer known especially for his work on <a data-offer-url="https://en.wikipedia.org/wiki/Information_theory" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://en.wikipedia.org/wiki/Information_theory&#34;}" href="https://en.wikipedia.org/wiki/Information_theory" rel="nofollow noopener" target="_blank">information theory</a> in the 1940s. Microsoft’s Bing search engine informed me that he had also foreseen the appearance of search algorithms, describing a 1948 research paper by Shannon called “A Short History of Searching” as “a seminal work in the field of computer science outlining the history of search algorithms and their evolution over time.”</p><p>Like a good AI tool, Bing also offers a few citations to show that it has checked its facts.</p><figure><p><span><p>Microsoft&#39;s Bing search engine served up this information about a research paper mathematician Claude Shannon never wrote as if it were true.</p>
</span><span>Microsoft via Will Knight</span></p></figure><p>There is just one big problem: Shannon did not write any such paper, and the citations offered by Bing consist of fabrications—or “hallucinations” in generative AI parlance—by two chatbots, <a data-offer-url="https://pi.ai/s/v49ubm6cg7yQcMaybKgST" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://pi.ai/s/v49ubm6cg7yQcMaybKgST&#34;}" href="https://pi.ai/s/v49ubm6cg7yQcMaybKgST" rel="nofollow noopener" target="_blank">Pi from Inflection AI</a> and <a data-offer-url="https://poe.com/s/NP7t5G7oZVuz4mzdLVcp" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://poe.com/s/NP7t5G7oZVuz4mzdLVcp&#34;}" href="https://poe.com/s/NP7t5G7oZVuz4mzdLVcp" rel="nofollow noopener" target="_blank">Claude from Anthropic</a>.</p><p>This generative-AI trap that caused Bing to offer up untruths was laid—purely by accident—by <a data-offer-url="https://danielsgriffin.com/" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://danielsgriffin.com/&#34;}" href="https://danielsgriffin.com/" rel="nofollow noopener" target="_blank">Daniel Griffin</a>, who recently finished a PhD on web search at UC Berkeley. In July he <a data-offer-url="https://danielsgriffin.com/weblinks/2023/07/05/a-short-history-of-searching.html" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://danielsgriffin.com/weblinks/2023/07/05/a-short-history-of-searching.html&#34;}" href="https://danielsgriffin.com/weblinks/2023/07/05/a-short-history-of-searching.html" rel="nofollow noopener" target="_blank">posted the fabricated responses</a> from the bots on his blog. Griffin had instructed both bots, “Please summarize Claude E. Shannon’s ‘A Short History of Searching’ (1948)”. He thought it a nice example of the kind of query that brings out the worst in large language models, because it asks for information that is similar to existing text found in its training data, encouraging the models to make very confident statements. Shannon did write an incredibly <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">important article</a> in 1948 titled “A Mathematical Theory of Communication,” which helped <a href="https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/">lay the foundation</a> for the field of information theory.</p><p>Last week, Griffin discovered that his blog post and the links to these chatbot results had inadvertently poisoned Bing with false information. On a whim, he tried feeding the same question into Bing and discovered that the chatbot hallucinations he had induced were highlighted above the search results in the same way as facts drawn from Wikipedia might be. “It gives no indication to the user that several of these results are actually sending you straight to conversations people have with LLMs,” Griffin says. (Although WIRED could initially replicate the troubling Bing result, after an enquiry was made to Microsoft it appears to have been resolved.)</p><p>Griffin’s accidental experiment shows how the rush to deploy ChatGPT-style AI is tripping up even the companies most familiar with the technology. And how the flaws in these impressive systems can harm services that millions of people use every day.</p></div></div></div></div>
  </body>
</html>
