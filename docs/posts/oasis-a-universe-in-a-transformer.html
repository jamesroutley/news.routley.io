<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://oasis-model.github.io/">Original</a>
    <h1>Oasis: A Universe in a Transformer</h1>
    
    <div id="readability-page-1" class="page"><div><p>We&#39;re excited to announce Oasis, the first playable, realtime, open-world AI model. It&#39;s a video game, but entirely generated by AI. Oasis is the first step in our research towards more complex interactive worlds. </p><p>Oasis takes in user keyboard input and generates real-time gameplay, including physics, game rules, and graphics. You can move around, jump, pick up items, break blocks, and more. There is no game engine; just a foundation model.</p><p>We believe fast transformer inference is the missing link to making generative video a reality. Using Decart&#39;s inference engine, we show that real-time video is possible. When Etched&#39;s transformer ASIC, Sohu, is released, we can run models like Oasis in 4K. Today, we&#39;re releasing Oasis&#39;s code, the weights of a 500M parameter model you can run locally, and a live playable demo of a larger checkpoint.</p><h2>Gameplay Results</h2><p>Oasis understands complex game mechanics, such as building, lighting physics, inventory management, object understanding, and more.</p><p>Oasis outputs a diverse range of settings, locations, and objects. This versatility gives us confidence that Oasis can be adapted to generate a wide range of new maps, games, features, and modifications with limited additional training.</p><p>Oasis is an impressive technical demo, but we believe this research will enable an exciting new generation of foundation models and consumer products. For example, rather than being controlled by actions, a game controlled completely by text, audio, or other modalities.</p><h2>Architecture</h2><p>The model is composed of two parts: a spatial autoencoder, and a latent diffusion backbone. Both are Transformer-based: the autoencoder is based on ViT<sup>[1]</sup>, and the backbone is based on DiT<sup>[2]</sup>. Contrasting from recent action-conditioned world models such as GameNGen<sup>[3]</sup> and DIAMOND<sup>[4]</sup>, we chose Transformers to ensure stable, predictable scaling, and fast inference on Etched&#39;s Transformer ASIC, Sohu.</p><p><img alt="Architecture" loading="lazy" width="1000" height="500" decoding="async" data-nimg="1" src="https://oasis-model.github.io/arch_new.png"/></p><p>In contrast to bidirectional models such as Sora<sup>[5]</sup>, Oasis generates frames autoregressively, with the ability to condition each frame on game input. This enables users to interact with the world in real-time. The model was trained using Diffusion Forcing<sup>[6]</sup>, which denoises with independent per-token noise levels, and allows for novel decoding schemes such as ours. We train on a subset of open-source Minecraft video data collected by OpenAI<sup>[9]</sup>.</p><p>One issue we focused on is temporal stability--making sure the model outputs make sense over long time horizons. In autoregressive models, errors compound, and small imperfections can quickly snowball into glitched frames. Solving this required innovations in long-context generation.</p><p>We solved this by deploying dynamic noising, which adjusts inference-time noise on a schedule, injecting noise in the first diffusion forward passes to reduce error accumulation, and gradually removing noise in the later passes so the model can find and persist high-frequency details in previous frames for improved consistency. Since our model saw noise during training, it learned to successfully deal with noisy samples at inference.</p><p><img alt="Dynamic Noising" loading="lazy" width="700" height="500" decoding="async" data-nimg="1" src="https://oasis-model.github.io/dyno.png"/></p><p>To learn more about the engineering underlying this model, and some of the specific optimizations in training and inference, check out the <a href="https://www.decart.ai/articles/oasis-interactive-ai-video-game-model">Decart blog post</a>.</p><h2>Performance</h2><p>Oasis generates real-time output in 20 frames per second. Current state-of-the-art text-to-video models with a similar DiT architecture (e.g. Sora<sup>[5]</sup>, Mochi-1<sup>[7]</sup> and Runway<sup>[8]</sup>) can take 10-20 seconds to create just one second of video, even on multiple GPUs. In order to match the experience of playing a game, however, our model must generate a new frame every 0.04 seconds, which is over 100x faster.</p><section><img alt="Performance" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" src="https://oasis-model.github.io/speed.png"/></section><p>With Decart&#39;s inference stack, the model runs at playable framerates, unlocking real-time interactivity for the first time. Read more about it on <a href="https://www.decart.ai/articles/oasis-interactive-ai-video-game-model">Decart&#39;s blog</a>.</p><p>However, to make the model an additional order of magnitude faster, and make it cost-efficient to run at scale, new hardware is needed. Oasis is optimized for Sohu, the Transformer ASIC built by Etched. Sohu can scale to massive 100B+ next-generation models in 4K resolution.</p><p>In addition, Oasis&#39; end-to-end Transformer architecture makes it extremely efficient on Sohu, which can serve &gt;10x more users even on 100B+ parameter models. We believe the price of serving models like Oasis is the hidden bottleneck to releasing generative video in production. See more performance figures and read more about Oasis and Sohu on <a href="https://etched.ai/blog">Etched&#39;s blog</a>.</p><section><img alt="Performance" loading="lazy" width="500" height="500" decoding="async" data-nimg="1" src="https://oasis-model.github.io/users.png"/></section><h2>Future Explorations</h2><p> With the many exciting results, there come areas for future development in the model. There are difficulties with the sometimes fuzzy video in the distance, the temporal consistency of uncertain objects, domain generalization, precise control over inventories, and difficulties over long contexts.</p><p>Following an in-depth sensitivity analysis on different configurations of the architecture alongside the data and model size, we hypothesize that the majority of these aspects may be addressed through scaling of the model and the datasets. Therefore, we are currently developing this direction alongside additional optimization techniques in order to enable such large-scale training efficiently. Further, once these larger models are developed, new breakthroughs in inferencing technology would be required in order to ensure a sustainable latency and cost trade-off. If you&#39;re interested in collaborating, reach out to <a href="mailto:tal@decart.ai">tal@decart.ai</a> and <a href="mailto:robert@etched.com">robert@etched.com</a>.</p></div></div>
  </body>
</html>
