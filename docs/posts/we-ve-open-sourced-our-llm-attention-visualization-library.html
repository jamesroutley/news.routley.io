<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/labmlai/inspectus">Original</a>
    <h1>Show HN: We&#39;ve open-sourced our LLM attention visualization library</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://badge.fury.io/py/inspectus" rel="nofollow"><img src="https://camo.githubusercontent.com/cc28cd06f948f58f5b661dccfa49e953b2372438632fdbbb78424db4c9ced364/68747470733a2f2f62616467652e667572792e696f2f70792f696e737065637475732e737667" alt="PyPI - Python Version" data-canonical-src="https://badge.fury.io/py/inspectus.svg"/></a>
<a href="https://pepy.tech/project/inspectus" rel="nofollow"><img src="https://camo.githubusercontent.com/cae2584709fd3b5f282f4a90d9caab6c6f27518ae95a26d8cbcc285a5adcd8ba/68747470733a2f2f706570792e746563682f62616467652f696e73706563747573" alt="PyPI Status" data-canonical-src="https://pepy.tech/badge/inspectus"/></a>
<a href="https://twitter.com/labmlai?ref_src=twsrc%5Etfw" rel="nofollow"><img src="https://camo.githubusercontent.com/abb6eb03b067ce40d1df3948e6adcdbae72393615d3a01c1c85cec4b3df794e2/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6c61626d6c61693f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/labmlai?style=social"/></a></p>

<p dir="auto">Inspectus is a versatile visualization tool for large language models.
It runs smoothly in Jupyter notebooks via an easy-to-use Python API. Inspectus provides multiple views, offering diverse insights into language model behaviors.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://organicdonut.com/labmlai/inspectus/blob/main/Images/preview.gif"><img src="https://organicdonut.com/labmlai/inspectus/raw/main/Images/preview.gif" alt="Inspectus" data-animated-image=""/></a></p>
<p dir="auto"><em>Click a token to select it and deselect others. Clicking again will select all again.
To change the state of only one token, do shift+click</em></p>

<p dir="auto"><strong>Attention Matrix</strong>:
Visualizes the attention scores between tokens, highlighting how each token focuses on others during processing.</p>
<p dir="auto"><strong>Query Token Heatmap</strong>:
Shows the sum of attention scores between each query and selected key tokens</p>
<p dir="auto"><strong>Key Token Heatmap</strong>:
Shows the sum of attention scores between each key and selected query tokens</p>
<p dir="auto"><strong>Dimension Heatmap</strong>:
Shows the sum of attention scores for each item in dimensions (Layers and Heads) normalized over the dimension.</p>




<p dir="auto">Import the library</p>
<div dir="auto" data-snippet-clipboard-copy-content="from inspectus import attention"><pre><span>from</span> <span>inspectus</span> <span>import</span> <span>attention</span></pre></div>
<p dir="auto">Simple usage</p>
<div dir="auto" data-snippet-clipboard-copy-content="# attn: Attention map; a 2-4D tensor or attention maps from Huggingface transformers
attention(attn, tokens)"><pre><span># attn: Attention map; a 2-4D tensor or attention maps from Huggingface transformers</span>
<span>attention</span>(<span>attn</span>, <span>tokens</span>)</pre></div>
<p dir="auto">For different query and key tokens</p>
<div dir="auto" data-snippet-clipboard-copy-content="attention(attns, query_tokens, key_tokens)"><pre><span>attention</span>(<span>attns</span>, <span>query_tokens</span>, <span>key_tokens</span>)</pre></div>
<p dir="auto">For detailed API documentation, please refer to the <a href="https://organicdonut.com/labmlai/inspectus/blob/main">official documentation - wip</a>.</p>


<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig
import torch
from inspectus import attention

# Initialize the tokenizer and model
context_length = 128
tokenizer = AutoTokenizer.from_pretrained(&#34;huggingface-course/code-search-net-tokenizer&#34;)

config = AutoConfig.from_pretrained(
    &#34;gpt2&#34;,
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)

model = GPT2LMHeadModel(config)

# Tokenize the input text
text= &#39;The quick brown fox jumps over the lazy dog&#39;
tokenized = tokenizer(
    text,
    return_tensors=&#39;pt&#39;,
    return_offsets_mapping=True
)
input_ids = tokenized[&#39;input_ids&#39;]

tokens = [text[s: e] for s, e in tokenized[&#39;offset_mapping&#39;][0]]

with torch.no_grad():
    res = model(input_ids=input_ids.to(model.device), output_attentions=True)

# Visualize the attention maps using the Inspectus library
attention(res[&#39;attentions&#39;], tokens)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span>, <span>GPT2LMHeadModel</span>, <span>AutoConfig</span>
<span>import</span> <span>torch</span>
<span>from</span> <span>inspectus</span> <span>import</span> <span>attention</span>

<span># Initialize the tokenizer and model</span>
<span>context_length</span> <span>=</span> <span>128</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>&#34;huggingface-course/code-search-net-tokenizer&#34;</span>)

<span>config</span> <span>=</span> <span>AutoConfig</span>.<span>from_pretrained</span>(
    <span>&#34;gpt2&#34;</span>,
    <span>vocab_size</span><span>=</span><span>len</span>(<span>tokenizer</span>),
    <span>n_ctx</span><span>=</span><span>context_length</span>,
    <span>bos_token_id</span><span>=</span><span>tokenizer</span>.<span>bos_token_id</span>,
    <span>eos_token_id</span><span>=</span><span>tokenizer</span>.<span>eos_token_id</span>,
)

<span>model</span> <span>=</span> <span>GPT2LMHeadModel</span>(<span>config</span>)

<span># Tokenize the input text</span>
<span>text</span><span>=</span> <span>&#39;The quick brown fox jumps over the lazy dog&#39;</span>
<span>tokenized</span> <span>=</span> <span>tokenizer</span>(
    <span>text</span>,
    <span>return_tensors</span><span>=</span><span>&#39;pt&#39;</span>,
    <span>return_offsets_mapping</span><span>=</span><span>True</span>
)
<span>input_ids</span> <span>=</span> <span>tokenized</span>[<span>&#39;input_ids&#39;</span>]

<span>tokens</span> <span>=</span> [<span>text</span>[<span>s</span>: <span>e</span>] <span>for</span> <span>s</span>, <span>e</span> <span>in</span> <span>tokenized</span>[<span>&#39;offset_mapping&#39;</span>][<span>0</span>]]

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>res</span> <span>=</span> <span>model</span>(<span>input_ids</span><span>=</span><span>input_ids</span>.<span>to</span>(<span>model</span>.<span>device</span>), <span>output_attentions</span><span>=</span><span>True</span>)

<span># Visualize the attention maps using the Inspectus library</span>
<span>attention</span>(<span>res</span>[<span>&#39;attentions&#39;</span>], <span>tokens</span>)</pre></div>
<p dir="auto">Check out the notebook here: <a href="https://organicdonut.com/labmlai/inspectus/blob/main/notebooks/gpt2.ipynb">Huggingface Tutorial</a></p>

<div dir="auto" data-snippet-clipboard-copy-content="import numpy as np
from inspectus import attention

# 2D attention representing attention values between Query and Key tokens
attn = np.random.rand(3, 3)

# Visualize the attention values using the Inspectus library
# The first argument is the attention matrix
# The second argument is the list of query tokens
# The third argument is the list of key tokens
attention(arr, [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;d&#39;, &#39;e&#39;, &#39;f&#39;])"><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>inspectus</span> <span>import</span> <span>attention</span>

<span># 2D attention representing attention values between Query and Key tokens</span>
<span>attn</span> <span>=</span> <span>np</span>.<span>random</span>.<span>rand</span>(<span>3</span>, <span>3</span>)

<span># Visualize the attention values using the Inspectus library</span>
<span># The first argument is the attention matrix</span>
<span># The second argument is the list of query tokens</span>
<span># The third argument is the list of key tokens</span>
<span>attention</span>(<span>arr</span>, [<span>&#39;a&#39;</span>, <span>&#39;b&#39;</span>, <span>&#39;c&#39;</span>], [<span>&#39;d&#39;</span>, <span>&#39;e&#39;</span>, <span>&#39;f&#39;</span>])</pre></div>
<p dir="auto">Check out the notebook here: <a href="https://organicdonut.com/labmlai/inspectus/blob/main/notebooks/custom_attn.ipynb">Custom attention map tutorial</a></p>
</article></div></div>
  </body>
</html>
