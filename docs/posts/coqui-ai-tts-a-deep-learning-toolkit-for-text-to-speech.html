<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/coqui-ai/TTS">Original</a>
    <h1>Coqui.ai TTS: A Deep Learning Toolkit for Text-to-Speech</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<ul dir="auto">
<li>📣 ⓍTTSv2 is here with 16 languages and better performance across the board.</li>
<li>📣 ⓍTTS fine-tuning code is out. Check the <a href="https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech">example recipes</a>.</li>
<li>📣 ⓍTTS can now stream with &lt;200ms latency.</li>
<li>📣 ⓍTTS, our production TTS model that can speak 13 languages, is released <a href="https://coqui.ai/blog/tts/open_xtts" rel="nofollow">Blog Post</a>, <a href="https://huggingface.co/spaces/coqui/xtts" rel="nofollow">Demo</a>, <a href="https://tts.readthedocs.io/en/dev/models/xtts.html" rel="nofollow">Docs</a></li>
<li>📣 <a href="https://github.com/suno-ai/bark">🐶Bark</a> is now available for inference with unconstrained voice cloning. <a href="https://tts.readthedocs.io/en/dev/models/bark.html" rel="nofollow">Docs</a></li>
<li>📣 You can use <a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms">~1100 Fairseq models</a> with 🐸TTS.</li>
<li>📣 🐸TTS now supports 🐢Tortoise with faster inference. <a href="https://tts.readthedocs.io/en/dev/models/tortoise.html" rel="nofollow">Docs</a></li>
</ul>

<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">💬 Where to ask questions</h2><a id="user-content--where-to-ask-questions" aria-label="Permalink: 💬 Where to ask questions" href="#-where-to-ask-questions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Please use our dedicated channels for questions and discussion. Help is much more valuable if it&#39;s shared publicly so that more people can benefit from it.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Platforms</th>
</tr>
</thead>
<tbody>
<tr>
<td>🚨 <strong>Bug Reports</strong></td>
<td><a href="https://github.com/coqui-ai/tts/issues">GitHub Issue Tracker</a></td>
</tr>
<tr>
<td>🎁 <strong>Feature Requests &amp; Ideas</strong></td>
<td><a href="https://github.com/coqui-ai/tts/issues">GitHub Issue Tracker</a></td>
</tr>
<tr>
<td>👩‍💻 <strong>Usage Questions</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/discussions">GitHub Discussions</a></td>
</tr>
<tr>
<td>🗯 <strong>General Discussion</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/discussions">GitHub Discussions</a> or <a href="https://discord.gg/5eXr5seRrv" rel="nofollow">Discord</a></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Type</th>
<th>Links</th>
</tr>
</thead>
<tbody>
<tr>
<td>💼 <strong>Documentation</strong></td>
<td><a href="https://tts.readthedocs.io/en/latest/" rel="nofollow">ReadTheDocs</a></td>
</tr>
<tr>
<td>💾 <strong>Installation</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/tree/dev#installation">TTS/README.md</a></td>
</tr>
<tr>
<td>👩‍💻 <strong>Contributing</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a></td>
</tr>
<tr>
<td>📌 <strong>Road Map</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/issues/378" data-hovercard-type="issue" data-hovercard-url="/coqui-ai/TTS/issues/378/hovercard">Main Development Plans</a></td>
</tr>
<tr>
<td>🚀 <strong>Released Models</strong></td>
<td><a href="https://github.com/coqui-ai/TTS/releases">TTS Releases</a> and <a href="https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models">Experimental Models</a></td>
</tr>
<tr>
<td>📰 <strong>Papers</strong></td>
<td><a href="https://github.com/erogol/TTS-papers">TTS Papers</a></td>
</tr>
</tbody>
</table>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png"><img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png" width="800"/></a></p>
<p dir="auto">Underlined &#34;TTS*&#34; and &#34;Judy*&#34; are <strong>internal</strong> 🐸TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.</p>

<ul dir="auto">
<li>High-performance Deep Learning models for Text2Speech tasks.
<ul dir="auto">
<li>Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).</li>
<li>Speaker Encoder to compute speaker embeddings efficiently.</li>
<li>Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)</li>
</ul>
</li>
<li>Fast and efficient model training.</li>
<li>Detailed training logs on the terminal and Tensorboard.</li>
<li>Support for Multi-speaker TTS.</li>
<li>Efficient, flexible, lightweight but feature complete <code>Trainer API</code>.</li>
<li>Released and ready-to-use models.</li>
<li>Tools to curate Text2Speech datasets under<code>dataset_analysis</code>.</li>
<li>Utilities to use and test your models.</li>
<li>Modular (but not too much) code base enabling easy implementation of new ideas.</li>
</ul>


<ul dir="auto">
<li>Tacotron: <a href="https://arxiv.org/abs/1703.10135" rel="nofollow">paper</a></li>
<li>Tacotron2: <a href="https://arxiv.org/abs/1712.05884" rel="nofollow">paper</a></li>
<li>Glow-TTS: <a href="https://arxiv.org/abs/2005.11129" rel="nofollow">paper</a></li>
<li>Speedy-Speech: <a href="https://arxiv.org/abs/2008.03802" rel="nofollow">paper</a></li>
<li>Align-TTS: <a href="https://arxiv.org/abs/2003.01950" rel="nofollow">paper</a></li>
<li>FastPitch: <a href="https://arxiv.org/pdf/2006.06873.pdf" rel="nofollow">paper</a></li>
<li>FastSpeech: <a href="https://arxiv.org/abs/1905.09263" rel="nofollow">paper</a></li>
<li>FastSpeech2: <a href="https://arxiv.org/abs/2006.04558" rel="nofollow">paper</a></li>
<li>SC-GlowTTS: <a href="https://arxiv.org/abs/2104.05557" rel="nofollow">paper</a></li>
<li>Capacitron: <a href="https://arxiv.org/abs/1906.03402" rel="nofollow">paper</a></li>
<li>OverFlow: <a href="https://arxiv.org/abs/2211.06892" rel="nofollow">paper</a></li>
<li>Neural HMM TTS: <a href="https://arxiv.org/abs/2108.13320" rel="nofollow">paper</a></li>
<li>Delightful TTS: <a href="https://arxiv.org/abs/2110.12612" rel="nofollow">paper</a></li>
</ul>

<ul dir="auto">
<li>ⓍTTS: <a href="https://coqui.ai/blog/tts/open_xtts" rel="nofollow">blog</a></li>
<li>VITS: <a href="https://arxiv.org/pdf/2106.06103" rel="nofollow">paper</a></li>
<li>🐸 YourTTS: <a href="https://arxiv.org/abs/2112.02418" rel="nofollow">paper</a></li>
<li>🐢 Tortoise: <a href="https://github.com/neonbjb/tortoise-tts">orig. repo</a></li>
<li>🐶 Bark: <a href="https://github.com/suno-ai/bark">orig. repo</a></li>
</ul>

<ul dir="auto">
<li>Guided Attention: <a href="https://arxiv.org/abs/1710.08969" rel="nofollow">paper</a></li>
<li>Forward Backward Decoding: <a href="https://arxiv.org/abs/1907.09006" rel="nofollow">paper</a></li>
<li>Graves Attention: <a href="https://arxiv.org/abs/1910.10288" rel="nofollow">paper</a></li>
<li>Double Decoder Consistency: <a href="https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/" rel="nofollow">blog</a></li>
<li>Dynamic Convolutional Attention: <a href="https://arxiv.org/pdf/1910.10288.pdf" rel="nofollow">paper</a></li>
<li>Alignment Network: <a href="https://arxiv.org/abs/2108.10447" rel="nofollow">paper</a></li>
</ul>

<ul dir="auto">
<li>GE2E: <a href="https://arxiv.org/abs/1710.10467" rel="nofollow">paper</a></li>
<li>Angular Loss: <a href="https://arxiv.org/pdf/2003.11982.pdf" rel="nofollow">paper</a></li>
</ul>

<ul dir="auto">
<li>MelGAN: <a href="https://arxiv.org/abs/1910.06711" rel="nofollow">paper</a></li>
<li>MultiBandMelGAN: <a href="https://arxiv.org/abs/2005.05106" rel="nofollow">paper</a></li>
<li>ParallelWaveGAN: <a href="https://arxiv.org/abs/1910.11480" rel="nofollow">paper</a></li>
<li>GAN-TTS discriminators: <a href="https://arxiv.org/abs/1909.11646" rel="nofollow">paper</a></li>
<li>WaveRNN: <a href="https://github.com/fatchord/WaveRNN/">origin</a></li>
<li>WaveGrad: <a href="https://arxiv.org/abs/2009.00713" rel="nofollow">paper</a></li>
<li>HiFiGAN: <a href="https://arxiv.org/abs/2010.05646" rel="nofollow">paper</a></li>
<li>UnivNet: <a href="https://arxiv.org/abs/2106.07889" rel="nofollow">paper</a></li>
</ul>

<ul dir="auto">
<li>FreeVC: <a href="https://arxiv.org/abs/2210.15418" rel="nofollow">paper</a></li>
</ul>
<p dir="auto">You can also help us implement more models.</p>

<p dir="auto">🐸TTS is tested on Ubuntu 18.04 with <strong>python &gt;= 3.9, &lt; 3.12.</strong>.</p>
<p dir="auto">If you are only interested in <a href="https://tts.readthedocs.io/en/latest/inference.html" rel="nofollow">synthesizing speech</a> with the released 🐸TTS models, installing from PyPI is the easiest option.</p>

<p dir="auto">If you plan to code or train models, clone 🐸TTS and install it locally.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/coqui-ai/TTS
pip install -e .[all,dev,notebooks]  # Select the relevant extras"><pre>git clone https://github.com/coqui-ai/TTS
pip install -e .[all,dev,notebooks]  <span><span>#</span> Select the relevant extras</span></pre></div>
<p dir="auto">If you are on Ubuntu (Debian), you can also run following commands for installation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.
$ make install"><pre>$ make system-deps  <span><span>#</span> intended to be used on Ubuntu (Debian). Let us know if you have a different OS.</span>
$ make install</pre></div>
<p dir="auto">If you are on Windows, 👑@GuyPaddock wrote installation instructions <a href="https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system" rel="nofollow">here</a>.</p>

<p dir="auto">You can also try TTS without install with the docker image.
Simply run the following command and you will be able to run TTS without installing it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu
python3 TTS/server/server.py --list_models #To get the list of available models
python3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server"><pre>docker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu
python3 TTS/server/server.py --list_models <span><span>#</span>To get the list of available models</span>
python3 TTS/server/server.py --model_name tts_models/en/vctk/vits <span><span>#</span> To start a server</span></pre></div>
<p dir="auto">You can then enjoy the TTS server <a href="http://%5B::1%5D:5002/" rel="nofollow">here</a>
More details about the docker images (like GPU support) can be found <a href="https://tts.readthedocs.io/en/latest/docker_images.html" rel="nofollow">here</a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Synthesizing speech by 🐸TTS</h2><a id="user-content-synthesizing-speech-by-tts" aria-label="Permalink: Synthesizing speech by 🐸TTS" href="#synthesizing-speech-by-tts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<div dir="auto"><h4 tabindex="-1" dir="auto">Running a multi-speaker and multi-lingual model</h4><a id="user-content-running-a-multi-speaker-and-multi-lingual-model" aria-label="Permalink: Running a multi-speaker and multi-lingual model" href="#running-a-multi-speaker-and-multi-lingual-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from TTS.api import TTS

# Get device
device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;

# List available 🐸TTS models
print(TTS().list_models())

# Init TTS
tts = TTS(&#34;tts_models/multilingual/multi-dataset/xtts_v2&#34;).to(device)

# Run TTS
# ❗ Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language
# Text to speech list of amplitude values as output
wav = tts.tts(text=&#34;Hello world!&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;)
# Text to speech to a file
tts.tts_to_file(text=&#34;Hello world!&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;, file_path=&#34;output.wav&#34;)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>TTS</span>.<span>api</span> <span>import</span> <span>TTS</span>

<span># Get device</span>
<span>device</span> <span>=</span> <span>&#34;cuda&#34;</span> <span>if</span> <span>torch</span>.<span>cuda</span>.<span>is_available</span>() <span>else</span> <span>&#34;cpu&#34;</span>

<span># List available 🐸TTS models</span>
<span>print</span>(<span>TTS</span>().<span>list_models</span>())

<span># Init TTS</span>
<span>tts</span> <span>=</span> <span>TTS</span>(<span>&#34;tts_models/multilingual/multi-dataset/xtts_v2&#34;</span>).<span>to</span>(<span>device</span>)

<span># Run TTS</span>
<span># ❗ Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language</span>
<span># Text to speech list of amplitude values as output</span>
<span>wav</span> <span>=</span> <span>tts</span>.<span>tts</span>(<span>text</span><span>=</span><span>&#34;Hello world!&#34;</span>, <span>speaker_wav</span><span>=</span><span>&#34;my/cloning/audio.wav&#34;</span>, <span>language</span><span>=</span><span>&#34;en&#34;</span>)
<span># Text to speech to a file</span>
<span>tts</span>.<span>tts_to_file</span>(<span>text</span><span>=</span><span>&#34;Hello world!&#34;</span>, <span>speaker_wav</span><span>=</span><span>&#34;my/cloning/audio.wav&#34;</span>, <span>language</span><span>=</span><span>&#34;en&#34;</span>, <span>file_path</span><span>=</span><span>&#34;output.wav&#34;</span>)</pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">Running a single speaker model</h4><a id="user-content-running-a-single-speaker-model" aria-label="Permalink: Running a single speaker model" href="#running-a-single-speaker-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Init TTS with the target model name
tts = TTS(model_name=&#34;tts_models/de/thorsten/tacotron2-DDC&#34;, progress_bar=False).to(device)

# Run TTS
tts.tts_to_file(text=&#34;Ich bin eine Testnachricht.&#34;, file_path=OUTPUT_PATH)

# Example voice cloning with YourTTS in English, French and Portuguese
tts = TTS(model_name=&#34;tts_models/multilingual/multi-dataset/your_tts&#34;, progress_bar=False).to(device)
tts.tts_to_file(&#34;This is voice cloning.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;en&#34;, file_path=&#34;output.wav&#34;)
tts.tts_to_file(&#34;C&#39;est le clonage de la voix.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;fr-fr&#34;, file_path=&#34;output.wav&#34;)
tts.tts_to_file(&#34;Isso é clonagem de voz.&#34;, speaker_wav=&#34;my/cloning/audio.wav&#34;, language=&#34;pt-br&#34;, file_path=&#34;output.wav&#34;)"><pre><span># Init TTS with the target model name</span>
<span>tts</span> <span>=</span> <span>TTS</span>(<span>model_name</span><span>=</span><span>&#34;tts_models/de/thorsten/tacotron2-DDC&#34;</span>, <span>progress_bar</span><span>=</span><span>False</span>).<span>to</span>(<span>device</span>)

<span># Run TTS</span>
<span>tts</span>.<span>tts_to_file</span>(<span>text</span><span>=</span><span>&#34;Ich bin eine Testnachricht.&#34;</span>, <span>file_path</span><span>=</span><span>OUTPUT_PATH</span>)

<span># Example voice cloning with YourTTS in English, French and Portuguese</span>
<span>tts</span> <span>=</span> <span>TTS</span>(<span>model_name</span><span>=</span><span>&#34;tts_models/multilingual/multi-dataset/your_tts&#34;</span>, <span>progress_bar</span><span>=</span><span>False</span>).<span>to</span>(<span>device</span>)
<span>tts</span>.<span>tts_to_file</span>(<span>&#34;This is voice cloning.&#34;</span>, <span>speaker_wav</span><span>=</span><span>&#34;my/cloning/audio.wav&#34;</span>, <span>language</span><span>=</span><span>&#34;en&#34;</span>, <span>file_path</span><span>=</span><span>&#34;output.wav&#34;</span>)
<span>tts</span>.<span>tts_to_file</span>(<span>&#34;C&#39;est le clonage de la voix.&#34;</span>, <span>speaker_wav</span><span>=</span><span>&#34;my/cloning/audio.wav&#34;</span>, <span>language</span><span>=</span><span>&#34;fr-fr&#34;</span>, <span>file_path</span><span>=</span><span>&#34;output.wav&#34;</span>)
<span>tts</span>.<span>tts_to_file</span>(<span>&#34;Isso é clonagem de voz.&#34;</span>, <span>speaker_wav</span><span>=</span><span>&#34;my/cloning/audio.wav&#34;</span>, <span>language</span><span>=</span><span>&#34;pt-br&#34;</span>, <span>file_path</span><span>=</span><span>&#34;output.wav&#34;</span>)</pre></div>

<p dir="auto">Converting the voice in <code>source_wav</code> to the voice of <code>target_wav</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="tts = TTS(model_name=&#34;voice_conversion_models/multilingual/vctk/freevc24&#34;, progress_bar=False).to(&#34;cuda&#34;)
tts.voice_conversion_to_file(source_wav=&#34;my/source.wav&#34;, target_wav=&#34;my/target.wav&#34;, file_path=&#34;output.wav&#34;)"><pre><span>tts</span> <span>=</span> <span>TTS</span>(<span>model_name</span><span>=</span><span>&#34;voice_conversion_models/multilingual/vctk/freevc24&#34;</span>, <span>progress_bar</span><span>=</span><span>False</span>).<span>to</span>(<span>&#34;cuda&#34;</span>)
<span>tts</span>.<span>voice_conversion_to_file</span>(<span>source_wav</span><span>=</span><span>&#34;my/source.wav&#34;</span>, <span>target_wav</span><span>=</span><span>&#34;my/target.wav&#34;</span>, <span>file_path</span><span>=</span><span>&#34;output.wav&#34;</span>)</pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">Example voice cloning together with the voice conversion model.</h4><a id="user-content-example-voice-cloning-together-with-the-voice-conversion-model" aria-label="Permalink: Example voice cloning together with the voice conversion model." href="#example-voice-cloning-together-with-the-voice-conversion-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This way, you can clone voices by using any model in 🐸TTS.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
tts = TTS(&#34;tts_models/de/thorsten/tacotron2-DDC&#34;)
tts.tts_with_vc_to_file(
    &#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;,
    speaker_wav=&#34;target/speaker.wav&#34;,
    file_path=&#34;output.wav&#34;
)"><pre><span>tts</span> <span>=</span> <span>TTS</span>(<span>&#34;tts_models/de/thorsten/tacotron2-DDC&#34;</span>)
<span>tts</span>.<span>tts_with_vc_to_file</span>(
    <span>&#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;</span>,
    <span>speaker_wav</span><span>=</span><span>&#34;target/speaker.wav&#34;</span>,
    <span>file_path</span><span>=</span><span>&#34;output.wav&#34;</span>
)</pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">Example text to speech using <strong>Fairseq models in ~1100 languages</strong> 🤯.</h4><a id="user-content-example-text-to-speech-using-fairseq-models-in-1100-languages-" aria-label="Permalink: Example text to speech using Fairseq models in ~1100 languages 🤯." href="#example-text-to-speech-using-fairseq-models-in-1100-languages-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For Fairseq models, use the following name format: <code>tts_models/&lt;lang-iso_code&gt;/fairseq/vits</code>.
You can find the language ISO codes <a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html" rel="nofollow">here</a>
and learn about the Fairseq models <a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms">here</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# TTS with on the fly voice conversion
api = TTS(&#34;tts_models/deu/fairseq/vits&#34;)
api.tts_with_vc_to_file(
    &#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;,
    speaker_wav=&#34;target/speaker.wav&#34;,
    file_path=&#34;output.wav&#34;
)"><pre><span># TTS with on the fly voice conversion</span>
<span>api</span> <span>=</span> <span>TTS</span>(<span>&#34;tts_models/deu/fairseq/vits&#34;</span>)
<span>api</span>.<span>tts_with_vc_to_file</span>(
    <span>&#34;Wie sage ich auf Italienisch, dass ich dich liebe?&#34;</span>,
    <span>speaker_wav</span><span>=</span><span>&#34;target/speaker.wav&#34;</span>,
    <span>file_path</span><span>=</span><span>&#34;output.wav&#34;</span>
)</pre></div>


<p dir="auto">Synthesize speech on command line.</p>
<p dir="auto">You can either use your trained model or choose a model from the provided list.</p>
<p dir="auto">If you don&#39;t specify any models, then it uses LJSpeech based English model.</p>

<ul dir="auto">
<li>
<p dir="auto">List provided models:</p>

</li>
<li>
<p dir="auto">Get model info (for both tts_models and vocoder_models):</p>
<ul dir="auto">
<li>
<p dir="auto">Query by type/name:
The model_info_by_name uses the name as it from the --list_models.</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;"><pre><code>$ tts --model_info_by_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts
$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2"><pre><code>$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts
$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2
</code></pre></div>
</li>
<li>
<p dir="auto">Query by type/idx:
The model_query_idx uses the corresponding idx from --list_models.</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_idx &#34;&lt;model_type&gt;/&lt;model_query_idx&gt;&#34;"><pre><code>$ tts --model_info_by_idx &#34;&lt;model_type&gt;/&lt;model_query_idx&gt;&#34;
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_idx tts_models/3"><pre><code>$ tts --model_info_by_idx tts_models/3
</code></pre></div>
</li>
<li>
<p dir="auto">Query info for model info by full name:</p>
<div data-snippet-clipboard-copy-content="$ tts --model_info_by_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;"><pre><code>$ tts --model_info_by_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;
</code></pre></div>
</li>
</ul>
</li>
<li>
<p dir="auto">Run TTS with default models:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run TTS and pipe out the generated TTS wav file data:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --pipe_out --out_path output/path/speech.wav | aplay"><pre><code>$ tts --text &#34;Text for TTS&#34; --pipe_out --out_path output/path/speech.wav | aplay
</code></pre></div>
</li>
<li>
<p dir="auto">Run a TTS model with its default vocoder model:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run with specific TTS and vocoder models from the list:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --vocoder_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --vocoder_name &#34;&lt;model_type&gt;/&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --out_path output/path/speech.wav
</code></pre></div>
<p dir="auto">For example:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --vocoder_name &#34;vocoder_models/en/ljspeech/univnet&#34; --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_name &#34;tts_models/en/ljspeech/glow-tts&#34; --vocoder_name &#34;vocoder_models/en/ljspeech/univnet&#34; --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run your own TTS model (Using Griffin-Lim Vocoder):</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav
</code></pre></div>
</li>
<li>
<p dir="auto">Run your own TTS and Vocoder models:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav
    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json"><pre><code>$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav
    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json
</code></pre></div>
</li>
</ul>

<ul dir="auto">
<li>
<p dir="auto">List the available speakers and choose a &lt;speaker_id&gt; among them:</p>
<div data-snippet-clipboard-copy-content="$ tts --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --list_speaker_idxs"><pre><code>$ tts --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --list_speaker_idxs
</code></pre></div>
</li>
<li>
<p dir="auto">Run the multi-speaker TTS model with the target speaker ID:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --speaker_idx &lt;speaker_id&gt;"><pre><code>$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34;  --speaker_idx &lt;speaker_id&gt;
</code></pre></div>
</li>
<li>
<p dir="auto">Run your own multi-speaker TTS model:</p>
<div data-snippet-clipboard-copy-content="$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx &lt;speaker_id&gt;"><pre><code>$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx &lt;speaker_id&gt;
</code></pre></div>
</li>
</ul>

<div data-snippet-clipboard-copy-content="$ tts --out_path output/path/speech.wav --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --source_wav &lt;path/to/speaker/wav&gt; --target_wav &lt;path/to/reference/wav&gt;"><pre><code>$ tts --out_path output/path/speech.wav --model_name &#34;&lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;&#34; --source_wav &lt;path/to/speaker/wav&gt; --target_wav &lt;path/to/reference/wav&gt;
</code></pre></div>


<div data-snippet-clipboard-copy-content="|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)
|- utils/           (common utilities.)
|- TTS
    |- bin/             (folder for all the executables.)
      |- train*.py                  (train your target model.)
      |- ...
    |- tts/             (text to speech models)
        |- layers/          (model layer definitions)
        |- models/          (model definitions)
        |- utils/           (model specific utilities.)
    |- speaker_encoder/ (Speaker Encoder models.)
        |- (same)
    |- vocoder/         (Vocoder models.)
        |- (same)"><pre><code>|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)
|- utils/           (common utilities.)
|- TTS
    |- bin/             (folder for all the executables.)
      |- train*.py                  (train your target model.)
      |- ...
    |- tts/             (text to speech models)
        |- layers/          (model layer definitions)
        |- models/          (model definitions)
        |- utils/           (model specific utilities.)
    |- speaker_encoder/ (Speaker Encoder models.)
        |- (same)
    |- vocoder/         (Vocoder models.)
        |- (same)
</code></pre></div>
</article></div></div>
  </body>
</html>
