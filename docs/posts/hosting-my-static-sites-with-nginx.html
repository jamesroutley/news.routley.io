<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jvns.ca/blog/2022/01/24/hosting-my-static-sites-with-nginx/">Original</a>
    <h1>Hosting my static sites with nginx</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Hello! Recently I’ve been thinking about putting my static sites on servers
that I run myself instead of using managed services like Netlify or GitHub
Pages.</p>
<p>Originally I thought that running my own servers would require a lot of
maintenance and be a huge pain, but I was chatting with Wesley about what kind
of maintainance <a href="https://blog.wesleyac.com/posts/how-i-run-my-servers">their servers</a> require, and
they convinced me that it might not be that bad.</p>
<p>So I decided to try out moving all my static sites to a $5/month server to see
what it was like.</p>
<p>Everything in here is pretty standard but I wanted to write down what I did
anyway because there are a surprising number of decisions and I like to see
what choices other people make.</p>
<h3 id="the-constraint-only-static-sites">the constraint: only static sites</h3>
<p>To keep things simple, I decided that this server would only run <code>nginx</code> and
only serve static sites. I have about 10 static sites right now, mostly projects for <a href="https://wizardzines.com">wizard zines</a>.</p>
<p>I decided to use a $5/month DigitalOcean droplet, which should very easily be
able to handle my existing traffic (about 3 requests per second and 100GB
of bandwidth per month). Right now it’s using about 1% of its CPU. I picked
DigitalOcean because it was what I’ve used before.</p>
<p>Also all the sites were already behind a CDN so they’re still behind the same
CDN.</p>
<h3 id="step-1-get-a-clean-git-repo-for-each-build">step 1: get a clean Git repo for each build</h3>
<p>This was the most interesting problem so let’s talk about it first!</p>
<p>Building the static sites might seem pretty easy – each one of them already
has a working build script.</p>
<p>But I have pretty bad hygiene around files on my laptop – often I have a bunch of
uncommitted files that I don’t want to go onto the live site. So I wanted to
start every build with a clean Git repo. I also wanted this to be <em>fast</em> – I’m
impatient so I wanted to be able to build and deploy most of my sites in less than 10
seconds.</p>
<p>I handled this by hacking together a tiny build system called
<a href="https://github.com/jvns/tinybuild/">tinybuild</a>. It’s
basically a 4-line bash script, but with extra some command line arguments and
error checking. Here are the 4 lines of bash:</p>
<pre><code>docker build - -t tinybuild &lt; Dockerfile
CONTAINER_ID=$(docker run -v &#34;$PWD&#34;:/src -v &#34;./deploy:/artifact&#34; -d -t tinybuild /bin/bash)
docker exec $CONTAINER_ID bash -c &#34;git clone /src /build &amp;&amp; cd /build &amp;&amp; bash /src/scripts/build.sh&#34;
docker exec $CONTAINER_ID bash -c &#34;mv /build/public/* /artifact&#34;
</code></pre>
<p>These 4 lines:</p>
<ol>
<li>Build a Dockerfile with all the dependencies for that build</li>
<li>Clone my repo into <code>/build</code> in the container, so that I always start with a clean Git repo</li>
<li>Run the build script (<code>/src/scripts/build.sh</code>)</li>
<li>Copy the build artifacts into <code>./deploy</code> in the local directory</li>
</ol>
<p>Then once I have <code>./deploy</code>, I can rsync the result onto the server</p>
<p>It’s fast because:</p>
<ul>
<li>the <code>docker build -</code> means I don’t send any state from the repository to
the Docker daemon. This matters because one of my repos is 1GB (it has a lot
of PDFs in it) and sending all that to the Docker daemon takes forever</li>
<li>the <code>git clone</code> is from the local filesystem and I have a SSD so it’s fast even for a 1GB repo</li>
<li>most of the build scripts just run <code>hugo</code> or <code>cat</code> so they’re fast. The <code>npm</code> build scripts take maybe 30 seconds.</li>
</ul>
<h3 id="apparently-local-git-clones-make-hard-links">apparently local git clones make hard links</h3>
<p>A tiny interesting fact: I tried to do <code>git clone --depth 1</code> to speed up my git
clone, but git gave me this warning:</p>
<pre><code>warning: --depth is ignored in local clones; use file:// instead.
</code></pre>
<p>I think what’s going on here is that git makes hard links of all the objects to
make a local clone (which is a lot faster than copying). So I guess with the
hard links approach <code>--depth 1</code> doesn’t make sense for some reason? And
<code>file://</code> forces git to copy all objects instead, which is actually slower.</p>
<h3 id="bonus-now-my-builds-are-faster-than-they-used-to-be">bonus: now my builds are faster than they used to be!</h3>
<p>One nice thing about this is that my build/deploy time is less than it was on
Netlify. For <code>jvns.ca</code> it’s about 7 seconds to build and deploy the site
instead of about a minute previously.</p>
<h3 id="running-the-builds-on-my-laptop-seems-nice">running the builds on my laptop seems nice</h3>
<p>I’m the only person who develops all of my sites, so doing all the builds in a
Docker container on my computer seems to make sense. My computer is pretty fast
and all the files are already right there! No giant downloads! And doing it in
a Docker container keeps the build isolated.</p>
<h3 id="example-build-scripts">example build scripts</h3>
<p>Here are the build scripts for this blog (<code>jvns.ca</code>).</p>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM ubuntu:20.04

RUN apt-get update &amp;&amp; apt-get install -y git
RUN apt-get install -y wget python2
RUN wget https://github.com/gohugoio/hugo/releases/download/v0.40.1/hugo_0.40.1_Linux-64bit.tar.gz
RUN wget https://github.com/sass/dart-sass/releases/download/1.49.0/dart-sass-1.49.0-linux-x64.tar.gz
RUN tar -xf dart-sass-1.49.0-linux-x64.tar.gz
RUN tar -xf hugo_0.40.1_Linux-64bit.tar.gz
RUN mv hugo /usr/bin/hugo
RUN mv dart-sass/sass /usr/bin/sass
</code></pre>
<p><strong>build-docker.sh</strong>:</p>
<pre><code>set -eu
scripts/parse_titles.py
sass sass/:static/stylesheets/
hugo
</code></pre>
<p><strong>deploy.sh</strong>:</p>
<pre><code>set -eu
tinybuild -s scripts/build-docker.sh \
          -l &#34;$PWD/deploy&#34; \
          -c /build/public

rsync-showdiff ./deploy/ <a href="https://jvns.ca/cdn-cgi/l/email-protection" data-cfemail="b4c6dbdbc0f4c7c0d5c0ddd7c7ddc0d1c7">[email protected]</a>:/var/www/jvns.ca
rm -rf ./deploy
</code></pre>
<h3 id="step-2-get-rsync-to-just-show-me-which-files-it-updated">step 2: get rsync to just show me which files it updated</h3>
<p>When I started using rsync to sync the files, it would list every single file
instead of just files that had changed. I think this was because I was
generating new files for every build, so the timestamps were always newer than
the files on the server.</p>
<p>I did a bunch of Googling and figured out this incantation to get rsync to just
show me files that were updated;</p>
<pre><code>rsync -avc --out-format=&#39;%n&#39; &#34;<a href="https://jvns.ca/cdn-cgi/l/email-protection" data-cfemail="c7e387">[email protected]</a>&#34; | grep --line-buffered -v &#39;/$&#39;
</code></pre>
<p>I put that in a script called <code>rsync-showdiff</code> so I could reuse it. There might
be a better way, but this seems to work.</p>
<h3 id="step-3-configuration-management">step 3: configuration management</h3>
<p>All I needed to do to set up the server was:</p>
<ul>
<li>install nginx</li>
<li>create directories in /var/www for each site, like <code>/var/www/jvns.ca</code></li>
<li>create an nginx configuration for each site, like <code>/etc/nginx/sites-enabled/jvns.ca.conf</code></li>
<li>deploy the files (with my deploy script above)</li>
</ul>
<p>I wanted to use some kind of configuration management to do this because that’s how I’m
used to managing servers. I’ve used Puppet a lot in the past at work, but I don’t
really <em>like</em> using Puppet. So I decided to use Ansible even though I’d never
used it before because it seemed simpler than using Puppet. Here’s <a href="https://gist.github.com/jvns/06754e9e65b49dd461fefa071dd4aace">my current Ansible configuration</a>,
minus some of the templates it depends on.</p>
<p>I didn’t use any Ansible plugins because I wanted to maximize the probability
that I would actually be able to run this thing in 3 years.</p>
<p>The most complicated thing in there is probably the <code>reload nginx</code> handler,
which makes sure that the configuration is still valid after I make an nginx
configuration update.</p>
<h3 id="step-4-replace-a-lambda-function">step 4: replace a lambda function</h3>
<p>I was using one Netlify lambda function to calculate purchasing power parity
(“PPP”) for countries that have a weaker currency relative to the US on <a href="https://wizardzines.com">https://wizardzines.com</a>. Basically it
gets your country using IP geolocation and then returns a discount code if
you’re in a country that has a discount code. (like 70% off for India, for
example). So I needed to replace it.</p>
<p>I handled this by rewriting the (very small) program in Go, copying the
static binary to the server, and adding a <code>proxy_pass</code> for that site.</p>
<p>The program just looks up the country code from the <a href="https://support.cloudflare.com/hc/en-us/articles/200168236-Configuring-Cloudflare-IP-Geolocation">geolocation HTTP header</a>
in a hashmap, so it doesn’t seem like it should cause maintenance problems.</p>
<h3 id="a-very-simple-nginx-config">a very simple nginx config</h3>
<p>I used the same nginx config file for templates for almost all my sites:</p>
<pre><code>server {
	listen 80;
	listen [::]:80;

	root /var/www/{{item.dir}};
	index index.html index.htm;
	server_name {{item.server}};

    location / {
        # First attempt to serve request as file, then
        # as directory, then fall back to displaying a 404.
        try_files $uri $uri/ =404;
    }
}
</code></pre>
<p>The <code>{{item.dir}}</code> is an Ansible thing.</p>
<p>I also added support for custom 404 pages (<code>error_page /404.html</code>) in the main <code>nginx.conf</code>.</p>
<p>I’ll probably add TLS support with certbot later. My CDN handles TLS to the
client, I just need to make the connection between the CDN and the origin
server use TLS</p>
<p>Also I don’t know if there are problems with using such a simple nginx config.
Maybe I’ll learn about them!</p>
<h3 id="bonus-i-can-find-404s-more-easily">bonus: I can find 404s more easily</h3>
<p>Another nice bonus of this setup is that it’s easier to see what’s happening
with my site – I can just look at the nginx logs!</p>
<p>I ran <code>grep 404 /var/log/nginx/access.log</code> to figure out if I’d broken
anything during the migration, and I actually ended up finding a lot of
links that had been broken for many years, but that I’d just never noticed.</p>
<p>Netlify’s analytics has a “Top resources not found” that shows you the most
common 404s, but I don’t think there’s any way to see <em>all</em> 404s.</p>
<h3 id="a-small-factor-costs">a small factor: costs</h3>
<p>Part of my motivation for this switch was – I was getting close to the Netlify
free tier’s bandwidth limit (100GB/month), and Netlify charges $20/100GB for
additional bandwidth. Digital Ocean charges $1/100GB for additional bandwidth
(20x less), and my droplet comes with 1TB of bandwidth. So the bandwidth
pricing feels a lot more reasonable to me.</p>
<h3 id="we-ll-see-how-it-goes">we’ll see how it goes!</h3>
<p>All my static sites are running on my own server now. I don’t really know what
this will be like to maintain, we’ll see how it goes – maybe I’ll like it!
maybe I’ll hate it! I definitely like the faster build times and that I can
easily look at my nginx logs.</p>
</div></div>
  </body>
</html>
