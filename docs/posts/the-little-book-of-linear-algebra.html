<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/the-litte-book-of/linear-algebra">Original</a>
    <h1>The Little Book of Linear Algebra</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">A concise, beginner-friendly introduction to the core ideas of linear algebra.</p>

<ul dir="auto">
<li><a href="https://blog.veitheller.de/the-litte-book-of/linear-algebra/blob/main/book.pdf">Download PDF</a> – print-ready version</li>
<li><a href="https://blog.veitheller.de/the-litte-book-of/linear-algebra/blob/main/book.epub">Download EPUB</a> – e-reader friendly</li>
<li><a href="https://blog.veitheller.de/the-litte-book-of/linear-algebra/blob/main/book.tex">View LaTeX</a> – Latex source</li>
</ul>


<p dir="auto">A scalar is a single numerical quantity, most often taken from the real numbers, denoted by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. Scalars are
the fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of
zero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger
structures such as vectors and matrices. They provide the weights by which more complex objects are measured and
combined. A vector is an ordered collection of scalars, arranged either in a row or a column. When the scalars are real
numbers, the vector is said to belong to <em>real</em> <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional space, written</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbb{R}^n = { (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} }.
$$</math-renderer></p>
<p dir="auto">An element of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> is called a vector of dimension <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> or an <em>n</em>-vector. The number <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> is called the
dimension of the vector space. Thus <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> is the space of all ordered pairs of real numbers, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> the
space of all ordered triples, and so on.</p>
<p dir="auto">Example 1.1.1.</p>
<ul dir="auto">
<li>A 2-dimensional vector: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3, -1) \in \mathbb{R}^2$</math-renderer>.</li>
<li>A 3-dimensional vector: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2, 0, 5) \in \mathbb{R}^3$</math-renderer>.</li>
<li>A 1-dimensional vector: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(7) \in \mathbb{R}^1$</math-renderer>, which corresponds to the scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$7$</math-renderer> itself.</li>
</ul>
<p dir="auto">Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} = \begin{bmatrix} 2 \ 0 \ 5 \end{bmatrix} \in \mathbb{R}^3.
$$</math-renderer></p>
<p dir="auto">The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.</p>

<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_1, x_2)$</math-renderer> can be visualized as an arrow starting at the origin <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,0)$</math-renderer> and ending at the
point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_1, x_2)$</math-renderer>. Its length corresponds to the distance from the origin, and its orientation gives a direction in the
plane. In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the same picture extends into three dimensions: a vector is an arrow from the origin
to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_1, x_2, x_3)$</math-renderer>. Beyond three dimensions, direct visualization is no longer possible, but the algebraic rules of
vectors remain identical. Even though we cannot draw a vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^{10}$</math-renderer>, it behaves under addition, scaling,
and transformation exactly as a 2- or 3-dimensional vector does. This abstract point of view is what allows linear
algebra to apply to data science, physics, and machine learning, where data often lives in very high-dimensional spaces.
Thus a vector may be regarded in three complementary ways:</p>
<ol dir="auto">
<li>As a point in space, described by its coordinates.</li>
<li>As a displacement or arrow, described by a direction and a length.</li>
<li>As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.</li>
</ol>

<ul dir="auto">
<li>Vectors are written in boldface lowercase letters: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}, \mathbf{w}, \mathbf{x}$</math-renderer>.</li>
<li>The <em>i</em>-th entry of a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> is written <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$v_i$</math-renderer>, where indices begin at 1.</li>
<li>The set of all <em>n</em>-dimensional vectors over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer> is denoted <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>.</li>
<li>Column vectors will be the default form unless otherwise stated.</li>
</ul>

<p dir="auto">Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear
transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once
vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to
subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with
powerful applications to geometry, computation, and data.</p>

<ol dir="auto">
<li>Write three different vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> and sketch them as arrows from the origin. Identify their coordinates
explicitly.</li>
<li>Give an example of a vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^4$</math-renderer>. Can you visualize it directly? Explain why high-dimensional
visualization is challenging.</li>
<li>Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, -3, 2)$</math-renderer>. Write <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> in column form and state <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$v_1, v_2, v_3$</math-renderer>.</li>
<li>In what sense is the set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^1$</math-renderer> both a line and a vector space? Illustrate with examples.</li>
<li>Consider the vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$</math-renderer>. What is special about this vector when <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> is
large? What might it represent in applications?</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">1.2 Vector Addition and Scalar Multiplication</h2><a id="user-content-12-vector-addition-and-scalar-multiplication" aria-label="Permalink: 1.2 Vector Addition and Scalar Multiplication" href="#12-vector-addition-and-scalar-multiplication"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two
fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations
satisfy simple but far-reaching rules that underpin the entire subject.</p>

<p dir="auto">Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} = (u_1, u_2, \dots, u_n), \quad
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$</math-renderer></p>
<p dir="auto">then their sum is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} + \mathbf{v} = (u_1+v_1, u_2+v_2, \dots, u_n+v_n).
$$</math-renderer></p>
<p dir="auto">Example 1.2.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (2, -1, 3)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, 0, -5)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} + \mathbf{v} = (2+4, -1+0, 3+(-5)) = (6, -1, -2).
$$</math-renderer></p>
<p dir="auto">Geometrically, vector addition corresponds to the <em>parallelogram rule</em>. If we draw both vectors as arrows from the
origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram
they form represents the resulting vector.</p>

<p dir="auto">Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is
negative, in which case the vector is also reversed. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
c \mathbf{v} = (c v_1, c v_2, \dots, c v_n).
$$</math-renderer></p>
<p dir="auto">Example 1.2.2.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3, -2)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c = -2$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
c\mathbf{v} = -2(3, -2) = (-6, 4).
$$</math-renderer></p>
<p dir="auto">This corresponds to flipping the vector through the origin and doubling its length.</p>

<p dir="auto">The interaction of addition and scalar multiplication allows us to form <em>linear combinations</em>. A linear combination of
vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$</math-renderer> is any vector of the form</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad c_i \in \mathbb{R}.
$$</math-renderer></p>
<p dir="auto">Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of
vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.</p>
<p dir="auto">Example 1.2.3.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1 = (1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2 = (0,1)$</math-renderer>. Then any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a,b)\in\mathbb{R}^2$</math-renderer> can be expressed as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
a\mathbf{v}_1 + b\mathbf{v}_2.
$$</math-renderer></p>
<p dir="auto">Thus <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer> form the basic building blocks of the plane.</p>

<ul dir="auto">
<li>Addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v}$</math-renderer> means component-wise addition.</li>
<li>Scalar multiplication: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c\mathbf{v}$</math-renderer> scales each entry of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer>.</li>
<li>Linear combination: a sum of the form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$</math-renderer>.</li>
</ul>

<p dir="auto">Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector
spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving
systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound
rules.</p>

<ol dir="auto">
<li>Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v}$</math-renderer> where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2,3)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, -1, 0)$</math-renderer>.</li>
<li>Find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3\mathbf{v}$</math-renderer> where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (-2,5)$</math-renderer>. Sketch both vectors to illustrate the scaling.</li>
<li>Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(5,7)$</math-renderer> can be written as a linear combination of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer>.</li>
<li>Write <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(4,4)$</math-renderer> as a linear combination of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,-1)$</math-renderer>.</li>
<li>Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$</math-renderer>,
then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$</math-renderer> for
scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c,d \in \mathbb{R}$</math-renderer>.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">1.3 Dot Product, Norms, and Angles</h2><a id="user-content-13-dot-product-norms-and-angles" aria-label="Permalink: 1.3 Dot Product, Norms, and Angles" href="#13-dot-product-norms-and-angles"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure
lengths, compute angles, and determine orthogonality. From this single definition flow the notions of <em>norm</em> and
<em>angle</em>, which give geometry to abstract vector spaces.</p>

<p dir="auto">For two vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, the dot product (also called the inner product) is defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$</math-renderer></p>
<p dir="auto">Equivalently, in matrix notation:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}.
$$</math-renderer></p>
<p dir="auto">Example 1.3.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (2, -1, 3)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4, 0, -2)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 2\cdot 4 + (-1)\cdot 0 + 3\cdot (-2) = 8 - 6 = 2.
$$</math-renderer></p>
<p dir="auto">The dot product outputs a single scalar, not another vector.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Norms (Length of a Vector)</h3><a id="user-content-norms-length-of-a-vector" aria-label="Permalink: Norms (Length of a Vector)" href="#norms-length-of-a-vector"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <em>Euclidean norm</em> of a vector is the square root of its dot product with itself:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{v}| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
$$</math-renderer></p>
<p dir="auto">This generalizes the Pythagorean theorem to arbitrary dimensions.</p>
<p dir="auto">Example 1.3.2.
For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3, 4)$</math-renderer>,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{v}| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5.
$$</math-renderer></p>
<p dir="auto">This is exactly the length of the vector as an arrow in the plane.</p>

<p dir="auto">The dot product also encodes the angle between two vectors. For nonzero vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}$</math-renderer>,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = |\mathbf{u}| , |\mathbf{v}| \cos \theta,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer> is the angle between them. Thus,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{\mathbf{u} \cdot \mathbf{v}}{|\mathbf{u}||\mathbf{v}|}.
$$</math-renderer></p>
<p dir="auto">Example 1.3.3.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (0,1)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 0, \quad |\mathbf{u}| = 1, \quad |\mathbf{v}| = 1.
$$</math-renderer></p>
<p dir="auto">Hence</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{0}{1\cdot 1} = 0 \quad \Rightarrow \quad \theta = \frac{\pi}{2}.
$$</math-renderer></p>
<p dir="auto">The vectors are perpendicular.</p>

<p dir="auto">Two vectors are said to be orthogonal if their dot product is zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$</math-renderer></p>
<p dir="auto">Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.</p>

<ul dir="auto">
<li>Dot product: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \cdot \mathbf{v}$</math-renderer>.</li>
<li>Norm (length): <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\mathbf{v}|$</math-renderer>.</li>
<li>Orthogonality: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \perp \mathbf{v}$</math-renderer> if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \cdot \mathbf{v} = 0$</math-renderer>.</li>
</ul>

<p dir="auto">The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of
perpendicularity. This foundation will later support the study of orthogonal projections, Gram–Schmidt
orthogonalization, eigenvectors, and least squares problems.</p>

<ol dir="auto">
<li>Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \cdot \mathbf{v}$</math-renderer> for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2,3)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4,5,6)$</math-renderer>.</li>
<li>Find the norm of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (2, -2, 1)$</math-renderer>.</li>
<li>Determine whether <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (1,-1,2)$</math-renderer> are orthogonal.</li>
<li>Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (3,4)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (4,3)$</math-renderer>. Compute the angle between them.</li>
<li>Prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$</math-renderer>. This
identity is the algebraic version of the Law of Cosines.</li>
</ol>

<p dir="auto">Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas
in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant
properties.</p>

<p dir="auto">Two vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$</math-renderer> are said to be orthogonal if their dot product is zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$</math-renderer></p>
<p dir="auto">This condition ensures that the angle between them is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\pi/2$</math-renderer> radians (90 degrees).</p>
<p dir="auto">Example 1.4.1.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, the vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,-1)$</math-renderer> are orthogonal since</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
(1,2) \cdot (2,-1) = 1\cdot 2 + 2\cdot (-1) = 0.
$$</math-renderer></p>

<p dir="auto">A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in
addition, each vector has norm 1, the set is called orthonormal.</p>
<p dir="auto">Example 1.4.2.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the standard basis vectors</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$</math-renderer></p>
<p dir="auto">form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.</p>

<p dir="auto">Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one
orthogonal to it. Given a nonzero vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> and any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>, the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>
onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}.
$$</math-renderer></p>
<p dir="auto">The difference</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})
$$</math-renderer></p>
<p dir="auto">is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>. Thus every vector can be decomposed uniquely into a parallel and perpendicular part with
respect to another vector.</p>
<p dir="auto">Example 1.4.3.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (2,3)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{(1,0)\cdot(2,3)}{(1,0)\cdot(1,0)} (1,0)
= \frac{2}{1}(1,0) = (2,0).
$$</math-renderer></p>
<p dir="auto">Thus</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v} = (2,3) = (2,0) + (0,3),
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,0)$</math-renderer> is parallel to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,3)$</math-renderer> is orthogonal to it.</p>

<p dir="auto">In general, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \neq \mathbf{0}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in \mathbb{R}^n$</math-renderer>, then</p>
<p dir="auto">$$
\mathbf{v} = \text{proj}<em>{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}</em>{\mathbf{u}}(\mathbf{v})\big),
$$</p>
<p dir="auto">where the first term is parallel to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> and the second term is orthogonal. This decomposition underlies methods
such as least squares approximation and the Gram–Schmidt process.</p>

<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \perp \mathbf{v}$</math-renderer>: vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> are orthogonal.</li>
<li>An orthogonal set: vectors pairwise orthogonal.</li>
<li>An orthonormal set: pairwise orthogonal, each of norm 1.</li>
</ul>

<p dir="auto">Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify
computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data
science (QR decomposition, least squares regression, PCA) rely on orthogonality.</p>

<ol dir="auto">
<li>Verify that the vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2,2)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,0,-1)$</math-renderer> are orthogonal.</li>
<li>Find the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3,4)$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>.</li>
<li>Show that any two distinct standard basis vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> are orthogonal.</li>
<li>Decompose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(5,2)$</math-renderer> into components parallel and orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,1)$</math-renderer>.</li>
<li>Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}$</math-renderer> are orthogonal and nonzero,
then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v}) = 0$</math-renderer>.</li>
</ol>

<div dir="auto"><h2 tabindex="-1" dir="auto">2.1 Definition and Notation</h2><a id="user-content-21-definition-and-notation" aria-label="Permalink: 2.1 Definition and Notation" href="#21-definition-and-notation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Matrices are the central objects of linear algebra, providing a compact way to represent and manipulate linear
transformations, systems of equations, and structured data. A matrix is a rectangular array of numbers arranged in rows
and columns.</p>

<p dir="auto">An <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer> matrix is an array with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m$</math-renderer> rows and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> columns, written</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Each entry <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer> is a scalar, located in the <em>i</em>-th row and <em>j</em>-th column. The size (or dimension) of the matrix is
denoted by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer>.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m = n$</math-renderer>, the matrix is square.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m = 1$</math-renderer>, the matrix is a row vector.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n = 1$</math-renderer>, the matrix is a column vector.</li>
</ul>
<p dir="auto">Thus, vectors are simply special cases of matrices.</p>

<p dir="auto">Example 2.1.1. A <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 3$</math-renderer> matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; -2 &amp; 4 \\
0 &amp; 3 &amp; 5
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{12} = -2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{23} = 5$</math-renderer>, and the matrix has 2 rows, 3 columns.</p>
<p dir="auto">Example 2.1.2. A <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> square matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
B = \begin{bmatrix}
2 &amp; 0 &amp; 1 \\
-1 &amp; 3 &amp; 4 \\
0 &amp; 5 &amp; -2
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">This will later serve as the representation of a linear transformation on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</p>

<ul dir="auto">
<li>Matrices are denoted by uppercase bold letters: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A, B, C$</math-renderer>.</li>
<li>Entries are written as <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer>, with the row index first, column index second.</li>
<li>The set of all real <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer> matrices is denoted <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^{m \times n}$</math-renderer>.</li>
</ul>
<p dir="auto">Thus, a matrix is a function <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A: {1,\dots,m} \times {1,\dots,n} \to \mathbb{R}$</math-renderer>, assigning a scalar to each row-column
position.</p>

<p dir="auto">Matrices generalize vectors and give us a language for describing linear operations systematically. They encode systems
of equations, rotations, projections, and transformations of data. With matrices, algebra and geometry come together: a
single compact object can represent both numerical data and functional rules.</p>

<ol dir="auto">
<li>Write a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 2$</math-renderer> matrix of your choice and identify its entries <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer>.</li>
<li>Is every vector a matrix? Is every matrix a vector? Explain.</li>
<li>Which of the following are square
matrices: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{4\times4}$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B \in \mathbb{R}^{3\times5}$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C \in \mathbb{R}^{1\times1}$</math-renderer>?</li>
<li>Let $D = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; 1 \end{bmatrix}$. What kind of matrix is this?</li>
<li>Consider the matrix $E = \begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix}$. Express <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$e_{11}, e_{12}, e_{21}, e_{22}$</math-renderer>
explicitly.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">2.2 Matrix Addition and Multiplication</h2><a id="user-content-22-matrix-addition-and-multiplication" aria-label="Permalink: 2.2 Matrix Addition and Multiplication" href="#22-matrix-addition-and-multiplication"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Once matrices are defined, the next step is to understand how they combine. Just as vectors gain meaning through
addition and scalar multiplication, matrices become powerful through two operations: addition and multiplication.</p>

<p dir="auto">Two matrices of the same size are added by adding corresponding entries. If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = [a_{ij}] \in \mathbb{R}^{m \times n}, \quad
B = [b_{ij}] \in \mathbb{R}^{m \times n},
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A + B = [a_{ij} + b_{ij}] \in \mathbb{R}^{m \times n}.
$$</math-renderer></p>
<p dir="auto">Example 2.2.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}, \quad
B = \begin{bmatrix}
-1 &amp; 0 \\
5 &amp; 2
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>

<p dir="auto">\begin{bmatrix}
0 &amp; 2 \
8 &amp; 6
\end{bmatrix}.
$$</p>
<p dir="auto">Matrix addition is commutative (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A+B = B+A$</math-renderer>) and associative ($(A+B)+C = A+(B+C)$). The zero matrix, with all entries 0,
acts as the additive identity.</p>

<p dir="auto">For a scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = [[a_{ij}]$</math-renderer>, we define</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
cA = [c \cdot a_{ij}].
$$</math-renderer></p>
<p dir="auto">This stretches or shrinks all entries of the matrix uniformly.</p>
<p dir="auto">Example 2.2.2.
If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
2 &amp; -1 \\
0 &amp; 3
\end{bmatrix}, \quad c = -2,
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
cA = \begin{bmatrix}
-4 &amp; 2 \\
0 &amp; -6
\end{bmatrix}.
$$</math-renderer></p>

<p dir="auto">The defining operation of matrices is multiplication. If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A \in \mathbb{R}^{m \times n}, \quad B \in \mathbb{R}^{n \times p},
$$</math-renderer></p>
<p dir="auto">then their product is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times p$</math-renderer> matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
AB = C = [c_{ij}], \quad c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
$$</math-renderer></p>
<p dir="auto">Thus, the entry in the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th row and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>-th column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB$</math-renderer> is the dot product of the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th row of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> with the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>-th
column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer>.</p>
<p dir="auto">Example 2.2.3.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 \\
0 &amp; 3
\end{bmatrix}, \quad
B = \begin{bmatrix}
4 &amp; -1 \\
2 &amp; 5
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>

<p dir="auto">\begin{bmatrix}
8 &amp; 9 \
6 &amp; 15
\end{bmatrix}.
$$</p>
<p dir="auto">Notice that matrix multiplication is not commutative in general: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB \neq BA$</math-renderer>. Sometimes <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$BA$</math-renderer> may not even be defined if
dimensions do not align.</p>

<p dir="auto">Matrix multiplication corresponds to the composition of linear transformations. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> transforms vectors
in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> transforms vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^p$</math-renderer>, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB$</math-renderer> represents applying <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> first, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>. This
makes matrices the algebraic language of transformations.</p>

<ul dir="auto">
<li>Matrix sum: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A+B$</math-renderer>.</li>
<li>Scalar multiple: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$cA$</math-renderer>.</li>
<li>Product: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB$</math-renderer>, defined only when the number of columns of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> equals the number of rows of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer>.</li>
</ul>

<p dir="auto">Matrix multiplication is the core mechanism of linear algebra: it encodes how transformations combine, how systems of
equations are solved, and how data flows in modern algorithms. Addition and scalar multiplication make matrices into a
vector space, while multiplication gives them an algebraic structure rich enough to model geometry, computation, and
networks.</p>

<ol dir="auto">
<li>Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A+B$</math-renderer> for</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 3 \ -1 &amp; 0 \end{bmatrix}, \quad
B = \begin{bmatrix} 4 &amp; -2 \ 5 &amp; 7 \end{bmatrix}.
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3A$</math-renderer> where</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; -4 \ 2 &amp; 6 \end{bmatrix}.
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>Multiply</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 0 &amp; 2 \ -1 &amp; 3 &amp; 1 \end{bmatrix}, \quad
B = \begin{bmatrix} 2 &amp; 1 \ 0 &amp; -1 \ 3 &amp; 4 \end{bmatrix}.
$$</math-renderer></p>
<ol start="4" dir="auto">
<li>Verify with an explicit example that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$AB \neq BA$</math-renderer>.</li>
<li>Prove that matrix multiplication is distributive: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A(B+C) = AB + AC$</math-renderer>.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">2.3 Transpose and Inverse</h2><a id="user-content-23-transpose-and-inverse" aria-label="Permalink: 2.3 Transpose and Inverse" href="#23-transpose-and-inverse"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Two special operations on matrices-the transpose and the inverse-give rise to deep algebraic and geometric properties.
The transpose rearranges a matrix by flipping it across its main diagonal, while the inverse, when it exists, acts as
the undo operation for matrix multiplication.</p>

<p dir="auto">The transpose of an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m \times n$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = [a_{ij}]$</math-renderer> is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times m$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T = [a_{ji}]$</math-renderer>, obtained by swapping
rows and columns.</p>
<p dir="auto">Formally,</p>
<p dir="auto">$$
(A^T)<em>{ij} = a</em>{ji}.
$$</p>
<p dir="auto">Example 2.3.1.
If</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 4 &amp; -2 \\
0 &amp; 3 &amp; 5
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T = \begin{bmatrix}
1 &amp; 0 \\
4 &amp; 3 \\
-2 &amp; 5
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Properties of the Transpose.</p>
<ol dir="auto">
<li>$ (A^T)^T = A$.</li>
<li>$ (A+B)^T = A^T + B^T$.</li>
<li>$ (cA)^T = cA^T$, for scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer>.</li>
<li>$ (AB)^T = B^T A^T$.</li>
</ol>
<p dir="auto">The last rule is crucial: the order reverses.</p>

<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is said to be invertible (or nonsingular) if there exists another
matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer> such that</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
AA^{-1} = A^{-1}A = I_n,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> identity matrix. In this case, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer> is called the inverse of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
<p dir="auto">Not every matrix is invertible. A necessary condition is that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>, a fact that will be developed in Chapter
6.</p>
<p dir="auto">Example 2.3.2.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Its determinant is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = (1)(4) - (2)(3) = -2 \neq 0$</math-renderer>. The inverse is</p>

<p dir="auto">\begin{bmatrix}
-2 &amp; 1 \
1.5 &amp; -0.5
\end{bmatrix}.
$$</p>
<p dir="auto">Verification:</p>

<p dir="auto">\begin{bmatrix}
1 &amp; 0 \
0 &amp; 1
\end{bmatrix}.
$$</p>

<ul dir="auto">
<li>The transpose corresponds to reflecting a linear transformation across the diagonal. For vectors, it switches between
row and column forms.</li>
<li>The inverse, when it exists, corresponds to reversing a linear transformation. For example, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> scales and rotates
vectors, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer> rescales and rotates them back.</li>
</ul>

<ul dir="auto">
<li>Transpose: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T$</math-renderer>.</li>
<li>Inverse: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^{-1}$</math-renderer>, defined only for invertible square matrices.</li>
<li>Identity: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer>, acts as the multiplicative identity.</li>
</ul>

<p dir="auto">The transpose allows us to define symmetric and orthogonal matrices, central to geometry and numerical methods. The
inverse underlies the solution of linear systems, encoding the idea of undoing a transformation. Together, these
operations set the stage for determinants, eigenvalues, and orthogonalization.</p>

<ol dir="auto">
<li>Compute the transpose of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; -1 &amp; 3 \ 0 &amp; 4 &amp; 5 \end{bmatrix}.
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(AB)^T = B^T A^T$</math-renderer> for</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 \ 0 &amp; 1 \end{bmatrix}, \quad
B = \begin{bmatrix} 3 &amp; 4 \ 5 &amp; 6 \end{bmatrix}.
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>Determine whether</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
C = \begin{bmatrix} 2 &amp; 1 \ 4 &amp; 2 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is invertible. If so, find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C^{-1}$</math-renderer>.</p>
<ol start="4" dir="auto">
<li>Find the inverse of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D = \begin{bmatrix} 0 &amp; 1 \ -1 &amp; 0 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">and explain its geometric action on vectors in the plane.</p>
<ol start="5" dir="auto">
<li>Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible, then so is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A^T)^{-1} = (A^{-1})^T$</math-renderer>.</li>
</ol>

<p dir="auto">Certain matrices occur so frequently in theory and applications that they are given special names. Recognizing their
properties allows us to simplify computations and understand the structure of linear transformations more clearly.</p>

<p dir="auto">The identity matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> is the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix with ones on the diagonal and zeros elsewhere:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
I_n = \begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">It acts as the multiplicative identity:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
AI_n = I_nA = A, \quad \text{for all } A \in \mathbb{R}^{n \times n}.
$$</math-renderer></p>
<p dir="auto">Geometrically, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> represents the transformation that leaves every vector unchanged.</p>

<p dir="auto">A diagonal matrix has all off-diagonal entries zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D = \begin{bmatrix}
d_{11} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; d_{22} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; d_{nn}
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Multiplication by a diagonal matrix scales each coordinate independently:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D\mathbf{x} = (d_{11}x_1, d_{22}x_2, \dots, d_{nn}x_n).
$$</math-renderer></p>
<p dir="auto">Example 2.4.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D = \begin{bmatrix} 2 &amp; 0 &amp; 0 \ 0 &amp; 3 &amp; 0 \ 0 &amp; 0 &amp; -1 \end{bmatrix}, \quad
\mathbf{x} = \begin{bmatrix} 1 \ 4 \ -2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
D\mathbf{x} = \begin{bmatrix} 2 \ 12 \ 2 \end{bmatrix}.
$$</math-renderer></p>

<p dir="auto">A permutation matrix is obtained by permuting the rows of the identity matrix. Multiplying a vector by a permutation
matrix reorders its coordinates.</p>
<p dir="auto">Example 2.4.2.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P\begin{bmatrix} a \ b \ c \end{bmatrix} =
\begin{bmatrix} b \ a \ c \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> swaps the first two coordinates.</p>
<p dir="auto">Permutation matrices are always invertible; their inverses are simply their transposes.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Symmetric and Skew-Symmetric Matrices</h3><a id="user-content-symmetric-and-skew-symmetric-matrices" aria-label="Permalink: Symmetric and Skew-Symmetric Matrices" href="#symmetric-and-skew-symmetric-matrices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">A matrix is symmetric if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T = A,
$$</math-renderer></p>
<p dir="auto">and skew-symmetric if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T = -A.
$$</math-renderer></p>
<p dir="auto">Symmetric matrices appear in quadratic forms and optimization, while skew-symmetric matrices describe rotations and
cross products in geometry.</p>

<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> is orthogonal if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q^T Q = QQ^T = I.
$$</math-renderer></p>
<p dir="auto">Equivalently, the rows (and columns) of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> form an orthonormal set. Orthogonal matrices preserve lengths and angles;
they represent rotations and reflections.</p>
<p dir="auto">Example 2.4.3.
The rotation matrix in the plane:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R(\theta) = \begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is orthogonal, since</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R(\theta)^T R(\theta) = I_2.
$$</math-renderer></p>

<p dir="auto">Special matrices serve as the building blocks of linear algebra. Identity matrices define the neutral element, diagonal
matrices simplify computations, permutation matrices reorder data, symmetric and orthogonal matrices describe
fundamental geometric structures. Much of modern applied mathematics reduces complex problems to operations involving
these simple forms.</p>

<ol dir="auto">
<li>Show that the product of two diagonal matrices is diagonal, and compute an example.</li>
<li>Find the permutation matrix that cycles <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a,b,c)$</math-renderer> into <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(b,c,a)$</math-renderer>.</li>
<li>Prove that every permutation matrix is invertible and its inverse is its transpose.</li>
<li>Verify that</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q = \begin{bmatrix} 0 &amp; 1 \ -1 &amp; 0 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is orthogonal. What geometric transformation does it represent?
5. Determine whether</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 3 \ 3 &amp; 2 \end{bmatrix}, \quad
B = \begin{bmatrix} 0 &amp; 5 \ -5 &amp; 0 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">are symmetric, skew-symmetric, or neither.</p>

<div dir="auto"><h2 tabindex="-1" dir="auto">3.1 Linear Systems and Solutions</h2><a id="user-content-31-linear-systems-and-solutions" aria-label="Permalink: 3.1 Linear Systems and Solutions" href="#31-linear-systems-and-solutions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">One of the central motivations for linear algebra is solving systems of linear equations. These systems arise naturally
in science, engineering, and data analysis whenever multiple constraints interact. Matrices provide a compact language
for expressing and solving them.</p>

<p dir="auto">A linear system consists of equations where each unknown appears only to the first power and with no products between
variables. A general system of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m$</math-renderer> equations in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> unknowns can be written as:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &amp;= b_1, \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &amp;= b_2, \\
&amp;\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &amp;= b_m.
\end{aligned}
$$</math-renderer></p>
<p dir="auto">Here the coefficients <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij}$</math-renderer> and constants <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$b_i$</math-renderer> are scalars, and the unknowns are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x_1, x_2, \dots, x_n$</math-renderer>.</p>

<p dir="auto">The system can be expressed compactly as:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{x} = \mathbf{b},
$$</math-renderer></p>
<p dir="auto">where</p>
<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer> is the coefficient matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$[a_{ij}]$</math-renderer>,</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x} \in \mathbb{R}^n$</math-renderer> is the column vector of unknowns,</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b} \in \mathbb{R}^m$</math-renderer> is the column vector of constants.</li>
</ul>
<p dir="auto">This formulation turns the problem of solving equations into analyzing the action of a matrix.</p>
<p dir="auto">Example 3.1.1.
The system</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + 2y = 5, \\
3x - y = 4
\end{cases}
$$</math-renderer></p>
<p dir="auto">can be written as</p>

<p dir="auto">\begin{bmatrix} 5 \ 4 \end{bmatrix}.
$$</p>

<p dir="auto">A linear system may have:</p>
<ol dir="auto">
<li>
<p dir="auto">No solution (inconsistent): The equations conflict.
Example:
$
\begin{cases}
x + y = 1 \
x + y = 2
\end{cases}
$
has no solution.</p>
</li>
<li>
<p dir="auto">Exactly one solution (unique): The system’s equations intersect at a single point.
Example: The above system with coefficient matrix $
\begin{bmatrix} 1 &amp; 2 \ 3 &amp; -1 \end{bmatrix}
$ has a unique solution.</p>
</li>
<li>
<p dir="auto">Infinitely many solutions: The equations describe overlapping constraints (e.g., multiple equations representing the
same line or plane).</p>
</li>
</ol>
<p dir="auto">The nature of the solution depends on the rank of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> and its relation to the augmented matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A|\mathbf{b})$</math-renderer>, which
we will study later.</p>

<ul dir="auto">
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, each linear equation represents a line. Solving a system means finding intersection points of
lines.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, each equation represents a plane. A system may have no solution (parallel planes), one solution (a
unique intersection point), or infinitely many (a line of intersection).</li>
<li>In higher dimensions, the picture generalizes: solutions form intersections of hyperplanes.</li>
</ul>

<p dir="auto">Linear systems are the practical foundation of linear algebra. They appear in balancing chemical reactions, circuit
analysis, least-squares regression, optimization, and computer graphics. Understanding how to represent and classify
their solutions is the first step toward systematic solution methods like Gaussian elimination.</p>

<ol dir="auto">
<li>
<p dir="auto">Write the following system in matrix form:
$
\begin{cases}
2x + 3y - z = 7, \
x - y + 4z = 1, \
3x + 2y + z = 5
\end{cases}
$</p>
</li>
<li>
<p dir="auto">Determine whether the system
$
\begin{cases}
x + y = 1, \
2x + 2y = 2
\end{cases}
$
has no solution, one solution, or infinitely many solutions.</p>
</li>
<li>
<p dir="auto">Geometrically interpret the system
$
\begin{cases}
x + y = 3, \
x - y = 1
\end{cases}
$
in the plane.</p>
</li>
<li>
<p dir="auto">Solve the system
$
\begin{cases}
2x + y = 1, \
x - y = 4
\end{cases}
$
and check your solution.</p>
</li>
<li>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, describe the solution set of
$
\begin{cases}
x + y + z = 0, \
2x + 2y + 2z = 0
\end{cases}
$.
What geometric object does it represent?</p>
</li>
</ol>

<p dir="auto">To solve linear systems efficiently, we use Gaussian elimination: a systematic method of transforming a system into a
simpler equivalent one whose solutions are easier to see. The method relies on elementary row operations that preserve
the solution set.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Elementary Row Operations</h3><a id="user-content-elementary-row-operations" aria-label="Permalink: Elementary Row Operations" href="#elementary-row-operations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">On an augmented matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A|\mathbf{b})$</math-renderer>, we are allowed three operations:</p>
<ol dir="auto">
<li>Row swapping: interchange two rows.</li>
<li>Row scaling: multiply a row by a nonzero scalar.</li>
<li>Row replacement: replace one row by itself plus a multiple of another row.</li>
</ol>
<p dir="auto">These operations correspond to re-expressing equations in different but equivalent forms.</p>

<p dir="auto">A matrix is in row echelon form (REF) if:</p>
<ol dir="auto">
<li>All nonzero rows are above any zero rows.</li>
<li>Each leading entry (the first nonzero number from the left in a row) is to the right of the leading entry in the row
above.</li>
<li>All entries below a leading entry are zero.</li>
</ol>
<p dir="auto">Further, if each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced row echelon
form (RREF).</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Algorithm of Gaussian Elimination</h3><a id="user-content-algorithm-of-gaussian-elimination" aria-label="Permalink: Algorithm of Gaussian Elimination" href="#algorithm-of-gaussian-elimination"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol dir="auto">
<li>Write the augmented matrix for the system.</li>
<li>Use row operations to create zeros below each pivot (the leading entry in a row).</li>
<li>Continue column by column until the matrix is in echelon form.</li>
<li>Solve by back substitution: starting from the last pivot equation and working upward.</li>
</ol>
<p dir="auto">If we continue to RREF, the solution can be read off directly.</p>

<p dir="auto">Example 3.2.1. Solve</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + 2y - z = 3, \\
2x + y + z = 7, \\
3x - y + 2z = 4.
\end{cases}
$$</math-renderer></p>
<p dir="auto">Step 1. Augmented matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
2 &amp; 1 &amp; 1 &amp; 7 \\
3 &amp; -1 &amp; 2 &amp; 4
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 2. Eliminate below the first pivot</p>
<p dir="auto">Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; -3 &amp; 3 &amp; 1 \\
0 &amp; -7 &amp; 5 &amp; -5
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 3. Pivot in column 2</p>
<p dir="auto">Divide row 2 by -3:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; 1 &amp; -1 &amp; -\tfrac{1}{3} \\
0 &amp; -7 &amp; 5 &amp; -5
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Add 7 times row 2 to row 3:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; 1 &amp; -1 &amp; -\tfrac{1}{3} \\
0 &amp; 0 &amp; -2 &amp; -\tfrac{22}{3}
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 4. Pivot in column 3</p>
<p dir="auto">Divide row 3 by -2:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 2 &amp; -1 &amp; 3 \\
0 &amp; 1 &amp; -1 &amp; -\tfrac{1}{3} \\
0 &amp; 0 &amp; 1 &amp; \tfrac{11}{3}
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Step 5. Back substitution</p>
<p dir="auto">From the last row:
$
z = \tfrac{11}{3}.
$</p>
<p dir="auto">Second row:
$
y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} = \tfrac{10}{3}.
$</p>
<p dir="auto">First row:
$
x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3.
$</p>
<p dir="auto">So
$
x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0.
$</p>
<p dir="auto">Solution:
$
(x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big).
$</p>

<p dir="auto">Gaussian elimination is the foundation of computational linear algebra. It reduces complex systems to a form where
solutions are visible, and it forms the basis for algorithms used in numerical analysis, scientific computing, and
machine learning.</p>

<ol dir="auto">
<li>
<p dir="auto">Solve by Gaussian elimination:
$
\begin{cases}
x + y = 2, \
2x - y = 0.
\end{cases}
$</p>
</li>
<li>
<p dir="auto">Reduce the following augmented matrix to REF:
$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 6 \
2 &amp; -1 &amp; 3 &amp; 14 \
1 &amp; 4 &amp; -2 &amp; -2
\end{array}\right].
$</p>
</li>
<li>
<p dir="auto">Show that Gaussian elimination always produces either:</p>
<ul dir="auto">
<li>a unique solution,</li>
<li>infinitely many solutions, or</li>
<li>a contradiction (no solution).</li>
</ul>
</li>
<li>
<p dir="auto">Use Gaussian elimination to find all solutions of
$
\begin{cases}
x + y + z = 0, \
2x + y + z = 1.
\end{cases}
$</p>
</li>
<li>
<p dir="auto">Explain why pivoting (choosing the largest available pivot element) is useful in numerical computation.</p>
</li>
</ol>

<p dir="auto">Gaussian elimination not only provides solutions but also reveals the structure of a linear system. Two key ideas are
the rank of a matrix and the consistency of a system. Rank measures the amount of independent information in the
equations, while consistency determines whether the system has at least one solution.</p>

<p dir="auto">The rank of a matrix is the number of leading pivots in its row echelon form. Equivalently, it is the maximum number of
linearly independent rows or columns.</p>
<p dir="auto">Formally,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{rank}(A) = \dim(\text{row space of } A) = \dim(\text{column space of } A).
$$</math-renderer></p>
<p dir="auto">The rank tells us the effective dimension of the space spanned by the rows (or columns).</p>
<p dir="auto">Example 3.3.1.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 4 &amp; 6 \\
3 &amp; 6 &amp; 9
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">row reduction gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = 1$</math-renderer>, since all rows are multiples of the first.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Consistency of Linear Systems</h3><a id="user-content-consistency-of-linear-systems" aria-label="Permalink: Consistency of Linear Systems" href="#consistency-of-linear-systems"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Consider the system <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} = \mathbf{b}$</math-renderer>.
The system is consistent (has at least one solution) if and only if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{rank}(A) = \text{rank}(A|\mathbf{b}),
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A|\mathbf{b})$</math-renderer> is the augmented matrix.
If the ranks differ, the system is inconsistent.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$</math-renderer> (number of unknowns), the system has a unique solution.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = \text{rank}(A|\mathbf{b}) &amp;lt; n$</math-renderer>, the system has infinitely many solutions.</li>
</ul>

<p dir="auto">Example 3.3.2.
Consider</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 1, \\
2x + 2y + 2z = 2, \\
x + y + z = 3.
\end{cases}
$$</math-renderer></p>
<p dir="auto">The augmented matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 1 \\
2 &amp; 2 &amp; 2 &amp; 2 \\
1 &amp; 1 &amp; 1 &amp; 3
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Row reduction gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = 1$</math-renderer>, but <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A|\mathbf{b}) = 2$</math-renderer>. Since the ranks differ, the system is inconsistent: no
solution exists.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Example with Infinite Solutions</h3><a id="user-content-example-with-infinite-solutions" aria-label="Permalink: Example with Infinite Solutions" href="#example-with-infinite-solutions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Example 3.3.3.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y = 2, \\
2x + 2y = 4,
\end{cases}
$$</math-renderer></p>
<p dir="auto">the augmented matrix reduces to</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{cc|c}
1 &amp; 1 &amp; 2 \\
0 &amp; 0 &amp; 0
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = \text{rank}(A|\mathbf{b}) = 1 &amp;lt; 2$</math-renderer>. Thus, infinitely many solutions exist, forming a line.</p>

<p dir="auto">Rank is a measure of independence: it tells us how many truly distinct equations or directions are present. Consistency
explains when equations align versus when they contradict. These concepts connect linear systems to vector spaces and
prepare for the ideas of dimension, basis, and the Rank–Nullity Theorem.</p>

<ol dir="auto">
<li>Compute the rank of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 &amp; 1 \\
0 &amp; 1 &amp; -1 \\
2 &amp; 5 &amp; -1
\end{bmatrix}.
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Determine whether the system</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 1, \\
2x + 3y + z = 2, \\
3x + 5y + 2z = 3
\end{cases}
$$</math-renderer></p>
<p dir="auto">is consistent.</p>
<ol start="3" dir="auto">
<li>
<p dir="auto">Show that the rank of the identity matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer> is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Give an example of a system in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> with infinitely many solutions, and explain why it satisfies the rank
condition.</p>
</li>
<li>
<p dir="auto">Prove that for any matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer>,
$
\text{rank}(A) \leq \min(m,n).
$</p>
</li>
</ol>

<p dir="auto">A homogeneous system is a linear system in which all constant terms are zero:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{x} = \mathbf{0},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0}$</math-renderer> is the zero vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^m$</math-renderer>.</p>

<p dir="auto">Every homogeneous system has at least one solution:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x} = \mathbf{0}.
$$</math-renderer></p>
<p dir="auto">This is called the trivial solution. The interesting question is whether <em>nontrivial solutions</em> (nonzero vectors) exist.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Existence of Nontrivial Solutions</h3><a id="user-content-existence-of-nontrivial-solutions" aria-label="Permalink: Existence of Nontrivial Solutions" href="#existence-of-nontrivial-solutions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Nontrivial solutions exist precisely when the number of unknowns exceeds the rank of the coefficient matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{rank}(A) &lt; n.
$$</math-renderer></p>
<p dir="auto">In this case, there are infinitely many solutions, forming a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>. The dimension of this solution
space is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\dim(\text{null}(A)) = n - \text{rank}(A),
$$</math-renderer></p>
<p dir="auto">where null(A) is the set of all solutions to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} = 0$</math-renderer>. This set is called the null space or kernel of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>

<p dir="auto">Example 3.4.1.
Consider</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 0, \\
2x + y - z = 0.
\end{cases}
$$</math-renderer></p>
<p dir="auto">The augmented matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 0 \\
2 &amp; 1 &amp; -1 &amp; 0
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">Row reduction:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; -3 &amp; 0
\end{array}\right]
\quad\to\quad
\left[\begin{array}{ccc|c}
1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 3 &amp; 0
\end{array}\right].
$$</math-renderer></p>
<p dir="auto">So the system is equivalent to:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + y + z = 0, \\
y + 3z = 0.
\end{cases}
$$</math-renderer></p>
<p dir="auto">From the second equation, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = -3z$</math-renderer>. Substituting into the first:
$
x - 3z + z = 0 \implies x = 2z.
$</p>
<p dir="auto">Thus solutions are:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
(x,y,z) = z(2, -3, 1), \quad z \in \mathbb{R}.
$$</math-renderer></p>
<p dir="auto">The null space is the line spanned by the vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2, -3, 1)$</math-renderer>.</p>

<p dir="auto">The solution set of a homogeneous system is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = n$</math-renderer>, the only solution is the zero vector.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = n-1$</math-renderer>, the solution set is a line through the origin.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = n-2$</math-renderer>, the solution set is a plane through the origin.</li>
</ul>
<p dir="auto">More generally, the null space has dimension <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n - \text{rank}(A)$</math-renderer>, known as the nullity.</p>

<p dir="auto">Homogeneous systems are central to understanding vector spaces, subspaces, and dimension. They lead directly to the
concepts of kernel, null space, and linear dependence. In applications, homogeneous systems appear in equilibrium
problems, eigenvalue equations, and computer graphics transformations.</p>

<ol dir="auto">
<li>Solve the homogeneous system</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x + 2y - z = 0, \\
2x + 4y - 2z = 0.
\end{cases}
$$</math-renderer></p>
<p dir="auto">What is the dimension of its solution space?</p>
<ol start="2" dir="auto">
<li>Find all solutions of</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
x - y + z = 0, \\
2x + y - z = 0.
\end{cases}
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>
<p dir="auto">Show that the solution set of any homogeneous system is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> matrix with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{rank}(A) = 2$</math-renderer>. What is the dimension of the null space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>?</p>
</li>
<li>
<p dir="auto">For</p>
</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 &amp; -1 \ 0 &amp; 1 &amp; 3 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">compute a basis for the null space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>

<div dir="auto"><h2 tabindex="-1" dir="auto">4.1 Definition of a Vector Space</h2><a id="user-content-41-definition-of-a-vector-space" aria-label="Permalink: 4.1 Definition of a Vector Space" href="#41-definition-of-a-vector-space"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Up to now we have studied vectors and matrices concretely in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>. The next step is to move beyond coordinates
and define vector spaces in full generality. A vector space is an abstract setting where the familiar rules of addition
and scalar multiplication hold, regardless of whether the elements are geometric vectors, polynomials, functions, or
other objects.</p>

<p dir="auto">A vector space over the real numbers <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer> is a set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> equipped with two operations:</p>
<ol dir="auto">
<li>Vector addition: For any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in V$</math-renderer>, there is a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v} \in V$</math-renderer>.</li>
<li>Scalar multiplication: For any scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer>, there is a
vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c\mathbf{v} \in V$</math-renderer>.</li>
</ol>
<p dir="auto">These operations must satisfy the following axioms (for all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$</math-renderer> and all
scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a,b \in \mathbb{R}$</math-renderer>):</p>
<ol dir="auto">
<li>Commutativity of addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$</math-renderer>.</li>
<li>Associativity of addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$</math-renderer>.</li>
<li>Additive identity: There exists a zero vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0} \in V$</math-renderer> such that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} + \mathbf{0} = \mathbf{v}$</math-renderer>.</li>
<li>Additive inverses: For each <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer>, there exists <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(-\mathbf{v} \in V$</math-renderer> such
that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$</math-renderer>.</li>
<li>Compatibility of scalar multiplication: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a(b\mathbf{v}) = (ab)\mathbf{v}$</math-renderer>.</li>
<li>Identity element of scalars: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1 \cdot \mathbf{v} = \mathbf{v}$</math-renderer>.</li>
<li>Distributivity over vector addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$</math-renderer>.</li>
<li>Distributivity over scalar addition: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$</math-renderer>.</li>
</ol>
<p dir="auto">If a set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> with operations satisfies all eight axioms, we call it a vector space.</p>

<p dir="auto">Example 4.1.1. Standard Euclidean space
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> with ordinary addition and scalar multiplication is a vector space. This is the model case from which the
axioms are abstracted.</p>
<p dir="auto">Example 4.1.2. Polynomials
The set of all polynomials with real coefficients, denoted <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}[x]$</math-renderer>, forms a vector space. Addition and scalar
multiplication are defined term by term.</p>
<p dir="auto">Example 4.1.3. Functions
The set of all real-valued functions on an interval, e.g. <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$f: [0,1] \to \mathbb{R}$</math-renderer>, forms a vector space, since
functions can be added and scaled pointwise.</p>

<p dir="auto">Not every set with operations qualifies. For instance, the set of positive real numbers under usual addition is not a
vector space, because additive inverses (negative numbers) are missing. The axioms must all hold.</p>

<p dir="auto">In familiar cases like <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> or <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, vector spaces provide the stage for geometry: vectors can be
added, scaled, and combined to form lines, planes, and higher-dimensional structures. In abstract settings like function
spaces, the same algebraic rules let us apply geometric intuition to infinite-dimensional problems.</p>

<p dir="auto">The concept of vector space unifies seemingly different mathematical objects under a single framework. Whether dealing
with forces in physics, signals in engineering, or data in machine learning, the common language of vector spaces allows
us to use the same techniques everywhere.</p>

<ol dir="auto">
<li>Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> with standard addition and scalar multiplication satisfies all eight vector space axioms.</li>
<li>Show that the set of integers <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{Z}$</math-renderer> with ordinary operations is not a vector space over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. Which
axiom fails?</li>
<li>Consider the set of all polynomials of degree at most 3. Show it forms a vector space over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. What is its
dimension?</li>
<li>Give an example of a vector space where the vectors are not geometric objects.</li>
<li>Prove that in any vector space, the zero vector is unique.</li>
</ol>

<p dir="auto">A subspace is a smaller vector space living inside a larger one. Just as lines and planes naturally sit inside
three-dimensional space, subspaces generalize these ideas to higher dimensions and more abstract settings.</p>

<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> be a vector space. A subset <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W \subseteq V$</math-renderer> is called a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> if:</p>
<ol dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0} \in W$</math-renderer> (contains the zero vector),</li>
<li>For all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in W$</math-renderer>, the sum <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} + \mathbf{v} \in W$</math-renderer> (closed under addition),</li>
<li>For all scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer> and vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in W$</math-renderer>, the product <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c\mathbf{v} \in W$</math-renderer> (closed under
scalar multiplication).</li>
</ol>
<p dir="auto">If these hold, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> is itself a vector space with the inherited operations.</p>

<p dir="auto">Example 4.2.1. Line through the origin in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>
The set</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
W = { (t, 2t) \mid t \in \mathbb{R} }
$$</math-renderer></p>
<p dir="auto">is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>. It contains the zero vector, is closed under addition, and is closed under scalar
multiplication.</p>
<p dir="auto">Example 4.2.2. The x–y plane in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>
The set</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
W = { (x, y, 0) \mid x,y \in \mathbb{R} }
$$</math-renderer></p>
<p dir="auto">is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>. It is the collection of all vectors lying in the plane through the origin parallel to
the x–y plane.</p>
<p dir="auto">Example 4.2.3. Null space of a matrix
For a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{m \times n}$</math-renderer>, the null space</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} }
$$</math-renderer></p>
<p dir="auto">is a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>. This subspace represents all solutions to the homogeneous system.</p>

<p dir="auto">Not every subset is a subspace.</p>
<ul dir="auto">
<li>The set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$</math-renderer> is not a subspace: it is not closed under scalar multiplication (a
negative scalar breaks the condition).</li>
<li>Any line in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> that does not pass through the origin is not a subspace, because it does not
contain <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{0}$</math-renderer>.</li>
</ul>

<p dir="auto">Subspaces are the linear structures inside vector spaces.</p>
<ul dir="auto">
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, the subspaces are: the zero vector, any line through the origin, or the entire plane.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the subspaces are: the zero vector, any line through the origin, any plane through the origin, or
the entire space.</li>
<li>In higher dimensions, the same principle applies: subspaces are the flat linear pieces through the origin.</li>
</ul>

<p dir="auto">Subspaces capture the essential structure of linear problems. Column spaces, row spaces, and null spaces are all
subspaces. Much of linear algebra consists of understanding how these subspaces intersect, span, and complement each
other.</p>

<ol dir="auto">
<li>Prove that the set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$</math-renderer> is a subspace.</li>
<li>Show that the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ (1+t, 2t) \mid t \in \mathbb{R} }$</math-renderer> is not a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>. Which condition fails?</li>
<li>Determine whether the set of all vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z) \in \mathbb{R}^3$</math-renderer> satisfying <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x+y+z=0$</math-renderer> is a subspace.</li>
<li>For the matrix</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">describe the null space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> as a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.
5. List all possible subspaces of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">4.3 Span, Basis, Dimension</h2><a id="user-content-43-span-basis-dimension" aria-label="Permalink: 4.3 Span, Basis, Dimension" href="#43-span-basis-dimension"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The ideas of span, basis, and dimension provide the language for describing the size and structure of subspaces.
Together, they tell us how a vector space is generated, how many building blocks it requires, and how those blocks can
be chosen.</p>

<p dir="auto">Given a set of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k} \subseteq V$</math-renderer>, the span is the collection of
all linear combinations:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{span}{\mathbf{v}_1, \dots, \mathbf{v}_k} = { c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k \mid c_i \in \mathbb{R} }.
$$</math-renderer></p>
<p dir="auto">The span is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>, namely the smallest subspace containing those vectors.</p>
<p dir="auto">Example 4.3.1.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, $ \text{span}{(1,0)} = {(x,0) \mid x \in \mathbb{R}},$ the x-axis.
Similarly, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{span}{(1,0),(0,1)} = \mathbb{R}^2.$</math-renderer></p>

<p dir="auto">A basis of a vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is a set of vectors that:</p>
<ol dir="auto">
<li>Span <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>.</li>
<li>Are linearly independent (no vector in the set is a linear combination of the others).</li>
</ol>
<p dir="auto">If either condition fails, the set is not a basis.</p>
<p dir="auto">Example 4.3.2.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the standard unit vectors</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$</math-renderer></p>
<p dir="auto">form a basis. Every vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z)$</math-renderer> can be uniquely written as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3.
$$</math-renderer></p>

<p dir="auto">The dimension of a vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>, written <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\dim(V)$</math-renderer>, is the number of vectors in any basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. This number is
well-defined: all bases of a vector space have the same cardinality.</p>
<p dir="auto">Examples 4.3.3.</p>
<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\dim(\mathbb{R}^2) = 2$</math-renderer>, with basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0), (0,1)$</math-renderer>.</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\dim(\mathbb{R}^3) = 3$</math-renderer>, with basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0,0), (0,1,0), (0,0,1)$</math-renderer>.</li>
<li>The set of polynomials of degree at most 3 has dimension 4, with basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1, x, x^2, x^3)$</math-renderer>.</li>
</ul>

<ul dir="auto">
<li>The span is like the reach of a set of vectors.</li>
<li>A basis is the minimal set of directions needed to reach everything in the space.</li>
<li>The dimension is the count of those independent directions.</li>
</ul>
<p dir="auto">Lines, planes, and higher-dimensional flats can all be described in terms of span, basis, and dimension.</p>

<p dir="auto">These concepts classify vector spaces and subspaces in terms of size and structure. Many theorems in linear algebra-such
as the Rank–Nullity Theorem-are consequences of understanding span, basis, and dimension. In practical terms, bases are
how we encode data in coordinates, and dimension tells us how much freedom a system truly has.</p>

<ol dir="auto">
<li>Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,0)$</math-renderer> span the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$xy$</math-renderer>-plane in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>. Are they a basis?</li>
<li>Find a basis for the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(2t,-3t,t) : t \in \mathbb{R}}$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>Determine the dimension of the subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> defined by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x+y+z=0$</math-renderer>.</li>
<li>Prove that any two different bases of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> must contain exactly <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> vectors.</li>
<li>Give a basis for the set of polynomials of degree <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\leq 2$</math-renderer>. What is its dimension?</li>
</ol>

<p dir="auto">Once a basis for a vector space is chosen, every vector can be expressed uniquely as a linear combination of the basis
vectors. The coefficients in this combination are called the coordinates of the vector relative to that basis.
Coordinates allow us to move between the abstract world of vector spaces and the concrete world of numbers.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Coordinates Relative to a Basis</h3><a id="user-content-coordinates-relative-to-a-basis" aria-label="Permalink: Coordinates Relative to a Basis" href="#coordinates-relative-to-a-basis"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> be a vector space, and let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathcal{B} = {\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n}
$$</math-renderer></p>
<p dir="auto">be an ordered basis for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. Every vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \in V$</math-renderer> can be written uniquely as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n.
$$</math-renderer></p>
<p dir="auto">The scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(c_1, c_2, \dots, c_n)$</math-renderer> are the coordinates of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> relative to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer>, written</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} c_1 \ c_2 \ \vdots \ c_n \end{bmatrix}.
$$</math-renderer></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Example in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>
</h3><a id="user-content-example-in-mathbbr2" aria-label="Permalink: Example in $\mathbb{R}^2$" href="#example-in-mathbbr2"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Example 4.4.1.
Let the basis be</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathcal{B} = { (1,1), (1,-1) }.
$$</math-renderer></p>
<p dir="auto">To find the coordinates of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (3,1)$</math-renderer> relative to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer>, solve</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
(3,1) = c_1(1,1) + c_2(1,-1).
$$</math-renderer></p>
<p dir="auto">This gives the system</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{cases}
c_1 + c_2 = 3, \\
c_1 - c_2 = 1.
\end{cases}
$$</math-renderer></p>
<p dir="auto">Adding: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2c_1 = 4 \implies c_1 = 2$</math-renderer>. Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c_2 = 1$</math-renderer>.</p>
<p dir="auto">So,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} 2 \ 1 \end{bmatrix}.
$$</math-renderer></p>

<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, the standard basis is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,\dots,0), \quad \mathbf{e}_2 = (0,1,0,\dots,0), \dots, \mathbf{e}_n = (0,\dots,0,1).
$$</math-renderer></p>
<p dir="auto">Relative to this basis, the coordinates of a vector are simply its entries. Thus, column vectors are coordinate
representations by default.</p>

<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {\mathbf{v}_1, \dots, \mathbf{v}_n}$</math-renderer> is a basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, the change of basis matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix} \mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">with basis vectors as columns. For any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>,</p>
<p dir="auto">$$
\mathbf{u} = P[\mathbf{u}]<em>{\mathcal{B}}, \qquad [\mathbf{u}]</em>{\mathcal{B}} = P^{-1}\mathbf{u}.
$$</p>
<p dir="auto">Thus, switching between bases reduces to matrix multiplication.</p>

<p dir="auto">Coordinates are the address of a vector relative to a chosen set of directions. Different bases are like different
coordinate systems: Cartesian, rotated, skewed, or scaled. The same vector may look very different numerically depending
on the basis, but its geometric identity is unchanged.</p>

<p dir="auto">Coordinates turn abstract vectors into concrete numerical data. Changing basis is the algebraic language for rotations
of axes, diagonalization of matrices, and principal component analysis in data science. Mastery of coordinates is
essential for moving fluidly between geometry, algebra, and computation.</p>

<ol dir="auto">
<li>Express <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(4,2)$</math-renderer> in terms of the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1), (1,-1)$</math-renderer>.</li>
<li>Find the coordinates of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2,3)$</math-renderer> relative to the standard basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {(2,0), (0,3)}$</math-renderer>, compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$[ (4,6) ]_{\mathcal{B}}$</math-renderer>.</li>
<li>Construct the change of basis matrix from the standard basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {(1,1), (1,-1)}$</math-renderer>.</li>
<li>Prove that coordinate representation with respect to a basis is unique.</li>
</ol>

<div dir="auto"><h2 tabindex="-1" dir="auto">5.1 Functions that Preserve Linearity</h2><a id="user-content-51-functions-that-preserve-linearity" aria-label="Permalink: 5.1 Functions that Preserve Linearity" href="#51-functions-that-preserve-linearity"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve
their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of
linear behavior.</p>

<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> be vector spaces over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>. A function</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T : V \to W
$$</math-renderer></p>
<p dir="auto">is called a linear transformation (or linear map) if for all vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v} \in V$</math-renderer> and all
scalars <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer>:</p>
<ol dir="auto">
<li>
<p dir="auto">Additivity:</p>
<p dir="auto">$$
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}),
$$</p>
</li>
<li>
<p dir="auto">Homogeneity:</p>
<p dir="auto">$$
T(c\mathbf{u}) = cT(\mathbf{u}).
$$</p>
</li>
</ol>
<p dir="auto">If both conditions hold, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> automatically respects linear combinations:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k) = c_1 T(\mathbf{v}_1) + \cdots + c_k T(\mathbf{v}_k).
$$</math-renderer></p>

<p dir="auto">Example 5.1.1. Scaling in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> be defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(x,y) = (2x, 2y).
$$</math-renderer></p>
<p dir="auto">This doubles the length of every vector, preserving direction. It is linear.</p>
<p dir="auto">Example 5.1.2. Rotation.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta: \mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> be</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_\theta(x,y) = (x\cos\theta - y\sin\theta, ; x\sin\theta + y\cos\theta).
$$</math-renderer></p>
<p dir="auto">This rotates vectors by angle <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer>. It satisfies additivity and homogeneity, hence is linear.</p>
<p dir="auto">Example 5.1.3. Differentiation.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D: \mathbb{R}[x] \to \mathbb{R}[x]$</math-renderer> be differentiation: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D(p(x)) = p&#39;(x)$</math-renderer>. Since derivatives respect addition and
scalar multiples, differentiation is a linear transformation.</p>

<p dir="auto">The map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$S:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
S(x,y) = (x^2, y^2)
$$</math-renderer></p>
<p dir="auto">is not linear, because <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$S(\mathbf{u} + \mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})$</math-renderer> in general.</p>

<p dir="auto">Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those
lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear
transformations bend or curve space, breaking these properties.</p>

<p dir="auto">Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can
be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding
these transformations, their representations, and their invariants.</p>

<ol dir="auto">
<li>
<p dir="auto">Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (3x-y, 2y)$</math-renderer> is a linear transformation on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (x+1, y)$</math-renderer> is not linear. Which axiom fails?</p>
</li>
<li>
<p dir="auto">Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$S$</math-renderer> are linear transformations, then so is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T+S$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Give an example of a linear transformation from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}[x] \to \mathbb{R}[x]$</math-renderer> be integration:</p>
<p dir="auto">$$
T(p(x)) = \int_0^x p(t),dt.
$$</p>
<p dir="auto">Prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> is a linear transformation.</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">5.2 Matrix Representation of Linear Maps</h2><a id="user-content-52-matrix-representation-of-linear-maps" aria-label="Permalink: 5.2 Matrix Representation of Linear Maps" href="#52-matrix-representation-of-linear-maps"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence
is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract
transformations.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">From Linear Map to Matrix</h3><a id="user-content-from-linear-map-to-matrix" aria-label="Permalink: From Linear Map to Matrix" href="#from-linear-map-to-matrix"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: \mathbb{R}^n \to \mathbb{R}^m$</math-renderer> be a linear transformation. Choose the standard
basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ \mathbf{e}_1, \dots, \mathbf{e}_n }$</math-renderer> of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>, where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{e}_i$</math-renderer> has a 1 in the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th position
and 0 elsewhere.</p>
<p dir="auto">The action of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> on each basis vector determines the entire transformation:</p>
<p dir="auto">$$
T(\mathbf{e}<em>j) = \begin{bmatrix} a</em>{1j} \ a_{2j} \ \vdots \ a_{mj} \end{bmatrix}.
$$</p>
<p dir="auto">Placing these outputs as columns gives the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then for any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x} \in \mathbb{R}^n$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(\mathbf{x}) = A\mathbf{x}.
$$</math-renderer></p>

<p dir="auto">Example 5.2.1. Scaling in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (2x, 3y)$</math-renderer>. Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(\mathbf{e}_1) = (2,0), \quad T(\mathbf{e}_2) = (0,3).
$$</math-renderer></p>
<p dir="auto">So the matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = \begin{bmatrix}
2 &amp; 0 \\
0 &amp; 3
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Example 5.2.2. Rotation in the plane.
The rotation transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta(x,y) = (x\cos\theta - y\sin\theta, ; x\sin\theta + y\cos\theta)$</math-renderer> has matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[R_\theta] = \begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Example 5.2.3. Projection onto the x-axis.
The map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P(x,y) = (x,0)$</math-renderer> corresponds to</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[P] = \begin{bmatrix}
1 &amp; 0 \\
0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>

<p dir="auto">Matrix representations depend on the chosen basis. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{C}$</math-renderer> are bases of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>
and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^m$</math-renderer>, then the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: \mathbb{R}^n \to \mathbb{R}^m$</math-renderer> with respect to these bases is obtained by
expressing <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(\mathbf{v}_j)$</math-renderer> in terms of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{C}$</math-renderer> for each <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_j \in \mathcal{B}$</math-renderer>. Changing bases
corresponds to conjugating the matrix by the appropriate change-of-basis matrices.</p>

<p dir="auto">Matrices are not just convenient notation-they <em>are</em> linear maps once a basis is fixed. Every rotation, reflection,
projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations
reduces to studying their matrices.</p>

<p dir="auto">Matrix representations make linear transformations computable. They connect abstract definitions to explicit
calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications
from graphics to machine learning depend on this translation.</p>

<ol dir="auto">
<li>Find the matrix representation of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (x+y, x-y)$</math-renderer>.</li>
<li>Determine the matrix of the linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^3 \to \mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y,z) = (x+z, y-2z)$</math-renderer>.</li>
<li>What matrix represents reflection across the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y=x$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>?</li>
<li>Show that the matrix of the identity transformation on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$I_n$</math-renderer>.</li>
<li>For the differentiation map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$</math-renderer>, where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}_k[x]$</math-renderer> is the space of
polynomials of degree at most <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer>, find the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> relative to the bases <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${1,x,x^2}$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${1,x}$</math-renderer>.</li>
</ol>

<p dir="auto">To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are
captured by the kernel and the image, two fundamental subspaces associated with any linear map.</p>

<p dir="auto">The kernel (or null space) of a linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to W$</math-renderer> is the set of all vectors in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> that map to the zero
vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\ker(T) = { \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} }.
$$</math-renderer></p>
<p dir="auto">The kernel is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. It measures the degeneracy of the transformation-directions that collapse to
nothing.</p>
<p dir="auto">Example 5.3.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^3 \to \mathbb{R}^2$</math-renderer> be defined by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(x,y,z) = (x+y, y+z).
$$</math-renderer></p>
<p dir="auto">In matrix form,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = \begin{bmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">To find the kernel, solve</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\begin{bmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}
\begin{bmatrix} x \ y \ z \end{bmatrix}
= \begin{bmatrix} 0 \ 0 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">This gives the equations <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x + y = 0$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y + z = 0$</math-renderer>. Hence <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x = -y, z = -y$</math-renderer>. The kernel is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\ker(T) = { (-t, t, -t) \mid t \in \mathbb{R} },
$$</math-renderer></p>
<p dir="auto">a line in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</p>

<p dir="auto">The image (or range) of a linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to W$</math-renderer> is the set of all outputs:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{im}(T) = { T(\mathbf{v}) \mid \mathbf{v} \in V } \subseteq W.
$$</math-renderer></p>
<p dir="auto">Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>.</p>
<p dir="auto">Example 5.3.2.
For the same transformation as above,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T] = \begin{bmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the columns are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer>. Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1) = (1,0) + (0,1)$</math-renderer>, the image is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{im}(T) = \text{span}{ (1,0), (0,1) } = \mathbb{R}^2.
$$</math-renderer></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Dimension Formula (Rank–Nullity Theorem)</h3><a id="user-content-dimension-formula-ranknullity-theorem" aria-label="Permalink: Dimension Formula (Rank–Nullity Theorem)" href="#dimension-formula-ranknullity-theorem"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For a linear transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to W$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> finite-dimensional,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\dim(\ker(T)) + \dim(\text{im}(T)) = \dim(V).
$$</math-renderer></p>
<p dir="auto">This fundamental result connects the lost directions (kernel) with the achieved directions (image).</p>

<ul dir="auto">
<li>The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).</li>
<li>The image describes the target subspace reached by the transformation.</li>
<li>The rank–nullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.</li>
</ul>

<p dir="auto">Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or
infinite solutions, and form the backbone of important results like the Rank–Nullity Theorem, diagonalization, and
spectral theory.</p>

<ol dir="auto">
<li>Find the kernel and image of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T(x,y) = (x-y, x+y)$</math-renderer>.</li>
<li>Let $A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 0 &amp; 1 &amp; 4 \end{bmatrix}$. Find bases for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\ker(A)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{im}(A)$</math-renderer>.</li>
<li>For the projection map <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P(x,y,z) = (x,y,0)$</math-renderer>, describe the kernel and image.</li>
<li>Prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\ker(T)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\text{im}(T)$</math-renderer> are always subspaces.</li>
<li>Verify the Rank–Nullity Theorem for the transformation in Example 5.3.1.</li>
</ol>

<p dir="auto">Linear transformations can look very different depending on the coordinate system we use. The process of rewriting
vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of
diagonalization, orthogonalization, and many computational techniques.</p>

<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional vector space, and let $\mathcal{B} = {\mathbf{v}_1, \dots, \mathbf{v}<em>n}$ be a
basis. Every vector $\mathbf{x} \in V$ has a coordinate vector $[\mathbf{x}]</em>{\mathcal{B}} \in \mathbb{R}^n$.</p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> is the change-of-basis matrix from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer> to the standard basis, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x} = P [\mathbf{x}]_{\mathcal{B}}.
$$</math-renderer></p>
<p dir="auto">Equivalently,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[\mathbf{x}]_{\mathcal{B}} = P^{-1} \mathbf{x}.
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> has the basis vectors of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer> as its columns:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
\mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n
\end{bmatrix}.
$$</math-renderer></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Transformation of Matrices</h3><a id="user-content-transformation-of-matrices" aria-label="Permalink: Transformation of Matrices" href="#transformation-of-matrices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to V$</math-renderer> be a linear transformation. Suppose its matrix in the standard basis is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>. In the
basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B}$</math-renderer>, the representing matrix becomes</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$</math-renderer></p>
<p dir="auto">Thus, changing basis corresponds to a similarity transformation of the matrix.</p>

<p dir="auto">Example 5.4.1.
Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T:\mathbb{R}^2 \to \mathbb{R}^2$</math-renderer> be given by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(x,y) = (3x + y, x + y).
$$</math-renderer></p>
<p dir="auto">In the standard basis, its matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
3 &amp; 1 \\
1 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Now consider the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = { (1,1), (1,-1) }$</math-renderer>. The change-of-basis matrix is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; -1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$</math-renderer></p>
<p dir="auto">Computing gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
[T]_{\mathcal{B}} =
\begin{bmatrix}
4 &amp; 0 \\
0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.</p>

<p dir="auto">Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its
description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a
transformation (often a diagonal basis) is a key theme in linear algebra.</p>

<p dir="auto">Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to
diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to
choosing a more natural coordinate system-whether in geometry, physics, or machine learning.</p>

<ol dir="auto">
<li>Let $A = \begin{bmatrix} 2 &amp; 1 \ 0 &amp; 2 \end{bmatrix}$. Compute its representation in the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(1,0),(1,1)}$</math-renderer>.</li>
<li>Find the change-of-basis matrix from the standard basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(2,1),(1,1)}$</math-renderer>.</li>
<li>Prove that similar matrices (related by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P^{-1}AP$</math-renderer>) represent the same linear transformation under different bases.</li>
<li>Diagonalize the matrix $A = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; -1 \end{bmatrix}$ in the basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(1,1),(1,-1)}$</math-renderer>.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathcal{B} = {(1,0,0),(1,1,0),(1,1,1)}$</math-renderer>. Construct the change-of-basis matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> and
compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P^{-1}$</math-renderer>.</li>
</ol>

<div dir="auto"><h2 tabindex="-1" dir="auto">6.1 Motivation and Geometric Meaning</h2><a id="user-content-61-motivation-and-geometric-meaning" aria-label="Permalink: 6.1 Motivation and Geometric Meaning" href="#61-motivation-and-geometric-meaning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Determinants are numerical values associated with square matrices. At first they may appear as a complicated formula,
but their importance comes from what they measure: determinants encode scaling, orientation, and invertibility of linear
transformations. They bridge algebra and geometry.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Determinants of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 2$</math-renderer> Matrices</h3><a id="user-content-determinants-of-2-times-2-matrices" aria-label="Permalink: Determinants of $2 \times 2$ Matrices" href="#determinants-of-2-times-2-matrices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 2$</math-renderer> matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} a &amp; b \ c &amp; d \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the determinant is defined as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = ad - bc.
$$</math-renderer></p>
<p dir="auto">Geometric meaning: If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> represents a linear transformation of the plane, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> is the area scaling factor.
For example, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 2$</math-renderer>, areas of shapes are doubled. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>, the transformation collapses the plane to
a line: all area is lost.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Determinants of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> Matrices</h3><a id="user-content-determinants-of-3-times-3-matrices" aria-label="Permalink: Determinants of $3 \times 3$ Matrices" href="#determinants-of-3-times-3-matrices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
a &amp; b &amp; c \\
d &amp; e &amp; f \\
g &amp; h &amp; i
\end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the determinant can be computed as</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).
$$</math-renderer></p>
<p dir="auto">Geometric meaning: In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> is the volume scaling factor. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) &amp;lt; 0$</math-renderer>, orientation is
reversed (a handedness flip), such as turning a right-handed coordinate system into a left-handed one.</p>

<p dir="auto">For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer>, the determinant is a scalar that measures how the linear transformation given
by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> scales n-dimensional volume.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>: the transformation squashes space into a lower dimension, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is not invertible.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) &amp;gt; 0$</math-renderer>: volume is scaled by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A)$</math-renderer>, orientation preserved.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) &amp;lt; 0$</math-renderer>: volume is scaled by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer>, orientation reversed.</li>
</ul>

<ol dir="auto">
<li>
<p dir="auto">Shear in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:
$A = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}$.
Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 1$</math-renderer>. The transformation slants the unit square into a parallelogram but preserves area.</p>
</li>
<li>
<p dir="auto">Projection in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:
$A = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; 0 \end{bmatrix}$.
Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>. The unit square collapses into a line segment: area vanishes.</p>
</li>
<li>
<p dir="auto">Rotation in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:
$R_\theta = \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix}$.
Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(R_\theta) = 1$</math-renderer>. Rotations preserve area and orientation.</p>
</li>
</ol>

<p dir="auto">The determinant is not just a formula-it is a measure of transformation. It tells us whether a matrix is invertible, how
it distorts space, and whether it flips orientation. This geometric insight makes the determinant indispensable in
analysis, geometry, and applied mathematics.</p>

<ol dir="auto">
<li>Compute the determinant of $\begin{bmatrix} 2 &amp; 3 \ 1 &amp; 4 \end{bmatrix}$. What area scaling factor does it
represent?</li>
<li>Find the determinant of the shear matrix $\begin{bmatrix} 1 &amp; 2 \ 0 &amp; 1 \end{bmatrix}$. What happens to the area of
the unit square?</li>
<li>For the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> matrix
$\begin{bmatrix} 1 &amp; 0 &amp; 0 \ 0 &amp; 2 &amp; 0 \ 0 &amp; 0 &amp; 3 \end{bmatrix}$, compute the determinant. How does it scale
volume in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>?</li>
<li>Show that any rotation matrix in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> has determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>.</li>
<li>Give an example of a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2 \times 2$</math-renderer> matrix with determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$-1$</math-renderer>. What geometric action does it represent?</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">6.2 Properties of Determinants</h2><a id="user-content-62-properties-of-determinants" aria-label="Permalink: 6.2 Properties of Determinants" href="#62-properties-of-determinants"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Beyond their geometric meaning, determinants satisfy a collection of algebraic rules that make them powerful tools in
linear algebra. These properties allow us to compute efficiently, test invertibility, and understand how determinants
behave under matrix operations.</p>

<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A, B \in \mathbb{R}^{n \times n}$</math-renderer>, and let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c \in \mathbb{R}$</math-renderer>. Then:</p>
<ol dir="auto">
<li>
<p dir="auto">Identity:</p>
<p dir="auto">$$
\det(I_n) = 1.
$$</p>
</li>
<li>
<p dir="auto">Triangular matrices:
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is upper or lower triangular, then</p>
<p dir="auto">$$
\det(A) = a_{11} a_{22} \cdots a_{nn}.
$$</p>
</li>
<li>
<p dir="auto">Row/column swap:
Interchanging two rows (or columns) multiplies the determinant by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$-1$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Row/column scaling:
Multiplying a row (or column) by a scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer> multiplies the determinant by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$c$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Row/column addition:
Adding a multiple of one row to another does not change the determinant.</p>
</li>
<li>
<p dir="auto">Transpose:</p>
<p dir="auto">$$
\det(A^T) = \det(A).
$$</p>
</li>
<li>
<p dir="auto">Multiplicativity:</p>
<p dir="auto">$$
\det(AB) = \det(A)\det(B).
$$</p>
</li>
<li>
<p dir="auto">Invertibility:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>.</p>
</li>
</ol>

<p dir="auto">Example 6.2.1.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 &amp; 0 \ 1 &amp; 3 &amp; 0 \ -1 &amp; 4 &amp; 5 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is lower triangular, so</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = 2 \cdot 3 \cdot 5 = 30.
$$</math-renderer></p>
<p dir="auto">Example 6.2.2.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
B = \begin{bmatrix} 1 &amp; 2 \ 3 &amp; 4 \end{bmatrix}, \quad
C = \begin{bmatrix} 0 &amp; 1 \ 1 &amp; 0 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(B) = 1\cdot 4 - 2\cdot 3 = -2, \quad \det(C) = -1.
$$</math-renderer></p>
<p dir="auto">Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$CB$</math-renderer> is obtained by swapping rows of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer>,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(CB) = -\det(B) = 2.
$$</math-renderer></p>
<p dir="auto">This matches the multiplicativity rule: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(CB) = \det(C)\det(B) = (-1)(-2) = 2.$</math-renderer></p>

<ul dir="auto">
<li>Row swaps: flipping orientation of space.</li>
<li>Scaling a row: stretching space in one direction.</li>
<li>Row replacement: sliding hyperplanes without altering volume.</li>
<li>Multiplicativity: performing two transformations multiplies their scaling factors.</li>
</ul>
<p dir="auto">These properties make determinants both computationally manageable and geometrically interpretable.</p>

<p dir="auto">Determinant properties connect computation with geometry and theory. They explain why Gaussian elimination works, why
invertibility is equivalent to nonzero determinant, and why determinants naturally arise in areas like volume
computation, eigenvalue theory, and differential equations.</p>

<ol dir="auto">
<li>
<p dir="auto">Compute the determinant of</p>
<p dir="auto">$$
A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \ 0 &amp; 1 &amp; 4 \ 0 &amp; 0 &amp; 2 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Show that if two rows of a square matrix are identical, then its determinant is zero.</p>
</li>
<li>
<p dir="auto">Verify <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A^T) = \det(A)$</math-renderer> for</p>
<p dir="auto">$$
A = \begin{bmatrix} 2 &amp; -1 \ 3 &amp; 4 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible, prove that</p>
<p dir="auto">$$
\det(A^{-1}) = \frac{1}{\det(A)}.
$$</p>
</li>
<li>
<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is a <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3\times 3$</math-renderer> matrix with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 5$</math-renderer>. What is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(2A)$</math-renderer>?</p>
</li>
</ol>

<p dir="auto">While determinants of small matrices can be computed directly from formulas, larger matrices require a systematic
method. The cofactor expansion (also known as Laplace expansion) provides a recursive way to compute determinants by
breaking them into smaller ones.</p>

<p dir="auto">For an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = [a_{ij}]$</math-renderer>:</p>
<ul dir="auto">
<li>The minor <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$M_{ij}$</math-renderer> is the determinant of the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(n-1) \times (n-1)$</math-renderer> matrix obtained by deleting the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th row and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>
-th column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</li>
<li>The cofactor <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{ij}$</math-renderer> is defined by</li>
</ul>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
C_{ij} = (-1)^{i+j} M_{ij}.
$$</math-renderer></p>
<p dir="auto">The sign factor <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(-1)^{i+j}$</math-renderer> alternates in a checkerboard pattern:</p>
<p dir="auto">$$
\begin{bmatrix}</p>
<ul dir="auto">
<li>&amp; - &amp; + &amp; - &amp; \cdots \</li>
</ul>
<ul dir="auto">
<li>&amp; + &amp; - &amp; + &amp; \cdots \</li>
</ul>
<ul dir="auto">
<li>&amp; - &amp; + &amp; - &amp; \cdots \
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{bmatrix}.
$$</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Cofactor Expansion Formula</h3><a id="user-content-cofactor-expansion-formula" aria-label="Permalink: Cofactor Expansion Formula" href="#cofactor-expansion-formula"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The determinant of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> can be computed by expanding along any row or any column:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = \sum_{j=1}^n a_{ij} C_{ij} \quad \text{(expansion along row (i))},
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = \sum_{i=1}^n a_{ij} C_{ij} \quad \text{(expansion along column (j))}.
$$</math-renderer></p>

<p dir="auto">Example 6.3.1.
Compute</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 2 &amp; 3 \\
0 &amp; 4 &amp; 5 \\
1 &amp; 0 &amp; 6
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Expand along the first row:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = 1 \cdot C_{11} + 2 \cdot C_{12} + 3 \cdot C_{13}.
$$</math-renderer></p>
<ul dir="auto">
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{11}$</math-renderer>:
$M_{11} = \det \begin{bmatrix} 4 &amp; 5 \ 0 &amp; 6 \end{bmatrix} = 24$, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{11} = (+1)(24) = 24$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{12}$</math-renderer>:
$M_{12} = \det \begin{bmatrix} 0 &amp; 5 \ 1 &amp; 6 \end{bmatrix} = 0 - 5 = -5$, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{12} = (-1)(-5) = 5$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{13}$</math-renderer>:
$M_{13} = \det \begin{bmatrix} 0 &amp; 4 \ 1 &amp; 0 \end{bmatrix} = 0 - 4 = -4$, so <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C_{13} = (+1)(-4) = -4$</math-renderer>.</li>
</ul>
<p dir="auto">Thus,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A) = 1(24) + 2(5) + 3(-4) = 24 + 10 - 12 = 22.
$$</math-renderer></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Properties of Cofactor Expansion</h3><a id="user-content-properties-of-cofactor-expansion" aria-label="Permalink: Properties of Cofactor Expansion" href="#properties-of-cofactor-expansion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol dir="auto">
<li>Expansion along any row or column yields the same result.</li>
<li>The cofactor expansion provides a recursive definition of determinant: a determinant of size <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> is expressed in
terms of determinants of size <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n-1$</math-renderer>.</li>
<li>Cofactors are fundamental in constructing the adjugate matrix, which gives a formula for inverses:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^{-1} = \frac{1}{\det(A)} , \text{adj}(A), \quad \text{where adj}(A) = [C_{ji}].
$$</math-renderer></p>

<p dir="auto">Cofactor expansion breaks down the determinant into contributions from sub-volumes defined by fixing one row or column
at a time. Each cofactor measures how that row/column influences the overall volume scaling.</p>

<p dir="auto">Cofactor expansion generalizes the small-matrix formulas and provides a conceptual definition of determinants. While not
the most efficient way to compute determinants for large matrices, it is essential for theory, proofs, and connections
to adjugates, Cramer’s rule, and classical geometry.</p>

<ol dir="auto">
<li>
<p dir="auto">Compute the determinant of</p>
<p dir="auto">$$
\begin{bmatrix}
2 &amp; 0 &amp; 1 \
3 &amp; -1 &amp; 4 \
1 &amp; 2 &amp; 0
\end{bmatrix}
$$</p>
<p dir="auto">by cofactor expansion along the first column.</p>
</li>
<li>
<p dir="auto">Verify that expanding along the second row of Example 6.3.1 gives the same determinant.</p>
</li>
<li>
<p dir="auto">Prove that expansion along any row gives the same value.</p>
</li>
<li>
<p dir="auto">Show that if a row of a matrix is zero, then its determinant is zero.</p>
</li>
<li>
<p dir="auto">Use cofactor expansion to prove that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = \det(A^T)$</math-renderer>.</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">6.4 Applications (Volume, Invertibility Test)</h2><a id="user-content-64-applications-volume-invertibility-test" aria-label="Permalink: 6.4 Applications (Volume, Invertibility Test)" href="#64-applications-volume-invertibility-test"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Determinants are not merely algebraic curiosities; they have concrete geometric and computational uses. Two of the most
important applications are measuring volumes and testing invertibility of matrices.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Determinants as Volume Scalers</h3><a id="user-content-determinants-as-volume-scalers" aria-label="Permalink: Determinants as Volume Scalers" href="#determinants-as-volume-scalers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Given vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \in \mathbb{R}^n$</math-renderer>, arrange them as columns of a matrix:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
| &amp; | &amp; &amp; | \\
\mathbf{v}_1 &amp; \mathbf{v}_2 &amp; \cdots &amp; \mathbf{v}_n \\
| &amp; | &amp; &amp; |
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> equals the volume of the parallelepiped spanned by these vectors.</p>
<ul dir="auto">
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> gives the area of the parallelogram spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2$</math-renderer>.</li>
<li>In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer> gives the volume of the parallelepiped spanned
by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$</math-renderer>.</li>
<li>In higher dimensions, it generalizes to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional volume (hypervolume).</li>
</ul>
<p dir="auto">Example 6.4.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v}_1 = (1,0,0), \quad \mathbf{v}_2 = (1,1,0), \quad \mathbf{v}_3 = (1,1,1).
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1
\end{bmatrix}, \quad \det(A) = 1.
$$</math-renderer></p>
<p dir="auto">So the parallelepiped has volume <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>, even though the vectors are not orthogonal.</p>

<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is invertible if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>.</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) = 0$</math-renderer>: the transformation collapses space into a lower dimension (area/volume is zero). No inverse exists.</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A) \neq 0$</math-renderer>: the transformation scales volume by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\det(A)|$</math-renderer>, and is reversible.</li>
</ul>
<p dir="auto">Example 6.4.2.
The matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
B = \begin{bmatrix} 2 &amp; 4 \ 1 &amp; 2 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">has determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(B) = 2 \cdot 2 - 4 \cdot 1 = 0$</math-renderer>.
Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> is not invertible. Geometrically, the two column vectors are collinear, spanning only a line
in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</p>

<p dir="auto">Determinants also provide an explicit formula for solving systems of linear equations when the matrix is invertible.
For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} = \mathbf{b}$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
x_i = \frac{\det(A_i)}{\det(A)},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A_i$</math-renderer> is obtained by replacing the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer>-th column of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b}$</math-renderer>.
While inefficient computationally, Cramer’s rule highlights the determinant’s role in solutions and uniqueness.</p>

<p dir="auto">The sign of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\det(A)$</math-renderer> indicates whether a transformation preserves or reverses orientation. For example, a reflection in
the plane has determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$-1$</math-renderer>, flipping handedness.</p>

<p dir="auto">Determinants condense key information: they measure scaling, test invertibility, and track orientation. These insights
are indispensable in geometry (areas and volumes), analysis (Jacobian determinants in calculus), and computation (
solving systems and checking singularity).</p>

<ol dir="auto">
<li>
<p dir="auto">Compute the area of the parallelogram spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,1)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,3)$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Find the volume of the parallelepiped spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0,0), (1,1,0), (1,1,1)$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Determine whether the matrix $\begin{bmatrix} 1 &amp; 2 \ 3 &amp; 6 \end{bmatrix}$ is invertible. Justify using
determinants.</p>
</li>
<li>
<p dir="auto">Use Cramer’s rule to solve</p>
<p dir="auto">$$
\begin{cases}
x + y = 3, \
2x - y = 0.
\end{cases}
$$</p>
</li>
<li>
<p dir="auto">Explain geometrically why a determinant of zero implies no inverse exists.</p>
</li>
</ol>

<div dir="auto"><h2 tabindex="-1" dir="auto">7.1 Inner Products and Norms</h2><a id="user-content-71-inner-products-and-norms" aria-label="Permalink: 7.1 Inner Products and Norms" href="#71-inner-products-and-norms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To extend the geometric ideas of length, distance, and angle beyond <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, we introduce
inner products. Inner products provide a way of measuring similarity between vectors, while norms derived from them
measure length. These concepts are the foundation of geometry inside vector spaces.</p>

<p dir="auto">An inner product on a real vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is a function</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
$$</math-renderer></p>
<p dir="auto">that assigns to each pair of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(\mathbf{u}, \mathbf{v})$</math-renderer> a real number, subject to the following properties:</p>
<ol dir="auto">
<li>
<p dir="auto">Symmetry:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$</math-renderer></p>
</li>
<li>
<p dir="auto">Linearity in the first argument:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$</math-renderer></p>
</li>
<li>
<p dir="auto">Positive-definiteness:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$</math-renderer>, and equality holds if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = \mathbf{0}$</math-renderer>.</p>
</li>
</ol>
<p dir="auto">The standard inner product on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> is the dot product:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$</math-renderer></p>

<p dir="auto">The norm of a vector is its length, defined in terms of the inner product:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{v}| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
$$</math-renderer></p>
<p dir="auto">For the dot product in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|(x_1, x_2, \dots, x_n)| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
$$</math-renderer></p>

<p dir="auto">The inner product allows us to define the angle <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer> between two nonzero vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}, \mathbf{v}$</math-renderer> by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{|\mathbf{u}| , |\mathbf{v}|}.
$$</math-renderer></p>
<p dir="auto">Thus, two vectors are orthogonal if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{u}, \mathbf{v} \rangle = 0$</math-renderer>.</p>

<p dir="auto">Example 7.1.1.
In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3,4)$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{u}, \mathbf{v} \rangle = 1\cdot 3 + 2\cdot 4 = 11.
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|\mathbf{u}| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad |\mathbf{v}| = \sqrt{3^2 + 4^2} = 5.
$$</math-renderer></p>
<p dir="auto">So,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\cos \theta = \frac{11}{\sqrt{5}\cdot 5}.
$$</math-renderer></p>
<p dir="auto">Example 7.1.2.
In the function space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C[0,1]$</math-renderer>, the inner product</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle f, g \rangle = \int_0^1 f(x) g(x), dx
$$</math-renderer></p>
<p dir="auto">defines a length</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|f| = \sqrt{\int_0^1 f(x)^2 dx}.
$$</math-renderer></p>
<p dir="auto">This generalizes geometry to infinite-dimensional spaces.</p>

<ul dir="auto">
<li>Inner product: measures similarity between vectors.</li>
<li>Norm: length of a vector.</li>
<li>Angle: measure of alignment between two directions.</li>
</ul>
<p dir="auto">These concepts unify algebraic operations with geometric intuition.</p>

<p dir="auto">Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality,
projections, Fourier series, least squares approximation, and many applications in physics and machine learning.</p>

<ol dir="auto">
<li>
<p dir="auto">Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle (2,-1,3), (1,4,0) \rangle$</math-renderer>. Then find the angle between them.</p>
</li>
<li>
<p dir="auto">Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|(x,y)| = \sqrt{x^2+y^2}$</math-renderer> satisfies the properties of a norm.</p>
</li>
<li>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,0)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,-1,0)$</math-renderer> are orthogonal.</p>
</li>
<li>
<p dir="auto">In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$C[0,1]$</math-renderer>, compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle f,g \rangle$</math-renderer> for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$f(x)=x$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$g(x)=1$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Prove the Cauchy–Schwarz inequality:</p>
<p dir="auto">$$
|\langle \mathbf{u}, \mathbf{v} \rangle| \leq |\mathbf{u}| , |\mathbf{v}|.
$$</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">7.2 Orthogonal Projections</h2><a id="user-content-72-orthogonal-projections" aria-label="Permalink: 7.2 Orthogonal Projections" href="#72-orthogonal-projections"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to
approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins
geometry, statistics, and numerical analysis.</p>

<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} \in \mathbb{R}^n$</math-renderer> be a nonzero vector. The line spanned by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
L = { c\mathbf{u} \mid c \in \mathbb{R} }.
$$</math-renderer></p>
<p dir="auto">Given a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>, the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer> is the vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$L$</math-renderer> closest
to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>. Geometrically, it is the shadow of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> on the line.</p>
<p dir="auto">The formula is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle} , \mathbf{u}.
$$</math-renderer></p>
<p dir="auto">The error vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$</math-renderer> is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>.</p>

<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u} = (1,2)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} = (3,1)$</math-renderer>.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{v}, \mathbf{u} \rangle = 3\cdot 1 + 1\cdot 2 = 5, \quad
\langle \mathbf{u}, \mathbf{u} \rangle = 1^2 + 2^2 = 5.
$$</math-renderer></p>
<p dir="auto">So</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{5}{5}(1,2) = (1,2).
$$</math-renderer></p>
<p dir="auto">The error vector is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3,1) - (1,2) = (2,-1)$</math-renderer>, which is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2)$</math-renderer>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Projection onto a Subspace</h3><a id="user-content-projection-onto-a-subspace" aria-label="Permalink: Projection onto a Subspace" href="#projection-onto-a-subspace"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Suppose <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W \subseteq \mathbb{R}^n$</math-renderer> is a subspace with orthonormal basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${ \mathbf{w}_1, \dots, \mathbf{w}_k }$</math-renderer>. The
projection of a vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\text{proj}_{W}(\mathbf{v}) = \langle \mathbf{v}, \mathbf{w}_1 \rangle \mathbf{w}_1 + \cdots + \langle \mathbf{v}, \mathbf{w}_k \rangle \mathbf{w}_k.
$$</math-renderer></p>
<p dir="auto">This is the unique vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> closest to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>. The difference <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} - \text{proj}_{W}(\mathbf{v})$</math-renderer> is
orthogonal to all of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Least Squares Approximation</h3><a id="user-content-least-squares-approximation" aria-label="Permalink: Least Squares Approximation" href="#least-squares-approximation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Orthogonal projection explains the method of least squares. To solve an overdetermined
system <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x} \approx \mathbf{b}$</math-renderer>, we seek the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}$</math-renderer> that makes <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A\mathbf{x}$</math-renderer> the projection
of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b}$</math-renderer> onto the column space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>. This gives the normal equations</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$</math-renderer></p>
<p dir="auto">Thus, least squares is just projection in disguise.</p>

<ul dir="auto">
<li>Projection finds the closest point in a subspace to a given vector.</li>
<li>It minimizes distance (error) in the sense of Euclidean norm.</li>
<li>Orthogonality ensures the error vector points directly away from the subspace.</li>
</ul>

<p dir="auto">Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the
theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we
fit data with a simpler model, projection is at work.</p>

<ol dir="auto">
<li>Compute the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(2,3)$</math-renderer> onto the vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>.</li>
<li>Show that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$</math-renderer> is orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}$</math-renderer>.</li>
<li>Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W = \text{span}{(1,0,0), (0,1,0)} \subseteq \mathbb{R}^3$</math-renderer>. Find the projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2,3)$</math-renderer> onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer>.</li>
<li>Explain why least squares fitting corresponds to projection onto the column space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</li>
<li>Prove that projection onto a subspace <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> is unique: there is exactly one closest vector in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$W$</math-renderer> to a
given <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">7.3 Gram–Schmidt Process</h2><a id="user-content-73-gramschmidt-process" aria-label="Permalink: 7.3 Gram–Schmidt Process" href="#73-gramschmidt-process"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The Gram–Schmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis.
This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate
comparisons, and projections take clean forms.</p>

<p dir="auto">Given a linearly independent set of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n}$</math-renderer> in an inner product
space, we want to construct an orthonormal set <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n}$</math-renderer> that spans the same
subspace.</p>
<p dir="auto">We proceed step by step:</p>
<ol dir="auto">
<li>Start with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1$</math-renderer>, normalize it to get <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>.</li>
<li>Subtract from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2$</math-renderer> its projection onto <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>, leaving a vector orthogonal to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>.
Normalize to get <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_2$</math-renderer>.</li>
<li>For each <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_k$</math-renderer>, subtract projections onto all previously
constructed $\mathbf{u}<em>1, \dots, \mathbf{u}</em>{k-1}$, then normalize.</li>
</ol>

<p dir="auto">For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k = 1, 2, \dots, n$</math-renderer>:</p>
<p dir="auto">$$
\mathbf{w}_k = \mathbf{v}<em>k - \sum</em>{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j,
$$</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_k = \frac{\mathbf{w}_k}{|\mathbf{w}_k|}.
$$</math-renderer></p>
<p dir="auto">The result <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1, \dots, \mathbf{u}_n}$</math-renderer> is an orthonormal basis of the span of the original vectors.</p>

<p dir="auto">Take <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</p>
<ol dir="auto">
<li>Normalize <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1$</math-renderer>:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_1 = \frac{1}{\sqrt{2}}(1,1,0).
$$</math-renderer></p>
<ol start="2" dir="auto">
<li>Subtract projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2$</math-renderer> on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_1$</math-renderer>:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{w}_2 = \mathbf{v}_2 - \langle \mathbf{v}_2,\mathbf{u}_1 \rangle \mathbf{u}_1.
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle \mathbf{v}_2,\mathbf{u}_1 \rangle = \frac{1}{\sqrt{2}}(1\cdot 1 + 0\cdot 1 + 1\cdot 0) = \tfrac{1}{\sqrt{2}}.
$$</math-renderer></p>
<p dir="auto">So</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{w}_2 = (1,0,1) - \tfrac{1}{\sqrt{2}}\cdot \tfrac{1}{\sqrt{2}}(1,1,0)
= (1,0,1) - \tfrac{1}{2}(1,1,0)
= \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$</math-renderer></p>
<p dir="auto">Normalize:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_2 = \frac{1}{\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)
= \frac{1}{\sqrt{\tfrac{3}{2}}}\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$</math-renderer></p>
<ol start="3" dir="auto">
<li>Subtract projections from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_3$</math-renderer>:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{w}_3 = \mathbf{v}_3 - \langle \mathbf{v}_3,\mathbf{u}_1 \rangle \mathbf{u}_1 - \langle \mathbf{v}_3,\mathbf{u}_2 \rangle \mathbf{u}_2.
$$</math-renderer></p>
<p dir="auto">After computing, normalize to obtain <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{u}_3$</math-renderer>.</p>
<p dir="auto">The result is an orthonormal basis of the span of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3}$</math-renderer>.</p>

<p dir="auto">Gram–Schmidt is like straightening out a set of vectors: you start with the original directions and adjust each new
vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while
preserving the span.</p>

<p dir="auto">Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier
to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal
polynomials, principal component analysis).</p>

<ol dir="auto">
<li>Apply Gram–Schmidt to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0), (1,1)$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</li>
<li>Orthogonalize <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,1), (1,0,1)$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>Prove that each step of Gram–Schmidt yields a vector orthogonal to all previous ones.</li>
<li>Show that Gram–Schmidt preserves the span of the original vectors.</li>
<li>Explain how Gram–Schmidt leads to the QR decomposition of a matrix.</li>
</ol>

<p dir="auto">An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit
length. Such bases are the most convenient possible coordinate systems: computations involving inner products,
projections, and norms become exceptionally simple.</p>

<p dir="auto">A set of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n}$</math-renderer> in an inner product space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer> is called an
orthonormal basis if</p>
<ol dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$</math-renderer> whenever <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i \neq j$</math-renderer> (orthogonality),</li>
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\mathbf{u}_i| = 1$</math-renderer> for all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer> (normalization),</li>
<li>The set spans <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>.</li>
</ol>

<p dir="auto">Example 7.4.1. In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>, the standard basis</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
$$</math-renderer></p>
<p dir="auto">is orthonormal under the dot product.</p>
<p dir="auto">Example 7.4.2. In <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>, the standard basis</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$</math-renderer></p>
<p dir="auto">is orthonormal.</p>
<p dir="auto">Example 7.4.3. Fourier basis on functions:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots}
$$</math-renderer></p>
<p dir="auto">is an orthogonal set in the space of square-integrable functions on <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$[-\pi,\pi]$</math-renderer> with inner product</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\langle f,g \rangle = \int_{-\pi}^{\pi} f(x) g(x), dx.
$$</math-renderer></p>
<p dir="auto">After normalization, it becomes an orthonormal basis.</p>

<ol dir="auto">
<li>
<p dir="auto">Coordinate simplicity: If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1,\dots,\mathbf{u}_n}$</math-renderer> is an orthonormal basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>, then any
vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}\in V$</math-renderer> has coordinates</p>
<p dir="auto">$$
[\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \ \vdots \ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}.
$$</p>
<p dir="auto">That is, coordinates are just inner products.</p>
</li>
<li>
<p dir="auto">Parseval’s identity:
For any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer>,</p>
<p dir="auto">$$
|\mathbf{v}|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2.
$$</p>
</li>
<li>
<p dir="auto">Projections:
The orthogonal projection onto the span of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${\mathbf{u}_1,\dots,\mathbf{u}_k}$</math-renderer> is</p>
<p dir="auto">$$
\text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i.
$$</p>
</li>
</ol>
<div dir="auto"><h3 tabindex="-1" dir="auto">Constructing Orthonormal Bases</h3><a id="user-content-constructing-orthonormal-bases" aria-label="Permalink: Constructing Orthonormal Bases" href="#constructing-orthonormal-bases"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Start with any linearly independent set, then apply the Gram–Schmidt process to obtain an orthonormal set spanning the
same subspace.</li>
<li>In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.</li>
</ul>

<p dir="auto">An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed
directly using coordinates without correction factors. They are the ideal rulers of linear algebra.</p>

<p dir="auto">Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions,
diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis
produces orthonormal directions capturing maximum variance.</p>

<ol dir="auto">
<li>Verify that <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1/\sqrt{2})(1,1)$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1/\sqrt{2})(1,-1)$</math-renderer> form an orthonormal basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>.</li>
<li>Express <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(3,4)$</math-renderer> in terms of the orthonormal basis <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">${(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)}$</math-renderer>.</li>
<li>Prove Parseval’s identity for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> with the dot product.</li>
<li>Find an orthonormal basis for the plane <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x+y+z=0$</math-renderer> in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer>.</li>
<li>Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.</li>
</ol>

<div dir="auto"><h2 tabindex="-1" dir="auto">8.1 Definitions and Intuition</h2><a id="user-content-81-definitions-and-intuition" aria-label="Permalink: 8.1 Definitions and Intuition" href="#81-definitions-and-intuition"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They
identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or
distortion.</p>

<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T: V \to V$</math-renderer> be a linear transformation on a vector space <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$V$</math-renderer>. A nonzero vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in V$</math-renderer> is called an
eigenvector of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer> if</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T(\mathbf{v}) = \lambda \mathbf{v}
$$</math-renderer></p>
<p dir="auto">for some scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda \in \mathbb{R}$</math-renderer> (or <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{C}$</math-renderer>). The scalar <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> is the eigenvalue corresponding
to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer>.</p>
<p dir="auto">Equivalently, if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is the matrix of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$T$</math-renderer>, then eigenvalues and eigenvectors satisfy</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{v} = \lambda \mathbf{v}.
$$</math-renderer></p>

<p dir="auto">Example 8.1.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; 3 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A(1,0)^T = 2(1,0)^T, \quad A(0,1)^T = 3(0,1)^T.
$$</math-renderer></p>
<p dir="auto">So <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer> is an eigenvector with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2$</math-renderer>, and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer> is an eigenvector with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3$</math-renderer>.</p>
<p dir="auto">Example 8.1.2.
Rotation matrix in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_\theta = \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta \neq 0, \pi$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta$</math-renderer> has no real eigenvalues: every vector is rotated, not scaled. Over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{C}$</math-renderer>,
however, it has eigenvalues <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$e^{i\theta}, e^{-i\theta}$</math-renderer>.</p>

<p dir="auto">Eigenvalues arise from solving the characteristic equation:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A - \lambda I) = 0.
$$</math-renderer></p>
<p dir="auto">This polynomial in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> is the characteristic polynomial. Its roots are the eigenvalues.</p>

<ul dir="auto">
<li>Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.</li>
<li>Eigenvalues tell us the scaling factor along those directions.</li>
<li>If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Applications in Geometry and Science</h3><a id="user-content-applications-in-geometry-and-science" aria-label="Permalink: Applications in Geometry and Science" href="#applications-in-geometry-and-science"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Stretching along principal axes of an ellipse (quadratic forms).</li>
<li>Stable directions of dynamical systems.</li>
<li>Principal components in statistics and machine learning.</li>
<li>Quantum mechanics, where observables correspond to operators with eigenvalues.</li>
</ul>

<p dir="auto">Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear
transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics,
physics, computer science-relies on eigen-analysis.</p>

<ol dir="auto">
<li>Find the eigenvalues and eigenvectors of
$\begin{bmatrix} 4 &amp; 0 \ 0 &amp; -1 \end{bmatrix}$.</li>
<li>Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.</li>
<li>Verify that the rotation matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta$</math-renderer> has no real eigenvalues unless <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta = 0$</math-renderer> or <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\pi$</math-renderer>.</li>
<li>Compute the characteristic polynomial of
$\begin{bmatrix} 1 &amp; 2 \ 2 &amp; 1 \end{bmatrix}$.</li>
<li>Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix
$\begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}$.</li>
</ol>

<p dir="auto">A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the
process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations
such as powers, exponentials, and solving differential equations far easier.</p>

<p dir="auto">A square matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is diagonalizable if there exists an invertible matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> such that</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P^{-1} A P = D,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> is a diagonal matrix.</p>
<p dir="auto">The diagonal entries of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> are eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>, and the columns of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> are the corresponding eigenvectors.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">When is a Matrix Diagonalizable?</h3><a id="user-content-when-is-a-matrix-diagonalizable" aria-label="Permalink: When is a Matrix Diagonalizable?" href="#when-is-a-matrix-diagonalizable"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>A matrix is diagonalizable if it has <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> linearly independent eigenvectors.</li>
<li>Equivalently, the sum of the dimensions of its eigenspaces equals <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>.</li>
<li>Symmetric matrices (over <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}$</math-renderer>) are always diagonalizable, with an orthonormal basis of eigenvectors.</li>
</ul>

<p dir="auto">Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 4 &amp; 1 \ 0 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<ol dir="auto">
<li>Characteristic polynomial:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\det(A - \lambda I) = (4-\lambda)(2-\lambda).
$$</math-renderer></p>
<p dir="auto">So eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_1 = 4$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_2 = 2$</math-renderer>.</p>
<ol start="2" dir="auto">
<li>Eigenvectors:</li>
</ol>
<ul dir="auto">
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 4$</math-renderer>, solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-4I)\mathbf{v}=0$</math-renderer>:
$\begin{bmatrix} 0 &amp; 1 \ 0 &amp; -2 \end{bmatrix}\mathbf{v} = 0$, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_1 = (1,0)$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 2$</math-renderer>: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-2I)\mathbf{v}=0$</math-renderer>, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}_2 = (1,-2)$</math-renderer>.</li>
</ul>
<ol start="3" dir="auto">
<li>Construct $P = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; -2 \end{bmatrix}$. Then</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P^{-1} A P = \begin{bmatrix} 4 &amp; 0 \ 0 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is diagonalizable.</p>

<ul dir="auto">
<li>
<p dir="auto">Computing powers:
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = P D P^{-1}$</math-renderer>, then</p>
<p dir="auto">$$
A^k = P D^k P^{-1}.
$$</p>
<p dir="auto">Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> is diagonal, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D^k$</math-renderer> is easy to compute.</p>
</li>
<li>
<p dir="auto">Matrix exponentials:
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$e^A = P e^D P^{-1}$</math-renderer>, useful in solving differential equations.</p>
</li>
<li>
<p dir="auto">Understanding geometry:
Diagonalization reveals the directions along which a transformation stretches or compresses space independently.</p>
</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Non-Diagonalizable Example</h3><a id="user-content-non-diagonalizable-example" aria-label="Permalink: Non-Diagonalizable Example" href="#non-diagonalizable-example"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Not all matrices can be diagonalized.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">has only one eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1$</math-renderer>, with eigenspace dimension 1. Since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n=2$</math-renderer> but we only have 1 independent
eigenvector, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is not diagonalizable.</p>

<p dir="auto">Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each
coordinate axis. It transforms complicated motion into independent 1D motions.</p>

<p dir="auto">Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting
point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.</p>

<ol dir="auto">
<li>
<p dir="auto">Diagonalize</p>
<p dir="auto">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; 3 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Determine whether</p>
<p dir="auto">$$
A = \begin{bmatrix} 1 &amp; 1 \ 0 &amp; 1 \end{bmatrix}
$$</p>
<p dir="auto">is diagonalizable. Why or why not?</p>
</li>
<li>
<p dir="auto">Find <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^5$</math-renderer> for</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 1 \ 0 &amp; 2 \end{bmatrix}
$$</p>
<p dir="auto">using diagonalization.</p>
</li>
<li>
<p dir="auto">Show that any <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> distinct eigenvalues is diagonalizable.</p>
</li>
<li>
<p dir="auto">Explain why real symmetric matrices are always diagonalizable.</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">8.3 Characteristic Polynomials</h2><a id="user-content-83-characteristic-polynomials" aria-label="Permalink: 8.3 Characteristic Polynomials" href="#83-characteristic-polynomials"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values
of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> for which the matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A - \lambda I$</math-renderer> fails to be invertible.</p>

<p dir="auto">For an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>, the characteristic polynomial is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = \det(A - \lambda I).
$$</math-renderer></p>
<p dir="auto">The roots of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$p_A(\lambda)$</math-renderer> are the eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>

<p dir="auto">Example 8.3.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = \det!\begin{bmatrix} 2-\lambda &amp; 1 \ 1 &amp; 2-\lambda \end{bmatrix}
= (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$</math-renderer></p>
<p dir="auto">Thus eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1, 3$</math-renderer>.</p>
<p dir="auto">Example 8.3.2.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 0 &amp; -1 \ 1 &amp; 0 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">(rotation by 90°),</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = \det!\begin{bmatrix} -\lambda &amp; -1 \ 1 &amp; -\lambda \end{bmatrix}
= \lambda^2 + 1.
$$</math-renderer></p>
<p dir="auto">Eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = \pm i$</math-renderer>. No real eigenvalues exist, consistent with pure rotation.</p>
<p dir="auto">Example 8.3.3.
For a triangular matrix</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 &amp; 0 \ 0 &amp; 3 &amp; 5 \ 0 &amp; 0 &amp; 4 \end{bmatrix},
$$</math-renderer></p>
<p dir="auto">the determinant is simply the product of diagonal entries minus <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p_A(\lambda) = (2-\lambda)(3-\lambda)(4-\lambda).
$$</math-renderer></p>
<p dir="auto">So eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2, 3, 4$</math-renderer>.</p>

<ol dir="auto">
<li>
<p dir="auto">The characteristic polynomial of an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix has degree <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>.</p>
</li>
<li>
<p dir="auto">The sum of the eigenvalues (counted with multiplicity) equals the trace of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>:</p>
<p dir="auto">$$
\text{tr}(A) = \lambda_1 + \cdots + \lambda_n.
$$</p>
</li>
<li>
<p dir="auto">The product of the eigenvalues equals the determinant of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>:</p>
<p dir="auto">$$
\det(A) = \lambda_1 \cdots \lambda_n.
$$</p>
</li>
<li>
<p dir="auto">Similar matrices have the same characteristic polynomial, hence the same eigenvalues.</p>
</li>
</ol>

<p dir="auto">The characteristic polynomial captures when <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A - \lambda I$</math-renderer> collapses space: its determinant is zero precisely when the
transformation <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A - \lambda I$</math-renderer> is singular. Thus, eigenvalues mark the critical scalings where the matrix loses
invertibility.</p>

<p dir="auto">Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace
and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis
in dynamical systems.</p>

<ol dir="auto">
<li>
<p dir="auto">Compute the characteristic polynomial of</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 2 \ 1 &amp; 3 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Verify that the sum of the eigenvalues of
$\begin{bmatrix} 5 &amp; 0 \ 0 &amp; -2 \end{bmatrix}$
equals its trace, and their product equals its determinant.</p>
</li>
<li>
<p dir="auto">Show that for any triangular matrix, the eigenvalues are just the diagonal entries.</p>
</li>
<li>
<p dir="auto">Prove that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$B$</math-renderer> are similar matrices, then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$p_A(\lambda) = p_B(\lambda)$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Compute the characteristic polynomial of
$\begin{bmatrix} 1 &amp; 1 &amp; 0 \ 0 &amp; 1 &amp; 1 \ 0 &amp; 0 &amp; 1 \end{bmatrix}$.</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">8.4 Applications (Differential Equations, Markov Chains)</h2><a id="user-content-84-applications-differential-equations-markov-chains" aria-label="Permalink: 8.4 Applications (Differential Equations, Markov Chains)" href="#84-applications-differential-equations-markov-chains"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across
mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing
Markov chains.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Linear Differential Equations</h3><a id="user-content-linear-differential-equations" aria-label="Permalink: Linear Differential Equations" href="#linear-differential-equations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Consider the system</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\frac{d\mathbf{x}}{dt} = A \mathbf{x},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> matrix and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}(t)$</math-renderer> is a vector-valued function.</p>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v}$</math-renderer> is an eigenvector of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer>, then the function</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}(t) = e^{\lambda t}\mathbf{v}
$$</math-renderer></p>
<p dir="auto">is a solution.</p>
<ul dir="auto">
<li>
<p dir="auto">Eigenvalues determine the growth or decay rate:</p>
<ul dir="auto">
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda &amp;lt; 0$</math-renderer>, solutions decay (stable).</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda &amp;gt; 0$</math-renderer>, solutions grow (unstable).</li>
<li>If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda$</math-renderer> is complex, oscillations occur.</li>
</ul>
</li>
</ul>
<p dir="auto">By combining eigenvector solutions, we can solve general initial conditions.</p>
<p dir="auto">Example 8.4.1.
Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; -1 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Then eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$2, -1$</math-renderer> with eigenvectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,1)$</math-renderer>. Solutions are</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}(t) = c_1 e^{2t}(1,0) + c_2 e^{-t}(0,1).
$$</math-renderer></p>
<p dir="auto">Thus one component grows exponentially, the other decays.</p>

<p dir="auto">A Markov chain is described by a stochastic matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>, where each column sums to 1 and entries are nonnegative.
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}_k$</math-renderer> represents the probability distribution after <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> steps, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}_{k+1} = P \mathbf{x}_k.
$$</math-renderer></p>
<p dir="auto">Iterating gives</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}_k = P^k \mathbf{x}_0.
$$</math-renderer></p>
<p dir="auto">Understanding long-term behavior reduces to analyzing powers of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>.</p>
<ul dir="auto">
<li>The eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1$</math-renderer> always exists. Its eigenvector gives the steady-state distribution.</li>
<li>All other eigenvalues satisfy <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\lambda| \leq 1$</math-renderer>. Their influence decays as <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k \to \infty$</math-renderer>.</li>
</ul>
<p dir="auto">Example 8.4.2.
Consider</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix} 0.9 &amp; 0.5 \ 0.1 &amp; 0.5 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Eigenvalues are <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_1 = 1$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_2 = 0.4$</math-renderer>. The eigenvector for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 1$</math-renderer> is proportional to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(5,1)$</math-renderer>.
Normalizing gives the steady state</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\pi = \left(\tfrac{5}{6}, \tfrac{1}{6}\right).
$$</math-renderer></p>
<p dir="auto">Thus, regardless of the starting distribution, the chain converges to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\pi$</math-renderer>.</p>

<ul dir="auto">
<li>In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.</li>
<li>In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.</li>
</ul>

<p dir="auto">Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and
finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Google’s
PageRank to modern machine learning.</p>

<ol dir="auto">
<li>
<p dir="auto">Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 &amp; 0 \ 0 &amp; -2 \end{bmatrix}\mathbf{x}$.</p>
</li>
<li>
<p dir="auto">Show that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> has a complex eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\alpha \pm i\beta$</math-renderer>, then solutions
of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$</math-renderer> involve oscillations of frequency <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\beta$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Find the steady-state distribution of</p>
<p dir="auto">$$
P = \begin{bmatrix} 0.7 &amp; 0.2 \ 0.3 &amp; 0.8 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Prove that for any stochastic matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer> is always an eigenvalue.</p>
</li>
<li>
<p dir="auto">Explain why all eigenvalues of a stochastic matrix satisfy <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$|\lambda| \leq 1$</math-renderer>.</p>
</li>
</ol>


<p dir="auto">A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms
appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy
functions).</p>

<p dir="auto">Let <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> be an <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n \times n$</math-renderer> symmetric matrix and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x} \in \mathbb{R}^n$</math-renderer>. The quadratic form associated with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}.
$$</math-renderer></p>
<p dir="auto">Expanded,</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
$$</math-renderer></p>
<p dir="auto">Because <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is symmetric (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$a_{ij} = a_{ji}$</math-renderer>), the cross-terms can be grouped naturally.</p>

<p dir="auto">Example 9.1.1.
For</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 3 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x \ y \end{bmatrix},
$$</math-renderer></p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = \begin{bmatrix} x &amp; y \end{bmatrix}
\begin{bmatrix} 2 &amp; 1 \ 1 &amp; 3 \end{bmatrix}
\begin{bmatrix} x \ y \end{bmatrix}
= 2x^2 + 2xy + 3y^2.
$$</math-renderer></p>
<p dir="auto">Example 9.1.2.
The quadratic form</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = x^2 + y^2
$$</math-renderer></p>
<p dir="auto">corresponds to the matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = I_2$</math-renderer>. It measures squared Euclidean distance from the origin.</p>
<p dir="auto">Example 9.1.3.
The conic section equation</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
4x^2 + 2xy + 5y^2 = 1
$$</math-renderer></p>
<p dir="auto">is described by the quadratic form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}^T A \mathbf{x} = 1$</math-renderer> with</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 4 &amp; 1 \ 1 &amp; 5 \end{bmatrix}.
$$</math-renderer></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Diagonalization of Quadratic Forms</h3><a id="user-content-diagonalization-of-quadratic-forms" aria-label="Permalink: Diagonalization of Quadratic Forms" href="#diagonalization-of-quadratic-forms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">By choosing a new basis consisting of eigenvectors of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>, we can rewrite the quadratic form without cross terms.
If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A = PDP^{-1}$</math-renderer> with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> diagonal, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = (P^{-1}\mathbf{x})^T D (P^{-1}\mathbf{x}).
$$</math-renderer></p>
<p dir="auto">Thus quadratic forms can always be expressed as a sum of weighted squares:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(\mathbf{y}) = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_i$</math-renderer> are the eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>

<p dir="auto">Quadratic forms describe geometric shapes:</p>
<ul dir="auto">
<li>In 2D: ellipses, parabolas, hyperbolas.</li>
<li>In 3D: ellipsoids, paraboloids, hyperboloids.</li>
<li>In higher dimensions: generalizations of ellipsoids.</li>
</ul>
<p dir="auto">Diagonalization aligns the coordinate axes with the principal axes of the shape.</p>

<p dir="auto">Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics (
covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms
leads directly to the spectral theorem.</p>

<ol dir="auto">
<li>Write the quadratic form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y) = 3x^2 + 4xy + y^2$</math-renderer> as <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}^T A \mathbf{x}$</math-renderer> for some symmetric matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</li>
<li>For $A = \begin{bmatrix} 1 &amp; 2 \ 2 &amp; 1 \end{bmatrix}$, compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y)$</math-renderer> explicitly.</li>
<li>Diagonalize the quadratic form <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y) = 2x^2 + 2xy + 3y^2$</math-renderer>.</li>
<li>Identify the conic section given by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(x,y) = x^2 - y^2$</math-renderer>.</li>
<li>Show that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is symmetric, quadratic forms defined by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T$</math-renderer> are identical.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">9.2 Positive Definite Matrices</h2><a id="user-content-92-positive-definite-matrices" aria-label="Permalink: 9.2 Positive Definite Matrices" href="#92-positive-definite-matrices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee
positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis,
and statistics.</p>

<p dir="auto">A symmetric matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is called:</p>
<ul dir="auto">
<li>
<p dir="auto">Positive definite if</p>
<p dir="auto">$$
\mathbf{x}^T A \mathbf{x} &gt; 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n.
$$</p>
</li>
<li>
<p dir="auto">Positive semidefinite if</p>
<p dir="auto">$$
\mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}.
$$</p>
</li>
</ul>
<p dir="auto">Similarly, negative definite (always &lt; 0) and indefinite (can be both &lt; 0 and &gt; 0) matrices are defined.</p>

<p dir="auto">Example 9.2.1.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 0 \ 0 &amp; 3 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">is positive definite, since</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = 2x^2 + 3y^2 &gt; 0
$$</math-renderer></p>
<p dir="auto">for all <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y) \neq (0,0)$</math-renderer>.</p>
<p dir="auto">Example 9.2.2.</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 1 &amp; 2 \ 2 &amp; 1 \end{bmatrix}
$$</math-renderer></p>
<p dir="auto">has quadratic form</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q(x,y) = x^2 + 4xy + y^2.
$$</math-renderer></p>
<p dir="auto">This matrix is not positive definite, since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q(1,-1) = -2 &amp;lt; 0$</math-renderer>.</p>

<p dir="auto">For a symmetric matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>:</p>
<ol dir="auto">
<li>
<p dir="auto">Eigenvalue test: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite if and only if all eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> are positive.</p>
</li>
<li>
<p dir="auto">Principal minors test (Sylvester’s criterion): <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite if and only if all leading principal minors (
determinants of top-left <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k \times k$</math-renderer> submatrices) are positive.</p>
</li>
<li>
<p dir="auto">Cholesky factorization: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite if and only if it can be written as</p>
<p dir="auto">$$
A = R^T R,
$$</p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R$</math-renderer> is an upper triangular matrix with positive diagonal entries.</p>
</li>
</ol>

<ul dir="auto">
<li>Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.</li>
<li>Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).</li>
<li>Indefinite matrices define hyperbolas or saddle-shaped surfaces.</li>
</ul>

<ul dir="auto">
<li>Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive
definite Hessians.</li>
<li>Statistics: Covariance matrices are positive semidefinite.</li>
<li>Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.</li>
</ul>

<p dir="auto">Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are
bounded below, optimization problems have unique solutions, and statistical models are meaningful.</p>

<ol dir="auto">
<li>
<p dir="auto">Use Sylvester’s criterion to check whether</p>
<p dir="auto">$$
A = \begin{bmatrix} 2 &amp; -1 \ -1 &amp; 2 \end{bmatrix}
$$</p>
<p dir="auto">is positive definite.</p>
</li>
<li>
<p dir="auto">Determine whether</p>
<p dir="auto">$$
A = \begin{bmatrix} 0 &amp; 1 \ 1 &amp; 0 \end{bmatrix}
$$</p>
<p dir="auto">is positive definite, semidefinite, or indefinite.</p>
</li>
<li>
<p dir="auto">Find the eigenvalues of</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 2 \ 2 &amp; 3 \end{bmatrix},
$$</p>
<p dir="auto">and use them to classify definiteness.</p>
</li>
<li>
<p dir="auto">Prove that all diagonal matrices with positive entries are positive definite.</p>
</li>
<li>
<p dir="auto">Show that if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> is positive definite, then so is <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P^T A P$</math-renderer> for any invertible matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer>.</p>
</li>
</ol>

<p dir="auto">The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always
be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal
directions), and applications (stability, optimization, statistics).</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Statement of the Spectral Theorem</h3><a id="user-content-statement-of-the-spectral-theorem" aria-label="Permalink: Statement of the Spectral Theorem" href="#statement-of-the-spectral-theorem"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer> is symmetric (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T = A$</math-renderer>), then:</p>
<ol dir="auto">
<li>
<p dir="auto">All eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> are real.</p>
</li>
<li>
<p dir="auto">There exists an orthonormal basis of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^n$</math-renderer> consisting of eigenvectors of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Thus, <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> can be written as</p>
<p dir="auto">$$
A = Q \Lambda Q^T,
$$</p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> is an orthogonal matrix (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q^T Q = I$</math-renderer>) and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Lambda$</math-renderer> is diagonal with eigenvalues of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer> on the diagonal.</p>
</li>
</ol>

<ul dir="auto">
<li>Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.</li>
<li>Quadratic forms <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}^T A \mathbf{x}$</math-renderer> can be expressed in terms of eigenvalues and eigenvectors, showing
ellipsoids aligned with eigen-directions.</li>
<li>Positive definiteness can be checked by confirming that all eigenvalues are positive.</li>
</ul>

<p dir="auto">Let</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<ol dir="auto">
<li>Characteristic polynomial:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
p(\lambda) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$</math-renderer></p>
<p dir="auto">Eigenvalues: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda_1 = 1, \ \lambda_2 = 3$</math-renderer>.</p>
<ol start="2" dir="auto">
<li>Eigenvectors:</li>
</ol>
<ul dir="auto">
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda=1$</math-renderer>: solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-I)\mathbf{v} = 0$</math-renderer>, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,-1)$</math-renderer>.</li>
<li>For <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda=3$</math-renderer>: solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A-3I)\mathbf{v} = 0$</math-renderer>, giving <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)$</math-renderer>.</li>
</ul>
<ol start="3" dir="auto">
<li>Normalize eigenvectors:</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{u}_1 = \tfrac{1}{\sqrt{2}}(1,-1), \quad \mathbf{u}_2 = \tfrac{1}{\sqrt{2}}(1,1).
$$</math-renderer></p>
<ol start="4" dir="auto">
<li>Then</li>
</ol>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
Q = \begin{bmatrix} \tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}} [6pt] -\tfrac{1}{\sqrt{2}} &amp; \tfrac{1}{\sqrt{2}} \end{bmatrix}, \quad
\Lambda = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; 3 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">So</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = Q \Lambda Q^T.
$$</math-renderer></p>

<p dir="auto">The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry,
this corresponds to stretching space along perpendicular axes.</p>
<ul dir="auto">
<li>Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.</li>
<li>Orthogonality ensures directions remain perpendicular after transformation.</li>
</ul>

<ul dir="auto">
<li>Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.</li>
<li>PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of
maximum variance.</li>
<li>Differential equations &amp; physics: Symmetric operators correspond to measurable quantities with real eigenvalues (
stability, energy).</li>
</ul>

<p dir="auto">The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms
of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.</p>

<ol dir="auto">
<li>
<p dir="auto">Diagonalize</p>
<p dir="auto">$$
A = \begin{bmatrix} 4 &amp; 2 \ 2 &amp; 3 \end{bmatrix}
$$</p>
<p dir="auto">using the spectral theorem.</p>
</li>
<li>
<p dir="auto">Prove that all eigenvalues of a real symmetric matrix are real.</p>
</li>
<li>
<p dir="auto">Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.</p>
</li>
<li>
<p dir="auto">Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.</p>
</li>
<li>
<p dir="auto">Apply the spectral theorem to the covariance matrix</p>
<p dir="auto">$$
\Sigma = \begin{bmatrix} 2 &amp; 1 \ 1 &amp; 2 \end{bmatrix},
$$</p>
<p dir="auto">and interpret the eigenvectors as principal directions of variance.</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">9.4 Principal Component Analysis (PCA)</h2><a id="user-content-94-principal-component-analysis-pca" aria-label="Permalink: 9.4 Principal Component Analysis (PCA)" href="#94-principal-component-analysis-pca"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its
core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal
components) that capture the maximum variance in data.</p>

<p dir="auto">Given a dataset of vectors <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n$</math-renderer>:</p>
<ol dir="auto">
<li>
<p dir="auto">Center the data by subtracting the mean vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\bar{\mathbf{x}}$</math-renderer>.</p>
</li>
<li>
<p dir="auto">Form the covariance matrix</p>
<p dir="auto">$$
\Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T.
$$</p>
</li>
<li>
<p dir="auto">Apply the spectral theorem: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma = Q \Lambda Q^T$</math-renderer>.</p>
<ul dir="auto">
<li>Columns of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$Q$</math-renderer> are orthonormal eigenvectors (principal directions).</li>
<li>Eigenvalues in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Lambda$</math-renderer> measure variance explained by each direction.</li>
</ul>
</li>
</ol>
<p dir="auto">The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum
variance.</p>

<p dir="auto">Suppose we have two-dimensional data points roughly aligned along the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = x$</math-renderer>. The covariance matrix is
approximately</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\Sigma = \begin{bmatrix} 2 &amp; 1.9 \ 1.9 &amp; 2 \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Eigenvalues are about <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3.9$</math-renderer> and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0.1$</math-renderer>. The eigenvector for <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\lambda = 3.9$</math-renderer> is approximately <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1)/\sqrt{2}$</math-renderer>.</p>
<ul dir="auto">
<li>First principal component: the line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = x$</math-renderer>.</li>
<li>Most variance lies along this direction.</li>
<li>Second component is nearly orthogonal (<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = -x$</math-renderer>), but variance there is tiny.</li>
</ul>
<p dir="auto">Thus PCA reduces the data to essentially one dimension.</p>

<ol dir="auto">
<li>Dimensionality reduction: Represent data with fewer features while retaining most variance.</li>
<li>Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.</li>
<li>Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.</li>
<li>Compression: PCA is used in image and signal compression.</li>
</ol>
<div dir="auto"><h3 tabindex="-1" dir="auto">Connection to the Spectral Theorem</h3><a id="user-content-connection-to-the-spectral-theorem" aria-label="Permalink: Connection to the Spectral Theorem" href="#connection-to-the-spectral-theorem"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The covariance matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma$</math-renderer> is always symmetric and positive semidefinite. Hence by the spectral theorem, it has an
orthonormal basis of eigenvectors and nonnegative real eigenvalues. PCA is nothing more than re-expressing data in this
eigenbasis.</p>

<p dir="auto">PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a
practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important
algorithms derived from the spectral theorem.</p>

<ol dir="auto">
<li>Show that the covariance matrix is symmetric and positive semidefinite.</li>
<li>Compute the covariance matrix of the dataset <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,2), (2,3), (3,4)$</math-renderer>, and find its eigenvalues and eigenvectors.</li>
<li>Explain why the first principal component captures the maximum variance.</li>
<li>In image compression, explain how PCA can reduce storage by keeping only the top <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> principal components.</li>
<li>Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.</li>
</ol>

<div dir="auto"><h2 tabindex="-1" dir="auto">10.1 Computer Graphics (Rotations, Projections)</h2><a id="user-content-101-computer-graphics-rotations-projections" aria-label="Permalink: 10.1 Computer Graphics (Rotations, Projections)" href="#101-computer-graphics-rotations-projections"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or
projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections
are all linear transformations, making matrices the natural tool for manipulating geometry.</p>

<p dir="auto">A counterclockwise rotation by an angle <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\theta$</math-renderer> in the plane is represented by</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_\theta =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">For any vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{v} \in \mathbb{R}^2$</math-renderer>, the rotated vector is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{v}&#39; = R_\theta \mathbf{v}.
$$</math-renderer></p>
<p dir="auto">This preserves lengths and angles, since <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$R_\theta$</math-renderer> is orthogonal with determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>.</p>

<p dir="auto">In three dimensions, rotations are represented by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$3 \times 3$</math-renderer> orthogonal matrices with determinant <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>. For example, a
rotation about the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$z$</math-renderer>-axis is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
R_z(\theta) =
\begin{bmatrix}
\cos\theta &amp; -\sin\theta &amp; 0 \\
\sin\theta &amp; \cos\theta &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Similar formulas exist for rotations about the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$x$</math-renderer>- and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y$</math-renderer>-axes.</p>
<p dir="auto">More general 3D rotations can be described by axis–angle representation or quaternions, but the underlying idea is still
linear transformations represented by matrices.</p>

<p dir="auto">To display 3D objects on a 2D screen, we use projections:</p>
<ol dir="auto">
<li>
<p dir="auto">Orthogonal projection: drops the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$z$</math-renderer>-coordinate, mapping <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z) \mapsto (x,y)$</math-renderer>.</p>
<p dir="auto">$$
P = \begin{bmatrix}
1 &amp; 0 &amp; 0 \
0 &amp; 1 &amp; 0
\end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Perspective projection: mimics the effect of a camera. A point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z)$</math-renderer> projects to</p>
<p dir="auto">$$
\left(\frac{x}{z}, \frac{y}{z}\right),
$$</p>
<p dir="auto">capturing how distant objects appear smaller.</p>
</li>
</ol>
<p dir="auto">These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in
homogeneous coordinates).</p>

<p dir="auto">To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D
point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z)$</math-renderer> is represented as a 4D vector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x,y,z,1)$</math-renderer>. Transformations are then <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$4 \times 4$</math-renderer> matrices, which can
represent rotations, scalings, and translations in a single framework.</p>
<p dir="auto">Example: Translation by <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(a,b,c)$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
T = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; a \\
0 &amp; 1 &amp; 0 &amp; b \\
0 &amp; 0 &amp; 1 &amp; c \\
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}.
$$</math-renderer></p>

<ul dir="auto">
<li>Rotations preserve shape and size, only changing orientation.</li>
<li>Projections reduce dimension: from 3D world space to 2D screen space.</li>
<li>Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a
single matrix multiplication.</li>
</ul>

<p dir="auto">Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining
simple matrix operations, complex transformations are applied efficiently to millions of points per second.</p>

<ol dir="auto">
<li>Write the rotation matrix for a 90° counterclockwise rotation in <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^2$</math-renderer>. Apply it to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0)$</math-renderer>.</li>
<li>Rotate the point <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,0)$</math-renderer> about the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$z$</math-renderer>-axis by 180°.</li>
<li>Show that the determinant of any 2D or 3D rotation matrix is 1.</li>
<li>Derive the orthogonal projection matrix from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbb{R}^3$</math-renderer> to the <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$xy$</math-renderer>-plane.</li>
<li>Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">10.2 Data Science (Dimensionality Reduction, Least Squares)</h2><a id="user-content-102-data-science-dimensionality-reduction-least-squares" aria-label="Permalink: 10.2 Data Science (Dimensionality Reduction, Least Squares)" href="#102-data-science-dimensionality-reduction-least-squares"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality
reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares
method, which underlies regression and model fitting.</p>

<p dir="auto">High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a
lower-dimensional subspace. Dimensionality reduction identifies these subspaces.</p>
<ul dir="auto">
<li>
<p dir="auto">PCA (Principal Component Analysis):
As introduced earlier, PCA diagonalizes the covariance matrix of the data.</p>
<ul dir="auto">
<li>Eigenvectors (principal components) define orthogonal directions of maximum variance.</li>
<li>Eigenvalues measure how much variance lies along each direction.</li>
<li>Keeping only the top <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> components reduces data from <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer>-dimensional space to <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer>-dimensional space while
retaining most variability.</li>
</ul>
</li>
</ul>
<p dir="auto">Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors
of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.</p>

<p dir="auto">Often, we have more equations than unknowns-an overdetermined system:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A\mathbf{x} \approx \mathbf{b}, \quad A \in \mathbb{R}^{m \times n}, \ m &gt; n.
$$</math-renderer></p>
<p dir="auto">An exact solution may not exist. Instead, we seek <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}$</math-renderer> that minimizes the error</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
|A\mathbf{x} - \mathbf{b}|^2.
$$</math-renderer></p>
<p dir="auto">This leads to the normal equations:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$</math-renderer></p>
<p dir="auto">The solution is the orthogonal projection of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{b}$</math-renderer> onto the column space of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A$</math-renderer>.</p>

<p dir="auto">Fit a line <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$y = mx + c$</math-renderer> to data points <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(x_i, y_i)$</math-renderer>.</p>
<p dir="auto">Matrix form:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A = \begin{bmatrix}
x_1 &amp; 1 \\
x_2 &amp; 1 \\
\vdots &amp; \vdots \\
x_m &amp; 1
\end{bmatrix},
\quad
\mathbf{b} = \begin{bmatrix} y_1 \ y_2 \ \vdots \ y_m \end{bmatrix},
\quad
\mathbf{x} = \begin{bmatrix} m \ c \end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">Solve <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T A \mathbf{x} = A^T \mathbf{b}$</math-renderer>. This yields the best-fit line in the least squares sense.</p>

<ul dir="auto">
<li>Dimensionality reduction: Find the best subspace capturing most variance.</li>
<li>Least squares: Project the target vector onto the subspace spanned by predictors.</li>
</ul>
<p dir="auto">Both are projection problems, solved using inner products and orthogonality.</p>

<p dir="auto">Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting
powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and
projections-core tools of linear algebra.</p>

<ol dir="auto">
<li>Explain why PCA reduces noise in datasets by discarding small eigenvalue components.</li>
<li>Compute the least squares solution to fitting a line through <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(0,0), (1,1), (2,2)$</math-renderer>.</li>
<li>Show that the least squares solution is unique if and only if <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^T A$</math-renderer> is invertible.</li>
<li>Prove that the least squares solution minimizes the squared error by projection arguments.</li>
<li>Apply PCA to the data points <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,0), (2,1), (3,2)$</math-renderer> and find the first principal component.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">10.3 Networks and Markov Chains</h2><a id="user-content-103-networks-and-markov-chains" aria-label="Permalink: 10.3 Networks and Markov Chains" href="#103-networks-and-markov-chains"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity
to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already
introduced in Section 8.4, are a central example of networks evolving over time.</p>

<p dir="auto">A network (graph) with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> nodes can be represented by an adjacency matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A \in \mathbb{R}^{n \times n}$</math-renderer>:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
A_{ij} =
\begin{cases}
1 &amp; \text{if there is an edge from node (i) to node (j)} \\
0 &amp; \text{otherwise.}
\end{cases}
$$</math-renderer></p>
<p dir="auto">For weighted graphs, entries may be positive weights instead of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0/1$</math-renderer>.</p>
<ul dir="auto">
<li>The number of walks of length <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> from node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer> to node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer> is given by the entry <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(A^k)_{ij}$</math-renderer>.</li>
<li>Powers of adjacency matrices thus encode connectivity over time.</li>
</ul>

<p dir="auto">Another important matrix is the graph Laplacian:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
L = D - A,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$D$</math-renderer> is the diagonal degree matrix ($D_{ii} = \text{degree}(i)$).</p>
<ul dir="auto">
<li>
<math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$L$</math-renderer> is symmetric and positive semidefinite.</li>
<li>The smallest eigenvalue is always <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0$</math-renderer>, with eigenvector <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1,1,\dots,1)$</math-renderer>.</li>
<li>The multiplicity of eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$0$</math-renderer> equals the number of connected components in the graph.</li>
</ul>
<p dir="auto">This connection between eigenvalues and connectivity forms the basis of spectral graph theory.</p>

<p dir="auto">A Markov chain can be viewed as a random walk on a graph. If <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> is the transition matrix where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P_{ij}$</math-renderer> is the
probability of moving from node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$i$</math-renderer> to node <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$j$</math-renderer>, then</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\mathbf{x}_{k+1} = P \mathbf{x}_k
$$</math-renderer></p>
<p dir="auto">describes the distribution of positions after <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$k$</math-renderer> steps.</p>
<ul dir="auto">
<li>The steady-state distribution is given by the eigenvector of <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$P$</math-renderer> with eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>.</li>
<li>The speed of convergence depends on the gap between the largest eigenvalue (which is always <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer>) and the second
largest eigenvalue.</li>
</ul>

<p dir="auto">Consider a simple 3-node cycle graph:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
P = \begin{bmatrix}
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0
\end{bmatrix}.
$$</math-renderer></p>
<p dir="auto">This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of
unity: <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1, e^{2\pi i/3}, e^{4\pi i/3}$</math-renderer>. The eigenvalue <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$1$</math-renderer> corresponds to the steady state, which is the uniform
distribution <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$(1/3,1/3,1/3)$</math-renderer>.</p>

<ul dir="auto">
<li>Search engines: Google’s PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank
pages.</li>
<li>Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.</li>
<li>Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.</li>
</ul>

<p dir="auto">Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow,
stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these
tools are indispensable.</p>

<ol dir="auto">
<li>
<p dir="auto">Write the adjacency matrix of a square graph with 4 nodes. Compute <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$A^2$</math-renderer> and interpret the entries.</p>
</li>
<li>
<p dir="auto">Show that the Laplacian of a connected graph has exactly one zero eigenvalue.</p>
</li>
<li>
<p dir="auto">Find the steady-state distribution of the Markov chain with</p>
<p dir="auto">$$
P = \begin{bmatrix} 0.5 &amp; 0.5 \ 0.4 &amp; 0.6 \end{bmatrix}.
$$</p>
</li>
<li>
<p dir="auto">Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.</p>
</li>
<li>
<p dir="auto">Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">10.4 Machine Learning Connections</h2><a id="user-content-104-machine-learning-connections" aria-label="Permalink: 10.4 Machine Learning Connections" href="#104-machine-learning-connections"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of
large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix
decompositions.</p>

<p dir="auto">A dataset with <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$m$</math-renderer> examples and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$n$</math-renderer> features is represented as a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$X \in \mathbb{R}^{m \times n}$</math-renderer>:</p>
<p dir="auto">$$
X =
\begin{bmatrix}</p>
<ul dir="auto">
<li>&amp; \mathbf{x}_1^T &amp; - \</li>
<li>&amp; \mathbf{x}_2^T &amp; - \
&amp; \vdots &amp; \</li>
<li>&amp; \mathbf{x}_m^T &amp; -
\end{bmatrix},
$$</li>
</ul>
<p dir="auto">where each row <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{x}_i \in \mathbb{R}^n$</math-renderer> is a feature vector. Linear algebra provides tools to analyze, compress,
and transform this data.</p>

<p dir="auto">At the heart of machine learning are linear predictors:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\hat{y} = X\mathbf{w},
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\mathbf{w}$</math-renderer> is the weight vector. Training often involves solving a least squares problem or a regularized
variant such as ridge regression:</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
\min_{\mathbf{w}} |X\mathbf{w} - \mathbf{y}|^2 + \lambda |\mathbf{w}|^2.
$$</math-renderer></p>
<p dir="auto">This is solved efficiently using matrix factorizations.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Singular Value Decomposition (SVD)</h3><a id="user-content-singular-value-decomposition-svd" aria-label="Permalink: Singular Value Decomposition (SVD)" href="#singular-value-decomposition-svd"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The SVD of a matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$X$</math-renderer> is</p>
<p dir="auto"><math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$$
X = U \Sigma V^T,
$$</math-renderer></p>
<p dir="auto">where <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$U, V$</math-renderer> are orthogonal and <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma$</math-renderer> is diagonal with nonnegative entries (singular values).</p>
<ul dir="auto">
<li>Singular values measure the importance of directions in feature space.</li>
<li>SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Eigenvalues in Machine Learning</h3><a id="user-content-eigenvalues-in-machine-learning" aria-label="Permalink: Eigenvalues in Machine Learning" href="#eigenvalues-in-machine-learning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal
variance.</li>
<li>Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.</li>
<li>Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.</li>
</ul>

<p dir="auto">Even deep learning, though nonlinear, uses linear algebra at its core:</p>
<ul dir="auto">
<li>Each layer is a matrix multiplication followed by a nonlinear activation.</li>
<li>Training requires computing gradients, which are expressed in terms of matrix calculus.</li>
<li>Backpropagation is essentially repeated applications of the chain rule with linear algebra.</li>
</ul>

<p dir="auto">Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the
algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would
be intractable.</p>

<ol dir="auto">
<li>
<p dir="auto">Show that ridge regression leads to the normal equations</p>
<p dir="auto">$$
(X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}.
$$</p>
</li>
<li>
<p dir="auto">Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.</p>
</li>
<li>
<p dir="auto">For a covariance matrix <math-renderer data-run-id="8c1bbf06f299f9f1a5fd03b84f361f2a">$\Sigma$</math-renderer>, show why its eigenvalues represent variances along principal components.</p>
</li>
<li>
<p dir="auto">Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.</p>
</li>
<li>
<p dir="auto">In a neural network with one hidden layer, write the forward pass in matrix form.</p>
</li>
</ol>
</article></div></div>
  </body>
</html>
