<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/deepseek-ai/DeepEP">Original</a>
    <h1>DeepSeek open source DeepEP â€“ library for MoE training and Inference</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.</p>
<p dir="auto">To align with the group-limited gating algorithm proposed in the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (Streaming Multiprocessors) number control.</p>
<p dir="auto">For latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.</p>
<p dir="auto">Notice: the implementation in this library may have some slight differences from the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper.</p>

<div dir="auto"><h3 tabindex="-1" dir="auto">Normal kernels with NVLink and RDMA forwarding</h3><a id="user-content-normal-kernels-with-nvlink-and-rdma-forwarding" aria-label="Permalink: Normal kernels with NVLink and RDMA forwarding" href="#normal-kernels-with-nvlink-and-rdma-forwarding"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Type</th>
<th>Dispatch #EP</th>
<th>Bottleneck bandwidth</th>
<th>Combine #EP</th>
<th>Bottleneck bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intranode</td>
<td>8</td>
<td>153 GB/s (NVLink)</td>
<td>8</td>
<td>158 GB/s (NVLink)</td>
</tr>
<tr>
<td>Internode</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>32</td>
<td>44 GB/s (RDMA)</td>
<td>32</td>
<td>47 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>64</td>
<td>46 GB/s (RDMA)</td>
<td>64</td>
<td>45 GB/s (RDMA)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h3 tabindex="-1" dir="auto">Low-latency kernels with pure RDMA</h3><a id="user-content-low-latency-kernels-with-pure-rdma" aria-label="Permalink: Low-latency kernels with pure RDMA" href="#low-latency-kernels-with-pure-rdma"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Dispatch #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
<th>Combine #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>163 us</td>
<td>46 GB/s</td>
<td>8</td>
<td>318 us</td>
<td>46 GB/s</td>
</tr>
<tr>
<td>16</td>
<td>173 us</td>
<td>43 GB/s</td>
<td>16</td>
<td>329 us</td>
<td>44 GB/s</td>
</tr>
<tr>
<td>32</td>
<td>182 us</td>
<td>41 GB/s</td>
<td>32</td>
<td>350 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>64</td>
<td>186 us</td>
<td>40 GB/s</td>
<td>64</td>
<td>353 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>128</td>
<td>192 us</td>
<td>39 GB/s</td>
<td>128</td>
<td>369 us</td>
<td>39 GB/s</td>
</tr>
<tr>
<td>256</td>
<td>194 us</td>
<td>39 GB/s</td>
<td>256</td>
<td>360 us</td>
<td>40 GB/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>


<ul dir="auto">
<li>Hopper GPUs (may support more architectures or devices later)</li>
<li>Python 3.8 and above</li>
<li>CUDA 12.3 and above</li>
<li>PyTorch 2.1 and above</li>
<li>NVLink for intranode communication</li>
<li>RDMA network for internode communication</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Download and install NVSHMEM dependency</h3><a id="user-content-download-and-install-nvshmem-dependency" aria-label="Permalink: Download and install NVSHMEM dependency" href="#download-and-install-nvshmem-dependency"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">DeepEP also depends on our modified NVSHMEM. Please refer to our <a href="https://github.com/deepseek-ai/DeepEP/blob/main/third-party/README.md">NVSHMEM Installation Guide</a> for instructions.</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Build and make symbolic links for SO files
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
# You may modify the specific SO names according to your own platform
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

# Run test cases
# NOTES: you may modify the `init_dist` function in `tests/utils.py`
# according to your own cluster settings, and launch into multiple nodes 
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py"><pre><span><span>#</span> Build and make symbolic links for SO files</span>
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
<span><span>#</span> You may modify the specific SO names according to your own platform</span>
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

<span><span>#</span> Run test cases</span>
<span><span>#</span> NOTES: you may modify the `init_dist` function in `tests/utils.py`</span>
<span><span>#</span> according to your own cluster settings, and launch into multiple nodes </span>
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install"><pre>NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install</pre></div>
<p dir="auto">Then, import <code>deep_ep</code> in your Python project, and enjoy!</p>

<p dir="auto">DeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (RoCE) as well.</p>

<p dir="auto">Traffic isolation is supported by InfiniBand through Virtual Lanes (VL).</p>
<p dir="auto">To prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:</p>
<ul dir="auto">
<li>workloads using normal kernels</li>
<li>workloads using low-latency kernels</li>
<li>other workloads</li>
</ul>
<p dir="auto">For DeepEP, you can control the virtual lane assignment by setting the <code>NVSHMEM_IB_SL</code> environment variable.</p>

<p dir="auto">Adaptive routing is an advanced routing feature provided by InfiniBand switches that can evenly distribute traffic across multiple paths. Currently, low-latency kernels support adaptive routing, while normal kernels do not (support may be added soon). <strong>Enabling adaptive routing for normal internode kernels may lead to deadlocks or data corruption issues</strong>.</p>
<p dir="auto">For low-latency kernels, enabling adaptive routing can completely eliminate network congestion caused by routing conflicts, but it also introduces additional latency. We recommend the following configuration for optimal performance:</p>
<ul dir="auto">
<li>enable adaptive routing in environments with heavy network loads</li>
<li>use static routing in environments with light network loads</li>
</ul>

<p dir="auto">Congestion control is disabled as we have not observed significant congestion in our production environment.</p>

<div dir="auto"><h3 tabindex="-1" dir="auto">Example use in model training or inference prefilling</h3><a id="user-content-example-use-in-model-training-or-inference-prefilling" aria-label="Permalink: Example use in model training or inference prefilling" href="#example-use-in-model-training-or-inference-prefilling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The normal kernels can be used in model training or the inference prefilling phase (without the backward part) as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import List, Tuple, Optional, Union

from deep_ep import Buffer, EventOverlap

# Communication buffer (will allocate at runtime)
_buffer: Optional[Buffer] = None

# Set the number of SMs to use
# NOTES: this is a static variable
Buffer.set_num_sms(24)


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -&gt; Buffer:
    global _buffer
    
    # NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests
    num_nvl_bytes, num_rdma_bytes = 0, 0
    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):
        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)
        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)

    # Allocate a buffer if not existed or not enough buffer size
    # NOTES: the adaptive routing configuration of the network **must be off**
    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes &lt; num_nvl_bytes or _buffer.num_rdma_bytes &lt; num_rdma_bytes:
        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)
    return _buffer


def get_hidden_bytes(x: torch.Tensor) -&gt; int:
    t = x[0] if isinstance(x, tuple) else x
    return t.size(1) * max(t.element_size(), 2)


def dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     topk_idx: torch.Tensor, topk_weights: torch.Tensor,
                     num_experts: int, previous_event: Optional[EventOverlap] = None) -&gt; \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:
    # NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency 
    # of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please
    # refer to the docs of `Buffer.dispatch`
    global _buffer

    # Calculate layout before actual dispatch
    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \
        _buffer.get_dispatch_layout(topk_idx, num_experts,
                                    previous_event=previous_event, async_finish=True,
                                    allocate_on_comm_stream=previous_event is not None)
    # Do MoE dispatch
    # NOTES: the CPU will wait for GPU&#39;s signal to arrive, so this is not compatible with CUDA graph
    # For more advanced usages, please refer to the docs of the `dispatch` function
    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \
        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,
                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,
                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert,
                         previous_event=previous_event, async_finish=True,
                         allocate_on_comm_stream=True)
    # For event management, please refer to the docs of the `EventOverlap` class
    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event


def dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -&gt; \
        Tuple[torch.Tensor, torch.Tensor, EventOverlap]:
    global _buffer

    # The backward process of MoE dispatch is actually a combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_grad_x, combined_grad_recv_topk_weights, event = \
        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_grad_x, combined_grad_recv_topk_weights, event


def combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -&gt; \
        Tuple[torch.Tensor, EventOverlap]:
    global _buffer

    # Do MoE combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,
                                           allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_x, event


def combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     handle: Tuple, previous_event: Optional[EventOverlap] = None) -&gt; \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:
    global _buffer

    # The backward process of MoE combine is actually a dispatch
    # For more advanced usages, please refer to the docs of the `combine` function
    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,
                                                 previous_event=previous_event,
                                                 allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return grad_x, event"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>List</span>, <span>Tuple</span>, <span>Optional</span>, <span>Union</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>, <span>EventOverlap</span>

<span># Communication buffer (will allocate at runtime)</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>

<span># Set the number of SMs to use</span>
<span># NOTES: this is a static variable</span>
<span>Buffer</span>.<span>set_num_sms</span>(<span>24</span>)


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>hidden_bytes</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span>global</span> <span>_buffer</span>
    
    <span># NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests</span>
    <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span> <span>=</span> <span>0</span>, <span>0</span>
    <span>for</span> <span>config</span> <span>in</span> (<span>Buffer</span>.<span>get_dispatch_config</span>(<span>group</span>.<span>size</span>()), <span>Buffer</span>.<span>get_combine_config</span>(<span>group</span>.<span>size</span>())):
        <span>num_nvl_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_nvl_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_nvl_bytes</span>)
        <span>num_rdma_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_rdma_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_rdma_bytes</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span># NOTES: the adaptive routing configuration of the network **must be off**</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>_buffer</span>.<span>num_nvl_bytes</span> <span>&lt;</span> <span>num_nvl_bytes</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span>)
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>get_hidden_bytes</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>) <span>-&gt;</span> <span>int</span>:
    <span>t</span> <span>=</span> <span>x</span>[<span>0</span>] <span>if</span> <span>isinstance</span>(<span>x</span>, <span>tuple</span>) <span>else</span> <span>x</span>
    <span>return</span> <span>t</span>.<span>size</span>(<span>1</span>) <span>*</span> <span>max</span>(<span>t</span>.<span>element_size</span>(), <span>2</span>)


<span>def</span> <span>dispatch_forward</span>(<span>x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>,
                     <span>num_experts</span>: <span>int</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>List</span>, <span>Tuple</span>, <span>EventOverlap</span>]:
    <span># NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency </span>
    <span># of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please</span>
    <span># refer to the docs of `Buffer.dispatch`</span>
    <span>global</span> <span>_buffer</span>

    <span># Calculate layout before actual dispatch</span>
    <span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span>, <span>num_tokens_per_expert</span>, <span>is_token_in_rank</span>, <span>previous_event</span> <span>=</span> \
        <span>_buffer</span>.<span>get_dispatch_layout</span>(<span>topk_idx</span>, <span>num_experts</span>,
                                    <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                    <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)
    <span># Do MoE dispatch</span>
    <span># NOTES: the CPU will wait for GPU&#39;s signal to arrive, so this is not compatible with CUDA graph</span>
    <span># For more advanced usages, please refer to the docs of the `dispatch` function</span>
    <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>dispatch</span>(<span>x</span>, <span>topk_idx</span><span>=</span><span>topk_idx</span>, <span>topk_weights</span><span>=</span><span>topk_weights</span>,
                         <span>num_tokens_per_rank</span><span>=</span><span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span><span>=</span><span>num_tokens_per_rdma_rank</span>,
                         <span>is_token_in_rank</span><span>=</span><span>is_token_in_rank</span>, <span>num_tokens_per_expert</span><span>=</span><span>num_tokens_per_expert</span>,
                         <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                         <span>allocate_on_comm_stream</span><span>=</span><span>True</span>)
    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span>


<span>def</span> <span>dispatch_backward</span>(<span>grad_recv_x</span>: <span>torch</span>.<span>Tensor</span>, <span>grad_recv_topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE dispatch is actually a combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>combine</span>(<span>grad_recv_x</span>, <span>handle</span>, <span>topk_weights</span><span>=</span><span>grad_recv_topk_weights</span>, <span>async_finish</span><span>=</span><span>True</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span>


<span>def</span> <span>combine_forward</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_x</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>combine</span>(<span>x</span>, <span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>, <span>previous_event</span><span>=</span><span>previous_event</span>,
                                           <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_x</span>, <span>event</span>


<span>def</span> <span>combine_backward</span>(<span>grad_combined_x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE combine is actually a dispatch</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>grad_x</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>dispatch</span>(<span>grad_combined_x</span>, <span>handle</span><span>=</span><span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                                 <span>previous_event</span><span>=</span><span>previous_event</span>,
                                                 <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>grad_x</span>, <span>event</span></pre></div>
<p dir="auto">Moreover, inside the dispatch function, we may not know how many tokens to receive for the current rank. So an implicit CPU wait for GPU received count signal will be involved, as the following figure shows.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/normal.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/normal.png" alt="normal"/></a></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Example use in inference decoding</h3><a id="user-content-example-use-in-inference-decoding" aria-label="Permalink: Example use in inference decoding" href="#example-use-in-inference-decoding"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The low latency kernels can be used in the inference decoding phase as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import Tuple, Optional

from deep_ep import Buffer

# Communication buffer (will allocate at runtime)
# NOTES: there is no SM control API for the low-latency kernels
_buffer: Optional[Buffer] = None


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, num_max_dispatch_tokens_per_rank: int, hidden: int, num_experts: int) -&gt; Buffer:
    # NOTES: the low-latency mode will consume much more space than the normal mode
    # So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256
    global _buffer
    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts)

    # Allocate a buffer if not existed or not enough buffer size
    if _buffer is None or _buffer.group != group or not _buffer.low_latency_mode or _buffer.num_rdma_bytes &lt; num_rdma_bytes:
        # NOTES: for best performance, the QP number **must** be equal to the number of the local experts
        assert num_experts % group.size() == 0
        _buffer = Buffer(group, 0, num_rdma_bytes, low_latency_mode=True, num_qps_per_rank=num_experts // group.size())
    return _buffer


def low_latency_dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, num_max_dispatch_tokens_per_rank: int, num_experts: int):
    global _buffer

    # Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)
    recv_hidden_states, recv_expert_count, handle, event, hook = \
        _buffer.low_latency_dispatch(hidden_states, topk_idx, num_max_dispatch_tokens_per_rank, num_experts,
                                     async_finish=False, return_recv_hook=True)

    # NOTES: the actual tensor will not be received only if you call `hook()`,
    # it is useful for double-batch overlapping, but **without any SM occupation**
    # If you don&#39;t want to overlap, please set `return_recv_hook=False`
    # Later, you can use our GEMM library to do the computation with this specific format
    return recv_hidden_states, recv_expert_count, handle, event, hook


def low_latency_combine(hidden_states: torch.Tensor,
                        topk_idx: torch.Tensor, topk_weights: torch.Tensor, handle: Tuple):
    global _buffer

    # Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)
    combined_hidden_states, event_overlap, hook = \
        _buffer.low_latency_combine(hidden_states, topk_idx, topk_weights, handle,
                                    async_finish=False, return_recv_hook=True)

    # NOTES: the same behavior as described in the dispatch kernel
    return combined_hidden_states, event_overlap, hook"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Tuple</span>, <span>Optional</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>

<span># Communication buffer (will allocate at runtime)</span>
<span># NOTES: there is no SM control API for the low-latency kernels</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>hidden</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span># NOTES: the low-latency mode will consume much more space than the normal mode</span>
    <span># So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256</span>
    <span>global</span> <span>_buffer</span>
    <span>num_rdma_bytes</span> <span>=</span> <span>Buffer</span>.<span>get_low_latency_rdma_size_hint</span>(<span>num_max_dispatch_tokens_per_rank</span>, <span>hidden</span>, <span>group</span>.<span>size</span>(), <span>num_experts</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>not</span> <span>_buffer</span>.<span>low_latency_mode</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span># NOTES: for best performance, the QP number **must** be equal to the number of the local experts</span>
        <span>assert</span> <span>num_experts</span> <span>%</span> <span>group</span>.<span>size</span>() <span>==</span> <span>0</span>
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>0</span>, <span>num_rdma_bytes</span>, <span>low_latency_mode</span><span>=</span><span>True</span>, <span>num_qps_per_rank</span><span>=</span><span>num_experts</span> <span>//</span> <span>group</span>.<span>size</span>())
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>low_latency_dispatch</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_dispatch</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>num_max_dispatch_tokens_per_rank</span>, <span>num_experts</span>,
                                     <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the actual tensor will not be received only if you call `hook()`,</span>
    <span># it is useful for double-batch overlapping, but **without any SM occupation**</span>
    <span># If you don&#39;t want to overlap, please set `return_recv_hook=False`</span>
    <span># Later, you can use our GEMM library to do the computation with this specific format</span>
    <span>return</span> <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span>


<span>def</span> <span>low_latency_combine</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>,
                        <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_combine</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>topk_weights</span>, <span>handle</span>,
                                    <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the same behavior as described in the dispatch kernel</span>
    <span>return</span> <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span></pre></div>
<p dir="auto">For two micro-batch overlapping, you can refer to the following figure. With our receiving hook interface, the RDMA network traffics are happening in the background, without costing any GPU SMs from the computation part. But notice, the overlapped parts can be adjusted, i.e. the 4 parts of attention/dispatch/MoE/combine may not have the exact same execution time. You may adjust the stage settings according to your workload.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/low-latency.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/low-latency.png" alt="low-latency"/></a></p>

<ul dir="auto">
<li>For extreme performance, we discover and use a behavior-out-of-doc PTX instruction: <code>ld.global.nc.L1::no_allocate.L2::256B</code>. This instruction will lead to an undefined behavior: accessing volatile GPU memory with non-coherent read-only PTX modifiers <code>.nc</code>. But the correctness is tested to be guaranteed with <code>.L1::no_allocate</code> on Hopper architectures, and performance will be much better. If you find kernels not working on some other platforms, you may add <code>DISABLE_AGGRESSIVE_PTX_INSTRS=1</code> to <code>setup.py</code> and disable this, or file an issue.</li>
<li>For better performance on your cluster, we recommend to run all the tests and use the best auto-tuned configuration. The default configurations are optimized on the DeepSeek&#39;s internal cluster.</li>
</ul>

<p dir="auto">This code repository is released under <a href="https://github.com/deepseek-ai/DeepEP/blob/main/LICENSE">the MIT License</a>, except for codes that reference NVSHMEM (including <code>csrc/kernels/ibgda_device.cuh</code> and <code>third-party/nvshmem.patch</code>), which are subject to <a href="https://docs.nvidia.com/nvshmem/api/sla.html" rel="nofollow">NVSHMEM SLA</a>.</p>

<p dir="auto">If you use this codebase, or otherwise found our work valuable, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{deepep2025,
      title={DeepEP: an efficient expert-parallel communication library},
      author={Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepEP}},
}"><pre><span>@misc</span>{<span>deepep2025</span>,
      <span>title</span>=<span><span>{</span>DeepEP: an efficient expert-parallel communication library<span>}</span></span>,
      <span>author</span>=<span><span>{</span>Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/DeepEP}<span>}</span></span>,
}</pre></div>
</article></div></div>
  </body>
</html>
