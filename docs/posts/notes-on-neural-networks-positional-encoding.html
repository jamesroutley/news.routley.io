<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://accelerated-computing.com/blog/notes-on-neural-networks-part-04/">Original</a>
    <h1>Notes on neural networks: positional encoding</h1>
    
    <div id="readability-page-1" class="page"><div>
<h2>Notes on neural networks: positional encoding</h2>

<h3>7 December 2023</h3>

<p>Disclaimer: this post contains content which might be just simply incorrect. I’m still internalising some of these concepts, and my hope is the act of writing them down will help to make them a bit more concrete. This is very much a “thinking out loud” kind of post.</p>
<h3 id="context-windows-and-positions">Context windows and positions</h3>
<p>I will write about attention and the transformer architecture in a (/ a number of) future posts, but their study has motivated this one so I’ll need to use a bit of that language to set the scene.</p>
<p>A transformer network takes a “context window” as its input. A context window is a sequence of length $n_{\text{context}}$ of (row) vectors in some embedding space of dimension $n_{\text{embed}}$. For example, if we have a two-dimensional word embedding, we might represent the sentence “the dog is good” as the matrix</p>
<p>$$
\begin{pmatrix}
0.1 &amp; -0.3 \\
0.6 &amp; 0.2 \\
-0.4 &amp; -0.1 \\
0.2 &amp; -0.7
\end{pmatrix}
$$</p>
<p>where eg. the embedding of “dog” is $(0.6, 0.2) \in \mathbb{R}^2$. In PyTorch, this might be represented by the following tensor of shape <code>(4, 2)</code>:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>the_dog_is_good</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span>
</span></span><span><span>    <span>[[</span><span>0.1</span><span>,</span> <span>-</span><span>0.3</span><span>],</span> <span>[</span><span>0.6</span><span>,</span> <span>0.2</span><span>],</span> <span>[</span><span>-</span><span>0.4</span><span>,</span> <span>-</span><span>0.1</span><span>],</span> <span>[</span><span>0.2</span><span>,</span> <span>-</span><span>0.7</span><span>]]</span>
</span></span><span><span><span>)</span>
</span></span></code></pre></div><p>There is a point during the calculation of attention scores where information of the positions of elements within their context is lost, so that eg. (in the absence of some additional processing) the sentences “only who can prevent forest fires” and “who can prevent forest fires only” would appear as indistinguishable to the network, despite having different meanings.</p>
<h3 id="the-trick">The trick</h3>
<p>One solution to this problem is to use positional encoding. My high level intuition on this is as follows:</p>
<ol>
<li>We start with some vocabulary of size $n_{\text{vocab}}$ (eg. words in the english dictionary, <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE-generated</a> subwords of scraped internet content, etc.).</li>
<li>Each item in this vocabulary gets embedded into some $n_{\text{embed}}$ dimensional space.</li>
<li>To account for position within a given context, a new composite embedding is formed by taking the original embedding, and creating $n_{\text{context}}$ new values for each vector in its image. Each value is translated by $n_{\text{context}}$ vectors.</li>
<li>The network then takes its inputs from values this new vocabulary, which has size $n_{\text{vocab}} \cdot n_{\text{context}}$.</li>
</ol>
<p>So, continuing our example of of embeddings in space of dimension $n_{\text{embed}} = 2$, with context windows of length $n_{\text{context}} = 4$, we would like four positional translations $p_i = (p_{i, x}, p_{i, y})$ for $i = 1, \ldots, 4$, so that any $(x, y)$ occurring at position $i$ becomes $(x + p_{i, x}, y + p_{i, y})$ by the time the network sees it.</p>
<h3 id="constructing-the-embeddings">Constructing the embeddings</h3>
<p>In the paper <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>, the authors present two concrete approaches – one with no additional network parameters and a second, simpler, version, which comes at the cost of an additional embedding matrix to learn.</p>
<h4 id="sinusoidal-method">Sinusoidal method</h4>
<p>The idea is to translate each point in the embedding space by one of $n_{\text{context}}$ points $p_i$, whose $k$-th component $p_{i, k}$ is defined to be</p>
<p>$$
p_{i, k} = \begin{cases}
\sin(\frac{i}{10000^{\frac{k}{n_{\text{embed}}}}}) &amp;\text{if } k \text{ is even,} \\
\cos(\frac{i}{10000^{\frac{k - 1}{n_{\text{embed}}}}}) &amp;\text{if } k \text{ is odd.}
\end{cases}
$$</p>
<p>Note that by considering the squares of each even-odd pair $(p_{i,k}, p_{i,k+1})$, we see that each $p_i$ lies on the sphere centered at the origin with radius $\sqrt{\frac{n_{\text{embed}}}{2}}$ – the original vocabulary is translated (by equal amounts across all possible positions) in different directions determined by each $p_i$. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>
<p>We can play around with the kinds of positional encodings for various context lengths and embedding dimensions with the following script:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>torch</span>
</span></span><span><span>
</span></span><span><span><span>CONTEXT_LENGTH</span> <span>=</span> <span>4</span>
</span></span><span><span><span>EMBEDDING_DIM</span> <span>=</span> <span>8</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>make_positional_embedding</span><span>(</span><span>context_length</span><span>,</span> <span>embedding_dim</span><span>):</span>
</span></span><span><span>    <span>positions</span> <span>=</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>context_length</span><span>)</span><span>.</span><span>float</span><span>()</span>
</span></span><span><span>    <span>coefficients</span> <span>=</span> <span>10000</span> <span>**</span> <span>-</span><span>(</span>
</span></span><span><span>        <span>((</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>embedding_dim</span><span>)</span><span>.</span><span>int</span><span>()</span> <span>&gt;&gt;</span> <span>1</span><span>)</span> <span>&lt;&lt;</span> <span>1</span><span>)</span> <span>/</span> <span>embedding_dim</span>
</span></span><span><span>    <span>)</span>
</span></span><span><span>    <span>radians</span> <span>=</span> <span>positions</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>)</span> <span>@</span> <span>coefficients</span><span>.</span><span>view</span><span>(</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>radians</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>evens</span> <span>=</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>step</span><span>=</span><span>2</span><span>)</span>
</span></span><span><span>    <span>odds</span> <span>=</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>1</span><span>,</span> <span>embedding_dim</span><span>,</span> <span>step</span><span>=</span><span>2</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>encodings</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>radians</span><span>)</span>
</span></span><span><span>    <span>encodings</span><span>[:,</span> <span>evens</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>sin</span><span>(</span><span>radians</span><span>[:,</span> <span>evens</span><span>])</span>
</span></span><span><span>    <span>encodings</span><span>[:,</span> <span>odds</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>cos</span><span>(</span><span>radians</span><span>[:,</span> <span>odds</span><span>])</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>encodings</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span><span>:</span>
</span></span><span><span>    <span>positional_embedding</span> <span>=</span> <span>make_positional_embedding</span><span>(</span><span>CONTEXT_LENGTH</span><span>,</span> <span>EMBEDDING_DIM</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>positional_embedding</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>positional_embedding</span><span>.</span><span>norm</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>)</span><span>=}</span><span>&#34;</span><span>)</span>
</span></span></code></pre></div><p>which for the context length and embedding dims as written produces the following output:</p>
<pre tabindex="0"><code>radians=tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.0000e+00, 1.0000e+00, 1.0000e-01, 1.0000e-01, 1.0000e-02, 1.0000e-02,
         1.0000e-03, 1.0000e-03],
        [2.0000e+00, 2.0000e+00, 2.0000e-01, 2.0000e-01, 2.0000e-02, 2.0000e-02,
         2.0000e-03, 2.0000e-03],
        [3.0000e+00, 3.0000e+00, 3.0000e-01, 3.0000e-01, 3.0000e-02, 3.0000e-02,
         3.0000e-03, 3.0000e-03]])
encodings=tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,
          1.0000e+00,  0.0000e+00,  1.0000e+00],
        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,
          9.9995e-01,  1.0000e-03,  1.0000e+00],
        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,
          9.9980e-01,  2.0000e-03,  1.0000e+00],
        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,
          9.9955e-01,  3.0000e-03,  1.0000e+00]])
encodings.norm(dim=1)=tensor([2., 2., 2., 2.])
</code></pre><p>Putting this together in a PyTorch module might look like:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>VOCAB_SIZE</span> <span>=</span> <span>128</span>
</span></span><span><span><span>EMBEDDING_DIM</span> <span>=</span> <span>64</span>
</span></span><span><span><span>CONTEXT_LENGTH</span> <span>=</span> <span>8</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>NetWithSinusoidalEmbedding</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
</span></span><span><span>        <span>self</span><span>.</span><span>emb</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>VOCAB_SIZE</span><span>,</span> <span>EMBEDDING_DIM</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>register_buffer</span><span>(</span>
</span></span><span><span>            <span>&#34;pos&#34;</span><span>,</span> <span>make_positional_embedding</span><span>(</span><span>CONTEXT_LENGTH</span><span>,</span> <span>EMBEDDING_DIM</span><span>)</span>
</span></span><span><span>        <span>)</span>
</span></span><span><span>        <span># ... other layers</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>emb</span><span>(</span><span>x</span><span>)</span>
</span></span><span><span>        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>pos</span>
</span></span><span><span>        <span># ... rest of forward pass</span>
</span></span></code></pre></div><h4 id="just-let-the-network-learn-it">Just let the network learn it</h4>
<p>When I first read the above paper, this alternative approach – to just learn an embedding – seemed preferable to me, but I appreciate now the touch of class the sinusoidal approach brings to the table.</p>
<p>The idea is to equip the network with an additional embedding and let it figure out how to use it to distinguish between positions. It’s less code, at the cost of some additional parameters to train. In PyTorch, it might look like this:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>torch</span>
</span></span><span><span><span>import</span> <span>torch.nn</span> <span>as</span> <span>nn</span>
</span></span><span><span>
</span></span><span><span><span>VOCAB_SIZE</span> <span>=</span> <span>128</span>
</span></span><span><span><span>EMBEDDING_DIM</span> <span>=</span> <span>64</span>
</span></span><span><span><span>CONTEXT_LENGTH</span> <span>=</span> <span>8</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>NetWithLearnedPositionalEncoding</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
</span></span><span><span>        <span>self</span><span>.</span><span>emb</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>VOCAB_SIZE</span><span>,</span> <span>EMBEDDING_DIM</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>pos</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>CONTEXT_LENGTH</span><span>,</span> <span>EMBEDDING_DIM</span><span>)</span>
</span></span><span><span>        <span># ... other layers</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>emb</span><span>(</span><span>x</span><span>)</span>
</span></span><span><span>        <span>p</span> <span>=</span> <span>self</span><span>.</span><span>pos</span><span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>CONTEXT_LENGTH</span><span>))</span>
</span></span><span><span>        <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>p</span>
</span></span><span><span>        <span># ... rest of forward pass</span>
</span></span></code></pre></div><p>The paper claims that in practice, both approaches yielded more or less identical results, so both approaches appear to be about as effective as each other.</p>
<h3 id="alternate-approaches">Alternate approaches</h3>
<p>Other approaches to positional encodings exist and have been explored since the transformer architecture exploded in popularity. My fellow RC participant <a href="https://swe-to-mle.pages.dev/">Régis</a> ran a few sessions to explore these further. We looked at <a href="https://arxiv.org/abs/2104.09864">RoPE</a>, which replaces positional encoding with a sequence of rotations, and <a href="https://arxiv.org/abs/2108.12409">ALiBi</a>, which substitutes positional encoding altogether with a modified query-key attention score process, which penalises attention scores between items that are far apart.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Putting these thoughts into words has made me realise there’s a good chance I’m missing some subtleties around positional embeddings, and indeed embeddings in general. An old colleague of mine has recommended <a href="https://sites.google.com/view/embeddings-in-nlp">Embeddings in Natural Language Processing</a> – maybe now’s a good time to pick it up.</p>
<h3 id="further-reading">Further reading</h3>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></li>
<li><a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a></li>
<li><a href="https://sites.google.com/view/embeddings-in-nlp">Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning</a></li>
</ul>




            </div></div>
  </body>
</html>
