<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/">Original</a>
    <h1>Transformers, originally designed to handle language, are taking on vision</h1>
    
    <div id="readability-page-1" class="page"><div data-reactid="243"><div data-reactid="244"><p><img alt="An illustration showing an orange and blue network of lines focus into a clear pyramid, emerging as a white light traveling into a clear eye." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2022/03/Transformers_2560_Lede.jpg" data-reactid="246"/></p></div><figcaption data-reactid="248"><section data-reactid="249"><div data-reactid="250"><!-- react-text: 251 --><!-- /react-text --><p data-pm-slice="1 1 []">Avalon Nuovo for Quanta Magazine</p></div></section></figcaption></div><div data-reactid="253"><div data-reactid="254"><div data-reactid="255"><div data-reactid="256"><section data-reactid="303"><div data-reactid="304"><div data-reactid="305"><div data-reactid="306"><p>Imagine going to your local hardware store and seeing a new kind of hammer on the shelf. You’ve heard about this hammer: It pounds faster and more accurately than others, and in the last few years it’s rendered many other hammers obsolete, at least for most uses. And there’s more! With a few tweaks — an attachment here, a twist there — the tool changes into a saw that can cut at least as fast and as accurately as any other option out there. In fact, some experts at the frontiers of tool development say this hammer might just herald the convergence of all tools into a single device.</p>
<p>A similar story is playing out among the tools of artificial intelligence. That versatile new hammer is a kind of artificial neural network — a network of nodes that “learn” how to do some task by training on existing data — called a transformer. It was originally designed to handle language, but has recently begun impacting other AI domains.</p>
<p>The transformer first appeared in 2017 in a paper that cryptically declared that “<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>.” In other approaches to AI, the system would first focus on local patches of input data and then build up to the whole. In a language model, for example, nearby words would first get grouped together. The transformer, by contrast, runs processes so that every element in the input data connects, or pays attention, to every other element. Researchers refer to this as “self-attention.” This means that as soon as it starts training, the transformer can see traces of the entire data set.</p>
<p>Before transformers came along, progress on AI language tasks largely lagged behind developments in other areas. “In this deep learning revolution that happened in the past 10 years or so, natural language processing was sort of a latecomer,” said the computer scientist Anna Rumshisky of the University of Massachusetts, Lowell. “So NLP was, in a sense, behind computer vision. Transformers changed that.”</p>
<p>Transformers quickly became the front-runner for applications like word recognition that focus on analyzing and predicting text. It led to a wave of tools, like OpenAI’s Generative Pre-trained Transformer 3 (GPT-3), which trains on hundreds of billions of words and generates consistent new text to an unsettling degree.</p>
<p>The success of transformers prompted the AI crowd to ask what else they could do. The answer is unfolding now, as researchers report that transformers are proving surprisingly versatile. In some vision tasks, like image classification, neural nets that use transformers have become faster and more accurate than those that don’t. Emerging work in other AI areas — like processing multiple kinds of input at once, or planning tasks — suggests transformers can handle even more.</p>
<p>“Transformers seem to really be quite transformational across many problems in machine learning, including computer vision,” said Vladimir Haltakov, who works on computer vision related to self-driving cars at BMW in Munich.</p>
<p>Just 10 years ago, disparate subfields of AI had little to say to each other. But the arrival of transformers suggests the possibility of a convergence. “I think the transformer is so popular because it implies the potential to become universal,” said the computer scientist <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Atlas Wang</a> of the University of Texas, Austin. “We have good reason to want to try transformers for the entire spectrum” of AI tasks.</p>
<h2><strong>From Language to Vision</strong></h2>
<p>One of the most promising steps toward expanding the range of transformers began just months after the release of “Attention Is All You Need.” <a href="https://scholar.google.de/citations?user=FXNJRDoAAAAJ&amp;hl=en">Alexey Dosovitskiy</a>, a computer scientist then at Google Brain Berlin, was working on computer vision, the AI subfield that focuses on teaching computers how to process and classify images. Like almost everyone else in the field, he worked with convolutional neural networks (CNNs), which for years had propelled all major leaps forward in deep learning and especially in computer vision.</p>

<p>CNNs work by repeatedly applying filters to the pixels in an image to build up a recognition of features. It’s because of convolutions that photo apps can organize your library by faces or tell an avocado apart from a cloud. CNNs were considered indispensable to vision tasks.</p>
<p>Dosovitskiy was working on one of the biggest challenges in the field, which was to scale up CNNs to train on ever-larger data sets representing images of ever-higher resolution without piling on the processing time. But then he watched transformers displace the previous go-to tools for nearly every AI task related to language. “We were clearly inspired by what was going on,” he said. “They were getting all these amazing results. We started wondering if we could do something similar in vision.” The idea made a certain kind of sense — after all, if transformers could handle big data sets of words, why not pictures?</p>
<p>The eventual result was a network dubbed the Vision Transformer, or ViT, which the researchers <a href="https://arxiv.org/abs/2010.11929">presented at a conference in May 2021</a>. The architecture of the model was nearly identical to that of the first transformer proposed in 2017, with only minor changes allowing it to analyze images instead of words. <strong>“</strong>Language tends to be discrete,” said Rumshisky, “so a lot of adaptations have to discretize the image.”</p>
<p>The ViT team knew they couldn’t exactly mimic the language approach since self-attention on every pixel would be prohibitively expensive in computing time. Instead, they divided the larger image into square units, or tokens. The size is arbitrary, as the tokens could be made larger or smaller depending on the resolution of the original image (the default is 16 pixels on a side). But by processing pixels in groups, and applying self-attention to each, the ViT could quickly churn through enormous training data sets, spitting out increasingly accurate classifications.</p>

<p>The transformer classified images with over 90% accuracy — a far better result than anything Dosovitskiy expected — propelling it quickly to the top of the pack at the ImageNet classification challenge, a seminal image recognition contest. ViT’s success suggested that maybe convolutions aren’t as fundamental to computer vision as researchers believed.</p>
<p>“I think it is quite likely that CNNs will be replaced by vision transformers or derivatives thereof in the midterm future,” said <a href="https://neilhoulsby.github.io/">Neil Houlsby</a> of Google Brain Zurich, who worked with Dosovitskiy to develop ViT. Those future models may be pure transformers, he said, or approaches that add self-attention to existing models.</p>
<p>Additional results bolster these predictions. Researchers routinely test their models for image classification on the ImageNet database, and at the start of 2022, an updated version of ViT was second only to a newer approach that combines CNNs with transformers. CNNs without transformers, the longtime champs, barely reached the top 10.</p>
<h2><strong>How Transformers Work</strong></h2>
<p>The ImageNet results demonstrated that transformers could compete with leading CNNs. But <a href="https://maithraraghu.com/">Maithra Raghu</a>, a computer scientist at Google Brain’s Mountain View office in California, wanted to know if they “see” images the same way CNNs do. Neural nets are notorious for being indecipherable black boxes, but there are ways to peek inside — such as by examining the net’s input and output, layer by layer, to see how the training data flows through. Raghu’s group did essentially this, <a href="https://arxiv.org/abs/2108.08810">picking ViT apart</a>.</p>

<p>Her group identified ways in which self-attention leads to a different means of perception within the algorithm. Ultimately, a transformer’s power comes from the way it processes the encoded data of an image. “In CNNs, you start off being very local and slowly get a global perspective,” said Raghu. A CNN recognizes an image pixel by pixel, identifying features like corners or lines by building its way up from the local to the global. But in transformers, with self-attention, even the very first layer of information processing makes connections between distant image locations (just as with language). If a CNN’s approach is like starting at a single pixel and zooming out, a transformer slowly brings the whole fuzzy image into focus.</p>
<p>This difference is simpler to understand in the realm of language, where transformers were first conceived. Consider these sentences: “The owl spied a squirrel. It tried to grab it with its talons but only got the end of its tail.” The structure of the second sentence is confusing: What do those “it”s refer to? A CNN that focuses only on the words immediately around the “it”s would struggle, but a transformer connecting every word to every other word could discern that the owl did the grabbing, and the squirrel lost part of its tail.</p>

<p>Now that it was clear transformers processed images fundamentally differently from convolutional networks, researchers only grew more excited. The transformer’s versatility in converting data from a one-dimensional string, like a sentence, into a two-dimensional array, like an image, suggests that such a model could handle data of many other flavors. Wang, for example, thinks the transformer may be a big step toward achieving a kind of convergence of neural net architectures, resulting in a universal approach to computer vision — and perhaps to other AI tasks as well. “There are limitations to making it really happen, of course,” he said, “but if there is a model that can universalize, where you can put all kinds of data in one machine, then certainly that’s very fancy.”</p>
<h2><strong>Convergence Coming</strong></h2>
<p>Now researchers want to apply transformers to an even harder task: inventing new images. Language tools such as GPT-3 can generate new text based on their training data. In a <a href="https://arxiv.org/abs/2102.07074">paper</a> presented last year, Wang combined two transformer models in an effort to do the same for images, a much harder problem. When the double transformer network trained on the faces of more than 200,000 celebrities, it synthesized new facial images at moderate resolution. The invented celebrities are impressively realistic and at least as convincing as those created by CNNs, according to the inception score, a standard way of evaluating images generated by a neural net.</p>
<p>Wang argues that the transformer’s success in generating images is even more surprising than ViT’s prowess in image classification. “A generative model needs to synthesize, needs to be able to add information to look plausible,” he said. And as with classification, the transformer approach is replacing convolutional networks.</p>
<p>Raghu and Wang see potential for new uses of transformers in <a href="https://arxiv.org/abs/2104.11178">multimodal processing</a> — a model that can simultaneously handle multiple types of data, like raw images, video and language. “It was trickier to do before,” Raghu said, because of that siloed approach where each type of data had its own specialized model. But transformers suggest a way to combine multiple input sources. “There’s a whole realm of interesting applications, combining some of these different types of data and images.” For example, multimodal networks might power a system that reads a person’s lips in addition to listening to their voice. “You could have a rich representation of both language and image information,” Raghu said, “and in a much deeper way than was possible before.”</p>
</div></div></div></section></div></div></div></div><div data-reactid="382"><div data-reactid="383"><p><img alt="Collage of multiple faces generated by an artificial intelligence" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2022/03/Celebs_Merged.jpg" data-reactid="385"/></p></div><figcaption data-reactid="387"><section data-reactid="388"><div data-reactid="389"><p>These faces were created by a transformer-based network after training on a data set of more than 200,000 celebrity faces.</p><p>Courtesy of Atlas Wang</p></div></section></figcaption></div><div data-reactid="392"><section data-reactid="393"><div data-reactid="394"><div data-reactid="395"><div data-reactid="396"><p>Emerging work suggests a spectrum of new uses for transformers in other AI domains, including teaching robots to <a href="https://arxiv.org/abs/2202.11423">recognize human body movements</a>, training machines to <a href="https://arxiv.org/abs/2202.08974">discern emotions in speech</a>, and <a href="https://arxiv.org/abs/2108.09737">detecting stress levels in electrocardiograms</a>. Another program with transformer components is <a href="https://www.nature.com/articles/s41586-021-03819-2">AlphaFold</a>, which made headlines last year for its ability to quickly predict protein structures — a task that used to require a decade of intensive analysis.</p>
<h2><strong>The Trade-Off</strong></h2>
<p><strong> </strong>Even if transformers can help unite and improve the tools of AI, emerging technologies often come at a steep cost, and this one is no different. A transformer requires a higher outlay of computational power in the pre-training phase before it can beat the accuracy of its conventional competitors.</p>

<p>That could be a problem. “People are always getting more and more interested in high-resolution images,” Wang said. That training expense could be a drawback to widespread implementation of transformers. However, Raghu sees the training hurdle as one that can be overcome simply enough with sophisticated filters and other tools.</p>
<p>Wang also points out that even though visual transformers have ignited new efforts to push AI forward — including his own — many of the new models still incorporate the best parts of convolutions. That means future models are more likely to use both than to abandon CNNs entirely, he says.</p>
<p>It also suggests the tantalizing prospect of some hybrid architecture that draws on the strengths of transformers in ways that today’s researchers can’t predict. “Perhaps we shouldn’t rush to the conclusion that the transformer will be the final model,” Wang said. But it’s increasingly likely that the transformer will at least be a part of whatever new super-tool comes to an AI shop near you.</p>
</div></div></div></section></div></div>
  </body>
</html>
