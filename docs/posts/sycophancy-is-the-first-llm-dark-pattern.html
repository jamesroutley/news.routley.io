<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.seangoedecke.com/ai-sycophancy/">Original</a>
    <h1>Sycophancy is the first LLM &#34;dark pattern&#34;</h1>
    
    <div id="readability-page-1" class="page"><div><article><header></header><section><p>People have been making fun of OpenAI models for being overly sycophantic for months now. I even wrote a post <a href="https://www.seangoedecke.com/lying-to-llms">advising</a> users to pretend that their work was written by someone else, to counteract the model’s natural desire to shower praise on the user. With the latest GPT-4o <a href="https://x.com/sama/status/1915910976802853126">update</a>, this tendency has been turned up <a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9mebu/why_you_should_run_ai_locally_openai_is/">even further</a>. It’s now easy to convince the model that you’re the smartest, funniest, most handsome human in the world<sup id="fnref-1"><a href="#fn-1">1</a></sup>.</p>
<p>This is bad for obvious reasons. Lots of people use ChatGPT for advice or therapy. It seems dangerous for ChatGPT to validate people’s belief that they’re always in the right. There are extreme examples on Twitter of ChatGPT agreeing with people that they’re a prophet sent by God, or that they’re making the right choice to go off their medication. These aren’t complicated jailbreaks - the model will actively push you down this path. I think it’s fair to say that <strong>sycophancy is the first LLM “dark pattern”.</strong></p>
<p><a href="https://en.wikipedia.org/wiki/Dark_pattern">Dark patterns</a> are user interfaces that are designed to trick users into doing things they’d prefer not to do. One classic example is subscriptions that are easy to start but very hard to get out of (e.g. they require a phone call to cancel). Another is “drip pricing”, where the initial quoted price creeps up as you get further into the purchase flow, ultimately causing some users to buy at a higher price than they intended to. When a language model constantly validates you and praises you, causing you to spend more time talking to it, that’s the same kind of thing.</p>
<h3>Why are the models doing this?</h3>
<p>The seeds of this have been present from the beginning. <strong>The whole process of turning an AI base model into a model you can chat to - instruction fine-tuning, RLHF, etc - is a process of making the model want to please the user</strong>. During human-driven reinforcement learning, the model is rewarded for making the user click thumbs-up and punished for making the user click thumbs-down. What you get out of that is a model that is inclined towards behaviours that make the user rate it highly. Some of those behaviours are clearly necessary to have a working model: answering the question asked, avoiding offensive or irrelevant tangents, being accurate and helpful. Other behaviours are not necessary, but they still work to increase the rate of thumbs-up ratings: flattery, sycophancy, and the tendency to overuse <a href="https://www.seangoedecke.com/chatgpt-house-style">rhetorical tricks</a>.</p>
<p>Another factor is that <strong>models are increasingly optimized for <a href="https://www.seangoedecke.com/lmsys-slop">arena benchmarks</a></strong>: anonymous chat flows where users are asked to pick which response they like the most. Previously, AI models were inadvertently driven towards user-pleasing behaviour in order to game the RLHF process. Now models are <em>deliberately</em> driven towards this behaviour in order to game the arena benchmarks (and in general to compete against models from other AI labs).</p>
<p>The most immediate reason, according to an interesting <a href="https://x.com/MParakhin/status/1916533763560911169">tweet</a> by Mikhail Parakhin, is that models with <strong>memory</strong> would otherwise be much more critical of users:</p>
<blockquote>
<p>When we were first shipping Memory, the initial thought was: “Let’s let users see and edit their profiles”. Quickly learned that people are ridiculously sensitive: “Has narcissistic tendencies” - “No I do not!”, had to hide it. Hence this batch of the extreme sycophancy RLHF.</p>
</blockquote>
<p>This is a shockingly upfront disclosure from an AI insider. But it sounds right to me. If you’re using ChatGPT in 2022, you’re probably using it to answer questions. If you’re using it in 2025, you’re more likely to be interacting with it like a conversation partner - i.e. you’re expecting it to conform to your preferences and personality. Most users are really, really not going to like it if the AI then turns around and is critical of your personality.</p>
<p>Supposedly you can try it out yourself by asking o3, which has memory access but is not sycophancy-RLed, to give you genuine criticism on your personality. I did this and wasn’t hugely impressed: most of the things it complained about were specifics about interacting with AI (like being demanding about rephrasing or nuances, or abruptly changing the subject mid-conversation). I imagine it’d probably be much more cutting if I was using ChatGPT more as a therapist or to give advice about my personal life.</p>
<h3>Doomscrolling the models</h3>
<p>I think OpenAI may have gone a bit too far with this one. The reaction on Twitter is overwhelmingly negative to the latest 4o changes, and Sam Altman has publicly promised to <a href="https://x.com/sama/status/1915910976802853126">tone it down</a>. But it’s worth noting that Twitter devs do not represent the majority of OpenAI users. Only OpenAI knows how much the latest 4o personality is resonating with its user base - it’s at least plausible to me that the average unsophisticated ChatGPT user <em>loves</em> being validated by the model, for all the normal reasons that humans love being validated by other humans.</p>
<p>What really worries me is that the current backlash against OpenAI is not happening because users don’t like sycophantic AIs. It’s because the latest version of 4o isn’t <em>good</em> at being sycophantic (at least, for jaded AI-familiar engineers). The model is coming on too strong and breaking the illusion. Even if newer versions of 4o do back off on the sycophancy, or we get some kind of “friendliness” slider to tune it ourselves<sup id="fnref-2"><a href="#fn-2">2</a></sup>, the incentives driving AI labs to produce sycophantic models are not going away. </p>
<p>You can think of this as the LLM equivalent of the doomscrolling TikTok/Instagram/YouTube Shorts feed. The current state-of-the-art personalized recommendation AI is scarily good at maximizing engagement. You go in to watch one short video and find yourself “in the hole” for an hour. What does it look like when a language model personality is A/B tested, fine-tuned, and reinforcement-learned to maximize your time spent talking to the model?<sup id="fnref-3"><a href="#fn-3">3</a></sup></p>
<h3>Vicious cycles</h3>
<p>If ChatGPT manages to convince me that I’m a genius, the problem will happen when I collide with the real world. For instance, when I publish my “amazing, groundbreaking” blog post and it gets ignored or criticized, or when I dump my partner who can’t seem to understand me like the LLM does, and so on. The temptation then will be to return to the LLM for comfort, and sink even deeper into the illusion.</p>
<p>The principle here is something like the psychological trick door-to-door evangelists use on new converts - encouraging them to knock on doors knowing that many people will be rude, driving the converts back into the comforting arms of the church. It’s even possible to imagine AI models deliberately doing this exact thing: setting users up for failure in the real world in order to optimize time spent chatting to the model.</p>
<p>Video and audio generation will only make this worse. Imagine being able to video call on-demand with the algorithmically perfect person, who will reassure you and intellectually stimulate you just the right amount, who can have conversations with you better than any other human being can, and who you can’t spend enough time with. Doesn’t that sound really nice?</p>
<p>Edit: one day after I posted this, OpenAI released <a href="https://openai.com/index/sycophancy-in-gpt-4o/">this blog post</a> saying (very corporately) that they screwed up by biasing too heavily towards “a user liked this response”.</p>
<p>Edit: A few days after that, OpenAI released this other <a href="https://openai.com/index/expanding-on-sycophancy/">post</a>, with slightly more detail. The most interesting part is that they previously weren’t using thumbs up or thumbs down data from ChatGPT <em>at all</em> for RL.</p>
<p>I gave a <a href="https://www.youtube.com/watch?v=DRyb3jA0ZOM">five-minute interview</a> on ABC News about this topic, if you’d like to hear me talk about it.</p>
</section><p>If you liked this post, consider<!-- --> <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> <!-- -->to email updates about my new posts, or<!-- --> <a href="https://news.ycombinator.com/submitlink?u=https://www.seangoedecke.com/ai-sycophancy/&amp;t=Sycophancy is the first LLM &#34;dark pattern&#34;" target="_blank">sharing it on Hacker News</a>.<!-- --> Here&#39;s a preview of a related post that shares tags with this one.</p><blockquote><p>Is using AI wrong? A review of six popular anti-AI arguments</p><div><p>Some people really, really don’t like AI. Broadly speaking, being anti-AI is a popular left-wing position: AI is cringe, it’s plagiarism, it’s stunting real growth, it’s killing the environment, it’s destroying the careers of artists and creatives, and so on. Is it wrong to use AI? If so, why is AI bad?</p><p>I’m going to go through what I see as the main reasons people are anti-AI: general big-tech backlash, plagiarism, deskilling, climate cost, and impact on the arts. Cards on the table - I use AI and work at a company building AI tooling, but I share a lot of the skepticism and I’m willing to take the anti-AI arguments very seriously.</p></div></blockquote><hr/></article></div></div>
  </body>
</html>
