<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://boydkane.com/essays/boss">Original</a>
    <h1>Beliefs that are true for regular software but false when applied to AI</h1>
    
    <div id="readability-page-1" class="page"><article>
<p>(a note for technical folk)<sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> | <a href="http://boydkane.com/assets/boss.pdf">read as pdf<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> | <a href="https://beyarkay.substack.com/p/why-your-boss-isnt-worried-about">Substack<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> | <a href="https://www.lesswrong.com/posts/ZFsMtjsa6GjeE22zX/why-your-boss-isn-t-worried-about-ai">LessWrong<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p>When it comes to understanding the dangers of AI systems, the general public
has the worst kind of knowledge: that what you know for sure that just ain’t
so.</p>
<p>After 40 years of persistent badgering, the software industry has convinced the
public that bugs can have disastrous consequences. This is great! It is <em>good</em>
that people understand that software can result in real-world harm. Not only
does the general public mostly understand the dangers, but they mostly
understand that bugs can be fixed. It might be expensive, it might be
difficult, but it can be done.</p>
<p>The problem is that this understanding, <em>when applied to AIs like
ChatGPT</em>, is completely wrong. The software that runs AI acts very differently
to the software that runs most of your computer or your phone. Good, sensible
assumptions about bugs in regular software actually end up being harmful and
misleading when you try to apply them to AI.</p>
<p>Attempting to apply regular-software assumptions to AI systems leads to
confusion, and remarks such as:</p>
<blockquote>
<p>“If something goes wrong with ChatGPT, can’t some boffin just think hard for
a bit, find the missing semi-colon or whatever, and then fix the bug?”</p>
</blockquote>
<p>or</p>
<blockquote>
<p>“Even if it’s hard for one person to understand everything the AI does,
surely still smart people who individually understand small parts of what the
AI does?”.</p>
</blockquote>
<p>or</p>
<blockquote>
<p>“Just because current systems don’t work perfectly, that’s not a problem
right? Because eventually we’ll iron out all the bugs so the AIs will get
more reliable over time, like old software is more reliable than new
software.”</p>
</blockquote>
<p>If you understand how modern AI systems work, these statements are all
painfully incorrect. But if you’re used to regular software, they’re completely
reasonable. I believe there is a gap between the experts and the novices in the
field:</p>
<ul>
<li>the experts don’t see the gap because it’s so obvious, so they don’t bother
explaining the gap</li>
<li>the novices don’t see the gap because they don’t know to look, so they don’t
realise where their confusion comes from.</li>
</ul>
<p>This leads to frustration on both sides, because the experts feel like their
arguments aren’t hitting home, and the novices feel like all arguments have
obvious flaws. In reality, the experts and the novices have different,
unspoken, assumptions about how AI systems work.</p>

<p>To make this more concrete, here are some example ideas that are perfectly true
when applied to regular software but become harmfully false when applied to
modern AIs:</p>
<h2 id="software-vulnerabilities-are-caused-by-mistakes-in-the-code">Software vulnerabilities are caused by mistakes in the code<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#software-vulnerabilities-are-caused-by-mistakes-in-the-code"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>In regular software, vulnerabilities are caused by mistakes in the lines of
code that make up the software. There might be hundreds of thousands of lines
of code, but code doesn’t take up much space so this is only around 50MB of
data, about the size of a small album of photos.</p>
<p>But in modern AI systems, vulnerabilities or bugs are usually caused by
problems in the data used to train an AI<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>. It takes thousands of gigabytes of
data to train modern AI systems, and bad behaviour isn’t caused by any single
bad piece of data, but by the combined effects of significant fractions of the
dataset. Because these datasets are so large, <em>nobody knows everything that an
AI is actually trained on</em>. One popular dataset, <a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, is about 11.25
trillion words long<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>, which, if you were reading at about 250 words per
minute, would take you over 85 thousand years to read. It’s just not possible
for any single human (or even a team of humans) to have read everything that an
LLM has read during training.</p>
<h2 id="bugs-in-the-code-can-be-found-by-carefully-analysing-the-code">Bugs in the code can be found by carefully analysing the code<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#bugs-in-the-code-can-be-found-by-carefully-analysing-the-code"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>With regular software, if there’s a bug, it’s possible for smart people to
carefully read through the code and logically figure out what must be causing
the bug.</p>
<p>With AI systems, almost all bad behaviour originates from the data that’s used
to train them<sup><a href="#user-content-fn-3" id="user-content-fnref-3-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>, but it’s basically impossible to look at misbehaving AI and
figure out parts of the training data caused that bad behaviour. In practice,
it’s rare to even attempt this, researchers will retrain the AI with more data
to try and counteract the bad behaviour, or they’ll start over and try to
curate the data to not include the bad data.</p>
<p>You cannot logically deduce what pieces of data caused the bad behaviour, you
can only make good guesses. For example, modern AIs are trained on lots of
mathematics proofs and programming tasks, because that seems to make them do
better at reasoning and logical thinking tasks. If an AI system makes a logical
reasoning mistake, it’s impossible to attribute that mistake to any portion of
the training data, the only answer we’ve got is to use more data next time.</p>
<p>I think I need to emphasise this: With regular software, we can pinpoint
mistakes precisely, walk step-by-step through the events leading up to the
mistake, and logically understand why that mistake happened. When AIs make
mistakes, <em>we don’t understand the steps that caused those mistakes</em>.
Even the people who made the AIs don’t understand why they make mistakes<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup>.
Nobody understands where these bugs come from. We sometimes kinda have a rough
idea about why they maybe did something unusual. But we’re far, far away from
anything that guarantees the AI won’t have any catastrophic failures.</p>
<h2 id="once-a-bug-is-fixed-it-wont-come-back-again">Once a bug is fixed, it won’t come back again<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#once-a-bug-is-fixed-it-wont-come-back-again"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>With regular software, once you’ve found the bug, you can fix the bug. And once
you’ve fixed the bug, it won’t re-appear<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>. There might be a bug that causes
similar problems, but it’s not the <em>same</em> bug as the one you fixed. This means
you can, if you’re patient, reduce the number of bugs over time and rest
assured that removing new bugs won’t cause old bugs to re-appear.</p>
<p>This is not the case with AI. It’s not really possible to “fix” a bug in an AI,
because even if the AI was behaving weirdly, and you retrained it, and now it’s
not behaving weirdly anymore, you can’t know for sure that the weird behaviour
is <em>gone</em>, just that it doesn’t happen for the prompts you tested. It’s
entirely possible that someone can find a prompt you forgot to test, and then
the buggy behaviour is back again!</p>
<h2 id="every-time-you-run-the-code-the-same-thing-happens">Every time you run the code, the same thing happens<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#every-time-you-run-the-code-the-same-thing-happens"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>With regular software, you can run the same piece of code multiple times and
it’ll behave in the same way. If you give it the same input, it’ll give you the
same output.</p>
<p>Now <em>technically</em> this is still true for AIs, if you give them exactly the
prompt they’ll respond in exactly the same way. But practically, it’s very far
from the truth<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>. Even <em>tiny</em> changes to the input of an AI can have dramatic
changes in the output. Even innocent changes like adding a question mark at the end
of your sentence or forgetting to start your sentence with a capital letter can
cause the AI to return something different.</p>
<p>Additionally, most AI companies will slightly change the way their AIs respond,
so that they say slightly different things to the same prompt. This helps their
AIs seem less robotic and more natural.</p>
<h2 id="if-you-give-specifications-beforehand-you-can-get-software-that-meets-those-specifications">If you give specifications beforehand, you can get software that meets those specifications<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#if-you-give-specifications-beforehand-you-can-get-software-that-meets-those-specifications"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>With regular software, this is true. You can sit with stakeholders to discuss
the requirements for some piece of software, and then write code to meet those
requirements. The requirements might change, but fundamentally you can write
code to serve some specific purpose and have confidence that it will serve that
specific purpose.</p>
<p>With AI systems, this is more or less false. Or at the very least, the creators
of modern AI systems have far far less control about the behaviour the AIs will
exhibit. We understand how to get an AI to meet narrow, testable specifications
like speaking English and writing code, but we don’t know how to get a brand
new AI to achieve a certain score on some particular test or to guarantee
global behaviour like “never tells the user to commit a crime”. The best AI
companies in the world have basically one lever which is “better”, and they can
pull that lever to make the AI better, but nobody knows precisely what to do to
ensure an AI writes formal emails correctly or summarises text accurately.</p>
<p>This means that we don’t know what an AI will be capable of before we’ve
trained it. It’s very common for AIs to be released to the public for months
before a random person on Twitter discovers some ability that the AI has
which even its creators didn’t know about. So far, these abilities have been
mostly just fun, like being good at <a href="https://x.com/swax/status/1912728143682760934">Geoguessr<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>:</p>
<p><img src="https://boydkane.com/assets/boss-geoguessr.png" alt="Geoguessr map"/></p>
<p>Or making photos look like they were from a <a href="https://x.com/GrantSlatton/status/1904631016356274286">Studio Ghibli film<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>:</p>
<p><img src="https://boydkane.com/assets/boss-ghibli.png" alt="Ghibli tweet"/></p>
<p>But there’s no reason for these hidden abilities to always be positive. It’s
entirely possible that some dangerous capability is hidden in ChatGPT, but
nobody’s figured out the right prompt just yet.</p>
<p>While it’s possible to demonstrate the safety of an AI for a specific test
suite or a known threat, it’s impossible for AI creators to definitively say
their AI will never act maliciously or dangerously for any prompt it could be
given.</p>

<p>It is <em>good</em> that most people know the dangers of poorly written or buggy
software. But this hard-won knowledge about regular software is misleading the
public when it gets applied to AI. Despite the cries of “inscrutable arrays of
floating point numbers”, I’d be surprised if a majority of people know that
modern AI is architecturally different from regular software.</p>
<p>AI safety is a complicated and subtle argument. The best we can do is to make
sure we’re starting from the same baseline, and that means conveying to our
contemporaries that if it all starts to go wrong, we cannot just “patch the
bug”<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup>.</p>
<p>If this essay was the first time you realised AI was fundamentally different
from regular software, let me know, and share this with a friend who might also
not realise the difference.</p>
<p>If you always knew that regular software and AIs are fundamentally different,
talk to your family and non-technical friends, or with a stranger at a coffee
shop. I think you’ll be surprised at how few people know that these two are
different.</p>
<hr/>
<p>If you’re interested the dynamics between experts and novices, and how gaps
between them arise, I’ve written more about the systemic biases encountered by
experts (and the difficulties endured by novices) in this essay: <a href="https://boydkane.com/essays/experts" data-slug="essays/experts">Experts have
it easy</a>.</p>
<p>Thanks to Sam Cross and Caleb for reviewing drafts of this essay.</p>
<p>Discuss this essay:</p>
<ul>
<li><a href="https://news.ycombinator.com/item?id=45583180">HackerNews<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (284 points, 218 comments)</li>
<li><a href="https://www.reddit.com/r/programming/comments/1o6n17u/why_your_boss_isnt_worried_about_ai_cant_you_just/">r/programming<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (55 points, 7 comments)</li>
<li><a href="https://www.reddit.com/r/slatestarcodex/comments/1o6n5ne/why_your_boss_isnt_worried_about_ai_cant_you_just/">/slatestarcodex<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (17 points, 19 comments)</li>
<li><a href="https://lobste.rs/s/roygg3/why_your_boss_isn_t_worried_about_ai_can_t">Lobse.rs<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (12 points, 4 comments)</li>
<li><a href="https://www.lesswrong.com/posts/ZFsMtjsa6GjeE22zX/why-your-boss-isn-t-worried-about-ai">LessWrong<svg viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (9 points, 2 comments)</li>
</ul>
</article></div>
  </body>
</html>
