<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://flower.dev/blog/2023-11-15-federated-finetuning-of-openai-whisper-with-flower/">Original</a>
    <h1>Federated finetuning of Whisper on Raspberry Pi 5</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p>Federated Learning has come a long way since it was formalised by <a href="https://arxiv.org/abs/1602.05629">McMahan et al. 2017</a>. Gone are the days where it was reduced to MNIST-level training or equivalent toy examples with small ML models. This blogpost introduces a code example that takes <a href="https://openai.com/research/whisper">Open AIâ€™s Whisper</a>, a state-of-the-art ASR model, and finetunes it for the downstream task of keyword spotting. You will learn how to perform this downstream in a federated manner. You can find the complete example <a href="https://github.com/adap/flower/tree/main/examples/whisper-federated-finetuning">on GitHub</a>.</p>
<h5>Federating Whisper for the downstream task of keyword spotting</h5>
<p>Federated Learning can leverage large models trained on publicly available data and downstream them using sensible/private data without having to copy the data to a central server. Flower takes the training to the data source, a critical first step towards ensuring client privacy.</p>
<p><img src="https://flower.dev/static/images/blog/content/keyword_spotting_overview.png" width="1024"/></p><p>This example walks you through the process of designing a Federated Learning pipeline with Flower for keyword spotting classification. Weâ€™ll use a <a href="https://huggingface.co/openai/whisper-tiny">pre-trained Whisper encoder</a> from ðŸ¤— Transformers, freeze its parameters, and federate the learning of a classification head to classify 1-second audio waveforms into one of twelve possible classes: &#39;yes&#39;, &#39;no&#39;, &#39;up&#39;, &#39;down&#39;, &#39;left&#39;, &#39;right&#39;, &#39;on&#39;, &#39;off&#39;, &#39;stop&#39;, &#39;go&#39;, a <em>silence</em>, or an <em>unknown</em> word. For this example, we will use the <a href="https://arxiv.org/abs/1804.03209">Google SpeechCommands</a> dataset.</p>
<p><img src="https://flower.dev/static/images/blog/content/federated_finetuning_flower_pipeline.png" width="1024"/></p><p>An overview of the FL pipeline implemented with Flower for this example is shown in the diagram above. It has four distinct stages:</p>
<ol>
<li>At the beginning of a round, the server samples some clients and sends them the classification head (i.e. the part of the model being federated).</li>
<li>Each client, with a frozen pre-trained Whisper encoder, trains the classification head using its own data.</li>
<li>Once on-site training is completed, each client communicates the updated classification head back to the server.</li>
<li>The server aggregates the classification heads and obtains a new <em>global</em> classification head that will be communicated to clients in the next round.</li>
</ol>
<h4>Running the example</h4>
<p>The example available <a href="https://github.com/adap/flower/tree/main/examples/whisper-federated-finetuning">on GitHub</a> splits the 2112 speakers in the SpeechCommands dataset into 100 groups. Each group can be seen as an office with 21 workers. This splitting creates 100 non-iid <em>offices</em>, each having different amounts of training data. We treat each of these offices as a FL client. The FL training uniformly samples 10 clients each round and uses <span>FedAvg</span> for aggregation. Within just a few rounds, the keyword spotting model can classify unseen keywords with an accuracy of over 97%. Recall that only the classification head (which has less than 0.8 M parameters) is being trained.</p>
<p><img src="https://flower.dev/static/images/blog/content/whisper_flower_acc.png" width="800"/></p><h4>Running on Raspberry Pi</h4>
<p>We used this example to also benchmark the new Raspberry Pi 5. It exhibits vastly superior performance across tasks compared to the previous Raspberry Pi 4, making it suitable for demanding on-device training workloads like the one in this example.</p>
<p>We benchmarked not only training times but also the time taken to pre-process the dataset partitions. A summary of the results are shown below. With a more detailed discussion in code <a href="https://github.com/adap/flower/tree/main/examples/whisper-federated-finetuning">example on GitHub</a>. Times are shown in minutes:seconds.</p>
<table><tbody><tr><th>Stage</th><th>Notes</th><th>RPi 4</th><th>RPi 5</th></tr><tr><td>Filter training set (~85k rows)</td><td>doing <span>.filter()</span> in <span>client.client_fn</span></td><td>1:58</td><td>0:37</td></tr><tr><td>Encode 845 rows with <span>WhisperProcessor</span></td><td>doing <span>.map()</span> passing <span>utils.prepare_dataset()</span></td><td>1:55</td><td>1:06</td></tr><tr><td>On-device training for 1 epoch (925 examples)</td><td>finetuning <span>classification</span> head with frozen <span>Whisper</span> encoder</td><td>39:45</td><td>20:06</td></tr></tbody></table></div></div></div></div>
  </body>
</html>
