<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.bitsand.cloud/posts/slashing-data-transfer-costs/">Original</a>
    <h1>Slashing Data Transfer Costs in AWS by 99%</h1>
    
    <div id="readability-page-1" class="page"><div>
        
        <p>There are lots of ways to accidentally spend too much money on AWS, and one of the easiest ways is by carelessly transferring data. As of writing, AWS charges the following rates for data transfer:</p>
<ul>
<li>
<p>Data transfer from AWS to the public Internet ranges from $0.09/GB in <code>us-east-1</code> (N. Virginia) to $0.154/GB in <code>af-south-1</code> (Cape Town). Therefore a single terabyte of data transfer will run you a cool $90 - $154.</p>
</li>
<li>
<p>Data transfer from one AWS region to another - e.g. if you were to transfer data from <code>us-east-1</code> to any other region - ranges from $0.02/GB in <code>us-east-1</code> to $0.147/GB in <code>af-south-1</code> (more than seven times as much!). Therefore a single terabyte of transferred data that <em>never leaves AWS’s network</em> will run you $20 - $147.</p>
</li>
</ul>
<p>In both of these transfers, you’re only paying for <em>egress</em> fees - i.e. you’re paying for data that leaves AWS regions, and not for incoming data. But now we’ll look at one of the trickier data transfer rates:</p>
<ul>
<li>Data transfer between availability zones <em>in the same AWS region</em> - e.g. from <code>us-east-1a</code> to <code>us-east-1b</code> - will cost you $0.01/GB in each direction. This pricing is the same in all regions. Therefore a single terabyte of transferred data between two same-region availability zones will cost you $10 for the egress and $10 for the ingress, for a total of $20.</li>
</ul>
<p>These prices are similar between all major cloud providers. Data transfer prices easily add up, and are an extremely high-margin source of profit for cloud providers - so high-margin that Cloudflare has introduced its competing R2 storage whose primary competitive distinction is a zero egress fee, along with publishing some <a href="https://blog.cloudflare.com/aws-egregious-egress/" target="_blank" rel="noopener">fairly strong rhetoric</a> lambasting AWS for its egress charges (conveniently failing to point out that Cloudflare’s whole business model is uniquely poised to competitively offer zero egress fees in a way that AWS is not).</p>
<div>
  <p><i aria-hidden="true"></i>Tip
  </p>
  <div><p>This is a good time to mention AWS PrivateLink and VPC endpoints. You might assume that if you set up an EC2 instance in <code>us-east-1</code> and transfer a terabyte from the instance to another region, you’ll pay the $20 cross-region data transfer charge. But by default, you might very well end up paying the $90 internet egress data transfer charge - e.g. if you transfer the data to a public S3 bucket in another region, you’ll effectively be transferring data over the internet.</p>
<p>AWS PrivateLink and VPC endpoints allow you to ensure that data between regions never leaves AWS’s network - useful not only in terms of pricing but also in terms of security. These features are not free, and come with their own limitations and pricing subtleties, but that’s beyond the scope of this blog post - AWS has a <a href="https://aws.amazon.com/blogs/networking-and-content-delivery/leveraging-aws-privatelink-for-volumetric-data-processing/" target="_blank" rel="noopener">couple of</a> <a href="https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/" target="_blank" rel="noopener">nice posts on the subject</a>, and <a href="https://www.vantage.sh/blog/nat-gateway-vpc-endpoint-savings#vpc-endpoint-pricing" target="_blank" rel="noopener">so does Vantage</a>.</p></div>
</div>

<p>It’s a common creed of AWS that setting resources up in multiple availability zones is best practice for ensuring reliability and availability. But this best practice easily opens up the door to funneling money into a furnace of cross-AZ costs - any application that involves sending data between resources in different availability zones will incur such charges.</p>
<p>Can we have our cake and eat it too? Can we set up cross-AZ infrastructure but avoid paying cross-AZ data transfer costs?</p>
<h2 id="sidestepping-data-transfer-costs-with-s3">
  Sidestepping Data Transfer Costs with S3
  <a href="#sidestepping-data-transfer-costs-with-s3">
    <i aria-hidden="true" title="Link to heading"></i>
    <span>Link to heading</span>
  </a>
</h2>
<p>S3 has an important characteristic - most storage classes in S3 store their buckets in <em>region</em> granularity rather than <em>availability zone</em> granularity. This means that you don’t upload data to a <code>us-east-1a</code> or a <code>us-east-1b</code> bucket, you upload data to a <code>us-east-1</code> bucket. Behind the scenes, AWS replicates this data in a minimum of three availability zones in the region - which is one of the reasons why S3 has such exceptionally high durability and availability.</p>
<div>
  <p><i aria-hidden="true"></i>Note
  </p>
  <p>There are two storage classes - S3 One Zone-Infrequent Access and the newly introduced S3 Express One Zone - that only store data in a single availability zone. You pay less for storage, but at a cost of availability - for instance, in <code>us-east-1</code>, S3 One Zone-Infrequent Access costs $0.01/GB as opposed to S3 Infrequent Access which costs $0.0125/GB, but it is designed for 99.5% availability as opposed to 99.99%.</p>
</div>

<p>This means that data in a standard S3 bucket is “equally” available to all AWS availability zones in its region - it doesn’t make a difference to S3 if you download data from <code>us-east-1a</code> or <code>us-east-1b</code>.</p>
<p>But wait - S3 has another important characteristic. For the standard storage class, downloading data from S3 is <em>free</em> - it only incurs standard data transfer charges if you’re downloading it between regions or to the public Internet. Moreover, uploading to S3 - in any storage class - is also free!</p>
<p>So let’s say I want to transfer 1TB between two EC2 instances in <code>us-east-1a</code> and <code>us-east-1b</code>. We saw above that if I transfer the data directly, it will cost me $20. But what if I upload the data to S3 from one instance and then download it from the other?</p>
<p>The upload will be free. The download will be free. The S3 storage will not be free, and in <code>us-east-1</code> costs $0.023/GB, or $23/TB, every month. This is charged on hour granularity, and we can design our upload/download such that no data persists in S3 for more than an hour. Let’s say there are 720 hours in a month, this means we’ll have to pay 1/720 of $23, or about $0.03. (We need to remember to delete the storage when we’re done!)</p>
<p>So instead of paying $20, this data transfer will cost only $0.03 - pretty cool! If we want to express these savings mathematically - assuming sub-hour data transfer rates, we’ve reduced our data transfer charges from $0.02/GB ($0.01 for egress and $0.01 for ingress) to  <strong>$0.000032/GB - just 0.15% (i.e. 15% of 1%) of the original charge</strong>. This gives us near-free cross-AZ data transfer costs. As an extreme example, transferring 1PB of data with this method will set you back about $32, as opposed to $20,000 with the standard way.</p>
<p>But wait, there’s more! S3 has another important characteristic - it is infinitely scalable. So this method makes it very convenient to replicate data from one AZ to as many instances as we want in another AZ - thousands of instances in the second AZ could download the same S3 object, and it should take up the same time as if just a single instance was downloading it. The S3 storage cost will remain constant, and the download cost will remain free. This is pretty cool too.</p>
<div>
  <p><i aria-hidden="true"></i>Warning
  </p>
  <p>S3 has another important characteristic - no single object can be more than 5TB. So if you’re using this method to transfer files bigger than 5TB, they need to be fragmented. Moreover, no single upload can be more than 5GB - you’ll need to use multi-part uploads if your files are bigger than this (<code>aws s3 cp</code> takes care of this automatically behind the scenes).</p>
</div>

<h2 id="demo">
  Demo
  <a href="#demo">
    <i aria-hidden="true" title="Link to heading"></i>
    <span>Link to heading</span>
  </a>
</h2>
<p>Let’s see this in action and be amazed. When I first thought of this method, the cost savings seemed too good to be true - even though all the fundamentals behind the method were solid, I didn’t believe it until I saw it in the Cost Explorer.</p>
<p>I want to start with clean AWS accounts so that there’s no noise when we’re examining the pricing. As such, I created two accounts for each part of the demo:</p>
<p><img src="https://www.bitsand.cloud/img/awsaccountsblurred.png" alt="AWS accounts"/></p>
<p>In each account, we will set up two EC2 instances - one in <code>us-east-1a</code> and another in <code>us-east-1b</code>. In each account, we’ll place both instances in a public subnet so we can easily SSH into them. And in each account, we’ll generate a random 1TB file in the <code>us-east-1a</code> instance, and our goal will be to transfer it to the <code>us-east-1b</code> instance.</p>
<p>We’ll run these two experiments:</p>
<ol>
<li>
<p>In the first experiment, we’ll place both instances in a VPC with private subnets in each of the two availability zones. We’ll set up a netcat server on the <code>us-east-1b</code> instance - on the interface connected to the private subnet. The <code>us-east-1a</code> instance will then transfer the 1TB file to the <code>us-east-1b</code> instance.</p>
</li>
<li>
<p>In the second experiment, we’ll place both instances in a VPC that has an <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html" target="_blank" rel="noopener">S3 Gateway endpoint</a>, we’ll create an S3 bucket, and the <code>us-east-1a</code> instance will upload the 1TB file to the bucket. Once this is done, the <code>us-east-1b</code> instance will download the 1TB file (and then delete it!).</p>
</li>
</ol>
<!-- raw HTML omitted -->
<p>In both experiments, we’ll have transferred 1TB from <code>us-east-1a</code> to <code>us-east-1b</code>. After this, we’ll wait for AWS’s Cost Explorer to update with the incurred costs to see that this method really works.</p>
<p>The experiments themselves are fairly straightforward, so they’re toggled away for brevity:</p>
<details>
    <summary>Standard Data Transfer Experiment</summary>
    <div><p>We’ll create a standard VPC, with both private and public subnets in <code>us-east-1a</code> and <code>us-east-1b</code>.</p>
<p>Note that the S3 Gateway endpoint is <em>deselected</em> - we’ll come back to this in the second experiment.</p>
<p><img src="https://www.bitsand.cloud/img/create-vpc-standard-data-transfer.png" alt="Standard Data Transfer VPC Creation"/></p>
<p>And we’ll create two EC2 instances - we’ll place them in the <em>public</em> subnets, so we can SSH into them easily. Since we want to create a 1TB file in the <code>us-east-1a</code> instance - and we don’t want to wait all day on the <code>dd</code> command - we’ll set it up with an io2 Block Express EBS volume, for maximum IOPS and throughput. (An even faster way would be an EC2 instance with a locally attached SSD instead of an EBS volume) [An even faster and substantially cheaper way, that I unfortunately realized only after finishing this post, would be to not use volumes at all but rather directly pipe <code>dd</code> into <code>nc</code> and on the receiver side pipe into <code>/dev/null</code>]</p>
<p><img src="https://www.bitsand.cloud/img/standard-data-transfer-instances.png" alt="Standard Data Transfer Created Instances"/></p>
<p>We’ll edit the <code>us-east-1b</code> security group to support TCP connections from the security group attached to the <code>us-east-1a</code> instance (I opened it up for all ports, but it’d be better to limit it to just the port we’re using for the netcat server):</p>
<p><img src="https://www.bitsand.cloud/img/standard-data-transfer-security-group.png" alt="Standard Data Transfer Security Group"/></p>
<p>Inside the <code>us-east-1b</code> instance we’ll set up a netcat listener on port 1234:</p>
<p>And finally, in the <code>us-east-1a</code> server we’ll create a random 1TB file and then transfer it using netcat - importantly, we’ll be transferring it to the <code>us-east-1b</code> instance’s <em>private IP</em> so that it remains inside the AWS network:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>~&gt; dd <span>if</span><span>=</span>/dev/urandom <span>of</span><span>=</span>random <span>bs</span><span>=</span>16K <span>count</span><span>=</span>64M
</span></span><span><span><span># Wait for completion</span>
</span></span><span><span>~&gt; nc 10.0.23.195 <span>1234</span> &lt; random
</span></span></code></pre></div><p>When the transfer is complete, we can tear down all resources - we’re expecting to see $20 of data transfer charges.</p></div>
</details>
<details>
    <summary>S3 Data Transfer Experiment</summary>
    <div><p>We’ll do a very similar process in this experiment - only this time, we don’t really need private subnets, and we’ll want to enable the S3 Gateway so that our S3 traffic doesn’t leave AWS’s network. We’ll also want to create an IAM role that lets our EC2 instances access S3:</p>
<p><img src="https://www.bitsand.cloud/img/s3datatransferrole.png" alt="S3 Data Transfer IAM Role"/></p>
<p>And attach it to the instances.</p>
<p>Once we’ve created the 1TB file, we’ll want to upload it to S3. As such, we’ll create a bucket:</p>
<p><img src="https://www.bitsand.cloud/img/s3datatransferbucket.png" alt="S3 Data Transfer S3 bucket"/></p>
<p>And then create a random file and upload it to the bucket from the <code>us-east-1a</code> instance:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>~&gt; dd <span>if</span><span>=</span>/dev/urandom <span>of</span><span>=</span>random <span>bs</span><span>=</span>16K <span>count</span><span>=</span>64M
</span></span><span><span><span># Wait for completion</span>
</span></span><span><span>~&gt; aws s3 cp random s3://s3-data-transfer-experiment/random
</span></span></code></pre></div><p>When this is done, we can actually tear down the <code>us-east-1a</code> instance, as it’s no longer needed. Then, from the second instance:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>~&gt; aws s3 cp s3://s3-data-transfer-experiment/random random
</span></span></code></pre></div><p>Once this is done, we can tear down the EC2 instance and delete the S3 object and bucket. We’re expecting to see only a few cents of S3 storage costs.</p></div>
</details>
<h2 id="results">
  Results
  <a href="#results">
    <i aria-hidden="true" title="Link to heading"></i>
    <span>Link to heading</span>
  </a>
</h2>
<p>A few hours later, Cost Explorer is updated with the billing data. Our control experiment of a standard data transfer - which we expected to cost $20 - indeed ended up costing $21.49 (I accidentally stopped the transfer at one point and had to restart it, accounting for some of the extra cost - also the created file was technically 1024GB so the base price was $20.48):</p>
<p><img src="https://www.bitsand.cloud/img/standard-data-transfer-cost-explorer.png" alt="Standard Data Transfer Cost Explorer"/></p>
<p>But the real experiment is the S3-based data transfer, which we expected to cost only a few cents in storage costs. And… 🥁🥁🥁:</p>
<p><img src="https://www.bitsand.cloud/img/s3-data-transfer-cost-explorer.png" alt="S3 Data Transfer Cost Explorer"/></p>
<p>Only eight cents!!! Let’s drill down and see how this S3 storage cost breaks down, and let’s also expand our filter so we can be convinced that there are no data transfer charges:</p>
<p><img src="https://www.bitsand.cloud/img/s3-data-transfer-cost-explorer2.png" alt="S3 Data Transfer Cost Explorer2"/></p>
<p>And indeed, we can see that there are no data transfer costs!! But, something’s weird - the only S3 usage types available have nothing to do with storage… Let’s filter down to S3 and group by usage type:</p>
<p><img src="https://www.bitsand.cloud/img/s3-data-transfer-cost-explorer3.png" alt="S3 Data Transfer Cost Explorer3"/></p>
<p>Wow. We actually weren’t charged <em>anything</em> for the storage <strong>at all</strong>. We were only charged for the S3 requests we made - PUT requests cost $0.005 for every 1000 requests and GET requests cost $0.0004 for every 1000 requests, and the amount of requests it takes to upload and download a 1TB file adds up. But shockingly, there are no storage costs - it’s almost as if S3 doesn’t charge you anything for transient storage? This is very unlike AWS, and I’m not sure how to explain this. I suspected that maybe the S3 free tier was hiding away costs, but - again, shockingly - my S3 storage free tier was totally unaffected by the experiment, none of it was consumed (as opposed to the requests free tier, which was 100% consumed).</p>
<p>But anyways, we’ve proved that the method works. Let’s finish up with some conclusions:</p>
<h2 id="conclusions">
  Conclusions
  <a href="#conclusions">
    <i aria-hidden="true" title="Link to heading"></i>
    <span>Link to heading</span>
  </a>
</h2>
<p>Behind the scenes, AWS replicates S3 data between availability zones for you - whatever this might cost AWS is hidden away in the storage costs you pay for your data. So at its most fundamental level, this method is unlocking free cross-AZ costs - because you’ve effectively already paid for the cross-AZ cost when you uploaded your data to S3! Indeed, if you were to leave your data stored in S3, you’d end up paying significantly more than the cross-AZ cost - but by deleting it immediately after transferring it, you unlock the 99% savings we were going for.</p>
<p>There are some obvious drawbacks to this method: It’s not a drop-in replacement for existing data transfer code, and it can have much higher latency than direct network connections. But if cost is your primary concern, this is an effective way of reducing costs by over 99%.</p>
<p>I really hope you find this method useful, and I think it goes to show just how far you can take cost savings in AWS - there are so many services with so much functionality and so many price points that there’s almost always room for more savings.</p>

      </div></div>
  </body>
</html>
