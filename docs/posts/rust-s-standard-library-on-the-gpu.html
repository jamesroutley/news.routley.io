<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.vectorware.com/blog/rust-std-on-gpu/">Original</a>
    <h1>Rustâ€™s Standard Library on the GPU</h1>
    
    <div id="readability-page-1" class="page"><article><header><a href="https://www.vectorware.com/"><img alt="VectorWare logo" loading="lazy" width="200" height="200" decoding="async" data-nimg="1" srcset="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvectorware_logo.6d5f5210.png&amp;w=256&amp;q=75&amp;dpl=dpl_64RagcK3PvSEkqjsK3wwoN7ekyQv 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvectorware_logo.6d5f5210.png&amp;w=640&amp;q=75&amp;dpl=dpl_64RagcK3PvSEkqjsK3wwoN7ekyQv 2x" src="https://www.vectorware.com/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fvectorware_logo.6d5f5210.png&amp;w=640&amp;q=75&amp;dpl=dpl_64RagcK3PvSEkqjsK3wwoN7ekyQv"/><span>VectorWare</span></a><div><p><a href="https://www.vectorware.com/blog/">Dispatches</a></p><div><p><time datetime="2026-01-20" aria-label="January 20, 2026">January 20, 2026</time><span aria-hidden="true">Â·</span><span>11<!-- --> min read</span></p><p><span>Pedantic mode:</span><span>Off</span></p></div></div><p>GPU code can now use Rust&#39;s standard library. We share the implementation approach and what this unlocks for GPU programming.</p></header><section><div><p>At <a href="https://www.vectorware.com/">VectorWare</a>, we are building the first<!-- -->
<a href="https://www.vectorware.com/blog/announcing-vectorware/">GPU-native software company</a>. Today, we are excited to
announce that we can successfully use Rust&#39;s standard library from GPUs. This milestone
marks a significant step towards our vision of enabling developers to write complex,
high-performance applications that leverage the full power of GPU hardware using
familiar Rust abstractions.</p>
<p>This post is a preview of what we&#39;ve built. We&#39;re preparing our work for potential
upstreaming and will share deeper technical details in future posts.</p>
<h2>Rust&#39;s <code>std</code> library</h2>
<p>Rust&#39;s <a href="https://doc.rust-lang.org/std/">standard library</a> is organized as a set of
layered abstractions:</p>
<ol>
<li><code>core</code> defines the language&#39;s foundation and assumes neither a heap nor an operating
system.</li>
<li><code>alloc</code> builds on <code>core</code> by adding heap allocation.</li>
<li><code>std</code> sits at the top of <code>core</code> and <code>alloc</code>. It adds APIs for operating system
concerns such as files, networking, threads, and processes.</li>
</ol>
<p>A defining feature of Rust is that layers 2 and 3 are <em>optional</em>. Code can opt out of
<code>std</code> via the <code>#![no_std]</code> annotation, relying only on <code>core</code> and, when needed, <code>alloc</code>.
This makes Rust usable in domains such as
<a href="https://docs.rust-embedded.org/book/">embedded</a>,
<a href="https://rust-osdev.github.io/uefi-rs/introduction.html">firmware</a>, and
<a href="https://rust-for-linux.com/">drivers</a>, which lack a traditional operating system.</p>
<h2>Why <code>std</code> isn&#39;t supported on GPUs today</h2>
<p>We are the maintainers of <a href="https://github.com/rust-gpu/Rust-cuda"><code>rust-cuda</code></a> and
<a href="https://github.com/rust-gpu/rust-gpu"><code>rust-gpu</code></a>, open source projects that enable
Rust code to run on the GPU.<!-- --> When targeting
the GPU with these projects, Rust code is compiled with <code>#![no_std]</code> as GPUs do not have
operating systems and therefore cannot support <code>std</code>.</p>
<p>Because <code>#![no_std]</code> is part of the Rust language, <code>#![no_std]</code> libraries on
<a href="https://crates.io">crates.io</a> written for other purposes can generally run on the GPU
without modification. This ability to reuse existing open source libraries is much
better than what exists in other (non-Rust) GPU ecosystems.</p>
<p>Still, there is a cost to opting out of <code>std</code>. Many of Rust&#39;s most useful and ergonomic
abstractions live in the standard library and the majority of open source libraries
assume <code>std</code>. Enabling meaningful <code>std</code> support on GPUs unlocks a much larger class of
applications and enables even more code reuse.</p>
<h2>Why <code>std</code> might be feasible on GPUs soon</h2>
<p>Modern GPU workloads like machine learning and AI require fast access to storage and
networking from the GPU. Technologies such as NVIDIA&#39;s <a href="https://docs.nvidia.com/gpudirect-storage/">GPUDirect
Storage</a>, <a href="https://docs.nvidia.com/cuda/gpudirect-rdma/">GPUDirect
RDMA</a>, and
<a href="https://www.nvidia.com/en-us/networking/ethernet-adapters/">ConnectX</a> make it possible
for GPUs to interact with disks and <a href="https://www.nvidia.com/en-us/networking/">networks</a>
more directly in the datacenter. With systems like <a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/">NVIDIA&#39;s DGX
Spark</a> and Apple&#39;s
M-series devices, similar capabilities are starting to appear in consumer hardware. APIs
and features traditionally provided by an operating system are becoming available to GPU
code and we only see that trend continuing.</p>
<p>We believe CPUs and GPUs are converging architecturally, as evidenced by designs such as
AMD&#39;s APUs as well as NPUs and TPUs being integrated into various CPUs. This convergence
makes it both more practical and more compelling to share abstractions across devices
rather than treating GPUs and other cores as isolated accelerators.</p>
<h2><code>std</code> as a GPU abstraction layer</h2>
<p>The GPU ecosystem is younger than the CPU ecosystem. GPU hardware capabilities are
changing rapidly and software standards are still emerging. Rust&#39;s <code>std</code> can provide a
stable surface while allowing the underlying implementation to change until the
ecosystem matures. For example, <code>std</code> may use
<a href="https://developer.nvidia.com/gpudirect">GPUDirect</a> on one system, fall back to host
mediation on another, or adapt between unified memory and explicit transfers depending
on hardware support. The key is that user code does not need to change due to the
underlying feature support.</p>
<p>At VectorWare, we are building extremely complex GPU-native applications where
higher-level abstractions are critical. We are excited to pioneer this space and look
forward to sharing more of what we have done in the future.</p>
<h2>A world first: Rust&#39;s <code>std</code> on the GPU</h2>
<p>The following is a simple GPU kernel that uses Rust&#39;s <code>std</code> library to perform various
tasks such as printing to stdout, reading user input, getting the current time, and
writing to a file. <strong>This code runs on the GPU</strong>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark github-light"><code data-language="rust" data-theme="github-dark github-light"><span data-line=""><span>use</span><span> std</span><span>::</span><span>io</span><span>::</span><span>Write</span><span>;</span></span>
<span data-line=""><span>use</span><span> std</span><span>::</span><span>time</span><span>::</span><span>{</span><span>SystemTime</span><span>, </span><span>UNIX_EPOCH</span><span>};</span></span>
<span data-line=""> </span>
<span data-line=""><span>#[</span><span>unsafe</span><span>(no_mangle)]</span></span>
<span data-line=""><span>pub</span><span> extern</span><span> &#34;gpu-kernel&#34;</span><span> fn</span><span> kernel_main</span><span>() {</span></span>
<span data-line=""><span>    println!</span><span>(</span><span>&#34;Hello from VectorWare and the GPU!&#34;</span><span>);</span></span>
<span data-line=""> </span>
<span data-line=""><span>    print!</span><span>(</span><span>&#34;Enter your name: &#34;</span><span>);</span></span>
<span data-line=""><span>    let</span><span> _ </span><span>=</span><span> std</span><span>::</span><span>io</span><span>::</span><span>stdout</span><span>()</span><span>.</span><span>flush</span><span>();</span></span>
<span data-line=""><span>    let</span><span> mut</span><span> name </span><span>=</span><span> String</span><span>::</span><span>new</span><span>();</span></span>
<span data-line=""><span>    std</span><span>::</span><span>io</span><span>::</span><span>stdin</span><span>()</span><span>.</span><span>read_line</span><span>(</span><span>&amp;mut</span><span> name)</span><span>.</span><span>unwrap</span><span>();</span></span>
<span data-line=""> </span>
<span data-line=""><span>    let</span><span> name </span><span>=</span><span> name</span><span>.</span><span>trim_end</span><span>();</span></span>
<span data-line=""><span>    println!</span><span>(</span><span>&#34;Hello, {}! Nice to meet you from the GPU!&#34;</span><span>, name);</span></span>
<span data-line=""> </span>
<span data-line=""><span>    let</span><span> now </span><span>=</span><span> SystemTime</span><span>::</span><span>now</span><span>();</span></span>
<span data-line=""><span>    let</span><span> duration_since_epoch </span><span>=</span><span> now</span><span>.</span><span>duration_since</span><span>(</span><span>UNIX_EPOCH</span><span>)</span><span>.</span><span>unwrap</span><span>();</span></span>
<span data-line=""><span>    println!</span><span>(</span></span>
<span data-line=""><span>        &#34;Current time (seconds since epoch): {}&#34;</span><span>,</span></span>
<span data-line=""><span>        duration_since_epoch</span><span>.</span><span>as_secs</span><span>()</span></span>
<span data-line=""><span>    );</span></span>
<span data-line=""> </span>
<span data-line=""><span>    let</span><span> msg </span><span>=</span><span> format!</span><span>(</span></span>
<span data-line=""><span>        &#34;This file was created *from a GPU*, using the Rust standard library ðŸ¦€</span><span>\n</span><span>\</span></span>
<span data-line=""><span>         We are {:?}</span><span>\n</span><span>\</span></span>
<span data-line=""><span>         Current time (seconds since epoch): {}</span><span>\n</span><span>\</span></span>
<span data-line=""><span>         User name: {}&#34;</span><span>,</span></span>
<span data-line=""><span>        &#34;VectorWare&#34;</span><span>.</span><span>to_owned</span><span>(),</span></span>
<span data-line=""><span>        duration_since_epoch</span><span>.</span><span>as_secs</span><span>(),</span></span>
<span data-line=""><span>        name</span></span>
<span data-line=""><span>    );</span></span>
<span data-line=""><span>    std</span><span>::</span><span>fs</span><span>::</span><span>File</span><span>::</span><span>create</span><span>(</span><span>&#34;rust_from_gpu.txt&#34;</span><span>)</span></span>
<span data-line=""><span>        .</span><span>unwrap</span><span>()</span></span>
<span data-line=""><span>        .</span><span>write_all</span><span>(msg</span><span>.</span><span>as_bytes</span><span>())</span></span>
<span data-line=""><span>        .</span><span>unwrap</span><span>();</span></span>
<span data-line=""> </span>
<span data-line=""><span>    println!</span><span>(</span><span>&#34;File written successfully!&#34;</span><span>);</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>The only difference from regular Rust code is the <code>#[unsafe(no_mangle)]</code> and <code>extern &#34;gpu-kernel&#34;</code> annotations, which indicate that this function is a GPU kernel entry
point. We will get rid of those with some compiler transformations in the future.</p>
<h2>Hostcalls</h2>
<p>This works because of our custom <code>hostcall</code> framework. A hostcall is analogous to a
<a href="https://en.wikipedia.org/wiki/System_call">syscall</a>. A hostcall is a structured request
from GPU code to the host CPU to perform something it cannot execute itself. You can
think of it like a remote procedure call from the GPU to the host to achieve
syscall-like functionality.</p>
<p>To end users, Rust&#39;s <code>std</code> APIs appear unchanged and act as one would expect. Under the
hood, however, certain <code>std</code> calls are reimplemented via hostcalls. While we can
implement any API with hostcalls, in this instance we implement a
<a href="https://en.wikipedia.org/wiki/C_standard_library"><code>libc</code></a> facade to minimize changes to
Rust&#39;s <code>std</code> (which uses <code>libc</code> to communicate with Unix-like operating systems). For
example,
<a href="https://doc.rust-lang.org/std/fs/struct.File.html#method.open"><code>std::fs::File::open</code></a>
is reimplemented to issue an <code>open</code> hostcall to the host, which performs the actual file
open operation using the host&#39;s filesystem APIs.</p>
<p>With <code>std</code> available from the GPU, developer ergonomics improve significantly and a much
larger portion of the Rust ecosystem can now be used on the GPU.</p>
<h3>Device/host split</h3>
<p>&#34;Hostcall&#34; is something of a misnomer. In reality, hostcall requests can be fulfilled on
either the host or the GPU. This enables progressive enhancement as GPU hardware and
software matures. For example,
<a href="https://doc.rust-lang.org/std/time/struct.Instant.html"><code>std::time::Instant</code></a> is
implemented on the GPU using a device timer on platforms with APIs such as CUDA&#39;s
<a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#special-registers-globaltimer"><code>%globaltimer</code></a>,
while
<a href="https://doc.rust-lang.org/std/time/struct.SystemTime.html"><code>std::time::SystemTime</code></a> is
implemented on the host due to missing wall-clock support on the GPU.</p>
<p>Being able to choose where a hostcall is fulfilled enables advanced features such as
device-side caches (where the GPU can cache hostcall results locally to avoid repeated
round-trips) and filesystem virtualization (where the GPU can maintain its own view of
the filesystem and sync with the host as needed).</p>
<p>We can even innovate and add new semantics without breaking compatibility. For example,
one could write a file to <code>/gpu/tmp</code> to indicate it should stay on the GPU, or one could
make a network call to <code>localdevice:42</code> to communicate with warp/workgroup 42. We will
share some of our work in this space in the future.</p>
<h3>Implementation details</h3>
<p>While the idea is fairly simple, the implementation has many details to ensure
correctness and performance. We&#39;ll deep-dive into the implementation in a later blog
post. For now, we will cover some high-level details.</p>
<p>We use standard GPU programming techniques such as double-buffering and atomic
operations and take care to avoid <a href="https://en.wikipedia.org/wiki/Torn_read">data
tearing</a> and ensure <a href="https://en.wikipedia.org/wiki/Memory_ordering">memory
consistency</a>. The protocol is
deliberately minimal to keep GPU-side logic simple and we support features like packing
results to avoid GPU heap allocations where appropriate. We leverage APIs like CUDA
streams to avoid blocking the GPU while the host processes requests. The host-side
hostcall handlers are implemented using <code>std</code>, not as a 1:1 <code>libc</code> trampoline, so that
they are portable and testable.</p>
<p><strong>As Rust programmers, we very much care about safety and correctness.</strong> We&#39;ve run a
modified version of the hostcall runtime / kernel under
<a href="https://github.com/rust-lang/miri">miri</a>, with CPU threads emulating the GPU, to check
if our code is minimally sound.<!-- --> We&#39;ll share
more about our testing and verification efforts in a future post.</p>
<p>Our current implementation targets Linux hosts with NVIDIA GPUs via CUDA. There is
nothing fundamental preventing support for other host operating systems or GPU
vendorsâ€”the hostcall protocol is vendor-agnostic, and we could target AMD GPUs via HIP
or use Vulkan with <code>rust-gpu</code>.</p>
<h2>Prior work</h2>
<p>There is substantial prior art in exposing system-like APIs to GPU code, primarily from
the CUDA and C++ ecosystems.</p>
<p>NVIDIA ships a GPU-side C and C++ runtime as part of CUDA, including
<a href="https://docs.nvidia.com/cuda/cuda-c-std/"><code>libcu++</code></a>,
<a href="https://docs.nvidia.com/cuda/libdevice-users-guide/index.html"><code>libdevice</code></a>, and the
CUDA device runtime (see the <a href="https://docs.nvidia.com/cuda/cuda-programming-guide/">CUDA Programming
Guide</a>). These components provide
implementations of large portions of the C and C++ standard libraries for device code.
However, they are tightly coupled to CUDA, are C/C++-specific, and are not designed to
present a general-purpose operating system abstraction. Facilities such as filesystems,
networking, and wall-clock time remain host-only concerns, exposed through CUDA-specific
APIs rather than a unified standard library interface.</p>
<p>There have also been experiments such as <a href="https://libc.llvm.org/gpu/">libc for GPUs</a> and
related projects that attempt to provide a C or POSIX-like layer for GPUs by forwarding
calls to the host. While we only became aware of these efforts after we started our own
work, we are excited to see we arrived at similar ideas and implementations
independently. We are also unaware of any prior work that integrates such a layer with a
higher-level language standard library like Rust&#39;s <code>std</code>.</p>
<p>Other GPU programming models, including <a href="https://www.khronos.org/opencl/">OpenCL</a>,
<a href="https://rocm.docs.amd.com/projects/HIP">HIP</a>, and
<a href="https://www.khronos.org/sycl/">SYCL</a>, provide their own standard libraries and runtime
abstractions. These libraries are intentionally minimal and domain-specific, focusing on
portability and performance rather than source compatibility with existing CPU codebases
or reuse of a general-purpose standard library ecosystem.</p>
<p>Our approach differs in two key ways. First, we target Rust&#39;s <code>std</code> directly rather than
introducing a new GPU-specific API surface. This preserves source compatibility with
existing Rust code and libraries. Second, we treat host mediation as an implementation
detail behind <code>std</code>, not as a visible programming model.</p>
<p>In that sense, this work is less about inventing a new GPU runtime and more about
extending Rust&#39;s existing abstraction boundary to span heterogeneous systems. At
VectorWare, we are bringing the GPU to Rust instead of just bringing Rust to the
GPU.</p>
<h2>Open sourcing and upstreaming</h2>
<p>We are cleaning up our changes and preparing to open source them. As the <a href="https://www.vectorware.com/team">VectorWare
team</a> includes multiple members of the <a href="https://rust-lang.org/governance/teams/compiler/">Rust compiler
team</a>, we are keen to work upstream as
much as possible. That said, changes to Rust&#39;s standard library require careful review
and take time to upstream.</p>
<p>One open question is where the correct abstraction boundary should live. While this
implementation uses a <code>libc</code>-style facade to minimize changes to Rust&#39;s existing <code>std</code>
library, it is not yet clear that <code>libc</code> is the right long-term layer. The hostcall
mechanism is quite versatile and we could alternatively make <code>std</code> GPU-aware via
Rust-native APIs. This requires more work on the <code>std</code> side but could be more safe and
efficient than mimicking <code>libc</code>.</p>
<h2>Is VectorWare only focused on Rust?</h2>
<p>As a company, we understand that not everyone uses Rust. Our future products will
support multiple programming languages and runtimes. However, we believe Rust is
uniquely well suited to building high-performance, reliable GPU-native applications and
that is what we are most excited about.</p>
<h2>Follow along</h2>
<p>Follow us on <a href="https://x.com/vectorware">X</a>,
<a href="https://bsky.app/profile/vectorware.com">Bluesky</a>,
<a href="https://www.linkedin.com/company/vectorware/">LinkedIn</a>, or subscribe to our
<a href="https://www.vectorware.com/blog">blog</a> to stay updated on our progress. We will be sharing more about our work in
the coming months, including deeper technical dives into our hostcall implementation and
other compiler work, testing and verification efforts, and our thoughts on the future
of GPU programming with Rust. You can also reach us at <a href="mailto:hello@vectorware.com">hello@vectorware.com</a>.</p>
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
</div></section></article></div>
  </body>
</html>
