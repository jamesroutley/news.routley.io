<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/setzer22/llama-rs">Original</a>
    <h1>Llama.rs â€“ Rust port of llama.cpp for fast LLaMA inference on CPU</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<blockquote>
<p dir="auto">Do the LLaMA thing, but now in Rust <g-emoji alias="crab" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f980.png">ðŸ¦€</g-emoji><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">ðŸš€</g-emoji><g-emoji alias="llama" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f999.png">ðŸ¦™</g-emoji></p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/setzer22/llama-rs/blob/main/doc/resources/logo2.png"><img src="https://github.com/setzer22/llama-rs/raw/main/doc/resources/logo2.png" alt="A llama riding a crab, AI-generated"/></a></p>
<blockquote>
<p dir="auto"><em>Image by <a href="https://github.com/darthdeus/">@darthdeus</a>, using Stable Diffusion</em></p>
</blockquote>
<p dir="auto"><a href="https://ko-fi.com/F1F8DNO5D" rel="nofollow"><img src="https://camo.githubusercontent.com/cd07f1a5d90e454e7bbf69d22ebe4cdbd3a0b3dcf56ba0b6c2495a8e99c776be/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg"/></a></p>
<p dir="auto"><a href="https://crates.io/crates/llama_rs" rel="nofollow"><img src="https://camo.githubusercontent.com/230782a094e2cc9e628dbde4a42367daba87d2c136ad719382341cddef074694/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6c6c616d612d72732e737667" alt="Latest version" data-canonical-src="https://img.shields.io/crates/v/llama-rs.svg"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/83d3746e5881c1867665223424263d8e604df233d0a11aae0813e0414d433943/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667"><img src="https://camo.githubusercontent.com/83d3746e5881c1867665223424263d8e604df233d0a11aae0813e0414d433943/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/setzer22/llama-rs/blob/main/doc/resources/llama_gif.gif"><img src="https://github.com/setzer22/llama-rs/raw/main/doc/resources/llama_gif.gif" alt="Gif showcasing language generation using llama-rs" data-animated-image=""/></a></p>
<p dir="auto"><strong>LLaMA-rs</strong> is a Rust port of the
<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> project. This allows running
inference for Facebook&#39;s <a href="https://github.com/facebookresearch/llama">LLaMA</a>
model on a CPU with good performance using full precision, f16 or 4-bit
quantized versions of the model.</p>
<p dir="auto">Just like its C++ counterpart, it is powered by the
<a href="https://github.com/ggerganov/ggml"><code>ggml</code></a> tensor library, achieving the same performance as the original code.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting started</h2>
<p dir="auto">Make sure you have a rust toolchain set up.</p>
<ol dir="auto">
<li>Get a copy of the model&#39;s weights<sup><a href="#user-content-fn-1-a731a988bfb4fe3797cad55da4c89af7" id="user-content-fnref-1-a731a988bfb4fe3797cad55da4c89af7" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></li>
<li>Clone the repository</li>
<li>Build (<code>cargo build --release</code>)</li>
<li>Run with <code>cargo run --release -- &lt;ARGS&gt;</code></li>
</ol>
<p dir="auto"><strong>NOTE</strong>: Make sure to build and run in release mode. Debug builds are currently broken.</p>
<p dir="auto">For example, you try the following prompt:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run --release -- -m /data/Llama/LLaMA/7B/ggml-model-q4_0.bin -p &#34;Tell me how cool the Rust programming language is"><pre>cargo run --release -- -m /data/Llama/LLaMA/7B/ggml-model-q4_0.bin -p <span><span>&#34;</span>Tell me how cool the Rust programming language is</span></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-qa" aria-hidden="true" href="#qa"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Q&amp;A</h2>
<ul dir="auto">
<li>
<p dir="auto"><strong>Q: Why did you do this?</strong></p>
</li>
<li>
<p dir="auto"><strong>A:</strong> It was not my choice. Ferris appeared to me in my dreams and asked me
to rewrite this in the name of the Holy crab.</p>
</li>
<li>
<p dir="auto"><strong>Q: Seriously now</strong></p>
</li>
<li>
<p dir="auto"><strong>A:</strong> Come on! I don&#39;t want to get into a flame war. You know how it goes,
<em>something something</em> memory <em>something something</em> cargo is nice, don&#39;t make
me say it, everybody knows this already.</p>
</li>
<li>
<p dir="auto"><strong>Q: I insist.</strong></p>
</li>
<li>
<p dir="auto"><strong>A:</strong> <em>Sheesh! Okaaay</em>. After seeing the huge potential for <strong>llama.cpp</strong>,
the first thing I did was to see how hard would it be to turn it into a
library to embed in my projects. I started digging into the code, and realized
the heavy lifting is done by <code>ggml</code> (a C library, easy to bind to Rust) and
the whole project was just around ~2k lines of C++ code (not so easy to bind).
After a couple of (failed) attempts to build an HTTP server into the tool, I
realized I&#39;d be much more productive if I just ported the code to Rust, where
I&#39;m more comfortable.</p>
</li>
<li>
<p dir="auto"><strong>Q: Is this the real reason?</strong></p>
</li>
<li>
<p dir="auto"><strong>A:</strong> Haha. Of course <em>not</em>. I just like collecting imaginary internet
points, in the form of little stars, that people seem to give to me whenever I
embark on pointless quests for <em>rewriting X thing, but in Rust</em>.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-known-issues--to-dos" aria-hidden="true" href="#known-issues--to-dos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Known issues / To-dos</h2>
<p dir="auto">Contributions welcome! Here&#39;s a few pressing issues:</p>
<ul>
<li> The code only sets the right CFLAGS on Linux. The <code>build.rs</code> script in
<code>ggml_raw</code> needs to be fixed, so inference <em>will be very slow on every
other OS</em>.</li>
<li> The quantization code has not been ported (yet). You can still use the
quantized models with llama.cpp.</li>
<li> The code needs to be &#34;library&#34;-fied. It is nice as a showcase binary, but
the real potential for this tool is to allow embedding in other services.</li>
<li> No crates.io release. The name <code>llama-rs</code> is reserved and I plan to do
this soon-ish.</li>
<li> Debug builds are currently broken.</li>
<li> Anything from the original C++ code.</li>
</ul>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-1-a731a988bfb4fe3797cad55da4c89af7">
<p dir="auto">The only legal source to get the weights at the time of writing is <a href="https://github.com/facebookresearch/llama/blob/main/README.md#llama">this repository</a>. The choice of words also may or may not hint at the existence of other kinds of sources. <a href="#user-content-fnref-1-a731a988bfb4fe3797cad55da4c89af7" data-footnote-backref="" aria-label="Back to reference 1"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">â†©</g-emoji></a></p>
</li>
</ol>
</section>
</article>
          </div></div>
  </body>
</html>
