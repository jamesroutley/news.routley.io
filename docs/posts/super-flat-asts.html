<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jhwlr.io/super-flat-ast/">Original</a>
    <h1>Super-Flat ASTs</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
                


Table of contents
<ul>

    <li>
        <a href="https://jhwlr.io/super-flat-ast/#re-introducing-simp">(Re-)introducing simp</a>
        
    </li>

    <li>
        <a href="https://jhwlr.io/super-flat-ast/#simple-ast">Simple AST</a>
        
            <ul>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#a-note-on-benchmarking">A note on benchmarking</a>
                    </li>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#simple-tree-ast-results">Simple tree AST results</a>
                    </li>
                
            </ul>
        
    </li>

    <li>
        <a href="https://jhwlr.io/super-flat-ast/#amortizing-allocations">Amortizing allocations</a>
        
            <ul>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#string-interning">String interning</a>
                    </li>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#interning-results">Interning results</a>
                    </li>
                
            </ul>
        
    </li>

    <li>
        <a href="https://jhwlr.io/super-flat-ast/#pointer-compression">Pointer compression</a>
        
            <ul>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#flat-ast-results">Flat AST results</a>
                    </li>
                
            </ul>
        
    </li>

    <li>
        <a href="https://jhwlr.io/super-flat-ast/#bump-allocation">Bump allocation</a>
        
            <ul>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#bump-allocation-results">Bump allocation results</a>
                    </li>
                
            </ul>
        
    </li>

    <li>
        <a href="https://jhwlr.io/super-flat-ast/#super-flat">Super flat</a>
        
            <ul>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#super-flat-implementation">Super flat implementation</a>
                    </li>
                
                    <li>
                        <a href="https://jhwlr.io/super-flat-ast/#super-flat-results">Super flat results</a>
                    </li>
                
            </ul>
        
    </li>

    <li>
        <a href="https://jhwlr.io/super-flat-ast/#fin">Fin</a>
        
    </li>

</ul>


<hr/>
<p>In my previous post, I introduced the <a href="https://jhwlr.io/intro-to-parsing">simp</a>le programming language.
Near the end of that post, I mentioned that the parser isn&#39;t very efficient.</p>
<p>Let&#39;s try to optimize it!</p>
<blockquote>
<p>If you&#39;re just curious about the title of the article,
you can skip ahead to <a href="https://jhwlr.io/super-flat-ast/#super-flat">that section</a>.</p>
</blockquote>
<h2 id="re-introducing-simp">(Re-)introducing <code>simp</code></h2>
<p>First, a quick recap on the programming language being implemented here:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>fibonacci</span><span>(n) {
</span><span>  </span><span>if</span><span> n &lt; </span><span>2 </span><span>{ n }
</span><span>  </span><span>else </span><span>{ </span><span>fib</span><span>(n-</span><span>1</span><span>) + </span><span>fib</span><span>(n-</span><span>2</span><span>) }
</span><span>}
</span><span>
</span><span>let</span><span> result = </span><span>fibonacci</span><span>(</span><span>30</span><span>);
</span></code></pre>
<p>It&#39;s a very <em>simple</em>, language consisting of:</p>
<ul>
<li>Variables</li>
<li>Functions</li>
<li>Various kinds of expressions</li>
<li>Conditionals</li>
</ul>
<p>It&#39;s not particularly useful, but it has enough <em>stuff</em> to be a good testing ground
for experimenting with parsers.</p>
<h2 id="simple-ast">Simple AST</h2>
<p>Currently, the implementation uses a <em>recursive descent</em> parser which produces an
<em>abstract syntax tree</em> (AST). The AST represents code in a more structured form,
which is easier to work with in subsequent compilation passes.</p>
<p>Here is a small subset of the full AST definition:</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Stmt {
</span><span>    Expr(Box&lt;Expr&gt;),
</span><span>    </span><span>// ... other statements
</span><span>}
</span><span>
</span><span>enum </span><span>Expr {
</span><span>    Block(Box&lt;ExprBlock&gt;),
</span><span>    </span><span>// ... more expressions
</span><span>}
</span><span>
</span><span>struct </span><span>ExprBlock {
</span><span>    </span><span>body</span><span>: Vec&lt;Stmt&gt;,
</span><span>}
</span></code></pre>
<p>It highlights a few things:</p>
<ul>
<li>Each kind of <code>Stmt</code> and <code>Expr</code> is in a <code>Box</code>, to allow these types to be <em>recursive</em>.</li>
<li>Sequences are stored in <code>Vec</code>s.</li>
</ul>
<p>This kind of design is simple and flexible.
Unfortunately, it also uses a lot of memory, and each syntax node requires many separate allocations.</p>
<h3 id="a-note-on-benchmarking">A note on benchmarking</h3>
<p>Benchmarking is difficult. It&#39;s very easy to introduce bias or measure the wrong thing.</p>
<p>We&#39;ll be relying on two metrics:</p>
<ul>
<li>Throughput in lines of code per second</li>
<li>Maximum memory usage</li>
</ul>
<p>The scenario will be to parse a set of files with different sizes, starting at a few kB
and going up to 100 MB. We&#39;re effectively measuring how much time it takes to <em>construct</em>
the AST, and how much memory it uses. We won&#39;t be measuring anything beyond that.</p>
<blockquote>
<p>A 100 MB file has nearly 10 million lines of code! The reason we go that high is to prevent
the CPU from storing the entire file and its AST in CPU cache.</p>
</blockquote>
<p>Don&#39;t over-interpret these numbers! All they&#39;ll tell you is the <em>relative performance</em>
of different kinds of tree representations.</p>
<h3 id="simple-tree-ast-results">Simple tree AST results</h3>
<img width="90%" src="https://jhwlr.io/super-flat-ast/0_loc.png"/>
<ul>
<li>The Y axis is in <em>millions</em> of lines of code per second.</li>
<li>The X axis is <code>log10(input size)</code>.</li>
</ul>
<p>We&#39;re using <code>log10</code> for the X axis, otherwise all the smaller inputs would be crammed to the
left side of the plot. This way the results are more evenly spread out.</p>
<img width="90%" src="https://jhwlr.io/super-flat-ast/0_memory.png"/>
<p>Our memory scales ~linearly with the size of the input, so no <code>log10</code> here, as a straight
line tells a better story.</p>
<p>We have nothing to compare this to right now, so these numbers are pretty meaningless.
We&#39;re clocking in at ~millions of lines of code per second, so it&#39;s not <em>too</em> slow...
but we can do better!</p>
<h2 id="amortizing-allocations">Amortizing allocations</h2>
<p>Here&#39;s <code>StmtFn</code>, the syntax node for function declarations:</p>
<pre data-lang="rust"><code data-lang="rust"><span>// `fn f(a,b,c) {...}`
</span><span>struct </span><span>StmtFn {
</span><span>    </span><span>// fn f(a,b,c) {...}
</span><span>    </span><span>//    ^
</span><span>    </span><span>name</span><span>: String,
</span><span>
</span><span>    </span><span>// fn f(a,b,c) {...}
</span><span>    </span><span>//      ^^^^^
</span><span>    </span><span>params</span><span>: Vec&lt;String&gt;,
</span><span>
</span><span>    </span><span>// fn f(a,b,c) {...}
</span><span>    </span><span>//             ^^^^^
</span><span>    </span><span>body</span><span>: Block,
</span><span>}
</span></code></pre>
<p>Each <em>name</em> and <em>parameter</em> in the declaration turns into a separately heap-allocated string.</p>
<h3 id="string-interning">String interning</h3>
<p>Instead of storing owned <code>String</code>s, we&#39;ll store an <em>index</em> into a string buffer.
That string buffer will also keep track of where each string is in the buffer,
so that we can find equivalent strings later.</p>
<p><em>Interning</em> is a &#34;get or insert&#34; operation on that string buffer. It will allow
us to re-use memory for strings and identifiers which appear multiple times in
the same source file.</p>
<p>Here&#39;s how our structure changes:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Ast {
</span><span>  </span><span>identifiers</span><span>: Interner&lt;IdentId&gt;,
</span><span>}
</span><span>
</span><span>struct </span><span>StmtFn {
</span><span>    </span><span>name</span><span>: IdentId,
</span><span>    </span><span>params</span><span>: Vec&lt;IdentId&gt;,
</span><span>    </span><span>body</span><span>: Block,
</span><span>}
</span></code></pre>
<p>We&#39;ll do the interning while parsing:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>parse_expr_ident</span><span>(</span><span>c</span><span>: &amp;</span><span>mut</span><span> Cursor, </span><span>ast</span><span>: &amp;</span><span>mut</span><span> Ast) -&gt; Result&lt;Expr&gt; {
</span><span>    </span><span>let</span><span> token = c.</span><span>must</span><span>(</span><span>LIT_IDENT</span><span>)?;
</span><span>    </span><span>let</span><span> name = c.</span><span>lexeme</span><span>(token);
</span><span>    </span><span>let</span><span> id = ast.</span><span>intern_ident</span><span>(name);
</span><span>    Ok(Expr::Ident(Box::new(ExprIdent { name: id })))
</span><span>}
</span></code></pre>
<p>This was a relatively painless change!</p>
<blockquote>
<p>A nice property of interning all strings is that to compare them, all you need
is their ID, because identical strings are guaranteed to have the same ID.
You don&#39;t have to compare the actual string memory.</p>
<p>O(1) string comparisons, nice!</p>
</blockquote>
<p>During traversal, we&#39;ll have to resolve these IDs to the actual strings if we
want to print them.</p>
<h3 id="interning-results">Interning results</h3>
<img width="90%" src="https://jhwlr.io/super-flat-ast/1_loc.png"/>
<p>In this graph, higher is better. So interning strings is actually faster!</p>
<p>This is a bit counter-intuitive, though. Why does it take <em>less time</em> to do
<em>more work</em>? We&#39;re now hashing each string, looking it up in a map, and then
either returning an ID after optionally allocating something. Previously we
were always allocating.</p>
<p>The next graph will partially answer that:</p>
<img width="90%" src="https://jhwlr.io/super-flat-ast/1_pagefaults.png"/>
<p>A <em>page fault</em> is a signal to the operating system that the process is trying
to use a page it has <em>allocated</em>, and so the page must be assigned a physical
address in RAM.</p>
<p>The more memory you use, the more page faults there are. And they add up quick!
It turns out that using less memory also means the CPU has to do less work.</p>
<p>So, are we actually using less memory?</p>
<img width="90%" src="https://jhwlr.io/super-flat-ast/1_memory.png"/>
<p>Yes, by quite a bit! We peak at ~2.3 GB for the &#34;simple tree&#34; representation,
and at ~1.8 GB when interning strings. That&#39;s more than a 20% reduction!</p>
<p>Memory usage and page faults are directly correlated. If you allocate and actually
fill 2 GB of memory, with a page size of 4096 bytes, we can predict ~500 thousand
page faults. That&#39;s exactly what we see in the graphs above.</p>
<p>Allocations are expensive!</p>
<p>Can we reduce memory usage even further?</p>
<h2 id="pointer-compression">Pointer compression</h2>
<p>Enter &#34;flat AST&#34;. <a href="https://www.cs.cornell.edu/~asampson/blog/flattening.html">Here</a>
is a nice article about this concept. To summarize:</p>
<p>Instead of storing full 64-bit pointers, we&#39;ll store 32-bit <em>indices</em>.
These indices are relative to the base of a <em>memory arena</em>.</p>
<p>The result is that nodes are allocated in contiguous arrays, which allow us to
amortize the cost of those allocations. Instead of doing a <code>malloc</code> call per node,
we&#39;ll be doing a <code>malloc</code> call for every <code>N</code> nodes.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>StmtFn {
</span><span>    </span><span>name</span><span>: IdentId,
</span><span>    </span><span>params</span><span>: Vec&lt;IdentId&gt;,
</span><span>    </span><span>body</span><span>: ExprId,
</span><span>}
</span></code></pre>
<blockquote>
<p>We&#39;ll also use interning in this AST.</p>
<p>I thought about whether to properly separate all the different methods in this article,
but ultimately decided not to. For each kind of AST, I have to copy and update the parser...
That&#39;s a lot of work, and I don&#39;t believe it would change the conclusion.</p>
</blockquote>
<p>Our <code>StmtFn</code> no longer directly stores an <code>ExprBlock</code> as its <code>body</code>, instead it
stores an <code>ExprId</code>. That makes the whole struct quite a bit smaller.</p>
<p>We&#39;ll need a place to store nodes:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Ast {
</span><span>    </span><span>exprs</span><span>: Vec&lt;Expr&gt;,
</span><span>    </span><span>stmts</span><span>: Vec&lt;Stmt&gt;,
</span><span>
</span><span>    </span><span>strings</span><span>: Interner&lt;StrId&gt;,
</span><span>    </span><span>idents</span><span>: Interner&lt;IdentId&gt;,
</span><span>}
</span></code></pre>
<blockquote>
<p>We could also just have a single array of <code>Node</code>, but that would require combining
<code>Stmt</code> and <code>Expr</code> into a new enum, and I wanted to avoid any extra indirection here.</p>
</blockquote>
<p>While parsing, We&#39;ll ask the <code>ast</code> to store new nodes, instead of wrapping each one
in a <code>Box</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>parse_stmt_fn</span><span>(</span><span>c</span><span>: &amp;</span><span>mut</span><span> Cursor, </span><span>ast</span><span>: &amp;</span><span>mut</span><span> Ast) -&gt; Result&lt;StmtId&gt; {
</span><span>    assert!(c.</span><span>eat</span><span>(</span><span>KW_FN</span><span>));
</span><span>
</span><span>    </span><span>let</span><span> name = </span><span>parse_ident</span><span>(c, ast)?;
</span><span>    </span><span>let</span><span> params = </span><span>parse_param_list</span><span>(c, ast)?;
</span><span>    </span><span>let</span><span> body = </span><span>parse_expr_block</span><span>(c, ast)?;
</span><span>
</span><span>    Ok(ast.</span><span>alloc_stmt</span><span>(Stmt::Fn(StmtFn { name, params, body })))
</span><span>}
</span></code></pre>
<p>The <code>ast</code> gives us a node ID in return. We can store that node ID inside other nodes.</p>
<blockquote>
<p>We&#39;re not <em>interning</em> these nodes. Even if you have two identical syntax trees, they&#39;ll
be duplicated in the final AST.</p>
</blockquote>
<p>This is pretty close to what <code>rustc</code> does! Is it any good?</p>
<h3 id="flat-ast-results">Flat AST results</h3>
<img width="90%" src="https://jhwlr.io/super-flat-ast/2_loc.png"/>
<p>Yeah, that is an improvement!</p>
<p>The results are a <em>bit</em> noisy on smaller files. At those scales, the overhead of whatever
setup the <code>flat</code> AST has to do is higher than the actual parsing. My computer is also
a bit noisy while running these benchmarks.</p>
<img width="90%" src="https://jhwlr.io/super-flat-ast/2_memory.png"/>
<p>We&#39;ve reduced the memory usage again! But not by a whole lot.</p>
<p>The reason is that each variant of the <code>Stmt</code> and <code>Expr</code> enum is <em>unboxed</em>. <code>rustc</code>
actually boxes the larger variants, which would reduce memory usage.
I didn&#39;t do that because it makes the benchmark slower, and it&#39;d negate the whole
point of demonstrating usage of a memory arena.</p>
<p>Still, nice speedup!</p>
<p>...can we do better?</p>
<h2 id="bump-allocation">Bump allocation</h2>
<p>A memory arena is a contiguous section of the <em>heap</em>. But the heap is <em>already</em> flat.
Why do we have to flatten it further?</p>
<blockquote>
<p>Well, sort of flat. Memory pages used by a process don&#39;t actually have to be contiguous.
They could be anywhere in physical memory.</p>
</blockquote>
<p>Let&#39;s bring in a library: <a href="https://crates.io/crates/bumpalo">bumpalo</a> implements a <em>bump allocator</em>,
which can be used to allocate arbitrary types in a contiguous <em>memory arena</em>. It provides us with address
stability, as it never <em>reallocates</em> an existing block of memory, instead allocating a new block
(twice the size of the previous one) when it runs out of space.</p>
<p>Our AST will go back to looking like it did initially, a tree of pointers. But this time, the
nodes are bump-allocated, amortizing the cost of allocation in much the same way as the <em>flat AST</em>
representation.</p>
<p>We&#39;re in Rust, with compiler-checked lifetimes. Each call to <code>Bump::alloc</code> will yield a <em>reference</em>
to the allocated value. That reference has a lifetime tied to the lifetime of the <code>Bump</code> allocator
it came from. So we&#39;ll have to annotate each syntax node with a <em>lifetime</em> in order to be able to
store them:</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Stmt&lt;&#39;a&gt; {
</span><span>    Fn(&amp;</span><span>&#39;a </span><span>StmtFn&lt;</span><span>&#39;a</span><span>&gt;),
</span><span>    Let(&amp;</span><span>&#39;a </span><span>StmtLet&lt;</span><span>&#39;a</span><span>&gt;),
</span><span>    Expr(&amp;</span><span>&#39;a </span><span>Expr&lt;</span><span>&#39;a</span><span>&gt;),
</span><span>}
</span><span>
</span><span>struct </span><span>StmtFn&lt;</span><span>&#39;a</span><span>&gt; {
</span><span>    </span><span>name</span><span>: IdentId,
</span><span>    </span><span>params</span><span>: Vec&lt;</span><span>&#39;a</span><span>, IdentId&gt;,
</span><span>    </span><span>body</span><span>: Block&lt;</span><span>&#39;a</span><span>&gt;,
</span><span>}
</span></code></pre>
<p>This lifetime is now &#34;infectious&#34;, and every time we allocate or use our AST, we&#39;ll have to contend with it.
Luckily, in most compilers, the lifetimes of intermediate data structures are quite linear:
After you parse, you use the AST, then discard it.</p>
<p>Is it worth it?</p>
<h3 id="bump-allocation-results">Bump allocation results</h3>
<img width="90%" src="https://jhwlr.io/super-flat-ast/3_loc.png"/>
<p>Yeah! Again, a huge improvement.</p>
<p>What about memory usage?</p>
<img width="90%" src="https://jhwlr.io/super-flat-ast/3_memory.png"/>
<p>We&#39;re actually using <em>less</em> memory. That&#39;s because each variant of <code>Stmt</code> and <code>Expr</code> is significantly smaller.</p>
<p>Note that each <code>Vec</code> is a little bit bigger, because it now also stores a reference to the <code>Bump</code> allocator.</p>
<p>I&#39;m convinced that just by adding string interning and bump allocation, our AST is already close to optimal,
given a <em>reasonable</em> level of effort.</p>
<p>But why stop at <em>reasonable</em>? We can do better.</p>
<h2 id="super-flat">Super flat</h2>
<p>Something bothers me about all the previous layouts. We can dictate the relative position of <em>child nodes</em>
in memory, but we still store an index or pointer to each one separately.</p>
<p>For example, given a <code>Binary</code> expression, we need to store two pointers to the <code>lhs</code> and <code>rhs</code> child nodes.</p>
<p>This gets <em>much</em> worse if we need a dynamic number of sub-nodes. Each <code>Vec</code> adds 24 bytes to a
syntax node!</p>
<p>We can take advantage of the fact that our AST nodes fit in one of two categories:</p>
<ul>
<li>Nodes with a <em>static</em> number of child nodes</li>
<li>Nodes with a <em>dynamic</em> number of child nodes</li>
</ul>
<p>Going back to the example of <code>Binary</code>, it&#39;s in the first category, as it always has exactly two child nodes.</p>
<p>What if each node&#39;s child nodes were <em>contiguous</em>? We would only have to store the index of the <em>first child</em>,
and the index of second child and onward would be simply relative to the first. So for <code>Binary</code>:</p>
<ul>
<li><code>lhs</code> is at <code>nodes[index+0]</code></li>
<li><code>rhs</code> is at <code>nodes[index+1]</code></li>
</ul>
<p>To store nodes with a dynamic number of child nodes, we&#39;ll also need to store the number of child nodes in some
kind of <code>length</code> field.</p>
<p>Each node will still have to store what its type is. For that, we&#39;ll use a <code>tag</code> field.</p>
<p>We can pack all of this information into just <em>8 bytes</em>. In pseudo-Rust with arbitrarily-sized integers:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Node {
</span><span>    </span><span>tag</span><span>: </span><span>u8</span><span>,
</span><span>    </span><span>length</span><span>: u24,
</span><span>    </span><span>first_child_index</span><span>: </span><span>u32
</span><span>}
</span></code></pre>
<p>The <code>length</code> and <code>first_child_index</code> fields aren&#39;t always going to be used. Sometimes a node is just its
tag, like a <code>Break</code> expression. For fixed-arity nodes like <code>Binary</code>, we don&#39;t need the <code>length</code>.</p>
<p>We can re-purpose these fields to store additional information:</p>
<ul>
<li>The <code>Binary</code> operator field is a <code>u8</code> enum, so we can encode it into the <code>length</code> field, since it&#39;s unused.</li>
<li>A <code>String</code> doesn&#39;t have any child nodes. We can store the interned string ID (a <code>u32</code>) in the available space!</li>
</ul>
<p>And so on. We can generalize this as <em>fixed-arity</em> nodes can hold a <em>3 byte</em> inline value, and <em>leaf</em> (tag only)
nodes can hold a <em>7 byte</em> inline value.</p>
<p>I don&#39;t know if this has a name already. I&#39;ve been referring to this as a <em>super flat</em> representation.
It&#39;s inspired by <em>flat ASTs</em>, but goes a step further in the flattening.</p>
<h3 id="super-flat-implementation">Super flat implementation</h3>
<p>This kind of representation is a <em>lot</em> more complex than anything we&#39;ve looked at previously. We&#39;ll need
to set up some infrastructure to define these nodes, as manually indexing into a <code>nodes</code> array can be
quite error-prone!</p>
<p>We&#39;ll probably want to generate the required code. I&#39;ve previously done it with <em>offline</em> codegen,
by writing a separate program which outputs a source file containing the AST definitions.</p>
<p>In this case, we&#39;ll use a <em>declarative macro</em>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>declare_node! {
</span><span>    #[</span><span>leaf</span><span>]
</span><span>    </span><span>struct </span><span>ExprStr (value: StrId);
</span><span>}
</span><span>
</span><span>declare_node! {
</span><span>    #[</span><span>tree</span><span>]
</span><span>    </span><span>struct </span><span>ExprBinary (op: BinaryOp) {
</span><span>        lhs: Expr = </span><span>0</span><span>,
</span><span>        rhs: Expr = </span><span>1</span><span>,
</span><span>    }
</span><span>}
</span><span>
</span><span>declare_node! {
</span><span>    #[</span><span>dyn_tree</span><span>]
</span><span>    </span><span>struct </span><span>ExprBlock {
</span><span>        #[</span><span>tail</span><span>]
</span><span>        </span><span>body</span><span>: [Stmt],
</span><span>    }
</span><span>}
</span><span>
</span></code></pre>
<p>The syntax here mimics Rust&#39;s struct syntax, but as you can tell, it&#39;s not quite the same.</p>
<p>A string expression has no child nodes, only an inline
<em>string id</em>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>leaf</span><span>]
</span><span>struct </span><span>ExprStr (value: StrId);
</span></code></pre>
<p>A binary expression has two child nodes:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>tree</span><span>]
</span><span>struct </span><span>ExprBinary (op: BinaryOp) {
</span><span>    lhs: Expr = </span><span>0</span><span>,
</span><span>    rhs: Expr = </span><span>1</span><span>,
</span><span>}
</span></code></pre>
<p>Due to the limitations of declarative macros (or maybe just a skill issue on my side),
we&#39;ll have to tell the macro which field belongs under which index.</p>
<p>Block expressions have a dynamic number of child nodes:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>dyn_tree</span><span>]
</span><span>struct </span><span>ExprBlock {
</span><span>    #[</span><span>tail</span><span>]
</span><span>    </span><span>body</span><span>: [Stmt],
</span><span>}
</span></code></pre>
<p>The macro expands to a function which <em>packs</em> the node into the AST,
and a few <em>getters</em> which retrieve child nodes.</p>
<p>There&#39;s also some miscellaneous runtime machinery to make it all work.</p>
<details><summary>Macro expansion</summary>
<p><strong>Block expression</strong>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>repr</span><span>(transparent)]
</span><span>pub struct </span><span>ExprBlock {
</span><span>    </span><span>packed</span><span>: Packed,
</span><span>}
</span><span>
</span><span>impl </span><span>ExprBlock {
</span><span>    </span><span>const </span><span>NUM_STATIC_FIELDS</span><span>: </span><span>usize </span><span>= </span><span>1 </span><span>+ </span><span>0</span><span>;
</span><span>    </span><span>pub fn </span><span>pack</span><span>(</span><span>ast</span><span>: &amp;</span><span>mut</span><span> AstBuilder, </span><span>tail</span><span>: Opt&lt;Expr&gt;, </span><span>body</span><span>: &amp;[Stmt]) -&gt; </span><span>Self </span><span>{
</span><span>        </span><span>let</span><span> index = ast.nodes.</span><span>len</span><span>() as </span><span>u64</span><span>;
</span><span>        </span><span>if </span><span>!(index &lt;= </span><span>u32</span><span>::</span><span>MAX </span><span>as </span><span>u64</span><span>) {
</span><span>            ::core::panicking::panic(&#34;</span><span>assertion failed: index &lt;= u32::MAX as u64</span><span>&#34;)
</span><span>        }
</span><span>        </span><span>let</span><span> index = index as </span><span>u32</span><span>;
</span><span>        </span><span>let</span><span> len = body.</span><span>len</span><span>() as </span><span>u64</span><span>;
</span><span>        </span><span>if </span><span>!(len &lt;= </span><span>U24_MASK</span><span>) {
</span><span>            ::core::panicking::panic(&#34;</span><span>assertion failed: len &lt;= U24_MASK</span><span>&#34;)
</span><span>        }
</span><span>        </span><span>let</span><span> len = len as </span><span>u32</span><span>;
</span><span>        ast.nodes.</span><span>push</span><span>(tail.packed);
</span><span>        </span><span>for</span><span> node in body {
</span><span>            ast.nodes.</span><span>push</span><span>(node.packed);
</span><span>        }
</span><span>        </span><span>Self </span><span>{
</span><span>            packed: DynTree::new(Tag::ExprBlock as </span><span>u8</span><span>, index, len).packed,
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>pub fn </span><span>tail</span><span>(</span><span>self</span><span>, </span><span>ast</span><span>: &amp;impl NodeStorage) -&gt; Opt&lt;Expr&gt; {
</span><span>        </span><span>if </span><span>true </span><span>{
</span><span>            </span><span>if </span><span>!</span><span>Self</span><span>::check_tag(</span><span>self</span><span>.packed.</span><span>tag</span><span>()) {
</span><span>                ::core::panicking::panic(
</span><span>                    &#34;</span><span>assertion failed: Self::check_tag(self.packed.tag())</span><span>&#34;,
</span><span>                )
</span><span>            }
</span><span>        }
</span><span>        </span><span>let</span><span> index = </span><span>self</span><span>.packed.</span><span>dyn_tree</span><span>().</span><span>child</span><span>() as </span><span>usize </span><span>+ </span><span>0</span><span>;
</span><span>        </span><span>let</span><span> node = ast.</span><span>get</span><span>(index).</span><span>unwrap</span><span>();
</span><span>        </span><span>unsafe </span><span>{ </span><span>transmute_node</span><span>(node) }
</span><span>    }
</span><span>    
</span><span>    </span><span>pub fn </span><span>body</span><span>(</span><span>self</span><span>, </span><span>ast</span><span>: &amp;impl NodeStorage) -&gt; &amp;[Stmt] {
</span><span>        </span><span>if </span><span>true </span><span>{
</span><span>            </span><span>if </span><span>!</span><span>Self</span><span>::check_tag(</span><span>self</span><span>.packed.</span><span>tag</span><span>()) {
</span><span>                ::core::panicking::panic(
</span><span>                    &#34;</span><span>assertion failed: Self::check_tag(self.packed.tag())</span><span>&#34;,
</span><span>                )
</span><span>            }
</span><span>        }
</span><span>        </span><span>let</span><span> index = </span><span>self</span><span>.packed.</span><span>dyn_tree</span><span>().</span><span>child</span><span>() as </span><span>usize
</span><span>            + </span><span>Self</span><span>::</span><span>NUM_STATIC_FIELDS</span><span>;
</span><span>        </span><span>let</span><span> len = </span><span>self</span><span>.packed.</span><span>dyn_tree</span><span>().</span><span>len</span><span>() as </span><span>usize</span><span>;
</span><span>        </span><span>let</span><span> nodes = ast.</span><span>get_slice</span><span>(index..index + len).</span><span>unwrap</span><span>();
</span><span>        </span><span>unsafe </span><span>{ </span><span>transmute_node_slice</span><span>(nodes) }
</span><span>    }
</span><span>}
</span></code></pre>
<p><strong>Binary expression</strong>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub struct </span><span>ExprBinary {
</span><span>    </span><span>packed</span><span>: Packed,
</span><span>}
</span><span>
</span><span>impl </span><span>ExprBinary {
</span><span>    </span><span>pub fn </span><span>pack</span><span>(</span><span>ast</span><span>: &amp;</span><span>mut</span><span> AstBuilder, </span><span>lhs</span><span>: Expr, </span><span>rhs</span><span>: Expr, </span><span>op</span><span>: BinaryOp) -&gt; </span><span>Self </span><span>{
</span><span>        </span><span>let</span><span> index = ast.nodes.</span><span>len</span><span>() as </span><span>u64</span><span>;
</span><span>        </span><span>if </span><span>!(index &lt;= </span><span>u32</span><span>::</span><span>MAX </span><span>as </span><span>u64</span><span>) {
</span><span>            ::core::panicking::panic(&#34;</span><span>assertion failed: index &lt;= u32::MAX as u64</span><span>&#34;)
</span><span>        }
</span><span>        </span><span>let</span><span> index = index as </span><span>u32</span><span>;
</span><span>        </span><span>let</span><span> value = op.</span><span>into_u24</span><span>();
</span><span>        </span><span>if </span><span>!(value &lt;= </span><span>U24_MASK </span><span>as </span><span>u32</span><span>) {
</span><span>            ::core::panicking::panic(&#34;</span><span>assertion failed: value &lt;= U24_MASK as u32</span><span>&#34;)
</span><span>        }
</span><span>        ast.nodes.</span><span>push</span><span>(lhs.packed);
</span><span>        ast.nodes.</span><span>push</span><span>(rhs.packed);
</span><span>        </span><span>Self </span><span>{
</span><span>            packed: Tree::new(Tag::ExprBinary as </span><span>u8</span><span>, index, value).packed,
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>pub fn </span><span>lhs</span><span>(</span><span>self</span><span>, </span><span>ast</span><span>: &amp;impl NodeStorage) -&gt; Expr {
</span><span>        </span><span>if </span><span>true </span><span>{
</span><span>            </span><span>if </span><span>!</span><span>Self</span><span>::check_tag(</span><span>self</span><span>.packed.</span><span>tag</span><span>()) {
</span><span>                ::core::panicking::panic(
</span><span>                    &#34;</span><span>assertion failed: Self::check_tag(self.packed.tag())</span><span>&#34;,
</span><span>                )
</span><span>            }
</span><span>        }
</span><span>        </span><span>let</span><span> index = </span><span>self</span><span>.packed.</span><span>tree</span><span>().</span><span>child</span><span>() as </span><span>usize </span><span>+ </span><span>0</span><span>;
</span><span>        </span><span>let</span><span> node = ast.</span><span>get</span><span>(index).</span><span>unwrap</span><span>();
</span><span>        </span><span>unsafe </span><span>{ </span><span>transmute_node</span><span>(node) }
</span><span>    }
</span><span>
</span><span>    </span><span>pub fn </span><span>rhs</span><span>(</span><span>self</span><span>, </span><span>ast</span><span>: &amp;impl NodeStorage) -&gt; Expr {
</span><span>        </span><span>if </span><span>true </span><span>{
</span><span>            </span><span>if </span><span>!</span><span>Self</span><span>::check_tag(</span><span>self</span><span>.packed.</span><span>tag</span><span>()) {
</span><span>                ::core::panicking::panic(
</span><span>                    &#34;</span><span>assertion failed: Self::check_tag(self.packed.tag())</span><span>&#34;,
</span><span>                )
</span><span>            }
</span><span>        }
</span><span>        </span><span>let</span><span> index = </span><span>self</span><span>.packed.</span><span>tree</span><span>().</span><span>child</span><span>() as </span><span>usize </span><span>+ </span><span>1</span><span>;
</span><span>        </span><span>let</span><span> node = ast.</span><span>get</span><span>(index).</span><span>unwrap</span><span>();
</span><span>        </span><span>unsafe </span><span>{ </span><span>transmute_node</span><span>(node) }
</span><span>    }
</span><span>
</span><span>    </span><span>pub fn </span><span>op</span><span>(</span><span>self</span><span>) -&gt; BinaryOp {
</span><span>        </span><span>if </span><span>true </span><span>{
</span><span>            </span><span>if </span><span>!</span><span>Self</span><span>::check_tag(</span><span>self</span><span>.packed.</span><span>tag</span><span>()) {
</span><span>                ::core::panicking::panic(
</span><span>                    &#34;</span><span>assertion failed: Self::check_tag(self.packed.tag())</span><span>&#34;,
</span><span>                )
</span><span>            }
</span><span>        }
</span><span>        </span><span>let</span><span> value = </span><span>self</span><span>.packed.</span><span>tree</span><span>().</span><span>value</span><span>();
</span><span>        &lt;BinaryOp as Inline24&gt;::from_u24(value)
</span><span>    }
</span><span>}
</span><span>
</span></code></pre>
<p><strong>String expression</strong>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[</span><span>repr</span><span>(transparent)]
</span><span>pub struct </span><span>ExprStr {
</span><span>    </span><span>packed</span><span>: Packed,
</span><span>}
</span><span>
</span><span>impl </span><span>ExprStr {
</span><span>    </span><span>pub fn </span><span>pack</span><span>(</span><span>value</span><span>: StrId) -&gt; </span><span>Self </span><span>{
</span><span>        </span><span>let</span><span> value = value.</span><span>into_u56</span><span>();
</span><span>        </span><span>if </span><span>!(value &lt;= </span><span>U56_MASK</span><span>) {
</span><span>            ::core::panicking::panic(&#34;</span><span>assertion failed: value &lt;= U56_MASK</span><span>&#34;)
</span><span>        }
</span><span>        </span><span>Self </span><span>{
</span><span>            packed: Leaf::new(Tag::ExprStr as </span><span>u8</span><span>, value).packed,
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>pub fn </span><span>value</span><span>(</span><span>self</span><span>) -&gt; StrId {
</span><span>        </span><span>if </span><span>true </span><span>{
</span><span>            </span><span>if </span><span>!</span><span>Self</span><span>::check_tag(</span><span>self</span><span>.packed.</span><span>tag</span><span>()) {
</span><span>                ::core::panicking::panic(
</span><span>                    &#34;</span><span>assertion failed: Self::check_tag(self.packed.tag())</span><span>&#34;,
</span><span>                )
</span><span>            }
</span><span>        }
</span><span>        </span><span>let</span><span> value = </span><span>self</span><span>.packed.</span><span>leaf</span><span>().</span><span>value</span><span>();
</span><span>        &lt;StrId as Inline56&gt;::from_u56(value)
</span><span>    }
</span><span>}
</span></code></pre>
</details>
<p>You may notice some <em>unsafe code</em> in the implementation. We&#39;re doing something that the Rust
compiler can&#39;t handle for us, so we take matters into our own hands. We know that the AST
is correct by construction, so we <em>assume</em> that a given node&#39;s child nodes exist, and they
are the correct type.</p>
<p>This was a <em>lot</em> of work. How does it perform?</p>
<h3 id="super-flat-results">Super flat results</h3>
<img width="90%" src="https://jhwlr.io/super-flat-ast/4_loc.png"/>
<p>To me, this isn&#39;t a surprise. Look at the next graph:</p>
<img width="90%" src="https://jhwlr.io/super-flat-ast/4_memory.png"/>
<p>A traditional tree representation uses <em>3x more</em> memory than our super flat representation.</p>
<p>Of course, you don&#39;t usually write 100 MB large files. Luckily, it performs better at every
scale.</p>
<h2 id="fin">Fin</h2>
<p>Thanks for reading!</p>
<p>As always, the code is <a href="https://github.com/jprochazk/simp">available on GitHub</a>,
this time including the benchmarks and plot generation.</p>


            </div></div>
  </body>
</html>
