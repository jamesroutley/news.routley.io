<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.newyorker.com/magazine/2023/04/03/the-data-delusion">Original</a>
    <h1>The Data Delusion</h1>
    
    <div id="readability-page-1" class="page"><div><div><div data-journey-hook="client-content" data-testid="BodyWrapper"><div><p>And why bother? By the nineteen-thirties, the fantasy of technological supremacy had found its fullest expression in the Technocracy movement, which, during the Depression, vied with socialism and fascism as an alternative to capitalism and liberal democracy. “Technocracy, briefly stated, is the application of science to the social order,” a pamphlet called “Technocracy in Plain Terms” explained in 1939. Technocrats proposed the abolition of all existing economic and political arrangements—governments and banks, for instance—and their replacement by engineers, who would rule by numbers. “Money cannot be used, and its function of purchasing must be replaced by a scientific unit of measurement,” the pamphlet elaborated, assuring doubters that nearly everyone “would probably come to like living under a Technate.” Under the Technate, humans would no longer need names; they would have numbers. (One Technocrat called himself 1x1809x56.) They dressed in gray suits and drove gray cars. If this sounds familiar—tech bros and their gray hoodies and silver Teslas, cryptocurrency and the abolition of currency—it should. As a political movement, Technocracy fell out of favor in the nineteen-forties, but its logic stuck around. Elon Musk’s grandfather was a leader of the Technocracy movement in Canada; he was arrested for being a member, and then, soon after South Africa announced its new policy of apartheid, he moved to Pretoria, where <a href="https://www.newyorker.com/tag/elon-musk">Elon Musk</a> was born, in 1971. One of Musk’s children is named <em>X Æ A</em>-12. Welcome to the Technate.</p><p>The move from a culture of numbers to a culture of data began during the Second World War, when statistics became more mathematical, largely for the sake of becoming more predictive, which was necessary for wartime applications involving everything from calculating missile trajectories to cracking codes. “This was not data in search of latent truths about humanity or nature,” Wiggins and Jones write. “This was not data from small experiments, recorded in small notebooks. This was data motivated by a pressing need—to supply answers in short order that could spur action and save lives.” That work continued during the Cold War, as an instrument of the national-security state. Mathematical modelling, increased data-storage capacity, and computer simulation all contributed to the pattern detection and prediction in classified intelligence work, military research, social science, and, increasingly, commerce.</p><p>Despite the benefit that these tools provided, especially to researchers in the physical and natural sciences—in the study of stars, say, or molecules—scholars in other fields lamented the distorting effect on their disciplines. In 1954, Claude Lévi-Strauss argued that social scientists need “to break away from the hopelessness of the ‘great numbers’—the raft to which the social sciences, lost in an ocean of figures, have been helplessly clinging.” By then, national funding agencies had shifted their priorities. The Ford Foundation announced that although it was interested in the human mind, it was no longer keen on non-predictive research in fields like philosophy and political theory, deriding such disciplines as “polemical, speculative, and pre-scientific.” The best research would be, like physics, based on “experiment, the accumulation of data, the framing of general theories, attempts to verify the theories, and prediction.” Economics and political science became predictive sciences; other ways of knowing in those fields atrophied.</p><p>The digitization of human knowledge proceeded apace, with libraries turning books first into microfiche and microfilm and then—through optical character recognition, whose origins date to the nineteen-thirties—into bits and bytes. The field of artificial intelligence, founded in the nineteen-fifties, at first attempted to sift through evidence in order to identify the rules by which humans reason. This approach hit a wall, in a moment known as “the knowledge acquisition bottleneck.” The breakthrough came with advances in processing power and the idea of using the vast stores of data that had for decades been compounding in the worlds of both government and industry to teach machines to teach themselves by detecting patterns: machines, learning. “Spies pioneered large-scale data storage,” Wiggins and Jones write, but, “starting with the data from airline reservations systems in the 1960s, industry began accumulating data about customers at a rapidly accelerating rate,” collecting everything from credit-card transactions and car rentals to library checkout records. In 1962, John Tukey, a mathematician at Bell Labs, called for a new approach that he termed “data analysis,” the ancestor of today’s “data science.” It has its origins in intelligence work and the drive to anticipate the Soviets: what would they do next? That Netflix can predict what you want to watch, that Google knows which sites to serve you—these miracles are the result of tools developed by spies during the Cold War. Commerce in the twenty-first century is espionage for profit.</p><p>While all this was going on—the accumulation of data, the emergence of machine learning, and the use of computers not only to calculate but also to communicate—the best thinkers of the age wondered what it might mean for humanity down the line. In 1965, the brilliant and far-seeing engineer J. C. R. Licklider, a chief pioneer of the early Internet, wrote “Libraries of the Future,” in which he considered the many disadvantages of books. “If human interaction with the body of knowledge is conceived of as a dynamic process involving repeated examinations and intercomparisons of very many small and scattered parts, then any concept of a library that begins with books on shelves is sure to encounter trouble,” Licklider wrote. “Surveying a million books on ten thousand shelves,” he explained, is a nightmare. “When information is stored in books, there is no practical way to transfer the information from the store to the user without physically moving the book or the reader or both.” But convert books into data that can be read by a computer, and you can move data from storage to the user, and to any number of users, much more easily. Taking the contents of all the books held in the Library of Congress as a proxy for the sum total of human knowledge, he considered several estimates of its size and figured that it was doubling every couple of decades. On the basis of these numbers, the sum total of human knowledge, as data, would, in the year 2020, be about a dozen petabytes. A zettabyte is a petabyte with six more zeroes after it. So Licklider, who really was a genius, was off by a factor of a hundred thousand.</p><p>Consider even the billions of documents that the U.S. government deems “classified,” a number that increases by fifty million every year. Good-faith research suggests that as many as nine out of ten of these documents really shouldn’t be classified. Unfortunately, no one is making much headway in declassifying them (thousands of documents relating to J.F.K.’s assassination, in 1963, for instance, remain classified). That is a problem for the proper working of government, and for the writing of history, and, not least, for former Presidents and Vice-Presidents.</p><p>In “<a data-offer-url="https://www.amazon.com/Declassification-Engine-History-Reveals-Americas/dp/1101871571/ref=sr_1_1?crid=RS1NGL2KTMQX&amp;keywords=The%20Declassification%20Engine:%20What%20History%20Reveals%20About%20America%E2%80%99s%20Top%20Secrets&amp;qid=1679930707&amp;s=books&amp;sprefix=the%20declassification%20engine%20what%20history%20reveals%20about%20america%20s%20top%20secrets,stripbooks,188&amp;sr=1-1" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://www.amazon.com/Declassification-Engine-History-Reveals-Americas/dp/1101871571/ref=sr_1_1?crid=RS1NGL2KTMQX&amp;keywords=The%20Declassification%20Engine:%20What%20History%20Reveals%20About%20America%E2%80%99s%20Top%20Secrets&amp;qid=1679930707&amp;s=books&amp;sprefix=the%20declassification%20engine%20what%20history%20reveals%20about%20america%20s%20top%20secrets,stripbooks,188&amp;sr=1-1&#34;}" href="https://www.amazon.com/Declassification-Engine-History-Reveals-Americas/dp/1101871571/ref=sr_1_1?crid=RS1NGL2KTMQX&amp;keywords=The%20Declassification%20Engine:%20What%20History%20Reveals%20About%20America%E2%80%99s%20Top%20Secrets&amp;qid=1679930707&amp;s=books&amp;sprefix=the%20declassification%20engine%20what%20history%20reveals%20about%20america%20s%20top%20secrets,stripbooks,188&amp;sr=1-1" rel="nofollow noopener" target="_blank">The Declassification Engine: What History Reveals About America’s Top Secrets</a>” (Pantheon), the historian Matthew Connelly uses tools first developed for intelligence and counterintelligence purposes—traffic analysis, anomaly detection, and the like—to build what he calls a “declassification engine,” a “technology that could help identify truly sensitive information,” speed up the declassification of everything else, and, along the way, produce important historical insights. (Connelly, like Wiggins and Jones, is affiliated with Columbia’s Data Science Institute.)</p><p>The problem is urgent and the project is promising; the results can be underwhelming. After scanning millions of declassified documents from the State Department’s “Foreign Relations of the United States” series, for instance, Connelly and his team identified the words most likely to appear before or after redacted text, and found that “Henry Kissinger’s name appears more than twice as often as anyone else’s.” (Kissinger, who was famously secretive, was the Secretary of State from 1973 to 1977.) This is a little like building a mapping tool, setting it loose on Google Earth, and concluding that there are more driveways in the suburbs than there are in the city.</p><p>By the beginning of the twenty-first century, commercial, governmental, and academic analysis of data had come to be defined as “data science.” From being just one tool with which to produce knowledge, it has become, in many quarters, the only tool. On college campuses across the country, data-science courses and institutes and entire data-science schools are popping up like dandelions in spring, and data scientist is one of the fastest-growing employment categories in the United States. The emergence of a new discipline is thrilling, and it would be even more thrilling if people were still opening all four drawers of that four-drawer filing cabinet, instead of renouncing all other ways of knowing. Wiggins and Jones are careful to note this hazard. “At its most hubristic, data science is presented as a master discipline, capable of reorienting the sciences, the commercial world, and governance itself,” they write.</p><p>It’s easy to think of the ills produced by the hubristic enthusiasm for numbers a century ago, from the I.Q. to the G.D.P. It’s easy, too, to think of the ills produced by the hubristic enthusiasm for data today, and for artificial intelligence (including in a part of the Bay Area now known as Cerebral Valley). The worst of those ills most often have to do with making predictions about human behavior and apportioning resources accordingly: using algorithms to set bail or sentences for people accused or convicted of crimes, for instance. Connelly proposes that the computational examination of declassified documents could serve as “the functional equivalent of CT scans and magnetic resonance imaging to examine the body politic.” He argues that “history as a data science has to prove itself in the most rigorous way possible: by making predictions about what newly available sources will reveal.” But history is not a predictive science, and if it were it wouldn’t be history. Legal scholars are making this same move. In “<a data-offer-url="https://www.amazon.com/Equality-Machine-Harnessing-Technology-Inclusive/dp/1541774752/ref=sr_1_1?crid=4T84VMXDBAOH&amp;keywords=The%20Equality%20Machine:%20Harnessing%20Digital%20Technology%20for%20a%20Brighter,%20More%20Inclusive%20Future&amp;qid=1679930761&amp;s=books&amp;sprefix=the%20equality%20machine%20harnessing%20digital%20technology%20for%20a%20brighter,%20more%20inclusive%20future,stripbooks,91&amp;sr=1-1" data-event-click="{&#34;element&#34;:&#34;ExternalLink&#34;,&#34;outgoingURL&#34;:&#34;https://www.amazon.com/Equality-Machine-Harnessing-Technology-Inclusive/dp/1541774752/ref=sr_1_1?crid=4T84VMXDBAOH&amp;keywords=The%20Equality%20Machine:%20Harnessing%20Digital%20Technology%20for%20a%20Brighter,%20More%20Inclusive%20Future&amp;qid=1679930761&amp;s=books&amp;sprefix=the%20equality%20machine%20harnessing%20digital%20technology%20for%20a%20brighter,%20more%20inclusive%20future,stripbooks,91&amp;sr=1-1&#34;}" href="https://www.amazon.com/Equality-Machine-Harnessing-Technology-Inclusive/dp/1541774752/ref=sr_1_1?crid=4T84VMXDBAOH&amp;keywords=The%20Equality%20Machine:%20Harnessing%20Digital%20Technology%20for%20a%20Brighter,%20More%20Inclusive%20Future&amp;qid=1679930761&amp;s=books&amp;sprefix=the%20equality%20machine%20harnessing%20digital%20technology%20for%20a%20brighter,%20more%20inclusive%20future,stripbooks,91&amp;sr=1-1" rel="nofollow noopener" target="_blank">The Equality Machine: Harnessing Digital Technology for a Brighter, More Inclusive Future</a>” (PublicAffairs), Orly Lobel, a University of San Diego law professor, argues that the solution to biases in algorithms is to write better algorithms. Fair enough, except that the result is still rule by algorithms. What if we stopped clinging to the raft of data, returned to the ocean of mystery, and went fishing for facts?</p><p>In 1997, when Sergey Brin was a graduate student at Stanford, he wrote a Listserv message about the possible malign consequences of detecting patterns in data and using them to make predictions about human behavior. He had a vague notion that discrimination was among the likely “results of data mining.” He considered the insurance industry. “Auto insurance companies analyse accident data and set insurance rates of individuals according to age, gender, vehicle type,” he pointed out. “If they were allowed to by law, they would also use race, religion, handicap, and any other attributes they find are related to accident rate.” Insurers have been minimizing risk since before the Code of Hammurabi, nearly four thousand years ago. It’s an awfully interesting story, but for Brin this was clearly a fleeting thought, not the beginning of an investigation into history, language, philosophy, and ethics. All he knew was that he didn’t want to make the world worse. “Don’t be evil” became Google’s model. But, if you put people’s brains in glass jars and burn all your books, bad things do tend to happen. ♦</p></div></div></div></div></div>
  </body>
</html>
