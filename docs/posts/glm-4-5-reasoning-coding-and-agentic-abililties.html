<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://z.ai/blog/glm-4.5">Original</a>
    <h1>GLM-4.5: Reasoning, Coding, and Agentic Abililties</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>2025-07-28 ¬∑ Research</p><p>Our latest frontier model series, excels in reasoning, coding and agentic tasks.</p></div><div><p>Today, we introduce two new GLM family members: GLM-4.5 and GLM-4.5-Air ‚Äî our latest flagship models. GLM-4.5 is built with 355 billion total parameters and 32 billion active parameters, and GLM-4.5-Air with 106 billion total parameters and 12 billion active parameters. Both are designed to unify reasoning, coding, and agentic capabilities into a single model in order to satisfy more and more complicated requirements of fast rising agentic applications.</p><p>Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models, offering:<!-- --> <em>thinking</em> mode for complex reasoning and tool using, and<!-- --> <em>non-thinking</em> mode for instant responses. They are available on<!-- --> <a href="https://chat.z.ai" target="_blank">Z.ai</a>,<!-- --> <a href="https://docs.z.ai/guides/llm/glm-4.5" target="_blank">Z.ai API</a> <!-- -->and open-weights are avaiable at<!-- --> <a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b" target="_blank">HuggingFace</a> <!-- -->and<!-- --> <a href="https://modelscope.cn/collections/GLM-45-b8693e2a08984f" target="_blank">ModelScope</a>.</p><p><strong>Background:</strong> LLM always targets at achieving human-level cognitive capabilities across a wide range of domains, rather than designed for specific tasks. As a good LLM model, it is necessary to deal with general problem solving, generalization, commen sense reasoning, and self-improvement. In the past five years, OpenAI&#39;s GPT-3 learns commen-sense knowledge, and o1 uses reinforcement learning to think before respond, significantly improving reasoning skills in coding, data analysis, and complex math. However, the resultant models are still not really general: some of them are good at coding, some good at math, and some good at reasoning, but none of them could achieve the best performance across all the different tasks. GLM-4.5 makes efforts toward the goal of unifying all the different capabilities.</p><p>We compare GLM-4.5 with various models from OpenAI, Anthropic, Google DeepMind, xAI, Alibaba, Moonshot, and DeepSeek on 12 benchmarks covering agentic (3), reasoning (7), and Coding (2). Overall, GLM-4.5 is ranked at the 3rd place and GLM-4.5 Air is ranked at the 6th.</p></div><p><img src="https://z-cdn.chatglm.cn/z-blog/SylKZkHPge.png"/></p><div><p><strong>Agentic Tasks</strong></p><p>GLM-4.5 is a foundation model optimized for agentic tasks. It provides 128k context length and native function calling capacity. We measure its agent ability on ùúè-bench and BFCL-v3 (Berkeley Function Calling Leaderboard v3). On both benchmarks, GLM-4.5 matches the performance of Claude 4 Sonnet.</p></div><p><img src="https://z-cdn.chatglm.cn/z-blog/HJ4VZkBDxe.jpg"/></p><p>Web browsing is a popular agentic application that requires complex reasoning and multi-turn tool using. We evaluate GLM-4.5 on the<!-- --> <a href="https://openai.com/index/browsecomp" target="_blank">BrowseComp</a> <!-- -->benchmark, a challenging benchmark for web browsing that consists of complicated questions that expect short answers. With access to the web browsing tool, GLM-4.5 gives correct answers for 26.4% of all questions, clearly outperforming Claude-4-Opus (18.8%) and close to o4-mini-high (28.3%). Below the figure shows the test-time scaling accuracy of GLM-4.5 on the BrowseComp.</p><p><img src="https://z-cdn.chatglm.cn/z-blog/HyiDl_Ewge.jpg"/></p><div><p>All the detailed results of different comparison models on the three Benchmarks used for eveluating model agent ability are listed in the following table.</p><div><table><thead><tr><th>Benchmark</th><th>GLM-4.5</th><th>GLM-4.5-Air</th><th>o3</th><th>o4-mini-high</th><th>GPT-4.1</th><th>Claude 4 Opus</th><th>Claude 4 Sonnet</th><th>Gemini 2.5 Pro</th><th>Qwen3 235B Thinking 2507</th><th>DeepSeek-R1-0528</th><th>DeepSeek V3 0324</th><th>Kimi K2</th><th>Grok 4</th></tr></thead><tbody><tr><td>TAU-bench</td><td>70.1</td><td>69.4</td><td>61.2</td><td>57.4</td><td>62.0</td><td>70.5</td><td>70.3</td><td>62.5</td><td>73.2</td><td>58.7</td><td>57.6</td><td>62.6</td><td>67.5</td></tr><tr><td>BFCL v3 (Full)</td><td>77.8</td><td>76.4</td><td>72.4</td><td>67.2</td><td>68.9</td><td>61.8</td><td>75.2</td><td>61.2</td><td>72.4</td><td>63.8</td><td>64.7</td><td>71.1</td><td>66.2</td></tr><tr><td>BrowseComp</td><td>26.4</td><td>21.3</td><td>49.7</td><td>28.3</td><td>4.1</td><td>18.8</td><td>14.7</td><td>7.6</td><td>4.6</td><td>3.2</td><td>1.5</td><td>7.9</td><td>32.6</td></tr></tbody></table></div><p><strong>Reasoning</strong></p><p>Under the thinking mode, GLM-4.5 and GLM-4.5-Air can solve complex reasoning problems including mathematics, science, and logical problems.</p><div><table><thead><tr><th>Benchmark</th><th>GLM-4.5</th><th>GLM-4.5-Air</th><th>o3</th><th>Claude 4 Opus</th><th>Gemini 2.5 Pro</th><th>DeepSeek-R1-0528</th><th>Qwen3-235B-Thinking 2507</th><th>Grok 4</th></tr></thead><tbody><tr><td>MMLU Pro</td><td>84.6</td><td>81.4</td><td>85.3</td><td>87.3</td><td>86.2</td><td>84.9</td><td>84.5</td><td>86.6</td></tr><tr><td>AIME24</td><td>91.0</td><td>89.4</td><td>90.3</td><td>75.7</td><td>88.7</td><td>89.3</td><td>94.1</td><td>94.3</td></tr><tr><td>MATH 500</td><td>98.2</td><td>98.1</td><td>99.2</td><td>98.2</td><td>96.7</td><td>98.3</td><td>98.0</td><td>99.0</td></tr><tr><td>SciCode</td><td>41.7</td><td>37.3</td><td>41.0</td><td>39.8</td><td>42.8</td><td>40.3</td><td>42.9</td><td>45.7</td></tr><tr><td>GPQA</td><td>79.1</td><td>75.0</td><td>82.7</td><td>79.6</td><td>84.4</td><td>81.3</td><td>81.1</td><td>87.7</td></tr><tr><td>HLE</td><td>14.4</td><td>10.6</td><td>20.0</td><td>11.7</td><td>21.1</td><td>14.9</td><td>15.8</td><td>23.9</td></tr><tr><td>LiveCodeBench (2407-2501)</td><td>72.9</td><td>70.7</td><td>78.4</td><td>63.6</td><td>80.1</td><td>77.0</td><td>78.2</td><td>81.9</td></tr><tr><td>AA-Index (Estimated)</td><td>67.7</td><td>64.8</td><td>70.0</td><td>64.4</td><td>70.5</td><td>68.3</td><td>69.4</td><td>73.2</td></tr></tbody></table></div><blockquote><p>For the AIME and GPQA benchmarks, we report the average accuracy over 32 and 8 samples respectively (Avg@32, Avg@8) to mitigate result variance. An LLM was used for automated answer validation. For the HLE benchmark, only the text-based questions were evaluated, with correctness judged by gpt-4o.</p></blockquote><p><strong>Coding</strong></p><p>GLM-4.5 excels at coding, including both building coding projects from scratch and agentically solving coding tasks in existing projects. It can be seamlessly combined with existing coding toolkits such as<!-- --> <a href="https://github.com/anthropics/claude-code" target="_blank">Claude Code</a>,<!-- --> <a href="https://github.com/RooCodeInc/Roo-Code" target="_blank">Roo Code</a>, and<!-- --> <a href="https://codegeex.cn/" target="_blank">CodeGeex</a>. To evaluate the coding capability, we compared different models on SWE-bench Verified and Terminal Bench. The following table presents the results.</p><div><table><thead><tr><th>Benchmark</th><th>GLM-4.5</th><th>GLM-4.5-Air</th><th>o3</th><th>GPT-4.1</th><th>Claude 4 Opus</th><th>Claude 4 Sonnet</th><th>Gemini 2.5 Pro</th><th>DeepSeek-R1-0528</th><th>Kimi K2</th></tr></thead><tbody><tr><td>SWE-bench Verified<sup>1</sup></td><td>64.2</td><td>57.6</td><td>69.1</td><td>48.6</td><td>67.8</td><td>70.4</td><td>49.0</td><td>41.4</td><td>65.4</td></tr><tr><td>Terminal-Bench<sup>2</sup></td><td>37.5</td><td>30</td><td>30.2</td><td>30.3</td><td>43.2</td><td>35.5</td><td>25.3</td><td>17.5</td><td>25.0</td></tr></tbody></table></div><blockquote><p><sup>1</sup> For SWE-bench Verified, we use OpenHands v0.34.0 with runs limited to 100 iterations and history truncation to prevent exceeding the 128K context limit, configured with temperature=0.6, top_p=1.0.</p><p><sup>2</sup> For Terminal-Bench, we use the Terminus framework for evaluation. We use standard function calling rather than direct prompting for evaluation.</p></blockquote><p>We conducted a Pareto Frontier analysis for all comparison models (as illustrated in the figure below). GLM-4.5 and GLM-4.5-Air demonstrate superior performance relative to models of comparable scale, achieving optimal efficiency on the performance-scale trade-off boundary.</p></div><p><img src="https://z-cdn.chatglm.cn/z-blog/S1tGskrPge.jpg"/></p><p>GLM-4.5 demonstrates comprehensive full-stack development capabilities, enabling seamless creation of web applications that encompass frontend implementation, database management, and backend deployment. The frontend interfaces generated by GLM-4.5 exhibit enhanced functionality and aesthetic appeal, demonstrating strong alignment with human design preferences. Furthermore, GLM-4.5 exhibits superior performance in generating presentation materials, including slides and posters, with capabilities significantly augmented when integrated with agentic tools for information retrieval and contextual enhancement.</p><p><img src="https://z-cdn.chatglm.cn/z-blog/ByZG-kBvgg.jpg"/></p><p>To assess GLM-4.5&#39;s agentic coding capabilities, we utilized Claude Code to evaluate performance against Claude-4-Sonnet, Kimi K2, and Qwen3-Coder across 52 coding tasks spanning frontend development, tool development, data analysis, testing, and algorithm implementation. All evaluations were performed in isolated testing environments through multi-round human interaction with standardized evaluation criteria to ensure consistency and reproducibility. The empirical results demonstrate that GLM-4.5 achieves a 53.9% win rate against Kimi K2 and exhibits dominant performance over Qwen3-Coder with an 80.8% success rate. While GLM-4.5 shows competitive performance, further optimization opportunities remain when compared to Claude-4-Sonnet.</p><p><img src="https://z-cdn.chatglm.cn/z-blog/H1jQbyHvlx.jpg"/></p><div><p>Notably, GLM-4.5 achieves the highest average tool calling success rate at 90.6%, outperforming Claude-4-Sonnet (89.5%), Kimi-K2 (86.2%), and Qwen3-Coder (77.1%), demonstrating superior reliability and efficiency in agentic coding tasks. The trajectories of all 52 coding tasks are publicly available<!-- --> <a href="https://huggingface.co/datasets/zai-org/CC-Bench-trajectories" target="_blank">here</a> <!-- -->for further community study.</p><h2>Artifacts</h2><p>GLM-4.5 enhances the complex code generation capabilities introduced in the April release of GLM-4. The model now creates sophisticated standalone artifacts‚Äîfrom interactive mini-games to physics simulations‚Äîacross HTML, SVG, Python and other formats. These improvements deliver superior user experiences while laying the foundation for advanced agentic coding applications.</p><h2>Slides Creation</h2><p>Leveraging GLM-4.5&#39;s powerful agentic tool usage and HTML coding capabilities, we developed a model-native PPT/Poster agent. Users can request simple or complex designs, or upload documents, the agent autonomously searches the web or retrieves images, then creates the slides.</p><h2>Full Stack Development</h2><p>GLM-4.5 excels in both frontend and backend development, making it powerful for building modern web applications. To better demonstrate its capabilities, we develop a coding agent inspired by Claude Code. By providing a basic full-stack website boilerplate, the agent enables users to create an entire website with just a few words. Users can effortlessly add features and refine their projects through multi-turn dialogue, making the coding process smooth and enjoyable. Just relax, and let GLM-4.5 turn your ideas into reality at<!-- --> <a href="https://chat.z.ai" target="_blank">Z.ai</a>.</p><p><strong>Chat with GLM-4.5 on<!-- --> <a href="https://chat.z.ai" target="_blank">Z.ai</a></strong></p><p>GLM-4.5 is accessible through the<!-- --> <a href="https://chat.z.ai" target="_blank">Z.ai</a> <!-- -->platform by selecting the GLM-4.5 model option. The platform provides comprehensive support for artifacts generation, presentation slide creation, and full-stack development capabilities.<a href="https://chat.z.ai" target="_blank">Z.ai</a>.</p><p><strong>Call GLM-4.5 API on<!-- --> <a href="https://docs.z.ai/guides/llm/glm-4.5" target="_blank">Z.ai API</a></strong></p><p>The<!-- --> <a href="https://docs.z.ai/guides/llm/glm-4.5" target="_blank">Z.ai API platform</a> <!-- -->offers OpenAI-compatible interfaces for both GLM-4.5 and GLM-4.5-Air models. For comprehensive API documentation and integration guidelines, please refer to<!-- --> <a href="https://docs.z.ai/guides/llm/glm-4.5" target="_blank">https://docs.z.ai/guides/llm/glm-4.5</a>.</p><p><strong>Use GLM-4.5 with Coding Agents</strong></p><p>Detailed instructions for integrating GLM-4.5 with Claude Code and other coding agent frameworks are available in the documentation at<!-- --> <a href="https://docs.z.ai/scenario-example/develop-tools/claude" target="_blank">Z.ai API</a>.</p><p><strong>Serve GLM-4.5 Locally</strong></p><p>Model weights for both base and chat variants of GLM-4.5 and GLM-4.5-Air are publicly available on<!-- --> <a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b" target="_blank">HuggingFace</a> <!-- -->and<!-- --> <a href="https://modelscope.cn/collections/GLM-45-b8693e2a08984f" target="_blank">ModelScope</a>. For local deployment, GLM-4.5 supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available in the official<!-- --> <a href="https://github.com/zai-org/GLM-4.5" target="_blank">GitHub repository</a>.</p><h2>Model Architecture &amp; Pre-training</h2><p>In the GLM-4.5 series, we adopt the MoE architecture, which improves the compute efficiency of both training and inference. We employ loss-free balance routing and sigmoid gates for MoE layers. Unlike DeepSeek-V3 and Kimi K2, we reduce the width (hidden dimension and number of routed experts) of the model while increasing the height (number of layers), as we found that deeper models exhibit better reasoning capacity. In the self-attention component, we employ Grouped-Query Attention with partial RoPE. Furthermore, we utilize 2.5 times more attention heads (96 heads for a 5120 hidden dimension). Counterintuitively, while this increased head count does not improve the training loss compared to models with fewer heads, it consistently enhances performance on reasoning benchmarks such as MMLU and BBH. For GLM-4.5, we utilize the Muon optimizer, which accelerates convergence and tolerates a larger batch size. We also incorporate QK-Norm to stabilize the range of attention logits. For both GLM-4.5 and GLM-4.5-Air, we add an MTP (Multi-Token Prediction) layer to support speculative decoding during inference.</p></div><p><img src="https://z-cdn.chatglm.cn/z-blog/SyDLepVveg.jpg"/></p><div><p>Our base model undergoes several training stages. During pre-training, the model is first trained on 15T tokens of a general pre-training corpus, followed by 7T tokens of a code &amp; reasoning corpus. After pre-training, we introduce additional stages to further enhance the model&#39;s performance on key downstream domains. Unlike the earlier pre-training stage on large-scale universal documents, these stages leverage medium-sized domain-specific datasets, including instruction data.</p><h2>RL for Large-Scale Models with slime</h2></div><p><img src="https://z-cdn.chatglm.cn/z-blog/B1pJ-JSvxx.jpg"/></p><div><p>To facilitate the highly efficient Reinforcement Learning (RL) training required for large-scale models such as GLM-4.5, we have designed, developed, and open-sourced<!-- --> <a href="https://github.com/THUDM/slime" target="_blank"><strong>slime</strong></a>. This RL infrastructure is engineered for exceptional flexibility, efficiency, and scalability, and we actively encourage community use and contributions.</p><p>slime&#39;s primary innovations are architected to overcome common RL bottlenecks, particularly in complex agentic tasks.</p><p> <!-- -->‚Ä¢ <strong>Flexible Hybrid Training Architecture:</strong> slime‚Äôs core strength is its versatile hybrid architecture. It supports both synchronous, co-located training, ideal for traditional applications like Reasoning and General RL, as well as a disaggregated, asynchronous training mode. This asynchronous paradigm is critical for advanced agentic RL, where data generation can be a slow, external process. By decoupling training from data collection, it ensures our training GPUs remain fully saturated, maximizing hardware utilization.<!-- --> </p><p>This cohesive design allows slime to seamlessly integrate multiple agent frameworks, support diverse tasks, and efficiently manage long-horizon rollouts through a unified, powerful interface.</p><h2>Post-Training with Reinforcement Learning for Agentic Capabilities</h2></div><p><img src="https://z-cdn.chatglm.cn/z-blog/B1krbyBvll.jpg"/></p><div><p>The post-training is crucial for LLMs to iteratively enhance their policies through self-generated exploratory experiences. Reinforcement Learning (RL) has been a pivotal step to push the boundary of model capabilities. For GLM-4.5, in addition to integrating the general capabilities from GLM-4-0414 and reasoning from GLM-Z1, we paticularly enhance the agentic capabilities, including agentic coding, deep search, and general tool-using.<!-- --> </p><p>The process begins with supervised fine-tuning on curated reasoning data and synthesized agentic scenarios, followd by a specialized RL phase to cultivate expert models, respectively.</p><p>‚Ä¢ For reasoning, we conduct a single-stage RL over the full 64K context with a difficulty-based curriculum, which we found superior to progressive scheduling. We introduced modified techniques to RL ensure stability: dynamic sampling temperatures to balance exploration-exploitation and adaptive clipping for robust policy updates on STEM problems.</p><p>Although the RL curriculum targets a limited set of verified tasks, the resulting gains transfer to adjacent abilities such as general tool use. Next, expert distillation consolidates these specialized skills, equipping GLM-4.5 with comprehensive strength across all tasks.</p></div></div></div>
  </body>
</html>
