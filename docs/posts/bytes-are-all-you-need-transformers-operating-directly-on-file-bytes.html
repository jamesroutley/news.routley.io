<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2306.00238">Original</a>
    <h1>Bytes are all you need: Transformers operating directly on file bytes</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2306.00238">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Modern deep learning approaches usually transform inputs into a
modality-specific form. For example, the most common deep learning approach to
image classification involves decoding image file bytes into an RGB tensor
which is passed into a neural network. Instead, we investigate performing
classification directly on file bytes, without the need for decoding files at
inference time. Using file bytes as model inputs enables the development of
models which can operate on multiple input modalities. Our model,
\emph{ByteFormer}, achieves an ImageNet Top-1 classification accuracy of
$77.33\%$ when training and testing directly on TIFF file bytes using a
transformer backbone with configuration similar to DeiT-Ti ($72.2\%$ accuracy
when operating on RGB images). Without modifications or hyperparameter tuning,
ByteFormer achieves $95.42\%$ classification accuracy when operating on WAV
files from the Speech Commands v2 dataset (compared to state-of-the-art
accuracy of $98.7\%$). Additionally, we demonstrate that ByteFormer has
applications in privacy-preserving inference. ByteFormer is capable of
performing inference on particular obfuscated input representations with no
loss of accuracy. We also demonstrate ByteFormer&#39;s ability to perform inference
with a hypothetical privacy-preserving camera which avoids forming full images
by consistently masking $90\%$ of pixel channels, while still achieving
$71.35\%$ accuracy on ImageNet. Our code will be made available at
<a href="https://github.com/apple/ml-cvnets/tree/main/examples/byteformer" rel="external noopener nofollow">this https URL</a>.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Maxwell Horton [<a href="https://arxiv.org/show-email/ef70564f/2306.00238">view email</a>]
      </p></div></div>
  </body>
</html>
