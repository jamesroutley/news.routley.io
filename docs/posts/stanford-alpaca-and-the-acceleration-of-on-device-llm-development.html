<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Mar/13/alpaca/">Original</a>
    <h1>Stanford Alpaca, and the acceleration of on-device LLM development</h1>
    
    <div id="readability-page-1" class="page"><div id="primary">

<div>




<p>On Saturday 11th March I wrote about how <a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their Stable Diffusion moment</a>. Today is Monday. Let’s look at what’s happened in the past three days.</p>
<ul>
<li>Later on Saturday: Artem Andreenko reports that <code>llama.cpp</code> can <a href="https://twitter.com/miolini/status/1634982361757790209">run the 4-bit quantized 7B LLaMA language model model on a 4GB RaspberryPi</a>—at 10 seconds per token, but still hugely impressive.</li>
<li>Sunday 12th March: <a href="https://twitter.com/cocktailpeanut">cocktailpeanut</a> releases <a href="https://cocktailpeanut.github.io/dalai/">Dalai</a>, a “dead simple way to run LLaMA on your computer”: <code>npx dalai llama</code> and <code>npx dalai serve</code>.</li>
<li>13th March (today): Anish Thite reports <code>llama.cpp</code> running <a href="https://twitter.com/thiteanish/status/1635188333705043969">on a Pixel 6 phone</a> (26 seconds per token).</li>
<li>Also today: a team at Stanford released <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca: A Strong Open-Source Instruction-Following Model</a>—fine-tuned from the LLaMA 7B model.</li>
</ul>
<p>When I talked about a “Stable Diffusion moment” this is the kind of thing I meant: the moment this stuff is available for people to experiment with, things accelerate.</p>
<p>I’m going to dive into Alpaca in detail.</p>
<h4>Stanford’s Alpaca</h4>
<p>Here’s the introduction to <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">the Alpaca announcement</a>:</p>
<blockquote>
<p>We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. Alpaca behaves similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (&lt;600$).</p>
</blockquote>
<p>The biggest weakness in the LLaMA models released by Meta research last month is their lack of instruction-tuning.</p>
<p>A language model is a sentence completion engine. You give it a sequence of words, “The first man on the moon was”, and it completes that sentence, hopefully with useful content.</p>
<p>One of the great innovations from OpenAI was their application of <a href="https://openai.com/research/instruction-following">instruction tuning</a> to GPT-3:</p>
<blockquote>
<p>To make our models safer, more helpful, and more aligned, we use an existing technique called reinforcement learning from human feedback (RLHF). On prompts submitted by our customers to the API, our labelers provide demonstrations of the desired model behavior, and rank several outputs from our models. We then use this data to fine-tune GPT-3.</p>
</blockquote>
<p>Prior to this, you had to think very carefully about how to construct your prompts. Thanks to instruction tuning you can be a lot more, well, human in the way you interact with the model. “Write me a poem about pandas!” now works as a prompt, instead of “Here is a poem about pandas:”.</p>
<p>The LLaMA models had not been through this process. The <a href="https://github.com/facebookresearch/llama/blob/57b0eb62de0636e75af471e49e2f1862d908d9d8/FAQ.md#2-generations-are-bad">LLaMA FAQ</a> acknowledges this:</p>
<blockquote>
<p>Keep in mind these models are not finetuned for question answering. As such, they should be prompted so that the expected answer is the natural continuation of the prompt. [...] Overall, always keep in mind that models are very sensitive to prompts (particularly when they have not been finetuned).</p>
</blockquote>
<p>This is an enormous usability problem.</p>
<p>One of my open questions about LLaMA was how difficult and expensive it would be to fine-tune it such that it could respond better to instructions.</p>
<p>Thanks to the team at Stanford we now have an answer: 52,000 training samples and $600 of training compute!</p>
<p>Something that stuns me about Alpaca is the quality they claim to be able to get from the 7B model—the smallest of the LLaMA models, and the one which has been seen running (albeit glacially slowly) on a RaspberryPi and a mobile phone! Here’s one example from their announcement:</p>
<p><img src="https://static.simonwillison.net/static/2023/alpaca-email.png" alt="Enter your instruction and press enter: Write an e-mail to congratulate new Stanford admits and mention that you are exited about meeting all of them in person Stanford-Alpaca-7B: An Open-Source Instruction-Following Language Model Dear Stanford Admits, Congratulations on your admission to Stanford! I am so excited to meet all of you in person. As a current student at Stanford, I am looking forward to welcoming you to the Stanford community and showing you all that the university has to offer. I wish you all the best in your future endeavors and I can&#39;t wait to see you on campus! Sincerely, Your Name"/></p>
<p>I would be impressed to see this from the 65B (largest) LLaMA model—but getting this from 7B is spectacular.</p>
<h4>Still not for commercial usage</h4>
<p>I’ll quote the Stanford announcement on this in full:</p>
<blockquote>
<p>We emphasize that Alpaca is intended <strong>only for academic research</strong> and any <strong>commercial use is prohibited</strong>. There are three factors in this decision: First, Alpaca is based on LLaMA, which has a non-commercial <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">license</a>, so we necessarily inherit this decision. Second, the instruction data is based OpenAI’s text-davinci-003, whose <a href="https://openai.com/policies/terms-of-use">terms of use</a> prohibit developing models that compete with OpenAI. Finally, we have not designed adequate safety measures, so Alpaca is not ready to be deployed for general use.</p>
</blockquote>
<p>So it’s still not something we can use to build commercial offerings—but for personal research and tinkering it’s yet another huge leap forwards.</p>
<h4>What does this demonstrate?</h4>
<p>The license of the LLaMA model doesn’t bother me too much. What’s exciting to me is what this all proves:</p>
<ul>
<li>LLaMA itself shows that it’s possible to train a GPT-3 class language model using openly available resources. The <a href="https://arxiv.org/abs/2302.13971">LLaMA paper</a> includes details of the training data, which is entirely from publicly available sources (which include CommonCrawl, GitHub, Wikipedia, ArXiv and ArXiv).</li>
<li>
<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> shows that you can then use some tricks to run that language model on consumer hardware—apparently anything with 4GB or more of RAM is enough to at least get it to start spitting out tokens!</li>
<li>Alpaca shows that you can apply fine-tuning with a feasible sized set of examples (52,000) and cost ($600) such that even the smallest of the LLaMA models—the 7B one, which can compress down to a 4GB file with 4-bit quantization—provides results that compare well to cutting edge <code>text-davinci-003</code> in initial human evaluation.</li>
</ul>
<p>One thing that’s worth noting: the Alpaca 7B comparison likely used the full-sized 13.48GB 16bit floating point 7B model, not the 4GB smaller 4bit floating point model used by <code>llama.cpp</code>. I’ve not yet seen a robust comparison of quality between the two.</p>
<h4>Exploring the Alpaca training data with Datasette Lite</h4>
<p>The Alpaca team released the 52,000 fine-tuning instructions they used as <a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json">a 21.7MB JSON file</a> in their GitHub repository.</p>
<p>My <a href="https://simonwillison.net/2022/May/4/datasette-lite/">Datasette Lite</a> tool has the ability to fetch JSON from GitHub and load it into an in-browser SQLite database. Here’s the URL to do that:</p>
<p><a href="https://lite.datasette.io/?json=https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json">https://lite.datasette.io/?json=https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json</a></p>
<p>This will let you browse the 52,000 examples in your browser.</p>
<p>But we can do a step better than that: here’s a SQL query that runs LIKE queries to search through those examples, considering all three text columns:</p>
<div><pre><span>select</span> instruction, input, output <span>from</span> alpaca_data
<span>where</span> instruction <span>||</span> <span><span>&#39;</span> <span>&#39;</span></span> <span>||</span> input <span>||</span> <span><span>&#39;</span> <span>&#39;</span></span> <span>||</span> output <span>like</span> <span><span>&#39;</span>%<span>&#39;</span></span> <span>||</span> :search <span>||</span> <span><span>&#39;</span>%<span>&#39;</span></span>
<span>order by</span> random()</pre></div>
<p>I’m using <code>order by random()</code> because why not? It’s more fun to explore that way.</p>
<p>The following link will both load the JSON file and populate and execute that SQL query, plus allow you to change the search term using a form in your browser:</p>
<p><a href="https://lite.datasette.io/?json=https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json#/data?sql=select+instruction%2C+input%2C+output+from+alpaca_data%0Awhere+instruction+%7C%7C+%27+%27+%7C%7C+input+%7C%7C+%27+%27+%7C%7C+output+like+%27%25%27+%7C%7C+%3Asearch+%7C%7C+%27%25%27%0Aorder+by+random%28%29&amp;search=occam">https://lite.datasette.io/?json=https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json#/data?sql=select+instruction%2C+input%2C+output+from+alpaca_data%0Awhere+instruction+%7C%7C+%27+%27+%7C%7C+input+%7C%7C+%27+%27+%7C%7C+output+like+%27%25%27+%7C%7C+%3Asearch+%7C%7C+%27%25%27%0Aorder+by+random%28%29&amp;search=occam</a></p>
<p><img src="https://static.simonwillison.net/static/2023/alpaca-datasette-lite.jpg" alt="Screenshot of Datasette executing that SQL query, retruning three results that match &#39;occam&#39;"/></p>
<h4>What’s next?</h4>
<p>This week is likely to be wild. OpenAI are rumored to have a big announcement on Tuesday—possibly GPT-4? And I’ve heard rumors of announcements from both Anthropic and Google this week as well.</p>
<p>I’m still more excited about seeing what happens next with LLaMA. Language models on personal devices is happening so much faster than I thought it would.</p>




</div>

</div></div>
  </body>
</html>
