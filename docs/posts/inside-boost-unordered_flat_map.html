<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://bannalia.blogspot.com/2022/11/inside-boostunorderedflatmap.html">Original</a>
    <h1>Inside boost::unordered_flat_map</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-6276904501422653074" itemprop="description articleBody">
<ul dir="auto"><li><a href="#introduction">Introduction</a></li><li><a href="#the-case-for-open-addressing">The case for open addressing</a>
<ul dir="auto"><li><a href="#simd-accelerated-lookup">SIMD-accelerated lookup</a></li></ul>
</li><li><a href="#boostunordered_flat_map-data-structure"><code>boost::unordered_flat_map</code> data structure</a></li><li><a href="#rehashing">Rehashing</a></li><li><a href="#hash-post-mixing">Hash post-mixing</a></li><li><a href="#statistical-properties-of-boostunordered_flat_map">Statistical properties of <code>boost::unordered_flat_map</code></a></li><li><a href="#benchmarks">Benchmarks</a>
<ul dir="auto"><li><a href="#running-n-plots">Running-<i>n</i> plots</a></li><li><a href="#aggregate-performance">Aggregate performance</a></li></ul>
</li><li><a href="#deviations-from-the-standard">Deviations from the standard</a></li><li><a href="#conclusions-and-next-steps">Conclusions and next steps</a></li></ul>
<a name="introduction"><p><b><span>Introduction</span></b></p></a>
<p dir="auto">Starting in Boost 1.81 (December 2022), <a href="https://www.boost.org/doc/libs/release/libs/unordered/" rel="nofollow">Boost.Unordered</a>
provides, in addition to its previous implementations of C++ unordered associative containers,
the new containers <code>boost::unordered_flat_map</code> and <code>boost::unordered_flat_set</code> (for the sake
of brevity, we will only refer to the former in the remaining of this article).
If <code>boost::unordered_map</code> strictly adheres to the C++ specification for <code>std::unordered_map</code>,
<code>boost::unordered_flat_map</code> deviates in a number of ways from the standard
to offer dramatic performance improvements in exchange; in fact, <code>boost::unordered_flat_map</code>
ranks amongst the fastest hash containers currently available to C++ users.</p>
<p dir="auto">We describe the internal structure of <code>boost::unordered_flat_map</code> and provide
theoretical analyses and benchmarking data to help readers gain insights into
the key design elements behind this container&#39;s excellent performance.
Interface and behavioral differences with the standard are also discussed.</p>
<a name="the-case-for-open-addressing"><p><b><span>The case for open addressing</span></b></p></a>
<p dir="auto">We have <a href="https://bannalia.blogspot.com/2022/06/advancing-state-of-art-for.html" rel="nofollow">previously discussed</a> why
<i>closed addressing</i> was chosen back in 2003 as the implicit layout for <code>std::unordered_map</code>.
20 years after, <a href="https://en.wikipedia.org/wiki/Hash_table#Open_addressing" rel="nofollow"><i>open addressing</i></a>
techniques have taken the lead in terms of performance, and the fastest hash containers in the market
all rely on some variation of open addressing, even if that means that some deviations have to be introduced
from the baseline interface of <code>std::unordered_map</code>.</p>
<p dir="auto">The defining aspect of open addressing is that elements are stored directly within the
bucket array (as opposed to closed addressing, where multiple elements can be held into the same
bucket, usually by means of a linked list of nodes). In modern CPU architectures, this layout
is extremely cache friendly:</p>
<ul dir="auto"><li>There&#39;s no indirection needed to go from the bucket position to the element contained.</li><li>Buckets are stored contiguously in memory, which improves cache locality.</li></ul>
<p dir="auto">The main technical challenge introduced by open addressing is what to do when elements
are mapped into the same bucket, i.e. when a <i>collision</i> happens: in fact, all open-addressing
variations are basically characterized by their collision management techniques.
We can divide these techniques into two broad classes:</p>
<ul dir="auto"><li><b>Non-relocating:</b> if an element is mapped to an occupied bucket, a <i>probing sequence</i> is
started from that position until a vacant bucket is located, and the element is inserted
there <i>permanently</i> (except, of course, if the element is deleted or if the bucket array is grown and elements <i>rehashed</i>).
Popular probing mechanisms are <i>linear probing</i> (buckets inspected at regular intervals),
<i>quadratic probing</i> and <a href="https://en.wikipedia.org/wiki/Double_hashing" rel="nofollow"><i>double hashing</i></a>.
There is a tradeoff between cache locality, which is better when the buckets probed are close
to each other, and <i>average probe length</i> (the expected number of buckets probed until a
vacant one is located), which grows larger (worse) precisely when probed buckets
are close —elements tend to form clusters instead of spreading uniformly throughout the bucket
array.</li><li><b>Relocating:</b> as part of the search process for a vacant bucket, elements can be
moved from their position to make room for the new element. This is done in order
to improve cache locality by keeping elements close to their &#34;natural&#34; location
(that indicated by the hash → bucket mapping). Well known relocating algorithms are
<a href="https://en.wikipedia.org/wiki/Cuckoo_hashing" rel="nofollow"><i>cuckoo hashing</i></a>,
<a href="https://en.wikipedia.org/wiki/Hopscotch_hashing" rel="nofollow"><i>hopscotch hashing</i></a> and
<a href="https://en.wikipedia.org/wiki/Hash_table#Robin_Hood_hashing" rel="nofollow"><i>Robin Hood hashing</i></a>.</li></ul>
<p dir="auto">If we take it as an important consideration to stay reasonably close to the original behavior
of <code>std::unordered_map</code>, relocating techniques pose the problem that <code>insert</code> may invalidate
iterators to other elements (so, they work more like <code>std::vector::insert</code>).</p>
<p dir="auto">On the other hand, non-relocating open addressing faces issues on deletion: lookup
starts at the original hash → bucket position and then keeps probing till the element is found
<i>or probing terminates</i>, which is signalled by the presence of a vacant bucket:</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCm7Mjoizsg_qsNOTa1nU3YIjPDdarUGBnuxC9eiMMXtR4zWnnWXLQLE3RGgXN203SLJcIJfGM7a25uOLapGYJtmcOIeU8yXkrhDM1bK1LmxYESoY_fPohrGtPQuggjHgivzVWDVfqND1ZvYv1MSBexyBMyZFFyN7VPsRt5opZfLlo1m2rZyLEpQ0_/s484/probe.png"><img data-original-height="62" data-original-width="484" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCm7Mjoizsg_qsNOTa1nU3YIjPDdarUGBnuxC9eiMMXtR4zWnnWXLQLE3RGgXN203SLJcIJfGM7a25uOLapGYJtmcOIeU8yXkrhDM1bK1LmxYESoY_fPohrGtPQuggjHgivzVWDVfqND1ZvYv1MSBexyBMyZFFyN7VPsRt5opZfLlo1m2rZyLEpQ0_/s16000/probe.png"/></a></p>
<p dir="auto">So, erasing an element can&#39;t just restore its holding bucket as vacant, since that would preclude
lookup from reaching elements further down the probe sequence:</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_UsPi-cCOsXgja14-LV4lFFY6tKVIq2XB_gB94qhDpZleeQh4fV7M02HgW72oGTFdjWxDbPSqgq65TkP9mY9KAz1OhQTcnDOE6k89eoHQucMK1vVmBoTi71pxyXm1GJlqGOzeTCUAcPllFUWdMvbXUkiT--s2drQO9NSJQITkhaI6aBkB5vhTvC89/s484/probe_interrupted.png"><img data-original-height="62" data-original-width="484" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_UsPi-cCOsXgja14-LV4lFFY6tKVIq2XB_gB94qhDpZleeQh4fV7M02HgW72oGTFdjWxDbPSqgq65TkP9mY9KAz1OhQTcnDOE6k89eoHQucMK1vVmBoTi71pxyXm1GJlqGOzeTCUAcPllFUWdMvbXUkiT--s2drQO9NSJQITkhaI6aBkB5vhTvC89/s16000/probe_interrupted.png"/></a></p>
<p dir="auto">A common techique to deal with this problem is to label buckets previously containing an
element with a <i>tombstone</i> marker: tombstones are good for inserting new elements but do not
stop probing on lookup:</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwq8kJr-DhGz29OqD_6PBYRQ5AIHa3x3_B3ZqWA09HRs6hKfp8pC46WjUAIbrMcMB5SU9hvfXW2G2h8ui8IcEY72fhNljgDgJx_VPKi3ZmWKQ7vDAzfD07sMrqF78XiV7MDH_LbAJDyVxq655gqA5Td1RJVQV1LWoUVbFkhIHB6GEwDCo0Cc5E3SMs/s484/probe_tombstone.png"><img data-original-height="62" data-original-width="484" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwq8kJr-DhGz29OqD_6PBYRQ5AIHa3x3_B3ZqWA09HRs6hKfp8pC46WjUAIbrMcMB5SU9hvfXW2G2h8ui8IcEY72fhNljgDgJx_VPKi3ZmWKQ7vDAzfD07sMrqF78XiV7MDH_LbAJDyVxq655gqA5Td1RJVQV1LWoUVbFkhIHB6GEwDCo0Cc5E3SMs/s16000/probe_tombstone.png"/></a></p>
<p dir="auto">Note that the introduction of tombstones implies that the average lookup probe length of the
container won&#39;t decrease on deletion —again, special measures can be taken to counter this.</p>
<a name="simd-accelerated-lookup"><p><span><b>SIMD-accelerated lookup</b></span></p></a>
<p dir="auto">SIMD technologies, such as <a href="https://en.wikipedia.org/wiki/SSE2" rel="nofollow">SSE2</a> and
<a href="https://en.wikipedia.org/wiki/ARM_architecture_family#Advanced_SIMD_(Neon)" rel="nofollow">Neon</a>,
provide advanced CPU instructions for parallel arithmetic and logical operations
on groups of contiguous data values: for instance, SSE2 <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm_cmpeq_epi8" rel="nofollow"><code>_mm_cmpeq_epi8</code></a> takes two packs of 16 bytes and compares
them for equality <i>pointwise</i>, returning the result as another pack of bytes. Although
SIMD was originally meant for acceleration of multimedia processing applications,
the implementors of some unordered containers, notably Google&#39;s
<a href="https://abseil.io/about/design/swisstables" rel="nofollow">Abseil&#39;s Swiss tables</a> and
Meta&#39;s <a href="https://engineering.fb.com/2019/04/25/developer-tools/f14/" rel="nofollow">F14</a>, realized
they could leverage this technology to improve lookup times in hash tables.</p>
<p dir="auto">The key idea is to maintain, in addition to the bucket array itself, a separate <i>metadata
array</i> holding <i>reduced hash values</i> (usually one byte in size)
obtained from the hash values of the elements stored in the corresponding buckets.
When looking up for an element, SIMD can be used on a pack of contiguous reduced
hash values to quickly discard non-matching buckets and move on to full comparison
for matching positions. This technique effectively checks a moderate number
of buckets (16 for Abseil, 14 for F14) in constant time. Another beneficial effect
of this approach is that special bucket markers (vacant, tombstone, etc.) can be
moved to the metadata array —otherwise, these markers would take up extra space in
the bucket itself, or else some representation values of the elements would have
to be restricted from user code and reserved for marking purposes.</p>
<a name="boostunordered_flat_map-data-structure"><p><b><span><code>boost::unordered_flat_map</code> data structure</span></b></p></a>
<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2oLcYKndxyhp0OW5b3xdoptzjKHjyLp_udDkmFb94SZzgpWPJqEUrad-unp_PNsrfKEkRQGapNWd3qxxzF8_s1bAEr4Rx4vKC2o9e-RxyBHwCeM7YIUALAHxMuOqr72kXWs-79J2lzc27B2op7-hawdTOrTfOoOB_c-TjEidw2pUvs3Es7btmNS29/s935/data_structure.png"><img data-original-height="124" data-original-width="935" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2oLcYKndxyhp0OW5b3xdoptzjKHjyLp_udDkmFb94SZzgpWPJqEUrad-unp_PNsrfKEkRQGapNWd3qxxzF8_s1bAEr4Rx4vKC2o9e-RxyBHwCeM7YIUALAHxMuOqr72kXWs-79J2lzc27B2op7-hawdTOrTfOoOB_c-TjEidw2pUvs3Es7btmNS29/w600/data_structure.png" width="600"/></a></td></tr><tr><td><br/></td></tr></tbody></table>
<p dir="auto"><code>boost::unordered_flat_map</code>&#39;s bucket array is logically split into 2<sup><i>n</i></sup>
groups of <i>N</i> = 15 buckets, and has a companion metadata array consisting of
2<sup><i>n</i></sup> 16-byte words. Hash mapping is done at the group level rather than
on individual buckets: so, to insert an element with hash value <i>h</i>, the group
at position <i>h</i> / 2<sup><i>W</i> − <i>n</i></sup> is selected and its first available bucket
used (<i>W</i> is 64 or 32 depending on whether the CPU architecture is 64- or 32-bit,
respectively); if the group is full, further groups are checked using a quadratic
probing sequence.</p>
<p dir="auto">The associated metadata is organized as follows (least significant byte depicted
rightmost):</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRBJiNHDjc3rW42atOBgkc2HL8zJ7gif-HzCENLnxIdvU8Fi6-RuA-8dcj-ZtCM8H13CiKZH6e2xkq7EoEqNGiDTFGyXNldDOGJKxBQOk9X6G40DxKwQpzWypY0BCFBRPSDI3O0HZ1Gfya33hegTrJuHOyopSYlrWM3aAcNU8-JsJCyyRrCoDtGBm9/s550/metadata.png"><img data-original-height="188" data-original-width="550" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRBJiNHDjc3rW42atOBgkc2HL8zJ7gif-HzCENLnxIdvU8Fi6-RuA-8dcj-ZtCM8H13CiKZH6e2xkq7EoEqNGiDTFGyXNldDOGJKxBQOk9X6G40DxKwQpzWypY0BCFBRPSDI3O0HZ1Gfya33hegTrJuHOyopSYlrWM3aAcNU8-JsJCyyRrCoDtGBm9/s16000/metadata.png"/></a></p>
<p dir="auto"><i>h</i><sub><i>i</i></sub> holds information about the <i>i</i>-th bucket of the group:</p>
<ul dir="auto"><li>0 if the bucket is empty,</li><li>1 to signal a <i>sentinel</i> (a special value at the end of the bucket array used to
finish container iteration).</li><li>otherwise, a reduced hash value in the range [2, 255] obtained from the least
significant byte of the element&#39;s hash value.</li></ul>
<p dir="auto">When looking up within a group for an element with hash value <i>h</i>, SIMD operations,
if available, are used to match the reduced value of <i>h</i> against the pack of
values {<i>h</i><sub>0</sub>, <i>h</i><sub>1</sub>, ... , <i>h</i><sub>14</sub>}. Locating
an empty bucket for insertion is equivalent to matching for 0.</p>
<p dir="auto"><i>ofw</i> is the so-called <i>overflow byte</i>: when  inserting an element with hash value <i>h</i>,
if the group is full then the (<i>h</i> mod 8)-th bit of <i>ofw</i> is set to 1 before
moving to the next group in the probing sequence. Lookup probing can then terminate
when the corresponding overflow bit is 0. Note that this procedure removes the need
to use tombstones.</p>
<p dir="auto">If neither SSE2 nor Neon is available on the target architecture, the logical
organization of metadata stays the same, but information is mapped to two physical
64-bit words using <i>bit interleaving</i> as shown in the figure:</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJISrbaZdbPuhdt62zJsqEtrcpkKg_QvBgLgziJu9GWy7lm2G3P_RSMV2oExl90Fk_d9w-Lt_00Rce_0OL_3QWd1SQXqOdl4Flc8ODvGSB4WTL6GXAuXJcplFittqFRKhgi1NBhz5mO6o6VUgLSmKyMQRNb65QplSg6tvzdiTYdQmsMKrTfnNOS-rR/s560/metadata_interleaving.png"><img data-original-height="140" data-original-width="560" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJISrbaZdbPuhdt62zJsqEtrcpkKg_QvBgLgziJu9GWy7lm2G3P_RSMV2oExl90Fk_d9w-Lt_00Rce_0OL_3QWd1SQXqOdl4Flc8ODvGSB4WTL6GXAuXJcplFittqFRKhgi1NBhz5mO6o6VUgLSmKyMQRNb65QplSg6tvzdiTYdQmsMKrTfnNOS-rR/s16000/metadata_interleaving.png"/></a></p>
<p dir="auto">Bit interleaving allows for a reasonably fast implementation of matching operations
in the absence of SIMD.</p>
<a name="rehashing"><p><b><span>Rehashing</span></b></p></a>
<p dir="auto">The maximum load factor of <code>boost::unordered_flat_map</code> is 0.875 and can&#39;t be changed
by the user. As discussed previously, non-relocating open addressing has the problem
that average probe length doesn&#39;t decrease on deletion when the erased elements
are in mid-sequence: so, continously inserting and erasing elements without triggering
a rehash will slowly degrade the container&#39;s performance; we call this phenomenon
<i>drifting</i>. <code>boost::unordered_flat_map</code> introduces the following anti-drift mechanism:
rehashing is controled by the container&#39;s <i>maximum load</i>, initially 0.875 times the
size of the bucket array; when erasing an element whose associated overflow bit is not
zero, the maximum load is decreased by one. Anti-drift guarantees that rehashing
will be eventually triggered in a scenario of repeated insertions and deletions.</p>
<a name="hash-post-mixing"><p><b><span>Hash post-mixing</span></b></p></a>
<p dir="auto">It is well known that open-addressing containers require that the hash function be
of good quality, in the sense that close input values (for some natural notion of
closeness) are mapped to distant hash values. In particular, a hash function is
said to have the <a href="https://en.wikipedia.org/wiki/Avalanche_effect" rel="nofollow"><i>avalanching property</i></a>
if flipping a bit in the physical representation of the input changes all bits of the
output value with probability 50%. Note that avalanching hash functions are extremely well
behaved, and less stringent behaviors are generally good enough in most open-addressing
scenarios.</p>
<p dir="auto">Being a general-purpose container, <code>boost::unordered_flat_map</code> does not impose any
condition on the user-provided hash function beyond what is required by the C++
standard for unordered associative containers. In order to cope with poor-quality
hash functions (such as the identity for integral types), an automatic bit-mixing stage
is added to hash values:</p>
<ul dir="auto"><li>64-bit architectures: we use the <code>xmx</code> function defined in Jon Maiga&#39;s
<a href="http://jonkagstrom.com/bit-mixer-construction/index.html" rel="nofollow">&#34;The construct of a bit mixer&#34;</a>.</li><li>32-bit architectures: the chosen mixer has been automatically generated by
<a href="https://github.com/skeeto/hash-prospector">Hash Function Prospector</a> and selected as the
best overall performer in internal benchmarks. Score assigned by Hash Prospector: 333.7934929677524.</li></ul>
<p dir="auto">There&#39;s an opt-out mechanism available to end users so that avalanching hash functions
can be marked as such and thus be used without post-mixing. In particular,
the specializations of <a href="https://www.boost.org/doc/libs/release/libs/container_hash/" rel="nofollow"><code>boost::hash</code></a>
for string types are marked as avalanching.</p>
<a name="statistical-properties-of-boostunordered_flat_map"><p><b><span>Statistical properties of <code>boost::unordered_flat_map</code></span></b></p></a>
<p dir="auto">We have written a <a href="https://github.com/joaquintides/boost_unordered_flat_map_stats">simulation program</a>
to calculate some statistical properties
of <code>boost::unordered_flat_map</code> as compared with Abseil&#39;s <code>absl::flat_hash_map</code>,
which is generally regarded as one of the fastest hash containers available.
For the purposes of this analysis, the main design characteristics of
<code>absl::flat_hash_map</code> are:</p>
<ul dir="auto"><li>Bucket array sizes are of the form 2<sup>n</sup>, <i>n</i> ≥ 4.</li><li>Hash mapping is done at the bucket level (rather than at the group
level as in <code>boost::unordered_flat_map</code>).</li><li>Metadata consists of one byte per bucket, where the most significant bit is set to 1
if the bucket is empty, deleted (tombstone) or a sentinel. The remaining 7 bits
hold the reduced hash value for occupied buckets.</li><li>Lookup/insertion uses SIMD to inspect the 16 contiguous buckets beginning at the
hash-mapped position, and then continues with further 16-bucket groups using
quadratic probing. Probing ends when a non-full group is found.
Note that the start positions of these groups are not aligned modulo 16.</li></ul>
<p dir="auto">The figure shows:</p>
<ul dir="auto"><li>the probability that a randomly selected group is full,</li><li>the average number of hops (i.e. the average probe length minus one) for
successful and unsuccessful lookup</li></ul>
<p dir="auto">as functions of the load factor, with perfectly random input and without intervening
deletions. Solid line is <code>boost::unordered_flat_map</code>, dashed line is
<code>absl::flat_hash_map</code>.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiclPJEg-PekZgcegdboCcZmMwYx4C6PrYqxwdqJVhfPcRj1Hf6ozHivEcKiVt3T6-jpfJu7ugSJQMrQfb6jVwElWdQueN9GJuwRnQq2i_TibmIL5mAD8x6UhA0Flru5rT7wvhRvWaOYO3VCoF5fFYoEr7f4KE1ZsHDjuGpnn1bq_DqOB9gv3dVLYQB/s804/stats1.png"><img data-original-height="536" data-original-width="804" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiclPJEg-PekZgcegdboCcZmMwYx4C6PrYqxwdqJVhfPcRj1Hf6ozHivEcKiVt3T6-jpfJu7ugSJQMrQfb6jVwElWdQueN9GJuwRnQq2i_TibmIL5mAD8x6UhA0Flru5rT7wvhRvWaOYO3VCoF5fFYoEr7f4KE1ZsHDjuGpnn1bq_DqOB9gv3dVLYQB/w500/stats1.png" width="500"/></a></p>
<p dir="auto">Some observations:</p>
<ul dir="auto"><li><i>Pr</i>(group is full) is higher for <code>boost::unordered_flat_map</code>. This follows
from the fact that free buckets cluster at the end of 15-aligned groups,
whereas for <code>absl::flat_hash_map</code> free buckets are uniformly distributed across
the array, which increases the probability that a contiguous 16-bucket chunk
contains at least one free position. Consequently, <i>E</i>(num hops) for successful
lookup is also higher in <code>boost::unordered_flat_map</code>.</li><li>By contrast, <i>E</i>(num hops) for <i>unsuccessful</i> lookup is considerably lower
in <code>boost::unordered_flat_map</code>: <code>absl::flat_hash_map</code> uses an all-or-nothing
condition for probe termination (group is non-full/full), whereas
<code>boost::unordered_flat_map</code> uses the 8 bits of information in the overflow byte
to allow for more finely-grained termination —effectively, making probe termination
~1.75 times more likely. The overflow byte acts as a sort of
<a href="https://en.wikipedia.org/wiki/Bloom_filter" rel="nofollow">Bloom filter</a> to check for probe
termination based on reduced hash value.</li></ul>
<p dir="auto">The next figure shows the average number of actual comparisons (i.e. when
the reduced hash value matched) for successful and unsuccessful lookup.
Again, solid line is <code>boost::unordered_flat_map</code> and
dashed line is <code>absl::flat_hash_map</code>.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYNDl36h_80BxAs3kOussDg1fzg9iDmU7F5HH4JM0nqyY-lX29osMs2H-aqMOkOeirbLGiyjttIQVGeFRVQcJIQOXDInqHfo1zJlUBf5NI8sGrujOxdj3QIYZ60-5Pl9rnXiYjOpUXBvDtmnCvE_zzngPFW8Aq_gfDUfWzCvH7Qc_vzamP5F-ktpFD/s804/stats2.png"><img data-original-height="536" data-original-width="804" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYNDl36h_80BxAs3kOussDg1fzg9iDmU7F5HH4JM0nqyY-lX29osMs2H-aqMOkOeirbLGiyjttIQVGeFRVQcJIQOXDInqHfo1zJlUBf5NI8sGrujOxdj3QIYZ60-5Pl9rnXiYjOpUXBvDtmnCvE_zzngPFW8Aq_gfDUfWzCvH7Qc_vzamP5F-ktpFD/w500/stats2.png" width="500"/></a></p>
<p dir="auto"><i>E</i>(num cmps) is a function of:</p>
<ul dir="auto"><li><i>E</i>(num hops) (lower better),</li><li>the size of the group (lower better),</li><li>the number of bits of the reduced hash value (higher better).</li></ul>
<p dir="auto">We see then that <code>boost::unordered_flat_map</code> approaches <code>absl::flat_hash_map</code> on
<i>E</i>(num cmps) for successful lookup (1% higher or less), despite its poorer
<i>E</i>(num hops) figures: this is so because <code>boost::unordered_flat_map</code>
uses smaller groups (15 vs. 16) and, most importantly, because its reduced
hash values contain log<sub>2</sub>(254) = 7.99 bits vs. 7 bits in <code>absl::flat_hash_map</code>,
and each additional bit in the hash reduced value decreases the number of negative
comparisons roughly by half. In the case of <i>E</i>(num cmps) for unsuccessful lookup,
<code>boost::unordered_flat_map</code> figures are up to 3.2 times lower under
high-load conditions.</p>
<a name="benchmarks"><p><b><span>Benchmarks</span></b></p></a>
<a name="running-n-plots"><p><span><b>Running-<i>n</i> plots</b></span></p></a>
<p dir="auto">We have measured the execution times of <code>boost::unordered_flat_map</code> against
<code>absl::flat_hash_map</code> and <code>boost::unordered_map</code> for basic operations
(insertion, erasure during iteration, successful lookup, unsuccessful lookup) with
container size <i>n</i> ranging from 10,000 to 10M. We provide the full benchmark code and
results for different 64- and 32-bit architectures in a
<a href="https://github.com/boostorg/boost_unordered_benchmarks/tree/boost_unordered_flat_map">dedicated repository</a>;
here, we just show the plots for GCC 11 in x64 mode on an
AMD EPYC Rome 7302P @ 3.0GHz.
Please note that each container uses its own default hash function, so a direct
comparison of execution times may be slightly biased.</p>
<table>
<thead>
<tr>
<th>  
<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgba397_JysbdpRTRYxpr47OWLiQzZ6bYCprlnKynq0GDYBkZJfYnxuMeHnVgW8js_QbAhpll9NBUsxjiNji9oNLEhnYf3A33GpUGKaWeH_H-OR-vHfPM_6pheB_f2inwWY0vF5G8RhvhGIuErkUwkuVuXBXRkOLbP8Sift7bVXUXjVbEADAI3R9bi2/s698/Running%20insertion.xlsx.plot.png"><img data-original-height="449" data-original-width="698" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgba397_JysbdpRTRYxpr47OWLiQzZ6bYCprlnKynq0GDYBkZJfYnxuMeHnVgW8js_QbAhpll9NBUsxjiNji9oNLEhnYf3A33GpUGKaWeH_H-OR-vHfPM_6pheB_f2inwWY0vF5G8RhvhGIuErkUwkuVuXBXRkOLbP8Sift7bVXUXjVbEADAI3R9bi2/w275/Running%20insertion.xlsx.plot.png" width="275"/></a>
</th>
<th>    
<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEif6tMoSlJUh0w5nP3YmBuQSIFmANcwqQfWihGKIea7Zu7F9YCIpGDWzuaqDvqR2ZYgOkRqy9a5H9qww3zCanRAR4D62voz7xHaLYsb6K6CYbsfUHZGfCHsdjXQ15HLo9Jox7QHHSYI621AT8ZyhBI4YtRVKSJ-bpStbCP37hnpypT0jbMogHV0Hqxf/s698/Running%20erasure.xlsx.plot.png"><img data-original-height="449" data-original-width="698" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEif6tMoSlJUh0w5nP3YmBuQSIFmANcwqQfWihGKIea7Zu7F9YCIpGDWzuaqDvqR2ZYgOkRqy9a5H9qww3zCanRAR4D62voz7xHaLYsb6K6CYbsfUHZGfCHsdjXQ15HLo9Jox7QHHSYI621AT8ZyhBI4YtRVKSJ-bpStbCP37hnpypT0jbMogHV0Hqxf/w275/Running%20erasure.xlsx.plot.png" width="275"/></a>
</th>
</tr></thead>
<tbody>
<tr>
<td><b>Running insertion</b></td>
<td><b>Running erasure</b></td>
</tr>
</tbody>
</table>
</div></div>
  </body>
</html>
