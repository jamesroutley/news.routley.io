<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://commoncog.com/run-smart-experiments-dont-know/">Original</a>
    <h1>How to Run Smart Experiments When You Just Donâ€™t Know</h1>
    
    <img src="https://commoncog.com/content/images/2025/08/how_run_smart_experiments.jpg" alt="How to Run Smart Experiments When You Just Don&#x2019;t Know"><p>A couple of years ago I had a Commoncog reader join me as a summer intern because she wanted to observe me doing &#x2018;startup stuff&#x2019;. The &#x2018;project&#x2019; I was working on was to figure out a scalable way to create new cases for the <a href="https://commoncog.com/c/">Commoncog Case Library</a>. At the end of the internship she gave me a couple of notes about the experience, and we met up for a meal to discuss those notes. At the top of her list was the following observation: &#x201C;I&#x2019;m surprised you didn&#x2019;t have a hypothesis before you ran this!&#x201D;</p><p>I was taken aback. I was then in the midst of implementing &#x2014; and then publishing &#x2014; a series of essays about <a href="https://commoncog.com/becoming-data-driven-in-business/">Becoming Data Driven in Business</a>. In the series I argued for creating hypotheses and then testing them in tightly-scoped, iterative bets. I wrote about how the <a href="https://commoncog.com/the-amazon-weekly-business-review/">Amazon Weekly Business Review</a> was less a metrics review meeting and more a way to generate knowledge about the &#x2018;causal model&#x2019; of a business. I also explained how the ideas of statistician and &#x2018;business philosopher&#x2019; <a href="https://commoncog.com/making-sense-of-deming/">W. Edwards Deming</a> (from which the WBR inherited from, by way of Six Sigma) gave you the tools for <a href="https://commoncog.com/the-secret-heart-of-continuous-improvement/">rigorous, single subject studies</a>.</p><p>And yet here I was, running a full blown experimentation loop over the summer, without any explicitly stated hypotheses. I didn&#x2019;t have pre-registered input/output metrics, or even a planning doc. <em>What was I doing?</em></p><p>I can&#x2019;t remember what I told my intern. I think I mumbled something about how, at the earliest stages of a bet, when you don&#x2019;t know anything about anything, you <em>can&#x2019;t</em> generate hypotheses. Instead, &#x201C;you just want to throw shit at the wall, and see what sticks.&#x201D;</p><p>All of these things were true, for I felt in my bones that generating hypotheses was the wrong thing to do for this situation. I had never hired a large group of writers all at once, for instance. I had no idea what problems would crop up, or what could go right with my approach. But I lacked the language to describe why I felt these things.</p><p>Until now.</p><h2 id="what-is-good-experimental-design">What is Good Experimental Design?</h2><p>In school, we are taught that good experiments should have solid experimental design and a pre-committed, falsifiable hypothesis. This attitude tends to dominate when executing in big companies. For instance, if you are a conscientious analytical type and you&#x2019;re used to large company execution, it might feel weird for you to launch a new project, product, or startup without articulating some hypotheses or setting some goals before you do so!</p><p>Here&#x2019;s a more concrete example. My friend <a href="https://www.linkedin.com/in/crystalwidjaja?ref=commoncog.com">Crystal Widjaja</a> has a framework for coming up with new conjectures in her Reforge course <a href="https://www.reforge.com/courses/data-for-product-managers/details?ref=commoncog.com">Mastering Product Analytics</a> &#x2014; an approach I&#x2019;ve adopted for my own operating bets. She argues that good &#x2018;product conjectures&#x2019; have three properties:</p><ul><li>They are debatable (that is, it is not immediately obvious that it would work).</li><li>They are testable (the bet is verifiable using some kind of experiment).</li><li>They are meaningful (the outcome of the bet, if it works, generates a meaningful impact on some output that leadership cares about).</li></ul><p>Widjaja&#x2019;s criterion is excellent and I&#x2019;ve used it in the months since she first taught it to me. But I&#x2019;ve noticed that it doesn&#x2019;t work at the earliest stages of an uncertain thing. What do I mean by uncertainty? Loosely speaking, &#x2018;uncertainty&#x2019; is when you <em>don&#x2019;t know and can&#x2019;t even imagine what all the outcomes are</em>. Not knowing about outcomes makes it hard to come up with conjectures in the first place. For instance, let&#x2019;s say that your boss asks you to scale up an existing social media campaign, going from $5000 in Facebook ads to $50k in weekly spend. Not everything is known, but the bits that are unknown are tractable enough that you can probably figure out a game plan, especially if you have some experience with marketing. Now compare that to your boss asking you to move to China next week &#x2014; a country you&#x2019;ve never been to &#x2014; to set up a new office to launch products from, something that nobody in your company has done before. One situation is <em>vastly</em> more uncertain than the other. There are more things that can go wrong (or right!) with the China project than with the ad spend project.</p><p>My point: it is difficult to do conventional experimental design when you know very little about the domain you&#x2019;re operating in. The more uncertain situation calls for a different approach.</p><p>So what do you do?</p><p>It should not surprise you that good entrepreneurs are very good at this stage. What they do is less like an &#x2018;experiment&#x2019; and more like &#x2018;throwing shit at the wall and seeing what sticks&#x2019;. A more acceptable way of describing what great entrepreneurs do is that they &#x2018;<a href="https://commoncog.com/when-action-beats-prediction/">take action to generate information</a>&#x2019;. But there is a method to their madness.</p><p>The short version is that entrepreneurs take action to generate answers to four questions:</p><ol><li>What are the possible outcomes?</li><li>What are the further actions?</li><li>What is the value of each outcome relative to the others?</li><li>What causal relationships exist?</li></ol><p>Or, another way of thinking about this is that folks who are good at high uncertainty execution are <em>more skilled at making sense of the splatter pattern <strong>after</strong> they&#x2019;ve thrown something at the wall.</em></p><p>In other words, their skill doesn&#x2019;t come from good experiment design! Their skill comes <em>from the sensemaking they do after they&#x2019;ve taken action to generate some information.</em></p><p>Let&#x2019;s go through the four questions quickly, and in order.</p>
  </body>
</html>
