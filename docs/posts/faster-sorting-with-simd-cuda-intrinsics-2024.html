<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://winwang.blog/posts/bitonic-sort/">Original</a>
    <h1>Faster sorting with SIMD CUDA intrinsics (2024)</h1>
    
    <div id="readability-page-1" class="page"><div><p>Full code on Github: <a href="https://github.com/wiwa/blog-code/" target="_blank" rel="noopener">https://github.com/wiwa/blog-code/</a></p><h3 id="hi">Hi
<a href="#hi"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>Recently, I finished a batch at the <a href="https://www.recurse.com" target="_blank" rel="noopener">Recurse Center</a>… is what I would have said if this post were written when I intended to write it (i.e. 3 months ago). My project there focused on a questionable application of CUDA (mostly irrelevant to this post), but it got me thinking more about other GPU-friendly algorithms.</p><p>Instead of my Recurse project (which I hope to write about in a later post), I want to simply begin writing about technical stuff I’ve played around with. Today will be about a high-level overview of a particular kind of parallel sorting algorithm called <a href="https://en.wikipedia.org/wiki/Bitonic_sorter" target="_blank" rel="noopener">bitonic sort</a> . I’ll go over the context behind around algorithm, a few basics of SIMD programming, a CUDA implementation, and how a small optimization grants it a +30% performance uplift. We’ll be using a SIMD-like operation (a CUDA instruction called <code>__shfl_sync</code>) to quickly sort 32-element “vectors”.</p><h3 id="what-is-a-bitonic-sort">What is a Bitonic Sort?
<a href="#what-is-a-bitonic-sort"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>But what does the word “bitonic” even mean? This was my first question when I happened on the algo. A <em>mono</em>tonic sequence is one whose elements are all non-decreasing or non-increasing (i.e. sorted sequences like <code>[1,2,3,4]</code> or <code>[4,3,2,1]</code>). A <em>bi</em>tonic sequence is a sequence formed by a concatenation of two monotonic sequences, e.g. <code>[1,3,5,4,3,2,1]</code>.</p><p>Bitonic sort is a species of <a href="https://en.wikipedia.org/wiki/Sorting_network" target="_blank" rel="noopener">sorting network</a>, a popular family of fast, parallel (comparison) sorting algorithms. Bitonic sort, for example, can sort in <code>O(log^2(n))</code> “time” (i.e. “parallel depth”/“delay”/“wall time”).
How is this possible when it’s proven that a comparison-based sort requires <code>O(n*log(n))</code> time? Because that’s the runtime requirement for a sequential algorithm – specifically, the “work” needed. Bitonic sort requires <code>O(n*log^2(n))</code> parallel work (total number of comparisons, or “cpu time”) which can be completely parallelized across <code>n</code> processors.
Usually, CS majors only care about the big-O of sorts, but constants also matter in the real world. Thankfully, bitonic sort seems to incur small constants and offers good cache locality.</p><p>Sorting networks are typically represented like a circuit with a series of parallel swaps (predicated, of course, upon a partial order). Since they are (virtual) circuits, a sorting network operates on a specific number of elements.</p><p>Here’s an example of a (“non-alternating”) bitonic sort on 16 elements:</p><p><img src="https://winwang.blog/BitonicSort.png" alt=""/></p><p>The types of bitonic sequences we’ll focus on will be made up of two monotonic sequences of equal length, e.g. if the first half is monotonically increasing, then the second half is deceasing. Implicitly, this means our sorting algorithm will only work on sequences whose size is a power of 2, at least naively. However, we can of course just pad any input with minimal/maximal values.</p><h3 id="sorting-and-simd">Sorting and SIMD
<a href="#sorting-and-simd"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>In reality, sorting performance is more important with a growing number of elements. How can we use a fast-but-small bitonic sort to speed up a general sort of a large sequence? Imagine a merge sort where, instead of recursing down to the 2-item case, we only recurse down to the 32-item case, with the bonus that this wider “base-case” can be solved faster than the naive implementation via hardware acceleration. This is where the GPU comes into play, but it should be noted that bitonic sort can also speed up sorting on CPUs, courtesy of <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data" target="_blank" rel="noopener">SIMD</a>. That being said, we won’t go into a (CPU) SIMD implementation but rather only focus on the CUDA equivalent: <a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/" target="_blank" rel="noopener">warp intrinsics</a>.</p><p>SIMD programming is quite different from typical sequential logic since it executes (in parallel) the same instruction across a vector of data (i.e. across vector <em>lanes</em>). Although SIMD is a parallel programming model, the term is also used to refer to “vector extensions” to CPU ISAs like AVX (x86) and NEON (ARM). Vector instructions allow us to easily hardware-accelerate data-parallel algorithms like sorting networks where each element is “small”, i.e. about the size of a word (64 bits).</p><p>An example of a SIMD data-parallel algorithm would be a <code>map</code>, i.e. map each item, <code>x</code>, in each SIMD lane to a function <code>f</code>, producing the resultant vector of <code>f(x)</code>s. An example: with 128-bit SIMD, we can have a “vector” of 4 32-bit integers, e.g. <code>[1,3,5,7]</code>, and simultaneously operate on them with a single instruction, e.g. <code>+ [1,1,1,1]</code> -&gt; <code>[2,4,6,8]</code>. Another possible operation is <code>reduce</code>, e.g. sum all the numbers in the vector. However, SIMD programming allows for more interesting cases as well.
Imagine we have a vector of integers, for example <code>[0, 1, ..., n-1]</code>, and you want to reverse it in place. One way to do this is to pair up each index with its “reverse”: <code>(0,n-1), (1,n-2), ...</code>, and swap the items for each pair. This is called a <a href="https://www.expertsmind.com/questions/butterfly-permutation-30138485.aspx" target="_blank" rel="noopener">butterfly permutation</a>, which is a special case of the more general idea of “shuffling”, i.e. moving data between SIMD lanes.</p><p>For those new to CUDA/GPU programming, the many different threads running on a GPU are organized into many SIMD-like executions of 32 threads each, called warps. If we have 4096 threads running, that means we have have 128 warps running in parallel, each instruction of which must execute like 32-element-wide vector instructions. However, CUDA allows the programmer to write programs as if each thread in the warp were a “normal” thread irrespective of its “neighbors” in its warp, a logical execution model which is called <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads" target="_blank" rel="noopener">SIMT</a>. Yossi Kreinin wrote a great blog post about these parallel execution models: <a href="https://yosefk.com/blog/simd-simt-smt-parallelism-in-nvidia-gpus.html" target="_blank" rel="noopener">SIMD &lt; SIMT &lt; SMT</a>.</p><p>For now, when I mention “SIMD”, let’s imagine we mean warp-level execution, i.e. each thread, or “lane”, executes the same instruction in lockstep with its neighbors. If AVX512 can deal with vectors of 16 INT32s, CUDA deals with vectors of 32 INT32s.</p><h3 id="cuda-and-code">CUDA and Code
<a href="#cuda-and-code"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>In CUDA, we can use the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#id35" target="_blank" rel="noopener"><code>__shfl_sync</code></a> built-in warp-level primitive to accomplish shuffling within a warp by directly exchanging values in registers. This is in contrast to the “traditional” method of GPU inter-thread communication in which each thread must write to/read from shared memory (L1 cache)<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. In practice, this offers a significant speedup<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> on its own, and I’d presume it would be even more useful if another block/kernel were already utilizing shared memory bandwidth – albeit this would probably be only a small effect considering that shared memory bandwidth is upwards of 17 TBps.</p><p>Compared to traditional CPU <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=mm256_shuffle&amp;ig_expand=4953" target="_blank" rel="noopener">SIMD shuffles</a>, <code>__shfl_sync</code> can handle arbitrary permutations of input lanes, like the SIMD <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_permutevar8x32_epi32&amp;ig_expand=4953" target="_blank" rel="noopener">“permutevar” ops</a> (which take an additional vector instead of an immediate value). There’s an interesting post about SIMD shuffling <a href="https://stackoverflow.com/a/58601259" target="_blank" rel="noopener">here</a>.</p><p>Here’s a code snippet illustrating the shuffle (and leaving out irrelevant details):</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// load data into register
</span></span></span><span><span><span></span>u32 datum <span>=</span> ...;
</span></span><span><span><span>// Loop:
</span></span></span><span><span><span>// calculate the lane to compare against
</span></span></span><span><span><span></span>u32 other_lane <span>=</span> ...;
</span></span><span><span><span>// shuffle!
</span></span></span><span><span><span></span>u32 other_datum <span>=</span> <span>__shfl_sync</span>(gpu<span>::</span>all_lanes, datum, other_lane);
</span></span><span><span><span>// swap if necessary between this lane and other_lane
</span></span></span><span><span><span>// such that the smaller datum is in the lower lane index
</span></span></span><span><span><span></span><span>if</span> (lane <span>&lt;</span> other_lane) {
</span></span><span><span>    datum <span>=</span> <span>min</span>(datum, other_datum);
</span></span><span><span>} <span>else</span> {
</span></span><span><span>    datum <span>=</span> <span>max</span>(datum, other_datum);
</span></span><span><span>}
</span></span></code></pre></div><p>The single call to <code>__shfl_sync</code> saves us a read and write to shared memory, as well as a call to <code>__syncwarp</code>. Warp synchronization is necessary to ensure that later reads from threads in the warp will always have the latest data. Here’s the equivalent alternative code:</p><div><pre tabindex="0"><code data-lang="c"><span><span>u32 other_datum <span>=</span> shared_data[other_lane];
</span></span><span><span><span>if</span> (lane <span>&lt;</span> other_lane) {
</span></span><span><span>    datum <span>=</span> <span>min</span>(datum, other_datum);
</span></span><span><span>} <span>else</span> {
</span></span><span><span>    datum <span>=</span> <span>max</span>(datum, other_datum);
</span></span><span><span>}
</span></span><span><span>shared_data[lane] <span>=</span> datum;
</span></span><span><span><span>// Big Green says implicit warp synchronization is an antipattern
</span></span></span><span><span><span></span><span>__syncwarp</span>();
</span></span></code></pre></div><p>With code in hand, it’s time to run a basic performance benchmark. The data preparation is simple: a large array of random integers. The task is to sort each 32-width subrange in a simple loop, which minimizes the effect of global memory pressure. Repeatedly the <code>__shfl_sync</code> version was 30% faster than using shared memory.</p><h3 id="what-now">What now?
<a href="#what-now"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>It’s not a new idea to use sorting networks to lift the base-case for a general GPU mergesort from sorting 2 elements (i.e. pairs) to sorting 32 elements. ModernGPU implements another type of sorting network for their <a href="https://moderngpu.github.io/mergesort.html" target="_blank" rel="noopener">pedagogical GPU mergesort</a>. Since they only use global memory, it’s not exactly performant, but I’d presume an industrial library like Thrust would already be using warp primitives.</p><p>But then, what else can we do with a faster 32-way sort aside from accelerating the base case? When it comes to sorting, another pairwise operation (for mergesort) is the pairwise merging of sorted sublists. In fact, that’s essentially the entire premise of <em>merge</em>sort. Well, what if we could somehow use it to accelerate a 32-way merge? Stay tuned for (an eventual) Part Two, because even if it’s not necessarily faster, it’s quite the funny algorithm!</p><p>For another bit of CUDA and perf, I also have a previous blog post about GPU hashmaps beating Rust by 10x: <a href="https://wiwa.substack.com/p/can-we-10x-rust-hashmap-throughput" target="_blank" rel="noopener">https://wiwa.substack.com/p/can-we-10x-rust-hashmap-throughput</a></p><h3 id="system-details">System Details
<a href="#system-details"><i aria-hidden="true" title="Link to heading"></i>
<span>Link to heading</span></a></h3><p>Ryzen 7950X3D + RTX 3090 with a PCIe 4.0x16 riser cable</p></div></div>
  </body>
</html>
