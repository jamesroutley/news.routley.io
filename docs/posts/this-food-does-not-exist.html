<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nyx-ai.github.io/stylegan2-flax-tpu/">Original</a>
    <h1>Show HN: This Food Does Not Exist</h1>
    
    <div id="readability-page-1" class="page"><div id="content" role="main">
      <h2 id="this-food-does-not-exist-">This Food Does Not Exist ğŸªğŸ°ğŸ£ğŸ¹</h2>

<p><strong>We have trained four StyleGAN2 models to generate food pictures. The images below are purely synthetic!</strong></p>

<p>The <a href="https://github.com/nyx-ai/stylegan2-flax-tpu">code</a> optimized for TPU training as well as the <a href="https://github.com/nyx-ai/stylegan2-flax-tpu/releases">pretrained models</a> are openly available.</p>

<p><code>cookie-256.pkl</code></p>

<p><img src="https://user-images.githubusercontent.com/140592/179369671-32cf8c67-a3d5-43a4-a200-1ba91e736ae2.png" alt="cookies"/></p>

<p><code>cheesecake-256.pkl</code></p>

<p><img src="https://user-images.githubusercontent.com/140592/179959973-df75351d-db07-4ff9-8f9f-97334bab20a8.png" alt="cheesecake"/></p>

<p><code>cocktail-256.pkl</code></p>

<p><img src="https://user-images.githubusercontent.com/140592/179956003-8db513d2-b0b1-4a1f-8f15-827b56bedb25.png" alt="cocktail"/></p>

<p><code>sushi-256.pkl</code></p>

<p><img src="https://user-images.githubusercontent.com/140592/179958220-45324fe7-90d8-49dd-94be-877b03201160.png" alt="sushi"/></p>

<p>Cherry-picked results, check out the Colab notebook to generate your own: <a href="https://colab.research.google.com/github/nyx-ai/stylegan2-flax-tpu/blob/master/notebook/image_generation.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

<p>Or train your own model: <a href="https://github.com/nyx-ai/stylegan2-flax-tpu">https://github.com/nyx-ai/stylegan2-flax-tpu</a></p>

<h2 id="why-not-dallediffusion-models-">Why not DALLÂ·E/diffusion models? ğŸ¤”</h2>

<p>Recent methods like diffusion and auto-regressive models are all the rage these days: <a href="https://openai.com/dall-e-2/">DALLÂ·E 2</a>, <a href="https://www.craiyon.com/">Craiyon</a> (formerly DALLÂ·E mini), <a href="https://rudalle.ru/en/">ruDALL-E</a>â€¦ Why not go in this direction?</p>

<p><strong>TL;DR: cos weâ€™re poor ğŸ¤·â€â™‚ï¸</strong></p>

<h3 id="realism-vs-control">Realism vs control</h3>

<p>StyleGAN models shine in terms of photorealism, as can be some by some of our food results. For another example, the website <a href="https://thispersondoesnotexist.com/">ThisPersonDoesNotExist.com</a> produces very believable face images. While GANs are still better at this, <a href="https://arxiv.org/abs/2105.05233">diffusion models are catching up</a> and this may change soon.</p>

<p>Diffusion models offer better control and flexibility, thanks in large part to text guidance. This comes at the cost of larger models and slower generation times.</p>

<h3 id="training-resources">Training resources</h3>

<p>We were able to train the provided models in less than 10h each using a single TPU v4-8:</p>

<p><img src="https://nyx-ai.github.io/stylegan2-flax-tpu/static/fid_pretrained.png" alt="Training plots"/></p>

<p><a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance">FID (FrÃ©chet inception distance)</a> is a metric used to assess the quality of images created by a generative model.</p>

<p>In comparison, Craiyon is being training on a v3-256 TPU pod which means 32x the resources (albeit using the previous TPU generation) and the training <a href="https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2">has been going on for over a month</a>.</p>

<h3 id="result-comparison">Result comparison</h3>

<p>No cherry-picking!</p>

<p>Ours</p>

<p><img src="https://user-images.githubusercontent.com/140592/179997085-53c0cb55-35ba-4333-b1fd-10df2ddbb238.png" alt="bdc76775-2c9f-4110-a2f1-fcbc07a588e7"/></p>

<p><a href="https://www.craiyon.com/">Craiyon</a> (â€œa pile of cookies on a plateâ€)</p>

<p><img src="https://user-images.githubusercontent.com/140592/179996330-6fe568d2-bc83-4755-b556-6059e1fdd231.jpeg" alt="a-pile-of-cookies-on-a-plate"/></p>

<p><a href="https://openai.com/dall-e-2/">DALLÂ·E 2</a> (â€œa pile of cookies on a plateâ€)</p>

<p><img width="2320" alt="Screenshot 2022-07-20 at 15 31 55" src="https://user-images.githubusercontent.com/140592/179996470-66c56b77-8305-4b25-92c0-f9570970a7b3.png"/></p>

<h2 id="acknowledgements-">Acknowledgements ğŸ™</h2>

<ul>
  <li>This work is based on Matthias Wrightâ€™s <a href="https://github.com/matthias-wright/flaxmodels/tree/main/training/stylegan2">stylegan2</a> implementation</li>
  <li>The project received generous support from <a href="https://sites.research.google/trc/about/">Googleâ€™s TPU Research Cloud (TRC)</a></li>
  <li>The image datasets were built using the <a href="https://laion.ai/blog/laion-5b/">LAION5B index</a></li>
  <li>We are grateful to <a href="https://wandb.ai/">Weights &amp; Biases</a> for preserving our sanity</li>
</ul>


      
    </div></div>
  </body>
</html>
