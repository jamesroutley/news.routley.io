<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dust3r.europe.naverlabs.com/">Original</a>
    <h1>DUSt3R: Geometric 3D Vision Made Easy</h1>
    
    <div id="readability-page-1" class="page">
		


		<!-- ----- Title ----- -->
		<p>
				<b>DUSt3R: Geometric 3D Vision Made Easy</b></p>

		

		<div>
			
			<video width="640" height="480" poster="nle-assets/supmat_dust3r_final.jpg" controls="" loop="">
				<source src="nle-assets/supmat_dust3r_final_light_h264.mp4" type="video/mp4"/>
				Your browser does not support the video tag.
			</video>
		</div>


		<!-- ----- Abstract ----- -->
		<div>
			<h3>Abstract</h3>
			<p>
				<strong>
					Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. 
					<i>In summary, DUSt3R makes geometric 3D vision tasks easy.</i>
				</strong>
			</p>
		</div>

		<!-- ----- Method ----- -->
		<div>
			<h3>An all-in-one method</h3>
			
			<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/dust3r_archi.png"/>
			</p>
		</div>

		<!-- ----- Demo ----- -->
		<div>
			<h3>Demonstration</h3>
		
			<!-- --- Scene01 --- -->
			<div id="demo-scene01">
			
				<!-- input frames -->
				<p><b>Input images</b></p>
				<div>
					<p>previous scene</p>
					
					
					<p>next scene</p>
				</div>
			
				<!-- 3D reconstruction output -->
				<p><b>3D reconstruction output</b></p>
				<model-viewer src="data/scene01/scene0102.glb" shadow-intensity="0" camera-controls="" touch-action="pan-y">
				</model-viewer>
			
				<!-- Other output -->
				<p><b>Other outputs</b></p>
	
				<p>input image</p>
				<p>depth map</p>
				<p>confidence map</p>
			
				

							
			</div>
			
			<!-- --- Scene02 --- -->
			
			
			<!-- --- Scene03 --- -->
			
			
			<!-- --- Scene04 --- -->
			

			<!-- --- Scene05 --- -->
			

			<!-- --- Scene06 --- -->
			

			<!-- --- Scene07 --- -->
			
			
			<!-- --- Scene08 --- -->
			
			
			<!-- --- Scene09 --- -->
			

		</div>

		<!-- ----- Visualizations ----- -->
		<div>
			<h3>Visualizations</h3>

			<div>
				<p><small>input image #1</small>
				</p>
				<p><small>input image #2</small>
				</p>
				<p><small>output point-cloud</small>
				</p>
				
				<div>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/bench/frame01.jpg"/>
					</p>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/bench/frame02.jpg"/>
					</p>
					<p>
						<video id="bench_video" height="200" autoplay="" muted="" loop="">
							<source src="nle-assets/examples/bench/backandforth.mp4" type="video/mp4"/>
							Your browser does not support the video tag.
						</video>
					</p>
				</div>
				<div>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/motorcycle/frame01.jpg"/>
					</p>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/motorcycle/frame02.jpg"/>
					</p>
					<p>
						<video id="bench_video" height="200" autoplay="" muted="" loop="">
							<source src="nle-assets/examples/motorcycle/backandforth.mp4" type="video/mp4"/>
							Your browser does not support the video tag.
						</video>
					</p>
				</div>
				<div>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/stopsign/frame01.jpg"/>
					</p>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/stopsign/frame02.jpg"/>
					</p>
					<p>
						<video id="bench_video" height="200" autoplay="" muted="" loop="">
							<source src="nle-assets/examples/stopsign/backandforth.mp4" type="video/mp4"/>
							Your browser does not support the video tag.
						</video>
					</p>
				</div>
				<div>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/toaster/frame01.jpg"/>
					</p>
					<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/toaster/frame02.jpg"/>
					</p>
					<p>
						<video id="bench_video" height="200" autoplay="" muted="" loop="">
							<source src="nle-assets/examples/toaster/backandforth.mp4" type="video/mp4"/>
							Your browser does not support the video tag.
						</video>
					</p>
				</div>				
			</div>
			
			<div>
				<p><small>one of the input images</small>
				</p>
				<p><small>output point-cloud</small>
				</p>
				<p><small>rendered with shading</small>
				</p>
				
				<p><img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/quali1.png"/>
				<img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/quali2.png"/>
				<img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/quali3.png"/>
				<img src="http://ablwr.github.io/blog/2024/03/01/researching-file-formats-27-microstation-dgn/nle-assets/examples/quali7.png"/>
			</p></div>
		</div>


		<!-- ----- Citations ----- -->
		<div> 
			<h3>BibTeX</h3> 
<pre>@journal{dust3r2023,
title={{DUSt3R: Geometric 3D Vision Made Easy}}, 
author={{Wang, Shuzhe and Leroy, Vincent and Cabon, Yohann and Chidlovskii, Boris and Revaud Jerome}}, 
journal={arXiv preprint 2312.14132},
year={2023}}</pre>
		</div>


		<!-- ----- See also ----- -->
		


		<!-- ----- Footer ----- -->
		
		
	
</div>
  </body>
</html>
