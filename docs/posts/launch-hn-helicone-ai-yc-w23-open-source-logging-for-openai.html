<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=35279155">Original</a>
    <h1>Launch HN: Helicone.ai (YC W23) – Open-source logging for OpenAI</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hi HN - Justin, Scott, and Barak here. We&#39;re excited to introduce Helicone (<a href="https://www.helicone.ai">https://www.helicone.ai</a>) an open-source logging solution for OpenAi applications. Helicone&#39;s one-line integration logs the prompts, completions, latencies, and costs of your OpenAI requests. It currently works with GPT, and can be integrated with one line of code. There’s a demo at <a href="https://www.helicone.ai/video">https://www.helicone.ai/video</a>.</p><p>Helicone&#39;s core technology is a proxy that routes all your OpenAI requests through our edge-deployed Cloudflare Workers. These workers are incredibly reliable and cause no discernible latency impact in production environments. As a proxy, we offer more than just observability: we provide caching and prompt formatting, and we&#39;ll soon add user rate limiting and model provider back off to make sure your app is still up when OpenAI is down.</p><p>Our web application then provides insights into key metrics, such as which users are disproportionately driving costs and what is the token usage broken down by prompts. You can filter this data based on custom logic and export it to other destinations.</p><p>Getting started with Helicone is quick and easy, regardless of the OpenAI SDK you use. Our proxy-based solution does not require a third party package—simply change your request&#39;s base URL from <a href="https://api.openai.com/v1" rel="nofollow">https://api.openai.com/v1</a> to <a href="https://oai.hconeai.com/v1" rel="nofollow">https://oai.hconeai.com/v1</a>. Helicone can be integrated with LangChain, LLama Index, and all other OpenAI native libraries. (<a href="https://docs.helicone.ai/quickstart/integrate-in-one-line-of-code">https://docs.helicone.ai/quickstart/integrate-in-one-line-of...</a>)</p><p>We have exciting new features coming up, one of which is an API to log user feedback. For instance, if you&#39;re developing a tool like GitHub Copilot, you can log when a user accepted or rejected a suggestion. Helicone will then aggregate your result quality into metrics and make finetuning suggestions for when you can save costs or improve performance.</p><p>Before launching Helicone, we developed several projects with GPT-3, including airapbattle.com, tabletalk.ai, and dreamsubmarine.com. For each project, we used a beta version of Helicone which gave us instant visibility into user engagement and result quality issues. As we talked to more builders and companies, we realized they were spending too much time building in-house solutions like this and that existing analytics products were not tailored to inference endpoints like GPT-3.</p><p>Helicone is developed under the Common Clause V1.0 w/ Apache 2.0 license so that you can use Helicone within your own infrastructure. If you do not want to self-host, we provide a hosted solution with 1k requests free per month to try our product. If you exceed that we offer a paid subscription as well, and you can view our pricing at <a href="https://www.helicone.ai/pricing">https://www.helicone.ai/pricing</a>.</p><p>We&#39;re thrilled to introduce Helicone to the HackerNews community and would love to hear your thoughts, ideas, and experiences related to LLM logging and analytics. We&#39;re eager to engage in meaningful discussions, so please don&#39;t hesitate to share your insights and feedback with us!</p></div></td></div></div>
  </body>
</html>
