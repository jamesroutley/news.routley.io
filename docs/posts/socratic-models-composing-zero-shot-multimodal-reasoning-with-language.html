<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://socraticmodels.github.io/">Original</a>
    <h1>Socratic Models – Composing Zero-Shot Multimodal Reasoning with Language</h1>
    
    <div id="readability-page-1" class="page"><div id="main"> 
            
            <!-- style="padding-bottom:1em" -->
            <div id="profile">
                <!-- <img src="images/profile.jpg"> -->
                <div id="profile-desc">
                    <p>Socratic Models</p>
                    <p>Composing Zero-Shot Multimodal Reasoning with Language</p>
                    <!-- <div id="profile-authors">
                        Andy Zeng                
                        Adrian Wong                
                        Stefan Welker                
                        Krzysztof Choromanski                
                        Federico Tombari<br>
                        Aveek Purohit          
                        Michael Ryoo         
                        Vikas Sindhwani         
                        Johnny Lee         
                        Vincent Vanhoucke         
                        Pete Florence<br>Google</div> -->
                </div>
                
                
            </div>
<!-- 
            <div class="section abstract">
            <p>
            <video width="70%" playsinline="" muted="" autoplay="" loop="">
                <source src="images/sm_1_venn.mp4" type="video/mp4">
            </video>
            </p>
            </div> -->

            
            <div>
                
                <!-- <img src="images/internet-venn.png" style="height: auto; width: 500px"> -->
                <video width="480px" playsinline="" muted="" autoplay="" loop="">
                    <source src="images/sm_1_venn.mp4" type="video/mp4"/>
                </video>
                <p>Large foundation models can exhibit unique capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT questions). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue -- in which new multimodal tasks are formulated as a guided language-based exchange between different pre-existing foundation models, without additional finetuning. In the context of egocentric perception, we present a case study of Socratic Models (SMs) that can provide meaningful results for complex tasks such as generating free-form answers to contextual questions about egocentric video, by formulating video Q&amp;A as short story Q&amp;A, i.e. summarizing the video into a short story, then answering questions about it. Additionally, SMs can generate captions for Internet images, and are competitive with state-of-the-art on zero-shot video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models zero-shot to capture new multimodal functionalities, without domain-specific data collection.</p>
                
            </div>

            <div>
            
            <p><img src="https://socraticmodels.github.io/images/ego-perception.png"/>
            </p></div>

            <div>
                
                <p>Our example Socratic-Model-based system for egocentric perception can respond to a variety of open-ended text prompts -- examples below:</p><!-- <img src="images/results-v4-website.png"> -->
                
                
                
                
            </div>


            

            
            <!-- <div class="divider"></div> -->

            <div>
                
                <p>We can also compose foundation models zero-shot to perform image captioning, through closed-loop Socratic dialogue:</p>
                </div>

            <div>
                <div data-flickity-options="{ &#34;wrapAround&#34;: true, &#34;draggable&#34;: false }">

                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/cc-02.jpeg"/>
                    </p>
                    <p>People gather under a blossoming cherry tree, enjoying the beauty of nature together.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/coco-03.jpeg"/>
                    </p>
                    <p>At the outdoor market, you can find everything from plantains to Japanese bananas.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/coco-02.jpeg"/>
                    </p>
                    <p>This image shows an inviting dining space with plenty of natural light.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/coco-04.jpeg"/>
                    </p>
                    <p>A family celebrates a special occasion with ice cream and cake.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/coco-05.jpeg"/>
                    </p>
                    <p>A wooden spoon and other kitchen utensils sit on a table in a restaurant kitchen.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/coco-06.jpeg"/>
                    </p>
                    <p>A motorcycle lies abandoned in a sandy desert.</p>
                  </div>

                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/cc-03.jpeg"/>
                    </p>
                    <p>This photo captures a person enjoying a meal at a restaurant. The spinach and nasturtium garnish on the plate makes for a beautiful and healthy meal.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/cc-06.jpeg"/>
                    </p>
                    <p>This cartoon shows one person enjoying a relaxing bath with their scrub bird.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/cc-04.jpeg"/>
                    </p>
                    <p>This photo was taken at a restaurant or pier. You can see the person enjoying their meal with a beautiful view of the water.</p>
                  </div>
                  <div>
                    <p><img src="https://socraticmodels.github.io/images/results/coco-01.jpeg"/>
                    </p>
                    <p>The three people in this photo appear to be enjoying a close encounter with an elephant. This majestic creature looks like a gentle giant, and the handlers seem to have a great rapport with her. What a fun and unique experience for these tourists!</p>
                  </div>

                </div>
            </div>
            
            <!-- <div class="divider"></div> -->

            <div>
                
                <p>We also can compose mutiple models to perform zero-shot video-to-text retrieval -- this achieves state-of-the-art for zero-shot methods, nearing the gap with finetuned-on-the-dataset methods:</p>
                </div>



            
            
            <p>Robotics and Augmented Reality at Google</p>
            
            <!-- <div class="divider"></div> -->






            <div>
                
                <!-- <p>This explainer is work in progress... ☕</p> -->
                <!-- <br> -->
                <p><img src="https://socraticmodels.github.io/images/SM-discussion.png"/></p><p>In this work we propose Socratic Models (SMs), a framework that uses structured dialogue between pre-existing foundation models, each of which can exhibit unique (but complementary) capabilities depending on the distributions of data on which they are trained. On various perceptual tasks, this work presents a case study of SMs with visual language models (VLMs, e.g., CLIP), large language models (LMs, e.g., GPT-3, RoBERTa), and audio language models (ALMs, e.g., Wav2CLIP, Speech2Text). From video search, to image captioning; from generating free-form answers to contextual reasoning questions, to forecasting future activities – SMs can provide meaningful results for complex tasks across classically challenging computer vision domains, without any model finetuning.</p>
                <!-- <img src="images/ego-vqa-results.png"> -->

                <!-- <img src="images/ego-perception.png">
                <img src="images/ego-vqa-system.png">
                <img src="images/ego-vqa-system.png">
                <p></p> -->




            </div>






            <div id="code">
                <div>
                    <h2>Code</h2>
                    <p>We plan to release prototypes in the form of self-contained colabs. They will be added to <a href="https://github.com/google-research/google-research/tree/master/socraticmodels">this repository</a> and linked here:<!-- <br>  •  Image Captioning    <a href="https://colab.research.google.com/drive/1KOlc9nN0NJ5GAif_dmuOqsRqlZycoIrc?usp=sharing"><img src="images/colabicon.png" style="height: 20px"></a> -->
                        
                        </p>
                    <br/>
                </div>
            </div>
            <div>
                <h2>Citation</h2>
                <div><p>@article{zeng2022socraticmodels,</p></div>
            </div>


            
            <!-- <div class="divider"></div> -->


            <div>
                
                <p>We thank Debidatta Dwibedi and Matthew O’Kelly for excellent feedback on improving this manuscript, Anelia Angelova, Jean-Jacques Slotine, Jonathan Tompson, Maria Attarian, Shuran Song, for fruitful technical discussions, Kan Huang for applications support, Ahmed Omran, Aren Jensen, Malcolm Slaney, Karolis Misiunas for advice on audio models, and Cody Wanner for YouTube videos.</p>
                <br/>
            </div>
            </div></div>
  </body>
</html>
