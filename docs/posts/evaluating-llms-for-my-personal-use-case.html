<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://darkcoding.net/software/personal-ai-evals-aug-2025/">Original</a>
    <h1>Evaluating LLMs for my personal use case</h1>
    
    <div id="readability-page-1" class="page"><section>
      <p>It’s great that AI can win maths Olympiads, but that’s not what I’m doing. I mostly ask basic Rust, Python, Linux and life questions. <em>So I did my own evaluation</em>.</p>
<p>I gathered 130 real prompts from my bash history (I use command line tool <a href="https://llm.datasette.io/en/stable/">llm</a>).</p>
<p>I had Qwen3 235B Thinking and Gemini 2.5 Pro group them into categories. They both chose very similar ones, broadly (with examples):</p>
<ul>
<li>Programming - “Write a bash script to ..”</li>
<li>Sysadmin  - “With <code>curl</code> how do I ..”</li>
<li>Technical explanations - “Explain underlay networks in a data center”</li>
<li>General knowledge and creative tasks - “Recipe for blackened seasoning”</li>
</ul>
<p>Then I had GPT-OSS-120B and GLM 4.5 pick three queries for each category from the 130 prompts. I used that to help me pick three entries per category, they are listed at the end.</p>
<p>I use <a href="https://openrouter.ai/">Open Router</a> everyday, and I used it for these evals. It’s the only place I know that has all the models, great prices, low latency, and a very sane API. I use my own fast and simple Rust CLI called <a href="https://github.com/grahamking/ort">ort</a>.</p>
<p>The set of models I chose to evaluate was based on my past experience with them, various leaderboards and their cost on Open Router. It is a mixture of reasoning, non-reasoning and hybrid models. I evaluated:</p>
<ul>
<li>anthropic/claude-sonnet-4 without reasoning</li>
<li>anthropic/claude-sonnet-4 with reasoning (I didn’t realise Sonnet can think!)</li>
<li>deepseek/deepseek-chat-v3-0324</li>
<li>deepseek/deepseek-r1-0528</li>
<li>google/gemini-2.5-flash</li>
<li>google/gemini-2.5-pro</li>
<li>moonshotai/kimi-k2</li>
<li>openai/gpt-oss-120b</li>
<li>qwen/qwen3-235b-a22b-2507</li>
<li>qwen/qwen3-235b-a22b-thinking-2507</li>
<li>z-ai/glm-4.5 with reasoning</li>
</ul>
<p>For the three programming questions I added these at the last minute, mostly because I was enjoying the process:</p>
<ul>
<li>inception/mercury-coder-small-beta</li>
<li>mistralai/devstral-medium-2507</li>
<li>qwen/qwen3-coder-480b-a35b-07-25</li>
<li>z-ai/glm-4.5-air without reasoning</li>
</ul>
<p>These extra coding models are not included in most of the results. Mercury Coder was decent and very fast, Qwen3 Coder was surprisingly bad.</p>
<p>I wrote a <a href="https://github.com/grahamking/ort/blob/master/src/bin/eval.rs">Rust eval script</a> to run the prompts against the models and blind the results, so I would not know which model the response comes from when I evaluate it. I recorded the cost, time-to-first-token (latency) and inter-token-latency (throughput) of each model.</p>
<blockquote>
<p>Why no closed OpenAI models?</p>
</blockquote>
<p>To access their best models via the API, OpenAI now requires you to complete a Know-You-Customer process similar to opening a bank account. That’s a bit intense for 3 seconds of GPU time. It’s best to think of OpenAI as not having an API.</p>
<blockquote>
<p>Why no Grok, Cohere, Ernie, etc?</p>
</blockquote>
<p>Grok I forgot about until it was too late. It was buried in Twitter/X for so long it’s not on my radar. The other models, well, there are so many good ones I had to cut somewhere. And Anthropic’s Opus is obviously too expensive.</p>
<h2 id="what-i-learnt">What I learnt </h2>
<p><strong>Evals are hard</strong>. One of my first questions was a recipe for blackened seasoning. Now I have 11 recipies. How do I evaluate that?!</p>
<p>For each eval I read all of the submissions which took a lot of time. For the programming ones I got three models to pick the three best entries based on correctness and conciseness (they never picked their own submission) and started with those. That helped me develop grading criteria when looking at the other submissions.</p>
<p><strong>All models are good</strong>. Almost all models got almost all my evaluations correct, I was happy with the vast majority of the results. They agree a lot too. Six of the eleven picked the same movie. Most of them used the same examples for the algebra question. I guess they share a lot of training data.</p>
<p><strong>Cost and latency vary dramatically</strong>, and are the tie-breaker. I want my questions to be so cheap that I never hesitate to ask. And they are! The cost of a query is usually in fractions of a cent. I also need it to be fast. I am using AI largely because it’s faster than searching the Internet.</p>
<p><strong>Closed models are not better</strong>. Even without taking into account cost or latency, the results from Google’s Gemini and Anthropic’s Claude were almost never the best of the bunch. They were fine, but one of the open models often had cleaner code, or a better explanation.</p>
<p><strong>Gemini 2.5 Flash is very fast</strong>, and <strong>Gemini 2.5 Pro is overpriced</strong>. Flash was consistenty the fastest response, and sometimes also the best. Note I am using it via OpenRouter same as all the other models, so there was no direct-to-Google advantage. Pro was by far the most expensive of the the models, because it’s expensive per token, and more verbose than open models (low token efficiency).</p>
<p><strong>Reasoning rarely helps</strong>. My questions are easy, speed is key. The major exception here was the poem (“Write a 10 line poem about Florida in the style of Shel Silverstein”). The three best poems were by thinking models, and there was a large gap in quality.</p>
<h2 id="prompts-and-winners">Prompts and winners </h2>
<p>Here are the prompts I used and the answer I prefered, by category. These are all real prompts from my bash history. Usually my simple questions had many good answers and cost and speed won the day.</p>
<h3 id="programming">Programming </h3>
<blockquote>
<p>Write a bash script to display a progress bar.</p>
</blockquote>
<p>Best: <code>inception/mercury-coder</code>. This is a <em>diffusion</em> model, and it is very fast. Really, try it! It’s not the strongest, but if it can answer, it wins on speed every time.</p>
<p>Also good, but slower: deepseek/deepseek-r1-0528, z-ai/glm-4.5</p>
<blockquote>
<p>Write a short Rust program that listens on a unix domain socket using only the standard library.</p>
</blockquote>
<p>Best: <code>deepseek/deepseek-chat-v3-0324</code>, <code>openai/gpt-oss-120b</code>, <code>qwen/qwen3-235b-a22b-2507</code>.</p>
<blockquote>
<p>In nvim using Lua, create a popup window that says “Hello”.</p>
</blockquote>
<p>Best: <code>z-ai/glm-4.5-air</code>. Perfect simple solution, very fast.</p>
<p>Almost identical code to glm-4.5-air, but slower and more expensive: qwen/qwen3-235b-a22b-thinking-2507, anthropic/claude-sonnet-4.</p>
<h3 id="sysadmin">Sysadmin </h3>
<blockquote>
<p>You are an expert system administrator. Tell me what Linux program I should use to rotate a log file. Suggest three possible answers and give short usage examples for each one, rotating file <code>/var/log/myfile.log</code> every 30 days.</p>
</blockquote>
<p>That “You are ..” formulation was an effective prompting technique back in the day, this one from way back in my bash history.</p>
<p>Best: <code>deepseek/deepseek-r1-0528</code></p>
<blockquote>
<p>With <code>jq</code>, how do I print only the keys of a JSON object?</p>
</blockquote>
<p>Best: <code>openai/gpt-oss-120b</code></p>
<p>I think every single model got this one right, but gpt-oss-120b was fast, cheap, and clear.</p>
<blockquote>
<p>On my Thinkpad Linux laptop with Intel Iris Xe graphics, I am seeing this error: “rust-lld: error: unable to find library -lOpenCL”. What Fedora package do I install to fix it?</p>
</blockquote>
<p>Best: <code>deepseek/deepseek-chat-v3-0324</code></p>
<h3 id="technical-explanations">Technical explanations </h3>
<blockquote>
<p>You are a data center networking expert and I am your intern. Explain what an underlay network is, and provide an example of why underlay networks are useful.</p>
</blockquote>
<p>Best: <code>anthropic/claude-sonnet-4</code> (not thinking), <code>deepseek/deepseek-r1-0528</code></p>
<p>I prefered Sonnet’s non-reasoning response to it’s reasoning response.</p>
<blockquote>
<p>You are a math tutor. Explain partial derivatives with respect to a variable, and provide a worked example for a beginner.</p>
</blockquote>
<p>Best: <code>moonshotai/kimi-k2</code></p>
<p>Difficult to evaluate this one because most of the models used the same examples. One big difference is whether they could emit UTF-8 for easy to read math symbols. The five that did were claude-sonnet-4, kimi-k2, qwen3-235b-a22b-thinking-2507, glm-4.5 and gemini-2.5-pro (flash did not).</p>
<blockquote>
<p>What is “quantization” in the context of large language models?</p>
</blockquote>
<p>Best: <code>deepseek/deepseek-chat-v3-0324</code>, <code>z-ai/glm-4.5</code> with thinking.</p>
<p>Again a bit difficult to evaluate, many explanations were very good. These two were good but also fast and cheap.</p>
<h3 id="general-knowledge-and-creativity">General knowledge and creativity </h3>
<blockquote>
<p>Write a 10 line poem about Florida in the style of Shel Silverstein</p>
</blockquote>
<p>Best equal: <code>qwen/qwen3-235b-a22b-thinking-2507</code>, <code>anthropic/claude-sonnet-4</code> w/ thinking, <code>deepseek/deepseek-r1-0528</code></p>
<p>Reasoning really helped here. My favorite poem is at the end of the post.</p>
<blockquote>
<p>Suggest a PG-13 rated movie that has great landscapes, feels calm and meditative, but is still enjoyable.</p>
</blockquote>
<p>Six of the eleven recommended <a href="https://www.imdb.com/title/tt0359950/">The Secret Life of Walter Mitty</a> which my whole family enjoyed. It was the perfect choice here, but I still found the consensus odd.</p>
<p>Best: <code>google/gemini-2.5-flash</code> got the answer first.</p>
<p>Also good: google/gemini-2.5-pro and anthropic/claude-sonnet-4 w/ thinking suggested movies I had not heard of which sounds interesting, so props to them for thinking outside the box.</p>
<p>Negative: openai/gpt-oss-120b hallucinated a movie. I could find no trace of this “Stargazing” movie, although I’d watch it. If it’s real please email me!</p>
<pre tabindex="0"><code> Alternative (if you prefer a more understated pace)**
 *Stargazing (2021) – PG‑13* – a slower, dialogue‑driven drama set in the Scottish Highlands, offering serene vistas and a reflective mood.
</code></pre><p>This was the only hallucination I noticed in all the whole process.</p>
<blockquote>
<p>How do you make cold brew coffee?</p>
</blockquote>
<p>OK this is a dumb eval. How to you decide?</p>
<p>Best: <code>qwen/qwen3-235b-a22b-2507</code></p>
<p>Cheap, fast, reasonable process clearly described.</p>
<blockquote>
<p>What or who is Louisiana named after?</p>
</blockquote>
<p>Best: <code>google/gemini-2.5-flash</code></p>
<p>All models got it. Flash was fastest.</p>
<h2 id="overall-results">Overall results </h2>
<p>There is no obvious winner.</p>
<h3 id="speed">Speed </h3>
<ul>
<li>The fastest: <code>google/gemini-2.5-flash</code></li>
<li>Also very fast:
<ul>
<li>moonshotai/kimi-k2</li>
<li>qwen/qwen3-235b-a22b-2507</li>
<li>deepseek/deepseek-chat-v3-0324</li>
<li>openai/gpt-oss-120b</li>
</ul>
</li>
<li>The slowest - thinking takes time:
<ul>
<li>qwen/qwen3-235b-a22b-thinking-2507</li>
<li>deepseek/deepseek-r1-0528</li>
<li>google/gemini-2.5-pro</li>
</ul>
</li>
</ul>
<p><a href="https://darkcoding.net/images/2025/08/evals-latency-by-model.png"><img src="https://darkcoding.net/images/2025/08/evals-latency-by-model.png" alt="Latency by model" width="auto"/></a></p>
<h3 id="price">Price </h3>
<ul>
<li>Cheapest:
<ul>
<li>moonshotai/kimi-k2</li>
<li>qwen/qwen3-235b-a22b-2507</li>
<li>deepseek/deepseek-chat-v3-0324</li>
<li>google/gemini-2.5-flash</li>
<li>openai/gpt-oss-120b</li>
</ul>
</li>
<li>Most expensive, by a long way - don’t use closed models unless you have to:
<ul>
<li>google/gemini-2.5-pro</li>
<li>anthropic/claude-sonnet-4</li>
</ul>
</li>
</ul>
<p><a href="https://darkcoding.net/images/2025/08/evals-spend-by-model.png"><img src="https://darkcoding.net/images/2025/08/evals-spend-by-model.png" alt="Spend by model" width="auto"/></a></p>
<p>Accuracy:</p>
<ul>
<li>Best: The two <code>DeepSeek</code> models and the two <code>Qwen3</code> models average the overall best.</li>
<li>Disappointing: Google’s Gemini 2.5 Pro and Anthropic’s Claude Sonnet are third place, even without considering price and speed, and are often quite poor. That really surprised me. I was glad I did the evals blind, because until now I thought Gemini 2.5 Pro was the best model.</li>
<li>Nearly all models passed nearly all evals fine. Accuracy is very good across the board.</li>
</ul>
<p>Since my evals, DeepSeek released hybrid model <code>deepseek/deepseek-chat-v3.1</code>, which replaces r1 (if you enable thinking) and v3 (if you disable thinking).</p>
<h2 id="my-decision-use-several-at-once">My decision: Use several at once </h2>
<p>Given that there is no overall winner, and that the best models are cheap and fast, why not query several at once? I use a <code>tmux</code> script to <a href="https://github.com/grahamking/ort?tab=readme-ov-file#tmux">split the window and query multiple models</a>.</p>
<p>Here is what I do today using some bash scripts and my <a href="https://github.com/grahamking/ort">ort openrouter CLI</a>:</p>
<ul>
<li>Quick everyday queries</li>
</ul>
<pre tabindex="0"><code>ort -p latency -m deepseek/deepseek-chat-v3.1 -r off -s &#34;Answer in as few words as possible. Use a brief style with short replies.&#34; &#34;-the-prompt-here-&#34;
</code></pre><p>It’s chat predecessor was the best average of latency, price and accuracy, so hopefully this one continues the tradition. 80% of the time this is all I need.</p>
<ul>
<li>Second opinion</li>
</ul>
<p>Sometimes I want to sanity check deepseek-chat, or I’m not entirely satisfied with that answer, then my script splits the window in two and runs:</p>
<pre tabindex="0"><code>ort -p latency -r low -m google/gemini-2.5-flash -s &#34;Answer in as few words as possible. Use a brief style with short replies.&#34; &#34;-prompt-&#34;
</code></pre><p>and</p>
<pre tabindex="0"><code>ort -p latency -m qwen/qwen3-235b-a22b-07-25 -s &#34;Answer in as few words as possible. Use a brief style with short replies.&#34; &#34;-prompt-&#34;
</code></pre><ul>
<li>Thinking</li>
</ul>
<p>If I think my query needs a bit more brainpower - which my evals taught me is far less often than I think - I split the tmux window into three panes and run these three:</p>
<pre tabindex="0"><code># I&#39;m not sure if thinking effort is configurable
ort -r medium -p latency -m qwen/qwen3-235b-a22b-thinking-2507 -s &#34;Make your answer concise but complete. No yapping. Direct professional tone. No emoji.&#34; &#34;-prompt-&#34;
</code></pre><p>and</p>
<pre tabindex="0"><code>ort -r 2048 -m anthropic/claude-sonnet-4 -s &#34;Make your answer concise but complete. No yapping. Direct professional tone. No emoji.&#34; &#34;-prompt-&#34;
</code></pre><p>and</p>
<pre tabindex="0"><code># Replaces DeepSeek R1
ort -r high -p latency -m deepseek/deepseek-chat-v3.1 -s &#34;Make your answer concise but complete. No yapping. Direct professional tone. No emoji.&#34; &#34;-prompt-&#34;
</code></pre><p>I change the system prompt here to be a little more verbose, and I add Sonnet which is expensive, almost as a sanity check on the open models. I’ve never actually needed it so far.</p>
<h2 id="caveats">Caveats </h2>
<p>I used Open Router for all this with it’s default priority. The cost and performance depend entirely on the providers available for each model. I think GLM 4.5 may have lost out here because it was quite new when I did the evals, so it didn’t have as solid a set of providers. GPT-OSS-120B was also rumoured to have some poor quality providers at the time.</p>
<p>If you’re an Open Router user, you can go into settings and allow models to train on your queries, that unlocks much cheaper providers. I did that here.</p>
<p>I used the default reasoning setting (typically “medium”). Setting it to low or high may have impacted result quality, and likely would impact performance.</p>
<p>And of course these evals are personal and very subjective.</p>
<h2 id="bonus---the-poem">Bonus - the poem </h2>
<p>One of the evals was:</p>
<blockquote>
<p>Write a 10 line poem about Florida in the style of Shel Silverstein</p>
</blockquote>
<p>Here is my favorite, courtesy of <code>qwen/qwen3-235b-a22b-thinking-2507</code>:</p>
<p>The gator in the swamp said, “Why </p>
<p>A crab clicked past on flaming sand, </p>
<p>The man just laughed and tossed shrimp near. </p>

    </section></div>
  </body>
</html>
