<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vgel.me/posts/handmade-transformer/">Original</a>
    <h1>I made a transformer by hand (no training)</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    
<blockquote>
<p><em>Intended audience: some familiarity with language models, interested in how transformers do stuff (but might be a bit rusty on matrices)</em></p>
</blockquote>
<p>I&#39;ve been wanting to understand transformers and attention better for awhile now—I&#39;d read The Illustrated Transformer, but still didn&#39;t feel like I had an intuitive understanding of what the various pieces of attention were <em>doing</em>.
What&#39;s the difference between <code>q</code> and <code>k</code>?
And don&#39;t even get me started on <code>v</code>!</p>
<p>So I decided to make a transformer to predict a simple sequence <small>(specifically, a decoder-only transformer with a similar architecture to GPT-2)</small> manually—not by training one, or using pretrained weights, but instead by <em>assigning each weight, by hand</em>, over an evening.
And—it worked!
I feel like I understand transformers much better now, and hopefully after reading this, so will you.</p>
<span id="continue-reading"></span>
<p>The basic outline of what we need to do is:</p>
<ul>
<li>Pick a task—not too easy, but not as hard as &#34;write fluent English text&#34;, since that needs at least a few hundred million parameters!</li>
<li>Pick our model dimensions for the task</li>
<li>Design position (<code>wpe</code>) and token (<code>wte</code>) embedding weights</li>
<li>The hard part—design a transformer block to do the actual computation!
<ul>
<li>First, we&#39;ll need a <code>c_attn</code> layer to generate <code>q</code>, <code>k</code>, and <code>v</code> matrices</li>
<li>Then, we&#39;ll need a <code>c_proj</code> layer to project that result back to an embedding</li>
</ul>
</li>
<li>Finally, use the token embedding weights from before (<code>wte</code>) to project that back to a set of next token logits!</li>
</ul>
<details>
<summary>Quick refresher on matrices</summary>
<p>We&#39;ll be doing a lot of matrix math, mostly scalar addition / multiplication and matrix multiplication (the <code>@</code> operator in Python).
If you don&#39;t have a strong math/ML background, or just are a little rusty, here&#39;s a quick rundown of matrix math.
Feel free to skip this section if you&#39;re confident!</p>
<p>Scalar addition and multiplication just adds or multiplies that scalar with every element of the matrix:</p>
<div>
  
  <p>* 2 + 1 =</p>
  <table>
    <tbody><tr><td>1 * 2 + 1</td><td>2 * 2 + 1</td></tr>
    <tr><td>3 * 2 + 1</td><td>4 * 2 + 1</td></tr>
  </tbody></table>
  <p>=</p>
  
</div>
<p>Adding or subtracting two matrices is also defined as an element-wise operation between corresponding elements in each matrix:</p>


<p>There&#39;s also an <em>element-wise</em> matrix multiplication that works the same way, but when people refer to matrix multiplication they usually mean the what&#39;s also called the <em>matrix product</em>, where two matrices of shape <code>A x B</code> and <code>B x C</code> are combined to produce a matrix with dimensions <code>A x C</code>.</p>
<p>For example, a matrix of shape 2 x 4 (2 rows, 4 columns) can be multiplied by a matrix of shape 4 x 3 (4 rows, 3 columns) because the number of columns in the first matrix matches the number of rows in the second (4), and produces a matrix of shape 2 x 3 (the number of rows in the first x the number of columns in the second).</p>
<p>The actual operation involves taking the dot product (sum of the element-wise product) of the corresponding row and column for each position in the output matrix.</p>
<p>The red row and column in the following example are used to calculate the red cell in the output matrix, and same for the green cell:</p>

<p>For the red cell in the output (row 0, column 0), we take the red row of the first matrix (row 0) and the red column of the second matrix (column 0), and do a dot product (<code>1*1 + 2*4 + 3*7 + 4*10</code>) to get 70.</p>
<p>Likewise, for the green cell in the output (row 1, column 2), we take the green row of the first matrix (row 1) and the green column of the second matrix (column 2), and do a dot product (<code>5*3 + 6*6 + 7*9 + 8*12</code>) to get 210.</p>
</details>
<h2 id="Picking_a_task"><a href="#Picking_a_task">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Picking_a_task"/>
</a>Picking a task</h2>
<p>I originally started with just predicting sequences like <code>&#34;ababababab&#34;</code>, but quickly realized that because transformers predict the <em>shifted</em> sequence, this would be too easy—it wouldn&#39;t require using the position embeddings.
<small>(Instead, the algorithm would just be &#34;If I am an <code>a</code>, then predict <code>b</code>, otherwise predict <code>a</code>.&#34;)</small></p>
<p>The more involved task I settled on was predicting the sequence <code>&#34;aabaabaabaab...&#34;</code> (that is to say, <code>(aab)*</code>), which requires querying the previous <em>two</em> tokens to know if the output should be an <code>a</code> (previous tokens are <code>ab</code> or <code>ba</code>) or a <code>b</code> (previous tokens are <code>aa</code>).</p>
<h2 id="Sidetrack:_Designing_a_tokenization_scheme"><a href="#Sidetrack:_Designing_a_tokenization_scheme">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Sidetrack:_Designing_a_tokenization_scheme"/>
</a>Sidetrack: Designing a tokenization scheme</h2>
<p>Since we only have two symbols to worry about, I used a very simple scheme where <code>a</code> is <code>0</code> and <code>b</code> is <code>1</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>CHARS </span><span>= </span><span>[</span><span>&#34;a&#34;</span><span>, </span><span>&#34;b&#34;</span><span>]
</span><span>def </span><span>tokenize</span><span>(</span><span>s</span><span>): </span><span>return </span><span>[CHARS.index(c) </span><span>for </span><span>c </span><span>in </span><span>s]
</span><span>def </span><span>untok</span><span>(</span><span>tok</span><span>): </span><span>return </span><span>CHARS[tok]
</span><span>
</span><span># examples:
</span><span>tokenize(</span><span>&#34;aabaa&#34;</span><span>) </span><span># =&gt; [0, 0, 1, 0, 0]
</span><span>untok(</span><span>0</span><span>) </span><span># =&gt; &#34;a&#34;
</span><span>untok(</span><span>1</span><span>) </span><span># =&gt; &#34;b&#34;
</span></code></pre>
<h2 id="Picking_a_model"><a href="#Picking_a_model">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Picking_a_model"/>
</a>Picking a model</h2>
<p>I based the code for the model off <a href="https://github.com/jaymody/picoGPT/blob/main/gpt2.py">jaymody&#39;s picoGPT implementation of GPT-2</a>, with a few changes to make things simpler.
This left me with the following architecture for assigning weights.
Don&#39;t worry if this doesn&#39;t make sense yet, I&#39;ll be explaining it as we go!</p>
<pre data-lang="python"><code data-lang="python"><span># based on https://github.com/jaymody/picoGPT/blob/main/gpt2.py (MIT license)
</span><span>
</span><span>import </span><span>numpy </span><span>as </span><span>np
</span><span>
</span><span>def </span><span>softmax</span><span>(</span><span>x</span><span>):
</span><span>  exp_x </span><span>= </span><span>np.exp(x </span><span>- </span><span>np.max(x, </span><span>axis</span><span>=-</span><span>1</span><span>, </span><span>keepdims</span><span>=</span><span>True</span><span>))
</span><span>  </span><span>return </span><span>exp_x </span><span>/ </span><span>np.sum(exp_x, </span><span>axis</span><span>=-</span><span>1</span><span>, </span><span>keepdims</span><span>=</span><span>True</span><span>)
</span><span>
</span><span># [m, in], [in, out], [out] -&gt; [m, out]
</span><span>def </span><span>linear</span><span>(</span><span>x</span><span>, </span><span>w</span><span>, </span><span>b</span><span>):
</span><span>  </span><span>return </span><span>x </span><span>@ </span><span>w </span><span>+ </span><span>b
</span><span>
</span><span># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]
</span><span>def </span><span>attention</span><span>(</span><span>q</span><span>, </span><span>k</span><span>, </span><span>v</span><span>, </span><span>mask</span><span>):
</span><span>  </span><span>return </span><span>softmax(q </span><span>@ </span><span>k.T </span><span>/ </span><span>np.sqrt(q.shape[</span><span>-</span><span>1</span><span>]) </span><span>+ </span><span>mask) </span><span>@ </span><span>v
</span><span>
</span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>def </span><span>causal_self_attention</span><span>(</span><span>x</span><span>, </span><span>c_attn</span><span>, </span><span>c_proj</span><span>):
</span><span>  </span><span># qkv projections
</span><span>  x </span><span>= </span><span>linear(x, </span><span>**</span><span>c_attn)  </span><span># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]
</span><span>
</span><span>  </span><span># split into qkv
</span><span>  q, k, v </span><span>= </span><span>np.split(x, </span><span>3</span><span>, </span><span>axis</span><span>=-</span><span>1</span><span>)  </span><span># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]
</span><span>
</span><span>  </span><span># causal mask to hide future inputs from being attended to
</span><span>  causal_mask </span><span>= </span><span>(</span><span>1 </span><span>- </span><span>np.tri(x.shape[</span><span>0</span><span>], </span><span>dtype</span><span>=</span><span>x.dtype)) </span><span>* -</span><span>1e10  </span><span># [n_seq, n_seq]
</span><span>
</span><span>  </span><span># perform causal self attention
</span><span>  x </span><span>= </span><span>attention(q, k, v, causal_mask)  </span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>
</span><span>  </span><span># out projection
</span><span>  x </span><span>= </span><span>linear(x, </span><span>**</span><span>c_proj)  </span><span># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]
</span><span>
</span><span>  </span><span>return </span><span>x
</span><span>
</span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>def </span><span>transformer_block</span><span>(</span><span>x</span><span>, </span><span>attn</span><span>):
</span><span>  x </span><span>= </span><span>x </span><span>+ </span><span>causal_self_attention(x, </span><span>**</span><span>attn)
</span><span>  </span><span>return </span><span>x
</span><span>
</span><span># [n_seq] -&gt; [n_seq, n_vocab]
</span><span>def </span><span>gpt</span><span>(</span><span>inputs</span><span>, </span><span>wte</span><span>, </span><span>wpe</span><span>, </span><span>blocks</span><span>):
</span><span>  </span><span># token + positional embeddings
</span><span>  x </span><span>= </span><span>wte[inputs] </span><span>+ </span><span>wpe[</span><span>range</span><span>(</span><span>len</span><span>(inputs))]  </span><span># [n_seq] -&gt; [n_seq, n_embd]
</span><span>
</span><span>  </span><span># forward pass through n_layer transformer blocks
</span><span>  </span><span>for </span><span>block </span><span>in </span><span>blocks:
</span><span>    x </span><span>= </span><span>transformer_block(x, </span><span>**</span><span>block)  </span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>
</span><span>  </span><span># project to vocab
</span><span>  </span><span>return </span><span>x </span><span>@ </span><span>wte.T  </span><span># [n_seq, n_embd] -&gt; [n_seq, n_vocab]
</span></code></pre>
<p>This roughly corresponds to the following architecture diagram:</p>
<center>
<img src="https://vgel.me/posts/handmade-transformer/transformer.drawio.svg"/>
</center>
<h3 id="Picking_the_model_dimensions"><a href="#Picking_the_model_dimensions">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Picking_the_model_dimensions"/>
</a>Picking the model dimensions</h3>
<p>There were 3 model parameters to choose here:</p>
<ul>
<li>Context length</li>
<li>Vocabulary size</li>
<li>Embedding size</li>
</ul>
<p>Context length is the maximum number of tokens the model will see at a time.
Theoretically this task only needs the previous 2 tokens—but let&#39;s go with 5 to make it a little more difficult, since then we&#39;ll also need to ignore the irrelevant tokens.</p>
<p>Vocabulary size is the number of distinct tokens the model will see.
In a real model, there are tradeoffs between generalization, number of distinct tokens to learn, context length usage, etc.
However, our task is much simpler, so in our model, we&#39;ll only need two tokens: <code>a</code> (<code>0</code>) and <code>b</code> (<code>1</code>).</p>
<p>Embedding size is the size of the vectors that the model will learn for each token/position, and will also use internally.
I picked 8 rather arbitrarily, which ended up being exactly the size required :-)</p>
<p>In summary,</p>
<pre data-lang="python"><code data-lang="python"><span>N_CTX </span><span>= </span><span>5
</span><span>N_VOCAB </span><span>= </span><span>2
</span><span>N_EMBED </span><span>= </span><span>8
</span></code></pre>
<h2 id="Designing_the_embedding_weights"><a href="#Designing_the_embedding_weights">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Designing_the_embedding_weights"/>
</a>Designing the embedding weights</h2>
<p>The first thing that happens is the list of token ids (<code>[0, 1, 0, ...]</code>) gets turned into a <code>seq_len x embedding_size</code> matrix that combines the position and type of each token:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>gpt</span><span>(</span><span>inputs</span><span>, </span><span>wte</span><span>, </span><span>wpe</span><span>, </span><span>blocks</span><span>):  </span><span># [n_seq] -&gt; [n_seq, n_vocab]
</span><span>  </span><span># token + positional embeddings
</span><span>  x </span><span>= </span><span>wte[inputs] </span><span>+ </span><span>wpe[</span><span>range</span><span>(</span><span>len</span><span>(inputs))]  </span><span># [n_seq] -&gt; [n_seq, n_embd]
</span><span>  </span><span>...
</span></code></pre>
<p>That means the first things we need to design are <code>wte</code> (weights for token embeddings) and <code>wpe</code> (weights for position embeddings).
We&#39;ll use a 1-hot scheme for each, meaning each class of thing has a <code>1</code> in a unique position.</p>
<p>We&#39;ll use first five embedding elements for the position 1-hot embedding: position 0 will be represented as <code>[1, 0, 0, 0, 0]</code>, position 1 as <code>[0, 1, 0, 0, 0]</code>, and so on up to position 4 as <code>[0, 0, 0, 0, 1]</code>.</p>
<p>Likewise, we&#39;ll use the next two embedding elements for the token id 1-hot embeddings: token <code>a</code> will be represented as <code>[1, 0]</code>, and token <code>b</code> as <code>[0, 1]</code>.</p>
<pre data-lang="python"><code data-lang="python"><span>MODEL </span><span>= </span><span>{
</span><span>  </span><span>&#34;wte&#34;</span><span>: np.array(
</span><span>    </span><span># one-hot token embeddings
</span><span>    [
</span><span>      [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># token `a` (id 0)
</span><span>      [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>],  </span><span># token `b` (id 1)
</span><span>    ]
</span><span>  ),
</span><span>  </span><span>&#34;wpe&#34;</span><span>: np.array(
</span><span>    </span><span># one-hot position embeddings
</span><span>    [
</span><span>      [</span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 0
</span><span>      [</span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 1
</span><span>      [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 2
</span><span>      [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 3
</span><span>      [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 4
</span><span>    ]
</span><span>  ),
</span><span>  </span><span>...</span><span>: </span><span>...</span><span>,
</span><span>}
</span></code></pre>
<p>If we encode an entire sequence <code>&#34;aabaa&#34;</code> with this scheme, we get the following embedding matrix of shape <code>5 x 8</code> (<code>seq_len x embedding_size</code>):</p>
<div>

<p>→</p>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 0, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 1, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td>
  <td>position 2, token <code>b</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 3, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td>
  <td>position 4, token <code>a</code></td>
</tr>
</tbody></table>
</div>
<p>This gives us the embedding matrix the rest of the model will work with, until it gets projected back to vocabulary-space at the end.
Note that the 7th position isn&#39;t used—that will be our scratch space in the transformer block.
Speaking of...</p>
<h2 id="Designing_the_transformer_block"><a href="#Designing_the_transformer_block">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Designing_the_transformer_block"/>
</a>Designing the transformer block</h2>
<p>The model code supports multiple transformer blocks, but we&#39;ll only be using one.
Our block consists of two parts: an attention head, and a linear network that will project the attention result matrix back to the common <code>seq_len x embedding_size</code> matrix the network usually works with.</p>
<p>Let&#39;s focus on the attention head first.</p>
<h3 id="Designing_the_attention_head"><a href="#Designing_the_attention_head">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Designing_the_attention_head"/>
</a>Designing the attention head</h3>
<p>The attention head is the main part of the transformer block—it&#39;s where attention happens, and as we all know, Attention Is All You Need!</p>
<p><small>(Most transformers nowadays have multiple parallel heads per transformer block (&#34;multi-head attention&#34;), which I believe is where the &#34;attention head&#34; terminology comes from, but we&#39;ll only use one for simplicity. For our purposes, &#34;attention head&#34; = attention layer.)</small></p>
<p>As you might expect, the main part of the attention head is attention, which in our case is defined as:</p>
<pre data-lang="python"><code data-lang="python"><span># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]
</span><span>def </span><span>attention</span><span>(</span><span>q</span><span>, </span><span>k</span><span>, </span><span>v</span><span>, </span><span>mask</span><span>):
</span><span>  </span><span>return </span><span>softmax(q </span><span>@ </span><span>k.T </span><span>/ </span><span>np.sqrt(q.shape[</span><span>-</span><span>1</span><span>]) </span><span>+ </span><span>mask) </span><span>@ </span><span>v
</span></code></pre>
<p>The parameters here are:</p>
<ul>
<li><code>q</code>, or &#34;query&#34;</li>
<li><code>k</code>, or &#34;key&#34;</li>
<li><code>v</code>, or &#34;value&#34;</li>
<li>and <code>mask</code>, which is a non-learned parameter used to prevent the model from cheating during training by looking at future tokens, which will make more sense later.</li>
</ul>
<p>These are usually explained by analogy to a dictionary lookup:</p>
<pre data-lang="python"><code data-lang="python"><span>DICT </span><span>= </span><span>{ k: v }
</span><span>DICT[q]
</span></code></pre>
<p>...but I never found this explanation very useful.
I think working through how attention actually works helped it make a lot more sense—so let&#39;s do that!</p>
<p>In our model, the weights for attention are defined in <code>c_attn</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>Lg </span><span>= </span><span>1024 </span><span># Large
</span><span>
</span><span>MODEL </span><span>= </span><span>{
</span><span>  </span><span>...</span><span>: </span><span>...</span><span>,
</span><span>  </span><span>&#34;blocks&#34;</span><span>: [
</span><span>    {
</span><span>      </span><span>&#34;attn&#34;</span><span>: {
</span><span>        </span><span>&#34;c_attn&#34;</span><span>: {  </span><span># generates qkv matrix
</span><span>          </span><span>&#34;b&#34;</span><span>: np.zeros(N_EMBED </span><span>* </span><span>3</span><span>),
</span><span>          </span><span>&#34;w&#34;</span><span>: np.array(
</span><span>            </span><span># this is where the magic happens
</span><span>            </span><span># fmt: off
</span><span>            [
</span><span>              [Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>              [Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>              [</span><span>0.</span><span>, Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>              [</span><span>0.</span><span>, </span><span>0.</span><span>, Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>              [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>              [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>], </span><span># v
</span><span>              [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>-</span><span>1</span><span>], </span><span># v
</span><span>              [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                  </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>            ]
</span><span>            </span><span># fmt: on
</span><span>          ),
</span><span>        },
</span><span>        </span><span>...</span><span>: </span><span>...</span><span>,
</span><span>      }
</span><span>    }
</span><span>  ]
</span><span>}
</span></code></pre>
<p>That looks intimidating!
But <code>c_attn</code> is just a regular fully-connected layer, with dimension <code>embed_size x (embed_size * 3)</code>.
When we take its product with the <code>seq_len x embed_size</code> embedding matrix we calculated above, we get a matrix of size <code>seq_len x (embed_size * 3)</code>—called the <code>qkv</code> matrix.
We then split that <code>qkv</code> matrix into 3 matrices of size <code>seq_len x embed_size</code>—<code>q</code>, <code>k</code>, and <code>v</code>.</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>causal_self_attention</span><span>(</span><span>x</span><span>, </span><span>c_attn</span><span>, </span><span>c_proj</span><span>):
</span><span>  </span><span># qkv projections
</span><span>  x </span><span>= </span><span>linear(x, </span><span>**</span><span>c_attn) </span><span># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]
</span><span>  </span><span># split into qkv
</span><span>  q, k, v </span><span>= </span><span>np.split(x, </span><span>3</span><span>, </span><span>axis</span><span>=-</span><span>1</span><span>) </span><span># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]
</span><span>  </span><span>...
</span></code></pre>
<p><small>(In the weights above, I formatted the <code>c_attn</code> weights on different lines to show which ones generate the <code>q</code>, <code>k</code>, and <code>v</code> parts of the <code>qkv</code> matrix.)</small></p>
<p>So let&#39;s just run the embedding matrix from before through <code>c_attn</code> and see what happens!
If we take the embedding...</p>
<div>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 0, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 1, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td>
  <td>position 2, token <code>b</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 3, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td>
  <td>position 4, token <code>a</code></td>
</tr>
</tbody></table>
</div>
<p>...and run it through <code>embedding @ c_attn[&#34;w&#34;] + c_attn[&#34;b&#34;]</code>, we get the following <code>5 x 24</code> (<code>seq_len x (embed_size * 3)</code>) <code>qkv</code> matrix. <small>(The thick lines here show where we will <code>np.split</code> this matrix in a second)</small>:</p>
<div>
<table>
<tbody><tr>
<th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th>
<th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th>
<th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th>
</tr>
<tr>
  <td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>Let&#39;s ignore <code>v</code> for now and focus on <code>q</code> and <code>k</code>.</p>
<p><code>k</code> might make sense—it&#39;s just the 1-hot position embeddings, isolated from the combined embedding matrix.
You can think of this as what each token is &#34;offering&#34;—its position.</p>
<p>But what is <code>q</code>?
If <code>k</code> is what each token is offering, then <code>q</code> is what each token is looking for—but what does that mean in practice?
Well, in attention, <code>k</code> will be transposed and multiplied with <code>q</code> in <code>q @ k.T</code>, producing a <code>seq_len x seq_len</code> matrix:</p>
<div>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td> </tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr><th colspan="5">k.T</th></tr>
<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
</tbody></table>
<p>=</p>
<table>
<tbody><tr>
  <td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td>
</tr>
</tbody></table>
</div>
<p>...and when we add the mask and softmax the whole thing (<code>softmax(q @ k.T + mask)</code>), suddenly it starts to make sense!</p>
<details>
<summary>Quick refresher on softmax</summary>
<p>Softmax is defined as:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>softmax</span><span>(</span><span>x</span><span>):
</span><span>    exp_x </span><span>= </span><span>np.exp(x </span><span>- </span><span>np.max(x, </span><span>axis</span><span>=-</span><span>1</span><span>, </span><span>keepdims</span><span>=</span><span>True</span><span>))
</span><span>    </span><span>return </span><span>exp_x </span><span>/ </span><span>np.sum(exp_x, </span><span>axis</span><span>=-</span><span>1</span><span>, </span><span>keepdims</span><span>=</span><span>True</span><span>)
</span></code></pre>
<p>Which looks intimidating!
But when you break it down, what this is doing is:</p>
<ul>
<li>For each row in the matrix (<code>axis = -1</code>)...</li>
<li>Subtract the largest value in the row from the others (so every element will be negative, except the largest which will be 0)</li>
<li>Calculate the exponential for each element (<code>matrix[i, j] = e^matrix[i, j]</code>)
<ul>
<li>This makes the largest element 1, because <code>e^0 = 1</code></li>
<li>Values slightly smaller than the largest element will be close to 1, because (e.g.) <code>e^-0.5~=0.6</code></li>
<li>Values much smaller than the largest element will be close to 0, because (e.g) <code>e^-10~=0.00004</code>.</li>
</ul>
</li>
<li>Finally, we divide each element in the row by the sum of all the values in the row, so that the row sums to 1 (and can be used as a probability distribution)</li>
</ul>
<p>We treat each row independently, because in our model, each row will represent a different input token with independent prediction probabilities.</p>
<p>Here&#39;s an example of softmax in action:</p>
<div>
<p>x =</p>

<p>; softmax(x) =</p>
<table>
<tbody><tr><td>0.58</td><td>0.21</td><td>0.21</td></tr>
<tr><td>0.58</td><td>0.21</td><td>0.21</td></tr>
<tr><td>0.9999</td><td>0.00005</td><td>0.00005</td></tr>
<tr><td>0.9999</td><td>0.00005</td><td>0.00005</td></tr>
</tbody></table>
</div>
<p>Notice how the largest number in the row is always given the largest value in the softmax result, but if the number is <em>much</em> larger than the other numbers in the row, it gets almost 100% of the value of the row.
Also notice that the <em>magnitude</em> of the largest value doesn&#39;t matter, just the <em>relative</em> value compared to the other values—this is because we subtract the largest value as the first step, so a row of <code>[10, 0, 0]</code> gets turned into <code>[0, -10, -10]</code> anyways.</p>
<p>A decent intuition for softmax is that it works like a smoothed argmax: argmax maps the largest element to 1 and every other element to 0, and softmax is a smoother version of that, where some of the mass is shifted to the other positions.</p>
</details>
<div>
<div>
<p>softmax(</p>
<table>
<tbody><tr>
  <td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td>
</tr>
</tbody></table>
<p>
+ mask)
</p>
</div>
<p>=</p>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0.5</td><td>0.5</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0.5</td><td>0.5</td>
</tr>
</tbody></table>
</div>
<p>Think of each row as what&#39;s needed to generate the prediction for that row (e.g., row 0 is the information needed to generate the model&#39;s prediction after seeing token 0, the first token), and each column as which tokens to pay attention to.
Furthermore, remember that the mask prevents the model from seeing into the future!
<small>(I&#39;ll explain how in a minute.)</small></p>
<p>That means the first prediction (row 0) isn&#39;t able to pay attention to any token except the first, so it puts 100% of its attention on that token—in other words, the first row has a <code>1</code> in the first column, and zeroes everywhere else.</p>
<p>But for all the other predictions, the model has at least two tokens to pay attention to, and for the <code>aabaabaab...</code> task, it doesn&#39;t ever need any more than two!
So the model splits its attention for that token evenly between the latest two accessible (non-masked) tokens.
That means the prediction for the second token (row 1) pays equal attention to token 0 and token 1, the prediction for the third token (row 3) pays equal attention to token 1 and token 2, and so on—so we see two non-zero cells, with <code>0.5</code> in each.</p>
<p>So what&#39;s the <code>mask</code> term added in <code>softmax(q @ k.T + mask)</code>?
It&#39;s just the following matrix:</p>
<div>
<table>
<tbody><tr>
  <td>0</td><td>-∞</td><td>-∞</td><td>-∞</td><td>-∞</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>-∞</td><td>-∞</td><td>-∞</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>-∞</td><td>-∞</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>-∞</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
</tr>
</tbody></table>
</div>
<p>All this does is prevent the model from &#34;cheating&#34; during regular gradient-descent training—without this mask, the model would be incentivized to generate its prediction for the first token based on the value of the <em>second</em> token!
By adding -∞, we force those positions down (see the softmax expando above for an explanation of why) so that the matrix coming out of <code>softmax</code> will have 0 in all masked (= future) token positions.
This forces the model to actually <em>learn</em> how to predict those positions instead of cheating by looking ahead.
In our case the mask isn&#39;t doing anything because this handmade transformer isn&#39;t designed to cheat, but leaving the mask in keeps things closer to the real GPT-2 architecture.</p>
<p><small>(Likewise, the scaling by <code>np.sqrt(q.shape[-1])</code> helps produce better gradients during real training, but doesn&#39;t affect our handmade transformer.)</small></p>
<p>But enough about the mask and scaling: all that really matters is the result of <code>softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask)</code> is:</p>
<div>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0.5</td><td>0.5</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0.5</td><td>0.5</td>
</tr>
</tbody></table>
</div>
<p>To recap, each row here represents the attention the model will pay to different token positions (columns) when making its prediction for that position—that is, predicting the next token.
To make the prediction for the first token (predicting the second token), we can only pay attention to the first token.
To make the prediction for the second token (predicting the third token), we split attention between the first and second tokens, and so on.</p>
<p>But what about <code>v</code>?
The final step of attention is multiplying the matrix from above with v: <code>softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v</code>, note the <code>@ v</code>.
So what&#39;s <code>v</code>?</p>
<p>Well, recall that passing the embedding matrix from before...</p>
<div>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 0, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 1, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td>
  <td>position 2, token <code>b</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td>
  <td>position 3, token <code>a</code></td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td>
  <td>position 4, token <code>a</code></td>
</tr>
</tbody></table>
</div>
<p>...through <code>linear</code> with <code>c_attn</code>, we get the following <code>qkv</code> matrix:</p>
<div>
<table>
<tbody><tr>
<th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th>
<th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th>
<th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th>
</tr>
<tr>
  <td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>Looking at the <code>v</code> part, we can see that it only ever has one element (column 7) ever set, and that element is <code>1</code> when the row is an <code>a</code> token, and <code>-1</code> when the row is a <code>b</code> token.
That means all that&#39;s happening in <code>v</code> is that the one-hot token encoding (<code>a = [1, 0], b = [0, 1]</code>) is being turned into a <code>1</code>/<code>-1</code> encoding!</p>
<p>That might sound pretty useless, but recall that our task is to predict <code>aabaab</code>, or in other words:</p>
<ul>
<li>if previous tokens are (<span>a</span>, <span>a</span>) =&gt; predict <span>b</span></li>
<li>if previous tokens are (<span>a</span>, <span>b</span>) =&gt; predict <span>a</span></li>
<li>if previous tokens are (<span>b</span>, <span>a</span>) =&gt; predict <span>a</span></li>
<li>if previous tokens are (<span>b</span>, <span>b</span>) =&gt; <span>error, out of domain</span></li>
</ul>
<p>Since we can safely ignore the (<span>b</span>, <span>b</span>) case as out of domain, this means we only want to predict a <code>b</code> token <em>if the tokens we&#39;re attending to are identical</em>!
Since matrix multiplications involve sums, this means we can take advantage of additive cancellation, or in other words: <code>0.5 + 0.5 = 1</code>, and <code>0.5 + (-0.5) = 0</code>.</p>
<p>By encoding <code>a</code> as <code>1</code> and <code>b</code> as <code>-1</code>, this simple equation does exactly what we want.
When the token prediction should be <code>a</code>, this equation equals <code>0</code>, and when the token prediction should be <code>b</code>, this equation equals <code>1</code>:</p>
<ul>
<li><span>a</span>, <span>b</span>  → 0.5 * <span>1</span> + 0.5 * <span>(-1)</span>  = 0</li>
<li><span>b</span>, <span>a</span> → 0.5 * <span>(-1)</span>  + 0.5 * <span>1</span> = 0</li>
<li><span>a</span>, <span>a</span> → 0.5 * <span>1</span> + 0.5 * <span>1</span> = 1</li>
</ul>
<p>If we take our softmax result matrix from before and multiply it with the split-off v matrix from before, performing that exact calculation for each row, we get the following attention result for the input sequence <code>aabaa</code>:</p>
<div>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0.5</td><td>0.5</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0.5</td><td>0.5</td>
</tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
<p>=</p>
<table>
<tbody><tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(1 * 1)</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>(0.5*1 + 0.5*(-1))</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>(0.5*(-1) + 0.5*1)</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
</tbody></table>
</div>
<p>The first row has a spurious <code>b</code> prediction because it doesn&#39;t have enough data (with only a single <code>a</code> token to go on, the result could be either <code>a</code> or <code>b</code>).
But the other two <code>b</code> predictions are right on: the second row predicts the next token should be <code>b</code>, which is correct, and the final row predicts the token following the sequence should also be <code>b</code>, which is also correct.</p>
<p>So to summarize, what the <code>c_attn</code> weights are doing is:</p>
<ul>
<li>Mapping the position embeddings into an &#34;attention window&#34; in <code>q</code></li>
<li>Extracting the position embeddings into <code>k</code></li>
<li>Transforming the token embeddings into a <code>1/-1</code> token encoding in <code>v</code></li>
<li>When <code>q</code> and <code>k</code> are combined in <code>softmax(q @ k.T / ... + mask)</code>, we get a <code>seq_len x seq_len</code> matrix that
<ul>
<li>in the first row, only attends to the first token</li>
<li>in the other rows result, attends equally to the two most recent tokens</li>
</ul>
</li>
<li>Finally, with <code>softmax(...) @ v</code>, we take advantage of additive cancellation to get
<ul>
<li>a <code>0</code> in position 7 of the row when the model should predict <code>a</code>,</li>
<li>and a <code>1</code> in position 7 of the row when the model should predict <code>b</code></li>
</ul>
</li>
</ul>
<p><small>(Another, more biological way to think of this is in terms of repressors and promoters: seeing an a in the attended tokens promotes position 7, and seeing a b represses it.)</small> </p>
<p>That all works, which means our attention head is done!</p>
<h3 id="Projecting_back_to_embedding_space"><a href="#Projecting_back_to_embedding_space">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Projecting_back_to_embedding_space"/>
</a>Projecting back to embedding space</h3>
<p>Next, to finish the transformer block we need to project the attention result back to a regular embedding.
Our attention head put its prediction in <code>embedding[row, 7]</code> (<code>1</code> for <code>b</code>, <code>0</code> for <code>a</code>), but elsewhere we use a one-hot scheme, with a positive value in <code>embedding[row, 5]</code> indicating <code>a</code> and a positive value in <code>embedding[row, 6]</code> indicating <code>b</code>.</p>
<p>For reasons, that will become clear in a bit, we don&#39;t want this layer to produce a plain 1-hot like <code>[..., 1, 0, ...]</code> or <code>[..., 0, 1, ...]</code>.
Instead, we want to produce a <em>scaled</em> 1-hot of <code>[..., 1024, 0, ...]</code> or <code>[..., 0, 1024, ...]</code>. </p>
<p>To do this, all we have to do is use the bias of <code>c_proj</code> to set <code>embedding[row, 5]</code> (the one-hot for token <code>a</code>) to <code>1024</code> by default, and then yoink in and appropriately scale the attention result&#39;s <code>embedding[row, 7]</code>:</p>
<pre data-lang="python"><code data-lang="python"><span>Lg </span><span>= </span><span>1024  </span><span># Large
</span><span>
</span><span>MODEL </span><span>= </span><span>{
</span><span>  </span><span>&#34;wte&#34;</span><span>: </span><span>...</span><span>,
</span><span>  </span><span>&#34;wpe&#34;</span><span>: </span><span>...</span><span>,
</span><span>  </span><span>&#34;blocks&#34;</span><span>: [
</span><span>    {
</span><span>      </span><span>&#34;attn&#34;</span><span>: {
</span><span>        </span><span>&#34;c_attn&#34;</span><span>: </span><span>...</span><span>,
</span><span>        </span><span>&#34;c_proj&#34;</span><span>: {  </span><span># weights to project attn result back to embedding space
</span><span>          </span><span>&#34;b&#34;</span><span>: [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, Lg, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>          </span><span>&#34;w&#34;</span><span>: np.array([
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>-</span><span>Lg, Lg, </span><span>0</span><span>],
</span><span>          ]),
</span><span>        },
</span><span>      },
</span><span>    },
</span><span>  ],
</span><span>}
</span></code></pre>
<p>In other words, after <code>c_proj</code>,</p>
<ul>
<li><code>embedding[row, 5]</code> (corresponding to <code>a</code>) equals <code>Lg + (-Lg) * prediction</code></li>
<li><code>embedding[row, 6]</code> (corresponding to <code>b</code>) equals <code>0 + Lg * prediction</code></li>
</ul>
<p>And after running the attention result from before through <code>c_proj</code>, we get this matrix, which is exactly what we want—one hot predictions, scaled by 1024!</p>
<div>
<table>
<tbody><tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1024</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1024</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1024</td><td>0</td>
</tr>
</tbody></table>
</div>
<p>Now we&#39;re done with <code>c_proj</code>, and can project the transformer block result back to vocabulary space to make a prediction!</p>
<h2 id="Projecting_back_to_vocabulary_space_and_extracting_probabilities"><a href="#Projecting_back_to_vocabulary_space_and_extracting_probabilities">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Projecting_back_to_vocabulary_space_and_extracting_probabilities"/>
</a>Projecting back to vocabulary space and extracting probabilities</h2>
<p>We start out with the result of running the transformer block:</p>
<div>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1024</td><td>1</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1025</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1024</td><td>0</td>
</tr>
</tbody></table>
</div>
<p>This is the original embedding added to the <code>c_prog</code> result from above.
The original embedding is added because of what&#39;s called a residual connection: in <code>transformer_block</code>, we do <code>x = x + causal_self_attention(x, ...)</code> <small>(note the <code>x +</code>)</small> instead of simply doing <code>x = causal_self_attention(x, ...)</code>.</p>
<p>Residual connections can help deep networks maintain information flow through lots of layers, but in our case it just gets in the way.
<em>This</em> is why the output of <code>c_proj</code> was scaled by<code>1024</code>: to drown out the unneeded residual signal.</p>
<p>The next step is to multiply the above matrix by the transposed token embedding weights (<code>wte</code>) we defined at the start to get the final logits:</p>
<div>
<table>
<tbody><tr>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1024</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1024</td><td>1</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1025</td><td>0</td><td>0</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1024</td><td>0</td>
</tr>
</tbody></table>
<p>@</p>

<p>=</p>
<table>
<tbody><tr><td>1</td><td>1024</td></tr>
<tr><td>1</td><td>1024</td></tr>
<tr><td>1024</td><td>1</td></tr>
<tr><td>1025</td><td>0</td></tr>
<tr><td>1</td><td>1024</td></tr>
</tbody></table>
</div>
<p>The red squares in the logits show where the model has a slight bias to repeat a token, because of the residual connection, but the opposed <code>1024</code> drowns that out, so the final predictions after <code>softmax</code> are all 100% one way or the other:</p>
<div>
<div>
<p>softmax(</p>
<table>
<tbody><tr><td>1</td><td>1024</td></tr>
<tr><td>1</td><td>1024</td></tr>
<tr><td>1024</td><td>1</td></tr>
<tr><td>1025</td><td>0</td></tr>
<tr><td>1</td><td>1024</td></tr>
</tbody></table>
<p>)</p>
</div>
<p>=</p>

</div>
<p>Or in other words, when given the context sequence <code>aabaa</code>, the model predicts:</p>
<ul>
<li>The token following <code>a</code> is <code>b</code> (acceptable, could be either)</li>
<li>The token following <code>aa</code> is <code>b</code> (correct!)</li>
<li>The token following <code>aab</code> is <code>a</code> (correct!)</li>
<li>The token following <code>aaba</code> is <code>a</code> (correct!)</li>
<li>The token following <code>aabaa</code> is <code>b</code> (correct!)</li>
</ul>
<p>Of course, for inference all we care about is that final prediction row: <code>b</code> follows <code>aabaa</code>. 
The other predictions are only useful for training the model.</p>
<p>With the finished model weights (below), we can write a small <code>complete</code> function and show that our handmade model always generates reasonable completions:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>complete</span><span>(</span><span>s</span><span>, </span><span>max_new_tokens</span><span>=</span><span>10</span><span>):
</span><span>  tokens </span><span>= </span><span>tokenize(s)
</span><span>  </span><span>while </span><span>len</span><span>(tokens) </span><span>&lt; </span><span>len</span><span>(s) </span><span>+ </span><span>max_new_tokens:
</span><span>    logits </span><span>= </span><span>gpt(np.array(tokens[</span><span>-</span><span>5</span><span>:]), </span><span>**</span><span>MODEL)
</span><span>    probs </span><span>= </span><span>softmax(logits)
</span><span>    pred </span><span>= </span><span>np.argmax(probs[</span><span>-</span><span>1</span><span>]) </span><span># greedy sample, but temperature sampling would give the same results in our case
</span><span>    tokens.append(pred)
</span><span>  </span><span>return </span><span>s </span><span>+ </span><span>&#34; :: &#34; </span><span>+ </span><span>&#34;&#34;</span><span>.join(untok(t) </span><span>for </span><span>t </span><span>in </span><span>tokens[</span><span>len</span><span>(s):])
</span><span>
</span><span>print</span><span>(complete(</span><span>&#34;a&#34;</span><span>)) </span><span># a :: baabaabaab
</span><span>print</span><span>(complete(</span><span>&#34;ba&#34;</span><span>)) </span><span># ba :: abaabaabaa
</span><span>print</span><span>(complete(</span><span>&#34;abaab&#34;</span><span>)) </span><span># abaab :: aabaabaaba
</span></code></pre>
<p>It can even recover from out of domain inputs!</p>
<pre data-lang="python"><code data-lang="python"><span>print</span><span>(complete(</span><span>&#34;ababa&#34;</span><span>)) </span><span># ababa :: abaabaabaa
</span><span>print</span><span>(complete(</span><span>&#34;bbbbb&#34;</span><span>)) </span><span># bbbbb :: aabaabaaba
</span></code></pre>
<p>If we write a small accuracy testing loop, the handmade model is 100% accurate (as long as it&#39;s given an unambiguous context):</p>
<pre data-lang="python"><code data-lang="python"><span>test </span><span>= </span><span>&#34;aab&#34; </span><span>* </span><span>10
</span><span>total, correct </span><span>= </span><span>0</span><span>, </span><span>0
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>2</span><span>, </span><span>len</span><span>(test) </span><span>- </span><span>1</span><span>):
</span><span>  ctx </span><span>= </span><span>test[:i]
</span><span>  expected </span><span>= </span><span>test[i]
</span><span>  total </span><span>+= </span><span>1
</span><span>  </span><span>if </span><span>untok(predict(ctx)) </span><span>== </span><span>expected:
</span><span>    correct </span><span>+= </span><span>1
</span><span>print</span><span>(</span><span>f</span><span>&#34;ACCURACY: </span><span>{correct </span><span>/ </span><span>total </span><span>* </span><span>100</span><span>}</span><span>% (</span><span>{correct}</span><span> / </span><span>{total}</span><span>)&#34;</span><span>)
</span><span># ACCURACY: 100.0% (27 / 27)
</span></code></pre>
<h2 id="Conclusion"><a href="#Conclusion">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Conclusion"/>
</a>Conclusion</h2>
<p>Thanks for reading!
Hopefully you have a more intuitive understanding of transformers and attention now, and maybe even feel inspired to make your own model by hand as well!</p>
<p>If you enjoyed this post, you may also enjoy:</p>
<ul>
<li><a href="https://vgel.me/posts">My other blog posts</a>, such as <a href="https://vgel.me/posts/tools-not-needed/">GPT-3 will ignore tools when it disagrees with them</a>, <a href="https://vgel.me/posts/gpt4-javascript">Does GPT-4 think better in Javascript?</a> and <a href="https://vgel.me/posts/adversarial-training-data">I&#39;m worried about adversarial training data</a></li>
<li><a href="https://vgel.me/">My other projects</a></li>
<li>My <a href="https://twitter.com/voooooogel/">Twitter</a>, where I post about new blog posts, smaller AI-related thoughts (e.g. <a href="https://twitter.com/voooooogel/status/1688730813746290688">1</a>, <a href="https://twitter.com/voooooogel/status/1683593486862520321">2</a>), and other things.</li>
</ul>
<p>If you have thoughts about this post, please feel free to <a href="https://vgel.me/contact">get in touch</a>!
I love hearing from people who read my posts.</p>
<h3 id="Thanks"><a href="#Thanks">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Thanks"/>
</a>Thanks</h3>
<p>Thanks to the following people for reviewing drafts of this post:</p>
<ul>
<li><a href="https://twitter.com/MF_FOOM">MF FOOM</a>—a great account to follow if you&#39;re interested in LLMs!</li>
<li><a href="https://twitter.com/manic_pixie_agi">Corinne</a></li>
<li>Susan Vogel</li>
</ul>
<h2 id="Completed_code"><a href="#Completed_code">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Completed_code"/>
</a>Completed code</h2>
<pre data-lang="python"><code data-lang="python"><span># Model ops from https://github.com/jaymody/picoGPT/blob/main/gpt2.py (MIT license)
</span><span>
</span><span>import </span><span>numpy </span><span>as </span><span>np
</span><span>
</span><span>
</span><span>def </span><span>softmax</span><span>(</span><span>x</span><span>):
</span><span>    exp_x </span><span>= </span><span>np.exp(x </span><span>- </span><span>np.max(x, </span><span>axis</span><span>=-</span><span>1</span><span>, </span><span>keepdims</span><span>=</span><span>True</span><span>))
</span><span>    </span><span>return </span><span>exp_x </span><span>/ </span><span>np.sum(exp_x, </span><span>axis</span><span>=-</span><span>1</span><span>, </span><span>keepdims</span><span>=</span><span>True</span><span>)
</span><span>
</span><span># [m, in], [in, out], [out] -&gt; [m, out]
</span><span>def </span><span>linear</span><span>(</span><span>x</span><span>, </span><span>w</span><span>, </span><span>b</span><span>):
</span><span>    </span><span>return </span><span>x </span><span>@ </span><span>w </span><span>+ </span><span>b
</span><span>
</span><span># [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]
</span><span>def </span><span>attention</span><span>(</span><span>q</span><span>, </span><span>k</span><span>, </span><span>v</span><span>, </span><span>mask</span><span>):
</span><span>    </span><span>return </span><span>softmax(q </span><span>@ </span><span>k.T </span><span>/ </span><span>np.sqrt(q.shape[</span><span>-</span><span>1</span><span>]) </span><span>+ </span><span>mask) </span><span>@ </span><span>v
</span><span>
</span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>def </span><span>causal_self_attention</span><span>(</span><span>x</span><span>, </span><span>c_attn</span><span>, </span><span>c_proj</span><span>):
</span><span>    </span><span># qkv projections
</span><span>    x </span><span>= </span><span>linear(x, </span><span>**</span><span>c_attn)  </span><span># [n_seq, n_embd] -&gt; [n_seq, 3*n_embd]
</span><span>
</span><span>    </span><span># split into qkv
</span><span>    q, k, v </span><span>= </span><span>np.split(x, </span><span>3</span><span>, </span><span>axis</span><span>=-</span><span>1</span><span>)  </span><span># [n_seq, 3*n_embd] -&gt; 3 of [n_seq, n_embd]
</span><span>
</span><span>    </span><span># causal mask to hide future inputs from being attended to
</span><span>    causal_mask </span><span>= </span><span>(</span><span>1 </span><span>- </span><span>np.tri(x.shape[</span><span>0</span><span>], </span><span>dtype</span><span>=</span><span>x.dtype)) </span><span>* -</span><span>1e10  </span><span># [n_seq, n_seq]
</span><span>
</span><span>    </span><span># perform causal self attention
</span><span>    x </span><span>= </span><span>attention(q, k, v, causal_mask)  </span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>
</span><span>    </span><span># out projection
</span><span>    x </span><span>= </span><span>linear(x, </span><span>**</span><span>c_proj)  </span><span># [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]
</span><span>
</span><span>    </span><span>return </span><span>x
</span><span>
</span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>def </span><span>transformer_block</span><span>(</span><span>x</span><span>, </span><span>attn</span><span>):
</span><span>    x </span><span>= </span><span>x </span><span>+ </span><span>causal_self_attention(x, </span><span>**</span><span>attn)
</span><span>    </span><span># NOTE: removed ffn
</span><span>    </span><span>return </span><span>x
</span><span>
</span><span># [n_seq] -&gt; [n_seq, n_vocab]
</span><span>def </span><span>gpt</span><span>(</span><span>inputs</span><span>, </span><span>wte</span><span>, </span><span>wpe</span><span>, </span><span>blocks</span><span>):
</span><span>    </span><span># token + positional embeddings
</span><span>    x </span><span>= </span><span>wte[inputs] </span><span>+ </span><span>wpe[</span><span>range</span><span>(</span><span>len</span><span>(inputs))]  </span><span># [n_seq] -&gt; [n_seq, n_embd]
</span><span>
</span><span>    </span><span># forward pass through n_layer transformer blocks
</span><span>    </span><span>for </span><span>block </span><span>in </span><span>blocks:
</span><span>        x </span><span>= </span><span>transformer_block(x, </span><span>**</span><span>block)  </span><span># [n_seq, n_embd] -&gt; [n_seq, n_embd]
</span><span>
</span><span>    </span><span># projection to vocab
</span><span>    </span><span>return </span><span>x </span><span>@ </span><span>wte.T  </span><span># [n_seq, n_embd] -&gt; [n_seq, n_vocab]
</span><span>
</span><span>
</span><span>N_CTX </span><span>= </span><span>5
</span><span>N_VOCAB </span><span>= </span><span>2
</span><span>N_EMBED </span><span>= </span><span>8
</span><span>
</span><span>Lg </span><span>= </span><span>1024  </span><span># Large
</span><span>
</span><span>MODEL </span><span>= </span><span>{
</span><span>    </span><span># EMBEDDING USAGE
</span><span>    </span><span>#  P = Position embeddings (one-hot)
</span><span>    </span><span>#  T = Token embeddings (one-hot, first is `a`, second is `b`)
</span><span>    </span><span>#  V = Prediction scratch space
</span><span>    </span><span>#
</span><span>    </span><span>#       [P, P, P, P, P, T, T, V]
</span><span>    </span><span>&#34;wte&#34;</span><span>: np.array(
</span><span>        </span><span># one-hot token embeddings
</span><span>        [
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># token `a` (id 0)
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>],  </span><span># token `b` (id 1)
</span><span>        ]
</span><span>    ),
</span><span>    </span><span>&#34;wpe&#34;</span><span>: np.array(
</span><span>        </span><span># one-hot position embeddings
</span><span>        [
</span><span>            [</span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 0
</span><span>            [</span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 1
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 2
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 3
</span><span>            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],  </span><span># position 4
</span><span>        ]
</span><span>    ),
</span><span>    </span><span>&#34;blocks&#34;</span><span>: [
</span><span>        {
</span><span>            </span><span>&#34;attn&#34;</span><span>: {
</span><span>                </span><span>&#34;c_attn&#34;</span><span>: {  </span><span># generates qkv matrix
</span><span>                    </span><span>&#34;b&#34;</span><span>: np.zeros(N_EMBED </span><span>* </span><span>3</span><span>),
</span><span>                    </span><span>&#34;w&#34;</span><span>: np.array(
</span><span>                        </span><span># this is where the magic happens
</span><span>                        </span><span># fmt: off
</span><span>                        [
</span><span>                          [Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>                          [Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>                          [</span><span>0.</span><span>, Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>                          [</span><span>0.</span><span>, </span><span>0.</span><span>, Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>                          [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, Lg, Lg, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>                          [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>1.</span><span>], </span><span># v
</span><span>                          [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>-</span><span>1</span><span>], </span><span># v
</span><span>                          [</span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># q
</span><span>                            </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>,  </span><span># k
</span><span>                              </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>, </span><span>0.</span><span>], </span><span># v
</span><span>                        ]
</span><span>                        </span><span># fmt: on
</span><span>                    ),
</span><span>                },
</span><span>                </span><span>&#34;c_proj&#34;</span><span>: {  </span><span># weights to project attn result back to embedding space
</span><span>                    </span><span>&#34;b&#34;</span><span>: [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, Lg, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                    </span><span>&#34;w&#34;</span><span>: np.array(
</span><span>                        [
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>],
</span><span>                            [</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>-</span><span>Lg, Lg, </span><span>0</span><span>],
</span><span>                        ]
</span><span>                    ),
</span><span>                },
</span><span>            },
</span><span>        }
</span><span>    ],
</span><span>}
</span><span>
</span><span>CHARS </span><span>= </span><span>[</span><span>&#34;a&#34;</span><span>, </span><span>&#34;b&#34;</span><span>]
</span><span>def </span><span>tokenize</span><span>(</span><span>s</span><span>): </span><span>return </span><span>[CHARS.index(c) </span><span>for </span><span>c </span><span>in </span><span>s]
</span><span>def </span><span>untok</span><span>(</span><span>tok</span><span>): </span><span>return </span><span>CHARS[tok]
</span><span>
</span><span>def </span><span>predict</span><span>(</span><span>s</span><span>):
</span><span>    tokens </span><span>= </span><span>tokenize(s)[</span><span>-</span><span>5</span><span>:]
</span><span>    logits </span><span>= </span><span>gpt(np.array(tokens), </span><span>**</span><span>MODEL)
</span><span>    probs </span><span>= </span><span>softmax(logits)
</span><span>
</span><span>    </span><span>for </span><span>i, tok </span><span>in </span><span>enumerate</span><span>(tokens):
</span><span>        pred </span><span>= </span><span>np.argmax(probs[i])
</span><span>        </span><span>print</span><span>(
</span><span>            </span><span>f</span><span>&#34;</span><span>{untok(tok)}</span><span> (</span><span>{tok}</span><span>): next=</span><span>{untok(pred)}</span><span> (</span><span>{pred}</span><span>) probs=</span><span>{probs[i]}</span><span> logits=</span><span>{logits[i]}</span><span>&#34;
</span><span>        )
</span><span>
</span><span>    </span><span>return </span><span>np.argmax(probs[</span><span>-</span><span>1</span><span>])
</span><span>
</span><span>def </span><span>complete</span><span>(</span><span>s</span><span>, </span><span>max_new_tokens</span><span>=</span><span>10</span><span>):
</span><span>    tokens </span><span>= </span><span>tokenize(s)
</span><span>    </span><span>while </span><span>len</span><span>(tokens) </span><span>&lt; </span><span>len</span><span>(s) </span><span>+ </span><span>max_new_tokens:
</span><span>        logits </span><span>= </span><span>gpt(np.array(tokens[</span><span>-</span><span>5</span><span>:]), </span><span>**</span><span>MODEL)
</span><span>        probs </span><span>= </span><span>softmax(logits)
</span><span>        pred </span><span>= </span><span>np.argmax(probs[</span><span>-</span><span>1</span><span>])
</span><span>        tokens.append(pred)
</span><span>    </span><span>return </span><span>s </span><span>+ </span><span>&#34; :: &#34; </span><span>+ </span><span>&#34;&#34;</span><span>.join(untok(t) </span><span>for </span><span>t </span><span>in </span><span>tokens[</span><span>len</span><span>(s):])
</span><span>
</span><span>test </span><span>= </span><span>&#34;aab&#34; </span><span>* </span><span>10
</span><span>total, correct </span><span>= </span><span>0</span><span>, </span><span>0
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>2</span><span>, </span><span>len</span><span>(test) </span><span>- </span><span>1</span><span>):
</span><span>    ctx </span><span>= </span><span>test[:i]
</span><span>    expected </span><span>= </span><span>test[i]
</span><span>    total </span><span>+= </span><span>1
</span><span>    </span><span>if </span><span>untok(predict(ctx)) </span><span>== </span><span>expected:
</span><span>        correct </span><span>+= </span><span>1
</span><span>print</span><span>(</span><span>f</span><span>&#34;ACCURACY: </span><span>{correct </span><span>/ </span><span>total </span><span>* </span><span>100</span><span>}</span><span>% (</span><span>{correct}</span><span> / </span><span>{total}</span><span>)&#34;</span><span>)
</span></code></pre>
<h2 id="Bonus:_Efficiency"><a href="#Bonus:_Efficiency">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Bonus:_Efficiency"/>
</a>Bonus: Efficiency</h2>
<p>For a full 5-token context, our model needs ~4,000 floating point operations to predict a single token, the majority used in the attention calculation.
This can be reduced by shrinking the context window, using fused multiply-adds, kv caching, and other techniques, but predicting a single token still needs ~hundreds of machine instructions.</p>
<p>In comparison, handwritten (x64) assembly needs eight:</p>
<pre data-lang="asm"><code data-lang="asm"><span>; dl: next token
</span><span>; rax: context addr
</span><span>; rcx: context len
</span><span>.next_token
</span><span>  </span><span>mov </span><span>dl</span><span>, </span><span>&#39;a&#39;
</span><span>  </span><span>cmp </span><span>byte ptr </span><span>[</span><span>rax </span><span>+ </span><span>rcx </span><span>- </span><span>1</span><span>], </span><span>&#39;a&#39;
</span><span>  </span><span>jne </span><span>.done
</span><span>  </span><span>cmp </span><span>rcx</span><span>, </span><span>1
</span><span>  </span><span>je </span><span>.return_b
</span><span>  </span><span>cmp </span><span>byte ptr </span><span>[</span><span>rax </span><span>+ </span><span>rcx </span><span>- </span><span>2</span><span>], </span><span>&#39;a&#39;
</span><span>  </span><span>jne </span><span>.done
</span><span>.return_b:
</span><span>  </span><span>mov </span><span>dl</span><span>, </span><span>&#39;b&#39;
</span><span>.done:
</span></code></pre>
<p>Could we somehow train language models that are 1000x more efficient—that&#39;s to say, as relatively efficient to current models at generating natural language as this assembly is at generating <code>(aab)*</code>?
If you figure out how, email me.
I&#39;ll send the first person $10 ;-)</p>
<hr/>


<!---->

<!---->


    <ul>
      
        <li><strong>Previous entry:</strong> <a href="https://vgel.me/posts/c500/">Writing a C compiler in 500 lines of Python</a></li>
      
      
    </ul>
</article></div>
  </body>
</html>
