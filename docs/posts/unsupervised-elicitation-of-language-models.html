<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2506.10139">Original</a>
    <h1>Unsupervised Elicitation of Language Models</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+J" rel="nofollow">Jiaxin Wen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ankner,+Z" rel="nofollow">Zachary Ankner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Somani,+A" rel="nofollow">Arushi Somani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hase,+P" rel="nofollow">Peter Hase</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Marks,+S" rel="nofollow">Samuel Marks</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goldman-Wetzler,+J" rel="nofollow">Jacob Goldman-Wetzler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Petrini,+L" rel="nofollow">Linda Petrini</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sleight,+H" rel="nofollow">Henry Sleight</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Burns,+C" rel="nofollow">Collin Burns</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+H" rel="nofollow">He He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+S" rel="nofollow">Shi Feng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Perez,+E" rel="nofollow">Ethan Perez</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leike,+J" rel="nofollow">Jan Leike</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2506.10139">View PDF</a>
    <a href="https://arxiv.org/html/2506.10139v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>To steer pretrained language models for downstream tasks, today&#39;s post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs&#39; capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Jiaxin Wen [<a href="https://arxiv.org/show-email/5affddae/2506.10139" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
