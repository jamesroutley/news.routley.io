<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://qwenlm.github.io/blog/qwen2.5-1m/">Original</a>
    <h1>Qwen2.5-1M: Deploy your own Qwen with context length up to 1M tokens</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><p><a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf" target="_blank">Tech Report</a>
<a href="https://huggingface.co/Qwen" target="_blank">HuggingFace</a>
<a href="https://modelscope.cn/organization/qwen" target="_blank">ModelScope</a>
<a href="https://chat.qwenlm.ai/" target="_blank">Qwen Chat</a>
<a href="https://huggingface.co/spaces/Qwen/Qwen2.5-1M-Demo" target="_blank">HuggingFace Demo</a>
<a href="https://www.modelscope.cn/studios/Qwen/Qwen2.5-1M-Demo" target="_blank">ModelScope Demo</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><p>Two months after upgrading <a href="https://qwenlm.github.io/blog/qwen2.5-turbo">Qwen2.5-Turbo</a> to support context length up to one million tokens, we are back with the open-source Qwen2.5-1M models and the corresponding inference framework support. Here’s what you can expect from this release:</p><ol><li><p><strong>Opensource Models:</strong> We’re releasing two new checkpoints, <strong>Qwen2.5-7B-Instruct-1M</strong> and <strong>Qwen2.5-14B-Instruct-1M</strong>, marking the first time we’ve upgraded our opensource Qwen models to handle 1M-token contexts.</p></li><li><p><strong>Inference Framework:</strong> To help developers deploy the Qwen2.5-1M series models more efficiently, we’ve fully open-sourced our inference framework based on <a href="https://github.com/vllm-project/vllm">vLLM</a>. With integration with sparse attention methods, our framework can process 1M-token inputs <strong>3x to 7x</strong> faster.</p></li><li><p><strong><a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf">Technical Report</a>:</strong> We’re also sharing the technical details behind the Qwen2.5-1M series, including design insights for training and inference frameworks, as well as ablation experiments.</p></li></ol><p>You can experience Qwen2.5-1M models online by visiting our demo on <a href="https://huggingface.co/spaces/Qwen/Qwen2.5-1M-Demo">Huggingface</a> and <a href="https://www.modelscope.cn/studios/Qwen/Qwen2.5-1M-Demo">Modelscope</a>.</p><p>Additionally, we recently introduced <strong><a href="https://chat.qwenlm.ai/">Qwen Chat</a></strong>, an advanced AI assistant from the Qwen series. With Qwen Chat, you can engage in conversations, write code, perform searches, generate images and videos, and utilize various tools. Notably, Qwen Chat also features the Qwen2.5-Turbo model, which supports long-context processing with a context length of up to 1M tokens.</p><p>Let’s start by diving into the performance of the Qwen2.5-1M series models, covering both long-context and short text tasks.</p><h2 id="long-context-tasks">Long-Context Tasks</h2><p>First off, we evaluate the Qwen2.5-1M models on the Passkey Retrieval task with a context length of 1 million tokens. The results show that these models can accurately retrieve hidden information from documents containing up to 1M tokens, with only minor errors observed in the 7B model.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/passkey_retrieval.png#center" width="100%"/></figure><p>For more complex long-context understanding tasks, we select <a href="https://github.com/hsiehjackson/RULER">RULER</a>, <a href="https://github.com/infinigence/LVEval">LV-Eval</a>, <a href="https://github.com/THUDM/LongAlign">LongbenchChat</a> used in <a href="https://qwenlm.github.io/blog/qwen2.5-turbo/#more-complex-long-text-tasks">this blog</a>.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/ruler.png#center" width="80%"/></figure><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/lv-eval.png#center" width="80%"/></figure><p>From these results, we can draw a few key conclusions:</p><ul><li><strong>Significantly Superior to the 128k Version:</strong> The Qwen2.5-1M series models significantly outperform their 128K counterparts in most long-context tasks, especially for sequences exceeding 64K in length.</li><li><strong>Notable Performance Advantage:</strong> The Qwen2.5-14B-Instruct-1M model not only beats Qwen2.5-Turbo but also consistently outperforms GPT-4o-mini across multiple datasets, offering a robust open-source alternative for long-context tasks.</li></ul><h2 id="short-context-tasks">Short-Context Tasks</h2><p>Besides performance on long sequences, we’re equally interested in how these models handle short sequences. So, we compare the Qwen2.5-1M models and their 128K versions on widely used academic benchmarks, throwing in GPT-4o-mini for comparison.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/short_result.png#center" width="80%"/></figure><p>Here’s what we find:</p><ul><li>Both Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M maintain performance on short text tasks that is similar to their 128K versions, ensuring the fundamental capabilities haven’t been compromised by the addition of long-sequence processing abilities.</li><li>Compared to GPT-4o-mini, both Qwen2.5-14B-Instruct-1M and Qwen2.5-Turbo achieve similar performance on short text tasks while supporting a context length that’s eight times longer.</li></ul><p>Here, we’ll briefly introduce the key techniques behind building Qwen2.5-1M. For more details, please check out our <a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf">technical report</a>.</p><h2 id="long-context-training">Long-Context Training</h2><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/training_stages.png#center" width="70%"/></figure><p>Training with long sequences demands substantial computational resources, so we adopt a progressive approach to expand the context length for Qwen2.5-1M through multiple stages:</p><ul><li>We begin with an intermediate checkpoint of pre-trained Qwen2.5, which had a 4K token context length.</li><li><strong>In Pretraining</strong>, we gradually increase the context length from 4K to 256K tokens while using <a href="https://arxiv.org/abs/2309.16039">Adjusted Base Frequency</a>, raising the RoPE base from 10,000 to 10,000,000.</li><li><strong>In Supervised Fine-tuning</strong>, we split this into two stages to preserve performance on shorter sequences:<ul><li><strong>Stage 1:</strong> Fine-tuned only on short instructions (up to 32K tokens) using the same data and steps as the 128K versions of Qwen2.5.</li><li><strong>Stage 2:</strong> Mixed short (up to 32K) and long (up to 256K) instructions to enhance long-context task performance while maintaining short-task quality.</li></ul></li><li><strong>In Reinforcement Learning</strong>, we train models on short texts up to 8K tokens, which sufficiently improves alignment with human preferences and generalizes well to long-context tasks.</li></ul><p>The final instruction-tuned models are capable of handling sequences up to 256K tokens.</p><p>During training, we develop an instruction-tuned model with a context length of 256K tokens. To extend this to 1M tokens, we employ length extrapolation techniques.</p><p>The degradation of LLMs based on RoPE in long-context tasks is mainly due to unseen, large relative positional distances between queries and keys in computing attention weight. We employ <a href="https://arxiv.org/abs/2402.17463"><strong>Dual Chunk Attention</strong></a> (DCA), which addresses this issue by remapping relative positions to smaller values, avoiding the large distances not seen during training.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca.png#center" width="70%"/></figure><p>We evaluat the Qwen2.5-1M models and their 128K counterparts with and without the length extrapolation method. We can find:</p><p>Even models trained on just 32K tokens, such as the Qwen2.5-7B-Instruct, achieve nearly perfect accuracy in passkey retrieval tasks with 1M-token contexts. This underscores the remarkable ability of DCA to extend supported context lengths, without any training required.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca_ablation.png#center" width="40%"/></figure><h2 id="sparse-attention">Sparse Attention</h2><p>For long-context language models, inference speed is crucial for user experience. We introduce a sparse attention mechanism based on <a href="https://arxiv.org/abs/2407.02490"><strong>MInference</strong></a> to accelerate the prefill phase. Furthermore, we propose several improvements:</p><ul><li><p><strong>Integrating with Chunked Prefill:</strong> Directly processing sequences of 1M tokens results in substantial memory overhead to store the activations in MLP layers, consuming 71GB of VRAM in Qwen2.5-7B. By integrating with chunk prefill with a chunk length of 32,768 tokens, activation VRAM usage is reduced by 96.7%, leading to a significant decrease in memory consumption.</p></li><li><p><strong>Integrating with Length Extrapolation:</strong> We integrate DCA with MInference in long-context processing, thereby enhancing inference efficiency and achieving greater accuracy.</p></li><li><p><strong>Sparsity Refinement on Long Sequences:</strong> MInference requires an offline search to determine the optimal sparsification configuration for each attention head. Due to the computational demand of full attention weights, this search is typically conducted on short sequences, which may not generalize well to longer sequences. We developed a method to refine the sparsification configuration specifically for sequences up to 1M tokens, which significantly reduces the accuracy loss brought by sparse attention.</p></li><li><p><strong>More Optimizations:</strong> We introduce additional optimizations, such as enhanced kernel efficiency and dynamic chunked pipeline parallelism, to fully unlock the potential of the entire framework.</p></li></ul><p>With these enhancements, our inference framework results in a 3.2x to 6.7x acceleration in the prefill speed across different model sizes and GPU devices for sequences of 1M token length.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/speed.png#center" width="85%"/></figure><p>Here we provide step-by-step instructions for deploying the Qwen2.5-1M models on your local devices.</p><h3 id="1-system-preparation">1. System Preparation</h3><p>To achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.</p><p>Ensure your system meets the following requirements:</p><ul><li><strong>CUDA Version</strong>: 12.1 or 12.3</li><li><strong>Python Version</strong>: &gt;=3.9 and &lt;=3.12</li></ul><p>VRAM Requirement for processing 1 million-token sequences:</p><ul><li><strong>Qwen2.5-7B-Instruct-1M</strong>: At least 120GB VRAM (total across GPUs).</li><li><strong>Qwen2.5-14B-Instruct-1M</strong>: At least 320GB VRAM (total across GPUs).</li></ul><p>If your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M models for shorter tasks.</p><h3 id="2-install-dependencies">2. Install Dependencies</h3><p>For now, you need to clone the vLLM repository from our custom branch and install it manually. We are working on getting our branch merged into the main vLLM project.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>git clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git
</span></span><span><span><span>cd</span> vllm
</span></span><span><span>pip install -e . -v
</span></span></code></pre></div><h3 id="3-launch-openai-compatible-api-service">3. Launch OpenAI-Compatible API Service</h3><p>Use the following command to start the service, configuring it based on your hardware setup:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>vllm serve Qwen/Qwen2.5-7B-Instruct-1M <span>\
</span></span></span><span><span><span></span>  --tensor-parallel-size <span>4</span> <span>\
</span></span></span><span><span><span></span>  --max-model-len <span>1010000</span> <span>\
</span></span></span><span><span><span></span>  --enable-chunked-prefill --max-num-batched-tokens <span>131072</span> <span>\
</span></span></span><span><span><span></span>  --enforce-eager <span>\
</span></span></span><span><span><span></span>  --max-num-seqs <span>1</span>
</span></span><span><span>
</span></span><span><span><span># --quantization fp8 # Enabling FP8 quantization for model weights can reduce memory usage.</span>
</span></span></code></pre></div><p>If you encounter any issues, please refer to the <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M#troubleshooting">Troubleshooting</a> section for more information.</p><p><strong>Parameter Explanations:</strong></p><ul><li><p><strong><code>--tensor-parallel-size</code></strong></p><ul><li>Set to the number of GPUs you are using. Max 4 GPUs for the 7B model, and 8 GPUs for the 14B model.</li></ul></li><li><p><strong><code>--max-model-len</code></strong></p><ul><li>Defines the maximum input sequence length. Reduce this value if you encounter Out of Memory issues.</li></ul></li><li><p><strong><code>--max-num-batched-tokens</code></strong></p><ul><li>Sets the chunk size in Chunked Prefill. A smaller value reduces activation memory usage but may slow down inference.</li><li>Recommend 131072 for optimal performance.</li></ul></li><li><p><strong><code>--max-num-seqs</code></strong></p><ul><li>Limits concurrent sequences processed.</li></ul></li></ul><h3 id="4-interact-with-the-model">4. Interact with the Model</h3><p>You can interact with the deployed model using one of the following methods:</p><p><strong>Option 1. Using Curl</strong></p><div><pre tabindex="0"><code data-lang="bash"><span><span>curl http://localhost:8000/v1/chat/completions <span>\
</span></span></span><span><span><span></span>  -H <span>&#34;Content-Type: application/json&#34;</span> <span>\
</span></span></span><span><span><span></span>  -d <span>&#39;{
</span></span></span><span><span><span>    &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct-1M&#34;,
</span></span></span><span><span><span>    &#34;messages&#34;: [
</span></span></span><span><span><span>      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}
</span></span></span><span><span><span>    ],
</span></span></span><span><span><span>    &#34;temperature&#34;: 0.7,
</span></span></span><span><span><span>    &#34;top_p&#34;: 0.8,
</span></span></span><span><span><span>    &#34;repetition_penalty&#34;: 1.05,
</span></span></span><span><span><span>    &#34;max_tokens&#34;: 512
</span></span></span><span><span><span>  }&#39;</span>
</span></span></code></pre></div><p><strong>Option 2. Using Python</strong></p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>
</span></span><span><span>
</span></span><span><span><span>openai_api_key</span> <span>=</span> <span>&#34;EMPTY&#34;</span>
</span></span><span><span><span>openai_api_base</span> <span>=</span> <span>&#34;http://localhost:8000/v1&#34;</span>
</span></span><span><span>
</span></span><span><span><span>client</span> <span>=</span> <span>OpenAI</span><span>(</span>
</span></span><span><span>    <span>api_key</span><span>=</span><span>openai_api_key</span><span>,</span>
</span></span><span><span>    <span>base_url</span><span>=</span><span>openai_api_base</span><span>,</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>prompt</span> <span>=</span> <span>(</span>
</span></span><span><span>    <span>&#34;There is an important info hidden inside a lot of irrelevant text. &#34;</span> <span>+</span>
</span></span><span><span>    <span>&#34;Find it and memorize it. I will quiz you about the important information there.</span><span>\n\n</span><span>&#34;</span> <span>+</span>
</span></span><span><span>    <span>&#34;The pass key is 28884. Remember it. 28884 is the pass key.</span><span>\n</span><span>&#34;</span> <span>+</span>
</span></span><span><span>    <span>&#34;The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. &#34;</span> <span>*</span> <span>800</span> <span>+</span>
</span></span><span><span>    <span>&#34;</span><span>\n</span><span>What is the pass key?&#34;</span>
</span></span><span><span>    <span># The prompt is approximately 20k tokens long. You can try longer prompts by increasing the multiplier.</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>chat_response</span> <span>=</span> <span>client</span><span>.</span><span>chat</span><span>.</span><span>completions</span><span>.</span><span>create</span><span>(</span>
</span></span><span><span>    <span>model</span><span>=</span><span>&#34;Qwen/Qwen2.5-7B-Instruct-1M&#34;</span><span>,</span>
</span></span><span><span>    <span>messages</span><span>=</span><span>[{</span><span>&#34;role&#34;</span><span>:</span> <span>&#34;user&#34;</span><span>,</span> <span>&#34;content&#34;</span><span>:</span> <span>prompt</span><span>}],</span>
</span></span><span><span>    <span>temperature</span><span>=</span><span>0</span><span>,</span>
</span></span><span><span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>&#34;Chat response:&#34;</span><span>,</span> <span>chat_response</span><span>.</span><span>choices</span><span>[</span><span>0</span><span>]</span><span>.</span><span>message</span><span>.</span><span>content</span><span>)</span>
</span></span></code></pre></div><p><strong>Other Options</strong></p><p>For more advanced use cases, consider exploring frameworks like <a href="https://github.com/QwenLM/Qwen-Agent/tree/main">Qwen-Agent</a>, which enable the model to read PDF files and perform other specialized tasks.</p><p>We recognize that long-context models still have a lot of room for improvement. Our goal is to build models that excel in both short and long-context tasks, making sure they bring real value to practical, long-context scenarios. We’re diving deep into more efficient training methods, model architectures, and inference methods to make them deployable effectively and perform exceptionally well even in environments with limited resources.
We’re confident that all these efforts will open up a whole new world of possibilities for long-context models, expanding their use across a much broader range of applications. Stay tuned as we keep pushing the boundaries!</p></div></article></div></div>
  </body>
</html>
