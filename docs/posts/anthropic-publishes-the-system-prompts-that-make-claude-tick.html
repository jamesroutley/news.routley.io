<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://techcrunch.com/2024/08/26/anthropic-publishes-the-system-prompt-that-makes-claude-tick/">Original</a>
    <h1>Anthropic publishes the &#39;system prompts&#39; that make Claude tick</h1>
    
    <div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Generative AI models aren’t <a href="https://techcrunch.com/2023/04/03/the-great-pretender/">actually humanlike</a>. They have no intelligence or personality — they’re simply statistical systems predicting the likeliest next words in a sentence. But like interns at a tyrannical workplace, they <em>do</em> follow instructions without complaint — including initial “system prompts” that prime the models with their basic qualities and what they should and shouldn’t do.</p>

<p>Every generative AI vendor, from OpenAI to Anthropic, uses system prompts to prevent (or at least try to prevent) models from behaving badly, and to steer the general tone and sentiment of the models’ replies. For instance, a prompt might tell a model it should be polite but never apologetic, or to be honest about the fact that it <a href="https://techcrunch.com/2024/08/14/study-suggests-that-even-the-best-ai-models-hallucinate-a-bunch/">can’t know everything</a>.</p>

	
	


<p>But vendors usually keep system prompts close to the chest — presumably for competitive reasons, but also perhaps because knowing the system prompt may suggest ways to circumvent it. The only way to expose <a href="https://techcrunch.com/2024/05/13/openais-newest-model-is-gpt-4o/">GPT-4o</a>‘s system prompt, for example, is through a <a href="https://techcrunch.com/2023/02/24/can-language-models-really-be-protected-from-text-based-attacks/">prompt injection attack</a>. And even then, the system’s output <a href="https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/">can’t be trusted completely</a>.</p>

<p>However, Anthropic, in its continued effort to <a rel="nofollow" href="https://www.vox.com/future-perfect/364384/its-practically-impossible-to-run-a-big-ai-company-ethically">paint itself as a more ethical, transparent AI vendor</a>, has <a rel="nofollow" href="https://docs.anthropic.com/en/release-notes/system-prompts#july-12th-2024">published</a> the system prompts for its latest models (<a href="https://techcrunch.com/2024/06/20/anthropic-claims-its-latest-model-is-best-in-class/">Claude 3 Opus, Claude 3.5 Sonnet and Claude 3 Haiku</a>) in the <a href="https://techcrunch.com/2024/08/21/anthropics-claude-surpasses-1m-in-mobile-app-revenue/">Claude iOS and Android apps</a> and on the web.</p>

<p>Alex Albert, head of Anthropic’s developer relations, said in a post on X that Anthropic plans to make this sort of disclosure a regular thing as it updates and fine-tunes its system prompts.</p>

<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">We&#39;ve added a new system prompts release notes section to our docs. We&#39;re going to log changes we make to the default system prompts on Claude dot ai and our mobile apps. (The system prompt does not affect the API.) <a rel="nofollow" href="https://t.co/9mBwv2SgB1">pic.twitter.com/9mBwv2SgB1</a></p>— Alex Albert (@alexalbert__) <a rel="nofollow" href="https://twitter.com/alexalbert__/status/1828107230656471442?ref_src=twsrc%5Etfw">August 26, 2024</a></blockquote>
</div></figure>

<p>The latest prompts, dated July 12, outline very clearly what the Claude models can’t do — e.g. “Claude cannot open URLs, links, or videos.” Facial recognition is a big no-no; the system prompt for Claude Opus tells the model to “always respond as if it is completely face blind” and to “avoid identifying or naming any humans in [images].”</p>

<p>But the prompts also describe certain personality traits and characteristics — traits and characteristics that Anthropic would have the Claude models exemplify.</p>

	
	



<p>The prompt for Claude 3 Opus, for instance, says that Claude is to appear as if it “[is] very smart and intellectually curious,” and “enjoys hearing what humans think on an issue and engaging in discussion on a wide variety of topics.” It also instructs Claude to treat controversial topics with impartiality and objectivity, providing “careful thoughts” and “clear information” — and never to begin responses with the words “certainly” or “absolutely.”</p>

	
	


<p>It’s all a bit strange to this human, these system prompts, which are written like an actor in a stage play might write a <a rel="nofollow" href="https://static1.squarespace.com/static/5e8a140ed7ce562e250e036b/t/5e99b758ebebe248d87ac9a6/1587132252980/MT+Character+Analysis+Worksheets.pdf">character analysis sheet</a>. The prompt for Opus ends with “Claude is now being connected with a human,” which gives the impression that Claude is some sort of consciousness on the other end of the screen whose only purpose is to fulfill the whims of its human conversation partners.</p>

<p>But of course that’s an illusion. If the prompts for Claude tell us anything, it’s that without human guidance and hand-holding, these models are frighteningly blank slates.</p>

<p>With these new system prompt changelogs — the first of their kind from a major AI vendor — Anthropic is exerting pressure on competitors to publish the same. We’ll have to see if the gambit works.</p>
</div></div>
  </body>
</html>
