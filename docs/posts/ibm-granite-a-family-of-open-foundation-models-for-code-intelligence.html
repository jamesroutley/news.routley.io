<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ibm-granite/granite-code-models">Original</a>
    <h1>IBM Granite: A Family of Open Foundation Models for Code Intelligence</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/ibm-granite/granite-code-models/blob/main/figures/granite-code-models-3x-v4.png"><img src="https://github.com/ibm-granite/granite-code-models/raw/main/figures/granite-code-models-3x-v4.png"/></a>
</p>
<p dir="auto">
  ðŸ“š <a href="https://arxiv.org/abs/2405.04324" rel="nofollow">Paper</a>Â  | ðŸ¤— <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">HuggingFace Collection</a>Â  | 
  ðŸ’¬ <a href="https://github.com/orgs/ibm-granite/discussions">Discussions Page</a>Â  | ðŸ“° <a href="http:" rel="nofollow">Blog (coming soon)</a>Â 
<br/>
</p><hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Introduction to Granite Code Models</h2><a id="user-content-introduction-to-granite-code-models" aria-label="Permalink: Introduction to Granite Code Models" href="#introduction-to-granite-code-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We introduce the Granite series of decoder-only code models for code generative tasks (e.g., fixing bugs, explaining code, documenting code), trained with code written in 116 programming languages. A comprehensive evaluation of the Granite Code model family on diverse tasks demonstrates that our models consistently reach state-of-the-art performance among available open-source code LLMs.Â </p>
<p dir="auto">The key advantages of Granite Code models include:</p>
<ul dir="auto">
<li>All-rounder Code LLM: Granite Code models achieve competitive or state-of-the-art performance on different kinds of code-related tasks, including code generation, explanation, fixing, editing, translation, and more. Demonstrating their ability to solve diverse coding tasks.</li>
<li>Trustworthy Enterprise-Grade LLM: All our models are trained on license-permissible data collected following <a href="https://www.ibm.com/impact/ai-ethics" rel="nofollow">IBM&#39;s AI Ethics principles</a> and guided by IBMâ€™s Corporate Legal team for trustworthy enterprise usage. We release all our Granite Code models under an <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache 2.0 license</a> license for research and commercial use.</li>
</ul>
<p dir="auto">The family of <strong>Granite Code Models</strong> comes in two main variants:</p>
<ul dir="auto">
<li>Granite Code Base Models: base foundational models designed for code-related tasks (e.g., code repair, code explanation, code synthesis).</li>
<li>Granite Code Instruct Models: instruction following models finetuned using a combination of Git commits paired with human instructions and open-source synthetically generated code instruction datasets.</li>
</ul>
<p dir="auto">Both base and instruct models are available in sizes of 3B, 8B, 20B, and 34B parameters.</p>

<p dir="auto">Our process to prepare code pretraining data involves several stages. First, we collect a combination of publicly available datasets (e.g., GitHub Code Clean, Starcoder data), public code repositories, and issues from GitHub. Second, we filter the code data collected based on the programming language in which data is written (which we determined based on file extension). Then, we also filter out data with low code quality. Third, we adopt an aggressive deduplication strategy that includes both exact and fuzzy deduplication to remove documents having (near) identical code content. Finally, we apply a HAP content filter that reduces models&#39; likelihood of generating hateful, abusive, or profane language. We also make sure to redact Personally Identifiable Information (PII) by replacing PII content (e.g., names, email addresses, keys, passwords) with corresponding tokens (e.g., âŸ¨NAMEâŸ©, âŸ¨EMAILâŸ©, âŸ¨KEYâŸ©, âŸ¨PASSWORDâŸ©). We also scan all datasets using ClamAV to identify and remove instances of malware in the source code. In addition to collecting code data for model training, we curate several publicly available high-quality natural language datasets for improving the modelâ€™s proficiency in language understanding and mathematical reasoning.</p>

<p dir="auto">The <strong>Granite Code Base</strong> models are trained on 3-4T tokens of code data and natural language datasets related to code. Data is tokenized via byte pair encoding (BPE), employing the same tokenizer as StarCoder. We utilize high-quality data with two phases of training as follows:</p>
<ul dir="auto">
<li>Phase 1 (code only training): During phase 1, 3B and 8B models are trained for 4 trillion tokens of code data comprising 116 languages. The 20B parameter model is trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after the depth upscaling which is done on the 1.6T checkpoint of 20B model.</li>
<li>Phase 2 (code + language training): In phase 2, we include additional high-quality publicly available data from various domains, including technical, mathematics, and web documents, to further improve the modelâ€™s performance. We train all our models for 500B tokens (80% code-20% language mixture) in phase 2 training.</li>
</ul>

<p dir="auto">Granite Code Instruct models are finetuned on the following types of instruction data: 1) code commits sourced from <a href="https://huggingface.co/datasets/bigcode/commitpackft" rel="nofollow">CommitPackFT</a>, 2) high-quality math datasets, specifically we used <a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct" rel="nofollow">MathInstruct</a> and <a href="https://huggingface.co/datasets/meta-math/MetaMathQA" rel="nofollow">MetaMathQA</a>, 3) Code instruction datasets such as <a href="https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3" rel="nofollow">Glaive-Code-Assistant-v3</a>, <a href="https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k" rel="nofollow">Self-OSS-Instruct-SC2</a>, <a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2" rel="nofollow">Glaive-Function-Calling-v2</a>, <a href="https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction" rel="nofollow">NL2SQL11</a> and a small collection of synthetic API calling datasets, and 4) high-quality language instruction datasets such as <a href="https://huggingface.co/datasets/nvidia/HelpSteer" rel="nofollow">HelpSteer</a> and an open license-filtered version of <a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus" rel="nofollow">Platypus</a>.</p>

<p dir="auto">We conduct an extensive evaluation of our code models on a comprehensive list of benchmarks that includes but is not limited to HumanEvalPack, MBPP, and MBPP+. This set of benchmarks encompasses different coding tasks across commonly used programming languages (e.g., Python, JavaScript, Java, Go, C++, Rust).</p>
<p dir="auto">Our findings reveal that Granite Code models outperform strong open-source models across model sizes. The figure below illustrates how <code>Granite-8B-Code-Base</code> outperforms <code>Mistral-7B</code>, <code>LLama-3-8B</code>, and other open-source models in three coding tasks. We provide further evaluation results in our paper.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibm-granite/granite-code-models/blob/main/figures/GraniteCodeFigure1.jpg"><img src="https://github.com/ibm-granite/granite-code-models/raw/main/figures/GraniteCodeFigure1.jpg"/></a></p>

<p dir="auto">To use any of our models, pick an appropriate <code>model_path</code> from:</p>
<ol dir="auto">
<li><code>ibm-granite/granite-3b-code-base</code></li>
<li><code>ibm-granite/granite-3b-code-instruct</code></li>
<li><code>ibm-granite/granite-8b-code-base</code></li>
<li><code>ibm-granite/granite-8b-code-instruct</code></li>
<li><code>ibm-granite/granite-20b-code-base</code></li>
<li><code>ibm-granite/granite-20b-code-instruct</code></li>
<li><code>ibm-granite/granite-34b-code-base</code></li>
<li><code>ibm-granite/granite-34b-code-instruct</code></li>
</ol>

<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

device = &#34;cuda&#34; # or &#34;cpu&#34;
model_path = &#34;ibm-granite/granite-3b-code-base&#34; # pick anyone from above list

tokenizer = AutoTokenizer.from_pretrained(model_path)

# drop device_map if running on CPU
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)
model.eval()

# change input text as desired
input_text = &#34;def generate():&#34;
# tokenize the text
input_tokens = tokenizer(input_text, return_tensors=&#34;pt&#34;)

# transfer tokenized inputs to the device
for i in input_tokens:
    input_tokens[i] = input_tokens[i].to(device)

# generate output tokens
output = model.generate(**input_tokens)
# decode output tokens into text
output = tokenizer.batch_decode(output)

# loop over the batch to print, in this example the batch size is 1
for i in output:
    print(i)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>device</span> <span>=</span> <span>&#34;cuda&#34;</span> <span># or &#34;cpu&#34;</span>
<span>model_path</span> <span>=</span> <span>&#34;ibm-granite/granite-3b-code-base&#34;</span> <span># pick anyone from above list</span>

<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_path</span>)

<span># drop device_map if running on CPU</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_path</span>, <span>device_map</span><span>=</span><span>device</span>)
<span>model</span>.<span>eval</span>()

<span># change input text as desired</span>
<span>input_text</span> <span>=</span> <span>&#34;def generate():&#34;</span>
<span># tokenize the text</span>
<span>input_tokens</span> <span>=</span> <span>tokenizer</span>(<span>input_text</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>)

<span># transfer tokenized inputs to the device</span>
<span>for</span> <span>i</span> <span>in</span> <span>input_tokens</span>:
    <span>input_tokens</span>[<span>i</span>] <span>=</span> <span>input_tokens</span>[<span>i</span>].<span>to</span>(<span>device</span>)

<span># generate output tokens</span>
<span>output</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>input_tokens</span>)
<span># decode output tokens into text</span>
<span>output</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>output</span>)

<span># loop over the batch to print, in this example the batch size is 1</span>
<span>for</span> <span>i</span> <span>in</span> <span>output</span>:
    <span>print</span>(<span>i</span>)</pre></div>

<p dir="auto">Codebase coming soon.</p>


<p dir="auto">The model cards for each model variant are available in their respective HuggingFace repository. Please visit our collection <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">here</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">How to Download our Models?</h2><a id="user-content-how-to-download-our-models" aria-label="Permalink: How to Download our Models?" href="#how-to-download-our-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The model of choice (granite-3b-code-base in this example) can be cloned using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://huggingface.co/ibm-granite/granite-3b-code-base"><pre>git clone https://huggingface.co/ibm-granite/granite-3b-code-base</pre></div>

<p dir="auto">All Granite Code Models are distributed under <a href="https://github.com/ibm-granite/granite-code-models/blob/main/LICENSE">Apache 2.0</a> license.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Would you like to provide feedback?</h2><a id="user-content-would-you-like-to-provide-feedback" aria-label="Permalink: Would you like to provide feedback?" href="#would-you-like-to-provide-feedback"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Please let us know your comments about our family of code models by visiting our <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">collection</a>. Select the repository of the model you would like to provide feedback about. Then, go to <em>Community</em> tab, and click on <em>New discussion</em>. Alternatively, you can also post any questions/comments on our <a href="https://github.com/orgs/ibm-granite/discussions">github discussions page</a>.</p>
</article></div></div>
  </body>
</html>
