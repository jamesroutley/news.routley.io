<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/mcp-use/mcp-use">Original</a>
    <h1>Show HN: Mcp-use ‚Äì Connect any LLM to any MCP</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="/mcp-use/mcp-use/raw/main/static/logo_white.svg"/>
    <source media="(prefers-color-scheme: light)" srcset="/mcp-use/mcp-use/raw/main/static/logo_black.svg"/>
    <img alt="mcp use logo" src="https://github.com/mcp-use/mcp-use/raw/main/static/logo-white.svg" width="80%"/>
  </picture></themed-picture>
</div>
</div>
<p dir="auto">üåê MCP-Use is the open source way to connect <strong>any LLM to any MCP server</strong> and build custom MCP agents that have tool access, without using closed source or application clients.</p>
<p dir="auto">üí° Let developers easily connect any LLM to tools like web browsing, file operations, and more.</p>
<ul dir="auto">
<li>If you want to get started quickly check out <a href="https://mcp-use.com/" rel="nofollow">mcp-use.com website</a> to build and deploy agents with your favorite MCP servers.</li>
<li>Visit the <a href="https://docs.mcp-use.com/" rel="nofollow">mcp-use docs</a> to get started with mcp-use library</li>
<li>For the TypeScript version, visit <a href="https://github.com/mcp-use/mcp-use-ts">mcp-use-ts</a></li>
</ul>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Supports</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Primitives</strong></td>
<td><a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/8fed3821959f393e6deaf144ef1d2058db93d94587871e3086277a1fe365ec64/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7072696d69746976652d746f6f6c73266c6162656c3d546f6f6c73267374796c653d666c6174" alt="Tools" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-tools&amp;label=Tools&amp;style=flat"/></a> <a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/de3cf077ed2d3094d50ef3e16f952c8e483436aec38cd2561739d076b77b02f7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7072696d69746976652d7265736f7572636573266c6162656c3d5265736f7572636573267374796c653d666c6174" alt="Resources" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-resources&amp;label=Resources&amp;style=flat"/></a> <a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/05675354878acec80500582eb599dc1e6ba4663de31d28b670b19aa74bccac09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7072696d69746976652d70726f6d707473266c6162656c3d50726f6d707473267374796c653d666c6174" alt="Prompts" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-prompts&amp;label=Prompts&amp;style=flat"/></a> <a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/f90b3540c3cd3ec65dfa4fc1cca24b3bb30ed9f8f844feb9d40b6232ba213fd9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7072696d69746976652d73616d706c696e67266c6162656c3d53616d706c696e67267374796c653d666c6174" alt="Sampling" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-sampling&amp;label=Sampling&amp;style=flat"/></a> <a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/eea87379ff94f34484b357716899e63b98b79d5bb37d575b3ef4a6f788059cb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7072696d69746976652d656c696369746174696f6e266c6162656c3d456c696369746174696f6e267374796c653d666c6174" alt="Elicitation" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-elicitation&amp;label=Elicitation&amp;style=flat"/></a></td>
</tr>
<tr>
<td><strong>Transports</strong></td>
<td><a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/56fa894608f9c22532ae3e4d9ed34cc49faa0f34e6c5080b37912ae3405f312b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7472616e73706f72742d737464696f266c6162656c3d537464696f267374796c653d666c6174" alt="Stdio" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=transport-stdio&amp;label=Stdio&amp;style=flat"/></a> <a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/d47c87e132f3ca3fb02bcbf9e423aca3fde4ab97dda010d7f5a1401d1f6a81c4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7472616e73706f72742d737365266c6162656c3d535345267374796c653d666c6174" alt="SSE" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=transport-sse&amp;label=SSE&amp;style=flat"/></a> <a href="https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml"><img src="https://camo.githubusercontent.com/1f2b1999732a18fae93261c5734a2f2f0f0508499e1976d0368180a3642d04c4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f70696574726f7a756c6c6f2f6d63702d7573652f74657374732e796d6c3f6a6f623d7472616e73706f72742d73747265616d61626c6548747470266c6162656c3d53747265616d61626c6525323048545450267374796c653d666c6174" alt="Streamable HTTP" data-canonical-src="https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=transport-streamableHttp&amp;label=Streamable%20HTTP&amp;style=flat"/></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<markdown-accessiblity-table><table>
  <tbody><tr>
    <th>Feature</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>üîÑ <a href="#quick-start"><strong>Ease of use</strong></a></td>
    <td>Create your first MCP capable agent you need only 6 lines of code</td>
  </tr>
  <tr>
    <td>ü§ñ <a href="#installing-langchain-providers"><strong>LLM Flexibility</strong></a></td>
    <td>Works with any langchain supported LLM that supports tool calling (OpenAI, Anthropic, Groq, LLama etc.)</td>
  </tr>
  <tr>
    <td>üåê <a href="https://mcp-use.com/builder" rel="nofollow"><strong>Code Builder</strong></a></td>
    <td>Explore MCP capabilities and generate starter code with the interactive <a href="https://mcp-use.com/builder" rel="nofollow">code builder</a>.</td>
  </tr>
  <tr>
    <td>üîó <a href="#http-connection-example"><strong>HTTP Support</strong></a></td>
    <td>Direct connection to MCP servers running on specific HTTP ports</td>
  </tr>
  <tr>
    <td>‚öôÔ∏è <a href="#dynamic-server-selection-server-manager"><strong>Dynamic Server Selection</strong></a></td>
    <td>Agents can dynamically choose the most appropriate MCP server for a given task from the available pool</td>
  </tr>
  <tr>
    <td>üß© <a href="#multi-server-support"><strong>Multi-Server Support</strong></a></td>
    <td>Use multiple MCP servers simultaneously in a single agent</td>
  </tr>
  <tr>
    <td>üõ°Ô∏è <a href="#tool-access-control"><strong>Tool Restrictions</strong></a></td>
    <td>Restrict potentially dangerous tools like file system or network access</td>
  </tr>
  <tr>
    <td>üîß <a href="#build-a-custom-agent"><strong>Custom Agents</strong></a></td>
    <td>Build your own agents with any framework using the LangChain adapter or create new adapters</td>
  </tr>
  <tr>
    <td>‚ùì <a href="https://mcp-use.com/what-should-we-build-next" rel="nofollow"><strong>What should we build next</strong></a></td>
    <td>Let us know what you&#39;d like us to build next</td>
  </tr>
</tbody></table></markdown-accessiblity-table>

<p dir="auto">With pip:</p>

<p dir="auto">Or install from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/pietrozullo/mcp-use.git
cd mcp-use
pip install -e ."><pre>git clone https://github.com/pietrozullo/mcp-use.git
<span>cd</span> mcp-use
pip install -e <span>.</span></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Installing LangChain Providers</h3><a id="user-content-installing-langchain-providers" aria-label="Permalink: Installing LangChain Providers" href="#installing-langchain-providers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">mcp_use works with various LLM providers through LangChain. You&#39;ll need to install the appropriate LangChain provider package for your chosen LLM. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# For OpenAI
pip install langchain-openai

# For Anthropic
pip install langchain-anthropic"><pre><span><span>#</span> For OpenAI</span>
pip install langchain-openai

<span><span>#</span> For Anthropic</span>
pip install langchain-anthropic</pre></div>
<p dir="auto">For other providers, check the <a href="https://python.langchain.com/docs/integrations/chat/" rel="nofollow">LangChain chat models documentation</a> and add your API keys for the provider you want to use to your <code>.env</code> file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="OPENAI_API_KEY=
ANTHROPIC_API_KEY="><pre>OPENAI_API_KEY=
ANTHROPIC_API_KEY=</pre></div>
<blockquote>
<p dir="auto"><strong>Important</strong>: Only models with tool calling capabilities can be used with mcp_use. Make sure your chosen model supports function calling or tool use.</p>
</blockquote>

<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def main():
    # Load environment variables
    load_dotenv()

    # Create configuration dictionary
    config = {
      &#34;mcpServers&#34;: {
        &#34;playwright&#34;: {
          &#34;command&#34;: &#34;npx&#34;,
          &#34;args&#34;: [&#34;@playwright/mcp@latest&#34;],
          &#34;env&#34;: {
            &#34;DISPLAY&#34;: &#34;:1&#34;
          }
        }
      }
    }

    # Create MCPClient from configuration dictionary
    client = MCPClient.from_dict(config)

    # Create LLM
    llm = ChatOpenAI(model=&#34;gpt-4o&#34;)

    # Create agent with the client
    agent = MCPAgent(llm=llm, client=client, max_steps=30)

    # Run the query
    result = await agent.run(
        &#34;Find the best restaurant in San Francisco&#34;,
    )
    print(f&#34;\nResult: {result}&#34;)

if __name__ == &#34;__main__&#34;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>import</span> <span>os</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># Load environment variables</span>
    <span>load_dotenv</span>()

    <span># Create configuration dictionary</span>
    <span>config</span> <span>=</span> {
      <span>&#34;mcpServers&#34;</span>: {
        <span>&#34;playwright&#34;</span>: {
          <span>&#34;command&#34;</span>: <span>&#34;npx&#34;</span>,
          <span>&#34;args&#34;</span>: [<span>&#34;@playwright/mcp@latest&#34;</span>],
          <span>&#34;env&#34;</span>: {
            <span>&#34;DISPLAY&#34;</span>: <span>&#34;:1&#34;</span>
          }
        }
      }
    }

    <span># Create MCPClient from configuration dictionary</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_dict</span>(<span>config</span>)

    <span># Create LLM</span>
    <span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>)

    <span># Create agent with the client</span>
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(<span>llm</span><span>=</span><span>llm</span>, <span>client</span><span>=</span><span>client</span>, <span>max_steps</span><span>=</span><span>30</span>)

    <span># Run the query</span>
    <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
        <span>&#34;Find the best restaurant in San Francisco&#34;</span>,
    )
    <span>print</span>(<span>f&#34;<span>\n</span>Result: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto">You can also add the servers configuration from a config file like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="client = MCPClient.from_config_file(
        os.path.join(&#34;browser_mcp.json&#34;)
    )"><pre><span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_config_file</span>(
        <span>os</span>.<span>path</span>.<span>join</span>(<span>&#34;browser_mcp.json&#34;</span>)
    )</pre></div>
<p dir="auto">Example configuration file (<code>browser_mcp.json</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;mcpServers&#34;: {
    &#34;playwright&#34;: {
      &#34;command&#34;: &#34;npx&#34;,
      &#34;args&#34;: [&#34;@playwright/mcp@latest&#34;],
      &#34;env&#34;: {
        &#34;DISPLAY&#34;: &#34;:1&#34;
      }
    }
  }
}"><pre>{
  <span>&#34;mcpServers&#34;</span>: {
    <span>&#34;playwright&#34;</span>: {
      <span>&#34;command&#34;</span>: <span><span>&#34;</span>npx<span>&#34;</span></span>,
      <span>&#34;args&#34;</span>: [<span><span>&#34;</span>@playwright/mcp@latest<span>&#34;</span></span>],
      <span>&#34;env&#34;</span>: {
        <span>&#34;DISPLAY&#34;</span>: <span><span>&#34;</span>:1<span>&#34;</span></span>
      }
    }
  }
}</pre></div>
<p dir="auto">For other settings, models, and more, check out the documentation.</p>

<p dir="auto">MCP-Use supports asynchronous streaming of agent output using the <code>astream</code> method on <code>MCPAgent</code>. This allows you to receive incremental results, tool actions, and intermediate steps as they are generated by the agent, enabling real-time feedback and progress reporting.</p>

<p dir="auto">Call <code>agent.astream(query)</code> and iterate over the results asynchronously:</p>
<div dir="auto" data-snippet-clipboard-copy-content="async for chunk in agent.astream(&#34;Find the best restaurant in San Francisco&#34;):
    print(chunk[&#34;messages&#34;], end=&#34;&#34;, flush=True)"><pre><span>async</span> <span>for</span> <span>chunk</span> <span>in</span> <span>agent</span>.<span>astream</span>(<span>&#34;Find the best restaurant in San Francisco&#34;</span>):
    <span>print</span>(<span>chunk</span>[<span>&#34;messages&#34;</span>], <span>end</span><span>=</span><span>&#34;&#34;</span>, <span>flush</span><span>=</span><span>True</span>)</pre></div>
<p dir="auto">Each chunk is a dictionary containing keys such as <code>actions</code>, <code>steps</code>, <code>messages</code>, and (on the last chunk) <code>output</code>. This enables you to build responsive UIs or log agent progress in real time.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Example: Streaming in Practice</h4><a id="user-content-example-streaming-in-practice" aria-label="Permalink: Example: Streaming in Practice" href="#example-streaming-in-practice"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def main():
    load_dotenv()
    client = MCPClient.from_config_file(&#34;browser_mcp.json&#34;)
    llm = ChatOpenAI(model=&#34;gpt-4o&#34;)
    agent = MCPAgent(llm=llm, client=client, max_steps=30)
    async for chunk in agent.astream(&#34;Look for job at nvidia for machine learning engineer.&#34;):
        print(chunk[&#34;messages&#34;], end=&#34;&#34;, flush=True)

if __name__ == &#34;__main__&#34;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>import</span> <span>os</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span>load_dotenv</span>()
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_config_file</span>(<span>&#34;browser_mcp.json&#34;</span>)
    <span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>)
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(<span>llm</span><span>=</span><span>llm</span>, <span>client</span><span>=</span><span>client</span>, <span>max_steps</span><span>=</span><span>30</span>)
    <span>async</span> <span>for</span> <span>chunk</span> <span>in</span> <span>agent</span>.<span>astream</span>(<span>&#34;Look for job at nvidia for machine learning engineer.&#34;</span>):
        <span>print</span>(<span>chunk</span>[<span>&#34;messages&#34;</span>], <span>end</span><span>=</span><span>&#34;&#34;</span>, <span>flush</span><span>=</span><span>True</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto">This streaming interface is ideal for applications that require real-time updates, such as chatbots, dashboards, or interactive notebooks.</p>

<div dir="auto"><h2 tabindex="-1" dir="auto">Web Browsing with Playwright</h2><a id="user-content-web-browsing-with-playwright" aria-label="Permalink: Web Browsing with Playwright" href="#web-browsing-with-playwright"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def main():
    # Load environment variables
    load_dotenv()

    # Create MCPClient from config file
    client = MCPClient.from_config_file(
        os.path.join(os.path.dirname(__file__), &#34;browser_mcp.json&#34;)
    )

    # Create LLM
    llm = ChatOpenAI(model=&#34;gpt-4o&#34;)
    # Alternative models:
    # llm = ChatAnthropic(model=&#34;claude-3-5-sonnet-20240620&#34;)
    # llm = ChatGroq(model=&#34;llama3-8b-8192&#34;)

    # Create agent with the client
    agent = MCPAgent(llm=llm, client=client, max_steps=30)

    # Run the query
    result = await agent.run(
        &#34;Find the best restaurant in San Francisco USING GOOGLE SEARCH&#34;,
        max_steps=30,
    )
    print(f&#34;\nResult: {result}&#34;)

if __name__ == &#34;__main__&#34;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>import</span> <span>os</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># Load environment variables</span>
    <span>load_dotenv</span>()

    <span># Create MCPClient from config file</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_config_file</span>(
        <span>os</span>.<span>path</span>.<span>join</span>(<span>os</span>.<span>path</span>.<span>dirname</span>(<span>__file__</span>), <span>&#34;browser_mcp.json&#34;</span>)
    )

    <span># Create LLM</span>
    <span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>)
    <span># Alternative models:</span>
    <span># llm = ChatAnthropic(model=&#34;claude-3-5-sonnet-20240620&#34;)</span>
    <span># llm = ChatGroq(model=&#34;llama3-8b-8192&#34;)</span>

    <span># Create agent with the client</span>
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(<span>llm</span><span>=</span><span>llm</span>, <span>client</span><span>=</span><span>client</span>, <span>max_steps</span><span>=</span><span>30</span>)

    <span># Run the query</span>
    <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
        <span>&#34;Find the best restaurant in San Francisco USING GOOGLE SEARCH&#34;</span>,
        <span>max_steps</span><span>=</span><span>30</span>,
    )
    <span>print</span>(<span>f&#34;<span>\n</span>Result: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
import os
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from mcp_use import MCPAgent, MCPClient

async def run_airbnb_example():
    # Load environment variables
    load_dotenv()

    # Create MCPClient with Airbnb configuration
    client = MCPClient.from_config_file(
        os.path.join(os.path.dirname(__file__), &#34;airbnb_mcp.json&#34;)
    )

    # Create LLM - you can choose between different models
    llm = ChatAnthropic(model=&#34;claude-3-5-sonnet-20240620&#34;)

    # Create agent with the client
    agent = MCPAgent(llm=llm, client=client, max_steps=30)

    try:
        # Run a query to search for accommodations
        result = await agent.run(
            &#34;Find me a nice place to stay in Barcelona for 2 adults &#34;
            &#34;for a week in August. I prefer places with a pool and &#34;
            &#34;good reviews. Show me the top 3 options.&#34;,
            max_steps=30,
        )
        print(f&#34;\nResult: {result}&#34;)
    finally:
        # Ensure we clean up resources properly
        if client.sessions:
            await client.close_all_sessions()

if __name__ == &#34;__main__&#34;:
    asyncio.run(run_airbnb_example())"><pre><span>import</span> <span>asyncio</span>
<span>import</span> <span>os</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>from</span> <span>langchain_anthropic</span> <span>import</span> <span>ChatAnthropic</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>

<span>async</span> <span>def</span> <span>run_airbnb_example</span>():
    <span># Load environment variables</span>
    <span>load_dotenv</span>()

    <span># Create MCPClient with Airbnb configuration</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_config_file</span>(
        <span>os</span>.<span>path</span>.<span>join</span>(<span>os</span>.<span>path</span>.<span>dirname</span>(<span>__file__</span>), <span>&#34;airbnb_mcp.json&#34;</span>)
    )

    <span># Create LLM - you can choose between different models</span>
    <span>llm</span> <span>=</span> <span>ChatAnthropic</span>(<span>model</span><span>=</span><span>&#34;claude-3-5-sonnet-20240620&#34;</span>)

    <span># Create agent with the client</span>
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(<span>llm</span><span>=</span><span>llm</span>, <span>client</span><span>=</span><span>client</span>, <span>max_steps</span><span>=</span><span>30</span>)

    <span>try</span>:
        <span># Run a query to search for accommodations</span>
        <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
            <span>&#34;Find me a nice place to stay in Barcelona for 2 adults &#34;</span>
            <span>&#34;for a week in August. I prefer places with a pool and &#34;</span>
            <span>&#34;good reviews. Show me the top 3 options.&#34;</span>,
            <span>max_steps</span><span>=</span><span>30</span>,
        )
        <span>print</span>(<span>f&#34;<span>\n</span>Result: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)
    <span>finally</span>:
        <span># Ensure we clean up resources properly</span>
        <span>if</span> <span>client</span>.<span>sessions</span>:
            <span>await</span> <span>client</span>.<span>close_all_sessions</span>()

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>run_airbnb_example</span>())</pre></div>
<p dir="auto">Example configuration file (<code>airbnb_mcp.json</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;mcpServers&#34;: {
    &#34;airbnb&#34;: {
      &#34;command&#34;: &#34;npx&#34;,
      &#34;args&#34;: [&#34;-y&#34;, &#34;@openbnb/mcp-server-airbnb&#34;]
    }
  }
}"><pre>{
  <span>&#34;mcpServers&#34;</span>: {
    <span>&#34;airbnb&#34;</span>: {
      <span>&#34;command&#34;</span>: <span><span>&#34;</span>npx<span>&#34;</span></span>,
      <span>&#34;args&#34;</span>: [<span><span>&#34;</span>-y<span>&#34;</span></span>, <span><span>&#34;</span>@openbnb/mcp-server-airbnb<span>&#34;</span></span>]
    }
  }
}</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from mcp_use import MCPAgent, MCPClient

async def run_blender_example():
    # Load environment variables
    load_dotenv()

    # Create MCPClient with Blender MCP configuration
    config = {&#34;mcpServers&#34;: {&#34;blender&#34;: {&#34;command&#34;: &#34;uvx&#34;, &#34;args&#34;: [&#34;blender-mcp&#34;]}}}
    client = MCPClient.from_dict(config)

    # Create LLM
    llm = ChatAnthropic(model=&#34;claude-3-5-sonnet-20240620&#34;)

    # Create agent with the client
    agent = MCPAgent(llm=llm, client=client, max_steps=30)

    try:
        # Run the query
        result = await agent.run(
            &#34;Create an inflatable cube with soft material and a plane as ground.&#34;,
            max_steps=30,
        )
        print(f&#34;\nResult: {result}&#34;)
    finally:
        # Ensure we clean up resources properly
        if client.sessions:
            await client.close_all_sessions()

if __name__ == &#34;__main__&#34;:
    asyncio.run(run_blender_example())"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>from</span> <span>langchain_anthropic</span> <span>import</span> <span>ChatAnthropic</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>

<span>async</span> <span>def</span> <span>run_blender_example</span>():
    <span># Load environment variables</span>
    <span>load_dotenv</span>()

    <span># Create MCPClient with Blender MCP configuration</span>
    <span>config</span> <span>=</span> {<span>&#34;mcpServers&#34;</span>: {<span>&#34;blender&#34;</span>: {<span>&#34;command&#34;</span>: <span>&#34;uvx&#34;</span>, <span>&#34;args&#34;</span>: [<span>&#34;blender-mcp&#34;</span>]}}}
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_dict</span>(<span>config</span>)

    <span># Create LLM</span>
    <span>llm</span> <span>=</span> <span>ChatAnthropic</span>(<span>model</span><span>=</span><span>&#34;claude-3-5-sonnet-20240620&#34;</span>)

    <span># Create agent with the client</span>
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(<span>llm</span><span>=</span><span>llm</span>, <span>client</span><span>=</span><span>client</span>, <span>max_steps</span><span>=</span><span>30</span>)

    <span>try</span>:
        <span># Run the query</span>
        <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
            <span>&#34;Create an inflatable cube with soft material and a plane as ground.&#34;</span>,
            <span>max_steps</span><span>=</span><span>30</span>,
        )
        <span>print</span>(<span>f&#34;<span>\n</span>Result: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)
    <span>finally</span>:
        <span># Ensure we clean up resources properly</span>
        <span>if</span> <span>client</span>.<span>sessions</span>:
            <span>await</span> <span>client</span>.<span>close_all_sessions</span>()

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>run_blender_example</span>())</pre></div>

<p dir="auto">MCP-Use supports initialization from configuration files, making it easy to manage and switch between different MCP server setups:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from mcp_use import create_session_from_config

async def main():
    # Create an MCP session from a config file
    session = create_session_from_config(&#34;mcp-config.json&#34;)

    # Initialize the session
    await session.initialize()

    # Use the session...

    # Disconnect when done
    await session.disconnect()

if __name__ == &#34;__main__&#34;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>create_session_from_config</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># Create an MCP session from a config file</span>
    <span>session</span> <span>=</span> <span>create_session_from_config</span>(<span>&#34;mcp-config.json&#34;</span>)

    <span># Initialize the session</span>
    <span>await</span> <span>session</span>.<span>initialize</span>()

    <span># Use the session...</span>

    <span># Disconnect when done</span>
    <span>await</span> <span>session</span>.<span>disconnect</span>()

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>

<p dir="auto">MCP-Use supports HTTP connections, allowing you to connect to MCP servers running on specific HTTP ports. This feature is particularly useful for integrating with web-based MCP servers.</p>
<p dir="auto">Here&#39;s an example of how to use the HTTP connection feature:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient

async def main():
    &#34;&#34;&#34;Run the example using a configuration file.&#34;&#34;&#34;
    # Load environment variables
    load_dotenv()

    config = {
        &#34;mcpServers&#34;: {
            &#34;http&#34;: {
                &#34;url&#34;: &#34;http://localhost:8931/sse&#34;
            }
        }
    }

    # Create MCPClient from config file
    client = MCPClient.from_dict(config)

    # Create LLM
    llm = ChatOpenAI(model=&#34;gpt-4o&#34;)

    # Create agent with the client
    agent = MCPAgent(llm=llm, client=client, max_steps=30)

    # Run the query
    result = await agent.run(
        &#34;Find the best restaurant in San Francisco USING GOOGLE SEARCH&#34;,
        max_steps=30,
    )
    print(f&#34;\nResult: {result}&#34;)

if __name__ == &#34;__main__&#34;:
    # Run the appropriate example
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>import</span> <span>os</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span>&#34;&#34;&#34;Run the example using a configuration file.&#34;&#34;&#34;</span>
    <span># Load environment variables</span>
    <span>load_dotenv</span>()

    <span>config</span> <span>=</span> {
        <span>&#34;mcpServers&#34;</span>: {
            <span>&#34;http&#34;</span>: {
                <span>&#34;url&#34;</span>: <span>&#34;http://localhost:8931/sse&#34;</span>
            }
        }
    }

    <span># Create MCPClient from config file</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_dict</span>(<span>config</span>)

    <span># Create LLM</span>
    <span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>)

    <span># Create agent with the client</span>
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(<span>llm</span><span>=</span><span>llm</span>, <span>client</span><span>=</span><span>client</span>, <span>max_steps</span><span>=</span><span>30</span>)

    <span># Run the query</span>
    <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
        <span>&#34;Find the best restaurant in San Francisco USING GOOGLE SEARCH&#34;</span>,
        <span>max_steps</span><span>=</span><span>30</span>,
    )
    <span>print</span>(<span>f&#34;<span>\n</span>Result: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span># Run the appropriate example</span>
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto">This example demonstrates how to connect to an MCP server running on a specific HTTP port. Make sure to start your MCP server before running this example.</p>

<p dir="auto">MCP-Use allows configuring and connecting to multiple MCP servers simultaneously using the <code>MCPClient</code>. This enables complex workflows that require tools from different servers, such as web browsing combined with file operations or 3D modeling.</p>

<p dir="auto">You can configure multiple servers in your configuration file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;mcpServers&#34;: {
    &#34;airbnb&#34;: {
      &#34;command&#34;: &#34;npx&#34;,
      &#34;args&#34;: [&#34;-y&#34;, &#34;@openbnb/mcp-server-airbnb&#34;, &#34;--ignore-robots-txt&#34;]
    },
    &#34;playwright&#34;: {
      &#34;command&#34;: &#34;npx&#34;,
      &#34;args&#34;: [&#34;@playwright/mcp@latest&#34;],
      &#34;env&#34;: {
        &#34;DISPLAY&#34;: &#34;:1&#34;
      }
    }
  }
}"><pre>{
  <span>&#34;mcpServers&#34;</span>: {
    <span>&#34;airbnb&#34;</span>: {
      <span>&#34;command&#34;</span>: <span><span>&#34;</span>npx<span>&#34;</span></span>,
      <span>&#34;args&#34;</span>: [<span><span>&#34;</span>-y<span>&#34;</span></span>, <span><span>&#34;</span>@openbnb/mcp-server-airbnb<span>&#34;</span></span>, <span><span>&#34;</span>--ignore-robots-txt<span>&#34;</span></span>]
    },
    <span>&#34;playwright&#34;</span>: {
      <span>&#34;command&#34;</span>: <span><span>&#34;</span>npx<span>&#34;</span></span>,
      <span>&#34;args&#34;</span>: [<span><span>&#34;</span>@playwright/mcp@latest<span>&#34;</span></span>],
      <span>&#34;env&#34;</span>: {
        <span>&#34;DISPLAY&#34;</span>: <span><span>&#34;</span>:1<span>&#34;</span></span>
      }
    }
  }
}</pre></div>

<p dir="auto">The <code>MCPClient</code> class provides methods for managing connections to multiple servers. When creating an <code>MCPAgent</code>, you can provide an <code>MCPClient</code> configured with multiple servers.</p>
<p dir="auto">By default, the agent will have access to tools from all configured servers. If you need to target a specific server for a particular task, you can specify the <code>server_name</code> when calling the <code>agent.run()</code> method.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Example: Manually selecting a server for a specific task
result = await agent.run(
    &#34;Search for Airbnb listings in Barcelona&#34;,
    server_name=&#34;airbnb&#34; # Explicitly use the airbnb server
)

result_google = await agent.run(
    &#34;Find restaurants near the first result using Google Search&#34;,
    server_name=&#34;playwright&#34; # Explicitly use the playwright server
)"><pre><span># Example: Manually selecting a server for a specific task</span>
<span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
    <span>&#34;Search for Airbnb listings in Barcelona&#34;</span>,
    <span>server_name</span><span>=</span><span>&#34;airbnb&#34;</span> <span># Explicitly use the airbnb server</span>
)

<span>result_google</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
    <span>&#34;Find restaurants near the first result using Google Search&#34;</span>,
    <span>server_name</span><span>=</span><span>&#34;playwright&#34;</span> <span># Explicitly use the playwright server</span>
)</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Dynamic Server Selection (Server Manager)</h2><a id="user-content-dynamic-server-selection-server-manager" aria-label="Permalink: Dynamic Server Selection (Server Manager)" href="#dynamic-server-selection-server-manager"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For enhanced efficiency and to reduce potential agent confusion when dealing with many tools from different servers, you can enable the Server Manager by setting <code>use_server_manager=True</code> during <code>MCPAgent</code> initialization.</p>
<p dir="auto">When enabled, the agent intelligently selects the correct MCP server based on the tool chosen by the LLM for a specific step. This minimizes unnecessary connections and ensures the agent uses the appropriate tools for the task.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from mcp_use import MCPClient, MCPAgent
from langchain_anthropic import ChatAnthropic

async def main():
    # Create client with multiple servers
    client = MCPClient.from_config_file(&#34;multi_server_config.json&#34;)

    # Create agent with the client
    agent = MCPAgent(
        llm=ChatAnthropic(model=&#34;claude-3-5-sonnet-20240620&#34;),
        client=client,
        use_server_manager=True  # Enable the Server Manager
    )

    try:
        # Run a query that uses tools from multiple servers
        result = await agent.run(
            &#34;Search for a nice place to stay in Barcelona on Airbnb, &#34;
            &#34;then use Google to find nearby restaurants and attractions.&#34;
        )
        print(result)
    finally:
        # Clean up all sessions
        await client.close_all_sessions()

if __name__ == &#34;__main__&#34;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPClient</span>, <span>MCPAgent</span>
<span>from</span> <span>langchain_anthropic</span> <span>import</span> <span>ChatAnthropic</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># Create client with multiple servers</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_config_file</span>(<span>&#34;multi_server_config.json&#34;</span>)

    <span># Create agent with the client</span>
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(
        <span>llm</span><span>=</span><span>ChatAnthropic</span>(<span>model</span><span>=</span><span>&#34;claude-3-5-sonnet-20240620&#34;</span>),
        <span>client</span><span>=</span><span>client</span>,
        <span>use_server_manager</span><span>=</span><span>True</span>  <span># Enable the Server Manager</span>
    )

    <span>try</span>:
        <span># Run a query that uses tools from multiple servers</span>
        <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
            <span>&#34;Search for a nice place to stay in Barcelona on Airbnb, &#34;</span>
            <span>&#34;then use Google to find nearby restaurants and attractions.&#34;</span>
        )
        <span>print</span>(<span>result</span>)
    <span>finally</span>:
        <span># Clean up all sessions</span>
        <span>await</span> <span>client</span>.<span>close_all_sessions</span>()

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>

<p dir="auto">MCP-Use allows you to restrict which tools are available to the agent, providing better security and control over agent capabilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from mcp_use import MCPAgent, MCPClient
from langchain_openai import ChatOpenAI

async def main():
    # Create client
    client = MCPClient.from_config_file(&#34;config.json&#34;)

    # Create agent with restricted tools
    agent = MCPAgent(
        llm=ChatOpenAI(model=&#34;gpt-4&#34;),
        client=client,
        disallowed_tools=[&#34;file_system&#34;, &#34;network&#34;]  # Restrict potentially dangerous tools
    )

    # Run a query with restricted tool access
    result = await agent.run(
        &#34;Find the best restaurant in San Francisco&#34;
    )
    print(result)

    # Clean up
    await client.close_all_sessions()

if __name__ == &#34;__main__&#34;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># Create client</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_config_file</span>(<span>&#34;config.json&#34;</span>)

    <span># Create agent with restricted tools</span>
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(
        <span>llm</span><span>=</span><span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#34;gpt-4&#34;</span>),
        <span>client</span><span>=</span><span>client</span>,
        <span>disallowed_tools</span><span>=</span>[<span>&#34;file_system&#34;</span>, <span>&#34;network&#34;</span>]  <span># Restrict potentially dangerous tools</span>
    )

    <span># Run a query with restricted tool access</span>
    <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(
        <span>&#34;Find the best restaurant in San Francisco&#34;</span>
    )
    <span>print</span>(<span>result</span>)

    <span># Clean up</span>
    <span>await</span> <span>client</span>.<span>close_all_sessions</span>()

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>

<p dir="auto">MCP-Use supports running MCP servers in a sandboxed environment using E2B&#39;s cloud infrastructure. This allows you to run MCP servers without having to install dependencies locally, making it easier to use tools that might have complex setups or system requirements.</p>

<p dir="auto">To use sandboxed execution, you need to install the E2B dependency:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install mcp-use with E2B support
pip install &#34;mcp-use[e2b]&#34;

# Or install the dependency directly
pip install e2b-code-interpreter"><pre><span><span>#</span> Install mcp-use with E2B support</span>
pip install <span><span>&#34;</span>mcp-use[e2b]<span>&#34;</span></span>

<span><span>#</span> Or install the dependency directly</span>
pip install e2b-code-interpreter</pre></div>
<p dir="auto">You&#39;ll also need an E2B API key. You can sign up at <a href="https://e2b.dev" rel="nofollow">e2b.dev</a> to get your API key.</p>

<p dir="auto">To enable sandboxed execution, use the sandbox parameter when creating your <code>MCPClient</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPAgent, MCPClient
from mcp_use.types.sandbox import SandboxOptions

async def main():
    # Load environment variables (needs E2B_API_KEY)
    load_dotenv()

    # Define MCP server configuration
    server_config = {
        &#34;mcpServers&#34;: {
            &#34;everything&#34;: {
                &#34;command&#34;: &#34;npx&#34;,
                &#34;args&#34;: [&#34;-y&#34;, &#34;@modelcontextprotocol/server-everything&#34;],
            }
        }
    }

    # Define sandbox options
    sandbox_options: SandboxOptions = {
        &#34;api_key&#34;: os.getenv(&#34;E2B_API_KEY&#34;),  # API key can also be provided directly
        &#34;sandbox_template_id&#34;: &#34;base&#34;,  # Use base template
    }

    # Create client with sandboxed mode enabled
    client = MCPClient(
        config=server_config,
        sandbox=True,
        sandbox_options=sandbox_options,

    )

    # Create agent with the sandboxed client
    llm = ChatOpenAI(model=&#34;gpt-4o&#34;)
    agent = MCPAgent(llm=llm, client=client)

    # Run your agent
    result = await agent.run(&#34;Use the command line tools to help me add 1+1&#34;)
    print(result)

    # Clean up
    await client.close_all_sessions()

if __name__ == &#34;__main__&#34;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>import</span> <span>os</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>mcp_use</span> <span>import</span> <span>MCPAgent</span>, <span>MCPClient</span>
<span>from</span> <span>mcp_use</span>.<span>types</span>.<span>sandbox</span> <span>import</span> <span>SandboxOptions</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># Load environment variables (needs E2B_API_KEY)</span>
    <span>load_dotenv</span>()

    <span># Define MCP server configuration</span>
    <span>server_config</span> <span>=</span> {
        <span>&#34;mcpServers&#34;</span>: {
            <span>&#34;everything&#34;</span>: {
                <span>&#34;command&#34;</span>: <span>&#34;npx&#34;</span>,
                <span>&#34;args&#34;</span>: [<span>&#34;-y&#34;</span>, <span>&#34;@modelcontextprotocol/server-everything&#34;</span>],
            }
        }
    }

    <span># Define sandbox options</span>
    <span>sandbox_options</span>: <span>SandboxOptions</span> <span>=</span> {
        <span>&#34;api_key&#34;</span>: <span>os</span>.<span>getenv</span>(<span>&#34;E2B_API_KEY&#34;</span>),  <span># API key can also be provided directly</span>
        <span>&#34;sandbox_template_id&#34;</span>: <span>&#34;base&#34;</span>,  <span># Use base template</span>
    }

    <span># Create client with sandboxed mode enabled</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>(
        <span>config</span><span>=</span><span>server_config</span>,
        <span>sandbox</span><span>=</span><span>True</span>,
        <span>sandbox_options</span><span>=</span><span>sandbox_options</span>,

    )

    <span># Create agent with the sandboxed client</span>
    <span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>)
    <span>agent</span> <span>=</span> <span>MCPAgent</span>(<span>llm</span><span>=</span><span>llm</span>, <span>client</span><span>=</span><span>client</span>)

    <span># Run your agent</span>
    <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>(<span>&#34;Use the command line tools to help me add 1+1&#34;</span>)
    <span>print</span>(<span>result</span>)

    <span># Clean up</span>
    <span>await</span> <span>client</span>.<span>close_all_sessions</span>()

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>

<p dir="auto">The <code>SandboxOptions</code> type provides configuration for the sandbox environment:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>api_key</code></td>
<td>E2B API key. Required - can be provided directly or via E2B_API_KEY environment variable</td>
<td>None</td>
</tr>
<tr>
<td><code>sandbox_template_id</code></td>
<td>Template ID for the sandbox environment</td>
<td>&#34;base&#34;</td>
</tr>
<tr>
<td><code>supergateway_command</code></td>
<td>Command to run supergateway</td>
<td>&#34;npx -y supergateway&#34;</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h2 tabindex="-1" dir="auto">Benefits of Sandboxed Execution</h2><a id="user-content-benefits-of-sandboxed-execution" aria-label="Permalink: Benefits of Sandboxed Execution" href="#benefits-of-sandboxed-execution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>No local dependencies</strong>: Run MCP servers without installing dependencies locally</li>
<li><strong>Isolation</strong>: Execute code in a secure, isolated environment</li>
<li><strong>Consistent environment</strong>: Ensure consistent behavior across different systems</li>
<li><strong>Resource efficiency</strong>: Offload resource-intensive tasks to cloud infrastructure</li>
</ul>

<p dir="auto">You can also build your own custom agent using the LangChain adapter:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from langchain_openai import ChatOpenAI
from mcp_use.client import MCPClient
from mcp_use.adapters.langchain_adapter import LangChainAdapter
from dotenv import load_dotenv

load_dotenv()


async def main():
    # Initialize MCP client
    client = MCPClient.from_config_file(&#34;examples/browser_mcp.json&#34;)
    llm = ChatOpenAI(model=&#34;gpt-4o&#34;)

    # Create adapter instance
    adapter = LangChainAdapter()
    # Get LangChain tools with a single line
    tools = await adapter.create_tools(client)

    # Create a custom LangChain agent
    llm_with_tools = llm.bind_tools(tools)
    result = await llm_with_tools.ainvoke(&#34;What tools do you have available ? &#34;)
    print(result)


if __name__ == &#34;__main__&#34;:
    asyncio.run(main())

"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>mcp_use</span>.<span>client</span> <span>import</span> <span>MCPClient</span>
<span>from</span> <span>mcp_use</span>.<span>adapters</span>.<span>langchain_adapter</span> <span>import</span> <span>LangChainAdapter</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>

<span>load_dotenv</span>()


<span>async</span> <span>def</span> <span>main</span>():
    <span># Initialize MCP client</span>
    <span>client</span> <span>=</span> <span>MCPClient</span>.<span>from_config_file</span>(<span>&#34;examples/browser_mcp.json&#34;</span>)
    <span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>)

    <span># Create adapter instance</span>
    <span>adapter</span> <span>=</span> <span>LangChainAdapter</span>()
    <span># Get LangChain tools with a single line</span>
    <span>tools</span> <span>=</span> <span>await</span> <span>adapter</span>.<span>create_tools</span>(<span>client</span>)

    <span># Create a custom LangChain agent</span>
    <span>llm_with_tools</span> <span>=</span> <span>llm</span>.<span>bind_tools</span>(<span>tools</span>)
    <span>result</span> <span>=</span> <span>await</span> <span>llm_with_tools</span>.<span>ainvoke</span>(<span>&#34;What tools do you have available ? &#34;</span>)
    <span>print</span>(<span>result</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())
</pre></div>

<p dir="auto">MCP-Use provides a built-in debug mode that increases log verbosity and helps diagnose issues in your agent implementation.</p>

<p dir="auto">There are two primary ways to enable debug mode:</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">1. Environment Variable (Recommended for One-off Runs)</h3><a id="user-content-1-environment-variable-recommended-for-one-off-runs" aria-label="Permalink: 1. Environment Variable (Recommended for One-off Runs)" href="#1-environment-variable-recommended-for-one-off-runs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Run your script with the <code>DEBUG</code> environment variable set to the desired level:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Level 1: Show INFO level messages
DEBUG=1 python3.11 examples/browser_use.py

# Level 2: Show DEBUG level messages (full verbose output)
DEBUG=2 python3.11 examples/browser_use.py"><pre><span><span>#</span> Level 1: Show INFO level messages</span>
DEBUG=1 python3.11 examples/browser_use.py

<span><span>#</span> Level 2: Show DEBUG level messages (full verbose output)</span>
DEBUG=2 python3.11 examples/browser_use.py</pre></div>
<p dir="auto">This sets the debug level only for the duration of that specific Python process.</p>
<p dir="auto">Alternatively you can set the following environment variable to the desired logging level:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export MCP_USE_DEBUG=1 # or 2"><pre><span>export</span> MCP_USE_DEBUG=1 <span><span>#</span> or 2</span></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">2. Setting the Debug Flag Programmatically</h3><a id="user-content-2-setting-the-debug-flag-programmatically" aria-label="Permalink: 2. Setting the Debug Flag Programmatically" href="#2-setting-the-debug-flag-programmatically"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can set the global debug flag directly in your code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import mcp_use

mcp_use.set_debug(1)  # INFO level
# or
mcp_use.set_debug(2)  # DEBUG level (full verbose output)"><pre><span>import</span> <span>mcp_use</span>

<span>mcp_use</span>.<span>set_debug</span>(<span>1</span>)  <span># INFO level</span>
<span># or</span>
<span>mcp_use</span>.<span>set_debug</span>(<span>2</span>)  <span># DEBUG level (full verbose output)</span></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">3. Agent-Specific Verbosity</h3><a id="user-content-3-agent-specific-verbosity" aria-label="Permalink: 3. Agent-Specific Verbosity" href="#3-agent-specific-verbosity"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you only want to see debug information from the agent without enabling full debug logging, you can set the <code>verbose</code> parameter when creating an MCPAgent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create agent with increased verbosity
agent = MCPAgent(
    llm=your_llm,
    client=your_client,
    verbose=True  # Only shows debug messages from the agent
)"><pre><span># Create agent with increased verbosity</span>
<span>agent</span> <span>=</span> <span>MCPAgent</span>(
    <span>llm</span><span>=</span><span>your_llm</span>,
    <span>client</span><span>=</span><span>your_client</span>,
    <span>verbose</span><span>=</span><span>True</span>  <span># Only shows debug messages from the agent</span>
)</pre></div>
<p dir="auto">This is useful when you only need to see the agent&#39;s steps and decision-making process without all the low-level debug information from other components.</p>

<p dir="auto"><a href="https://www.star-history.com/#pietrozullo/mcp-use&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/311fde38492458b0d8df86781966a5538f84845eb4eff36715323586b747d10d/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d70696574726f7a756c6c6f2f6d63702d75736526747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=pietrozullo/mcp-use&amp;type=Date"/></a></p>

<p dir="auto">We love contributions! Feel free to open issues for bugs or feature requests. Look at <a href="https://github.com/mcp-use/mcp-use/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for guidelines.</p>

<p dir="auto">Thanks to all our amazing contributors!</p>
<a href="https://github.com/mcp-use/mcp-use/graphs/contributors">
  <img src="https://camo.githubusercontent.com/2bb8152b37ff465a8d6c220c1a46d99e7f9432934e6066b833183c8a95cf13aa/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6d63702d7573652f6d63702d757365" data-canonical-src="https://contrib.rocks/image?repo=mcp-use/mcp-use"/>
</a>


<markdown-accessiblity-table><table>
  <tbody><tr>
    <th>Repository</th>
    <th>Stars</th>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/170207473?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/170207473?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/tavily-ai/meeting-prep-agent"><strong>tavily-ai/meeting-prep-agent</strong></a></td>
    <td>‚≠ê 127</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/205593730?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/205593730?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/Qingyon-AI/Revornix"><strong>Qingyon-AI/Revornix</strong></a></td>
    <td>‚≠ê 108</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/20041231?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/20041231?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/krishnaik06/MCP-CRASH-Course"><strong>krishnaik06/MCP-CRASH-Course</strong></a></td>
    <td>‚≠ê 57</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/892404?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/892404?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/truemagic-coder/solana-agent-app"><strong>truemagic-coder/solana-agent-app</strong></a></td>
    <td>‚≠ê 30</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/8344498?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/8344498?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/schogini/techietalksai"><strong>schogini/techietalksai</strong></a></td>
    <td>‚≠ê 23</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/201161342?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/201161342?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/autometa-dev/whatsapp-mcp-voice-agent"><strong>autometa-dev/whatsapp-mcp-voice-agent</strong></a></td>
    <td>‚≠ê 22</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/100749943?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/100749943?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/Deniscartin/mcp-cli"><strong>Deniscartin/mcp-cli</strong></a></td>
    <td>‚≠ê 18</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/6688805?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/6688805?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/innovaccer/Healthcare-MCP"><strong>innovaccer/Healthcare-MCP</strong></a></td>
    <td>‚≠ê 12</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/6764390?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/6764390?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/elastic/genai-workshops"><strong>elastic/genai-workshops</strong></a></td>
    <td>‚≠ê 10</td>
  </tr>
  <tr>
    <td><a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/68845761?s=40&amp;v=4"><img src="https://avatars.githubusercontent.com/u/68845761?s=40&amp;v=4" width="20" height="20"/></a> <a href="https://github.com/entbappy/MCP-Tutorials"><strong>entbappy/MCP-Tutorials</strong></a></td>
    <td>‚≠ê 6</td>
  </tr>
</tbody></table></markdown-accessiblity-table>


<ul dir="auto">
<li>Python 3.11+</li>
<li>MCP implementation (like Playwright MCP)</li>
<li>LangChain and appropriate model libraries (OpenAI, Anthropic, etc.)</li>
</ul>

<p dir="auto">MIT</p>

<p dir="auto">If you use MCP-Use in your research or project, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@software{mcp_use2025,
  author = {Zullo, Pietro},
  title = {MCP-Use: MCP Library for Python},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/pietrozullo/mcp-use}
}"><pre><span>@software</span>{<span>mcp_use2025</span>,
  <span>author</span> = <span><span>{</span>Zullo, Pietro<span>}</span></span>,
  <span>title</span> = <span><span>{</span>MCP-Use: MCP Library for Python<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2025<span>}</span></span>,
  <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
  <span>url</span> = <span><span>{</span>https://github.com/pietrozullo/mcp-use<span>}</span></span>
}</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/abb77e1c826f427e07d110bf90ea34931f9d41685d4c317198141569b47ddc39/68747470733a2f2f7374617469632e73636172662e73682f612e706e673f782d707869643d37333235383962362d363835302d346238632d616132352d39303663303937396534323626706167653d524541444d452e6d64"><img src="https://camo.githubusercontent.com/abb77e1c826f427e07d110bf90ea34931f9d41685d4c317198141569b47ddc39/68747470733a2f2f7374617469632e73636172662e73682f612e706e673f782d707869643d37333235383962362d363835302d346238632d616132352d39303663303937396534323626706167653d524541444d452e6d64" data-canonical-src="https://static.scarf.sh/a.png?x-pxid=732589b6-6850-4b8c-aa25-906c0979e426&amp;page=README.md"/></a></p>
</article></div></div>
  </body>
</html>
