<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://render.com/blog/knative">Original</a>
    <h1>Scaling Knative to 100K&#43; Webapps</h1>
    
    <div id="readability-page-1" class="page"><p>Render takes your infrastructure problems away and gives you a battle-tested, powerful, and cost-effective cloud with an outstanding developer experience. Focus on building your apps, shipping fast, and delighting your customers, and leave your cloud infrastructure to us.</p><div><div><div><p>In November 2021, Render introduced a <a href="https://render.com/docs/free">free tier</a> for hobbyist developers and teams who want to kick the tires. Adoption grew at a steady, predictable rate—until Heroku announced the end of <em>their</em> free offering ten months later:</p>
<p><span>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/0a7dee25a43efba6f5a5706350024c60/3122e/free-adoption-growth.webp 245w,
/static/0a7dee25a43efba6f5a5706350024c60/eeaca/free-adoption-growth.webp 490w,
/static/0a7dee25a43efba6f5a5706350024c60/285d6/free-adoption-growth.webp 980w,
/static/0a7dee25a43efba6f5a5706350024c60/90fc6/free-adoption-growth.webp 1211w" sizes="(max-width: 980px) 100vw, 980px" type="image/webp"/>
          <source srcset="/static/0a7dee25a43efba6f5a5706350024c60/232f7/free-adoption-growth.png 245w,
/static/0a7dee25a43efba6f5a5706350024c60/9319d/free-adoption-growth.png 490w,
/static/0a7dee25a43efba6f5a5706350024c60/2b72d/free-adoption-growth.png 980w,
/static/0a7dee25a43efba6f5a5706350024c60/44079/free-adoption-growth.png 1211w" sizes="(max-width: 980px) 100vw, 980px" type="image/png"/>
          <img src="https://render.com/static/0a7dee25a43efba6f5a5706350024c60/2b72d/free-adoption-growth.png" alt="Graph of free-tier app creation" title="" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p><sup><i>Comparison of Render free-tier apps <b>created</b> each week, late 2022</i></sup></p>
</span></p><p><strong>Render’s free-tier adoption rate doubled immediately and grew from there</strong> (awesome), causing our infrastructure to creak under the load (less awesome). In the span of a month, we experienced four incidents related to this surge. We knew that if Free usage continued to grow (and it very much has—as of this writing, <strong>tens of thousands</strong> of free-tier apps are created each week), we needed to make it much more scalable. This post describes the first step we took along that path.</p>
<h2 id="how-we-initially-built-free"><a href="#how-we-initially-built-free" aria-label="how we initially built free permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How we initially built Free</h2>
<p>Some background: unlike other services on Render, free-tier web services “scale to zero” (as in, they stop running) if they go 15 minutes without receiving traffic. They start up again whenever they next receive an incoming request. This hibernation behavior helps us provide a no-cost offering without breaking the bank.</p>
<p>However, this desired behavior presented an immediate development challenge. Render uses Kubernetes (K8s) behind the scenes, and K8s didn’t natively support scale-to-zero (<a href="https://github.com/kubernetes/enhancements/pull/2022" target="_blank" rel="nofollow">it still doesn’t, as of September 2023</a>). In looking for a solution that did, we found and settled on <a href="https://knative.dev/docs/" target="_blank" rel="nofollow">Knative</a> (kay-NAY-tiv). Knative extended Kubernetes with serverless support—a natural fit for services that would regularly spin up and down.</p>
<p>In the interest of shipping quickly, we deployed Knative with its default configuration. And, until our growth spurt nearly a year later, those defaults worked without issue.</p>
<h2 id="where-we-hit-a-wall"><a href="#where-we-hit-a-wall" aria-label="where we hit a wall permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Where we hit a wall</h2>
<p>With the free-tier surge, the total number of apps on Render effectively quadrupled. This put significant strain on the networking layer of each of our Kubernetes clusters. To understand the nature of that strain, let’s look at how this layer operates.</p>
<p>Two networking components run on every node in every cluster: <a href="https://github.com/projectcalico/calico" target="_blank" rel="nofollow">Calico</a> and <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/" target="_blank" rel="nofollow">kube-proxy</a>.</p>
<p><strong>Calico</strong> mainly takes care of IP address management, or IPAM: assigning IP addresses to Pods and Services (we’re using capital-S <strong>Service</strong> to refer to a <a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank" rel="nofollow">Kubernetes Service</a>, to distinguish from the services that customers create on Render.). It also enforces <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" target="_blank" rel="nofollow">Network Policies</a> by managing iptables rules on the node.</p>
<p><strong>kube-proxy</strong> configures a different set of routing rules on the node to ensure traffic destined for a Service is load-balanced across all backing Pods.</p>
<p>Both of these components do their jobs by listening for creates, updates, and deletes to all Pods and Services in the cluster. As you can imagine, having <em>more</em> Pods and Services that changed <em>more</em> frequently resulted in <em>more</em> work:</p>
<ul>
<li>
<p><strong>More work meant more CPU consumption.</strong> Remember, both Calico and kube-proxy run on <em>every</em> node. The more CPU these components used, the less we had left to run our customers’ apps.</p>
</li>
<li>
<p><strong>More work meant higher update latency.</strong> As the work queue grew, each networking change took longer to propagate due to increased time spent waiting in the queue. This delay is defined as the <strong>network programming latency</strong>, or NPL (read more about NPL <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/network_programming_latency.md" target="_blank" rel="nofollow">here</a>). When there was high NPL, traffic could be routed using stale rules that led nowhere (the Pod had already been destroyed), causing connections to fail intermittently.</p>
</li>
</ul>
<p>To mitigate these issues, we needed to reduce the overhead each free-tier app added to our networking machinery.</p>
<h2 id="serviceless-knative"><a href="#serviceless-knative" aria-label="serviceless knative permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>“Serviceless” Knative</h2>
<p>As mentioned, we’d deployed out-of-the-box Knative to handle free-tier resource provisioning. We took a closer look at exactly <em>what</em> K8s primitives were being provisioned for each free-tier app:</p>
<ul>
<li>One Pod (for running the application code). Expected.</li>
<li><code>2N + 1</code> Services, where <code>N</code> is the number of times the app was deployed. This is because Knative manages changes with <a href="https://knative.dev/docs/concepts/serving-resources/revisions/" target="_blank" rel="nofollow">Revisions</a>, and retained resources belonging to historical Revisions. <i>Un</i>expected.</li>
</ul>
<p>We figured the Pod needed to stay, but did we really need all those Kubernetes Services? What if we could get away with fewer—or even zero?</p>
<p>We dove deeper into how those resources interacted in a cluster:</p>
<p><span>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/7275f4971a15f8642a7380a8c9b15dfb/3122e/knative-free-before.webp 245w,
/static/7275f4971a15f8642a7380a8c9b15dfb/eeaca/knative-free-before.webp 490w,
/static/7275f4971a15f8642a7380a8c9b15dfb/285d6/knative-free-before.webp 980w,
/static/7275f4971a15f8642a7380a8c9b15dfb/6fd4b/knative-free-before.webp 985w" sizes="(max-width: 980px) 100vw, 980px" type="image/webp"/>
          <source srcset="/static/7275f4971a15f8642a7380a8c9b15dfb/232f7/knative-free-before.png 245w,
/static/7275f4971a15f8642a7380a8c9b15dfb/9319d/knative-free-before.png 490w,
/static/7275f4971a15f8642a7380a8c9b15dfb/2b72d/knative-free-before.png 980w,
/static/7275f4971a15f8642a7380a8c9b15dfb/cdfe7/knative-free-before.png 985w" sizes="(max-width: 980px) 100vw, 980px" type="image/png"/>
          <img src="https://render.com/static/7275f4971a15f8642a7380a8c9b15dfb/2b72d/knative-free-before.png" alt="Knative defaults in Render K8s clusters" title="" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
</span></p><p>And learned what each of the Knative-provisioned Services (in purple above) was for:</p>
<ul>
<li>The <strong>Placeholder Service</strong> was a dummy service that existed to prevent naming collisions among resources for Knative-managed apps. There was one for every free-tier app.</li>
<li>The <strong>Public Service</strong> routed incoming traffic to the app from the <em>public internet</em>.</li>
<li>The <strong>Private Service</strong> routed incoming <em>cluster-local</em> traffic based on whether the app was scaled up.
<ul>
<li><strong>If scaled up</strong>, traffic was routed to the Pod.</li>
<li><strong>If scaled down</strong>, traffic was routed to the cluster’s Knative proxy (called the <a href="https://knative.dev/docs/serving/knative-kubernetes-services/#service-activator" target="_blank" rel="nofollow">activator</a>), which handled scaling up the app by creating a Pod.</li>
</ul>
</li>
</ul>
<p>Armed with this newfound knowledge, we devised a path to remove all of these Services.</p>
<h3 id="step-by-step"><a href="#step-by-step" aria-label="step by step permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step by step</h3>
<p>We started simple with the dummy <strong>Placeholder Service</strong>, which did <em>literally nothing</em>. There was no risk of naming collisions among our Knative-managed resources, so we updated the Knative Route controller to stop creating the Placeholder Service. ❌</p>
<p>Next! While the <strong>Public Service</strong> (for public internet routing) is needed for plenty of Knative use cases out there, in Render-land, all requests from the public Internet must pass through our load-balancing layer. This means requests are guaranteed to be <em>cluster-local</em> by the time they reach Pods, so the <strong>Public Service</strong> <em>also</em> had nothing to do! We patched Knative to stop reconciling it and its related Endpoint resources. ❌</p>
<p>Finally, the <strong>Private Service</strong> (for cluster-local routing). We put together the concepts that Services are used to balance load across backing Pods, and that a free-tier app can have at most only one Pod receiving traffic at a time, making load balancing <em>slightly</em> unnecessary. There were two changes we needed to make:</p>
<ul>
<li>
<p>Streamline traffic to flow exclusively through the activator, as we no longer had a Service to split traffic to when the app is scaled up. With a little experimentation, we discovered that the activator could both wake Pods <em>and</em> reverse-proxy to a woke Pod, even though that behavior wasn’t documented! We just needed to set the right headers.</p>
</li>
<li>
<p>Patch the activator to listen for changes to Pod readiness states, and route directly to Pod IP addresses (thanks, Calico!). By default, the activator listens for changes to EndpointSlices, but those are tied to the Services we were hoping to delete. See the code additions in the diff:</p>
</li>
</ul>
<details>
<summary><strong>+ Click to show diff</strong></summary>
<div data-language="diff"><pre><code>diff --git a/pkg/activator/net/helpers.go b/pkg/activator/net/helpers.go
index 3a8e5ef49..0e7d5c3d0 100644
<span>--- a/pkg/activator/net/helpers.go</span>
<span>+++ b/pkg/activator/net/helpers.go</span>
@@ -92,6 +92,47 @@ func endpointsToDests(endpoints *corev1.Endpoints, portName string) (ready, notR
<span><span> </span>	return ready, notReady
<span> </span>}
</span>
<span><span>+</span>// podsToDests returns the pod&#39;s IP as the ready set if the pod is Ready,
<span>+</span>// and as the notReady set if not.
<span>+</span>func podsToDests(pods []*corev1.Pod) (_ready, _notReady sets.String) {
<span>+</span>	ready := sets.String{}
<span>+</span>	notReady := sets.String{}
<span>+</span>	// 8012 is the port of the queue-proxy container that runs alongside
<span>+</span>	// the user-server container.
<span>+</span>	port := &#34;8012&#34;
<span>+</span>	for _, pod := range pods {
<span>+</span>		if podIsReady(pod) {
<span>+</span>			for _, ip := range pod.Status.PodIPs {
<span>+</span>				ready.Insert(net.JoinHostPort(ip.IP, port))
<span>+</span>			}
<span>+</span>		} else {
<span>+</span>			for _, ip := range pod.Status.PodIPs {
<span>+</span>				notReady.Insert(net.JoinHostPort(ip.IP, port))
<span>+</span>			}
<span>+</span>		}
<span>+</span>
<span>+</span>	}
<span>+</span>	return ready, notReady
<span>+</span>}
<span>+</span>
<span>+</span>func podIsReady(pod *corev1.Pod) bool {
<span>+</span>	// This is the same logic that the K8s endpoints controller uses to list
<span>+</span>	// the Pod&#39;s IP under the Endpoint&#39;s `Addresses` (as opposed to `NotReadyAddresses`).
<span>+</span>	//
<span>+</span>	// https://github.com/kubernetes/kubernetes/blob/8a3f72074e6390f8f14a89a7b78cef4379737216/pkg/controller/endpoint/endpoints_controller.go#L624
<span>+</span>	//
<span>+</span>	// If we &#34;mistime&#34; it by interpreting the pod as ready before it is indeed
<span>+</span>	// ready, then the request that triggers scale-from-zero will always return
<span>+</span>	// some connection error, as the activator will try to reach the user-server
<span>+</span>	// container before it can serve requests.
<span>+</span>	for _, cond := range pod.Status.Conditions {
<span>+</span>		if cond.Type == corev1.PodReady &amp;&amp; cond.Status == corev1.ConditionTrue {
<span>+</span>			return true
<span>+</span>		}
<span>+</span>	}
<span>+</span>	return false
<span>+</span>}
<span>+</span>
</span><span><span> </span>// getServicePort takes a service and a protocol and returns the port number of
<span> </span>// the port named for that protocol. If the port is not found then ok is false.
<span> </span>func getServicePort(protocol networking.ProtocolType, svc *corev1.Service) (int, bool) {
</span>
diff --git a/pkg/activator/net/revision_backends.go b/pkg/activator/net/revision_backends.go
index 3d1524925..94758eae1 100644
<span>--- a/pkg/activator/net/revision_backends.go</span>
<span>+++ b/pkg/activator/net/revision_backends.go</span>
@@ -31,6 +31,8 @@ import (
<span><span> </span>	&#34;go.uber.org/zap&#34;
<span> </span>	&#34;go.uber.org/zap/zapcore&#34;
<span> </span>	&#34;golang.org/x/sync/errgroup&#34;
</span><span><span>+</span>	&#34;k8s.io/apimachinery/pkg/labels&#34;
<span>+</span>	podsinformer &#34;knative.dev/pkg/client/injection/kube/informers/core/v1/pod&#34;
</span>
<span><span> </span>	corev1 &#34;k8s.io/api/core/v1&#34;
<span> </span>	&#34;k8s.io/apimachinery/pkg/types&#34;
</span>@@ -47,6 +49,7 @@ import (
<span><span> </span>	&#34;knative.dev/pkg/logging&#34;
<span> </span>	&#34;knative.dev/pkg/logging/logkey&#34;
<span> </span>	&#34;knative.dev/pkg/reconciler&#34;
</span><span><span>+</span>
</span><span><span> </span>	&#34;knative.dev/serving/pkg/apis/serving&#34;
<span> </span>	revisioninformer &#34;knative.dev/serving/pkg/client/injection/informers/serving/v1/revision&#34;
<span> </span>	servinglisters &#34;knative.dev/serving/pkg/client/listers/serving/v1&#34;
</span>@@ -441,6 +444,7 @@ type revisionBackendsManager struct {
<span><span> </span>	ctx            context.Context
<span> </span>	revisionLister servinglisters.RevisionLister
<span> </span>	serviceLister  corev1listers.ServiceLister
</span><span><span>+</span>	podLister      corev1listers.PodLister
</span>
<span><span> </span>	revisionWatchers    map[types.NamespacedName]*revisionWatcher
<span> </span>	revisionWatchersMux sync.RWMutex
</span>@@ -466,6 +470,7 @@ func newRevisionBackendsManagerWithProbeFrequency(ctx context.Context, tr http.R
<span><span> </span>		ctx:              ctx,
<span> </span>		revisionLister:   revisioninformer.Get(ctx).Lister(),
<span> </span>		serviceLister:    serviceinformer.Get(ctx).Lister(),
</span><span><span>+</span>		podLister:        podsinformer.Get(ctx).Lister(),
</span><span><span> </span>		revisionWatchers: make(map[types.NamespacedName]*revisionWatcher),
<span> </span>		updateCh:         make(chan revisionDestsUpdate),
<span> </span>		transport:        tr,
</span>@@ -478,6 +483,7 @@ func newRevisionBackendsManagerWithProbeFrequency(ctx context.Context, tr http.R
<span><span> </span>	endpointsInformer.Informer().AddEventHandler(cache.FilteringResourceEventHandler{
<span> </span>		FilterFunc: reconciler.ChainFilterFuncs(
<span> </span>			reconciler.LabelExistsFilterFunc(serving.RevisionUID),
</span><span><span>+</span>			reconciler.Not(reconciler.LabelExistsFilterFunc(serving.RenderServicelessKey)),
</span><span><span> </span>			// We are only interested in the private services, since that is
<span> </span>			// what is populated by the actual revision backends.
<span> </span>			reconciler.LabelFilterFunc(networking.ServiceTypeKey, string(networking.ServiceTypePrivate), false),
</span>@@ -488,6 +494,18 @@ func newRevisionBackendsManagerWithProbeFrequency(ctx context.Context, tr http.R
<span><span> </span>			DeleteFunc: rbm.endpointsDeleted,
<span> </span>		},
<span> </span>	})
</span><span><span>+</span>	podsInformer := podsinformer.Get(ctx)
<span>+</span>	podsInformer.Informer().AddEventHandler(cache.FilteringResourceEventHandler{
<span>+</span>		FilterFunc: reconciler.ChainFilterFuncs(
<span>+</span>			reconciler.LabelExistsFilterFunc(serving.RevisionUID),
<span>+</span>			reconciler.LabelExistsFilterFunc(serving.RenderServicelessKey),
<span>+</span>		),
<span>+</span>		Handler: cache.ResourceEventHandlerFuncs{
<span>+</span>			AddFunc:    rbm.podUpdated,
<span>+</span>			UpdateFunc: controller.PassNew(rbm.podUpdated),
<span>+</span>			DeleteFunc: rbm.podDeleted,
<span>+</span>		},
<span>+</span>	})
</span>
<span><span> </span>	go func() {
<span> </span>		// updateCh can only be closed after revisionWatchers are done running
</span>@@ -566,6 +584,40 @@ func (rbm *revisionBackendsManager) endpointsUpdated(newObj interface{}) {
<span><span> </span>	}
<span> </span>}
</span>
<span><span>+</span>// podUpdated is a handler function to be used by the Endpoints informer.
<span>+</span>// It updates the endpoints in the RevisionBackendsManager if the hosts changed
<span>+</span>func (rbm *revisionBackendsManager) podUpdated(newObj interface{}) {
<span>+</span>	// Ignore the updates when we&#39;ve terminated.
<span>+</span>	select {
<span>+</span>	case &lt;-rbm.ctx.Done():
<span>+</span>		return
<span>+</span>	default:
<span>+</span>	}
<span>+</span>	pod := newObj.(*corev1.Pod)
<span>+</span>	revID := types.NamespacedName{Namespace: pod.Namespace, Name: pod.Labels[serving.RevisionLabelKey]}
<span>+</span>
<span>+</span>	rw, err := rbm.getOrCreateRevisionWatcher(revID)
<span>+</span>	if err != nil {
<span>+</span>		rbm.logger.Errorw(&#34;Failed to get revision watcher&#34;, zap.Error(err), zap.String(logkey.Key, revID.String()))
<span>+</span>		return
<span>+</span>	}
<span>+</span>	selector, _ := labels.Parse(fmt.Sprintf(&#34;%s=%s&#34;, serving.RevisionLabelKey, pod.Labels[serving.RevisionLabelKey]))
<span>+</span>	allPods, err := rbm.podLister.Pods(pod.Namespace).List(selector)
<span>+</span>	if err != nil {
<span>+</span>		rbm.logger.Errorw(&#34;Failed to list pods belonging to revision&#34;, zap.Error(err), zap.String(logkey.Key, revID.String()))
<span>+</span>		return
<span>+</span>	}
<span>+</span>	ready, notReady := podsToDests(allPods)
<span>+</span>	select {
<span>+</span>	case &lt;-rbm.ctx.Done():
<span>+</span>		return
<span>+</span>	case rw.destsCh &lt;- dests{ready: ready, notReady: notReady}:
<span>+</span>	}
<span>+</span>}
<span>+</span>
</span><span><span> </span>// deleteRevisionWatcher deletes the revision watcher for rev if it exists. It expects
<span> </span>// a write lock is held on revisionWatchersMux when calling.
<span> </span>func (rbm *revisionBackendsManager) deleteRevisionWatcher(rev types.NamespacedName) {
</span>@@ -590,3 +642,7 @@ func (rbm *revisionBackendsManager) endpointsDeleted(obj interface{}) {
<span><span> </span>	defer rbm.revisionWatchersMux.Unlock()
<span> </span>	rbm.deleteRevisionWatcher(revID)
<span> </span>}
</span><span><span>+</span>
<span>+</span>func (rbm *revisionBackendsManager) podDeleted(obj interface{}) {
<span>+</span>	rbm.podUpdated(obj)
<span>+</span>}
</span>
diff --git a/pkg/apis/serving/register.go b/pkg/apis/serving/register.go
index 61a2e8b4a..c067f962b 100644
<span>--- a/pkg/apis/serving/register.go</span>
<span>+++ b/pkg/apis/serving/register.go</span>
@@ -25,6 +25,8 @@ const (
<span><span> </span>	// GroupNamePrefix is the prefix for label key and annotation key
<span> </span>	GroupNamePrefix = GroupName + &#34;/&#34;
</span>
<span><span>+</span>	RenderServicelessKey = &#34;render.com/knative-serviceless&#34;
<span>+</span>
</span><span><span> </span>	// ConfigurationLabelKey is the label key attached to a Revision indicating by
<span> </span>	// which Configuration it is created.
<span> </span>	ConfigurationLabelKey = GroupName + &#34;/configuration&#34;
</span></code></pre></div>
</details>
<p>And just like that, the <strong>Private Service</strong> was no more. ❌</p>
<p><span data-block="help">
<p>Want to go deeper under the hood? Check out an abridged version of <a href="https://render.com/blog/knative-design-doc">the design doc</a> for removing the <strong>Private Service</strong>.</p>
</span></p><p>At the end of this entire optimization pass, the networking architecture for a free-tier app had been simplified to the following:</p>
<p><span>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/03fcde75139f0b9aa07c4fe09e551fa9/3122e/knative-free-after.webp 245w,
/static/03fcde75139f0b9aa07c4fe09e551fa9/eeaca/knative-free-after.webp 490w,
/static/03fcde75139f0b9aa07c4fe09e551fa9/a3fe1/knative-free-after.webp 656w" sizes="(max-width: 656px) 100vw, 656px" type="image/webp"/>
          <source srcset="/static/03fcde75139f0b9aa07c4fe09e551fa9/232f7/knative-free-after.png 245w,
/static/03fcde75139f0b9aa07c4fe09e551fa9/9319d/knative-free-after.png 490w,
/static/03fcde75139f0b9aa07c4fe09e551fa9/abbf1/knative-free-after.png 656w" sizes="(max-width: 656px) 100vw, 656px" type="image/png"/>
          <img src="https://render.com/static/03fcde75139f0b9aa07c4fe09e551fa9/abbf1/knative-free-after.png" alt="Free-tier architecture after Knative Service removal" title="" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
</span></p><p><em>Zero</em> Kubernetes Services per free-tier app! Predictably, K8s Service counts plummeted across our clusters:</p>
<p><span>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/078632874886cd166c047c04eff4ac84/3122e/service-metrics-change.webp 245w,
/static/078632874886cd166c047c04eff4ac84/eeaca/service-metrics-change.webp 490w,
/static/078632874886cd166c047c04eff4ac84/285d6/service-metrics-change.webp 980w,
/static/078632874886cd166c047c04eff4ac84/0c953/service-metrics-change.webp 1253w" sizes="(max-width: 980px) 100vw, 980px" type="image/webp"/>
          <source srcset="/static/078632874886cd166c047c04eff4ac84/232f7/service-metrics-change.png 245w,
/static/078632874886cd166c047c04eff4ac84/9319d/service-metrics-change.png 490w,
/static/078632874886cd166c047c04eff4ac84/2b72d/service-metrics-change.png 980w,
/static/078632874886cd166c047c04eff4ac84/c40ef/service-metrics-change.png 1253w" sizes="(max-width: 980px) 100vw, 980px" type="image/png"/>
          <img src="https://render.com/static/078632874886cd166c047c04eff4ac84/2b72d/service-metrics-change.png" alt="Chart of Service count by cluster over time" title="" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p><sup><i>Change in K8s Service count in each Render cluster, November 2022</i></sup></p>
</span></p><p>With these improvements, Calico and kube-proxy’s combined usage fell by <strong>hundreds of CPU seconds</strong> in our largest cluster.</p>
<p><span>
<p><span>
      <span></span>
  <picture>
          <source srcset="/static/3f257270780f098f95e4a7d3410d2dda/3122e/cpu-usage-change.webp 245w,
/static/3f257270780f098f95e4a7d3410d2dda/eeaca/cpu-usage-change.webp 490w,
/static/3f257270780f098f95e4a7d3410d2dda/b6870/cpu-usage-change.webp 570w" sizes="(max-width: 570px) 100vw, 570px" type="image/webp"/>
          <source srcset="/static/3f257270780f098f95e4a7d3410d2dda/232f7/cpu-usage-change.png 245w,
/static/3f257270780f098f95e4a7d3410d2dda/9319d/cpu-usage-change.png 490w,
/static/3f257270780f098f95e4a7d3410d2dda/2cee3/cpu-usage-change.png 570w" sizes="(max-width: 570px) 100vw, 570px" type="image/png"/>
          <img src="https://render.com/static/3f257270780f098f95e4a7d3410d2dda/2cee3/cpu-usage-change.png" alt="Chart of CPU usage over time" title="" loading="lazy" decoding="async"/>
        </picture>
    </span></p>
<p><sup><i>CPU usage for Calico and kube-proxy in one Render cluster, November 2022</i></sup></p>
</span></p><p>With compute resources freed up, free-tier network latency and stability improved dramatically. But even so, we knew we had more work to do.</p>
<h2 id="a-moving-target"><a href="#a-moving-target" aria-label="a moving target permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A moving target</h2>
<p>Our Knative tweaks bought us some much-needed breathing room, but ultimately, free-tier usage began to put a strain even on this optimized architecture. The time was quickly approaching for us to rip out Knative entirely, in favor of a home-grown solution that was tailor-made for Render’s needs.</p>
<p>But that’s a story for another post!</p>
<p>—</p>

<ul>
<li><a href="https://render.com/blog/knative-design-doc">Design doc for removing the <strong>Private Service</strong></a> (abridged)</li>
<li><a href="https://knative.dev/docs/concepts/" target="_blank" rel="nofollow">Knative documentation</a></li>
<li><a href="https://render.com/careers">Careers at Render</a> 😉</li>
</ul></div></div></div></div>
  </body>
</html>
