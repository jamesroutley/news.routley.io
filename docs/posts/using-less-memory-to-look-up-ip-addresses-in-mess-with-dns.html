<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jvns.ca/blog/2024/10/27/asn-ip-address-memory/">Original</a>
    <h1>Using less memory to look up IP addresses in Mess With DNS</h1>
    
    <p>I&rsquo;ve been having problems for the last 3 years or so where <a href="https://messwithdns.net/">Mess With DNS</a>
periodically runs out of memory and gets OOM killed.</p>
<p>This hasn&rsquo;t been a big priority for me: usually it just goes down for a few
minutes while it restarts, and it only happens once a day at most, so I&rsquo;ve just
been ignoring. But last week it started actually causing a problem so I decided
to look into it.</p>
<p>This was kind of winding road where I learned a lot so here&rsquo;s a table of contents:</p>
<ul>
<li><a href="#there-s-about-100mb-of-memory-available">there&rsquo;s about 100MB of memory available</a></li>
<li><a href="#the-problem-oom-killing-the-backup-script">the problem: OOM killing the backup script</a></li>
<li><a href="#attempt-1-use-sqlite">attempt 1: use SQLite</a>
<ul>
<li><a href="#problem-how-to-store-ipv6-addresses">problem: how to store IPv6 addresses</a></li>
<li><a href="#problem-it-s-500x-slower">problem: it&rsquo;s 500x slower</a></li>
<li><a href="#time-for-explain-query-plan">time for EXPLAIN QUERY PLAN</a></li>
</ul>
</li>
<li><a href="#attempt-2-use-a-trie">attempt 2: use a trie</a>
<ul>
<li><a href="#some-notes-on-memory-profiling">some notes on memory profiling</a></li>
</ul>
</li>
<li><a href="#attempt-3-make-my-array-use-less-memory">attempt 3: make my array use less memory</a>
<ul>
<li><a href="#idea-3-1-deduplicate-the-name-and-country">idea 3.1: deduplicate the Name and Country</a></li>
<li><a href="#how-big-are-asns">how big are ASNs?</a></li>
<li><a href="#idea-3-2-use-netip-addr-instead-of-net-ip">idea 3.2: use netip.Addr instead of net.IP</a></li>
<li><a href="#the-result-saved-70mb-of-memory">the result: saved 70MB of memory!</a></li>
</ul>
</li>
</ul>
<h3 id="there-s-about-100mb-of-memory-available">there&rsquo;s about 100MB of memory available</h3>
<p>I run Mess With DNS on a VM without about 465MB of RAM, which according to
<code>ps aux</code> (the <code>RSS</code> column) is split up something like:</p>
<ul>
<li>100MB for PowerDNS</li>
<li>200MB for Mess With DNS</li>
<li>40MB for <a href="https://fly.io/blog/ssh-and-user-mode-ip-wireguard/">hallpass</a></li>
</ul>
<p>That leaves about 110MB of memory free.</p>
<p>A while back I set <a href="https://tip.golang.org/doc/gc-guide">GOMEMLIMIT</a> to 250MB
to try to make sure the garbage collector ran if Mess With DNS used more than
250MB of memory, and I think this helped but it didn&rsquo;t solve everything.</p>
<h3 id="the-problem-oom-killing-the-backup-script">the problem: OOM killing the backup script</h3>
<p>A few weeks ago I started backing up Mess With DNS&rsquo;s database for the first time <a href="https://jvns.ca/til/restic-for-backing-up-sqlite-dbs/">using restic</a>.</p>
<p>This has been working okay, but since Mess With DNS operates without much extra
memory I think <code>restic</code> sometimes needed more memory than was available on the
system, and so the backup script sometimes got OOM killed.</p>
<p>This was a problem because</p>
<ol>
<li>backups might be corrupted sometimes</li>
<li>more importantly, restic takes out a lock when it runs, and so I&rsquo;d have to manually do an
unlock if I wanted the backups to continue working. Doing manual work like
this is the #1 thing I try to avoid with all my web services (who has time
for that!) so I really wanted to do something about it.</li>
</ol>
<p>There&rsquo;s probably more than one solution to this, but I decided to try to make
Mess With DNS use less memory so that there was more available memory on the
system, mostly because it seemed like a fun problem to try to solve.</p>
<h3 id="what-s-using-memory-ip-addresses">what&rsquo;s using memory: IP addresses</h3>
<p>I&rsquo;d run a memory profile of Mess With DNS a bunch of times in the past, so I
knew exactly what was using most of Mess With DNS&rsquo;s memory: IP addresses.</p>
<p>When it starts, Mess With DNS loads this <a href="https://iptoasn.com/">database where you can look up the
ASN of every IP address</a> into memory, so that when it
receives a DNS query it can take the source IP address like <code>74.125.16.248</code> and
tell you that IP address belongs to <code>GOOGLE</code>.</p>
<p>This database by itself used about 117MB of memory, and a simple <code>du</code> told me
that was too much &ndash; the original text files were only 37MB!</p>
<pre><code>$ du -sh *.tsv
26M	ip2asn-v4.tsv
11M	ip2asn-v6.tsv
</code></pre>
<p>The way it worked originally is that I had an array of these:</p>
<pre><code>type IPRange struct {
	StartIP net.IP
	EndIP   net.IP
	Num     int
	Name    string
	Country string
}
</code></pre>
<p>and I searched through it with a binary search to figure out if any of the
ranges contained the IP I was looking for. Basically the simplest possible
thing and it&rsquo;s super fast, my machine can do about 9 million lookups per
second.</p>
<h3 id="attempt-1-use-sqlite">attempt 1: use SQLite</h3>
<p>I&rsquo;ve been using SQLite recently, so my first thought was &ndash; maybe I can store
all of this data on disk in an SQLite database, give the tables an index, and
that&rsquo;ll use less memory.</p>
<p>So I:</p>
<ul>
<li>wrote a quick Python script using <a href="https://sqlite-utils.datasette.io/en/stable/">sqlite-utils</a> to import the TSV files into an SQLite database</li>
<li>adjusted my code to select from the database instead</li>
</ul>
<p>This did solve the initial memory goal (after a GC it now hardly used any
memory at all because the table was on disk!), though I&rsquo;m not sure how much GC
churn this solution would cause if we needed to do a lot of queries at once. I
did a quick memory profile and it seemed to allocate about 1KB of memory per
lookup.</p>
<p>Let&rsquo;s talk about the issues I ran into with using SQLite though.</p>
<h3 id="problem-how-to-store-ipv6-addresses">problem: how to store IPv6 addresses</h3>
<p>SQLite doesn&rsquo;t have support for big integers and IPv6 addresses are 128 bits,
so I decided to store them as text. I think <code>BLOB</code> might have been better, I
originally thought <code>BLOB</code>s couldn&rsquo;t be compared but the <a href="https://www.sqlite.org/datatype3.html#sort_order">sqlite docs</a> say they can.</p>
<p>I ended up with this schema:</p>
<pre><code>CREATE TABLE ipv4_ranges (
   start_ip INTEGER NOT NULL,
   end_ip INTEGER NOT NULL,
   asn INTEGER NOT NULL,
   country TEXT NOT NULL,
   name TEXT NOT NULL
);
CREATE TABLE ipv6_ranges (
   start_ip TEXT NOT NULL,
   end_ip TEXT NOT NULL,
   asn INTEGER,
   country TEXT,
   name TEXT
);
CREATE INDEX idx_ipv4_ranges_start_ip ON ipv4_ranges (start_ip);
CREATE INDEX idx_ipv6_ranges_start_ip ON ipv6_ranges (start_ip);
CREATE INDEX idx_ipv4_ranges_end_ip ON ipv4_ranges (end_ip);
CREATE INDEX idx_ipv6_ranges_end_ip ON ipv6_ranges (end_ip);
</code></pre>
<p>Also I learned that Python has an <code>ipaddress</code> module, so I could use
<code>ipaddress.ip_address(s).exploded</code> to make sure that the IPv6 addresses were
expanded so that a string comparison would compare them properly.</p>
<h3 id="problem-it-s-500x-slower">problem: it&rsquo;s 500x slower</h3>
<p>I ran a quick microbenchmark, something like this. It printed out that it could
look up 17,000 IPv6 addresses per second, and similarly for IPv4 addresses.</p>
<p>This was pretty discouraging &ndash; being able to look up 17k addresses per section
is kind of fine (Mess With DNS does not get a lot of traffic), but I compared it to
the original binary search code and the original code could do 9 million per second.</p>
<pre><code>	ips := []net.IP{}
	count := 20000
	for i := 0; i &lt; count; i++ {
		// create a random IPv6 address
		bytes := randomBytes()
		ip := net.IP(bytes[:])
		ips = append(ips, ip)
	}
	now := time.Now()
	success := 0
	for _, ip := range ips {
		_, err := ranges.FindASN(ip)
		if err == nil {
			success++
		}
	}
	fmt.Println(success)
	elapsed := time.Since(now)
	fmt.Println(&quot;number per second&quot;, float64(count)/elapsed.Seconds())
</code></pre>
<h3 id="time-for-explain-query-plan">time for EXPLAIN QUERY PLAN</h3>
<p>I&rsquo;d never really done an EXPLAIN in sqlite, so I thought it would be a fun
opportunity to see what the query plan was doing.</p>
<pre><code>sqlite&gt; explain query plan select * from ipv6_ranges where '2607:f8b0:4006:0824:0000:0000:0000:200e' BETWEEN start_ip and end_ip;
QUERY PLAN
`--SEARCH ipv6_ranges USING INDEX idx_ipv6_ranges_end_ip (end_ip&gt;?)
</code></pre>
<p>It looks like it&rsquo;s just using the <code>end_ip</code> index and not the <code>start_ip</code> index,
so maybe it makes sense that it&rsquo;s slower than the binary search.</p>
<p>I tried to figure out if there was a way to make SQLite use both indexes, but I
couldn&rsquo;t find one and maybe it knows best anyway.</p>
<p>At this point I gave up on the SQLite solution, I didn&rsquo;t love that it was
slower and also it&rsquo;s a lot more complex than just doing a binary search. I felt
like I&rsquo;d rather keep something much more similar to the binary search.</p>
<h3 id="attempt-2-use-a-trie">attempt 2: use a trie</h3>
<p>My next idea was to use a
<a href="https://medium.com/basecs/trying-to-understand-tries-3ec6bede0014">trie</a>,
because I had some vague idea that maybe a trie would use less memory, and
I found this library called
<a href="https://github.com/seancfoley/ipaddress-go">ipaddress-go</a> that lets you look up IP addresses using a trie.</p>
<p>I tried using it <a href="https://gist.github.com/jvns/3ce617796b22127017590ac62c57fddd">here&rsquo;s the code</a>, but I
think I was doing something wildly wrong because, compared to my naive array + binary search:</p>
<ul>
<li>it used WAY more memory (800MB to store just the IPv4 addresses)</li>
<li>it was a lot slower to do the lookups (it could do only 100K/second instead of 9 million/second)</li>
</ul>
<p>I&rsquo;m not really sure what went wrong here but I gave up on this approach and
decided to just try to make my array use less memory and stick to a simple
binary search.</p>
<h3 id="some-notes-on-memory-profiling">some notes on memory profiling</h3>
<p>One thing I learned about memory profiling is that you can use <code>runtime</code>
package to see how much memory is currently allocated in the program. That&rsquo;s
how I got all the memory numbers in this post. Here&rsquo;s the code:</p>
<pre><code>func memusage() {
	runtime.GC()
	var m runtime.MemStats
	runtime.ReadMemStats(&amp;m)
	fmt.Printf(&quot;Alloc = %v MiB\n&quot;, m.Alloc/1024/1024)
	// write mem.prof
	f, err := os.Create(&quot;mem.prof&quot;)
	if err != nil {
		log.Fatal(err)
	}
	pprof.WriteHeapProfile(f)
	f.Close()
}
</code></pre>
<p>Also I learned that if you use <code>pprof</code> to analyze a heap profile there are two
ways to analyze it: you can pass either <code>--alloc-space</code> or <code>--inuse-space</code> to
<code>go tool pprof</code>. I don&rsquo;t know how I didn&rsquo;t realize this before but
<code>alloc-space</code> will tell you about everything that was allocated, and
<code>inuse-space</code> will just include memory that&rsquo;s currently in use.</p>
<p>Anyway I ran <code>go tool pprof -pdf --inuse_space mem.prof &gt; mem.pdf</code> a lot. Also
every time I use pprof I find myself referring to <a href="https://jvns.ca/blog/2017/09/24/profiling-go-with-pprof/">my own intro to pprof</a>, it&rsquo;s probably
the blog post I wrote that I use the most often. I should add <code>--alloc-space</code>
and <code>--inuse-space</code> to it.</p>
<h3 id="attempt-3-make-my-array-use-less-memory">attempt 3: make my array use less memory</h3>
<p>I was storing my ip2asn entries like this:</p>
<pre><code>type IPRange struct {
	StartIP net.IP
	EndIP   net.IP
	Num     int
	Name    string
	Country string
}
</code></pre>
<p>I had 3 ideas for ways to improve this:</p>
<ol>
<li>There was a lot of repetition of <code>Name</code> and the <code>Country</code>, because a lot of IP ranges belong to the same ASN</li>
<li><code>net.IP</code> is an <code>[]byte</code> under the hood, which felt like it involved an unnecessary pointer, was there a way to inline it into the struct?</li>
<li>Maybe I didn&rsquo;t need both the start IP and the end IP, often the ranges were consecutive so maybe I could rearrange things so that I only had the start IP</li>
</ol>
<h3 id="idea-3-1-deduplicate-the-name-and-country">idea 3.1: deduplicate the Name and Country</h3>
<p>I figured I could store the ASN info in an array, and then just store the index
into the array in my <code>IPRange</code> struct. Here are the structs so you can see what
I mean:</p>
<pre><code>type IPRange struct {
	StartIP netip.Addr
	EndIP   netip.Addr
	ASN     uint32
	Idx     uint32
}

type ASNInfo struct {
	Country string
	Name    string
}

type ASNPool struct {
	asns   []ASNInfo
	lookup map[ASNInfo]uint32
}
</code></pre>
<p>This worked! It brought memory usage from 117MB to 65MB &ndash; a 50MB savings. I felt good about this.</p>
<p><a href="https://github.com/jvns/mess-with-dns/blob/94f77b4bb1597b5e2a6768e33bd6c285919aa1bf/api/streamer/ip2asn/ip2asn.go#L18-L54">Here&rsquo;s all of the code for that part</a>.</p>
<h3 id="how-big-are-asns">how big are ASNs?</h3>
<p>As an aside &ndash; I&rsquo;m storing the ASN in a <code>uint32</code>, is that right? I looked in the ip2asn
file and the biggest one seems to be 401307, though there are a few lines that
say <code>4294901931</code> which is much bigger and would not fit in a uint32. I decided
to just ignore the gigantic ASNs and assume I could fit everything in a uint32.</p>
<p>Here&rsquo;s one of the lines with a gigantic ASN from the file:</p>
<pre><code>59.101.179.0	59.101.179.255	4294901931	Unknown	AS4294901931
</code></pre>
<h3 id="idea-3-2-use-netip-addr-instead-of-net-ip">idea 3.2: use <code>netip.Addr</code> instead of <code>net.IP</code></h3>
<p>It turns out that I&rsquo;m not the only one who felt that <code>net.IP</code> was using an
unnecessary amount of memory &ndash; in 2021 the folks at Tailscale released a new
IP address library for Go which solves this and many other issues. <a href="https://tailscale.com/blog/netaddr-new-ip-type-for-go">They wrote a great blog post about it</a>.</p>
<p>I discovered (to my delight) that not only does this new IP address library exist and do exactly what I want, it&rsquo;s also now in the Go
standard library as <a href="https://pkg.go.dev/net/netip#Addr">netip.Addr</a>. Switching to <code>netip.Addr</code> was
very easy and saved another 20MB of memory, bringing us to 46MB.</p>
<p>I didn&rsquo;t try my third idea (remove the end IP from the struct) because I&rsquo;d
already been programming for long enough on a Saturday morning and I was happy
with my progress.</p>
<p>It&rsquo;s always such a great feeling when I think &ldquo;hey, I don&rsquo;t like this, there
must be a better way&rdquo; and then immediately discover that someone has already
made the exact thing I want, thought about it a lot more than me, and
implemented it much better than I would have.</p>
<h3 id="all-of-this-was-messier-in-real-life">all of this was messier in real life</h3>
<p>Even though I tried to explain this in a simple linear way &ldquo;I tried X, then I
tried Y, then I tried Z&rdquo;, that&rsquo;s kind of a lie &ndash; I always try to take my
actual debugging process (total chaos) and make it seem more linear and
understandable because the reality is just too annoying to write down. It&rsquo;s
more like:</p>
<ul>
<li>try sqlite</li>
<li>try a trie</li>
<li>second guess everything that I concluded about sqlite, go back and look at
the results again</li>
<li>wait what about indexes</li>
<li>very very belatedly realize that I can use <code>runtime</code> to check how much
memory everything is using, start doing that</li>
<li>look at the trie again, maybe I misunderstood everything</li>
<li>give up and go back to binary search</li>
<li>look at all of the numbers for tries/sqlite again to make sure I didn&rsquo;t misunderstand</li>
</ul>
<h3 id="the-result-saved-70mb-of-memory">the result: saved 70MB of memory!</h3>
<p>I deployed the new version and now Mess With DNS is using less memory! Hooray!</p>
<p>A few other notes:</p>
<ul>
<li>lookups are a little slower &ndash; in my microbenchmark they went from 9 million
lookups/second to 6 million, maybe because I added a little indirection.
Using less memory and a little more CPU seemed like a good tradeoff though.</li>
<li>it&rsquo;s still using more memory than the raw text files do (46MB vs 37MB), I
guess pointers take up space and that&rsquo;s okay.</li>
</ul>
<p>I&rsquo;m honestly not sure if this will solve all my memory problems, probably not!
But I had fun, I learned a few things about SQLite, I still don&rsquo;t know what to
think about tries, and it made me love binary search even more than I already
did.</p>

  </body>
</html>
