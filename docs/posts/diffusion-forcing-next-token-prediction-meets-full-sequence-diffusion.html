<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://boyuan.space/diffusion-forcing/">Original</a>
    <h1>Diffusion Forcing: Next-Token Prediction Meets Full-Sequence Diffusion</h1>
    
    <div id="readability-page-1" class="page">
    <section>
      <div>
        <div>
          <div>
            <div>
              
              
              <p><sup>*</sup> Work done while being a visiting student at MIT.
              </p>

              </div>
          </div>
        </div>
      </div>
    </section>

    <section>
      <div>
        <!-- Abstract. -->
        <div>
          <div>
            <h2>Abstract</h2>
            <p>
              This paper presents Diffusion Forcing, a new training paradigm
              where a diffusion model is trained to denoise a set of tokens with
              independent per-token noise levels. We apply Diffusion Forcing to
              sequence generative modeling by training a causal next-token
              prediction model to generate one or several future tokens without
              fully diffusing past ones. Our approach is shown to combine the
              strengths of next-token prediction models, such as variable-length
              generation, with the strengths of full-sequence diffusion models,
              such as the ability to guide sampling to desirable trajectories.
              Our method offers a range of additional capabilities, such as (1)
              rolling-out sequences of continuous tokens, such as video, with
              lengths past the training horizon, where baselines diverge and (2)
              new sampling and guiding schemes that uniquely profit from
              Diffusion Forcing&#39;s variable-horizon and causal architecture, and
              which lead to marked performance gains in decision-making and
              planning tasks. In addition to its empirical success, our method
              is proven to optimize a variational lower bound on the likelihoods
              of all subsequences of tokens drawn from the true joint
              distribution.
              </p>
          </div>
        </div>
      </div>
    </section>

    

    <section>
      <div>
        <div>
          <div>
            <h2>Diffusion Forcing</h2>
            <div>
              <p>
                The name &#34;<strong>Diffusion Forcing</strong>&#34; comes from
                &#34;teacher forcing&#34; and &#34;diffusion models&#34;.
              </p>
              <p>
                Diffusion Forcing enjoys key strengths of both next-token
                autoregressive models and full-sequence diffusion models. By
                training Diffusion Forcing once, one can flexibly control its
                behavior at sampling time to simultaneously perform flexible and
                compositional geneation like next-token models, and perform
                sequence level guidance like full-sequence diffusion models.
              </p>
            </div>
            </div>
        </div>
      </div>
    </section>

    

    <section>
      <div>
        <div>
          <div>
            <h2>Video Prediction</h2>
            <p>
                We provide a list of synthesized videos directly generated by
                models (without VAE / superresolution). The below results are
                sampled without cherry-picking.
              </p>

            
            <p>
                Video Prediction by Diffusion Forcing (ours) and baselines in
                DMLab dataset (0.25x speed). Teacher forcing easily blows up
                while causal full-sequence diffusion models suffer from serious
                consistency issues. Diffusion Forcing can achieve stable and and
                consistent video prediction. PNG visualizations are provided
                below to reflect the original quality of generated samples.
              </p>

            <div>
              <p><img src="https://boyuan.space/diffusion-forcing/static/images/dmlab_df_0.png" height="auto" width="100%"/></p><hr/>

              <p><img src="https://boyuan.space/diffusion-forcing/static/images/dmlab_df_1.png" height="auto" width="100%"/></p><hr/>
            </div>

            
            <p>
                Video Prediction by Diffusion Forcing (ours) and baselines in
                Minecraft dataset (0.5x speed). Teacher forcing easily blows up
                while causal full-sequence diffusion models suffer from serious
                consistency issues. Diffusion Forcing can achieve stable and and
                consistent video prediction. PNG visualizations are provided
                below to reflect the original quality of generated samples.
              </p>
            <div>
              <p><img src="https://boyuan.space/diffusion-forcing/static/images/minecraft_df_0.png" height="auto" width="100%"/></p><hr/>

              <p><img src="https://boyuan.space/diffusion-forcing/static/images/minecraft_df_1.png" height="auto" width="100%"/>
            </p></div>
          </div>
        </div>
      </div>
    </section>

    

    <section>
      <div>
        <div>
          <div>
            <h2>
              Stablizing Infinite Rollout without Sliding Window
            </h2>
            <p>
                In addition, one can rollout much longer videos with our method
                than the maximum sequence length it&#39;s trained on. Remarkly, we
                can do this without Sliding Window. That is, we rollout RNN
                without ever resetting the latent z to initial latent z0,
                showing stablization effect of Diffusion Forcing thanks to its
                stablization effect. Videos are compressed for loading speed.
                The results are sampled without cherry-picking.
              </p>
            <video id="dmlab_long" autoplay="" muted="" loop="" playsinline="" width="100%">
              <source src="static/videos/video_prediction/dmlab_long_compressed.mp4" type="video/mp4"/>
            </video>

            <div>
              <p>
                <b>Quality of the video is decreased due to mp4 compression of
                  long videos! We provide PNG visualizations below to reflect
                  original quality of generated samples longer than training
                  horizon.</b>
              </p>
              <p>
                Diffusion Forcing (ours) trained on 36 frames can rollout for
                2000 frames or more on DMLab dataset, without sliding window
                thanks to its stablization effect. Videos are compressed for
                loading speed. Original dataset resolution is 64x64.
              </p>
            </div>
            <p><img src="https://boyuan.space/diffusion-forcing/static/images/df_dmlab_long_0.png" height="auto" width="100%"/>
            </p>

            <video id="minecraft_long" autoplay="" muted="" loop="" playsinline="" width="100%">
              <source src="static/videos/video_prediction/minecraft_long_compressed.mp4" type="video/mp4"/>
            </video>

            <div>
              <p>
                <b>Quality of the video is decreased due to mp4 compression of
                  long videos! We provide PNG visualizations below to reflect
                  original quality of generated samples longer than training
                  horizon.
                </b>
              </p>
              <p>
                Diffusion Forcing (ours) trained on 72 frames rolloutss for 2000
                frames or more on Minecraft dataset without blowing up, without
                sliding window. Original dataset resolution is 128x128. In
                certain scenarios, the agent will get stuck in front of two
                block high dirt or stone blocks until it switches direction,
                which is an instrinsics issue of the dataset collection.
              </p>
            </div>

            <p><img src="https://boyuan.space/diffusion-forcing/static/images/df_minecraft_long_0.png" height="auto" width="100%"/>
            </p>
          </div>
        </div>
      </div>
    </section>

    <hr/>

    <section>
      <div>
        <div>
          <div>
            <h2>Diffusion Planning</h2>
            <p>
                Similar to prior works like Diffuser, we can use test-time
                guidance to make our diffusion sequence a planner. However, we
                explictly model the causal relationship by defining each token
                as [a_t, o_{t+1}]. By doing so, we have a belief over action to
                take and the observation it&#39;s leading to, but can also update
                this belief to posterior estimation when new observation is made
                after the action is taken.
              </p>
            <video id="planning" autoplay="" controls="" muted="" loop="" playsinline="" width="100%">
              <source src="static/videos/planning/planning.mp4" type="video/mp4"/>
            </video>
            <p>
                Visualization of the diffusion planning process of Diffusion
                Forcing as a decision-making framework. To model the causal
                uncertainty of future, diffusion forcing&#39;s plan can have near
                future at lower noise level while having far future at higher
                noise level.
              </p>
          </div>
        </div>
      </div>
    </section>

    <section>
      <div>
        <div>
          <div>
            <h2>
              Long Horizon Imitation Learning
            </h2>

            <p>
                Many real world tasks are not markovian and requires long
                horizon memory to accomplish. In our real robot task, a robot
                arm is asked to swap the slots of two fruits using a third slot.
                Since the fruits are input in random slots at the beginning, one
                cannot determine the next steps from a single observation
                without knowledge of the initial placement of the fruits.
              </p>
            <p><img src="https://boyuan.space/diffusion-forcing/static/images/robot.png" height="auto" width="100%"/>
            </p>
            <p>
                We simply remove guidance from the planning experiments and
                jointly diffuses action-observation sequences to perform
                feedback control.
              </p>
            
            <p>
                The above video shows multiple continuous successes before a
                failure happens. One can observe that the robot is able to
                accomplish the task even when the fruit location is randomized
                by the previous run. On the other hand, we tried SOTA imitation
                learning techniques Diffusion Forcing but it cannot perform the
                task due to non-markovianess.
              </p>
            
            <p>
                In addition, diffusion forcing can be prompted to treat incoming
                observation as noisy ones to be robust to unseen distractions at
                test time. In the video above, we illustrate our distraction
                method of randomly throwing a shopping bag into the field of
                view.
              </p>
          </div>
        </div>
      </div>
    </section>

    

    <section id="BibTeX">
      <div>
        <h2>BibTeX</h2>
        <pre><code>
@misc{chen2024diffusionforcingnexttokenprediction,
      title={Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion}, 
      author={Boyuan Chen and Diego Marti Monso and Yilun Du and Max Simchowitz and Russ Tedrake and Vincent Sitzmann},
      year={2024},
      eprint={2407.01392},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.01392}, 
}
        </code></pre>
      </div>
    </section>
  

</div>
  </body>
</html>
