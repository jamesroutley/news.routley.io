<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/SubscriberLink/953144/b85b695d0c760692/">Original</a>
    <h1>A Nouveau graphics driver update</h1>
    
    <div id="readability-page-1" class="page"><div>
<!-- $Id: slink-none,v 1.2 2005-11-04 22:11:18 corbet Exp $ -->
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>
<p>
Support for NVIDIA graphics processors has traditionally been a sore point
for Linux users; NVIDIA has not felt the need to cooperate with the kernel
community or make free drivers available, and the reverse-engineered
Nouveau driver has often struggled to keep up with product releases.  There
have, however, been signs of improvement in recent years.  At the <a href="https://lpc.events/event/17/page/198-lpc-2023-overview">2023 Linux
Plumbers Conference</a>, graphics subsystem maintainer Dave Airlie provided
an update on the state of support for NVIDIA GPUs and what remains to be
done. 
</p><p>
The kernel community&#39;s relationship with NVIDIA &#34;has gone up and down&#34; over
the years, Airlie began.  Recently, though, the company has rearchitected
its products, adding a large RISC-V processor (the GPU system processor, or
GSP) and moving much of the functionality once handled by drivers into the
GSP firmware.  The company allows that firmware to be used by Linux and shipped
by distributors.  This arrangement brings a number of advantages; for
example, it is now possible for the kernel to do reclocking of NVIDIA GPUs,
running them at full speed just like the proprietary drivers can.
It is, he said, a big improvement over the Nouveau-only firmware that was
provided previously.
</p><p>

<a href="https://lwn.net/Articles/953147/"><img src="https://static.lwn.net/images/conf/2023/lpc/DaveAirlie-sm.png" alt="[Dave Airlie]" title="Dave Airlie"/></a>

There are a number of disadvantages too, though.  The firmware provides no
stable ABI, and a lot of the calls it provides are not documented.  The
firmware files themselves are large, in the range of 20-30MB, and two of
them are required for any given device.  That significantly bloats a
system&#39;s <tt>/boot</tt> directory and initramfs image (which must provide
every version of the firmware that the kernel might need), and forces the
Nouveau developers to be strict and careful about picking up firmware
updates.
</p><p>
Nouveau work has taken a bit of a setback since longtime developer Ben
Skeggs left the project, but he did manage to do a lot of refactoring
before he went.  Nouveau now has initial GSP support for one firmware
version; that code was merged in for the 6.7-rc1 release.  It is only
enabled for the <a href="https://www.nvidia.com/en-us/geforce/ada-lovelace-architecture/">Ada</a>
series of GPUs by default; with a command-line argument it can be made to
work with <a href="https://www.nvidia.com/en-us/geforce/turing/">Turing</a>
and <a href="https://www.nvidia.com/en-us/data-center/ampere-architecture/">Ampere</a>
devices as well.  It is missing some features, including fault handling
(which &#34;shouldn&#39;t be too hard&#34; to add) and sensor monitoring, which doesn&#39;t
work at all.
</p><p>
NVIDIA&#39;s firmware, Airlie said, comes with a set of include files that, in
turn, define structures that change over time.  To deal with these changes,
the driver is going to need some sort of automated ABI generation; he noted
that the developers working on the Apple M1 GPU driver have run into the
same problem.  This problem could be made easier to tackle, he suggested,
if the driver were, like the M1 driver, to be rewritten in Rust.
</p><h4>Next steps</h4>
<p>
Supporting the GSP firmware is just the beginning, though; at this point,
Airlie took a step back and talked about the task of making a useful GPU
driver in general.  Years ago, a graphics card came with some video RAM
and a <a href="https://en.wikipedia.org/wiki/Graphics_address_remapping_table">graphics
translation table (GTT)</a>.  The driver would map system memory into the
graphics card; user space could then submit buffer handles that would be
relocated for the graphics device.  This approach works, he said, but it is
slow.
</p><p>
Current GPUs have full virtual memory, instead, which saves a lot of that
overhead.  The kernel has grown a number of subsystems for working with
this virtual memory, including the <a href="https://docs.kernel.org/gpu/drm-mm.html#the-graphics-execution-manager-gem">graphics
execution manager (GEM)</a> for buffer-object management, the <a href="https://docs.kernel.org/gpu/drm-mm.html#the-translation-table-manager-ttm">translation
table manager (TTM)</a> for discrete video-RAM buffer-object management, 
and a bunch of synchronization and fencing code.  Initially, the DRM
subsystem would tie the allocation of a buffer to an allocation of virtual
memory at the same time; that was easy to do and sufficed to implement
OpenGL.  But, he said, the graphics world moved on from there.
</p><p>
Specifically, <a href="https://www.vulkan.org/">Vulkan</a> came along.  It
brought the concept of sparse memory and, with it, virtual memory that is
managed by user space.  Vulkan can handle both synchronous and asynchronous
virtual-area updates, but it &#34;gets complicated&#34;.  Various drivers started
inventing their own virtual-area management; as a way of bringing that work
back together, the <a href="https://www.kernel.org/doc/html/latest/gpu/drm-vm-bind-async.html">VM_BIND</a>
API was developed.
</p><p>
This is consistent with a recurring pattern, Airlie said.  The DRM
developers work to share common code between graphics drivers, but the
driver developers keep trying to reinvent wheels, a tendency that has to be
resisted.  The subsystem did well with regard to mode setting, he said, but
less well on the acceleration side; there is a &#34;common GPU scheduler&#34; that
is only used by one driver, for example.  Similarly, there are a lot of
drivers implementing VM_BIND by doing their own virtual-area management.
</p><p>
In response, Airlie came up with the &#34;good idea&#34; of getting somebody else
to write a common virtual-area manager, called GPUVM, inspired by the
amdgpu code.  It is intended to be useful for all drivers; it is used by
the Nouveau, Xe (Intel&#39;s new driver), and Panfrost drivers now.  Hopefully
the amdgpu and MSM drivers will pick it up as well.  The best part is that
there are multiple developers who understand it and can help to keep it
from going off in the wrong direction.  GPUVM has been through a lot of
iterations, he said, providing &#34;lots of learning experiences&#34;.
</p><p>
As an example, he talked about the problem of fence signaling.  A fence
indicates when a series of GPU operations has been completed; waits for
these fences have to be time-bounded, or the memory-management subsystem
might deadlock.  In short, a GPU can easily pin down all of a system&#39;s RAM
if given the opportunity.  There is a shrinker that can be called when
memory gets tight, but it will have to wait for fences to be signaled to
know when memory can be freed.  If the code that set the fence decides to
allocate more memory while this is happening, a deadlock results.

To avoid this outcome, developers have to strictly limit the operations
that can be performed in fence-signaling critical sections; care must also
be taken before acquiring any locks.  It would be nice to be able to update
the page tables during this code, but that ran into deadlock problems and
had to be backed out.
</p><p>
Returning to Nouveau, Airlie said that the initial VM_BIND API, using
GPUVM, synchronous objects, and integration with the scheduler, was merged
for the 6.6 release.  There are a lot of improvements in the works that
should land in 6.8.  At this point, he said, we have the core of a modern
GPU driver for NVIDIA hardware — for graphics, at least.  More work will be
required before Nouveau can support compute applications.
</p><p>
On the user-space side, Faith Ekstrand has been developing the NVK Vulkan
driver for Nouveau; this driver recently <a href="https://www.collabora.com/news-and-blog/news-and-events/nvk-reaches-vulkan-conformance.html">reached
Vulkan 1.0 conformance</a>.  This work involved creating a new
compiler, called NAK, that has just been merged into Mesa; this compiler
yields far better performance (from 20 frames per second to over 1000) than
the old &#34;codegen&#34; compiler did.  Naturally, this compiler is written in
Rust.  The next step, Airlie concluded, is to move forward to
Vulkan 1.3. 
</p><p>
<a href="https://www.youtube.com/live/LipsVK5d_vM?si=MvJmtsFKbpa5JyqC&amp;t=326m21s">Video</a>
and <a href="https://lpc.events/event/17/contributions/1505/attachments/1315/2641/Nouveau%20GSP%20GPU%20VA%20management.pdf">slides</a>
from the talk are available.
</p><p>

[Thanks to the Linux Foundation, LWN&#39;s travel sponsor, for supporting our
travel to this event.]<br clear="all"/></p><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#Device_drivers-Graphics">Device drivers/Graphics</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#Linux_Plumbers_Conference-2023">Linux Plumbers Conference/2023</a></td></tr>
            </tbody></table></div></div>
  </body>
</html>
