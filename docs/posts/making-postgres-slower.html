<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://byteofdev.com/posts/making-postgres-slow/">Original</a>
    <h1>Making Postgres slower</h1>
    
    <div id="readability-page-1" class="page"><div id="article-content"> <p>Everyone is always wondering how to make Postgres <em>faster</em>, <em>more efficient</em>, etc, but nobody ever thinks about how to make Postgres slower. Now, of course, most of those people are being paid to focus on speed, but I am not (although, if you wanted to change that, let me know). As I was writing a <a href="https://byteofdev.com/posts/tuning-postgres-intro/">slightly more useful guide</a>, I decided someone needed to try to create a Postgres configuration optimized to process queries as slowly as possible. Why? I am not sure, but this is what came of that thought.</p>
<h2 id="the-parameters"> The Parameters <a tabindex="-1" href="#the-parameters" aria-label="Link to The Parameters section"></a> </h2>
<p>I can’t make this too easy. This is a Postgres tuning challenge, not a throttle-your-CPU-to-one-megahertz-and-delete-indexes challenge, so <strong>all changes must be on parameters in <code>postgresql.conf</code></strong>. Additionally, the database will still need to have the capability to process at least one transaction within a reasonable amount of time—it would be too simple just to grind Postgres to a halt. This is more difficult than it might seem, because Postgres tries to make it as difficult as possible to make decisions this stupid by enforcing limits and minimizing configuration.</p>
<p>To measure performance, I will use TPC-C with 128 warehouses as implemented in Benchbase, using 100 connections each attempting to output 10k transactions per second, all being processed by Postgres 19devel (latest as of 7/14/2025) running on a Linux 6.15.6 with a Ryzen 7950x, 32GB of RAM, and a 2TB SSD. Each test will last 120 seconds and will execute twice: first to warm the cache and second to collect measurements.</p>
<p>I measured a baseline with everything left to its default in <code>postgresql.conf</code>, except for basic tweaks increasing <code>shared_buffers</code>, <code>work_mem</code>, and the number of worker processes. In that test, I got a nice 7082 TPS. Now, let’s see how much slower Postgres will go.</p>
<h2 id="caching?-nah…"> Caching? Nah… <a tabindex="-1" href="#caching?-nah…" aria-label="Link to Caching? Nah… section"></a> </h2>
<p>One of the ways Postgres can respond to read queries efficiently is through extensive caching. Accessing data from the disk is <em>slow</em>, so whenever Postgres reads a block of data from the disk, it caches that block in RAM, allowing the next query that requires that block to read it from RAM. Of course, I want to force all queries to use the slowest possible reading method, so the smaller this cache is, the better. I can freely control the size of the buffer cache and other elements of Postgres’s shared memory using the <code>shared_buffers</code> knob. Unfortunately, I cannot simply set this to 0, as Postgres also uses the buffer cache as the area where active database pages are processed. Luckily, I can still get it pretty low.</p>
<p>First, I tried going from the <code>10GB</code> I allocated in the baseline to <code>8MB</code>.</p>
<pre data-language="ini"><code is:raw=""><span>shared_buffers</span> <span>=</span> <span>8MB</span>
</code></pre>
<figure>   </figure>
<p>Already, Postgres is operating at only 1/7th of its initial speed. The reduced buffer cache forced Postgres to keep fewer pages in RAM, meaning the percentage of page requests that could be fulfilled without going to the operating system plummeted from 99.90% to 70.52%, resulting in an almost 300x increase in the number of read syscalls.</p>
<p>But we can do better. 70% is still too high, and it should be possible to reduce the cache size further. Next, I tried 128kB.</p>
<figure>   </figure>
<p>Oops. 128kB of shared buffers can only store a maximum of 16 database pages (excluding any other content in the shared buffers), and Postgres likely requires simultaneous access to more than 16 pages. After some messing around, I found that the lowest possible value was approximately 2 MB. Postgres is now below 500 TPS.</p>
<pre data-language="ini"><code is:raw=""><span>shared_buffers</span> <span>=</span> <span>2MB</span>
</code></pre>
<figure>   </figure>
<h2 id="making-postgres-perform-as-much-background-work-as-possible"> Making Postgres Perform As Much Background Work As Possible <a tabindex="-1" href="#making-postgres-perform-as-much-background-work-as-possible" aria-label="Link to Making Postgres Perform As Much Background Work As Possible section"></a> </h2>
<p>Postgres has several tasks beyond processing transactions that can be computationally expensive. I can use this to my advantage. To minimize storage fragmentation, Postgres runs an autovacuum process that finds empty space (from operations like deletions) and fills that space with other tuples. Usually, this only runs after a certain number of changes are made to prevent an excessive performance penalty, but I can reconfigure autovacuum to minimize the time between each run.</p>
<pre data-language="ini"><code is:raw=""><span>autovacuum_vacuum_insert_threshold</span> <span>=</span> <span>1 # autovacuum can be triggered with only 1 insert</span>
<span>autovacuum_vacuum_threshold</span> <span>=</span> <span>0 # minimum number of inserts, updates, or deletes needed to trigger a vacuum</span>
<span>autovacuum_vacuum_scale_factor</span> <span>=</span> <span>0 # proportion of the unfrozen table size to consider when calculating thresholds</span>
<span>autovacuum_vacuum_max_threshold</span> <span>=</span> <span>1 # max number of inserts, updates, or deletes needed to trigger a vacuum</span>
<span>autovacuum_naptime</span> <span>=</span> <span>1 # the minimum delay between autovacuums in seconds; unfortunately, this cannot be set below 1, which limits us</span>
<span>vacuum_cost_limit</span> <span>=</span> <span>10000 # query cost limit, which, if exceeded, will cause the vacuum to pause; I don&#39;t want the vacuum to ever stop, so I maxed this out</span>
<span>vacuum_cost_page_dirty</span> <span>=</span> <span>0</span>
<span>vacuum_cost_page_hit</span> <span>=</span> <span>0</span>
<span>vacuum_cost_page_miss</span> <span>=</span> <span>0 # all of these minimize the cost for operations when calculating for `vacuum_cost_limit`</span>
</code></pre>
<p>I also reconfigured the autovacuum analyzer, which collects statistics to guide vacuuming and query planning (spoiler: accurate statistics shouldn’t stop me from messing with the query planner):</p>
<pre data-language="ini"><code is:raw=""><span>autovacuum_analyze_threshold</span> <span>=</span> <span>0 # same as autovacuum_vacuum_threshold, but for ANALYZE</span>
<span>autovacuum_analyze_scale_factor</span> <span>=</span> <span>0 # same as autovacuum_vacuum_scale_factor</span>
</code></pre>
<p>I also tried to make the vacuuming process itself as slow as possible:</p>
<pre data-language="ini"><code is:raw=""><span>maintenance_work_mem</span> <span>=</span> <span>128kB # the amount of memory allocated for vacuuming processes</span>
<span>log_autovacuum_min_duration</span> <span>=</span> <span>0 # the duration (in milliseconds) that a autovacuum operation is required to run for before it is logged; I might as well log everything;</span>
<span>logging_collector</span> <span>=</span> <span>on # enables logging in general</span>
<span>log_destination</span> <span>=</span> <span>stderr,jsonlog # sets the output format/file for logs</span>
</code></pre>
<p>I should note that the opposite approach might also work: if I disable autovacuuming entirely, pages will fill with dead tuples, and performance will gradually decrease. However, because this is an insert-heavy workload that only runs for 2 minutes, I didn’t see that approach as being as inefficient.</p>
<figure>   </figure>
<p>Postgres is now operating at less than 1/20th of its original speed. I confirmed the source of its performance hit by checking the logs:</p>
<pre data-language="log"><code is:raw=""><span>2025-07-20</span> <span>09:10:20.455</span> EDT <span>[</span><span>25210</span><span>]</span> <span>LOG:</span>  automatic vacuum of table <span>&#34;benchbase.public.warehouse&#34;</span><span>:</span> index scans<span>:</span> <span>0</span>
 <span>pages:</span> <span>0</span> removed<span>,</span> <span>222</span> remain<span>,</span> <span>222</span> scanned <span>(</span><span>100.00</span><span>%</span> of total<span>)</span><span>,</span> <span>0</span> eagerly scanned
 <span>tuples:</span> <span>0</span> removed<span>,</span> <span>354</span> remain<span>,</span> <span>226</span> are dead but not yet removable
 <span>removable cutoff:</span> <span>41662928</span><span>,</span> which was <span>523</span> XIDs old when operation ended
 <span>frozen:</span> <span>0</span> pages from table <span>(</span><span>0.00</span><span>%</span> of total<span>)</span> had <span>0</span> tuples frozen
 <span>visibility map:</span> <span>0</span> pages set all<span>-</span>visible<span>,</span> <span>0</span> pages set all<span>-</span>frozen <span>(</span><span>0</span> were all<span>-</span>visible<span>)</span>
 <span>index scan not needed:</span> <span>0</span> pages from table <span>(</span><span>0.00</span><span>%</span> of total<span>)</span> had <span>0</span> dead item identifiers removed
 <span>avg read rate:</span> <span>116.252</span> MB<span>/</span>s<span>,</span> avg write rate<span>:</span> <span>4.824</span> MB<span>/</span>s
 <span>buffer usage:</span> <span>254</span> hits<span>,</span> <span>241</span> reads<span>,</span> <span>10</span> dirtied
 <span>WAL usage:</span> <span>2</span> records<span>,</span> <span>2</span> full page images<span>,</span> <span>16336</span> bytes<span>,</span> <span>1</span> buffers full
 <span>system usage:</span> <span>CPU:</span> <span>user:</span> <span>0.00</span> s<span>,</span> system<span>:</span> <span>0.00</span> s<span>,</span> elapsed<span>:</span> <span>0.01</span> s
<span>2025-07-20</span> <span>09:10:20.773</span> EDT <span>[</span><span>25210</span><span>]</span> <span>LOG:</span>  automatic analyze of table <span>&#34;benchbase.public.warehouse&#34;</span>
 <span>avg read rate:</span> <span>8.332</span> MB<span>/</span>s<span>,</span> avg write rate<span>:</span> <span>0.717</span> MB<span>/</span>s
 <span>buffer usage:</span> <span>311</span> hits<span>,</span> <span>337</span> reads<span>,</span> <span>29</span> dirtied
 <span>WAL usage:</span> <span>36</span> records<span>,</span> <span>5</span> full page images<span>,</span> <span>42524</span> bytes<span>,</span> <span>4</span> buffers full
 <span>system usage:</span> <span>CPU:</span> <span>user:</span> <span>0.00</span> s<span>,</span> system<span>:</span> <span>0.00</span> s<span>,</span> elapsed<span>:</span> <span>0.31</span> s
<span>2025-07-20</span> <span>09:10:20.933</span> EDT <span>[</span><span>25210</span><span>]</span> <span>LOG:</span>  automatic vacuum of table <span>&#34;benchbase.public.district&#34;</span><span>:</span> index scans<span>:</span> <span>0</span>
 <span>pages:</span> <span>0</span> removed<span>,</span> <span>1677</span> remain<span>,</span> <span>1008</span> scanned <span>(</span><span>60.11</span><span>%</span> of total<span>)</span><span>,</span> <span>0</span> eagerly scanned
 <span>tuples:</span> <span>4</span> removed<span>,</span> <span>2047</span> remain<span>,</span> <span>557</span> are dead but not yet removable
 <span>removable cutoff:</span> <span>41662928</span><span>,</span> which was <span>686</span> XIDs old when operation ended
 <span>frozen:</span> <span>0</span> pages from table <span>(</span><span>0.00</span><span>%</span> of total<span>)</span> had <span>0</span> tuples frozen
 <span>visibility map:</span> <span>0</span> pages set all<span>-</span>visible<span>,</span> <span>0</span> pages set all<span>-</span>frozen <span>(</span><span>0</span> were all<span>-</span>visible<span>)</span>
 <span>index scan bypassed:</span> <span>2</span> pages from table <span>(</span><span>0.12</span><span>%</span> of total<span>)</span> have <span>9</span> dead item identifiers
 <span>avg read rate:</span> <span>50.934</span> MB<span>/</span>s<span>,</span> avg write rate<span>:</span> <span>9.945</span> MB<span>/</span>s
 <span>buffer usage:</span> <span>1048</span> hits<span>,</span> <span>1009</span> reads<span>,</span> <span>197</span> dirtied
 <span>WAL usage:</span> <span>6</span> records<span>,</span> <span>1</span> full page images<span>,</span> <span>8707</span> bytes<span>,</span> <span>0</span> buffers full
 <span>system usage:</span> <span>CPU:</span> <span>user:</span> <span>0.00</span> s<span>,</span> system<span>:</span> <span>0.00</span> s<span>,</span> elapsed<span>:</span> <span>0.15</span> s
<span>2025-07-20</span> <span>09:10:21.220</span> EDT <span>[</span><span>25210</span><span>]</span> <span>LOG:</span>  automatic analyze of table <span>&#34;benchbase.public.district&#34;</span>
 <span>avg read rate:</span> <span>47.235</span> MB<span>/</span>s<span>,</span> avg write rate<span>:</span> <span>1.330</span> MB<span>/</span>s
 <span>buffer usage:</span> <span>115</span> hits<span>,</span> <span>1705</span> reads<span>,</span> <span>48</span> dirtied
 <span>WAL usage:</span> <span>30</span> records<span>,</span> <span>1</span> full page images<span>,</span> <span>17003</span> bytes<span>,</span> <span>1</span> buffers full
 <span>system usage:</span> <span>CPU:</span> <span>user:</span> <span>0.00</span> s<span>,</span> system<span>:</span> <span>0.00</span> s<span>,</span> elapsed<span>:</span> <span>0.28</span> s
<span>2025-07-20</span> <span>09:10:21.543</span> EDT <span>[</span><span>25212</span><span>]</span> <span>LOG:</span>  automatic vacuum of table <span>&#34;benchbase.public.warehouse&#34;</span><span>:</span> index scans<span>:</span> <span>0</span>
 <span>pages:</span> <span>0</span> removed<span>,</span> <span>222</span> remain<span>,</span> <span>222</span> scanned <span>(</span><span>100.00</span><span>%</span> of total<span>)</span><span>,</span> <span>0</span> eagerly scanned
 <span>tuples:</span> <span>0</span> removed<span>,</span> <span>503</span> remain<span>,</span> <span>375</span> are dead but not yet removable
 <span>removable cutoff:</span> <span>41662928</span><span>,</span> which was <span>845</span> XIDs old when operation ended
 <span>frozen:</span> <span>0</span> pages from table <span>(</span><span>0.00</span><span>%</span> of total<span>)</span> had <span>0</span> tuples frozen
 <span>visibility map:</span> <span>0</span> pages set all<span>-</span>visible<span>,</span> <span>0</span> pages set all<span>-</span>frozen <span>(</span><span>0</span> were all<span>-</span>visible<span>)</span>
 <span>index scan not needed:</span> <span>0</span> pages from table <span>(</span><span>0.00</span><span>%</span> of total<span>)</span> had <span>0</span> dead item identifiers removed
 <span>avg read rate:</span> <span>131.037</span> MB<span>/</span>s<span>,</span> avg write rate<span>:</span> <span>5.083</span> MB<span>/</span>s
 <span>buffer usage:</span> <span>268</span> hits<span>,</span> <span>232</span> reads<span>,</span> <span>9</span> dirtied
 <span>WAL usage:</span> <span>1</span> records<span>,</span> <span>0</span> full page images<span>,</span> <span>258</span> bytes<span>,</span> <span>0</span> buffers full
 <span>system usage:</span> <span>CPU:</span> <span>user:</span> <span>0.00</span> s<span>,</span> system<span>:</span> <span>0.00</span> s<span>,</span> elapsed<span>:</span> <span>0.01</span> s
<span>2025-07-20</span> <span>09:10:21.813</span> EDT <span>[</span><span>25212</span><span>]</span> <span>LOG:</span>  automatic analyze of table <span>&#34;benchbase.public.warehouse&#34;</span>
 <span>avg read rate:</span> <span>10.244</span> MB<span>/</span>s<span>,</span> avg write rate<span>:</span> <span>0.851</span> MB<span>/</span>s
 <span>buffer usage:</span> <span>307</span> hits<span>,</span> <span>337</span> reads<span>,</span> <span>28</span> dirtied
 <span>WAL usage:</span> <span>33</span> records<span>,</span> <span>3</span> full page images<span>,</span> <span>30864</span> bytes<span>,</span> <span>2</span> buffers full
 <span>system usage:</span> <span>CPU:</span> <span>user:</span> <span>0.00</span> s<span>,</span> system<span>:</span> <span>0.00</span> s<span>,</span> elapsed<span>:</span> <span>0.25</span> s
<span>#</span> <span>.</span><span>.</span><span>.</span> it continues similarly
</code></pre>
<p>Postgres runs automatic vacuum and analysis operations on hot tables every second, which, because the buffer cache hit rate is already low, forces it to read significant amounts from disk. Better yet, these are doing almost nothing because so little has changed between each run. Of course, 293 TPS is still too much.</p>
<h2 id="turning-postgres-into-brandon-sanderson"> Turning Postgres into Brandon Sanderson <a tabindex="-1" href="#turning-postgres-into-brandon-sanderson" aria-label="Link to Turning Postgres into Brandon Sanderson section"></a> </h2>
<p><a href="https://theportalist.com/how-brandon-sanderson-writes-the-stormlight-archive-so-stormin-fast">Bradon Sanderson writes <em>a lot</em></a>. You know what else (will) write(s) a lot? My Postgres instance, once I am done messing with the WAL configurations. Before committing changes to the actual database files, Postgres writes them to a WAL (write-ahead-log), and then commits those changes in a checkpointing operation. The WAL is very configurable, which I can use to our advantage. First, Postgres typically keeps some of the WAL in memory before flushing to disk. I can’t let that happen.</p>
<pre data-language="ini"><code is:raw=""><span>wal_writer_flush_after</span> <span>=</span> <span>0 # the minimum amount of WAL produced that requires a flush</span>
<span>wal_writer_delay</span> <span>=</span> <span>1 # the minimum delay between flushes</span>
</code></pre>
<p>I also want to get the WAL to checkpoint as often as possible.</p>
<pre data-language="ini"><code is:raw=""><span>min_wal_size</span> <span>=</span> <span>32MB # minimum WAL size after checkpointing; I want to checkpoint as much as possible</span>
<span>max_wal_size</span> <span>=</span> <span>32MB # max WAL size, after which a checkpoint will happen. Unfortunately, I have to set both at 32MB minimum to match 2 WAL segments</span>
<span>checkpoint_timeout</span> <span>=</span> <span>30 # max time between checkpoints in seconds; 30s is the minimum</span>
<span>checkpoint_flush_after</span> <span>=</span> <span>1 # flush writes to disk after every 8kB</span>
</code></pre>
<p>And, of course, I still need to maximize the WAL’s writes.</p>
<pre data-language="ini"><code is:raw=""><span>wal_sync_method</span> <span>=</span> <span>open_datasync # the method of flushing to disk; this should be the slowest</span>
<span>wal_level</span> <span>=</span> <span>logical # makes the WAL output additional information for replication. The extra info isn&#39;t needed, but it hurts performance</span>
<span>wal_log_hints</span> <span>=</span> <span>on # forces the WAL to write out full modified pages</span>
<span>summarize_wal</span> <span>=</span> <span>on # another extra process for backups</span>
<span>track_wal_io_timing</span> <span>=</span> <span>on # more information collected</span>
<span>checkpoint_completion_target</span> <span>=</span> <span>0 # prevents spreading the I/O load at all</span>
</code></pre>
<figure>   </figure>
<p>Postgres is now processing transactions at a rate in the double digits, at less than 1/70th of its original rate. Just like with autovacuum, I can confirm that this is due to WAL inefficiency by taking a look at the logs:</p>
<pre data-language="log"><code is:raw=""><span>2025-07-20</span> <span>12:33:17.211</span> EDT <span>[</span><span>68697</span><span>]</span> <span>LOG:</span>  <span>checkpoint complete:</span> wrote <span>19</span> buffers <span>(</span><span>7.4</span><span>%</span><span>)</span><span>,</span> wrote <span>2</span> SLRU buffers<span>;</span> <span>0</span> WAL file<span>(</span>s<span>)</span> added<span>,</span> <span>3</span> removed<span>,</span> <span>0</span> recycled<span>;</span> write<span>=</span><span>0.094</span> s<span>,</span> sync<span>=</span><span>0.042</span> s<span>,</span> total<span>=</span><span>0.207</span> s<span>;</span> sync files<span>=</span><span>57</span><span>,</span> longest<span>=</span><span>0.004</span> s<span>,</span> average<span>=</span><span>0.001</span> s<span>;</span> distance<span>=</span><span>31268</span> kB<span>,</span> estimate<span>=</span><span>31268</span> kB<span>;</span> lsn<span>=</span><span>1B7</span><span>/</span><span>3CDC1B80</span><span>,</span> redo lsn<span>=</span><span>1B7</span><span>/</span><span>3C11CD48</span>
<span>2025-07-20</span> <span>12:33:17.458</span> EDT <span>[</span><span>68697</span><span>]</span> <span>LOG:</span>  checkpoints are occurring too frequently <span>(</span><span>0</span> seconds apart<span>)</span>
<span>2025-07-20</span> <span>12:33:17.458</span> EDT <span>[</span><span>68697</span><span>]</span> <span>HINT:</span>  Consider increasing the configuration parameter <span>&#34;max_wal_size&#34;</span><span>.</span>
<span>2025-07-20</span> <span>12:33:17.494</span> EDT <span>[</span><span>68697</span><span>]</span> <span>LOG:</span>  <span>checkpoint starting:</span> wal
<span>2025-07-20</span> <span>12:33:17.738</span> EDT <span>[</span><span>68697</span><span>]</span> <span>LOG:</span>  <span>checkpoint complete:</span> wrote <span>18</span> buffers <span>(</span><span>7.0</span><span>%</span><span>)</span><span>,</span> wrote <span>1</span> SLRU buffers<span>;</span> <span>0</span> WAL file<span>(</span>s<span>)</span> added<span>,</span> <span>2</span> removed<span>,</span> <span>0</span> recycled<span>;</span> write<span>=</span><span>0.089</span> s<span>,</span> sync<span>=</span><span>0.047</span> s<span>,</span> total<span>=</span><span>0.280</span> s<span>;</span> sync files<span>=</span><span>50</span><span>,</span> longest<span>=</span><span>0.009</span> s<span>,</span> average<span>=</span><span>0.001</span> s<span>;</span> distance<span>=</span><span>34287</span> kB<span>,</span> estimate<span>=</span><span>34287</span> kB<span>;</span> lsn<span>=</span><span>1B7</span><span>/</span><span>3F1F7B18</span><span>,</span> redo lsn<span>=</span><span>1B7</span><span>/</span><span>3E298BA0</span>
<span>2025-07-20</span> <span>12:33:17.923</span> EDT <span>[</span><span>68697</span><span>]</span> <span>LOG:</span>  checkpoints are occurring too frequently <span>(</span><span>0</span> seconds apart<span>)</span>
<span>2025-07-20</span> <span>12:33:17.923</span> EDT <span>[</span><span>68697</span><span>]</span> <span>HINT:</span>  Consider increasing the configuration parameter <span>&#34;max_wal_size&#34;</span><span>.</span>
<span>2025-07-20</span> <span>12:33:17.971</span> EDT <span>[</span><span>68697</span><span>]</span> <span>LOG:</span>  <span>checkpoint starting:</span> wal
</code></pre>
<p>Yeah, normally WAL checkpoints should not be happening (<em>checks notes</em> 487 milliseconds apart). But I am still not done.</p>
<h2 id="essentially-deleting-indexes"> Essentially Deleting Indexes <a tabindex="-1" href="#essentially-deleting-indexes" aria-label="Link to Essentially Deleting Indexes section"></a> </h2>
<p>Remember in the intro, when I said we couldn’t mess with the indexes? Well, we don’t really need to. Postgres considers random access of pages from disk differently from sequential access when calculating query plans, because randomly accessed pages are typically slower to load on hard drives. Querying a table with an index typically requires accessing pages randomly, whereas a table scan usually involves sequential access, meaning that adjusting the relative costs of random pages should enable us to prevent the use of any indexes.</p>
<pre data-language="ini"><code is:raw=""><span>random_page_cost</span> <span>=</span> <span>1e300 # sets the cost of accessing a random page</span>
<span>cpu_index_tuple_cost</span> <span>=</span> <span>1e300 # sets the cost of processing one tuple from an index</span>
</code></pre>
<p>Those are the only two parameters I need to change to disable indexes in almost all cases. I ended up having to increase the <code>shared_buffers</code> value back up to <code>8MB</code> to prevent errors with table scans, but it evidently didn’t help much performance-wise.</p>
<figure>   </figure>
<p>Postgres is now under one transaction per second, more than 7,000x slower than the default tuning, all without changing anything outside of <code>postgresql.conf</code>. However, I still have one last trick up my sleeve.</p>
<h2 id="forcing-i/o-into-one-thread"> Forcing I/O Into One Thread <a tabindex="-1" href="#forcing-i/o-into-one-thread" aria-label="Link to Forcing I/O Into One Thread section"></a> </h2>
<p>I cannot make Postgres single-threaded, because each of the 100 connections has its own process. However, with new options in Postgres 18, I can still make I/O single-threaded. Postgres 18 introduces a new knob, <code>io_method</code>, which controls whether threads synchronously issue I/O syscalls (<code>io_method = sync</code>), asynchronously ask worker threads to issue syscalls (<code>io_method = worker</code>), or use the new <code>io_uring</code> Linux API (<code>io_method = io_uring</code>). In combination with <code>io_workers</code>, which establishes the max number of worker threads when using <code>io_method=worker</code>, I can force all I/O into one worker thread.</p>
<pre data-language="ini"><code is:raw=""><span>io_method</span> <span>=</span> <span>worker</span>
<span>io_workers</span> <span>=</span> <span>1</span>
</code></pre>
<figure>   </figure>
<p>Well, Postgres is now well below even 0.1 TPS: more than 42,000 times slower than what we started with. If you exclude the transactions that didn’t finish due to deadlocks, the story gets even worse (better?): across 100 connections and 120 seconds, only 11 transactions successfully completed.</p>
<h2 id="final-thoughts"> Final Thoughts <a tabindex="-1" href="#final-thoughts" aria-label="Link to Final Thoughts section"></a> </h2>
<p>Well, a few hours and 32 knobs later, I have successfully killed a Postgres database. Who would’ve thought you could do that much damage to Postgres performance just by messing with <code>postgresql.conf</code>? I figured I could get down to the single-digit TPS, but I didn’t think Postgres would let me do this much. If you want to try to reproduce this yourself, here are the knobs changed from default:</p>
<pre data-language="ini"><code is:raw=""><span>shared_buffers</span> <span>=</span> <span>8MB</span>
<span>autovacuum_vacuum_insert_threshold</span> <span>=</span> <span>1</span>
<span>autovacuum_vacuum_threshold</span> <span>=</span> <span>0</span>
<span>autovacuum_vacuum_scale_factor</span> <span>=</span> <span>0</span>
<span>autovacuum_vacuum_max_threshold</span> <span>=</span> <span>1</span>
<span>autovacuum_naptime</span> <span>=</span> <span>1</span>
<span>vacuum_cost_limit</span> <span>=</span> <span>10000</span>
<span>vacuum_cost_page_dirty</span> <span>=</span> <span>0</span>
<span>vacuum_cost_page_hit</span> <span>=</span> <span>0</span>
<span>vacuum_cost_page_miss</span> <span>=</span> <span>0</span>
<span>autovacuum_analyze_threshold</span> <span>=</span> <span>0</span>
<span>autovacuum_analyze_scale_factor</span> <span>=</span> <span>0</span>
<span>maintenance_work_mem</span> <span>=</span> <span>128kB</span>
<span>log_autovacuum_min_duration</span> <span>=</span> <span>0</span>
<span>logging_collector</span> <span>=</span> <span>on</span>
<span>log_destination</span> <span>=</span> <span>stderr,jsonlog</span>
<span>wal_writer_flush_after</span> <span>=</span> <span>0</span>
<span>wal_writer_delay</span> <span>=</span> <span>1</span>
<span>min_wal_size</span> <span>=</span> <span>32MB</span>
<span>max_wal_size</span> <span>=</span> <span>32MB</span>
<span>checkpoint_timeout</span> <span>=</span> <span>30</span>
<span>checkpoint_flush_after</span> <span>=</span> <span>1</span>
<span>wal_sync_method</span> <span>=</span> <span>open_datasync</span>
<span>wal_level</span> <span>=</span> <span>logical</span>
<span>wal_log_hints</span> <span>=</span> <span>on</span>
<span>summarize_wal</span> <span>=</span> <span>on</span>
<span>track_wal_io_timing</span> <span>=</span> <span>on</span>
<span>checkpoint_completion_target</span> <span>=</span> <span>0</span>
<span>random_page_cost</span> <span>=</span> <span>1e300</span>
<span>cpu_index_tuple_cost</span> <span>=</span> <span>1e300</span>
<span>io_method</span> <span>=</span> <span>worker</span>
<span>io_workers</span> <span>=</span> <span>1</span>
</code></pre>
<p>You can benchmark the configuration by installing <a href="https://github.com/cmu-db/benchbase">BenchBase Postgres</a> and using the example TPC-C configuration with a length of 120 seconds, warmup of 120 seconds, 128 warehouses, and 100 connections with a max throughput of 50k TPS. You could also attempt to further worsen performance. I focused on the knobs that I thought would impact Postgres performance the most, and left most knobs untested.</p>
<p>Alright, in the course of writing this, my lower back has begun hurting, so I think it is time for me to go outside or something.</p> </div></div>
  </body>
</html>
