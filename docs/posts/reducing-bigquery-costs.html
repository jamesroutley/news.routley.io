<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.peerdb.io/reducing-bigquery-costs-by-260x">Original</a>
    <h1>Reducing BigQuery Costs</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content-parent"><div id="post-content-wrapper"><p>In this blog post, we&#39;ll do a deep-dive into a simple trick that can reduce BigQuery costs by orders of magnitude. Specifically, we&#39;ll explore how <a target="_blank" href="https://cloud.google.com/bigquery/docs/clustered-tables"><strong>clustering</strong></a> (similar to indexing in BigQuery world) large tables can significantly impact costs. We will walk through an example use-case for a common query pattern (<a target="_blank" href="https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement">MERGE</a>), where clustering reduces the amount of data processed by BigQuery from 10GB to 37MB, resulting in a cost reduction of ~260X.</p>
<h2 id="heading-setup">Setup</h2>
<p>The setup involves a common Data Warehousing scenario of transforming raw data from multiple sources into a consumable format through the MERGE command. Now, let&#39;s delve into the data model:</p>
<ol>
<li><p>There are two groups of tables, each containing two tables. One group of tables is clustered based on the <code>id</code> column, while the tables in the other group are not clustered.</p>
</li>
<li><p>One of tables represents staging/raw data and the other represents the final data.</p>
</li>
<li><p>The staging table is seeded with 10 million rows and the final table is seeded with 200 million rows.</p>
</li>
<li><p>The <code>MERGE</code> command is used to merge a subset of data (~10K rows filtered on <code>id</code>) in the staging table to the final table.</p>
</li>
</ol>
<h3 id="heading-creating-tables-and-seeding-data">Creating Tables and Seeding Data</h3>
<p>Here goes the <a target="_blank" href="https://gist.github.com/saisrirampur/22b3399077cf8490301039aa5ca7ab94">script</a> that we used to create and seed tables.</p>
<h3 id="heading-merge-command-on-clustered-and-unclustered-tables">MERGE command on Clustered and Unclustered tables</h3>
<pre><code>
<span>MERGE</span> sai_tests.&lt;final_table&gt; <span>AS</span> <span>final</span>
<span>USING</span> (
    <span>SELECT</span> * <span>FROM</span> sai_tests.&lt;staging_table&gt;
    
    <span>WHERE</span> <span>id</span> &gt; <span>500000</span> <span>AND</span> <span>id</span> &lt; <span>510000</span> 
) <span>AS</span> staging
<span>ON</span> final.id = staging.id 
<span>WHEN</span> <span>MATCHED</span>  <span>THEN</span>
    <span>UPDATE</span> <span>SET</span> final.column_string = staging.column_string, 
    final.column_int = staging.column_int, 
    final.column_float = staging.column_float, 
    final.column_timestamp = staging.column_timestamp
<span>WHEN</span> <span>NOT</span> <span>MATCHED</span>  <span>THEN</span>
    <span>INSERT</span> (<span>id</span>, column_string, column_int, column_float, column_timestamp)
    <span>VALUES</span> (staging.id, staging.column_string, staging.column_int, 
    staging.column_float, staging.column_timestamp);
</code></pre>
<h2 id="heading-analyzing-the-merge-command-on-the-unclustered-tables">Analyzing the MERGE command on the Unclustered tables</h2>
<p>Below is the execution graph of the MERGE command on the unclustered tables:</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707094507323/3ad7d497-77d2-4092-8811-916737fc3986.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p><strong>Now let us analyze important step of the execution graph:</strong></p>
<ol>
<li><p>In the first step (S00), BigQuery infers the join across both tables and pushes the <code>id</code> filter to the final table. Despite pushing the filter down to the final table, BigQuery still reads all 200 million records to retrieve the approximately 10K filtered records. This is due to the absence of clustering on the &#39;id&#39; column in the final table.</p>
<ol>
<li><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707094752871/bda203c8-ab63-42df-878d-fa8ea617aaff.png?auto=compress,format&amp;format=webp" alt=""/></li>
</ol>
</li>
<li><p>Now, let us analyze the JOIN (S02). As a part of the JOIN, BigQuery scans the entire 10 million records in the staging table, even though there is a WHERE clause on the <code>id</code> that filters approximately 10K records. This is due to the absence of clustering on the <code>id</code> column in the staging table.</p>
<p> <img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707094806579/abb6ad06-a639-4c16-9673-d1867f3d0ac0.png?auto=compress,format&amp;format=webp" alt=""/></p>
</li>
<li><p>If you observe, lack of CLUSTERING made BigQuery generate suboptimal plan processing 9.69GB of data costing 4.7 cents.</p>
<ol>
<li><p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707095086689/0f00e633-3e7a-4d7d-9b0a-f7ddeafb38fa.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p> <img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707095414333/45438275-b8b1-4067-a098-79ca3b692fd4.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p> <img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707095157716/7492fd44-41f7-43b3-a4f8-c8d7668de3c2.png?auto=compress,format&amp;format=webp" alt=""/></p>
</li>
</ol>
</li>
</ol>
<h2 id="heading-analyzing-the-merge-command-on-the-unclustered-tables-1">Analyzing the MERGE command on the Unclustered tables</h2>
<p>Below is the execution graph of the MERGE command on the clustered tables:</p>
<p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707093206188/e8776677-56ab-43d0-9781-c9e90402ccc4.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p><strong>Now let us analyze each step of the execution graph:</strong></p>
<ol>
<li><p>First, BigQuery infers the join across both tables and pushes the <code>id</code> filter to the final table. Since the final table is CLUSTERED on the <code>id</code> column, BigQuery leverages this clustering to efficiently retrieve approximately 368K records out of the 200 million records in the final table.</p>
<ol>
<li><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707093370991/955c73e2-2854-482d-97ca-414037e49ca5.png?auto=compress,format&amp;format=webp" alt=""/></li>
</ol>
</li>
<li><p>Now, let&#39;s analyze the JOIN (S02). As part of the JOIN, BigQuery utilizes the CLUSTERED <code>id</code> to read only 362K records from the staging table, which contains 10 million records.</p>
</li>
<li><p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707093521527/ed3e790b-f52b-42f3-a836-cd1083da626c.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p> If you observe BigQuery optimally filtered data across both the staging and final table using the <code>CLUSTER</code> (index) on id processing only ~37MB of data costing 0.017 cents</p>
</li>
<li><p><img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707094093894/91417a6e-f39d-45ce-a0dd-6f9739571d43.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p> <img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707095440054/ea78d9b2-e7e0-4b5b-8169-12897679d30e.png?auto=compress,format&amp;format=webp" alt=""/></p>
<p> <img loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1707094185157/5851a167-953e-44fe-96e4-c01dbc9e4133.png?auto=compress,format&amp;format=webp" alt=""/></p>
</li>
</ol>
<h2 id="heading-results-unclustered-vs-clustered-tables">Results: Unclustered vs Clustered Tables</h2>
<div>
<table>
<thead>
<tr>
<td></td><td>Unclustered</td><td>Clustered</td><td>Difference</td></tr>
</thead>
<tbody>
<tr>
<td><strong>Bytes Processed</strong></td><td>9.69GB</td><td>37MB</td><td>~260X</td></tr>
<tr>
<td><a target="_blank" href="https://cloud.google.com/bigquery/docs/slots"><strong>Slot time</strong></a></td><td>40.5min</td><td>2.5min</td><td>~20x</td></tr>
<tr>
<td><a target="_blank" href="https://cloud.google.com/bigquery/pricing"><strong>Costs</strong></a></td><td>4.7 cents</td><td>0.017 cents</td><td>~268X</td></tr>
</tbody>
</table>
</div><ol>
<li><p>The amount of bytes processed by the MERGE on the clustered set of tables is 260 times lower than the unclustered group.</p>
</li>
<li><p>Slot time is 20 times lower on clustered tables compared to unclustered tables.</p>
</li>
<li><p>Costs ($$) are 268 times lower for MERGE on clustered tables compared to unclustered tables.</p>
</li>
</ol>
<p>This is the impact a simple CLUSTER can have on compute costs for BigQuery. Based on the MERGE commands that you use, observe the join columns and columns in the WHERE clause, and intelligently <a target="_blank" href="https://cloud.google.com/bigquery/docs/creating-clustered-tables">CLUSTER</a> tables on those columns. This can signficantly reduce costs. With a similar approach.</p>
<h2 id="heading-auto-clustering-and-partitioning-in-peerdb">Auto Clustering and Partitioning in PeerDB</h2>
<p>We at <a target="_blank" href="https://www.peerdb.io/">PeerDB</a> are building a fast and cost-effective way to replicate data from Postgres to Data Warehouses and Queues. Aligned with the above blog post, we <a target="_blank" href="https://github.com/PeerDB-io/peerdb/pull/915">automatically cluster and partition</a> the raw and the final tables on BigQuery. The raw table is cluster based on 2 columns that have WHERE clause filters and partition based on timestamp. The final table is clustered based on the primary key. We&#39;ve seen significant cost reduction (2x-10x) for our customers with this optimization.</p>
<h2 id="heading-references">References</h2>
<p>Hope you enjoyed reading the blog, here are a few references that you might find interesting:</p>
<ol>
<li><p><a target="_blank" href="https://shopify.engineering/reducing-bigquery-costs">Shopify reduced their BigQuery spend from ~$1,000,000 to ~$1000</a>.</p>
</li>
<li><p><a target="_blank" href="https://blog.peerdb.io/five-useful-queries-to-get-bigquery-costs">Five Useful Queries to get BigQuery costs.</a></p>
</li>
<li><p><a target="_blank" href="https://app.peerdb.cloud/">Try PeerDB Cloud for free.</a></p>
</li>
<li><p>Visit PeerDB&#39;s <a target="_blank" href="https://github.com/PeerDB-io/peerdb"><strong>GitHub</strong></a> repository to Get Started.</p>
</li>
</ol>
</div></div></div>
  </body>
</html>
