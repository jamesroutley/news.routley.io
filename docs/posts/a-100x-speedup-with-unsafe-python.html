<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://yosefk.com/blog/a-100x-speedup-with-unsafe-python.html">Original</a>
    <h1>A 100x speedup with unsafe Python</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>We&#39;re going to speed up some numpy code by 100x using &#34;unsafe Python.&#34; Which is not quite the same as unsafe Rust, but it&#39;s a
bit similar, and I&#39;m not sure what else to call it... you&#39;ll see. It&#39;s not something you&#39;d use in most Python code, but it&#39;s
handy on occasion, and I think it shows &#34;the nature of Python‚Äù from an interesting angle.</p>
<p>So let&#39;s say you use <a href="https://pyga.me/">pygame</a> to write a simple game in Python.</p>
<p>(Is pygame the way to go today? I&#39;m not the guy to ask; to its credit, it has a very simple screen / mouse / keyboard APIs,
and is quite portable because it&#39;s built on top of <a href="https://www.libsdl.org/">SDL</a>. It runs on the major desktop
platforms, and with a bit of fiddling, you can run it on Android using <a href="https://buildozer.readthedocs.io/en/latest/">Buildozer</a>. In any case, pygame is just one real-life example where a
problem arises that &#34;unsafe Python&#34; can solve.)</p>
<p>Let us furthermore assume that you&#39;re resizing images a lot, so you want to optimize this. And so you discover the somewhat
unsurprising fact that <a href="https://opencv.org/">OpenCV</a>&#39;s resizing is faster than pygame&#39;s, as measured by the following
simple microbenchmark:</p>
<pre>from contextlib import contextmanager
import time

@contextmanager
def Timer(name):
    start = time.time()
    yield
    finish = time.time()
    print(<i><b>f&#39;{name} took {finish-start:.4f} sec&#39;</b></i>)

import pygame as pg
import numpy as np
import cv2

IW = 1920
IH = 1080
OW = IW // 2
OH = IH // 2

repeat = 100

isurf = pg.Surface((IW,IH), pg.SRCALPHA)
with Timer(<i><b>&#39;pg.Surface with smoothscale&#39;</b></i>):
    for i in range(repeat):
        <b>pg.transform.smoothscale</b>(isurf, (OW,OH))

def cv2_resize(image):
    return cv2.resize(image, (OH,OW), interpolation=cv2.INTER_AREA)

i1 = np.zeros((IW,IH,3), np.uint8)
with Timer(<i><b>&#39;np.zeros with cv2&#39;</b></i>):
    for i in range(repeat):
        o1 = <b>cv2_resize</b>(i1)
</pre>
<p>This prints:</p>
<pre>pg.Surface with smoothscale took <b>0.2002</b> sec
np.zeros with cv2 took <b>0.0145</b> sec
</pre>
<p>Tempted by the nice 13x speedup reported by the mircobenchmark, you go back to your game, and use
<code>pygame.surfarray.pixels3d</code> to get zero-copy access to the pixels as a numpy array. Full of hope, you pass this array
to <code>cv2.resize</code>, and observe that everything got much <em>slower</em>. Dammit! &#34;Caching,&#34; you think, &#34;or something.
Never trust a mircobenchmark!&#34;</p>
<p>Anyway, just in case, you call cv2.resize on the pixels3d array in your mircobenchmark. Perhaps the slowdown will
reproduce?..</p>
<pre>i2 = <b>pg.surfarray.pixels3d</b>(isurf)
with Timer(<b><i>&#39;pixels3d with cv2&#39;</i></b>):
    for i in range(repeat):
        o2 = cv2_resize(i2)
</pre>
<p>Sure enough, this is very slow, just like you saw in your larger program:</p>
<pre>pixels3d with cv2 took <b>1.3625</b> sec
</pre>
<p>So 7x slower than smoothscale - and more shockingly, almost <strong>100x</strong> slower than cv2.resize called with
numpy.zeros! What gives?! Like, we have two zero-initialized numpy arrays <strong>of the same shape and datatype.</strong> And
of course the resized output arrays have the same shape &amp; datatype, too:</p>
<pre>print(<b><i>&#39;i1==i2 is&#39;</i></b>, np.all(i1==i2))
print(<b><i>&#39;o1==o2 is&#39;</i></b>, np.all(o1==o2))
print(<b><i>&#39;input shapes&#39;</i></b>, i1.shape,i2.shape)
print(<b><i>&#39;input types&#39;</i></b>, i1.dtype,i2.dtype)
print(<b><i>&#39;output shapes&#39;</i></b>, o1.shape,o2.shape)
print(<b><i>&#39;output types&#39;</i></b>, o1.dtype,o2.dtype)
</pre>
<p>Just like you&#39;d expect, this prints that everything is the same:</p>
<pre><code>i1==i2 is True
o1==o2 is True
input shapes (1920, 1080, 3) (1920, 1080, 3)
input types uint8 uint8
output shapes (960, 540, 3) (960, 540, 3)
output types uint8 uint8</code></pre>
<p>How could a function run 100x more slowly on one array relatively to the other, seemingly identical array?.. I mean, you
would hope SDL <em>wouldn&#39;t</em> allocate pixels in some particularly slow-to-access RAM area - even though it theoretically
<em>could</em> do stuff like that, with a little help from the kernel (like, create a non-cachable memory area or something.) Or
is the surface stored in GPU memory and we&#39;re going thru PCI to get every pixel?!.. It doesn&#39;t work this way, <em>does it?</em>
- is there some horrible memory coherence protocol for these things that I missed?.. And if not - if it&#39;s the same kind of
memory of the same shape and size with both arrays - what&#39;s different that costs us a 100x slowdown?..</p>
<p>It turns out... And I confess that I only found out by accident, after giving up on this<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> and moving on to something else. Entirely incidentally, that other thing
involved passing numpy data to C code, so I had to learn what this data looks like from C. So, it turns out that the shape and
datatype aren&#39;t all there is to a numpy array:</p>
<pre>print(<b><i>&#39;input strides&#39;</i></b>,i1.strides,i2.strides)
print(<b><i>&#39;output strides&#39;</i></b>,o1.strides,o2.strides)
</pre>
<p>Ah, <em>strides.</em> Same in the output arrays, but very different in the input arrays:</p>
<pre>input strides <b>(3240, 3, 1) (4, 7680, -1)</b>
output strides (1620, 3, 1) (1620, 3, 1)
</pre>
<p>As we&#39;ll see, this difference between the strides does in fact fully account for the 100x slowdown. Can we fix this? We can,
but first, the post itself will need to seriously slow down to explain these strides, because they&#39;re so weird. And then we&#39;ll
snatch our 100x right back from these damned strides.</p>
<h2 id="numpy-array-memory-layout">numpy array memory layout</h2>
<p>So, what&#39;s a &#34;stride&#34;? A stride tells you how many bytes you have to, well, stride from one pixel to the next. For instance,
let&#39;s say we have a 3D array like an RGB image. Then given the array base pointer and the 3 strides, the address of
<code>array[x,y,z]</code> will be <code>base + x*xstride + y*ystride + z*zstride</code> (where with images, z is one of 0, 1 or
2, for the 3 channels of an RGB image.)</p>
<p>In other words, <strong>the strides define the layout of the array in memory</strong>. And for better or worse, <strong>numpy
is very flexible with respect to what this layout might be</strong>, because it supports many different stride values for a
given array shape &amp; datatype.</p>
<p>The two layouts at hand - numpy&#39;s default layout, and SDL&#39;s - are... well, I don&#39;t even know which of the two offends me
more. As you can see from the stride values, the layout numpy uses by default for a 3D array is
<code>base + x*3*height + y*3 + z</code>.</p>
<p><img alt="numpy-layout.png" height="178" src="https://yosefk.com/img/numpy-perf/numpy-layout.png" width="576"/></p>
<p>This means that the RGB values of a pixel are stored in 3 adjacent bytes, and the pixels of a <em>column</em> are stored
contiguously in memory - a <a href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">column-major order</a>. And I,
for one, find this <em>offensive</em>, because images are <em>traditionally</em> stored in a row-major order, in particular,
image sensors send them this way (and <em>capture</em> them this way, as you can see from the <a href="https://en.wikipedia.org/wiki/Rolling_shutter">rolling shutter</a> - every <em>row</em> is captured at a slightly
different time, not <em>column.</em>)</p>
<p>&#34;Why, we <em>do</em> follow that respected tradition as well,&#34; say popular numpy-based image libraries. &#34;See for yourself -
save an array of shape <code>(1920, 1080)</code> to a PNG file, and you&#39;ll get a 1080x1920 image.&#34; Which is true, and of course
makes it even worse: if you index with <code>arr[x,y]</code>, then x, aka dimension zero, actually corresponds to <em>the
vertical dimension</em> in the corresponding PNG file, and y, aka dimension one, corresponds to <em>the horizontal
dimension.</em> And thus numpy array columns correspond to PNG image rows. Which makes the numpy image layout &#34;row-major&#34; in
some sense, at the cost of x and y having the opposite of their usual meaning.</p>
<p>...Unless you got your numpy array from a pygame Surface object, in which case x actually <em>does</em> index into the
horizontal dimension. And so saving <code>pixels3d(surface)</code> with, say, imageio will produce a <em>transposed</em> PNG
relatively to the PNG created by <code>pygame.image.save(surface)</code>. And in case adding <em>that</em> insult to the injury
wasn&#39;t enough, cv2.resize gets a <code>(width, height)</code> tuple as the destination size, producing an output array of shape
<code>(height, width)</code>.</p>
<p>Against the backdrop of these insults and injuries, SDL has an inviting, civilized-looking layout where x is x, y is y, and
the data is stored in an honest row-major order, for all the meanings of &#34;row.&#34; But then upon closer look, the layout just
tramples all over my feelings: <code>base + x*4 + y*4*width - z</code>.</p>
<p>Like, the part where we have 4 in the strides instead of 3 as expected for an RGB image - I can get that part. We did ask for
an <em>RGBA</em> image, with an alpha channel, when we passed <code>SRCALPHA</code> to the Surface constructor. So I guess it
keeps the alpha channel together with the RGB pixels, and the 4 in the strides is needed to skip the As in RBGA. But then why,
may I ask, are there separate <code>pixels3d</code> and <code>pixels_alpha</code> functions? It&#39;s always annoying to have to
deal with RGB and alphas separately when using numpy with pygame surfaces. Why not a single <code>pixels4d</code>
function?..</p>
<p>But OK, the 4 instead of the 3 I could live with. But a zstride of -1? MINUS ONE? You start at the address of your Red pixel,
and to get to Green, you walk back one byte?! Now you&#39;re just fucking with me.</p>
<p>It turns out that SDL supports both RGB and BGR layout (in particular, apparently surfaces loaded from files are RGB, and
those created in memory are BGR?.. or is it even hairier than this?..) And if you use pygame&#39;s APIs, you needn&#39;t worry about RGB
vs BGR, the APIs handle it transparently. If you use <code>pixels3d</code> for numpy interop, you <em>also</em> needn&#39;t worry
about RGB vs BGR, because numpy&#39;s flexibility with strides lets pygame give you an array that <em>looks</em> like RGB despite it
being BGR in memory. For that, z stride is set to -1, and the base pointer of the array points to the first pixel&#39;s red value -
two pixels ahead of where the array memory starts, which is where the first pixel&#39;s <em>blue</em> value is.</p>
<p><img alt="SDL-layout.png" height="178" src="https://yosefk.com/img/numpy-perf/SDL-layout.png" width="576"/></p>
<p>Wait a minute... <strong><em>now</em> I get why we have pixels3d and pixels_alpha but no pixels4d!!</strong> Because SDL has
RGBA and BGRA images - <em>BGRA, not ABGR</em> - and you can&#39;t make BGRA data look like an RGBA numpy array, no matter what
weird values you use for strides. There&#39;s a limit to layout flexibility... or rather, there really isn&#39;t any limit beyond the
limits of computability, but thankfully numpy stops at configurable strides and doesn&#39;t let you specify a generic callback
function <code>addr(base, x, y, z)</code> for a fully programmable layout<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>So to support RGBA and BGRA transparently, pygame is forced to give us 2 numpy arrays - one for RGB (or BGR, depending on the
surface), and another for the alpha. And these numpy arrays have the right <em>shape</em>, and let us access the right
<em>data</em>, but their <em>layouts</em> are very different from normal arrays of their shape.</p>
<p>And <strong>different memory layout can definitely explain major differences in performance</strong>. We could try to figure
out exactly why the performance difference is almost 100x. But when possible, I prefer to just get rid of garbage, rather than
study it in detail. So instead of understanding this in depth, we&#39;ll simply show that the layout difference indeed accounts for
the 100x - and then get rid of the slowdown <em>without</em> changing the layout, which is where &#34;unsafe Python&#34; finally comes
in.</p>
<p>How can we show that the layout alone, and not some other property of the pygame Surface data (like the memory it&#39;s allocated
in) explains the slowdown? We can benchmark cv2.resize on a numpy array we create ourselves, with the same layout as
<code>pixels3d</code> gives us:</p>
<pre><i># create a byte array of zeros, and attach
# a numpy array with pygame-like strides
# to this byte array using the buffer argument.</i>
i3 = np.ndarray((IW, IH, 3), np.uint8,
        <b>strides=(4, IW*4, -1),</b>
        buffer=b&#39;\0&#39;*(IW*IH*4),
        offset=2) <i># start at the &#34;R&#34; of BGR</i>

with Timer(<b><i>&#39;pixels3d-like layout with cv2&#39;</i></b>):
    for i in range(repeat):
        o2 = <b>cv2_resize</b>(i3)
</pre>
<p>Indeed this is about as slow as we measured on pygame Surface data:</p>
<pre>pixels3d-like layout with cv2 took <b>1.3829</b> sec
</pre>
<p>Out of curiosity, we can check what happens if we merely copy data between these layouts:</p>
<pre>i4 = np.empty(i2.shape, i2.dtype)
with Timer(<b><i>&#39;pixels3d-like copied to same-shape array&#39;</i></b>):
    for i in range(repeat):
        <b>i4[:] = i2</b>

with Timer(<b><i>&#39;pixels3d-like to same-shape array, copyto&#39;</i></b>):
    for i in range(repeat):
        <b>np.copyto</b>(i4, i2)
</pre>
<p>Both the assignment operator and <code>copyto</code> are very slow, almost as slow as cv2.resize:</p>
<pre>pixels3d-like copied to same-shape array took <b>1.2681</b> sec
pixels3d-like to same-shape array, copyto took <b>1.2702</b> sec
</pre>
<h2 id="fooling-the-code-into-running-faster">Fooling the code into running faster</h2>
<p>What can we do about this? We can&#39;t change the layout of pygame Surface data. And we <em>seriously</em> don&#39;t want to copy
the C++ code of cv2.resize, with its various platform-specific optimizations, to see if we can adapt it to the Surface layout
without losing performance. <strong>What we <em>can</em> do is feed Surface data to cv2.resize using an array <em>with numpy&#39;s
default layout</em></strong> (instead of straightforwardly passing the array object returned by pixel3d.)</p>
<p>Not that this would actually <em>work</em> with any given function, mind you. But it <em>will</em> work specifically with
resizing, because it doesn&#39;t really care about certain aspects of the data, which we&#39;re incidentally going to blatantly
misrepresent:</p>
<ul>
<li>Resizing code doesn&#39;t care if a given channel represents red or blue. (Unlike, for instance, converting RGB to greyscale,
which <em>would</em> care.) If you give it BGR data and lie that it&#39;s RGB, the code will produce the same result as it would
given actual RGB data.</li>
<li>Similarly, it doesn&#39;t matter for resizing which array dimension represents width, and which is height.</li>
</ul>
<p>Now, let&#39;s take another look at the memory representation of pygame&#39;s BGRA array of shape <code>(width, height)</code>.</p>
<p><img alt="SDL-layout.png" height="178" src="https://yosefk.com/img/numpy-perf/SDL-layout.png" width="576"/></p>
<p>This representation is actually the same as an RGBA array of shape <code>(height, width)</code> with numpy&#39;s default strides!
I mean, not really - if we reinterpret this data as an RGBA array, we&#39;re treating red channel values as blue and vice versa.
Likewise, if we reinterpret this data as a <code>(height, width)</code> array with numpy&#39;s default strides, we&#39;re implicitly
transposing the image. But resizing wouldn&#39;t care!</p>
<p>And, as an added bonus, we&#39;d get a single RGBA array, and resize it with one call to cv2.resize, instead of resizing pixels3d
and pixels_alpha separately. Yay!</p>
<p>Here&#39;s code taking a pygame surface and returning the base pointer of the underlying RGBA or BGRA array, and a flag telling
if it&#39;s BGR or RGB:</p>
<pre>import ctypes

def arr_params(surface):
    pixels = pg.surfarray.pixels3d(surface)
    width, height, depth = pixels.shape
    assert depth == 3
    xstride, ystride, zstride = pixels.strides
    oft = 0
    bgr = 0
    if <b>zstride == -1</b>: <i># BGR image - walk back
        # 2 bytes to get to the first blue pixel</i>
        <b>oft = -2</b>
        zstride = 1
        bgr = 1
    <i># validate our assumptions about the data layout</i>
    assert xstride == 4
    assert zstride == 1
    assert ystride == width*4
    base = <b>pixels.ctypes.data_as</b>(ctypes.c_void_p)
    ptr = ctypes.c_void_p(base.value + oft)
    return ptr, width, height, bgr
</pre>
<p>Now that we have the underlying C pointer to the pixel data, we can wrap it in a numpy array with the default strides,
implicitly transposing the image and swapping the R &amp; B channels. <strong>And once we &#34;attach&#34; a numpy array with default
strides to both the input and the output data, our call to cv2.resize will run 100x faster!</strong></p>
<pre>def rgba_buffer(p, w, h):
    <i># attach a WxHx4 buffer to the base pointer</i>
    type = ctypes.c_uint8 * (w * h * 4)
    return <b>ctypes.cast(p, ctypes.POINTER(type)).contents</b>

def <b>cv2_resize_surface</b>(src, dst):
    iptr, iw, ih, ibgr = arr_params(src)
    optr, ow, oh, obgr = arr_params(dst)

    <i># our trick only works if both surfaces are BGR,
    # or they&#39;re both RGB. if their layout doesn&#39;t match,
    # our code would actually swap R &amp; B channels</i>
    <b>assert ibgr == obgr</b>

    ibuf = rgba_buffer(iptr, iw, ih)

    <i># numpy&#39;s default strides are height*4, 4, 1</i>
    iarr = np.ndarray(<b>(ih,iw,4)</b>, np.uint8, <b>buffer=ibuf</b>)
    
    obuf = rgba_buffer(optr, ow, oh)

    oarr = np.ndarray(<b>(oh,ow,4)</b>, np.uint8, <b>buffer=obuf</b>)

    <b>cv2.resize</b>(iarr, (ow,oh), oarr, interpolation=cv2.INTER_AREA)
</pre>
<p>Sure enough, we finally get a speedup instead of a slowdown from using cv2.resize on Surface data, and we&#39;re as fast as
resizing an RGBA numpy.zeros array (where originally we benchmarked an <em>RGB</em> array, not RGBA):</p>
<pre>osurf = pg.Surface((OW,OH), pg.SRCALPHA)
with Timer(<b><i>&#39;attached RGBA with cv2&#39;</i></b>):
    for i in range(repeat):
        <b>cv2_resize_surface</b>(isurf, osurf)

i6 = np.zeros((IW,IH,4), np.uint8)
with Timer(<b><i>&#39;np.zeros RGBA with cv2&#39;</i></b>):
    for i in range(repeat):
        o6 = <b>cv2_resize</b>(i6) 
</pre>
<p>The benchmark says we got our 100x back:</p>
<pre>attached RGBA with cv2 took <b>0.0097</b> sec
np.zeros RGBA with cv2 took <b>0.0066</b> sec
</pre>
<p>All of the ugly code above is <a href="https://github.com/yosefk/BlogCodeSamples/blob/main/numpy-perf.py">on GitHub</a>.
Since this code is ugly, you can&#39;t be sure it actually resizes the image correctly, so there&#39;s some more code over there that
tests resizing on non-zero images. If you run it, you will get the following gorgeous output image:</p>
<p><img alt="resized.png" height="540" src="https://yosefk.com/img/numpy-perf/resized.png" width="960"/></p>
<p>Did we really get a 100x speedup? It depends on how you count. We got cv2.resize to run 100x faster relatively to calling it
straightforwardly with the pixel3d array. But specifically for resizing, pygame has smoothscale, and our speedup relatively to
it is 13-15x. There are some more benchmarks on GitHub for functions other than resize, some of which don&#39;t have a corresponding
pygame API:</p>
<ul>
<li>Copying with <code>dst[:] = src</code>: <strong>28x</strong></li>
<li>Inverting with <code>dst[:] = 255 - src</code>: <strong>24x</strong></li>
<li><code>cv2.warpAffine</code>: <strong>12x</strong></li>
<li><code>cv2.medianBlur</code>: <strong>15x</strong></li>
<li><code>cv2.GaussianBlur</code>: <strong>200x</strong></li>
</ul>
<p>So not &#34;exactly&#34; 100x, though I feel it&#39;s fair enough to call it &#34;100x&#34; for short.</p>
<p>In any case, I&#39;d be surprised if that many people use SDL from Python for this specific issue to be broadly relevant. But I&#39;d
guess that numpy arrays with weird layouts come up in other places, too, so this kind of trick might be relevant elsewhere.</p>
<h2 id="unsafe-python">&#34;Unsafe Python&#34;</h2>
<p>The code above uses &#34;the C kind of knowledge&#34; to get a speedup (Python generally hides data layout from you, whereas C
proudly exposes it.) It also, unfortunately, has the memory (un)safety of C - we get a C base pointer to the pixel data, and
from that point on, if we mess up the pointer arithmetic, or use the data after it was already freed, we&#39;re going to crash or
corrupt data. And yet we wrote no C code - it&#39;s all Python.</p>
<p><a href="https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html">Rust has an &#34;unsafe&#34; keyword</a> where the compiler forces
you to realize that you&#39;re calling an API which voids the normal safety guarantees. But the Rust compiler <em>doesn&#39;t</em> make
you mark your function as &#34;unsafe&#34; just because you have an unsafe block in that function. Rather, it trusts <em>you</em> to
decide whether your function is itself unsafe or not.</p>
<p>(In our example, <code>cv2_resize_surface</code> is a safe API, assuming I don&#39;t have a bug, because none of the horror
escapes into the outside world - outside, we just see that the output surface was filled with the output data. But
<code>arr_params</code> is a completely unsafe API, since it returns a C pointer that you can do anything with. And
<code>rgba_buffer</code> is <em>also</em> unsafe - although we return a numpy array, a &#34;safe&#34; object, nothing prevents you from
using it after the data was freed, for example. In the general case, no static analysis can tell whether you&#39;ve built something
safe from unsafe building blocks or not.)</p>
<p>Python doesn&#39;t have an <code>unsafe</code> keyword - which is in character for a dynamic language with sparse static
annotation. But otherwise, Python + <code>ctypes</code> + C libraries is sort of similar in spirit to Rust with
<code>unsafe</code>. The language is safe by default, but you have your escape hatch when you need it.</p>
<p>&#34;Unsafe Python&#34; exemplifies a general principle: <strong>there&#39;s <em>a lot</em> of C in Python</strong>. C is Python&#39;s evil
twin, or, in chronological order, Python is C&#39;s good-natured twin. C gives you performance, and doesn&#39;t care about usability or
safety; if any of the footguns go off, tell it to your healthcare provider, C isn&#39;t interested. Python on the other hand gives
you safety, and it&#39;s based on <a href="https://en.wikipedia.org/wiki/ABC_(programming_language)">a decade&#39;s worth of
research</a> into usability for beginners. It doesn&#39;t, however, care about performance. They&#39;re both optimized aggressively for
two opposite goals, at the cost of ignoring the other&#39;s goals.</p>
<p>But on top of that, Python was built with C extensions in mind from the start. Today, from my point of view, <em>Python
functions as a packaging system</em> for popular C/C++ libraries. I have way less appetite for downloading and building OpenCV
to use it from C++ than <code>pip install</code>ing OpenCV binaries and using them from Python, because C++ doesn&#39;t have a
standard package management system, and Python does. There are a lot of high-performance libraries (for instance in scientific
computing and deep learning) with more code calling them in Python than in C/C++. And on the other hand, if you want seriously
optimized Python code and a small deployment footprint / low startup time, you&#39;d use <a href="https://cython.org/">Cython</a> to
produce an extension &#34;as if written in C&#34; to spare the overhead of an otherwise &#34;more Pythonic&#34; JIT-based system like <a href="https://numba.pydata.org/">numba</a>.</p>
<p>Not only is there a lot of C in Python, but, being opposites of sorts, they complement each other fairly well. A good way to
make Python code fast is using C libraries in the right ways. Conversely, a good way to use C safely is to write the core in C
and a lot of the logic on top of it in Python. The Python &amp; C/C++/Rust mix - either a C program with a massive Python
extension API, or a Python program with all of the heavy lifting done in C - seems quite dominant in high-performance, numeric,
desktop / server areas. And while I&#39;m not sure this fact is very inspiring, I think it&#39;s a fact<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>, and things will stay this way for a long time.</p>
<p><em>Thanks to Dan Luu for reviewing a draft of this post.</em></p>
</div></div>
  </body>
</html>
