<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">Original</a>
    <h1>A Visual Exploration of Gaussian Processes (2019)</h1>
    
    <div id="readability-page-1" class="page"><div>
  <p>
    Even if you have spent some time reading about machine learning, chances are that you have never heard of Gaussian processes.
    And if you have, rehearsing the basics is always a good way to refresh your memory.
    With this blog post we want to give an introduction to Gaussian processes and make the mathematical intuition behind them more approachable.
  </p>
  <p>
    Gaussian processes are a powerful tool in the machine learning toolbox<d-cite key="Rasmussen2004"></d-cite>.
    They allow us to make predictions about our data by incorporating prior knowledge.
    Their most obvious area of application is <i>fitting</i> a function to the data.
    This is called regression and is used, for example, in robotics or time series forecasting.
    But Gaussian processes are not limited to regression — they can also be extended to classification and clustering tasks<d-cite key="Kapoor2010"></d-cite><d-cite key="Kim2007"></d-cite>.

    For a given set of training points, there are potentially infinitely many functions that fit the data.
    Gaussian processes offer an elegant solution to this problem by assigning a probability to each of these functions<d-cite key="Rasmussen2004"></d-cite>.
    The mean of this probability distribution then represents the most probable characterization of the data.
    Furthermore, using a probabilistic approach allows us to incorporate the confidence of the prediction into the regression result.
  </p>

  <p>
    We will first explore the mathematical foundation that Gaussian processes are built on — we invite you to follow along using the interactive figures and hands-on examples.
    They help to explain the impact of individual components, and show the flexibility of Gaussian processes.
    After following this article we hope that you will have a visual intuition on how Gaussian processes work and how you can configure them for different types of data.
  </p>

    <h2>Multivariate Gaussian distributions</h2>
  <p>
    Before we can explore Gaussian processes, we need to understand the mathematical concepts they are based on.
    As the name suggests, the Gaussian distribution (which is often also referred to as <i>normal</i> distribution) is the basic building block of Gaussian processes.
    In particular, we are interested in the multivariate case of this distribution, where each random variable is distributed normally and their joint distribution is also Gaussian.
    The multivariate Gaussian distribution is defined by a mean vector <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> and a covariance matrix <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span>.
    You can see an interactive example of such distributions in <a href="#Multivariate">the figure below</a>.
  </p>

  <p>
    The mean vector <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> describes the expected value of the distribution.
    Each of its components describes the mean of the corresponding dimension.
    <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span> models the variance along each dimension and determines how the different random variables are correlated.
    The covariance matrix is always symmetric and positive semi-definite<d-cite key="Hazewinkel1994"></d-cite>.
    The diagonal of <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span> consists of the variance <span><span><span><math><semantics><mrow><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_i^2</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>σ</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> of the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th random variable.
    And the off-diagonal elements <span><span><span><math><semantics><mrow><msub><mi>σ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\sigma_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>σ</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> describe the correlation between the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th and <span><span><span><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>j</span></span></span></span></span>-th random variable.
  </p>

    <span><span><span><span><math><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>X</mi><mn>1</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>X</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">⋮</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>X</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mi>μ</mi><mo separator="true">,</mo><mi mathvariant="normal">Σ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">
        X = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma)
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span><span>=</span><span><span><span><span><span><span><span><span></span><span><span>⎣</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎡</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span><span><span><span><span><span><span></span><span><span><span>X</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span><span><span></span><span><span><span>X</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span><span><span></span><span><span>⋮</span></span></span><span><span></span><span><span><span>X</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span><span>⎦</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎤</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span>∼</span><span><span>N</span></span><span>(</span><span>μ</span><span>,</span><span>Σ</span><span>)</span></span></span></span></span></span>

    <p>We say <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span> follows a normal distribution. The covariance matrix <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span> describes the shape of the distribution. It is defined in terms of the expected value <span><span><span><math><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>E</span></span></span></span></span>:</p>

    <span><span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mtext>Cov</mtext><mo>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>X</mi><mi>j</mi></msub><mo>)</mo><mo>=</mo><mi>E</mi><mrow><mo fence="true">[</mo><mo>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>i</mi></msub><mo>)</mo><mo>(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>j</mi></msub><msup><mo>)</mo><mi>T</mi></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">
      \Sigma = \text{Cov}(X_i, X_j) = E \left[ (X_i - \mu_i)(X_j - \mu_j)^T \right]
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span><span>=</span><span><span>Cov</span></span><span>(</span><span><span>X</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>,</span><span><span>X</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>)</span><span>=</span><span>E</span><span><span><span>[</span></span><span>(</span><span><span>X</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>−</span><span><span>μ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>)</span><span>(</span><span><span>X</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>−</span><span><span>μ</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span><span><span>]</span></span></span></span></span></span></span></span>

    <p>Visually, the distribution is centered around the mean and the covariance matrix defines its shape. The <a href="#Multivariate">following figure</a> shows the influence of these parameters on a two-dimensional Gaussian distribution. The variances for each random variable are on the diagonal of the covariance matrix, while the other values show the covariance between them.</p>

    <figure>
    <d-figure id="Multivariate"></d-figure>
    </figure>
    
    <p>
      Gaussian distributions are widely used to model the real world.
      For example, we can employ them to describe errors of measurements or phenomena under the assumptions of the <i>central limit theorem</i>
      <d-footnote>
        One of the implications of this theorem is that a collection of independent, identically distributed random variables with finite variance have a mean that is distributed normally.
        A good introduction to the central limit theorem is given by <a href="https://www.khanacademy.org/math/ap-statistics/sampling-distribution-ap/sampling-distribution-mean/v/central-limit-theorem" target="_blank">this video</a> from <a href="https://www.khanacademy.org" target="_blank">Khan Academy</a>.
      </d-footnote>.
      In the next section we will take a closer look at how to manipulate Gaussian distributions and extract useful information from them.
    </p>

    <h3 id="MargCond">Marginalization and Conditioning</h3>
  <p>
    Gaussian distributions have the nice algebraic property of being closed under conditioning and marginalization.
    Being closed under conditioning and marginalization means that the resulting distributions from these operations are also Gaussian, which makes many problems in statistics and machine learning tractable.
    In the following we will take a closer look at both of these operations, as they are the foundation for Gaussian processes.
  </p>

    <p><i>Marginalization</i> and <i>conditioning</i> both work on subsets of the original distribution and we will use the following notation:</p>

    <span><span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>X</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>Y</mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mi>μ</mi><mo separator="true">,</mo><mi mathvariant="normal">Σ</mi><mo>)</mo><mo>=</mo><mrow><mi mathvariant="script">N</mi></mrow><mrow><mo fence="true">(</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>μ</mi><mi>X</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>μ</mi><mi>Y</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mspace width="0.16667em"></mspace><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>X</mi></mrow></msub><mspace width="0.16667em"></mspace><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>Y</mi></mrow></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{X,Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma) = \mathcal{N} \left( \begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} \, \Sigma_{XY} \\ \Sigma_{YX} \, \Sigma_{YY} \end{bmatrix} \right)</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>,</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>=</span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span>X</span></span></span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span><span>]</span></span></span><span>∼</span><span><span>N</span></span><span>(</span><span>μ</span><span>,</span><span>Σ</span><span>)</span><span>=</span><span><span>N</span></span><span><span><span>(</span></span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span><span>μ</span><span><span><span><span><span><span></span><span><span>X</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span><span><span></span><span><span><span>μ</span><span><span><span><span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span><span>]</span></span></span><span>,</span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span></span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span><span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span></span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span><span>]</span></span></span><span><span>)</span></span></span></span></span></span></span></span>

    <p>With <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span> and <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span> representing subsets of original random variables.</p>

    <p>Through <i>marginalization</i> we can extract partial information from multivariate probability distributions. In particular, given a normal probability distribution <span><span><span><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>P</span><span>(</span><span>X</span><span>,</span><span>Y</span><span>)</span></span></span></span></span> over vectors of random variables <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span> and <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span>, we can determine their marginalized probability distributions in the following way:</p>

    <span><span><span><span><math><semantics><mrow><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>X</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><msub><mi>μ</mi><mi>X</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>Y</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><msub><mi>μ</mi><mi>Y</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>Y</mi></mrow></msub><mo>)</mo></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">
      \begin{aligned}
        X &amp;\sim \mathcal{N}(\mu_X, \Sigma_{XX}) \\
        Y &amp;\sim \mathcal{N}(\mu_Y, \Sigma_{YY})
      \end{aligned}
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span><span><span><span><span><span><span></span><span><span>X</span></span></span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span>∼</span><span><span>N</span></span><span>(</span><span><span>μ</span><span><span><span><span><span><span></span><span><span>X</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>,</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>)</span></span></span><span><span></span><span><span></span><span>∼</span><span><span>N</span></span><span>(</span><span><span>μ</span><span><span><span><span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>,</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>)</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span></span></span> 

    <p>
      The interpretation of this equation is that each partition <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span> and <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span> only depends on its corresponding entries in <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> and <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span>.
      To marginalize out a random variable from a Gaussian distribution we can simply drop the variables from <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> and <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span>.
    </p>

    <span><span><span><span><math><semantics><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msub><mo>∫</mo><mi>y</mi></msub><msub><mi>p</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo>)</mo><mi>d</mi><mi>y</mi><mo>=</mo><msub><mo>∫</mo><mi>y</mi></msub><msub><mi>p</mi><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></msub><mo>(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>y</mi><mo>)</mo><msub><mi>p</mi><mi>Y</mi></msub><mo>(</mo><mi>y</mi><mo>)</mo><mi>d</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">
        p_X(x) = \int_y p_{X,Y}(x,y)dy = \int_y p_{X|Y}(x|y) p_Y(y) dy
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>p</span><span><span><span><span><span><span></span><span><span>X</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>x</span><span>)</span><span>=</span><span><span>∫</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>X</span><span>,</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>x</span><span>,</span><span>y</span><span>)</span><span>d</span><span>y</span><span>=</span><span><span>∫</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>X</span><span>∣</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>x</span><span>∣</span><span>y</span><span>)</span><span><span>p</span><span><span><span><span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>y</span><span>)</span><span>d</span><span>y</span></span></span></span></span></span>

  <p>The way to interpret this equation is that if we are interested in the probability density of
    <span><span><span><math><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span><span>=</span><span>x</span></span></span></span></span>, we need to consider all possible outcomes of
    <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span> that can jointly lead to the result
    <d-footnote>The corresponding <a href="https://en.wikipedia.org/wiki/Marginal_distribution" target="_blank">Wikipedia
      article</a> has a good description of the marginal distribution, including several examples.
    </d-footnote>.
  </p>

  <p>
  Another important operation for Gaussian processes is <i>conditioning</i>.
  It is used to determine the probability of one variable depending on another variable.
  Similar to marginalization, this operation is also closed and yields a modified Gaussian distribution.
  This operation is the cornerstone of Gaussian processes since it allows Bayesian inference, which we will talk about in the <a href="#GaussianProcesses">next section</a>.
  Conditioning is defined by:
  </p>

    <span><span><span><span><math><semantics><mrow><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mspace width="0.22222em"></mspace><msub><mi>μ</mi><mi>X</mi></msub><mo>+</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>Y</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>(</mo><mi>Y</mi><mo>−</mo><msub><mi>μ</mi><mi>Y</mi></msub><mo>)</mo><mo separator="true">,</mo><mspace width="0.22222em"></mspace><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub><mo>−</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>Y</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>X</mi></mrow></msub><mspace width="0.22222em"></mspace><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mspace width="0.22222em"></mspace><msub><mi>μ</mi><mi>Y</mi></msub><mo>+</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>X</mi></mrow></msub><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>X</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>(</mo><mi>X</mi><mo>−</mo><msub><mi>μ</mi><mi>X</mi></msub><mo>)</mo><mo separator="true">,</mo><mspace width="0.22222em"></mspace><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>Y</mi></mrow></msub><mo>−</mo><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>X</mi></mrow></msub><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>X</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mspace width="0.22222em"></mspace><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">
      \begin{aligned}
      X|Y &amp;\sim \mathcal{N}(\:\mu_X + \Sigma_{XY}\Sigma_{YY}^{-1}(Y - \mu_Y),\: \Sigma_{XX}-\Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}\:) \\
      Y|X &amp;\sim \mathcal{N}(\:\mu_Y + \Sigma_{YX}\Sigma_{XX}^{-1}(X - \mu_X),\: \Sigma_{YY}-\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\:) \\
      \end{aligned}
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span><span><span><span><span><span><span></span><span><span>X</span><span>∣</span><span>Y</span></span></span><span><span></span><span><span>Y</span><span>∣</span><span>X</span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span>∼</span><span><span>N</span></span><span>(</span><span><span></span><span>μ</span><span><span><span><span><span><span></span><span><span>X</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>+</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>Y</span></span></span></span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>Y</span><span>−</span><span><span>μ</span><span><span><span><span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>)</span><span>,</span><span><span></span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>−</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>Y</span></span></span></span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span></span><span>)</span></span></span></span><span><span></span><span><span></span><span>∼</span><span><span>N</span></span><span>(</span><span><span></span><span>μ</span><span><span><span><span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>+</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>X</span></span></span></span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>X</span><span>−</span><span><span>μ</span><span><span><span><span><span><span></span><span><span>X</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>)</span><span>,</span><span><span></span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>−</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>X</span></span></span></span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span><span></span><span>)</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span></span></span>

    <p>
    Note that the new mean only depends on the conditioned variable, while the covariance matrix is independent from this variable.
    </p>

    <p>
    Now that we have worked through the necessary equations, we will think about how we can understand the two operations visually.
    While marginalization and conditioning can be applied to multivariate distributions of many dimensions, it makes sense to consider the two-dimensional case as shown in the <a href="#MarginalizationConditioning">following figure</a>.
    Marginalization can be seen as integrating along one of the dimensions of the Gaussian distribution, which is in line with the general definition of the marginal distribution.
    Conditioning also has a nice geometric interpretation — we can imagine it as making a cut through the multivariate distribution, yielding a new Gaussian distribution with fewer dimensions.
    </p>

  <figure>
    <d-figure id="MarginalizationConditioning"></d-figure>
    <figcaption>
      A bivariate normal distribution in the center.
      On the left you can see the result of marginalizing this distribution for Y, akin to integrating along the X axis. On the right you can see the distribution conditioned on a given X, which is similar to a cut through the original distribution. The Gaussian distribution and the conditioned variable can be changed by dragging the handles.</figcaption>
    </figure>

    
    <h2 id="GaussianProcesses">Gaussian Processes</h2>
    <p>
      Now that we have recalled some of the basic properties of multivariate Gaussian distributions, we will combine them together to define Gaussian processes, and show how they can be used to tackle regression problems.
    </p>

    <p>
      First, we will move from the continuous view to the discrete representation of a function:
      rather than finding an implicit function, we are interested in predicting the function values at concrete points, which we call <i>test points</i> <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span>.
      So how do we derive this functional view from the multivariate normal distributions that we have considered so far?
      Stochastic processes, such as Gaussian processes, are essentially a set of random variables.
      In addition, each of these random variables has a corresponding index <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>.
      We will use this index to refer to the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th dimension of our <span><span><span><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>n</span></span></span></span></span>-dimensional multivariate distributions. 
      The <a href="#DimensionSwap">following figure</a> shows an example of this for two dimensions:
    </p>

    <figure>
      <d-figure id="DimensionSwap"></d-figure>
      <figcaption>
        Here, we have a two-dimensional normal distribution.
        Each dimension <span><span><span><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>x</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> is assigned an index <span><span><span><math><semantics><mrow><mi>i</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo>}</mo></mrow><annotation encoding="application/x-tex">i \in \{1,2\}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span><span>∈</span><span>{</span><span>1</span><span>,</span><span>2</span><span>}</span></span></span></span></span>.
        You can drag the handles to see how a particular sample (left) corresponds to functional values (right).
        This representation also allows us to understand the connection between the covariance and the resulting values:
        the underlying Gaussian distribution has a positive covariance between <span><span><span><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>x</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><span><math><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>x</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> — this means that <span><span><span><math><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>x</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> will increases as <span><span><span><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>x</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> gets larger and vice versa.
        You can also drag the handles in the figure to the right and observe the probability of such a configuration in the figure to the left.
      </figcaption>
    </figure>

    <p>
      Now, the goal of Gaussian processes is to learn this underlying distribution from <i>training data</i>.
      Respective to the test data <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span>, we will denote the training data as <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span>.
      As we have mentioned before, the key idea of Gaussian processes is to model the underlying distribution of <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span> together with <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span> as a multivariate normal distribution.
      That means that the joint probability distribution <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X,Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>,</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> spans the space of possible function values for the function that we want to predict.
      Please note that this joint distribution of test and training data has <span><span><span><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo>+</mo><mi mathvariant="normal">∣</mi><mi>Y</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|X| + |Y|</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>∣</span><span>X</span><span>∣</span><span>+</span><span>∣</span><span>Y</span><span>∣</span></span></span></span></span> dimensions.
    </p>

    <p>
      In order to perform regression on the training data, we will treat this problem as <i>Bayesian inference</i>.
      The essential idea of Bayesian inference is to update the current hypothesis as new information becomes available. 
      In the case of Gaussian processes, this information is the training data.
      Thus, we are interested in the conditional probability <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X|Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>∣</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span>.  
      Finally, we recall that Gaussian distributions are closed under conditioning — so <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X|Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>∣</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> is also distributed normally.
    </p>

    <p>
      Now that we have the basic framework of Gaussian processes together, there is only one thing missing:
      how do we set up this distribution and define the mean <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> and the covariance matrix <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span>?
      The covariance matrix <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span> is determined by its <i>covariance function</i> <span><span><span><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>k</span></span></span></span></span>, which is often also called the <i>kernel</i> of the Gaussian process.
      We will talk about this in detail in the next section.
      But before we come to this, let us reflect on how we can use multivariate Gaussian distributions to estimate function values.
      The <a href="#PriorFigure">following figure</a> shows an example of this using ten test points at which we want to predict our function:
    </p>

    <figure>
    <d-figure id="PriorFigure"></d-figure>
    </figure>

    <p>
      In Gaussian processes we treat each test point as a random variable. 
      A multivariate Gaussian distribution has the same number of dimensions as the number of random variables.
      Since we want to predict the function values at
      <span><span><span><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo>=</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">|X|=N</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>∣</span><span>X</span><span>∣</span><span>=</span><span>N</span></span></span></span></span>
      test points, the corresponding multivariate Gaussian distribution is also
      <span><span><span><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>N</span></span></span></span></span>
      -dimensional.
      Making a prediction using a Gaussian process ultimately boils down to drawing samples from this distribution. 
      We then interpret the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th component of the resulting vector as the function value corresponding to the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th test point.
    </p>

    <h3 id="Kernels">Kernels</h3>
    <p>
      Recall that in order to set up our distribution, we need to define <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> and <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span>.
      In Gaussian processes it is often assumed that <span><span><span><math><semantics><mrow><mi>μ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu = 0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span><span>=</span><span>0</span></span></span></span></span>, which simplifies the necessary equations for conditioning.
      We can always assume such a distribution, even if <span><span><span><math><semantics><mrow><mi>μ</mi><mo>≠</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu \neq 0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span><span>≠</span><span>0</span></span></span></span></span>, and add <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> back to the resulting function values after the prediction step.
      This process is also called <i>centering</i> of the data.
      So configuring <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span> is straight forward — it gets more interesting when we look at the other parameter of the distribution.
    </p>

    <p>
      The clever step of Gaussian processes is how we set up the covariance matrix <span><span><span><math><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Σ</span></span></span></span></span>.
      The covariance matrix will not only describe the shape of our distribution, but ultimately determines the characteristics of the function that we want to predict.
      We generate the covariance matrix by evaluating the kernel <span><span><span><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>k</span></span></span></span></span>, which is often also called <i>covariance function</i>, pairwise on all the points.
      The kernel receives two points <span><span><span><math><semantics><mrow><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">t,t’ \in \mathbb{R}^n</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>t</span><span>,</span><span><span>t</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>∈</span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span>n</span></span></span></span></span></span></span></span></span></span></span></span> as an input and returns a similarity measure between those points in the form of a scalar:
    </p>

    <span><span><span><span><math><semantics><mrow><mi>k</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>×</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>→</mo><mrow><mi mathvariant="double-struck">R</mi></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mi mathvariant="normal">Σ</mi><mo>=</mo><mtext>Cov</mtext><mo>(</mo><mi>X</mi><mo separator="true">,</mo><msup><mi>X</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo><mo>=</mo><mi>k</mi><mo>(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">
      k: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R},\quad 
      \Sigma = \text{Cov}(X,X’) = k(t,t’)
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>k</span><span>:</span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span>n</span></span></span></span></span></span></span></span><span>×</span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span>n</span></span></span></span></span></span></span></span><span>→</span><span><span>R</span></span><span>,</span><span><span></span><span>Σ</span></span><span>=</span><span><span>Cov</span></span><span>(</span><span>X</span><span>,</span><span><span>X</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>)</span><span>=</span><span>k</span><span>(</span><span>t</span><span>,</span><span><span>t</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>)</span></span></span></span></span></span>

    <p>
      We evaluate this function for each pairwise combination of the test points to retrieve the covariance matrix.
      This step is also depicted in the <a href="#PriorFigure">figure above</a>.
      In order to get a better intuition for the role of the kernel, let’s think about what the entries in the covariance matrix describe.
      The entry <span><span><span><math><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Sigma_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> describes how much influence the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th and <span><span><span><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>j</span></span></span></span></span>-th point have on each other.
      This follows from the definition of the multivariate Gaussian distribution, which states that <span><span><span><math><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Sigma_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> defines the correlation between the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th and the <span><span><span><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>j</span></span></span></span></span>-th random variable.
      Since the kernel describes the similarity between the values of our function, it controls the possible shape that a fitted function can adopt.
      Note that when we choose a kernel, we need to make sure that the resulting matrix adheres to the properties of a covariance matrix.
    </p> 

    <p>
      Kernels are widely used in machine learning, for example in <i>support vector machines</i><d-cite key="Vapnik2000"></d-cite>.
      The reason for this is that they allow similarity measures that go far beyond the standard euclidean distance (<span><span><span><math><semantics><mrow><mi>L</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">L2</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>L</span><span>2</span></span></span></span></span>-distance).
      Many of these kernels conceptually embed the input points into a higher dimensional space in which they then measure the similarity<d-footnote>If the kernel follows Mercer’s theorem it can be used to define a Hilbert space. More information on this can be found on <a href="https://en.wikipedia.org/wiki/Kernel_method">Wikipedia</a>.</d-footnote>.
      The <a href="#MultipleKernels">following figure</a> shows examples of some common kernels for Gaussian processes.
      For each kernel, the covariance matrix has been created from <span><span><span><math><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">N=25</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>N</span><span>=</span><span>2</span><span>5</span></span></span></span></span> linearly-spaced values ranging from <span><span><span><math><semantics><mrow><mo>[</mo><mo>−</mo><mn>5</mn><mo separator="true">,</mo><mn>5</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">[-5,5]</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>[</span><span>−</span><span>5</span><span>,</span><span>5</span><span>]</span></span></span></span></span>. Each entry in the matrix shows the covariance between points in the range of <span><span><span><math><semantics><mrow><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>[</span><span>0</span><span>,</span><span>1</span><span>]</span></span></span></span></span>.
    </p>

  <figure>
    <d-figure id="MultipleKernels">
    </d-figure>
    <figcaption>This figure shows various kernels that can be used with Gaussian processes. Each kernel has different
      parameters, which can be changed by adjusting the according sliders. When grabbing a slider,
      information on how the current parameter influences the kernel will be shown on the right.
    </figcaption>
    </figure>

  <p> Kernels can be separated into <i>stationary</i> and <i>non-stationary</i> kernels. <i>Stationary</i> kernels, such
    as the RBF kernel or the periodic kernel, are functions invariant to translations, and the covariance of two points is only
    dependent on their relative position. <i>Non-stationary</i> kernels, such as the linear kernel, do not have this
    constraint and depend on an absolute location. The stationary nature of the RBF kernel can be observed in the
    banding around the diagonal of its covariance matrix (as shown in <a href="#MultipleKernels">this figure</a>). Increasing the length parameter increases the banding, as
    points further away from each other become more correlated. For the periodic kernel, we have an additional parameter
    <span><span><span><math><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>P</span></span></span></span></span> that determines the periodicity, which controls the distance between each repetition of the function.
    In contrast, the parameter <span><span><span><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>C</span></span></span></span></span> of the linear kernel allows us to change the point on which all functions hinge.
  </p>

  <p>
    There are many more kernels that can describe different classes of functions, which can be used to model the desired shape of the function.
    A good overview of different kernels is given by Duvenaud<d-cite key="Duvenaud2014"></d-cite>.
    It is also possible to combine several kernels — but we will get to this later.
  </p>

    <h3>Prior Distribution</h3>
    <p>
      We will now shift our focus back to the original task of regression.
      As we have mentioned earlier, Gaussian processes define a probability distribution over possible functions.
      In <a href="#DimensionSwap">this figure above</a>, we show this connection:
      each sample of our multivariate normal distribution represents one realization of our function values.
      Because this distribution is a multivariate Gaussian distribution, the distribution of functions is normal.
      Recall that we usually assume <span><span><span><math><semantics><mrow><mi>μ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu=0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span><span>=</span><span>0</span></span></span></span></span>.
      For now, let’s consider the case where we have not yet observed any training data.
      In the context of Bayesian inference, this is called the <i>prior</i> distribution <span><span><span><math><semantics><mrow><msub><mi>P</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">P_X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span>X</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span>.
    </p>

    <p>
      If we have not yet observed any training examples, this distribution revolves around <span><span><span><math><semantics><mrow><mi>μ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu=0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span><span>=</span><span>0</span></span></span></span></span>, according to our original assumption.
      The prior distribution will have the same dimensionality as the number of test points <span><span><span><math><semantics><mrow><mi>N</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">N = |X|</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>N</span><span>=</span><span>∣</span><span>X</span><span>∣</span></span></span></span></span>.
      We will use the kernel to set up the covariance matrix, which has the dimensions <span><span><span><math><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \times N</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>N</span><span>×</span><span>N</span></span></span></span></span>.
    </p>

    <p>
      In the previous section we have looked at examples of different kernels.
      The kernel is used to define the entries of the covariance matrix.
      Consequently, the covariance matrix determines which type of functions from the space of all possible functions are more probable.
      As the prior distribution does not yet contain any additional information, it is perfect to visualize the influence of the kernel on the distribution of functions.
      The <a href="#Prior">following figure</a> shows samples of potential functions from prior distributions that were created using different kernels:
    </p>

    <figure>
    <d-figure id="Prior">
    </d-figure>
      <figcaption> Clicking on the graph results in continuous <span>samples</span> drawn from a
        Gaussian process using the selected
        kernel. After each draw, the previous sample fades into the background. Over time, it is possible to see that
        functions are distributed normally around the <span>mean µ </span>.
      </figcaption>
    </figure>

  <p>
    Adjusting the parameters allows you to control the shape of the resulting functions. 
    This also varies the confidence of the prediction.
    When decreasing the variance <span><span><span><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>σ</span></span></span></span></span>, a common parameter for all kernels, sampled functions are more concentrated around the mean <span><span><span><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span></span></span></span></span>.
    For the <i>Linear</i> kernel, setting the variance <span><span><span><math><semantics><mrow><msub><mi>σ</mi><mi>b</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sigma_b=0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>σ</span><span><span><span><span><span><span></span><span><span>b</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>=</span><span>0</span></span></span></span></span> results in a set of functions constrained to perfectly intersect the offset point <span><span><span><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>c</span></span></span></span></span>.
    If we set <span><span><span><math><semantics><mrow><msub><mi>σ</mi><mi>b</mi></msub><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\sigma_b=0.2</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>σ</span><span><span><span><span><span><span></span><span><span>b</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>=</span><span>0</span><span>.</span><span>2</span></span></span></span></span> we can model uncertainty, resulting in functions that pass close to <span><span><span><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>c</span></span></span></span></span>.
  </p>

    <h3>Posterior Distribution</h3>
   
    <p>
      So what happens if we observe training data?
      Let’s get back to the model of Bayesian inference, which states that we can incorporate this additional information into our model, yielding the <i>posterior</i> distribution <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X|Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>∣</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span>.
      We will now take a closer look at how to do this for Gaussian processes.
    </p>

    <p>
      First, we form the joint distribution <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X,Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>,</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> between the test points <span><span><span><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span></span></span></span></span> and the training points <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span>.
      The result is a multivariate Gaussian distribution with dimensions <span><span><span><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi>Y</mi><mi mathvariant="normal">∣</mi><mo>+</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|Y| + |X|</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>∣</span><span>Y</span><span>∣</span><span>+</span><span>∣</span><span>X</span><span>∣</span></span></span></span></span>.
      As you can see in the <a href="#PosteriorFigure">figure below</a>, we concatenate the training and the test points to compute the corresponding covariance matrix.
    </p>

    <p>
      For the next step we need one operation on Gaussian distributions that we have defined earlier.
      Using <i>conditioning</i> we can find <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X|Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>∣</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> from <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X,Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>,</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span>.
      The dimensions of this new distribution matches the number of test points <span><span><span><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>N</span></span></span></span></span> and the distribution is also normal.
      It is important to note that conditioning leads to derived versions of the mean and the standard deviation: <span><span><span><math><semantics><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><msup><mi>μ</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msup><mi mathvariant="normal">Σ</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">X|Y \sim \mathcal{N}(\mu’, \Sigma’)</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>X</span><span>∣</span><span>Y</span><span>∼</span><span><span>N</span></span><span>(</span><span><span>μ</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>,</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>)</span></span></span></span></span>. 
      More details can be found in the <a href="#MargCond">related section</a> on conditioning multivariate Gaussian distributions.
      The intuition behind this step is that the training points constrain the set of functions to those that pass through the training points.
    </p>

    <figure>
    <d-figure id="PosteriorFigure"></d-figure>
    </figure>

    <p>
       As mentioned before, the conditional distribution <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X|Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>∣</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> forces the set of functions to precisely pass through each training point.
       In many cases this can lead to fitted functions that are unnecessarily complex.
       Also, up until now, we have considered the training points <span><span><span><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span></span></span></span></span> to be perfect measurements.
       But in real-world scenarios this is an unrealistic assumption, since most of our data is afflicted with measurement errors or uncertainty.
       Gaussian processes offer a simple solution to this problem by modeling the error of the measurements.
       For this, we need to add an error term <span><span><span><math><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msup><mi>ψ</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">\epsilon \sim \mathcal{N}(0, \psi^2)</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>ϵ</span><span>∼</span><span><span>N</span></span><span>(</span><span>0</span><span>,</span><span><span>ψ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span></span> to each of our training points:
    </p>
    <span><span><span><span><math><semantics><mrow><mi>Y</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">
       Y = f(X) + \epsilon
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>Y</span><span>=</span><span>f</span><span>(</span><span>X</span><span>)</span><span>+</span><span>ϵ</span></span></span></span></span></span>
    <p>
       We do this by slightly modifying the setup of the joint distribution <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X,Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>,</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span>:
    </p>

    <span><span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo separator="true">,</mo><mi>Y</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>X</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>Y</mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">Σ</mi><mo>)</mo><mo>=</mo><mrow><mi mathvariant="script">N</mi></mrow><mrow><mo fence="true">(</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>X</mi></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>X</mi></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="normal">Σ</mi><mrow><mi>Y</mi><mi>Y</mi></mrow></msub><mo>+</mo><msup><mi>ψ</mi><mn>2</mn></msup><mi>I</mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{X,Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(0, \Sigma) = \mathcal{N} \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} &amp; \Sigma_{XY} \\ \Sigma_{YX} &amp; \Sigma_{YY}+\psi^2I \end{bmatrix} \right)</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>,</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>=</span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span>X</span></span></span><span><span></span><span><span>Y</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span><span>]</span></span></span><span>∼</span><span><span>N</span></span><span>(</span><span>0</span><span>,</span><span>Σ</span><span>)</span><span>=</span><span><span>N</span></span><span><span><span>(</span></span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span><span>]</span></span></span><span>,</span><span><span><span>[</span></span><span><span><span><span><span><span><span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span><span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>X</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>X</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span><span><span></span><span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>Y</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>+</span><span><span>ψ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>I</span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span><span><span>]</span></span></span><span><span>)</span></span></span></span></span></span></span></span>
    <p>
      Again, we can use conditioning to derive the predictive distribution <span><span><span><math><semantics><mrow><msub><mi>P</mi><mrow><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{X|Y}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>P</span><span><span><span><span><span><span></span><span><span><span>X</span><span>∣</span><span>Y</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span>.
      In this formulation, <span><span><span><math><semantics><mrow><mi>ψ</mi></mrow><annotation encoding="application/x-tex">\psi</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>ψ</span></span></span></span></span> is an additional parameter of our model.
    </p>

    <p>
      Analogous to the prior distribution, we could obtain a prediction for our function values by sampling from this distribution.
      But, since sampling involves randomness, the resulting fit to the data would not be deterministic and our prediction could end up being an outlier.
      In order to make a more meaningful prediction we can use the other basic operation of Gaussian distributions.
    </p>

    <p>
      Through the <i>marginalization</i> of each random variable, we can extract the respective mean function value <span><span><span><math><semantics><mrow><msubsup><mi>μ</mi><mi>i</mi><mo mathvariant="normal">′</mo></msubsup></mrow><annotation encoding="application/x-tex">\mu’_i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>μ</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span>′</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> and standard deviation <span><span><span><math><semantics><mrow><msubsup><mi>σ</mi><mi>i</mi><mo mathvariant="normal">′</mo></msubsup><mo>=</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>i</mi><mi>i</mi></mrow><mo mathvariant="normal">′</mo></msubsup></mrow><annotation encoding="application/x-tex">\sigma’_i = \Sigma’_{ii}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>σ</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span>′</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>=</span><span><span>Σ</span><span><span><span><span><span><span></span><span><span><span>i</span><span>i</span></span></span></span><span><span></span><span><span><span>′</span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> for the <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>-th test point.
      In contrast to the prior distribution, where we set the mean to <span><span><span><math><semantics><mrow><mi>μ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu=0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span><span>=</span><span>0</span></span></span></span></span>, the result of conditioning the joint distribution of test and training data will most likely have a non-zero mean <span><span><span><math><semantics><mrow><msup><mi>μ</mi><mo mathvariant="normal">′</mo></msup><mo>≠</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu’ \neq 0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>μ</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>≠</span><span>0</span></span></span></span></span>.
      Extracting <span><span><span><math><semantics><mrow><msup><mi>μ</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">\mu’</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>μ</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span> and <span><span><span><math><semantics><mrow><msup><mi>σ</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">\sigma’</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>σ</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span></span></span></span></span> does not only lead to a more meaningful prediction, it also allows us to make a statement about the confidence of the prediction.
    </p>

    <p>
      The <a href="#Posterior">following figure</a> shows an example of the conditional distribution.
      At first, no training points have been observed.
      Accordingly, the mean prediction remains at <span><span><span><math><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>0</span></span></span></span></span> and the standard deviation is the same for each test point.
      By hovering over the covariance matrix you can see the influence of each point on the current test point.
      As long as no training points have been observed, the influence of neighboring points is limited locally.
    </p>

    <p>
      The training points can be activated by clicking on them, which leads to a constrained distribution.
      This change is reflected in the entries of the covariance matrix, and leads to an adjustment of the mean and the standard deviation of the predicted function.
      As we would expect, the uncertainty of the prediction is small in regions close to the training data and grows as we move further away from those points.
    </p>

    <figure>
        <d-figure id="Posterior"></d-figure>
    </figure>
  <p>
    In the constrained covariance matrix, we can see that the correlation of neighbouring points is affected by the
    training data.
    If a predicted point lies on the training data, there is no correlation with other points.
    Therefore, the function must pass directly through it.
    Predicted values further away are also affected by the training data — proportional to their distance.
  </p>

    <h3>Combining different kernels</h3>

    <p>
      As described earlier, the power of Gaussian processes lies in the choice of the kernel function.
      This property allows experts to introduce domain knowledge into the process and lends Gaussian processes their flexibility to capture trends in the training data.
      For example, by choosing a suitable bandwidth for the RBF kernel, we can control how smooth the resulting function will be.
    </p>
    <p>
      A big benefit that kernels provide is that they can be combined together, resulting in a more specialized kernel.
      The decision which kernel to use is highly dependent on prior knowledge about the data, e.g. if certain characteristics are expected.
      Examples for this would be stationary nature, or global trends and patterns.
      As introduced in the <a href="#Kernels">section on kernels</a>, stationary means that a kernel is translation invariant and therefore not dependent on the index <span><span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>i</span></span></span></span></span>.
      This also means that we cannot model global trends using a strictly stationary kernel.
      Remember that the covariance matrix of Gaussian processes has to be positive semi-definite.
      When choosing the optimal kernel combinations, all methods that preserve this property are allowed.
      The most common kernel combinations would be addition and multiplication.
    </p>
    <p>
      Let’s consider two kernels, a linear kernel <span><span><span><math><semantics><mrow><msub><mi>k</mi><mtext>lin</mtext></msub></mrow><annotation encoding="application/x-tex">k_{\text{lin}}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>k</span><span><span><span><span><span><span></span><span><span><span><span>lin</span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span> and a periodic kernel <span><span><span><math><semantics><mrow><msub><mi>k</mi><mtext>per</mtext></msub></mrow><annotation encoding="application/x-tex">k_{\text{per}}</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>k</span><span><span><span><span><span><span></span><span><span><span><span>per</span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span></span></span></span></span>, for example.
      This is how we would multiply the two:
    </p>

    <span><span><span><span><math><semantics><mrow><msup><mi>k</mi><mo>∗</mo></msup><mo>(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo><mo>=</mo><msub><mi>k</mi><mtext>lin</mtext></msub><mo>(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo><mo>⋅</mo><msub><mi>k</mi><mtext>per</mtext></msub><mo>(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal">′</mo></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">
        k^{\ast}(t,t’) = k_{\text{lin}}(t,t’) \cdot k_{\text{per}}(t,t’)
    </annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span><span>k</span><span><span><span><span><span><span></span><span><span><span>∗</span></span></span></span></span></span></span></span></span><span>(</span><span>t</span><span>,</span><span><span>t</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>)</span><span>=</span><span><span>k</span><span><span><span><span><span><span></span><span><span><span><span>lin</span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>t</span><span>,</span><span><span>t</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>)</span><span>⋅</span><span><span>k</span><span><span><span><span><span><span></span><span><span><span><span>per</span></span></span></span></span></span><span>​</span></span><span><span></span></span></span></span></span><span>(</span><span>t</span><span>,</span><span><span>t</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>)</span></span></span></span></span></span>

  <p>
    However, combinations are not limited to the above example, and there are more possibilities such as concatenation or composition with a function<d-cite key="mackay2003information"></d-cite>.
    To show the impact of a kernel combination and how it might retain qualitative features of the individual kernels, take a look at the <a href="#KernelCombinationsStatic">figure below</a>.
    If we add a periodic and a linear kernel, the global trend of the linear kernel is incorporated into the combined kernel.
    The result is a periodic function that follows a linear trend.
    When combining the same kernels through multiplication instead, the result is a periodic function with a linearly growing amplitude away from linear kernel parameter <span><span><span><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>c</span></span></span></span></span>.
  </p>

  <figure>
    <d-figure id="KernelCombinationsStatic">
    </d-figure>
    
    <figcaption>
      If we draw samples from a combined linear and periodic kernel, we can observe the different retained characteristics in the new sample.
      Addition results in a periodic function with a global trend, while the multiplication increases the periodic amplitude outwards.
    </figcaption>
  </figure>
    
    <p>
      Knowing more about how kernel combinations influence the shape of the resulting distribution, we can move on to a more complex example.
      In the <a href="#KernelCombinations">figure below</a>, the observed training data has an ascending trend with a periodic deviation.
      Using only a linear kernel, we can mimic a normal linear regression of the points.
      At first glance, the RBF kernel accurately approximates the points.
      But since the RBF kernel is stationary it will always return to <span><span><span><math><semantics><mrow><mi>μ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu=0</annotation></semantics></math></span><span aria-hidden="true"><span></span><span></span><span><span>μ</span><span>=</span><span>0</span></span></span></span></span> in regions further away from observed training data.
      This decreases the accuracy for predictions that reach further into the past or the future.
      An improved model can be created by combining the individual kernels through addition, which maintains both the periodic nature and the global ascending trend of the data.
      This procedure can be used, for example, in the analysis of weather data.
    </p>

  <figure>
    <d-figure id="KernelCombinations">
    </d-figure>
        
    <figcaption> Using the checkboxes, different kernels can be combined to form a new Gaussian process. Only by using a
      combination of kernels, it is possible to capture the characteristics of more complex training data.
    </figcaption>
    </figure>

    <p>
      As discussed in the <a href="#GaussianProcesses">section about GPs</a>, a Gaussian process can model uncertain observations.
      This can be seen when only selecting the linear kernel, as it allows us to perform linear regression even if more than two points have been observed, and not all functions have to pass directly through the observed training data.
    </p>

  <h2>Conclusion</h2>
    <p>
      With this article, you should have obtained an overview of Gaussian processes, and developed a deeper
      understanding on how they work.
      As we have seen, Gaussian processes offer a flexible framework for regression and several extensions exist that
      make them even more versatile.
    </p>

    <p>
      For instance, sometimes it might not be possible to describe the kernel in simple terms.
      To overcome this challenge, learning specialized kernel functions from the underlying data, for example by using deep learning<d-cite key="Hinton2008,Wilson2016"></d-cite>, is an area of ongoing research.
      Furthermore, links between Bayesian inference, Gaussian processes and deep learning have been described in several papers<d-cite key="Lee2018,Damianou2013"></d-cite>.
      Even though we mostly talk about Gaussian processes in the context of regression, they can be adapted for
      different purposes, e.g. <i>model-peeling</i> and hypothesis testing.
      By comparing different kernels on the dataset, domain experts can introduce additional knowledge through
      appropriate combination and parameterization of the kernel.
    </p>

  <p>
  If we have sparked your interest, we have compiled a list of further <a href="#FurtherReading">blog posts</a> on the topic of Gaussian processes.
  In addition, we have linked two <a href="#FurtherReading">Python notebooks</a> that will give you some hands-on experience and help you to get started right away.
  </p>
</div></div>
  </body>
</html>
