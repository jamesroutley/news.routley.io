<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/pydantic/monty">Original</a>
    <h1>Monty: A minimal, secure Python interpreter written in Rust for use by AI</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto">
  <div dir="auto"><h3 tabindex="-1" dir="auto">A minimal, secure Python interpreter written in Rust for use by AI.</h3><a id="user-content-a-minimal-secure-python-interpreter-written-in-rust-for-use-by-ai" aria-label="Permalink: A minimal, secure Python interpreter written in Rust for use by AI." href="#a-minimal-secure-python-interpreter-written-in-rust-for-use-by-ai"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</div>
<p><a href="https://github.com/pydantic/monty/actions/workflows/ci.yml?query=branch%3Amain"><img src="https://github.com/pydantic/monty/actions/workflows/ci.yml/badge.svg" alt="CI"/></a>
  <a href="https://codspeed.io/pydantic/monty?utm_source=badge" rel="nofollow"><img src="https://camo.githubusercontent.com/8dad61a3ac354a42b12bb7a2c0a672c496faf5b24408d56c247e324f9c860078/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6453706565642d506572666f726d616e6365253230547261636b65642d626c75653f6c6f676f3d646174613a696d6167652f7376672b786d6c3b6261736536342c50484e325a79423361575230614430694d5459694947686c6157646f644430694d54596949485a705a58644362336739496a41674d4341784e6941784e6949675a6d6c7362443069626d39755a53496765473173626e4d39496d6830644841364c79393364336375647a4d7562334a6e4c7a49774d44417663335a6e496a3438634746306143426b50534a4e4f434177544441674f45773449444532544445324944684d4f434177576949675a6d6c736244306964326870644755694c7a34384c334e325a7a343d" alt="Codspeed" data-canonical-src="https://img.shields.io/badge/CodSpeed-Performance%20Tracked-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNOCAwTDAgOEw4IDE2TDE2IDhMOCAwWiIgZmlsbD0id2hpdGUiLz48L3N2Zz4="/></a>
  <a href="https://codecov.io/gh/pydantic/monty" rel="nofollow"><img src="https://camo.githubusercontent.com/cc1e0d740b6425e467165c0952310c87025558eed6f6a2f412527f2ec8bfb756/68747470733a2f2f636f6465636f762e696f2f67682f707964616e7469632f6d6f6e74792f67726170682f62616467652e7376673f746f6b656e3d48583452445158354f47" alt="Coverage" data-canonical-src="https://codecov.io/gh/pydantic/monty/graph/badge.svg?token=HX4RDQX5OG"/></a>
  <a href="https://pypi.python.org/pypi/pydantic-monty" rel="nofollow"><img src="https://camo.githubusercontent.com/26cfb4e82aaef72860dcd1f28b7f433e4c1881b52cfccefada4819015ff1b792/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f707964616e7469632d6d6f6e74792e737667" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/pydantic-monty.svg"/></a>
  <a href="https://github.com/pydantic/monty"><img src="https://camo.githubusercontent.com/7c7bb07017a92fc1e7dad4fb66ef4a2537ee1ae9f6a227040358d52356adc6c9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f707964616e7469632d6d6f6e74792e737667" alt="versions" data-canonical-src="https://img.shields.io/pypi/pyversions/pydantic-monty.svg"/></a>
  <a href="https://github.com/pydantic/monty/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/da94340d95ce34e16e779627d15b70e3674eebcb8db26a7024cf83a0857594f5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f707964616e7469632f6d6f6e74792e7376673f763d32" alt="license" data-canonical-src="https://img.shields.io/github/license/pydantic/monty.svg?v=2"/></a>
  <a href="https://logfire.pydantic.dev/docs/join-slack/" rel="nofollow"><img src="https://camo.githubusercontent.com/baa9b9b02e5dd6a293c1c23eabdcddca96d92fce408d515f1d49b678888eacd7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f536c61636b2d4a6f696e253230536c61636b2d3441313534423f6c6f676f3d736c61636b" alt="Join Slack" data-canonical-src="https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack"/></a>
</p>
<hr/>
<p dir="auto"><strong>Experimental</strong> - This project is still in development, and not ready for the prime time.</p>
<p dir="auto">A minimal, secure Python interpreter written in Rust for use by AI.</p>
<p dir="auto">Monty avoids the cost, latency, complexity and general faff of using a full container based sandbox for running LLM generated code.</p>
<p dir="auto">Instead, it lets you safely run Python code written by an LLM embedded in your agent, with startup times measured in single digit microseconds not hundreds of milliseconds.</p>
<p dir="auto">What Monty <strong>can</strong> do:</p>
<ul dir="auto">
<li>Run a reasonable subset of Python code - enough for your agent to express what it wants to do</li>
<li>Completely block access to the host environment: filesystem, env variables and network access are all implemented via external function calls the developer can control</li>
<li>Call functions on the host - only functions you give it access to</li>
<li>Run typechecking - monty supports full modern python type hints and comes with <a href="https://docs.astral.sh/ty/" rel="nofollow">ty</a> included in a single binary to run typechecking</li>
<li>Be snapshotted to bytes at external function calls, meaning you can store the interpreter state in a file or database, and resume later</li>
<li>Startup extremely fast (&lt;1Î¼s to go from code to execution result), and has runtime performance that is similar to CPython (generally between 5x faster and 5x slower)</li>
<li>Be called from Rust, Python, or Javascript - because Monty has no dependencies on cpython, you can use it anywhere you can run Rust</li>
<li>Control resource usage - Monty can track memory usage, allocations, stack depth, and execution time and cancel execution if it exceeds preset limits</li>
<li>Collect stdout and stderr and return it to the caller</li>
<li>Run async or sync code on the host via async or sync code on the host</li>
</ul>
<p dir="auto">What Monty <strong>cannot</strong> do:</p>
<ul dir="auto">
<li>Use the standard library (except a few select modules: <code>sys</code>, <code>typing</code>, <code>asyncio</code>, <code>dataclasses</code> (soon), <code>json</code> (soon))</li>
<li>Use third party libraries (like Pydantic), support for external python library is not a goal</li>
<li>define classes (support should come soon)</li>
<li>use match statements (again, support should come soon)</li>
</ul>
<hr/>
<p dir="auto">In short, Monty is extremely limited and designed for <strong>one</strong> use case:</p>
<p dir="auto"><strong>To run code written by agents.</strong></p>
<p dir="auto">For motivation on why you might want to do this, see:</p>
<ul dir="auto">
<li><a href="https://blog.cloudflare.com/code-mode/" rel="nofollow">Codemode</a> from Cloudflare</li>
<li><a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling" rel="nofollow">Programmatic Tool Calling</a> from Anthropic</li>
<li><a href="https://www.anthropic.com/engineering/code-execution-with-mcp" rel="nofollow">Code Execution with MCP</a> from Anthropic</li>
<li><a href="https://github.com/huggingface/smolagents">Smol Agents</a> from Hugging Face</li>
</ul>
<p dir="auto">In very simple terms, the idea of all the above is that LLMs can work faster, cheaper and more reliably if they&#39;re asked to write Python (or Javascript) code, instead of relying on traditional tool calling. Monty makes that possible without the complexity of a sandbox or risk of running code directly on the host.</p>
<p dir="auto"><strong>Note:</strong> Monty will (soon) be used to implement <code>codemode</code> in <a href="https://github.com/pydantic/pydantic-ai">Pydantic AI</a></p>

<p dir="auto">Monty can be called from Python, JavaScript/TypeScript or Rust.</p>

<p dir="auto">To install:</p>

<p dir="auto">(Or <code>pip install pydantic-monty</code> for the boomers)</p>
<p dir="auto">Usage:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from typing import Any

import pydantic_monty

code = &#34;&#34;&#34;
async def agent(prompt: str, messages: Messages):
    while True:
        print(f&#39;messages so far: {messages}&#39;)
        output = await call_llm(prompt, messages)
        if isinstance(output, str):
            return output
        messages.extend(output)

await agent(prompt, [])
&#34;&#34;&#34;

type_definitions = &#34;&#34;&#34;
from typing import Any

Messages = list[dict[str, Any]]

async def call_llm(prompt: str, messages: Messages) -&gt; str | Messages:
    raise NotImplementedError()

prompt: str = &#39;&#39;
&#34;&#34;&#34;

m = pydantic_monty.Monty(
    code,
    inputs=[&#39;prompt&#39;],
    external_functions=[&#39;call_llm&#39;],
    script_name=&#39;agent.py&#39;,
    type_check=True,
    type_check_stubs=type_definitions,
)


Messages = list[dict[str, Any]]


async def call_llm(prompt: str, messages: Messages) -&gt; str | Messages:
    if len(messages) &lt; 2:
        return [{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;example response&#39;}]
    else:
        return f&#39;example output, message count {len(messages)}&#39;


async def main():
    output = await pydantic_monty.run_monty_async(
        m,
        inputs={&#39;prompt&#39;: &#39;testing&#39;},
        external_functions={&#39;call_llm&#39;: call_llm},
    )
    print(output)
    #&gt; example output, message count 2


if __name__ == &#39;__main__&#39;:
    import asyncio

    asyncio.run(main())"><pre><span>from</span> <span>typing</span> <span>import</span> <span>Any</span>

<span>import</span> <span>pydantic_monty</span>

<span>code</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>async def agent(prompt: str, messages: Messages):</span>
<span>    while True:</span>
<span>        print(f&#39;messages so far: {messages}&#39;)</span>
<span>        output = await call_llm(prompt, messages)</span>
<span>        if isinstance(output, str):</span>
<span>            return output</span>
<span>        messages.extend(output)</span>
<span></span>
<span>await agent(prompt, [])</span>
<span>&#34;&#34;&#34;</span>

<span>type_definitions</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>from typing import Any</span>
<span></span>
<span>Messages = list[dict[str, Any]]</span>
<span></span>
<span>async def call_llm(prompt: str, messages: Messages) -&gt; str | Messages:</span>
<span>    raise NotImplementedError()</span>
<span></span>
<span>prompt: str = &#39;&#39;</span>
<span>&#34;&#34;&#34;</span>

<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(
    <span>code</span>,
    <span>inputs</span><span>=</span>[<span>&#39;prompt&#39;</span>],
    <span>external_functions</span><span>=</span>[<span>&#39;call_llm&#39;</span>],
    <span>script_name</span><span>=</span><span>&#39;agent.py&#39;</span>,
    <span>type_check</span><span>=</span><span>True</span>,
    <span>type_check_stubs</span><span>=</span><span>type_definitions</span>,
)


<span>Messages</span> <span>=</span> <span>list</span>[<span>dict</span>[<span>str</span>, <span>Any</span>]]


<span>async</span> <span>def</span> <span>call_llm</span>(<span>prompt</span>: <span>str</span>, <span>messages</span>: <span>Messages</span>) <span>-&gt;</span> <span>str</span> <span>|</span> <span>Messages</span>:
    <span>if</span> <span>len</span>(<span>messages</span>) <span>&lt;</span> <span>2</span>:
        <span>return</span> [{<span>&#39;role&#39;</span>: <span>&#39;system&#39;</span>, <span>&#39;content&#39;</span>: <span>&#39;example response&#39;</span>}]
    <span>else</span>:
        <span>return</span> <span>f&#39;example output, message count <span><span>{</span><span>len</span>(<span>messages</span>)<span>}</span></span>&#39;</span>


<span>async</span> <span>def</span> <span>main</span>():
    <span>output</span> <span>=</span> <span>await</span> <span>pydantic_monty</span>.<span>run_monty_async</span>(
        <span>m</span>,
        <span>inputs</span><span>=</span>{<span>&#39;prompt&#39;</span>: <span>&#39;testing&#39;</span>},
        <span>external_functions</span><span>=</span>{<span>&#39;call_llm&#39;</span>: <span>call_llm</span>},
    )
    <span>print</span>(<span>output</span>)
    <span>#&gt; example output, message count 2</span>


<span>if</span> <span>__name__</span> <span>==</span> <span>&#39;__main__&#39;</span>:
    <span>import</span> <span>asyncio</span>

    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">Iterative Execution with External Functions</h4><a id="user-content-iterative-execution-with-external-functions" aria-label="Permalink: Iterative Execution with External Functions" href="#iterative-execution-with-external-functions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Use <code>start()</code> and <code>resume()</code> to handle external function calls iteratively,
giving you control over each call:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import pydantic_monty

code = &#34;&#34;&#34;
data = fetch(url)
len(data)
&#34;&#34;&#34;

m = pydantic_monty.Monty(code, inputs=[&#39;url&#39;], external_functions=[&#39;fetch&#39;])

# Start execution - pauses when fetch() is called
result = m.start(inputs={&#39;url&#39;: &#39;https://example.com&#39;})

print(type(result))
#&gt; &lt;class &#39;pydantic_monty.MontySnapshot&#39;&gt;
print(result.function_name)  # fetch
#&gt; fetch
print(result.args)
#&gt; (&#39;https://example.com&#39;,)

# Perform the actual fetch, then resume with the result
result = result.resume(return_value=&#39;hello world&#39;)

print(type(result))
#&gt; &lt;class &#39;pydantic_monty.MontyComplete&#39;&gt;
print(result.output)
#&gt; 11"><pre><span>import</span> <span>pydantic_monty</span>

<span>code</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>data = fetch(url)</span>
<span>len(data)</span>
<span>&#34;&#34;&#34;</span>

<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>code</span>, <span>inputs</span><span>=</span>[<span>&#39;url&#39;</span>], <span>external_functions</span><span>=</span>[<span>&#39;fetch&#39;</span>])

<span># Start execution - pauses when fetch() is called</span>
<span>result</span> <span>=</span> <span>m</span>.<span>start</span>(<span>inputs</span><span>=</span>{<span>&#39;url&#39;</span>: <span>&#39;https://example.com&#39;</span>})

<span>print</span>(<span>type</span>(<span>result</span>))
<span>#&gt; &lt;class &#39;pydantic_monty.MontySnapshot&#39;&gt;</span>
<span>print</span>(<span>result</span>.<span>function_name</span>)  <span># fetch</span>
<span>#&gt; fetch</span>
<span>print</span>(<span>result</span>.<span>args</span>)
<span>#&gt; (&#39;https://example.com&#39;,)</span>

<span># Perform the actual fetch, then resume with the result</span>
<span>result</span> <span>=</span> <span>result</span>.<span>resume</span>(<span>return_value</span><span>=</span><span>&#39;hello world&#39;</span>)

<span>print</span>(<span>type</span>(<span>result</span>))
<span>#&gt; &lt;class &#39;pydantic_monty.MontyComplete&#39;&gt;</span>
<span>print</span>(<span>result</span>.<span>output</span>)
<span>#&gt; 11</span></pre></div>

<p dir="auto">Both <code>Monty</code> and <code>MontySnapshot</code> can be serialized to bytes and restored later.
This allows caching parsed code or suspending execution across process boundaries:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import pydantic_monty

# Serialize parsed code to avoid re-parsing
m = pydantic_monty.Monty(&#39;x + 1&#39;, inputs=[&#39;x&#39;])
data = m.dump()

# Later, restore and run
m2 = pydantic_monty.Monty.load(data)
print(m2.run(inputs={&#39;x&#39;: 41}))
#&gt; 42

# Serialize execution state mid-flight
m = pydantic_monty.Monty(&#39;fetch(url)&#39;, inputs=[&#39;url&#39;], external_functions=[&#39;fetch&#39;])
progress = m.start(inputs={&#39;url&#39;: &#39;https://example.com&#39;})
state = progress.dump()

# Later, restore and resume (e.g., in a different process)
progress2 = pydantic_monty.MontySnapshot.load(state)
result = progress2.resume(return_value=&#39;response data&#39;)
print(result.output)
#&gt; response data"><pre><span>import</span> <span>pydantic_monty</span>

<span># Serialize parsed code to avoid re-parsing</span>
<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>&#39;x + 1&#39;</span>, <span>inputs</span><span>=</span>[<span>&#39;x&#39;</span>])
<span>data</span> <span>=</span> <span>m</span>.<span>dump</span>()

<span># Later, restore and run</span>
<span>m2</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>.<span>load</span>(<span>data</span>)
<span>print</span>(<span>m2</span>.<span>run</span>(<span>inputs</span><span>=</span>{<span>&#39;x&#39;</span>: <span>41</span>}))
<span>#&gt; 42</span>

<span># Serialize execution state mid-flight</span>
<span>m</span> <span>=</span> <span>pydantic_monty</span>.<span>Monty</span>(<span>&#39;fetch(url)&#39;</span>, <span>inputs</span><span>=</span>[<span>&#39;url&#39;</span>], <span>external_functions</span><span>=</span>[<span>&#39;fetch&#39;</span>])
<span>progress</span> <span>=</span> <span>m</span>.<span>start</span>(<span>inputs</span><span>=</span>{<span>&#39;url&#39;</span>: <span>&#39;https://example.com&#39;</span>})
<span>state</span> <span>=</span> <span>progress</span>.<span>dump</span>()

<span># Later, restore and resume (e.g., in a different process)</span>
<span>progress2</span> <span>=</span> <span>pydantic_monty</span>.<span>MontySnapshot</span>.<span>load</span>(<span>state</span>)
<span>result</span> <span>=</span> <span>progress2</span>.<span>resume</span>(<span>return_value</span><span>=</span><span>&#39;response data&#39;</span>)
<span>print</span>(<span>result</span>.<span>output</span>)
<span>#&gt; response data</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="use monty::{MontyRun, MontyObject, NoLimitTracker, StdPrint};

let code = r#&#34;
def fib(n):
    if n &lt;= 1:
        return n
    return fib(n - 1) + fib(n - 2)

fib(x)
&#34;#;

let runner = MontyRun::new(code.to_owned(), &#34;fib.py&#34;, vec![&#34;x&#34;.to_owned()], vec![]).unwrap();
let result = runner.run(vec![MontyObject::Int(10)], NoLimitTracker, &amp;mut StdPrint).unwrap();
assert_eq!(result, MontyObject::Int(55));"><pre><span>use</span> monty<span>::</span><span>{</span><span>MontyRun</span><span>,</span> <span>MontyObject</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>StdPrint</span><span>}</span><span>;</span>

<span>let</span> code = <span>r#&#34;</span>
<span>def fib(n):</span>
<span>    if n &lt;= 1:</span>
<span>        return n</span>
<span>    return fib(n - 1) + fib(n - 2)</span>
<span></span>
<span>fib(x)</span>
<span>&#34;#</span><span>;</span>

<span>let</span> runner = <span>MontyRun</span><span>::</span><span>new</span><span>(</span>code<span>.</span><span>to_owned</span><span>(</span><span>)</span><span>,</span> <span>&#34;fib.py&#34;</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>&#34;x&#34;</span><span>.</span>to_owned<span>(</span><span>)</span><span>]</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>]</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>let</span> result = runner<span>.</span><span>run</span><span>(</span><span>vec</span><span>!</span><span>[</span><span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>10</span><span>)</span><span>]</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>&amp;</span><span>mut</span> <span>StdPrint</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>assert_eq</span><span>!</span><span>(</span>result<span>,</span> <span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>55</span><span>)</span><span>)</span><span>;</span></pre></div>

<p dir="auto"><code>MontyRun</code> and <code>RunProgress</code> can be serialized using the <code>dump()</code> and <code>load()</code> methods:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use monty::{MontyRun, MontyObject, NoLimitTracker, StdPrint};

// Serialize parsed code
let runner = MontyRun::new(&#34;x + 1&#34;.to_owned(), &#34;main.py&#34;, vec![&#34;x&#34;.to_owned()], vec![]).unwrap();
let bytes = runner.dump().unwrap();

// Later, restore and run
let runner2 = MontyRun::load(&amp;bytes).unwrap();
let result = runner2.run(vec![MontyObject::Int(41)], NoLimitTracker, &amp;mut StdPrint).unwrap();
assert_eq!(result, MontyObject::Int(42));"><pre><span>use</span> monty<span>::</span><span>{</span><span>MontyRun</span><span>,</span> <span>MontyObject</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>StdPrint</span><span>}</span><span>;</span>

<span>// Serialize parsed code</span>
<span>let</span> runner = <span>MontyRun</span><span>::</span><span>new</span><span>(</span><span>&#34;x + 1&#34;</span><span>.</span><span>to_owned</span><span>(</span><span>)</span><span>,</span> <span>&#34;main.py&#34;</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>&#34;x&#34;</span><span>.</span>to_owned<span>(</span><span>)</span><span>]</span><span>,</span> <span>vec</span><span>!</span><span>[</span><span>]</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>let</span> bytes = runner<span>.</span><span>dump</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>

<span>// Later, restore and run</span>
<span>let</span> runner2 = <span>MontyRun</span><span>::</span><span>load</span><span>(</span><span>&amp;</span>bytes<span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>let</span> result = runner2<span>.</span><span>run</span><span>(</span><span>vec</span><span>!</span><span>[</span><span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>41</span><span>)</span><span>]</span><span>,</span> <span>NoLimitTracker</span><span>,</span> <span>&amp;</span><span>mut</span> <span>StdPrint</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
<span>assert_eq</span><span>!</span><span>(</span>result<span>,</span> <span>MontyObject</span><span>::</span><span>Int</span><span>(</span><span>42</span><span>)</span><span>)</span><span>;</span></pre></div>

<p dir="auto">Monty will power code-mode in
<a href="https://github.com/pydantic/pydantic-ai">Pydantic AI</a>. Instead of making
sequential tool calls, the LLM writes Python code that calls your tools
as functions and Monty executes it safely.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pydantic_ai import Agent
from pydantic_ai.toolsets.code_mode import CodeModeToolset
from pydantic_ai.toolsets.function import FunctionToolset
from typing_extensions import TypedDict


class WeatherResult(TypedDict):
    city: str
    temp_c: float
    conditions: str


toolset = FunctionToolset()


@toolset.tool
def get_weather(city: str) -&gt; WeatherResult:
    &#34;&#34;&#34;Get current weather for a city.&#34;&#34;&#34;
    # your real implementation here
    return {&#39;city&#39;: city, &#39;temp_c&#39;: 18, &#39;conditions&#39;: &#39;partly cloudy&#39;}


@toolset.tool
def get_population(city: str) -&gt; int:
    &#34;&#34;&#34;Get the population of a city.&#34;&#34;&#34;
    return {&#39;london&#39;: 9_000_000, &#39;paris&#39;: 2_100_000, &#39;tokyo&#39;: 14_000_000}.get(
        city.lower(), 0
    )


toolset = CodeModeToolset(toolset)

agent = Agent(
    &#39;anthropic:claude-sonnet-4-5&#39;,
    toolsets=[toolset],
)

result = agent.run_sync(
    &#39;Compare the weather and population of London, Paris, and Tokyo.&#39;
)
print(result.output)"><pre><span>from</span> <span>pydantic_ai</span> <span>import</span> <span>Agent</span>
<span>from</span> <span>pydantic_ai</span>.<span>toolsets</span>.<span>code_mode</span> <span>import</span> <span>CodeModeToolset</span>
<span>from</span> <span>pydantic_ai</span>.<span>toolsets</span>.<span>function</span> <span>import</span> <span>FunctionToolset</span>
<span>from</span> <span>typing_extensions</span> <span>import</span> <span>TypedDict</span>


<span>class</span> <span>WeatherResult</span>(<span>TypedDict</span>):
    <span>city</span>: <span>str</span>
    <span>temp_c</span>: <span>float</span>
    <span>conditions</span>: <span>str</span>


<span>toolset</span> <span>=</span> <span>FunctionToolset</span>()


<span>@<span>toolset</span>.<span>tool</span></span>
<span>def</span> <span>get_weather</span>(<span>city</span>: <span>str</span>) <span>-&gt;</span> <span>WeatherResult</span>:
    <span>&#34;&#34;&#34;Get current weather for a city.&#34;&#34;&#34;</span>
    <span># your real implementation here</span>
    <span>return</span> {<span>&#39;city&#39;</span>: <span>city</span>, <span>&#39;temp_c&#39;</span>: <span>18</span>, <span>&#39;conditions&#39;</span>: <span>&#39;partly cloudy&#39;</span>}


<span>@<span>toolset</span>.<span>tool</span></span>
<span>def</span> <span>get_population</span>(<span>city</span>: <span>str</span>) <span>-&gt;</span> <span>int</span>:
    <span>&#34;&#34;&#34;Get the population of a city.&#34;&#34;&#34;</span>
    <span>return</span> {<span>&#39;london&#39;</span>: <span>9_000_000</span>, <span>&#39;paris&#39;</span>: <span>2_100_000</span>, <span>&#39;tokyo&#39;</span>: <span>14_000_000</span>}.<span>get</span>(
        <span>city</span>.<span>lower</span>(), <span>0</span>
    )


<span>toolset</span> <span>=</span> <span>CodeModeToolset</span>(<span>toolset</span>)

<span>agent</span> <span>=</span> <span>Agent</span>(
    <span>&#39;anthropic:claude-sonnet-4-5&#39;</span>,
    <span>toolsets</span><span>=</span>[<span>toolset</span>],
)

<span>result</span> <span>=</span> <span>agent</span>.<span>run_sync</span>(
    <span>&#39;Compare the weather and population of London, Paris, and Tokyo.&#39;</span>
)
<span>print</span>(<span>result</span>.<span>output</span>)</pre></div>

<p dir="auto">There are generally two responses when you show people Monty:</p>
<ol dir="auto">
<li>Oh my god, this solves so many problems, I want it.</li>
<li>Why not X?</li>
</ol>
<p dir="auto">Where X is some alternative technology. Oddly often these responses are combined, suggesting people have not yet found an alternative that works for them, but are incredulous that there&#39;s really no good alternative to creating an entire Python implementation from scratch.</p>
<p dir="auto">I&#39;ll try to run through the most obvious alternatives, and why there aren&#39;t right for what we wanted.</p>
<p dir="auto">NOTE: all these technologies are impressive and have widespread uses, this commentary on their limitations for our use case should not be seen as a criticism. Most of these solutions were not conceived with the goal of providing an LLM sandbox, which is why they&#39;re not necessary great at it.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Tech</th>
<th>Language completeness</th>
<th>Security</th>
<th>Start latency</th>
<th>Cost</th>
<th>Setup complexity</th>
<th>File mounting</th>
<th>Snapshotting</th>
</tr>
</thead>
<tbody>
<tr>
<td>Monty</td>
<td>partial</td>
<td>strict</td>
<td>0.06ms</td>
<td>free</td>
<td>easy</td>
<td>easy</td>
<td>easy</td>
</tr>
<tr>
<td>Docker</td>
<td>full</td>
<td>good</td>
<td>195ms</td>
<td>free</td>
<td>intermediate</td>
<td>easy</td>
<td>intermediate</td>
</tr>
<tr>
<td>Pyodide</td>
<td>full</td>
<td>poor</td>
<td>2800ms</td>
<td>free</td>
<td>intermediate</td>
<td>easy</td>
<td>hard</td>
</tr>
<tr>
<td>starlark-rust</td>
<td>very limited</td>
<td>good</td>
<td>1.7ms</td>
<td>free</td>
<td>easy</td>
<td>not available?</td>
<td>impossible?</td>
</tr>
<tr>
<td>sandboxing service</td>
<td>full</td>
<td>strict</td>
<td>1033ms</td>
<td>not free</td>
<td>intermediate</td>
<td>hard</td>
<td>intermediate</td>
</tr>
<tr>
<td>YOLO Python</td>
<td>full</td>
<td>non-existent</td>
<td>0.1ms / 30ms</td>
<td>free</td>
<td>easy</td>
<td>easy / scary</td>
<td>hard</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">See <a href="https://github.com/pydantic/monty/blob/main/scripts/startup_performance.py">./scripts/startup_performance.py</a> for the script used to calculate the startup performance numbers.</p>
<p dir="auto">Details on each row below:</p>

<ul dir="auto">
<li><strong>Language completeness</strong>: No classes (yet), limited stdlib, no third-party libraries</li>
<li><strong>Security</strong>: Explicitly controlled filesystem, network, and env access, strict limits on execution time and memory usage</li>
<li><strong>Start latency</strong>: Starts in microseconds</li>
<li><strong>Setup complexity</strong>: just <code>pip install pydantic-monty</code> or <code>npm install @pydantic/monty</code>, ~4.5MB download</li>
<li><strong>File mounting</strong>: Strictly controlled, see <a href="https://github.com/pydantic/monty/pull/85" data-hovercard-type="pull_request" data-hovercard-url="/pydantic/monty/pull/85/hovercard">#85</a></li>
<li><strong>Snapshotting</strong>: Monty&#39;s pause and resume functionality with <code>dump()</code> and <code>load()</code> makes it trivial to pause, resume and fork execution</li>
</ul>

<ul dir="auto">
<li><strong>Language completeness</strong>: Full CPython with any library</li>
<li><strong>Security</strong>: Process and filesystem isolation, network policies, but container escapes exist, memory limitation is possible</li>
<li><strong>Start latency</strong>: Container startup overhead (~195ms measured)</li>
<li><strong>Setup complexity</strong>: Requires Docker daemon, container images, orchestration, <code>python:3.14-alpine</code> is 50MB - docker can&#39;t be installed from PyPI</li>
<li><strong>File mounting</strong>: Volume mounts work well</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal, or snapshotting an image and saving it as a Docker image.</li>
</ul>

<ul dir="auto">
<li><strong>Language completeness</strong>: Full CPython compiled to WASM, almost all libraries available</li>
<li><strong>Security</strong>: Relies on browser/WASM sandbox - not designed for server-side isolation, python code can run arbitrary code in the JS runtime, only deno allows isolation, memory limits are hard/impossible to enforce with deno</li>
<li><strong>Start latency</strong>: WASM runtime loading is slow (~2800ms cold start)</li>
<li><strong>Setup complexity</strong>: Need to load WASM runtime, handle async initialization, pyodide NPM package is ~12MB, deno is ~50MB - Pyodide can&#39;t be called with just PyPI packages</li>
<li><strong>File mounting</strong>: Virtual filesystem via browser APIs</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal presumably, but hard</li>
</ul>

<p dir="auto">See <a href="https://github.com/facebook/starlark-rust">starlark-rust</a>.</p>
<ul dir="auto">
<li><strong>Language completeness</strong>: Configuration language, not Python - no classes, exceptions, async</li>
<li><strong>Security</strong>: Deterministic and hermetic by design</li>
<li><strong>Start latency</strong>: runs embedded in the process like Monty, hence impressive startup time</li>
<li><strong>Setup complexity</strong>: Usable in python via <a href="https://github.com/inducer/starlark-pyo3">starlark-pyo3</a></li>
<li><strong>File mounting</strong>: No file handling by design AFAIK?</li>
<li><strong>Snapshotting</strong>: Impossible AFAIK?</li>
</ul>

<p dir="auto">Services like <a href="https://daytona.io" rel="nofollow">Daytona</a>, <a href="https://e2b.dev" rel="nofollow">E2B</a>, <a href="https://modal.com" rel="nofollow">Modal</a>.</p>
<p dir="auto">There are similar challenges, more setup complexity but lower network latency for setting up your own sandbox setup with k8s.</p>
<ul dir="auto">
<li><strong>Language completeness</strong>: Full CPython with any library</li>
<li><strong>Security</strong>: Professionally managed container isolation</li>
<li><strong>Start latency</strong>: Network round-trip and container startup time. I got ~1s cold start time with Daytona EU from London, Daytona advertise sub 90ms latency, presumably that&#39;s for an existing container, not clear if it includes network latency</li>
<li><strong>Cost</strong>: Pay per execution or compute time</li>
<li><strong>Setup complexity</strong>: API integration, auth tokens - fine for startups but generally a non-start for enterprises</li>
<li><strong>File mounting</strong>: Upload/download via API calls</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal, also the services offer some solutions for this, I think based con docker containers</li>
</ul>

<p dir="auto">Running Python directly via <code>exec()</code> (~0.1ms) or subprocess (~30ms).</p>
<ul dir="auto">
<li><strong>Language completeness</strong>: Full CPython with any library</li>
<li><strong>Security</strong>: None - full filesystem, network, env vars, system commands</li>
<li><strong>Start latency</strong>: Near-zero for <code>exec()</code>, ~30ms for subprocess</li>
<li><strong>Setup complexity</strong>: None</li>
<li><strong>File mounting</strong>: Direct filesystem access (that&#39;s the problem)</li>
<li><strong>Snapshotting</strong>: Possible with durable execution solutions like Temporal</li>
</ul>
</article></div></div>
  </body>
</html>
