<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://softwarescalability.com/editorial/real-time-object-detection-with-webrtc-and-yolo">Original</a>
    <h1>Multi-camera real-time object detection with WebRTC and YOLO</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>Ever seen Jason Bourne trilogy?</p>
<p>In the Jason Bourne trilogy, cameras play a crucial role in the hunt for the titular character, a former CIA assassin suffering from dissociative amnesia who is on the run and trying to uncover the truth about his past.</p>
<p><img alt="Scene from Jason Bourne movie thrilogy" srcset="/_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fmovie_bourne_scene_001.gif&amp;w=640&amp;q=75 1x, /_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fmovie_bourne_scene_001.gif&amp;w=828&amp;q=75 2x" src="https://softwarescalability.com/_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fmovie_bourne_scene_001.gif&amp;w=828&amp;q=75" width="400" height="169" decoding="async" data-nimg="1" loading="lazy"/></p>
<p>In <a href="https://www.imdb.com/title/tt0258463/" target="_blank" rel="noopener noreferrer">&#34;The Bourne Identity&#34;</a>, cameras are used by the CIA to track Bourne&#39;s movements and try to locate him.</p>
<p>In <a href="https://www.imdb.com/title/tt0372183/" target="_blank" rel="noopener noreferrer">&#34;The Bourne Supremacy&#34;</a>, the use of cameras intensifies as Bourne is framed for a crime he didn&#39;t commit and the CIA is determined to bring him in.</p>
<p>In <a href="https://www.imdb.com/title/tt0440963/" target="_blank" rel="noopener noreferrer">&#34;The Bourne Ultimatum&#34;</a>, cameras continue to be a key tool in the pursuit of Bourne, as the agency uses them to monitor his every move and try to anticipate his next steps.</p>
<p>Despite the extensive use of cameras and other surveillance technology, Bourne is able to stay one step ahead of his pursuers and evade capture thanks to his intelligence, resourcefulness, and combat skills.</p>
<p>Nevertheless, it is possible to build similar similar system by using cameras for object detection through the use of computer vision and machine learning techniques.
These techniques allow for the automatic identification and tracking of objects in real-time video streams, enabling a wide range of applications such as security and surveillance.</p>
<p>There are also several challenges to using cameras for object detection such as issues related to data privacy and security, as well as technical limitations such as low image quality, lighting conditions and available compute power.</p>
<p>Nevertheless, you will learn to build a surveillance system that uses WebRTC for low-latency, and YOLO for object detection.</p>
<h2 id="what-is-webrtc">What is WebRTC?</h2>
<p>WebRTC (Web Real-Time-Communication) is a collection of specifications enabling a web application running on any device to exchange real-time video, voice, or generic data with remote party in a peer-to-peer (P2P) fashion.</p>
<blockquote>
<p>WebRTC is maintainted in the IETF (Internet Engineering Task Force) by the <a href="https://datatracker.ietf.org/wg/rtcweb/documents/" target="_blank" rel="noopener noreferrer">RTCWEB (Real-Time Communication in WEB-browsers) working group</a>.</p>
</blockquote>
<p>On the high-level, the WebRTC contains of the following 3 components:</p>
<ul>
<li><strong>Signaling:</strong> Middleman between peers</li>
<li><strong>Connecting:</strong> How peer-to-peer happens</li>
<li><strong>Secure Communication:</strong> Sending and receiving video, and audio</li>
</ul>
<h2 id="signaling-middleman-between-peers">Signaling: Middleman between peers</h2>
<p>In order to communicate with other peers, you first need to share who you are and what kind of data you provide or accept.
For that you are going to use signaling.</p>
<p>Signaling is a communication coordinating process.
You can think of it as a middleman between peers.
Signaling uses Session Description Protocol (SDP), as defined in <a href="https://www.rfc-editor.org/rfc/rfc8866.html" target="_blank" rel="noopener noreferrer">RFC8866</a>, for communication format.</p>
<p>SDP is plain-text protocol that defines a format for session description as a group of key values in text format, one field per line. Below is example from the <a href="https://www.rfc-editor.org/rfc/rfc8866.html" target="_blank" rel="noopener noreferrer">RFC8866</a>:</p>
<div data-rehype-pretty-code-fragment=""><pre data-language="text" data-theme="default"><code data-language="text" data-theme="default"><span><span>v=0</span></span>
<span><span>o=jdoe 3724394400 3724394405 IN IP4 198.51.100.1</span></span>
<span><span>s=Call to John Smith</span></span>
<span><span>i=SDP Offer #1</span></span>
<span><span>u=http://www.jdoe.example.com/home.html</span></span>
<span><span>e=Jane Doe &lt;jane@jdoe.example.com&gt;</span></span>
<span><span>p=+1 617 555-6011</span></span>
<span><span>c=IN IP4 198.51.100.1</span></span>
<span><span>t=0 0</span></span>
<span><span>m=audio 49170 RTP/AVP 0</span></span>
<span><span>m=audio 49180 RTP/AVP 0</span></span>
<span><span>m=video 51372 RTP/AVP 99</span></span>
<span><span>c=IN IP6 2001:db8::2</span></span>
<span><span>a=rtpmap:99 h263-1998/90000</span></span></code></pre></div>
<p>Beware, not all key values are used by WebRTC.
Only keys defined in JavaScript Session Establishment Protocol (JSEP) (<a href="https://datatracker.ietf.org/doc/html/rfc8829" target="_blank" rel="noopener noreferrer">RFC 8829</a>) are.</p>
<blockquote>
<p>While the protocol is simple to read and understand, the meaning of values can make your head spin. I suggest you to check out <a href="https://webrtchacks.github.io/sdp-anatomy/" target="_blank" rel="noopener noreferrer">anatomoy of a WebRTC SDP</a> for interactive explanation of what these values mean.</p>
</blockquote>
<p>WebRTC does not handle signaling for you nor does the SDP tell you what kind of transport you must use. It is up to you to use either <a href="https://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm" target="_blank" rel="noopener noreferrer">HTTP REST API</a>, RPC, WS (WebSocket) server, or even email for sending necessary information before the peer connection is initiated. With most public WebRTC examples, human acts as a transport layer where you have to copy SDPs to initiate peer-to-peer connection.</p>
<p><strong>NB!</strong> You are <strong><em>RESPONSIBLE</em></strong> for making signaling secure. If you do not secure signaling, it is possible to hijack signaling which results control over sessions, connections, and content. In order to secure signaling, use HTTPS and WSS/TLS to ensure messages cannot be intercepted unencrypted.</p>
<h2 id="connecting-how-peer-to-peer-happens">Connecting: How peer-to-peer happens</h2>
<p>Once session description has been communicated, WebRTC establishes peer-to-peer (P2P) connections in order to share media and data. However, establishing peer-to-peer connections can be quite difficult as peers are usually in different networks. On top of that, there are network constraints such as Network Address Translations (NATs), firewalls, antiviruses, corporate policies, etc. In some cases, the peers don&#39;t speak even the same network protocols (different IP versions for example).</p>
<blockquote>
<p>Network Address Translations (NAT) is technique to delay the exhaustion of the available address pool of IPV4.</p>
</blockquote>
<p>Interactive Connectivity Establishment (ICE) solves this by making use of the Session Traversal Utilities for NAT (STUN) protocol, and its extension Traversal Using Relay NAT (TURN).</p>
<blockquote>
<p>You can read about ICE (Interactive Connectivity Establishment) in rather lengthy <a href="https://www.rfc-editor.org/rfc/rfc8445.html" target="_blank" rel="noopener noreferrer">RFC8445</a>.</p>
</blockquote>
<p>In short, ICE finds the best way to communicate between computers (agents). Using either STUN or TURN protocols, ICE finds out the network architecture and provides some transport addresses (ICE canditaes) on which the ICE agent can be contacted.</p>
<blockquote>
<p>Network Address Translation (NAT) is the magic what makes WebRTC possible.
To make this communication happen you establish a NAT mapping. Agent 1 uses port 7000 to establish a WebRTC connection with Agent 2. This creates a binding of <code>192.168.0.1:7000</code> to <code>5.0.0.1:7000</code>. This then allows Agent 2 to reach Agent 1 by sending packets to <code>5.0.0.1:7000</code>. Creating a NAT mapping like in this example is like an automated version of doing port forwarding in your router.
<a href="https://tools.ietf.org/html/rfc4787" target="_blank" rel="noopener noreferrer">RFC 4787</a> describes in detail this behaviour.</p>
</blockquote>
<h2 id="stun-and-turn">STUN and TURN</h2>
<p>STUN identifies the unique IP address of the agent (peer) for the peer to peer connection. However, if network conditions are such that the true IP address for an agent is masked, the STUN server fails to supply specific information which results failure in media and data exchange between the agents (peers).
TURN, as defined in <a href="https://www.rfc-editor.org/rfc/rfc8656" target="_blank" rel="noopener noreferrer">RFC 8656</a>, is backup for such cases when direct connectivity is not possible. TURN uses dedicated server which acts as a proxy for agents. It does introduce some things that you must be aware:</p>
<ul>
<li>TURN is extra dependency in your system which takes resources</li>
<li>Latency increase between peers</li>
<li>TURN server might receive a lot of traffic so make sure it scales</li>
</ul>
<p>Now you know how peers reach other, and how data exchange takes place, it is time to talk about the last piece that completes the picture.</p>
<h2 id="secure-communication-sending-and-receiving-video-and-audio">Secure Communication: Sending and receiving video, and audio</h2>
<p>Theoretically you can send and receive unlimited audio and video streams with WebRTC meaning you could send desktop screen streaming while using headphones microphone for audio stream. And the best part of it is that you really do not have to think about details such as codecs, fragile network conditions as bandwith fluxations, packets losses, etc. because it is all handled by WebRTC for you. Awesome, right?</p>
<blockquote>
<p>Small word of caution: while you can send media in any codec, the client receiving it must support the used codec as well!</p>
</blockquote>
<p>The media transmission is done by RTP (Real-time Transport Protocol), and RTCP (RTP Control Protocol) protocols, both defined in <a href="https://tools.ietf.org/html/rfc1889" target="_blank" rel="noopener noreferrer">RFC 1889</a>. In short, RTP carries the media streams (audio and video) while RTCP monitors transmission statistics and quality of service.</p>
<p>Every WebRTC connection is authenticated and encrypted with Datagram Transport Layer Security (<a href="https://tools.ietf.org/html/rfc6347" target="_blank" rel="noopener noreferrer">DTLS</a>) and the Secure Real-time Transport Protocol (<a href="https://tools.ietf.org/html/rfc3711" target="_blank" rel="noopener noreferrer">SRTP</a>).</p>
<p>DTLS allows you to negotiate a session and then exchange data securely between two peers. It is a sibling of TLS, the same technology that powers HTTPS, but DTLS uses UDP instead of TCP as the transport layer. SRTP is specifically designed for exchanging media securely.</p>
<h2 id="object-detection">Object Detection</h2>
<p>In computer vision (CV) object detection is an aspect of image processing that detects and classifies different objects within an image. It is a fundamental problem in computer vision that has a wide range of applications, including robotics, surveillance, and image and video analysis.</p>
<p>Some examples of object detection include:</p>
<ul>
<li>Identifying pedestrians in an image or video taken from a self-driving car.</li>
<li>Detecting cars, pedestrians, and other objects in an image or video taken from a surveillance camera.</li>
<li>Detecting objects in images taken by a drone for mapping or inspection purposes.</li>
<li>Identifying objects in images or videos taken by a smartphone camera for augmented reality applications.</li>
<li>Detecting objects in images or videos for image or video annotation or classification purposes.</li>
</ul>
<p><img alt="Pattern Classification" srcset="/_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fpattern_classification_example.png&amp;w=1080&amp;q=75 1x, /_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fpattern_classification_example.png&amp;w=2048&amp;q=75 2x" src="https://softwarescalability.com/_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fpattern_classification_example.png&amp;w=2048&amp;q=75" width="1000" height="657" decoding="async" data-nimg="1" loading="lazy"/></p>
<p>Traditional computer vision techniques, such as detecting colors, shapes, and contours, can be effective for simple object detection tasks where the objects of interest have distinctive features that can be easily identified. These techniques are generally fast and efficient.</p>
<p>However, traditional techniques can be limited in their ability to handle more complex object detection tasks, such as detecting objects that vary in appearance or are occluded by other objects. In these cases, machine learning-based approaches, such as deep learning, is more approriate fit. This leads us to YOLO.</p>
<blockquote>
<p>Deep learning neural networks are able to learn complex patterns in data and can be trained to perform object detection tasks with high accuracy. These approaches are generally more flexible and can handle a wider range of object detection tasks.</p>
</blockquote>
<h2 id="yolo">YOLO</h2>
<p>YOLO (You Only Look Once) is a deep learning model developed by <a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener noreferrer">Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi in 2015</a>. It is a single convolutional neural network (CNN) that is trained to perform object detection by identifying objects in images and predicting the bounding boxes around them.</p>
<p>The key idea behind YOLO is that it only processes an image once, rather than performing object detection in multiple stages. This makes it much faster than other approaches, and allows it to be used in real-time applications.</p>
<p>YOLO works by dividing the input image into a grid of cells, and each cell is responsible for predicting a certain number of bounding boxes. The CNN processes the entire image and produces a set of predictions for each cell in the grid. Each prediction consists of several components: the class probabilities, the bounding box coordinates, and the confidence score.</p>
<blockquote>
<p>Convolutional neural network (CNN) is a type of artificial neural network specifically designed to process data that has a grid-like topology, such as an image. CNNs are particularly effective at image recognition tasks because they are able to learn features directly from the input data, rather than requiring the features to be hand-engineered by a human.</p>
</blockquote>
<p>The class probabilities indicate the likelihood that the bounding box contains an object of each class. The bounding box coordinates define the location and size of the bounding box, and the confidence score reflects the confidence of the prediction.</p>
<p>To make the predictions more accurate, YOLO uses anchor boxes, which are predefined bounding boxes with different aspect ratios. The anchor boxes help the model to better detect objects that have unusual shapes or sizes, as well as handle multiple objects in the same bounding box.</p>
<p>Once the predictions have been made, YOLO uses non-maximum suppression to remove overlapping bounding boxes and keep only the most confident ones. The final set of bounding boxes is then used to label the objects in the image.</p>
<h2 id="solution">Solution</h2>
<p>As you now know about WebRTC and YOLO, let&#39;s proceed with solving the problem: find a person via object-detection by using <code>n</code> amount of camera feeds in preferrably real-time.</p>
<p>If the <code>n</code> was equal to 1 or 2, capturing data and sending it to central server for object detection would be reasonable thing to do.</p>
<p>What if <code>n</code> was equal to 1000? You would need a lot of resources to run this kind of central processing system. Even if you had access to such resources, scaling the system will become very expensive as the higher the <code>n</code>, the higher the compute power need.</p>
<p>What if you used edge computing instead?</p>
<h2 id="edge-computing">Edge computing</h2>
<p>Edge computing is a distributed computing paradigm in which computing and data processing tasks are performed at the edge of the network, closer to the devices generating or collecting the data. This is in contrast to traditional computing, in which tasks are performed in a centralized location, often far from the devices generating or collecting the data.</p>
<p>The main motivation for edge computing is to reduce the amount of data that needs to be transmitted over the network, reduce latency, and improve the performance of real-time applications that require low-latency processing of data.</p>
<p>This can is useful for object detection, as it allows for real-time processing of data. It also combines well with WebRTC.</p>
<p>As the diagram below shows, a camera equipped with processing service uses YOLO for object detection, and sends the data via WebRTC to the end-user. This scales well assuming each camera is attached to a resource-constrained node.</p>
<svg aria-labelledby="chart-title-mermaid-svg-69" aria-roledescription="flowchart-v2" viewBox="-7.5 -8 1229 293" style="max-width:1229px" xmlns="http://www.w3.org/2000/svg" width="100%" id="mermaid-svg-69"><title id="chart-title-mermaid-svg-69">Real time object detection solution</title><g><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="10" viewBox="0 0 10 10" id="flowchart-pointEnd"><path style="stroke-width:1;stroke-dasharray:1, 0" d="M 0 0 L 10 5 L 0 10 z"></path></marker><marker orient="auto" markerHeight="12" markerWidth="12" markerUnits="userSpaceOnUse" refY="5" refX="0" viewBox="0 0 10 10" id="flowchart-pointStart"><path style="stroke-width:1;stroke-dasharray:1, 0" d="M 0 5 L 10 10 L 10 0 z"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="11" viewBox="0 0 10 10" id="flowchart-circleEnd"><circle style="stroke-width:1;stroke-dasharray:1, 0" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="-1" viewBox="0 0 10 10" id="flowchart-circleStart"><circle style="stroke-width:1;stroke-dasharray:1, 0" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="12" viewBox="0 0 11 11" id="flowchart-crossEnd"><path style="stroke-width:2;stroke-dasharray:1, 0" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="-1" viewBox="0 0 11 11" id="flowchart-crossStart"><path style="stroke-width:2;stroke-dasharray:1, 0" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><g><g></g><g><path marker-end="url(#flowchart-pointEnd)" style="fill:none" id="L-one-four-0" d="M185.5,104L185.5,109.75C185.5,115.5,185.5,127,227.40104166666666,141.35912138756927C269.3020833333333,155.71824277513858,353.1041666666667,172.93648555027713,395.0052083333333,181.5456069378464L436.90625,190.15472832541568"></path><path marker-end="url(#flowchart-pointEnd)" style="fill:none" id="L-two-four-0" d="M606.5,104L606.5,109.75C606.5,115.5,606.5,127,606.5,138.5C606.5,150,606.5,161.5,606.5,167.25L606.5,173"></path><path marker-end="url(#flowchart-pointEnd)" style="fill:none" id="L-three-four-0" d="M1027.5,104L1027.5,109.75C1027.5,115.5,1027.5,127,985.5989583333334,141.35912138756927C943.6979166666666,155.71824277513858,859.8958333333334,172.93648555027713,817.9947916666666,181.5456069378464L776.09375,190.15472832541568"></path></g><g><g transform="translate(185.5, 138.5)"><g transform="translate(-32.421875, -9.5)"><foreignObject height="19" width="64.84375"><p><span>WebRTC</span></p></foreignObject></g></g><g transform="translate(606.5, 138.5)"><g transform="translate(-32.421875, -9.5)"><foreignObject height="19" width="64.84375"><p><span>WebRTC</span></p></foreignObject></g></g><g transform="translate(1027.5, 138.5)"><g transform="translate(-32.421875, -9.5)"><foreignObject height="19" width="64.84375"><p><span>WebRTC</span></p></foreignObject></g></g></g><g><g transform="translate(429.40625, 165)"><g><g id="four"><rect height="104" width="339.1875" y="8" x="8" ry="0" rx="0"></rect><g transform="translate(125.0390625, 8)"><foreignObject height="19" width="105.109375"><p><span>Central Node</span></p></foreignObject></g></g></g><g><path marker-end="url(#flowchart-pointEnd)" style="fill:none" id="L-d1-d2-0" d="M186.109375,60L190.27604166666666,60C194.44270833333334,60,202.77604166666666,60,211.109375,60C219.44270833333334,60,227.77604166666666,60,231.94270833333334,60L236.109375,60"></path></g><g><g><g transform="translate(0, 0)"><foreignObject height="0" width="0"></foreignObject></g></g></g><g><g transform="translate(279.1484375, 60)" id="flowchart-d2-35"><rect height="34" width="86.078125" y="-17" x="-43.0390625" ry="0" rx="0"></rect><g transform="translate(-35.5390625, -9.5)"><foreignObject height="19" width="71.078125"><p><span>End-user</span></p></foreignObject></g></g><g transform="translate(109.5546875, 60)" id="flowchart-d1-34"><rect height="34" width="153.109375" y="-17" x="-76.5546875" ry="0" rx="0"></rect><g transform="translate(-69.0546875, -9.5)"><foreignObject height="19" width="138.109375"><p><span>Signaling Service</span></p></foreignObject></g></g></g></g><g transform="translate(834.5, -8)"><g><g id="three"><rect height="104" width="371" y="8" x="8" ry="0" rx="0"></rect><g transform="translate(158.28125, 8)"><foreignObject height="19" width="70.4375"><p><span>Node #3</span></p></foreignObject></g></g></g><g><path marker-end="url(#flowchart-pointEnd)" style="fill:none" id="L-c1-c2-0" d="M139.34375,60L143.51041666666666,60C147.67708333333334,60,156.01041666666666,60,164.34375,60C172.67708333333334,60,181.01041666666666,60,185.17708333333334,60L189.34375,60"></path></g><g><g><g transform="translate(0, 0)"><foreignObject height="0" width="0"></foreignObject></g></g></g><g><g transform="translate(271.671875, 60)" id="flowchart-c2-33"><rect height="34" width="164.65625" y="-17" x="-82.328125" ry="5" rx="5"></rect><g transform="translate(-74.828125, -9.5)"><foreignObject height="19" width="149.65625"><p><span>Processing Service</span></p></foreignObject></g></g><g transform="translate(86.171875, 60)" id="flowchart-c1-32"><rect height="34" width="106.34375" y="-17" x="-53.171875" ry="5" rx="5"></rect><g transform="translate(-45.671875, -9.5)"><foreignObject height="19" width="91.34375"><p><span>Video Input</span></p></foreignObject></g></g></g></g><g transform="translate(413.5, -8)"><g><g id="two"><rect height="104" width="371" y="8" x="8" ry="0" rx="0"></rect><g transform="translate(158.28125, 8)"><foreignObject height="19" width="70.4375"><p><span>Node #2</span></p></foreignObject></g></g></g><g><path marker-end="url(#flowchart-pointEnd)" style="fill:none" id="L-b1-b2-0" d="M139.34375,60L143.51041666666666,60C147.67708333333334,60,156.01041666666666,60,164.34375,60C172.67708333333334,60,181.01041666666666,60,185.17708333333334,60L189.34375,60"></path></g><g><g><g transform="translate(0, 0)"><foreignObject height="0" width="0"></foreignObject></g></g></g><g><g transform="translate(271.671875, 60)" id="flowchart-b2-31"><rect height="34" width="164.65625" y="-17" x="-82.328125" ry="5" rx="5"></rect><g transform="translate(-74.828125, -9.5)"><foreignObject height="19" width="149.65625"><p><span>Processing Service</span></p></foreignObject></g></g><g transform="translate(86.171875, 60)" id="flowchart-b1-30"><rect height="34" width="106.34375" y="-17" x="-53.171875" ry="5" rx="5"></rect><g transform="translate(-45.671875, -9.5)"><foreignObject height="19" width="91.34375"><p><span>Video Input</span></p></foreignObject></g></g></g></g><g transform="translate(-7.5, -8)"><g><g id="one"><rect height="104" width="371" y="8" x="8" ry="0" rx="0"></rect><g transform="translate(158.28125, 8)"><foreignObject height="19" width="70.4375"><p><span>Node #1</span></p></foreignObject></g></g></g><g><path marker-end="url(#flowchart-pointEnd)" style="fill:none" id="L-a1-a2-0" d="M139.34375,60L143.51041666666666,60C147.67708333333334,60,156.01041666666666,60,164.34375,60C172.67708333333334,60,181.01041666666666,60,185.17708333333334,60L189.34375,60"></path></g><g><g><g transform="translate(0, 0)"><foreignObject height="0" width="0"></foreignObject></g></g></g><g><g transform="translate(271.671875, 60)" id="flowchart-a2-29"><rect height="34" width="164.65625" y="-17" x="-82.328125" ry="5" rx="5"></rect><g transform="translate(-74.828125, -9.5)"><foreignObject height="19" width="149.65625"><p><span>Processing Service</span></p></foreignObject></g></g><g transform="translate(86.171875, 60)" id="flowchart-a1-28"><rect height="34" width="106.34375" y="-17" x="-53.171875" ry="5" rx="5"></rect><g transform="translate(-45.671875, -9.5)"><foreignObject height="19" width="91.34375"><p><span>Video Input</span></p></foreignObject></g></g></g></g></g></g></g></svg>

<blockquote>
<p>You can optimize the solution as a node doesn&#39;t have to process just 1 camera. You could also use mesh network instead where multiple cameras feeds are processed in parellel by a hub.</p>
</blockquote>
<p>As you can see, only processing service needs to be built, and bundle it with desired hardware for processing the camera input.</p>
<h2 id="processing-service-poc">Processing service PoC</h2>
<p>Let&#39;s build the processing service with Python.</p>
<blockquote>
<p>Want to learn Python? Check out O&#39;Reilly platform by <a href="https://www.oreilly.com/online-learning/individuals/start-learning-python.html?https://www.oreilly.com/online-learning/individuals/start-learning-python.html?utm_medium=affiliate&amp;utm_source=cj&amp;utm_campaign=trial&amp;utm_content=aff+learning+platform+python+300x250banner_cj_13714333_1" target="_blank" rel="noopener noreferrer">clicking here</a>.</p>
</blockquote>
<p>Install some Python dependencies.</p>
<pre><code>pip3 install opencv-python aiortc numpy aiohttp
</code></pre>
<blockquote>
<p>The solution code can be found at Github - <a href="https://github.com/genert/real-time-object-detection-with-webrtc-and-yolo" target="_blank" rel="noopener noreferrer">https://github.com/genert/real-time-object-detection-with-webrtc-and-yolo</a></p>
</blockquote>
<p>Copy &amp; paste the <code>videostream-cli</code> example from <a href="https://github.com/aiortc/aiortc/blob/main/examples/videostream-cli/cli.py" target="_blank" rel="noopener noreferrer">aiortc</a>.</p>
<p>This will serve as boilerplate for the YOLO detection code as you only need to work with a class that extends aiortc&#39;s <code>VideoStreamTrack</code>.</p>
<p>You are going to use <a href="https://github.com/WongKinYiu/yolov7" target="_blank" rel="noopener noreferrer">YOLOV7</a> which is based on the <a href="https://arxiv.org/abs/2207.02696" target="_blank" rel="noopener noreferrer">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors paper</a>.</p>
<p>Let&#39;s proceed with writing the class.</p>
<div data-rehype-pretty-code-fragment=""><pre data-language="python" data-theme="default"><code data-language="python" data-theme="default"><span><span>model </span><span>=</span><span> </span><span>&#34;yolov7-tiny_480x640.onnx&#34;</span></span>
<span> </span>
<span><span>class</span><span> </span><span>YOLOVideoStreamTrack</span><span>(</span><span>VideoStreamTrack</span><span>):</span></span>
<span><span>    </span><span>&#34;&#34;&#34;</span></span>
<span><span>    A video track thats returns camera track with annotated detected objects.</span></span>
<span><span>    &#34;&#34;&#34;</span></span>
<span><span>    </span><span>def</span><span> </span><span>__init__</span><span>(self, conf_thres</span><span>=</span><span>0.7</span><span>, iou_thres</span><span>=</span><span>0.5</span><span>):</span></span>
<span><span>        </span><span>super</span><span>().</span><span>__init__</span><span>()  </span><span># don&#39;t forget this!</span></span>
<span><span>        </span><span>self</span><span>.conf_threshold </span><span>=</span><span> conf_thres</span></span>
<span><span>        </span><span>self</span><span>.iou_threshold </span><span>=</span><span> iou_thres</span></span>
<span> </span>
<span><span>        video </span><span>=</span><span> cv2.VideoCapture(</span><span>0</span><span>)</span></span>
<span><span>        </span><span>self</span><span>.video </span><span>=</span><span> video</span></span>
<span> </span>
<span><span>        </span><span># Initialize model</span></span>
<span><span>        </span><span>self</span><span>.net </span><span>=</span><span> cv2.dnn.readNet(model)</span></span>
<span><span>        input_shape </span><span>=</span><span> os.path.splitext(os.path.basename(model))[</span><span>0</span><span>].split(</span><span>&#39;_&#39;</span><span>)[</span><span>-</span><span>1</span><span>].split(</span><span>&#39;x&#39;</span><span>)</span></span>
<span><span>        </span><span>self</span><span>.input_height </span><span>=</span><span> </span><span>int</span><span>(input_shape[</span><span>0</span><span>])</span></span>
<span><span>        </span><span>self</span><span>.input_width </span><span>=</span><span> </span><span>int</span><span>(input_shape[</span><span>1</span><span>])</span></span>
<span> </span>
<span><span>        </span><span>self</span><span>.class_names </span><span>=</span><span> </span><span>list</span><span>(</span><span>map</span><span>(</span><span>lambda</span><span> x: x.strip(), </span><span>open</span><span>(</span><span>&#39;coco.names&#39;</span><span>, </span><span>&#39;r&#39;</span><span>).readlines()))</span></span>
<span><span>        </span><span>self</span><span>.colors </span><span>=</span><span> np.random.default_rng(</span><span>3</span><span>).uniform(</span><span>0</span><span>, </span><span>255</span><span>, </span><span>size</span><span>=</span><span>(</span><span>len</span><span>(</span><span>self</span><span>.class_names), </span><span>3</span><span>))</span></span>
<span> </span>
<span><span>        </span><span>self</span><span>.output_names </span><span>=</span><span> </span><span>self</span><span>.net.getUnconnectedOutLayersNames()</span></span></code></pre></div>
<p>The YOLOVideoStreamTrack class has a constructor which takes two optional arguments: <code>conf_thres</code> and <code>iou_thres</code>. Both of these values will be used for selecting the most appropriate bounding box from the result returned from the object detection model.</p>
<p>A cv2.VideoCapture object is created and initialized with a value of 0, which indicates that it should capture video from the default camera.</p>
<p>The YOLOVideoStreamTrack class also has instance variables for storing a deep learning model, the input shape for the model, a list of class names for the objects that the model can detect, and a list of random colors to use when visualizing the detected objects. These are all initialized in the constructor.</p>
<p>The output_names instance variable is set to a list of the names of the unconnected output layers of the deep learning model. This information is used when running object detection with the YOLO algorithm.</p>
<p>Now, let&#39;s start building the object detection logic. First, you need to prepare the captured frame that is sent to the deep learning model.</p>
<div data-rehype-pretty-code-fragment=""><pre data-language="python" data-theme="default"><code data-language="python" data-theme="default"><span><span>def</span><span> </span><span>prepare_input</span><span>(self, image):</span></span>
<span><span>    </span><span>self</span><span>.img_height, </span><span>self</span><span>.img_width </span><span>=</span><span> image.shape[:</span><span>2</span><span>]</span></span>
<span><span>    input_img </span><span>=</span><span> cv2.cvtColor(image, cv2.</span><span>COLOR_BGR2RGB</span><span>)</span></span>
<span><span>    input_img </span><span>=</span><span> cv2.resize(input_img, (</span><span>self</span><span>.input_width, </span><span>self</span><span>.input_height))</span></span>
<span><span>    </span><span>return</span><span> input_img</span></span></code></pre></div>
<p>The method first stores the height and width of the input image in the img_height and img_width instance variables of the class.</p>
<p>The input image is then converted from the BGR color space to the RGB color space using the cv2.cvtColor function. This is done because many image processing and computer vision libraries use the RGB color space by default.</p>
<p>The input image is then resized to the dimensions specified by the input_width and input_height instance variables using the cv2.resize function. These dimensions are the expected input size for the deep learning model used for object detection.</p>
<p>Now, let&#39;s proceed with detection method.</p>
<div data-rehype-pretty-code-fragment=""><pre data-language="python" data-theme="default"><code data-language="python" data-theme="default"><span><span>def</span><span> </span><span>detect</span><span>(self, frame):</span></span>
<span><span>    input_img </span><span>=</span><span> </span><span>self</span><span>.prepare_input(frame)</span></span>
<span><span>    blob </span><span>=</span><span> cv2.dnn.blobFromImage(input_img, </span><span>1</span><span> </span><span>/</span><span> </span><span>255.0</span><span>)</span></span>
<span><span>    </span><span># Perform inference on the image</span></span>
<span><span>    </span><span>self</span><span>.net.setInput(blob)</span></span>
<span><span>    </span><span># Runs the forward pass to get output of the output layers</span></span>
<span><span>    outputs </span><span>=</span><span> </span><span>self</span><span>.net.forward(</span><span>self</span><span>.output_names)</span></span>
<span> </span>
<span><span>    boxes, scores, class_ids </span><span>=</span><span> </span><span>self</span><span>.process_output(outputs)</span></span>
<span><span>    </span><span>return</span><span> boxes, scores, class_ids</span></span></code></pre></div>
<p>The method first calls the prepare_input method to preprocess the input image. The preprocessed image is then passed to the cv2.dnn.blobFromImage function to create a blob (a multi-dimensional array) that can be passed as input to the deep learning model.</p>
<p>The model is then run on the input data using the net.forward method, which returns the output of the model.</p>
<p>The method then calls the process_output method, passing in the model outputs as an argument. This method processes the output of the model and returns three lists: boxes, scores, and class_ids. The boxes list contains the bounding boxes for the detected objects, the scores list contains the confidence scores for the detected objects, and the class_ids list contains the class IDs for the detected objects.</p>
<div data-rehype-pretty-code-fragment=""><pre data-language="python" data-theme="default"><code data-language="python" data-theme="default"><span><span>def</span><span> </span><span>process_output</span><span>(self, output):</span></span>
<span><span>    predictions </span><span>=</span><span> np.squeeze(output[</span><span>0</span><span>])</span></span>
<span> </span>
<span><span>    </span><span># Filter out object confidence scores below threshold</span></span>
<span><span>    obj_conf </span><span>=</span><span> predictions[:, </span><span>4</span><span>]</span></span>
<span><span>    predictions </span><span>=</span><span> predictions[obj_conf </span><span>&gt;</span><span> </span><span>self</span><span>.conf_threshold]</span></span>
<span><span>    obj_conf </span><span>=</span><span> obj_conf[obj_conf </span><span>&gt;</span><span> </span><span>self</span><span>.conf_threshold]</span></span>
<span> </span>
<span><span>    </span><span># Multiply class confidence with bounding box confidence</span></span>
<span><span>    predictions[:, </span><span>5</span><span>:] </span><span>*=</span><span> obj_conf[:, np.newaxis]</span></span>
<span> </span>
<span><span>    </span><span># Get the scores</span></span>
<span><span>    scores </span><span>=</span><span> np.max(predictions[:, </span><span>5</span><span>:], </span><span>axis</span><span>=</span><span>1</span><span>)</span></span>
<span> </span>
<span><span>    </span><span># Filter out the objects with a low score</span></span>
<span><span>    valid_scores </span><span>=</span><span> scores </span><span>&gt;</span><span> </span><span>self</span><span>.conf_threshold</span></span>
<span><span>    predictions </span><span>=</span><span> predictions[valid_scores]</span></span>
<span><span>    scores </span><span>=</span><span> scores[valid_scores]</span></span>
<span> </span>
<span><span>    </span><span># Get the class with the highest confidence</span></span>
<span><span>    class_ids </span><span>=</span><span> np.argmax(predictions[:, </span><span>5</span><span>:], </span><span>axis</span><span>=</span><span>1</span><span>)</span></span>
<span> </span>
<span><span>    </span><span># Get bounding boxes for each object</span></span>
<span><span>    boxes </span><span>=</span><span> </span><span>self</span><span>.extract_boxes(predictions)</span></span>
<span> </span>
<span><span>    </span><span># Apply non-maxima suppression to suppress weak, overlapping bounding boxes</span></span>
<span><span>    indices </span><span>=</span><span> cv2.dnn.NMSBoxes(boxes.tolist(), scores.tolist(), </span><span>self</span><span>.conf_threshold, </span><span>self</span><span>.iou_threshold)</span></span>
<span><span>    </span><span>if</span><span> </span><span>len</span><span>(indices) </span><span>&gt;</span><span> </span><span>0</span><span>:</span></span>
<span><span>        indices </span><span>=</span><span> indices.flatten()</span></span>
<span> </span>
<span><span>    </span><span>return</span><span> boxes[indices], scores[indices], class_ids[indices]</span></span>
<span> </span>
<span><span>def</span><span> </span><span>rescale_boxes</span><span>(self, boxes):</span></span>
<span><span>    input_shape </span><span>=</span><span> np.array([</span><span>self</span><span>.input_width, </span><span>self</span><span>.input_height, </span><span>self</span><span>.input_width, </span><span>self</span><span>.input_height])</span></span>
<span><span>    boxes </span><span>=</span><span> np.divide(boxes, input_shape, </span><span>dtype</span><span>=</span><span>np.float32)</span></span>
<span><span>    boxes </span><span>*=</span><span> np.array([</span><span>self</span><span>.img_width, </span><span>self</span><span>.img_height, </span><span>self</span><span>.img_width, </span><span>self</span><span>.img_height])</span></span>
<span><span>    </span><span>return</span><span> boxes</span></span>
<span> </span>
<span><span>def</span><span> </span><span>extract_boxes</span><span>(self, predictions):</span></span>
<span><span>    </span><span># Extract boxes from predictions</span></span>
<span><span>    boxes </span><span>=</span><span> predictions[:, :</span><span>4</span><span>]</span></span>
<span> </span>
<span><span>    </span><span># Scale boxes to original image dimensions</span></span>
<span><span>    boxes </span><span>=</span><span> </span><span>self</span><span>.rescale_boxes(boxes)</span></span>
<span> </span>
<span><span>    </span><span># Convert boxes to xywh format</span></span>
<span><span>    boxes_ </span><span>=</span><span> np.copy(boxes)</span></span>
<span><span>    boxes_[</span><span>...</span><span>, </span><span>0</span><span>] </span><span>=</span><span> boxes[</span><span>...</span><span>, </span><span>0</span><span>] </span><span>-</span><span> boxes[</span><span>...</span><span>, </span><span>2</span><span>] </span><span>*</span><span> </span><span>0.5</span></span>
<span><span>    boxes_[</span><span>...</span><span>, </span><span>1</span><span>] </span><span>=</span><span> boxes[</span><span>...</span><span>, </span><span>1</span><span>] </span><span>-</span><span> boxes[</span><span>...</span><span>, </span><span>3</span><span>] </span><span>*</span><span> </span><span>0.5</span></span>
<span><span>    </span><span>return</span><span> boxes_</span></span></code></pre></div>
<p>The process_output method first selects the first element of the output list and squeezes it to remove any dimensions with size 1. This results in a 2D NumPy array called predictions.</p>
<p>The method then filters out predictions with object confidence scores below the conf_threshold instance variable.</p>
<p>The method then extracts the class IDs for each prediction by taking the index of the maximum value in the last 80 columns of the predictions array and storing the result in the class_ids variable.</p>
<p>The method then applies non-maxima suppression (NMS) to the bounding boxes using the cv2.dnn.NMSBoxes function. NMS is a technique used to suppress weak, overlapping bounding boxes and select only the most confident bounding boxes for each object. The indices variable is created to store the indices of the bounding boxes that pass the NMS criteria.</p>
<p>If there are any indices in the indices list, the method flattens the list and uses it to select the bounding boxes, scores, and class IDs for the detected objects. These are then returned as the result of the process_output method.</p>
<p>The rescale_boxes method takes a single argument boxes, which is a list of bounding boxes in the format [x, y, width, height]. The method scales the bounding boxes to the original dimensions of the input image.</p>
<p>The extract_boxes method takes a single argument predictions, which is a 2D NumPy array containing the predictions for each object. The method extracts the bounding boxes from the predictions array and scales them to the original dimensions of the input image using the rescale_boxes method. The method then converts the bounding boxes to the format [x, y, width, height] and returns the resulting list of bounding boxes.</p>
<p>Lastly, you just need a piece of code that draws these on the camera frame.</p>
<div data-rehype-pretty-code-fragment=""><pre data-language="python" data-theme="default"><code data-language="python" data-theme="default"><span><span>def</span><span> </span><span>draw_detections</span><span>(self, frame, boxes, scores, class_ids):</span></span>
<span><span>    </span><span>for</span><span> box, score, class_id </span><span>in</span><span> </span><span>zip</span><span>(boxes, scores, class_ids):</span></span>
<span><span>        x, y, w, h </span><span>=</span><span> box.astype(</span><span>int</span><span>)</span></span>
<span><span>        color </span><span>=</span><span> </span><span>self</span><span>.colors[class_id]</span></span>
<span> </span>
<span><span>        </span><span># Draw rectangle</span></span>
<span><span>        cv2.rectangle(frame, (x, y), (x</span><span>+</span><span>w, y</span><span>+</span><span>h), color, </span><span>thickness</span><span>=</span><span>2</span><span>)</span></span>
<span><span>        label </span><span>=</span><span> </span><span>self</span><span>.class_names[class_id]</span></span>
<span><span>        label </span><span>=</span><span> </span><span>f</span><span>&#39;</span><span>{</span><span>label</span><span>}</span><span> </span><span>{int</span><span>(score </span><span>*</span><span> </span><span>100</span><span>)</span><span>}</span><span>%&#39;</span></span>
<span><span>        cv2.getTextSize(label, cv2.</span><span>FONT_HERSHEY_SIMPLEX</span><span>, </span><span>0.5</span><span>, </span><span>1</span><span>)</span></span>
<span><span>        cv2.putText(frame, label, (x, y </span><span>-</span><span> </span><span>10</span><span>), cv2.</span><span>FONT_HERSHEY_SIMPLEX</span><span>, </span><span>1</span><span>, color, </span><span>thickness</span><span>=</span><span>2</span><span>)</span></span></code></pre></div>
<p>The <code>draw_detections</code> uses the cv2.rectangle function to draw a rectangle on the frame image using the bounding box coordinates and the color for the current object. The method also draws a label for the object on the image using the cv2.putText function. The label includes the class name and the confidence score for the object.</p>
<p>If you run the following code with public webcamera, then this is the result:</p>
<p><img alt="POC result" srcset="/_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fpoc_result.jpeg&amp;w=1920&amp;q=75 1x, /_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fpoc_result.jpeg&amp;w=3840&amp;q=75 2x" src="https://softwarescalability.com/_next/image?url=%2Fassets%2Fblog%2Freal-time-object-detection-with-webrtc-and-yolo%2Fpoc_result.jpeg&amp;w=3840&amp;q=75" width="1280" height="698" decoding="async" data-nimg="1" loading="lazy"/></p>
<blockquote>
<p>The solution code can be found at Github - <a href="https://github.com/genert/real-time-object-detection-with-webrtc-and-yolo" target="_blank" rel="noopener noreferrer">https://github.com/genert/real-time-object-detection-with-webrtc-and-yolo</a></p>
<p>Please keep in mind that this is PoC, and you need additional code to handle different scenarios such as errors, capture not working, no device found, etc.</p>
</blockquote>
<p>And that&#39;s it! Just with some Python libraries, and adding not that complicated code, you have built a service that consumes camera feed which gets proccessed and sent to the end-user.</p>
<h2 id="going-futher">Going futher</h2>
<p>At large scale, you most likely need to optimize the processing service to run on the specific hardware.</p>
<p>First of all, you would consider using C++ or Rust. If you are going to use C++, I recommend using <a href="https://github.com/Tencent/ncnn" target="_blank" rel="noopener noreferrer">ncnn</a>, a high-performance neural network inference framework optimized for the mobile platform. Also, beware that working with WebRTC C++ library can cause some major headache therefore as a word of caution, you are better off writing your own implementation for it.</p>
<p>Secondly, you could use other object detection algorithms although YOLO is currently the most efficient one, detecting unique entities, counting entities, and so on.</p>
<p>Thirdly, you might consider using data channel for sending data with the media channel instead of annotating media frames. You could also use data channel for whenever motion is detected, etc.</p>
<p>Nevertheless, I hope you learned something new!</p>
<p>Thank you for reading, and subscribe to keep up with new articles.</p></div></div></div>
  </body>
</html>
