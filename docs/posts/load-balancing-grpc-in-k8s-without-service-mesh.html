<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nayak.io/posts/grpc_loadbalancing/">Original</a>
    <h1>Load balancing gRPC in K8s without service mesh</h1>
    
    <div id="readability-page-1" class="page"><div><section><article><header><div><p><span><i aria-hidden="true"></i>
<time datetime="2022-04-19T15:54:51+05:30">April 19, 2022</time></span>
<span><i aria-hidden="true"></i>
4-minute read</span></p></div></header><div><p><img src="https://nayak.io/grpc/grpc_loadbalance.png" alt="gRPC loadbalancer"/></p><p>Loadbalancing communication in <a href="https://majidfn.com/blog/grpc-load-balancing/">gRPC is tricky</a>! This is due to the fact that the selling point of gRPC is it’s ability to create sticky connections. In short, gRPC uses a single TCP connection and multiplexes requests on top of that connection.</p><p>This means that the <a href="https://www.nginx.com/resources/glossary/layer-4-load-balancing">layer 4 load balancer</a> provided by <a href="https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/">K8s doesn’t work well for gRPC</a>. Usually this problem is solved by using a service mesh, which will do the load balancing on layer 7 (see <a href="https://linkerd.io/">Linkerd</a>, <a href="https://istio.io/latest/">Istio</a>). This is the ideal solution to the problem of load balancing gRPC connections in k8s.</p><h2 id="but-what-happens-if-you-dont-have-the-option-of-using-a-service-mesh">But what happens if you don’t have the option of using a service mesh?
<a href="#but-what-happens-if-you-dont-have-the-option-of-using-a-service-mesh"><i aria-hidden="true"></i></a></h2><p>At my <a href="https://www.deliveryhero.com/">current workplace</a>, I was stuck in a scenario wherein we had control over both the client and server code, but infrastructure was managed by a dedicated team. The infra team was still beta-testing service mesh on k8s, but only on non-production environment’s.</p><p>Most teams (us included), continued to use gRPC in new services because of multiple reasons (future readiness, protobuf enforcement…so on), but there was one big problem. How do you circumvent the load balancing issue?</p><p>So the solution we were following was simple, use a new gRPC connection per request. Yes, it does kinda render the main feature of gRPC (sticky connections useless). But who doesn’t like creating tech debt?</p><h2 id="context-deadlines">Context Deadlines
<a href="#context-deadlines"><i aria-hidden="true"></i></a></h2><p>So with this solution in mind, we created a new service and used gRPC for communication. Slowly we started noticing a lot of <code>rpc error: code = DeadlineExceeded desc = context deadline exceeded</code> errors between our services. The problem spiked a lot of interest, There was no-clear sign of why this was happening.</p><p>This got me interested about the effectiveness of our solution. That’s when I decided to experiment and collect data:</p><h2 id="experiment">Experiment
<a href="#experiment"><i aria-hidden="true"></i></a></h2><p>For this experiment we’ll create a simple server and client which communicate in gRPC.</p><ul><li>The server will simply echo the message sent by the client</li><li>The server will sleep for 100ms to emulate work being done for each request</li><li>Clients have a timeout of 300ms set</li><li>We’ll run the experiment 5 times over multiple connection sizes to average out the values</li><li>The entire code is in Go and we’ll use protoc to generate the go code from the protobuf</li><li>We’ll test this with 1 client and 1 server initially</li></ul><h2 id="single-connection-with-requests-multiplexed">Single connection with requests multiplexed
<a href="#single-connection-with-requests-multiplexed"><i aria-hidden="true"></i></a></h2><p><img src="https://nayak.io/grpc/single_grpc.png" alt="gRPC single client server"/></p><p>Here we create a single connection and requests are multiplexed over it. This is the way gRPC was meant be used. This will act as our base case for comparison.</p><p><img src="https://nayak.io/grpc/sg_error.png" alt="single connection: error rate"/>
<img src="https://nayak.io/grpc/sg_latency.png" alt="single connection: latency"/></p><p>Notice that the error rate is 0 upto 10000 connections and slowly starts to peak soon after. All errors here are context deadlines due to lots of requests being made to the server and the server not being able to respond within the set timeout (300ms).</p><p>Code: <a href="https://github.com/KarthikNayak/grpc_test/tree/single_connection">https://github.com/KarthikNayak/grpc_test/tree/single_connection</a></p><h2 id="new-connection-for-each-request">New connection for each request
<a href="#new-connection-for-each-request"><i aria-hidden="true"></i></a></h2><p><img src="https://nayak.io/grpc/multi_grpc.png" alt="gRPC single client server"/></p><p>This is the current solution we’re using, here for each request we create a new gRPC connection. The performance can be expected to be bad here since we’re recreating connections. In fact, is might be worse than HTTP/1.1 which uses persistent connections (remember gRPC uses HTTP/2).</p><p><img src="https://nayak.io/grpc/mc_error.png" alt="error rate"/>
<img src="https://nayak.io/grpc/mc_latency.png" alt="latency"/></p><p>From this we can see that using a new connection per request is much worse, the error rate spikes up much earlier (at around 1000 requests). Even the latency is much higher, the lower latency for higher number of connections is mostly due to the increase in error rate.</p><p>Code: <a href="https://github.com/KarthikNayak/grpc_test/tree/connection_per_request">https://github.com/KarthikNayak/grpc_test/tree/connection_per_request</a></p><h2 id="using-keepalive-on-the-server-side">Using Keepalive on the server side
<a href="#using-keepalive-on-the-server-side"><i aria-hidden="true"></i></a></h2><p>While searching for a solution which might be closer in performance to using a single connection, while also providing load-balancing, I stumbled upon the <a href="https://pkg.go.dev/google.golang.org/grpc/keepalive?utm_source=godoc#ServerParameters">keepalive parameters for gRPC</a>.</p><p>If you look at <code>ServerParameters.MaxConnectionAge</code>, it says that the server can end connections as per the set duration. So the question is, if the server sends a GOAWAY to the client, does the client reconnect to the same server, or does it try to resolve to a new server IP.</p><p>To test this, I extended our code to be run on 3 servers on minikube. What we can see is that the <strong>client does DNS resolution each time</strong>. This is amazing, since this means, we get load balancing as a side effect over time. To summarize, the client creates a sticky connection with one server, after the configured time, the server sends a GOAWAY signal to the client. The client then re-establishes the connection either with the same or a new server based on the DNS resolution. Over time the sticky connections should be balanced over all the servers available.</p><p><img src="https://nayak.io/grpc/server_timeout.png" alt="Server Timeout"/></p><p>We can see this in the gRPC logs:</p><p><img src="https://nayak.io/grpc/logs.png" alt="gRPC logs"/></p><p>Also checking the logs of the three servers, we can see that the load is balanced over the lifespan:</p><p><img src="https://nayak.io/grpc/minikube.png" alt="minikube logs"/></p><p>Code: <a href="https://github.com/KarthikNayak/grpc_test/tree/server_timeout">https://github.com/KarthikNayak/grpc_test/tree/server_timeout</a></p><p>The best part is that the error rate and latency is almost comparable to the ideal scenario (<em>Note: we used 1s max timeout</em>).</p><p><img src="https://nayak.io/grpc/timeout_error.png" alt="error rate"/>
<img src="https://nayak.io/grpc/timeout_latency.png" alt="latency"/></p><h3 id="summary">Summary
<a href="#summary"><i aria-hidden="true"></i></a></h3><p>When you want to loadbalance gRPC connections, use a service mesh. But if that’s not something you have setup, using max timeout settings on the server side is a really good alternative!</p></div></article></section></div></div>
  </body>
</html>
