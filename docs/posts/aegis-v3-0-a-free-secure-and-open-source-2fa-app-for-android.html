<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/beemdevelopment/Aegis/releases/tag/v3.0">Original</a>
    <h1>Aegis v3.0 – a free, secure and open source 2FA app for Android</h1>
    
    <div id="readability-page-1" class="page"><div>
		


		<div>
			
					
			<div>
				<p>Inspired by others at the Recurse Center, I recently built a ray tracer by working through the excellent book <em><a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">Ray Tracing in a Weekend</a></em>. I’ve been learning C and was attracted to this project because it was complex enough to be interesting but small enough to be time-constrained (my implementation is ~1,000 lines of code); there’s minimal dependencies so I can focus on learning C instead of learning APIs; and the end result is a pretty picture:</p>
<p><img src="https://github.com/beemdevelopment/Aegis/releases/tag/cover.png" alt="cover"/></p>
<p><em>Cover image of Ray Tracing in a Weekend, rendered by my implementation.</em></p>
<p>I was excited to create this magical, photorealistic image but was completely surprised at how <em>slow</em> my program was. Rendering this image required 3+ hours of wall-clock time on my fancy new MacBook Pro.</p>
<p>I didn’t know what to expect, but that sure felt slow. I wanted to make it faster. I pair programmed with a couple people and they quickly pointed out a very simple solution: I was not passing any optimization level flags to the compiler. We added <code>-O2</code> to my <code>Makefile</code>, and my program was suddenly 15x faster.</p>
<p>This <strong>shocked</strong> me! The computer <em>knew</em>. It knew its default behavior was slow and shitty and it had a literal “make it go faster” button. What was it <em>doing</em>? I had a mental model of how my C code translated into low-level CPU operations, and I did not understand how something as seemingly benign as a compiler flag could make such a huge difference. I wanted to understand. I did some googling, and chatGPT’ing, and found vague answers referring to inlining and loop unrolling but I didn’t have a strong intuition for how much that mattered or how they applied to my program. I found <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">a list</a> of the ~50 flags that go into each optimization level, and I iteratively benchmarked my code with each flag turned on or off, but it didn’t replicate the overall speedup (and I <a href="https://stackoverflow.com/questions/60386091/what-exactly-is-the-difference-between-various-optimization-levels-in-gcc-g">later read</a> that those flags only make up a subset of what <code>-O2</code> does).</p>
<p>So I took things into my own hands. The first thing I did was look at my program’s disassembly:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>objdump -d my_ray_tracer | less</span></span></code></pre></div>
<p>The optimized version was ~2200 lines of assembly while the unoptimized was ~3500. 60% less code is significant! But also not 15x. So I opened it up and tried to read what it said.</p>
<p>Unfortunately for me, I had never looked at arm64 assembly before. I felt I had a reasonable grasp of assembly because I’d recently spent 150 hours working through a binary reverse engineering CTF (<a href="https://microcorruption.com/">microcorruption</a>, it’s the best), but this was foreign. I tried reading documentation but quickly got bored, so I instead took another cue from microcorruption and began to interact with my program dynamically. I opened it up in <code>lldb</code>, a <code>gdb</code>-like debugger for Mac. Microcorruption’s UI was such an effective learning tool that I wanted to replicate it, and I luckily found <a href="https://github.com/gdbinit/lldbinit">lldbinit</a>, an <code>lldb</code> extension that gives you a view of the disassembly, the original source code (if you compile with debugging symbols), and the registers. As you step through instruction-by-instruction, you can see which registers change and how.</p>
<p><img src="https://github.com/beemdevelopment/Aegis/releases/tag/lldbinit.png" alt="lldbinit"/>
<em>Snapshot of <code>lldbinit</code>, showing the registers, stack memory, and disassembly.</em></p>
<p>I first focused on the smallest functions in my program, simple things that multiply or subtract vectors, and stepped through the debugger for both versions. I expected these functions to purely consist of math operations, but the unoptimized version was inundated with load and store instructions. I wasn’t sure if this was widespread, or if these functions were called ten times or a billion times. I needed to collect more data.</p>
<p>Luckily, <code>lldb</code> is completely scriptable. You can write Python code to plug into a <a href="https://lldb.llvm.org/use/python-reference.html">broad swath of its functionality</a>. So I googled some more and copied some example code and made a plugin that logged every executed instruction to a file. I set breakpoints around the innermost loop of my ray tracer, propagated a handful of rays in the optimized and unoptimized versions, and made a histogram of the output.</p>
<p><img src="https://github.com/beemdevelopment/Aegis/releases/tag/instruction_count.png" alt="instruction_count"/>
<em>Y-axis shows instruction, X-axis shows # of executions during 5 iterations of the innermost loop. The unoptimized version is in blue, orange is the same code compiled with <code>-O2</code>.</em></p>
<p>It turned out the unoptimized version was <em>mostly</em> doing loads and store operations (<code>ldr</code>, <code>ldur</code>, <code>str</code>, <code>stur</code>), 10x more frequently than the optimized version. When I looked back at the disassembly, I could see this happening everywhere—at the beginning and end of function calls, in between arithmetic operations, seemingly for no reason. Similarly, instructions associated with function calls (<code>b</code>, <code>bl</code>, <code>ret</code>) were 10x more frequent in the unoptimized version. While I knew “function inlining” was a thing and it made programs faster, I was surprised by the sheer proportion of instructions that were apparently filler.</p>
<p>Some instructions were <em>more</em> common in the optimized version. I’m not sure why the optimized version is doing more floating-point math, but the higher prevalence of <code>ldp</code> instructions looked like a small win: instead of loading one value at a time, this instruction loads two values into a pair of registers at once.</p>
<p>With this high-level understanding, I could now make more sense of the assembly. At a high-level, my ray tracer has four nested for-loops. The innermost loop, a short math routine to calculate whether a ray hits a sphere, is executed a trillion times and dominates the run time. This is what it looks like in C:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span><span> 3
</span></span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="c"><span><span><span>point3_t</span> center <span>=</span> sphere<span>-&gt;</span>center;
</span></span><span><span><span>double</span> radius <span>=</span> sphere<span>-&gt;</span>radius;
</span></span><span><span><span>vec3_t</span> a_c <span>=</span> <span>subtract</span>(r<span>-&gt;</span>origin, center);
</span></span><span><span><span>double</span> a <span>=</span> <span>length_squared</span>(<span>&amp;</span>r<span>-&gt;</span>direction);
</span></span><span><span><span>double</span> half_b <span>=</span> <span>dot</span>(r<span>-&gt;</span>direction, a_c);
</span></span><span><span><span>double</span> c <span>=</span> <span>length_squared</span>(<span>&amp;</span>a_c) <span>-</span> radius<span>*</span>radius;
</span></span><span><span><span>double</span> discriminant <span>=</span> half_b<span>*</span>half_b <span>-</span> a<span>*</span>c;
</span></span><span><span><span>if</span> (discriminant <span>&lt;</span> <span>0</span>) {
</span></span><span><span>  <span>return</span> <span>false</span>;
</span></span><span><span>} <span>else</span> {
</span></span><span><span>   <span>// do stuff
</span></span></span><span><span><span></span>}</span></span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>Here is the disassembly (arm64, intel syntex) after compiling it with <code>-O2</code>:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span><span> 3
</span></span><span><span> 4
</span></span><span><span> 5
</span></span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="armasm"><span><span>ldp     <span>d25</span>, <span>d22</span>, <span>[</span><span>x13</span>, #<span>-</span><span>40</span><span>]!</span>
</span></span><span><span>ldp     <span>d23</span>, <span>d24</span>, <span>[</span><span>x13</span>, #<span>16</span><span>]</span>
</span></span><span><span>fsub    <span>d19</span>, <span>d1</span>, <span>d25</span>
</span></span><span><span>fsub    <span>d27</span>, <span>d2</span>, <span>d22</span>
</span></span><span><span>fsub    <span>d28</span>, <span>d4</span>, <span>d23</span>
</span></span><span><span>fmul    <span>d26</span>, <span>d27</span>, <span>d6</span>
</span></span><span><span>fmadd   <span>d26</span>, <span>d5</span>, <span>d19</span>, <span>d26</span>
</span></span><span><span>fmadd   <span>d26</span>, <span>d0</span>, <span>d28</span>, <span>d26</span>
</span></span><span><span>fmul    <span>d27</span>, <span>d27</span>, <span>d27</span>
</span></span><span><span>fmadd   <span>d19</span>, <span>d19</span>, <span>d19</span>, <span>d27</span>
</span></span><span><span>fmadd   <span>d19</span>, <span>d28</span>, <span>d28</span>, <span>d19</span>
</span></span><span><span>fmsub   <span>d19</span>, <span>d24</span>, <span>d24</span>, <span>d19</span>
</span></span><span><span>fmul    <span>d19</span>, <span>d19</span>, <span>d16</span>
</span></span><span><span>fmadd   <span>d19</span>, <span>d26</span>, <span>d26</span>, <span>d19</span>
</span></span><span><span>fcmp    <span>d19</span>, #<span>0.0</span>
</span></span><span><span>b<span>.mi</span>    <span>0</span><span>x10</span><span>00033</span><span>c0</span> <span>&lt;_ray_color+0</span><span>x88</span><span>&gt;</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>To get oriented I’ve highlighted the subtract call in both. This assembly is roughly how I imagined my C code was being executed: we load the sphere’s coordinates into registers (<code>ldp</code>), and we start subtracting and multiplying those registers (<code>fsub</code>, <code>fmul</code>). Great.</p>
<p>However, here is a snippet of assembly after compiling the same C code <em>without</em> optimization (the <code>subtract()</code> function call is again highlighted):</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span><span>24
</span></span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="armasm"><span><span>sub     <span>sp</span>, <span>sp</span>, #<span>288</span>
</span></span><span><span>stp     <span>x20</span>, <span>x19</span>, <span>[sp</span>, #<span>256</span><span>]</span>
</span></span><span><span>stp     <span>x29</span>, <span>x30</span>, <span>[sp</span>, #<span>272</span><span>]</span>
</span></span><span><span>add     <span>x29</span>, <span>sp</span>, #<span>272</span>
</span></span><span><span>stur    <span>x0</span>, <span>[</span><span>x29</span>, #<span>-</span><span>32</span><span>]</span>
</span></span><span><span>stur    <span>x1</span>, <span>[</span><span>x29</span>, #<span>-</span><span>40</span><span>]</span>
</span></span><span><span>stur    <span>x2</span>, <span>[</span><span>x29</span>, #<span>-</span><span>48</span><span>]</span>
</span></span><span><span>stur    <span>x3</span>, <span>[</span><span>x29</span>, #<span>-</span><span>56</span><span>]</span>
</span></span><span><span>ldur    <span>x8</span>, <span>[</span><span>x29</span>, #<span>-</span><span>32</span><span>]</span>
</span></span><span><span>ldr     <span>q0</span>, <span>[</span><span>x8</span><span>]</span>
</span></span><span><span>ldr     <span>x8</span>, <span>[</span><span>x8</span>, #<span>16</span><span>]</span>
</span></span><span><span>stur    <span>x8</span>, <span>[</span><span>x29</span>, #<span>-</span><span>64</span><span>]</span>
</span></span><span><span>stur    <span>q0</span>, <span>[</span><span>x29</span>, #<span>-</span><span>80</span><span>]</span>
</span></span><span><span>ldur    <span>x8</span>, <span>[</span><span>x29</span>, #<span>-</span><span>32</span><span>]</span>
</span></span><span><span>ldr     <span>d0</span>, <span>[</span><span>x8</span>, #<span>24</span><span>]</span>
</span></span><span><span>stur    <span>d0</span>, <span>[</span><span>x29</span>, #<span>-</span><span>88</span><span>]</span>
</span></span><span><span>ldur    <span>x8</span>, <span>[</span><span>x29</span>, #<span>-</span><span>40</span><span>]</span>
</span></span><span><span>ldr     <span>d2</span>, <span>[</span><span>x8</span>, #<span>16</span><span>]</span>
</span></span><span><span>ldr     <span>d1</span>, <span>[</span><span>x8</span>, #<span>8</span><span>]</span>
</span></span><span><span>ldr     <span>d0</span>, <span>[</span><span>x8</span><span>]</span>
</span></span><span><span>ldur    <span>d5</span>, <span>[</span><span>x29</span>, #<span>-</span><span>64</span><span>]</span>
</span></span><span><span>ldur    <span>d4</span>, <span>[</span><span>x29</span>, #<span>-</span><span>72</span><span>]</span>
</span></span><span><span>ldur    <span>d3</span>, <span>[</span><span>x29</span>, #<span>-</span><span>80</span><span>]</span>
</span></span><span><span>bl      <span>0</span><span>x10</span><span>00018dc</span> <span>&lt;_subtract&gt;</span>
</span></span><span><span>stur    <span>d2</span>, <span>[</span><span>x29</span>, #<span>-</span><span>96</span><span>]</span>
</span></span><span><span>stur    <span>d1</span>, <span>[</span><span>x29</span>, #<span>-</span><span>104</span><span>]</span>
</span></span><span><span>stur    <span>d0</span>, <span>[</span><span>x29</span>, #<span>-</span><span>112</span><span>]</span>
</span></span><span><span>ldur    <span>x8</span>, <span>[</span><span>x29</span>, #<span>-</span><span>40</span><span>]</span>
</span></span><span><span>add     <span>x0</span>, <span>x8</span>, #<span>24</span>
</span></span><span><span>bl      <span>0</span><span>x10</span><span>00017</span><span>f8</span> <span>&lt;_length_squared&gt;</span>
</span></span><span><span><span>; ...much more code below here...
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>This is a mess! While the optimized version required 16 instructions for the entire routine, this version takes 24 just to get to the subtract call. Before that we see a function prologue, a series of loads and stores of certain registers. Inlining optimizes all of this away. Then there are more loads and stores until finally the subtract function is called (not inlined, so it will have its <em>own</em> function prologue, etc.). And most interestingly, in between each operation (<code>subtract(), </code>length_squared()`) it stores the result to memory and loads the next value, which will be slow. When the compiler optimizes, it somehow identifies the most-used variables and keeps them in registers, avoiding unnecessary memory reads/writes.</p>
<p>I later learned that <code>clang</code> is happy to tell you all about the optimizations it does if you pass it the flags <code>-Rpass=.*</code> and <code>-fsave-optimization-record=yaml</code>. But this investigation gave me a much stronger intuition for what these optimizations do and why they work. And it also gave me ideas for more speed-ups: I know SIMD is a thing, but I don’t see it being used here (except perhaps the <code>ldp</code> instruction to load a pair of values?).</p>
<p>Can we use SIMD to make this go faster?</p>

			</div>

			</div>
	</div></div>
  </body>
</html>
