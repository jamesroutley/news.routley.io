<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blogs.nvidia.com/blog/2022/09/08/hopper-mlperf-inference/">Original</a>
    <h1>Nvidia Hopper Sweeps AI Inference Benchmarks in MLPerf Debut</h1>
    
    <div id="readability-page-1" class="page"><div id="bsf_rt_marker"><div data-url="https://blogs.nvidia.com/blog/2022/09/08/hopper-mlperf-inference/" data-title="NVIDIA Hopper Sweeps AI Inference Benchmarks in MLPerf Debut" data-hashtags=""><p>In their debut on the <a href="https://www.nvidia.com/en-us/data-center/mlperf/">MLPerf</a> industry-standard AI benchmarks, <a href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a> set world records in inference on all workloads, delivering up to 4.5x more performance than previous-generation GPUs.</p>
<p>The results demonstrate that Hopper is the premium choice for users who demand utmost performance on advanced AI models.</p>
<p>Additionally, <a href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100 Tensor Core GPUs</a> and the <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson AGX Orin</a> module for AI-powered robotics continued to deliver overall leadership inference performance across all MLPerf tests: image and speech recognition, natural language processing and recommender systems.</p>
<p>The H100, aka Hopper, raised the bar in per-accelerator performance across all six neural networks in the round. It demonstrated leadership in both throughput and speed in separate server and offline scenarios.</p>
<figure id="attachment_59428" aria-describedby="caption-attachment-59428"><a href="https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-scaled.jpg"><picture>
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-672x369.jpg.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-400x220.jpg.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-768x422.jpg.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-1536x844.jpg.webp 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-scaled.jpg.webp 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-819x450.jpg.webp 819w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-392x215.jpg.webp 392w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-182x100.jpg.webp 182w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-1280x703.jpg.webp 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
<img src="https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-672x369.jpg" alt="Hopper performance on MLPerf AI inference tests" width="672" height="369" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-672x369.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-400x220.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-768x422.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-1536x844.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-819x450.jpg 819w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-392x215.jpg 392w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-182x100.jpg 182w, https://blogs.nvidia.com/wp-content/uploads/2022/09/H100-final-1280x703.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
</picture>
</a><figcaption id="caption-attachment-59428">NVIDIA H100 GPUs set new high watermarks on all workloads in the data center category.</figcaption></figure>
<p>The NVIDIA Hopper architecture delivered up to 4.5x more performance than NVIDIA Ampere architecture GPUs, which continue to provide overall leadership in MLPerf results.</p>
<p>Thanks in part to its <a href="https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/">Transformer Engine</a>, Hopper excelled on the popular BERT model for natural language processing. It’s among the largest and most performance-hungry of the MLPerf AI models.</p>
<p>These inference benchmarks mark the first public demonstration of H100 GPUs, which will be available later this year. The H100 GPUs will participate in future MLPerf rounds for training.</p>
<h2><b>A100 GPUs Show Leadership</b></h2>
<p>NVIDIA A100 GPUs, available today from major cloud service providers and systems manufacturers, continued to show overall leadership in mainstream performance on AI inference in the latest tests.</p>
<p>A100 GPUs won more tests than any submission in data center and edge computing categories and scenarios. In <a href="https://blogs.nvidia.com/blog/2022/06/29/nvidia-partners-ai-mlperf/">June</a>, the A100 also delivered overall leadership in MLPerf training benchmarks, demonstrating its abilities across the AI workflow.</p>
<p>Since their July 2020 <a href="https://blogs.nvidia.com/blog/2020/07/29/mlperf-training-benchmark-records/">debut</a> on MLPerf, A100 GPUs<a href="https://blogs.nvidia.com/blog/2022/06/29/nvidia-partners-ai-mlperf/"> have advanced their performance by 6x</a>, thanks to continuous improvements in NVIDIA AI software.</p>
<p>NVIDIA AI is the only platform to run all MLPerf inference workloads and scenarios in data center and edge computing.</p>
<h2><b>Users Need Versatile Performance</b></h2>
<p>The ability of NVIDIA GPUs to deliver leadership performance on all major AI models makes users the real winners. Their real-world applications typically employ many neural networks of different kinds.</p>
<p>For example, an AI application may need to understand a user’s spoken request, classify an image, make a recommendation and then deliver a response as a spoken message in a human-sounding voice. Each step requires a different type of AI model.</p>
<p>The MLPerf benchmarks cover these and other popular AI workloads and scenarios — computer vision, natural language processing, recommendation systems, speech recognition and more. The tests ensure users will get performance that’s dependable and flexible to deploy.</p>
<p>Users rely on MLPerf results to make informed buying decisions, because the tests are transparent and objective. The benchmarks enjoy backing from a broad group that includes Amazon, Arm, Baidu, Google, Harvard, Intel, Meta, Microsoft, Stanford and the University of Toronto.</p>
<h2><b>Orin Leads at the Edge</b></h2>
<p>In edge computing, NVIDIA Orin ran every MLPerf benchmark, winning more tests than any other low-power system-on-a-chip. And it showed  up to a 50% gain in energy efficiency compared to <a href="https://blogs.nvidia.com/blog/2022/04/06/mlperf-edge-ai-inference-orin/">its debut</a> on MLPerf in April.</p>
<p>In the previous round, Orin ran up to 5x faster than the prior-generation Jetson AGX Xavier module, while delivering an average of 2x better energy efficiency.</p>
<figure id="attachment_59458" aria-describedby="caption-attachment-59458"><a href="https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-scaled.jpg"><picture loading="lazy">
<source type="image/webp" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-672x384.jpg.webp 672w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-400x229.jpg.webp 400w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-768x439.jpg.webp 768w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-1536x879.jpg.webp 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-scaled.jpg.webp 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-787x450.jpg.webp 787w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-376x215.jpg.webp 376w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-175x100.jpg.webp 175w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-1280x732.jpg.webp 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
<img loading="lazy" src="https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-672x384.jpg" alt="Orin leads MLPerf in edge inference" width="672" height="384" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-672x384.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-400x229.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-768x439.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-1536x879.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-787x450.jpg 787w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-376x215.jpg 376w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-175x100.jpg 175w, https://blogs.nvidia.com/wp-content/uploads/2022/09/Orin-final-NU-1280x732.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px"/>
</picture>
</a><figcaption id="caption-attachment-59458">Orin delivered up to 50% gains in energy efficiency for AI inference at the edge.</figcaption></figure>
<p>Orin integrates into a single chip an <a href="https://www.nvidia.com/en-us/data-center/ampere-architecture/">NVIDIA Ampere architecture</a> GPU and a cluster of powerful Arm CPU cores. It’s available today in the <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson AGX Orin</a> developer kit and production modules for robotics and autonomous systems, and supports the full NVIDIA AI software stack, including platforms for autonomous vehicles (<a href="https://developer.nvidia.com/drive/drive-hyperion">NVIDIA Hyperion</a>), medical devices (<a href="https://www.nvidia.com/en-us/clara/medical-devices/">Clara Holoscan</a>) and robotics (<a href="https://www.nvidia.com/en-us/deep-learning-ai/industries/robotics/">Isaac</a>).</p>
<h2><b>Broad NVIDIA AI Ecosystem</b></h2>
<p>The MLPerf results show NVIDIA AI is backed by the industry’s broadest ecosystem in machine learning.</p>
<p>More than 70 submissions in this round ran on the NVIDIA platform.  For example, Microsoft Azure submitted results running NVIDIA AI on its cloud services.</p>
<p>In addition, 19 <a href="https://www.nvidia.com/en-us/data-center/products/certified-systems/">NVIDIA-Certified Systems</a> appeared in this round from 10 systems makers, including ASUS, Dell Technologies, Fujitsu, GIGABYTE, Hewlett Packard Enterprise, Lenovo and Supermicro.</p>
<p>Their work shows users can get great performance with NVIDIA AI both in the cloud and in servers running in their own data centers.</p>
<p>NVIDIA partners participate in MLPerf because they know it’s a valuable tool for customers evaluating AI platforms and vendors. Results in the latest round demonstrate that the performance they deliver to users today will grow with the NVIDIA platform.</p>
<p>All the software used for these tests is available from the MLPerf repository, so anyone can get these world-class results. Optimizations are continuously folded into containers available on <a href="https://ngc.nvidia.com/catalog">NGC</a>, NVIDIA’s catalog for GPU-accelerated software. That’s where you’ll also find <a href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT</a>, used by every submission in this round to optimize AI inference.</p>
<p><em>Read our Technical Blog for a deeper dive into the <a href="https://developer.nvidia.com/blog/full-stack-innovation-fuels-highest-mlperf-inference-2-1-results-for-nvidia/">technology fueling NVIDIA’s MLPerf performance</a>.</em></p>
</div></div></div>
  </body>
</html>
