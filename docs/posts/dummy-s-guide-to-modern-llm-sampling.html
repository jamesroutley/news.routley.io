<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rentry.co/samplers">Original</a>
    <h1>Dummy&#39;s Guide to Modern LLM Sampling</h1>
    
    <div id="readability-page-1" class="page"><div>
                <div>
                    
                    <article>
                        
                        <div>

<h2 id="intro-knowledge">Intro Knowledge<a href="#intro-knowledge" title="Permanent link"> </a></h2>
<p>Large Language Models (LLMs) work by taking a piece of text (e.g. user prompt) and calculating the next word. In more technical terms, <em>tokens</em>. LLMs have a vocabulary, or a dictionary, of valid tokens, and will reference those in training and inference (the process of generating text). More on that below. You need to understand why we use tokens (sub-words) instead of words or letters first. But first, a short glossary of some technical terms that aren&#39;t explained in the sections below in-depth:</p>
<h3 id="short-glossary">Short Glossary<a href="#short-glossary" title="Permanent link"> </a></h3>
<p><strong>Logits</strong>: The raw, unnormalized scores output by the model for each token in its vocabulary. Higher logits indicate tokens the model considers more likely to come next.</p>
<h3 id="why-tokens">Why tokens?<a href="#why-tokens" title="Permanent link"> </a></h3>
<p>Your first instinct might be using a vocabulary of words or letters for an LLM. But instead, we use sub-words: some common words are preserved as whole in the vocabulary (e.g. <code>the</code>, or <code>apple</code> might be a single token due to how common they are in the English language), but others are fragmented into common sub-words (e.g. <code>bi-fur-cat-ion</code> Why is this? There are several, very good reasons:</p>
<h5 id="why-not-letters">Why not letters?<a href="#why-not-letters" title="Permanent link"> </a></h5>
<p>Many reasons. To name a few: LLMs have a limited context window (amount of tokens it can process at once). With character-level tokenization, even a moderate amount of text would lead to sequence length explosion (too many tokens for too little text). The word <code>tokenization</code> would be 12 tokens instead of, for example, 2 or 3 in a sub-word system. Longer sequences also require much more computation for <a href="https://arxiv.org/abs/1706.03762" rel="" target="_blank">self-attention</a>. But more importantly, the model would need to learn higher-level patterns spanning <em>a lot</em> more positions. Understanding that <code>t-h-e</code> represents a single concept requires connecting information across three positions instead of one. This may also lead to meaningful relationships becoming more distant. Related concepts might be dozens or hundreds of positions apart.</p>
<h5 id="why-not-whole-words">Why not whole words?<a href="#why-not-whole-words" title="Permanent link"> </a></h5>
<p>A pure world-level tokenization would need us to create a vocabulary spanning the entire possible word list in the English language, and if we&#39;re doing multiple languages, then many times that. This would make our embedding matrix unreasonably large and expensive. It would also struggle with <strong>new</strong> or <strong>rare</strong> words. When a model encounters a word not in its vocabulary, it typically replaces it with an &#34;unknown&#34; token, losing virtually all semantic information. Sub-word tokenization would represent new words by combining existing subword tokens. For example, if we invent a new word called &#34;grompuficious&#34;, a sub-word tokenizer might represent it as <code>g-romp-u-ficious</code>, depending on the tokenizer. </p>
<h4 id="how-are-the-sub-words-chosen">How are the sub-words chosen?<a href="#how-are-the-sub-words-chosen" title="Permanent link"> </a></h4>
<p>If a language model uses a new tokenizer, the development team may decide to take a representative sample of their training data, and <em>train</em> a tokenizer to find the most commond sub-words in the dataset. They will set a vocabulary size beforehand, and then the tokenizer will try and find enough sub-words to fill up the list.</p>
<h3 id="how-does-the-model-generate-text">How does the model generate text?<a href="#how-does-the-model-generate-text" title="Permanent link"> </a></h3>
<p>During training, the model sees many terabytes worth of text and builds an internal probability map for tokens. For example, after it&#39;s seen the tokens for <code>How are</code> are usually followed by the tokens <code>you?</code>, it will learn that to be the most probable next set of tokens. Once this map has been built internally to a satisfactory degree, the training is stopped and a checkpoint is released to the public (or kept private and served from an API, e.g. OpenAI). During inference, the user will provide the LLM with a text, and the LLM, based on the probabilities it&#39;s learned through training, will decide what token comes next. <strong>However</strong>, it will not decide just one token: it will take into consideration <em>every possible token</em> that exists in its vocabulary, assigns a probability score to each, and (depending on your sampler) will only output the most probable token, i.e. the one with the highest score. This would make for a rather boring output (unless you need determinism), so this is where Sampling comes in.</p>
<h3 id="from-tokens-to-text-how-llms-generate-content">From Tokens to Text: How LLMs Generate Content<a href="#from-tokens-to-text-how-llms-generate-content" title="Permanent link"> </a></h3>
<p>Now that we understand how LLMs break down and represent text using tokens, let&#39;s explore how they actually generate content. The process of text generation in LLMs involve two key steps:</p>
<ol>
<li><strong>Prediction</strong>: For each position, the model calculates the probability distribution over all possible next tokens in its vocabulary.</li>
<li><strong>Selection</strong>: The model must choose one token from this distribution to add to the growing text.</li>
</ol>
<p>The first step is fixed - determined by the model&#39;s parameters after training. However, the second step - token selection - is where  <strong>sampling</strong> occurs. While we could simply always choose the most likely token (known as &#34;greedy&#34; sampling), this tends to produce repetitive, deterministic text. Sampling introduces controlled randomness to make outputs more varied.</p>
<h2 id="sampling">Sampling<a href="#sampling" title="Permanent link"> </a></h2>
<p>As explained above, LLMs will pick the most probable token to generate. Sampling is the practice of introducing <em>controlled randomness</em>. With pure &#34;greedy&#34; sampling, it would pick the #1 option every time, but that&#39;s boring! We use sampling methods like temperature, penalties, or truncation to allow for a bit of creative variation. This document will go through every popular sampling method, and explains how all of them word; both from a simple-to-understand and technical perspectives.</p>
<h3 id="notes-on-algorithm-presentations">Notes on Algorithm Presentations<a href="#notes-on-algorithm-presentations" title="Permanent link"> </a></h3>
<p>Throughout this document, algorithms are presented in pseudo-code format that combines mathematical notation with programming concepts. Here are some guidelines to help interpret these:</p>
<h5 id="notation-guide">Notation Guide<a href="#notation-guide" title="Permanent link"> </a></h5>
<ul>
<li><strong>L</strong>: the logits tensor (raw scores output by the model)</li>
<li><strong>P</strong>: probabilities (after applying softmax to logits)</li>
<li><strong>←</strong>: assignment operation (equal to <code>=</code> in programming)</li>
<li><strong>∑</strong>: summation</li>
<li><strong>|x|</strong>: either absolute value or length/size of <code>x</code>, depending on context</li>
<li><strong>x[i]</strong>: accessing the <code>i</code>-th element of <code>x</code></li>
<li><strong>v</strong>: logical OR operation</li>
<li><strong>¬</strong>: logical NOT operation</li>
<li><strong>∞</strong>: infinity (often used to mask out tokens by setting logits to negative infinity)</li>
<li><strong>argmax(x)</strong>: returns the index of the maximum value in <code>x</code></li>
<li><strong>∈</strong>: &#34;element of&#34; (e.g., <code>x ∈ X</code> means <code>x</code> is an element of set <code>X</code>)</li>
</ul>
<h5 id="implementation-considerations">Implementation Considerations<a href="#implementation-considerations" title="Permanent link"> </a></h5>
<p>The algorithms provided are written for clarity rather than optimization. Production implementations would typically:</p>
<ol>
<li>Vectorize operations where possible for efficiency</li>
<li>handle edge cases and numerical stability issues (though parts that need this have occasionally been highlighted in the algorithms below)</li>
<li>Incorporate batch processing for multiple sequences, if necessary for the framework</li>
<li>Cache intermediate results where beneficial</li>
</ol>
<h3 id="temperature">Temperature<a href="#temperature" title="Permanent link"> </a></h3>
<p>Think of this as the &#34;creativity knob&#34; on your LLM. At low temperatures (close to 0), the model becomes very cautious and predictable - it almost always picks the most likely next word. It&#39;s like ordering the same dish at your favourite restaurant every time because you know you&#39;ll like it (or maybe you don&#39;t know any better). At higher temperatures (like 0.7-1.0), the model gets very creative and willing to take chances. It may choose the 3rd or 4th most likely word instead of always the top choice. This makes text more varied and interesting, but also increases the chance of errors. Very high temperatures (above 1.0) make the model wild and unpredictable, unless you use it in conjunction with other sampling methods (e.g. min-p) to reign it in.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-1-1"><a id="L-1-1" name="L-1-1"></a><span>Algorithm</span> <span>1</span> <span>Temperature</span> <span>Sampling</span>
</span><span id="L-1-2"><a id="L-1-2" name="L-1-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>temperature</span> <span>parameter</span> <span>T</span>
</span><span id="L-1-3"><a id="L-1-3" name="L-1-3"></a><span>Output</span>: <span>Modified</span> <span>logits</span> <span>with</span> <span>adjusted</span> <span>probability</span> <span>distribution</span>
</span><span id="L-1-4"><a id="L-1-4" name="L-1-4"></a><span>1</span>: <span>T</span> ← <span>max</span><span>(</span><span>T</span>, ε<span>)</span>  <span>//</span> <span>Prevent</span> <span>numerical</span> <span>issues</span> <span>with</span> <span>extremely</span> <span>low</span> <span>temperatures</span>
</span><span id="L-1-5"><a id="L-1-5" name="L-1-5"></a><span>2</span>: <span>if</span> <span>T</span> <span>&lt;</span> <span>0</span>.<span>1</span> <span>then</span>
</span><span id="L-1-6"><a id="L-1-6" name="L-1-6"></a><span>3</span>:     <span>L</span> ← <span>L</span> <span>-</span> <span>max</span><span>(</span><span>L</span><span>)</span> <span>+</span> <span>1</span>  <span>//</span> <span>Shift</span> <span>range</span> <span>to</span> [<span>-</span><span>inf</span>, <span>1</span>] <span>for</span> <span>numerical</span> <span>stability</span>
</span><span id="L-1-7"><a id="L-1-7" name="L-1-7"></a><span>4</span>: <span>end</span> <span>if</span>
</span><span id="L-1-8"><a id="L-1-8" name="L-1-8"></a><span>5</span>: <span>L</span> ← <span>L</span> <span>/</span> <span>T</span>  <span>//</span> <span>Apply</span> <span>temperature</span> <span>scaling</span>
</span><span id="L-1-9"><a id="L-1-9" name="L-1-9"></a><span>6</span>: <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="presence-penalty">Presence Penalty<a href="#presence-penalty" title="Permanent link"> </a></h3>
<p>This discourages the model from repeating any token that has appeared before, regardless of how many times it&#39;s been used. Think of it like a party host who wants to make sure everyone gets a turn to speak. If Tim has already spoken once, he gets slightly discouraged from speaking again, whether he spoke once or ten times before. This is generally not recommend, since better penalty strategies exist (see: DRY).</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-2-1"><a id="L-2-1" name="L-2-1"></a><span>Algorithm</span> <span>2</span> <span>Presence</span> <span>Penalty</span>
</span><span id="L-2-2"><a id="L-2-2" name="L-2-2"></a><span>Required</span><span>:</span> <span>Logits</span> <span>tensor</span> <span>L</span><span>,</span> <span>output</span> <span>tokens</span> <span>O</span><span>,</span> <span>penalty</span> <span>weight</span> <span>λp</span>
</span><span id="L-2-3"><a id="L-2-3" name="L-2-3"></a><span>Output</span><span>:</span> <span>Modified</span> <span>logits</span> <span>with</span> <span>penalty</span> <span>applied</span> <span>for</span> <span>token</span> <span>presence</span>
</span><span id="L-2-4"><a id="L-2-4" name="L-2-4"></a><span>1</span><span>:</span> <span>Vsize</span> <span>←</span> <span>|</span><span>L</span><span>[</span><span>0</span><span>]</span><span>|</span>  <span>//</span> <span>Vocabulary</span> <span>size</span> <span>from</span> <span>logits</span> <span>dimension</span>
</span><span id="L-2-5"><a id="L-2-5" name="L-2-5"></a><span>2</span><span>:</span> <span>Moutput</span> <span>←</span> <span>BinaryMask</span><span>(</span><span>O</span><span>,</span> <span>Vsize</span><span>)</span>  <span>//</span> <span>Create</span> <span>binary</span> <span>mask</span> <span>where</span> <span>token</span> <span>has</span> <span>appeared</span> <span>at</span> <span>least</span> <span>once</span>
</span><span id="L-2-6"><a id="L-2-6" name="L-2-6"></a><span>3</span><span>:</span> <span>P</span> <span>←</span> <span>λp</span> <span>·</span> <span>Moutput</span>  <span>//</span> <span>Calculate</span> <span>penalty</span> <span>matrix</span>
</span><span id="L-2-7"><a id="L-2-7" name="L-2-7"></a><span>4</span><span>:</span> <span>L</span> <span>←</span> <span>L</span> <span>-</span> <span>P</span>  <span>//</span> <span>Apply</span> <span>presence</span> <span>penalty</span> <span>to</span> <span>logits</span>
</span><span id="L-2-8"><a id="L-2-8" name="L-2-8"></a><span>5</span><span>:</span> <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="frequency-penalty">Frequency Penalty<a href="#frequency-penalty" title="Permanent link"> </a></h3>
<p>Discourages tokens based on how many times they&#39;ve already been used. This is simply Presence Penalty but with the number of occurrences being taken into account. The more frequently a word has appeared, the less likely it will appear again.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-3-1"><a id="L-3-1" name="L-3-1"></a><span>Algorithm</span> <span>3</span> <span>Frequency</span> <span>Penalty</span>
</span><span id="L-3-2"><a id="L-3-2" name="L-3-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>output</span> <span>tokens</span> <span>O</span>, <span>penalty</span> <span>weight</span> λ<span>f</span>
</span><span id="L-3-3"><a id="L-3-3" name="L-3-3"></a><span>Output</span>: <span>Modified</span> <span>logits</span> <span>with</span> <span>penalty</span> <span>applied</span> <span>proportional</span> <span>to</span> <span>token</span> <span>frequency</span>
</span><span id="L-3-4"><a id="L-3-4" name="L-3-4"></a><span>1</span>: <span>Vsize</span> ← <span>|</span><span>L</span>[<span>0</span>]<span>|</span>  <span>//</span> <span>Vocabulary</span> <span>size</span> <span>from</span> <span>logits</span> <span>dimension</span>
</span><span id="L-3-5"><a id="L-3-5" name="L-3-5"></a><span>2</span>: <span>Coutput</span> ← <span>TokenCounts</span><span>(</span><span>O</span>, <span>Vsize</span><span>)</span>  <span>//</span> <span>Count</span> <span>occurrences</span> <span>of</span> <span>each</span> <span>token</span> <span>in</span> <span>output</span>
</span><span id="L-3-6"><a id="L-3-6" name="L-3-6"></a><span>3</span>: <span>P</span> ← λ<span>f</span> · <span>Coutput</span>  <span>//</span> <span>Calculate</span> <span>penalty</span> <span>matrix</span> <span>proportional</span> <span>to</span> <span>counts</span>
</span><span id="L-3-7"><a id="L-3-7" name="L-3-7"></a><span>4</span>: <span>L</span> ← <span>L</span> <span>-</span> <span>P</span>  <span>//</span> <span>Apply</span> <span>frequency</span> <span>penalty</span> <span>to</span> <span>logits</span>
</span><span id="L-3-8"><a id="L-3-8" name="L-3-8"></a><span>5</span>: <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="repetition-penalty">Repetition Penalty<a href="#repetition-penalty" title="Permanent link"> </a></h3>
<p>The repetition penalty works a bit differently from the other two: it penalizes both tokens from the prompt and generated output, affecting positive and negative logits differently. For positive scores, it divides the score by the penalty (making it smaller); for negative scores, it multiplies by the penalty (making it more negative). Useful for breaking out of loops, with the cost of coherency at more aggressive values.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-4-1"><a id="L-4-1" name="L-4-1"></a><span>Algorithm</span> <span>4</span> <span>Repetition</span> <span>Penalty</span>
</span><span id="L-4-2"><a id="L-4-2" name="L-4-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>prompt</span> <span>tokens</span> <span>P</span>, <span>output</span> <span>tokens</span> <span>O</span>, <span>penalty</span> <span>factor</span> λ<span>r</span>
</span><span id="L-4-3"><a id="L-4-3" name="L-4-3"></a><span>Output</span>: <span>Modified</span> <span>logits</span> <span>with</span> <span>asymmetric</span> <span>penalty</span> <span>for</span> <span>repeated</span> <span>tokens</span>
</span><span id="L-4-4"><a id="L-4-4" name="L-4-4"></a><span>1</span>: <span>Vsize</span> ← <span>|</span><span>L</span>[<span>0</span>]<span>|</span>  <span>//</span> <span>Vocabulary</span> <span>size</span> <span>from</span> <span>logits</span> <span>dimension</span>
</span><span id="L-4-5"><a id="L-4-5" name="L-4-5"></a><span>2</span>: <span>Mprompt</span> ← <span>BinaryMask</span><span>(</span><span>P</span>, <span>Vsize</span><span>)</span>  <span>//</span> <span>Create</span> <span>binary</span> <span>mask</span> <span>for</span> <span>tokens</span> <span>in</span> <span>prompt</span>
</span><span id="L-4-6"><a id="L-4-6" name="L-4-6"></a><span>3</span>: <span>Moutput</span> ← <span>BinaryMask</span><span>(</span><span>O</span>, <span>Vsize</span><span>)</span>  <span>//</span> <span>Create</span> <span>binary</span> <span>mask</span> <span>for</span> <span>tokens</span> <span>in</span> <span>output</span>
</span><span id="L-4-7"><a id="L-4-7" name="L-4-7"></a><span>4</span>: <span>M</span> ← <span>Mprompt</span> ∨ <span>Moutput</span>  <span>//</span> <span>Combined</span> <span>mask</span> <span>for</span> <span>tokens</span> <span>in</span> <span>either</span> <span>prompt</span> <span>or</span> <span>output</span>
</span><span id="L-4-8"><a id="L-4-8" name="L-4-8"></a><span>5</span>: <span>R</span> ← <span>matrix</span> <span>of</span> <span>size</span> <span>|</span><span>L</span><span>|</span> × <span>Vsize</span> <span>filled</span> <span>with</span> λ<span>r</span>
</span><span id="L-4-9"><a id="L-4-9" name="L-4-9"></a><span>6</span>: <span>R</span>[¬<span>M</span>] ← <span>1</span>.<span>0</span>  <span>//</span> <span>No</span> <span>penalty</span> <span>for</span> <span>tokens</span> <span>not</span> <span>previously</span> <span>seen</span>
</span><span id="L-4-10"><a id="L-4-10" name="L-4-10"></a><span>7</span>: <span>for</span> <span>each</span> <span>position</span> <span>(</span><span>i</span>,<span>j</span><span>)</span> <span>in</span> <span>L</span> <span>do</span>
</span><span id="L-4-11"><a id="L-4-11" name="L-4-11"></a><span>8</span>:     <span>if</span> <span>L</span>[<span>i</span>,<span>j</span>] <span>&gt;</span> <span>0</span> <span>then</span>
</span><span id="L-4-12"><a id="L-4-12" name="L-4-12"></a><span>9</span>:         <span>L</span>[<span>i</span>,<span>j</span>] ← <span>L</span>[<span>i</span>,<span>j</span>] <span>/</span> <span>R</span>[<span>i</span>,<span>j</span>]  <span>//</span> <span>Divide</span> <span>positive</span> <span>logits</span>
</span><span id="L-4-13"><a id="L-4-13" name="L-4-13"></a><span>10</span>:     <span>else</span>
</span><span id="L-4-14"><a id="L-4-14" name="L-4-14"></a><span>11</span>:         <span>L</span>[<span>i</span>,<span>j</span>] ← <span>L</span>[<span>i</span>,<span>j</span>] <span>*</span> <span>R</span>[<span>i</span>,<span>j</span>]  <span>//</span> <span>Multiply</span> <span>negative</span> <span>logits</span>
</span><span id="L-4-15"><a id="L-4-15" name="L-4-15"></a><span>12</span>:     <span>end</span> <span>if</span>
</span><span id="L-4-16"><a id="L-4-16" name="L-4-16"></a><span>13</span>: <span>end</span> <span>for</span>
</span><span id="L-4-17"><a id="L-4-17" name="L-4-17"></a><span>14</span>: <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="dry-dont-repeat-yourself">DRY (Don&#39;t Repeat Yourself)<a href="#dry-dont-repeat-yourself" title="Permanent link"> </a></h3>
<p>DRY sampling is like having an editor who watches for repetitive patterns in your writing. Let&#39;s say you&#39;re writing a story and you&#39;ve already used the phrase &#34;once upon a time&#34; - DRY will discourage you from using that exact same phrase again. But it&#39;s much smarter than simple word repetition prevention (like the 3 penalties above).</p>
<p>DRY looks for repeating patterns (called n-grams) in your text. If it notices you&#39;ve written something like &#34;the cat sat on the&#34; before and you&#39;re about to repeat this pattern, it discourages the next word that would continue the repetition. The longer the repeating pattern, the stronger the discouragement. This prevents the text from falling into loops or recycling the same phrases and keeps the output fresh and varied. What makes DRY special is that it considers existing patterns. It&#39;s particularly useful for creative writing where repetitive text would sound unnatural.</p>
<p><strong>Technical</strong>:</p>
<p>The penalty is applied exponentially based on the length of the matching pattern, with longer matches receiving stronger penalties. This creates a dynamic system that prevents repetition while still allowing natural text flow, making output text more diverse and human-like by avoiding the repetitive patterns that often plague language models.</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-5-1"><a id="L-5-1" name="L-5-1"></a><span>Algorithm</span> <span>5</span> <span>DRY</span> <span>(</span><span>Don</span><span>&#39;</span><span>t</span> <span>Repeat</span> <span>Yourself</span><span>)</span> <span>Sampling</span>
</span><span id="L-5-2"><a id="L-5-2" name="L-5-2"></a><span>Required</span><span>:</span> <span>Logits</span> <span>tensor</span> <span>L</span><span>,</span> <span>input</span> <span>tokens</span> <span>I</span><span>,</span> <span>output</span> <span>tokens</span> <span>O</span><span>,</span> <span>multiplier</span> <span>λ</span><span>,</span> <span>base</span> <span>b</span><span>,</span> 
</span><span id="L-5-3"><a id="L-5-3" name="L-5-3"></a>         <span>minimum</span> <span>n</span><span>-</span><span>gram</span> <span>length</span> <span>Nmin</span><span>,</span> <span>sequence</span> <span>breaker</span> <span>tokens</span> <span>B</span><span>,</span> <span>range</span> <span>limit</span> <span>r</span><span>,</span>
</span><span id="L-5-4"><a id="L-5-4" name="L-5-4"></a>         <span>maximum</span> <span>n</span><span>-</span><span>gram</span> <span>length</span> <span>Nmax</span><span>,</span> <span>maximum</span> <span>occurrences</span> <span>M</span><span>,</span> <span>early</span> <span>exit</span> <span>threshold</span> <span>E</span>
</span><span id="L-5-5"><a id="L-5-5" name="L-5-5"></a><span>Output</span><span>:</span> <span>Modified</span> <span>logits</span> <span>with</span> <span>penalties</span> <span>for</span> <span>repeating</span> <span>patterns</span>
</span><span id="L-5-6"><a id="L-5-6" name="L-5-6"></a>
</span><span id="L-5-7"><a id="L-5-7" name="L-5-7"></a><span>1</span><span>:</span> <span>for</span> <span>each</span> <span>sequence</span> <span>s</span> <span>where</span> <span>λ</span> <span>&gt;</span> <span>0</span> <span>do</span>
</span><span id="L-5-8"><a id="L-5-8" name="L-5-8"></a><span>2</span><span>:</span>     <span>//</span> <span>Prepare</span> <span>token</span> <span>sequence</span> <span>by</span> <span>concatenating</span> <span>prompt</span> <span>and</span> <span>generated</span> <span>tokens</span>
</span><span id="L-5-9"><a id="L-5-9" name="L-5-9"></a><span>3</span><span>:</span>     <span>p</span> <span>←</span> <span>Length</span> <span>of</span> <span>valid</span> <span>tokens</span> <span>in</span> <span>I</span><span>[</span><span>s</span><span>]</span>
</span><span id="L-5-10"><a id="L-5-10" name="L-5-10"></a><span>4</span><span>:</span>     <span>q</span> <span>←</span> <span>Length</span> <span>of</span> <span>valid</span> <span>tokens</span> <span>in</span> <span>O</span><span>[</span><span>s</span><span>]</span>
</span><span id="L-5-11"><a id="L-5-11" name="L-5-11"></a><span>5</span><span>:</span>     <span>T</span> <span>←</span> <span>Concatenate</span><span>(</span><span>I</span><span>[</span><span>s</span><span>][</span><span>1</span><span>:p</span><span>],</span> <span>O</span><span>[</span><span>s</span><span>][</span><span>1</span><span>:q</span><span>])</span>
</span><span id="L-5-12"><a id="L-5-12" name="L-5-12"></a><span>6</span><span>:</span>
</span><span id="L-5-13"><a id="L-5-13" name="L-5-13"></a><span>7</span><span>:</span>     <span>if</span> <span>r</span> <span>&gt;</span> <span>0</span> <span>then</span>
</span><span id="L-5-14"><a id="L-5-14" name="L-5-14"></a><span>8</span><span>:</span>         <span>T</span> <span>←</span> <span>T</span><span>[</span><span>-</span><span>r</span><span>:]</span>  <span>//</span> <span>Consider</span> <span>only</span> <span>the</span> <span>last</span> <span>r</span> <span>tokens</span> <span>if</span> <span>range</span> <span>limit</span> <span>is</span> <span>set</span>
</span><span id="L-5-15"><a id="L-5-15" name="L-5-15"></a><span>9</span><span>:</span>     <span>end</span> <span>if</span>
</span><span id="L-5-16"><a id="L-5-16" name="L-5-16"></a><span>10</span><span>:</span>
</span><span id="L-5-17"><a id="L-5-17" name="L-5-17"></a><span>11</span><span>:</span>     <span>if</span> <span>|</span><span>T</span><span>|</span> <span>&lt;</span> <span>2</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>  <span>//</span> <span>Skip</span> <span>if</span> <span>sequence</span> <span>too</span> <span>short</span>
</span><span id="L-5-18"><a id="L-5-18" name="L-5-18"></a><span>12</span><span>:</span>
</span><span id="L-5-19"><a id="L-5-19" name="L-5-19"></a><span>13</span><span>:</span>     <span>last</span> <span>←</span> <span>T</span><span>[</span><span>-</span><span>1</span><span>]</span>  <span>//</span> <span>Last</span> <span>token</span> <span>in</span> <span>sequence</span>
</span><span id="L-5-20"><a id="L-5-20" name="L-5-20"></a><span>14</span><span>:</span>     <span>if</span> <span>last</span> <span>∈</span> <span>B</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>  <span>//</span> <span>Skip</span> <span>if</span> <span>last</span> <span>token</span> <span>is</span> <span>a</span> <span>sequence</span> <span>breaker</span>
</span><span id="L-5-21"><a id="L-5-21" name="L-5-21"></a><span>15</span><span>:</span>
</span><span id="L-5-22"><a id="L-5-22" name="L-5-22"></a><span>16</span><span>:</span>     <span>//</span> <span>Create</span> <span>mask</span> <span>for</span> <span>sequence</span> <span>breaker</span> <span>positions</span>
</span><span id="L-5-23"><a id="L-5-23" name="L-5-23"></a><span>17</span><span>:</span>     <span>breakMask</span> <span>←</span> <span>ZeroVector</span><span>(|</span><span>T</span><span>|)</span>
</span><span id="L-5-24"><a id="L-5-24" name="L-5-24"></a><span>18</span><span>:</span>     <span>for</span> <span>each</span> <span>token</span> <span>id</span> <span>b</span> <span>∈</span> <span>B</span> <span>do</span>
</span><span id="L-5-25"><a id="L-5-25" name="L-5-25"></a><span>19</span><span>:</span>         <span>breakMask</span> <span>←</span> <span>breakMask</span> <span>∨</span> <span>(</span><span>T</span> <span>=</span> <span>b</span><span>)</span>
</span><span id="L-5-26"><a id="L-5-26" name="L-5-26"></a><span>20</span><span>:</span>     <span>end</span> <span>for</span>
</span><span id="L-5-27"><a id="L-5-27" name="L-5-27"></a><span>21</span><span>:</span>
</span><span id="L-5-28"><a id="L-5-28" name="L-5-28"></a><span>22</span><span>:</span>     <span>//</span> <span>Find</span> <span>maximum</span> <span>allowed</span> <span>n</span><span>-</span><span>gram</span> <span>length</span> <span>before</span> <span>hitting</span> <span>a</span> <span>sequence</span> <span>breaker</span>
</span><span id="L-5-29"><a id="L-5-29" name="L-5-29"></a><span>23</span><span>:</span>     <span>maxN</span> <span>←</span> <span>0</span>
</span><span id="L-5-30"><a id="L-5-30" name="L-5-30"></a><span>24</span><span>:</span>     <span>for</span> <span>i</span> <span>from</span> <span>1</span> <span>to</span> <span>min</span><span>(|</span><span>breakMask</span><span>|,</span> <span>Nmax</span><span>)</span> <span>do</span>
</span><span id="L-5-31"><a id="L-5-31" name="L-5-31"></a><span>25</span><span>:</span>         <span>if</span> <span>breakMask</span><span>[</span><span>-</span><span>i</span><span>]</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>
</span><span id="L-5-32"><a id="L-5-32" name="L-5-32"></a><span>26</span><span>:</span>         <span>maxN</span> <span>←</span> <span>i</span>
</span><span id="L-5-33"><a id="L-5-33" name="L-5-33"></a><span>27</span><span>:</span>     <span>end</span> <span>for</span>
</span><span id="L-5-34"><a id="L-5-34" name="L-5-34"></a><span>28</span><span>:</span>
</span><span id="L-5-35"><a id="L-5-35" name="L-5-35"></a><span>29</span><span>:</span>     <span>if</span> <span>maxN</span> <span>≤</span> <span>Nmin</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>  <span>//</span> <span>Skip</span> <span>if</span> <span>maximum</span> <span>n</span><span>-</span><span>gram</span> <span>too</span> <span>short</span>
</span><span id="L-5-36"><a id="L-5-36" name="L-5-36"></a><span>30</span><span>:</span>
</span><span id="L-5-37"><a id="L-5-37" name="L-5-37"></a><span>31</span><span>:</span>     <span>//</span> <span>Initialize</span> <span>array</span> <span>to</span> <span>track</span> <span>longest</span> <span>matching</span> <span>n</span><span>-</span><span>gram</span> <span>for</span> <span>each</span> <span>token</span>
</span><span id="L-5-38"><a id="L-5-38" name="L-5-38"></a><span>32</span><span>:</span>     <span>ngramLengths</span> <span>←</span> <span>ZeroVector</span><span>(</span><span>VocabularySize</span><span>)</span>
</span><span id="L-5-39"><a id="L-5-39" name="L-5-39"></a><span>33</span><span>:</span>
</span><span id="L-5-40"><a id="L-5-40" name="L-5-40"></a><span>34</span><span>:</span>     <span>//</span> <span>Find</span> <span>all</span> <span>positions</span> <span>where</span> <span>the</span> <span>last</span> <span>token</span> <span>appears</span>
</span><span id="L-5-41"><a id="L-5-41" name="L-5-41"></a><span>35</span><span>:</span>     <span>endpoints</span> <span>←</span> <span>FindIndices</span><span>(</span><span>T</span> <span>=</span> <span>last</span><span>)</span>
</span><span id="L-5-42"><a id="L-5-42" name="L-5-42"></a><span>36</span><span>:</span>     <span>if</span> <span>|</span><span>endpoints</span><span>|</span> <span>&lt;</span> <span>2</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>
</span><span id="L-5-43"><a id="L-5-43" name="L-5-43"></a><span>37</span><span>:</span>
</span><span id="L-5-44"><a id="L-5-44" name="L-5-44"></a><span>38</span><span>:</span>     <span>//</span> <span>Remove</span> <span>the</span> <span>last</span> <span>occurrence</span> <span>(</span><span>current</span> <span>position</span><span>)</span>
</span><span id="L-5-45"><a id="L-5-45" name="L-5-45"></a><span>39</span><span>:</span>     <span>endpoints</span> <span>←</span> <span>endpoints</span><span>[:-</span><span>1</span><span>]</span>
</span><span id="L-5-46"><a id="L-5-46" name="L-5-46"></a><span>40</span><span>:</span>
</span><span id="L-5-47"><a id="L-5-47" name="L-5-47"></a><span>41</span><span>:</span>     <span>//</span> <span>Limit</span> <span>number</span> <span>of</span> <span>previous</span> <span>occurrences</span> <span>to</span> <span>check</span>
</span><span id="L-5-48"><a id="L-5-48" name="L-5-48"></a><span>42</span><span>:</span>     <span>if</span> <span>|</span><span>endpoints</span><span>|</span> <span>&gt;</span> <span>M</span> <span>then</span>
</span><span id="L-5-49"><a id="L-5-49" name="L-5-49"></a><span>43</span><span>:</span>         <span>endpoints</span> <span>←</span> <span>endpoints</span><span>[</span><span>-</span><span>M</span><span>:</span><span>]</span>
</span><span id="L-5-50"><a id="L-5-50" name="L-5-50"></a><span>44</span><span>:</span>     <span>end</span> <span>if</span>
</span><span id="L-5-51"><a id="L-5-51" name="L-5-51"></a><span>45</span><span>:</span>
</span><span id="L-5-52"><a id="L-5-52" name="L-5-52"></a><span>46</span><span>:</span>     <span>//</span> <span>Check</span> <span>each</span> <span>previous</span> <span>occurrence</span> <span>of</span> <span>the</span> <span>last</span> <span>token</span> <span>for</span> <span>matching</span> <span>contexts</span>
</span><span id="L-5-53"><a id="L-5-53" name="L-5-53"></a><span>47</span><span>:</span>     <span>for</span> <span>each</span> <span>idx</span> <span>in</span> <span>Reverse</span><span>(</span><span>endpoints</span><span>)</span> <span>do</span>
</span><span id="L-5-54"><a id="L-5-54" name="L-5-54"></a><span>48</span><span>:</span>         <span>if</span> <span>idx</span> <span>=</span> <span>|</span><span>T</span><span>|</span> <span>-</span> <span>1</span> <span>then</span> <span>continue</span> <span>end</span> <span>if</span>
</span><span id="L-5-55"><a id="L-5-55" name="L-5-55"></a><span>49</span><span>:</span>
</span><span id="L-5-56"><a id="L-5-56" name="L-5-56"></a><span>50</span><span>:</span>         <span>matchLen</span> <span>←</span> <span>0</span>
</span><span id="L-5-57"><a id="L-5-57" name="L-5-57"></a><span>51</span><span>:</span>         <span>//</span> <span>Look</span> <span>backward</span> <span>to</span> <span>find</span> <span>matching</span> <span>context</span>
</span><span id="L-5-58"><a id="L-5-58" name="L-5-58"></a><span>52</span><span>:</span>         <span>for</span> <span>u</span> <span>from</span> <span>1</span> <span>to</span> <span>min</span><span>(</span><span>idx</span><span>,</span> <span>maxN</span><span>)</span> <span>do</span>
</span><span id="L-5-59"><a id="L-5-59" name="L-5-59"></a><span>53</span><span>:</span>             <span>if</span> <span>breakMask</span><span>[</span><span>idx</span> <span>-</span> <span>u</span><span>]</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>
</span><span id="L-5-60"><a id="L-5-60" name="L-5-60"></a><span>54</span><span>:</span>             <span>if</span> <span>T</span><span>[</span><span>idx</span> <span>-</span> <span>u</span><span>]</span> <span>≠</span> <span>T</span><span>[</span><span>-</span><span>u</span> <span>-</span> <span>1</span><span>]</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>
</span><span id="L-5-61"><a id="L-5-61" name="L-5-61"></a><span>55</span><span>:</span>             <span>matchLen</span> <span>←</span> <span>u</span>
</span><span id="L-5-62"><a id="L-5-62" name="L-5-62"></a><span>56</span><span>:</span>         <span>end</span> <span>for</span>
</span><span id="L-5-63"><a id="L-5-63" name="L-5-63"></a><span>57</span><span>:</span>
</span><span id="L-5-64"><a id="L-5-64" name="L-5-64"></a><span>58</span><span>:</span>         <span>if</span> <span>matchLen</span> <span>&gt;</span> <span>0</span> <span>then</span>
</span><span id="L-5-65"><a id="L-5-65" name="L-5-65"></a><span>59</span><span>:</span>             <span>nextToken</span> <span>←</span> <span>T</span><span>[</span><span>idx</span> <span>+</span> <span>1</span><span>]</span>  <span>//</span> <span>Token</span> <span>that</span> <span>followed</span> <span>this</span> <span>pattern</span> <span>before</span>
</span><span id="L-5-66"><a id="L-5-66" name="L-5-66"></a><span>60</span><span>:</span>             <span>newLen</span> <span>←</span> <span>matchLen</span> <span>+</span> <span>1</span>
</span><span id="L-5-67"><a id="L-5-67" name="L-5-67"></a><span>61</span><span>:</span>             <span>ngramLengths</span><span>[</span><span>nextToken</span><span>]</span> <span>←</span> <span>max</span><span>(</span><span>ngramLengths</span><span>[</span><span>nextToken</span><span>],</span> <span>newLen</span><span>)</span>
</span><span id="L-5-68"><a id="L-5-68" name="L-5-68"></a><span>62</span><span>:</span>             
</span><span id="L-5-69"><a id="L-5-69" name="L-5-69"></a><span>63</span><span>:</span>             <span>if</span> <span>newLen</span> <span>≥</span> <span>E</span> <span>then</span> <span>break</span> <span>end</span> <span>if</span>  <span>//</span> <span>Early</span> <span>exit</span> <span>if</span> <span>match</span> <span>is</span> <span>long</span> <span>enough</span>
</span><span id="L-5-70"><a id="L-5-70" name="L-5-70"></a><span>64</span><span>:</span>         <span>end</span> <span>if</span>
</span><span id="L-5-71"><a id="L-5-71" name="L-5-71"></a><span>65</span><span>:</span>     <span>end</span> <span>for</span>
</span><span id="L-5-72"><a id="L-5-72" name="L-5-72"></a><span>66</span><span>:</span>     
</span><span id="L-5-73"><a id="L-5-73" name="L-5-73"></a><span>67</span><span>:</span>     <span>//</span> <span>Apply</span> <span>penalties</span> <span>to</span> <span>tokens</span> <span>that</span> <span>would</span> <span>continue</span> <span>repeating</span> <span>patterns</span>
</span><span id="L-5-74"><a id="L-5-74" name="L-5-74"></a><span>68</span><span>:</span>     <span>penaltyMask</span> <span>←</span> <span>(</span><span>ngramLengths</span> <span>&gt;</span> <span>0</span><span>)</span>
</span><span id="L-5-75"><a id="L-5-75" name="L-5-75"></a><span>69</span><span>:</span>     <span>if</span> <span>any</span><span>(</span><span>penaltyMask</span><span>)</span> <span>then</span>
</span><span id="L-5-76"><a id="L-5-76" name="L-5-76"></a><span>70</span><span>:</span>         <span>scales</span> <span>←</span> <span>b</span> <span>^</span> <span>(</span><span>ngramLengths</span><span>[</span><span>penaltyMask</span><span>]</span> <span>-</span> <span>Nmin</span><span>)</span>  <span>//</span> <span>Exponential</span> <span>scaling</span> <span>by</span> <span>pattern</span> <span>length</span>
</span><span id="L-5-77"><a id="L-5-77" name="L-5-77"></a><span>71</span><span>:</span>         <span>L</span><span>[</span><span>s</span><span>][</span><span>penaltyMask</span><span>]</span> <span>←</span> <span>L</span><span>[</span><span>s</span><span>][</span><span>penaltyMask</span><span>]</span> <span>-</span> <span>λ</span> <span>*</span> <span>scales</span>
</span><span id="L-5-78"><a id="L-5-78" name="L-5-78"></a><span>72</span><span>:</span>     <span>end</span> <span>if</span>
</span><span id="L-5-79"><a id="L-5-79" name="L-5-79"></a><span>73</span><span>:</span> <span>end</span> <span>for</span>
</span><span id="L-5-80"><a id="L-5-80" name="L-5-80"></a><span>74</span><span>:</span> <span>return</span> <span>L</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-k">Top-K<a href="#top-k" title="Permanent link"> </a></h3>
<p>Instead of considering all possible next words (which could be tens of thousands), the model narrows down to only the K most likely candidates. If K is 40, the model will only choose from the top 40 most likely next words. This approach prevents the model from selecting extremely unlikely words while still maintaining some randomness.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-6-1"><a id="L-6-1" name="L-6-1"></a><span>Algorithm</span><span> </span><span>6</span><span> </span><span>Top</span><span>-</span><span>K</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-6-2"><a id="L-6-2" name="L-6-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>k</span><span></span>
</span><span id="L-6-3"><a id="L-6-3" name="L-6-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>all</span><span> </span><span>but</span><span> </span><span>top</span><span>-</span><span>k</span><span> </span><span>options</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-6-4"><a id="L-6-4" name="L-6-4"></a><span>1</span><span>:</span><span> </span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>L</span><span>,</span><span> </span><span>descending</span><span>=</span><span>False</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>logits</span><span> </span><span>in</span><span> </span><span>ascending</span><span> </span><span>order</span><span></span>
</span><span id="L-6-5"><a id="L-6-5" name="L-6-5"></a><span>2</span><span>:</span><span> </span><span>kth</span><span> </span><span>←</span><span> </span><span>Lsorted</span><span>[</span><span>|Lsorted| - k</span><span>]</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>the</span><span> </span><span>kth</span><span> </span><span>largest</span><span> </span><span>logit</span><span> </span><span>value</span><span></span>
</span><span id="L-6-6"><a id="L-6-6" name="L-6-6"></a><span>3</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>Lsorted</span><span> </span><span>&lt;</span><span> </span><span>kth</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>values</span><span> </span><span>below</span><span> </span><span>the</span><span> </span><span>threshold</span><span></span>
</span><span id="L-6-7"><a id="L-6-7" name="L-6-7"></a><span>4</span><span>:</span><span> </span><span>Lsorted</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-6-8"><a id="L-6-8" name="L-6-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span> </span><span>←</span><span> </span><span>Unsort</span><span>(</span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span></span>
</span><span id="L-6-9"><a id="L-6-9" name="L-6-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-p">Top-P<a href="#top-p" title="Permanent link"> </a></h3>
<p>Instead of picking a fixed number of options like Top-K, Top-P selects the smallest set of words whose combined probability exceeds threshold <code>P</code>. It&#39;s like saying &#34;I&#39;ll only consider dishes that make up 90% of all orders at this restaurant.&#34; If <code>P</code> is 0.9, the model includes just enough of the highest-probability words to reach 90% cumulative probability, whether that&#39;s 5 words or 500. In situations where the model is very confident, it might only need a few options, but when uncertainty is high, it can consider more possibilities.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-7-1"><a id="L-7-1" name="L-7-1"></a><span>Algorithm</span><span> </span><span>7</span><span> </span><span>Top</span><span>-</span><span>P</span><span> </span><span>(</span><span>Nucleus</span><span>)</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-7-2"><a id="L-7-2" name="L-7-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>probability</span><span> </span><span>threshold</span><span> </span><span>p</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-7-3"><a id="L-7-3" name="L-7-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>smallest</span><span> </span><span>probability</span><span> </span><span>tokens</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-7-4"><a id="L-7-4" name="L-7-4"></a><span>1</span><span>:</span><span> </span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>L</span><span>,</span><span> </span><span>descending</span><span>=</span><span>False</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>logits</span><span> </span><span>in</span><span> </span><span>ascending</span><span> </span><span>order</span><span></span>
</span><span id="L-7-5"><a id="L-7-5" name="L-7-5"></a><span>2</span><span>:</span><span> </span><span>Psorted</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>Lsorted</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-7-6"><a id="L-7-6" name="L-7-6"></a><span>3</span><span>:</span><span> </span><span>Pcum</span><span> </span><span>←</span><span> </span><span>CumulativeSum</span><span>(</span><span>Psorted</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>cumulative</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-7-7"><a id="L-7-7" name="L-7-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>Pcum</span><span> </span><span>≤</span><span> </span><span>(</span><span>1</span><span> </span><span>-</span><span> </span><span>p</span><span>)</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-7-8"><a id="L-7-8" name="L-7-8"></a><span>5</span><span>:</span><span> </span><span>mask</span><span>[</span><span>|mask| - 1</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>at</span><span> </span><span>least</span><span> </span><span>one</span><span> </span><span>token</span><span></span>
</span><span id="L-7-9"><a id="L-7-9" name="L-7-9"></a><span>6</span><span>:</span><span> </span><span>Lsorted</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-7-10"><a id="L-7-10" name="L-7-10"></a><span>7</span><span>:</span><span> </span><span>L</span><span> </span><span>←</span><span> </span><span>Unsort</span><span>(</span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span></span>
</span><span id="L-7-11"><a id="L-7-11" name="L-7-11"></a><span>8</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="min-p">Min-P<a href="#min-p" title="Permanent link"> </a></h3>
<p>Min-P sets a quality threshold relative to these best option. Using the restaurant analogy again, imagine you&#39;re at a restaurant with your friend who always orders the most popular dish. You decide you&#39;ll only consider dishes that are at least 20% as popular as their top choice. If the most popular dish gets ordered 100 times a day, you set Min-P to 0.2, you&#39;ll only consider dishes ordered at least 20 times a day. When the model is very confident about the best choice, the threshold becomes higher, and fewer alternatives are considered. When the model is uncertain, more alternatives pass the threshold.</p>
<p>Min-P is usually used in conjunction with higher temperature values (1.0-1.2), and used in very low values (0.1).</p>
<p><strong>Technical</strong>:</p>
<p>Any token with a probability below this threshold is masked out by setting its logit to <code>-inf</code>. This creates a dynamic filtering mechanism where the cutoff adapts to the confidence level of the model for each prediction. Min-P doesn&#39;t require sorting the entire vocabulary like Top-K or Top-P, so it ends up being more efficient as well.</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-8-1"><a id="L-8-1" name="L-8-1"></a><span>Algorithm</span><span> </span><span>8</span><span> </span><span>Min</span><span>-</span><span>P</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-8-2"><a id="L-8-2" name="L-8-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>fraction</span><span> </span><span>min_p</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-8-3"><a id="L-8-3" name="L-8-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>low</span><span> </span><span>probability</span><span> </span><span>tokens</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-8-4"><a id="L-8-4" name="L-8-4"></a><span>1</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-8-5"><a id="L-8-5" name="L-8-5"></a><span>2</span><span>:</span><span> </span><span>Pmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>probability</span><span></span>
</span><span id="L-8-6"><a id="L-8-6" name="L-8-6"></a><span>3</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>min_p</span><span> </span><span>×</span><span> </span><span>Pmax</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>dynamic</span><span> </span><span>threshold</span><span></span>
</span><span id="L-8-7"><a id="L-8-7" name="L-8-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-8-8"><a id="L-8-8" name="L-8-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-8-9"><a id="L-8-9" name="L-8-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-a">Top-A<a href="#top-a" title="Permanent link"> </a></h3>
<p>This one applies a filter that gets stricter when the model is more confident. Top-A creates a threshold that&#39;s proportional to the square of the highest probability token. When the model is very confident, the threshold becomes much higher due to the squaring effect, dramatically limiting options only to the very best alternatives. When the model is less confident, the threshold drops more rapidly.</p>
<p><strong>Technical</strong>:</p>
<p>Top-A is essentially if min-p was squared instead of linear. Note that Top-A predates Min-P.</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-9-1"><a id="L-9-1" name="L-9-1"></a><span>Algorithm</span><span> </span><span>9</span><span> </span><span>Top</span><span>-</span><span>A</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-9-2"><a id="L-9-2" name="L-9-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>a</span><span></span>
</span><span id="L-9-3"><a id="L-9-3" name="L-9-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>options</span><span> </span><span>below</span><span> </span><span>squared</span><span> </span><span>threshold</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-9-4"><a id="L-9-4" name="L-9-4"></a><span>1</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-9-5"><a id="L-9-5" name="L-9-5"></a><span>2</span><span>:</span><span> </span><span>Pmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>probability</span><span></span>
</span><span id="L-9-6"><a id="L-9-6" name="L-9-6"></a><span>3</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>Pmax²</span><span> </span><span>×</span><span> </span><span>a</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>squared</span><span> </span><span>threshold</span><span></span>
</span><span id="L-9-7"><a id="L-9-7" name="L-9-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-9-8"><a id="L-9-8" name="L-9-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-9-9"><a id="L-9-9" name="L-9-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="xtc-exclude-top-choices">XTC (eXclude Top Choices)<a href="#xtc-exclude-top-choices" title="Permanent link"> </a></h3>
<p>XTC works by occasionally excluding the most likely option based on two parameters: a probability of activation, and a threshold for determining which choices to exclude. When XTC activates (based on random chance), it looks at all the top choices whose probabilities exceed the threshold and removes all but the lowest-scoring one among them.</p>
<p>This forces the model to occasionally &#34;think outside the box&#34; and select words it wouldn&#39;t normally. Unlike other samplers that just filter out unlikely options, XTC specifically targets the most predictable choices.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-10-1"><a id="L-10-1" name="L-10-1"></a><span>Algorithm</span><span> </span><span>10</span><span> </span><span>XTC</span><span> </span><span>(</span><span>eXclude</span><span> </span><span>Top</span><span> </span><span>Choices</span><span>)</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-10-2"><a id="L-10-2" name="L-10-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>t</span><span>,</span><span> </span><span>activation</span><span> </span><span>probability</span><span> </span><span>p</span><span></span>
</span><span id="L-10-3"><a id="L-10-3" name="L-10-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>top</span><span> </span><span>choices</span><span> </span><span>filtered</span><span> </span><span>out</span><span> </span><span>except</span><span> </span><span>for</span><span> </span><span>the</span><span> </span><span>lowest</span><span>-</span><span>scoring</span><span> </span><span>one</span><span></span>
</span><span id="L-10-4"><a id="L-10-4" name="L-10-4"></a><span>1</span><span>:</span><span> </span><span>apply</span><span> </span><span>←</span><span> </span><span>RandomBernoulli</span><span>(</span><span>p</span><span>)</span><span>  </span><span>//</span><span> </span><span>Determine</span><span> </span><span>which</span><span> </span><span>sequences</span><span> </span><span>to</span><span> </span><span>apply</span><span> </span><span>XTC</span><span> </span><span>to</span><span></span>
</span><span id="L-10-5"><a id="L-10-5" name="L-10-5"></a><span>2</span><span>:</span><span> </span><span>if</span><span> </span><span>not</span><span> </span><span>any</span><span>(</span><span>apply</span><span>)</span><span> </span><span>then</span><span> </span><span>return</span><span> </span><span>L</span><span> </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-10-6"><a id="L-10-6" name="L-10-6"></a><span>3</span><span>:</span><span></span>
</span><span id="L-10-7"><a id="L-10-7" name="L-10-7"></a><span>4</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-10-8"><a id="L-10-8" name="L-10-8"></a><span>5</span><span>:</span><span> </span><span>Psorted</span><span>,</span><span> </span><span>Pidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>P</span><span>,</span><span> </span><span>descending</span><span>=</span><span>True</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>probabilities</span><span> </span><span>in</span><span> </span><span>descending</span><span> </span><span>order</span><span></span>
</span><span id="L-10-9"><a id="L-10-9" name="L-10-9"></a><span>6</span><span>:</span><span></span>
</span><span id="L-10-10"><a id="L-10-10" name="L-10-10"></a><span>7</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>sequence</span><span> </span><span>i</span><span> </span><span>where</span><span> </span><span>apply</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>True</span><span> </span><span>do</span><span></span>
</span><span id="L-10-11"><a id="L-10-11" name="L-10-11"></a><span>8</span><span>:</span><span>     </span><span>//</span><span> </span><span>Find</span><span> </span><span>tokens</span><span> </span><span>above</span><span> </span><span>threshold</span><span> </span><span>(</span><span>starting</span><span> </span><span>from</span><span> </span><span>second</span><span>-</span><span>highest</span><span>)</span><span></span>
</span><span id="L-10-12"><a id="L-10-12" name="L-10-12"></a><span>9</span><span>:</span><span>     </span><span>aboveThreshold</span><span> </span><span>←</span><span> </span><span>Psorted</span><span>[</span><span>i, 1:</span><span>]</span><span> </span><span>≥</span><span> </span><span>t</span><span></span>
</span><span id="L-10-13"><a id="L-10-13" name="L-10-13"></a><span>10</span><span>:</span><span></span>
</span><span id="L-10-14"><a id="L-10-14" name="L-10-14"></a><span>11</span><span>:</span><span>     </span><span>//</span><span> </span><span>Count</span><span> </span><span>how</span><span> </span><span>many</span><span> </span><span>tokens</span><span> </span><span>qualify</span><span> </span><span>(</span><span>including</span><span> </span><span>the</span><span> </span><span>top</span><span> </span><span>one</span><span>)</span><span></span>
</span><span id="L-10-15"><a id="L-10-15" name="L-10-15"></a><span>12</span><span>:</span><span>     </span><span>count</span><span> </span><span>←</span><span> </span><span>Sum</span><span>(</span><span>aboveThreshold</span><span>)</span><span> </span><span>+</span><span> </span><span>1</span><span></span>
</span><span id="L-10-16"><a id="L-10-16" name="L-10-16"></a><span>13</span><span>:</span><span></span>
</span><span id="L-10-17"><a id="L-10-17" name="L-10-17"></a><span>14</span><span>:</span><span>     </span><span>if</span><span> </span><span>count</span><span> </span><span>&gt;</span><span> </span><span>1</span><span> </span><span>then</span><span></span>
</span><span id="L-10-18"><a id="L-10-18" name="L-10-18"></a><span>15</span><span>:</span><span>         </span><span>//</span><span> </span><span>Get</span><span> </span><span>indices</span><span> </span><span>of</span><span> </span><span>the</span><span> </span><span>high</span><span>-</span><span>probability</span><span> </span><span>tokens</span><span> </span><span>to</span><span> </span><span>remove</span><span></span>
</span><span id="L-10-19"><a id="L-10-19" name="L-10-19"></a><span>16</span><span>:</span><span>         </span><span>tokensToRemove</span><span> </span><span>←</span><span> </span><span>Pidx</span><span>[</span><span>i, 0:count-1</span><span>]</span><span></span>
</span><span id="L-10-20"><a id="L-10-20" name="L-10-20"></a><span>17</span><span>:</span><span></span>
</span><span id="L-10-21"><a id="L-10-21" name="L-10-21"></a><span>18</span><span>:</span><span>         </span><span>//</span><span> </span><span>Mask</span><span> </span><span>out</span><span> </span><span>all</span><span> </span><span>high</span><span>-</span><span>probability</span><span> </span><span>tokens</span><span> </span><span>except</span><span> </span><span>the</span><span> </span><span>lowest</span><span>-</span><span>scoring</span><span> </span><span>one</span><span></span>
</span><span id="L-10-22"><a id="L-10-22" name="L-10-22"></a><span>19</span><span>:</span><span>         </span><span>L</span><span>[</span><span>i, tokensToRemove</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span></span>
</span><span id="L-10-23"><a id="L-10-23" name="L-10-23"></a><span>20</span><span>:</span><span>     </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-10-24"><a id="L-10-24" name="L-10-24"></a><span>21</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-10-25"><a id="L-10-25" name="L-10-25"></a><span>22</span><span>:</span><span></span>
</span><span id="L-10-26"><a id="L-10-26" name="L-10-26"></a><span>23</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="top-n-sigma">Top-N-Sigma<a href="#top-n-sigma" title="Permanent link"> </a></h3>
<p>This one sets a statistical quality bar for word choices. Think of it like selecting players for a sports team and decide to take anyone who scores within 2 standard deviations of the top performer.</p>
<p>The sampler uses basic statistics to create a more adaptive threshold. It looks at how spread out (standard deviation) the scores for all possible next words are, then sets a cutoff at <code>N</code> standard deviations below the highest-scoring word. In situations where the model has a few very strong preferences and many mediocre options, the standard deviation will be small, creating a stricter threshold. But when there are many similarly good options, the standard deviation increases. Essentially, when the model is certain about a few good choices, it stays focused on those. When there are multiple reasonable options (like in creative writing), it allows for more variety without including truly poor choices.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-11-1"><a id="L-11-1" name="L-11-1"></a><span>Algorithm</span><span> </span><span>11</span><span> </span><span>Top</span><span>-</span><span>N</span><span>-</span><span>Sigma</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-11-2"><a id="L-11-2" name="L-11-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>n</span><span> </span><span>(</span><span>number</span><span> </span><span>of</span><span> </span><span>standard</span><span> </span><span>deviations</span><span>)</span><span></span>
</span><span id="L-11-3"><a id="L-11-3" name="L-11-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>values</span><span> </span><span>below</span><span> </span><span>statistical</span><span> </span><span>threshold</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-11-4"><a id="L-11-4" name="L-11-4"></a><span>1</span><span>:</span><span> </span><span>σ</span><span> </span><span>←</span><span> </span><span>StandardDeviation</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>standard</span><span> </span><span>deviation</span><span> </span><span>of</span><span> </span><span>logits</span><span> </span><span>across</span><span> </span><span>vocabulary</span><span></span>
</span><span id="L-11-5"><a id="L-11-5" name="L-11-5"></a><span>2</span><span>:</span><span> </span><span>Lmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>logit</span><span> </span><span>value</span><span></span>
</span><span id="L-11-6"><a id="L-11-6" name="L-11-6"></a><span>3</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>Lmax</span><span> </span><span>-</span><span> </span><span>n</span><span> </span><span>×</span><span> </span><span>σ</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>threshold</span><span> </span><span>n</span><span> </span><span>standard</span><span> </span><span>deviations</span><span> </span><span>below</span><span> </span><span>maximum</span><span></span>
</span><span id="L-11-7"><a id="L-11-7" name="L-11-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>L</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-11-8"><a id="L-11-8" name="L-11-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-11-9"><a id="L-11-9" name="L-11-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="tail-free-sampling">Tail-Free Sampling<a href="#tail-free-sampling" title="Permanent link"> </a></h3>
<p>TFS is like looking at the slope change in a graph of word probabilities and cutting it off where there&#39;s a significant drop. For example, you&#39;re lining up candidates for a job based on their scores - instead of using a fixed cutoff, you look for where there&#39;s a big gap between consecutive scores and say &#34;everyone past this point is significantly worse.&#34;</p>
<p>TFS examines how the probability distribution changes by looking at second derivatives (the &#34;curvature&#34; or rate of change in the slope). It focues on points where the distribution starts to flatten out, marking the transition from &#34;good&#34; candidates to &#34;the long tail of mediocre options.&#34; What makes it special is that it adapts to the natural structure of the distribution rather than imposing arbitrary thresholds. It finds the &#34;elbow point&#34; where good options end and the less relevant ones begin.</p>
<p><strong>Technical</strong>:</p>
<p>TFS ultimately focues on the shape of the distribution rather than absolute values.</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-12-1"><a id="L-12-1" name="L-12-1"></a><span>Algorithm</span><span> </span><span>12</span><span> </span><span>Tail</span><span>-</span><span>Free</span><span> </span><span>Sampling</span><span> </span><span>(</span><span>TFS</span><span>)</span><span></span>
</span><span id="L-12-2"><a id="L-12-2" name="L-12-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>t</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-12-3"><a id="L-12-3" name="L-12-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>&#34;tail&#34;</span><span> </span><span>of</span><span> </span><span>distribution</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-12-4"><a id="L-12-4" name="L-12-4"></a><span>1</span><span>:</span><span> </span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span> </span><span>←</span><span> </span><span>Sort</span><span>(</span><span>L</span><span>,</span><span> </span><span>descending</span><span>=</span><span>True</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>logits</span><span> </span><span>in</span><span> </span><span>descending</span><span> </span><span>order</span><span></span>
</span><span id="L-12-5"><a id="L-12-5" name="L-12-5"></a><span>2</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>Lsorted</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-12-6"><a id="L-12-6" name="L-12-6"></a><span>3</span><span>:</span><span> </span><span>D1</span><span> </span><span>←</span><span> </span><span>Diff</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>First</span><span> </span><span>differences</span><span> </span><span>(</span><span>slope</span><span>)</span><span></span>
</span><span id="L-12-7"><a id="L-12-7" name="L-12-7"></a><span>4</span><span>:</span><span> </span><span>D2</span><span> </span><span>←</span><span> </span><span>|</span><span>Diff</span><span>(</span><span>D1</span><span>)</span><span>|</span><span>  </span><span>//</span><span> </span><span>Absolute</span><span> </span><span>second</span><span> </span><span>differences</span><span> </span><span>(</span><span>change</span><span> </span><span>in</span><span> </span><span>slope</span><span>)</span><span></span>
</span><span id="L-12-8"><a id="L-12-8" name="L-12-8"></a><span>5</span><span>:</span><span> </span><span>D2norm</span><span> </span><span>←</span><span> </span><span>D2</span><span> </span><span>/</span><span> </span><span>Sum</span><span>(</span><span>D2</span><span>)</span><span>  </span><span>//</span><span> </span><span>Normalize</span><span> </span><span>curvature</span><span></span>
</span><span id="L-12-9"><a id="L-12-9" name="L-12-9"></a><span>6</span><span>:</span><span> </span><span>CurvatureCDF</span><span> </span><span>←</span><span> </span><span>CumulativeSum</span><span>(</span><span>D2norm</span><span>)</span><span>  </span><span>//</span><span> </span><span>Cumulative</span><span> </span><span>distribution</span><span> </span><span>of</span><span> </span><span>curvature</span><span></span>
</span><span id="L-12-10"><a id="L-12-10" name="L-12-10"></a><span>7</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>CurvatureCDF</span><span> </span><span>&gt;</span><span> </span><span>t</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>where</span><span> </span><span>curvature</span><span> </span><span>CDF</span><span> </span><span>exceeds</span><span> </span><span>threshold</span><span></span>
</span><span id="L-12-11"><a id="L-12-11" name="L-12-11"></a><span>8</span><span>:</span><span> </span><span>//</span><span> </span><span>Add</span><span> </span><span>boundary</span><span> </span><span>conditions</span><span> </span><span>(</span><span>keep</span><span> </span><span>top</span><span> </span><span>token</span><span>,</span><span> </span><span>mask</span><span> </span><span>after</span><span> </span><span>end</span><span> </span><span>of</span><span> </span><span>vocabulary</span><span>)</span><span></span>
</span><span id="L-12-12"><a id="L-12-12" name="L-12-12"></a><span>9</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>Concatenate</span><span>(</span><span>[</span><span>False</span><span>]</span><span>,</span><span> </span><span>mask</span><span>,</span><span> </span><span>[</span><span>True</span><span>]</span><span>)</span><span></span>
</span><span id="L-12-13"><a id="L-12-13" name="L-12-13"></a><span>10</span><span>:</span><span> </span><span>Lsorted</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>in</span><span> </span><span>the</span><span> </span><span>&#34;tail&#34;</span><span></span>
</span><span id="L-12-14"><a id="L-12-14" name="L-12-14"></a><span>11</span><span>:</span><span> </span><span>L</span><span> </span><span>←</span><span> </span><span>Unsort</span><span>(</span><span>Lsorted</span><span>,</span><span> </span><span>Lidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span></span>
</span><span id="L-12-15"><a id="L-12-15" name="L-12-15"></a><span>12</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="eta-cutoff">Eta Cutoff<a href="#eta-cutoff" title="Permanent link"> </a></h3>
<p>Eta Cutoff adapts to how certain or uncertain the model is. Using the previous analogy again, let&#39;s say you have a stack of resumes. If there&#39;s one clearly outstanding candidate, you might be very selective about who else gets an interview. But if all candidates are similarly qualified, you might interview more of them. Eta cutoff works on this principle by looking at both the individual probability of each word and the overall entropy (uncertainty) of the distribution. When the model is very certain about what comes next (low entropy), the cutoff threshold becomes stricter, keeping only the most probable options. When the model is uncertain (high entropy), the threshold becomes more lenient.</p>
<p><strong>Technical</strong>:</p>
<p>Any token with a probability below this threshold is masked out.</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-13-1"><a id="L-13-1" name="L-13-1"></a><span>Algorithm</span><span> </span><span>13</span><span> </span><span>Eta</span><span> </span><span>Cutoff</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-13-2"><a id="L-13-2" name="L-13-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>parameter</span><span> </span><span>η</span><span> </span><span>∈</span><span> </span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span><span></span>
</span><span id="L-13-3"><a id="L-13-3" name="L-13-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>dynamic</span><span> </span><span>low</span><span>-</span><span>probability</span><span> </span><span>filtering</span><span></span>
</span><span id="L-13-4"><a id="L-13-4" name="L-13-4"></a><span>1</span><span>:</span><span> </span><span>LN</span><span> </span><span>←</span><span> </span><span>LogSoftmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Log</span><span>-</span><span>normalized</span><span> </span><span>logits</span><span></span>
</span><span id="L-13-5"><a id="L-13-5" name="L-13-5"></a><span>2</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Exp</span><span>(</span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-13-6"><a id="L-13-6" name="L-13-6"></a><span>3</span><span>:</span><span> </span><span>H</span><span> </span><span>←</span><span> </span><span>-</span><span>Sum</span><span>(</span><span>P</span><span> </span><span>×</span><span> </span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>negative</span><span> </span><span>entropy</span><span> </span><span>(</span><span>higher</span><span> </span><span>means</span><span> </span><span>more</span><span> </span><span>certainty</span><span>)</span><span></span>
</span><span id="L-13-7"><a id="L-13-7" name="L-13-7"></a><span>4</span><span>:</span><span> </span><span>threshold</span><span> </span><span>←</span><span> </span><span>Min</span><span>(</span><span>η</span><span>,</span><span> </span><span>√</span><span>η</span><span> </span><span>×</span><span> </span><span>Exp</span><span>(</span><span>H</span><span>))</span><span>  </span><span>//</span><span> </span><span>Dynamic</span><span> </span><span>threshold</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>certainty</span><span></span>
</span><span id="L-13-8"><a id="L-13-8" name="L-13-8"></a><span>5</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>threshold</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-13-9"><a id="L-13-9" name="L-13-9"></a><span>6</span><span>:</span><span> </span><span>topIdx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>index</span><span> </span><span>of</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-13-10"><a id="L-13-10" name="L-13-10"></a><span>7</span><span>:</span><span> </span><span>mask</span><span>[</span><span>topIdx</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-13-11"><a id="L-13-11" name="L-13-11"></a><span>8</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-13-12"><a id="L-13-12" name="L-13-12"></a><span>9</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="epsilon-cutoff">Epsilon Cutoff<a href="#epsilon-cutoff" title="Permanent link"> </a></h3>
<p>Like setting a minimum vote threshold in an election - any candidate who doesn&#39;t get at least a certain percentage of votes is eliminated. Basically a simpler version of filtering that removes all options below a fixed probability threshold.</p>
<p>It&#39;s a straightforward approach that removes extremely unlikely words without affecting the relative probabilities of the remaining options. It&#39;s useful for cleaning up the long tail of improbable tokens that might otherwise occasionally be selected due to random chance. Not as adaptive as other methods, but it&#39;s simple and predictable, and removes undesirables without dramatically changing the distribution&#39;s shape.</p>
<p><strong>Technical</strong>:</p>
<p>its difference with eta cutoff is that epsilon uses a fixed threshold <em>regardless</em> of the distribution&#39;s properties. Both methods are ultimately a way to prune the vocabulary space without complex sorting operations, which may make inference slower.</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-14-1"><a id="L-14-1" name="L-14-1"></a><span>Algorithm</span><span> </span><span>14</span><span> </span><span>Epsilon</span><span> </span><span>Cutoff</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-14-2"><a id="L-14-2" name="L-14-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>ε</span><span> </span><span>∈</span><span> </span><span>(</span><span>0</span><span>,</span><span>1</span><span>)</span><span></span>
</span><span id="L-14-3"><a id="L-14-3" name="L-14-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>fixed</span><span> </span><span>low</span><span>-</span><span>probability</span><span> </span><span>filtering</span><span></span>
</span><span id="L-14-4"><a id="L-14-4" name="L-14-4"></a><span>1</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-14-5"><a id="L-14-5" name="L-14-5"></a><span>2</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>P</span><span> </span><span>&lt;</span><span> </span><span>ε</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>fixed</span><span> </span><span>threshold</span><span></span>
</span><span id="L-14-6"><a id="L-14-6" name="L-14-6"></a><span>3</span><span>:</span><span> </span><span>topIdx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>P</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>index</span><span> </span><span>of</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-14-7"><a id="L-14-7" name="L-14-7"></a><span>4</span><span>:</span><span> </span><span>mask</span><span>[</span><span>topIdx</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-14-8"><a id="L-14-8" name="L-14-8"></a><span>5</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>below</span><span> </span><span>threshold</span><span></span>
</span><span id="L-14-9"><a id="L-14-9" name="L-14-9"></a><span>6</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="locally-typical-sampling">Locally Typical Sampling<a href="#locally-typical-sampling" title="Permanent link"> </a></h3>
<p>This sampler is like focusing on the most &#34;average&#34; word choices rather than the most probable ones. Like you&#39;re writing a story and want it to sound natural but not predictable. Instead of always picking the most obvious words or completely random ones, you&#39;d choose words that are neither too surprising nor too boring — words that feel &#34;typically human.&#34;</p>
<p>The method itself works by measuring how much each potential word deviates from the average surprisal (unexpectedness) of all options. Words that are extremely predictable or extremely surprising are considered less &#34;typical&#34; than those with middle-of-the-road surprisal values. Typical sampling keeps tokens whose surprisal is close to the average, and might make the text feel more &#34;balanced.&#34; It doesn&#39;t look at raw probabilities but at how predictable each word is within the context.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-15-1"><a id="L-15-1" name="L-15-1"></a><span>Algorithm</span><span> </span><span>15</span><span> </span><span>Locally</span><span> </span><span>Typical</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-15-2"><a id="L-15-2" name="L-15-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>threshold</span><span> </span><span>p</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-15-3"><a id="L-15-3" name="L-15-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>tokens</span><span> </span><span>furthest</span><span> </span><span>from</span><span> </span><span>mean</span><span> </span><span>surprisal</span><span> </span><span>filtered</span><span> </span><span>out</span><span></span>
</span><span id="L-15-4"><a id="L-15-4" name="L-15-4"></a><span>1</span><span>:</span><span> </span><span>LN</span><span> </span><span>←</span><span> </span><span>LogSoftmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Log</span><span>-</span><span>normalized</span><span> </span><span>logits</span><span></span>
</span><span id="L-15-5"><a id="L-15-5" name="L-15-5"></a><span>2</span><span>:</span><span> </span><span>P</span><span> </span><span>←</span><span> </span><span>Exp</span><span>(</span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-15-6"><a id="L-15-6" name="L-15-6"></a><span>3</span><span>:</span><span> </span><span>H</span><span> </span><span>←</span><span> </span><span>Sum</span><span>(</span><span>P</span><span> </span><span>×</span><span> </span><span>LN</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>negative</span><span> </span><span>entropy</span><span> </span><span>(</span><span>expected</span><span> </span><span>surprisal</span><span>)</span><span></span>
</span><span id="L-15-7"><a id="L-15-7" name="L-15-7"></a><span>4</span><span>:</span><span> </span><span>S</span><span> </span><span>←</span><span> </span><span>|</span><span>H</span><span> </span><span>-</span><span> </span><span>LN</span><span>|</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>surprisal</span><span> </span><span>deviation</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>token</span><span></span>
</span><span id="L-15-8"><a id="L-15-8" name="L-15-8"></a><span>5</span><span>:</span><span> </span><span>Sidx</span><span> </span><span>←</span><span> </span><span>ArgsortAscending</span><span>(</span><span>S</span><span>)</span><span>  </span><span>//</span><span> </span><span>Sort</span><span> </span><span>indices</span><span> </span><span>by</span><span> </span><span>surprisal</span><span> </span><span>deviation</span><span> </span><span>(</span><span>most</span><span> </span><span>typical</span><span> </span><span>first</span><span>)</span><span></span>
</span><span id="L-15-9"><a id="L-15-9" name="L-15-9"></a><span>6</span><span>:</span><span> </span><span>Preordered</span><span> </span><span>←</span><span> </span><span>GatherByIndices</span><span>(</span><span>P</span><span>,</span><span> </span><span>Sidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Reorder</span><span> </span><span>probabilities</span><span> </span><span>by</span><span> </span><span>typicality</span><span></span>
</span><span id="L-15-10"><a id="L-15-10" name="L-15-10"></a><span>7</span><span>:</span><span> </span><span>Pcum</span><span> </span><span>←</span><span> </span><span>CumulativeSum</span><span>(</span><span>Preordered</span><span>)</span><span>  </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>cumulative</span><span> </span><span>sum</span><span> </span><span>of</span><span> </span><span>reordered</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-15-11"><a id="L-15-11" name="L-15-11"></a><span>8</span><span>:</span><span> </span><span>maskSorted</span><span> </span><span>←</span><span> </span><span>Pcum</span><span> </span><span>≥</span><span> </span><span>p</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>where</span><span> </span><span>cumulative</span><span> </span><span>typical</span><span> </span><span>probability</span><span> </span><span>exceeds</span><span> </span><span>threshold</span><span></span>
</span><span id="L-15-12"><a id="L-15-12" name="L-15-12"></a><span>9</span><span>:</span><span> </span><span>maskSorted</span><span>[</span><span>0:MinTokensToKeep</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>at</span><span> </span><span>least</span><span> </span><span>MinTokensToKeep</span><span> </span><span>tokens</span><span></span>
</span><span id="L-15-13"><a id="L-15-13" name="L-15-13"></a><span>10</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>ScatterByIndices</span><span>(</span><span>maskSorted</span><span>,</span><span> </span><span>Sidx</span><span>)</span><span>  </span><span>//</span><span> </span><span>Restore</span><span> </span><span>original</span><span> </span><span>ordering</span><span> </span><span>of</span><span> </span><span>mask</span><span></span>
</span><span id="L-15-14"><a id="L-15-14" name="L-15-14"></a><span>11</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>that</span><span> </span><span>are</span><span> </span><span>least</span><span> </span><span>typical</span><span></span>
</span><span id="L-15-15"><a id="L-15-15" name="L-15-15"></a><span>12</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="quadratic-sampling">Quadratic Sampling<a href="#quadratic-sampling" title="Permanent link"> </a></h3>
<p>Whereas most sampling techniques either filter out tokens (like Top-K) or simply adjust randomness (like Temperature), Quadratic Sampling takes a more nuanced approach by reshaping the entire probability distribution using mathematical transformations.</p>
<p>Quadratic works by applying a mathematical transformation that adjusts the gap between high and low prob tokens using quadratic and cubic equations (it was actually called Cubic sampling before the quadratic function was added later). The transformation is centered around the highest-scoring token, with two key params controlling the effect: the smoothing factor determines the overall strength of the adjustment, while the smoothing curve controls the shape of the transformation. It can essentially make the highest probability tokens more prominent while gently supressing the lower ones, or it can flatten the distribution to give more tokens a chance.</p>
<p><strong>Technical</strong>:</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-16-1"><a id="L-16-1" name="L-16-1"></a><span>Algorithm</span><span> </span><span>16</span><span> </span><span>Quadratic</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-16-2"><a id="L-16-2" name="L-16-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>smoothing</span><span> </span><span>factor</span><span> </span><span>α</span><span>,</span><span> </span><span>smoothing</span><span> </span><span>curve</span><span> </span><span>β</span><span></span>
</span><span id="L-16-3"><a id="L-16-3" name="L-16-3"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>non</span><span>-</span><span>linear</span><span> </span><span>transformation</span><span> </span><span>applied</span><span></span>
</span><span id="L-16-4"><a id="L-16-4" name="L-16-4"></a><span>1</span><span>:</span><span> </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>coefficients</span><span> </span><span>for</span><span> </span><span>quadratic</span><span> </span><span>and</span><span> </span><span>cubic</span><span> </span><span>terms</span><span></span>
</span><span id="L-16-5"><a id="L-16-5" name="L-16-5"></a><span>2</span><span>:</span><span> </span><span>k</span><span> </span><span>←</span><span> </span><span>α</span><span> </span><span>×</span><span> </span><span>(</span><span>3</span><span> </span><span>-</span><span> </span><span>β</span><span>)</span><span> </span><span>/</span><span> </span><span>2</span><span>  </span><span>//</span><span> </span><span>Quadratic</span><span> </span><span>coefficient</span><span></span>
</span><span id="L-16-6"><a id="L-16-6" name="L-16-6"></a><span>3</span><span>:</span><span> </span><span>s</span><span> </span><span>←</span><span> </span><span>α</span><span> </span><span>×</span><span> </span><span>(</span><span>β</span><span> </span><span>-</span><span> </span><span>1</span><span>)</span><span> </span><span>/</span><span> </span><span>2</span><span>  </span><span>//</span><span> </span><span>Cubic</span><span> </span><span>coefficient</span><span></span>
</span><span id="L-16-7"><a id="L-16-7" name="L-16-7"></a><span>4</span><span>:</span><span></span>
</span><span id="L-16-8"><a id="L-16-8" name="L-16-8"></a><span>5</span><span>:</span><span> </span><span>//</span><span> </span><span>Apply</span><span> </span><span>transformation</span><span> </span><span>only</span><span> </span><span>where</span><span> </span><span>smoothing</span><span> </span><span>factor</span><span> </span><span>is</span><span> </span><span>non</span><span>-</span><span>zero</span><span></span>
</span><span id="L-16-9"><a id="L-16-9" name="L-16-9"></a><span>6</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>(</span><span>α</span><span> </span><span>≠</span><span> </span><span>0</span><span>)</span><span></span>
</span><span id="L-16-10"><a id="L-16-10" name="L-16-10"></a><span>7</span><span>:</span><span> </span><span>if</span><span> </span><span>not</span><span> </span><span>any</span><span>(</span><span>mask</span><span>)</span><span> </span><span>then</span><span> </span><span>return</span><span> </span><span>L</span><span> </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-16-11"><a id="L-16-11" name="L-16-11"></a><span>8</span><span>:</span><span></span>
</span><span id="L-16-12"><a id="L-16-12" name="L-16-12"></a><span>9</span><span>:</span><span> </span><span>Ltarget</span><span> </span><span>←</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span>  </span><span>//</span><span> </span><span>Select</span><span> </span><span>logits</span><span> </span><span>to</span><span> </span><span>transform</span><span></span>
</span><span id="L-16-13"><a id="L-16-13" name="L-16-13"></a><span>10</span><span>:</span><span> </span><span>Lmax</span><span> </span><span>←</span><span> </span><span>Max</span><span>(</span><span>Ltarget</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>maximum</span><span> </span><span>logit</span><span> </span><span>value</span><span></span>
</span><span id="L-16-14"><a id="L-16-14" name="L-16-14"></a><span>11</span><span>:</span><span> </span>
</span><span id="L-16-15"><a id="L-16-15" name="L-16-15"></a><span>12</span><span>:</span><span> </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>differences</span><span> </span><span>from</span><span> </span><span>maximum</span><span> </span><span>and</span><span> </span><span>apply</span><span> </span><span>transformation</span><span></span>
</span><span id="L-16-16"><a id="L-16-16" name="L-16-16"></a><span>13</span><span>:</span><span> </span><span>Δ</span><span> </span><span>←</span><span> </span><span>Ltarget</span><span> </span><span>-</span><span> </span><span>Lmax</span><span>  </span><span>//</span><span> </span><span>Distance</span><span> </span><span>from</span><span> </span><span>maximum</span><span> </span><span>logit</span><span></span>
</span><span id="L-16-17"><a id="L-16-17" name="L-16-17"></a><span>14</span><span>:</span><span></span>
</span><span id="L-16-18"><a id="L-16-18" name="L-16-18"></a><span>15</span><span>:</span><span> </span><span>//</span><span> </span><span>Apply</span><span> </span><span>quadratic</span><span> </span><span>transformation</span><span>:</span><span> </span><span>Δ</span><span> </span><span>-=</span><span> </span><span>Δ²</span><span>(</span><span>s</span><span>·</span><span>Δ</span><span> </span><span>-</span><span> </span><span>k</span><span>)</span><span></span>
</span><span id="L-16-19"><a id="L-16-19" name="L-16-19"></a><span>16</span><span>:</span><span> </span><span>Δnew</span><span> </span><span>←</span><span> </span><span>Δ</span><span> </span><span>-</span><span> </span><span>(</span><span>Δ²</span><span> </span><span>×</span><span> </span><span>(</span><span>s</span><span> </span><span>×</span><span> </span><span>Δ</span><span> </span><span>-</span><span> </span><span>k</span><span>))</span><span></span>
</span><span id="L-16-20"><a id="L-16-20" name="L-16-20"></a><span>17</span><span>:</span><span></span>
</span><span id="L-16-21"><a id="L-16-21" name="L-16-21"></a><span>18</span><span>:</span><span> </span><span>//</span><span> </span><span>Handle</span><span> </span><span>potential</span><span> </span><span>numerical</span><span> </span><span>issues</span><span></span>
</span><span id="L-16-22"><a id="L-16-22" name="L-16-22"></a><span>19</span><span>:</span><span> </span><span>Δnew</span><span>[</span><span>isNaN(Δnew)</span><span>]</span><span> </span><span>←</span><span> </span><span>0</span><span></span>
</span><span id="L-16-23"><a id="L-16-23" name="L-16-23"></a><span>20</span><span>:</span><span></span>
</span><span id="L-16-24"><a id="L-16-24" name="L-16-24"></a><span>21</span><span>:</span><span> </span><span>//</span><span> </span><span>Apply</span><span> </span><span>transformed</span><span> </span><span>differences</span><span></span>
</span><span id="L-16-25"><a id="L-16-25" name="L-16-25"></a><span>22</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>Ltarget</span><span> </span><span>-</span><span> </span><span>Δnew</span><span></span>
</span><span id="L-16-26"><a id="L-16-26" name="L-16-26"></a><span>23</span><span>:</span><span></span>
</span><span id="L-16-27"><a id="L-16-27" name="L-16-27"></a><span>24</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="mirostat-sampling">Mirostat Sampling<a href="#mirostat-sampling" title="Permanent link"> </a></h3>
<p>Mirostat is like an adaptive thermostat for text generation that automatically adjusts to maintain a consistent level of &#34;surprise&#34; or unpredictability. Just as a thermostat keeps your room temperature stable by turning heating on and off, Mirostat keeps text generation at a consistent level of unpredictability by dynamically adjusting how conservative or creative the sampling is. It works by measuring the &#34;surprisal&#34; (how unexpected each token is) and comparing it to a target value. If recent text has been to predictable, Mirostat allows more surprising tokens to be selected. If it&#39;s been too chaotic, Mirostat tightens the constraints to focus on more predictable ones. This creates a feedback loop that maintains a consistent perplexity throughput the generated text. </p>
<p>Mirostat&#39;s main selling point is that it&#39;s self-regulating, adapting to different contexts without requiring manual parameter adjustment. </p>
<p><strong>Technical</strong>:</p>
<p>The core algorithm has two main phases:</p>
<ol>
<li><strong>Filtering phase</strong>:<br/>
</li>
</ol>
<ul>
<li>Calculate the &#34;surprisal&#34; (negative logprob, converted to bits) for each potential token.</li>
<li>Use an adaptive threshold (&#34;mu&#34;) to filter out tokens that are too surprising.</li>
<li>Ensure at least one token (the most likely) remains available for selection.</li>
<li>Effectively create a dynamic Top-K filter where K varies based on the current <code>mu</code> value.</li>
</ul>
<ol start="2">
<li><strong>Update phase</strong>:<br/>
</li>
</ol>
<ul>
<li>After a token is selected, calculate the actual surprisal of the chosen token.</li>
<li>Compare this surprisal to a target value (&#34;tau&#34;).</li>
<li>Update the <code>mu</code> threshold based on the difference between actual and target surprisal.</li>
<li>An <code>eta</code> parameter controls how quickly <code>mu</code> adjusts (learning rate).</li>
</ul>
<p>The mathematical foundation is a control system using the equation <code>mu_{t+1} = mu_t - η × (surprisal_t - τ)</code></p>
<p>Where:</p>
<ul>
<li><code>mu</code> is the adaptively changing surprisal threshold</li>
<li><code>τ</code> (tau) is the target surprisal (perplexity)</li>
<li><code>η</code> (eta) is the learning rate</li>
<li><code>surprisal_t</code> is the surprisal of the chosen token</li>
</ul>
<p>This gives us a negative feedback loop that pushes generation towards the target surprisal level. If tokens are too surprising, <code>mu</code> decreases. If tokens are too precitable, <code>mu</code> increases. This sampling method is likely the most sophisticated and complex to exist.</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-17-1"><a id="L-17-1" name="L-17-1"></a><span>Algorithm</span><span> </span><span>17</span><span> </span><span>Mirostat</span><span> </span><span>v2</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-17-2"><a id="L-17-2" name="L-17-2"></a><span>Required</span><span>:</span><span> </span><span>Logits</span><span> </span><span>tensor</span><span> </span><span>L</span><span>,</span><span> </span><span>mu</span><span> </span><span>parameter</span><span> </span><span>(</span><span>surprisal</span><span> </span><span>threshold</span><span>),</span><span> </span>
</span><span id="L-17-3"><a id="L-17-3" name="L-17-3"></a><span>          </span><span>tau</span><span> </span><span>parameter</span><span> </span><span>(</span><span>target</span><span> </span><span>surprisal</span><span>),</span><span> </span><span>eta</span><span> </span><span>parameter</span><span> </span><span>(</span><span>learning</span><span> </span><span>rate</span><span>)</span><span></span>
</span><span id="L-17-4"><a id="L-17-4" name="L-17-4"></a><span>Output</span><span>:</span><span> </span><span>Modified</span><span> </span><span>logits</span><span> </span><span>with</span><span> </span><span>dynamic</span><span> </span><span>filtering</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>surprisal</span><span> </span><span>control</span><span></span>
</span><span id="L-17-5"><a id="L-17-5" name="L-17-5"></a>
</span><span id="L-17-6"><a id="L-17-6" name="L-17-6"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>1</span><span>:</span><span> </span><span>Filter</span><span> </span><span>tokens</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>current</span><span> </span><span>surprisal</span><span> </span><span>threshold</span><span></span>
</span><span id="L-17-7"><a id="L-17-7" name="L-17-7"></a><span>1</span><span>:</span><span> </span><span>LN</span><span> </span><span>←</span><span> </span><span>LogSoftmax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Log</span><span>-</span><span>normalized</span><span> </span><span>logits</span><span></span>
</span><span id="L-17-8"><a id="L-17-8" name="L-17-8"></a><span>2</span><span>:</span><span> </span><span>S</span><span> </span><span>←</span><span> </span><span>-</span><span>LN</span><span> </span><span>/</span><span> </span><span>log</span><span>(</span><span>2</span><span>)</span><span>  </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>surprisal</span><span> </span><span>in</span><span> </span><span>bits</span><span></span>
</span><span id="L-17-9"><a id="L-17-9" name="L-17-9"></a><span>3</span><span>:</span><span> </span><span>mask</span><span> </span><span>←</span><span> </span><span>S</span><span> </span><span>&gt;</span><span> </span><span>μ</span><span>  </span><span>//</span><span> </span><span>Create</span><span> </span><span>mask</span><span> </span><span>for</span><span> </span><span>tokens</span><span> </span><span>with</span><span> </span><span>surprisal</span><span> </span><span>exceeding</span><span> </span><span>threshold</span><span></span>
</span><span id="L-17-10"><a id="L-17-10" name="L-17-10"></a><span>4</span><span>:</span><span> </span><span>topIdx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>L</span><span>)</span><span>  </span><span>//</span><span> </span><span>Find</span><span> </span><span>index</span><span> </span><span>of</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-17-11"><a id="L-17-11" name="L-17-11"></a><span>5</span><span>:</span><span> </span><span>mask</span><span>[</span><span>topIdx</span><span>]</span><span> </span><span>←</span><span> </span><span>False</span><span>  </span><span>//</span><span> </span><span>Always</span><span> </span><span>keep</span><span> </span><span>highest</span><span> </span><span>probability</span><span> </span><span>token</span><span></span>
</span><span id="L-17-12"><a id="L-17-12" name="L-17-12"></a><span>6</span><span>:</span><span> </span><span>L</span><span>[</span><span>mask</span><span>]</span><span> </span><span>←</span><span> </span><span>-</span><span>∞</span><span>  </span><span>//</span><span> </span><span>Filter</span><span> </span><span>out</span><span> </span><span>tokens</span><span> </span><span>above</span><span> </span><span>surprisal</span><span> </span><span>threshold</span><span></span>
</span><span id="L-17-13"><a id="L-17-13" name="L-17-13"></a><span>7</span><span>:</span><span> </span><span>//</span><span> </span><span>At</span><span> </span><span>this</span><span> </span><span>point</span><span>,</span><span> </span><span>token</span><span> </span><span>selection</span><span> </span><span>would</span><span> </span><span>occur</span><span> </span><span>using</span><span> </span><span>the</span><span> </span><span>filtered</span><span> </span><span>logits</span><span></span>
</span><span id="L-17-14"><a id="L-17-14" name="L-17-14"></a>
</span><span id="L-17-15"><a id="L-17-15" name="L-17-15"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>2</span><span>:</span><span> </span><span>Update</span><span> </span><span>mu</span><span> </span><span>based</span><span> </span><span>on</span><span> </span><span>selected</span><span> </span><span>token</span><span> </span><span>(</span><span>performed</span><span> </span><span>after</span><span> </span><span>token</span><span> </span><span>selection</span><span>)</span><span></span>
</span><span id="L-17-16"><a id="L-17-16" name="L-17-16"></a><span>8</span><span>:</span><span> </span><span>t</span><span> </span><span>←</span><span> </span><span>SelectedToken</span><span>  </span><span>//</span><span> </span><span>Token</span><span> </span><span>chosen</span><span> </span><span>after</span><span> </span><span>filtering</span><span></span>
</span><span id="L-17-17"><a id="L-17-17" name="L-17-17"></a><span>9</span><span>:</span><span> </span><span>St</span><span> </span><span>←</span><span> </span><span>-</span><span>LogProb</span><span>(</span><span>t</span><span>)</span><span> </span><span>/</span><span> </span><span>log</span><span>(</span><span>2</span><span>)</span><span>  </span><span>//</span><span> </span><span>Surprisal</span><span> </span><span>of</span><span> </span><span>selected</span><span> </span><span>token</span><span> </span><span>in</span><span> </span><span>bits</span><span></span>
</span><span id="L-17-18"><a id="L-17-18" name="L-17-18"></a><span>10</span><span>:</span><span> </span><span>μnew</span><span> </span><span>←</span><span> </span><span>μ</span><span> </span><span>-</span><span> </span><span>η</span><span> </span><span>×</span><span> </span><span>(</span><span>St</span><span> </span><span>-</span><span> </span><span>τ</span><span>)</span><span>  </span><span>//</span><span> </span><span>Update</span><span> </span><span>mu</span><span> </span><span>using</span><span> </span><span>control</span><span> </span><span>equation</span><span></span>
</span><span id="L-17-19"><a id="L-17-19" name="L-17-19"></a><span>11</span><span>:</span><span> </span><span>StoreForNextStep</span><span>(</span><span>μnew</span><span>)</span><span>  </span><span>//</span><span> </span><span>Save</span><span> </span><span>updated</span><span> </span><span>mu</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span> </span><span>generation</span><span></span>
</span><span id="L-17-20"><a id="L-17-20" name="L-17-20"></a><span>12</span><span>:</span><span> </span><span>return</span><span> </span><span>L</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="dynamic-temperature-sampling">Dynamic Temperature Sampling<a href="#dynamic-temperature-sampling" title="Permanent link"> </a></h3>
<p>This method adjusts the temperature value based on the entropy (uncertainty) of the current token distribution. When the model is very certain about what comes next (low entropy), it uses a higher temperature to introduce more diversity. When the model is already uncertain (high entropy), it uses a lower temperature to keep the output more focused and coherent. You simply set a minimum and maximum temperature, then an exponent that controls how quickly it transitions between them based on the context&#39;s entropy, and the sampler will do the rest.</p>
<p><strong>Technical</strong>:</p>
<p>This entropy is then normalized by dividing the maximum possible entropy (log of the number of tokens with finite logits). This gives us a value between 0 and 1, where 0 represents minimum entropy and 1 represents maximum entropy. The normalized entropy is raised to a power controlled by the exponent parameter, allowing for non-linear mapping between entropy and temperature. This transformed value is then used to interpolate between the min and max temp settings:</p>
<p><code>temperature = min_temp + (max_temp - min_temp) * (normalized_entropy ^ exponent)</code></p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-18-1"><a id="L-18-1" name="L-18-1"></a><span>Algorithm</span> <span>18</span> <span>Dynamic</span> <span>Temperature</span> <span>Sampling</span>
</span><span id="L-18-2"><a id="L-18-2" name="L-18-2"></a><span>Required</span>: <span>Logits</span> <span>tensor</span> <span>L</span>, <span>minimum</span> <span>temperature</span> <span>Tmin</span>, <span>maximum</span> <span>temperature</span> <span>Tmax</span>, <span>exponent</span> α
</span><span id="L-18-3"><a id="L-18-3" name="L-18-3"></a><span>Output</span>: <span>Temperature</span> <span>value</span> <span>T</span> <span>dynamically</span> <span>calculated</span> <span>based</span> <span>on</span> <span>distribution</span> <span>entropy</span>
</span><span id="L-18-4"><a id="L-18-4" name="L-18-4"></a><span>1</span>: <span>P</span> ← <span>softmax</span><span>(</span><span>L</span><span>)</span>  <span>//</span> <span>Calculate</span> <span>probability</span> <span>distribution</span>
</span><span id="L-18-5"><a id="L-18-5" name="L-18-5"></a><span>2</span>: <span>H</span> ← <span>-</span>∑<span>(</span><span>P</span> · <span>log</span><span>(</span><span>P</span><span>))</span>  <span>//</span> <span>Calculate</span> <span>entropy</span> <span>of</span> <span>the</span> <span>distribution</span>
</span><span id="L-18-6"><a id="L-18-6" name="L-18-6"></a><span>3</span>: <span>Vvalid</span> ← <span>|</span>{<span>l</span> ∈ <span>L</span> <span>|</span> <span>l</span> <span>&gt;</span> <span>-</span>∞}<span>|</span>  <span>//</span> <span>Count</span> <span>tokens</span> <span>with</span> <span>finite</span> <span>logits</span>
</span><span id="L-18-7"><a id="L-18-7" name="L-18-7"></a><span>4</span>: <span>Hmax</span> ← <span>log</span><span>(</span><span>Vvalid</span><span>)</span>  <span>//</span> <span>Maximum</span> <span>possible</span> <span>entropy</span>
</span><span id="L-18-8"><a id="L-18-8" name="L-18-8"></a><span>5</span>: <span>Hnorm</span> ← <span>H</span> <span>/</span> <span>Hmax</span>  <span>//</span> <span>Normalize</span> <span>entropy</span> <span>to</span> <span>range</span> [<span>0</span>,<span>1</span>]
</span><span id="L-18-9"><a id="L-18-9" name="L-18-9"></a><span>6</span>: <span>Htrans</span> ← <span>Hnorm</span><span>^</span>α  <span>//</span> <span>Apply</span> <span>non</span><span>-</span><span>linear</span> <span>transformation</span> <span>based</span> <span>on</span> <span>exponent</span>
</span><span id="L-18-10"><a id="L-18-10" name="L-18-10"></a><span>7</span>: <span>T</span> ← <span>Tmin</span> <span>+</span> <span>(</span><span>Tmax</span> <span>-</span> <span>Tmin</span><span>)</span> · <span>Htrans</span>  <span>//</span> <span>Interpolate</span> <span>between</span> <span>min</span> <span>and</span> <span>max</span> <span>temperature</span>
</span><span id="L-18-11"><a id="L-18-11" name="L-18-11"></a><span>8</span>: <span>T</span> ← <span>max</span><span>(</span><span>T</span>, ε<span>)</span>  <span>//</span> <span>Ensure</span> <span>temperature</span> <span>is</span> <span>within</span> <span>valid</span> <span>bounds</span>
</span><span id="L-18-12"><a id="L-18-12" name="L-18-12"></a><span>9</span>: <span>return</span> <span>T</span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="beam-search">Beam Search<a href="#beam-search" title="Permanent link"> </a></h3>
<p>Beam search is like exploring multiple paths on a map simultaneously to find the best overall route. Instead of making a single choice at each step and hoping it leads to a good outcome (as in greedy, i.e. 0 temperature, or random sampling, i.e. every other sampling method described here), beam search maintains multiple &#34;parallel universes&#34; of text generation and continuously evaluates which ones are most promising. Imagine you&#39;re writing a story and reach a point where you could continue in several different ways. Rather than committing to just one direction, beam search would explore, say, the top 5 continuations simultaneously. At the next decision point, it&#39;d consider possible continuations for all 5 paths, then keep only the 5 best paths overall. This process repeats until the generation is complete.</p>
<p>Beam search isn&#39;t used much anymore, because it&#39;s expensive, and there are better sampling methods out there.</p>
<p><strong>Technical</strong>:</p>
<p>Typically, we sample twice the beam width candidates at each step to ensure enough valid candidates remain after filtering out completed sequence. This approach was popularized by Google&#39;s Tensor2Tensor library and is now the standard. Mathematically, beam search chooses the <code>k</code> sequences with the highest scores, where the score for a sequence is the sum of logprobs of all tokens in that sequence. This maximizes the joint probability of the entire sequence rather than making locally optimal choices at each step, which is what greedy decoding does. </p>
<p>Unlike random sampling, beam search is deterministic and will always produce the same output given the same input (assuming ties are broken consistently).</p>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-19-1"><a id="L-19-1" name="L-19-1"></a><span>Algorithm</span><span> </span><span>19</span><span> </span><span>Beam</span><span> </span><span>Search</span><span> </span><span>Sampling</span><span></span>
</span><span id="L-19-2"><a id="L-19-2" name="L-19-2"></a><span>Required</span><span>:</span><span> </span><span>Input</span><span> </span><span>sequence</span><span>(</span><span>s</span><span>),</span><span> </span><span>beam</span><span> </span><span>width</span><span> </span><span>k</span><span>,</span><span> </span><span>vocabulary</span><span> </span><span>size</span><span> </span><span>V</span><span>,</span><span> </span><span>language</span><span> </span><span>model</span><span> </span><span>LM</span><span></span>
</span><span id="L-19-3"><a id="L-19-3" name="L-19-3"></a><span>Output</span><span>:</span><span> </span><span>Top</span><span> </span><span>k</span><span> </span><span>most</span><span> </span><span>likely</span><span> </span><span>sequence</span><span> </span><span>continuations</span><span></span>
</span><span id="L-19-4"><a id="L-19-4" name="L-19-4"></a>
</span><span id="L-19-5"><a id="L-19-5" name="L-19-5"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>1</span><span>:</span><span> </span><span>Initialization</span><span></span>
</span><span id="L-19-6"><a id="L-19-6" name="L-19-6"></a><span>1</span><span>:</span><span> </span><span>Initialize</span><span> </span><span>empty</span><span> </span><span>beam</span><span> </span><span>B</span><span> </span><span>=</span><span> </span><span>[]</span><span></span>
</span><span id="L-19-7"><a id="L-19-7" name="L-19-7"></a><span>2</span><span>:</span><span> </span><span>Initialize</span><span> </span><span>first</span><span> </span><span>sequence</span><span> </span><span>with</span><span> </span><span>empty</span><span> </span><span>text</span><span> </span><span>and</span><span> </span><span>score</span><span> </span><span>0</span><span></span>
</span><span id="L-19-8"><a id="L-19-8" name="L-19-8"></a>
</span><span id="L-19-9"><a id="L-19-9" name="L-19-9"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>2</span><span>:</span><span> </span><span>Prompt</span><span> </span><span>Processing</span><span></span>
</span><span id="L-19-10"><a id="L-19-10" name="L-19-10"></a><span>3</span><span>:</span><span> </span><span>logprobs</span><span> </span><span>←</span><span> </span><span>LM</span><span>(</span><span>prompt</span><span>)</span><span>  </span><span>//</span><span> </span><span>Get</span><span> </span><span>logprobs</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span> </span><span>given</span><span> </span><span>prompt</span><span></span>
</span><span id="L-19-11"><a id="L-19-11" name="L-19-11"></a><span>4</span><span>:</span><span> </span><span>candidates</span><span> </span><span>←</span><span> </span><span>[]</span><span></span>
</span><span id="L-19-12"><a id="L-19-12" name="L-19-12"></a><span>5</span><span>:</span><span> </span><span>//</span><span> </span><span>Select</span><span> </span><span>2</span><span>k</span><span> </span><span>tokens</span><span> </span><span>to</span><span> </span><span>ensure</span><span> </span><span>enough</span><span> </span><span>valid</span><span> </span><span>candidates</span><span> </span><span>remain</span><span></span>
</span><span id="L-19-13"><a id="L-19-13" name="L-19-13"></a><span>6</span><span>:</span><span> </span><span>tokens</span><span>,</span><span> </span><span>scores</span><span> </span><span>←</span><span> </span><span>TopK</span><span>(</span><span>logprobs</span><span>,</span><span> </span><span>2</span><span>k</span><span>)</span><span></span>
</span><span id="L-19-14"><a id="L-19-14" name="L-19-14"></a><span>7</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>token</span><span> </span><span>t</span><span>,</span><span> </span><span>score</span><span> </span><span>s</span><span> </span><span>in</span><span> </span><span>(</span><span>tokens</span><span>,</span><span> </span><span>scores</span><span>)</span><span> </span><span>do</span><span></span>
</span><span id="L-19-15"><a id="L-19-15" name="L-19-15"></a><span>8</span><span>:</span><span>     </span><span>candidates</span><span>.</span><span>append</span><span>((</span><span>prompt</span><span> </span><span>+</span><span> </span><span>t</span><span>,</span><span> </span><span>s</span><span>,</span><span> </span><span>0</span><span>))</span><span>  </span><span>//</span><span> </span><span>(</span><span>sequence</span><span>,</span><span> </span><span>score</span><span>,</span><span> </span><span>parent_idx</span><span>)</span><span></span>
</span><span id="L-19-16"><a id="L-19-16" name="L-19-16"></a><span>9</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-17"><a id="L-19-17" name="L-19-17"></a><span>10</span><span>:</span><span> </span><span>Sort</span><span> </span><span>candidates</span><span> </span><span>by</span><span> </span><span>score</span><span> </span><span>(</span><span>descending</span><span>)</span><span></span>
</span><span id="L-19-18"><a id="L-19-18" name="L-19-18"></a><span>11</span><span>:</span><span> </span><span>B</span><span> </span><span>←</span><span> </span><span>candidates</span><span>[</span><span>:k</span><span>]</span><span>  </span><span>//</span><span> </span><span>Keep</span><span> </span><span>top</span><span> </span><span>k</span><span> </span><span>candidates</span><span></span>
</span><span id="L-19-19"><a id="L-19-19" name="L-19-19"></a>
</span><span id="L-19-20"><a id="L-19-20" name="L-19-20"></a><span>//</span><span> </span><span>Phase</span><span> </span><span>3</span><span>:</span><span> </span><span>Generation</span><span> </span><span>Phase</span><span></span>
</span><span id="L-19-21"><a id="L-19-21" name="L-19-21"></a><span>12</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>decoding</span><span> </span><span>step</span><span> </span><span>do</span><span></span>
</span><span id="L-19-22"><a id="L-19-22" name="L-19-22"></a><span>13</span><span>:</span><span>     </span><span>candidates</span><span> </span><span>←</span><span> </span><span>[]</span><span></span>
</span><span id="L-19-23"><a id="L-19-23" name="L-19-23"></a><span>14</span><span>:</span><span>     </span><span>//</span><span> </span><span>For</span><span> </span><span>each</span><span> </span><span>sequence</span><span> </span><span>in</span><span> </span><span>the</span><span> </span><span>beam</span><span></span>
</span><span id="L-19-24"><a id="L-19-24" name="L-19-24"></a><span>15</span><span>:</span><span>     </span><span>for</span><span> </span><span>i</span><span> </span><span>from</span><span> </span><span>0</span><span> </span><span>to</span><span> </span><span>|</span><span>B</span><span>|-</span><span>1</span><span> </span><span>do</span><span></span>
</span><span id="L-19-25"><a id="L-19-25" name="L-19-25"></a><span>16</span><span>:</span><span>         </span><span>seq</span><span>,</span><span> </span><span>cum_score</span><span>,</span><span> </span><span>_</span><span> </span><span>←</span><span> </span><span>B</span><span>[</span><span>i</span><span>]</span><span></span>
</span><span id="L-19-26"><a id="L-19-26" name="L-19-26"></a><span>17</span><span>:</span><span>         </span><span>logprobs</span><span> </span><span>←</span><span> </span><span>LM</span><span>(</span><span>seq</span><span>)</span><span>  </span><span>//</span><span> </span><span>Get</span><span> </span><span>logprobs</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span> </span><span>given</span><span> </span><span>sequence</span><span></span>
</span><span id="L-19-27"><a id="L-19-27" name="L-19-27"></a><span>18</span><span>:</span><span>         </span><span>//</span><span> </span><span>Add</span><span> </span><span>cumulative</span><span> </span><span>score</span><span> </span><span>to</span><span> </span><span>current</span><span> </span><span>logprobs</span><span></span>
</span><span id="L-19-28"><a id="L-19-28" name="L-19-28"></a><span>19</span><span>:</span><span>         </span><span>full_scores</span><span> </span><span>←</span><span> </span><span>logprobs</span><span> </span><span>+</span><span> </span><span>cum_score</span><span></span>
</span><span id="L-19-29"><a id="L-19-29" name="L-19-29"></a><span>20</span><span>:</span><span>         </span><span>//</span><span> </span><span>Flatten</span><span> </span><span>scores</span><span> </span><span>across</span><span> </span><span>all</span><span> </span><span>beam</span><span> </span><span>sequences</span><span> </span><span>and</span><span> </span><span>vocabulary</span><span></span>
</span><span id="L-19-30"><a id="L-19-30" name="L-19-30"></a><span>21</span><span>:</span><span>         </span><span>all_scores</span><span>.</span><span>append</span><span>(</span><span>full_scores</span><span>)</span><span></span>
</span><span id="L-19-31"><a id="L-19-31" name="L-19-31"></a><span>22</span><span>:</span><span>     </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-32"><a id="L-19-32" name="L-19-32"></a><span>23</span><span>:</span><span></span>
</span><span id="L-19-33"><a id="L-19-33" name="L-19-33"></a><span>24</span><span>:</span><span>     </span><span>//</span><span> </span><span>Select</span><span> </span><span>2</span><span>k</span><span> </span><span>highest</span><span> </span><span>scoring</span><span> </span><span>(</span><span>sequence</span><span>,</span><span> </span><span>next_token</span><span>)</span><span> </span><span>combinations</span><span></span>
</span><span id="L-19-34"><a id="L-19-34" name="L-19-34"></a><span>25</span><span>:</span><span>     </span><span>flat_scores</span><span> </span><span>←</span><span> </span><span>Flatten</span><span>(</span><span>all_scores</span><span>)</span><span>  </span><span>//</span><span> </span><span>Size</span><span>:</span><span> </span><span>(</span><span>beam_width</span><span> </span><span>×</span><span> </span><span>vocab_size</span><span>)</span><span></span>
</span><span id="L-19-35"><a id="L-19-35" name="L-19-35"></a><span>26</span><span>:</span><span>     </span><span>top_indices</span><span>,</span><span> </span><span>top_scores</span><span> </span><span>←</span><span> </span><span>TopK</span><span>(</span><span>flat_scores</span><span>,</span><span> </span><span>2</span><span>k</span><span>)</span><span></span>
</span><span id="L-19-36"><a id="L-19-36" name="L-19-36"></a><span>27</span><span>:</span><span></span>
</span><span id="L-19-37"><a id="L-19-37" name="L-19-37"></a><span>28</span><span>:</span><span>     </span><span>//</span><span> </span><span>Extract</span><span> </span><span>parent</span><span> </span><span>sequence</span><span> </span><span>index</span><span> </span><span>and</span><span> </span><span>token</span><span> </span><span>id</span><span> </span><span>from</span><span> </span><span>flat</span><span> </span><span>indices</span><span></span>
</span><span id="L-19-38"><a id="L-19-38" name="L-19-38"></a><span>29</span><span>:</span><span>     </span><span>for</span><span> </span><span>each</span><span> </span><span>idx</span><span>,</span><span> </span><span>score</span><span> </span><span>in</span><span> </span><span>(</span><span>top_indices</span><span>,</span><span> </span><span>top_scores</span><span>)</span><span> </span><span>do</span><span></span>
</span><span id="L-19-39"><a id="L-19-39" name="L-19-39"></a><span>30</span><span>:</span><span>         </span><span>parent_idx</span><span> </span><span>←</span><span> </span><span>idx</span><span> </span><span>÷</span><span> </span><span>V</span><span>  </span><span>//</span><span> </span><span>Integer</span><span> </span><span>division</span><span></span>
</span><span id="L-19-40"><a id="L-19-40" name="L-19-40"></a><span>31</span><span>:</span><span>         </span><span>token_id</span><span> </span><span>←</span><span> </span><span>idx</span><span> </span><span>mod</span><span> </span><span>V</span><span></span>
</span><span id="L-19-41"><a id="L-19-41" name="L-19-41"></a><span>32</span><span>:</span><span>         </span><span>parent_seq</span><span>,</span><span> </span><span>parent_score</span><span>,</span><span> </span><span>_</span><span> </span><span>←</span><span> </span><span>B</span><span>[</span><span>parent_idx</span><span>]</span><span></span>
</span><span id="L-19-42"><a id="L-19-42" name="L-19-42"></a><span>33</span><span>:</span><span>         </span><span>candidates</span><span>.</span><span>append</span><span>((</span><span>parent_seq</span><span> </span><span>+</span><span> </span><span>token_id</span><span>,</span><span> </span><span>score</span><span>,</span><span> </span><span>parent_idx</span><span>))</span><span></span>
</span><span id="L-19-43"><a id="L-19-43" name="L-19-43"></a><span>34</span><span>:</span><span>     </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-44"><a id="L-19-44" name="L-19-44"></a><span>35</span><span>:</span><span></span>
</span><span id="L-19-45"><a id="L-19-45" name="L-19-45"></a><span>36</span><span>:</span><span>     </span><span>//</span><span> </span><span>Handle</span><span> </span><span>completed</span><span> </span><span>sequences</span><span> </span><span>separately</span><span> </span><span>(</span><span>if</span><span> </span><span>needed</span><span>)</span><span></span>
</span><span id="L-19-46"><a id="L-19-46" name="L-19-46"></a><span>37</span><span>:</span><span>     </span><span>...</span><span> </span>
</span><span id="L-19-47"><a id="L-19-47" name="L-19-47"></a><span>38</span><span>:</span><span></span>
</span><span id="L-19-48"><a id="L-19-48" name="L-19-48"></a><span>39</span><span>:</span><span>     </span><span>//</span><span> </span><span>Update</span><span> </span><span>beam</span><span> </span><span>with</span><span> </span><span>top</span><span> </span><span>k</span><span> </span><span>candidates</span><span></span>
</span><span id="L-19-49"><a id="L-19-49" name="L-19-49"></a><span>40</span><span>:</span><span>     </span><span>Sort</span><span> </span><span>candidates</span><span> </span><span>by</span><span> </span><span>score</span><span> </span><span>(</span><span>descending</span><span>)</span><span></span>
</span><span id="L-19-50"><a id="L-19-50" name="L-19-50"></a><span>41</span><span>:</span><span>     </span><span>B</span><span> </span><span>←</span><span> </span><span>candidates</span><span>[</span><span>:k</span><span>]</span><span></span>
</span><span id="L-19-51"><a id="L-19-51" name="L-19-51"></a><span>42</span><span>:</span><span></span>
</span><span id="L-19-52"><a id="L-19-52" name="L-19-52"></a><span>43</span><span>:</span><span>     </span><span>if</span><span> </span><span>stopping_criteria_met</span><span> </span><span>then</span><span></span>
</span><span id="L-19-53"><a id="L-19-53" name="L-19-53"></a><span>44</span><span>:</span><span>         </span><span>break</span><span></span>
</span><span id="L-19-54"><a id="L-19-54" name="L-19-54"></a><span>45</span><span>:</span><span>     </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-19-55"><a id="L-19-55" name="L-19-55"></a><span>46</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-19-56"><a id="L-19-56" name="L-19-56"></a>
</span><span id="L-19-57"><a id="L-19-57" name="L-19-57"></a><span>47</span><span>:</span><span> </span><span>return</span><span> </span><span>B</span><span>  </span><span>//</span><span> </span><span>Return</span><span> </span><span>top</span><span> </span><span>k</span><span> </span><span>sequences</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h3 id="contrastive-search">Contrastive Search<a href="#contrastive-search" title="Permanent link"> </a></h3>
<p>When choosing the next word, CS balances two competing goals: picking words that make sense in context (high probability) while avoiding repetivie patterns (low similarity to what&#39;s already been written). Similar to beam search, this sampling method is not widely used anymore.</p>
<p>CS was invented to address a fundamental limitation of likelihood-based sampling: the tendency to either be too repetitive (at low temps) or too incoherent (at high temps). Instead of just adjusting randomness, it explicitly rewards diversity.</p>
<p><strong>Technical</strong>:</p>
<p>Mathematically, it is:</p>
<p>Where:</p>
<ul>
<li><code>P(x)</code> is the probability of token <code>x</code></li>
<li><code>sim(x, context)</code> is the semantic similarity between <code>x</code> and the context</li>
<li><code>α</code> is the balancing coefficient.</li>
</ul>
<p><strong>Algorithm</strong><br/>
</p><div>

<table><tbody><tr><td></td><td><div><pre><span></span><span id="L-20-1"><a id="L-20-1" name="L-20-1"></a><span>Algorithm</span><span> </span><span>20</span><span> </span><span>Contrastive</span><span> </span><span>Search</span><span></span>
</span><span id="L-20-2"><a id="L-20-2" name="L-20-2"></a><span>Required</span><span>:</span><span> </span><span>Input</span><span> </span><span>prompt</span><span>,</span><span> </span><span>model</span><span> </span><span>M</span><span>,</span><span> </span><span>top</span><span>-</span><span>k</span><span> </span><span>parameter</span><span> </span><span>k</span><span>,</span><span> </span><span>alpha</span><span> </span><span>parameter</span><span> </span><span>α</span><span> </span><span>∈</span><span> </span><span>[</span><span>0,1</span><span>]</span><span></span>
</span><span id="L-20-3"><a id="L-20-3" name="L-20-3"></a><span>Output</span><span>:</span><span> </span><span>Generated</span><span> </span><span>text</span><span> </span><span>that</span><span> </span><span>balances</span><span> </span><span>likelihood</span><span> </span><span>and</span><span> </span><span>diversity</span><span></span>
</span><span id="L-20-4"><a id="L-20-4" name="L-20-4"></a>
</span><span id="L-20-5"><a id="L-20-5" name="L-20-5"></a><span>1</span><span>:</span><span> </span><span>seq</span><span> </span><span>←</span><span> </span><span>prompt</span><span>  </span><span>//</span><span> </span><span>Initialize</span><span> </span><span>with</span><span> </span><span>input</span><span> </span><span>prompt</span><span></span>
</span><span id="L-20-6"><a id="L-20-6" name="L-20-6"></a><span>2</span><span>:</span><span></span>
</span><span id="L-20-7"><a id="L-20-7" name="L-20-7"></a><span>3</span><span>:</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>decoding</span><span> </span><span>step</span><span> </span><span>do</span><span></span>
</span><span id="L-20-8"><a id="L-20-8" name="L-20-8"></a><span>4</span><span>:</span><span>     </span><span>//</span><span> </span><span>Get</span><span> </span><span>logits</span><span> </span><span>from</span><span> </span><span>the</span><span> </span><span>model</span><span> </span><span>for</span><span> </span><span>next</span><span> </span><span>token</span><span></span>
</span><span id="L-20-9"><a id="L-20-9" name="L-20-9"></a><span>5</span><span>:</span><span>     </span><span>logits</span><span> </span><span>←</span><span> </span><span>M</span><span>(</span><span>seq</span><span>)</span><span></span>
</span><span id="L-20-10"><a id="L-20-10" name="L-20-10"></a><span>6</span><span>:</span><span></span>
</span><span id="L-20-11"><a id="L-20-11" name="L-20-11"></a><span>7</span><span>:</span><span>     </span><span>//</span><span> </span><span>Get</span><span> </span><span>top</span><span>-</span><span>k</span><span> </span><span>most</span><span> </span><span>likely</span><span> </span><span>tokens</span><span></span>
</span><span id="L-20-12"><a id="L-20-12" name="L-20-12"></a><span>8</span><span>:</span><span>     </span><span>topk_logits</span><span>,</span><span> </span><span>topk_indices</span><span> </span><span>←</span><span> </span><span>TopK</span><span>(</span><span>logits</span><span>,</span><span> </span><span>k</span><span>)</span><span></span>
</span><span id="L-20-13"><a id="L-20-13" name="L-20-13"></a><span>9</span><span>:</span><span></span>
</span><span id="L-20-14"><a id="L-20-14" name="L-20-14"></a><span>10</span><span>:</span><span>     </span><span>//</span><span> </span><span>Convert</span><span> </span><span>to</span><span> </span><span>probabilities</span><span></span>
</span><span id="L-20-15"><a id="L-20-15" name="L-20-15"></a><span>11</span><span>:</span><span>     </span><span>topk_probs</span><span> </span><span>←</span><span> </span><span>Softmax</span><span>(</span><span>topk_logits</span><span>)</span><span></span>
</span><span id="L-20-16"><a id="L-20-16" name="L-20-16"></a><span>12</span><span>:</span><span></span>
</span><span id="L-20-17"><a id="L-20-17" name="L-20-17"></a><span>13</span><span>:</span><span>     </span><span>//</span><span> </span><span>Get</span><span> </span><span>hidden</span><span> </span><span>representations</span><span> </span><span>from</span><span> </span><span>the</span><span> </span><span>model</span><span>&#39;</span><span>s</span><span> </span><span>last</span><span> </span><span>layer</span><span></span>
</span><span id="L-20-18"><a id="L-20-18" name="L-20-18"></a><span>14</span><span>:</span><span>     </span><span>H</span><span> </span><span>←</span><span> </span><span>GetHiddenStates</span><span>(</span><span>M</span><span>,</span><span> </span><span>seq</span><span>)</span><span></span>
</span><span id="L-20-19"><a id="L-20-19" name="L-20-19"></a><span>15</span><span>:</span><span>     </span><span>h_last</span><span> </span><span>←</span><span> </span><span>H</span><span>[</span><span>-1</span><span>]</span><span>  </span><span>//</span><span> </span><span>Hidden</span><span> </span><span>state</span><span> </span><span>for</span><span> </span><span>the</span><span> </span><span>last</span><span> </span><span>token</span><span></span>
</span><span id="L-20-20"><a id="L-20-20" name="L-20-20"></a><span>16</span><span>:</span><span></span>
</span><span id="L-20-21"><a id="L-20-21" name="L-20-21"></a><span>17</span><span>:</span><span>     </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>degeneration</span><span> </span><span>penalty</span><span> </span><span>for</span><span> </span><span>each</span><span> </span><span>candidate</span><span> </span><span>token</span><span></span>
</span><span id="L-20-22"><a id="L-20-22" name="L-20-22"></a><span>18</span><span>:</span><span>     </span><span>penalties</span><span> </span><span>←</span><span> </span><span>[]</span><span></span>
</span><span id="L-20-23"><a id="L-20-23" name="L-20-23"></a><span>19</span><span>:</span><span>     </span><span>for</span><span> </span><span>each</span><span> </span><span>candidate</span><span> </span><span>token</span><span> </span><span>t</span><span> </span><span>in</span><span> </span><span>topk_indices</span><span> </span><span>do</span><span></span>
</span><span id="L-20-24"><a id="L-20-24" name="L-20-24"></a><span>20</span><span>:</span><span>         </span><span>//</span><span> </span><span>Get</span><span> </span><span>hidden</span><span> </span><span>representation</span><span> </span><span>if</span><span> </span><span>we</span><span> </span><span>were</span><span> </span><span>to</span><span> </span><span>append</span><span> </span><span>this</span><span> </span><span>token</span><span></span>
</span><span id="L-20-25"><a id="L-20-25" name="L-20-25"></a><span>21</span><span>:</span><span>         </span><span>h_candidate</span><span> </span><span>←</span><span> </span><span>GetHiddenState</span><span>(</span><span>M</span><span>,</span><span> </span><span>seq</span><span> </span><span>+</span><span> </span><span>t</span><span>)</span><span>[</span><span>-1</span><span>]</span><span></span>
</span><span id="L-20-26"><a id="L-20-26" name="L-20-26"></a><span>22</span><span>:</span><span></span>
</span><span id="L-20-27"><a id="L-20-27" name="L-20-27"></a><span>23</span><span>:</span><span>         </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>similarity</span><span> </span><span>between</span><span> </span><span>candidate</span><span> </span><span>and</span><span> </span><span>context</span><span></span>
</span><span id="L-20-28"><a id="L-20-28" name="L-20-28"></a><span>24</span><span>:</span><span>         </span><span>sim</span><span> </span><span>←</span><span> </span><span>CosineSimilarity</span><span>(</span><span>h_candidate</span><span>,</span><span> </span><span>h_last</span><span>)</span><span></span>
</span><span id="L-20-29"><a id="L-20-29" name="L-20-29"></a><span>25</span><span>:</span><span></span>
</span><span id="L-20-30"><a id="L-20-30" name="L-20-30"></a><span>26</span><span>:</span><span>         </span><span>//</span><span> </span><span>Store</span><span> </span><span>similarity</span><span> </span><span>as</span><span> </span><span>the</span><span> </span><span>degeneration</span><span> </span><span>penalty</span><span></span>
</span><span id="L-20-31"><a id="L-20-31" name="L-20-31"></a><span>27</span><span>:</span><span>         </span><span>penalties</span><span>.</span><span>append</span><span>(</span><span>sim</span><span>)</span><span></span>
</span><span id="L-20-32"><a id="L-20-32" name="L-20-32"></a><span>28</span><span>:</span><span>     </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-20-33"><a id="L-20-33" name="L-20-33"></a><span>29</span><span>:</span><span></span>
</span><span id="L-20-34"><a id="L-20-34" name="L-20-34"></a><span>30</span><span>:</span><span>     </span><span>//</span><span> </span><span>Calculate</span><span> </span><span>final</span><span> </span><span>scores</span><span> </span><span>using</span><span> </span><span>weighted</span><span> </span><span>combination</span><span></span>
</span><span id="L-20-35"><a id="L-20-35" name="L-20-35"></a><span>31</span><span>:</span><span>     </span><span>//</span><span> </span><span>Higher</span><span> </span><span>alpha</span><span> </span><span>weights</span><span> </span><span>likelihood</span><span> </span><span>more</span><span>,</span><span> </span><span>lower</span><span> </span><span>alpha</span><span> </span><span>weights</span><span> </span><span>diversity</span><span> </span><span>more</span><span></span>
</span><span id="L-20-36"><a id="L-20-36" name="L-20-36"></a><span>32</span><span>:</span><span>     </span><span>scores</span><span> </span><span>←</span><span> </span><span>α</span><span> </span><span>*</span><span> </span><span>topk_probs</span><span> </span><span>-</span><span> </span><span>(</span><span>1</span><span>-</span><span>α</span><span>)</span><span> </span><span>*</span><span> </span><span>penalties</span><span></span>
</span><span id="L-20-37"><a id="L-20-37" name="L-20-37"></a><span>33</span><span>:</span><span></span>
</span><span id="L-20-38"><a id="L-20-38" name="L-20-38"></a><span>34</span><span>:</span><span>     </span><span>//</span><span> </span><span>Select</span><span> </span><span>token</span><span> </span><span>with</span><span> </span><span>highest</span><span> </span><span>combined</span><span> </span><span>score</span><span></span>
</span><span id="L-20-39"><a id="L-20-39" name="L-20-39"></a><span>35</span><span>:</span><span>     </span><span>best_idx</span><span> </span><span>←</span><span> </span><span>ArgMax</span><span>(</span><span>scores</span><span>)</span><span></span>
</span><span id="L-20-40"><a id="L-20-40" name="L-20-40"></a><span>36</span><span>:</span><span>     </span><span>next_token</span><span> </span><span>←</span><span> </span><span>topk_indices</span><span>[</span><span>best_idx</span><span>]</span><span></span>
</span><span id="L-20-41"><a id="L-20-41" name="L-20-41"></a><span>37</span><span>:</span><span></span>
</span><span id="L-20-42"><a id="L-20-42" name="L-20-42"></a><span>38</span><span>:</span><span>     </span><span>//</span><span> </span><span>Append</span><span> </span><span>to</span><span> </span><span>sequence</span><span></span>
</span><span id="L-20-43"><a id="L-20-43" name="L-20-43"></a><span>39</span><span>:</span><span>     </span><span>seq</span><span> </span><span>←</span><span> </span><span>seq</span><span> </span><span>+</span><span> </span><span>next_token</span><span></span>
</span><span id="L-20-44"><a id="L-20-44" name="L-20-44"></a><span>40</span><span>:</span><span></span>
</span><span id="L-20-45"><a id="L-20-45" name="L-20-45"></a><span>41</span><span>:</span><span>     </span><span>if</span><span> </span><span>stopping_criteria_met</span><span> </span><span>then</span><span></span>
</span><span id="L-20-46"><a id="L-20-46" name="L-20-46"></a><span>42</span><span>:</span><span>         </span><span>break</span><span></span>
</span><span id="L-20-47"><a id="L-20-47" name="L-20-47"></a><span>43</span><span>:</span><span>     </span><span>end</span><span> </span><span>if</span><span></span>
</span><span id="L-20-48"><a id="L-20-48" name="L-20-48"></a><span>44</span><span>:</span><span> </span><span>end</span><span> </span><span>for</span><span></span>
</span><span id="L-20-49"><a id="L-20-49" name="L-20-49"></a><span>45</span><span>:</span><span></span>
</span><span id="L-20-50"><a id="L-20-50" name="L-20-50"></a><span>46</span><span>:</span><span> </span><span>return</span><span> </span><span>seq</span><span></span>
</span></pre></div>
</td></tr></tbody></table>
</div>
<h2 id="sampler-order">Sampler Order<a href="#sampler-order" title="Permanent link"> </a></h2>
<p>The document thus far has explained individual sampling methods, but in real-world LLM implementations, these techniques are often applied in a carefully orchestrated sequence. Some libraries allow for the user to customize the order of samplers per-request, but most do not.</p>
<h3 id="the-typical-sampling-pipeline">The Typical Sampling Pipeline<a href="#the-typical-sampling-pipeline" title="Permanent link"> </a></h3>
<p>A typical sampling pipeline in production LLM systems follow these sequential steps:</p>
<ol>
<li><strong>Generate Raw Logits</strong>: The model produces unnormalized logits for each token in the vocabulary.</li>
<li><strong>Apply Token Filtering/Banning</strong>: Remove tokens that shouldn&#39;t be considred at all.</li>
<li><strong>Apply Penalties</strong>: Apply repetition, frequency, and presence penalties to discourage repetitive outputs.</li>
<li><strong>Apply Pattern-Based Techniques</strong>: Methods like DRY (Don&#39;t Repeat Yourself) are applied to identify and penalize repetitive patterns.</li>
<li><strong>Apply Temperature Scaling</strong>: Temperature is either applied as the first or the last sampler (outside penalties and post-softmax samplers), depending on the implementation. For most tasks, temperature is applied first. For creative writing, it is usually last.</li>
<li><strong>Apply Distribution-Shaping Methods</strong>: Techniques like Top-K, Top-P, Min-P, etc. that filter or reshape the probability distribution.</li>
<li><strong>Sample from the Final Distribution</strong>: After all modifications, select a token from the resulting probability distribution.</li>
</ol>
<h3 id="effects-and-interactions-of-samplers-with-each-other">Effects and Interactions of Samplers with Each Other<a href="#effects-and-interactions-of-samplers-with-each-other" title="Permanent link"> </a></h3>
<p>The order in which sampling techniques are applied have serious implications for text generation. This is an oft-unexplored topic, and libraries usually follow a somewhat standardized order without any flexibility. Each sampler modifies the probability distribution, creating a transformed landscape that subsequent samplers work with. Understanding these is very crucial.</p>
<h4 id="how-samplers-transform-distributions">How Samplers Transform Distributions<a href="#how-samplers-transform-distributions" title="Permanent link"> </a></h4>
<p>To understand sampler interaction, let&#39;s first visualize it:</p>
<ol>
<li><strong>Original Distribution</strong>: A typical token distribution has a few high-probability tokens followed by a long tail of increasingly unlikely options.</li>
<li><strong>After Penalties</strong>: Penalties flatten peaks for previously used tokens, so they elevate alternatives that haven&#39;t appeared yet.</li>
<li><strong>After Temperature</strong>: Lower temperatures (&lt;1.0) sharpen the distribution, making peaks higher and valleys lower. Higher temperatures (&gt;1.0) flatten the distribution.</li>
<li><strong>After Filtering (Top-K/P/etc.)</strong>: These truncate the distribution, removing lowest-probability tokens and often renormalizing the remaining probs.</li>
</ol>
<p>Each transformation, as you may guess, fundamentally changes what subsequent samplers <strong>see</strong> and can dramatically influence the final output.</p>
<h4 id="critical-order-dependent-interactions">Critical Order-Dependent Interactions<a href="#critical-order-dependent-interactions" title="Permanent link"> </a></h4>
<h5 id="temperature-before-vs-after-filtering">Temperature Before vs. After Filtering<a href="#temperature-before-vs-after-filtering" title="Permanent link"> </a></h5>
<p>One of the most important ordering decisions is whether to apply temperature scaling before or after filtering methods:</p>
<p><strong>Temperature → Filtering</strong>:</p>
<ul>
<li>Temperature first reshapes the entire distribution</li>
<li>Low temperature concentrates probability mass on fewer tokens before filtering</li>
<li>High temperature spreads probability mass more evenly before filtering</li>
<li>Filtering then operates on this reshaped distribution</li>
</ul>
<p><strong>Filtering → Temperature</strong>:</p>
<ul>
<li>Filtering first truncates the distribution to only the most likely tokens</li>
<li>Temperature then only affects the relative probabilities among these filtered tokens</li>
<li>Can produce more constrained outputs even with high temperatures</li>
</ul>
<p>Example: With a Top-K of 40 and temperature of 1.5, applying temperature first might allow some tokens from outside the original top 40 to become likely enough to survive filtering. Applying filtering first ensures only the original top 40 tokens remain, regardless of temperature.</p>
<h5 id="penalties-before-vs-after-other-samplers">Penalties Before vs. After Other Samplers<a href="#penalties-before-vs-after-other-samplers" title="Permanent link"> </a></h5>
<p><strong>Penalties → Temperature</strong>:</p>
<ul>
<li>Penalties first reduce probabilities of repeated tokens</li>
<li>Temperature then amplifies or diminishes these adjustments</li>
<li>With high temperatures, penalties may be effectively erased</li>
<li>With low temperatures, penalties may be excessively amplified</li>
</ul>
<p><strong>Temperature → Penalties</strong>:</p>
<ul>
<li>Temperature first reshapes the entire distribution</li>
<li>Penalties then operate on this temperature-modified distribution</li>
<li>Can lead to more balanced and predictable penalty effects</li>
</ul>
<h5 id="drys-position-matters">DRY&#39;s Position Matters<a href="#drys-position-matters" title="Permanent link"> </a></h5>
<p>The DRY (Don&#39;t Repeat Yourself) sampler, which penalizes continuing n-gram patterns, is particularly sensitive to positioning:</p>
<p><strong>DRY Early in Pipeline</strong>:</p>
<ul>
<li>Applied to the raw or lightly modified distribution</li>
<li>Has strong effect on preventing repetition</li>
<li>Subsequent samplers may still introduce repetition by elevating penalized tokens</li>
</ul>
<p><strong>DRY Late in Pipeline</strong>:</p>
<ul>
<li>Applied after other samplers have modified the distribution</li>
<li>May have weaker effect if previous samplers have already eliminated some options</li>
<li>Final arbiter against repetition before token selection</li>
</ul>
<h4 id="synergies-and-conflicts">Synergies and Conflicts<a href="#synergies-and-conflicts" title="Permanent link"> </a></h4>
<h5 id="synergistic-combos">Synergistic Combos<a href="#synergistic-combos" title="Permanent link"> </a></h5>
<ol>
<li><strong>Top-K + Top-P</strong>: Top-K provides a hard limit on tokens considered, while Top-P adapts to the confidence of the model. Together they give us guardrails and enough flexibility.</li>
<li><strong>Temperature + Min-P</strong>: High temperature flattens the distribution, while Min-P establishes a quality floor relative to the best token. This combination increases creativity while filtering out truly poor options.</li>
</ol>
<h5 id="conflicting-combos">Conflicting Combos<a href="#conflicting-combos" title="Permanent link"> </a></h5>
<ol>
<li><strong>High Temperature + Low Top-K</strong>: High temperature tries to flatten the distribution, while low Top-K severely restricts options. The Top-K will essentially override much of the temperature&#39;s effect.</li>
<li><strong>Multiple Filtering Methods</strong>: Applying Top-K, Top-P, Min-P, and TFS together often results in the most restrictive one dominating, making the others redundant.</li>
<li><strong>XTC + Top-A</strong>: Both methods aim to exclude top choices but in different ways. Using both can overly restrict the sampling space.</li>
</ol></div>
                    </article>
                </div>
            </div></div>
  </body>
</html>
