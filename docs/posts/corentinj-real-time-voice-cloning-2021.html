<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">Original</a>
    <h1>CorentinJ: Real-Time Voice Cloning (2021)</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This repository is an implementation of <a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow">Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis</a> (SV2TTS) with a vocoder that works in real-time. This was my <a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow">master&#39;s thesis</a>.</p>
<p dir="auto">SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.</p>
<p dir="auto"><strong>Video demonstration</strong> (click the picture):</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"><img src="https://camo.githubusercontent.com/4fbd119863c1135130ec9fdcb9340523c327b1e147a57341bee9e592b4694d4e/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png"/></a></p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>URL</th>
<th>Designation</th>
<th>Title</th>
<th>Implementation source</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"><strong>1806.04558</strong></a></td>
<td><strong>SV2TTS</strong></td>
<td><strong>Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</strong></td>
<td>This repo</td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow">1802.08435</a></td>
<td>WaveRNN (vocoder)</td>
<td>Efficient Neural Audio Synthesis</td>
<td><a href="https://github.com/fatchord/WaveRNN">fatchord/WaveRNN</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1703.10135.pdf" rel="nofollow">1703.10135</a></td>
<td>Tacotron (synthesizer)</td>
<td>Tacotron: Towards End-to-End Speech Synthesis</td>
<td><a href="https://github.com/fatchord/WaveRNN">fatchord/WaveRNN</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow">1710.10467</a></td>
<td>GE2E (encoder)</td>
<td>Generalized End-To-End Loss for Speaker Verification</td>
<td>This repo</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:</p>
<ul dir="auto">
<li>Check out <a href="https://paperswithcode.com/task/speech-synthesis/" rel="nofollow">paperswithcode</a> for other repositories and recent research in the field of speech synthesis.</li>
<li>Check out <a href="https://github.com/resemble-ai/chatterbox">Chatterbox</a> for a similar project up to date with the 2025 SOTA in voice cloning</li>
</ul>


<ol dir="auto">
<li>Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.</li>
<li>Python 3.7 is recommended. Python 3.5 or greater should work, but you&#39;ll probably have to tweak the dependencies&#39; versions. I recommend setting up a virtual environment using <code>venv</code>, but this is optional.</li>
<li>Install <a href="https://ffmpeg.org/download.html#get-packages" rel="nofollow">ffmpeg</a>. This is necessary for reading audio files.</li>
<li>Install <a href="https://pytorch.org/get-started/locally/" rel="nofollow">PyTorch</a>. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.</li>
<li>Install the remaining requirements with <code>pip install -r requirements.txt</code></li>
</ol>
<div dir="auto"><h3 tabindex="-1" dir="auto">2. (Optional) Download Pretrained Models</h3><a id="user-content-2-optional-download-pretrained-models" aria-label="Permalink: 2. (Optional) Download Pretrained Models" href="#2-optional-download-pretrained-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Pretrained models are now downloaded automatically. If this doesn&#39;t work for you, you can manually download them <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models">here</a>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">3. (Optional) Test Configuration</h3><a id="user-content-3-optional-test-configuration" aria-label="Permalink: 3. (Optional) Test Configuration" href="#3-optional-test-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Before you download any dataset, you can begin by testing your configuration with:</p>
<p dir="auto"><code>python demo_cli.py</code></p>
<p dir="auto">If all tests pass, you&#39;re good to go.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">4. (Optional) Download Datasets</h3><a id="user-content-4-optional-download-datasets" aria-label="Permalink: 4. (Optional) Download Datasets" href="#4-optional-download-datasets"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For playing with the toolbox alone, I only recommend downloading <a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"><code>LibriSpeech/train-clean-100</code></a>. Extract the contents as <code>&lt;datasets_root&gt;/LibriSpeech/train-clean-100</code> where <code>&lt;datasets_root&gt;</code> is a directory of your choosing. Other datasets are supported in the toolbox, see <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets">here</a>. You&#39;re free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.</p>

<p dir="auto">You can then try the toolbox:</p>
<p dir="auto"><code>python demo_toolbox.py -d &lt;datasets_root&gt;</code></p>
<p dir="auto">depending on whether you downloaded any datasets. If you are running an X-server or if you have the error <code>Aborted (core dumped)</code>, see <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590" data-hovercard-type="issue" data-hovercard-url="/CorentinJ/Real-Time-Voice-Cloning/issues/11/hovercard">this issue</a>.</p>
</article></div></div>
  </body>
</html>
