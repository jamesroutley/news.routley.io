<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hanlab.mit.edu/blog/svdquant">Original</a>
    <h1>SVDQuant: 4-Bit Quantization Powers 12B Flux on a 16GB 4090 GPU with 3x Speedup</h1>
    
    <div id="readability-page-1" class="page"><div><p>Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han</p><p>November 7, 2024</p></div><p>A new post-training training quantization paradigm for diffusion models, which quantize both the weights and activations of FLUX.1 to 4 bits, achieving 3.5× memory and 8.7× latency reduction on a 16GB laptop 4090 GPU.</p><div><p>
  <img src="https://github.com/mit-han-lab/nunchaku/blob/main/assets/demo.gif?raw=true" width="70%"/>
</p><p>Check our interactive demo at <a href="https://svdquant.mit.edu">https://svdquant.mit.edu</a>! Our quantization library is at <a href="https://github.com/mit-han-lab/deepcompressor">github.com/mit-han-lab/deepcompressor</a> and inference engine is at <a href="https://github.com/mit-han-lab/nunchaku">github.com/mit-han-lab/nunchaku</a>. Our paper is at <a href="http://arxiv.org/abs/2411.05007">this link</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672d1bcef3c3ec127e8078fd_672d1b2115081c1e8ac82ea9_teaser.jpeg" loading="lazy" alt=""/></p></figure><p>SVDQuant is a post-training quantization technique for 4-bit weights and activations that well maintains visual fidelity. On 12B FLUX.1-dev, it achieves 3.6× memory reduction compared to the BF16 model. By eliminating CPU offloading, it offers 8.7× speedup over the 16-bit model when on a 16GB laptop 4090 GPU, 3× faster than the NF4 W4A16 baseline. On PixArt-∑, it demonstrates significantly superior visual quality over other W4A4 or even W4A8 baselines.</p><h2>Background</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672a9dcbacb7d59271c5662e_672a9a6c364fccc4e4443187_trend.jpeg" loading="lazy" alt=""/></p><figcaption>Computation <em>v.s.</em> parameters for LLMs and diffusion models.</figcaption></figure><p>Diffusion models are revolutionizing AI with their ability to generate high-quality images from text prompts. To improve image quality and improve the alignment between text and image, researchers are scaling up these models. As shown in the right figure, while Stable Diffusion 1.4 has 800 million parameters, newer models like AuraFlow and FLUX.1 reach billions, delivering more refined and detailed outputs. However, scaling brings challenges: these models become computationally heavy, demanding high memory and longer processing times, making them prohibitive for real-time applications.</p><p>As Moore&#39;s law slows down, hardware vendors are turning to low-precision inference, such as NVIDIA&#39;s new 4-bit floating point (FP4) precision in Blackwell. In large language models (LLMs), quantization has helped reduce model sizes and speed up inference, primarily by addressing latency from loading model weights. Diffusion models, however, are computationally bound, even for single batches, so quantizing weights alone yields limited gains. To achieve measured speedups, both weights and activations must be quantized to the same bit width; otherwise, the lower precision is upcast during computation, negating any performance benefits.</p><p>In this blog, we introduce SVDQuant to quantize both the weights and activations of diffusion models to 4 bits. At such an aggressive level, conventional post-training methods fall short. Unlike smoothing, which redistributes outliers, SVDQuant absorbs them through a high-precision low-rank branch, significantly preserving image quality. Visual examples demonstrate its effectiveness. See the above figure for some visual examples.</p><h2>SVDQuant: Absorbing Outliers via Low-Rank Branch</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672a66701e9812afb49e0cfa_672a651614cb88ed4fd9dc29_intuition-animate.gif" loading="lazy" alt=""/></p></figure><div>


    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>LaTeX Rendering Example</title>
    


    <p>
The key idea behind SVDQuant is to introduce an additional low-rank branch that can absorb quantization difficulties in both weights and activations. As shown in the above animation, originally, both the activation \( \boldsymbol{X} \) and weights \( \boldsymbol{W} \) contain massive outliers, making 4-bit quantization challenging. We can first aggregate the outliers by migrating them from activations to weights via smoothing, resulting in the updated activation \( \hat{\boldsymbol{X}} \) and weights \( \hat{\boldsymbol{W}} \). While \( \hat{\boldsymbol{X}} \) becomes easier to quantize, \( \hat{\boldsymbol{W}} \) now becomes more difficult. At the last stage, SVDQuant further decomposes \( \hat{\boldsymbol{W}} \) into a low-rank component \( \boldsymbol{L}_1 \boldsymbol{L}_2 \) and a residual \( \hat{\boldsymbol{W}} - \boldsymbol{L}_1 \boldsymbol{L}_2 \) with Singular Value Decomposition (SVD). As the singular value distribution of \( \hat{\boldsymbol{W}} \) is highly imbalanced, with only the first several values being significantly larger, removing these dominant values can dramatically reduce \( \hat{\boldsymbol{W}} \)’s magnitude and outliers, as suggested by <a href="https://en.wikipedia.org/wiki/Low-rank_approximation">Eckart-Young-Mirsky theorem</a>. Thus, the quantization difficulty is alleviated by the low-rank branch, which runs at 16-bit precision. The below figure illustrates an example value distribution of the input activations and weights in PixArt-∑.
    </p>

</div><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672abca95c5f67bff08b7664_672abca0fde8974bb6d63e6d_distribution.jpeg" loading="lazy" alt=""/></p><figcaption>Example value distribution of inputs and weights in PixArt-∑.</figcaption></figure><h2>Nunchaku: Fusing Low-Rank and Low-Bit Branch Kernels</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672d1fccad9d41d739c84c2c_672a6ac09cf7fc46071c3fb8_672a6a991adf3346a0ee4dbb_engine.jpeg" loading="lazy" alt=""/></p></figure><p>Although the low-rank branch adds only minor computational costs on paper, running it separately can lead to significant latency overhead—about 50% of the 4-bit branch&#39;s latency, as shown in figure (a). This is because, despite reduced computation costs with a small rank, the data size of input and output activations remains the same, shifting the bottleneck to memory access instead of computation.</p><p>To address this, we co-designed our inference engine, Nunchaku, with the SVDQuant algorithm. Specifically, we noted that the down projection in the low-rank branch uses the same input as the quantization kernel in the low-bit branch, and the up projection shares the same output as the 4-bit computation kernel, as shown in figure (b). By fusing the down projection with the quantization kernel and the up projection with the 4-bit computation kernel, the low-rank branch can now share activations with the low-bit branch. This eliminates extra memory access and cuts the number of kernel calls in half. As a result, the low-rank branch now adds only 5–10% additional latency, making its cost almost negligible.</p><h2>Performance</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672d1bcdf3c3ec127e8078f3_672d1b5772f1247fa24e231d_efficiency.jpeg" loading="lazy" alt=""/></p></figure><p>SVDQuant reduces the model size of the 12B FLUX.1 by 3.6×. Additionally, Nunchaku further cuts memory usage of the 16-bit model by 3.5× and delivers 3.0× speedups over the NF4 W4A16 baseline on both the desktop and laptop NVIDIA RTX 4090 GPUs. Remarkably, on laptop 4090, it achieves in total 10.1× speedup by eliminating CPU offloading. </p><h2>Quality</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672ac9098069a7c6f2baedbd_672ac78c0b59dc8da3c15987_visual.jpeg" loading="lazy" alt=""/></p></figure><p>On FLUX.1 models, our 4-bit models outperform the NF4 W4A16 baselines, demonstrating superior text alignment and closer similarity to the 16-bit models. For instance, NF4 misinterprets &#34;dinosaur style,&#34; generating a real dinosaur. On PixArt-∑ and SDXL-Turbo, our 4-bit results demonstrate noticeably better visual quality than ViDiT-Q&#39;s and MixDQ&#39;s W4A8 results.</p><p>‍</p><h2>Integrate with LoRA</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672a6c74c47e63fff4b5b216_672a6c25ef1acf8704ef65af_lora.jpeg" loading="lazy" alt=""/></p></figure><p>Traditional quantization methods require fusing LoRA branches and then requantizing the model when integrating LoRAs. Our SVDQuant, however, avoids redundant memory access, making it possible to add a separate LoRA branch directly. The figure above shows visual examples of our INT4 FLUX.1-dev model with LoRAs applied in five distinct styles—<a href="https://huggingface.co/XLabs-AI/flux-RealismLora">Realism</a>, <a href="https://huggingface.co/aleksa-codes/flux-ghibsky-illustration">Ghibsky Illustration</a>, <a href="https://huggingface.co/alvdansen/sonny-anime-fixed">Anime</a>, <a href="https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Children-Simple-Sketch">Children Sketch</a>, and <a href="https://huggingface.co/linoyts/yarn_art_Flux_LoRA">Yarn Art</a>. Our INT4 model adapts seamlessly to each style, maintaining the image quality of the original 16-bit version.</p></div></div>
  </body>
</html>
