<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mina.naguib.ca/blog/2012/10/22/the-little-ssh-that-sometimes-couldnt.html">Original</a>
    <h1>The little SSH that sometimes couldn&#39;t (2012)</h1>
    
    <div id="readability-page-1" class="page"><div>
    <h3 id="preface">Preface</h3>
<p>This is a technical article chronicling one of the most interesting bug hunts I’ve had the pleasure of chasing down.</p>
<p>At <a href="http://adgear.com/">AdGear Technologies Inc.</a> where I work, ssh is king.  We use it for management, monitoring, deployments, log file harvesting, even some event streaming.  It’s solid, reliable, has all the predictability of a native unix tool, and just works.</p>
<p>Until one day, random cron emails started flowing about it not working.</p>
<h3 id="the-timeout">The timeout</h3>
<p>The machines in our London data center were randomly failing to send their event log files to our data machines in our Montreal data center.  This job is initiated periodically from cron, and the failure manifested itself as:</p>
<ul>
<li>cron emails stating that the ssh was unsuccessful
<ul>
<li>Sometimes hangs</li>
<li>Sometimes exits with a timeout error</li>
</ul>
</li>
<li>monitoring warnings down the line for in-house sanity checks detecting the missing data in Montreal</li>
</ul>
<p>We logged into the London machines, manually ran the push command, and it worked successfully.  We brushed it off as temporary network partitions.</p>
<h3 id="the-timeouts">The timeouts</h3>
<p>But the failures kept popping up randomly.  Once a day, a couple of times a day, then one Friday morning, several times an hour.  It was clear something’s getting worse.  We kept up with manually pushing the files until we figure out what the problem was.</p>
<p>There were 17 hops between London and Montreal.  We built a profile of latency and packet loss for them, and found that a couple were losing 1-3% of packets.  We filed a ticket with our London DC ops to route away from them.</p>
<p>While London DC ops were verifying the packet loss, we started seeing random timeouts from London to our SECOND data center in Montreal, and hops to that data center did not share the same routes we observed the packet loss at.  We concluded packet loss is not the main problem around the same time London DC ops replied saying they’re not able to replicate the packet loss or timeouts and that everything looked healthy on their end.</p>
<h3 id="the-revelation">The revelation</h3>
<p>While manually keeping up with failed cron uploads, we noticed an interesting pattern.  A file transfer either succeeded at a high speed, or didn’t succeed at all and hung/timed out.  There were no instances of a file uploading slowly and finishing successfully.</p>
<p>Removing the large volume of data from the equation, we were able to recreate the scenario via simple vanilla ssh.  On a London machine an “ssh mtl-machine” would either work immediately, or hang and never establish a connection.  Eyebrows started going up.</p>
<h3 id="where-the-wild-packets-are">Where the wild packets are</h3>
<p>We triple-checked the ssh server configs and health in Montreal:</p>
<ul>
<li>The servers appeared healthy by all measures</li>
<li>SSHd DNS reverse lookup was not enabled</li>
<li>SSHd Maximum client connections was high enough</li>
<li>We were not under attack</li>
<li>Bandwidth usage was nowhere near saturation</li>
</ul>
<p>Besides, even if something was off, we were observing the hangs talking to 2 completely distinct data centers in Montreal.  Furthermore, our other data centers (non-London) were talking happily to Montreal.  Something about London was off.</p>
<p>We fired up tcpdump and started looking at the packets, both in summary and in captured pcaps loaded into wireshark.  We saw telltale signs of packet loss and retransmission, but it was minimal and not particularly worrisome.</p>
<p>We then captured full connections from cases where ssh established successfully, and full connections from cases where the ssh connection hung.</p>
<p>Here’s what we logically saw when a connection from London to Montreal hung:</p>
<ul>
<li>Normal TCP handshake</li>
<li>Bunch of ssh-specific back and forth, with normal TCP ACK packets where they should be</li>
<li>A particular packet sent from London and received in Montreal</li>
<li>The same packet re-sent (and re-sent, several times) from London and received in Montreal</li>
<li>Montreal’s just not responding to it!</li>
</ul>
<p>It didn’t make sense why Montreal was not responding (hence London re-transmitting it).  The connection was stalled at this point, as the layer 4 protocol was at a stalemate.  More infuriatingly, if you kill the ssh attempt in London and re-launched it immediately, odds are it worked successfully.  When it did, tcpdump showed Montreal receiving the packet but responding to it, and things moved on.</p>
<p>We enabled verbose debugging (-vvv) on the ssh client in London, and the hang occurred after it logged:</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>debug2: kex_parse_kexinit: first_kex_follows 0 
</span></span><span><span>debug2: kex_parse_kexinit: reserved 0 
</span></span><span><span>debug2: mac_setup: found hmac-md5
</span></span><span><span>debug1: kex: server-&gt;client aes128-ctr hmac-md5 none
</span></span><span><span>debug2: mac_setup: found hmac-md5
</span></span><span><span>debug1: kex: client-&gt;server aes128-ctr hmac-md5 none
</span></span><span><span>debug1: SSH2_MSG_KEX_DH_GEX_REQUEST(1024&lt;1024&lt;8192) sent
</span></span><span><span>debug1: expecting SSH2_MSG_KEX_DH_GEX_GROUP</span></span></code></pre></div>
<p>Googling “ssh hang SSH2_MSG_KEX_DH_GEX_GROUP” has many results - from bad WiFi, to windows TCP bugs, to buggy routers discarding TCP fragments.  One solution for LANs was to figure out the path’s MSS and set that as the MTU on both ends.</p>
<p>I kept decrementing the MTU on a London server down from 1500 - it didn’t help until I hit the magic value 576.  At that point, I was no longer able to get the ssh hanging behavior replicated.  I had an ssh loop script running, and it was on-demand that I could cause timeouts by bringing the MTU back up to 1500, or make them disappear by setting it to 576.</p>
<p>Unfortunately these are public web servers and globally setting the MTU to 576 won’t cut it, but the above did suggest that perhaps packet fragmentation or reassembly is broken somewhere.</p>
<p>Going back to check the received packets with tcpdump, there was no evidence of fragmentation.  The received packet size matched exactly the packet size sent.  If something did fragment the packet at byte 576+, something else reassembled it successfully.</p>
<h3 id="twinkle-twinkle-little-mis-shapen-star">Twinkle twinkle little mis-shapen star</h3>
<p>Digging in some more, I was now looking at full packet dumps (tcpdump -s 0 -X) instead of just the headers.  Comparing that magic packet in instances of ssh success vs ssh hang showed very little difference aside from TCP/IP header variations.  It was however clear that this is the first packet in the TCP connection that had enough data to bypass the 576-byte mark - all previous packets were much smaller.</p>
<p>Comparing the same packet, during a hanging instance, as it left London, and as captured in Montreal, something caught my eye.  Something very subtle, and I brushed it off as fatigue (it was late Friday at this point), but sure enough after a few refreshes and comparisons, I wasn’t imagining things.</p>
<p>Here’s the packet as it left London (minus the first few bytes identifying the IP addresses):</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>0x0040:  0b7c aecc 1774 b770 ad92 0000 00b7 6563  .|...t.p......ec
</span></span><span><span>0x0050:  6468 2d73 6861 322d 6e69 7374 7032 3536  dh-sha2-nistp256
</span></span><span><span>0x0060:  2c65 6364 682d 7368 6132 2d6e 6973 7470  ,ecdh-sha2-nistp
</span></span><span><span>0x0070:  3338 342c 6563 6468 2d73 6861 322d 6e69  384,ecdh-sha2-ni
</span></span><span><span>0x0080:  7374 7035 3231 2c64 6966 6669 652d 6865  stp521,diffie-he
</span></span><span><span>0x0090:  6c6c 6d61 6e2d 6772 6f75 702d 6578 6368  llman-group-exch
</span></span><span><span>0x00a0:  616e 6765 2d73 6861 3235 362c 6469 6666  ange-sha256,diff
</span></span><span><span>0x00b0:  6965 2d68 656c 6c6d 616e 2d67 726f 7570  ie-hellman-group
</span></span><span><span>0x00c0:  2d65 7863 6861 6e67 652d 7368 6131 2c64  -exchange-sha1,d
</span></span><span><span>0x00d0:  6966 6669 652d 6865 6c6c 6d61 6e2d 6772  iffie-hellman-gr
</span></span><span><span>0x00e0:  6f75 7031 342d 7368 6131 2c64 6966 6669  oup14-sha1,diffi
</span></span><span><span>0x00f0:  652d 6865 6c6c 6d61 6e2d 6772 6f75 7031  e-hellman-group1
</span></span><span><span>0x0100:  2d73 6861 3100 0000 2373 7368 2d72 7361  -sha1...#ssh-rsa
</span></span><span><span>0x0110:  2c73 7368 2d64 7373 2c65 6364 7361 2d73  ,ssh-dss,ecdsa-s
</span></span><span><span>0x0120:  6861 322d 6e69 7374 7032 3536 0000 009d  ha2-nistp256....
</span></span><span><span>0x0130:  6165 7331 3238 2d63 7472 2c61 6573 3139  aes128-ctr,aes19
</span></span><span><span>0x0140:  322d 6374 722c 6165 7332 3536 2d63 7472  2-ctr,aes256-ctr
</span></span><span><span>0x0150:  2c61 7263 666f 7572 3235 362c 6172 6366  ,arcfour256,arcf
</span></span><span><span>0x0160:  6f75 7231 3238 2c61 6573 3132 382d 6362  our128,aes128-cb
</span></span><span><span>0x0170:  632c 3364 6573 2d63 6263 2c62 6c6f 7766  c,3des-cbc,blowf
</span></span><span><span>0x0180:  6973 682d 6362 632c 6361 7374 3132 382d  ish-cbc,cast128-
</span></span><span><span>0x0190:  6362 632c 6165 7331 3932 2d63 6263 2c61  cbc,aes192-cbc,a
</span></span><span><span>0x01a0:  6573 3235 362d 6362 632c 6172 6366 6f75  es256-cbc,arcfou
</span></span><span><span>0x01b0:  722c 7269 6a6e 6461 656c 2d63 6263 406c  r,rijndael-cbc@l
</span></span><span><span>0x01c0:  7973 6174 6f72 2e6c 6975 2e73 6500 0000  ysator.liu.se...
</span></span><span><span>0x01d0:  9d61 6573 3132 382d 6374 722c 6165 7331  .aes128-ctr,aes1
</span></span><span><span>0x01e0:  3932 2d63 7472 2c61 6573 3235 362d 6374  92-ctr,aes256-ct
</span></span><span><span>0x01f0:  722c 6172 6366 6f75 7232 3536 2c61 7263  r,arcfour256,arc
</span></span><span><span>0x0200:  666f 7572 3132 382c 6165 7331 3238 2d63  four128,aes128-c
</span></span><span><span>0x0210:  6263 2c33 6465 732d 6362 632c 626c 6f77  bc,3des-cbc,blow
</span></span><span><span>0x0220:  6669 7368 2d63 6263 2c63 6173 7431 3238  fish-cbc,cast128
</span></span><span><span>0x0230:  2d63 6263 2c61 6573 3139 322d 6362 632c  -cbc,aes192-cbc,
</span></span><span><span>0x0240:  6165 7332 3536 2d63 6263 2c61 7263 666f  aes256-cbc,arcfo
</span></span><span><span>0x0250:  7572 2c72 696a 6e64 6165 6c2d 6362 6340  ur,rijndael-cbc@
</span></span><span><span>0x0260:  6c79 7361 746f 722e 6c69 752e 7365 0000  lysator.liu.se..
</span></span><span><span>0x0270:  00a7 686d 6163 2d6d 6435 2c68 6d61 632d  ..hmac-md5,hmac-
</span></span><span><span>0x0280:  7368 6131 2c75 6d61 632d 3634 406f 7065  sha1,umac-64@ope
</span></span><span><span>0x0290:  6e73 7368 2e63 6f6d 2c68 6d61 632d 7368  nssh.com,hmac-sh
</span></span><span><span>0x02a0:  6132 2d32 3536 2c68 6d61 632d 7368 6132  a2-256,hmac-sha2
</span></span><span><span>0x02b0:  2d32 3536 2d39 362c 686d 6163 2d73 6861  -256-96,hmac-sha
</span></span><span><span>0x02c0:  322d 3531 322c 686d 6163 2d73 6861 322d  2-512,hmac-sha2-
</span></span><span><span>0x02d0:  3531 322d 3936 2c68 6d61 632d 7269 7065  512-96,hmac-ripe
</span></span><span><span>0x02e0:  6d64 3136 302c 686d 6163 2d72 6970 656d  md160,hmac-ripem
</span></span><span><span>0x02f0:  6431 3630 406f 7065 6e73 7368 2e63 6f6d  <a href="https://mina.naguib.ca/cdn-cgi/l/email-protection" data-cfemail="1377222523537c63767d60607b3d707c7e">[email protected]</a>
</span></span><span><span>0x0300:  2c68 6d61 632d 7368 6131 2d39 362c 686d  ,hmac-sha1-96,hm
</span></span><span><span>0x0310:  6163 2d6d 6435 2d39 3600 0000 a768 6d61  ac-md5-96....hma
</span></span><span><span>0x0320:  632d 6d64 352c 686d 6163 2d73 6861 312c  c-md5,hmac-sha1,
</span></span><span><span>0x0330:  756d 6163 2d36 3440 6f70 656e 7373 682e  umac-64@openssh.
</span></span><span><span>0x0340:  636f 6d2c 686d 6163 2d73 6861 322d 3235  com,hmac-sha2-25
</span></span><span><span>0x0350:  362c 686d 6163 2d73 6861 322d 3235 362d  6,hmac-sha2-256-
</span></span><span><span>0x0360:  3936 2c68 6d61 632d 7368 6132 2d35 3132  96,hmac-sha2-512
</span></span><span><span>0x0370:  2c68 6d61 632d 7368 6132 2d35 3132 2d39  ,hmac-sha2-512-9
</span></span><span><span>0x0380:  362c 686d 6163 2d72 6970 656d 6431 3630  6,hmac-ripemd160
</span></span><span><span>0x0390:  2c68 6d61 632d 7269 7065 6d64 3136 3040  ,hmac-ripemd160@
</span></span><span><span>0x03a0:  6f70 656e 7373 682e 636f 6d2c 686d 6163  openssh.com,hmac
</span></span><span><span>0x03b0:  2d73 6861 312d 3936 2c68 6d61 632d 6d64  -sha1-96,hmac-md
</span></span><span><span>0x03c0:  352d 3936 0000 0015 6e6f 6e65 2c7a 6c69  5-96....none,zli
</span></span><span><span>0x03d0:  6240 6f70 656e 7373 682e 636f 6d00 0000  <a href="https://mina.naguib.ca/cdn-cgi/l/email-protection" data-cfemail="0567456a75606b76766d2b666a68">[email protected]</a>...
</span></span><span><span>0x03e0:  156e 6f6e 652c 7a6c 6962 406f 7065 6e73  .none,zlib@opens
</span></span><span><span>0x03f0:  7368 2e63 6f6d 0000 0000 0000 0000 0000  sh.com..........
</span></span><span><span>0x0400:  0000 0000 0000 0000 0000 0000            ............</span></span></code></pre></div>
<p>And here’s the same packet as it arrived in Montreal:</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>0x0040:  0b7c aecc 1774 b770 ad92 0000 00b7 6563  .|...t.p......ec
</span></span><span><span>0x0050:  6468 2d73 6861 322d 6e69 7374 7032 3536  dh-sha2-nistp256
</span></span><span><span>0x0060:  2c65 6364 682d 7368 6132 2d6e 6973 7470  ,ecdh-sha2-nistp
</span></span><span><span>0x0070:  3338 342c 6563 6468 2d73 6861 322d 6e69  384,ecdh-sha2-ni
</span></span><span><span>0x0080:  7374 7035 3231 2c64 6966 6669 652d 6865  stp521,diffie-he
</span></span><span><span>0x0090:  6c6c 6d61 6e2d 6772 6f75 702d 6578 6368  llman-group-exch
</span></span><span><span>0x00a0:  616e 6765 2d73 6861 3235 362c 6469 6666  ange-sha256,diff
</span></span><span><span>0x00b0:  6965 2d68 656c 6c6d 616e 2d67 726f 7570  ie-hellman-group
</span></span><span><span>0x00c0:  2d65 7863 6861 6e67 652d 7368 6131 2c64  -exchange-sha1,d
</span></span><span><span>0x00d0:  6966 6669 652d 6865 6c6c 6d61 6e2d 6772  iffie-hellman-gr
</span></span><span><span>0x00e0:  6f75 7031 342d 7368 6131 2c64 6966 6669  oup14-sha1,diffi
</span></span><span><span>0x00f0:  652d 6865 6c6c 6d61 6e2d 6772 6f75 7031  e-hellman-group1
</span></span><span><span>0x0100:  2d73 6861 3100 0000 2373 7368 2d72 7361  -sha1...#ssh-rsa
</span></span><span><span>0x0110:  2c73 7368 2d64 7373 2c65 6364 7361 2d73  ,ssh-dss,ecdsa-s
</span></span><span><span>0x0120:  6861 322d 6e69 7374 7032 3536 0000 009d  ha2-nistp256....
</span></span><span><span>0x0130:  6165 7331 3238 2d63 7472 2c61 6573 3139  aes128-ctr,aes19
</span></span><span><span>0x0140:  322d 6374 722c 6165 7332 3536 2d63 7472  2-ctr,aes256-ctr
</span></span><span><span>0x0150:  2c61 7263 666f 7572 3235 362c 6172 6366  ,arcfour256,arcf
</span></span><span><span>0x0160:  6f75 7231 3238 2c61 6573 3132 382d 6362  our128,aes128-cb
</span></span><span><span>0x0170:  632c 3364 6573 2d63 6263 2c62 6c6f 7766  c,3des-cbc,blowf
</span></span><span><span>0x0180:  6973 682d 6362 632c 6361 7374 3132 382d  ish-cbc,cast128-
</span></span><span><span>0x0190:  6362 632c 6165 7331 3932 2d63 6263 2c61  cbc,aes192-cbc,a
</span></span><span><span>0x01a0:  6573 3235 362d 6362 632c 6172 6366 6f75  es256-cbc,arcfou
</span></span><span><span>0x01b0:  722c 7269 6a6e 6461 656c 2d63 6263 406c  r,rijndael-cbc@l
</span></span><span><span>0x01c0:  7973 6174 6f72 2e6c 6975 2e73 6500 0000  ysator.liu.se...
</span></span><span><span>0x01d0:  9d61 6573 3132 382d 6374 722c 6165 7331  .aes128-ctr,aes1
</span></span><span><span>0x01e0:  3932 2d63 7472 2c61 6573 3235 362d 6374  92-ctr,aes256-ct
</span></span><span><span>0x01f0:  722c 6172 6366 6f75 7232 3536 2c61 7263  r,arcfour256,arc
</span></span><span><span>0x0200:  666f 7572 3132 382c 6165 7331 3238 2d63  four128,aes128-c
</span></span><span><span>0x0210:  6263 2c33 6465 732d 6362 632c 626c 6f77  bc,3des-cbc,blow
</span></span><span><span>0x0220:  6669 7368 2d63 6263 2c63 6173 7431 3238  fish-cbc,cast128
</span></span><span><span>0x0230:  2d63 6263 2c61 6573 3139 322d 6362 632c  -cbc,aes192-cbc,
</span></span><span><span>0x0240:  6165 7332 3536 2d63 6263 2c61 7263 666f  aes256-cbc,arcfo
</span></span><span><span>0x0250:  7572 2c72 696a 6e64 6165 6c2d 6362 7340  ur,rijndael-cbs@
</span></span><span><span>0x0260:  6c79 7361 746f 722e 6c69 752e 7365 1000  lysator.liu.se..
</span></span><span><span>0x0270:  00a7 686d 6163 2d6d 6435 2c68 6d61 732d  ..hmac-md5,hmas-
</span></span><span><span>0x0280:  7368 6131 2c75 6d61 632d 3634 406f 7065  sha1,umac-64@ope
</span></span><span><span>0x0290:  6e73 7368 2e63 6f6d 2c68 6d61 632d 7368  nssh.com,hmac-sh
</span></span><span><span>0x02a0:  6132 2d32 3536 2c68 6d61 632d 7368 7132  a2-256,hmac-shq2
</span></span><span><span>0x02b0:  2d32 3536 2d39 362c 686d 6163 2d73 7861  -256-96,hmac-sxa
</span></span><span><span>0x02c0:  322d 3531 322c 686d 6163 2d73 6861 322d  2-512,hmac-sha2-
</span></span><span><span>0x02d0:  3531 322d 3936 2c68 6d61 632d 7269 7065  512-96,hmac-ripe
</span></span><span><span>0x02e0:  6d64 3136 302c 686d 6163 2d72 6970 756d  md160,hmac-ripum
</span></span><span><span>0x02f0:  6431 3630 406f 7065 6e73 7368 2e63 7f6d  <a href="https://mina.naguib.ca/cdn-cgi/l/email-protection" data-cfemail="4f2b7e797f0f203f2a213c3c27612c6122">[email protected]</a>
</span></span><span><span>0x0300:  2c68 6d61 632d 7368 6131 2d39 362c 786d  ,hmac-sha1-96,xm
</span></span><span><span>0x0310:  6163 2d6d 6435 2d39 3600 0000 a768 7d61  ac-md5-96....h}a
</span></span><span><span>0x0320:  632d 6d64 352c 686d 6163 2d73 6861 312c  c-md5,hmac-sha1,
</span></span><span><span>0x0330:  756d 6163 2d36 3440 6f70 656e 7373 782e  umac-64@openssx.
</span></span><span><span>0x0340:  636f 6d2c 686d 6163 2d73 6861 322d 3235  com,hmac-sha2-25
</span></span><span><span>0x0350:  362c 686d 6163 2d73 6861 322d 3235 362d  6,hmac-sha2-256-
</span></span><span><span>0x0360:  3936 2c68 6d61 632d 7368 6132 2d35 3132  96,hmac-sha2-512
</span></span><span><span>0x0370:  2c68 6d61 632d 7368 6132 2d35 3132 3d39  ,hmac-sha2-512=9
</span></span><span><span>0x0380:  362c 686d 6163 2d72 6970 656d 6431 3630  6,hmac-ripemd160
</span></span><span><span>0x0390:  2c68 6d61 632d 7269 7065 6d64 3136 3040  ,hmac-ripemd160@
</span></span><span><span>0x03a0:  6f70 656e 7373 682e 636f 6d2c 686d 7163  openssh.com,hmqc
</span></span><span><span>0x03b0:  2d73 6861 312d 3936 2c68 6d61 632d 7d64  -sha1-96,hmac-}d
</span></span><span><span>0x03c0:  352d 3936 0000 0015 6e6f 6e65 2c7a 7c69  5-96....none,z|i
</span></span><span><span>0x03d0:  6240 6f70 656e 7373 682e 636f 6d00 0000  <a href="https://mina.naguib.ca/cdn-cgi/l/email-protection" data-cfemail="482a0827382d263b3b20662b2725">[email protected]</a>...
</span></span><span><span>0x03e0:  156e 6f6e 652c 7a6c 6962 406f 7065 6e73  .none,zlib@opens
</span></span><span><span>0x03f0:  7368 2e63 6f6d 0000 0000 0000 0000 0000  sh.com..........
</span></span><span><span>0x0400:  0000 0000 0000 0000 0000 0000            ............</span></span></code></pre></div>
<p>Did something there catch your eye ?  If not, I don’t blame you.  Feel free to copy each into a text editor and rapidly switch back-and-forth to see some characters dance.  Here’s what it looks like when they’re placed in vimdiff:</p>
<p><img src="https://mina.naguib.ca/images/blog/vimdiff_packets.png" alt="Vim diff packet"/></p>
<p>Well well well. It’s not packet loss, it’s packet corruption!  Very subtle, very predictable packet corruption.</p>
<p>Some interesting notes:</p>
<ul>
<li>The lower part of the packet (&lt;576 bytes) is unaffected</li>
<li>The affected portion is predictably corrupted on the 15th byte of every 16</li>
<li>The corruption is predictable.  All instances of “h” become “x”, all instances of “c” become “s”</li>
</ul>
<p>Some readers might have already checked ASCII charts and reached the conclusion:  There’s a single bit statically stuck at “1” somewhere.  Flipping the 4th bit in a byte to 1 would reliably corrupt the above letters on the left side to the value on the right side.</p>
<p>The obvious culprits within our control (NIC cards, receiving machines) are not suspect due to the pattern of failure observed (several London machines -&gt; Several Montreal data centers and machines).  It’s got to be something upstream and close to London.</p>
<p>Going back to validate, things started to make sense.  I also noticed a little hint in tcpdump verbose mode (tcp cksum bad) which was missed before.  A Montreal machine receiving this packet discarded it at the kernel level after realizing it’s corrupt, never passing it to the userland ssh daemon.  London then re-transmitted it, going through the same corruption, getting the same silent treatment.  From ssh and sshd’s perspective, the connection was at a stalemate.  From tcpdump’s perspective, there was no loss, and Montreal machines appeared to be just ignoring data.</p>
<p>We sent these findings to our London DC ops, and within a few minutes they changed outbound routes dramatically.  The first router hop, and most hops afterwards, were different.  The hanging problem disappeared.</p>
<p>Late Friday night fixes are nice because you can relax and not carry problems and support staff into the weekend :)</p>
<h3 id="wheres-waldo">Where’s Waldo</h3>
<p>Happy that we were no longer suffering from this problem and that our systems are caught up with the backlog, I decided I’d try my hand at actually finding the device causing the corruption.</p>
<p>Having the London routes updated to not go through the old path meant that I couldn’t reproduce the problem easily.  I asked around until I found a friend with a FreeBSD box in Montreal I could use, which was still accessed through the old routes from London.</p>
<p>Next, I wanted to make sure that the corruption is predictable even without ssh involvement.  This was trivially proven with a few pipes.</p>
<p>In Montreal:</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>nc -l -p 4000 &gt; /dev/null</span></span></code></pre></div>
<p>Then in London:</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>cat /dev/zero | nc mtl 4000</span></span></code></pre></div>
<p>Again, accounting for the randomness factor and settings things up in a retry loop, I got a few packets which remove any doubt about the previous conclusions.  Here’s part of one - remember that we’re sending just a stream of nulls(zeroes):</p>
<div><pre tabindex="0"><code data-lang="text"><span><span>0x0210  .....
</span></span><span><span>0x0220  0000 0000 0000 0000 0000 0000 0000 0000 ................
</span></span><span><span>0x0230  0000 0000 0000 0000 0000 0000 0000 0000 ................
</span></span><span><span>0x0240  0000 0000 0000 0000 0000 0000 0000 0000 ................
</span></span><span><span>0x0250  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0260  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0270  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0280  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0290  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x02a0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x02b0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x02c0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x02d0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x02e0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x02f0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0300  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0310  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0320  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0330  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0340  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0350  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0360  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0370  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0380  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x0390  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x03a0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x03b0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x03c0  0000 0000 0000 0000 0000 0000 0000 1000 ................
</span></span><span><span>0x03d0  0000 0000 0000 0000 0000 0000 0000 0000 ................
</span></span><span><span>0x03e0  .....</span></span></code></pre></div>
<p>With the bug replicated, I needed to find a way to isolate which of the 17 hops along that path cause the corruption.  There was simply no way to call up the provider of each cluster to ask them to check their systems.</p>
<p>I decided pinging each router, incrementally, might be the way to go.  I crafted special ICMP packets that are large enough to go over the 576 safety margin, and filled entirely with NULLs.  Then pinged the Montreal machine with them from London.</p>
<p>They came back perfectly normal.  There was no corruption.</p>
<p>I tried all variations of speed, padding, size - to no avail.  I simply could not observe corruption in the returned ICMP ping packets.</p>
<p>I replaced the netcat pipes with UDP instead of TCP.  Again there was no corruption.</p>
<p>The corruption needed TCP to be reproducible - and TCP needs 2 cooperating endpoints.  I tried in vain to see if all 17 router hops had an open TCP port I can talk to directly, to no avail.</p>
<p>It seemed there was no easy way an external party can pinpoint the bad apple. Or was there ?</p>
<h3 id="mirror-mirror-on-the-wall">Mirror mirror on the wall</h3>
<p>To detect whether corruption occurred or not, we need one of these scenarios:</p>
<ul>
<li>Control over the TCP peer we’re talking to inspect the packet at the destination
<ul>
<li>Not just in userland, where the packet would not get delivered if the TCP checksum failed, but root + tcpdump to inspect it as it arrives</li>
</ul>
</li>
<li>A TCP peer that acts as an echo server to mirror back the data it received, so we get to inspect it at the sending node and detect corruption there</li>
</ul>
<p>It suddenly occurred to me that the second data point is available to us.  Not per-se, but consider this:  In our very first taste of the problem, we observed ssh clients hanging when talking to ssh servers over the corrupting hop.  This is a good passive signal that we can use instead of the active “echo” signal.</p>
<p>… and there are lots of open ssh servers out there on the internet to help us out.</p>
<p>We don’t need actual accounts on these servers - we just need to kickstart the ssh connection and see if the cipher exchange phase succeeds or hangs (with a reasonable number of retries to account for corruption randomness).</p>
<p>So this plan was hatched:</p>
<ul>
<li>Use the wonderful <strong>nmap</strong> tool - specifically - its “random IP” mode - to make a list of geographically distributed open ssh servers</li>
<li>Test each server to determine whether it is:
<ul>
<li>Unresponsive/unpredictable/firewalled -&gt; Ignore it</li>
<li>Negotiates successfully after being retried N times -&gt; mark as “good”</li>
<li>Negotiates with hangs at the telltale phase after being retried N times -&gt; mark as “bad”</li>
</ul>
</li>
<li>For both “good” and “bad” servers, remember the traceroute to them</li>
</ul>
<p>The idea was this:  All servers marked as “bad” will share a few hops in their traceroute.  We can then take that set of suspect hops, and subtract from it any that appear in the traceroutes of the “good” servers.  Hopefully what’s left is only one or two.</p>
<p>After spending an hour manually doing the above exercise, I stopped to inspect the data.  I had classified 16 servers as “BAD” and 25 servers as “GOOD”.</p>
<p>The first exercise was to find the list of hops that appear in all the traceroutes of the “BAD” servers.  As I cleaned and trimmed the list, I realized I won’t even need to get to the “GOOD” list to remove false positives.  Within the “BAD” lists alone, there remained only 1 that was common to all of them.</p>
<p>For what it’s worth, it was 2 providers away:  London -&gt; N hops upstream1 -&gt; Y hops upstream2</p>
<p>It was the first in Y hops of upstream2 - right at the edge between upstream1 and upstream2, corrupting random TCP packets, causing many retries, and, depending on the protocol’s logical back-and-forth, hangs, or reduced transmission rates.  You may have been a telephony provider who sufferred dropped calls, a retailer who lost a few customers or sales, the possibilities really are endless.</p>
<p>I followed up with our London DC ops with the single hop’s IP address.  Hopefully with their direct relationship with upstream1 they can escalate through there and get it fixed.</p>
<p>/filed under crazy devops war stories</p>
<h3 id="update">Update</h3>
<p>Through upstream1, I got confirmation that the hop I pointed out (first in upstream2) had an internal “management module failure” which affected BGP and routing between two internal networks.  It’s still down (they’ve routed around it) until they receive a replacement for the faulty module.</p>
<p>Thanks for the kind words and great comments here on Disqus, Reddit (<a href="http://www.reddit.com/r/linux/comments/11x7ld/the_little_ssh_that_sometimes_couldnt/">/r/linux</a> &amp; <a href="http://www.reddit.com/r/sysadmin/comments/129bpf/the_little_ssh_that_sometimes_couldnt/">/r/sysadmin</a>) and <a href="http://news.ycombinator.com/item?id=4709438">hacker news</a></p>
<h3 id="if-you-liked-this-you-might-also-like">If you liked this, you might also like</h3>
<ul>
<li><a href="http://www.fragmentationneeded.net/2012/01/dispatches-from-trading-floor-moldudp.html">Dispatches From The Trading Floor - MoldUDP</a></li>
<li><a href="http://blog.krisk.org/2013/02/packets-of-death.html">Packets of Death</a></li>
<li><a href="http://www.ibiblio.org/harris/500milemail.html">The case of the 500-mile email</a></li>
<li><a href="https://code.facebook.com/posts/1499322996995183/solving-the-mystery-of-link-imbalance-a-metastable-failure-state-at-scale/">Solving the Mystery of Link Imbalance: A Metastable Failure State at Scale</a></li>
<li><a href="http://www.pagerduty.com/blog/the-discovery-of-apache-zookeepers-poison-packet/">The Discovery of Apache ZooKeeper’s Poison Packet</a></li>
<li><a href="https://blog.cloudflare.com/the-story-of-one-latency-spike/">The story of one latency spike</a> &amp; <a href="https://blog.cloudflare.com/revenge-listening-sockets/">The revenge of the listening sockets</a></li>
<li><a href="https://mailman.nanog.org/pipermail/nanog/2018-September/096871.html">Service provider story about tracking down TCP RSTs</a></li>
<li><a href="https://cloud.google.com/blog/products/management-tools/sre-keeps-digging-to-prevent-problems">Finding a problem at the bottom of the Google stack</a></li>
<li><a href="https://news.sherlock.stanford.edu/posts/tracking-nfs-problems-down-to-the-sfp-level">Tracking NFS problems down to the SFP level</a></li>
<li><a href="https://engineering.skroutz.gr/blog/uncovering-a-24-year-old-bug-in-the-linux-kernel/">Uncovering a 24-year-old bug in the Linux Kernel</a></li>
<li><a href="https://dirtypipe.cm4all.com/">The Dirty Pipe Vulnerability</a></li>
<li><a href="https://blog.ando.fyi/posts/diagnosing-an-unsual-wifi-issue/">Resolving an unusual wifi issue</a></li>
<li><a href="https://patrickthomson.tumblr.com/post/2499755681/the-best-debugging-story-ive-ever-heard">The Best Debugging Story I’ve Ever Heard</a></li>
<li><a href="https://notes.valdikss.org.ru/jabber.ru-mitm/">Encrypted traffic interception on Hetzner and Linode targeting the largest Russian XMPP (Jabber) messaging service</a></li>
<li><a href="https://medium.com/adevinta-tech-blog/its-not-always-dns-unless-it-is-16858df17d3f">It’s not always DNS — unless it is</a></li>
<li><a href="https://www.youtube.com/watch?v=XrlrbfGZo2k">37C3 - Breaking “DRM” in Polish trains</a></li>
</ul>

  </div></div>
  </body>
</html>
