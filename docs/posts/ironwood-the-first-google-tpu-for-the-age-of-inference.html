<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/">Original</a>
    <h1>Ironwood: The first Google TPU for the age of inference</h1>
    
    <div id="readability-page-1" class="page"><article>

    
    


<section>
  
</section>


    

    
      

<div data-analytics-module="{
    &#34;module_name&#34;: &#34;Hero Menu&#34;,
    &#34;section_header&#34;: &#34;Ironwood: The first Google TPU for the age of inference&#34;
  }">
  
  <div>
    <div>
      <div>
        <div>
          
            <p>Apr 09, 2025</p>
          
          
            <p><span aria-hidden="true">·</span></p><p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
        




      </div>
      
        <p>
          Ironwood is our most powerful, capable and energy efficient TPU yet, designed to power thinking, inferential AI models at scale.
        </p>
      
    </div>
  </div>
  
  <div>
    
    
      
        


<div data-component="uni-ai-generated-summary" data-analytics-module="{
    &#34;event&#34;: &#34;module_impression&#34;,
    &#34;module_name&#34;: &#34;ai_summary&#34;,
    &#34;section_header&#34;: &#34;CTA&#34;
  }">
  <div data-component="uni-ai-summary-btn">
    <div>
      
        <div data-summary-id="ai_summary_1">
          <h2>General summary</h2>
          <p>Google has released Ironwood, its seventh-generation Tensor Processing Unit (TPU), specifically designed for inference. This powerful AI accelerator is built to handle the massive computational demands of &#34;thinking models,&#34; like large language models and mixture of experts. Ironwood scales up to 9,216 chips, offering 42.5 Exaflops of compute power, making it more powerful than the world&#39;s largest supercomputer.</p>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
      
        <div data-summary-id="ai_summary_2">
          <h2>Bullet points</h2>
          <ul>
<li>Google introduces Ironwood, its seventh-generation TPU, designed specifically for inference.</li>
<li>Ironwood is Google&#39;s most powerful and energy-efficient TPU, built to support the &#34;age of inference.&#34;</li>
<li>It scales up to 9,216 chips, offering over 24x the compute power of the world&#39;s largest supercomputer.</li>
<li>Ironwood features enhanced SparseCore, increased HBM capacity and bandwidth, and improved ICI networking.</li>
<li>This new TPU enables Google Cloud customers to tackle demanding AI workloads with high performance and efficiency.</li>
</ul>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
      
        <div data-summary-id="ai_summary_4">
          <h2>Shakespeare-ish</h2>
          <p>A new TPU, Ironwood, Google doth bring,</p>
<p>From models vast, with thinking minds so keen,</p>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
      

      
      
      

      </div>
  </div>
</div>

      
    
    
  </div>
</div>

    

    
      <div>
  <div>
    <div>
      <div>
        <video autoplay="" muted="" loop="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Ironwood_video_1.mp4" title="a picture of processing units with the phrase &#34;Ironwood&#34; on them" poster="
            
              https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/TPUv7_Hero-Image_2096x1182.png
            ">
          Sorry, your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can
            <a href="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Ironwood_video_1.mp4">download it</a>
            and watch it with your favorite video player!
        </video>
      </div>
      
    </div>
  </div>
</div>

    

    
    <section>
      <div>
        
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Ironwood: The first Google TPU for the age of inference" listen-to-article="Listen to article" data-date-modified="2025-04-09T15:00:07.703390+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;Ironwood: The first Google TPU for the age of inference&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="jyf9e">Today at Google Cloud Next 25, we’re introducing Ironwood, our seventh-generation Tensor Processing Unit (TPU) — our most performant and scalable custom AI accelerator to date, and the first designed specifically for inference. For more than a decade, TPUs have powered Google’s most demanding AI training and serving workloads, and have enabled our Cloud customers to do the same. Ironwood is our most powerful, capable and energy efficient TPU yet. And it&#39;s purpose-built to power thinking, inferential AI models at scale.</p><p data-block-key="9rrl9">Ironwood represents a significant shift in the development of AI and the infrastructure that powers its progress. It’s a move from <i>responsive</i> AI models that provide real-time information for people to interpret, to models that provide the <i>proactive</i> generation of insights and interpretation. This is what we call the “age of inference” where AI agents will proactively retrieve and generate data to collaboratively deliver insights and answers, not just data.</p><p data-block-key="7kvmh">Ironwood is built to support this next phase of generative AI and its tremendous computational and communication requirements. It scales up to 9,216 liquid cooled chips linked with breakthrough Inter-Chip Interconnect (ICI) networking spanning nearly 10 MW. It is one of several new components of <a href="https://cloud.google.com/blog/products/compute/whats-new-with-ai-hypercomputer">Google Cloud AI Hypercomputer</a> architecture, which optimizes hardware and software together for the most demanding AI workloads. With Ironwood, developers can also leverage Google’s own <a href="https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/">Pathways</a> software stack to reliably and easily harness the combined computing power of tens of thousands of Ironwood TPUs.</p><p data-block-key="at188">Here’s a closer look at how these innovations work together to take on the most demanding training and serving workloads with unparalleled performance, cost and power efficiency.</p></div>
      </div>
    </div>
  

  
    







<uni-related-content-tout title="Google Cloud Next 25" cta="See more" summary="Here’s a look at what we announced at Google Cloud Next 25." hideimage="False" eyebrow="Collection" image-alt-text="" role="none" fullurl="https://blog.google/products/google-cloud/next-2025/" pagetype="collectiondetailpage" isarticlepage="">
  
    
  
</uni-related-content-tout>

  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;Ironwood: The first Google TPU for the age of inference&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><h2 data-block-key="qleld">Powering the age of inference with Ironwood</h2><p data-block-key="4cq8f">Ironwood is designed to gracefully manage the complex computation and communication demands of &#34;thinking models,&#34; which encompass Large Language Models (LLMs), Mixture of Experts (MoEs) and advanced reasoning tasks. These models require massive parallel processing and efficient memory access. In particular, Ironwood is designed to minimize data movement and latency on chip while carrying out massive tensor manipulations. At the frontier, the computation demands of thinking models extend well beyond the capacity of any single chip. We designed Ironwood TPUs with a low-latency, high bandwidth ICI network to support coordinated, synchronous communication at full TPU pod scale.</p><p data-block-key="2ec6a">For Google Cloud customers, Ironwood comes in two sizes based on AI workload demands: a 256 chip configuration and a 9,216 chip configuration.</p><ul><li data-block-key="6aolq">When scaled to 9,216 chips per pod for a total of 42.5 Exaflops, Ironwood supports more than 24x the compute power of the world’s largest supercomputer – El Capitan – which offers just 1.7 Exaflops per pod. Ironwood delivers the massive parallel processing power necessary for the most demanding AI workloads, such as super large size dense LLM or MoE models with thinking capabilities for training and inference. Each individual chip boasts peak compute of 4,614 TFLOPs. This represents a monumental leap in AI capability. Ironwood’s memory and network architecture ensures that the right data is always available to support peak performance at this massive scale.</li><li data-block-key="11gmm">Ironwood also features an enhanced <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#sparsecore">SparseCore</a>, a specialized accelerator for processing ultra-large embeddings common in advanced ranking and recommendation workloads. Expanded SparseCore support in Ironwood allows for a wider range of workloads to be accelerated, including moving beyond the traditional AI domain to financial and scientific domains.</li><li data-block-key="9p4p1">Pathways, Google’s own ML runtime <a href="https://arxiv.org/abs/2203.12533">developed</a> by Google DeepMind, enables efficient distributed computing across multiple TPU chips. Pathways on Google Cloud makes moving beyond a single Ironwood Pod straightforward, enabling hundreds of thousands of Ironwood chips to be composed together to rapidly advance the frontiers of gen AI computation.</li></ul></div>
      </div>
    </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="a green bar chart showing progressive improvement in the performance of TPUs" external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Ironwood: The first Google TPU for the age of inference" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="j21ql">Figure 1. Improvement in the total FP8 peak flops performance relative to TPU v2, Google’s first external Cloud TPU.</p>
    </div>
  
  
    <p><img alt="a green bar chart showing progressive improvement in the performance of TPUs" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_PeakPerformanceGraph.width-100.format-webp.webp" loading="lazy" data-loading="{
            &#34;mobile&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_PeakPerformanceGraph.width-500.format-webp.webp&#34;,
            &#34;desktop&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_PeakPerformanceGrap.width-1000.format-webp.webp&#34;
          }"/>
    </p>
  
</uni-image-full-width>


  

  
    






<uni-image-full-width alignment="full" alt-text="a side by side illustration of recent TPUs including details like &#34;peak flops per chip&#34;" external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Ironwood: The first Google TPU for the age of inference" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="41u5e">Figure 2. Side by side comparison of technical specifications of the 3D torus version of Cloud TPU products including the latest generation Ironwood. FP8 peak TFlops emulated for v4 and v5p, but natively supported for Ironwood.</p>
    </div>
  
  
    <p><img alt="a side by side illustration of recent TPUs including details like &#34;peak flops per chip&#34;" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_TPUComparison.width-100.format-webp.webp" loading="lazy" data-loading="{
            &#34;mobile&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_TPUComparison.width-500.format-webp.webp&#34;,
            &#34;desktop&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_TPUComparison.width-1000.format-webp.webp&#34;
          }"/>
    </p>
  
</uni-image-full-width>


  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;Ironwood: The first Google TPU for the age of inference&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><h3 data-block-key="qleld">Ironwood’s key features</h3><p data-block-key="dli6h">Google Cloud is the only hyperscaler with more than a decade of experience in delivering AI compute to support cutting edge research, seamlessly integrated into planetary-scale services for billions of users every day with Gmail, Search and more. All of this expertise is at the heart of Ironwood’s capabilities. Key features include:</p><ul><li data-block-key="8t139"><b>Significant performance gains while also focusing on power efficiency, allowing AI workloads to run more cost-effectively.</b> Ironwood perf/watt is 2x relative to Trillium, our sixth generation TPU <a href="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus?e=48754805">announced last year</a>. At a time when available power is one of the constraints for delivering AI capabilities, we deliver significantly more capacity per watt for customer workloads. Our advanced liquid cooling solutions and optimized chip design can reliably sustain up to twice the performance of standard air cooling even under continuous, heavy AI workloads. In fact, Ironwood is nearly 30x more power efficient than our first Cloud TPU from 2018.</li><li data-block-key="891q5"><b>Substantial increase in High Bandwidth Memory (HBM) capacity.</b> Ironwood offers 192 GB per chip, 6x that of Trillium, which enables processing of larger models and datasets, reducing the need for frequent data transfers and improving performance.</li><li data-block-key="7ra0o"><b>Dramatically improved HBM bandwidth, reaching 7.2 Tbps per chip, 4.5x of Trillium’s.</b> This high bandwidth ensures rapid data access, crucial for memory-intensive workloads common in modern AI.</li><li data-block-key="dii84"><b>Enhanced Inter-Chip Interconnect (ICI) bandwidth.</b> This has been increased to 1.2 Tbps bidirectional, 1.5x of Trillium’s, enabling faster communication between chips, facilitating efficient distributed training and inference at scale.</li></ul></div>
      </div>
    </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="a green bar chart showing the power efficiency improvements of Google TPU" external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Ironwood: The first Google TPU for the age of inference" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="j21ql">Figure 3. Improvement of Google’s TPU power efficiency relative to the earliest generation Cloud TPU v2. Measured by peak FP8 flops delivered per watt of thermal design power per chip package.</p>
    </div>
  
  
    <p><img alt="a green bar chart showing the power efficiency improvements of Google TPU" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_PeakFlopsGraph.width-100.format-webp.webp" loading="lazy" data-loading="{
            &#34;mobile&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_PeakFlopsGraph.width-500.format-webp.webp&#34;,
            &#34;desktop&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/TPUv7_Inline_PeakFlopsGraph.width-1000.format-webp.webp&#34;
          }"/>
    </p>
  
</uni-image-full-width>


  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;Ironwood: The first Google TPU for the age of inference&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><h3 data-block-key="qleld">Ironwood solves the AI demands of tomorrow</h3><p data-block-key="f9npc">Ironwood represents a unique breakthrough in the age of inference with increased computation power, memory capacity, ICI networking advancements and reliability. These breakthroughs, coupled with a nearly 2x improvement in power efficiency, mean that our most demanding customers can take on training and serving workloads with the highest performance and lowest latency, all while meeting the exponential rise in computing demand. Leading thinking models like Gemini 2.5 and the Nobel Prize winning AlphaFold all run on TPUs today, and with Ironwood we can’t wait to see what AI breakthroughs are sparked by our own developers and Google Cloud customers when it becomes available later this year.</p></div>
      </div>
    </div>
  


            
            

            
              




            
          </div>
        
      </div>
    </section>
  </article></div>
  </body>
</html>
