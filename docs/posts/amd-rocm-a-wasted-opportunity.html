<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://threedots.ovh/blog/2022/05/amd-rocm-a-wasted-opportunity/">Original</a>
    <h1>AMD ROCm: A Wasted Opportunity</h1>
    
    <div id="readability-page-1" class="page"><div>
	
	

	

	
	<main id="content">

	<div>
		<div>
						<article id="post-434">
				<div>
<p>This post will mainly focus on ROCm HIP wasted opportunities for adoption. I wrote an overview of ROCm earlier at <a href="https://threedots.ovh/blog/2021/11/what-is-amd-rocm/">https://threedots.ovh/blog/2021/11/what-is-amd-rocm/</a>. This post will specifically eschew talking about issues about the API renames in ROCm HIP – breaking source compatibility with CUDA quite needlessly.</p>



<h2>Hardware support</h2>



<h3>Hardware that you can buy</h3>



<p>Official support for GPUs really matters for adoption, on that point ROCm really doesn’t shine.</p>



<figure><img decoding="async" src="https://media.discordapp.net/attachments/807434218903699478/974349947715616808/unknown.png" alt="Image" width="819" height="328"/><figcaption>List of officially supported GPUs on AMD ROCm (from <a href="https://docs.amd.com/bundle/ROCm-Getting-Started-Guide-v5.1/page/Overview_of_ROCm_Installation.html" target="_blank" rel="noreferrer noopener">documentation</a>)</figcaption></figure>



<p>This official support list essentially ensures that almost no hobbyists have <em>officially</em> access to the platform, which is a bad choice to make.</p>



<h3>Amazon EC2</h3>



<p>In December 2020, AWS launched the <a rel="noreferrer noopener" href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-g4ad-instances-featuring-amd-gpus-for-graphics-workloads/" target="_blank">G4ad</a> instances with Radeon Pro V520 GPUs. This GPU uses the Navi12 die. As of May 2022, AMD still didn’t officially support it as part of ROCm.</p>



<p>This is a big wasted opportunity. Having a supported ROCm configuration easily accessible on Amazon EC2 would have helped development and testing quite a lot. It could have been a nice beginning position for bigger adoption of ROCm across the ecosystem.</p>



<h3>“Unofficial” support</h3>



<p>If a customer GPU happens to share the same die as a supported part (for example, Radeon VII or 6800 XT) then in practice it can work. However, giving no guarantees of that being the case in the documentation is very unusual for a company seeking to have adoption of their products. <em>Enablement</em> is not <em>support</em> either, vendors have to stand behind their products and give support guarantees for them.</p>



<p>And worse, different binary slices are used between different dies of the same product line. As an example, for RDNA2, ROCm math libraries are compiled only for Navi21. This means that on a (smaller) Navi22 die (notably present in the 6700 XT), those components aren’t functional. The workaround is manually recompiling ROCm with support for more targets. Such a roadblock is very discouraging for adopters – and does complicate application distribution too.</p>



<h3>Compatibility between different GPUs</h3>



<p>This behaviour of having a separate target per GPU die is unlike what NVIDIA does – (<em>outside of Tegra, which has its own problems</em> <em>that can force going down the PTX route</em>) – where SASS/hardware ISA are compatible between different products of the same generation. And even more than that, within the same SM major release. This means that binary slices targeted for Volta can run on Turing GPUs for example. </p>



<p>And NVIDIA also has PTX, an intermediate representation that can have slices of bundled into program binaries to guarantee compatibility with future GPUs yet to be designed, without recompilation. ROCm has no equivalent to PTX today.</p>



<p>Meanwhile, Intel uses OpenCL 3.0 and oneAPI Level Zero as the API abstractions provided by the driver. Those use SPIR-V as the intermediate representation, providing compatibility for a given GPU program binary between different hardware generations.</p>



<p>AMD not having an equivalent for PTX or SPIR-V results in binaries having to limit themselves to hipRTC (the NVRTC clone/API rename) runtime compilation mechanism if compatibility with future hardware is needed. This results in the single-source programming model not being supported.</p>



<h2>Support is important</h2>



<p>Official support for all of a product range is really important for the adoption of a proper GPGPU stack across the ecosystem. Having to recompile ROCm manually with support for a specific GPU die is to be truly avoided.</p>



<p>Supporting enterprise products only shrinks the software pool massively – not allowing to benefit from the huge user base of people experimenting on their own machines. Those issues combined result in the lack of adoption of ROCm today. Not supporting the world’s most popular desktop operating system doesn’t help either.</p>



<h2>Is OpenCL a good alternative on AMD?</h2>



<p>AMD’s OpenCL implementation doesn’t support SPIR (was present previously, then removed) or SPIR-V ingestion. This means that the OpenCL implementation is significantly less useful than it could have been.</p>



<p>A complete OpenCL implementation (including SPIR-V ingestion) would allow running identical <em>binaries</em> across both Intel and AMD GPUs, leveraging standardised infrastructure.</p>



<p>So no, it isn’t one…</p>
</div>

			</article>
			
		</div>
	</div>

</main><!--/.neve-main-->



</div></div>
  </body>
</html>
