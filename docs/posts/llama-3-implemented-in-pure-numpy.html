<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://docs.likejazz.com/llama3.np/">Original</a>
    <h1>Llama 3 implemented in pure NumPy</h1>
    
    <div id="readability-page-1" class="page"><div>
    <main>
    
    

    <article>
  
  <time datetime="2024-05-16T00:00:00+00:00">16 May 2024</time>
  <p>
Understand the exact structure with working implementation of the Llama 3 model.
</p>

<p><small>
<em>May 16, 2024</em>
</small></p>

<ul>
  <li><a href="#overview">Overview</a></li>
  <li><a href="#structure">Structure</a>
    <ul>
      <li><a href="#rope-1">RoPE #1</a></li>
      <li><a href="#rmsnorm">RMSNorm</a></li>
      <li><a href="#qkv">QKV</a></li>
      <li><a href="#rope-2">RoPE #2</a></li>
      <li><a href="#kv-cache">KV Cache</a></li>
      <li><a href="#gqagrouped-query-attention">GQA(Grouped-Query Attention)</a></li>
      <li><a href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a></li>
      <li><a href="#feed-forward">Feed Forward</a>
        <ul>
          <li><a href="#swiglu">SwiGLU</a></li>
        </ul>
      </li>
      <li><a href="#linear">Linear</a></li>
    </ul>
  </li>
  <li><a href="#generation">Generation</a></li>
  <li><a href="#example">Example</a></li>
  <li><a href="#github">GitHub</a></li>
  <li><a href="#references">References</a></li>
</ul>




<p><a href="https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6">Llama 3 model</a> unveiled at Meta is creating a buzz.</p>

<p>As expected, the scale and performance is overwhlming. 24K GPUs, 15T training data, 10M instruction data, 1.3M GPU hours, it’s all overwhelming. One interesting fact is that the model structure hasn’t changed. Of course, Llama 3 have changed to using GQA, but this was already implemented in Llama 2 70B, so it’s practically the same model structure.</p>

<p>We’ll let it run for an accurate implementation, and we’ll use only NumPy to make the model structure more intuitive to understand. We use the <a href="https://github.com/karpathy/llama2.c?tab=readme-ov-file#models">stories15M model</a> that Andrej Karpathy trained while creating llama.2, by converting it to a NumPy compressed format using a <a href="https://github.com/hscspring/llama.np/blob/main/convert_bin_llama_to_np.py">converter</a>. We will actually read in the model that Karpathy trained with the Llama 2 structure and implement it as executable code. One thing to note is that the stories15M model does not use GQA, so while we implement GQA in our code but not apply it to model behavior.</p>


<p>Llama 3 model structure is exactly same with the 42dot LLM, so we import the illustration from the <a href="https://42dot.ai/blog/178">42dot Blog</a>, we’ll get the following:</p>

<p><img src="https://docs.likejazz.com/images/2024/c64ef866-4220-4d0b-a532-248288469ce2Frame%2061%20%287%29.png" width="60%"/></p>

<p>The Model has the following parameters:</p>

<div><div><pre><code><span># Model params for ./stories15M.model.npz
</span><span>dim</span><span>:</span> <span>int</span>                    <span>=</span> <span>288</span>       <span># D
</span><span>n_layers</span><span>:</span> <span>int</span>               <span>=</span> <span>6</span>
<span>n_heads</span><span>:</span> <span>int</span>                <span>=</span> <span>6</span>         <span># QHN, HN, HD = 48
</span><span>n_kv_heads</span><span>:</span> <span>Optional</span><span>[</span><span>int</span><span>]</span>   <span>=</span> <span>None</span>      <span># KVHN = 6
</span><span>vocab_size</span><span>:</span> <span>int</span>             <span>=</span> <span>32000</span>     <span># VS
</span><span>max_seq_len</span><span>:</span> <span>int</span>            <span>=</span> <span>256</span>       <span># M
</span><span>max_new_tokens</span><span>:</span> <span>int</span>         <span>=</span> <span>50</span>
</code></pre></div></div>

<p>The designations <code>D</code>, <code>HN</code>, <code>HD</code>, <code>VS</code>, <code>M</code> etc. in the comments are ised to manage the shape of each variable in code. Also note that unlike the 24x in the model illustation, stories15M model has 6 layers, so it iterates 6x.</p>

<h2 id="rope-1">RoPE #1</h2>
<p>The first step is to precompute cos and sin for RoPE embedding. These values are later used by <code>Q</code> and <code>K</code>. This calculation only needs to be done once for every request, so it can be cached. The size is <code>HD(48)//2</code>, which is an exponential multiple of <code>base(10000)</code>, so it can be a larger value, but the maximum value is never more than 1, so it is converted to a scaled value between <code>0 ~ 1</code>, and then again to a value between \(1 \sim \frac{1}{10000}\).</p>

<div><div><pre><code>                        np.arange(0, 48, 2)  # [24,]
  1.0 / (base(10000) ** ([0, 2,          ..., 44,         46] / 48))
= 1.0 / (base(10000) **  [0, 0.04166667, ..., 0.9166667,  0.958333344])
= 1.0 /                  [1, 1.4677993,  ..., 4641.59,    6812.9194]
=                        [1, 0.68129206, ..., 0.00021544, 0.00014678]
</code></pre></div></div>

<p>The result of the calculation is <code>np.outer</code> multiplied by <code>max_seq_len(256)</code>, and then cos and sin are calculated.</p>
<div><div><pre><code><span># [256,] x [24,] = [256, 24]
</span><span>freqs</span> <span>=</span> <span>np</span><span>.</span><span>outer</span><span>([</span><span>0</span> <span>~</span> <span>255</span><span>],</span> <span>[</span><span>1</span><span>,</span> <span>0.68129206</span><span>,</span> <span>...,</span> <span>0.00021544</span><span>,</span> <span>0.00014678</span><span>])</span>
<span>self</span><span>.</span><span>freqs_cos</span><span>:</span> <span>Array</span><span>[</span><span>&#34;M, HD//2&#34;</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>cos</span><span>(</span><span>freqs</span><span>)</span>
<span>self</span><span>.</span><span>freqs_sin</span><span>:</span> <span>Array</span><span>[</span><span>&#34;M, HD//2&#34;</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>sin</span><span>(</span><span>freqs</span><span>)</span>
</code></pre></div></div>

<p>The heatmap of cos and sin looks like this:</p>

<p><img src="https://docs.likejazz.com/images/2024/Screenshot%202024-05-13%20at%209.51.06%E2%80%AFPM.png" width="45%"/>
<img src="https://docs.likejazz.com/images/2024/Screenshot%202024-05-13%20at%209.51.22%E2%80%AFPM.png" width="45%"/></p>

<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChwGA60e6kgAAAABJRU5ErkJggg=="/></p>

<p>The stories15M model is <code>max_seq_len(256)</code>, but I think it could scale up to 8K if we utilize all values up to horizontal axis 24.</p>

<h2 id="rmsnorm">RMSNorm</h2>
<p>RMSNorm normalizes activation values based on the Root Mean Square of the activation values, as opposed to using traditional Mini Batch or Layer statistics. This has the advantage of scaling activation consistently, regardless of Mini Batch size or Layer. Like other normalization techniques, it also has separate training parameters.</p>

<p><img src="https://docs.likejazz.com/images/2024/0_x9v2ncPj0Im5BvuB.webp" width="70%"/><sup id="fnref:fn-swiglue" role="doc-noteref"><a href="#fn:fn-swiglue" rel="footnote">1</a></sup></p>

<p>The formula implementation is as follows:</p>
<div><div><pre><code><span>z</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, 1&#34;</span><span>]</span> <span>=</span> <span>(</span><span>x</span> <span>**</span> <span>2</span><span>).</span><span>mean</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>)</span> <span>+</span> <span>self</span><span>.</span><span>eps</span>
<span>z</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, D&#34;</span><span>]</span> <span>=</span> <span>x</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>z</span><span>)</span>
<span>return</span> <span>z</span> <span>*</span> <span>self</span><span>.</span><span>weight</span>
</code></pre></div></div>

<h2 id="qkv">QKV</h2>
<p>The way to calculate QKV is to matmul one weight in GPT and then split it, but Llama have their own weights for QKV, so we need to matmul them separately. Then, for Multi-Head Attention, we reshape each one to separate them by Multi-Head.</p>

<div><div><pre><code><span># QKV
</span><span>xq</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, D&#34;</span><span>]</span> <span>=</span> <span>x</span> <span>@</span> <span>self</span><span>.</span><span>q_weight</span>
<span>xk</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, D&#34;</span><span>]</span> <span>=</span> <span>x</span> <span>@</span> <span>self</span><span>.</span><span>k_weight</span>
<span>xv</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, D&#34;</span><span>]</span> <span>=</span> <span>x</span> <span>@</span> <span>self</span><span>.</span><span>v_weight</span>

<span># [&#34;B, L or 1, D&#34;] -&gt; [&#34;B, L or 1, QHN or KVHN,  HD&#34;]
</span><span>xq</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, QHN,  HD&#34;</span><span>]</span> <span>=</span> <span>xq</span><span>.</span><span>reshape</span><span>(</span><span>B</span><span>,</span> <span>L</span><span>,</span> <span>self</span><span>.</span><span>n_local_heads</span><span>,</span> <span>self</span><span>.</span><span>head_dim</span><span>)</span>
<span>xk</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, KVHN, HD&#34;</span><span>]</span> <span>=</span> <span>xk</span><span>.</span><span>reshape</span><span>(</span><span>B</span><span>,</span> <span>L</span><span>,</span> <span>self</span><span>.</span><span>n_local_kv_heads</span><span>,</span> <span>self</span><span>.</span><span>head_dim</span><span>)</span>
<span>xv</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, KVHN, HD&#34;</span><span>]</span> <span>=</span> <span>xv</span><span>.</span><span>reshape</span><span>(</span><span>B</span><span>,</span> <span>L</span><span>,</span> <span>self</span><span>.</span><span>n_local_kv_heads</span><span>,</span> <span>self</span><span>.</span><span>head_dim</span><span>)</span>
</code></pre></div></div>

<h2 id="rope-2">RoPE #2</h2>
<p>Now it’s time to actually apply the RoPE using the values we calculated earlier.</p>

<p><img src="https://docs.likejazz.com/images/2024/0_YycOQR_AW4FhwSJM.webp" width="80%"/><sup id="fnref:fn-rotary" role="doc-noteref"><a href="#fn:fn-rotary" rel="footnote">2</a></sup></p>

<p>RoPE is a new type of position encoding technique that has the characteristics of both absolute and relative, and performs well because it has the characteristics of both. It only applies to Q and K, dividing each input by the sum of its parts, then multiplying by cos and sin, adding and subtracting the results, and returning the sum back to reshape.</p>

<div><div><pre><code><span>xq_out_r</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, QHN,  HD//2&#34;</span><span>]</span> <span>=</span> <span>xq_r</span> <span>*</span> <span>freqs_cos</span> <span>-</span> <span>xq_i</span> <span>*</span> <span>freqs_sin</span>
<span>xq_out_i</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, QHN,  HD//2&#34;</span><span>]</span> <span>=</span> <span>xq_r</span> <span>*</span> <span>freqs_sin</span> <span>+</span> <span>xq_i</span> <span>*</span> <span>freqs_cos</span>
<span>xk_out_r</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, KVHN, HD//2&#34;</span><span>]</span> <span>=</span> <span>xk_r</span> <span>*</span> <span>freqs_cos</span> <span>-</span> <span>xk_i</span> <span>*</span> <span>freqs_sin</span>
<span>xk_out_i</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, KVHN, HD//2&#34;</span><span>]</span> <span>=</span> <span>xk_r</span> <span>*</span> <span>freqs_sin</span> <span>+</span> <span>xk_i</span> <span>*</span> <span>freqs_cos</span>

<span>xq_out</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, QHN,  HD//2, 2&#34;</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>stack</span><span>([</span><span>xq_out_r</span><span>,</span> <span>xq_out_i</span><span>],</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>
<span>xk_out</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, KVHN, HD//2, 2&#34;</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>stack</span><span>([</span><span>xk_out_r</span><span>,</span> <span>xk_out_i</span><span>],</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>
<span>xq_out</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, QHN,  HD&#34;</span><span>]</span> <span>=</span> <span>xq_out</span><span>.</span><span>reshape</span><span>(</span><span>xq_out</span><span>.</span><span>shape</span><span>[:</span><span>-</span><span>2</span><span>]</span> <span>+</span> <span>(</span><span>-</span><span>1</span><span>,))</span>
<span>xk_out</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, KVHN, HD&#34;</span><span>]</span> <span>=</span> <span>xk_out</span><span>.</span><span>reshape</span><span>(</span><span>xk_out</span><span>.</span><span>shape</span><span>[:</span><span>-</span><span>2</span><span>]</span> <span>+</span> <span>(</span><span>-</span><span>1</span><span>,))</span>
</code></pre></div></div>

<p>RoPE are applied after the Q and K have been multiplied by the weights in the attention mechanism, while in the vanilla transformer they’re applied before.</p>

<h2 id="kv-cache">KV Cache</h2>
<p><img src="https://docs.likejazz.com/images/2024/key-value-caching_.png" width="70%"/><sup id="fnref:fn-kvcache" role="doc-noteref"><a href="#fn:fn-kvcache" rel="footnote">3</a></sup></p>

<p>Since the GPT-style generative model is Masked Attention, it is possible to KV Cache. Since the previous result will always be the same, regardless of what comes after it, since we are not allowed to see the next word, we can cache K and V, and Q only needs to compute the last value. The cache is held by <code>max_seq_len(256)</code>, so the result of the calculation is put in and then extracted back to the only current length.</p>

<div><div><pre><code><span># KV Cache
</span><span>self</span><span>.</span><span>cache_k</span><span>[:</span><span>B</span><span>,</span> <span>start_pos</span><span>:</span> <span>start_pos</span> <span>+</span> <span>L</span><span>]</span> <span>=</span> <span>xk</span>
<span>self</span><span>.</span><span>cache_v</span><span>[:</span><span>B</span><span>,</span> <span>start_pos</span><span>:</span> <span>start_pos</span> <span>+</span> <span>L</span><span>]</span> <span>=</span> <span>xv</span>
<span>ks</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L, KVHN, HD&#34;</span><span>]</span> <span>=</span> <span>self</span><span>.</span><span>cache_k</span><span>[:</span><span>B</span><span>,</span> <span>:</span> <span>start_pos</span> <span>+</span> <span>L</span><span>]</span>
<span>vs</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L, KVHN, HD&#34;</span><span>]</span> <span>=</span> <span>self</span><span>.</span><span>cache_v</span><span>[:</span><span>B</span><span>,</span> <span>:</span> <span>start_pos</span> <span>+</span> <span>L</span><span>]</span>
<span># (1, 256, 6, 48) -&gt; (1, 5, 6, 48)
</span>
<span># GQA
</span><span>xk</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L, HN, HD&#34;</span><span>]</span> <span>=</span> <span>repeat_kv</span><span>(</span><span>ks</span><span>,</span> <span>self</span><span>.</span><span>n_rep</span><span>)</span>
<span>xv</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L, HN, HD&#34;</span><span>]</span> <span>=</span> <span>repeat_kv</span><span>(</span><span>vs</span><span>,</span> <span>self</span><span>.</span><span>n_rep</span><span>)</span>

<span>xq</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, HN, L or 1, HD&#34;</span><span>]</span> <span>=</span> <span>xq</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>)</span>
<span>xk</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, HN, L, HD&#34;</span><span>]</span> <span>=</span> <span>xk</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>)</span>
<span>xv</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, HN, L, HD&#34;</span><span>]</span> <span>=</span> <span>xv</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>)</span>
</code></pre></div></div>

<p>Here, we fetch the cache values and then transpose them back to reshape them, but this could be done more efficiently by skipping this step. For reference, the maximum size of the KV Cache is \(1 \times 256 \times 6 \times 48 \times 2 \times 6 = 884K\) on batch size 1. Since it is a 15M model, it takes up about 6% more memory.</p>

<h2 id="gqagrouped-query-attention">GQA(Grouped-Query Attention)</h2>
<p><img src="https://docs.likejazz.com/images/2024/Screenshot%202024-05-13%20at%204.46.38%E2%80%AFPM.png" width="70%"/><sup id="fnref:fn-gqa" role="doc-noteref"><a href="#fn:fn-gqa" rel="footnote">4</a></sup></p>

<p>MQA, which is a Multi-query, has the advantage of being compact and memory-saving compared to MHA, which is a Multi-head, but it suffers from poor performance and unstable learning. Therefore, Grouped-query, GQA, was introduced in Llama 2. In Llama 2, GQA was only applied to 70B, but from Llama 3, GQA was applied to all models above 8B. Since we are using a model that was trained without GQA, we do not use GQA, but we have implemented it in the code. We have implemented it by simply copying it by a multiple, and it can be improved by referencing the previous value for future optimization. We have avoided using GQA when <code>n_rep==1</code>.</p>

<div><div><pre><code><span>if</span> <span>n_rep</span> <span>==</span> <span>1</span><span>:</span>
    <span>return</span> <span>x</span>
<span>z</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L, QHN, HD&#34;</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>repeat</span><span>(</span><span>x</span><span>,</span> <span>n_rep</span><span>,</span> <span>axis</span><span>=</span><span>2</span><span>)</span>
</code></pre></div></div>

<h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h2>
<p>Attentions are calculated separately by Multi-Head.</p>

<p>\(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)</p>
<div><div><pre><code><span># Scaled Dot-Product Attention
# [&#34;B, HN, L or 1, HD&#34;] @ [&#34;B, HN, HD, L&#34;] -&gt; [&#34;B, HN, L or 1, L&#34;]
</span><span>attention</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, HN, L or 1, L&#34;</span><span>]</span> <span>=</span> <span>xq</span> <span>@</span> <span>xk</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>)</span> <span>/</span> <span>math</span><span>.</span><span>sqrt</span><span>(</span><span>self</span><span>.</span><span>head_dim</span><span>)</span>
<span># `mask` is used only once at the beginning.
</span><span>if</span> <span>mask</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
    <span>attention</span> <span>=</span> <span>attention</span> <span>+</span> <span>mask</span><span>[</span><span>None</span><span>,</span> <span>None</span><span>,</span> <span>:,</span> <span>:]</span>
<span>attention</span> <span>=</span> <span>softmax</span><span>(</span><span>attention</span><span>)</span>
<span>output</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, HN, L or 1, HD&#34;</span><span>]</span> <span>=</span> <span>attention</span> <span>@</span> <span>xv</span>
</code></pre></div></div>
<p>Masking is only done at the beginning and only the last Q needs to be processed afterward, so no masking is needed. The result can then be obtained with softmax and matmul. Finally, the result of the Multi-Head calculation is reshaped to full dimension to combine the heads and matmul once more.</p>

<div><div><pre><code><span># [&#34;B, HN, L or 1, HD&#34;] -&gt; [&#34;B, L or 1, D&#34;]
</span><span>output</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, D&#34;</span><span>]</span> <span>=</span> <span>output</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>,</span> <span>3</span><span>).</span><span>reshape</span><span>(</span><span>B</span><span>,</span> <span>L</span><span>,</span> <span>-</span><span>1</span><span>)</span>
<span>output</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, D&#34;</span><span>]</span> <span>=</span> <span>output</span> <span>@</span> <span>self</span><span>.</span><span>o_weight</span>
</code></pre></div></div>

<p>Computing the entire QKV at once is only done in the Prefill Phase. At this time, TTFT (Time To First Token) is called Prefill Latency, and only ‘vector @ matrix’ operations need to be performed from the Decode Phase onward. Flash Attention is also effective only when reducing the Prefill Latency during inference, and it performs somewhat well when the input is long.</p>

<h2 id="feed-forward">Feed Forward</h2>
<p>In Llama model, Feed Forward uses 3 linear with matmul only and no bias, so unlike GPT, it is not a complete fully-connected layer. We create a swish value from the silu result, multiply it with <code>x_V</code> up-scaled from <code>D</code> to <code>FD</code>, and down-scale it again. Here, the size of <code>FD</code> is <code>FD = 2 * 4 * D / 3</code>, which is <code>D(288)</code>, so <code>FD(768)</code>.</p>

<div><div><pre><code><span>swish</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, FD&#34;</span><span>]</span> <span>=</span> <span>silu</span><span>(</span><span>x</span> <span>@</span> <span>self</span><span>.</span><span>gate_weight</span><span>)</span>
<span>x_V</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, FD&#34;</span><span>]</span> <span>=</span> <span>x</span> <span>@</span> <span>self</span><span>.</span><span>up_weight</span>
<span>x</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, FD&#34;</span><span>]</span> <span>=</span> <span>swish</span> <span>*</span> <span>x_V</span>
<span>x</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, L or 1, D&#34;</span><span>]</span> <span>=</span> <span>x</span> <span>@</span> <span>self</span><span>.</span><span>down_weight</span>
</code></pre></div></div>

<h3 id="swiglu">SwiGLU</h3>
<p>In the paper, the SwiGLU formula looks like this:</p>

<p><img src="https://docs.likejazz.com/images/2024/1_J9HF2sX3VWQ0wWkKIP-CqQ.webp" width="50%"/><sup id="fnref:fn-swiglue:1" role="doc-noteref"><a href="#fn:fn-swiglue" rel="footnote">1</a></sup></p>

<p>Multiplying <code>x_V</code> with swish and matmul it with <code>W_2</code> is called SwiGLU. This unique combination of multiple feed forwards layers increases the performance of the model. Let <code>x</code> be a real number between approximately \(-14 \sim 11\), which is the input to the silu function. The silu implementation is as follows:</p>

<div><div><pre><code><span>x</span> <span>*</span> <span>(</span><span>1</span> <span>/</span> <span>(</span><span>1</span> <span>+</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>x</span><span>)))</span>
</code></pre></div></div>

<h2 id="linear">Linear</h2>
<p>After passing through all the transformer blocks, the final output is only the last logit computed by matmul to speed things up. The transformer block always outputs <code>[&#34;1, D&#34;]</code> as the result after the Prefill Phase.</p>

<div><div><pre><code><span># [&#34;B, 1, VS&#34;] = [&#34;B, 1(L), D&#34;] @ [&#34;D, VS&#34;]
</span><span>logit</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, 1, VS&#34;</span><span>]</span> <span>=</span> <span>h</span><span>[:,</span> <span>[</span><span>-</span><span>1</span><span>],</span> <span>:]</span> <span>@</span> <span>self</span><span>.</span><span>lm_head_weight</span>
</code></pre></div></div>


<p>Now, we generate tokens one after the other using the extracted logit. For simplicity, we’ve omitted sampling from the generation process and only output the Greedy result.</p>

<div><div><pre><code><span>for</span> <span>i</span><span>,</span> <span>curr_pos</span> <span>in</span> <span>enumerate</span><span>(</span><span>range</span><span>(</span><span>L</span><span>,</span> <span>max_new_tokens</span><span>)):</span>
    <span>if</span> <span>i</span> <span>==</span> <span>0</span><span>:</span>  <span># Prefill Phase
</span>        <span>inputs</span> <span>=</span> <span>input_ids</span>
        <span>pos</span> <span>=</span> <span>0</span>
    <span>else</span><span>:</span>       <span># Decode Phase
</span>        <span>inputs</span> <span>=</span> <span>next_id</span>
        <span>pos</span> <span>=</span> <span>curr_pos</span>
    <span>logits</span><span>:</span> <span>Array</span><span>[</span><span>&#34;B, 1, VS&#34;</span><span>]</span> <span>=</span> <span>self</span><span>(</span><span>inputs</span><span>,</span> <span>pos</span><span>)</span>
    <span>next_id</span> <span>=</span> <span>logits</span><span>[:,</span> <span>-</span><span>1</span><span>,</span> <span>:].</span><span>argmax</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>)</span>
    <span>yield</span> <span>next_id</span>
</code></pre></div></div>

<p>The first step is Prefill Phase, or sometimes called Summarization. It passes all input and starts at position 0. This is also where Flash Attention comes into play.</p>

<p>From then on, it is the Decode Phase and thanks to the KV Cache, only the last token ID is passed to Q and the result is the also last logit. Here, we omit sampling and only extract the maximum value. If you want to add a sampling process, you can take softmax and implement top_p and top_k.</p>

<p>You can now yield the token ID we generated as a result, decode it in the next step and print the output token to finalize the process.</p>


<p>You can run it like this:</p>
<div><div><pre><code><span>$ </span>python llama3.py <span>&#34;I have a dream&#34;</span>
<span>&#34;&#34;&#34;
I have a dream. He dream of a big, beautiful garden full of flower and tree. He dream of playing with hi friend and eating yummy snack.
One day, he wa walking in the garden when he saw

Token count: 50, elapsed: 1.53s, 33 tokens/s
&#34;&#34;&#34;</span>
</code></pre></div></div>

<p>Karpathy has trained the model to a certain extent, and the result is that model is not performing badly. It ran at 33 tokens/s on an M2 MacBook Air.</p>


<p>The full source code is available at <a href="https://github.com/likejazz/llama3.np">likejazz/llama3.np</a>.</p>




</article>


    

    
    </main>
  </div></div>
  </body>
</html>
