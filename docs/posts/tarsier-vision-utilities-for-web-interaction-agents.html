<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/reworkd/tarsier">Original</a>
    <h1>Show HN: Tarsier ‚Äì Vision utilities for web interaction agents</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/reworkd/Tarsier/main/.github/assets/tarsier.png"><img src="https://raw.githubusercontent.com/reworkd/Tarsier/main/.github/assets/tarsier.png" height="300" alt="Tarsier Monkey"/></a>
</p>
<p dir="auto">
  <em>üôà Vision utilities for web interaction agents üôà</em>
</p>
<p dir="auto">
    <a href="https://pypi.org/project/tarsier/" rel="nofollow">
        <img alt="Python" src="https://camo.githubusercontent.com/0562f16a4ae7e35dae6087bf8b7805fb7e664a9e7e20ae6d163d94e56b94f32d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d3336373041303f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d666664643534" data-canonical-src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&amp;logo=python&amp;logoColor=ffdd54"/>
        <img alt="Version" src="https://camo.githubusercontent.com/e3c0bb6224b756f0057970306b1daf2b9d4f063ce276f67a892cbbdac4c82ccb/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f746172736965723f7374796c653d666f722d7468652d626164676526636f6c6f723d333637304130" data-canonical-src="https://img.shields.io/pypi/v/tarsier?style=for-the-badge&amp;color=3670A0"/>
    </a>
</p>
<p dir="auto">
<a href="https://reworkd.ai/" rel="nofollow">üîó Main site</a>
<span>¬†¬†‚Ä¢¬†¬†</span>
<a href="https://twitter.com/khoomeik/status/1723432848739483976" rel="nofollow">üê¶ Twitter</a>
<span>¬†¬†‚Ä¢¬†¬†</span>
<a href="https://discord.gg/gcmNyAAFfV" rel="nofollow">üì¢ Discord</a>
</p>

<p dir="auto">If you&#39;ve tried using an LLM to automate web interactions, you&#39;ve probably run into questions like:</p>
<ul dir="auto">
<li>How should you feed the webpage to an LLM? (e.g. HTML, Accessibility Tree, Screenshot)</li>
<li>How do you map LLM responses back to web elements?</li>
<li>How can you inform a text-only LLM about the page&#39;s visual structure?</li>
</ul>
<p dir="auto">At Reworkd, we iterated on all these problems across tens of thousands of real web tasks to build a powerful perception system for web agents... Tarsier!
In the video below, we use Tarsier to provide webpage perception for a minimalistic GPT-4 LangChain web agent.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description tarsier.mp4">tarsier.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/50181239/282260008-af12beda-89b5-4add-b888-d780b353304b.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTU4MDc5NTcsIm5iZiI6MTcxNTgwNzY1NywicGF0aCI6Ii81MDE4MTIzOS8yODIyNjAwMDgtYWYxMmJlZGEtODliNS00YWRkLWI4ODgtZDc4MGIzNTMzMDRiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTE1VDIxMTQxN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWM2MjRiZTliZmQ1NTY1MzQyMjUzNDdjMmZlMWM1YzIwMzU0Mjg4ZDlhNzNhOTQ4MWE4Mjc1YThkYjRiZGQ1MzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.MuZbL4GT5401VBaezI_l-o3Z98iOUMyiMPVsbyi8txU" data-canonical-src="https://private-user-images.githubusercontent.com/50181239/282260008-af12beda-89b5-4add-b888-d780b353304b.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTU4MDc5NTcsIm5iZiI6MTcxNTgwNzY1NywicGF0aCI6Ii81MDE4MTIzOS8yODIyNjAwMDgtYWYxMmJlZGEtODliNS00YWRkLWI4ODgtZDc4MGIzNTMzMDRiLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTE1VDIxMTQxN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWM2MjRiZTliZmQ1NTY1MzQyMjUzNDdjMmZlMWM1YzIwMzU0Mjg4ZDlhNzNhOTQ4MWE4Mjc1YThkYjRiZGQ1MzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.MuZbL4GT5401VBaezI_l-o3Z98iOUMyiMPVsbyi8txU" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto">Tarsier visually tags interactable elements on a page via brackets + an ID e.g. <code>[23]</code>.
In doing this, we provide a mapping between elements and IDs for an LLM to take actions upon (e.g. <code>CLICK [23]</code>).
We define interactable elements as buttons, links, or input fields that are visible on the page; Tarsier can also tag all textual elements if you pass <code>tag_text_elements=True</code>.</p>
<p dir="auto">Furthermore, we&#39;ve developed an OCR algorithm to convert a page screenshot into a whitespace-structured string (almost like ASCII art) that an LLM <em>even without vision</em> can understand.
Since current vision-language models still lack fine-grained representations needed for web interaction tasks, this is critical.
On our internal benchmarks, unimodal GPT-4 + Tarsier-Text beats GPT-4V + Tarsier-Screenshot by 10-20%!</p>
<table>
<thead>
<tr>
<th>Tagged Screenshot</th>
<th>Tagged Text Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/reworkd/tarsier/blob/main/.github/assets/tagged.png"><img src="https://github.com/reworkd/tarsier/raw/main/.github/assets/tagged.png" alt="tagged"/></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/reworkd/tarsier/blob/main/.github/assets/tagged_text.png"><img src="https://github.com/reworkd/tarsier/raw/main/.github/assets/tagged_text.png" alt="tagged"/></a></td>
</tr>
</tbody>
</table>



<p dir="auto">Visit our <a href="https://github.com/reworkd/Tarsier/tree/main/cookbook">cookbook</a> for agent examples using Tarsier:</p>
<ul dir="auto">
<li><a href="https://github.com/reworkd/tarsier/blob/main/cookbook/langchain-web-agent.ipynb">An autonomous LangChain web agent</a> ü¶ú‚õìÔ∏è</li>
<li><a href="https://github.com/reworkd/tarsier/blob/main/cookbook/llama-index-web-agent.ipynb">An autonomous LlamaIndex web agent</a> ü¶ô</li>
</ul>
<p dir="auto">Otherwise, basic Tarsier usage might look like the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio

from playwright.async_api import async_playwright
from tarsier import Tarsier, GoogleVisionOCRService
import json

def load_google_cloud_credentials(json_file_path):
    with open(json_file_path) as f:
        credentials = json.load(f)
    return credentials

async def main():
    # To create the service account key, follow the instructions on this SO answer https://stackoverflow.com/a/46290808/1780891
    google_cloud_credentials = load_google_cloud_credentials(&#39;./google_service_acc_key.json&#39;)

    ocr_service = GoogleVisionOCRService(google_cloud_credentials)
    tarsier = Tarsier(ocr_service)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()
        await page.goto(&#34;https://news.ycombinator.com&#34;)

        page_text, tag_to_xpath = await tarsier.page_to_text(page)

        print(tag_to_xpath)  # Mapping of tags to x_paths
        print(page_text)  # My Text representation of the page


if __name__ == &#39;__main__&#39;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>

<span>from</span> <span>playwright</span>.<span>async_api</span> <span>import</span> <span>async_playwright</span>
<span>from</span> <span>tarsier</span> <span>import</span> <span>Tarsier</span>, <span>GoogleVisionOCRService</span>
<span>import</span> <span>json</span>

<span>def</span> <span>load_google_cloud_credentials</span>(<span>json_file_path</span>):
    <span>with</span> <span>open</span>(<span>json_file_path</span>) <span>as</span> <span>f</span>:
        <span>credentials</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
    <span>return</span> <span>credentials</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span># To create the service account key, follow the instructions on this SO answer https://stackoverflow.com/a/46290808/1780891</span>
    <span>google_cloud_credentials</span> <span>=</span> <span>load_google_cloud_credentials</span>(<span>&#39;./google_service_acc_key.json&#39;</span>)

    <span>ocr_service</span> <span>=</span> <span>GoogleVisionOCRService</span>(<span>google_cloud_credentials</span>)
    <span>tarsier</span> <span>=</span> <span>Tarsier</span>(<span>ocr_service</span>)

    <span>async</span> <span>with</span> <span>async_playwright</span>() <span>as</span> <span>p</span>:
        <span>browser</span> <span>=</span> <span>await</span> <span>p</span>.<span>chromium</span>.<span>launch</span>(<span>headless</span><span>=</span><span>False</span>)
        <span>page</span> <span>=</span> <span>await</span> <span>browser</span>.<span>new_page</span>()
        <span>await</span> <span>page</span>.<span>goto</span>(<span>&#34;https://news.ycombinator.com&#34;</span>)

        <span>page_text</span>, <span>tag_to_xpath</span> <span>=</span> <span>await</span> <span>tarsier</span>.<span>page_to_text</span>(<span>page</span>)

        <span>print</span>(<span>tag_to_xpath</span>)  <span># Mapping of tags to x_paths</span>
        <span>print</span>(<span>page_text</span>)  <span># My Text representation of the page</span>


<span>if</span> <span>__name__</span> <span>==</span> <span>&#39;__main__&#39;</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto">Keep in mind that Tarsier tags different types of elements differently to help your LLM identify what actions are performable on each element. Specifically:</p>
<ul dir="auto">
<li><code>[#ID]</code>: text-insertable fields (e.g. <code>textarea</code>, <code>input</code> with textual type)</li>
<li><code>[@ID]</code>: hyperlinks (<code>&lt;a&gt;</code> tags)</li>
<li><code>[$ID]</code>: other interactable elements (e.g. <code>button</code>, <code>select</code>)</li>
<li><code>[ID]</code>: plain text (if you pass <code>tag_text_elements=True</code>)</li>
</ul>


<p dir="auto">We have provided a handy setup script to get you up and running with Tarsier development.</p>

<p dir="auto">If you modify any TypeScript files used by Tarsier, you&#39;ll need to execute the following command.
This compiles the TypeScript into JavaScript, which can then be utilized in the Python package.</p>


<p dir="auto">We use <a href="https://docs.pytest.org" rel="nofollow">pytest</a> for testing. To run the tests, simply run:</p>


<p dir="auto">Prior to submitting a potential PR, please run the following to format your code:</p>


<ul>
<li> <a href="https://cloud.google.com/vision" rel="nofollow">Google Cloud Vision</a></li>
<li> <a href="https://aws.amazon.com/textract/" rel="nofollow">Amazon Textract</a> (Coming Soon)</li>
<li> <a href="https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/" rel="nofollow">Microsoft Azure Computer Vision</a> (Coming Soon)</li>
</ul>

<ul>
<li> Add documentation and examples</li>
<li> Clean up interfaces and add unit tests</li>
<li> Launch</li>
<li> Improve OCR text performance</li>
<li> Add options to customize tagging styling</li>
<li> Add support for other browsers drivers as necessary</li>
</ul>

<div data-snippet-clipboard-copy-content="bibtex
@misc{reworkd2023tarsier,
  title        = {Tarsier},
  author       = {Rohan Pandey and Adam Watkins and Asim Shrestha and Srijan Subedi},
  year         = {2023},
  howpublished = {GitHub},
  url          = {https://github.com/reworkd/tarsier}
}"><pre><code>bibtex
@misc{reworkd2023tarsier,
  title        = {Tarsier},
  author       = {Rohan Pandey and Adam Watkins and Asim Shrestha and Srijan Subedi},
  year         = {2023},
  howpublished = {GitHub},
  url          = {https://github.com/reworkd/tarsier}
}
</code></pre></div>
</article></div></div>
  </body>
</html>
