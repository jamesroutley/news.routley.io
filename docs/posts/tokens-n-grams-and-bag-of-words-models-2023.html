<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://zilliz.com/learn/introduction-to-natural-language-processing-tokens-ngrams-bag-of-words-models">Original</a>
    <h1>Tokens, n-grams, and bag-of-words models (2023)</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><a href="https://zilliz.com/learn/ChatGPT-Vector-Database-Prompt-as-code">ChatGPT</a> (GPT-3.5) and other <a href="https://zilliz.com/glossary/large-language-models-(llms)">LLMs</a>, such as Pi, Claude, Bard, etc, have taken the world by storm. How exactly do these language models work, and why do they work so well for the tasks they&#39;re trained on? While nobody knows the complete answer, having a proper understanding of Natural Language Processing fundamentals can help demystify their inner workings. In particular, knowledge of <em>tokens</em> and <em>n-grams</em> is invaluable for understanding nearly all modern autoregressive and autoencoding models (and are broadly applicable to today&#39;s LLMs). Let&#39;s dive right in.</p>
<h2 id="Tokens-and-N-grams">Tokens and N-grams</h2><p>Introductory computer science courses in C or C++ often teach the concept of strings early on. A string in C, for example, can be represented as an array of characters terminated by the null character:</p>
<pre><code><span>char</span> my_str[<span>128</span>] = <span>&#34;Milvus&#34;</span>;
</code></pre>
<p>In this example, each character can be thought of as a discrete unit, and the combination of each of these put together results in text that has meaning - in this case, <code>my_str</code> corresponds to the world&#39;s most widely adopted <a href="https://zilliz.com/learn/what-is-vector-database">vector database</a>. Put simply, this is the definition of an <em>n-gram</em>: a series of characters (or some other discrete unit which we&#39;ll discuss in the next paragraph), which, when strung together, have coherent meaning. <code>N</code> in this instance would correspond to the total number of characters in the string (<code>7</code>, in this case).</p>
<p>The concept of n-grams doesn&#39;t have to be single characters - they can be extended to words as well. The string below, for example, is a trigram (3-gram) of words:</p>
<pre><code><span>char</span> my_str[<span>128</span>] = <span>&#34;Milvus vector database&#34;</span>
</code></pre>
<p>In the example above, it&#39;s fairly clear that <code>my_str</code> is composed of three words, but things get a bit tricky once we take punctuation into consideration:</p>
<pre><code><span>char</span> my_str[<span>128</span>] = <span>&#34;Milvus&#39;s architecture is unparalleled&#34;</span>
</code></pre>
<p>The above string, strictly speaking, is four words, but the first word <code>Milvus&#39;s</code> is a possessive noun which uses another word <code>Milvus</code> as the base. For a language model, it can be helpful to split words such as these into discrete units so that the extra context is preserved: <code>Milvus</code> and <code>&#39;s</code>. These are called <em>tokens</em>, and the underlying method for splitting sentences into words is called <em>tokenization</em>. With this strategy, the string above is now a 5-gram of tokens.</p>
<p>All modern language models perform some sort of input tokenization before the data is transformed. There are a number of different tokenizers out there - <a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">WordPiece</a>, for example, is a popular one which is used in most flavors of BERT. We won&#39;t dive too deep into tokenizers in this series - for folks who want to learn a bit more, check out <a href="https://huggingface.co/docs/transformers/main/tokenizer_summary">Huggingface&#39;s tokenizer summary</a>.</p>
<p>Now that we have a better understanding of n-grams, we can turn our attention towards <em>n-gram models</em>. In simple terms, an n-gram model is a simple, probabilistic language model that outputs the probability that a particular token appears after an existing string of tokens. For example, we can model the probability (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span>) that a particular token follows another token (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∣</span></span></span></span>) in a sentence or phrase:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">p(database | vector) = 0.1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span>d</span><span>a</span><span>t</span><span>aba</span><span>se</span><span>∣</span><span>v</span><span>ec</span><span>t</span><span>or</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>0.1</span></span></span></span></span></p>
<p>The above statement says that, for this particular language model, the word &#34;vector&#34; follows the word &#34;database&#34; with 10% probability. For n-gram models, these models are always computed by looking at the count of bigrams in the input corpus of documents, but in other language models, they can be set manually or taken from the output of a machine learning model.</p>
<p>The example above is a bigram model, but we can extend this to sequences of arbitrary length. Here&#39;s an example of a trigram:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi mathvariant="normal">∣</mi><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mo separator="true">,</mo><mi>v</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">p(database | Milvus, vector) = 0.9</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span>d</span><span>a</span><span>t</span><span>aba</span><span>se</span><span>∣</span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>,</span><span></span><span>v</span><span>ec</span><span>t</span><span>or</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>0.9</span></span></span></span></span></p>
<p>This encodes the fact that the word &#34;database&#34; will follow the tokens &#34;Milvus vector&#34; with 90% probability. Likewise, we can write:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>c</mi><mi>h</mi><mi>o</mi><mi>c</mi><mi>o</mi><mi>l</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi mathvariant="normal">∣</mi><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mo separator="true">,</mo><mi>v</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">p(chocolate | Milvus, vector) = 0.001</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span>c</span><span>h</span><span>oco</span><span>l</span><span>a</span><span>t</span><span>e</span><span>∣</span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>,</span><span></span><span>v</span><span>ec</span><span>t</span><span>or</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>0.001</span></span></span></span></span></p>
<p>This encodes the fact that the subsequent word after &#34;Milvus vector&#34; is unlikely to be &#34;chocolate&#34; (0.1% probability, to be exact). Applying this to a sequence of longer length, we have:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mi mathvariant="normal">∣</mi><mi>t</mi><mi>h</mi><mi>e</mi><mo separator="true">,</mo><mi>m</mi><mi>o</mi><mi>s</mi><mi>t</mi><mo separator="true">,</mo><mi>w</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>y</mi><mo separator="true">,</mo><mi>a</mi><mi>d</mi><mi>o</mi><mi>p</mi><mi>t</mi><mi>e</mi><mi>d</mi><mo separator="true">,</mo><mi>v</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mo separator="true">,</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mo separator="true">,</mo><mi>i</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.999</mn></mrow><annotation encoding="application/x-tex">p(Milvus | the, most, widely, adopted, vector, database, is) = 0.999</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>∣</span><span>t</span><span>h</span><span>e</span><span>,</span><span></span><span>m</span><span>os</span><span>t</span><span>,</span><span></span><span>w</span><span>i</span><span>d</span><span>e</span><span>l</span><span>y</span><span>,</span><span></span><span>a</span><span>d</span><span>o</span><span>pt</span><span>e</span><span>d</span><span>,</span><span></span><span>v</span><span>ec</span><span>t</span><span>or</span><span>,</span><span></span><span>d</span><span>a</span><span>t</span><span>aba</span><span>se</span><span>,</span><span></span><span>i</span><span>s</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>0.999</span></span></span></span></span></p>
<p>Now that we have a solid understanding of probabilistic n-gram models, let&#39;s turn our attention to an arguably more important question: how do we compute these probabilities? The short and simple answer is: we count the number of occurrences in our document or corpus of documents. Let&#39;s walk through an example with the following three phrases (the <code>&lt;S&gt;</code> at the start of each sentence denotes a special start-of-sentence token). For clarity, I&#39;ve also added an extra space between the ending period and its preceding token in each sentence.</p>
<ul>
<li><code>&lt;S&gt; Milvus is the most widely adopted vector database .</code></li>
<li><code>&lt;S&gt; vector search with Milvus .</code></li>
<li><code>&lt;S&gt; Milvus rocks .</code></li>
</ul>
<p>Let&#39;s list out bigrams that begin with either <code>&lt;S&gt;</code>, <code>Milvus</code>, or <code>vector</code>:</p>
<pre><code>some_bigrams = {
    
    (<span>&#34;&lt;S&gt;&#34;</span>, <span>&#34;Milvus&#34;</span>): <span>2</span>,
    (<span>&#34;&lt;S&gt;&#34;</span>, <span>&#34;vector&#34;</span>): <span>1</span>,
    
    (<span>&#34;Milvus&#34;</span>, <span>&#34;is&#34;</span>): <span>1</span>,
    (<span>&#34;Milvus&#34;</span>, <span>&#34;.&#34;</span>): <span>1</span>,
    (<span>&#34;Milvus&#34;</span>, <span>&#34;rocks&#34;</span>): <span>1</span>,
    
    (<span>&#34;vector&#34;</span>, <span>&#34;database&#34;</span>): <span>1</span>,
    (<span>&#34;vector&#34;</span>, <span>&#34;search&#34;</span>): <span>1</span>
}
</code></pre>
<p>Given these occurrences, we can compute probabilities by normalizing across the total number of times each token appears. For example:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mi mathvariant="normal">∣</mi><mo>&lt;</mo><mi>S</mi><mo>&gt;</mo><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mo stretchy="false">(</mo><mo>&lt;</mo><mi>S</mi><mo>&gt;</mo><mo separator="true">,</mo><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>c</mi><mo stretchy="false">(</mo><mo>&lt;</mo><mi>S</mi><mo>&gt;</mo><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">p(Milvus | &lt;S&gt;) = \frac{c(&lt;S&gt;, Milvus)}{c(&lt;S&gt;)} = \frac{2}{3}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>∣</span><span></span><span>&lt;</span><span></span></span><span><span></span><span>S</span><span></span><span>&gt;</span></span><span><span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>c</span><span>(</span><span>&lt;</span><span></span><span>S</span><span></span><span>&gt;</span><span>)</span></span></span><span><span></span><span></span></span><span><span></span><span><span>c</span><span>(</span><span>&lt;</span><span></span><span>S</span><span></span><span>&gt;</span><span>,</span><span></span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>3</span></span></span><span><span></span><span></span></span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></p>
<p>Similarly:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>r</mi><mi>o</mi><mi>c</mi><mi>k</mi><mi>s</mi><mi mathvariant="normal">∣</mi><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mo stretchy="false">(</mo><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mo separator="true">,</mo><mi>r</mi><mi>o</mi><mi>c</mi><mi>k</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>c</mi><mo stretchy="false">(</mo><mi>M</mi><mi>i</mi><mi>l</mi><mi>v</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mi>p</mi><mo stretchy="false">(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mo stretchy="false">(</mo><mi>v</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>b</mi><mi>a</mi><mi>s</mi><mi>e</mi><mo stretchy="false">)</mo></mrow><mrow><mi>c</mi><mo stretchy="false">(</mo><mi>v</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">p(rocks | Milvus) = \frac{c(Milvus, rocks)}{c(Milvus)} = \frac{1}{3} p(database | vector) = \frac{c(vector database)}{c(vector)} = \frac{1}{2}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span>roc</span><span>k</span><span>s</span><span>∣</span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>c</span><span>(</span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>)</span></span></span><span><span></span><span></span></span><span><span></span><span><span>c</span><span>(</span><span>M</span><span>i</span><span>l</span><span>vu</span><span>s</span><span>,</span><span></span><span>roc</span><span>k</span><span>s</span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>3</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>p</span><span>(</span><span>d</span><span>a</span><span>t</span><span>aba</span><span>se</span><span>∣</span><span>v</span><span>ec</span><span>t</span><span>or</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>c</span><span>(</span><span>v</span><span>ec</span><span>t</span><span>or</span><span>)</span></span></span><span><span></span><span></span></span><span><span></span><span><span>c</span><span>(</span><span>v</span><span>ec</span><span>t</span><span>or</span><span>d</span><span>a</span><span>t</span><span>aba</span><span>se</span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span></span></span></span></p>
<p>Armed with this knowledge, let&#39;s generalize this and write some code to build a bigram model. For simplicity, we&#39;ll assume each token in all input documents are separated by some whitespace (recall from the earlier section that modern tokenizers often have much more complex rules). Let&#39;s start off by defining the model itself, i.e. bigram counts and token counts.</p>
<pre><code><span>from</span> typing <span>import</span> <span>Dict</span>, <span>Tuple</span>
<span>from</span> collections <span>import</span> defaultdict




token_counts = defaultdict(<span>int</span>)



bigram_counts = defaultdict(<span>int</span>)


<span>def</span> <span>build_bigram_model</span>(<span>corpus: <span>Dict</span>[<span>str</span>]</span>):
    <span>&#34;&#34;&#34;Bigram model.
    &#34;&#34;&#34;</span>

    
    <span>for</span> doc <span>in</span> corpus:

        prev = <span>&#34;&lt;S&gt;&#34;</span>
        <span>for</span> word <span>in</span> doc.split():

            
            token_counts[word] += <span>1</span>

            
            bigram = (prev, word)
            bigram_counts[bigram] += <span>1</span>

            prev = word

        
        bigram_counts[(word, <span>&#34;&lt;/S&gt;&#34;</span>)] += <span>1</span>

    <span>return</span> (token_counts, bigram_counts)


<span>def</span> <span>bigram_probability</span>(<span>bigram: <span>Tuple</span>[<span>str</span>]</span>):
    <span>&#34;&#34;&#34;Computes the likelihood of the bigram from the corpus.
    &#34;&#34;&#34;</span>

    <span>return</span> bigram_counts[bigram] / token_counts[bigram[<span>0</span>]]

</code></pre>
<p>The <code>build_bigram_model</code> then loops through the entire corpus of documents, splitting each by whitespace before storing bigram and token counts. We can then call the <code>bigram_probability</code> function, which looks up the corresponding bigram count and token count and returns the ratio.</p>
<p>Now let&#39;s test this on President Biden&#39;s 2022 State of the Union address. You can download the transcript <a href="https://gist.github.com/fzliu/973bb1d659a740b1d78a659f90be4a02">here</a> and try it out with the code above, if you&#39;d like.</p>
<pre><code><span>&gt;&gt;&gt; </span><span>with</span> <span>open</span>(<span>&#34;state_of_the_union.txt&#34;</span>, <span>&#34;r&#34;</span>) <span>as</span> f:
<span>... </span>    build_bigram_model([f.read()])
...
<span>&gt;&gt;&gt; </span><span>print</span>(bigram_probability((<span>&#34;keep&#34;</span>, <span>&#34;moving&#34;</span>)))
<span>0.07692307692307693</span>
</code></pre>
<p>Following on our discussion of n-grams, it&#39;s also worth it to briefly mention <em>bag-of-words models</em> (BoW). BoW models represent a document or corpus of documents as an unordered set of tokens - in this sense, it maintains the frequency with which each token appears, but disregards the order in which they appear within each document. As such, whole documents in BoW models can be converted to a sparse vector, where each entry of the vector corresponds to the frequency with which a particular word appears in the document. Here, we&#39;ll represent the document &#34;Milvus is the most widely adopted <a href="https://zilliz.com/">vector database</a>. <a href="https://zilliz.com/blog/vector-similarity-search">Vector search</a> with Milvus is easy.&#34; as a BoW sparse vector:</p>
<pre><code>
bow_vector = [
    <span>0</span>, 
    <span>1</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>1</span>, 
    <span>1</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>2</span>, 
    <span>0</span>, 
    <span>2</span>, 
    <span>1</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>0</span>, 
    <span>1</span>, 
    <span>1</span>, 
    <span>0</span>, 
    <span>2</span>, 
    <span>1</span>, 
    <span>1</span>, 
    <span>0</span>, 
    <span>0</span>, 
]
</code></pre>
<p>These sparse vectors can then be used in a variety of NLP tasks, such as text and sentiment classification. We won&#39;t discuss training and inference of BoW models in this blog post; if you&#39;d like to know a bit more, <a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/">Jason Brownlee&#39;s blog</a> is a good resource.</p>
<p>While BoW models are simple to understand and use, they have clear limitations. They do not capture context nor the semantic meaning of individual tokens, making them less suitable for anything beyond the simplest tasks. For this reason, neural networks are exclusively used today; a topic we will discuss in a later post.</p>
<p>In this post, we discussed three core <a href="https://zilliz.com/learn/A-Beginner-Guide-to-Natural-Language-Processing">fundamentals of Natural Language Processing</a> - tokenization, n-grams, and bag-of-words models. In particular, concepts around n-grams will be broadly useful in later topics around ways autoregressive and autoencoding models are trained today.</p>
<p>In the next tutorial, we&#39;ll continue our Embeddings for NLP series by discussing &#34;modern&#34; NLP, i.e. recurrent networks and text embeddings. Stay tuned!</p>
</div></div></div>
  </body>
</html>
