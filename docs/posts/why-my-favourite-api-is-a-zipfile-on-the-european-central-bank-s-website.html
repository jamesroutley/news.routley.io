<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://csvbase.com/blog/5">Original</a>
    <h1>Why my favourite API is a zipfile on the European Central Bank&#39;s website</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        <p>When was the Dollar highest against the Euro?</p>
<p>Here is a small program that calculates it:</p>
<div><pre><span></span>curl -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span>\</span>
<span>|</span> gunzip <span>\</span>
<span>|</span> sqlite3 -csv <span>&#39;:memory:&#39;</span> <span>&#39;.import /dev/stdin stdin&#39;</span> <span>\</span>
  <span>&#34;select Date from stdin order by USD asc limit 1;&#34;</span>
</pre></div>
<p>The output: <code>2000-10-26</code>.  (Try running it yourself.)</p>
<p>How it works:</p>
<p>The <code>curl</code> bit downloads the <a href="https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html">official historical data that the European
Central Bank
publishes</a>
on the position of the Euro against other currencies.  (The <code>-s</code> flag just
removes some noise from standard error.)</p>
<p>That data comes as a zipfile, which <code>gunzip</code> will decompress.</p>
<p><code>sqlite3</code> queries the csv inside.  <code>:memory</code> tells sqlite to use an in-memory
file.  After that, <code>.import /dev/stdin stdin</code> tells sqlite to load standard
input into a table called <code>stdin</code>.  The string that follows that is a SQL
query.</p>
<h2>Cleanup in column 42</h2>
<p>Although pulling out a simple max is easy, the data shape is not ideal.  It&#39;s
in &#34;wide&#34; format - a <code>Date</code> column, and then an extra column for every
currency.  Here&#39;s the csv header for that file:</p>
<div><pre><span></span>Date,USD,JPY,BGN,CYP,CZK,DKK,EEK,GBP,HUF,LTL,LVL,MTL,[and on, and on]
</pre></div>
<p>When doing filters and aggregations, life is easier if the data is in &#34;long&#34;
format, like this:</p>

<p>Switching from wide to long is a simple operation, commonly called a &#34;melt&#34;.
Unfortunately, it&#39;s not available in SQL.</p>
<p>No matter, you can melt with pandas:</p>
<div><pre><span></span>curl -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span>|</span> <span>\</span>
gunzip <span>|</span> <span>\</span>
python3 -c <span>&#39;import sys, pandas as pd</span>
<span>pd.read_csv(sys.stdin).melt(&#34;Date&#34;).to_csv(sys.stdout, index=False)&#39;</span>
</pre></div>
<p>There is one more problem.  The file mungers at ECB have wrongly put a trailing
comma at the end of every line.  The makes csv parsers pick up an extra, blank
column at the end.  Our sqlite query didn&#39;t notice, but these commas interfere
with the melt, creating a whole set of junk rows at the end:</p>
<p><img src="https://csvbase.com/blog-static/melts-are-for-melts.png" alt="csv file in terminal with junk at the end"/></p>
<p>The effects of that extra comma can be removed via pandas by adding one more
thing to our method chain: <code>.iloc[:, :-1]</code>, which effectively says &#34;give me all
rows (&#34;<code>:</code>&#34;) and all but the last column (&#34;<code>:-1</code>&#34;).  So:</p>
<div><pre><span></span>curl -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span>|</span> <span>\</span>
gunzip <span>|</span> <span>\</span>
python3 -c <span>&#39;import sys, pandas as pd</span>
<span>pd.read_csv(sys.stdin).iloc[:, :-1].melt(&#34;Date&#34;)\</span>
<span>.to_csv(sys.stdout, index=False)&#39;</span>
</pre></div>
<p>Does everyone who uses this file have to repeat this data shitwork?</p>
<p>Tragically, the answer is yes.  As they say: &#34;data janitor: nobody&#39;s dream,
everyone&#39;s job&#34;.</p>
<p>In full fairness, though, the ECB foreign exchange data is probably in the top
10% of all open data releases.  Usually, getting viable tabular data out of
someone is a much more tortuous and involved process.</p>
<p>Some things we didn&#39;t have to do in this case: negotiate access (for example by
paying money or talking to a salesman); deposit our email address/company
name/job title into someone&#39;s database of qualified leads, observe any quota;
authenticate (often a substantial side-quest of its own), read any API docs at
all or deal with any issues more serious than basic formatting and shape.</p>
<p>So <code>eurofxref-hist.zip</code> is, relatively speaking, pretty nice actually.</p>
<p>But anyway - I&#39;ll put my cleaned up copy <a href="https://csvbase.com/calpaterson/eurofxref-hist">into a csvbase
table</a> so you, dear reader, can
skip the tedium and just have fun.</p>
<p>Here&#39;s how I do that:</p>
<div><pre><span></span>curl -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span>|</span> <span>\</span>
gunzip <span>|</span> <span>\</span>
python3 -c <span>&#39;import sys, pandas as pd</span>
<span>pd.read_csv(sys.stdin).iloc[:, :-1].melt(&#34;Date&#34;)\</span>
<span>.to_csv(sys.stdout, index=False)&#39;</span> <span>|</span> <span>\</span>
<span># this is the new bit: \</span>
curl -n --upload-file - <span>\</span>
<span>&#39;https://csvbase.com/calpaterson/eurofxref-hist?public=yes&#39;</span>
</pre></div>
<p>All I&#39;ve done is add another <code>curl</code>, to HTTP PUT the csv file into csvbase.
<code>--upload-file -</code> uploads from standard input to the given url (via HTTP PUT).
If the table doesn&#39;t already exist in csvbase, it is created.  <code>-n</code> adds my
<a href="https://csvbase.com/calpaterson/eurofxref-hist/docs#authentication">credentials</a>
from my <code>~/.netrc</code>.  That&#39;s it.  Simples.</p>
<h2>Drawing pretty graphs</h2>
<p>Alright, now the data cleaning phase is over, let&#39;s do some more interesting stuff.</p>
<p>Let&#39;s graph the data:</p>
<div><pre><span></span>curl -s https://csvbase.com/calpaterson/eurofxref-hist <span>|</span> <span>\</span>
grep USD <span>|</span> <span>\</span>
cut --delim<span>=</span>, -f <span>2</span>,4 <span>|</span> <span>\</span>
gnuplot -e <span>&#34;set datafile separator &#39;,&#39;; set term dumb; \</span>
<span>plot &#39;-&#39; using 1:2 with lines title &#39;usd&#39;&#34;</span>
</pre></div>
<p><img src="https://csvbase.com/blog-static/dumb-term.png" alt="a gnuplot graph in drawn in the
terminal"/></p>
<p>That&#39;s somewhat legible for over 6000 datapoints in an 80x25 character
terminal.  You can make out the broad trend.  A reasonable <a href="https://infovis-wiki.net/wiki/Data-Ink_Ratio">data-ink
ratio</a>.</p>
<p>(If you&#39;re wondering how
<a href="https://csvbase.com/calpaterson/eurofxref-hist">https://csvbase.com/calpaterson/eurofxref-hist</a>
returns a webpage to your browser but a csv file to curl, see an <a href="https://csvbase.com/blog/2">earlier
blogpost</a>.)</p>
<p><code>gnuplot</code> is like a little mini-programming language of it&#39;s own.  Here&#39;s what
the above snippet does:</p>
<ul>
<li><code>set datafile separator &#39;,&#39;</code> - says it&#39;s a csv</li>
<li><code>set term dumb</code> - draw ascii-art!</li>
<li><code>plot -</code> plot the data coming from standard input</li>
<li><code>using 1:2 with lines</code> draw lines from columns 1 and 2 (the date and the rate
respectively)</li>
<li><code>title &#39;usd&#39;</code> name the line</li>
</ul>
<p>You can, of course, also draw graphs to proper images:</p>
<div><pre><span></span>curl -s https://csvbase.com/calpaterson/eurofxref-hist <span>|</span> <span>\</span>
grep USD <span>|</span> <span>\</span>
cut --delim<span>=</span>, -f <span>2</span>,4 <span>|</span> <span>\</span>
gnuplot -e <span>&#34;set datafile separator &#39;,&#39;; set term svg; \</span>
<span>set output &#39;usd.svg&#39;; set xdata time; set timefmt &#39;%Y-%m-%d&#39;; \</span>
<span>set format x &#39;%Y-%m-%d&#39;; set xtics rotate; \</span>
<span>plot &#39;-&#39; using 1:2 with lines title &#39;usd&#39;&#34;</span>
</pre></div>
<p><img alt="a gnuplot graph of usd:eur" src="https://csvbase.com/blog-static/usd.svg"/>
</p>
<p>Outputting to SVG is only a bit more complicated than ascii art.  In order for
it look decent you need to help gnuplot understand that it&#39;s &#34;timeseries&#34;
data - ie: that the x axis is time; give a format for that time and then tell
it to rotate the markings on the x axis so that they are readable.  It&#39;s a bit
wordy though: let&#39;s bind it to bash function so we can reuse it:</p>
<div><pre><span></span>plot_timeseries_to_svg <span>()</span> <span>{</span>
    <span># $1 is the first param</span>
    gnuplot -e <span>&#34;set datafile separator &#39;,&#39;; set term svg; \</span>
<span>    set output &#39;</span><span>$1</span><span>.svg&#39;; set xdata time; set timefmt &#39;%Y-%m-%d&#39;; \</span>
<span>    set format x &#39;%Y-%m-%d&#39;; set xtics rotate; \</span>
<span>    plot &#39;-&#39; using 1:2 with lines title &#39;</span><span>$1</span><span>&#39;&#34;</span>
<span>}</span>
</pre></div>
<h2>Rolling averages and new tools</h2>
<p>So far, so good.  But it would be nice to try out more sophisticated analyses:
let&#39;s try putting a nice rolling average in so that we can see a trend line:</p>
<div><pre><span></span>curl -s https://csvbase.com/calpaterson/eurofxref-hist <span>|</span> <span>\</span>
duckdb -csv -c <span>&#34;select Date, avg(value) over \</span>
<span>(order by date rows between 100 preceding and current row) \</span>
<span>as rolling from read_csv_auto(&#39;/dev/stdin&#39;)</span>
<span>where variable = &#39;USD&#39;;&#34;</span> <span>|</span> <span>\</span>
plot_timeseries_to_svg rolling
</pre></div>
<p><img alt="a rolling averaged gnuplot graph of usd:eur" src="https://csvbase.com/blog-static/rolling.svg"/>
</p>
<p>Smooth.  If you don&#39;t have <code>duckdb</code> installed, it&#39;s not hard to adapt the above
for <code>sqlite3</code> (the query is the same).  DuckDB is a tool I wanted to show
because it&#39;s a lot like sqlite but instead is columnar (rather than
row-oriented).  However for me the main value is that it has a lot of easy
ergonomics.</p>
<p>Here is one of them: you can load csvs into table files straight from HTTP:</p>
<div><pre><span></span><span>-- it works with csvbase!</span>
<span>CREATE</span><span> </span><span>TABLE</span><span> </span><span>eurofxref_hist</span><span> </span><span>AS</span><span> </span><span>SELECT</span><span> </span><span>*</span><span> </span><span>FROM</span><span></span>
<span>read_csv_auto</span><span>(</span><span>&#34;https://csvbase.com/calpaterson/eurofxref-hist&#34;</span><span>);</span><span></span>
</pre></div>
<p><img src="https://csvbase.com/blog-static/duckdb-download.png" alt="eurofxref-hist in duckdb"/></p>
<p>That&#39;s pretty easy, and DuckDB does a reasonable job of inferring types.  There
are a lot of other usability niceties too: for example, it helpfully detects
your terminal size and abridges tables by default rather than flooding your
terminal with an enormous resultset.  It has a progress bar for big queries!
It can output markdown tables!  Etc!</p>
<h2>Open data is also an open API</h2>
<p>A lot is possible with a zipfile of data and just the programs that are either
already installed or a quick <code>brew install</code>/<code>apt install</code> away.  I remember how
impressed I was when I was first shown this <code>eurofxref-hist.zip</code> by an old hand
from foreign exchange when I worked in a bank.  It was so simple: the simplest
cross-organisation data interchange protocol I had then seen (and probably
since).</p>
<p>A mere zipfile with a csv in it seems so diminutive, but in fact an enormous
mass of financial applications use this particular zipfile every day.  I&#39;m
pretty sure that&#39;s why they&#39;ve left those commas in - if they removed them now
they&#39;d break a lot of code.</p>
<p>When open data is made really easily available, it also functions double duty
as an open API.  After all, for the largeish fraction of APIs in which are less
about calling remote functions than about exchanging data, what is the
functional difference?</p>
<p>So I think the ECB&#39;s zipfile is a pretty good starting point for a data
interchange format.  I love the simplicity - and I&#39;ve tried to keep that with
csvbase.</p>
<p>In csvbase, every table has a single url, following the form:</p>
<p><code>https://csvbase.com/&lt;username&gt;/&lt;table_name&gt;</code></p>
<p>eg</p>
<p><a href="https://csvbase.com/calpaterson/eurofxref-hist">https://csvbase.com/calpaterson/eurofxref-hist</a></p>
<p>And on each url, there are four main verbs:</p>
<p>When you <code>GET</code>: you get a csv (<a href="https://csvbase.com/blog/2">or a web page, if you&#39;re in a
browser</a>).</p>
<p>When you <code>PUT</code> a new csv: you create a new table, or overwrite the existing one.</p>
<p>When you <code>POST</code> a new csv: you bulk add more rows to an existing table.</p>
<p>When you <code>DELETE</code>: that table is no more.</p>
<p>To authenticate, just use <a href="https://csvbase.com/calpaterson/eurofxref-hist/docs#authentication">HTTP Basic
Auth</a>.</p>
<p>Could it be any simpler?  If you can think of a way: <a href="mailto:cal@calpaterson.com">write me an
email</a>.</p>
<h2>Notes</h2>
<p>I said above that most SQL databases don&#39;t have a &#34;melt&#34; operation.  The ones
that I know of that do are
<a href="https://docs.snowflake.com/en/sql-reference/constructs/unpivot">Snowflake</a> and
<a href="https://learn.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-ver16">MS SQL
Server</a>.
One question that SQL-knowers frequently ask is: why does anyone use R or
Pandas at all when SQL already exists?  A key reason is that R and Pandas are
very strong on data cleanup.</p>
<p>One under-appreciated feature of bash pipelines is that they are multi-process.
Each program runs independently, in it&#39;s own process.  While curl is
downloading data from the web, grep is filtering it, sqlite is querying it and
perhaps curl is uploading it again, etc.  All in parallel, which can,
surprisingly, make it <a href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">very competitive with fancy cloud
alternatives</a>.</p>
<p>Why was the Euro so weak back in 2000?  It was launched, without coins or
notes, in January 1999.  The Euro was, initially, a sort of in-game currency
for the European Union.  It existed only inside banks - so there were no notes
or coins for it.  That all came later.  So did belief - early on it didn&#39;t look
like the little Euro was going to make it: so the rate against the Dollar was
0.8252.  That means that in October 2000, a Dollar would buy you 1.21 Euros (to
reverse exchange rates, do <code>1/rate</code>).  Nowadays the Euro is much stronger: a
Dollar would buy you less than 1 Euro.</p>

      </div>
    </div></div>
  </body>
</html>
