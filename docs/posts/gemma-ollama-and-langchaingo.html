<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/">Original</a>
    <h1>Gemma, Ollama and LangChainGo</h1>
    
    <div id="readability-page-1" class="page"><div>
                
                <p>Yesterday Google released Gemma - an open LLM that folks can run locally on
their machines (similarly to <tt>llama2</tt>). I was wondering how easy it would be
to run Gemma on my computer, chat with it and interact with it from a Go
program.</p>
<p>Turns it - thanks to <a href="https://ollama.com/download">Ollama</a> - it&#39;s extremely
easy! Gemma was already <a href="https://ollama.com/library/gemma">added to Ollama</a>,
so all one has to do is run:</p>

<p>And wait for a few minutes while the model downloads. From this point on, my
previous post about <a href="https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/">using Ollama locally in Go</a> applies
with pretty much no changes. Gemma becomes available through a REST API locally,
and can be accessed from ollama-aware libraries like <a href="https://github.com/tmc/langchaingo">LangChainGo</a>.</p>
<p>I went ahead and added a <tt><span>--model</span></tt> flag to all my <a href="https://github.com/eliben/code-for-blog/tree/master/2023/ollama-go-langchain">code samples from that post</a>,
and they can all run with <tt><span>--model</span> gemma</tt> now. It all just works, due to the
magic of standard interfaces:</p>
<ul>
<li>Gemma is packaged in a standard interface for inclusion in Ollama</li>
<li>Ollama then presents a standardized REST API for this model, just like it
does for other compatible models</li>
<li>LangChainGo has an Ollama provider that lets us write code to interact with
any model running through Ollama</li>
</ul>
<p>So we can write code like:</p>
<div><pre><span></span><span>package</span><span> </span><span>main</span><span></span>

<span>import</span><span> </span><span>(</span><span></span>
<span>  </span><span>&#34;context&#34;</span><span></span>
<span>  </span><span>&#34;flag&#34;</span><span></span>
<span>  </span><span>&#34;fmt&#34;</span><span></span>
<span>  </span><span>&#34;log&#34;</span><span></span>

<span>  </span><span>&#34;github.com/tmc/langchaingo/llms&#34;</span><span></span>
<span>  </span><span>&#34;github.com/tmc/langchaingo/llms/ollama&#34;</span><span></span>
<span>)</span><span></span>

<span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span><span></span>
<span>  </span><span>modelName</span><span> </span><span>:=</span><span> </span><span>flag</span><span>.</span><span>String</span><span>(</span><span>&#34;model&#34;</span><span>,</span><span> </span><span>&#34;&#34;</span><span>,</span><span> </span><span>&#34;ollama model name&#34;</span><span>)</span><span></span>
<span>  </span><span>flag</span><span>.</span><span>Parse</span><span>()</span><span></span>

<span>  </span><span>llm</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>ollama</span><span>.</span><span>New</span><span>(</span><span>ollama</span><span>.</span><span>WithModel</span><span>(</span><span>*</span><span>modelName</span><span>))</span><span></span>
<span>  </span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span></span>
<span>    </span><span>log</span><span>.</span><span>Fatal</span><span>(</span><span>err</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>

<span>  </span><span>query</span><span> </span><span>:=</span><span> </span><span>flag</span><span>.</span><span>Args</span><span>()[</span><span>0</span><span>]</span><span></span>
<span>  </span><span>ctx</span><span> </span><span>:=</span><span> </span><span>context</span><span>.</span><span>Background</span><span>()</span><span></span>
<span>  </span><span>completion</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>llms</span><span>.</span><span>GenerateFromSinglePrompt</span><span>(</span><span>ctx</span><span>,</span><span> </span><span>llm</span><span>,</span><span> </span><span>query</span><span>)</span><span></span>
<span>  </span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span></span>
<span>    </span><span>log</span><span>.</span><span>Fatal</span><span>(</span><span>err</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>

<span>  </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>&#34;Response:\n&#34;</span><span>,</span><span> </span><span>completion</span><span>)</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>And then run it as follows:</p>
<div><pre><span></span>$ go run ollama-completion-arg.go --model gemma &#34;what should be added to 91 to make -20?&#34;
Response:
 The answer is -111.

91 + (-111) = -20
</pre></div>
<p>Gemma seems relatively fast for a model running on a CPU.
I find that the default 7B model, while much more capable than the default 7B
llama2 based on published benchmarks - also runs about 30% faster on my machine.</p>

            </div></div>
  </body>
</html>
