<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://interjectedfuture.com/year-2025-in-review/">Original</a>
    <h1>Year 2025 in Review</h1>
    
    <div id="readability-page-1" class="page"><div>

    <article>

        <header>

            

            <div>
                <p><a href="https://linuxdaw.org/author/wil/">
                                <img src="https://www.gravatar.com/avatar/915aac3dfde2fb502ce415d77643a72d?s=250&amp;d=mm&amp;r=x" alt="Wil Chung"/>
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2025-12-29">29 Dec 2025</time>
                            <span><span>—</span> 12 min read</span>
                    </p>
                </div>
            </div>

                <figure>
        <img srcset="/content/images/size/w320/2025/12/multi-use-zoning.jpg 320w,
                    /content/images/size/w600/2025/12/multi-use-zoning.jpg 600w,
                    /content/images/size/w960/2025/12/multi-use-zoning.jpg 960w,
                    /content/images/size/w1200/2025/12/multi-use-zoning.jpg 1200w,
                    /content/images/size/w2000/2025/12/multi-use-zoning.jpg 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://linuxdaw.org/content/images/size/w1200/2025/12/multi-use-zoning.jpg" alt="Year 2025 in Review"/>
    </figure>

        </header>

        <section>
            <p>It&#39;s another year again, and I look back on what I did. This is mainly for myself, as I didn&#39;t write one of these in 2022, and now I can&#39;t remember what I spent all my time doing for the whole year.</p><ul><li><a href="https://interjectedfuture.com/year-2024-in-review/">Year 2024 in Review</a></li><li><a href="https://interjectedfuture.com/year-2023-in-review/">Year 2023 in Review</a></li><li><a href="https://interjectedfuture.com/2021-year-in-review/">Year 2021 in Review</a></li></ul><h2 id="away-from-databases">Away from Databases</h2><p>I had started the year by cutting my losses, and moving away from doing database work. A couple years back, there weren&#39;t any databases that were immutable, local-first, open-source, reactive, and any other number of properties that I wanted. I spent a good amount of time getting up to speed, but I never got to a point where I had remotely the output necessary to make it a reality. </p><p>I reminded myself that databases were never the point, so I&#39;d cut my losses and work on the reactive collaborative notebook with effects. I had to reload my brain with the research I did on reactive systems and algebraic effects around January, and started coding up an algebraic effects system through March.</p><figure><a href="https://interjectedfuture.com/lab-notes-058-free-monad-effects-and-algebraic-effects/"><div><p>Lab note #058 Free Monad Effects and Algebraic Effects</p><p>I’d finished up looking into reactivity (with the exception of data type enforced reactivity, like differential dataflow), and I pulled out code with pedagogical sample code implementing signals and autotracking. * iamwilhelm/pedago_signals: A pedagogical implementation of hybrid push-pull signals in Python * iamwilhelm/pedago_autotracking: A pedagogical implementation of autotracking</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-26.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/DALL-E-2025-02-04-11.29.15---An-illustrative-16_9-image-representing--Free-Monad-Effects-and-Algebraic-Effects.--The-design-features-a-dual-layered-composition_-one-layer-showing-.jpeg" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><h2 id="visual-programming">Visual Programming</h2><p>I&#39;m not currently working on visual programming, but I seem to have opinions that resonated with others about it.  </p><figure><a href="https://interjectedfuture.com/visual-programming-is-stuck-on-the-form/"><div><p>Visual programming is stuck on the form</p><p>Underlying great creations that you love—be it music, art, or technology—its form (what it looks like) is driven by an underpinning internal logic (how it works). I noticed this pattern while watching a talk on cellular automaton and realized it’s “form follows function” paraphrased from a slightly different</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-24.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/DALL-E-2025-02-15-15.52.43---A-vibrant-watercolor-painting-of-a-Cellpond-style-cellular-automaton-in-a-regular-grid.-The-cells-form-intricate--organic-patterns--flowing-like-rippl-1.jpeg" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><p>I wrote this over the course of four days, and had to rewrite it once or twice, as I was trying to figure out what I was really trying to say. It&#39;s notable as a year in review, because I remember the feeling of having something important to say, but struggling to figure out how to say it. </p><p>I&#39;m glad that I did, because it ended up on my year in review as something I&#39;m proud of doing. In 2026, I need to listen to that feeling more, and pause to do more writing.</p><h2 id="dbsp">DBSP</h2><p><a href="https://arxiv.org/pdf/2203.16684?ref=interjectedfuture.com" rel="noreferrer">DBSP</a> is one of those things that is terribly attractive to me, but I feel guilty working on it. Is it core to a reactive notebook with effects? No, but it&#39;s certainly tangential. I spent about a month and half on a half-baked implementation. </p><p>I used AI to help me read the paper, and I learned a lot from both looking at a reference (but bad) implementation, and doing the implementation myself. It was really nice to have something built up like theorem from the ground up. However, the issue was how to structure certain parts of it outside of the theorem. I had little to no experience with this kind of domain, and there was a lot of rewriting of code. I learned a lot, but I felt it wasn&#39;t enough to tell people about it.</p><p>At around this time, Claude Code had come out, and I tried to vibe code DBSP with Claude Code using Sonnet 4.0, but it was a big failure. The code was wrong in completely subtle ways, and only after I had to work with it. It felt a bit depressing.</p><p>Maybe the problem was that I was too worried about ruining an existing codebase with AI-based slop. So I took a detour of a detour to have something &#34;like&#34; a notebook, but a sample project to vibe code upon without worrying about terrible code.</p><figure><a href="https://interjectedfuture.com/lab-note-070-vibe-coding-is-half-a-skill-issue/"><div><p>Lab note #070 Vibe-coding is half a skill issue</p><p>I’ve been doing an implementation of DBSP, which is a way of doing incremental computation built up from some basic concepts from digital signal processing. While I use LLMs in my daily work to ask it questions, spitball with it, and do some basic stuff, I hardly ever ask it</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-27.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/ChatGPT-Image-Jun-10--2025--10_15_02-PM.jpg" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><h2 id="personal-consumer-review">Personal Consumer Review</h2><p>I noticed that one of the topics I often asked ChatGPT was to do recommendations for a product category. I&#39;ve often wanted Wirecutter to tell me not just what they recommend, but <em>what</em> to look for. I wanted them to tell me what good looked like and why that was the case. I thought a GPT-based product recommendation canvas would be a good tool to have.</p><p>I vibe coded it and was surprised that the initial prototype looked as good and worked as well as it did. I did end up abandoning the project because I figured it was too obvious, and I saw OpenAI was also doing product shopping and recommendations. </p><figure><a href="https://interjectedfuture.com/lab-note-071-agentic-loop-for-buying-decisions/"><div><p>Lab note #071 Agentic loop for buying decisions</p><p>Switched gears this week to start a new, but related project. I started a prototype on an app that helps users make buying decisions with an AI agent. Lately, I’ve been using ChatGPT to help me make buying decisions, but I found some aspect of the experience wanting. So I</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-25.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/ChatGPT-Image-Jun-18--2025--02_30_27-PM.png" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><p>It was around this time that I saw people misusing Claude Code.</p><h2 id="misusing-claude-code">Misusing Claude Code</h2><p>There were people that started using Claude Code for non-coding purposes on their personal knowledge bases in the file system. I thought it was an interesting experiment, so I finally exported myself out of Roam Research into Logseq. Then I asked Claude Code some questions after it explored my personal knowledge base. The result was a breath of fresh air. </p><p>Normally, I don&#39;t have anyone to talk to about the things I&#39;m thinking or reading about. But Claude Code with access to my knowledge base, I didn&#39;t have to explain context to have a conversation with it. It felt like instant connection, because it mimicked when someone gets you. Of course, there are downsides to this, but I soon saw other people misusing Claude Code in different ways, from doing their SOC2 compliance to running their morning retrospectives.</p><p>I found it was a powerful combination. I could run a coach for customer development, which I could never do before.</p><figure><a href="https://interjectedfuture.com/customer-developments-missing-feedback-loop/"><div><p>Doing More Customer Interviews Won’t Make You Better At Them</p><p>Founders and product managers fail at customer development, even when they’re consistently talking to users. But this is less from conducting too few interviews and more from a missing feedback loop. The art of posing the right question is counterintuitive and cognitively taxing, so founders can’t analyze their own</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-28.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/ChatGPT-Image-Oct-6--2025--11_15_16-PM.png" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><p>This was the first time I had cobbled some off-the-shelf tools in order to live in the future. I was going to pivot again, this time from notebook to a wiki where agents and humans work in the same digital space.</p><h2 id="system-evals">System Evals</h2><p>While I was doing this, I took <a href="https://maven.com/parlance-labs/evals?ref=interjectedfuture.com" rel="noreferrer">Hamel and Shreya&#39;s course on system evals</a>. While Sri and I did write an eZine on system evals, called Forest Friends, I felt like most of the knowledge of evals came from Sri, and I needed to shore up on it. </p><figure><a href="https://forestfriends.tech/?ref=interjectedfuture.com"><div><p>Forest Friends Zine</p><p>A guide for AI Engineers building the wild world of LLM system evals</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-30.ico" alt=""/><span>Forest Friends Zine</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/Go-from-Zero-to-Eval.jpg" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><p>In the past, I never would have ponied up the money for such a course, but later in my career, it&#39;s a quick way to get up to speed on something that would have taken me longer to gather all the materials myself.</p><p>I learned some stuff here and there, but excitingly, I could also misuse Claude Code to run a system eval! </p><figure><a href="https://interjectedfuture.com/system-eval-with-obsidian-and-claude-code/"><div><p>System eval with Obsidian and Claude Code</p><p>Lately, I’ve been experimenting with a different system eval stack than what you might normally hear about: Obsidian and Claude Code. Typically, Obsidian is used as a personal note taking app/knowledge base and Claude Code is used as a coding agent. I’m misusing both to hack together a system</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-29.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/ChatGPT-Image-Aug-26--2025--10_42_49-PM.jpg" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><h2 id="where-agents-and-humans-roam">Where Agents and Humans Roam</h2><p>After these experiences, I wanted a wiki that agents and humans can work on in the same digital space together. The set up of Claude Code and Obsidian seemed to have legs, and powerfully so. However, I thought there were improvements that could be made. The ideas I had for the notebook before wouldn&#39;t die, but they&#39;d be eventually incorporated.</p><p>Around August is when I decided to start work on this, and that&#39;s when I went silent in the lab notes, and just worked on writing code. </p><figure><img src="https://interjectedfuture.com/content/images/2025/12/Screenshot-2025-12-28-at-9.01.24---PM.png" alt="" loading="lazy" width="1496" height="406" srcset="https://interjectedfuture.com/content/images/size/w600/2025/12/Screenshot-2025-12-28-at-9.01.24---PM.png 600w, https://interjectedfuture.com/content/images/size/w1000/2025/12/Screenshot-2025-12-28-at-9.01.24---PM.png 1000w, https://interjectedfuture.com/content/images/2025/12/Screenshot-2025-12-28-at-9.01.24---PM.png 1496w" sizes="(min-width: 720px) 720px"/></figure><p>It wasn&#39;t really with any desire for secrecy, as much as it was that context switching between writing code and writing prose was hard for me. So I&#39;d occasionally write an essay and went back to writing code. However, I&#39;m not sure exactly how to articulate what this is yet, but I know I need to do so in 2026. I just thought I&#39;d be further along than I am right now.</p><p>The current application is pretty rough UX-wise. A lot of hand-wringing and code-wrangling was spent around supporting offline-editing. I discovered first-hand, where the state of local-first software is, and there are still major pieces that are missing before this is a no-brainer for mainstream devs.</p><figure><img src="https://interjectedfuture.com/content/images/2025/12/Screenshot-2025-12-28-at-10.11.42---PM.png" alt="" loading="lazy" width="2000" height="1072" srcset="https://interjectedfuture.com/content/images/size/w600/2025/12/Screenshot-2025-12-28-at-10.11.42---PM.png 600w, https://interjectedfuture.com/content/images/size/w1000/2025/12/Screenshot-2025-12-28-at-10.11.42---PM.png 1000w, https://interjectedfuture.com/content/images/size/w1600/2025/12/Screenshot-2025-12-28-at-10.11.42---PM.png 1600w, https://interjectedfuture.com/content/images/size/w2400/2025/12/Screenshot-2025-12-28-at-10.11.42---PM.png 2400w" sizes="(min-width: 720px) 720px"/></figure><h2 id="working-with-agents">Working with Agents</h2><p>The cross-cutting theme for this year was trying to see vibe coding worked, and whether it worked for me. Claude Code had come out in February, and there were people that were <a href="https://every.to/source-code/the-three-ways-i-work-with-llms?ref=interjectedfuture.com" rel="noreferrer">claiming success with it in production</a>. </p><p>Something I&#39;ve always struggled with was speed vs quality. I tend towards quality, refactoring as I go. However, I always feel terribly slow, compared to other devs. Agentic coding promised a productivity boost, and as desperate as I was for it to work, I just couldn&#39;t get it to work for me.</p><p>In retrospect, this wasn&#39;t really my fault. There were a couple things working against me. </p><ol><li>Vibe coding works well for well-known stacks. While I was using Python at the time, I wasn&#39;t using any frameworks.</li><li>Vibe coding works well for well-known domains. What I was working on was not a web app, and DBSP is hardly a common domain.</li><li>Vibe coding works well when you have better models. Since GPT-5.1 in Codex and Claude Sonnet 4.5 in Claude Code came on the scene, I&#39;ve noticed much better results.</li><li>Vibe coding works well when you have lots of scaffolding for the coding agent to lean on, such as unit tests, type checking, documentation, comments, PRs, etc. If you&#39;re just a single dev, you may not do these things because it doesn&#39;t make sense for a single dev to do it.</li><li>Vibe coding works well when you know how to break things down for an agent. It was rather unintuitive to me that I needed to break a feature down into code base research, planning, and implementation stages. It was also an eye opener that you don&#39;t need to write every instruction yourself. You can ask the AI to ask you questions in order to write the prompt. </li><li>Vibe coding works well when you&#39;re used to managing others. If you don&#39;t like managing people, you&#39;re going to dislike vibe coding.</li></ol><figure><a href="https://interjectedfuture.com/vibe-coding-for-experts-needs-scaffolding/"><div><p>The Hidden Scaffolding Behind Production Vibe Coding</p><p>I’m not near a level where it’s 10k LOC per day, but the volume of commits is noticeably higher w/ Claude/Codex now. Back in Feb/Mar when Claude Code came out, I was writing an implementation of DBSP (incremental computation with some mathy stuff) and I couldn’</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-31.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/ChatGPT-Image-Sep-9--2025--11_43_18-AM.jpg" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><p>It&#39;s been a hard adjustment. And even now, I can&#39;t completely let go of writing code by hand. There are some parts of my app that I leaned completely into vibe coding, such as sections that are scaffolding that I know I&#39;ll get rid of, or various bespoked tools not yet available for LiveStore. </p><p>One great use is for me to generate prototypes to click around. Sometimes, I need to have something to click on to know how it feels, and vibe coding is a great way to explore the space before doubling down on something. </p><figure><img src="https://interjectedfuture.com/content/images/2025/12/wide_first_narrow_later.png" alt="" loading="lazy" width="2000" height="1113" srcset="https://interjectedfuture.com/content/images/size/w600/2025/12/wide_first_narrow_later.png 600w, https://interjectedfuture.com/content/images/size/w1000/2025/12/wide_first_narrow_later.png 1000w, https://interjectedfuture.com/content/images/size/w1600/2025/12/wide_first_narrow_later.png 1600w, https://interjectedfuture.com/content/images/2025/12/wide_first_narrow_later.png 2256w" sizes="(min-width: 720px) 720px"/></figure><p>Another great use is in bridging over the adjacent ecosystems and spaces that would have had too much activation energy to get into, such as Lean.</p><figure><a href="https://interjectedfuture.com/the-best-way-to-learn-might-be-starting-at-the-end/"><div><p>The Best Way to Learn Might Be Starting at the End: Writing a Proof in Lean</p><p>AI leveraged learning lets you start with the application at the end. Curiosity guides what you learn, fundamentals backfill when you need them.</p><p><img src="https://interjectedfuture.com/content/images/icon/favicon-32.ico" alt=""/><span>Interjected Future</span><span>Wil Chung</span></p></div><p><img src="https://interjectedfuture.com/content/images/thumbnail/ChatGPT-Image-Oct-17--2025--10_22_41-PM.jpg" alt="" onerror="this.style.display = &#39;none&#39;"/></p></a></figure><p>I haven&#39;t leaned completely into vibe coding the core aspect of the code base. At best, I take a middle road coined as &#34;<a href="https://www.geoffreylitt.com/2025/10/24/code-like-a-surgeon?ref=interjectedfuture.com" rel="noreferrer">Coding like a Surgeon</a>&#34; by Geoffrey Litt. While I do ask Codex/Claude Code to research the code base and make a plan, I found it better if I asked it to generate a tutorial for me to implement.</p><p>On one hand, this could seem like I&#39;m just bottlenecking myself. But I think the benefits are that I get to keep abreast of the code base for future edits, be in control of bloat, and use my taste and view of the future to correctly draw the system boundaries. </p><p>One day, I won&#39;t have to do this, but for now, coding agents aren&#39;t good at, or need a lot of context in order to draw the correct system boundaries for code bases that are out-of-distribution. </p><h2 id="retrospect-on-2025">Retrospect on 2025</h2><p>When I look back, I only work on something for about 3 to 6 months before moving on to something else. So while the ideas in my head build upon each other and compound, my work doesn&#39;t. I think it was because I never really had full conviction with anything I was working on, and I always felt guilty about it if it didn&#39;t directly lead to a product. </p><p>However, its&#39; not happening this time with this wiki with agents thing. I had wanted it to be personally usable by the end of the year, but it looks like it&#39;ll be another month before that can happen. I hope to privately launch it by the Spring. </p><blockquote>My thesis is that a reactive notebook with effects will make building LLM-driven applications much more quickly and pleasant. But that&#39;s just a hunch. That&#39;s not something verifiable in a book, but needs to be wrought with the friction of users coming up against product. If I don&#39;t get there within 2025, I&#39;ll be disappointed in myself.</blockquote><p>I&#39;ve changed the thesis a bit, so I&#39;m not too hard on myself there. However, I&#39;m disappointed in that I haven&#39;t created as many digital by-products to make my work more legible. I have dozens and dozens of blog post drafts and ideas that aren&#39;t finished, and I need to find a different way to do this.</p><h2 id="commitment-to-2026">Commitment to 2026</h2><p>With a new year comes a new aesthetic. I wanted blog post images that stood out, so for a while I was using pastel origami. But I&#39;ve been thinking that I want to explore an aesthetic that we started with Forest Friends: a urban architecture with organic curvilinear shapes mated with half-timbered architecture. I want to see what that can look like. </p><p>But more seriously, there&#39;s two things I can commit to for 2026:</p><ul><li>Launch a product.</li><li>Write a lot more to be legible.</li></ul><p>I think with the clarity and conviction that I have now, it seems a little easier. However, I know that there&#39;s going to be events that will test that conviction in the coming year.</p>
        </section>

    </article>


</div></div>
  </body>
</html>
