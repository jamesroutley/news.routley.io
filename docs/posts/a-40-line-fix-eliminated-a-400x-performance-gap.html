<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://questdb.com/blog/jvm-current-thread-user-time/">Original</a>
    <h1>A 40-line fix eliminated a 400x performance gap</h1>
    
    <div id="readability-page-1" class="page"><article><div><p>I have a habit of skimming the OpenJDK commit log every few weeks. Many commits are too complex for me to grasp in the limited time I have reserved for this ... <em>special hobby</em>. But occasionally something catches my eye.</p>
<p>Last week, <a href="https://github.com/openjdk/jdk/commit/858d2e434dd">this commit</a> stopped me mid-scroll:</p>
<div><div><div><pre><p><span>858d2e434dd 8372584: [Linux]: Replace reading proc to get thread CPU</span></p><p><span>time with clock_gettime</span></p></pre></div></div></div>
<p>The diffstat was interesting: <code>+96 insertions, -54 deletions</code>. The changeset adds a 55-line JMH benchmark, which means the production code itself is actually reduced.</p>
<h2 id="the-deleted-code"><a href="#the-deleted-code">The Deleted Code</a></h2>
<p>Here&#39;s what got removed from <code>os_linux.cpp</code>:</p>
<div><div><div><pre><p><span>static jlong user_thread_cpu_time(Thread *thread) {</span></p><p><span>  pid_t  tid = thread-&gt;osthread()-&gt;thread_id();</span></p><p><span>  char *s;</span></p><p><span>  char stat[2048];</span></p><p><span>  size_t statlen;</span></p><p><span>  char proc_name[64];</span></p><p><span>  int count;</span></p><p><span>  long sys_time, user_time;</span></p><p><span>  char cdummy;</span></p><p><span>  int idummy;</span></p><p><span>  long ldummy;</span></p><p><span>  FILE *fp;</span></p><p><span>  os::snprintf_checked(proc_name, 64, &#34;/proc/self/task/%d/stat&#34;, tid);</span></p><p><span>  fp = os::fopen(proc_name, &#34;r&#34;);</span></p><p><span>  if (fp == nullptr) return -1;</span></p><p><span>  statlen = fread(stat, 1, 2047, fp);</span></p><p><span>  stat[statlen] = &#39;\0&#39;;</span></p><p><span>  fclose(fp);</span></p><p><span>  // Skip pid and the command string. Note that we could be dealing with</span></p><p><span>  // weird command names, e.g. user could decide to rename java launcher</span></p><p><span>  // to &#34;java 1.4.2 :)&#34;, then the stat file would look like</span></p><p><span>  //                1234 (java 1.4.2 :)) R ... ...</span></p><p><span>  // We don&#39;t really need to know the command string, just find the last</span></p><p><span>  // occurrence of &#34;)&#34; and then start parsing from there. See bug 4726580.</span></p><p><span>  s = strrchr(stat, &#39;)&#39;);</span></p><p><span>  if (s == nullptr) return -1;</span></p><p><span>  // Skip blank chars</span></p><p><span>  do { s++; } while (s &amp;&amp; isspace((unsigned char) *s));</span></p><p><span>  count = sscanf(s,&#34;%c %d %d %d %d %d %lu %lu %lu %lu %lu %lu %lu&#34;,</span></p><p><span>                 &amp;cdummy, &amp;idummy, &amp;idummy, &amp;idummy, &amp;idummy, &amp;idummy,</span></p><p><span>                 &amp;ldummy, &amp;ldummy, &amp;ldummy, &amp;ldummy, &amp;ldummy,</span></p><p><span>                 &amp;user_time, &amp;sys_time);</span></p><p><span>  if (count != 13) return -1;</span></p><p><span>  return (jlong)user_time * (1000000000 / os::Posix::clock_tics_per_second());</span></p><p><span>}</span></p></pre></div></div></div>
<p>This was the implementation behind <code>ThreadMXBean.getCurrentThreadUserTime()</code>. To get the current thread&#39;s user CPU time, the old code was:</p>
<ol>
<li>Formatting a path to <code>/proc/self/task/&lt;tid&gt;/stat</code></li>
<li>Opening that file</li>
<li>Reading into a stack buffer</li>
<li>Parsing through a hostile format where the command name can contain parentheses (hence the <code>strrchr</code> for the last <code>)</code>)</li>
<li>Running <code>sscanf</code> to extract fields 13 and 14</li>
<li>Converting clock ticks to nanoseconds</li>
</ol>
<p>For comparison, here&#39;s what <code>getCurrentThreadCpuTime()</code> does and has always done:</p>
<div><div><div><pre><p><span>jlong os::current_thread_cpu_time() {</span></p><p><span>  return os::Linux::thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);</span></p><p><span>}</span></p><p><span>jlong os::Linux::thread_cpu_time(clockid_t clockid) {</span></p><p><span>  struct timespec tp;</span></p><p><span>  clock_gettime(clockid, &amp;tp);</span></p><p><span>  return (jlong)(tp.tv_sec * NANOSECS_PER_SEC + tp.tv_nsec);</span></p><p><span>}</span></p></pre></div></div></div>
<p>Just a single <code>clock_gettime()</code> call. There is no file I/O, no complex parsing and no buffer to manage.</p>
<h2 id="the-performance-gap"><a href="#the-performance-gap">The Performance Gap</a></h2>
<p>The <a href="https://bugs.openjdk.org/browse/JDK-8210452">original bug report</a>, filed back in 2018, quantified the difference:</p>
<blockquote>
<p>&#34;getCurrentThreadUserTime is 30x-400x slower than getCurrentThreadCpuTime&#34;</p>
</blockquote>
<p>The gap widens under concurrency. Why is <code>clock_gettime()</code> so much faster? Both approaches require kernel entry, but the difference is in what happens next.</p>
<p><strong>The <code>/proc</code> path:</strong></p>
<ul>
<li><code>open()</code> syscall</li>
<li>VFS dispatch + dentry lookup</li>
<li>procfs synthesizes file content at read time</li>
<li>kernel formats string into buffer</li>
<li><code>read()</code> syscall, copy to userspace</li>
<li>userspace <code>sscanf()</code> parsing</li>
<li><code>close()</code> syscall</li>
</ul>
<p><strong>The <code>clock_gettime(CLOCK_THREAD_CPUTIME_ID)</code> path:</strong></p>
<ul>
<li>single syscall → <code>posix_cpu_clock_get()</code> → <code>cpu_clock_sample()</code> → <code>task_sched_runtime()</code> → reads directly from <a href="https://github.com/torvalds/linux/blob/8449d3252c2603a51ffc7c36cb5bd94874378b7d/include/linux/sched.h#L586"><code>sched_entity</code></a></li>
</ul>
<p>The <code>/proc</code> path involves multiple syscalls, VFS machinery, string formatting kernel-side, and parsing userspace-side. The <code>clock_gettime()</code> path is one syscall with a direct function call chain.</p>
<p>Under concurrent load, the <code>/proc</code> approach also suffers from kernel lock contention. The <a href="https://bugs.openjdk.org/browse/JDK-8372584">bug report</a> notes:</p>
<blockquote>
<p>&#34;Reading proc is slow (hence why this procedure is put under the method slow_thread_cpu_time(...)) and may lead to noticeable spikes in case of contention for kernel resources.&#34;</p>
</blockquote>
<h2 id="why-two-implementations?"><a href="#why-two-implementations?">Why Two Implementations?</a></h2>
<p>So why didn&#39;t <code>getCurrentThreadUserTime()</code> just use <code>clock_gettime()</code> from the start?</p>
<p>The answer is (probably) POSIX. The standard mandates that <code>CLOCK_THREAD_CPUTIME_ID</code> returns total CPU time (user + system). There&#39;s no portable way to request user time only. Hence the <code>/proc</code>-based implementation.</p>
<p>The Linux port of OpenJDK isn&#39;t limited to what POSIX defines, it can use Linux-specific features. Let&#39;s see how.</p>
<h2 id="the-clockid-bit-hack"><a href="#the-clockid-bit-hack">The Clockid Bit Hack</a></h2>
<p>Linux kernels since 2.6.12 (released in 2005) <a href="https://github.com/torvalds/linux/blob/eee51b0ae5c52a77ed65ad59b55002d1397b40d5/include/linux/posix-timers_types.h#L10-L19">encode clock type information directly</a> into the <code>clockid_t</code> value. When you call <a href="https://man7.org/linux/man-pages/man3/pthread_getcpuclockid.3.html"><code>pthread_getcpuclockid()</code></a>, you get back a clockid with a specific bit pattern:</p>
<div><div><div><pre><p><span>Bit 2:    Thread vs process clock</span></p><p><span>Bits 1-0: Clock type</span></p><p><span>  00 = PROF</span></p><p><span>  01 = VIRT  (user time only)</span></p><p><span>  10 = SCHED (user + system, POSIX-compliant)</span></p><p><span>  11 = FD</span></p></pre></div></div></div>
<p>The remaining bits encode the target PID/TID. We’ll come back to that in the bonus section.</p>
<p>The POSIX-compliant <code>pthread_getcpuclockid()</code> returns a clockid with bits <code>10</code> (SCHED). But if you flip those low bits to <code>01</code> (VIRT), <code>clock_gettime()</code> will return user time only.</p>
<p>The new implementation:</p>
<div><div><div><pre><p><span>static bool get_thread_clockid(Thread* thread, clockid_t* clockid, bool total) {</span></p><p><span>  constexpr clockid_t CLOCK_TYPE_MASK = 3;</span></p><p><span>  constexpr clockid_t CPUCLOCK_VIRT = 1;</span></p><p><span>  int rc = pthread_getcpuclockid(thread-&gt;osthread()-&gt;pthread_id(), clockid);</span></p><p><span>  if (rc != 0) {</span></p><p><span>    // Thread may have terminated</span></p><p><span>    assert_status(rc == ESRCH, rc, &#34;pthread_getcpuclockid failed&#34;);</span></p><p><span>    return false;</span></p><p><span>  }</span></p><p><span>  if (!total) {</span></p><p><span>    // Flip to CPUCLOCK_VIRT for user-time-only</span></p><p><span>    *clockid = (*clockid &amp; ~CLOCK_TYPE_MASK) | CPUCLOCK_VIRT;</span></p><p><span>  }</span></p><p><span>  return true;</span></p><p><span>}</span></p><p><span>static jlong user_thread_cpu_time(Thread *thread) {</span></p><p><span>  clockid_t clockid;</span></p><p><span>  bool success = get_thread_clockid(thread, &amp;clockid, false);</span></p><p><span>  return success ? os::Linux::thread_cpu_time(clockid) : -1;</span></p><p><span>}</span></p></pre></div></div></div>
<p>And that&#39;s it. The new version has no file I/O, no buffer and certainly no <code>sscanf()</code> with thirteen format specifiers.</p>
<h2 id="profiling-time!"><a href="#profiling-time!">Profiling time!</a></h2>
<p>Let&#39;s have a look at how it performs in practice. For this exercise, I am taking the <a href="https://github.com/openjdk/jdk/commit/858d2e434dd#diff-0151b4192746117a72e6da834a5e97aaad57c4d99b0438ae7eba9c0002826f0a">JMH test included in the fix</a>, the only change is that I increased the number of threads from 1 to 16
and added a <code>main()</code> method for simple execution from an IDE:</p>
<div><div><div><pre><p><span>@State(Scope.Benchmark)</span></p><p><span>@Warmup(iterations = 2, time = 5)</span></p><p><span>@Measurement(iterations = 5, time = 5)</span></p><p><span>@BenchmarkMode(Mode.SampleTime)</span></p><p><span>@OutputTimeUnit(TimeUnit.MICROSECONDS)</span></p><p><span>@Threads(16)</span></p><p><span>@Fork(value = 1)</span></p><p><span>public class ThreadMXBeanBench {</span></p><p><span>    static final ThreadMXBean mxThreadBean = ManagementFactory.getThreadMXBean();</span></p><p><span>    static long user; // To avoid dead-code elimination</span></p><p><span>    @Benchmark</span></p><p><span>    public void getCurrentThreadUserTime() throws Throwable {</span></p><p><span>        user = mxThreadBean.getCurrentThreadUserTime();</span></p><p><span>    }</span></p><p><span>    public static void main(String[] args) throws RunnerException {</span></p><p><span>        Options opt = new OptionsBuilder()</span></p><p><span>                .include(ThreadMXBeanBench.class.getSimpleName())</span></p><p><span>                .build();</span></p><p><span>        new Runner(opt).run();</span></p><p><span>    }</span></p><p><span>}</span></p></pre></div></div></div>
<blockquote>
<p>Aside: This is a rather unscientific benchmark, I have other processes running on my desktop etc. Anyway, here is the setup: Ryzen 9950X, JDK main branch at commit <a href="https://github.com/openjdk/jdk/commit/8ab7d3b89f656e5c">8ab7d3b89f656e5c</a>. For the &#34;before&#34; case, I reverted the fix rather than checking out an older revision.</p>
</blockquote>
<p>Here is the result:</p>
<div><div><div><pre><p><span>Benchmark                                             Mode      Cnt     Score   Error  Units</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime          sample  8912714    11.186 ± 0.006  us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample              2.000          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample             10.272          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample             17.984          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample             20.832          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample             27.552          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample             56.768          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample             79.709          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample           1179.648          us/op</span></p></pre></div></div></div>
<p>We can see that a single invocation took 11 microseconds on average and the median was about 10 microseconds per invocation.</p>
<p>The CPU profile looks like this:</p>
<figure><div><p><img alt="CPU profile before the fix" src="https://blog.veitheller.de/images/blog/2026-01-13/before.svg" loading="lazy"/></p><figcaption>Click to zoom, open in a new tab for interactivity</figcaption></div></figure>
<p>The CPU profile confirms that each invocation of <code>getCurrentThreadUserTime()</code> does multiple syscalls. In fact, most of the CPU time
is spent in syscalls. We can see files being opened and closed. Closing alone results in multiple syscalls, including futex locks.</p>
<p>Let&#39;s see the benchmark result with the fix applied:</p>
<div><div><div><pre><p><span>Benchmark                                             Mode       Cnt     Score   Error  Units</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime          sample  11037102     0.279 ± 0.001  us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample               0.070          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample               0.310          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample               0.440          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample               0.530          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample               0.610          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample               1.030          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample               3.088          us/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample            1230.848          us/op</span></p></pre></div></div></div>
<p>The average went down from 11 microseconds to 279 nanos. This means the latency of the fixed version is 40x lower
than the old version. While this is not a 400x improvement, it&#39;s within the 30x - 400x range from the original report. Chances are
the delta would be higher with a different setup.
Let&#39;s have a look at the new profile:</p>
<figure><div><p><img alt="CPU profile after the fix" src="https://blog.veitheller.de/images/blog/2026-01-13/after.svg" loading="lazy"/></p><figcaption>Click to zoom, open in a new tab for interactivity</figcaption></div></figure>
<p>The profile is much cleaner. There is just a single syscall. If the profile is to be trusted then most of the time is spent in JVM, outside of the kernel.</p>
<h2 id="how-documented-is-this?"><a href="#how-documented-is-this?">How Documented Is This?</a></h2>
<p>Barely. The bit encoding is stable. It hasn&#39;t changed in 20 years, but you won&#39;t find it in the <a href="https://linux.die.net/man/3/clock_gettime"><code>clock_gettime(2)</code> man page</a>.
The closest thing to official documentation is the kernel source itself, in <a href="https://github.com/torvalds/linux/blob/4702f4eceb639b6af199151e352e570943619d98/kernel/time/posix-cpu-timers.c"><code>kernel/time/posix-cpu-timers.c</code></a> and the <a href="https://github.com/torvalds/linux/blob/eee51b0ae5c52a77ed65ad59b55002d1397b40d5/include/linux/posix-timers_types.h"><code>CPUCLOCK_*</code> macros</a>.</p>
<p>The kernel&#39;s policy is clear: <a href="https://lkml.org/lkml/2012/12/23/75">don&#39;t break userspace</a>.</p>
<figure><div><p><img alt="Linus on kernel stability: Don&#39;t break userspace" src="https://blog.veitheller.de/images/blog/2026-01-13/linus.webp" loading="lazy"/></p><figcaption>Linus&#39;s position on ABI stability is... unambiguous.</figcaption></div></figure>
<p>My take: If <a href="https://github.com/bminor/glibc/blob/bd569425330c6f5644c232b4b253e9ab905fcdba/sysdeps/unix/sysv/linux/kernel-posix-cpu-timers.h">glibc depends on it</a>, it&#39;s not going away.</p>
<h2 id="pushing-further"><a href="#pushing-further">Pushing Further</a></h2>
<p>When looking at profiler data from the &#39;after&#39; run, I spotted a further optimization opportunity: A good portion of the remaining syscall is spent inside a radix tree lookup. Have a look:</p>
<figure><div><p><img alt="Zoomed-in CPU profile showing radix tree lookup" src="https://blog.veitheller.de/images/blog/2026-01-13/radix.webp" loading="lazy"/></p><figcaption>Click to zoom</figcaption></div></figure>
<p>When the JVM calls <code>pthread_getcpuclockid()</code>, it receives a <code>clockid</code> that encodes the thread&#39;s ID. When this <code>clockid</code> is passed to <code>clock_gettime()</code>,
the kernel extracts the thread ID and performs a radix tree lookup to find the <a href="https://github.com/torvalds/linux/blob/8ec7c826d97b390879df2a03dfb035c70af86779/include/linux/pid.h#L57"><code>pid</code> structure</a> associated with that ID.</p>
<p>However, the Linux kernel has a fast-path. If the encoded PID in the <code>clockid</code> is 0, the kernel interprets this as &#34;the current thread&#34; and skips the radix tree lookup entirely, jumping to the current task&#39;s structure directly.</p>
<p>The OpenJDK fix currently obtains the specific TID, flips the bits, and passes it to <code>clock_gettime()</code>. This forces the kernel to take the &#34;generalized path&#34; (the radix tree lookup).</p>
<p>The <a href="https://github.com/torvalds/linux/blob/4702f4eceb639b6af199151e352e570943619d98/kernel/time/posix-cpu-timers.c#L57-L95">source code</a> looks like this:</p>
<div><div><div><pre><p><span>/*</span></p><p><span> * Functions for validating access to tasks.</span></p><p><span> */</span></p><p><span>static struct pid *pid_for_clock(const clockid_t clock, bool gettime)</span></p><p><span>{</span></p><p><span>[...]</span></p><p><span>  /*</span></p><p><span>  * If the encoded PID is 0, then the timer is targeted at current</span></p><p><span>  * or the process to which current belongs.</span></p><p><span>  */</span></p><p><span>  if (upid == 0)</span></p><p><span>      // the fast path: current task lookup, cheap</span></p><p><span>      return thread ? task_pid(current) : task_tgid(current);</span></p><p><span>  // the generalized path: radix tree lookup, more expensive</span></p><p><span>  pid = find_vpid(upid);</span></p><p><span>  [...]</span></p></pre></div></div></div>
<p>If the JVM constructed the entire <code>clockid</code> manually with PID=0 encoded (rather than obtaining the <code>clockid</code> via <code>pthread_getcpuclockid()</code>), the kernel could take the fast-path and avoid the radix tree lookup altogether.
The JVM already pokes bits in the <code>clockid</code>, so constructing it entirely from scratch wouldn&#39;t be a bigger leap compatibility-wise.</p>
<p>Let&#39;s try it!</p>
<p>First, a refresher on the <code>clockid</code> encoding. The <code>clockid</code> is constructed like this:</p>
<div><div><div><pre><p><span>clockid for TID=42, user-time-only:</span></p><p><span>  1111_1111_1111_1111_1111_1110_1010_1101</span></p><p><span>  └───────────────~42────────────────┘│└┘</span></p><p><span>                                      │ └─ 01 = VIRT (user time only)</span></p><p><span>                                      └─── 1 = per-thread</span></p></pre></div></div></div>
<p>For the current thread, we want PID=0 encoded, which gives <code>~0</code> in the upper bits:</p>
<div><div><div><pre><p><span>  1111_1111_1111_1111_1111_1111_1111_1101</span></p><p><span>  └─────────────── ~0 ───────────────┘│└┘</span></p><p><span>                                      │ └─ 01 = VIRT (user time only)</span></p><p><span>                                      └─── 1 = per-thread</span></p></pre></div></div></div>
<p>We can translate this into C++ as follows:</p>
<div><div><div><pre><p><span>// Linux Kernel internal bit encoding for dynamic CPU clocks:</span></p><p><span>// [31:3] : Bitwise NOT of the PID or TID (~0 for current thread)</span></p><p><span>// [2]    : 1 = Per-thread clock, 0 = Per-process clock</span></p><p><span>// [1:0]  : Clock type (0 = PROF, 1 = VIRT/User-only, 2 = SCHED)</span></p><p><span>static_assert(sizeof(clockid_t) == 4, &#34;Linux clockid_t must be 32-bit&#34;);</span></p><p><span>constexpr clockid_t CLOCK_CURRENT_THREAD_USERTIME = static_cast&lt;clockid_t&gt;(~0u &lt;&lt; 3 | 4 | 1);</span></p></pre></div></div></div>
<p>And then make a tiny teensy change to <code>user_thread_cpu_time()</code>:</p>
<div><div><div><pre><p><span>jlong os::current_thread_cpu_time(bool user_sys_cpu_time) {</span></p><p><span>  if (user_sys_cpu_time) {</span></p><p><span>    return os::Linux::thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);</span></p><p><span>  } else {</span></p><p><span>   - return user_thread_cpu_time(Thread::current());</span></p><p><span>   + return os::Linux::thread_cpu_time(CLOCK_CURRENT_THREAD_USERTIME);</span></p><p><span>  }</span></p></pre></div></div></div>
<p>The <a href="https://github.com/openjdk/jdk/compare/master...jerrinot:jdk:jh_faster_getCurrentThreadUserTime?diff=unified&amp;w">change above</a> is sufficient to make <code>getCurrentThreadUserTime()</code> use the fast-path in the kernel.</p>
<p>Given that we are in nanoseconds territory already, we tweak the test a bit:</p>
<ul>
<li>Increase the iteration and fork count</li>
<li>Use just a single thread to minimize noise</li>
<li>Switch to nanos</li>
</ul>
<p>The benchmark changes are meant to eliminate noise from the rest of my system and get a more precise measurement of the small delta we expect:</p>
<div><div><div><pre><p><span>@State(Scope.Benchmark)</span></p><p><span>@Warmup(iterations = 4, time = 5)</span></p><p><span>@Measurement(iterations = 10, time = 5)</span></p><p><span>@BenchmarkMode(Mode.SampleTime)</span></p><p><span>@OutputTimeUnit(TimeUnit.NANOSECONDS)</span></p><p><span>@Threads(1)</span></p><p><span>@Fork(value = 3)</span></p><p><span>public class ThreadMXBeanBench {</span></p><p><span>    static final ThreadMXBean mxThreadBean = ManagementFactory.getThreadMXBean();</span></p><p><span>    static long user; // To avoid dead-code elimination</span></p><p><span>    @Benchmark</span></p><p><span>    public void getCurrentThreadUserTime() throws Throwable {</span></p><p><span>        user = mxThreadBean.getCurrentThreadUserTime();</span></p><p><span>    }</span></p><p><span>    public static void main(String[] args) throws RunnerException {</span></p><p><span>        Options opt = new OptionsBuilder()</span></p><p><span>                .include(ThreadMXBeanBench.class.getSimpleName())</span></p><p><span>                .build();</span></p><p><span>        new Runner(opt).run();</span></p><p><span>    }</span></p><p><span>}</span></p></pre></div></div></div>
<p>The version currently in JDK main branch gives:</p>
<div><div><div><pre><p><span>Benchmark                                             Mode      Cnt       Score   Error  Units</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime          sample  4347067      81.746 ± 0.510  ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample               69.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample               80.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample               90.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample               90.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample               90.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample              230.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample             1980.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample           653312.000          ns/op</span></p></pre></div></div></div>
<p>With the manual <code>clockid</code> construction, which uses the kernel fast-path, we get:</p>
<div><div><div><pre><p><span>Benchmark                                             Mode      Cnt       Score   Error  Units</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime          sample  5081223      70.813 ± 0.325  ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.00    sample               59.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.50    sample               70.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.90    sample               70.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.95    sample               70.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.99    sample               80.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.999   sample              170.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p0.9999  sample             1830.000          ns/op</span></p><p><span>ThreadMXBeanBench.getCurrentThreadUserTime:p1.00    sample           425472.000          ns/op</span></p></pre></div></div></div>
<p>The average went down from 81.7 ns to 70.8 ns, so about a 13% improvement. The improvements are visible across all percentiles as well.
Is it worth the loss of clarity from constructing the <code>clockid</code> manually rather than using <code>pthread_getcpuclockid()</code>?
I am not entirely sure. The absolute gain is small and makes additional assumptions about kernel internals, including the size of <code>clockid_t</code>. On the other hand, it&#39;s still a gain without any downside in practice. <em>(famous last words...)</em></p>
<h2 id="browsing-for-gems"><a href="#browsing-for-gems">Browsing for Gems</a></h2>
<p>This is why I like browsing commits of large open source projects. A 40-line deletion eliminated a 400x performance gap. The fix required no new kernel features, just knowledge of a stable-but-obscure Linux ABI detail.</p>
<p>The lessons:</p>
<p><strong>Read the kernel source.</strong> POSIX tells you what&#39;s portable. The kernel source code tells you what&#39;s possible. Sometimes there&#39;s a 400x difference between the two. Whether it is worth exploiting is a different question.</p>
<p><strong>Check the old assumptions.</strong> The <code>/proc</code> parsing approach made sense when it was written, before anyone realized it could be exploited this way. Assumptions get baked into code. Revisiting them occasionally pays off.</p>
<p>The change landed on December 3, 2025. Just one day before the <a href="https://openjdk.org/projects/jdk/26/">JDK 26 feature freeze</a>. If you&#39;re using <code>ThreadMXBean.getCurrentThreadUserTime()</code>, JDK 26 (releasing March 2026) brings you a free 30-400x speedup!</p></div></article></div>
  </body>
</html>
