<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://e2eml.school/transformers.html">Original</a>
    <h1>Transformers from Scratch</h1>
    
    <div id="readability-page-1" class="page"><div id="main_content_wrap">
      <section id="main_content">
        <p>
          I procrastinated a deep dive into transformers for a few years.
          Finally the discomfort
          of not knowing what makes them tick grew too great for me.
          Here is that dive.
        </p>
        <p>
          Transformers were introduced in this 2017
          <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">
            paper</a>
          as a tool for sequence transduction—converting
          one sequence of symbols to another. The most popular
          examples of this are translation, as in English to German.
          It has also been modified to perform sequence
          completion—given a starting prompt, carry on in the
          same vein and style. They have quickly become an indispensible tool
          for research and product development in natural language processing.
        </p>
        <p>
          Before we start, just a heads-up.
          We&#39;re going to be talking a lot about matrix
          multiplications and touching on backpropagation
          (the algorithm for training the model), but you don&#39;t
          need to know any of it beforehand. We&#39;ll add the concepts we need
          one at a time, with explanation.
        </p>
        <p>
          This isn&#39;t a short journey, but I hope you&#39;ll be glad you came.
        </p>

        <ul>
          <li>
            ► <a target="_self" href="#one_hot">One-hot encoding</a>
          </li>
          <li>
            ► <a target="_self" href="#dot_product">Dot product</a>
          </li>
          <li>
            ► <a target="_self" href="#matrix_multiplication">Matrix multiplication</a>
          </li>
          <li>
            ► <a target="_self" href="#table_lookup">Matrix multiplication as a table lookup</a>
          </li>
          <li>
            ► <a target="_self" href="#markov_chain">First order sequence model</a>
          </li>
          <li>
            ► <a target="_self" href="#second_order">Second order sequence model</a>
          </li>
          <li>
            ► <a target="_self" href="#second_order_skips">Second order sequence model with skips</a>
          </li>
          <li>
            ► <a target="_self" href="#masking">Masking</a>
          </li>
          <li>
            ► <a target="_self" href="#rest_stop">Rest Stop and an Off Ramp</a>
          </li>
          <li>
            ► <a target="_self" href="#attention">Attention as matrix multiplication</a>
          </li>
          <li>
            ► <a target="_self" href="#second_order_matrix_mult">
              Second order sequence model as matrix multiplications</a>
          </li>
          <li>
            ► <a target="_self" href="#sequence_completion">Sequence completion</a>
          </li>
          <li>
            ► <a target="_self" href="#embeddings">Embeddings</a>
          </li>
          <li>
            ► <a target="_self" href="#positional_encoding">Positional encoding</a>
          </li>
          <li>
            ► <a target="_self" href="#deembeddings">De-embeddings</a>
          </li>
          <li>
            ► <a target="_self" href="#softmax">Softmax</a>
          </li>
          <li>
            ► <a target="_self" href="#multihead">Multi-head attention</a>
          </li>
          <li>
            ► <a target="_self" href="#attention_revisited">Single head attention revisited</a>
          </li>
          <li>
            ► <a target="_self" href="#skip_connections">Skip connection</a>
          </li>
          <li>
            ► <a target="_self" href="#layer_stack">Multiple layers</a>
          </li>
          <li>
            ► <a target="_self" href="#decoder">Decoder stack</a>
          </li>
          <li>
            ► <a target="_self" href="#encoder">Encoder stack</a>
          </li>
          <li>
            ► <a target="_self" href="#cross_attention">Cross-attention</a>
          </li>
          <li>
            ► <a target="_self" href="#tokenizing">Tokenizing</a>
          </li>
          <li>
            ► <a target="_self" href="#bpe">Byte pair encoding</a>
          </li>
          <li>
            ► <a target="_self" href="#audio_input">Audio input</a>
          </li>
          <li>
            ► <a target="_self" href="#resources">Resources and credits</a>
          </li>
        </ul>

        <h3 id="one_hot">One-hot encoding</h3>
        <p>
          In the beginning were the words. So very many words.
          Our first step is to convert all the words
          to numbers so we can do math on them.
        </p>
        <p>
          Imagine that our goal is to create the computer that responds
          to our voice commands. It’s our job to build the
          transformer that converts (or <strong>transduces</strong>)
          a sequence of sounds to a sequence of words.
        </p>
        <p>
          We start by choosing our <strong>vocabulary</strong>,
          the collection of symbols
          that we are going to be working with in each sequence.
          In our case, there will be two different sets of symbols,
          one for the input sequence to represent vocal sounds
          and one for the output sequence to represent words.
        </p>
        <p>
          For now, let&#39;s assume we&#39;re working with English.
          There are tens of thousands of words in the English language,
          and perhaps another few thousand to cover computer-specific
          terminology. That would give us a vocabulary size that is
          the better part of a hundred thousand. One way to convert
          words to numbers is to start counting at one and
          assign each word its
          own number. Then a sequence of words can be represented as a
          list of numbers.
        </p>
        <p>
          For example, consider a tiny language with a vocabulary size
          of three: <em>files</em>, <em>find</em>, and <em>my</em>.
          Each word could be swapped
          out for a number, perhaps
          <em>files</em> = 1, <em>find</em> = 2, and <em>my</em> = 3.
          Then the sentence &#34;Find my files&#34;, consisting of the word sequence
          [
          <em>find</em>,
          <em>my</em>,
          <em>files</em>
          ]
          could be represented instead as the sequence of numbers [2, 3, 1].
        </p>
        <p>
          This is a perfectly valid way to convert symbols to numbers,
          but it turns out that there&#39;s another format that&#39;s even
          easier for computers to work with, <strong>one-hot encoding</strong>.
          In one-hot encoding a symbol is represented by an array of
          mostly zeros, the same length of the vocabulary, with only a single
          element having a value of one. Each element in the array corresponds
          to a separate symbol. 
        </p>
        <p>
          Another way to think about one-hot encoding is that
          each word still gets assigned its own number, but now that number
          is an
          index to an array. Here is our example above, in one-hot
          notation.
        </p>
        <p>
          <img title="A one-hot encoded vocabulary" src="https://x.st/spinning-diagrams-with-css/images/transformers/one_hot_vocabulary.png" alt="A one-hot encoded vocabulary"/>
        </p>
        <p>
          So the sentence &#34;Find my files&#34; becomes a sequence of one-dimensional
          arrays,
          which, after you squeeze them together,
          starts to look like a two-dimensional array.
        </p>
        <p>
          <img title="A one-hot encoded sentence" src="https://x.st/spinning-diagrams-with-css/images/transformers/one_hot_sentence.png" alt="A one-hot encoded sentence"/>
        </p>
        <p>
          Heads-up, I&#39;ll be using the terms &#34;one-dimensional array&#34; and
          &#34;<strong>vector</strong>&#34; interchangeably. Likewise with
          &#34;two-dimensional array&#34; and &#34;<strong>matrix</strong>&#34;.
        </p>

        <h3 id="dot_product">Dot product</h3>
        <p>
          One really useful thing about the one-hot representation is that
          it lets us compute
          <a href="https://en.wikipedia.org/wiki/Dot_product">dot products</a>.
          These are also known by other
          intimidating names like inner product and scalar product.
          To get the dot product of two vectors, multiply their
          corresponding elements, then add the results.
        </p>
        <p>
          <img title="Dot product illustration" src="https://x.st/spinning-diagrams-with-css/images/transformers/dot_product.png" alt="Dot product illustration"/>
        </p>
        <p>
          Dot products are especially useful when we&#39;re working with our
          one-hot word representations. The dot product of any one-hot
          vector with itself is one.
        </p>
        <p>
          <img title="Dot product of matching vectors" src="https://x.st/spinning-diagrams-with-css/images/transformers/match.png" alt="Dot product of matching vectors"/>
        </p>
        <p>
          And the dot product of any one-hot vector with any other one-hot
          vector is zero.
        </p>
        <p>
          <img title="Dot product of non-matching vectors" src="https://x.st/spinning-diagrams-with-css/images/transformers/non_match.png" alt="Dot product of non-matching vectors"/>
        </p>
        <p>
          The previous two examples show how dot products
          can be used to measure similarity. As another example, 
          consider a vector of values that represents a combination of words
          with varying weights.
          A one-hot encoded word can be compared against it with the
          dot product
          to show how strongly that word is represented.
        </p>
        <p>
          <img title="Dot product gives the similarity between two vectors" src="https://x.st/spinning-diagrams-with-css/images/transformers/similarity.png" alt="Dot product gives the similarity between two vectors"/>
        </p>

        <h3 id="matrix_multiplication">Matrix multiplication</h3>
        <p>
          The dot product is the building block of matrix multiplication,
          a very particular way to combine a pair of two-dimensional arrays.
          We&#39;ll call the first of these matrices <em>A</em> and the second
          one <em>B</em>.
          In the simplest case, when <em>A</em> has only one row and
          <em>B</em> has only one column, the result of matrix multiplication
          is the dot product of the two.
        </p>
        <p>
          <img title="multiplication of a single row matrix and a single column matrix" src="https://x.st/spinning-diagrams-with-css/images/transformers/matrix_mult_one_row_one_col.png" alt="multiplication of a single row matrix and a single column matrix"/>
        </p>
        <p>
          Notice how the number of columns in <em>A</em> and the number of
          rows in <em>B</em> needs to be the same for the two arrays
          to match up and for the dot product to work out.
        </p>
        <p>
          When <em>A</em> and <em>B</em> start to grow, matrix multiplication
          starts to get trippy. To handle more than one row in <em>A</em>,
          take the dot product of <em>B</em> with each row separately.
          The answer will have as many rows as <em>A</em> does.
        </p>
        <p>
          <img title="multiplication of a two row matrix and a single column matrix" src="https://x.st/spinning-diagrams-with-css/images/transformers/matrix_mult_two_row_one_col.png" alt="multiplication of a two row matrix and a single column matrix"/>
        </p>
        <p>
          When <em>B</em> takes on more columns, take the dot product of
          each column with <em>A</em> and stack the results in successive
          columns.
        </p>
        <p>
          <img title="multiplication of a one row matrix and a two column matrix" src="https://x.st/spinning-diagrams-with-css/images/transformers/matrix_mult_one_row_two_col.png" alt="multiplication of a one row matrix and a two column matrix"/>
        </p>
        <p>
         Now we can extend this to mutliplying any two matrices, as long as
         the number of columns in <em>A</em> is the same as the number of
         rows in <em>B</em>. The result will have the same 
         number of rows as <em>A</em> and the same number of columns as
         <em>B</em>.
        </p>
        <p>
          <img title="multiplication of a three row matrix and a two column matrix" src="https://x.st/spinning-diagrams-with-css/images/transformers/matrix_mult_three_row_two_col.png" alt="multiplication of a one three matrix and a two column matrix"/>
        </p>
        <p>
          If this is the first time you&#39;re seeing this, it might
          feel needlessly complex, but I promise it pays off later.
        </p>

        <h4 id="table_lookup">Matrix multiplication as a table lookup</h4>
        <p>
          Notice how matrix multiplication acts as a lookup table here.
          Our <em>A</em> matrix is made up of a stack of one-hot vectors.
          They have ones in the first column, the fourth column,
          and the third column, respectively. When we work through the
          matrix multiplication, this serves to pull out the first row,
          the fourth row, and the third row of the <em>B</em> matrix,
          in that order. This trick of using a one-hot vector to pull
          out a particular row of a matrix is at the core of how
          transformers work.
        </p>

        <h3 id="markov_chain">First order sequence model</h3>
        <p>
          We can set aside matrices for a minute and get back to what
          we really care about, sequences of words. Imagine that as we
          start to develop our natural language computer interface
          we want to handle just three possible commands:
          </p><ul>
            <li>
              <em>Show me my directories please</em>.
            </li>
            <li>
              <em>Show me my files please</em>.
            </li>
            <li>
              <em>Show me my photos please</em>.
            </li>
          </ul>
          Our vocabulary size is now seven:<!--
        <p>
          To build an intuition for what self-attention and cross-attention
          are doing, it's helpful to pluck out the attention matrix,
          softmax(<em>Q K^T</em>), before it's used to filter the values.
          It shows which elements in the sequence are attending to which
          other elements and shines light on the core mechanism
          of transformers. In
          <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">
          The Annotated Transformer</a>
          the attention matrices for a translation example on a trained
          transformer are pulled out and visualized.
        </p>
        <p>
          This particular transformer was trained on a sentence translation
          task, using pairs of English and German sentences to make an
          English-to-German sentence translator. The example they illustrate
          shows the translation of the English sentence 
          "The log file
          can be sent secretly with email or FTP to a
          specified receiver." to the German sentence
          "Die Protokolldatei kann heimlich per E-Mail oder
          FTP an einen bestimmten Empfänger gesendet werden."
          In a step called <strong>tokenization</strong>, both the English
          and German sentences are broken up into words and pieces of words
          in such a way that the transformer can handle them efficiently.
          (More on this in a moment.) The tokenized sentences substitute
          underscores for spaces and look like
          [<em>▁The,▁log,▁file,
          ▁can,▁be,▁sent,▁secret,ly,▁with,▁email,▁or,▁FTP,▁to,▁a,
          ▁specified,▁receiver,.</em>] and
          [<em>▁Die,▁Protokoll,datei,▁kann,▁,heimlich,▁per,▁E,-,Mail,▁oder,
          ▁FTP,▁an,▁einen,▁bestimmte,n,▁Empfänger,▁gesendet,▁werden,.</em>].
        </p>
        -->

        
        
        
        
        
        
        
        
      
        
        
        
        
        

        
        
        
        
        
        
        
        

        
        

        
        

        
      </section>
    </div></div>
  </body>
</html>
