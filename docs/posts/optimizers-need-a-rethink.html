<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://typesanitizer.com/blog/rethink-optimizers.html">Original</a>
    <h1>Optimizers need a rethink</h1>
    
    <div id="readability-page-1" class="page"><article>
          <section>
	          
<p>Intended audience: People working on, interested in, or adjacent to compiler and/or database development, and people who’ve been bitten by compiler/database optimizer failures in the past.</p>
<p>Based on my experience in compiler development,
reading about new optimizations coming out,
as well as more recently
trying to debug slow queries in Postgres,
it seems like optimizers usually tend
to have the following characteristics in common:</p>
<ol type="1">
<li><p><strong>Contingent criticality</strong>: The optimizer’s behavior
is in some contexts load-bearing and has little margin for error
(e.g. missing an optimization).</p>
<p>For example, in GCed languages, there are situations
where you do not want any allocations to be triggered.
For this, you want to make sure that the optimizer unboxes any
boxing operations if the language boxes ~everything
by default.</p>
<p>In the context of more low-level languages like C++,
depending on the code in question,
one might care about the compiler unrolling loops properly,
using (or avoiding) branchless instructions,
the right set of SIMD instructions,
and so on.</p>
<p>For languages using ref-counting combined
with copy-on-write container types,
lifetime contraction can even
lead to changes in algorithmic complexity,
not just constant factors.</p></li>
<li><p><strong>Limited control</strong>: The knobs to steer the optimizer are limited.</p>
<p>Usually, these take the form of compiler pragmas or
specialized builtins/intrinsics, and occasionally
language features. Some examples:</p>
<ul>
<li>Rust has a <a href="https://doc.rust-lang.org/beta/std/intrinsics/fn.black_box.html">black_box</a>
intrinsic to conveniently put optimizer barriers for benchmarking.</li>
<li>Many languages have ways of marking inlining information.
For example, the Swift stdlib currently has 890 uses
of <a href="https://sourcegraph.com/search?q=context:global+repo:swiftlang/swift+file:%5Estdlib/+%40inline%28__always%29+lang:Swift+count:all&amp;patternType=keyword&amp;sm=0"><code>@inline(__always)</code></a>
and 3.1K uses of <a href="https://sourcegraph.com/search?q=context:global+repo:swiftlang/swift+file:%5Estdlib/+%40inlinable+lang:Swift+count:all&amp;patternType=keyword&amp;sm=0"><code>@inlinable</code></a>.</li>
<li>MySQL has <a href="https://dev.mysql.com/doc/refman/8.4/en/optimizer-hints.html">optimizer hints</a>
which are written as magic comments. Hints that cannot be applied
are silently ignored without warning.</li>
</ul>
<p><strong>EDIT(2024 Oct 28)</strong>:
In some cases, languages have features which enforce
performance-related properties at the semantic checking layer,
hence, granting more control that integrates with
semantic checks instead of relying on the optimizer:</p>
<ul>
<li>D has first-class support for marking functions as <a href="https://dlang.org/spec/function.html#nogc-functions">“no GC”</a>.</li>
<li>C# has support for <code>ref</code> parameters, and types,
with a <a href="https://em-tg.github.io/csborrow/">limited form of borrow-checking</a>.</li>
</ul></li>
<li><p><strong>Limited testability</strong>: The facilities to check the optimizer’s output
are meant for debugging only,
and are not amenable to standard testing techniques.</p>
<p>Typically, this takes the form of viewing the compiler output,
which takes the form of assembly for low-level languages
and some intermediate representation (IR) for higher-level languages.
However, there is no way to feed this output back into
the tool to check that a new version of the code
(or the same code being compiled against a newer tool version)
has the ~same optimized output.</p>
<p>One interesting counter-example here is tail call elimination which
has support for explicit checking in many compilers:
Clang and GCC have <code>musttail</code>, Dotty (Scala 3) has <code>tailrec</code>,
OCamlc has <code>@tailcall</code>.</p></li>
<li><p><strong>Limited visibility</strong>: Sometimes, the optimizer seems to “have a mind of its own”.</p>
<p>For example, in the Postgres case recently,
we have been baffled at the index choice being made by Postgres,
preferring a very low selectivity index
over the primary key index.<span><label for="sn-0"></label><span>Debugging the situation is made tricker by the fact that the index choice made in prod doesn’t match the index choice made in dev. Yes, we have checked that the index statistics are up-to-date in prod.</span></span></p>
<p>If you go through compiler bug trackers,
it’s not hard to come across situations
where optimizations do not trigger
when users expect them to.
Sometimes this is down to incorrect user expectations,
sometimes due to optimizer bugs,
and sometimes due to “insufficient smarts” (more on that shortly).</p></li>
</ol>
<!-- Interestingly enough, these black box properties are very similar -->
<!-- to those of a more recent innovation that you're probably familiar with: -->
<!-- Large Language Models (LLMs). Unlike LLMs though, -->
<!-- optimizers are deterministic, so that's nice. -->
<p>One last point worth mentioning which is more about optimizations
and not about optimizers per se:
optimizations tend to interfere with debugging,
which encourages the use of different optimization levels based on
context (local development vs production)
for ahead-of-time (AOT) compilation.
A counter-example to this is Go,
which does not conventionally have separate dev and release builds.</p>
<p>For high-level languages, at least some level of optimization
is required for any kind of reasonable performance,
which, depending on the language,
means that at least some debugging information must be erased.
<a href="https://en.wikipedia.org/wiki/Tail_call">Tail-call elimination</a>
is a common example of this,
and is sometimes mandatory for languages like Scheme.
I believe the <a href="https://xnning.github.io/papers/perceus.pdf">Functional-But-In-Place technique</a>
used in Roc, Lean and Koka is likely to have
the same problem.</p>
<h2 id="whats-wrong-with-the-status-quo">What’s wrong with the status quo</h2>
<p>If you look at in the past section,
there seems to be a contradiction between
the first point on contingent criticality
and the other points. If some system is critical
to program behavior, surely we need good mechanisms
to be able to inspect, understand, control and check it,
right?</p>
<blockquote>
<p>Software engineering is programming integrated over time</p>
<p>– <a href="https://www.linkedin.com/in/tituswinters/">Titus Winters</a></p>
</blockquote>
<p>In a similar vibe, here’s a longer quote in the context of
DB query planners from <a href="https://nelhage.com/">Nelson Elhage</a>’s
<a href="https://blog.nelhage.com/post/some-opinionated-sql-takes/#i-dislike-query-planners">Some opinionated thoughts on SQL databases</a>:</p>
<blockquote>
<p><strong>I dislike query planners</strong></p>
<p>[..] For an online application with consistent data access patterns
and high throughput, performance is part of the database’s interface;
if a database continues serving queries
but with substantially worse latency, that’s an incident!</p>
<p>The query planner is the antithesis of predictability;
<em>most</em> of the time it will choose right,
but it’s hard to know when it won’t or what the impact will be if it doesn’t.
[..]
It’s not a question of whether or not their planner is smart enough
[..]
At some point, transparency, explicitness,
and predictability are really important values.</p>
</blockquote>
<p>Connecting the two quotes is the theme of maintainability:
“transparency, explicitness and predictability” are important
for maintainability, and hence it’s critical that our tools
support us in embedding these values into the software we write.</p>
<p>Applying the point to optimizers and optimizations,
if an optimization is critical to overall system function,
and if the system needs to be maintained over a long time,
the programmer needs to be able to:</p>
<ol type="1">
<li><p>Have a good mental model of what the optimizer can and cannot do.</p></li>
<li><p>Easily debug situations when an optimization is missed.</p></li>
<li><p>Easily steer the optimizer in the desired direction
if it cannot figure out something by itself.
If the optimizer fails to follow the programmer’s guidance,
it must issue an error.
Optionally, the optimizer may allow the programmer to issue
“weak guidance” which downgrades optimization misses
to warnings instead of errors.</p></li>
<li><p>Easily write regression tests over the expected behavior
of the optimizer. These tests should be able to have the
other properties that are typically expected of good tests.<span><label for="sn-1"></label><span>A non-exhaustive list of properties the test should satisfy: (1) correctness - the test should fail if the necessary optimization doesn’t fire (2) robustness - the test shouldn’t fail due to irrelevant factors (3) debuggability - failures should be easy to reproduce and debug (4) automation - the test should be easily maintained in version control and runnable in typical CI environments (5) performance - the test should run reasonably quickly.</span></span></p></li>
</ol>
<h2 id="where-do-we-go-from-here">Where do we go from here</h2>
<p>Optimizers, including their tooling and documentation,
need to evolve to support the 4 properties outlined above.<span><label for="sn-2"></label><span>Since this is the internet, perhaps I should add the obligatory disclaimer: I’m not asking people to do this work for free.</span></span></p>
<h3 id="helping-users-build-a-solid-grounded-mental-model">Helping users build a solid, grounded mental model</h3>
<p>Optimizers should have clear docs
on the optimizations they perform,
as well as the order the optimizations
may be applied in.</p>
<p>Positive examples of optimizations kicking in should
be present in the docs and machine-checked as doctests
to avoid regressions.</p>
<p>Perhaps even more important are negative examples of optimizations
not being automatically triggered.
These should have explanations on why an optimization is not triggered
in a specific situation,
and ideally be based on misunderstandings from users
as reflected in the issue tracker.
Finally, negative examples should ideally have
some recommendations on how the user can guide the optimizer
to do what they want, if that makes sense in the given context
(e.g. the optimization must not introduce unsoundness
in case some assumptions are not upheld).</p>
<h3 id="evolving-tooling-for-debugging-optimization-misses">Evolving tooling for debugging optimization misses</h3>
<p>LLVM supports an interesting feature called
<a href="https://llvm.org/docs/Remarks.html">Optimization Remarks</a>
– these remarks track whether an optimization was performed or missed.
Clang support recording remarks using <code>-fsave-optimization-record</code>
and Rustc supports <code>-Zremark-dir=&lt;blah&gt;</code>.
There are also some tools
(<a href="https://llvm.org/docs/Remarks.html#opt-viewer">opt-viewer.py</a>,
<a href="https://github.com/OfekShilon/optview2">optview2</a>)
to help view and understand the output.</p>
<p>Other optimizers can potentially take inspiration from this approach
to do something similar.</p>
<p>As a baseline, compilers using LLVM should expose a way
to get the same data out.
Compilers doing optimizations on their own IRs could expose their own optimization records.
For example, Swift has <a href="https://sourcegraph.com/github.com/swiftlang/swift@4a29db51e19d8d020e0021e94d4abaac4f3c3516/-/blob/lib/DriverTool/sil_opt_main.cpp?L495-529">SIL optimization remarks</a>
for its own intermediate language.</p>
<p>In the context of databases,
I think a similar approach can be used,
perhaps at the level of relational operators,
with the added caveat that all other run time information
that was utilized as part of the optimization decisions
ought to be recorded as well.</p>
<p>It would be valuable to integrate this tooling
with editor extensions such as LSPs for ease of use.
Increased use will likely necessitate tooling improvements,
further driving use, potentially creating a virtuous cycle
if maintainer bandwidth can be effectively managed.</p>
<p>Lastly, users need to be easily able to share missed optimization reports
with optimizer developers. In my experience,
the biggest challenge for this is that it can
be quite time-consuming for less savvy users
to attempt to minimize and anonymize code,
especially when multiple functions get involved.
It would be valuable to integrate code anonymization<span><label for="sn-3"></label><span>By “code anonymization”, I mean things like bulk renaming identifiers,
stripping comments and garbling string/byte/character literals,
so that essentially ~only the control flow structure is preserved.
Yes, I recognize there are language features which potentially
make this more complex, such as reflection. However, anonymization
for optimization purposes doesn’t need to support 100% of the language’s
feature set.</span></span>
into bug-reporting tooling to make this easier for users.</p>
<h3 id="guidance-over-hints">Guidance over hints</h3>
<p>If you’re going through the effort of debugging a performance issue,
and trying to figure out how to provide some assistance to the optimizer,
chances are you do not really want the best thing you can do
to just be a “hint” which the optimizer is free to silently ignore.</p>
<p>Users need to be able to provide guidance to the optimizer
which the optimizer cannot ignore without loudly signalling
that it cannot follow the guidance.<span><label for="sn-4"></label><span><strong>EDIT(2024 Oct 28)</strong>: For example, as in the case of D and C# as documented before, one can have extra checks during semantic analysis which enforce performance-related properties even before the code gets to the optimizer. This is a reasonable design choice for certain common properties like “no GC here”, but it is not sufficient as a full solution for the various performance-related aspects of a program that people care about.</span></span> This may be an error
or a warning depending on what makes sense in a given context.<span><label for="sn-5"></label><span><strong>EDIT(2024 Oct 28)</strong>: For example, for code that is dynamically compiled on end-user devices, such as JavaScript and WebAssembly in a browser context, the browser engine might want to only allow a “warning” level.</span></span></p>
<p>One might counter: wouldn’t this make code more brittle
as new versions of the optimizer are released?
For example, let’s say your dependency X is using some
optimization guidance. You upgrade your toolchain.
Suddenly, X stops compiling because an optimization didn’t fire.</p>
<p>There are a few answers to this.</p>
<ol type="1">
<li>You should be auditing your dependencies for optimization guidance.
For example, it could be required that you need to specify an
<code>&#34;optimization-guidance&#34;</code> key-value pair in the package manifest.
This could be surfaced in auto-generated docs so that downstream consumers
better understand the risk of regressions on toolchain upgrades.</li>
<li>For open source dependencies, you can fork the dependency anyways
to remove the guidance if you think the guidance is not important.
Optionally, the package manifest could allow you to ignore
guidance violations in your dependencies (or downgrade them
to warnings).</li>
<li>Missed optimization bugs are also bugs; making it easy to uncover
and report them in a timely manner is a worthwhile endeavor.
Like other bugs, their severity needs to be analyzed
based on the surrounding context
instead of being
automatically assumed to be less important
than other kinds of bugs,
just because the program’s dynamic semantics are not affected.
As your compiler/database evolve, there may be regressions.
Silently ignoring regressions is not a solid strategy
for improving tooling.</li>
</ol>
<p>In the context of databases specifically, it might make sense
to add support for a query language more explicit than SQL
instead of simply adding support for more magic comments.<span><label for="sn-6"></label><span>Loosely related but a good read: Jamie Brandon’s <a href="https://www.scattered-thoughts.net/writing/against-sql">Against SQL</a>.</span></span>
<strong>EDIT(Oct 28 2024)</strong>: This is not to imply that we should
get rid of SQL or get rid of query planning entirely.
Rather, more explicit planning would be an additional tool
in database user’s toolbelt.</p>
<h3 id="optimizer-regression-testing-infrastructure">Optimizer regression testing infrastructure</h3>
<p>You encounter a bug in production where the throughput
of some performance sensitive code is much slower than
it ought to be.
You go read the docs and understand how the optimizer works.
You debug the issue and narrow it down to a missed optimization bug.
You report the issue and it is fixed by some engineer;
they add a regression test with a minimal example.
You verify the fix works for your not-so-minimal code,
upgrade the tool version,
post your victory in the team’s Slack channel
and relax back in your chair feeling smug.</p>
<p>It would be a real shame if you had to go through the same
loop after a few releases again, right?
The smugness would change to annoyance very quickly.</p>
<p>In situations where the optimization is done purely
based on compile time information,
adding guidance is sufficient as a test;
if the guidance is violated, the build will fail.
However, this approach has the downside of being inflexible:
every new guidance requires a toolchain change.
This might make sense for more common things such
as avoiding allocations and loop unrolling.
However, for more specialized use cases, such as for
checking specific instructions, it would be valuable
to be able to make assertions on the optimized code
(could be IR or assembly) from test code.<span><label for="sn-7"></label><span>Yes, I recognize this introduces new challenges around IR design, API design and maintaining compatibility across optimizer versions. I don’t have solutions to all of them right now; my point is that this seems like a problem worth tackling.</span></span></p>
<p>EDIT(2024/10/24): <a href="https://github.com/brendanzab">Brendan Zabarauskas</a> reminded me that there
is a Glasgow Haskell Compiler (GHC) plugin for
<a href="https://hackage.haskell.org/package/inspection-testing">inspection testing</a>
which allows making high-level assertions around
allocations, checking usages of a particular function, etc.
(<a href="https://github.com/haskell/text/pull/337">usage example</a>)
Thanks Brendan!</p>
<p>There are also situations where optimization
is not based purely on compile time information:
interpreters and just-in-time (JIT) compilers.</p>
<p>For those cases, one needs to be able to write
tests that assert properties of the generated code
across different classes of inputs.</p>
<p>For example, let’s say I want to add optimization
guidance to a database query stating that a specific index Y
should be used for an index-only scan for a given table T.
The underlying assumption is that using the index Y
is always going to be better than using other indexes for T.
However, this assumption may be violated based
on index statistics, or when my colleague (or future self)
adds another index Z which is actually a better fit.</p>
<p>There are two ways for going about capturing this
assumption using tests depending on the strength of the guidance:</p>
<ol type="1">
<li>If I’m using strong guidance where I force the query planner to use Y, I can:
<ul>
<li>Add a test across different index statistics<span><label for="sn-8"></label><span>These would likely have to be property-based tests for effectiveness, or perhaps even <a href="https://en.wikipedia.org/wiki/Abstract_interpretation">abstract interpretation</a>.</span></span>
which show that query plans using Y are cheaper
than alternate choices
(e.g. by leaving the choice to the optimizer).
This test should fail when Z is added.</li>
</ul></li>
<li>If I’m using weak guidance where the query planner issues a warning if it doesn’t use Y, I can:
<ul>
<li>Add a test for the relevant set of input statistics
which assert that the query planner always picks Y
when the stats fall in the expected range of operation.</li>
<li>Wire up weak guidance violations to my alerting infra.</li>
<li>Set up an alert for checking that the actual statistics
at run time fall in the operating range being used in the test.</li>
</ul></li>
</ol>
<p>Overall, while I think this is a hard problem to solve,
and there’s unlikely to be a one-size-fits-all solution
in terms of testing APIs,
it seems like a problem worth solving.</p>
<h2 id="closing-thoughts">Closing thoughts</h2>
<p>You might have heard of <a href="https://proebsting.cs.arizona.edu/law.html">Proebstring’s Law</a>:</p>
<blockquote>
<p>compiler optimization advances double computing power every 18 years</p>
</blockquote>
<p>Turns out, things are actually worse than that - depending on what exactly you look at, that number might as well be 36 years or worse. Here are just some of the articles and papers that I found:</p>
<ul>
<li>2022: <a href="https://zeux.io/2022/01/08/on-proebstings-law/">On Proebsting’s Law</a>, Arseny Kapoulkine</li>
<li>2009: <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=8ac77ff99e6ed5db06327ef87617865b3b805519">Benefits of Compiler Optimization</a> (PDF), Nicholas FitzRoy-Dale</li>
<li>2001: <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=0a2b1aa8bb63fb545f7f41233e5d6c0206486ccc">On Proebsting’s Law</a> (PDF), Kevin Scott</li>
</ul>
<p>At the end, Proebsting concludes:</p>
<blockquote>
<p>Perhaps this means Programming Language Research should be concentrating on something other than optimizations. Perhaps programmer productivity is a more fruitful arena.</p>
</blockquote>
<p>I think there’s at least some amount of opportunity
in exploring the intersection of programmer productivity and optimizations
– by making optimizers more understandable, reliable and predictable
and designing them to also work together with programmers
instead of just silently behind the scenes.</p>



<!-- [^x]: Somewhat ironically, Make actually makes this easier compared to other more modern build systems which have more bells and whistles. -->
          </section>
        </article></div>
  </body>
</html>
