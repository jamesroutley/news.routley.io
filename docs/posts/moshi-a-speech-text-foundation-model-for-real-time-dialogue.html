<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/kyutai-labs/moshi">Original</a>
    <h1>Moshi: A speech-text foundation model for real time dialogue</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kyutai-labs/moshi/workflows/precommit/badge.svg"><img src="https://github.com/kyutai-labs/moshi/workflows/precommit/badge.svg" alt="precommit badge"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/kyutai-labs/moshi/workflows/Rust%20CI/badge.svg"><img src="https://github.com/kyutai-labs/moshi/workflows/Rust%20CI/badge.svg" alt="rust ci badge"/></a></p>
<p dir="auto"><a href="https://kyutai.org/Moshi.pdf" rel="nofollow">[Read the paper]</a> <a href="https://moshi.chat" rel="nofollow">[Demo]</a> <a href="https://huggingface.co/collections/kyutai/moshi-v01-release-66eaeaf3302bef6bd9ad7acd" rel="nofollow">[Hugging Face]</a></p>
<p dir="auto"><a href="https://kyutai.org/Moshi.pdf" rel="nofollow">Moshi</a> is a speech-text foundation model and <strong>full-duplex</strong> spoken dialogue framework.
It uses <a href="https://kyutai.org/Moshi.pdf" rel="nofollow">Mimi</a>, a state-of-the-art streaming neural audio codec. Mimi processes 24 kHz audio, down to a 12.5 Hz representation
with a bandwidth of 1.1 kbps, in a fully streaming manner (latency of 80ms, the frame size),
yet performs better than existing, non-streaming, codec like
<a href="https://github.com/ZhangXInFD/SpeechTokenizer">SpeechTokenizer</a> (50 Hz, 4kbps), or <a href="https://github.com/haoheliu/SemantiCodec-inference">SemantiCodec</a> (50 Hz, 1.3kbps).</p>
<p dir="auto">Moshi models <strong>two streams of audio</strong>: one corresponds to Moshi, and the other one to the user.
At inference, the stream from the user is taken from the audio input,
and the one for Moshi is sampled from the model&#39;s output. Along these two audio streams, Moshi predicts text tokens corresponding to its own speech, its <strong>inner monologue</strong>,
which greatly improves the quality of its generation. A small Depth Transformer models inter codebook dependencies for a given time step,
while a large, 7B parameter Temporal Transformer models the temporal dependencies. Moshi achieves a theoretical latency
of 160ms (80ms for the frame size of Mimi + 80ms of acoustic delay), with a practical overall latency as low as 200ms on an L4 GPU.</p>
<p dir="auto"><a href="https://moshi.chat" rel="nofollow">Talk to Moshi</a> now on our live demo.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://vishnubharathi.codes/kyutai-labs/moshi/blob/main/moshi.png"><img src="https://vishnubharathi.codes/kyutai-labs/moshi/raw/main/moshi.png" alt="Schema representing the structure of Moshi. Moshi models two streams of audio:
    one corresponds to Moshi, and the other one to the user. At inference, the audio stream of the user is taken from the audio input, and the audio stream for Moshi is sampled from the model&#39;s output. Along that, Moshi predicts text tokens corresponding to its own speech for improved accuracy. A small Depth Transformer models inter codebook dependencies for a given step." width="650px"/></a></p>
<p dir="auto">Mimi builds on previous neural audio codecs such as <a href="https://arxiv.org/abs/2107.03312" rel="nofollow">SoundStream</a>
and <a href="https://github.com/facebookresearch/encodec">EnCodec</a>, adding a Transformer both in the encoder and decoder,
and adapting the strides to match an overall frame rate of 12.5 Hz. This allows Mimi to get closer to the
average frame rate of text tokens (~3-4 Hz), and limit the number of autoregressive steps in Moshi.
Similarly to SpeechTokenizer, Mimi uses a distillation loss so that the first codebook tokens match
a self-supervised representation from <a href="https://arxiv.org/abs/2110.13900" rel="nofollow">WavLM</a>, which allows modeling semantic and acoustic information with a single model. Interestingly, while Mimi is fully causal and streaming, it learns to match sufficiently well the non-causal
representation from WavLM, without introducing any delays. Finally, and similarly to <a href="https://arxiv.org/pdf/2210.14090" rel="nofollow">EBEN</a>,
Mimi uses <strong>only an adversarial training loss</strong>, along with feature matching, showing strong improvements in terms of
subjective quality despite its low bitrate.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://vishnubharathi.codes/kyutai-labs/moshi/blob/main/mimi.png"><img src="https://vishnubharathi.codes/kyutai-labs/moshi/raw/main/mimi.png" alt="Schema representing the structure of Mimi, our proposed neural codec. Mimi contains a Transformer
in both its encoder and decoded, and achieves a frame rate closer to that of text tokens. This allows us to reduce
the number of auto-regressive steps taken by Moshi, thus reducing the latency of the model." width="800px"/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Organisation of the repository</h2><a id="user-content-organisation-of-the-repository" aria-label="Permalink: Organisation of the repository" href="#organisation-of-the-repository"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">There are three separate versions of the moshi inference stack in this repo.</p>
<ul dir="auto">
<li>The Python version using PyTorch is in the <a href="https://vishnubharathi.codes/kyutai-labs/moshi/blob/main/moshi"><code>moshi/</code></a> directory.</li>
<li>The Python version using MLX for M series Macs is in the <a href="https://vishnubharathi.codes/kyutai-labs/moshi/blob/main/moshi_mlx"><code>moshi_mlx/</code></a> directory.</li>
<li>The Rust version used in production is in the <a href="https://vishnubharathi.codes/kyutai-labs/moshi/blob/main/rust"><code>rust/</code></a> directory.
This contains in particular a Mimi implementation in Rust, with Python bindings available
as <code>rustymimi</code>.</li>
</ul>
<p dir="auto">Finally, the code for the live demo is provided in the <a href="https://vishnubharathi.codes/kyutai-labs/moshi/blob/main/client"><code>client/</code></a> directory.</p>

<p dir="auto">We release three models:</p>
<ul dir="auto">
<li>our speech codec Mimi,</li>
<li>Moshi fine-tuned on a male synthetic voice (Moshiko),</li>
<li>Moshi fine-tuned on a female synthetic voice (Moshika).</li>
</ul>
<p dir="auto">Depending on the backend, the file format and quantization available will vary. Here is the list
of the HuggingFace repo with each model. Mimi is bundled in each of those, and always use the same checkpoint format.</p>
<ul dir="auto">
<li>Moshika for PyTorch (bf16): <a href="https://huggingface.co/kyutai/moshika-pytorch-bf16" rel="nofollow">kyutai/moshika-pytorch-bf16</a>.</li>
<li>Moshiko for PyTorch (bf16): <a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16" rel="nofollow">kyutai/moshiko-pytorch-bf16</a>.</li>
<li>Moshika for MLX (int4, int8, bf16): <a href="https://huggingface.co/kyutai/moshika-mlx-q4" rel="nofollow">kyutai/moshika-mlx-q4</a>, <a href="https://huggingface.co/kyutai/moshika-mlx-q8" rel="nofollow">kyutai/moshika-mlx-q8</a>,  <a href="https://huggingface.co/kyutai/moshika-mlx-bf16" rel="nofollow">kyutai/moshika-mlx-bf16</a>.</li>
<li>Moshiko for MLX (int4, int8, bf16): <a href="https://huggingface.co/kyutai/moshiko-mlx-q4" rel="nofollow">kyutai/moshiko-mlx-q4</a>, <a href="https://huggingface.co/kyutai/moshiko-mlx-q8" rel="nofollow">kyutai/moshiko-mlx-q8</a>,  <a href="https://huggingface.co/kyutai/moshiko-mlx-bf16" rel="nofollow">kyutai/moshiko-mlx-bf16</a>.</li>
<li>Moshika for Rust/Candle (int8, bf16): <a href="https://huggingface.co/kyutai/moshika-candle-q8" rel="nofollow">kyutai/moshika-candle-q8</a>,  <a href="https://huggingface.co/kyutai/moshika-candle-bf16" rel="nofollow">kyutai/moshika-mlx-bf16</a>.</li>
<li>Moshiko for Rust/Candle (int8, bf16): <a href="https://huggingface.co/kyutai/moshiko-candle-q8" rel="nofollow">kyutai/moshiko-candle-q8</a>,  <a href="https://huggingface.co/kyutai/moshiko-candle-bf16" rel="nofollow">kyutai/moshiko-mlx-bf16</a>.</li>
</ul>
<p dir="auto">All models are released under the CC-BY 4.0 license.</p>

<p dir="auto">You will need at least Python 3.10. For specific requirements, please check the individual backends
directories. You can install the PyTorch and MLX clients with the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install moshi      # moshi PyTorch, from PyPI
pip install moshi_mlx  # moshi MLX, from PyPI
# Or the bleeding edge versions for Moshi and Moshi-MLX.
pip install -e &#34;git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi&amp;subdirectory=moshi&#34;
pip install -e &#34;git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi_mlx&amp;subdirectory=moshi_mlx&#34;

pip install rustymimi  # mimi, rust implementation with Python bindings from PyPI"><pre>pip install moshi      <span><span>#</span> moshi PyTorch, from PyPI</span>
pip install moshi_mlx  <span><span>#</span> moshi MLX, from PyPI</span>
<span><span>#</span> Or the bleeding edge versions for Moshi and Moshi-MLX.</span>
pip install -e <span><span>&#34;</span>git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi&amp;subdirectory=moshi<span>&#34;</span></span>
pip install -e <span><span>&#34;</span>git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi_mlx&amp;subdirectory=moshi_mlx<span>&#34;</span></span>

pip install rustymimi  <span><span>#</span> mimi, rust implementation with Python bindings from PyPI</span></pre></div>
<p dir="auto">If you get an error when installing <code>moshi_mlx</code> or <code>rustymimi</code> (which <code>moshi_mlx</code> depends on),
you might need to install the <a href="https://rustup.rs/" rel="nofollow">Rust toolchain</a> to install <code>rustymimi</code> from sources.</p>
<p dir="auto">While we hope that the present codebase will work on Windows, we do not provide official support for it.
We have tested the MLX version on a MacBook Pro M3. At the moment, we do not support quantization
for the PyTorch version, so you will need a GPU with a significant amount of memory (24GB).</p>
<p dir="auto">For using the Rust backend, you will need a recent version of the <a href="https://rustup.rs/" rel="nofollow">Rust toolchain</a>.
To compile GPU support, you will also need the <a href="https://developer.nvidia.com/cuda-toolkit" rel="nofollow">CUDA</a> properly installed for your GPU, in particular with <code>nvcc</code>.</p>

<p dir="auto">If you wish to install from a clone of this repository, maybe to further develop Moshi, you can do the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# From the root of the clone of the repo
pip install -e &#39;moshi[dev]&#39;
pip install -e &#39;moshi_mlx[dev]&#39;
pre-commit install"><pre><span><span>#</span> From the root of the clone of the repo</span>
pip install -e <span><span>&#39;</span>moshi[dev]<span>&#39;</span></span>
pip install -e <span><span>&#39;</span>moshi_mlx[dev]<span>&#39;</span></span>
pre-commit install</pre></div>
<p dir="auto">If you wish to build locally <code>rustymimi</code> (assuming you have Rust properly installed):</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install maturin
maturin dev -r -m rust/mimi-pyo3/Cargo.toml"><pre>pip install maturin
maturin dev -r -m rust/mimi-pyo3/Cargo.toml</pre></div>

<p dir="auto">The PyTorch based API can be found in the <code>moshi</code> directory. It provides a streaming
version of the audio tokenizer (mimi) and the language model (moshi).</p>
<p dir="auto">In order to run in interactive mode, you need to start a server which will
run the model, you can then use either the web UI or a command line client.</p>
<p dir="auto">Start the server with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m moshi.server [--gradio-tunnel] [--hf-repo kyutai/moshika-pytorch-bf16]"><pre>python -m moshi.server [--gradio-tunnel] [--hf-repo kyutai/moshika-pytorch-bf16]</pre></div>
<p dir="auto">And then access the web UI on <a href="http://localhost:8998" rel="nofollow">localhost:8998</a>. If your GPU is on a distant machine
with no direct access, <code>--gradio-tunnel</code> will create a tunnel with a URL accessible from anywhere.
Keep in mind that this tunnel goes through the US and can add significant latency (up to 500ms from Europe).
You can use <code>--gradio-tunnel-token</code> to set a fixed secret token and reuse the same address over time.
Alternatively, you might want to use SSH to redirect your connection.</p>
<p dir="auto">You can use <code>--hf-repo</code> to select a different pretrained model, by setting the proper Hugging Face repository.</p>
<p dir="auto">Accessing a server that is not localhost via http may cause issues with using
the microphone in the web UI (in some browsers this is only allowed using
https).</p>
<p dir="auto">A local client is also available, as</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m moshi.client [--url URL_TO_GRADIO]"><pre>python -m moshi.client [--url URL_TO_GRADIO]</pre></div>
<p dir="auto">However note that, unlike the web browser, this client is barebone: It does not perform any echo cancellation,
nor does it try to compensate for a growing lag by skipping frames.</p>
<p dir="auto">For more information, in particular on how to use the API directly, please
checkout <a href="https://vishnubharathi.codes/kyutai-labs/moshi/blob/main/moshi/README.md">moshi/README.md</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Python (MLX) for local inference on macOS</h2><a id="user-content-python-mlx-for-local-inference-on-macos" aria-label="Permalink: Python (MLX) for local inference on macOS" href="#python-mlx-for-local-inference-on-macos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Once you have installed <code>moshi_mlx</code>, you can run</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m moshi_mlx.local -q 4   # weights quantized to 4 bits
python -m moshi_mlx.local -q 8   # weights quantized to 8 bits
# And using a different pretrained model:
python -m moshi_mlx.local -q 4 --hf-repo kyutai/moshika-mlx-q4
python -m moshi_mlx.local -q 8 --hf-repo kyutai/moshika-mlx-q8
# be careful to always match the `-q` and `--hf-repo` flag."><pre>python -m moshi_mlx.local -q 4   <span><span>#</span> weights quantized to 4 bits</span>
python -m moshi_mlx.local -q 8   <span><span>#</span> weights quantized to 8 bits</span>
<span><span>#</span> And using a different pretrained model:</span>
python -m moshi_mlx.local -q 4 --hf-repo kyutai/moshika-mlx-q4
python -m moshi_mlx.local -q 8 --hf-repo kyutai/moshika-mlx-q8
<span><span>#</span> be careful to always match the `-q` and `--hf-repo` flag.</span></pre></div>
<p dir="auto">This command line interface is also barebone. It does not perform any echo cancellation,
nor does it try to compensate for a growing lag by skipping frames.</p>
<p dir="auto">Alternatively you can run <code>python -m moshi_mlx.local_web</code> to use
the web UI, the connection is via http and will be at <a href="http://localhost:8998" rel="nofollow">localhost:8998</a>.</p>

<p dir="auto">In order to run the Rust inference server, use the following command from within
the <code>rust</code> directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run --features cuda --bin moshi-backend -r -- --config moshi-backend/config.json standalone"><pre>cargo run --features cuda --bin moshi-backend -r -- --config moshi-backend/config.json standalone</pre></div>
<p dir="auto">When using macOS, you can replace <code>--features cuda</code> with <code>--features metal</code>.</p>
<p dir="auto">Alternatively you can use <code>config-q8.json</code> rather than <code>config.json</code> to use the
quantized q8 model. You can select a different pretrained model, e.g. Moshika,
by changing the <code>&#34;hf_repo&#34;</code> key in either file.</p>
<p dir="auto">Once the server has printed &#39;standalone worker listening&#39;, you can use the web
UI. By default the Rust server uses https so it will be at
<a href="https://localhost:8998" rel="nofollow">localhost:8998</a>.</p>
<p dir="auto">You will get warnings about the site being unsafe. When using chrome you
can bypass these by selecting &#34;Details&#34; or &#34;Advanced&#34;, then &#34;Visit this unsafe
site&#34; or &#34;Proceed to localhost (unsafe)&#34;.</p>

<p dir="auto">We recommend using the web UI as it provides additional echo cancellation that helps
the overall model quality. Note that most command will directly serve this UI
in the provided URL, and there is in general nothing more to do.</p>
<p dir="auto">Alternatively, we provide command line interfaces
for the Rust and Python versions, the protocol is the same as with the web UI so
there is nothing to change on the server side.</p>
<p dir="auto">For reference, here is the list of clients for Moshi.</p>

<p dir="auto">From within the <code>rust</code> directory, run the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run --bin moshi-cli -r -- tui --host localhost"><pre>cargo run --bin moshi-cli -r -- tui --host localhost</pre></div>



<p dir="auto">The web UI can be built from this repo via the
following steps (these will require <code>npm</code> being installed).</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd client
npm install
npm run build"><pre><span>cd</span> client
npm install
npm run build</pre></div>
<p dir="auto">The web UI can then be found in the <code>client/dist</code> directory.</p>

<p dir="auto">The present code is provided under the MIT license for the Python parts, and Apache license for the Rust backend.
The web client code is provided under the MIT license.
Note that parts of this code is based on <a href="https://github.com/facebookresearch/audiocraft">AudioCraft</a>, released under
the MIT license.</p>
<p dir="auto">The weights for the models are released under the CC-BY 4.0 license.</p>

<p dir="auto">If you use either Mimi or Moshi, please cite the following paper,</p>
<div data-snippet-clipboard-copy-content="@techreport{kyutai2024moshi,
    author = {Alexandre D\&#39;efossez and Laurent Mazar\&#39;e and Manu Orsini and Am\&#39;elie Royer and
			  Patrick P\&#39;erez and Herv\&#39;e J\&#39;egou and Edouard Grave and Neil Zeghidour},
    title = {Moshi: a speech-text foundation model for real-time dialogue},
    institution = {Kyutai},
    year={2024},
    month={September},
    url={http://kyutai.org/Moshi.pdf},
}"><pre><code>@techreport{kyutai2024moshi,
    author = {Alexandre D\&#39;efossez and Laurent Mazar\&#39;e and Manu Orsini and Am\&#39;elie Royer and
			  Patrick P\&#39;erez and Herv\&#39;e J\&#39;egou and Edouard Grave and Neil Zeghidour},
    title = {Moshi: a speech-text foundation model for real-time dialogue},
    institution = {Kyutai},
    year={2024},
    month={September},
    url={http://kyutai.org/Moshi.pdf},
}
</code></pre></div>
</article></div></div>
  </body>
</html>
