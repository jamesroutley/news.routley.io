<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sdomi.pl/weblog/26-nobody-here-is-free-of-sin/">Original</a>
    <h1>You can&#39;t parse XML with regex. Let&#39;s do it anyways</h1>
    
    <div id="readability-page-1" class="page"><div><h3>You can&#39;t parse XML with regex. Let&#39;s do it anyways.</h3>
<p><img src="https://sdomi.pl/weblog/26-nobody-here-is-free-of-sin/img/cover.png"/>
<small>this scene came to me in a dream</small></p><div><p><img src="https://sdomi.pl/img/haruhi.jpg"/></p><h3>Haruhi says...</h3><p><span>&#34;They didn&#39;t even get to the blogpost <span id="n1"><i>stuff</i><sup><a href="#note1">1</a></sup></span> and they&#39;re already making a contradictory statement. <span><span>#1</span></span>Has to be some sort of record! Fortunately, this contradiction is <i>far</i> from being the last in this post.
&#34;</span></p></div>
<p>Attempting to parse HTML with <a href="https://en.wikipedia.org/wiki/Regexp">regular expressions</a> is an infamous pitfall, and a great example of using the wrong tool for the job. It&#39;s generally accepted to be a bad idea, for a multitude of reasons.</p>

<p><img src="https://sdomi.pl/weblog/26-nobody-here-is-free-of-sin/img/1.png"/>
<small>Picture 1 - he keeps on going for like 3 more screens (<a href="https://stackoverflow.com/a/1732454">Stack Overflow link</a>)</small></p><p>There&#39;s this famous Stack Overflow answer about why you should <b>never, ever</b> do it. In fact, this answer got so popular that it was used like a copypasta in some circles. Every time I stumbled upon it, I would think how there&#39;s <i>a lot of truth in it</i> - but at the same time, I couldn&#39;t agree in full...</p>

<h3>But... can&#39;t you, really?</h3>

<p><img src="https://sdomi.pl/weblog/26-nobody-here-is-free-of-sin/img/2.png"/>
<small>Picture 2 - did you know that XML has a logo? I&#39;m not joking, I only learnt today too</small></p><p>While I assume that all readers of this weblog have at least a <i>vague</i> understanding of XML, it&#39;s worth to recap for the sake of later arguments. Quoting the <a href="https://en.wikipedia.org/wiki/XML">Wikipedia article on XML</a>:</p>


<p><span>Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.

</span></p><p>I&#39;d like to focus on three parts:</p>

<ol>
<li><b>It&#39;s a markup language</b>: unlike JSON or <span id="n3">TOML<sup><a href="#note3">3</a></sup></span>, <span><span>#3</span></span>XML defines a much more specific structure for the document. Other <a href="https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language">SGML</a> derivatives are a bit more lax with enforcing said structure - remember this fact for later.
</li>
<li><b>It&#39;s machine-readable</b>: it&#39;s designed to be parsed and interpreted into a tree.</li>
<li><b>It&#39;s human-readable</b>: no specialized tools are required to <i>look at</i> and <i>understand</i> the data contained within an XML document.</li>
</ol>

<p>What Wikipedia doesn&#39;t <i>immediately</i> convey (you have to scroll down to section 11) is that <b>XML is horribly complex</b>. JSON, TOML and many other human-readable data interchange formats are simple enough that many self-taught developers learn them through osmosis. Heck, <a href="https://www.rfc-editor.org/rfc/pdfrfc/rfc8259.txt.pdf">RFC8259, &#34;The JavaScript Object Notation (JSON) Data Interchange Format&#34;</a>, is 16 pages long, out of which the actual format description takes maybe 8. In contrast, the base <a href="https://www.w3.org/TR/2000/REC-xml-20001006.pdf">XML 1.0 (Second Edition)</a> spec is 59 pages long, and that doesn&#39;t include various extensions that have grown onto it since 2000. Unsurprisingly, this larger surface area <a href="https://en.wikipedia.org/wiki/XML_external_entity_attack">becomes a security liability</a> when developers aren&#39;t familiar with the whole feature set.</p>

<p>This lack of in-depth knowledge about the format is why newbies even consider parsing XML with a regex. It&#39;s a &#34;<a href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect">you don&#39;t know what you don&#39;t know</a>&#34; problem, which leads to a vastly different approach when writing a parser.</p>

<h3>Your parser ≠ My parser</h3>

<p>Let&#39;s get back to the &#34;machine-readable&#34; vs. &#34;human-readable&#34; part; Assume we have a stack-based parser; this makes it easy to illustrate where the parser is in a given structure. (To refresh, a stack is a queue/array where the operations are &#34;push&#34;, that adds a value to the end, and &#34;pop&#34;, which removes and returns that value to our program.)</p>

<p><code>&lt;a&gt;
    &lt;b&gt;
        &lt;c&gt;meow&lt;/c&gt;
        &lt;d&gt;nya&lt;/d&gt;
    &lt;/b&gt;
&lt;/a&gt;
</code>

<small>Figure 1 - a very simple XML-like object tree</small></p><p>Here&#39;s a simplified view of how a parser may &#34;walk&#34; a tree:</p>

<p><code>                     #              stack=()
&lt;a&gt;                  # push a;      stack=(a)
    &lt;b&gt;              # push b;      stack=(a b)
        &lt;c&gt;meow&lt;/c&gt;  # push c;      stack=(a b c)
        &lt;d&gt;nya&lt;/d&gt;   # pop; push d; stack=(a b d)
    &lt;/b&gt;             # pop;         stack=(a b)
&lt;/a&gt;                 # pop;         stack=(a)
                     # pop;         stack=()
</code>


<small>Figure 2 - same tree, now hastily annotated with actions and state</small></p><p>While the example above doesn&#39;t show anything useful happening with our tree, it&#39;s actually quite simple to incorporate a DOM-like selector query system on top of this. The following snippet implements a very naïve XML-like parser, which can be used to extract strings from objects:</p>



<p><code>#!/usr/bin/env bash
# Please don&#39;t actually use this. xoxo, dmi
stack=()
tokens=()
buf=

# QUERY=(a b c)
QUERY=($@)

flush() {
	if [[ &#34;$buf&#34; ]]; then
		tokens+=(&#34;$buf&#34;)
	fi
	buf=
}

search() {
	(( ${#stack[@]} &lt; ${#QUERY[@]} )) &amp;&amp; return
	[[ ${tokens[-1]} != &#34;lbrack&#34; ]] &amp;&amp; return
	for (( i=0; i&lt;${#QUERY[@]}; i++ )); do
		if [[ &#34;${QUERY[i]}&#34; != &#34;${stack[-${#QUERY[@]}+i]}&#34; ]]; then
			return
		fi
	done

	echo &#34;query result: ${tokens[-2]}&#34;
}

while read -rn1 chr; do
	if [[ &#34;$chr&#34; == &#34;&lt;&#34; ]]; then
		flush
		tokens+=(&#34;lbrack&#34;)
	elif [[ &#34;$chr&#34; == &#34;&gt;&#34; ]]; then
		if [[ &#34;${tokens[-1]}&#34; == &#34;lbrack&#34; ]]; then
			flush # get tag contents
			stack+=(&#34;${tokens[-1]}&#34;) # put it onto the stack
		elif [[ &#34;${tokens[-1]}&#34; == &#34;slash&#34; ]]; then
			unset stack[${#stack[@]}-1] # pop last element
		fi
		tokens+=(&#34;rbrack&#34;)
	elif [[ &#34;$chr&#34; == &#34;/&#34; &amp;&amp; &#34;${tokens[-1]}&#34; == &#34;lbrack&#34; ]]; then
		tokens+=(&#34;slash&#34;)
	else
		buf+=&#34;$chr&#34;
	fi

	search
done
</code>


<small>Figure 3 - bash parser for our markup.</small></p><p>The result is:</p>


<p><code>## in DOM selector terms, &#39;a b c&#39; would be &#39;a &gt; b &gt; c&#39;
$ ./parse.sh a b c &lt; test.xml 
query result: meow
$ ./parse.sh a b d &lt; test.xml 
query result: nya
</code>

<small>Figure 4 - parser demonstration</small></p><p>This &#34;walking&#34; behavior can be visualized even better after adding <span>declare -p stack</span> to every loop:</p>

<p><code>$ ./parse.sh a b d &lt; test 
declare -a stack=()
declare -a stack=()
declare -a stack=([0]=&#34;a&#34;)
declare -a stack=([0]=&#34;a&#34;)
declare -a stack=([0]=&#34;a&#34;)
# (...)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34;)
# (...)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;c&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;c&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;c&#34;)
# (...)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34;)
# (...)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;d&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;d&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;d&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;d&#34;)
query result: nya
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;d&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;d&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34; [2]=&#34;d&#34;)
declare -a stack=([0]=&#34;a&#34; [1]=&#34;b&#34;)
# (...)
declare -a stack=([0]=&#34;a&#34;)
# (...)
declare -a stack=()
</code>

<small>Figure 5 - stack in action</small></p><p>Due to the single-pass nature of our parser (which combines tokenization and a few other steps into one), I had to remove some repetition. Furthermore, this parser is for demonstrational purposes only and cannot parse arbitrary XML. Real-world XML has a lot of special objects, self-terminating tags, and other gotchas that have to be accounted for, even during a simple text extraction.</p>

<h3>How <b>your brain</b> reads XML</h3>

<p>Now that you have a gist of how an algorithm for parsing XML may work (and hopefully understand that writing a parser is a lot of pain), let&#39;s step back and consider how we, creatures of protein and flesh, parse XML. To make things harder, let&#39;s look at the <i>raw, true form</i> of XML - no pretty-printing allowed.</p>


<p><code>&lt;a&gt;&lt;b&gt;&lt;c&gt;meow&lt;/c&gt;&lt;d&gt;nya&lt;/d&gt;&lt;/b&gt;&lt;/a&gt;
</code>


<small>Figure 6 - example from before, compacted</small></p><p>To an untrained eye, this doesn&#39;t <i>look</i> like a tree.</p>

<p><code><span>    &lt;a &gt;
   &lt;b   &gt;
 &lt;d &gt;nya&lt;/d&gt;
&lt;c  &gt;meow&lt;/c&gt;</span>
<span>    &lt;/b&gt;
    &lt;/a&gt;</span>
</code>

<small>Figure 7 - the same structure, with whitespace arranged to form an x-mas tree</small></p><p>Ah, much better! This is <i>semantically equivalent</i> to all the snippets I&#39;ve attached before, but you have to think really hard to picture that <span>a &gt; b &gt; (c, d)</span>. To me, this snippet is first and foremost <b>a string</b>.</p>

<h3>String parsing</h3>

<p>Approaching XML or any other structured data format <i>as a string</i> is like dumpster-diving for parts. I don&#39;t mean this in a bad way; both regex and dumpster diving have awarded me some <i>great stuff</i>. But they also give me the urge to shower immediately afterwards.</p>

<p>To continue the analogy, you can&#39;t inquire about <i>why</i> something got thrown out (as in, why given data is present and why it is formatted the way it is). This information is lost. You can make educated guesses if you stare at it long enough, but you can&#39;t know for sure. Worse even, if your data changes (as may happen with XML returned by an API), the whole tree may get ordered in a slightly different way, rendering your meticulously crafted parser useless. For this - and many other reasons - it&#39;s best to parse <i>XML</i> with a real parser.</p>

<p>I&#39;ll explore actual string parsing techniques later in this post. Before that, we have an elephant in the room to address...</p>

<h3>HTML: XML but <i>quirky</i></h3>

<details><summary>Pedantry Corner</summary><p><img src="https://sdomi.pl/img/pedantry.png"/><span>Some might argue that both HTML and XML were derived from <a href="https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language">SGML</a>, not from each other, so this section title doesn&#39;t make sense.</span></p></details>
<p>HTML is the main language used for presentation online. The web <i>lives</i> and <i>breathes</i> HTML. You can make webapps without WebAssembly, without ECMAScript, or even without CSS. But <span id="n2">you absolutely <b>need</b><sup><a href="#note2">2</a></sup> HTML</span> (... or XHTML - hold that thought).
<span><span>#2</span></span>
</p>

<p>A few thousand bytes ago, I touched on how XML is <b>extremely</b> strict in the layout. HTML is the exact opposite, allowing for unclosed tags and broken grammar. An XML parser would get a heart attack if asked to parse HTML found online.</p>

<h4>Parsing HTML is near-impossible</h4>

<p><a href="https://en.wikipedia.org/wiki/Well-formed_element">Well-formed</a> HTML is fine. However, browsers are designed to make educated guesses instead of failing outright when the markup doesn&#39;t fit. This was a compromise made for accessibility. Today&#39;s devtools make debugging easy, but in the early 90s? There was virtually no tooling for this. Having the parsers accept slightly mangled input no doubt improved adoption when HTML was all new.</p>

<p>Sadly, this means that HTML is already two layers removed from XML. Quirks mode is largely based on how things got implemented by IE and Netscape 30 years ago. Standards compliance mode somewhat improves the situation, but it will still accept missing closing tags or quotes.</p>

<p>That being said, virtually all of those situations are defined by the standard, and contemporary browsers implement it <b>extremely</b> closely. Why is it &#34;near-impossible&#34; then? HTML <a href="https://html.spec.whatwg.org/print.pdf">living standard</a> dwarfs the base of XML, being over 1500 pages long! ...Okay, perhaps that&#39;s a bit unfair - at the time of writing, only 114 of those pages actually deal with parsing (thanks for checking, Linus!). Regardless, that&#39;s still over x2 the length of the XML standard, and this growth is <i>mostly</i> defining edge-cases! Unless you&#39;re using an actual browser, chances are that your DOM tree will parse slightly differently on pages that aren&#39;t well-formed.</p>

<h3>HTML4.01? Ridiculous! We need to develop a better alternative that suits everyone&#39;s needs</h3>

<p><small>Situation: there are two sibling standards.</small></p><p><a href="https://en.wikipedia.org/wiki/XHTML">XHTML</a> is... a weird creature. It was first introduced in late 1998 and refined into a standard that was adopted as a W3C recommendation in January 2000. Unfortunately, it wasn&#39;t widely adopted (unlike later HTML5)...</p>

<p><span>The attempt to get the world to switch to XML, including quotes around attribute values and slashes in empty tags and namespaces all at once didn&#39;t work. The large HTML-generating public did not move, largely because the browsers didn&#39;t complain. Some large communities did shift and are enjoying the fruits of well-formed systems, but not all.</span></p><p>I&#39;m only mentioning XHTML here because, <i>technically</i>, we&#39;ve had a strict, well-defined HTML alternative for almost 3 decades by now, despite not many people knowing about it. Heck, XHTML5 exists too! You can use it <b>right now</b>! It&#39;s really cool! (famfo keeps telling me about it, so it <i>has</i> to be true.)</p>

<h3>Finally: actually parsing HTML with regex</h3>

<p>The following section is entirely a product of my attempts to scrape various webpages over the years. I&#39;m aware how badly the practice of scraping is viewed in some circles, and I&#39;d like to assure the reader that the bots I&#39;ve built in the past have always been slow to request, and used extensive caching. GenAI scrapers constantly DoSing the internet can go to hell.</p>

<h3>Benefits</h3>

<div><p><img src="https://sdomi.pl/img/haruhi.jpg"/></p><h3>Haruhi says...</h3><p><span>&#34;Bet you didn&#39;t expect them to talk about <i>benefits</i> after they spent so long rambling about how hard it is to parse HTML. Ha!&#34;</span></p></div><ol>
<li>Development speed</li>
<p>Modern websites often have hundreds, if not thousands of nested elements. Writing a selector for something <i>really</i> deep down can take a while, especially if additional constraints are present (randomized class names? the developer only knowing about div-s?).</p>
<p>Writing a regex takes me 30 seconds. But hacking up a good selector and debugging why it doesn&#39;t work on the next request? Tens of minutes of cursing.</p>

<li>Adaptability</li>
<p>Selectors are strict. They either give you a result or fail. This is <i>great</i> when you trust the other side of the system to send you good, accurate markup. HOWEVER, this is not something you can expect when scraping. For instance:</p>



<code>    (...)
            &lt;tr id=&#34;trainStation0&#34; class=&#34;altrow&#34;&gt;
        &lt;td class=&#34;no_border&#34;&gt;Peterborough&lt;/td&gt;
        &lt;td class=&#34;centre_col no_border platform&#34;&gt;1&lt;/td&gt;
        &lt;td class=&#34;centre_col no_border&#34;&gt;1801&lt;/td&gt;
        &lt;td class=&#34;centre_col no_border on_time&#34;&gt;On Time&lt;/td&gt;
        
    &lt;/tr&gt;

            &lt;tr id=&#34;callingPoints0&#34; class=&#34;altrow&#34;&gt;
    &lt;td class=&#34;calling_list&#34; colspan=&#34;4&#34;&gt;
        &lt;div id=&#34;scroll0&#34; class=&#34;scrollable&#34;&gt;

            
            
                    &lt;span class=&#34;cp_header&#34;&gt;Calling at:&lt;/span&gt;
                    
                
                    &lt;span&gt;Ifield&lt;/span&gt;
                    &lt;span&gt;(1805), &lt;/span&gt;
                
                    &lt;span&gt;Crawley&lt;/span&gt;
                    &lt;span&gt;(1808), &lt;/span&gt;
                
                    &lt;span&gt;Three Bridges&lt;/span&gt;
                    &lt;span&gt;(1812), &lt;/span&gt;
                
                    &lt;span&gt;Gatwick Airport&lt;/span&gt;
                    &lt;span&gt;(1817), &lt;/span&gt;
    (...)
</code>


<small>Figure 8 - Excerpt from a departure table of the least used train station in West Sussex</small>

<p>Say, if we wanted to extract all the stations where this train calls at.</p>
<ul>
	<li>In ECMAScript, that&#39;d be <span>document.querySelectorAll(&#34;#scroll0 &gt; span&#34;)</span>... And then you have to join the strings, so more like <span>let a=&#34;&#34;; document.querySelectorAll(&#34;#scroll0 &gt; span&#34;).forEach((e)=&gt;{a+=e.innerText;}); console.log(a);</span></li>
	<li>With regex, I&#39;d start by matching for <span>scroll0&#34;.*?&lt;/div</span>, followed by removing everything that matches <span>&lt;[/a-zA-Z]&gt;</span>. This leaves us with a lot of spaces, which can be mitigated by matching for <span>  </span> (two spaces in a row). My shell one-liner looks something like: <span>curl (...) | tr -d &#39;\r\n&#39; | grep -Poh &#39;scroll0.*?&lt;/div&#39; | sed &#39;s@&lt;[/a-zA-Z]*&gt;@@g;s/  //g;&#39;</span></li>
</ul>
<p>This leaves us with the following payload:</p>
<code>scroll0&#34; class=&#34;scrollable&#34;&gt;&lt;span class=&#34;cp_header&#34;&gt;Calling at:Ifield(1835), Crawley(1838), Three Bridges(1842), Gatwick Airport(1847), Horley(1851), Redhill(1859), Merstham(1903), Coulsdon South(1908), East Croydon(1915), London Bridge(1930), London Blackfriars(1936), City Thameslink(1938), Farringdon(1941), London St Pancras (Intl)(1945), Finsbury Park(1952), Stevenage(2013), Hitchin(2020), Arlesey(2026), Biggleswade(2031), Sandy(2035), St Neots(2042), Huntingdon(2049), &lt;span class=&#34;cp_dest&#34;&gt;Peterborough&lt;span class=&#34;cp_dest&#34;&gt;(2105)&lt;/div</code>
	<small>Figure 9 - Slightly dirty, but nonetheless extracted</small>
<p>The remaining HTML markup could be removed with a few more sed filters, but that&#39;s not exactly the point of this example. Instead, imagine that National Rail changed their markup to stop using those ridiculous spans:</p>



<code>        (...)
        &lt;div id=&#34;scroll0&#34; class=&#34;scrollable&#34;&gt;
                    &lt;span class=&#34;cp_header&#34;&gt;Calling at:&lt;/span&gt;
                
                    Ifield
                    (1805), 
                
                    Crawley
                    (1808), 
                
                    Three Bridges
                    (1812), 
                
                    Gatwick Airport
                    (1817), 
        (...)
</code>


<small>Figure 10 - Example modification that could happen to a page</small>

<p>In this scenario, our existing selector <span>#scroll0 &gt; span</span> <i>only</i> matches the first element, returning the words <span>Calling at:</span>, without the actual list of stations. On the other hand, the shell one-liner <i>doesn&#39;t break</i> because it didn&#39;t rely on markup for context, only anchoring. Please note: in some scenarios this may garble the output - especially during large layout changes. I still count it as a win for team regex.</p>

<li>Complexity</li>
<p>HTML is not designed to be consumed by any program that isn&#39;t a browser. At heart, it&#39;s a language for <i>presentation</i>, not <i>data interchange</i>. This makes traditional selectors unsuitable for some tasks. Consider the following snippet:</p>


<code>    &lt;li&gt;Screen size: 21.37&#34; &lt;/li&gt;
    &lt;li&gt;Battery capacity :  154Wh &lt;/li&gt;
</code>


<small>Figure 11 - a mock item info list</small>

<p>There&#39;s nothing that could help the parser extract the key/value pairs. <span>&lt;li&gt;</span> is generic enough that it&#39;s probably present in a few different areas of the page. Even if you had a proper selector for it, you&#39;d still have to split on <span>:</span>, then remove whitespace, and fuzzy-match the key. With regex, extracting the screen size is just a matter of <span>grep -Pohi &#39;screen size ?:.*?&#34;&#39; | grep -Poh &#39;[0-9]*\.[0-9]*&#34;&#39;</span> - half of which you would need to do either way.</p>

</ol>

<h3>Some broad tips</h3>

<ol>
<li>Ask yourself whether regex is the right tool for the job. Unless you&#39;re scraping, or you&#39;re really low on available space for new libraries (writing for embedded?), then the correct approach is using some XML parser library. For some scraping workflows (extremely regular data, i.e. autogenerated tables), an HTML parser may prove more helpful than hacks described below.</li>
<li>If you are <i>actually</i> dealing with irregularly generated HTML, check if you can get the data elsewhere. A smart hacker doesn&#39;t scrape HTML when there&#39;s an Android app with readily available API endpoints returning JSON.</li>
<li>Don&#39;t try to actually parse the tree structure. This removes all benefits of using regex for scraping and welcomes you to a world of pain.</li>
<li>PCRE is a must. Standard regex doesn&#39;t have the <span>?</span> non-greedy match operator. This means that <span>a.*b</span> will match <i>the first &#34;a&#34;</i>, and <i>the last &#34;b&#34;</i>, whereas doing <span>a.*?b</span> will match <i>the first &#34;a&#34;</i>, and <i>the first &#34;b&#34;</i> that follows. This is VERY useful for anchoring to some unique text, and then matching the next closing tag (like this: <span>meow.*?<!--</span-->).</span></li>
<li>If you <i>don&#39;t</i> have PCRE, non-greedy matching can be emulated through:
	
	<ol>
		<li>anchoring somewhere (<span>meow.*</span>)</li>
		<li>replacing the first occurence of the end marker with a unique string (<span>s|&lt;/|￾OwO￾|</span>)</li>
		<li>and finally, matching to the unique character (<span>meow.*￾OwO￾</span>)</li>
	</ol>
	Much more painful than <span>?</span>, but it works in a pinch.</li>
<li>Always match the tightest variety of characters you can. <span>[A-Za-z0-9]*</span> is much better than <span>.*?</span></li>
<li>If possible, anchor to unique strings of text. <span>List of departures</span> is less likely to change than a class name.</li>
<li>If there is interesting whitespace, use it to your advantage! All parser rules go out of the window - if each list element is on a separate line, don&#39;t make your life harder; just iterate over those lines.</li>
<li>Remove the <b>uninteresting</b> whitespace as soon as you can. Some regex engines allow matching <span>\s*</span>, in others you will have to define <span>[ \t\r\n]*</span> or something similar. Matching multiples (<span>\s{2}</span>, <span>\s{3}</span>, etc) and replacing them with single spaces in a loop can help to preserve the <b>good</b> whitespace where you need it.</li>
<li>If your browser requests a thing, you can most likely request it through curl. There are enough gotchas about this point to warrant a separate blogpost, but for now, remember about Dev Tools -&gt; Network -&gt; (right click on request) -&gt; <i>Copy as cURL</i>.</li>
<li>Extraction <b>will</b> break. Good scraping bots account for it (and fail safe).</li>
</ol>

<h3>The exhibit</h3>

<p>
	I&#39;ve had some trouble trying to settle on a specific thing to use as a sample, so I finally went with something pointless and silly. Here&#39;s a small scraper that extracts data from <a href="https://openrct2.io">OpenRCT2</a>&#39;s download webpage.</p>



<p><code><span>#!/bin/bash</span>
unset IFS 
data=&#34;$(curl <span>&#39;https://openrct2.io/download/release&#39;</span>)&#34;
<span># data=&#34;$(cat /tmp/dump)&#34; # a good scraper always prototypes on a local dump</span>

<span># match line that contains &#39;v&#39; and &#39;latest&#39;, to get the current game version</span>
latest_ver=&#34;$(grep -Poh <span>&#39;v.*?(?=\(latest)&#39;</span> &lt;&lt;&lt; <span>&#34;$data&#34;</span>)&#34;

<span># let&#39;s split the list of releases into something we can use:</span>
<span>#   1. make sure that there&#39;s no 0x01/0x02. we&#39;ll use those as our delimeters</span>
<span>#   2. split the page on &#34;card-header&#34;, which occurs only between list items</span>
releases=&#34;$(tr -d <span>$&#39;\x01\x02&#39;</span> &lt;&lt;&lt; <span>&#34;$data&#34;</span> | sed <span>&#39;s/card-header/\x01/&#39;</span>)&#34;

<span># now, let&#39;s iterate over what we found:</span>
while read -d<span>$&#39;\x01&#39;</span> -r release; do
	<span># &#39;btn btn-link&#39; seems to always preceed the version. let&#39;s use that fact :)</span>
	version=&#34;$(grep -A1 <span>&#39;btn btn-link&#39;</span> &lt;&lt;&lt; <span>&#34;$release&#34;</span> | tail -n1 | sed <span>&#39;s/ //g&#39;</span>)&#34;
	version=<span>&#34;${version#* }&#34;</span> <span># trim all whitespace from the beginning of the string</span>
	if [[ ! <span>&#34;$version&#34;</span> ]]; then <span># empty? has to be the 1st item, let&#39;s fall back</span>
		version=<span>&#34;$latest_ver&#34;</span>
	fi

	<span># &#39;card-title&#39; is our top anchor.</span>
	<span># then, we can iterate over &#39;Download&#39;, which occurs at the end.</span>
	<span># also here: cleanup all closing tags, since they&#39;re at the end of each line anyways.</span>
	meta=&#34;$(grep card-title -C10 &lt;&lt;&lt; <span>&#34;$release&#34;</span> | sed <span>&#39;s@&lt;/.*@@&#39;</span> | sed <span>$&#39;s/Download/\x02/&#39;</span>)&#34;

	echo -e <span>&#34;OpenRCT $version\n-------&#34;</span> <span># CLI design is my passion</span>

	<span># finally, we iterate over each artifact:</span>
	while read -d<span>$&#39;\x02&#39;</span> -r entry; do
		<span># we&#39;re lucky, h5 is only used for the platform. remove all preceeding garbage.</span>
		platform=&#34;$(grep h5 &lt;&lt;&lt; <span>&#34;$entry&#34;</span> | sed <span>&#39;s/.*&gt;//&#39;</span>)&#34;

		<span># architecture is right after a line with h6.</span>
		<span># let&#39;s match h6 and the next line, then discard the first line</span>
		arch=&#34;$(grep -A1 h6 &lt;&lt;&lt; <span>&#34;$entry&#34;</span> | tail -n1 | sed <span>&#39;s/ *//;s/,//&#39;</span>)&#34;

		<span># artifact type is just one line below the arch.</span>
		<span># match h6 and *two* following lines, then discard first two.</span>
		type=&#34;$(grep -A2 h6 &lt;&lt;&lt; <span>&#34;$entry&#34;</span> | tail -n1 | sed <span>&#39;s/ *//&#39;</span>)&#34;

		<span># url is simple, just extract data from quotes.</span>
		<span># the space after &#34; is load-bearing! otherwise we&#39;d need to do additional cleanup</span>
		url=&#34;$(grep <span>&#39;&lt;a href&#39;</span> &lt;&lt;&lt; <span>&#34;$entry&#34;</span> | sed -E <span>&#39;s@.*href=&#34;(.*)&#34; .*@\1@&#39;</span>)&#34;

		<span># finally, presentation</span>
		cat &lt;&lt;EOF
* $platform 
  - Arch: $arch
  - Type: $type
  - URL: $url
EOF
	done &lt;&lt;&lt; <span>&#34;$meta&#34;</span>
	echo -e <span>&#39;\n\n&#39;</span>
done &lt;&lt;&lt; <span>&#34;$releases&#34;</span>
</code>
<small>Figure 12 - annotated example scraper</small></p><p>
	At the present, running this code shows a list of artifacts, divided by platform and version:
</p>

<p><img src="https://sdomi.pl/weblog/26-nobody-here-is-free-of-sin/img/3.jpg"/>
<small>Picture 3 - sample output of the scraper</small></p><h3>Ending remarks</h3>

<p>I suppose that the original Stack Overflow answer was about <i>actually parsing</i> the structure, retaining all the inherent metadata in the process. I agree with this point (who wouldn&#39;t? XML isn&#39;t regular, that&#39;s a solid fact). However, if &#34;parsing&#34; is treated as a means to an end (like regex usually is...), it absolutely is possible to <i>extract</i> data from HTML or XML with a sufficiently complex regular expression. That&#39;s, more or less, the asterisk that has been floating in my mind for all those years.</p>

<p>At some point, I&#39;d like to expand on the whole <i>scraping</i> aspect of this post, possibly into a mini-series. Send me ACKs through avian carrier if you&#39;d like to see that happen.</p>

<hr/><p>

Notes:

</p><ul>
	<li><a href="#n1" id="note1">#1</a> - Content is a word of the enemy. Companies will say &#34;content&#34; instead of calling it artworks, writings, pieces, and such, as if all media is something interchangeable meant to fill a box. Referring to &#34;art&#34; as &#34;content&#34; nowadays is often pejorative. If I ever make a &#34;CDN&#34; (Content Delivery Network), I will call it an SDN instead. Sounds much comfier.</li>
	<li><a href="#n2" id="note2">#2</a> - Before publication, Lisa argued that you <i>technically</i> can make pages without HTML:
	<ul>
		<li>SVG</li>
		<li>Java Applets</li>
		<li>Flash</li>
		<li>PDF</li>
	</ul>
	One could discredit the last three options, as they&#39;re external technologies that aren&#39;t a part of any Web spec. However, SVG is much tougher to ignore. It&#39;s a <a href="https://www.w3.org/TR/SVG2/">W3C Recommendation</a>, which makes it at least adjacent. It also specifies <a href="https://www.w3.org/TR/SVG2/linking.html#Links">the &lt;a&gt; tag</a>, so <i>technically</i> SVG could be used &#34;without HTML&#34; to create a webpage. I remain sceptical.</li>

	<li><a href="#n3" id="note3">#3</a> - This sentence originally mentioned YAML. This post isn&#39;t about YAML, and yet I got a lot of complaints for implying that YAML could be considered simple. This discussion has <b>absolutely no relevance to this post</b>, so I replaced it with TOML. Don&#39;t like TOML? Think of INI. Don&#39;t like INI? Think of CSV, etc.</li>
</ul>

<hr/>

<p>Edited by <a href="https://mk.absturztau.be/@iro_miya">Miya</a>. Standards technical consulting by <a href="https://linus.dev">Linus</a>. Proofreading by <a href="https://linus.dev">Linus</a>, <a href="https://riedler.wien">Riedler</a>, <a href="https://famfo.xyz">famfo</a>, <a href="https://lili.lgbt">Lili</a>, <a href="https://is-a.cat/@ar">ari</a>, <a href="https://liberda.nl">lauren</a>, <a href="https://filmroellchen.eu">kleines Filmröllchen</a>, <a href="https://shift.gay/">shebang</a>, <a href="https://kwolek.io">irth</a>, <a href="https://compilercrim.es">mei</a> and <a href="https://sakamoto.pl/~multi/">Multi</a> (damn that&#39;s a lot). Thanks a lot to everyone for helping out, and thank <i>you</i> for reading until the end. &lt;3</p>
<hr/><p><a href="https://ko-fi.com/sdomi/"><img alt="Support me on ko-fi!" src="https://sdomi.pl/img/ko-fi.png"/></a></p><hr/><h3>Comments:</h3></div><p>oh i was completely unaware of grep -P and the &#34;.*?&#34; matcher, guess it&#39;s time to actually read through grep&#39;s manpage

as always awesome co-, uh, stuff - thanks for the evening read domi :3c</p></div>
  </body>
</html>
