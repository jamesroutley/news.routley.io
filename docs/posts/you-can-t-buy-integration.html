<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://martinfowler.com/articles/cant-buy-integration.html">Original</a>
    <h1>You Can&#39;t Buy Integration</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>In the early days of computing, vendors sold software, including compilers
    and operating systems, as part of the hardware they ran on. That
    changed in 1974, when the US Commission on New Technological Uses of
    Copyrighted Works (CONTU) decided that computer programs were subject to
    copyright, creating a market for what were initially called “program
    products.” Despite the resistance movement of the Free Software Foundation
    and open source, there was, and is, a clear market for commercial software
    products. “Build versus buy” decisions are everywhere today, and rightly so.
    Building software is risky and expensive, and software product companies can
    spread that risk and expense across multiple customers. </p>

<p>However, as you may have guessed by the title of this article, such
    decisions don&#39;t apply to <i>all</i> contexts.</p>

<section id="YouCanx2019tBuyIntegration">
<h2>You can’t buy integration</h2>

<p>Despite a wide range of tools that aim to simplify wiring systems
      together, you can’t buy integration.</p>

<p>You <i>can</i> buy programming languages. After the 1974 CONTU ruling, it
      became common to pay for the compiler. Bill Gates’ famous <a href="https://en.wikipedia.org/wiki/Open_Letter_to_Hobbyists">Open
      Letter To Hobbyists</a> was a clarion call for the community to pay for
      Micro-Soft’s Altair BASIC interpreter (they dropped the dash in later
      years). The Free Software Foundation’s GCC compiler opened the door to the
      commoditization of programming languages but left open a commercial market
      for developer tooling. I’m happy to program in Java for example — now
      freely available — but I would not be excited to do so in vi or
      Notepad.</p>

<p>Integration software products — ESBs, ETL tools, API platforms, and
      cloud integration services — are not products that directly solve a
      business problem. They are not in the same category, for example, as fraud
      detection products or analytics products or CRMs. They are programming
      languages, bundled with a toolchain and a runtime to support the
      compilation process. When you buy an integration product, you are agreeing
      to build the integration itself in a commercial programming language.</p>

<p>Integration tools are almost always low-code platforms, which means
      they aim to simplify the development effort by providing a graphical
      design palette you can drag and drop integration workflow on top of. The
      source code is typically saved in a markup
      language that can be interpreted by the runtime. You might drag and drop
      some workflow onto a palette, but underneath the hood, the platform saves
      the source code as JSON or XML, and embeds a runtime that knows how to
      interpret the markup into actual machine code, no different than
      Micro-Soft’s early compiler knew how to convert BASIC code into machine
      code on the Altair platform. For example, here is the “Hello, World”
      source code for Step Functions, an AWS orchestration engine:</p>

<div id="step-functions.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/step-functions.png"/></p><p>Figure 1: Step Functions represents a workflow
      with both JSON and graphical design palette</p>
</div>



<p>Many integration tools, including AWS Step Functions, let you program
      using either the graphical palette or the markup language directly. While
      the palette is often preferred for reasons obvious to anyone who read
      Charles Petzold’s famous <a href="http://www.charlespetzold.com/etc/CSAML.html">
      April Fools joke about CSAML</a>, the complexity of
      configuring integration steps in the palette means that, in practice,
      competent developers gain some facility with the underlying markup
      language itself. In effect, there is a bidirectional mapping from the
      graphical palette to the markup language such that changing one can
      immediately be reflected in the other. If I’ve understood the vernacular
      of mathematics correctly, that’s what’s called an
      <a href="https://en.wikipedia.org/wiki/Morphism#Isomorphisms">isomorphism</a>, so I’ll
      call the resulting structure “source-diagram isomorphism,” where both the
      palette and the markup language represent source code and can be
      seamlessly translated back and forth. That of course represents a
      developer-centric view of the world; the runtime itself only cares about
      the markup language.</p>

<p>This is quite different from most software programming, where the developer
      directly edits the source code absent a graphical palette, a practice I’ll call
      “source <a href="https://en.wikipedia.org/wiki/Morphism#Endomorphisms_and_automorphisms">
      endomorphism</a>,” although you can also call it “normal” if that’s easier
      to remember. There are tools, of course, that visualize class diagrams in Java
      and perhaps even let you make edits that are reflected back in the source code,
      but the usual activity of a Java developer is to directly edit Java source code
      in an IDE.</p>

<p>The advantage of providing a graphical design palette is that it provides a
      way of organizing thought, a <a href="https://martinfowler.com/books/dsl.html">
      domain specific language</a> (DSL) for integration
      problems, allowing you to focus on the narrow problem of wiring systems together
      absent extraneous complexity. Java may be better at solving general purpose
      problems, but the constraints of the design palette and declarative markup
      language purport to solve integration and workflow concerns more elegantly, in
      the same way that Excel functions let you solve a budgeting problem more
      elegantly than writing custom Java code. Similarly, in a number of contexts, I’d
      much prefer the calculator on my iPhone over the impressive
      <a href="https://www.amazon.com/HP-HP50G-50g-Graphing-Calculator/dp/B000GTPRPS/">
      HP 50g graphic calculator</a>, with its support for Reverse Polish Notation and
      scientific calculations.</p>

<div id="calculators.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/calculators.png"/></p><p>Figure 2: A good DSL removes complexity by focusing on the core problem</p>
</div>



<p>When you buy integration tools, you are agreeing to build the actual
      integration itself. What you are buying is a promise that the integration
      can be solved more efficiently and more simply than using a general
      purpose language. The job of the architect then comes down to
      understanding in what contexts that promise is likely to hold true, and
      to avoid the understandable temptation to convert the &#34;buy&#34; decision into
      a mandate to use the tool outside of those contexts in order to justify its
      ROI.</p>

<p>Some integration DSLs are simpler projections of the problem space,
      like my iPhone calculator. Others are indeed Turing complete, meaning, in
      a theoretical sense, they have the same algorithmic power as a general
      purpose language. While true, academic discussions of computability fail
      to account for software engineering, which a
      <a href="https://www.amazon.com/Software-Engineering-Google-Lessons-Programming/dp/1492082791">
      group of Googlers</a> defined as
      “programming over time.” If programming requires working with abstractions, then programming
      over time means evolving those abstractions in a complex ecosystem as the environment
      changes, and requires active consideration of team agreements, quality practices, and
      delivery mechanics. We’ll examine how
      programming-over-time concerns affect integration in more detail shortly and how
      they inform the appropriate contexts for low-code integration tools. First, though, I
      want to challenge the idea that the primary goal of integration is wiring systems
      together, as I believe a broader definition allows us to better segregate the parts
      of the ecosystem where simplifying abstractions facilitate programming and where
      the additional complexity of programming-over-time concerns requires a general purpose
      programming language, a claim I&#39;ll defend shortly.</p>
</section>

<section id="PutMostOfYourEnergyIntoBuildingCleanInterfaces">
<h2>Put most of your energy into building clean interfaces</h2>

<p>For most people, the word
      “integration” creates the impression of connecting systems together, of
      sharing data to keep systems in sync. I believe that definition of
      integration is insufficient to meet the demands of a modern digital
      business, and that the real goal of integration done well is to create
      clean interfaces between capabilities. </p>

<p>When our primary focus is connecting systems, we can measure how
      successful our integration approach is by how quickly we can wire
      a new system into an existing technical estate. The <i>systems</i>
      become the primary value driver inside that estate, and integration becomes
      a necessary evil to make the systems behave properly. When instead we
      shift our primary focus to creating clean interfaces over digital
      capabilities, we measure success by increasing digital agility over time,
      and those digital capabilities become the primary value driver, arguably
      even more important than the systems themselves. There&#39;s a lot to unpack
      in that difference, starting with the emphasis on interface over
      implementation.</p>

<p>Digital organizations shift the primary focus of integration
      from the <i>systems</i> to the <i>capabilities</i>, emphasizing clean
      interfaces over those capabilities.</p>

<p>Simplifying interfaces are one of the critical elements in creating a
      successful product and to scaling inside a complex ecosystem. I have very
      little understanding of the mechanical-electrical implementation
      underlying the keyboard I’m typing on, for example, or the input system
      drivers or operating system interrupts that magically make the key I’m
      typing show up on my screen. Somebody had to figure that all out — many
      somebodies, more likely, since the keyboard and system driver and
      operating system and monitor and application are all separate “products” — but
      all I have to worry about is pressing the right key at the right
      time to integrate the thoughts in my brain to words on the screen. </p>

<p>That, of course, has an interesting corollary: the key (no pun
      intended) to simplifying the interface is to accept a more complex
      implementation. </p>

<p>There is nothing controversial about that statement when we think of
      digital products that face off with the market. Google search is
      unimaginably complex underneath the hood and uncannily easy for even a
      digitally unsavvy user to use. We also accept it for digital products that
      face off with business users. The sales team excited about bringing in
      Salesforce surely understands that, while the user interface may be more
      intuitive for their needs than the older CRM, it requires a significant
      amount of effort to maintain and evolve the product itself, which is why
      the subscription fees feel justifiable. Yet we treat integration
      differently. Intuitively, we understand that the two-dimensional boxes on
      our architecture diagrams may hide considerable complexity, but expect the
      one-dimensional lines to be somehow different. </p>

<p>(They <i>are</i> different in one regard. You can buy the boxes but you can’t
      buy the lines, because you can’t buy integration.)</p>

<p>While we have historically drawn up our project plans and costs around
      the boxes — the digital products we are introducing — the lines are the
      hidden and often primary driver of organizational tech debt. They are the
      reason that things just take longer now than they used to.</p>

<div id="spaghetti-sprawl.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/spaghetti-sprawl.png"/></p><p>Figure 3: We think of projects in terms of the
      applications they introduce, but the lines between those applications become
      the critical cost driver over time</p>
</div>



<p>Simplifying that glue code is certainly a noble effort, and integration
      tools can help, but not at the expense of building
      clean interfaces over capabilities. Importantly, the only effective judges
      of how easy an interface is to use are the actual users of it. Google
      could have asked us for more information to make their search
      implementation easier — geographical, recency, and popularity
      information, for example — but instead they offered only a single text
      box to type a search in and had to learn how to apply those factors into
      their algorithm. The same concern applies to API design (which I define
      broadly to include synchronous calls and asynchronous events).</p>

<p>Clean interfaces hide implementation details, and one of those
      implementation details in integration contexts is the choice of
      programming language. I have yet to see an architecture diagram that puts
      the primary focus on the programming languages of the systems
      involved:</p>

<div id="languages-in-diagram.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/languages-in-diagram.png"/></p><p>Figure 4: Emphasizing the implementation
      languages in architecture diagrams is unusual</p>
</div>



<p>Yet I have seen all too many variations of diagrams that do exactly
      that for integration. Such a view reinforces
      a tactical understanding of integration as wiring systems together, as
      it emphasizes the wiring toolchain instead of the digital capabilities.</p>





<p>Another implementation detail our API consumers would be happy to not
      care about is which systems the data comes from. Outside of the
      business users who work in SAP and the IT staff surrounding them, nobody
      in your organization should have to care about the quirks of the SAP
      system. They only care about how to get access to customer data or how to
      create an order. That observation is worth calling out separately, as it
      is one of the most commonly violated principles I see in integration
      strategies, and one of the strongest indicators of an implicit philosophy
      of integration as wiring systems together instead of creating clean interfaces
      over digital capabilities. You don’t need an SAP API, because your API users don’t care
      about SAP, but you might need an order management API. Abstract the
      capability, not the system.</p>

<p>Your users don’t stand still, and quite often good APIs add value
      through reuse. It’s easy to over-index on reuse as a primary goal of APIs
      (I believe taming complexity is a more important goal) but it’s still a
      useful aspiration. Keeping up with your users’ evolving needs means
      breaking previous assumptions, a classic programming-over-time concern.
      Carrying on with my previous metaphor, the job of a keyboard is to
      seamlessly integrate its users thoughts into on-screen text. As a native
      English speaker, I’ve never had to struggle with the
      <a href="https://en.wikipedia.org/wiki/Pinyin">Pinyin transliteration</a>
      that native Chinese speakers have to, but for several
      years I unnecessarily tortured myself by typing in the
      <a href="https://colemak.com/">Colemak</a> keyboard
      layout. Because my physical keyboard was incapable of magically adapting
      to the software layout, there was an impedance mismatch between the
      letters on the keyboard and what showed up on screen. Normally, that’s not
      a problem: as a (not particularly fast) touch typist, I’m used to not
      looking at the keyboard. However, that impedance mismatch made the
      learning process painfully difficult as I constantly had to look at an
      on-screen mapping to QWERTY and look down at the keys while my brain
      worked through the resultant confusion. I’m sure there are keyboards out
      there that are backlit and project the letter on the physical key in
      consonance with the keyboard layout. The price of that improved interface,
      of course, is more implementation complexity, and that evolution is a
      programming-over-time concern. </p>

<p>Integration interfaces that fail to adapt to users over time, or that
      change too easily with the underlying systems for implementation
      convenience, are point-in-time integrations, which are really just
      point-to-point integrations with multiple layers. They may wear API clothing,
      but show their true stripes every time a new system is wired into the estate
      and the API is duplicated or abused to solve an implementation problem.
      Point-in-time integrations add to inter-system tech debt.</p>

<p>Treating integration as primarily about systems results in a
      landscape littered with point-in-time integrations, decreasing
      organizational agility.</p>

<p>Of course, your creaking systems of record will resist any attempt to
      put them in a box. The ERP was specifically designed to do everything, so
      trying to externalize a new capability that still has to integrate with
      the ERP will be a challenge. It can require significant architectural
      skill to contain the resulting integration complexity and to hide it from
      the user, but the alternative is to increase your organizational tech
      debt, adding another noodle to the spaghetti mess of point-to-point or
      point-in-time integrations. The only way I’m aware of to pay that tech
      debt down is to hold the line on creating a clean interface for your users
      and create the needed transformations, caching, and orchestration to the
      downstream systems. If you don’t do that, you are forcing all users of the
      API to tackle that complexity, and they will have much less context than
      you.</p>

<p>We need to invert the mindset, from thinking of how to solve
      integration problems with our tools to instead thinking of how to build
      the right interfaces to maximize agility. </p>
</section>

<section id="UseAGeneralPurposeLanguageToManageTheInterfaceEvolution">
<h2>Use a general purpose language to manage the interface evolution</h2>

<p>Many commercial integration tools market their ability to own the
      integration landscape and call out to general purpose languages as needed. While I
      can appreciate the marketing behind such messaging — it promotes product
      penetration and lock-in — as architectural guidance, it is exactly
      backwards. Instead, we should almost always manage the interface evolution
      in a general purpose language for at least two reasons: so we can better
      manage the complexity of maintaining a clean interface, and so that we
      avoid the gravitational pull of our tool&#39;s mental model when making
      strategic integration decisions.</p>

<section id="GeneralPurposeLanguagesExcelAtProgrammingOverTime">
<h3>General purpose languages excel at programming over time</h3>

<p>Programming over time means making changes to source code over time,
        and this is one area where source-diagram isomorphism pales in
        comparison to normal development. The ability to “diff” changes between
        source code commits is a developer superpower, an invaluable debugging
        technique to understand the source of a defect or the context behind a
        change. Diffing the markup source code language of an integration tool
        is much harder than diffing Java code for at least three reasons:
        modularity, syntax, and translation.</p>

<p>Normally, the developer is in charge of the modularity of the source
        code. It is of course possible to throw all logic into a single file in
        Java  —  the classic <a href="https://en.wikipedia.org/wiki/God_object">
        God object</a>  —  but competent developers create clean
        boundaries in an application. Because they edit the textual source code
        directly, those module boundaries of the language correspond to
        filesystem boundaries. For example, in Java, packages correspond to
        directories and classes to files. A source code commit may change a
        number of lines of code, but those lines are likely to be localized to
        natural boundaries in the code that the team understands. With
        integration DSLs, the design palette has some control over the
        modularity of the underlying textual source code, the price you pay for
        source-diagram isomorphism. It is not uncommon to create, for example,
        the entire workflow in one file.</p>

<p>Similarly the markup language itself may consist of syntax that makes
        diffing harder. The good news is that the tools I’ve looked at do a good
        job of “pretty printing” the markup language, which adds line endings to
        make diffing easier. However, structural changes in a workflow are still
        more likely to cause, for example, a re-ordering of elements in the
        markup language, which will make a diff show many more lines of code
        changed than such an operation might intuitively warrant. Additionally, some
        languages, XML in particular, add a significant amount of noise,
        obscuring the actual logic change.</p>

<p>Finally, because you are programming at a higher level of abstraction
        in integration DSLs, you have a two step process to examine a diff.
        First, as you would with Java, you have to understand the changed lines
        in the context of the commit itself. With Java, since that source code
        is the same source code you edit, the understanding stops there. With an
        integration DSL, you have to make the additional mental leap of
        understanding what those changed lines of markup mean to the overall
        workflow, effectively mentally mapping them to what you would see on the
        design palette. The delta between source code commits can only be
        represented textually; graphical palettes are not designed to represent
        change over time. The net effect of all of this is to increase the
        cognitive load on the developer. </p>

<p>Gregor Hohpe has a brilliant story demonstrating the debuggability
        shortcomings of low code platforms. In
        <a href="https://www.amazon.com/Software-Architect-Elevator-Redefining-Architects/dp/1492077542">
        The Software Architect Elevator</a>,
        he describes his experience when vendors shop their wares at his
        company. Once they’ve shown how simple it is to drag and drop a solution
        together, he asks the technical sales person if she could leave the room
        for two minutes while Gregor tweaks something randomly in the underlying
        markup language so he could then see how she debugs it when she comes
        back in. So far, at least as of the publication of the book, no vendor
        has taken him up on his offer.</p>

<p>Commercial integration DSLs also make it harder to scale
        development within the same codebase. Not only is it harder to
        understand the context of changes over time for a single source file,
        it’s also harder to have multiple developers edit the same source file
        in parallel. This isn’t pain-free in a general purpose language, but is
        made possible by direct developer control over the modularity of the
        source code, which is why you rarely see teams of only one or two Java
        developers. With integration DSLs, given the constraints of source code
        modularity and the additional mental leap it takes to understand the
        source code — the markup source itself and the graphical workflow
        abstractions they represent — merging is considerably more painful.
        With such tools, it is quite common to constrain parallel development on
        the same codebase, and instead break the problem down into separate
        components that can be developed in parallel.</p>

<p>Programming over time requires advanced testing and environment
        promotion practices. Many integration tool vendors go out of their way
        to demonstrate their support for such practices, but once again, it is
        an inferior developer experience. Each test run, for example, will
        require spinning up the runtime that interprets the XML source code into
        machine code. In practical terms, that friction eliminates the
        possibility of short test driven development “red, green, refactor”
        feedback loops. Additionally, you will likely be limited to the vendor’s
        framework for any type of unit testing.</p>

<p>The ecosystems with general purpose programming languages evolve at a
        rapid clip. Advances in testing tools, IDEs, observability tools, and
        better abstractions benefit from the sheer scale of the community such
        languages operate in. Low-code platforms have much smaller ecosystems,
        limiting the ability to advance at the same pace, and the platform
        constraints will almost certainly force developers to use toolchains
        provided by the vendor to write and test code. That naturally has
        implications for security concerns like supply chain and static analysis
        scans. Such tooling gets a lot of attention for, say, Java open source libraries,
        but far less attention in the walled gardens of the low-code world.</p>

<p>Finally, integration tools offer comparatively impoverished
        operational support in their runtimes. Whereas observability tooling and
        resiliency patterns get a lot of attention for general purpose
        programming languages and the platforms that support them, those are
        not the main focus of integration tools. I&#39;ve seen multiple large-scale
        adoptions of low code integration tools result in considerable
        performance concerns, a problem that grows worse over time. It is
        usually addressed initially by additional licensing costs, until that
        too becomes prohibitive. Unfortunately, by that point, there is
        significant platform lock-in.</p>

<p>Low-code tools are insufficient to handle the same type of complexity
        that general purpose programming languages can handle. A colleague of
        mine described a contentious environment where he was dealing with a
        mandate to use TIBCO BusinessWorks, a well-known commercial integration
        tool. He challenged the TIBCO team to a bake-off: he would send his best
        Java / Spring developer to create an integration to another COTS
        product’s web services — SOAP interfaces coded in Apache Axis — and they
        could bring their best TIBCO developers to do the same. The Java
        developer had a working implementation by lunch. The TIBCO team
        discovered that the tool did not support the older version of Apache
        Axis used by the COTS product, the type of legacy complexity common
        in large enterprises. Following the mandate would have meant
        going back to the vendor and changing their roadmap or adding an
        extension in a general programming language. Fred Brooks called such
        extensions “accidental complexity” in his famous
        <a href="http://worrydream.com/refs/Brooks-NoSilverBullet.pdf"><i>
        No Silver Bullet</i></a> essay:
        they add complexity due to the choice of solution, and have nothing to
        do with the problem. Every mandate to use low-code tools for all
        integration will accrue significant accidental complexity.</p>

<p>Even more concerning than the accidental complexity needed to run all
        integration through commercial tooling, though, is the way such a
        mandate puts the emphasis on implementation over interface, on systems
        over capabilities.</p>
</section>

<section id="IntegrationToolsx201cthinkx201dInTermsOfImplementation">
<h3>Integration tools “think” in terms of implementation</h3>

<p>Integration tools were created, and continue to thrive today, because
        of the complexity of unlocking data and capabilities across the spectrum
        of IT systems. Your actual customer master data may reside within, for
        example, SAP, but the early part of a customer’s lifecycle exists in a
        Siebel CRM. The IBM mainframe system still handles core billing for some
        customers; an Oracle ERP for others. Now the business wants to replace
        Siebel with Salesforce. The business team bringing in a new product
        naturally understands that it will take some time to get the
        configuration right for adapting it to their sales intake process, but
        the last thing any of them want is to be told of long IT timelines just
        to sort out the glue between systems. It’s SaaS, after all!</p>

<p>Traditionally, those long timelines were the result of point-to-point
        integration, which did not allow for learning. Every new wire between
        systems meant teams had to re-learn how to connect, how to interpret the
        data, how to route between systems, and so on. Integration tools broke
        the problem down into smaller pieces, some of which could be reused,
        especially the connectivity into systems. Take a look at some of the
        actions available on the AWS Step Functions palette we looked at
        earlier:</p>

<div id="step-functions-commands.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/step-functions-commands.png"/></p><p>Figure 6: Each step in an AWS Step
        Functions workflow describes an implementation concern</p>
</div>



<p>Step Functions describes all of the actions in terms of some action
        on some AWS
        service. You can configure each box in the workflow to describe, for
        example, the DynamoDB table name, allowing you to focus on the overall
        flow in the main part of the palette. While Step Functions is a
        relatively new integration tool with an obvious bias towards cloud
        native AWS services, all integration tools that I’m familiar with tend
        to work along similar lines with their focus on implementation concerns.
        The early on-prem equivalents for application integration were
        enterprise service buses (ESBs), which separated out system connectivity
        as a reusable component from orchestration and routing. You can see that
        separation in a simplified view of
        <a href="https://www.mulesoft.com/resources/esb/what-esb">Mulesoft’s ESB</a>,
        so named because it aimed to remove the “donkey work” of integration:</p>

<div id="mulesoft-esb.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/mulesoft-esb.png"/></p><p>Figure 7: ESBs separate connectivity from orchestration
        and routing</p>
</div>



<p>There were some natural false starts in the ESB world as the industry
        aspired to have enterprise-wide canonical formats on the bus, but all of
        them shared the notion of adapters to the inputs and outputs of the bus — the
        systems being integrated. In the happy path, you could describe
        your integration in a language like BPEL, which could provide a
        graphical design palette and source-diagram isomorphism as it described
        the process in XML. </p>

<p>The industry has largely moved on from ESBs, but you can see their
        heritage in modern API platforms. Take a look, for example, at
        <a href="https://blogs.mulesoft.com/learn-apis/api-led-connectivity/what-is-api-led-connectivity/">
        Mulesoft’s three layer API architecture</a>:</p>

<div id="three-layer-architecture.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/three-layer-architecture.png"/></p><p>Figure 8: Mulesoft&#39;s three layer architecture
        maintains the separation of connectivity with experience and system APIs</p>
</div>



<p>Mulesoft sells both an API management platform and a low-code runtime
        for building APIs. You can and often should buy middleware infrastructure, and it is
        entirely possible to divorce the API gateway from the runtime, proxying
        to APIs built in a general purpose programming language. If you do so,
        the question arises: would you use Mulesoft’s three layer architecture
        if you built all of the APIs outside the Mulesoft runtime?</p>

<p>I quite like the idea of experience APIs. The name is less jargony
        than the one that’s caught on in the microservice
        community — <a href="https://samnewman.io/patterns/architectural/bff/">backends
        for frontends</a> — although I prefer the term “channel API” over both as
        it more obviously covers a broader range of concerns. For example,
        narrowing access to core APIs in a B2B scenario is clearly a channel
        concern, less obviously an “experience” or “frontend” concern. Whatever
        the name, providing an optimized channel-specific API is a valuable
        pattern, one that allows the channel to evolve at a different rate than
        the underlying capabilities and to narrow the surface area for
        attackers.</p>

<p>I’m less excited about the prescriptive separation between process
        and system APIs because of their focus on implementation over interface:
        the system layer focuses on connectivity and the process layer focuses
        on orchestration . I’ve redrawn their
        simplified ESB picture above to show that the similarity on implementation
        concerns to connect systems is hard to overlook:</p>

<div id="api-esb.png"><p><img src="https://blog.zulip.com/2021/12/01/zulip-server-4-8-security-release/cant-buy-integration/api-esb.png"/></p><p>Figure 9: The three layer architecture emphasizes
        implementation details, showing its ESB heritage</p>
</div>



<p>Part of the value proposition of a platform like Mulesoft — both its
        ESB and API runtime — lies in the built in library of connectors to
        systems like SAP and Salesforce, connectors that can save you time at
        the edges of the system (especially the system layer). The three
        layer architecture simplifies use of those connectors and separates
        orchestration and aggregation to encourage their reuse. </p>

<p>Conceptually, the three layer architecture serves to constrain
        designing APIs such that they fit inside Mulesoft’s ESB heritage. In
        theory, the architecture allows more reuse across layers. In practice,
        you are limited by programming-across-time concerns of evolving process
        APIs to multiple consumers. In fact, I have seen many APIs that
        are not APIs at all, but rather ETL in API clothing, with the system layer
        managing the extract, the process layer managing the transform, and the
        experience layer managing the load. That should not be surprising,
        because integration tools think in terms of implementation.</p>

<p>The allure of buying integration tools is that they make the tactical
        concern of wiring systems together cheaper, avoiding the usual expense and risk of
        custom software. Unfortunately, when we frame the problem space that
        way, we have allowed our tools to think for us.</p>
</section>
</section>

<div>
<p>We&#39;re releasing this article in installments. The next installment will
    examine where commercial integration tools can add considerable value.
      
    
    </p>

<p> To find out when we publish the next installment subscribe to the
    site&#39;s
    <a href="https://blog.zulip.com/feed.atom">RSS feed</a>,
    <a href="https://twitter.com/BrandonByars">Brandon&#39;s twitter stream</a>, or
    <a href="https://twitter.com/martinfowler">Martin&#39;s twitter stream</a></p>

</div>

<hr/>
</div></div>
  </body>
</html>
