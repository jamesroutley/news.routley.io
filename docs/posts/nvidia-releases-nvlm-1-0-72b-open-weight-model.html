<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/nvidia/NVLM-D-72B">Original</a>
    <h1>Nvidia releases NVLM 1.0 72B open weight model</h1>
    
    <div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START --><p>
  <img width="300" alt="Image Description" src="https://huggingface.co/nvidia/NVLM-D-72B/resolve/main/nvlm-logo-light.png"/>
</p>



<h2>
	<a rel="nofollow" href="#model-details" id="model-details">
		
	</a>
	<span>
		Model Details
	</span>
</h2>
<p>Today (September 17th, 2024), we introduce <a rel="nofollow" href="https://arxiv.org/abs/2409.11402">NVLM 1.0</a>, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. </p>
<p>In this repo, we are open-sourcing NVLM-1.0-D-72B (decoder-only architecture), the decoder-only model weights and code for the community.</p>
<h2>
	<a rel="nofollow" href="#other-resources" id="other-resources">
		
	</a>
	<span>
		Other Resources
	</span>
</h2>
<p><a rel="nofollow" href="https://huggingface.co/nvidia/NVLM-D-72B/tree/main">Inference Code (HF)</a>   <a rel="nofollow" href="">Training Code (Coming soon)</a>   <a rel="nofollow" href="https://research.nvidia.com/labs/adlr/NVLM-1/">Website</a>   <a rel="nofollow" href="https://arxiv.org/abs/2409.11402">Paper</a></p>
<h2>
	<a rel="nofollow" href="#benchmark-results" id="benchmark-results">
		
	</a>
	<span>
		Benchmark Results
	</span>
</h2>
<p>We train our model with legacy <a rel="nofollow" href="https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/legacy">Megatron-LM</a> and adapt the codebase to Huggingface for model hosting, reproducibility, and inference.
We observe numerical differences between the Megatron and Huggingface codebases, which are within the expected range of variation. 
We provide the results from both the Huggingface codebase and the Megatron codebase for reproducibility and comparison with other models.</p>
<p>Results (as of September 17th, 2024) in the multimodal benchmarks are as follows:</p>
<div>
	<table>
		<thead><tr>
<th>Benchmark</th>
<th>MMMU (val / test)</th>
<th>MathVista</th>
<th>OCRBench</th>
<th>AI2D</th>
<th>ChartQA</th>
<th>DocVQA</th>
<th>TextVQA</th>
<th>RealWorldQA</th>
<th>VQAv2</th>
</tr>

		</thead><tbody><tr>
<td>NVLM-D 1.0 72B (Huggingface)</td>
<td>58.7 / 54.9</td>
<td>65.2</td>
<td>852</td>
<td>94.2</td>
<td>86.0</td>
<td>92.6</td>
<td>82.6</td>
<td>69.5</td>
<td>85.4</td>
</tr>
<tr>
<td>NVLM-D 1.0 72B (Megatron)</td>
<td>59.7 / 54.6</td>
<td>65.2</td>
<td>853</td>
<td>94.2</td>
<td>86.0</td>
<td>92.6</td>
<td>82.1</td>
<td>69.7</td>
<td>85.4</td>
</tr>
<tr>
<td>Llama 3.2 90B</td>
<td>60.3 / -</td>
<td>57.3</td>
<td>-</td>
<td>92.3</td>
<td>85.5</td>
<td>90.1</td>
<td>-</td>
<td>-</td>
<td>78.1</td>
</tr>
<tr>
<td>Llama 3-V 70B</td>
<td>60.6 / -</td>
<td>-</td>
<td>-</td>
<td>93.0</td>
<td>83.2</td>
<td>92.2</td>
<td>83.4</td>
<td>-</td>
<td>79.1</td>
</tr>
<tr>
<td>Llama 3-V 405B</td>
<td>64.5 / -</td>
<td>-</td>
<td>-</td>
<td>94.1</td>
<td>85.8</td>
<td>92.6</td>
<td>84.8</td>
<td>-</td>
<td>80.2</td>
</tr>
<tr>
<td>InternVL2-Llama3-76B</td>
<td>55.2 / -</td>
<td>65.5</td>
<td>839</td>
<td>94.8</td>
<td>88.4</td>
<td>94.1</td>
<td>84.4</td>
<td>72.2</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4V</td>
<td>56.8 / 55.7</td>
<td>49.9</td>
<td>645</td>
<td>78.2</td>
<td>78.5</td>
<td>88.4</td>
<td>78.0</td>
<td>61.4</td>
<td>77.2</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>69.1 / -</td>
<td>63.8</td>
<td>736</td>
<td>94.2</td>
<td>85.7</td>
<td>92.8</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Claude 3.5 Sonnet</td>
<td>68.3 / -</td>
<td>67.7</td>
<td>788</td>
<td>94.7</td>
<td>90.8</td>
<td>95.2</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Gemini 1.5 Pro (Aug 2024)</td>
<td>62.2 / -</td>
<td>63.9</td>
<td>754</td>
<td>94.4</td>
<td>87.2</td>
<td>93.1</td>
<td>78.7</td>
<td>70.4</td>
<td>80.2</td>
</tr>
</tbody>
	</table>
</div>
<h2>
	<a rel="nofollow" href="#how-to-use" id="how-to-use">
		
	</a>
	<span>
		How to use
	</span>
</h2>
<p>When converting Megatron checkpoint to Huggingface, we adapt <a rel="nofollow" href="https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B">InternVL codebase</a> to support model loading and multi-GPU inference in HF. For training, please refer to <a rel="nofollow" href="">Megatron-LM (Coming soon)</a>.</p>
<h3>
	<a rel="nofollow" href="#prepare-the-environment" id="prepare-the-environment">
		
	</a>
	<span>
		Prepare the environment
	</span>
</h3>
<p>We provide a docker build file in the <a rel="nofollow" href="https://huggingface.co/nvidia/NVLM-D-72B/blob/main/Dockerfile">Dockerfile</a> for reproduction. </p>
<p>The docker image is based on <code>nvcr.io/nvidia/pytorch:23.09-py3</code>. </p>
<p><em>Note: We observe that different transformer versions / CUDA versions / docker versions can lead to slight benchmark number differences. We recommend using the Dockerfile above for precise reproduction.</em></p>
<h3>
	<a rel="nofollow" href="#model-loading" id="model-loading">
		
	</a>
	<span>
		Model loading
	</span>
</h3>
<pre><code><span>import</span> torch
<span>from</span> transformers <span>import</span> AutoModel

path = <span>&#34;nvidia/NVLM-D-72B&#34;</span>
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=<span>True</span>,
    use_flash_attn=<span>False</span>,
    trust_remote_code=<span>True</span>).<span>eval</span>()
</code></pre>
<h3>
	<a rel="nofollow" href="#multiple-gpus" id="multiple-gpus">
		
	</a>
	<span>
		Multiple GPUs
	</span>
</h3>
<p>The model can be loaded on multiple GPUs as follows:</p>
<pre><code><span>import</span> torch
<span>import</span> math
<span>from</span> transformers <span>import</span> AutoModel

<span>def</span> <span>split_model</span>():
    device_map = {}
    world_size = torch.cuda.device_count()
    num_layers = <span>80</span>
    
    num_layers_per_gpu = math.ceil(num_layers / (world_size - <span>0.5</span>))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[<span>0</span>] = math.ceil(num_layers_per_gpu[<span>0</span>] * <span>0.5</span>)
    layer_cnt = <span>0</span>
    <span>for</span> i, num_layer <span>in</span> <span>enumerate</span>(num_layers_per_gpu):
        <span>for</span> j <span>in</span> <span>range</span>(num_layer):
            device_map[<span>f&#39;language_model.model.layers.<span>{layer_cnt}</span>&#39;</span>] = i
            layer_cnt += <span>1</span>
    device_map[<span>&#39;vision_model&#39;</span>] = <span>0</span>
    device_map[<span>&#39;mlp1&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.model.tok_embeddings&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.model.embed_tokens&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.output&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.model.norm&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.lm_head&#39;</span>] = <span>0</span>
    device_map[<span>f&#39;language_model.model.layers.<span>{num_layers - <span>1</span>}</span>&#39;</span>] = <span>0</span>

    <span>return</span> device_map

path = <span>&#34;nvidia/NVLM-D-72B&#34;</span>
device_map = split_model()
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=<span>True</span>,
    use_flash_attn=<span>False</span>,
    trust_remote_code=<span>True</span>,
    device_map=device_map).<span>eval</span>()
</code></pre>
<h3>
	<a rel="nofollow" href="#inference" id="inference">
		
	</a>
	<span>
		Inference
	</span>
</h3>
<pre><code><span>import</span> torch
<span>from</span> transformers <span>import</span> AutoTokenizer, AutoModel
<span>import</span> math
<span>from</span> PIL <span>import</span> Image
<span>import</span> torchvision.transforms <span>as</span> T
<span>from</span> torchvision.transforms.functional <span>import</span> InterpolationMode


<span>def</span> <span>split_model</span>():
    device_map = {}
    world_size = torch.cuda.device_count()
    num_layers = <span>80</span>
    
    num_layers_per_gpu = math.ceil(num_layers / (world_size - <span>0.5</span>))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[<span>0</span>] = math.ceil(num_layers_per_gpu[<span>0</span>] * <span>0.5</span>)
    layer_cnt = <span>0</span>
    <span>for</span> i, num_layer <span>in</span> <span>enumerate</span>(num_layers_per_gpu):
        <span>for</span> j <span>in</span> <span>range</span>(num_layer):
            device_map[<span>f&#39;language_model.model.layers.<span>{layer_cnt}</span>&#39;</span>] = i
            layer_cnt += <span>1</span>
    device_map[<span>&#39;vision_model&#39;</span>] = <span>0</span>
    device_map[<span>&#39;mlp1&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.model.tok_embeddings&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.model.embed_tokens&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.output&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.model.norm&#39;</span>] = <span>0</span>
    device_map[<span>&#39;language_model.lm_head&#39;</span>] = <span>0</span>
    device_map[<span>f&#39;language_model.model.layers.<span>{num_layers - <span>1</span>}</span>&#39;</span>] = <span>0</span>

    <span>return</span> device_map


IMAGENET_MEAN = (<span>0.485</span>, <span>0.456</span>, <span>0.406</span>)
IMAGENET_STD = (<span>0.229</span>, <span>0.224</span>, <span>0.225</span>)


<span>def</span> <span>build_transform</span>(<span>input_size</span>):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(<span>lambda</span> img: img.convert(<span>&#39;RGB&#39;</span>) <span>if</span> img.mode != <span>&#39;RGB&#39;</span> <span>else</span> img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    <span>return</span> transform


<span>def</span> <span>find_closest_aspect_ratio</span>(<span>aspect_ratio, target_ratios, width, height, image_size</span>):
    best_ratio_diff = <span>float</span>(<span>&#39;inf&#39;</span>)
    best_ratio = (<span>1</span>, <span>1</span>)
    area = width * height
    <span>for</span> ratio <span>in</span> target_ratios:
        target_aspect_ratio = ratio[<span>0</span>] / ratio[<span>1</span>]
        ratio_diff = <span>abs</span>(aspect_ratio - target_aspect_ratio)
        <span>if</span> ratio_diff &lt; best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        <span>elif</span> ratio_diff == best_ratio_diff:
            <span>if</span> area &gt; <span>0.5</span> * image_size * image_size * ratio[<span>0</span>] * ratio[<span>1</span>]:
                best_ratio = ratio
    <span>return</span> best_ratio


<span>def</span> <span>dynamic_preprocess</span>(<span>image, min_num=<span>1</span>, max_num=<span>12</span>, image_size=<span>448</span>, use_thumbnail=<span>False</span></span>):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    
    target_ratios = <span>set</span>(
        (i, j) <span>for</span> n <span>in</span> <span>range</span>(min_num, max_num + <span>1</span>) <span>for</span> i <span>in</span> <span>range</span>(<span>1</span>, n + <span>1</span>) <span>for</span> j <span>in</span> <span>range</span>(<span>1</span>, n + <span>1</span>) <span>if</span>
        i * j &lt;= max_num <span>and</span> i * j &gt;= min_num)
    target_ratios = <span>sorted</span>(target_ratios, key=<span>lambda</span> x: x[<span>0</span>] * x[<span>1</span>])

    
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    
    target_width = image_size * target_aspect_ratio[<span>0</span>]
    target_height = image_size * target_aspect_ratio[<span>1</span>]
    blocks = target_aspect_ratio[<span>0</span>] * target_aspect_ratio[<span>1</span>]

    
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    <span>for</span> i <span>in</span> <span>range</span>(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + <span>1</span>) * image_size,
            ((i // (target_width // image_size)) + <span>1</span>) * image_size
        )
        
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    <span>assert</span> <span>len</span>(processed_images) == blocks
    <span>if</span> use_thumbnail <span>and</span> <span>len</span>(processed_images) != <span>1</span>:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    <span>return</span> processed_images


<span>def</span> <span>load_image</span>(<span>image_file, input_size=<span>448</span>, max_num=<span>12</span></span>):
    image = Image.<span>open</span>(image_file).convert(<span>&#39;RGB&#39;</span>)
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=<span>True</span>, max_num=max_num)
    pixel_values = [transform(image) <span>for</span> image <span>in</span> images]
    pixel_values = torch.stack(pixel_values)
    <span>return</span> pixel_values

path = <span>&#34;nvidia/NVLM-D-72B&#34;</span>
device_map = split_model()
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=<span>True</span>,
    use_flash_attn=<span>False</span>,
    trust_remote_code=<span>True</span>,
    device_map=device_map).<span>eval</span>()

<span>print</span>(model)

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=<span>True</span>, use_fast=<span>False</span>)
generation_config = <span>dict</span>(max_new_tokens=<span>1024</span>, do_sample=<span>False</span>)


question = <span>&#39;Hello, who are you?&#39;</span>
response, history = model.chat(tokenizer, <span>None</span>, question, generation_config, history=<span>None</span>, return_history=<span>True</span>)
<span>print</span>(<span>f&#39;User: <span>{question}</span>\nAssistant: <span>{response}</span>&#39;</span>)


pixel_values = load_image(<span>&#39;path/to/your/example/image.jpg&#39;</span>, max_num=<span>6</span>).to(
    torch.bfloat16)
question = <span>&#39;&lt;image&gt;\nPlease describe the image shortly.&#39;</span>
response = model.chat(tokenizer, pixel_values, question, generation_config)
<span>print</span>(<span>f&#39;User: <span>{question}</span>\nAssistant: <span>{response}</span>&#39;</span>)
</code></pre>
<h2>
	<a rel="nofollow" href="#correspondence-to" id="correspondence-to">
		
	</a>
	<span>
		Correspondence to
	</span>
</h2>
<p>Wenliang Dai* (<a rel="nofollow" href="mailto:wdai@nvidia.com">wdai@nvidia.com</a>), Nayeon Lee* (<a rel="nofollow" href="mailto:nayeonl@nvidia.com">nayeonl@nvidia.com</a>), Boxin Wang* (<a rel="nofollow" href="mailto:boxinw@nvidia.com">boxinw@nvidia.com</a>), Zhuolin Yang* (<a rel="nofollow" href="mailto:zhuoliny@nvidia.com">zhuoliny@nvidia.com</a>), Wei Ping* (<a rel="nofollow" href="mailto:wping@nvidia.com">wping@nvidia.com</a>)</p>
<p>*Equal contribution</p>
<h2>
	<a rel="nofollow" href="#citation" id="citation">
		
	</a>
	<span>
		Citation
	</span>
</h2>
<pre>@article{nvlm2024,
  title={NVLM: Open Frontier-Class Multimodal LLMs},
  author={Dai, Wenliang and Lee, Nayeon and Wang, Boxin and Yang, Zhuolin and Liu, Zihan and Barker, Jon and Rintamaki, Tuomas and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint},
  year={2024}}
</pre>


<h2>
	<a rel="nofollow" href="#license" id="license">
		
	</a>
	<span>
		License
	</span>
</h2>
<p>The use of this model is governed by the <a rel="nofollow" href="https://spdx.org/licenses/CC-BY-NC-4.0">cc-by-nc-4.0</a></p>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
