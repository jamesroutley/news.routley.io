<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.tansu.io/articles/broker-aws-free-tier">Original</a>
    <h1>Small Kafka: Tansu and SQLite on a free t3.micro</h1>
    
    <div id="readability-page-1" class="page"><div data-mdx-content="true"><p>AWS has a
<a href="https://aws.amazon.com/free/">free tier</a>
that can be used with the
<a href="https://aws.amazon.com/ec2/instance-types/t3/">t3micro</a> instance,
having 1GiB of memory and an EBS
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html">baseline throughput of ~10MB/s</a>.
Great for <strong>kick starting</strong> an early stage project.
A maximum <a href="https://aws.amazon.com/ebs/">EBS</a> throughput of ~260MB/s, creates some headroom to flex up.
Keep a
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html">beady eye on those CPU credits</a>
though!</p>
<p>I thought I&#39;d try the Tansu broker on a <code>t3micro</code> using the
embedded <a href="https://docs.tansu.io/docs/storage-engine-sqlite">SQLite</a> storage engine.
All meta and message data is stored in this database.
To backup or restore an environment, is as simple as copying <code>tansu.db</code>.
In a zero downtime environment, the
<a href="https://docs.tansu.io/docs/storage-engine-s3">S3</a> storage engine could be used,
allowing multiple stateless brokers to use the same bucket concurrently.</p>
<p>Let&#39;s spin up a <code>t3micro</code> instance with <code>Amazon Linux 2023</code>, to setup the following:</p>
<ul>
<li>The Supplementary Packages for Amazon Linux (<a href="https://docs.aws.amazon.com/linux/al2023/ug/spal.html">SPAL</a>) package repository</li>
<li><a href="https://docs.docker.com/compose/">Docker Compose</a> to run the Tansu broker docker image</li>
<li>Enable the <code>containerd</code> (docker) service</li>
<li>Add the <code>docker</code> group to the <code>ec2-user</code> so that we can run docker commands</li>
<li>Reboot after installing the services</li>
</ul>
<p>Using the following commands:</p>
<pre><code><span>sudo</span> dnf <span>install</span> -y spal-release
<span>sudo</span> dnf <span>install</span> -y <span>docker-compose</span>
<span>sudo</span> systemctl <span>enable</span> containerd.service
<span>sudo</span> <span>usermod</span> -a -G <span>docker</span> ec2-user
<span>sudo</span> /sbin/shutdown -r now
</code></pre>
<p>Log back in and create a <code>compose.yaml</code>, that will do the following:</p>
<ul>
<li>We are using the latest Tansu docker image from <a href="https://github.com/tansu-io/tansu/pkgs/container/tansu">ghcr.io/tansu-io/tansu</a>. The image is <a href="https://hub.docker.com/_/scratch">from scratch</a> containing <strong>only</strong> the Tansu statically linked binary (well, maybe a <a href="https://github.com/tansu-io/tansu/blob/main/LICENSE">LICENSE</a> and some SSL certificates too!).</li>
<li>Substitute the <code>ADVERTISED_LISTENER_URL</code> with the name of your instance</li>
<li>Default logging to the <code>warn</code> level</li>
<li>Use the SQLite <code>STORAGE_ENGINE</code> using the <code>/data</code> directory in the <strong>container</strong></li>
<li>Map the <code>/data</code> directory in the <strong>container</strong> to the current directory on the host</li>
<li>Expose the Kafka API on port 9092</li>
</ul>
<p>My <code>compose.yaml</code>, ensure that you change <code>ADVERTISED_LISTENER_URL</code> with the name of your instance:</p>
<pre><code><span>---</span>
<span>services</span><span>:</span>
  <span>tansu</span><span>:</span>
    <span>image</span><span>:</span> ghcr.io/tansu<span>-</span>io/tansu
    <span>environment</span><span>:</span>
      <span>ADVERTISED_LISTENER_URL</span><span>:</span> tcp<span>:</span>//ec2<span>-</span>35<span>-</span>179<span>-</span>120<span>-</span>103.eu<span>-</span>west<span>-</span>2.compute.amazonaws.com<span>:</span>9092/
      <span>RUST_LOG</span><span>:</span> $<span>{</span>RUST_LOG<span>:</span><span>-</span>warn<span>}</span>
      <span>STORAGE_ENGINE</span><span>:</span> <span>&#34;sqlite://data/tansu.db&#34;</span>
    <span>volumes</span><span>:</span>
      <span>-</span> ./<span>:</span>/data/
    <span>ports</span><span>:</span>
      <span>-</span> 9092<span>:</span><span>9092</span>
</code></pre>
<p>Start <code>tansu</code> with:</p>
<pre><code><span>docker</span> compose up -d
</code></pre>
<p>Check on the broker memory being used:</p>
<pre><code><span>ps</span> -p <span><span>$(</span>pgrep tansu<span>)</span></span> -o <span>rss</span><span>=</span> <span>|</span> <span>awk</span> <span>&#39;{print $1/1024 &#34; MB&#34;}&#39;</span>
<span>18.9336</span> MB
</code></pre>
<p>Create a <code>test</code> topic using the Tansu CLI:</p>
<pre><code><span>docker</span> compose <span>exec</span> tansu /tansu topic create <span>test</span>
</code></pre>
<p>Verify that the <code>test</code> topic has been created with:</p>
<pre><code><span>docker</span> compose <span>exec</span> tansu /tansu topic list <span>|</span> jq <span>&#39;.[].name&#39;</span>
</code></pre>
<p>Hopefully, at this point you see the name of the <code>test</code> topic.</p>
<p>I ran <code>kafka-producer-perf-test</code> on my local Mac Mini 4,
connecting to the ec2 instance in <code>eu-west-2</code> over the internet using a
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html">security group</a>
to restrict access:</p>
<pre><code>kafka-producer-perf-test <span>\</span>
 --topic <span>test</span> <span>\</span>
 --num-records <span>200000</span> <span>\</span>
 --record-size <span>1024</span> <span>\</span>
 --throughput <span>7000</span> <span>\</span>
 --producer-props bootstrap.servers<span>=</span>ec2-35-179-120-103.eu-west-2.compute.amazonaws.com:9092

<span>34518</span> records sent, <span>6892.6</span> records/sec <span>(</span><span>6.73</span> MB/sec<span>)</span>, <span>69.8</span> ms avg latency, <span>206.0</span> ms max latency.
<span>35357</span> records sent, <span>7067.2</span> records/sec <span>(</span><span>6.90</span> MB/sec<span>)</span>, <span>42.0</span> ms avg latency, <span>109.0</span> ms max latency.
<span>35031</span> records sent, <span>7004.8</span> records/sec <span>(</span><span>6.84</span> MB/sec<span>)</span>, <span>22.7</span> ms avg latency, <span>63.0</span> ms max latency.
<span>35058</span> records sent, <span>7010.2</span> records/sec <span>(</span><span>6.85</span> MB/sec<span>)</span>, <span>24.6</span> ms avg latency, <span>54.0</span> ms max latency.
<span>35025</span> records sent, <span>7005.0</span> records/sec <span>(</span><span>6.84</span> MB/sec<span>)</span>, <span>26.9</span> ms avg latency, <span>91.0</span> ms max latency.
<span>200000</span> records sent, <span>6989.3</span> records/sec <span>(</span><span>6.83</span> MB/sec<span>)</span>, <span>35.56</span> ms avg latency, <span>206.00</span> ms max latency, <span>26</span> ms 50th, <span>89</span> ms 95th, <span>163</span> ms 99th, <span>184</span> ms <span>99</span>.9th.
</code></pre>
<p>While the above is not going to beat any speed records, that&#39;s kind of the idea.
Kick start a project on low cost (virtual) hardware (in this case ~$0/hr).
Accumulate credit while the CPU is below baseline.
Spend those credits during periods of demand.
Scale onto bigger instances with demand, just by copying <code>tansu.db</code> into the new instance.</p>
<p>Breakdown of <code>t3</code> instance types with an emphasis on CPU baseline and credit accumulation:</p>
<table><thead><tr><th>name</th><th>vCPU</th><th>Baseline vCPU</th><th>CPU Credit/hr</th></tr></thead><tbody><tr><td>t3.micro</td><td>2</td><td>10%</td><td>12</td></tr><tr><td>t3.small</td><td>2</td><td>20%</td><td>24</td></tr><tr><td>t3.medium</td><td>2</td><td>20%</td><td>24</td></tr><tr><td>t3.large</td><td>2</td><td>30%</td><td>36</td></tr><tr><td>t3.xlarge</td><td>4</td><td>40%</td><td>96</td></tr><tr><td>t3.2xlarge</td><td>8</td><td>40%</td><td>192</td></tr></tbody></table>
<p>Broker memory size:</p>
<pre><code><span>[</span>ec2-user@ip-172-31-47-213 ~<span>]</span>$ <span>ps</span> -p <span><span>$(</span>pgrep tansu<span>)</span></span> -o <span>rss</span><span>=</span> <span>|</span> <span>awk</span> <span>&#39;{print $1/1024 &#34; MB&#34;}&#39;</span>
<span>27.3008</span> MB
</code></pre>
<p>The broker is generally pretty frugal, with tuning to
<a href="https://blog.tansu.io/articles/performance-tuning-i">reduce allocations</a>
and <a href="https://blog.tansu.io/articles/performance-tuning-ii-sqlite">cpu bottlenecks</a>.
It uses <a href="https://docs.rs/bytes/latest/bytes/">bytes</a> extensively, as an efficient container for storing and operating on contiguous slices of memory.</p>
<p>Free and used memory in the system:</p>
<pre><code><span>[</span>ec2-user@ip-172-31-47-213 ~<span>]</span>$ <span>free</span> -m
</code></pre>
<p>Plenty of memory remaining in reserve and for usage as the <a href="https://en.wikipedia.org/wiki/Page_cache">page cache</a>:</p>
<table><thead><tr><th>total</th><th>used</th><th>free</th><th>shared</th><th>buff/cache</th><th>available</th></tr></thead><tbody><tr><td>916 MiB</td><td>230 MiB</td><td>96 MiB</td><td>0 MiB</td><td>589 MiB</td><td>552 MiB</td></tr></tbody></table>
<p>Database size:</p>
<pre><code><span>[</span>ec2-user@ip-172-31-47-213 ~<span>]</span>$ <span>ls</span> -lh tansu.db
-rw-r--r--. <span>1</span> root root 265M Jan  <span>9</span> <span>11</span>:43 tansu.db
</code></pre>
<p><code>tansu.db</code> is a standard SQLite database file, you can use existing tools to inspect and update the data.</p>
<p>Obviously, other cloud providers are available. Any provider that can spin up a docker image can be used in this article. A statically linked Linux binary is also available with each release for environments that can&#39;t or if you prefer to run the broker directly.</p>
<p>Want to try it out for yourself? Clone (and ‚≠ê) Tansu at <a href="https://github.com/tansu-io/tansu">https://github.com/tansu-io/tansu</a>.</p>
<hr/>
<p>Tansu is an
Apache licensed Open Source Kafka compatible <a href="https://docs.tansu.io/docs/cli-broker">broker</a>,
<a href="https://docs.tansu.io/docs/cli-proxy">proxy</a>
and (early) <a href="https://docs.rs/tansu-client/latest/tansu_client/">client</a>
API written in async Rust with multiple storage engines
(<a href="https://docs.tansu.io/docs/storage-engine-memory">memory</a>,
<a href="https://docs.tansu.io/docs/storage-engine-null">null</a>,
<a href="https://docs.tansu.io/docs/storage-engine-pg">PostgreSQL</a>,
<a href="https://docs.tansu.io/docs/storage-engine-sqlite">SQLite</a>
and
<a href="https://docs.tansu.io/docs/storage-engine-s3">S3</a>).</p>
<p>Other articles include:</p>
<ul>
<li><a href="https://blog.tansu.io/articles/performance-tuning-i">Tuning the broker</a> with the <a href="https://docs.tansu.io/docs/storage-engine-null">null</a> storage engine using <a href="https://github.com/flamegraph-rs/flamegraph">cargo flamegraph</a></li>
<li><a href="https://blog.tansu.io/articles/performance-tuning-ii-sqlite">CPU bottlenecks starting with a regular expression</a>, stopped copying uncompressed data and used a faster CRC32 implementation using the <a href="https://docs.tansu.io/docs/storage-engine-sqlite">SQLite</a> storage engine</li>
<li><a href="https://blog.tansu.io/articles/route-layer-service">Route, Layer and Process Kafka Messages with Tansu Services</a>, the composable layers that are used to build the Tansu <a href="https://docs.tansu.io/docs/cli-broker">broker</a> and <a href="https://docs.tansu.io/docs/cli-proxy">proxy</a></li>
<li><a href="https://blog.tansu.io/articles/serde-kafka-protocol">Apache Kafka protocol with serde, quote, syn and proc_macro2</a>, a walk through of the low level Kafka protocol implementation used by Tansu</li>
<li><a href="https://blog.tansu.io/articles/parquet">Effortlessly Convert Kafka Messages to Apache Parquet with Tansu: A Step-by-Step Guide</a>, using a schema backed topic to write data into the Parquet open table format</li>
<li><a href="https://blog.tansu.io/articles/fly-with-tigris">Using Tansu with Tigris on Fly</a>, spin up (and down!) a broker on demand</li>
<li><a href="https://blog.tansu.io/articles/gh-smoke-test-with-bats">Smoke Testing with the Bash Automated Testing System ü¶á</a>, a look at the integration tests that are part of the Tansu CI system</li>
</ul></div></div>
  </body>
</html>
