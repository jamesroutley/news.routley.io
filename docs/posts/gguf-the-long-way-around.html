<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/">Original</a>
    <h1>GGUF, the Long Way Around</h1>
    
    <div id="readability-page-1" class="page"><div><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/dbbb8ee7-f19f-44df-bce7-2612817cacd2" width="400"/></figure><p><strong>Table of Contents</strong></p><ul><li><a href="#how-we-use-llm-artifacts">How We Use LLM Artifacts</a></li><li><a href="#what-is-a-machine-learning-model">What is a machine learning model</a><ul><li><a href="#starting-with-a-simple-model">Starting with a simple model</a></li></ul></li><li><a href="#writing-the-model-code">Writing the model code</a><ul><li><a href="#instantiating-the-model-object">Instantiating the model object</a></li><li><a href="#serializing-our-objects">Serializing our objects</a></li></ul></li><li><a href="#what-is-a-file">What is a file</a></li><li><a href="#how-does-pytorch-write-objects-to-files">How does PyTorch write objects to files?</a><ul><li><a href="#how-pickle-works">How Pickle works</a></li><li><a href="#from-pickle-to-safetensors">From pickle to safetensors</a></li><li><a href="#how-safetensors-works">How safetensors works</a></li><li><a href="#checkpoint-files">Checkpoint files</a></li><li><a href="#ggml">GGML</a></li><li><a href="#finally-gguf">Finally, GGUF</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><p>Large language models today are <a href="https://vickiboykis.com/2024/01/15/whats-new-with-ml-in-production/">consumed in one of several ways</a>:</p><ol><li>As API endpoints for proprietary models hosted by OpenAI, Anthropic, or major cloud providers</li><li>As model artifacts downloaded from HuggingFace’s Model Hub and/or trained/fine-tuned using HuggingFace libraries and hosted on local storage</li><li>As model artifacts available in a format optimized for local inference, typically GGUF, and accessed via applications like <code>llama.cpp</code> or <code>ollama</code></li><li>As <a href="https://onnx.ai/">ONNX</a>, a format which optimizes sharing between backend ML frameworks</li></ol><p>For a side project, I’m using <code>llama.cpp</code>, a <code>C/C++</code>-based LLM inference engine targeting <a href="https://github.com/ggerganov/llama.cpp/discussions/4167">M-series GPUs on Apple Silicon</a>.</p><p>When running <code>llama.cpp</code>, you get a long log that consists primarily of key-value pairs of metadata about your model architecture and then its performance (and <a href="https://twitter.com/vboykis/status/1751307750712156662">no yapping</a>).</p><div><pre tabindex="0"><code data-lang="bash"><span><span>make -j <span>&amp;&amp;</span> ./main -m /Users/vicki/llama.cpp/models/mistral-7b-instruct-v0.2.Q8_0.gguf -p <span>&#34;What is Sanremo? no yapping&#34;</span>
</span></span><span><span>
</span></span><span><span>Sanremo Music Festival <span>(</span>Festival di Sanremo<span>)</span> is an annual Italian music competition held in the city of Sanremo since 1951. It<span>&#39;</span>s considered one of the most prestigious and influential events in the Italian music scene. The festival features both newcomers and established artists competing <span>for</span> various awards, including the Big Award <span>(</span>Gran Premio<span>)</span>, which grants the winner the right to represent Italy in the Eurovision Song Contest. The event consists of several live shows where artists perform their original songs, and a jury composed of musicians, critics, and the public determines the winners through a combination of points. <span>[</span>end of text<span>]</span>
</span></span><span><span>
</span></span><span><span>llama_print_timings:        load time <span>=</span>   11059.32 ms
</span></span><span><span>llama_print_timings:      sample time <span>=</span>      11.62 ms /   <span>140</span> runs   <span>(</span>    0.08 ms per token, 12043.01 tokens per second<span>)</span>
</span></span><span><span>llama_print_timings: prompt eval time <span>=</span>      87.81 ms /    <span>10</span> tokens <span>(</span>    8.78 ms per token,   113.88 tokens per second<span>)</span>
</span></span><span><span>llama_print_timings:        eval time <span>=</span>    3605.10 ms /   <span>139</span> runs   <span>(</span>   25.94 ms per token,    38.56 tokens per second<span>)</span>
</span></span><span><span>llama_print_timings:       total time <span>=</span>    3730.78 ms /   <span>149</span> tokens
</span></span><span><span>ggml_metal_free: deallocating
</span></span><span><span>Log end
</span></span></code></pre></div><p>These logs can be found in the <code>Llama.cpp</code> codebase. There, you’ll also find GGUF. <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">GGUF (GPT-Generated Unified Format)</a> is the file format used to serve models on <code>Llama.cpp</code> and other local runners like <a href="https://semaphoreci.com/blog/local-llm">Llamafile, Ollama and GPT4All.</a></p><p>To understand how GGUF works, we need to first take a deep dive into machine learning models and the kinds of artifacts they produce.</p><p>Let’s start by describing a machine learning model. At its simplest, a model is a file or a collection of files that contain the model architecture and weights and biases of the model generated from a training loop.</p><p>In LLM land, we’re generally interested in <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">transformer-style models and architectures.</a></p><p>In a transformer, we have many moving parts.</p><ul><li><strong>For the input</strong>, we use <a href="https://arxiv.org/abs/2310.20707">training data corpuses aggregated from human-generated nautural language content</a></li><li>For <strong>the algorithm</strong>, we<ul><li>Convert that data <a href="https://vickiboykis.com/what_are_embeddings/">into embeddings</a></li><li><a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html#positional-encoding">Positionally encoding the embeddings</a> to provide information about where the words are in relation to each other in the sequence</li><li>Creating multi-headed <a href="https://ai.stackexchange.com/a/43892">self-attention</a> for each word in relation to each other word in the sequence based on an initialized combinations of weights</li><li><a href="https://arxiv.org/abs/2302.06461">Normalize layers via softmax</a></li><li>Run the resulting matrix through a feedfoward neural network</li><li>Project the output into the correct vector space for the desired task</li><li>Calculate loss and then update model parameters</li></ul></li><li><strong>The output</strong>: Generally for for chat completions tasks, <a href="https://arxiv.org/abs/2311.17301">the model returns the statistical likelihood</a> that any given word completes a phrase. It does this again and again for every word in the phrase, because of its autoregressive nature.</li></ul><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/86da416b-c6d8-4fb7-abf9-fcd2a80a1614" width="400"/></figure><a href="https://arxiv.org/abs/2311.17301">Source.</a><p>If the model is served as a consumer end-product, it only returns the actual text output based on the highest probabilities, with numerous strategies for <a href="https://huggingface.co/blog/how-to-generate">how that text is selected.</a></p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/cb311adb-79f3-4eea-85be-7329e4aeb111" width="600"/></figure><p>In short, we convert inputs to outputs using an equation. In addition to the model’s output, we also have the model itself that is generated as an artifact of the modeling process.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/67102dbc-d049-4131-997a-df1fa5378f91" width="600"/></figure><h2 id="starting-with-a-simple-model">Starting with a simple model</h2><p>Let’s take a step back from the complexity of transformers and build a small linear regression model in PyTorch. Lucky for us, <a href="https://d2l.ai/chapter_linear-regression/index.html">linear regression is also</a> a (shallow) neural network, so we can work with it in PyTorch and map our simple model to more complex ones using the same framework.</p><p>Linear regression takes a set of numerical inputs and generates a set of numerical outputs. (In contrast to transformers, which take a set of text inputs and generates a set of text inputs and their related numerical probabilities.)</p><p>For example, let’s say that we produce <a href="https://www.greatitalianchefs.com/features/hazelnuts-piedmont">artisinal hazlenut spread</a> for statisticians, and want to predict how many jars of Nulltella we’ll produce on any given day. Let’s say we have some data available to us, and that is, how many hours of sunshine we have per day, and how many jars of Nulltella we’ve been able to produce every day.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/66ca00e6-1baf-4eb0-9d3d-112966beb797" width="200"/></figure><p>It turns out that we feel more inspired to produce hazlenut spread when it’s sunny out, and we can clearly see this relationship between input and output in our data (we do not produce Nulltella Friday-Sunday because we prefer to spend those days writing about data serialization formats):</p><pre tabindex="0"><code>| day_id | hours   | jars |
|--------|---------|------|
| mon    | 1       | 2    |
| tues   | 2       | 4    |
| wed    | 3       | 6    |
| thu    | 4       | 8    |
</code></pre><p>This is the data we’ll use to train our model. We’ll need to split this data into three parts:</p><ol><li>used to train our model (training data)</li><li>used to test the accuracy of our model (test data)</li><li>used to tune our hyperparameters, meta-aspects of our model like the <a href="https://en.wikipedia.org/wiki/Learning_rate">learning rate</a>, (validation set) during the model training phase.</li></ol><p>In the specific case of linear regression, there technically are no hyperparameters, although we can plausibly consider the learning rate we set in PyTorch to be one. Let’s assume we have 100 of these data points values.</p><p>We split the data into train, test, and validation. A <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/sam.11583">usual accepted split</a> is to use 80% of data for training/validation and 20% for testing. We want our model to have access to as much data as possible so it learns a more accurate representation, so we leave most data for train.</p><p>Now that we have our data, we need to write our algorithm. The equation to get output \(Y\) from inputs \(X\) for linear regression is:</p><p>$$y = \beta_0 + \beta_1 x_1 + \varepsilon $$</p><p>This tells us that the output, \(y\) (the number of jars of Nulltella), can be predicted by:</p><ul><li>\(x_1\) - one input variable (or feature), (hours of sunshine)</li><li>\(\beta_1\) - with its given weight, also called parameters, (how important that feature is)</li><li>plus an error term \(\varepsilon\) that is the difference between the observed and actual values in a population that captures the noise of the model</li></ul><p>Our task is to continuously predict and adjust our weights to optimally solve this equation for the difference between our actual \(Y\) as presented by our data and a predicted \(\hat Y\) based on the algorithm to find the smallest sum of squared differences, \(\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}\), between each point and the line. In other words, we’d like to minimize \(\varepsilon\), because it will mean that, at each point, our \(\hat Y\) is as close to our actual \(Y\) as we can get it, given the other points.</p><p>We optimize this function <a href="https://arxiv.org/abs/1609.04747">through gradient descent</a>, where we start with either zeros or randomly-initialized weights and continue recalculating both the weights and error term until we come to an optimal stopping point.
We’ll know we’re succeeding because our loss, as calculated by RMSE should incrementally decrease in every training iteration.</p><p>Here’s the whole model learning process end-to-end (with the exception of tokenization, which we only do for models where features are text and we want to do language modeling):</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/9f8fb4b8-4b19-45e2-bb04-7657e447d42f" width="600"/></figure><p>Now, let’s get more concrete and describe these ideas in code. When we train our model, we initialize our function with a set of feature values.</p><p>Let’s add our data into the model by initializing both \(x_1\) and \(Y\) as <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch Tensor objects</a>.</p><div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span># Hours of sunshine</span>
</span></span><span><span>X <span>=</span> torch<span>.</span>tensor([[<span>1.0</span>], [<span>2.0</span>], [<span>3.0</span>], [<span>4.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>
</span></span><span><span><span># Jars of Nulltella</span>
</span></span><span><span>y <span>=</span> torch<span>.</span>tensor([[<span>2.0</span>], [<span>4.0</span>], [<span>6.0</span>], [<span>8.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span></code></pre></div><p>Within code, our input data is <code>X</code>, which is a torch tensor object, and our output data is <code>y</code>. We initialize a LinearRegression which subclasses the PyTorch Module, with one linear layer, which has one input feature (sunshine) and one output feature (jars of Nulltella).</p><p>I’m going to include the code for the whole model, and then we’ll talk through it piece by piece.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch
</span></span><span><span><span>import</span> torch.nn <span>as</span> nn
</span></span><span><span><span>import</span> torch.optim <span>as</span> optim
</span></span><span><span>
</span></span><span><span>X <span>=</span> torch<span>.</span>tensor([[<span>1.0</span>], [<span>2.0</span>], [<span>3.0</span>], [<span>4.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>y <span>=</span> torch<span>.</span>tensor([[<span>2.0</span>], [<span>4.0</span>], [<span>6.0</span>], [<span>8.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>
</span></span><span><span><span># Define a linear regression model and its forward pass </span>
</span></span><span><span><span>class</span> <span>LinearRegression</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>def</span> __init__(self):
</span></span><span><span>        super(LinearRegression, self)<span>.</span>__init__()
</span></span><span><span>        self<span>.</span>linear <span>=</span> nn<span>.</span>Linear(<span>1</span>, <span>1</span>)  <span># 1 input feature, 1 output feature</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        <span>return</span> self<span>.</span>linear(x)
</span></span><span><span>
</span></span><span><span><span># Instantiate the model</span>
</span></span><span><span>model <span>=</span> LinearRegression()
</span></span><span><span>
</span></span><span><span><span># Inspect the model&#39;s state dictionary</span>
</span></span><span><span>print(model<span>.</span>state_dict())
</span></span><span><span>
</span></span><span><span><span># Define loss function and optimizer</span>
</span></span><span><span>criterion <span>=</span> nn<span>.</span>MSELoss() 
</span></span><span><span><span># setting our learning rate &#34;hyperparameter&#34; here</span>
</span></span><span><span>optimizer <span>=</span> optim<span>.</span>SGD(model<span>.</span>parameters(), lr<span>=</span><span>0.01</span>)  
</span></span><span><span>
</span></span><span><span><span># Training loop that includes forward and backward pass </span>
</span></span><span><span>num_epochs <span>=</span> <span>100</span>
</span></span><span><span><span>for</span> epoch <span>in</span> range(num_epochs):
</span></span><span><span>    <span># Forward pass</span>
</span></span><span><span>    outputs <span>=</span> model(X)
</span></span><span><span>    loss <span>=</span> criterion(outputs, y)
</span></span><span><span>    RMSE_loss  <span>=</span> torch<span>.</span>sqrt(loss)
</span></span><span><span>
</span></span><span><span>    <span># Backward pass and optimization</span>
</span></span><span><span>    optimizer<span>.</span>zero_grad()  <span># Zero out gradients</span>
</span></span><span><span>    RMSE_loss<span>.</span>backward()  <span># Compute gradients</span>
</span></span><span><span>    optimizer<span>.</span>step()  <span># Update weights</span>
</span></span><span><span>
</span></span><span><span>    <span># Print progress</span>
</span></span><span><span>    <span>if</span> (epoch<span>+</span><span>1</span>) <span>%</span> <span>10</span> <span>==</span> <span>0</span>:
</span></span><span><span>        print(<span>f</span><span>&#39;Epoch [</span><span>{</span>epoch<span>+</span><span>1</span><span>}</span><span>/</span><span>{</span>num_epochs<span>}</span><span>], Loss: </span><span>{</span>loss<span>.</span>item()<span>:</span><span>.4f</span><span>}</span><span>&#39;</span>)
</span></span><span><span>
</span></span><span><span><span># After training, let&#39;s test the model</span>
</span></span><span><span>test_input <span>=</span> torch<span>.</span>tensor([[<span>5.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>predicted_output <span>=</span> model(test_input)
</span></span><span><span>print(<span>f</span><span>&#39;Prediction for input </span><span>{</span>test_input<span>.</span>item()<span>}</span><span>: </span><span>{</span>predicted_output<span>.</span>item()<span>}</span><span>&#39;</span>)
</span></span></code></pre></div><p>Once we have our input data, we then initialize our model, a <code>LinearRegression</code> which subclasses <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">Module base class</a> specifically for <a href="https://github.com/pytorch/pytorch/blob/372d078f361e726bb4ac0884ac334b04c58179ef/torch/nn/modules/linear.py#L49">linear regression.</a></p><p>A forward pass involves feeding our data into the neural network and making sure it propogagtes through all the layers. Since we only have one, we have to pass our data to a single linear layer. The forward pass is what calculates our predicted <code>Y</code>.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>LinearRegression</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>def</span> __init__(self):
</span></span><span><span>        super(LinearRegression, self)<span>.</span>__init__()
</span></span><span><span>        self<span>.</span>linear <span>=</span> nn<span>.</span>Linear(<span>1</span>, <span>1</span>)  <span># 1 input feature, 1 output feature</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        <span>return</span> self<span>.</span>linear(x)
</span></span></code></pre></div><p>We pick how we’d like to optimize the results of the model, aka how its loss should converge. In this case, we start with <code>mean squared error</code>, and then modify it to use <code>RMSE</code>, the square root of the average squared difference between the predicted values and the actual values in a dataset.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Define loss function and optimizer</span>
</span></span><span><span>criterion <span>=</span> torch<span>.</span>sqrl(nn<span>.</span>MSELoss())  <span># RMSE in the training loop</span>
</span></span><span><span>optimizer <span>=</span> optim<span>.</span>SGD(model<span>.</span>parameters(), lr<span>=</span><span>0.01</span>)
</span></span><span><span>
</span></span><span><span><span>....</span>
</span></span><span><span><span>for</span> epoch <span>in</span> range(num_epochs):
</span></span><span><span>    <span># Forward pass</span>
</span></span><span><span>    outputs <span>=</span> model(X)
</span></span><span><span>    loss <span>=</span> criterion(outputs, y)
</span></span><span><span>    RMSE_loss  <span>=</span> torch<span>.</span>sqrt(loss)
</span></span></code></pre></div><p>Now that we’ve defined how we’d like the model to run, we can instantiate the model object itself:</p><h2 id="instantiating-the-model-object">Instantiating the model object</h2><div><pre tabindex="0"><code data-lang="python"><span><span>model <span>=</span> LinearRegression()
</span></span><span><span>print(model<span>.</span>state_dict())
</span></span></code></pre></div><p>Notice that when we instantiate a <code>nn.Module</code>, it has an attribute called the “state_dict”. This is important. <a href="https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html">The state dict</a> holds the information about each layer and the parameters in each layer, aka the weights and biases.</p><p>At its heart, <a href="https://github.com/pytorch/pytorch/blob/637cf4a3f2cfdd364005681636ca885bdc4d5887/torch/nn/modules/module.py#L1842">it’s a Python dictionary.</a></p><p>In this case, the implementation for LinearRegression returns an ordered dict with each layer of the network and values of those layers. Each of the values is a <code>Tensor</code>.</p><div><pre tabindex="0"><code data-lang="python"><span><span>OrderedDict([(<span>&#39;linear.weight&#39;</span>, tensor([[<span>0.5408</span>]])), (<span>&#39;linear.bias&#39;</span>, tensor([<span>-</span><span>0.8195</span>]))])
</span></span><span><span>
</span></span><span><span><span>for</span> param_tensor <span>in</span> model<span>.</span>state_dict():
</span></span><span><span>    print(param_tensor, <span>&#34;</span><span>\t</span><span>&#34;</span>, model<span>.</span>state_dict()[param_tensor]<span>.</span>size())
</span></span><span><span>
</span></span><span><span>linear<span>.</span>weight    torch<span>.</span>Size([<span>1</span>, <span>1</span>])
</span></span><span><span>linear<span>.</span>bias      torch<span>.</span>Size([<span>1</span>])
</span></span></code></pre></div><p>For our tiny model, it’s a small <code>OrderedDict</code> of tuples. You can imagine that this collection of tensors becomes extremely large and memory-intensive in a large network such as a transformer. If each parameter (each Tensor object) takes up 2 bytes in memory, <a href="https://github.com/ray-project/llm-numbers?tab=readme-ov-file#2x-number-of-parameters-typical-gpu-memory-requirements-of-an-llm-for-serving">a 7-billion parameter model can take up 14GB in GPU.</a></p><p>We then run the forward and backward passes for the model in loops. In each step, we do a forward pass to perform the calculation, a backward pass to update the weights of our model object, and then we add all that information to our model parameters.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Define loss function and optimizer</span>
</span></span><span><span>criterion <span>=</span> nn<span>.</span>MSELoss() 
</span></span><span><span>optimizer <span>=</span> optim<span>.</span>SGD(model<span>.</span>parameters(), lr<span>=</span><span>0.01</span>)  
</span></span><span><span>
</span></span><span><span><span># Training loop </span>
</span></span><span><span>num_epochs <span>=</span> <span>100</span>
</span></span><span><span><span>for</span> epoch <span>in</span> range(num_epochs):
</span></span><span><span>    <span># Forward pass</span>
</span></span><span><span>    outputs <span>=</span> model(X)
</span></span><span><span>    loss <span>=</span> criterion(outputs, y)
</span></span><span><span>    RMSE_loss  <span>=</span> torch<span>.</span>sqrt(loss)
</span></span><span><span>
</span></span><span><span>    <span># Backward pass and optimization</span>
</span></span><span><span>    optimizer<span>.</span>zero_grad()  <span># Zero out gradients</span>
</span></span><span><span>    RMSE_loss<span>.</span>backward()  <span># Compute gradients</span>
</span></span><span><span>    optimizer<span>.</span>step()  <span># Update weights</span>
</span></span><span><span>
</span></span><span><span>    <span># Print progress</span>
</span></span><span><span>    <span>if</span> (epoch<span>+</span><span>1</span>) <span>%</span> <span>10</span> <span>==</span> <span>0</span>:
</span></span><span><span>        print(<span>f</span><span>&#39;Epoch [</span><span>{</span>epoch<span>+</span><span>1</span><span>}</span><span>/</span><span>{</span>num_epochs<span>}</span><span>], Loss: </span><span>{</span>loss<span>.</span>item()<span>:</span><span>.4f</span><span>}</span><span>&#39;</span>)
</span></span></code></pre></div><p>Once we’ve completed these loops, we’ve trained the model artifact. What we now have once we have trained a model is an in-memory object that represents the weights, biases, and metadata of that model, stored within our instance of our <code>LinearRegression</code> module.</p><p>As we run the training loop, we can see our loss shrink. That is, the actual values are getting closer to the predicted:</p><div><pre tabindex="0"><code data-lang="python"><span><span>Epoch [<span>10</span><span>/</span><span>100</span>], Loss: <span>33.0142</span>
</span></span><span><span>Epoch [<span>20</span><span>/</span><span>100</span>], Loss: <span>24.2189</span>
</span></span><span><span>Epoch [<span>30</span><span>/</span><span>100</span>], Loss: <span>16.8170</span>
</span></span><span><span>Epoch [<span>40</span><span>/</span><span>100</span>], Loss: <span>10.8076</span>
</span></span><span><span>Epoch [<span>50</span><span>/</span><span>100</span>], Loss: <span>6.1890</span>
</span></span><span><span>Epoch [<span>60</span><span>/</span><span>100</span>], Loss: <span>2.9560</span>
</span></span><span><span>Epoch [<span>70</span><span>/</span><span>100</span>], Loss: <span>1.0853</span>
</span></span><span><span>Epoch [<span>80</span><span>/</span><span>100</span>], Loss: <span>0.4145</span>
</span></span><span><span>Epoch [<span>90</span><span>/</span><span>100</span>], Loss: <span>0.3178</span>
</span></span><span><span>Epoch [<span>100</span><span>/</span><span>100</span>], Loss: <span>0.2974</span>
</span></span></code></pre></div><p>We can also see if we print out the <code>state_dict</code> that the parameters have changed as we’ve computed the gradients and updated the weights in the backward pass:</p><div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span>&#34;&#34;&#34;before&#34;&#34;&#34;</span>
</span></span><span><span>OrderedDict([(<span>&#39;linear.weight&#39;</span>, tensor([[<span>-</span><span>0.6216</span>]])), (<span>&#39;linear.bias&#39;</span>, tensor([<span>0.7633</span>]))])
</span></span><span><span>linear<span>.</span>weight    torch<span>.</span>Size([<span>1</span>, <span>1</span>])
</span></span><span><span>linear<span>.</span>bias      torch<span>.</span>Size([<span>1</span>])
</span></span><span><span>{<span>&#39;state&#39;</span>: {}, <span>&#39;param_groups&#39;</span>: [{<span>&#39;lr&#39;</span>: <span>0.01</span>, <span>&#39;momentum&#39;</span>: <span>0</span>, <span>&#39;dampening&#39;</span>: <span>0</span>, <span>&#39;weight_decay&#39;</span>: <span>0</span>, <span>&#39;nesterov&#39;</span>: <span>False</span>, <span>&#39;maximize&#39;</span>: <span>False</span>, <span>&#39;foreach&#39;</span>: <span>None</span>, <span>&#39;differentiable&#39;</span>: <span>False</span>, <span>&#39;params&#39;</span>: [<span>0</span>, <span>1</span>]}]}
</span></span><span><span>Epoch [<span>10</span><span>/</span><span>100</span>], Loss: <span>33.0142</span>
</span></span><span><span>Epoch [<span>20</span><span>/</span><span>100</span>], Loss: <span>24.2189</span>
</span></span><span><span>Epoch [<span>30</span><span>/</span><span>100</span>], Loss: <span>16.8170</span>
</span></span><span><span>Epoch [<span>40</span><span>/</span><span>100</span>], Loss: <span>10.8076</span>
</span></span><span><span>Epoch [<span>50</span><span>/</span><span>100</span>], Loss: <span>6.1890</span>
</span></span><span><span>Epoch [<span>60</span><span>/</span><span>100</span>], Loss: <span>2.9560</span>
</span></span><span><span>Epoch [<span>70</span><span>/</span><span>100</span>], Loss: <span>1.0853</span>
</span></span><span><span>Epoch [<span>80</span><span>/</span><span>100</span>], Loss: <span>0.4145</span>
</span></span><span><span>Epoch [<span>90</span><span>/</span><span>100</span>], Loss: <span>0.3178</span>
</span></span><span><span>Epoch [<span>100</span><span>/</span><span>100</span>], Loss: <span>0.2974</span>
</span></span><span><span>
</span></span><span><span><span>&#34;&#34;&#34;after&#34;&#34;&#34;</span>
</span></span><span><span>OrderedDict([(<span>&#39;linear.weight&#39;</span>, tensor([[<span>1.5441</span>]])), (<span>&#39;linear.bias&#39;</span>, tensor([<span>1.3291</span>]))])
</span></span></code></pre></div><p>The optimizer, as we see, has its own <code>state_dict</code>, which consists of these hyperparameters we discussed before: the learning rate, the weight decay, and more:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(optimizer<span>.</span>state_dict())
</span></span><span><span>{<span>&#39;state&#39;</span>: {}, <span>&#39;param_groups&#39;</span>: [{<span>&#39;lr&#39;</span>: <span>0.01</span>, <span>&#39;momentum&#39;</span>: <span>0</span>, <span>&#39;dampening&#39;</span>: <span>0</span>, <span>&#39;weight_decay&#39;</span>: <span>0</span>, <span>&#39;nesterov&#39;</span>: <span>False</span>, <span>&#39;maximize&#39;</span>: <span>False</span>, <span>&#39;foreach&#39;</span>: <span>None</span>, <span>&#39;differentiable&#39;</span>: <span>False</span>, <span>&#39;params&#39;</span>: [<span>0</span>, <span>1</span>]}]}
</span></span></code></pre></div><p>Now that we have a trained model object, we can pass in new feature values for the model to evaluate. For example we can pass in an <code>X</code> value of <code>5</code> hours of sunshine and see how many jars of Nulltella we expect to make.</p><p>We do this by passing in <code>5</code> to the instantiated model object, which is now a combination of the method used to run the linear regression equation and our state dict, the weights, the current set of weights and biases to give a new predicted value. We get <code>9</code> jars, which pretty close to what we’d expect.</p><div><pre tabindex="0"><code data-lang="python"><span><span>test_input <span>=</span> torch<span>.</span>tensor([[<span>5.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>predicted_output <span>=</span> model(test_input)
</span></span><span><span>print(<span>f</span><span>&#39;Prediction for input </span><span>{</span>test_input<span>.</span>item()<span>}</span><span>: </span><span>{</span>predicted_output<span>.</span>item()<span>}</span><span>&#39;</span>)
</span></span><span><span>Prediction <span>for</span> input <span>5.0</span>: <span>9.049455642700195</span>
</span></span></code></pre></div><p>I’m abstracting away <a href="https://horace.io/brrr_intro.html">an enormous amount of detail</a> for the sake of clarity, namely the massive amount of work PyTorch does in moving this data in and out of GPUs and working with <a href="https://developer.nvidia.com/gpugems/gpugems2/part-iv-general-purpose-computation-gpus-primer/chapter-33-implementing-efficient">GPU-efficient datatypes</a> for efficient computing which is a large part of the work of the library. We’ll skip these for now for simplicity.</p><h2 id="serializing-our-objects">Serializing our objects</h2><p>So far, so good. We now have stateful Python objects in-memory that convey the state of our model. But what happens when we need to persist this very large model, that we likely spent 24+ hours training, and use it again?</p><p>This scenario is described <a href="https://blog.nelhage.com/post/pickles-and-ml/">here</a>,</p><blockquote><p>Suppose a researcher is experimenting with a new deep-learning model architecture, or a variation on an existing one. Her architecture is going to have a whole bunch of configuration options and hyperparameters: the number of layers, the types of each layers, the dimensionality of various vectors, where and how to normalize activations, which nonlinearity(ies) to use, and so on. Many of the model components will be standard layers provided by the ML framework, but the researcher will be inserting bits and pieces of novel logic as well.</p></blockquote><blockquote><p>Our researcher needs a way to describe a particular concrete model – a specific combination of these settings – which can be serialized and then reloaded later. She needs this for a few related reasons:</p></blockquote><blockquote><p>She likely has access to a compute cluster containing GPUs or other accelerators she can use to run jobs. She needs a way to submit a model description to code running on that cluster so it can run her model on the cluster.</p></blockquote><blockquote><p>While those models are training, she needs to save snapshots of their progress in such a way that they can be reloaded and resumed, in case the hardware fails or the job is preempted. Once models are trained, the researcher will want to load them again (potentially both a final snapshot, and some of the partially-trained checkpoints) in order to run evaluations and experiments on them.</p></blockquote><p>What do we mean by serialization? It’s the process of writing objects and classes from our programming runtime to a file. Deserialization is the process of converting data on disk to programming language objects in memory. We now need to seralize the data into a bytestream that we can write to a file.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/47228a07-abc0-410f-b62b-48b756e3b30a" width="600"/></figure><p>Why <a href="https://stackoverflow.com/questions/28552540/why-is-serialization-called-serialization">“serialization”</a>? Because back in the Old Days, data used to be stored on tape, which required bits to be in order sequentially on tape.</p><p>Since many transformer-style models are trained using PyTorch these days, artifacts use PyTorch’s <code>save</code> implementation for serializing objects to disk.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/da1b0b43-de7e-4e78-ae06-32a21a018a08" width="600"/></figure><p>Again, let’s abstract away the GPU for simplicity and assume we’re performing all these computations in CPU. Python objects <a href="https://docs.python.org/3/c-api/memory.html">live in memory</a>. This memory is allocated in a special private heap at the beginning of <a href="https://anvil.works/articles/pointers-in-my-python-3">their lifecycle</a>, in <a href="https://stackoverflow.com/questions/10200628/heap-memory-in-c-programming">private heap</a> managed by the Python memory manager, with specialized heaps for different object types.</p><p>When we initialize our PyTorch model object, the operating system allocates memory through lower-level C functions, namely <code>malloc</code>, via <a href="https://docs.python.org/3/c-api/memory.html#default-memory-allocators">default memory allocators</a>.</p><p>When we run our code <a href="https://docs.python.org/3/library/tracemalloc.html">with tracemalloc</a>, we can see how memory for PyTorch is actually allocated on CPU (keep in mind that, again, GPU operations are completely different).</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> tracemalloc
</span></span><span><span>
</span></span><span><span>tracemalloc<span>.</span>start()
</span></span><span><span>
</span></span><span><span><span>.....</span>
</span></span><span><span>pytorch
</span></span><span><span><span>...</span>
</span></span><span><span>
</span></span><span><span>snapshot <span>=</span> tracemalloc<span>.</span>take_snapshot()
</span></span><span><span>top_stats <span>=</span> snapshot<span>.</span>statistics(<span>&#39;lineno&#39;</span>)
</span></span><span><span>
</span></span><span><span>print(<span>&#34;[ Top 10 ]&#34;</span>)
</span></span><span><span><span>for</span> stat <span>in</span> top_stats[:<span>10</span>]:
</span></span><span><span>    print(stat)
</span></span><span><span>
</span></span><span><span>[ Top <span>10</span> ]
</span></span><span><span><span>&lt;</span>frozen importlib<span>.</span>_bootstrap_external<span>&gt;</span>:<span>672</span>: size<span>=</span><span>21.1</span> MiB, count<span>=</span><span>170937</span>, average<span>=</span><span>130</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>inspect<span>.</span>py:<span>2156</span>: size<span>=</span><span>577</span> KiB, count<span>=</span><span>16</span>, average<span>=</span><span>36.0</span> KiB
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>71</span>: size<span>=</span><span>512</span> KiB, count<span>=</span><span>3</span>, average<span>=</span><span>171</span> KiB
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>dataclasses<span>.</span>py:<span>434</span>: size<span>=</span><span>410</span> KiB, count<span>=</span><span>4691</span>, average<span>=</span><span>90</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>368</span>: size<span>=</span><span>391</span> KiB, count<span>=</span><span>7122</span>, average<span>=</span><span>56</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>397</span>: size<span>=</span><span>349</span> KiB, count<span>=</span><span>1237</span>, average<span>=</span><span>289</span> B
</span></span><span><span><span>&lt;</span>frozen importlib<span>.</span>_bootstrap_external<span>&gt;</span>:<span>128</span>: size<span>=</span><span>213</span> KiB, count<span>=</span><span>1390</span>, average<span>=</span><span>157</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>functools<span>.</span>py:<span>58</span>: size<span>=</span><span>194</span> KiB, count<span>=</span><span>2554</span>, average<span>=</span><span>78</span> B
</span></span><span><span><span>/</span>Users<span>/</span>vicki<span>/.</span>pyenv<span>/</span>versions<span>/</span><span>3.10.0</span><span>/</span>lib<span>/</span>python3<span>.10</span><span>/</span>site<span>-</span>packages<span>/</span>torch<span>/</span>_dynamo<span>/</span>allowed_functions<span>.</span>py:<span>373</span>: size<span>=</span><span>136</span> KiB, count<span>=</span><span>2540</span>, average<span>=</span><span>55</span> B
</span></span><span><span><span>&lt;</span>frozen importlib<span>.</span>_bootstrap_external<span>&gt;</span>:<span>1607</span>: size<span>=</span><span>127</span> KiB, count<span>=</span><span>1133</span>, average<span>=</span><span>115</span> B
</span></span></code></pre></div><p>Here, we can see we imported 170k objects from imports, and that the rest of the allocation came from allowed_functions in torch.</p><p>We can also more explicitly see the types of these objects in memory. Among all the other objects created by PyTorch and Python system libraries, we can see our <code>Linear</code> object here, which has <code>state_dict</code> as a property. We need to serialize this object into a bytestream so we can write it to disk.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> gc
</span></span><span><span><span># Get all live objects</span>
</span></span><span><span>all_objects <span>=</span> gc<span>.</span>get_objects()
</span></span><span><span>
</span></span><span><span><span># Extract distinct object types</span>
</span></span><span><span>distinct_types <span>=</span> set(type(obj) <span>for</span> obj <span>in</span> all_objects)
</span></span><span><span>
</span></span><span><span><span># Print distinct object types</span>
</span></span><span><span><span>for</span> obj_type <span>in</span> distinct_types:
</span></span><span><span>    print(obj_type<span>.</span>__name__)
</span></span><span><span>
</span></span><span><span>InputKind
</span></span><span><span>KeyedRef
</span></span><span><span>ReLU
</span></span><span><span>Manager
</span></span><span><span>_Call
</span></span><span><span>UUID
</span></span><span><span>Pow
</span></span><span><span>Softmax
</span></span><span><span>Options 
</span></span><span><span>_Environ
</span></span><span><span><span>**</span>Linear<span>**</span>
</span></span><span><span>CFunctionType
</span></span><span><span>SafeUUID
</span></span><span><span>_Real
</span></span><span><span>JSONDecoder
</span></span><span><span>StmtBuilder
</span></span><span><span>OutDtypeOperator
</span></span><span><span>MatMult
</span></span><span><span>attrge
</span></span></code></pre></div><p>PyTorch <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">serializes objects to disk</a> using Python’s pickle framework and wrapping the pickle <code>load</code> and <code>dump</code> methods.</p><p>Pickle traverses the object’s inheritance hierarchy and converts each object encountered into streamable artifacts. It does this recursively for nested representations (for example, understanding nn.<code>Module</code> and <code>Linear</code> inheriting from <code>nn.Module</code>) and converting these representations to byte representations so that they can be written to file.</p><p>As an example, let’s take a simple function and write it to a pickle file.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch.nn <span>as</span> nn
</span></span><span><span><span>import</span> torch.optim <span>as</span> optim
</span></span><span><span><span>import</span> pickle
</span></span><span><span>
</span></span><span><span>X <span>=</span> torch<span>.</span>tensor([[<span>1.0</span>], [<span>2.0</span>], [<span>3.0</span>], [<span>4.0</span>]], dtype<span>=</span>torch<span>.</span>float32)
</span></span><span><span>
</span></span><span><span><span>with</span> open(<span>&#39;tensors.pkl&#39;</span>, <span>&#39;wb&#39;</span>) <span>as</span> f: 
</span></span><span><span>    pickle<span>.</span>dump(X, f) 
</span></span></code></pre></div><p>when we inspect the <a href="https://docs.python.org/3/library/pickletools.html">pickled object with pickletools</a>, we get an idea of how the data is organized.</p><p>We import some functions that load the data as a tensor, then the actual storage of that data, then its type. The module does the inverse when converting from pickle files to Python objects.</p><pre tabindex="0"><code>python -m pickletools tensors.pkl
    0: \x80 PROTO      4
    2: \x95 FRAME      398
   11: \x8c SHORT_BINUNICODE &#39;torch._utils&#39;
   25: \x94 MEMOIZE    (as 0)
   26: \x8c SHORT_BINUNICODE &#39;_rebuild_tensor_v2&#39;
   46: \x94 MEMOIZE    (as 1)
   47: \x93 STACK_GLOBAL
   48: \x94 MEMOIZE    (as 2)
   49: (    MARK
   50: \x8c     SHORT_BINUNICODE &#39;torch.storage&#39;
   65: \x94     MEMOIZE    (as 3)
   66: \x8c     SHORT_BINUNICODE &#39;_load_from_bytes&#39;
   84: \x94     MEMOIZE    (as 4)
   85: \x93     STACK_GLOBAL
   86: \x94     MEMOIZE    (as 5)
   87: B        BINBYTES   b&#39;\x80\x02\x8a\nl\xfc\x9cF\xf9 j\xa8P\x19.\x80\x02M\xe9\x03.\x80\x02}q\x00(X\x10\x00\x00\x00protocol_versionq\x01M\xe9\x03X\r\x00\x00\x00little_endianq\x02\x88X\n\x00\x00\x00type_sizesq\x03}q\x04(X\x05\x00\x00\x00shortq\x05K\x02X\x03\x00\x00\x00intq\x06K\x04X\x04\x00\x00\x00longq\x07K\x04uu.\x80\x02(X\x07\x00\x00\x00storageq\x00ctorch\nFloatStorage\nq\x01X\n\x00\x00\x006061074080q\x02X\x03\x00\x00\x00cpuq\x03K\x04Ntq\x04Q.\x80\x02]q\x00X\n\x00\x00\x006061074080q\x01a.\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x80?\x00\x00\x00@\x00\x00@@\x00\x00\x80@&#39;
  351: \x94     MEMOIZE    (as 6)
  352: \x85     TUPLE1
  353: \x94     MEMOIZE    (as 7)
  354: R        REDUCE
  355: \x94     MEMOIZE    (as 8)
  356: K        BININT1    0
  358: K        BININT1    4
  360: K        BININT1    1
  362: \x86     TUPLE2
  363: \x94     MEMOIZE    (as 9)
  364: K        BININT1    1
  366: K        BININT1    1
  368: \x86     TUPLE2
  369: \x94     MEMOIZE    (as 10)
  370: \x89     NEWFALSE
  371: \x8c     SHORT_BINUNICODE &#39;collections&#39;
  384: \x94     MEMOIZE    (as 11)
  385: \x8c     SHORT_BINUNICODE &#39;OrderedDict&#39;
  398: \x94     MEMOIZE    (as 12)
  399: \x93     STACK_GLOBAL
  400: \x94     MEMOIZE    (as 13)
  401: )        EMPTY_TUPLE
  402: R        REDUCE
  403: \x94     MEMOIZE    (as 14)
  404: t        TUPLE      (MARK at 49)
  405: \x94 MEMOIZE    (as 15)
  406: R    REDUCE
  407: \x94 MEMOIZE    (as 16)
  408: .    STOP
highest protocol among opcodes = 4
</code></pre><p>The main issue with pickle as a file format is that it not only bundles executable code, but that there are no checks on the code being read, and without schema guarantees, <a href="https://nedbatchelder.com/blog/202006/pickles_nine_flaws.html">you can pass something to the pickle that’s malicious</a>,</p><blockquote><p>The insecurity is not because pickles contain code, but because they create objects by calling constructors named in the pickle. Any callable can be used in place of your class name to construct objects. Malicious pickles will use other Python callables as the “constructors.” For example, instead of executing “models.MyObject(17)”, a dangerous pickle might execute “os.system(‘rm -rf /’)”. The unpickler can’t tell the difference between “models.MyObject” and “os.system”. Both are names it can resolve, producing something it can call. The unpickler executes either of them as directed by the pickle.&#39;</p></blockquote><h2 id="how-pickle-works">How Pickle works</h2><p>Pickle initially worked for Pytorch-based models because it was also closely coupled to the Python ecosystem and initial ML library artifacts were not the key outputs of deep learning systems.</p><blockquote><p>The primary output of research is knowledge, not software artifacts. Research teams write software to answer research questions and improve their/their team’s/their field’s understanding of a domain, more so than they write software in order to have software tools or solutions.</p></blockquote><p>However, as the use of transformer-based models picked up after the release of the Transformer paper in 2017, so did the use of the <code>transformers</code> library, which delegates the <a href="https://github.com/huggingface/transformers/blob/08cd694ef07d53f6e08e60ea6e1483dbb156924d/src/transformers/models/auto/configuration_auto.py#L1006">load</a> call to PyTorch’s <code>load</code> methods, which uses pickle.</p><p>Once practitioners started creating and uploading <a href="https://arxiv.org/abs/2401.13177">pickled model artifacts to model hubs like HuggingFace</a>, <a href="https://www.youtube.com/watch?v=2ethDz9KnLk&amp;t=1103s">machine learning model supply chain security</a> became an issue.</p><h2 id="from-pickle-to-safetensors">From pickle to safetensors</h2><p>As machine learning with deep learning models trained with PyTorch exploded, these security issues came to a head, and in 2021, Trail of Bits released a post <a href="https://github.com/trailofbits/fickling">the insecurity of pickle files.</a></p><p>Engineers at HuggingFace started developing a library known as <a href="https://github.com/huggingface/safetensors/tree/main">safetensors</a> as an alternative to pickle. Safetensors was a <a href="https://github.com/huggingface/safetensors/discussions/111">developed</a> to be efficient, but, also safer and more ergonomic than pickle.</p><p>First, <code>safetensors</code> is not bound to Python as closely as Pickle: with pickle, you can only read or write files in Python. Safetensors is compatible across languages. Second, safetensors also limits language execution, functionality available on serialization and deserialization. Third, because the backend of safetensors is written in Rust, it enforces type safety more rigorously. Finally, safetensors was optimized for work specifically with tensors as a datatype in a way that Pickle was not. That, combined with the fact that it was wirtten in Rust <a href="https://huggingface.co/docs/safetensors/en/speed.">makes it really fast for reads and writes.</a></p><p>After a concerted push from both <a href="https://www.trailofbits.com/">Trail of Bits</a> and <a href="https://www.eleuther.ai/">EleutherAI</a>, a security audit of safetensors was conducted and found satisfactory, <a href="https://huggingface.co/blog/safetensors-security-audit">which led to HuggingFace adapting it as the default format for models on the Hub.</a> going forward. (Big thanks to <a href="https://twitter.com/vboykis/status/1759268551129452654">Stella and Suha</a> for this history and context, and to everyone who contributed to the Twitter thread.)</p><h2 id="how-safetensors-works">How safetensors works</h2><p>How does the safetensors format work? As with most things in LLMs at the bleeding edge, the code and commit history will do most of the talking. <a href="https://github.com/huggingface/safetensors#yet-another-format-">Let’s take a look at the file spec.</a></p><ul><li><strong>8 bytes</strong>: N, an unsigned little-endian 64-bit integer, containing the size of the header</li><li><strong>N bytes</strong>: a JSON UTF-8 string representing the header.
The header data MUST begin with a { character (0x7B).
The header data MAY be trailing padded with whitespace (0x20).
The header is a dict like {“TENSOR_NAME”: {“dtype”: “F16”, “shape”: [1, 16, 256], “data_offsets”: [BEGIN, END]}, “NEXT_TENSOR_NAME”: {…}, …},
data_offsets point to the tensor data relative to the beginning of the byte buffer (i.e. not an absolute position in the file), with BEGIN as the starting offset and END as the one-past offset (so total tensor byte size = END - BEGIN).
A special key <strong>metadata</strong> is allowed to contain free form string-to-string map. Arbitrary JSON is not allowed, all values must be strings.</li><li>Rest of the file: byte-buffer.</li></ul><p>This is different than <code>state_dict</code> and <code>pickle</code> file specifications, but the addition of safetensors follows the natural evolution from Python objects, to full-fledged file format.</p><p>A file is a way of storing our data generated from programming language objects, in bytes on disk. In looking at different file format specs (<a href="https://arrow.apache.org/docs/format/CDataInterface.html">Arrow</a>,<a href="https://parquet.apache.org/docs/file-format/">Parquet</a>, <a href="https://protobuf.dev/">protobuf</a>), we’ll start to notice some patterns around how they’re laid out.</p><ol><li>In the file, we need some indicator that this is a type of file “X”. Usually this is represented by a <a href="https://en.wikipedia.org/wiki/List_of_file_signatures"><strong>magic byte</strong>.</a></li><li>Then, there is a <strong>header</strong> that represents the metadata of the file (In the case of machine learning, how many layers we have, the learning rate, and other aspects. )</li><li>The actual <strong>data</strong>. (In the case of machine learning files, the tensors)</li><li>We then need a <strong>spec</strong> that tells us what to expect in a file as we read it and what kinds of data types are in the file and how they’re represented as bytes. Essentially, documentation for the file’s layout and API so that we can program a file reader against it.</li><li>One feature the file spec usually tells us is whether data is little or big-endian, that is - whether we store the largest number first or last. This becomes important as we expect files to be read on systems with different default byte layouts.</li><li>We then implement code that reads and writes to that filespec specifically.</li></ol><p>One thing we start to notice from having looked at statedicts and pickle files before, is that machine learning data storage follow a pattern: we need to store:</p><ol><li>a large collection of vectors,</li><li>metadata about those vectors and</li><li>hyperparameters</li></ol><p>We then need to be able to instantiate model objects that we can hydrate (fill) with that data and run model operations on.</p><p>As an example for safetensors from the documentation: We start with a Python dictionary, aka a state dict, save, and load the file.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch
</span></span><span><span><span>from</span> safetensors <span>import</span> safe_open
</span></span><span><span><span>from</span> safetensors.torch <span>import</span> save_file
</span></span><span><span>
</span></span><span><span>tensors <span>=</span> {
</span></span><span><span>   <span>&#34;weight1&#34;</span>: torch<span>.</span>zeros((<span>1024</span>, <span>1024</span>)),
</span></span><span><span>   <span>&#34;weight2&#34;</span>: torch<span>.</span>zeros((<span>1024</span>, <span>1024</span>))
</span></span><span><span>}
</span></span><span><span>save_file(tensors, <span>&#34;model.safetensors&#34;</span>)
</span></span><span><span>
</span></span><span><span>tensors <span>=</span> {}
</span></span><span><span><span>with</span> safe_open(<span>&#34;model.safetensors&#34;</span>, framework<span>=</span><span>&#34;pt&#34;</span>, device<span>=</span><span>&#34;cpu&#34;</span>) <span>as</span> f:
</span></span><span><span>   <span>for</span> key <span>in</span> f<span>.</span>keys():
</span></span><span><span>       tensors[key] <span>=</span> f<span>.</span>get_tensor(key)
</span></span></code></pre></div><p>we use the save_file(model.state_dict(), ‘my_model.st’) method to render the file to safetensors</p><p>In the conversion process from pickle to safetensors, we also start <a href="https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py">with the state dict.</a></p><p>Safetensors quickly became the leading format for sharing model weights and architectures to use in further fine-tuning, and in some cases, inference</p><h2 id="checkpoint-files">Checkpoint files</h2><p>We’ve so far taken a look at simple <code>state_dict</code> files and single <code>safetensors</code> files. But if you’re training a long-running model, you’ll likely have more than just weights and biases to save, and you want to save your state every so often so you can revert if you start to see issues in your trianing run. <a href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">PyTorch has checkpoints</a>. A checkpoint is a file that has a model <code>state_dict</code>, but also</p><blockquote><p>the optimizer’s state_dict, as this contains buffers and parameters that are updated as the model trains. Other items that you may want to save are the epoch you left off on, the latest recorded training loss, external torch.nn.Embedding layers, and more. This is also saved as a Dictionary and pickled, then unpickled when you need it. All of this is also saved to a dictionary, the <code>optimizer_state_dict</code>, distinct from the <code>model_state_dict</code>.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span><span># Additional information</span>
</span></span><span><span>EPOCH <span>=</span> <span>5</span>
</span></span><span><span>PATH <span>=</span> <span>&#34;model.pt&#34;</span>
</span></span><span><span>LOSS <span>=</span> <span>0.4</span>
</span></span><span><span>
</span></span><span><span>torch<span>.</span>save({
</span></span><span><span>            <span>&#39;epoch&#39;</span>: EPOCH,
</span></span><span><span>            <span>&#39;model_state_dict&#39;</span>: net<span>.</span>state_dict(),
</span></span><span><span>            <span>&#39;optimizer_state_dict&#39;</span>: optimizer<span>.</span>state_dict(),
</span></span><span><span>            <span>&#39;loss&#39;</span>: LOSS,
</span></span><span><span>            }, PATH)
</span></span></code></pre></div><p>In addition, most large language models also now include accompanying files like tokenizers, and on HuggingFace, metadata, etc. So if you’re working with PyTorch models as artifacts generated via the Transformers library, you’ll get a repo <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1/tree/main">that looks like this</a>.</p><h2 id="ggml">GGML</h2><p>As work to migrate from pickle to safetensors was ongoing for <a href="https://www.reddit.com/r/LocalLLaMA/comments/1ayd4xr/for_those_who_dont_know_what_different_model/">generalized model fine-tuning and inference</a>, Apple Silicon <a href="https://appleinsider.com/articles/23/12/13/apple-silicon-m3-pro-blows-away-nvidia-rtx-4090-gpu-in-ai-benchmark">continued to get a lot better.</a>. As a result, people started bringing modeling work and inference from large GPU-based computing clusters, to local and on-edge devices.</p><p>Georgi Gerganov’s project to make OpenAI’s Whisper run locally with <a href="https://github.com/ggerganov/whisper.cpp">Whisper.cpp.</a> was a success and the catalyst for later projects. The combination of the release of <a href="https://about.fb.com/news/2023/07/llama-2/">Llama-2 as a mostly open-source model</a>, combined with the rise of model compression techniques like <a href="https://huggingface.co/docs/peft/main/en/developer_guides/lora">LoRA</a>, large language models, which were typically only accessible on lab or industry-grade GPU hardware (inspie of the small CPU-based examples we’ve run here), also acted as a catalyst for thinking about working with and running personalized models locally.</p><p>Based on the interest and success of <code>whisper.cpp</code>, Gerganov created <a href="https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022">llama.cpp</a>, a package for working with Llama model weights, originaly in pickle format, in GGML format, for local inference.</p><p>GGML was initialy both a library and a complementary format created specifically for on-edge inference for whisper. You can also <a href="https://www.reddit.com/r/LocalLLaMA/comments/15y9m64/fine_tuningggml_quantiziation_on_apple_silicon/">perform fine-tuning</a> with it, but generally it’s used to read models trained on PyTorch in GPU Linux-based environments and converted to GGML to run on Apple Silicon.</p><p>As an example, here is script for <a href="https://github.com/ggerganov/ggml">GGML</a> which <a href="https://github.com/ggerganov/ggml/blob/master/examples/gpt-2/convert-ckpt-to-ggml.py">converts PyTorch GPT-2 checkpoints</a> to the correct format, <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L64">read as a <code>.bin</code> file.</a>. The files are <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/download-model.sh#L41C64-L41C131">downloaded from OpenAI</a>.</p><p><a href="https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022">The resulting GGML file compresses all of these into one and contains</a>:</p><ul><li><p>a magic number with an <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L91">optional version number</a></p></li><li><p><a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L92">model-specific hyperparameters</a>, including
metadata about the model, such as the number of layers, the number of heads, etc.
a ftype that describes the type of the majority of the tensors,
for GGML files, the quantization version is encoded in the ftype divided by 1000</p></li><li><p>an embedded vocabulary, which is a list of strings with length prepended.</p></li><li><p>finally, a <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/examples/gpt-2/convert-ckpt-to-ggml.py#L137">list of tensors</a> with their length-prepended name, type, and tensor data</p></li></ul><p>There are several elements that make GGML more efficient for local inference than checkpoint files. First, <a href="https://github.com/ggerganov/ggml/blob/b458250b736a7473f7ff3560d47c93f1644f3290/src/ggml-impl.h#L45">it makes use of 16-bit floating point representations</a> of model weights. Generally, <code>torch</code> initializes floating point datatypes in <a href="https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html">32-bit floats by default</a>. 16-bit, or half precision means that model weights use <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">50% less memory</a> at compute and inference time without significant loss in model accuracy. Other architectural choices include using C, which offers <a href="https://www.interviewbit.com/blog/difference-between-c-and-python/">more efficient memory allocation than Python</a>. And finally, GGML was built <a href="https://developer.apple.com/documentation/apple-silicon/tuning-your-code-s-performance-for-apple-silicon">optimized for Silicon.</a></p><p>Unfortunately, in its move to efficiency, GGML contained <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#drawbacks">a number of breaking changes</a> that created issues for users.</p><p>The largest one was that, since everything, both data and metadata and hyperparameters, was written into the same file, if a model added hyperparameters, it would break backward compatibility that the new file couldn’t pick up. Additionally, no model architecture metadata is present in the file, and each architecture required its own conversion script. All of this led to brittle performance and the creation of <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#gguf">GGUF.</a></p><h2 id="finally-gguf">Finally, GGUF</h2><p>GGUF has the same type of layout as GGML, with metadata and tensor data in a single file, but in addition is also designed to be backwards-compatible. The key difference is that previously instead of a list of values for the hyperparameters, the new file format uses a key-value lookup tables which accomodate shifting values.</p><p>The intiution we spent building up around how machine learning models work and file formats are laid out now allows us to understand the <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#file-structure">GGUF format.</a></p><p>First, we know that GGUF models are little-endian by default for specific architectures, which we remember is when the least significant bytes come first and is optimized for different computer hardware architectures.</p><p>Then, we have <code>gguf_header_t</code>, which is the header</p><p>It includes the magic byte that tells us this is a GGUF file:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>Must be <span>`</span>GGUF<span>`</span> at the byte level: <span>`</span>0x47<span>`</span> <span>`</span>0x47<span>`</span> <span>`</span>0x55<span>`</span> <span>`</span>0x46<span>`</span>. 
</span></span></code></pre></div><p>as well as the key-value pairs:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// The metadata key-value pairs.
</span></span></span><span><span><span></span>    <span>gguf_metadata_kv_t</span> metadata_kv[metadata_kv_count];
</span></span></code></pre></div><p>This file format also offers versioning, in this case we see this is version 3 of the file format.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>// Must be `3` for version described in this spec, which introduces big-endian support.
</span></span></span><span><span><span></span>    <span>//
</span></span></span><span><span><span></span>    <span>// This version should only be increased for structural changes to the format.
</span></span></span></code></pre></div><p>Then, we have the tensors</p><p>The entire file looks like this, and when we work with readers like <code>llama.cpp</code> and <code>ollama</code>, they take this spec and write code to open these files and read them.</p><figure><img src="https://github.com/veekaybee/veekaybee.github.io/assets/3837836/0da77173-fd21-470c-90d1-fa31bcfc7119" width="600"/></figure><p>We’ve been on a whirlwind adventure to build up our intuition of how machine learning models work, what artifacts they produce, how the machine learning artifact storage story has changed over the past couple years, and finally ended up in GGUF’s documentation to better understand the log that is presented to us when we perform local inference on artifacts in GGUF. Hope this is helpful, and good luck!</p></div></div>
  </body>
</html>
