<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2210.11399">Original</a>
    <h1>Transcending Scaling Laws with 0.1% Extra Compute</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tay%2C+Y">Yi Tay</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+J">Jason Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chung%2C+H+W">Hyung Won Chung</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tran%2C+V+Q">Vinh Q. Tran</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=So%2C+D+R">David R. So</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shakeri%2C+S">Siamak Shakeri</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Garcia%2C+X">Xavier Garcia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng%2C+H+S">Huaixiu Steven Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rao%2C+J">Jinfeng Rao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+D">Denny Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Metzler%2C+D">Donald Metzler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Petrov%2C+S">Slav Petrov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Houlsby%2C+N">Neil Houlsby</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q+V">Quoc V. Le</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dehghani%2C+M">Mostafa Dehghani</a></p></div>
      
    
  
    <p><a href="https://arxiv.org/pdf/2210.11399">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Scaling language models improves performance but comes with significant
computational costs. This paper proposes UL2R, a method that substantially
improves existing language models and their scaling curves with a relatively
tiny amount of extra compute. The key idea is to continue training a
state-of-the-art large language model (e.g., PaLM) on a few more steps with
UL2&#39;s mixture-of-denoiser objective. We show that, with almost negligible extra
computational costs and no new sources of data, we are able to substantially
improve the scaling properties of large language models on downstream metrics.
In this paper, we continue training PaLM with UL2R, introducing a new set of
models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B
scale, we show an approximately 2x computational savings rate where U-PaLM
achieves the same performance as the final PaLM 540B model at around half its
computational budget (i.e., saving $\sim$4.4 million TPUv4 hours). We further
show that this improved scaling curve leads to &#39;emergent abilities&#39; on
challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM
on some tasks or demonstrates better quality at much smaller scale (62B as
opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many
few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question
answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual
tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide
qualitative examples showing the new capabilities of U-PaLM for single and
multi-span infilling.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Yi Tay [<a href="https://arxiv.org/show-email/e28a4671/2210.11399">view email</a>]
      </p></div></div>
  </body>
</html>
