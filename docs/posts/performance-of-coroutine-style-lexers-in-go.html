<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eli.thegreenplace.net/2022/performance-of-coroutine-style-lexers-in-go/">Original</a>
    <h1>Performance of coroutine-style lexers in Go</h1>
    
    <div id="readability-page-1" class="page"><div>
                
                <p>Back in 2011, Rob Pike gave a talk on the topic of
<a href="https://www.youtube.com/watch?v=HxaD_trXwRE">Lexical Scanning in Go</a>, where
he presented an interesting approach to lexical analysis with coroutines
(implemented via goroutines communicating over a channel). Since I&#39;ve
been pondering the <a href="https://eli.thegreenplace.net/2009/08/29/co-routines-as-an-alternative-to-state-machines/">connection between coroutines and state machines</a>
in the past, Rob&#39;s talk inspired me to try approximating this approach in Python
<a href="https://eli.thegreenplace.net/2012/08/09/using-sub-generators-for-lexical-scanning-in-python/">using sub-generators</a>.</p>
<p>Since 2011, I&#39;ve seen this talk and the technique presented in it mentioned many
times, both in Go forums and in general programming communities. There&#39;s
something in this approach that feels elegant - it&#39;s a problem very well suited
for coroutines. However, there was always a small nagging voice in the back of
my head doubting the efficiency of the approach.</p>
<p>Since I&#39;ve recently found myself <a href="https://eli.thegreenplace.net/tag/lexer">playing with lexers again</a>, this seemed like a good
opportunity to revisit Rob Pike&#39;s lexer and compare its performance to other
approaches, using the same problem and benchmark for fairness.</p>
<div id="rob-pike-s-original-lexer">
<h2>Rob Pike&#39;s original lexer</h2>
<p>In the talk, Rob is describing a lexer he designed for Go&#39;s templating package.
The lexer presented in the talk and <a href="https://talks.golang.org/2011/lex.slide#1">slides</a> is relatively simple; a much more
featureful version of it still lives in the <tt>text/template</tt> package - <a href="https://cs.opensource.google/go/go/+/master:src/text/template/parse/lex.go">lex.go</a>.
As such, this lexer is heavily used in production every day.</p>
<p>I&#39;ve transcribed the original lexer from the talk into my GitHub repository;
it&#39;s available <a href="https://github.com/eliben/code-for-blog/tree/master/2022/go-coroutine-lexer">here, with tests</a>.</p>
<p>The main aesthetic appeal of this lexer is avoiding an explicit state machine
by using a separate goroutine to perform the lexing. This goroutine switches
states by returning a new &#34;state function&#34;, and emits tokens onto a channel
which can be read by the lexer&#39;s clients.</p>
<p><img alt="Diagram showing a lexing goroutine and a main goroutine" src="https://eli.thegreenplace.net/images/2022/lexing-channel.png"/></p><p>This approach is particularly attractive when parsing templates because it
oscillates between two major states - lexing free-standing text and
lexing inside actions delimited by <tt>{{ }}</tt>. Using the concurrent approach
avoids the need to have an explicit &#34;am I inside an action&#34; state flag that
has to be checked every time a new token is requested <a href="#id3" id="id1">[1]</a>.</p>
</div>
<div id="lexing-tablegen">
<h2>Lexing TableGen</h2>
<p>To be able to have a meaningful performance comparison, I&#39;ve rewritten my
TableGen lexer <a href="https://eli.thegreenplace.net/2022/a-faster-lexer-in-go/">once again</a>, this time using
the coroutine approach. The full code for this lexer with tests is <a href="https://github.com/eliben/code-for-blog/tree/master/2022/go-coroutine-tablegen-lexer">available
here</a>.</p>
<p>The API is very similar to my <a href="https://eli.thegreenplace.net/2022/a-faster-lexer-in-go/">previous TableGen lexers</a> - all the
implementation details (like having a channel to read tokens from) are hidden
from the user. The token type is the same:</p>
<div><pre><span></span><span>type</span> <span>Token</span> <span>struct</span> <span>{</span>
  <span>Name</span> <span>TokenName</span>
  <span>Val</span>  <span>string</span>
  <span>Pos</span>  <span>int</span>
<span>}</span>
</pre></div>
<p>The <tt>NextToken</tt> method also has the same signature, though the implementation
uses a channel now:</p>
<div><pre><span></span><span>func</span> <span>(</span><span>l</span> <span>*</span><span>Lexer</span><span>)</span> <span>NextToken</span><span>()</span> <span>Token</span> <span>{</span>
  <span>return</span> <span>&lt;-</span><span>l</span><span>.</span><span>tokens</span>
<span>}</span>
</pre></div>
<p>The constructor creates a new <tt>Lexer</tt>, creates a channel for the emitted
tokens to go into and launches the goroutine that does the actual lexing:</p>
<div><pre><span></span><span>// Lex creates a new Lexer</span>
<span>func</span> <span>Lex</span><span>(</span><span>input</span> <span>string</span><span>)</span> <span>*</span><span>Lexer</span> <span>{</span>
  <span>l</span> <span>:=</span> <span>&amp;</span><span>Lexer</span><span>{</span>
    <span>input</span><span>:</span>  <span>input</span><span>,</span>
    <span>tokens</span><span>:</span> <span>make</span><span>(</span><span>chan</span> <span>Token</span><span>),</span>
  <span>}</span>
  <span>go</span> <span>l</span><span>.</span><span>run</span><span>()</span>
  <span>return</span> <span>l</span>
<span>}</span>
</pre></div>
<p>The <tt>run</tt> method serves as a trampoline to advance the lexer from state to
state (while the state functions emit tokens into the channel):</p>
<div><pre><span></span><span>type</span> <span>stateFn</span> <span>func</span><span>(</span><span>*</span><span>Lexer</span><span>)</span> <span>stateFn</span>

<span>func</span> <span>(</span><span>l</span> <span>*</span><span>Lexer</span><span>)</span> <span>run</span><span>()</span> <span>{</span>
  <span>for</span> <span>state</span> <span>:=</span> <span>lexText</span><span>;</span> <span>state</span> <span>!=</span> <span>nil</span><span>;</span> <span>{</span>
    <span>state</span> <span>=</span> <span>state</span><span>(</span><span>l</span><span>)</span>
  <span>}</span>
  <span>close</span><span>(</span><span>l</span><span>.</span><span>tokens</span><span>)</span> <span>// no more tokens will be delivered</span>
<span>}</span>
</pre></div>
<p>And so on. The implementation follows Rob Pike&#39;s lexer very closely, with the
same primitives. For the TableGen language, which does not have the &#34;two states&#34;
feature of templates, I found this approach to be less of a stylistic win,
though it still makes state management simpler.</p>
</div>
<div id="performance">
<h2>Performance</h2>
<p>In the <a href="https://eli.thegreenplace.net/2022/a-faster-lexer-in-go/">previous post</a>, the fastest Go
lexer achieved with Go 1.18 runs the benchmark in about 6 ms (with a
<tt>GOGC=1600</tt> setting).</p>
<p>For a level playing field, I ran the new coroutine-style lexer on the same input
file, with the same benchmarking invocation and the same <tt>GOGC</tt> setting:</p>
<div><pre><span></span>$ GOGC=1600 TDINPUT=&lt;path to input file&gt; go test -bench=Preall -benchtime=5s
goos: linux
goarch: amd64
pkg: example.com
cpu: Intel(R) Core(TM) i7-4771 CPU @ 3.50GHz
BenchmarkLexerPrealloc-8            80          70507009 ns/op
PASS
ok    example.com     5.885s
</pre></div>
<p>It takes... 70 ms, more than 10x slower!</p>
<p>While I&#39;m not surprised that this approach is slower, I <em>am</em> somewhat surprised
at the magnitude. Let&#39;s think this through. What does each lexer do in its
hot inner loop?</p>
<p>In my original lexer, each call to the <tt>NextLexer</tt> function:</p>
<ol>
<li>Skips whitespace: iterates over the input string rune by rune until a
non-whitespace rune is encountered.</li>
<li>Examines the current rune and decides which kind of token it belongs to.</li>
<li>Finishes lexing the token and returns it as a string slice.</li>
</ol>
<p>Whereas in the coroutine-style lexer, each call to <tt>NextLexer</tt>:</p>
<ol>
<li>Invokes a channel receive on the token channel.</li>
</ol>
<p>In the meantime, the lexing goroutine:</p>
<ol>
<li>Skips whitespace and examines the current rune, just like in the regular
lexer.</li>
<li>Invokes a channel send on the token channel.</li>
</ol>
<p>The channel send/receive is the main culprit for the large performance
difference. Channels in Go are fully synchronized, which implies locking inside
the inner loop. Moreover, since there are two goroutines involved that perform
channel operations, the Go runtime has much more work to do to handle suspending
goroutines when the channel blocks and waking them up when it unblocks. All
these operations are highly optimized in Go, but when they appear in the inner
loop of a fast scanning process, the relative cost is high.</p>
<p>If we profile the new scanner with pprof, this cost is easily observed:</p>
<p><img alt="Pprof diagram showing where the time goes for the channel lexer" src="https://eli.thegreenplace.net/images/2022/lex-channel-pprof.png"/></p><p>In the previous lexer, the &#34;fetch next rune&#34; method is very dominant. Here it
accounts for only 5.8% of the execution time! Much more time goes on
<tt>chansend1</tt> and <tt>chanrecv1</tt> which are runtime functions with names that
should be self-describing. There is also goroutine management code in the
runtime that accounts for a large % there.</p>
</div>
<div id="using-a-buffered-channel">
<h2>Using a buffered channel</h2>
<p>Go&#39;s <tt>make</tt> primitive creates an <em>unbuffeered</em> channel by default, meaning
that every send into it blocks until a receive takes the item out. What would
happen if we created a buffered channel instead? Theoretically, this should
improve the lexer&#39;s execution time as there will be less suspension and waking
up of goroutines.</p>
<p>Let&#39;s see what different values of the buffer give us; I re-ran the benchmark
with buffer sizes starting from 1 to 16384 in jumps of 4x:</p>
<p><img alt="Benchmark results for different sizes of channel buffer" src="https://eli.thegreenplace.net/images/2022/lexer-runtime-channel-size.png"/></p><p>As expected, using a buffered channel makes lexing significantly faster,
leveling out at 1024 where it takes about 24 ms for our benchmark. This is
a large improvement, though still much slower than the 6 ms we had with our
non-concurrent lexer.</p>
<p>Channels have many uses in Go; sometimes they are used as synchronization
mechanisms, so having a large buffer is not always feasible. In cases like our
lexer, a buffer actually makes sense because there should be no problem for the
lexing goroutine to run ahead a little bit. Note that this doesn&#39;t work for any
input kind, though; had we been lexing C, for instance, we&#39;d might want to have
a feedback mechanism back into the lexer (for handling the grammar&#39;s <a href="https://eli.thegreenplace.net/2007/11/24/the-context-sensitivity-of-cs-grammar/">context
sensitivity</a>).</p>
<p>FWIW, the template lexer Rob Pike added to the standard library uses an
unbuffered channel. Maybe it would make sense to add a modest buffer there to
<a href="https://go-review.googlesource.com/c/go/+/410296">speed it up somewhat</a> :-)</p>
</div>
<div id="does-performance-matter-here">
<h2>Does performance matter here?</h2>
<p>For the task at hand, the coroutine-style lexer is still plenty fast. Note that
it&#39;s <em>much</em> faster than some of the Python and JS-based lexers I wrote for the
same task <a href="https://eli.thegreenplace.net/2013/06/25/regex-based-lexical-analysis-in-python-and-javascript/">a while ago</a>.</p>
<p>This lexer is used by the standard library for parsing templates, but these
are (1) rarely very big and (2) almost always OK to parse just once during the
lifetime of a program, so the time it takes to parse them is not too important;
in other words, it&#39;s very unlikely to dominate your application&#39;s benchmarks.</p>
<p>That said, I can envision scenarios where this <em>could</em> matter. Suppose you&#39;re
writing a frontend for a nontrivial programming language (or configuration
language etc.) and a fast interpreter for this language <a href="#id4" id="id2">[2]</a>. This could lead
to the frontend&#39;s performance being a bottleneck. In these scenarios, it just
<em>may</em> be important to have the fastest lexer you can reasonably implement.</p>
<hr/>


</div>

            </div></div>
  </body>
</html>
