<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stackoverflow.com/questions/72306573/why-does-this-code-execute-more-slowly-after-strength-reducing-multiplications-t">Original</a>
    <h1>Why does this code execute more slowly after strength-reducing multiplications?</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="text">
                
<p>This optimization <em>can</em> give a speedup over the best you can do re-evaluating the polynomial separately for each <code>i</code>.  But only if you generalize it to a larger stride, to still have enough parallelism in the loop.  <strong>My version can store 1 vector (4 doubles) per clock cycle on my Skylake</strong>.  Or in theory 1 vector per 2 clocks on earlier Intel, including Sandybridge where a 256-bit store takes 2 clock cycles in the store port.</p>
<hr/>
<p>This <a href="https://en.wikipedia.org/wiki/Strength_reduction" rel="noreferrer">strength-reduction optimization</a> (just adding instead of starting with a fresh <code>i</code> and multiplying) <strong>introduces a serial dependency across loop iterations</strong>, involving FP math rather than integer increment.</p>
<p>The original has <strong>data parallelism across every output element</strong>: each one only depends on constants and its own <code>i</code> value.  Compilers can auto-vectorize with SIMD (SSE2, or AVX if you use <code>-O3 -march=native</code>), and CPUs can overlap the work across loop iterations with out-of-order execution.  Despite the amount of extra work, the CPU is able to apply sufficient brute force, with the compiler&#39;s help.</p>
<p>But the version that calculates <code>poly(i+1)</code> in terms of <code>poly(i)</code> has very limited parallelism; no SIMD vectorization, and your CPU can only run two scalar adds per 4 cycles, for example, where 4 cycles is the latency of FP addition on Intel Skylake through Tiger Lake.  (<a href="https://uops.info/" rel="noreferrer">https://uops.info/</a>).</p>
<hr/>
<p>@huseyin tugrul buyukisik&#39;s answer shows how you can get close to maxing out the throughput of the original version on a more modern CPU, with two FMA operations to evaluate the polynomial (Horner&#39;s scheme), plus an int-&gt;FP conversion or an FP increment.  (The latter creates an FP dep chain which you need to unroll to hide.)</p>
<p>So best case you have 3 FP math operations per SIMD vector of output.  (Plus a store).  Current Intel CPUs only have two FP execution units that can run FP math operations.  (With 512-bit vectors with AVX-512, current CPUs shut down the vector ALU on port 1 so there are only 2 SIMD ALU ports at all, so non-FP-math ops like integer increment will also compete for throughput).</p>
<p>AMD since Zen2 has two FMA/mul units on two ports, and two FP add/sub units on two different ports, so if you use FMA to do addition, you have a theoretical max of four SIMD additions per clock cycle.</p>
<p>Haswell/Broadwell have 2/clock FMA, but only 1/clock FP add/sub (with lower latency).  This is good for naive code, <a href="https://electronics.stackexchange.com/questions/452181/why-does-intels-haswell-chip-allow-floating-point-multiplication-to-be-twice-as/452366#452366">not great</a> for code that has been optimized to have lots of parallelism.  That&#39;s probably why Intel changed it in Skylake.</p>
<p>Your Sandybridge (E5-1620) and Nehalem (W5580) CPUs have 1/clock FP add/sub, 1/clock FP mul, on separate ports.  This is what Haswell was building on.  And why adding extra multiplies isn&#39;t a big problem: they can run in parallel with the existing adds.</p>
<h2>Finding parallelism: strength-reduction with an arbitrary stride</h2>
<p>Your <code>compute2</code> calculates the next Y and next Z in terms of the immediately previous value.  i.e. with a stride of 1, the values you need for <code>data[i+1]</code>.  So each iteration is dependent on the immediately previous one.</p>
<p>If you generalize that to other strides, you can advance 4, 8, 16, or more separate Y and Z values so they all leapfrog in lockstep with each other, all independently of each other.  <strong>This regains some parallelism which the compiler and/or CPU can take advantage of.</strong></p>
<pre><code>poly(i)   = A i^2             + B i          + C

poly(i+4) = A (i+4)^2         + B (i+4)      + C

          = A*i^2 + A*8i + A*16  +  B i + 4B   + C
          = poly(i) + 4*(2A)*i + 16*A  + 4B
</code></pre>
<p>(I picked a fixed <code>4</code> partly because writing exponents is messy in ASCII text.  On paper I might have just done <code>(i+s)^2 = i^2 + 2is + s^2</code> and so on.
Another approach would be to taking differences between successive points, and then differences of those differences.  At least that&#39;s where we want to end up, with values we can add to generate the sequence (of FP values).  This is basically like taking the 1st and 2nd derivative, and then the optimized loop is effectively integrating to recover the original function.)</p>
<ul>
<li>The 2nd-derivative part (formerly <code>Z += A*2</code>)</li>
<li>The linear growth (formerly the initial <code>Z = A+B;</code>)</li>
</ul>
<p>As you can see, for <code>stride=1</code> (no unroll) these simplify to the original values.  But with larger <code>stride</code>, they still let this method work, with no extra multiplies or anything in the loop.</p>
<pre><code>// UNTESTED, but compiles to asm that looks right.
// Hopefully I got the math right.

#include &lt;stdalign.h&gt;
#include &lt;stdlib.h&gt;
#define LEN 1024
alignas(64) double data[LEN];

void compute2_unrolled(void)
{
    const double A = 1.1, B = 2.2, C = 3.3;
    const int stride = 16;   // unroll factor.  1 reduces to the original
    const double deriv2 = A * (stride * 2);
    double Z[stride], Y[stride];
    for (int j = 0 ; j&lt;stride ; j++){  // this loop will fully unroll
            Y[j] = j*j*A + j*B + C;         // poly(j) starting values to increment
            Z[j] = (A*stride + B) * stride;  // 1st derivative = same initial Z for all elements
    }

    for(size_t i=0; i &lt; LEN - (stride-1); i+=stride) {
#if 0
        for (int j = 0 ; j&lt;stride ; j++){
            data[i+j] = Y[j];
            Y[j] += Z[j];
            Z[j] += deriv2;
        }
#else
        // loops that are easy(?) for a compiler to roll up into some SIMD vectors
        for (int j=0 ; j&lt;stride ; j++)  data[i+j] = Y[j];  // store
        for (int j=0 ; j&lt;stride ; j++)  Y[j] += Z[j];      // add
        for (int j=0 ; j&lt;stride ; j++)  Z[j] += deriv2;    // add
#endif
    }


    // cleanup for the last few i values
    for (int j = 0 ; j &lt; LEN % stride ; j++) {
        // letting the compiler see the %stride can help it decide *not* to auto-vectorize this part for non-huge strides.
        size_t i = LEN - (stride-1) + j;
        //data[i] = poly(i);
    }
}
</code></pre>
<p>(<a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYM9DgDJ4GmADl3ACNMYglpAAdUBUJbBmc3Dz1o2JsBX38gllDwqQtMK3SGIQImYgJE908uAqL40vKCTMCQsIiLMoqq5NqFLua/Vpz2qQBKC1RXYmR2DgBSACYAZj9kNywAannlx370MTxgBgA6BB3seY0AQSXVhnXXLZ29gnRaPGCzi6vblawaP5Nt5sAFNlwNItJL9DscmAoIAA2SRjTboKbBehophleYAVgAQiCAviACI7Am/X4AN1QeHQmzQLEirgImEWxlcDGILno6AgtPpYypAHZKTdNpLGQJ%2BmiMVjrttlqTwSdapsCUqVYsTotSJtHFrNssTssKb8pdKGLK/ARNv1iPTMEauIiKVKAPQezZcnm0WibKhMawkE6SribYiYdCuWYKTZEBMIZ0kI5%2BMQWqVoa129GuTHOrCO6mLI2KgBUmwgDqdm0ri2Fy3FiqleYLmwAWviCTWsGT9QBNbu9zBk80SqX8YhV22bbRGjRKzXaF4jpdzpaUxYE4ViyVepN4eO0VCoSKbADudADVFc/oAnj7ubzM5a35sh4SV3iVTsVdpywAxVNznctNRAxx3XffdvWiWh7wgbRUX6Jo/GATZqTEVxMHjRM1ijNhBFfaDJS7L8ySNCBrnLNcQJ3Ot7QIR0%2BybGDwVlIs8EwmxqWdP97VYZ0/DiMRO0DEhNjEANCkwQiCAUYj5hFckbmIqdqzwAAvTBjDtPA/w0d08CVQ1iU2ABaKsR3MrhG01fTtz/EdUSU5s7jwKhNkMid3ynGdBDnBd1xXXZaNYldHO3XdmxIy0DlxQkHIJb9f2VD9uxS8cW1i9LyJ/bZHLSsjkrHJtiJI4qUoKyk0s4ksspIpSVL%2BZZCiUcrPW9E8z1whAcUkqNNkweEEJ2AAxVE/KYaVmToMIE1QSNeR9c9bUWhRUDYTYhAASQAWRVXiQ2IBSfLfPyIFnELSUXd0QteZi%2BPCzdN1RbEEoJJKqv4z8Sp/KCD36EhRzOy0LqugzgtXJjazul6oslX7vsKlVKtKzU3wPJh0HQDrJXBgLrtu57Qph55nsi%2BjOwyiiXtqsIuMWKC2Ox3HVJWBwPMU5SqVUs6D3WYaGFcc8/IIZNNgMWUqEwC9NmMzC3BwtSJMuwmgrukzgVBAq8UYx6ocplyxTxzYD3oAgbCMJNnSZSI5unJRnXFvjFjxNdRAYTZk1oVbc0wZBa3LBhUAIStEyYVlUHMo6iEdbTD3jSImnE6cQ4YcyEFcYBnRHBQTlN2JtN0%2BWjTMyzqzJzAbJc7cNzK0HOviphu30/L%2BLghC8Ds7nmqa3nbhuQUGTt1lMC4AU6XQYVVJNs7sw4%2BVnWAtKuDVfVwLSnU9QNI0TTNBvB%2By%2BWAv0w%2BwZIS6DKMl5iRvo3tjn4%2B4pxFvEtptLqLwctjLon/qoNA1KU/dZ6kg4BMWgnA8S8E8BwLQpBUCcENBtaYswCrLB4KQAgmhwETAANYgGWCKE4iJSF4i4MsPEeINAijxCKAAnHqSBHBJAwJwQgzgvAFAgA0FgnBEw4CwBgIgFAm17b0DIBQCAdsHYoHWIYYAUgNC8JoLQNkJ1KDBHYcEdMxB7ycEwUyOSAB5Bg8F2FYBYAo8QcDeD4CjNYLiOF2GYFUAHMeBjeC2kKOwj4wRiDlHvM4LAnisGOhYJ4iYVADDAAUAANTwLLYxkRGChP4IIEQYh2D5HSfIJQah2G6FqAYIwIBTDGHMH47hkAJhnmKNwjg5ljHLAslQIM/RzJWPFhZAA6lJCyViZjnGVAoPB94DB4OdJ0/xhhkDJgUOZK84s/DmWWNucywQpgMAOI6HCXCZKOLsBABwPQaikB8EMbIuQUgxDiAIU5Ny0jxBaFc0YdQA7FEaN0Fw1Q9CWA%2BQ0AYLy2h5E6E0B5fQgWXJBRICYKCZjZLCZgOYPAIFQLYbYjhHBVAAA5ETmWRIyEp6EpAnA0GSqsjh9S4EIBJO46pnCzQkeg2yvBsG2LGBMZM2N2gQDRSw3gETqGkFgfAxBHAuE8L4Ry0ggiREyIkeQSgCr2iB2QMgRYkIuDCtUeo7hEAtGYp0cwPRoSjGMAIKY8xmLLHWLmPA%2BxALeINPga49xbJQneOYfAvxAS9HBJRWy8JkS%2BAxPiYki8yTUncF4LkzJ4gcmyEUCodQmLdBMIUWUsw%2BhPjVL5QgyI9TOBNJaeZNp8ICCdJxAgXp/Sq1DKcmMiZmB9n1COScn5vRznbOBSMUFqQ7kJE7WcgdxRe3XL6Acz5AwIXvMOSUKFWQYWQvBcOv5i7hgTrhVMBFegmLIpDcw6BIr2HitxfiwleB1WbE1YsNeeIKUQCpZsGlccWX6kZeI%2BadwGxsv4Vy4aRZKD4JACKclkgViQmoXQvEiIIT0P0JwVhpAhW8NFbwcVkreHsq0Jy0gBDEQ6mWMRyQeLJCSEYRBmhiGODLAxWKzh0rcP8sWPRjDjGcO4NILxE68QQCSCAA" rel="noreferrer"><strong>Godbolt compiler explorer</strong></a>) This does actually auto-vectorize nicely with <code>stride=16</code> (4x YMM vectors of 4 <code>double</code>s each) with clang14 <code>-O3 -march=skylake -ffast-math</code>.</p>
<p>It looks like clang has further unrolled by 2 (but without inventing new accumulators, of course). So each asm loop iteration 2x 8 <code>vaddpd</code> instructions and 2x 4 stores.  And one <code>add/jne</code> macro-fused uop of loop overhead, since clang is counting up towards zero, indexing from the end of the array.  So with L1d cache hits for the stores, this should run (on Skylake) at 1 store per clock cycle, also doing 2x <code>vaddpd</code> every cycle.  This is max throughput for both of those things.  The front-end only needs to keep up with a bit over 3 uops / clock cycle, but it&#39;s been 4-wide since Core2, and the uop cache in Sandybridge-family makes that no problem.  (Unless you run into the JCC erratum on Skylake, so I used <code>-mbranches-within-32B-boundaries</code> <a href="https://stackoverflow.com/questions/61256646/how-can-i-mitigate-the-impact-of-the-intel-jcc-erratum-on-gcc">to have clang pad instructions to avoid that</a>.)</p>
<p>I didn&#39;t actually benchmark it on my desktop; the <code>vaddpd</code> latency is 4 cycles, so 4 dep chains is just barely enough to keep 4 independent operations in flight.  And with two interlocking dep chains, it&#39;s probably easy for imperfect scheduling to result in lost cycles.  So a bit more unrolling would be appropriate if we could get the compiler to still make nice asm for <code>stride=32</code>.</p>
<p>(I&#39;m not sure why <code>-ffast-math</code> is necessary for clang to vectorize decently with this version; I don&#39;t think clang&#39;s re-associating anything, just doing all the math operations in source order unless there&#39;s something I&#39;m missing; I didn&#39;t stare closely at the register numbers.  The source was carefully written in a SIMD-friendly way to make it possible to compile to nice asm.  Update: another version that takes a pointer as an arg does vectorize with or without <code>-ffast-math</code>.)</p>
<p>The other way to write the loops is with one inner loop instead of all the Y ops, then all the Z ops.  clang doesn&#39;t auto-vectorize this, though, even with <code>-ffast-math</code>, only the SIMD-style above.  I included it inside a <code>#if 0</code> / <code>#else</code> / <code>#endif</code> block on Godbolt.  Update: in the version taking a pointer arg, this actually vectorizes better in some cases.</p>
<pre><code>    for(int i=0; i &lt; LEN - (stride-1); i+=stride) {
        for (int j = 0 ; j&lt;stride ; j++){  // doesn&#39;t auto-vectorize with clang or GCC
            data[i+j] = Y[j];
            Y[j] += Z[j];
            Z[j] += deriv2;
        }
    }
</code></pre>
<p><strong>We have to manually choose an appropriate unroll amount</strong>. Too large an unroll factor can even stop the compiler from seeing what&#39;s going on and keeping the temp arrays in registers.  e.g. <code>32</code> is a problem for clang, but not <code>24</code>.  There may be some tuning options to force the compiler to unroll loops up to a certain count; there are for GCC which can sometimes be used to let it see through something at compile time.</p>
<p>(Another approach would be manual vectorization with <code>#include &lt;immintrin.h&gt;</code> and <code>__m256d Z[4]</code> instead of <code>double Z[16]</code>.  But this version can vectorize for other ISAs like AArch64.)</p>
<p>Other downsides to a large unroll factor are leaving more cleanup work when the problem-size isn&#39;t a multiple of the unroll.  (You might use the <code>compute1</code> strategy for cleanup, letting the compiler vectorize that for an iteration or two before doing scalar.)</p>
<hr/>
<p>In theory a compiler would be <em>allowed</em> to do this for you with <code>-ffast-math</code>, either from <code>compute1</code> doing the strength-reduction on the original polynomial, or from <code>compute2</code> seeing how the stride accumulates.  But in practice that&#39;s really complicated and something humans have to do themselves.  Unless / until someone gets around to teaching compilers how to look for patterns like this and apply differential calculus (or successive differences with a choice of stride).</p>
<hr/>
<h3>Experimental results:</h3>
<p>On my desktop (i7-6700k) with clang13.0.0, I tested and was pleasantly surprised that this does in fact run at 1 SIMD store per clock cycle with the combination of compiler options (fast-math or not) and <code>#if 0</code> vs. <code>#if 1</code> on the inner loop strategy, with a version of @huseyin tugrul buyukisik&#39;s benchmark framework improved to repeat a more measurable amount between <code>rdtsc</code> instructions. And to compensate for the difference between core clock frequency and the <a href="https://stackoverflow.com/questions/13772567/how-to-get-the-cpu-cycle-count-in-x86-64-from-c/51907627#51907627">&#34;reference&#34; frequency of the clock <code>rdtsc</code> reads</a>, in my case 3.9GHz vs. 4008 MHz.  (Rated max turbo is 4.2GHz, but with EPP = <code>balance_performance</code> on Linux, it only wants to clock up to 3.9 GHz.)</p>
<p>Source code I tested <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1DIApACYAQuYukl9ZATwDKjdAGFUtAK4sGIAGykrgAyeAyYAHI%2BAEaYxCAAHNIADqgKhE4MHt6%2BAaSp6Y4CoeFRLLHxSbaY9kUMQgRMxATZPn4ArNW1mQ1NBCWRMXGJ0gqNza25nWN9A2UVIwCUtqhexMjsHAD0WwDUFozICCxNANa7VMSsmADuJOdM6EzJBJjoF8SoLLsIXkoAnmEwBwFKRdjdCAgLnhVJgFLtDO8DMRgHFdsRMMlMEwCLs0F5BAoTBoAILEkk7XYCXYAcQw0U8BDBAFoTusECYAMwAEWYjgAbphdqzkhiqHFmYKHCRmRD0AQOTz2lwzHjVrR3rFdmFXhixmFgOTKRCFVsTQhVrjmVQqEwxqycRzSZTol5cTchegBMDcacGKgbrsFUKACpCNy7fkKAB0aoxeNoqGQ5yuRWjRr2Qi%2BmEcbHhdPQDNouOIBPh1JJAFludqGDMGBsiaTyeZOWFkN4sLsuW5kGNiAaudgW2Y2w3O0Ke2N8IJo4rsLtdpSe32CDOCEPtfDRZhxcQMe8wrs3NZrGDXbjpzq5%2BD7jGW83R3gqFgqLsAPqV8PvgBq2AASiOnK1h2Xhdj2OoDgwc5DkBNRKEBIETt2nJuKoCT%2BJBYQwZyw6PpyrjPg%2BFJ7KgLyZGI4JXMkWLENqb7/KsuxegwPrgoYuJELs2h/LifxCu%2B77EPKCjIBAiy7JggrEP8NwIHEmDkmEtBhIppJeDq/iSO%2BJbYugYZuOJ3YAOxWKSi6LpS74sCw760OKDaYOJXIWJZpHkQIYi0P87GEBcJCSU0KlomECh1kGqDojmeDxrE/DxhijwGkG8kJkmpzkhZuwaYIWk6UGokoTWgnCQQonOZyZkkllVk2XZDkbBVrlLu5dReT50SJsmuwGLqtb9l4DiZPCBKOLQ6IicgUWOHqmUWRiBBrAwBWmJVLbGdyxGts%2BuwaBZlK/ACQIgpGcTpAIGaCTiBADhemCCVA/p4MkTCLIs5KvCwyS9YpqE6rsDCwaSymqeS/KoHg7xUBAXpeJ1QoAFRqnWuIlXCN14A4gnMTir0mKZxGLmIeDAMwCgQFpEnI2MzGrPD%2BxFbsZjRmYLlzcTpN2hTkhU2gKO03D9DHoznLRpybPmUTKmc%2BTlMC/TkMmO0VjtDWXJq6ZGikFwpBmKQnKkNInSBMZpAJKQACc2ta1wOsqtrBtSNr7T45ta2Swi0tk9zEmwwr6Cs8rStqzyJkWFwgRcKbXDm1wVtmFrZg62Yeujrr0hmJ0ZiBGYptmObZhW5yWuclwrsS2SHvdlX8UQLWuJ4OrGgudqPaA5VreWOrnJmBJ%2BOuYTWVD4uraikwwAnLSbgRng/JYMkw%2BL0vy8r6va/V9Vi%2B1/X3FNy52g9hH%2B%2BnpY71V1l/dzcvHPe3L8tC3qXjFozXDRmXHdX4vN9c3f99Co/xZVTq12K/d%2BVUR7n3XhZT%2BS8AG4gRurRWysD6qwrmvOB3Yu6hysO7TeK8MEIJ5EgiwKC3bNWgZA4eGDTyhxPB3Dea8niNCVhYRulhSGMzgS3YeMCh68KoXCJ%2BBBVSEO5JDQOJDg5oNXnAoBWCaw4PIfwoesjdiiPESw0h0j8GCMAZgnBNY6HgNXkwpgLC2GSMsEfVWnDdHCO0Tw5slCl7EI4TQmsPdozN1wVAjRyDg76OAZ47xA9nGL2UeXSBn9XYb0iWSUchEqDEU%2Bt9HEv03D/WCNgCIQMSRXQIBjO6D0IBPQYCpcIpAnovTekpMpoNSTg0hmqL6bpMBmBhnTIWSM%2BY0zRv2TGBBsamLPmSAmVcem4j9kLaqwDQFggHqHZmethZBLFg4iZO9%2BngVDkfehLVsoME%2BLQcatppTEFjCAqK6BBpwgiilIUJASZhDEJ/DZUzPRxFnkA0O1UkYQC2YjJm70fEWXebsAAWiwgFwcwQAE0oUYywFIkFi54q7AgP9bQjM9otwPqhAFKFXIHy7qffubkUp4HhImMi4I6AnKft5A5RzaDKKyvC/xNjgHaARtymZlhuIIwHvyoxy9KSpG8hAbQEkZjNGSvyMQXhblcXbBiNgghWUWUhRykONYIAkgRgS6w%2BwJJI2hfQykXAaZYAHPKgUk5Q4KGuLWDIlFwX%2BTol5SS9A1VlWiRtQeqKSD/LwAAL3ug3PeHc8AoQjFkiIwp0UAuZFwYFrkLHqwBX3MZ8S2xvi4PtPY0RHi0oVAiXYwBExFvGk0K4/xzxul2AobMuZbmxAKWic0ZbUg6jiMotFGLBDcWxYS3e%2BLEX2qJSfCw71s2MNxuY6wbjQ7sskaglFy8V1uPkRCzRyLjEry1autW27rVfIcRff1%2BF4JqTwUPSk1LkjwgVDiBE8ZsQKH%2BBALkAAxCSaKmDNOSHQNEXFmXZQXjqSKTa2C7CEAASWrKdM5TZb1ZX7ZipuI68VuENR3YlVhT6LlMQu9hATgGbr3eSsYJAb1L3Q4OvF3IcV4anOOrDU6qYUc5duw9Wi9kFoROgdAfaAoDtxIx5jk6x0Dm2ZOkl07Fy8bIyez5/JWb8cpI8YTV6GD4CSVXOJAb9kdmxAwLwC80XBh6naXE4pAzRvld4OEn96PieHbimNuw42YPaI2tjuKONh2UfenMjgjD3MA8BuiSghRWfMO0AlohlryVoBByZmBkCQ0Rv6AgSMuJMDdKgSUGWiADjDRS7cfR3UAwEMyX4qI/MybhOmZxOx0hhvytG4B3nmSJvHcmvu/K8X7rvVsYjytG6ctDuKz9eBU1%2Bs2s2S9lcVvbXHGBe1GS0g3WxCwXJtTcQnDCM5JbzVXkCDGJgVQooeoCGAADF%2BGgzCSARs9pcjbQ2YFQNDd582q6JnCwD%2B7qhHnhdmQ47%2BsseZ/xxsw5WgM10jbBSwMIbh/gmfhLMjQ2Psctxc0G/6GIsQ4kjRYIn2INyoS4C7Sq5OSfyZGRekbWUCtcQIO/bkiV9LhiasFrYSTULt2wDDXGf3K4WwmVdm7/0W1E8xzsnHuPcHEgtq5oMdFgGSfV552XmIiQdxuoF8ldPcQPofBbFXaAWmvHU24IXIvGhi6XjscGvVgMw/SAwc4wDxvh1znu83OwICNPQIsT3GV10s8K0GdTnO9IGV5048XYLkDvlou%2Be1urhHMnZ4sLYHTBaYEWCUhGuvH1O8XtOEAKBLQ9h7HiNPcQM8xrrynKKe5DhCmQOj%2Bg240SmK9ZgH16KSb%2BigvdxaxAGSSQIMgaM712h2/MOp5nAnqQsB8lgBQpwiALzg9lJQ7wrPlNbTUAM5uapbFT%2BnxGQSLbY68e9yQ2OEheO4ZSDw%2B4SuZAuEwM51WN9tRjJmR/BjJsdzhO0ANn8NAEhdhKwAAJENdvNKZMetXEZfKtQwDYRvYgeKE4RyZfKkMpHyegMqbUXEYASKLiUWC2WkRAvnKvGvN0OvVCBvG/FvVCQg7vDHXYWiWHADGoIfRgXECANAL/BwN4D4L4ZA7g3vO5MQrvLqU4BQBfJfFOc9CyFHLIHvW5YBRgrQiALQtHDHMEa/JvIvBxQzGuAnQdRuHkEJOwpfSqdNUlMJRcEHcfbsdoYcUOMTRYX3SbMhUbA/WLVKUxC5ODbkAAaXBAQB8ioGSGZCHH%2Bj5mknOmgkXH3yIFQHOAVEpSpAbW%2BwixN1uxpVAzeBuRq0pVowskYPxEpzoQjGXw8PVkIJYIjA8MHHXTqNr04NYOXyVjty0LxB0L7zogH2AQGL6LcCMNGI4IjGBEGNYg4AcQWiWl2grjiQ4GWFoE4HaF4D8A4C0FIFQE4BPHk0bVWHWEnFHB4FIAIE0G2OWFOBAE5EkDFn8ASHaGLiew0DAM5Atk5ECF2I4EkAOMeJOM4F4AUBAC1geKOO2NIDgFgCQCtyA3oDIAoFEK%2BHROGA7EMGAC4B%2BL4DoF1BhIgGiAhOiGeRkk4DuKtx9QAHliCISsB8CK1Nhjj8AMQhpBQYSESghYRkBWkISe0QTjiVJogrgZIPAsA6TeAMYWB5TlgqADBgAFAfw8BbhGSsRDi7j%2BBBARAxB2AnYDT5AlA1AITdAdYDAjAUAp19A8BogYTIBlgyI6h%2BTmRpx1ZSYvAp0o5hRGTgIHR2QM1Th/gDBTghRWQpSsD5IFBZRIQwhmQe4LBmQGQCQngBw4RoTBChpnAIBXBJg/AdYQhVJ5hhgdYCgMgBBiy9Bqy6g5ghh4gdY7Bv8BBegJhPA2g9A2z8z6hxh%2BhyzmzezBy6zWzBymzyhKzlgm01gNg9AdtNgeAdi9jwSBTTiOB0J/AQDJAEwCSQFRYNBH8IBziCMbBdhcBCAApWwdZjwcSotMFS5FgFTHi3pSB5JHhhhxJSAXjORX9c5OR2gNALYMII4uANB2hOgQSwTSAlSIKtZDjjjNzoTYT7i3ykSYBEAQBLRkg3RyBKA0SosIhrhOBtzdz9zwsiSvE35eA3hryZM9AzSjTxBTTZBFAVB1ABTrTSAbhqJlT9A1zKkITNzGS3Q8LcQijyKtJKL7tqLjy9pTyHyMSnyU1XyET3yXjc4xZ2hjILZ/ANBbZjJPioKU5BLQT1zkKoTbA0L4StB3yQSzBLLeAUL0KNLlg0jMgQBJAgA" rel="noreferrer">on Godbolt</a>: using one inner loop, rather than 3 separate <code>j&lt;16</code> loops, and <em>not</em> using <code>-ffast-math</code>.  And using <code>__attribute__((noinline))</code> to keep this from inlining into the repeat loop.  Some other variations of options and source led to some <code>vpermpd</code> shuffles inside the loop.</p>
<pre><code>$ clang++ -std=gnu++17 -O3 -march=native -mbranches-within-32B-boundaries poly-eval.cpp -Wall
# warning about noipa, only GCC knows that attribute
$ perf stat --all-user -etask-clock,context-switches,cpu-migrations,page-faults,cycles,instructions,uops_issued.any,uops_executed.thread,fp_arith_inst_retired.256b_packed_double -r10 ./a.out
... (10 runs of the whole program, ending with)
0.252072 cycles per data element (corrected from ref cycles to core clocks)
0.252236 cycles per data element (corrected from ref cycles to core clocks)
0.252408 cycles per data element (corrected from ref cycles to core clocks)
0.251945 cycles per data element (corrected from ref cycles to core clocks)
0.252442 cycles per data element (corrected from ref cycles to core clocks)
0.252295 cycles per data element (corrected from ref cycles to core clocks)
0.252109 cycles per data element (corrected from ref cycles to core clocks)
xor=4303
min cycles per data = 0.251868

 Performance counter stats for &#39;./a.out&#39; (10 runs):

            298.92 msec task-clock                #    0.989 CPUs utilized            ( +-  0.49% )
                 0      context-switches          #    0.000 /sec                   
                 0      cpu-migrations            #    0.000 /sec                   
               129      page-faults               #  427.583 /sec                     ( +-  0.56% )
     1,162,430,637      cycles                    #    3.853 GHz                      ( +-  0.49% )  # time spent in the kernel for system calls and interrupts isn&#39;t counted, that&#39;s why it&#39;s not 3.90 GHz
     3,772,516,605      instructions              #    3.22  insn per cycle           ( +-  0.00% )
     3,683,072,459      uops_issued.any           #   12.208 G/sec                    ( +-  0.00% )
     4,824,064,881      uops_executed.thread      #   15.990 G/sec                    ( +-  0.00% )
     2,304,000,000      fp_arith_inst_retired.256b_packed_double #    7.637 G/sec                  

           0.30210 +- 0.00152 seconds time elapsed  ( +-  0.50% )
</code></pre>
<p><code>fp_arith_inst_retired.256b_packed_double</code> counts 1 for each FP add or mul instruction (2 for FMA), so <strong>we&#39;re getting 1.98 <code>vaddpd</code> instructions per clock cycle</strong> for the whole program, including printing and so on.  That&#39;s very close to the theoretical max 2/clock, apparently not suffering from sub-optimal scheduling of the add instructions.  (I bumped up the repeat loop so it spends more of its time there, so it&#39;s more useful to use perf stat on the whole program.)</p>
<p>The goal of this optimization was to get the same work done with fewer FLOPS, but that also means we&#39;re essentially maxing out the 8 FLOP/clock limit for Skylake without using FMA.  (And getting 30.58 GFLOP/s at 3.9GHz on a single core).</p>
<p>Asm of the non-inline function (<code>objdump -drwC -Mintel</code>); clang used 4 sets of 2 pairs of YMM vectors, and unrolled the loop a further 3x.  Note the <code>add  rax,0x30</code> doing 3 * <code>0x10</code> doubles per iteration.  So it works out to an exact multiple of 24KiB with no cleanup.</p>
<pre><code>0000000000001440 &lt;void compute2&lt;3072&gt;(double*)&gt;:
# just loading constants; the setup loop did flatten out and optimize into vector constants
    1440:       c5 fd 28 0d 18 0c 00 00         vmovapd ymm1,YMMWORD PTR [rip+0xc18]        # 2060 &lt;_IO_stdin_used+0x60&gt;
    1448:       c5 fd 28 15 30 0c 00 00         vmovapd ymm2,YMMWORD PTR [rip+0xc30]        # 2080 &lt;_IO_stdin_used+0x80&gt;
    1450:       c5 fd 28 1d 48 0c 00 00         vmovapd ymm3,YMMWORD PTR [rip+0xc48]        # 20a0 &lt;_IO_stdin_used+0xa0&gt;
    1458:       c4 e2 7d 19 25 bf 0b 00 00      vbroadcastsd ymm4,QWORD PTR [rip+0xbbf]        # 2020 &lt;_IO_stdin_used+0x20&gt;
    1461:       c5 fd 28 2d 57 0c 00 00         vmovapd ymm5,YMMWORD PTR [rip+0xc57]        # 20c0 &lt;_IO_stdin_used+0xc0&gt;
    1469:       48 c7 c0 d0 ff ff ff    mov    rax,0xffffffffffffffd0
    1470:       c4 e2 7d 19 05 af 0b 00 00      vbroadcastsd ymm0,QWORD PTR [rip+0xbaf]        # 2028 &lt;_IO_stdin_used+0x28&gt;
    1479:       c5 fd 28 f4             vmovapd ymm6,ymm4
    147d:       c5 fd 28 fc             vmovapd ymm7,ymm4
    1481:       c5 7d 28 c4             vmovapd ymm8,ymm4
    1485:       66 66 2e 0f 1f 84 00 00 00 00 00        data16 cs nop WORD PTR [rax+rax*1+0x0]

# top of outer loop.  The NOP before this is to align it.
    1490:       c5 fd 11 ac c7 80 01 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x180],ymm5
    1499:       c5 d5 58 ec             vaddpd ymm5,ymm5,ymm4
    149d:       c5 dd 58 e0             vaddpd ymm4,ymm4,ymm0
    14a1:       c5 fd 11 9c c7 a0 01 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x1a0],ymm3
    14aa:       c5 e5 58 de             vaddpd ymm3,ymm3,ymm6
    14ae:       c5 cd 58 f0             vaddpd ymm6,ymm6,ymm0
    14b2:       c5 fd 11 94 c7 c0 01 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x1c0],ymm2
    14bb:       c5 ed 58 d7             vaddpd ymm2,ymm2,ymm7
    14bf:       c5 c5 58 f8             vaddpd ymm7,ymm7,ymm0
    14c3:       c5 fd 11 8c c7 e0 01 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x1e0],ymm1
    14cc:       c5 bd 58 c9             vaddpd ymm1,ymm8,ymm1
    14d0:       c5 3d 58 c0             vaddpd ymm8,ymm8,ymm0
    14d4:       c5 fd 11 ac c7 00 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x200],ymm5
    14dd:       c5 d5 58 ec             vaddpd ymm5,ymm5,ymm4
    14e1:       c5 dd 58 e0             vaddpd ymm4,ymm4,ymm0
    14e5:       c5 fd 11 9c c7 20 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x220],ymm3
    14ee:       c5 e5 58 de             vaddpd ymm3,ymm3,ymm6
    14f2:       c5 cd 58 f0             vaddpd ymm6,ymm6,ymm0
    14f6:       c5 fd 11 94 c7 40 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x240],ymm2
    14ff:       c5 ed 58 d7             vaddpd ymm2,ymm2,ymm7
    1503:       c5 c5 58 f8             vaddpd ymm7,ymm7,ymm0
    1507:       c5 fd 11 8c c7 60 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x260],ymm1
    1510:       c5 bd 58 c9             vaddpd ymm1,ymm8,ymm1
    1514:       c5 3d 58 c0             vaddpd ymm8,ymm8,ymm0
    1518:       c5 fd 11 ac c7 80 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x280],ymm5
    1521:       c5 d5 58 ec             vaddpd ymm5,ymm5,ymm4
    1525:       c5 dd 58 e0             vaddpd ymm4,ymm4,ymm0
    1529:       c5 fd 11 9c c7 a0 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x2a0],ymm3
    1532:       c5 e5 58 de             vaddpd ymm3,ymm3,ymm6
    1536:       c5 cd 58 f0             vaddpd ymm6,ymm6,ymm0
    153a:       c5 fd 11 94 c7 c0 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x2c0],ymm2
    1543:       c5 ed 58 d7             vaddpd ymm2,ymm2,ymm7
    1547:       c5 c5 58 f8             vaddpd ymm7,ymm7,ymm0
    154b:       c5 fd 11 8c c7 e0 02 00 00      vmovupd YMMWORD PTR [rdi+rax*8+0x2e0],ymm1
    1554:       c5 bd 58 c9             vaddpd ymm1,ymm8,ymm1
    1558:       c5 3d 58 c0             vaddpd ymm8,ymm8,ymm0
    155c:       48 83 c0 30             add    rax,0x30
    1560:       48 3d c1 0b 00 00       cmp    rax,0xbc1
    1566:       0f 82 24 ff ff ff       jb     1490 &lt;void compute2&lt;3072&gt;(double*)+0x50&gt;
    156c:       c5 f8 77                vzeroupper 
    156f:       c3                      ret    
</code></pre>
<hr/>
<h3>Related:</h3>
<ul>
<li><a href="https://stackoverflow.com/questions/63095394/latency-bounds-and-throughput-bounds-for-processors-for-operations-that-must-occ">Latency bounds and throughput bounds for processors for operations that must occur in sequence</a> - analysis of code with two dep chains, one reading from the other and earlier in itself.  Same dependency pattern as the strength-reduced loop, except one of its chains is an FP multiply.  (It&#39;s also a polynomial evaluation scheme, but for one large polynomial.)</li>
<li><a href="https://stackoverflow.com/questions/47983660/simd-optimization-of-a-curve-computed-from-the-second-derivative">SIMD optimization of a curve computed from the second derivative</a> another case of being able to stride along the serial dependency.</li>
<li><a href="https://stackoverflow.com/questions/53804648/is-it-possible-to-use-simd-on-a-serial-dependency-in-a-calculation-like-an-expo">Is it possible to use SIMD on a serial dependency in a calculation, like an exponential moving average filter?</a> - <em>If there&#39;s a closed-form formula for n steps ahead, you can use that to sidestep serial dependencies.</em></li>
<li><a href="https://stackoverflow.com/questions/71012124/out-of-order-execution-how-to-solve-true-dependency">Out of Order Execution, How to Solve True Dependency?</a> - CPUs have to wait when an instruction depends on one that hasn&#39;t executed yet.</li>
<li><a href="https://stackoverflow.com/questions/36739118/dependency-chain-analysis">Dependency chain analysis</a> a non-loop-carried dependency chain analysis, from one of Agner Fog&#39;s examples.</li>
<li><a href="https://www.lighterra.com/papers/modernmicroprocessors/" rel="noreferrer">Modern Microprocessors
A 90-Minute Guide!</a> - general background on out-of-order exec and pipelines.  Modern CPU-style short-vector SIMD exists in this form to get more work through the pipeline of a single CPU without widening the pipeline.  By contrast, GPUs have many simple pipelines.</li>
<li><a href="https://stackoverflow.com/questions/45113527/why-does-mulss-take-only-3-cycles-on-haswell-different-from-agners-instruction">Why does mulss take only 3 cycles on Haswell, different from Agner&#39;s instruction tables? (Unrolling FP loops with multiple accumulators)</a> - Some experimental numbers with unrolling to hide the latency of FP dependency chains, and some CPU-architecture background on register renaming.</li>
</ul>
    </div></div>
  </body>
</html>
