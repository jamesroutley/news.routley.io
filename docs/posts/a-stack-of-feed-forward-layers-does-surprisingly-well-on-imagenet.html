<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2105.02723">Original</a>
    <h1>A stack of feed-forward layers does surprisingly well on ImageNet</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/2105.02723">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  The strong performance of vision transformers on image classification and
other vision tasks is often attributed to the design of their multi-head
attention layers. However, the extent to which attention is responsible for
this strong performance remains unclear. In this short report, we ask: is the
attention layer even necessary? Specifically, we replace the attention layer in
a vision transformer with a feed-forward layer applied over the patch
dimension. The resulting architecture is simply a series of feed-forward layers
applied over the patch and feature dimensions in an alternating fashion. In
experiments on ImageNet, this architecture performs surprisingly well: a
ViT/DeiT-base-sized model obtains 74.9\% top-1 accuracy, compared to 77.9\% and
79.9\% for ViT and DeiT respectively. These results indicate that aspects of
vision transformers other than attention, such as the patch embedding, may be
more responsible for their strong performance than previously thought. We hope
these results prompt the community to spend more time trying to understand why
our current models are as effective as they are.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Luke Melas-Kyriazi [<a href="https://arxiv.org/show-email/fa940cf6/2105.02723">view email</a>]
      </p></div></div>
  </body>
</html>
