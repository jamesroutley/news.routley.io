<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.primeintellect.ai/blog/intellect-2-release">Original</a>
    <h1>Intellect-2 Release: The First 32B Model Trained Through Globally Distributed RL</h1>
    
    <div id="readability-page-1" class="page"><div><section id="sectionFixed"><div><div><div><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-500.png 500w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-800.png 800w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-1080.png 1080w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-1600.png 1600w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-2000.png 2000w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report.png 2160w"/></p></div></div><div data-w-id="e5e1e688-1022-43bb-86ee-2597b4f74f82"><div id="blogRichText" fs-toc-element="contents" fs-toc-offsettop="8rem" fs-richtext-element="rich-text"><p>We&#39;re excited to release INTELLECT-2, the first 32B parameter model trained via globally distributed reinforcement learning.  Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning language model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.</p><p>To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.</p><p>Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B.</p><p>We open-source INTELLECT-2 along with our code and data, hoping to enable more open research in the field of decentralized training</p><ul role="list"><li><strong>Detailed Technical Report: </strong><a href="http://primeintellect.ai/intellect-2">primeintellect.ai/intellect-2</a></li><li><a href="https://huggingface.co/collections/PrimeIntellect/intellect-2-68205b03343a82eabc802dc2"><strong>INTELLECT-2 on Hugging Face</strong></a><ul role="list"><li>Chat Interface to try it out: <a href="http://chat.primeintellect.ai/">chat.primeintellect.ai</a></li></ul></li><li><a href="https://github.com/PrimeIntellect-ai/prime-rl"><strong>prime-rl: – our async RL framework</strong></a></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/68210770ed19e45df800ade8_Screenshot%202025-03-25%20at%2017.46.32%20(2).png" loading="lazy" alt=""/></p></figure><h2><strong>Paradigm Shift for Decentralized Training</strong></h2><p>Test-time compute scaling with reinforcement learning has emerged as a new scaling axis for large language models (LLMs), enabling improvements by allowing models to spend more time reasoning.</p><p>However, reinforcement learning training is typically centralized, requiring large clusters of co-located GPUs and fast interconnect speeds. With INTELLECT-2, we showcase a paradigm shift: reinforcement learning is inherently more asynchronous and well suited for decentralized, globally distributed compute.</p><h2>Training Infrastructure</h2><p>We introduce the following key open-source infrastructure components for training INTELLECT-2:</p><ul role="list"><li><a href="https://github.com/PrimeIntellect-ai/prime-rl"><strong>PRIME-RL</strong>:</a><ul role="list"><li>Fully asynchronous reinforcement learning framework designed for decentralized training. Decouples rollout generation, model training, and weight broadcasting. It enables training across heterogeneous, unreliable networks.</li><li>The trainer implementation uses PyTorch FSDP2, inference uses vLLM and the verifiers use the <a href="https://github.com/PrimeIntellect-ai/genesys">GENESYS</a> schema introduced in <a href="https://www.primeintellect.ai/blog/synthetic-1-release">SYNTHETIC-1</a>.</li></ul></li><li><strong>SHARDCAST</strong>: A library for distributing large files via a HTTP-based tree-topology network that efficiently propagates updated model weights to the decentralized inference workers.</li></ul><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/67fc98b6b2ae65e17cf41926_Blog%20Asset.png" loading="lazy" alt=""/></p></figure><ul role="list"><li><strong>TOPLOC</strong>:<ul role="list"><li>A locality-sensitive hashing scheme for efficient verifiable inference. It detects tampering or precision changes in model inference and works reliably across nondeterministic GPU hardware.</li><li>Inference workers generate the rollouts, these rollout files are uploaded via signed URLs, an on-chain event triggers TOPLOC validators to check them; accepted files feed the trainer, while invalid ones slash and remove the submitting node from the pool.</li></ul></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107b3b1d00e49e0e1d232_toploc-validator.png" loading="lazy" alt=""/></p></figure><ul role="list"><li><a href="https://github.com/PrimeIntellect-ai/pi-protocol"><strong>Protocol Testnet</strong>:</a> Provides the infrastructure to aggregate and coordinate global computeresources.<ul role="list"><li>Rust-based orchestrator and discovery service coordinate permissionless workers—nodes auto-register with hardware checks, heartbeats, and <em>pull</em> Docker-container tasks while the orchestrator schedules workloads, tracks health, and records pool ownership and contributions.</li></ul></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107d311028667c6db24ad_image%20(1).png" loading="lazy" alt=""/></p></figure><h2>Training Recipe</h2><ul role="list"><li><strong>Training Data &amp; Rewards:</strong><ul role="list"><li>285k verifiable tasks (math &amp; coding) from <strong>NuminaMath-1.5, Deepscaler</strong>, and <a href="https://www.primeintellect.ai/blog/synthetic-1-release">SYNTHETIC-1</a>.</li><li>Binary task reward + length reward lets users budget thinking tokens at inference time.</li></ul></li><li><strong>Two-step asynchronous RL:</strong> The broadcast of new policy weights is fully overlapped with ongoing inference and training—eliminating the communication bottleneck</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107ef11028667c6db337a_async-rl-schedule.png" loading="lazy" alt=""/></p></figure><ul role="list"><li><strong>Two-Sided GRPO Clipping:</strong> Stabilizes training by mitigating gradient spikes with two-sided token probability ratio clipping.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/6821080742d40ea0c31cfc42_two-sided-clipping.png" loading="lazy" alt=""/></p></figure><ul role="list"><li><strong>Advanced Data Filtering:</strong> Combines offline and online filtering to select challenging tasks, significantly enhancing model learning efficiency.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/6821086978179d3909d2a696_data-filtering.png" loading="lazy" alt=""/></p></figure><ul role="list"><li><strong>Aggressive Gradient Clipping:</strong> Addresses escalating gradient norms at scale, providing improved training stability.</li></ul><h2>Experiments</h2><p>We report results from two main experiments: TARGET-SHORT, an experimental run with short target lengths to train an efficient reasoning model, and, TARGET-LONG, our main run with longer target lengths.</p><ul role="list"><li><strong>Compute Utilization:</strong> During the two main experiments, we successfully overlapped communication with computation through two-step asynchronous reinforcement learning.</li><li><strong>Reward Trajectories:</strong><ul role="list"><li>Throughout training, we saw significant improvements of our task rewards, indicating that the model improved its performance on our mathematics and coding problems. We also saw a reduction of length penalties, but a much slower one than during our ablation experiments</li></ul></li></ul><p>‍</p><ul role="list"><li><strong>Benchmark Performance:</strong> We were able to increase the performance of QwQ-32B on mathematics and coding benchmarks.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682108892e5b8c350cb40532_benchmarks.png" loading="lazy" alt=""/></p></figure><ul role="list"><li>Overall, as QwQ-32B was already extensively trained with RL, it was difficult to obtain huge amounts of generalized improvement on benchmarks beyond our improvements on the training dataset. To see stronger improvements, it is likely that better base models such as the now available Qwen3, or higher quality datasets and RL environments are needed.</li></ul><h2>Future Work</h2><p>INTELLECT-2 is a first step towards open frontier reasoning models trained in a decentralized fashion. Over the coming months, we’ll work on:</p><ul role="list"><li><strong>Increasing the Ratio of Inference to Training Compute</strong><ul role="list"><li>Inference is embarrassingly parallel and communication-free, so more complex RL environments that devote <em>more</em> FLOPs to inference are a natural fit for decentralized training.</li></ul></li><li><strong>Tool Calls &amp; Multi-Turn RL</strong><ul role="list"><li>To fully leverage inference-time compute for scientific and research progress, we need to give models built-in tools in its reasoning chain—web search, Python interpreters, etc.</li></ul></li><li><strong>Crowdsourcing RL Tasks &amp; Environments</strong><ul role="list"><li>We believe open-source has a unique advantage here. Distributed RL is still in its early days, and with the right community and contributions, open-source AI can outpace the closed labs.</li></ul></li><li><strong>Model Merging &amp; DiLoCo</strong><ul role="list"><li>Fuse independently trained RL models—either at the end or continuously via DiLoCo—to create a single unified model, scaling decentralized RL to a lot more compute.</li></ul></li></ul><p>Check out the technical reports for more details: <a href="https://www.primeintellect.ai/intellect-2">primeintellect.ai/intellect-2</a></p><h2><strong>Join Us</strong></h2><p>INTELLECT-2 demonstrates that globally decentralized RL works. Now, we’re focusing on tool-assisted reasoning, crowdsourcing higher-quality data, and optimizing our infrastructure and training recipe to build frontier open models.</p><p>Join us if you want to build open source and decentralized AGI with us.</p></div><div data="chatbox"><div><p>Eurus-2-PRIME</p><div><div><p>When tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process. [ASSESS] [ADVANCE] [VERIFY] [SIMPLIFY] [SYNTHESIZE] [PIVOT] [OUTPUT] You should strictly follow the format below: [ACTION NAME] # Your action step 1 # Your action step 2 # Your action step 3 ... Next action: [NEXT ACTION NAME]</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div></div></div><div><p>INTELLECT-MATH-7B-SFT (ours)</p><div><div><p>Solve the following math problem efficiently and clearly. Think carefully and step by step about your response and reason before providing a final response. Conclude your response with: \n\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\n\nWhere [answer] is just the final number or expression that solves the problem.</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c75/678edcd5f221a97f3337fc68_Ellipse%201.png" loading="lazy" alt=""/></p><p>INTELLECT-MATH-7B-SFT</p></div><div><div><p>
So I&#39;ve got this problem here about quadratic polynomials $P(x)$ and $Q(x)$. $P(x)$ has a leading coefficient of $2$, and $Q(x)$ has a leading coefficient of $-2$. Both polynomials pass through the points $(16,54)$ and $(20,53)$. I need to find $P(0) + Q(0)$.</p></div></div></div></div></div><div><p>INTELLECT-MATH-7B (ours)</p><div><div><p>Solve the following math problem efficiently and clearly. Think carefully and step by step about your response and reason before providing a final response. Conclude your response with: \n\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\n\nWhere [answer] is just the final number or expression that solves the problem.</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c75/678edcd5f221a97f3337fc68_Ellipse%201.png" loading="lazy" alt=""/></p><p>INTELLECT-MATH-7B</p></div><div><div><p>
So I have two quadratic polynomials, $P(x)$ and $Q(x)$, with leading coefficients 2 and -2, respectively. Both pass through the points $(16,54)$ and $(20,53)$. I need to find $P(0) + Q(0)$.</p></div></div></div></div></div></div></div><div><div><p>Acknowledgements</p><p>Prime Intellect Research Team: Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Matthew Di Ferrante, Kushal Thaman, Felix Gabriel, Fares Obeid, Kemal Erdem, Michael Keiblinger, Johannes Hagemann</p></div></div></div></div></div><section></section></section></div></div>
  </body>
</html>
