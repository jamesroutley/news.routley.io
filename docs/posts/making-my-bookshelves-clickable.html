<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jamesg.blog/2024/02/14/clickable-bookshelves/">Original</a>
    <h1>Making my bookshelves clickable</h1>
    
    <div id="readability-page-1" class="page"><div id="main">
    	 <!-- This page uses microformats to structure different pieces of information.
    I use h-entry to state this is a post. Any class name that starts with h-, p-, or -e is a microformat.
    By specifying microformats, some web tools can better understand this page. For example, IndieNews can use
    the p-name to figure out the title of the post (without the "| James' Coffee Blog" I add to the <title> tag of my website.
    Learn more about h-entry: https://indieweb.org/h-entry
-->
<article>
	<header>
		
		<p><em>Published on
			<a href="https://words.filippo.io/2024/02"><time datetime="2024-02-14T00:00:00">February 14, 2024</time></a>
			 under the <a href="https://jamesg.blog/category/indieweb">IndieWeb</a> category. <a onclick="document.getElementsByTagName(&#39;incoming-links&#39;)[0].toggle(); document.getElementsByTagName(&#39;outgoing-links&#39;)[0].toggle();">Toggle Memex mode</a></em></p>
		  
		
		
		
	</header>
	<section>
		<div>
			<p>You can make regions of an image clickable with a number of techniques, from overlyaing an SVG that contains <code>onclick</code> JavaScript handlers all the way to using image maps. I love this idea. I started to think about how I could create an image of my bookshelves that you could click to learn more about each book I am reading. This would be more engaging than a traditional list of text.</p>
<p>I built a script that takes in an image of a bookshelf and makes each book clickable. When you click on a book, you are taken to the Google Books page associated with the book. You do not need to manually annotate any book or map each book to its title. This happens automatically. You can <a href="https://capjamesg.github.io/cv-book-svg/">try a demo of a clickable bookshelf on GitHub</a>.</p>
<p>Here is a video of my clicking through different books on my bookshelf:</p>
<video width="320" height="240" controls="">
  <source src="/assets/book_demo.mov" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

<p>The red border indicates the polygon whose contents are clickable.</p>
<p>In this blog post, I am going to discuss how I made this project. This post uses computer vision, but I will do my best to explain all jargon. You shouldn&#39;t need a computer vision background to enjoy this post. If any details do not make sense, email me at readers [at] jamesg [dot] blog. If you want to learn how to use this tool yourself, <a href="https://github.com/capjamesg/cv-book-svg">refer to the project GitHub repository setup instructions</a>.</p>
<p>Without further ado, let&#39;s get started!</p>
<h2>The problem and solution</h2>
<p>The problem: I wanted to make an image of bookshelves clickable. </p>
<p>How could I go about addressing this problem? Here are the steps I had in mind when I started to work on this project:</p>
<ol>
<li>Isolate the region of each book in the image.</li>
<li>Retrieve the title for each book using Optical Character Recognition (OCR).</li>
<li>Retrieve the Google Books URL for each book.</li>
<li>Map each URL to its respective region.</li>
<li>Create an SVG that can be overlaid onto an image.</li>
</ol>
<p>Let&#39;s talk through each of these steps.</p>
<h2>Isolating book regions</h2>
<p>We need to know where books are in an image before we can make them clickable. We could manually annotate each book. I made a tool for manually drawing polygons called PolygonZone that you can use to manually annotate regions. But, I wanted to make a solution that is automatic. For that, I needed a computer vision model.</p>
<p>For this project, I decided to use a combination of two models: Grounding DINO and Segment Anything (SAM). The combination is called Grounded SAM.</p>
<p>If you don&#39;t have any experience in computer vision, stay with me!</p>
<p>Grounding DINO lets you identify objects in images. You can give Grounding DINO a text prompt (i.e. &#34;book spine&#34;) and the model will try to identify all instances of that object in an image. Here is an example of the result from Grounding DINO when passing through an image of my bookshelf:</p>
<p><a href="https://words.filippo.io/assets/book_bboxes.png"></a><a href="https://words.filippo.io/assets/book_bboxes.png"><img src="https://words.filippo.io/assets/book_bboxes.png" alt="Book bounding boxes"/></a></p>
<p>There is a box around (most of) the books in the image.</p>
<p>This is great! We now know where each book is. But, each box is larger than the book it represents. This is because every book is angled in the image. We could use these boxes to make each book clickable, but some regions would overlap. This would be confusing and unintuitive.</p>
<p>We can use a segmentation model to identify the <em>exact region</em> of each book. This is where the Segment Anything Model (SAM) comes in. We can use SAM to retrieve masks for each book. You can convert masks into polygons to get the outline of an object.</p>
<p>Here is an example of the bookshelf processed with Grounding DINO then SAM:</p>
<p><a href="https://words.filippo.io/assets/book_masks.png"></a><a href="https://words.filippo.io/assets/book_masks.png"><img src="https://words.filippo.io/assets/book_masks.png" alt="Book segmentation masks"/></a></p>
<p>The purple regions are polygons. If you look closely, you can see boundaries between each book that are not in purple. This shows our model is segmenting individual books.</p>
<p>There are some regions highlighted that are not books. These regions do not have text in them. Thus, GPT will not be able to find data for them. We can only plot polygons for which we can retrieve a title to ensure that only relevant polygons are displayed in the output. In addition, a few books are not highlighted. This means the model we are using -- a combination of Grounding DINO and SAM -- could not isolate a region for the book. This could be manually corrected using a polygon annotation tool, but is not ideal. I need to think through what solution would be easiest for users.</p>
<p>The process of generating masks takes ~15 seconds on an M1 Macbook Air.</p>
<h2>Retrieving book titles</h2>
<p>We now know where our books are in an image. Next, we need to figure out the title and author of each book. This involves a few steps. First, we need to isolate each book. Then, we need to read the characters on each book. At minimum, we should get the title of a book. We may also get the author name, depending on if the author name is on the spine. We can then use this information to search for a book on Google Books.</p>
<p>Reading characters in an image is a domain called Optical Character Recognition (OCR). There are many ways to do OCR, but for this project I chose to use GPT-4 with Vision, which has been accurate in many OCR tests I have run and seen run. GPT-4 with Vision allows you to ask questions about images. In this case, I could request the model identify the characters in each book image.</p>
<p>Before sending an image to GPT-4 with Vision, I isolated the region of each book. I then rotated the book to the left by 90 degrees so it would be horizontal instead of vertical. This should boost OCR performance. Here is an example of an image sent to GPT-4 with Vision:</p>
<p><a href="https://words.filippo.io/assets/isolated_book_region.jpeg"></a><a href="https://words.filippo.io/assets/isolated_book_region.jpeg"><img src="https://words.filippo.io/assets/isolated_book_region.jpeg" alt="Isolated book"/></a></p>
<p>In this image, one specific book is isolated. We can send this image to GPT-4 with Vision to retrieve the characters on the book.</p>
<p>I used the following prompt with the image:</p>
<blockquote>
<p>Read the text on the book spine. Only say the book cover title and author if you can find them. Say the book that is most prominent. Return the format [title] [author], with no punctuation.</p>
</blockquote>
<p>Here is an example response:</p>
<blockquote>
<p>The Poetry Pharmacy Returns William Sieghart</p>
</blockquote>
<p>With this information, we can look up the book with the Google Books API. The Google Books search API uses the following syntax:</p>
<pre><code>
https://www.googleapis.com/books/v1/volumes?q={book}
</code></pre>
<p>You can add any text in the <code>{book}</code> section above. In this script, we send the book name and, if available, the author name. I didn&#39;t separate them out. Including both pieces of information seemed to work well.</p>
<p>This API returns several pieces of information about a book. For my script, I gathered the:</p>
<ul>
<li>Author name</li>
<li>ISBN</li>
<li>Google Books URL</li>
</ul>
<p>Here is an example books URL for the Google Books listing URL for the book matching <code>The Poetry Pharmacy Returns William Sieghart</code>:</p>
<pre><code>
https://play.google.com/store/books/details?id=vdOXDwAAQBAJ&amp;source=gbs_api
</code></pre>

<p>This entire process -- calling the GPT-4 with Vision and Google Books API -- takes a few seconds per book.</p>
<h2>Create a clickable SVG</h2>
<p>All of the information collected with GPT-4 with Vision and the Google Books search API is associated with each book and region in the image. Each mask -- the form returned by Segment Anything -- is converted to a polygon so it can be used in an SVG that I can overlay over my image. Using these polygons, I can generate a HTML file with two components:</p>
<ol>
<li>The source image, and;</li>
<li>An SVG file that can be overlaid over the image.</li>
</ol>
<p>The SVG can include JavaScript. For this project, I have an <code>onclick</code> handler that opens the Google Books URL associated with each book.</p>
<p>I generate a HTML file with an SVG. In the HTML file, I embed my source image and overlay the SVG. The SVG uses polygons to represent each book region. Each polygon typically has dozens of points. The <code>onclick</code> handler redirects the user to the corresponding Google Books page when a book is clicked. Here is a screenshot of the resulting page (the books are not clickable because this is a screenshot):</p>
<p><a href="https://words.filippo.io/assets/book_result.png"></a><a href="https://words.filippo.io/assets/book_result.png"><img src="https://words.filippo.io/assets/book_result.png" alt="Result"/></a></p>
<p><a href="https://capjamesg.github.io/cv-book-svg/">You can try the demo -- and click the books! -- on GitHub pages</a>.</p>
<h2>Conclusion and Next Steps</h2>
<p>My system to make clickable bookshelves is designed to be autonomous. You should be able to upload an arbitrary bookshelf and generate clickable regions as above. With that said, there are limitations. If a book title is hard to read, the GPT-4 with Vision API may struggle to run OCR. Thus, you will not be able to associate a region with a book URL. If a book is not on Google Books, you would need to use another URL. In one test, a book URL was entirely wrong because the book wasn&#39;t available on Google Books.</p>
<p>Of course, Google Books can be swapped with any source. If you have a blog, the source of URLs could be your blog. You could make each book clickable and take the user to your review of the book.</p>
<p>There are a few improvements I have in mind that I would like to make:</p>
<ol>
<li>Use EfficientSAM, a faster version of SAM.</li>
<li>Make the polygons look nicer.</li>
<li>Maybe add a manual correction system so if the system can&#39;t read the text of a book I can fix it.</li>
</ol>
<p>If you have questions about this project, email me at readers [at] jamesg [dot] blog.</p>
			
				<p>Tagged in <a href="https://words.filippo.io/tag/indieweb/">IndieWeb</a>.</p>
			
		</div>
		
		<p><a href="https://notbyai.fyi/"><img src="https://jamesg.blog/assets/ai.png" alt="Written by human, not by AI"/></a></p>
	</section>
	
	<section>
    <h2>Responses</h2>
    
    <h2>Comment on this post</h2>
    <p>Respond to this post by sending a <a href="https://indieweb.org/Webmention">Webmention</a>.</p>
    <p>Have a comment? Email me at <a href="mailto:readers@jamesg.blog?subject=Making my bookshelves clickable">readers@jamesg.blog</a>.</p>  
</section>
</article>
      </div></div>
  </body>
</html>
