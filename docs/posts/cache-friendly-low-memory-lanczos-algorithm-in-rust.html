<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lukefleed.xyz/posts/cache-friendly-low-memory-lanczos/">Original</a>
    <h1>Cache-friendly, low-memory Lanczos algorithm in Rust</h1>
    
    <div id="readability-page-1" class="page"><article id="article"> <p>The standard Lanczos method for computing matrix functions has a brutal memory requirement: storing an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span>k</span></span></span></span> basis matrix that grows with every iteration. For a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>500.000</mn></mrow><annotation encoding="application/x-tex">500.000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>500.000</span></span></span></span>-variable problem needing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1000</mn></mrow><annotation encoding="application/x-tex">1000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1000</span></span></span></span> iterations, that’s roughly 4 GB just for the basis.</p>
<p>In this post, we will explore one of the most straightforward solutions to this problem: a two-pass variant of the Lanczos algorithm that only requires <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span>)</span></span></span></span> memory at the cost of doubling the number of matrix-vector products. The surprising part is that when implemented carefully, the two-pass version isn’t just memory-efficient—it can be faster for certain problems. We will dig into why.</p>
<ul>
<li>All code is available on GitHub: <a href="https://github.com/lukefleed/two-pass-lanczos">two-pass-lanczos</a></li>
<li>The full technical report with proofs and additional experiments: <a href="https://github.com/lukefleed/two-pass-lanczos/raw/master/tex/report.pdf">report.pdf</a></li>
</ul>
<hr/>
<h2 id="table-of-contents">Table of Contents<a href="#table-of-contents"><span aria-hidden="true">#</span></a></h2>
<details><summary>Open Table of Contents</summary>
<ul>
<li><a href="#computing-matrix-functions">Computing Matrix Functions</a>
<ul>
<li><a href="#krylov-projection">Krylov Projection</a>
<ul>
<li><a href="#building-an-orthonormal-basis">Building an Orthonormal Basis</a></li>
<li><a href="#solving-in-the-reduced-space">Solving in the Reduced Space</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#the-lanczos-algorithm">The Lanczos Algorithm</a>
<ul>
<li><a href="#three-term-recurrence">Three-Term Recurrence</a></li>
<li><a href="#reconstructing-the-solution">Reconstructing the Solution</a></li>
</ul>
</li>
<li><a href="#two-pass-algorithm">Two-Pass Algorithm</a>
<ul>
<li><a href="#first-pass-compute-the-projected-problem">First Pass: Compute the Projected Problem</a></li>
<li><a href="#second-pass-reconstruct-and-accumulate">Second Pass: Reconstruct and Accumulate</a>
<ul>
<li><a href="#a-subtle-numerical-point">A Subtle Numerical Point</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#recurrence-step">Recurrence Step</a></li>
<li><a href="#an-iterator-for-state-management">An Iterator for State Management</a></li>
<li><a href="#first-pass-computing-the-decomposition">First Pass: Computing the Decomposition</a></li>
<li><a href="#second-pass-reconstructing-the-solution">Second Pass: Reconstructing the Solution</a></li>
<li><a href="#the-public-api">The Public API</a>
<ul>
<li><a href="#example-solving-a-linear-system">Example: Solving a Linear System</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#some-interesting-results">Some interesting results</a>
<ul>
<li><a href="#memory-and-computation-trade-off">Memory and Computation Trade-off</a>
<ul>
<li><a href="#memory-usage">Memory Usage</a></li>
<li><a href="#runtime-where-theory-breaks">Runtime: Where Theory Breaks</a></li>
<li><a href="#medium-scale-behavior">Medium-Scale Behavior</a></li>
<li><a href="#what-about-dense-matrices">What About Dense Matrices?</a></li>
</ul>
</li>
<li><a href="#scalability">Scalability</a></li>
</ul>
</li>
</ul>
</details>

<p>Let’s consider the problem of computing the action of matrix functions on a vector:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">x</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{x} = f(\mathbf{A})\mathbf{b}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span><span></span><span>=</span><span></span></span><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span><span>b</span></span></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> is a large sparse Hermitian matrix and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> is a matrix function defined on the spectrum of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span>. This is a problem that appears pretty often in scientific computing: solving linear systems corresponds to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>z</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span>, exponential integrators for PDEs use <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>t</mi><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(z) = \exp(tz)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>z</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>exp</span><span>(</span><span>t</span><span>z</span><span>)</span></span></span></span>, and many other problems require functions like <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>−</mo><mn>1</mn><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1/2}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>z</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1/2</span></span></span></span></span></span></span></span></span></span></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>sign</mtext><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(z) = \text{sign}(z)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>z</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>sign</span></span><span>(</span><span>z</span><span>)</span></span></span></span>.</p>
<p>Indeed, there are a lot problems with computing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span></span></span></span> directly. First of all, even if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> is sparse, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span></span></span></span> is generally dense. Storing it explicitly is out of the question for large problems. Even if we could store it, computing it directly would require algorithms like the Schur-Parlett method that scale as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span>)</span></span></span></span>, which is impractical for large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span>.</p>
<p>However we know that given any matrix function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> defined on the spectrum of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span>, we can express <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span></span></span></span> as a polynomial in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> of degree at most <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> (the size of the matrix) such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A}) = p_{n}(\mathbf{A})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>n</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>A</span><span>)</span></span></span></span> (this is a consequence of the Cayley-Hamilton theorem). This polynomial interpolates <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> and its derivatives in the Hermitian sense at the eigenvalues of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span>.</p>
<p>This gives us a good and a bad news: the good news is that, well, we can express <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span></span></span></span> as a polynomial in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span>. The bad news is that the degree of this polynomial can be as high as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span>, which is huge for large problems. The idea is then to find a low-degree polynomial approximation to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> that is <em>good enough</em> for our purposes. If we can find a polynomial <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">p_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> of degree <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>≪</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k \ll n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span><span></span><span>≪</span><span></span></span><span><span></span><span>n</span></span></span></span> such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mo>≈</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_k(\mathbf{A}) \approx f(\mathbf{A})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>A</span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span></span></span></span>, then we can approximate the solution as:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi><mo>≈</mo><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><msub><mi>c</mi><mi>i</mi></msub><msup><mi mathvariant="bold">A</mi><mi>i</mi></msup><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">f(\mathbf{A})\mathbf{b} \approx p_k(\mathbf{A})\mathbf{b} = \sum_{i=0}^k c_i \mathbf{A}^i \mathbf{b}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>A</span><span>)</span><span>b</span><span></span><span>≈</span><span></span></span><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>A</span><span>)</span><span>b</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>0</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>c</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>A</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span></span></span></span></span><span>b</span></span></span></span></span>
<p>This polynomial only involves vectors within a specific subspace.</p>
<h2 id="krylov-projection">Krylov Projection<a href="#krylov-projection"><span aria-hidden="true">#</span></a></h2>
<p>We can notice that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">p_k(\mathbf{A})\mathbf{b}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>A</span><span>)</span><span>b</span></span></span></span> only depends on vectors in the Krylov subspace of order <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span></p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">K</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo separator="true">,</mo><mi mathvariant="bold">b</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>span</mtext><mo stretchy="false">{</mo><mi mathvariant="bold">b</mi><mo separator="true">,</mo><mrow><mi mathvariant="bold">A</mi><mi mathvariant="bold">b</mi></mrow><mo separator="true">,</mo><msup><mi mathvariant="bold">A</mi><mn>2</mn></msup><mi mathvariant="bold">b</mi><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi mathvariant="bold">A</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="bold">b</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{K}_k(\mathbf{A}, \mathbf{b}) = \text{span}\{\mathbf{b}, \mathbf{Ab}, \mathbf{A}^2\mathbf{b}, \ldots, \mathbf{A}^{k-1}\mathbf{b}\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>A</span><span>,</span><span></span><span>b</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>span</span></span><span>{</span><span>b</span><span>,</span><span></span><span><span>Ab</span></span><span>,</span><span></span><span><span>A</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>b</span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>A</span><span><span><span><span><span><span></span><span><span><span>k</span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span>b</span><span>}</span></span></span></span></span>
<p>This is fortunate: we can compute an approximate solution by staying within this space, which only requires repeated matrix-vector products with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span>. For large sparse matrices, that’s the only operation we can do efficiently anyway.</p>
<blockquote>
<p>We don’t need to construct <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">A</mi><mi>j</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{A}^j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>A</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span></span></span></span></span></span></span></span> explicitly. We compute iteratively: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold">A</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="bold">b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{A}(\mathbf{A}^{j-1}\mathbf{b})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span><span>(</span><span><span>A</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span>b</span><span>)</span></span></span></span>.</p>
</blockquote>
<p>But there’s a problem: the raw vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msup><mi mathvariant="bold">A</mi><mi>j</mi></msup><mi mathvariant="bold">b</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\mathbf{A}^j\mathbf{b}\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>A</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span></span></span></span></span><span>b</span><span>}</span></span></span></span> form a terrible basis. They quickly become nearly parallel, making any computation numerically unstable. We need an orthonormal basis.</p>
<h3 id="building-an-orthonormal-basis">Building an Orthonormal Basis<a href="#building-an-orthonormal-basis"><span aria-hidden="true">#</span></a></h3>
<p>The standard method is the Arnoldi process, which is Gram-Schmidt applied to Krylov subspaces. We start by normalizing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo>=</mo><mi mathvariant="bold">b</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_1 = \mathbf{b} / \|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>b</span><span>/∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. Then, iteratively:</p>
<ol>
<li>Compute a new candidate: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{w}_j = \mathbf{A}\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>A</span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></li>
<li>Orthogonalize against all existing basis vectors:</li>
</ol>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mi>j</mi></msub><mo>=</mo><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>j</mi></munderover><mo stretchy="false">(</mo><msubsup><mi mathvariant="bold">v</mi><mi>i</mi><mi>H</mi></msubsup><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\tilde{\mathbf{v}}_j = \mathbf{w}_j - \sum_{i=1}^j (\mathbf{v}_i^H \mathbf{w}_j) \mathbf{v}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>H</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<ol start="3">
<li>Normalize: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mi>j</mi></msub><mi mathvariant="normal">/</mi><mi mathvariant="normal">∥</mi><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mi>j</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1} = \tilde{\mathbf{v}}_j / \|\tilde{\mathbf{v}}_j\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/∥</span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></li>
</ol>
<p>The coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msubsup><mi mathvariant="bold">v</mi><mi>i</mi><mi>H</mi></msubsup><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_{ij} = \mathbf{v}_i^H \mathbf{w}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>h</span><span><span><span><span><span><span></span><span><span><span>ij</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>H</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> become entries of a projected matrix. After <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> iterations, we have:</p>
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{V}_k = [\mathbf{v}_1, \ldots, \mathbf{v}_k]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>[</span><span><span>v</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>]</span></span></span></span>: an orthonormal basis for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">K</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo separator="true">,</mo><mi mathvariant="bold">b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{K}_k(\mathbf{A}, \mathbf{b})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>A</span><span>,</span><span></span><span>b</span><span>)</span></span></span></span></li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>: an upper Hessenberg matrix representing the projection of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> onto this subspace</li>
</ul>
<p>We can express this relationship with the Arnoldi decomposition:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo>+</mo><msub><mi>h</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo><mi>k</mi></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><msubsup><mi mathvariant="bold">e</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{A}\mathbf{V}_k = \mathbf{V}_k \mathbf{H}_k + h_{k+1,k} \mathbf{v}_{k+1} \mathbf{e}_k^T</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>h</span><span><span><span><span><span><span></span><span><span><span>k</span><span>+</span><span>1</span><span>,</span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>k</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<h3 id="solving-in-the-reduced-space">Solving in the Reduced Space<a href="#solving-in-the-reduced-space"><span aria-hidden="true">#</span></a></h3>
<p>Now we approximate our original problem by solving it in the small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>-dimensional space. Using the Full Orthogonal Method (FOM), we enforce that the residual is orthogonal to
the Krylov subspace. This gives:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is computed as:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{H}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>f</span><span>(</span><span><span>H</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>The heavy lifting is now on computing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{H}_k)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span><span>H</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>, a small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k \times k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span><span></span><span>×</span><span></span></span><span><span></span><span>k</span></span></span></span> matrix.
Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>≪</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k \ll n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span><span></span><span>≪</span><span></span></span><span><span></span><span>n</span></span></span></span>, we can afford direct methods like Schur-Parlett (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>k</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k^3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>k</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span>)</span></span></span></span>).</p>
<blockquote>
<p>For <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>z</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span> (linear systems), this reduces to solving <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_k \mathbf{y}_k = \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> with LU decomposition.</p>
</blockquote>

<p>When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> is Hermitian (or symmetric in the real case), the general Arnoldi
process simplifies dramatically. We can prove that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">V</mi><mi>k</mi><mi>H</mi></msubsup><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_k = \mathbf{V}_k^H \mathbf{A} \mathbf{V}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span>H</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>A</span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> must also be Hermitian. A matrix that is both upper Hessenberg <em>and</em> Hermitian must be real, symmetric, and tridiagonal. This is a <em>huge</em> simplification.</p>
<p>In the literature, this projected matrix is denoted <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to highlight its
tridiagonal structure:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>α</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>β</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>β</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>α</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>β</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>β</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋱</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋱</mo></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋱</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>α</mi><mi>k</mi></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{T}_k = \begin{pmatrix}
\alpha_1 &amp; \beta_1 &amp; &amp; \\
\beta_1 &amp; \alpha_2 &amp; \beta_2 &amp; \\
&amp; \beta_2 &amp; \ddots &amp; \ddots \\
&amp; &amp; \ddots &amp; \alpha_k
\end{pmatrix}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span><span></span><span><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="4.800em" viewBox="0 0 875 4800"><path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,1284c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-1292c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span><span><span><span><span><span></span><span><span><span>α</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span>β</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>β</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span>α</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span>β</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span></span></span><span><span></span><span><span><span>β</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span>⋱</span></span></span><span><span></span><span><span>⋱</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span></span></span><span><span></span><span></span></span><span><span></span><span><span>⋱</span></span></span><span><span></span><span><span><span>α</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span><svg xmlns="http://www.w3.org/2000/svg" width="0.875em" height="4.800em" viewBox="0 0 875 4800"><path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,1209
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-1344c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\alpha_j \in \mathbb{R}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span></span><span><span></span><span>R</span></span></span></span> are the diagonal elements and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\beta_j \in \mathbb{R}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span></span><span><span></span><span>R</span></span></span></span> are the off-diagonals (subdiagonals from the orthogonalization).</p>
<h2 id="three-term-recurrence">Three-Term Recurrence<a href="#three-term-recurrence"><span aria-hidden="true">#</span></a></h2>
<p>This tridiagonal structure leads to a beautiful simplification. To build the next basis
vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, we don’t need the entire history of vectors. We only need
the two previous ones. Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> is Hermitian, this guarantees that
any new vector is <em>automatically</em> orthogonal to all earlier vectors (beyond the previous two). So we can skip the full orthogonalization and use a simple three-term recurrence:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>=</mo><msub><mi>β</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>α</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>+</mo><msub><mi>β</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{A}\mathbf{v}_j = \beta_{j-1}\mathbf{v}_{j-1} + \alpha_j \mathbf{v}_j + \beta_j \mathbf{v}_{j+1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>Rearranging gives us an algorithm to compute <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> directly:</p>
<ol>
<li>Compute the candidate: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_{j+1} = \mathbf{A}\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>A</span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></li>
<li>Extract the diagonal coefficient: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">v</mi><mi>j</mi><mi>H</mi></msubsup><msub><mi>w</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_j = \mathbf{v}_j^H w_{j+1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>H</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></li>
<li>Orthogonalize against the two previous vectors:</li>
</ol>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>α</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>−</mo><msub><mi>β</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\tilde{\mathbf{v}}_{j+1} = w_{j+1} - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<ol start="4">
<li>Normalize: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="normal">∥</mi><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\beta_j = \|\tilde{\mathbf{v}}_{j+1}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>∥</span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">/</mi><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1} = \tilde{\mathbf{v}}_{j+1} / \beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></li>
</ol>
<p>This is known as the Lanczos algorithm. It’s more efficient than Arnoldi because each iteration only orthogonalizes against two previous vectors instead of all prior ones.</p>
<h2 id="reconstructing-the-solution">Reconstructing the Solution<a href="#reconstructing-the-solution"><span aria-hidden="true">#</span></a></h2>
<p>After <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> iterations, we end up with the tridiagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> basis vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{V}_k = [\mathbf{v}_1, \ldots, \mathbf{v}_k]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>[</span><span><span>v</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>]</span></span></span></span>. We can then reconstruct the approximate solution as:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>f</span><span>(</span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is solved from the small tridiagonal matrix.</p>
<p>There is a timing problem however: we cannot compute the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>
until all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> iterations are complete. The full matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is only available
at the end, so we must store every basis vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> along the way, leading to a memory cost of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nk)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>nk</span><span>)</span></span></span></span>.</p>
<p>So we’re left with a choice: whether we store all the basis vectors and solve the problem in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> passes, or find a way to avoid storing them. There is a middle ground.</p>
<blockquote>
<p>There are also techniques to compress the basis vectors, have a look <a href="https://arxiv.org/abs/2403.04390">here</a></p>
</blockquote>

<p>Here’s where we break the timing deadlock. The insight that we don’t actually need to store the basis vectors if we can afford to compute them twice</p>
<p>Think about what we have after the first pass. We’ve computed all the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> coefficients that compose the entire tridiagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. These numbers are small compared to the full basis. What if we kept only these scalars, discarded all the vectors, and then replayed the Lanczos recurrence a second time? We’d regenerate the same basis, and this time we’d use it to build the solution.</p>
<p>This comes at a cost. We run Lanczos twice, so we pay for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>k</span></span></span></span> matrix-vector products instead of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>. But we only ever store a constant number of vectors in memory, no <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nk)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>nk</span><span>)</span></span></span></span> basis matrix. The memory complexity drops to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span>)</span></span></span></span>.</p>
<p>It sounds like a bad trade at first. But as we’ll see later, the cache behavior of this
two-pass approach can actually make it as fast (or even faster) on real hardware if well optimized.</p>
<h2 id="first-pass-compute-the-projected-problem">First Pass: Compute the Projected Problem<a href="#first-pass-compute-the-projected-problem"><span aria-hidden="true">#</span></a></h2>
<p>We initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo>=</mo><mi mathvariant="bold">b</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_1 = \mathbf{b} / \|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>b</span><span>/∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_0 = 0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>0</span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>0</mn></msub><mo>=</mo><mn mathvariant="bold">0</mn></mrow><annotation encoding="application/x-tex">\mathbf{v}_0 = \mathbf{0}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>0</span></span></span></span>.Then we run the standard Lanczos recurrence:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j = \mathbf{A}\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>A</span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">v</mi><mi>j</mi><mi>H</mi></msubsup><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j = \mathbf{v}_j^H w_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>H</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mi>j</mi></msub><mo>−</mo><msub><mi>α</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>−</mo><msub><mi>β</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\tilde{\mathbf{v}}_{j+1} = w_j - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="normal">∥</mi><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">/</mi><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j = \|\tilde{\mathbf{v}}_{j+1}\|_2, \quad \mathbf{v}_{j+1} = \tilde{\mathbf{v}}_{j+1} / \beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>∥</span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span></span><span>v</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>At each step, we record <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. But we <em>do not</em> store <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.
Instead, we discard it immediately after computing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. In this way we only keep in memory at most just three vectors at any time (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and the working vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>).</p>
<p>After <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> iterations, we have the full set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>α</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>β</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>α</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>β</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_1, \beta_1, \ldots, \alpha_k, \beta_k\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>α</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span>. These <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>k</span><span>)</span></span></span></span> scalars define the tridiagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. We can now solve:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>f</span><span>(</span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>This is the solution in the reduced space. Now that we have the coefficients we need to build <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</p>
<h2 id="second-pass-reconstruct-and-accumulate">Second Pass: Reconstruct and Accumulate<a href="#second-pass-reconstruct-and-accumulate"><span aria-hidden="true">#</span></a></h2>
<p>With <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> in memory, we replay the Lanczos recurrence <em>exactly as before</em>. We start with the same initialization (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>) and apply the same sequence of operations, using the stored scalars <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to reconstruct each basis vector on demand. We can write some rust-like <em>pseudocode</em> for this second pass to get a feel for it:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>let</span><span> mut</span><span> x_k</span><span> =</span><span> vec!</span><span>[</span><span>0</span><span>.</span><span>0</span><span>; </span><span>n</span><span>];</span></span>
<span><span>let</span><span> mut</span><span> v_prev</span><span> =</span><span> vec!</span><span>[</span><span>0</span><span>.</span><span>0</span><span>; </span><span>n</span><span>];</span></span>
<span><span>let</span><span> mut</span><span> v_curr</span><span> =</span><span> b</span><span>.</span><span>clone</span><span>() </span><span>/</span><span> b_norm</span><span>;</span></span>
<span></span>
<span><span>for</span><span> j</span><span> in</span><span> 1</span><span>..=</span><span>k</span><span> {</span></span>
<span><span>    let</span><span> w</span><span> =</span><span> A</span><span> @</span><span> v_curr</span><span>;  </span><span>// Matrix-vector product</span></span>
<span></span>
<span><span>    // We don&#39;t recompute alpha/beta; we already have them from pass 1</span></span>
<span><span>    let</span><span> alpha_j</span><span> =</span><span> alphas</span><span>[</span><span>j</span><span> -</span><span> 1</span><span>];</span></span>
<span><span>    let</span><span> beta_prev</span><span> =</span><span> j</span><span> &gt;</span><span> 1</span><span> ?</span><span> betas</span><span>[</span><span>j</span><span> -</span><span> 2</span><span>] </span><span>:</span><span> 0</span><span>.</span><span>0</span><span>;</span></span>
<span></span>
<span><span>    // Accumulate the solution</span></span>
<span><span>    x_k</span><span> +=</span><span> y_k</span><span>[</span><span>j</span><span> -</span><span> 1</span><span>] </span><span>*</span><span> v_curr</span><span>;</span></span>
<span></span>
<span><span>    // Regenerate the next basis vector for the *next* iteration</span></span>
<span><span>    let</span><span> v_next</span><span> =</span><span> (</span><span>w</span><span> -</span><span> alpha_j</span><span> *</span><span> v_curr</span><span> -</span><span> beta_prev</span><span> *</span><span> v_prev</span><span>) </span><span>/</span><span> betas</span><span>[</span><span>j</span><span> -</span><span> 1</span><span>];</span></span>
<span></span>
<span><span>    // Slide the window forward</span></span>
<span><span>    v_prev</span><span> =</span><span> v_curr</span><span>;</span></span>
<span><span>    v_curr</span><span> =</span><span> v_next</span><span>;</span></span>
<span><span>}</span></span></code></pre></div>
<p>This loop regenerates each <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> on demand and immediately uses it to update the solution.
Once we’ve accumulated <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><msub><mo stretchy="false">)</mo><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">(\mathbf{y}_k)_j \mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, we discard the vector. We never store the full basis.</p>
<h3 id="a-subtle-numerical-point">A Subtle Numerical Point<a href="#a-subtle-numerical-point"><span aria-hidden="true">#</span></a></h3>
<p>There is one detail worth noting: floating-point arithmetic is deterministic. When we replay the Lanczos recurrence in the second pass with the exact same inputs and the exact same order of operations, we get bitwise-identical vectors. The <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> regenerated in pass 2 are identical to the ones computed in pass 1.</p>
<p>However, the order in which we accumulate the solution differs. In a standard Lanczos,
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is built as a single matrix-vector product: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> (a <code>gemv</code> call in BLAS). In the two-pass method, it’s built as a loop of scaled vector additions (a series of <code>axpy</code> calls). These operations accumulate rounding error differently, so the final solution differs slightly, typically by machine epsilon. This rarely matters in practice, and convergence is unaffected.</p>

<p>Building this in Rust forces us to think concretely about where data lives and how it flows through the cache hierarchy. We need to control memory layout, decide when allocations happen, and choose abstractions that cost us nothing at runtime.</p>
<p>For linear algebra, we reach for <a href="https://github.com/sarah-ek/faer-rs"><code>faer</code></a>. Three design choices in this library matter for what we’re building:</p>
<ul>
<li><strong>Stack allocation via <code>MemStack</code>:</strong> Pre-allocated scratch space that lives for the entire computation. The hot path becomes allocation-free.</li>
<li><strong>Matrix-free operators:</strong> The <code>LinOp</code> trait defines an operator by its action (<code>apply</code>) without materializing a matrix. For large sparse problems, this is the only viable approach.</li>
<li><strong>SIMD-friendly loops:</strong> The <code>zip!</code> macro generates code that compiles to packed instructions.</li>
</ul>
<h2 id="recurrence-step">Recurrence Step<a href="#recurrence-step"><span aria-hidden="true">#</span></a></h2>
<p>Our starting point is the Lanczos three-term recurrence that we derived earlier:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>−</mo><msub><mi>α</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>−</mo><msub><mi>β</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\beta_j \mathbf{v}_{j+1} = \mathbf{A}\mathbf{v}_j - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>A</span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>We can translate this into a recurrence step function. The signature looks like this:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>fn</span><span> lanczos_recurrence_step</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>, </span><span>O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>O</span><span>,</span></span>
<span><span>    mut</span><span> w</span><span>:</span><span> MatMut</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_curr</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_prev</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    beta_prev</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> (T</span><span>::</span><span>Real</span><span>, </span><span>Option</span><span>&lt;</span><span>T</span><span>::</span><span>Real</span><span>&gt;)</span></span></code></pre></div>
<p>The function is generic over the field type <code>T</code> (<code>f64</code>, <code>c64</code>, etc.) and the operator type <code>O</code>. It operates on matrix views (<code>MatMut</code> and <code>MatRef</code>) to avoid unnecessary data copies. The return type gives us the diagonal element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and, <em>if no breakdown occurs</em>, the off-diagonal <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</p>
<p>Now we can implement the body by following the math. The first step is the most expensive:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>// 1. Apply operator: w = A * v_curr</span></span>
<span><span>operator</span><span>.</span><span>apply</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>, Par</span><span>::</span><span>Seq</span><span>, </span><span>stack</span><span>);</span></span></code></pre></div>
<p>The matrix-vector product dominates the computational cost. Everything else is secondary.</p>
<p>Next, we orthogonalize against <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. This is where we benefit from <code>faer</code>’s design. The <code>zip!</code> macro fuses this operation into a single loop that the compiler vectorizes into SIMD instructions.</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>// 2. Orthogonalize against v_{j-1}: w -= β_{j-1} * v_{j-1}</span></span>
<span><span>let</span><span> beta_prev_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>beta_prev</span><span>);</span></span>
<span><span>zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_prev</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_prev_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>    *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>beta_prev_scaled</span><span>, </span><span>v_prev_i</span><span>));</span></span>
<span><span>});</span></span></code></pre></div>
<p>With <code>w</code> partially orthogonalized, we can compute the diagonal coefficient via an inner product. Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> is Hermitian, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is guaranteed real.</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>// 3. Compute α_j = v_j^H * w</span></span>
<span><span>let</span><span> alpha</span><span> =</span><span> T</span><span>::</span><span>real_part_impl</span><span>(</span><span>&amp;</span><span>(</span><span>v_curr</span><span>.</span><span>adjoint</span><span>() </span><span>*</span><span> w</span><span>.</span><span>rb</span><span>())[(</span><span>0</span><span>, </span><span>0</span><span>)]);</span></span></code></pre></div>
<p>We complete the orthogonalization against <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> with another <code>zip!</code> loop.</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>// 4. Orthogonalize against v_j: w -= α_j * v_j</span></span>
<span><span>let</span><span> alpha_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>alpha</span><span>);</span></span>
<span><span>zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_curr_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>    *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>alpha_scaled</span><span>, </span><span>v_curr_i</span><span>));</span></span>
<span><span>});</span></span></code></pre></div>
<p>Now <code>w</code> holds the unnormalized next basis vector. We compute its norm to get <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. If this norm is numerically zero, the Krylov subspace is invariant, the iteration has reached its natural stopping point. This is called breakdown.</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>// 5. Compute β_j = ||w||_2 and check for breakdown</span></span>
<span><span>let</span><span> beta</span><span> =</span><span> w</span><span>.</span><span>rb</span><span>()</span><span>.</span><span>norm_l2</span><span>();</span></span>
<span><span>let</span><span> tolerance</span><span> =</span><span> breakdown_tolerance</span><span>::</span><span>&lt;T</span><span>::</span><span>Real</span><span>&gt;();</span></span>
<span></span>
<span><span>if</span><span> beta</span><span> &lt;=</span><span> tolerance</span><span> {</span></span>
<span><span>    (</span><span>alpha</span><span>, </span><span>None</span><span>)</span></span>
<span><span>} </span><span>else</span><span> {</span></span>
<span><span>    (</span><span>alpha</span><span>, </span><span>Some</span><span>(</span><span>beta</span><span>))</span></span>
<span><span>}</span></span></code></pre></div>
<p>The function returns <code>None</code> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> when breakdown occurs, signaling to the caller that no further iterations should proceed.</p>
<h2 id="an-iterator-for-state-management">An Iterator for State Management<a href="#an-iterator-for-state-management"><span aria-hidden="true">#</span></a></h2>
<p>The recurrence step is a pure function, but calling it in a loop is both inefficient and awkward. We’d need to manually pass vectors in and out of each iteration. More critically, we’d create copies when we should be reusing memory.</p>
<p>The iterator pattern solves this. We create a struct that encapsulates the state:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>struct</span><span> LanczosIteration</span><span>&lt;&#39;</span><span>a</span><span>, </span><span>T</span><span>:</span><span> ComplexField</span><span>, </span><span>O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;&gt; {</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>&#39;</span><span>a</span><span> O</span><span>,</span></span>
<span><span>    v_prev</span><span>:</span><span> Mat</span><span>&lt;</span><span>T</span><span>&gt;,       </span><span>// v_{j-1}</span></span>
<span><span>    v_curr</span><span>:</span><span> Mat</span><span>&lt;</span><span>T</span><span>&gt;,       </span><span>// v_j</span></span>
<span><span>    work</span><span>:</span><span> Mat</span><span>&lt;</span><span>T</span><span>&gt;,         </span><span>// Workspace for the next vector</span></span>
<span><span>    beta_prev</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,   </span><span>// β_{j-1}</span></span>
<span><span>    // ... iteration counters</span></span>
<span><span>}</span></span></code></pre></div>
<p>The main design choice here is that vectors are <strong>owned</strong> (<code>Mat&lt;T&gt;</code>), not borrowed. This enables an optimization in the <code>next_step</code> method. After computing the next vector and normalizing it into <code>work</code>, we cycle the state without allocating or copying:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>// Inside next_step, after normalization...</span></span>
<span><span>core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>v_prev, </span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>v_curr);</span></span>
<span><span>core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>v_curr, </span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>work);</span></span></code></pre></div>
<p>On x86-64, swapping two <code>Mat&lt;T&gt;</code> structures (fat pointers) compiles to three <code>mov</code> instructions. The pointers change, but no vector data moves. After the swap, <code>v_prev</code> points to what <code>v_curr</code> held, <code>v_curr</code> points to <code>work</code>’s allocation, and <code>work</code> points to the old <code>v_prev</code> data. In the next iteration, <code>work</code> gets reused.</p>
<p>We keep exactly three n-dimensional vectors live in memory. The same allocations cycle through the computation, staying hot in L1 cache. This is the core reason the two-pass method can be faster than expected, the working set never leaves cache.</p>
<h2 id="first-pass-computing-the-decomposition">First Pass: Computing the Decomposition<a href="#first-pass-computing-the-decomposition"><span aria-hidden="true">#</span></a></h2>
<p>The first pass runs the Lanczos iteration and collects the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>α</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_j, \beta_j\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span>. Basis vectors are discarded after each step.</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>pub</span><span> fn</span><span> lanczos_pass_one</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>impl</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>    b</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    k</span><span>:</span><span> usize</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>LanczosDecomposition</span><span>&lt;</span><span>T</span><span>::</span><span>Real</span><span>&gt;, </span><span>LanczosError</span><span>&gt; {</span></span>
<span><span>    // ...</span></span>
<span><span>}</span></span></code></pre></div>
<p>We allocate vectors for the coefficients with a capacity hint to avoid reallocations:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>let</span><span> mut</span><span> alphas</span><span> =</span><span> Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>k</span><span>);</span></span>
<span><span>let</span><span> mut</span><span> betas</span><span> =</span><span> Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>k</span><span> -</span><span> 1</span><span>);</span></span></code></pre></div>
<p>Then we construct the iterator. This allocates the three work vectors once. After this point, the hot path is allocation-free:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>let</span><span> mut</span><span> lanczos_iter</span><span> =</span><span> LanczosIteration</span><span>::</span><span>new</span><span>(</span><span>operator</span><span>, </span><span>b</span><span>, </span><span>k</span><span>, </span><span>b_norm</span><span>)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>for</span><span> i</span><span> in</span><span> 0</span><span>..</span><span>k</span><span> {</span></span>
<span><span>    if</span><span> let</span><span> Some</span><span>(</span><span>step</span><span>) </span><span>=</span><span> lanczos_iter</span><span>.</span><span>next_step</span><span>(</span><span>stack</span><span>) {</span></span>
<span><span>        alphas</span><span>.</span><span>push</span><span>(</span><span>step</span><span>.</span><span>alpha);</span></span>
<span><span>        steps_taken</span><span> +=</span><span> 1</span><span>;</span></span>
<span></span>
<span><span>        let</span><span> tolerance</span><span> =</span><span> breakdown_tolerance</span><span>::</span><span>&lt;T</span><span>::</span><span>Real</span><span>&gt;();</span></span>
<span><span>        if</span><span> step</span><span>.</span><span>beta </span><span>&lt;=</span><span> tolerance</span><span> {</span></span>
<span><span>            break</span><span>;</span></span>
<span><span>        }</span></span>
<span></span>
<span><span>        if</span><span> i</span><span> &lt;</span><span> k</span><span> -</span><span> 1</span><span> {</span></span>
<span><span>            betas</span><span>.</span><span>push</span><span>(</span><span>step</span><span>.</span><span>beta);</span></span>
<span><span>        }</span></span>
<span><span>    } </span><span>else</span><span> {</span></span>
<span><span>        break</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre></div>
<p>The check for breakdown stops the iteration when the residual becomes numerically zero. This means we’ve found an invariant subspace and there’s no value in continuing.</p>
<p>At the end, we collect the scalars into a <code>LanczosDecomposition</code> struct. The memory footprint throughout this pass is constant: three n-dimensional vectors plus two small arrays that grow to at most <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> elements.</p>
<h2 id="second-pass-reconstructing-the-solution">Second Pass: Reconstructing the Solution<a href="#second-pass-reconstructing-the-solution"><span aria-hidden="true">#</span></a></h2>
<p>Now we face a different problem. We have the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>α</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_j, \beta_j\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span> coefficients from the first pass and the coefficient vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>f</span><span>(</span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> from solving the projected problem. We need to reconstruct the solution:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo stretchy="false">(</mo><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><msub><mo stretchy="false">)</mo><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \sum_{j=1}^k (\mathbf{y}_k)_j \mathbf{v}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>
<p>without storing the full basis matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</p>
<p>The recurrence step in this pass is structurally similar to the first pass, but with a key difference: we no longer compute inner products or norms. We already know the coefficients, so the step becomes pure reconstruction.</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>fn</span><span> lanczos_reconstruction_step</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>, </span><span>O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>O</span><span>,</span></span>
<span><span>    mut</span><span> w</span><span>:</span><span> MatMut</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_curr</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_prev</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    alpha_j</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,</span></span>
<span><span>    beta_prev</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) {</span></span>
<span><span>    // Apply operator</span></span>
<span><span>    operator</span><span>.</span><span>apply</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>, Par</span><span>::</span><span>Seq</span><span>, </span><span>stack</span><span>);</span></span>
<span></span>
<span><span>    // Orthogonalize using stored α_j and β_{j-1}</span></span>
<span><span>    let</span><span> beta_prev_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>beta_prev</span><span>);</span></span>
<span><span>    zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_prev</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_prev_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>beta_prev_scaled</span><span>, </span><span>v_prev_i</span><span>));</span></span>
<span><span>    });</span></span>
<span></span>
<span><span>    let</span><span> alpha_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>alpha_j</span><span>);</span></span>
<span><span>    zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_curr_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>alpha_scaled</span><span>, </span><span>v_curr_i</span><span>));</span></span>
<span><span>    });</span></span>
<span><span>}</span></span></code></pre></div>
<p>This is cheaper than the first-pass recurrence. We’ve eliminated the inner products that computed <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and the norm calculation for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. What remains is pure orthogonalization and the operator application.</p>
<p><code>lanczos_pass_two</code> implements this reconstruction. We initialize the three work vectors and the solution accumulator:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>pub</span><span> fn</span><span> lanczos_pass_two</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>impl</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>    b</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    decomposition</span><span>:</span><span> &amp;</span><span>LanczosDecomposition</span><span>&lt;</span><span>T</span><span>::</span><span>Real</span><span>&gt;,</span></span>
<span><span>    y_k</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>LanczosError</span><span>&gt; {</span></span>
<span><span>    let</span><span> mut</span><span> v_prev</span><span> =</span><span> Mat</span><span>::</span><span>&lt;</span><span>T</span><span>&gt;</span><span>::</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>nrows</span><span>(), </span><span>1</span><span>);</span></span>
<span><span>    let</span><span> inv_norm</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>T</span><span>::</span><span>Real</span><span>::</span><span>recip_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>b_norm));</span></span>
<span><span>    let</span><span> mut</span><span> v_curr</span><span> =</span><span> b</span><span> *</span><span> Scale</span><span>(</span><span>inv_norm</span><span>);  </span><span>// v_1</span></span>
<span></span>
<span><span>    let</span><span> mut</span><span> work</span><span> =</span><span> Mat</span><span>::</span><span>&lt;</span><span>T</span><span>&gt;</span><span>::</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>nrows</span><span>(), </span><span>1</span><span>);</span></span>
<span></span>
<span><span>    // Initialize solution with first component</span></span>
<span><span>    let</span><span> mut</span><span> x_k</span><span> =</span><span> &amp;</span><span>v_curr</span><span> *</span><span> Scale</span><span>(T</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>y_k</span><span>[(</span><span>0</span><span>, </span><span>0</span><span>)]));</span></span></code></pre></div>
<p>We build the solution incrementally by starting with the first basis vector scaled by its coefficient. The main loop then regenerates each subsequent vector: we regenerate each subsequent basis vector, normalize it using the stored <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and immediately accumulate its contribution:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>for</span><span> j</span><span> in</span><span> 0</span><span>..</span><span>decomposition</span><span>.</span><span>steps_taken </span><span>-</span><span> 1</span><span> {</span></span>
<span><span>    let</span><span> alpha_j</span><span> =</span><span> T</span><span>::</span><span>Real</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>alphas[</span><span>j</span><span>]);</span></span>
<span><span>    let</span><span> beta_j</span><span> =</span><span> T</span><span>::</span><span>Real</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>betas[</span><span>j</span><span>]);</span></span>
<span><span>    let</span><span> beta_prev</span><span> =</span><span> if</span><span> j</span><span> ==</span><span> 0</span><span> {</span></span>
<span><span>        T</span><span>::</span><span>Real</span><span>::</span><span>zero_impl</span><span>()</span></span>
<span><span>    } </span><span>else</span><span> {</span></span>
<span><span>        T</span><span>::</span><span>Real</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>betas[</span><span>j</span><span> -</span><span> 1</span><span>])</span></span>
<span><span>    };</span></span>
<span></span>
<span><span>    // 1. Regenerate the unnormalized next vector</span></span>
<span><span>    lanczos_reconstruction_step</span><span>(</span></span>
<span><span>        operator</span><span>,</span></span>
<span><span>        work</span><span>.</span><span>as_mut</span><span>(),</span></span>
<span><span>        v_curr</span><span>.</span><span>as_ref</span><span>(),</span></span>
<span><span>        v_prev</span><span>.</span><span>as_ref</span><span>(),</span></span>
<span><span>        alpha_j</span><span>,</span></span>
<span><span>        beta_prev</span><span>,</span></span>
<span><span>        stack</span><span>,</span></span>
<span><span>    );</span></span>
<span></span>
<span><span>    // 2. Normalize using stored β_j</span></span>
<span><span>    let</span><span> inv_beta</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>T</span><span>::</span><span>Real</span><span>::</span><span>recip_impl</span><span>(</span><span>&amp;</span><span>beta_j</span><span>));</span></span>
<span><span>    zip!</span><span>(</span><span>work</span><span>.</span><span>as_mut</span><span>())</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>w_i</span><span> =</span><span> mul</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>inv_beta</span><span>);</span></span>
<span><span>    });</span></span>
<span></span>
<span><span>    // 3. Accumulate: x_k += y_{j+1} * v_{j+1}</span></span>
<span><span>    let</span><span> coeff</span><span> =</span><span> T</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>y_k</span><span>[(</span><span>j</span><span> +</span><span> 1</span><span>, </span><span>0</span><span>)]);</span></span>
<span><span>    zip!</span><span>(</span><span>x_k</span><span>.</span><span>as_mut</span><span>(), </span><span>work</span><span>.</span><span>as_ref</span><span>())</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>x_i</span><span>, </span><span>v_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>x_i</span><span> =</span><span> add</span><span>(</span><span>x_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>coeff</span><span>, </span><span>v_i</span><span>));</span></span>
<span><span>    });</span></span>
<span></span>
<span><span>    // 4. Cycle vectors for the next iteration</span></span>
<span><span>    core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> v_prev</span><span>, </span><span>&amp;</span><span>mut</span><span> v_curr</span><span>);</span></span>
<span><span>    core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> v_curr</span><span>, </span><span>&amp;</span><span>mut</span><span> work</span><span>);</span></span>
<span><span>}</span></span></code></pre></div>
<p>The accumulation <code>x_k += y_{j+1} * v_{j+1}</code> is implemented as a fused multiply-add in the <code>zip!</code> loop. On hardware with FMA support, this becomes a single instruction per element, not three separate operations.</p>
<p>Note that we accumulate the solution incrementally. After each iteration, <code>x_k</code> contains a partial result. We cycle through the same three vectors (<code>v_prev</code>, <code>v_curr</code>, <code>work</code>), keeping the working set small and resident in L1 cache.</p>
<p>Compare this to the standard method’s final reconstruction step: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. This is a dense matrix-vector product where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span>k</span></span></span></span>. When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> are both large, this matrix no longer fits in cache. The CPU must stream it from main memory, paying the cost of memory latency. Each element requires a load, multiply, and accumulate, but the load operations dominate—the CPU stalls waiting for data.</p>
<p>In our two-pass reconstruction, the operator <code>$\mathbf{A}$</code> is applied <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> times, but against vectors that stay in cache. The memory bandwidth is spent on reading the sparse structure of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> and the vector elements, not on scanning a dense <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span>k</span></span></span></span> matrix.</p>
<p>This is the reason the two-pass method can be faster on real hardware despite performing twice as many matrix-vector products. The cache behavior of the reconstruction phase overwhelms the savings of storing the basis.</p>
<h2 id="the-public-api">The Public API<a href="#the-public-api"><span aria-hidden="true">#</span></a></h2>
<p>We can wrap the two passes into a single entry point:</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>pub</span><span> fn</span><span> lanczos_two_pass</span><span>&lt;</span><span>T</span><span>, </span><span>O</span><span>, </span><span>F</span><span>&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>O</span><span>,</span></span>
<span><span>    b</span><span>:</span><span> MatRef</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    k</span><span>:</span><span> usize</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>    mut</span><span> f_tk_solver</span><span>:</span><span> F</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>LanczosError</span><span>&gt;</span></span>
<span><span>where</span></span>
<span><span>    T</span><span>:</span><span> ComplexField</span><span>,</span></span>
<span><span>    O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>    F</span><span>:</span><span> FnMut</span><span>(</span><span>&amp;</span><span>[T</span><span>::</span><span>Real</span><span>], </span><span>&amp;</span><span>[T</span><span>::</span><span>Real</span><span>]) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>anyhow</span><span>::</span><span>Error</span><span>&gt;,</span></span>
<span><span>{</span></span>
<span><span>    // First pass: compute T_k coefficients</span></span>
<span><span>    let</span><span> decomposition</span><span> =</span><span> lanczos_pass_one</span><span>(</span><span>operator</span><span>, </span><span>b</span><span>, </span><span>k</span><span>, </span><span>stack</span><span>)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>    if</span><span> decomposition</span><span>.</span><span>steps_taken </span><span>==</span><span> 0</span><span> {</span></span>
<span><span>        return</span><span> Ok</span><span>(</span><span>Mat</span><span>::</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>nrows</span><span>(), </span><span>1</span><span>));</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // Solve projected problem: y_k&#39; = f(T_k) * e_1</span></span>
<span><span>    let</span><span> y_k_prime</span><span> =</span><span> f_tk_solver</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>alphas, </span><span>&amp;</span><span>decomposition</span><span>.</span><span>betas)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>    // Scale by ||b||</span></span>
<span><span>    let</span><span> y_k</span><span> =</span><span> &amp;</span><span>y_k_prime</span><span> *</span><span> Scale</span><span>(T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>b_norm));</span></span>
<span></span>
<span><span>    // Second pass: reconstruct solution</span></span>
<span><span>    lanczos_pass_two</span><span>(</span><span>operator</span><span>, </span><span>b</span><span>, </span><span>&amp;</span><span>decomposition</span><span>, </span><span>y_k</span><span>.</span><span>as_ref</span><span>(), </span><span>stack</span><span>)</span></span>
<span><span>}</span></span></code></pre></div>
<p>The design separates concerns. The <code>f_tk_solver</code> closure is where we inject the specific matrix function. We compute the Lanczos decomposition, then pass the coefficients to the user-provided solver, which computes <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold">y</mi><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k&#39; = f(\mathbf{T}_k) \mathbf{e}_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span><span>′</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>f</span><span>(</span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> for whatever function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> is needed. This decoupling means we handle linear solves, matrix exponentials, or any other function without modifying the core algorithm.</p>
<p>The caller provides <code>f_tk_solver</code> as a closure. It receives the raw <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>α</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_j, \beta_j\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>α</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>β</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span> arrays and must return the coefficient vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold">y</mi><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{y}_k&#39;</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span><span><span></span><span><span><span>′</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. We then scale it by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\|\mathbf{b}\|_2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∥</span><span>b</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and pass everything to the second pass.</p>
<h3 id="example-solving-a-linear-system">Example: Solving a Linear System<a href="#example-solving-a-linear-system"><span aria-hidden="true">#</span></a></h3>
<p>To see this in practice, consider solving <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="bold">A</mi><mi mathvariant="bold">x</mi></mrow><mo>=</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{Ax} = \mathbf{b}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Ax</span></span><span></span><span>=</span><span></span></span><span><span></span><span>b</span></span></span></span>. We compute <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>z</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>z</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span>, which means the <code>f_tk_solver</code> must solve the small tridiagonal system <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><msup><mi mathvariant="bold">y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k \mathbf{y}&#39; = \mathbf{e}_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</p>
<p>Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is tridiagonal, we can exploit its structure. A sparse LU factorization solves it in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>k</span><span>)</span></span></span></span> time instead of the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>k</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k^3)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>k</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span>)</span></span></span></span> cost of a dense method.</p>
<div><pre tabindex="0" data-language="rust"><code><span><span>let</span><span> f_tk_solver</span><span> =</span><span> |</span><span>alphas</span><span>:</span><span> &amp;</span><span>[</span><span>f64</span><span>], </span><span>betas</span><span>:</span><span> &amp;</span><span>[</span><span>f64</span><span>]</span><span>|</span><span> -&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>f64</span><span>&gt;, </span><span>anyhow</span><span>::</span><span>Error</span><span>&gt; {</span></span>
<span><span>    let</span><span> steps</span><span> =</span><span> alphas</span><span>.</span><span>len</span><span>();</span></span>
<span><span>    if</span><span> steps</span><span> ==</span><span> 0</span><span> {</span></span>
<span><span>        return</span><span> Ok</span><span>(</span><span>Mat</span><span>::</span><span>zeros</span><span>(</span><span>0</span><span>, </span><span>1</span><span>));</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // 1. Assemble T_k from coefficients using triplet format</span></span>
<span><span>    let</span><span> mut</span><span> triplets</span><span> =</span><span> Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>3</span><span> *</span><span> steps</span><span> -</span><span> 2</span><span>);</span></span>
<span><span>    for</span><span> (</span><span>i</span><span>, </span><span>&amp;</span><span>alpha</span><span>) </span><span>in</span><span> alphas</span><span>.</span><span>iter</span><span>()</span><span>.</span><span>enumerate</span><span>() {</span></span>
<span><span>        triplets</span><span>.</span><span>push</span><span>(</span><span>Triplet</span><span> { </span><span>row</span><span>:</span><span> i</span><span>, </span><span>col</span><span>:</span><span> i</span><span>, </span><span>val</span><span>:</span><span> alpha</span><span> });</span></span>
<span><span>    }</span></span>
<span><span>    for</span><span> (</span><span>i</span><span>, </span><span>&amp;</span><span>beta</span><span>) </span><span>in</span><span> betas</span><span>.</span><span>iter</span><span>()</span><span>.</span><span>enumerate</span><span>() {</span></span>
<span><span>        triplets</span><span>.</span><span>push</span><span>(</span><span>Triplet</span><span> { </span><span>row</span><span>:</span><span> i</span><span>, </span><span>col</span><span>:</span><span> i</span><span> +</span><span> 1</span><span>, </span><span>val</span><span>:</span><span> beta</span><span> });</span></span>
<span><span>        triplets</span><span>.</span><span>push</span><span>(</span><span>Triplet</span><span> { </span><span>row</span><span>:</span><span> i</span><span> +</span><span> 1</span><span>, </span><span>col</span><span>:</span><span> i</span><span>, </span><span>val</span><span>:</span><span> beta</span><span> });</span></span>
<span><span>    }</span></span>
<span><span>    let</span><span> t_k_sparse</span><span> =</span><span> SparseColMat</span><span>::</span><span>try_new_from_triplets</span><span>(</span><span>steps</span><span>, </span><span>steps</span><span>, </span><span>&amp;</span><span>triplets</span><span>)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>    // 2. Construct e_1</span></span>
<span><span>    let</span><span> mut</span><span> e1</span><span> =</span><span> Mat</span><span>::</span><span>zeros</span><span>(</span><span>steps</span><span>, </span><span>1</span><span>);</span></span>
<span><span>    e1</span><span>.</span><span>as_mut</span><span>()[(</span><span>0</span><span>, </span><span>0</span><span>)] </span><span>=</span><span> 1</span><span>.</span><span>0</span><span>;</span></span>
<span></span>
<span><span>    // 3. Solve T_k * y&#39; = e_1 via sparse LU</span></span>
<span><span>    Ok</span><span>(</span><span>t_k_sparse</span><span>.</span><span>as_ref</span><span>()</span><span>.</span><span>sp_lu</span><span>()</span><span>?.</span><span>solve</span><span>(</span><span>e1</span><span>.</span><span>as_ref</span><span>()))</span></span>
<span><span>};</span></span></code></pre></div>
<p>The closure takes the coefficient arrays, constructs the sparse tridiagonal matrix, and solves the system. The triplet format lets us build the matrix efficiently without knowing its structure in advance. The sparse LU solver leverages the tridiagonal structure to avoid dense factorization.</p>

<p>Now that we have a working implementation we can run some tests. The core idea of what we have done is simple: trade flops for better memory access. But does this trade actually pay off on real hardware? To find out, we need a reliable way to benchmark it.</p>
<p>For the data, we know that the performance of any Krylov method is tied to the operator’s spectral properties. We need a way to generate a family of test problems where we can precisely control the size, sparsity, and numerical difficulty. A great way to do this is with Karush-Kuhn-Tucker (KKT) systems, which are sparse, symmetric, and have a specific block structure.</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>D</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mi>E</mi><mi>T</mi></msup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>E</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">A =
\begin{pmatrix}
    D &amp; E^T \\
    E &amp; 0
\end{pmatrix}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>(</span></span><span><span><span><span><span><span><span><span></span><span><span>D</span></span></span><span><span></span><span><span>E</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>E</span><span><span><span><span><span><span></span><span><span>T</span></span></span></span></span></span></span></span></span></span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span>)</span></span></span></span></span></span></span>
<p>This structure gives us two critical knobs to turn. First, with the <a href="https://commalab.di.unipi.it/files/Data/MCF/netgen.tgz">netgen</a> utility, we can control the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span></span></span></span> matrix, which lets us dial in the problem dimension, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span>. Second, we build the diagonal block D with random entries from a range <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>C</mi><mi>D</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1, C_D]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>[</span><span>1</span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>]</span></span></span></span>. This parameter, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">C_D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, gives us direct control over the numerical difficulty of the problem.</p>
<p>For a symmetric matrix like <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>, the 2-norm condition number, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>κ</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\kappa_2(D)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>κ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>D</span><span>)</span></span></span></span>, is the ratio of its largest to its smallest eigenvalue: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>κ</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>λ</mi><mi>max</mi><mo>⁡</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><msub><mi>λ</mi><mi>min</mi><mo>⁡</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\kappa_2(D) = \lambda_{\max}(D) / \lambda_{\min}(D)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>κ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>D</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span>a</span><span>x</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>D</span><span>)</span><span>/</span><span><span>λ</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span>i</span><span>n</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>D</span><span>)</span></span></span></span>. Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span> is diagonal, its eigenvalues are simply its diagonal entries. We are drawing these entries from a uniform distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>C</mi><mi>D</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">U[1, C_D]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>U</span><span>[</span><span>1</span><span>,</span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>]</span></span></span></span>, so we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>max</mi><mo>⁡</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_{\max}(D) \approx C_D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span>a</span><span>x</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>D</span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>min</mi><mo>⁡</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>≈</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lambda_{\min}(D) \approx 1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span><span><span>m</span><span>i</span><span>n</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>D</span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span>1</span></span></span></span>. This means we get direct control, as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>κ</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">\kappa_2(D) \approx C_D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>κ</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>D</span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.The spectral properties of this block heavily influence the spectrum of the entire matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span>. A large condition number in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span> leads to a more ill-conditioned system for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span>. The convergence rate of Krylov methods like Lanczos is fundamentally governed by the distribution of the operator’s eigenvalues. An ill-conditioned matrix, with a wide spread of eigenvalues, will require more iterations, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>, to reach the desired accuracy. By simply adjusting the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">C_D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span><span><span><span><span><span><span></span><span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> parameter, we can generate everything from well-conditioned problems that converge quickly to ill-conditioned ones that force us to run a large number of iterations. This is exactly what we need to rigorously test our implementation.</p>
<h2 id="memory-and-computation-trade-off">Memory and Computation Trade-off<a href="#memory-and-computation-trade-off"><span aria-hidden="true">#</span></a></h2>
<p>We measure the algorithm against two hypotheses on a large sparse problem with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>500</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=500,000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>=</span><span></span></span><span><span></span><span>500</span><span>,</span><span></span><span>000</span></span></span></span>, varying the number of iterations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>.</p>
<p><strong>Hypothesis 1 (Memory):</strong> The one-pass method stores the full basis <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> with complexity <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nk)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>nk</span><span>)</span></span></span></span>. We expect its memory to grow linearly with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>. The two-pass method operates with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span>n</span><span>)</span></span></span></span> memory, so it should have a flat profile.</p>
<p><strong>Hypothesis 2 (Runtime):</strong> The two-pass method performs <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>k</span></span></span></span> matrix-vector products instead of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>. If all else were equal, we’d expect it to run twice as slow.</p>
<h3 id="memory-usage">Memory Usage<a href="#memory-usage"><span aria-hidden="true">#</span></a></h3>
<p><img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs500k_rho3_memory.png" alt="Memory vs Iterations"/></p>
<p>The memory data confirms Hypothesis 1 exactly. The one-pass method’s footprint scales as a straight line—each additional iteration adds one vector to the basis. The two-pass method remains flat. No allocation growth happens after initialization.</p>
<h3 id="runtime-where-theory-breaks">Runtime: Where Theory Breaks<a href="#runtime-where-theory-breaks"><span aria-hidden="true">#</span></a></h3>
<p><img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs500k_rho3_time.png" alt="Runtime vs Iterations"/></p>
<p>The runtime data contradicts Hypothesis 2. The two-pass method is slower, but never by a factor of two. For small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>, the gap is minimal. As <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> grows, the two-pass runtime diverges slowly from the one-pass method, not by doubling, but by a much smaller margin.</p>
<p>This difference comes from memory access patterns. Both methods perform matrix-vector products, but they differ in how they reconstruct the solution.</p>
<p>The one-pass method computes <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> in a single dense matrix-vector product. When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> are large, the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span>k</span></span></span></span> basis matrix exceeds all cache levels. The CPU cannot keep the data resident; instead, it streams <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> from main memory. This is a memory-bandwidth-bound operation. The processor stalls, waiting for each load to complete. Instruction-level parallelism collapses.</p>
<p>The two-pass method reconstructs the solution incrementally. At each iteration, it operates on exactly three n-dimensional vectors: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mtext>prev</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{\text{prev}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span><span>prev</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mtext>curr</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{\text{curr}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span><span><span>curr</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. This working set fits in L1 cache. The processor performs <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>k</span></span></span></span> matrix-vector products (each one reading the sparse operator, then applying it to a cached vector), but the solution accumulation happens entirely within cache. The additional matrix-vector products are cheaper than the memory latency of the standard method.</p>
<p>The cost of re-computing basis vectors is less than the latency cost of scanning an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>×</span><span></span></span><span><span></span><span>k</span></span></span></span> dense matrix from main memory.</p>
<h3 id="medium-scale-behavior">Medium-Scale Behavior<a href="#medium-scale-behavior"><span aria-hidden="true">#</span></a></h3>
<p><img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs50k_rho3_time.png" alt="Medium Scale Runtime vs Iterations"/>
<img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs50k_rho3_memory.png" alt="Medium Scale Memory Usage vs Iterations"/></p>
<p>At <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>50</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=50,000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>=</span><span></span></span><span><span></span><span>50</span><span>,</span><span></span><span>000</span></span></span></span> we can observe an equilibrium. The two methods have nearly identical runtime. The standard method’s <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> matrix is smaller; it fits partially in cache. The cache-miss penalty here becomes manageable. The two-pass method still has the advantage of cache-local accumulation, but the difference is marginal.</p>
<h3 id="what-about-dense-matrices">What About Dense Matrices?<a href="#what-about-dense-matrices"><span aria-hidden="true">#</span></a></h3>
<p>To be sure of our hypothesis, we can test it directly using a dense matrix of size <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>10</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=10,000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>=</span><span></span></span><span><span></span><span>10</span><span>,</span><span></span><span>000</span></span></span></span>. For dense problems, the matrix-vector product is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>O</span><span>(</span><span><span>n</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>)</span></span></span></span>, it dominates all other costs. Memory latency will become negligible relative to the compute work and the cache efficiency advantage should disappear.</p>
<p><img src="https://lukefleed.xyz/assets/lanczos/dense-tradeoff.png" alt="Dense Matrix Runtime vs Iterations"/></p>
<p>We can see that the two-pass method runs almost exactly twice as slow as the one-pass method. The slope ratio is <em>exactly</em> 2:1. In a compute-bound regime, the extra matrix-vector products cannot be hidden by cache effects. Here, the theoretical trade-off holds perfectly.</p>
<h2 id="scalability">Scalability<a href="#scalability"><span aria-hidden="true">#</span></a></h2>
<p>Now, let’s fix the iteration count at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">k=500</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span><span></span><span>=</span><span></span></span><span><span></span><span>500</span></span></span></span> and vary <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">50,000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>50</span><span>,</span><span></span><span>000</span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>500</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">500,000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>500</span><span>,</span><span></span><span>000</span></span></span></span> to measure scalability. Based on what we have seen before, we would expect the two-pass memory to scale linearly with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> but with a small constant factor (three vectors, plus scalars). The one-pass method should also scale linearly, but with a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span>-dependent slope.</p>
<p><img src="https://lukefleed.xyz/assets/lanczos/scalability_k500_rho3_memory.png" alt="Scalability Memory Usage"/></p>
<p>Here we have to use a logarithmic y-axis to show both curves; the two-pass line is so flat relative to the one-pass line that it’s otherwise invisible.</p>
<p><img src="https://lukefleed.xyz/assets/lanczos/scalability_k500_rho3_time.png" alt="Scalability Runtime"/></p>
<p>Runtime scales linearly with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> for both methods, as expected. Below <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>150</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=150,000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span><span></span><span>=</span><span></span></span><span><span></span><span>150</span><span>,</span><span></span><span>000</span></span></span></span>, the two methods have similar performance. This is the regime where both basis and working set fit in cache, or where the problem is small enough that memory latency is not the bottleneck.</p>
<p>As <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>n</span></span></span></span> increases beyond <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>150</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">150,000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>150</span><span>,</span><span></span><span>000</span></span></span></span>, the matrix-vector product time dominates. The sparse structure of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> ensures that each matvec requires multiple memory accesses per element. For the one-pass method, the final reconstruction of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> begins to cost more as the matrix grows. For the two-pass method, performing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>k</span></span></span></span> matrix-vector products means the matvec cost accumulates more rapidly. The divergence is gradual, not sharp, because the advantage of cache locality in accumulation persists—but it cannot overcome the fundamental cost of doubling the number of expensive operations.</p>
<hr/>
<p>Well, that’s it. If you want to have a better look at the code or use it, it’s all open source:</p>
<ul>
<li><a href="https://github.com/lukefleed/two-pass-lanczos">Github Repository</a></li>
<li><a href="https://github.com/lukefleed/two-pass-lanczos/raw/master/tex/report.pdf">LaTeX Report</a></li>
</ul>
<p>This was more of an exploration than a production-ready library, so expect rough edges. But I hope it gives an interesting perspective on how algorithm engineering and low-level implementation details can alter what seems like a straightforward trade-off on a blackboard.</p> </article></div>
  </body>
</html>
