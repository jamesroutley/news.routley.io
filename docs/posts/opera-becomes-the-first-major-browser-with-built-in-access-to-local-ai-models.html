<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://press.opera.com/2024/04/03/ai-feature-drops-local-llms/">Original</a>
    <h1>Opera becomes the first major browser with built-in access to local AI models</h1>
    
    <div id="readability-page-1" class="page"><div id="container">
  <div>
    <section id="content" role="main">
              <article id="post-3273">
<header>
 <p>
        <span>April 3, 2024</span>
</p></header>
<section>
  <img width="2560" height="1440" src="https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/OperaDeveloperLocalLLMs-scaled.jpg" alt="A pop-up directs Opera Developer users to the Settings, where they can select their LLM of choice." decoding="async" fetchpriority="high" srcset="https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/OperaDeveloperLocalLLMs-scaled.jpg 2560w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/OperaDeveloperLocalLLMs-300x169.jpg 300w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/OperaDeveloperLocalLLMs-1024x576.jpg 1024w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/OperaDeveloperLocalLLMs-768x432.jpg 768w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/OperaDeveloperLocalLLMs-1536x864.jpg 1536w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/OperaDeveloperLocalLLMs-2048x1152.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"/>  
<p><strong>Oslo, Norway – April 3rd, 2024</strong> – Browser innovator Opera announced today that it’s adding experimental support for 150 local LLM (Large Language Model) variants from approximately 50 families of models to its Opera One browser in developer stream. This step marks the first time local LLMs can be easily accessed and managed from a major browser through a built-in feature. The local AI models are a complimentary addition to Opera’s online Aria AI service. Among the supported local LLMs are: </p>



<ul>
<li>Llama from Meta</li>



<li>Vicuna</li>



<li>Gemma from Google</li>



<li>Mixtral from Mistral AI</li>



<li>And many families more</li>
</ul>



<figure><img decoding="async" width="1024" height="576" src="https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/Dropdown-with-models-1-1024x576.jpg" alt="Different LLMs available for Opera Developer users." srcset="https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/Dropdown-with-models-1-1024x576.jpg 1024w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/Dropdown-with-models-1-300x169.jpg 300w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/Dropdown-with-models-1-768x432.jpg 768w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/Dropdown-with-models-1-1536x864.jpg 1536w, https://www-static-sites.operacdn.com/wp-content/uploads/sites/5/2024/04/Dropdown-with-models-1-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>Using Local Large Language Models means users’ data is kept locally on their device, allowing them to use generative AI without the need to send information to a server. Opera is testing this new set of local LLMs in the developer stream of Opera One as part of its new <a href="https://press.opera.com/2024/03/08/opera-one-ai-feature-drops/">AI Feature Drops Program</a>, which allows early adopters to test early, often experimental versions of the browser’s AI feature set. </p>



<p>As of today, the Opera One Developer users are getting the opportunity to select the model they want to process their input with. To test the models, they have to upgrade to the newest version of <a href="https://opr.as/dyib">Opera Developer</a> and <a href="https://blogs.opera.com/news/2024/04/ai-feature-drops-local-llms/">follow several steps</a> to activate the new feature. Choosing a local LLM will then download it to their machine. The local LLM, which typically requires 2-10 GB of local storage space per variant, will then be used instead of Aria, Opera’s native browser AI, until a user starts a new chat with the AI or switches Aria back on.</p>



<p>“Introducing Local LLMs in this way allows Opera to start exploring ways of building experiences and knowhow within the fast-emerging local AI space,” said Krystian Kolondra, EVP Browsers and Gaming at Opera.</p>



<figure><img decoding="async" src="https://lh7-eu.googleusercontent.com/WZT7C7ZKmYIuDKSi4NnM69AdtxwLaztQmijY7LPMBPNkBDOfDRCR9OIhyOgiRmVhCUVtpKc2Yw9ISD4r14colo4c8lb2-tecXjuqoxSspyluUCCnC8QiHcTYv09BeMZeYWjFbT84rlecY-evLcEM-7c" alt="Mixtral writes the user a poem."/></figure>



<p><strong>Track record or innovation in the AI space</strong></p>



<p>In early 2023, Opera presented Opera One, its AI-centric flagship browser based on Modular Design principles and a new browser architecture with a multithreaded compositor that allows for a smoother-than-ever processing of UX elements. Opera One features the Aria browser AI, which can be accessed via the browser sidebar of the browser command line. Aria is also available in the gamer-centric Opera GX, as well as in Opera browser on iOS and Android.</p>



<p>To test local LLMs in Opera One Developer, <a href="https://opr.as/dyib">go here</a>.</p>



<p><strong>About Opera</strong></p>
  
</section>
 
</article>
                    
      
    </section>

</div>
</div></div>
  </body>
</html>
