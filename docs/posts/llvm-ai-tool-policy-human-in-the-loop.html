<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://discourse.llvm.org/t/rfc-llvm-ai-tool-policy-human-in-the-loop/89159">Original</a>
    <h1>LLVM AI tool policy: human in the loop</h1>
    
    <div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
      <meta itemprop="headline" content="[RFC] LLVM AI tool policy: human in the loop"/>
      
      <meta itemprop="datePublished" content="2025-12-17T19:09:48Z"/>
        <meta itemprop="articleSection" content="LLVM Project"/>
      <meta itemprop="keywords" content=""/>
      


          <div id="post_1">
            <div>
              


              <p><span>
                  <time datetime="2025-12-17T19:09:48Z">
                    December 17, 2025,  7:09pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-17T19:30:50Z"/>
              <span itemprop="position">1</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>Hey folks, I got a lot of feedback from various meetings on the proposed LLVM AI contribution policy, and I made some significant changes based on that feedback. The current draft proposal focuses on the idea of requiring a <strong>human in the loop</strong> who understands their contribution well enough to <strong>answer questions about it during review</strong>. The idea here is that contributors are not allowed to offload the work of validating LLM tool output to maintainers. I’ve mostly removed the Fedora policy in an effort to move from the vague notion of “owning the contribution” to a more explicit “contributors have to review their contributions and be prepared to answer questions about them”. Contributors should never find themselves in the position of saying “I don’t know, an LLM did it”. I felt the change here was significant, and deserved a new thread.</p>
<p>From an informal show of hands at the round table at the US LLVM developer meeting, most contributors (or at least the subset with the resources and interest in attending this round table in person) are interested in using LLM assistance to increase their productivity, and I really do want to enable them to do so, while also making sure we give maintainers a useful policy tool for pushing back against unwanted contributions.</p>
<p>I’ve updated <a href="https://github.com/llvm/llvm-project/pull/154441">the PR</a>, and I’ve pasted the markdown below as well, but you can also <a href="https://github.com/rnk/llvm-project/blob/tool-policy/llvm/docs/AIToolPolicy.md">view it on GitHub</a>.</p>
<hr/>

<h2><a name="p-355841-policy-2" href="#p-355841-policy-2" aria-label="Heading link"></a>Policy</h2>
<p>LLVM’s policy is that contributors can use whatever tools they would like to</p>
<p>We expect that new contributors will be less confident in their contributions,</p>
<p>Contributors are expected to <strong>be transparent and label contributions that</strong>. Our policy on</p>
<p>An important implication of this policy is that it bans agents that take action</p>
<p>This policy includes, but is not limited to, the following kinds of</p>
<ul>
<li>Code, usually in the form of a pull request</li>
<li>RFCs or design proposals</li>
<li>Issues or security vulnerabilities</li>
<li>Comments and feedback on pull requests</li>
</ul>
<h2><a name="p-355841-extractive-contributions-3" href="#p-355841-extractive-contributions-3" aria-label="Heading link"></a>Extractive Contributions</h2>
<p>The reason for our “human-in-the-loop” contribution policy is that processing</p>
<p>Our <strong>golden rule</strong> is that a contribution should be worth more to the project</p>
<blockquote>
<p>&#34;When attention is being appropriated, producers need to weigh the costs and</p>
</blockquote>
<p>Prior to the advent of LLMs, open source project maintainers would often review</p>
<p>Reviewing changes from new contributors is part of growing the next generation</p>
<h2><a name="p-355841-handling-violations-4" href="#p-355841-handling-violations-4" aria-label="Heading link"></a>Handling Violations</h2>
<p>If a maintainer judges that a contribution is <em>extractive</em> (i.e. it doesn’t</p>
<pre><code>This PR appears to be extractive, and requires additional justification for
why it is valuable enough to the project for us to review it. Please see
our developer policy on AI-generated contributions:
http://llvm.org/docs/AIToolPolicy.html
</code></pre>
<p>Other reviewers should use the label to prioritize their review time.</p>
<p>The best ways to make a change less extractive and more valuable are to reduce</p>
<p>If a contributor responds but doesn’t make their change meaningfully less</p>
<h2><a name="p-355841-copyright-5" href="#p-355841-copyright-5" aria-label="Heading link"></a>Copyright</h2>
<p>Artificial intelligence systems raise many questions around copyright that have</p>
<h2><a name="p-355841-examples-6" href="#p-355841-examples-6" aria-label="Heading link"></a>Examples</h2>
<p>Here are some examples of contributions that demonstrate how to apply</p>
<ul>
<li><a href="https://github.com/llvm/llvm-project/pull/142869">This PR</a> contains a proof from Alive2, which is a strong signal of</li>
<li>This <a href="https://discourse.llvm.org/t/searching-for-gsym-documentation/85185/2">generated documentation</a> was reviewed for correctness by a</li>
</ul>
<h2><a name="p-355841-references-7" href="#p-355841-references-7" aria-label="Heading link"></a>References</h2>
<p>Our policy was informed by experiences in other communities:</p>
<ul>
<li><a href="https://communityblog.fedoraproject.org/council-policy-proposal-policy-on-ai-assisted-contributions/">Fedora Council Policy Proposal: Policy on AI-Assisted Contributions (fetched</a>: Some of the text above was copied from the Fedora</li>
<li><a href="https://github.com/rust-lang/compiler-team/issues/893">Rust draft policy on burdensome PRs</a></li>
<li><a href="https://sethmlarson.dev/slop-security-reports">Seth Larson’s post</a></li>
<li>The METR paper <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">Measuring the Impact of Early-2025 AI on Experienced</a>.</li>
<li><a href="https://www.qemu.org/docs/master/devel/code-provenance.html#use-of-ai-content-generators">QEMU bans use of AI content generators</a></li>
<li><a href="https://simonwillison.net/2024/May/8/slop/">Slop is the new name for unwanted AI-generated content</a></li>
</ul>
            </div>

            

                

            
          </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/cmtice"><span itemprop="name">cmtice</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-17T19:27:38Z">
                    December 17, 2025,  7:27pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-17T19:27:38Z"/>
              <span itemprop="position">2</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Hi Reid,</p>
<p>I understand and generally agree with the sentiment that prompted this proposal, but I think maybe your current policy is <strong>too</strong> restrictive and you are throwing out the baby with the bathwater.  In particular I can imagine cases where we might want to make exceptions to the general policy, e.g. for AI tools that are designed to handle a small, restricted, and easily automatable set of maintenance-type changes.  I think this policy should include a well-defined path for obtaining exceptions to the general rule (that a human must be in the loop before a PR can be posted).</p>
<p>An example of such a path might be:</p>
<ul>
<li>
<p>Post an RFC detailing what problem the proposed AI agent will solve and how it will solve it.</p>
</li>
<li>
<p>Get approval for the RFC</p>
</li>
<li>
<p>Have a short testing period, where humans must check their proposed changes before allowing them to be posted upstream, and must comment in the PR both that the original content came from AI, and whether or not the human needed to update the original content.</p>
</li>
<li>
<p>Final review by small committee (possibly one of the area leads teams) on whether or not the AI is generating acceptable quality PRs; grants the exception (or not).</p>
</li>
</ul>
<p>Note that’s just a rough outline, and would probably need refinement.  Just my 2 cents.</p>
            </div>

            


            
          </div>
          <div id="post_3" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/shafik"><span itemprop="name">shafik</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-17T20:58:14Z">
                    December 17, 2025,  8:58pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-17T20:58:14Z"/>
              <span itemprop="position">3</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Thank you, this feels like it take a lot of the feedback into account. one aspect that I don’t see covered here but I have had trouble with is that results from LLMs tend to be very very verbose. Often they feel like giant walls of text. There is important and useful information there but I have to put a lot more work into getting it. It is not that anything is necessarily incorrect or wrong but a human would have said in a few sentences and still conveyed the important information.</p>
<p>It feels very hard to push back on this because it feels to some degree subjective (but I know it when I see it) but if a serious amounts of reviews became that much more verbose it would be a large cost in reviewer time. I will feel highly unmotivated to review PRs/issues that are walls of text. Especially if (as I often am) making hard trade-offs on my time.</p>
<p>There is some degree on irony here, in that I am often pushing folks on PRs to provide more and more detailed information but as the meme’s often say “not like that”.</p>
            </div>

            


            
          </div>
          <div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/danilaml"><span itemprop="name">danilaml</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-18T15:19:19Z">
                    December 18, 2025,  3:19pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-18T15:19:19Z"/>
              <span itemprop="position">4</span>
              </span>
            </p>
            <p>Not sure if it’s explicit from the current wording but I assume the intention for “human in the loop” is that the human won’t just forward questions to LLM and post its answers as if they are their own instead of just going “I don’t know, an LLM did it” either (because it’s essentially the same as the latter but wastes way more reviewer time).</p>

            


            
          </div>
          <div id="post_5" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/Endill"><span itemprop="name">Endill</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-18T18:29:59Z">
                    December 18, 2025,  6:29pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-18T18:29:59Z"/>
              <span itemprop="position">5</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>I don’t agree with this characterization. The problem we have today is not that we have a lot of LLM-based tools we desperately need to integrate into our automation. Instead, I think the problem is that reviewers struggle with a wave of LLM output coming as contributions, where contributors don’t have enough understanding of their PRs. Policy that Reid drafted in this thread is a great step towards addressing that later problem.</p>
<p>We’ve been months into AI policy discussions, and I don’t think that hypothetical future LLM automation is worth delaying much needed changes any longer.</p>
            </div>

            


            
          </div>
          <div id="post_6" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/Sirraide"><span itemprop="name">Sirraide</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-18T18:43:01Z">
                    December 18, 2025,  6:43pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-18T18:43:01Z"/>
              <span itemprop="position">6</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>Yeah, agreed. If we eventually do find that we want to carve out some sort of exception for some tool, we can just update the policy at that point.</p>
            </div>

            


            
          </div>
          <div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/jrtc27"><span itemprop="name">jrtc27</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-18T19:11:22Z">
                    December 18, 2025,  7:11pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-18T19:11:22Z"/>
              <span itemprop="position">7</span>
              </span>
            </p>
            <p>I don’t see why this is incompatible with that. A policy introduced by this RFC can be overridden by a future RFC, including one for a specific case that would like an exception.</p>

            


            
          </div>
          <div id="post_8" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/cmtice"><span itemprop="name">cmtice</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-19T16:11:10Z">
                    December 19, 2025,  4:11pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-19T16:11:10Z"/>
              <span itemprop="position">8</span>
              </span>
            </p>
            <p>While I am fully convinced that we will end up needing well-defined path for exceptions to this policy, which I why I brought it up, if everyone else would prefer to skip that for now, I can live with the policy going in as is. With the full expectation that we will need to update it in the future to define a principled way to obtain exceptions. <img src="https://emoji.discourse-cdn.com/google/slight_smile.png?v=15" title=":slight_smile:" alt=":slight_smile:" loading="lazy" width="20" height="20"/></p>

            


            
          </div>
          <div id="post_9" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/artagnon"><span itemprop="name">artagnon</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-23T22:01:35Z">
                    December 23, 2025, 10:01pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-23T22:01:35Z"/>
              <span itemprop="position">9</span>
              </span>
            </p>
            <div itemprop="text">
              <p>I thought some more about why crafting an AI policy for an open source project is so hard, and why so many projects are struggling with it. For a large open source project like LLVM or Linux, a large share of the contributors are working at corporations – and a large majority of corporations have adopted AI coding-assistants org-wide, many of them seeing material benefits from the adoption. It would be natural to question whether the same productivity gain can be replicated in an open source project like ours. The interest in drafting a policy may also come from the perspective of being welcoming to new contributors – a lot of young people today are playing with coding-assistants prior to joining industry. My earlier position was that it wouldn’t be useful in LLVM, from personal experience, but I’m probably old-fashioned and biased.</p>
<p>The core issue is that the people writing code in corporations are very different from the general public – rouge entities misusing AI in corporations can be let go, but the general public is the wild west. Several open source projects are burdened with a flood of AI-generated bug reports, and huge AI-generated PRs. I think that, due to the inherent nature of this technology, it will always be a corporate thing, and is perhaps a poor fit for an open source project – I know this is somewhat defeatist, but I really don’t know what kind of safeguards will protect us and our valuable time.</p>
            </div>

            


            
          </div>
          <div id="post_10" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/ms178"><span itemprop="name">ms178</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-26T09:41:30Z">
                    December 26, 2025,  9:41am
                  </time>
                  <meta itemprop="dateModified" content="2025-12-26T09:41:30Z"/>
              <span itemprop="position">10</span>
              </span>
            </p>
            <p>True, but there is a higher argumentative and social effort needed to deviate from a once established policy.</p>

            


            
          </div>
          <div id="post_11" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/ms178"><span itemprop="name">ms178</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-26T09:51:53Z">
                    December 26, 2025,  9:51am
                  </time>
                  <meta itemprop="dateModified" content="2025-12-26T09:51:53Z"/>
              <span itemprop="position">11</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>Without any empirical evidence this remains a unsubstantiated claim. I’d argue that such a LLM-assisted review could be part of the learning curve. It also does not reflect the rapid improvements in AI quality which might mitigate the issues related to bad output quality over time.</p>
<p>Instead of shutting the door for non-programmers with such language, I propose hard objective criteria to act as the AI quality filter, e.g. measurable and reproducable improvements (performance numbers, crash fixes etc.) that need to be explicitly mentioned within the MR/issue.</p>
            </div>

            


            
          </div>
          <div id="post_12" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              
<p>This is a very strange statement - the burden of proof of the value of <code>&lt;X_NEW_THING&gt;</code> is on  <code>&lt;X_NEW_THING&gt;</code> (and its proponents) not everyone else.</p>
            </div>

            


            
          </div>
          <div id="post_13" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              <p>I can share a few practices from ASF communities.</p>
<p>Some ASF projects (e.g., Apache DataFusion and Apache Kvrocks) have already published AI-assisted contribution policies, and they converge with the LLVM proposal: if you use an LLM, you’re still expected to understand the change and be able to explain and iterate on it.</p>
<p>When the author can’t really explain what the LLM produced, a high-quality issue is often a better outcome than a PR. It can avoid wasted review cycles - especially for smaller communities where maintainer bandwidth and review burnout are real constraints. In projects I maintain, I’ve seen more LLM-generated PRs recently, and many stall because the author can’t effectively respond to review feedback beyond acting as an LLM “proxy”, which increases communication cost and slows everything down.</p>
<p>Overall, “human in the loop” feels like the most practical policy today. If LLM capability improves significantly, we can revisit the policy, but based on what I’m seeing, I think we’re not there yet.</p>
<p>References:</p>
<ul>
<li><a href="https://datafusion.apache.org/contributor-guide/index.html#ai-assisted-contributions" rel="noopener nofollow ugc">Apache DataFusion: AI-Assisted contributions</a></li>
<li><a href="https://kvrocks.apache.org/community/contributing#guidelines-for-ai-assisted-contributions" rel="noopener nofollow ugc">Apache Kvrocks: Guidelines for AI-assisted Contributions</a></li>
<li><a href="https://www.apache.org/legal/generative-tooling.html" rel="noopener nofollow ugc">ASF Generative Tooling Guidance</a></li>
</ul>
<p>AI policies from more communities:</p>
<ul>
<li><a href="https://github.com/zulip/zulip/blob/main/CONTRIBUTING.md#ai-use-policy-and-guidelines" rel="noopener nofollow ugc">Zulip: AI use policy and guidelines</a></li>
<li><a href="https://devguide.python.org/getting-started/generative-ai/" rel="noopener nofollow ugc">CPython: Generative AI</a></li>
<li><a href="https://lore.kernel.org/ksummit/20251114183528.1239900-1-dave.hansen@linux.intel.com/" rel="noopener nofollow ugc">Linux Kernel: Kernel Guidelines for Tool Generated Content</a></li>
</ul>
            </div>

            


            
          </div>
          <div id="post_14" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/shafik"><span itemprop="name">shafik</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-27T18:34:14Z">
                    December 27, 2025,  6:34pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-27T18:34:14Z"/>
              <span itemprop="position">14</span>
              </span>
            </p>
            <p>Thank you so much for sharing your communities experience. It is good to know that we are not the only one struggling with these issues and it is also good to know how other communities are attempting to deal with them.</p>

            


            
          </div>
          <div id="post_15" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/ms178"><span itemprop="name">ms178</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-27T22:19:51Z">
                    December 27, 2025, 10:19pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-27T23:06:59Z"/>
              <span itemprop="position">15</span>
              </span>
            </p>
            <div itemprop="text">
              <p>No, it makes total sense: As I read it, this new AI policy introduces restrictions and additional burden on contributors, effectively excluding non-programmers that want to contribute AI-assisted code changes. The burden of proof is on the proponents of such restrictions to show that there is a neccessity for introducing such a change. It’s not the other way around as there is currently no AI policy with such restrictions (e.g. everything that is not specifically forbidden is allowed).</p>
<p>Again, I argue that some “AI slop filter” needs to be based on objective criteria. It is not about the status of the people contributing code or their lack of programming experience but about the (minimum) quality of the code contribution to warrent further reviewer time and effort. In my view, it makes more sense to define such a set of objective criteria for the expected (minimum) code quality which need to be met than to exclude people due to the lack of programming experience.</p>
            </div>

            


            
          </div>
          <div id="post_16" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/resistor"><span itemprop="name">resistor</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-28T00:59:16Z">
                    December 28, 2025, 12:59am
                  </time>
                  <meta itemprop="dateModified" content="2025-12-28T00:59:16Z"/>
              <span itemprop="position">16</span>
              </span>
            </p>
            <div itemprop="text">
              <p>It’s worth noting that the current permissiveness is quite new in the overall history of the project. The reality is that the LLVM community has discouraged automated and/or bulk changes of any form for a very long time, including changes generated by tooling as straightforward as a <code>sed</code> script. When such changes have been allowed in the past, it has been after receiving prior approval from the community. I view AI-generated changes as falling into precisely the same bucket.</p>
<p>I will restate my outlook from the earlier thread: while being inclusive and welcoming is <em>a</em> goal of the LLVM project, it does not supersede <em>the</em> goal of building a suite of compiler &amp; toolchain components that benefit our users. Most of the time these goals are not in conflict, but (again IMO) an overly permissive AI contribution policy betrays the duty of care we have to our users.</p>
            </div>

            


            
          </div>
          <div id="post_17" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <div itemprop="text">
              
<p>At first glance, this sounds reasonable, but I don’t think it works in practice. In theory you can define “objective” standards, but determining whether a PR actually meets those standards still costs maintainer time and attention. And I strongly suspect that cost is often close to what it would take to just do a full review anyway.</p>
<p>Let’s also acknowledge something: in open source we generally <em>don’t</em> judge contributions by a contributor’s résumé or claimed experience. But interpersonal trust and publicly earned merit (see <a href="https://www.apache.org/foundation/how-it-works/#meritocracy" rel="noopener nofollow ugc">ASF’s explanation of Meritocracy</a> and <a href="https://www.apache.org/theapacheway/#what-makes-the-apache-way-so-hard-to-define" rel="noopener nofollow ugc">the Apache Way</a>, for example) are genuinely important in open-source communities. That’s not because people are biased; it’s because, as imperfect as it may be, it’s the lowest-cost way to make decisions at scale. If we discard that dynamic entirely, I’m not sure open source would function as effectively as it does today.</p>
            </div>

            


            
          </div>
          <div id="post_18" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/ms178"><span itemprop="name">ms178</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-28T08:36:53Z">
                    December 28, 2025,  8:36am
                  </time>
                  <meta itemprop="dateModified" content="2025-12-28T08:36:53Z"/>
              <span itemprop="position">18</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>I fully understand the implications of being swamped with AI contributions on the reviewer side (AI only accelerates the problem, a ton of new novice contributors would expose essentially the same scaling problem). But this is an area were open source communities need to evolve and find new ways to 1) stay as open as possible to harness new discoveries and 2) remain functioning effectively at scale.</p>
<p>I’ve lobbied for an more open alternative approach <a href="https://seylaw.blogspot.com/2025/10/the-shepherd-and-flux-capacitor.html" rel="noopener nofollow ugc">publicly</a> that aims to serve both goals (admittedly without much acceptance from the LLVM community so far):</p>
<p><em>”[…] this entire defensive model is value-destructive. It is a system designed with only one success state (accepting a perfect contribution) and one failure state (rejecting an imperfect one). It has no mechanism for a third, more productive outcome: refinement. When a contribution with a verifiable, valuable payload is rejected because its packaging is flawed, the value is not put on hold; it is permanently lost. The performance gain I found is not sitting in a queue waiting for me to learn C++; it is simply gone from the project. The fortress, in its zeal to keep out the “slop,” has also barred the door to the raw ore from which treasure might have been forged.</em></p>
<p><em>This is not a sustainable model in an era where the tools of discovery are becoming democratized at an explosive rate. The fundamental misunderstanding is that ownership is not a ticket that one must purchase before entering the park. Ownership is the outcome of a successful collaborative journey. By demanding it at the very beginning, we are ensuring that for a growing class of potential innovators, that journey never even begins. The fortress may remain pure, but it will also become stagnant, isolated from the very world it is meant to serve. [</em>…]</p>
<p><em>To critique the fortress is not enough. We must offer a blueprint for a better structure: a harbor. A harbor, unlike a fortress, does not have a simple binary function of letting things in or keeping them out. It is an active, intelligent system with channels, docks, workshops, and expert pilots, all designed to guide valuable cargo safely to shore, no matter the state of the vessel that carries it. This is the model we must adopt for open source in a post-AI world. The anxiety over “extractive” contributions is real, but the solution is not a higher wall; it is a smarter intake process. <a href="https://discourse.llvm.org/t/rfc-llvm-ai-tool-policy-start-small-no-slop/88476/28">I propose a concrete, actionable framework for this harbor</a>: the Contribution Triage Pipeline.”</em></p>
            </div>

            


            
          </div>
          <div id="post_19" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/rengolin"><span itemprop="name">rengolin</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-28T15:07:33Z">
                    December 28, 2025,  3:07pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-28T15:07:33Z"/>
              <span itemprop="position">19</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>This is categorically false. LLVM’s review policy is directly and wholeheartedly based on refinement, and the new AI policy does not goes against it.</p>
<p>LLVM’s policy also focus on code quality, which requires the author being able to explain the changes, and defend their choices to the wider community. Here the fail-safe is a human author being able to explain the changes without offloading that role to maintainers and the wider community.</p>
<p>LLMs are, by construction, unable to defend their own choices (without hallucination) with the technical rigour that we need for tools like compilers, kernels and other key infrastructure components of modern societies.</p>

<p>Tools of discovery should be used to find solutions in a large sea of choices, not to accept the output without validation, nor to offload the validation to the wider community.</p>
<p>The <em>“value”</em> created by some automatic tool, as well as humans, may be acceptable to some people, but not all. It may be relevant to a set of problems, but not all. It may be correct with certain assumptions, but not all. Without understanding of those limitations, contributions provide <em>negative</em> value, as they waste the time of more people than their contributions would provide.</p>
<p>Anyone willing to provide justifications and explanations for decisions made in a PR, be it created by humans or machines, are absolutely encouraged to contribute. But if their justifications are not at the level of rigour we require to merge a PR, the code should not be merged, and upon insistence without further merit, maintainers are encouraged to ignore and/or close the PRs. This is true regardless of how the code was produced.</p>

<p>I believe the policy offers exactly that. The “wall” is the same size as it has always been: technical rigour, honest justification, willingness to adapt.</p>
<p>Extractive contributions are not just dangerous due to their net-negative costs on merge, but also their accumulated extraction of whole-system knowledge and reuse of bad practices that just <em>“happen to work”</em>. This is a human action as well, but code review exposes refactory opportunities, which are much more costly than normal code review.</p>
<p>Accepting generated code without human review, and without human justification, increases the difficulty of refactory over time. This is not exclusive to machines, but it is unrestricted on machines contributions.</p>
<p>Until such a day when code generators can have the same technical rigour as humans (for example generate proofs of quality or trigger reasonable refactory actions), we should not allow auto-generated code to merge without human review and justification.</p>
            </div>

            


            
          </div>
          <div id="post_20" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://discourse.llvm.org/u/amara"><span itemprop="name">amara</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-30T05:02:42Z">
                    December 30, 2025,  5:02am
                  </time>
                  <meta itemprop="dateModified" content="2025-12-30T05:02:42Z"/>
              <span itemprop="position">20</span>
              </span>
            </p>
            <div itemprop="text">
              <blockquote>
<p>No, it makes total sense: As I read it, this new AI policy introduces restrictions and additional burden on contributors, effectively excluding non-programmers that want to contribute AI-assisted code changes. The burden of proof is on the proponents of such restrictions to show that there is a neccessity for introducing such a change. It’s not the other way around as there is currently no AI policy with such restrictions (e.g. everything that is not specifically forbidden is allowed).</p>
</blockquote>
<p>I really don’t agree with this. LLVM is, at its core, a compiler framework that’s responsible for the running code on the majority of the world’s computing systems. From embedded devices to servers and everything in between. Like the Linux kernel, quality and stability are IMO the highest priorities here. We don’t need <em>more</em> contributors who aren’t programmers to contribute code. That may suck for those people who want to feel good by having some code accepted into a prestigious project like LLVM for their resumé without learning compilers, but I couldn’t care less about those people. Those people can take their contributions to one of the other million OSS projects, LLVM will be just fine without them.</p>
<p>What we need more of are contributors who’ve built experience, can review code, learn and teach others through discussions. We need people with good <em>taste</em> that comes with years of interaction with LLVM. We need people who can look at a PR that seems to do a correct thing on a local level, but explain that it’s not the right place to be making those changes in the long term.</p>
<p>To sum it all up, I strongly agree with this RFC and we can evolve it as needed in future.</p>
            </div>

            


            
          </div>
    </div></div>
  </body>
</html>
