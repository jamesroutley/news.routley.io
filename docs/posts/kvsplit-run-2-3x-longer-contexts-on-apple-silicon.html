<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/dipampaul17/KVSplit">Original</a>
    <h1>Show HN: KVSplit – Run 2-3x longer contexts on Apple Silicon</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">

<p dir="auto"><strong>Differentiated KV Cache Quantization for Apple Silicon</strong></p>
<p dir="auto"><a href="https://github.com/dipampaul17/KVSplit/stargazers"><img src="https://camo.githubusercontent.com/e0303260b83f266fb26524b0c11e415c6878324211d0c8edeae0fc1c0b3c9b2a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646970616d7061756c31372f4b5653706c69743f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562" alt="GitHub Stars" data-canonical-src="https://img.shields.io/github/stars/dipampaul17/KVSplit?style=for-the-badge&amp;logo=github"/></a>
<a href="https://github.com/dipampaul17/KVSplit/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/0e46334f6de85981ecf0095ccdacded7efca9c4dce3dde71f93adc96988902f5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e7376673f7374796c653d666f722d7468652d6261646765" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge"/></a>
<a href="https://github.com/dipampaul17/KVSplit/blob/main"><img src="https://camo.githubusercontent.com/8b0dd35d753ba4b14face3f93eafee56c2a5d58f05ee9eae4c8d00e55ca22e02/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f506c6174666f726d2d4170706c6525323053696c69636f6e2d626c61636b3f7374796c653d666f722d7468652d6261646765266c6f676f3d6170706c65" alt="Platform" data-canonical-src="https://img.shields.io/badge/Platform-Apple%20Silicon-black?style=for-the-badge&amp;logo=apple"/></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/kv_cache_memory_usage.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/kv_cache_memory_usage.png" alt="KV Cache Memory Usage" width="70%"/></a>
</p></div>

<p dir="auto">Run <strong>larger context windows</strong> and <strong>heavier LLMs</strong> on your Mac by applying different quantization precision to keys vs values in the attention mechanism&#39;s KV cache. KVSplit enables you to:</p>
<ul dir="auto">
<li><strong>Reduce memory usage by up to 72%</strong> with minimal quality loss</li>
<li><strong>Run 2-3x longer contexts</strong> in the same memory budget</li>
<li><strong>Maintain or improve inference speed</strong> compared to FP16</li>
<li><strong>Optimize for Apple Silicon</strong> with full Metal support</li>
</ul>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>VRAM @ 8K tokens</th>
<th>Tokens/sec</th>
<th>Perplexity Change</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (base)</td>
<td>176.00 MB (100%)</td>
<td>54,360</td>
<td>--</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>93.50 MB (47%)</td>
<td>51,503</td>
<td>+0.03%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>71.50 MB (41%)</strong></td>
<td><strong>57,438</strong></td>
<td><strong>+0.86%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>71.50 MB (41%)</td>
<td>58,690</td>
<td>+6.06%</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>49.50 MB (28%)</td>
<td>55,193</td>
<td>+6.15%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h3 tabindex="-1" dir="auto">Memory Savings by Sequence Length</h3><a id="user-content-memory-savings-by-sequence-length" aria-label="Permalink: Memory Savings by Sequence Length" href="#memory-savings-by-sequence-length"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>128 tokens</th>
<th>2048 tokens</th>
<th>4096 tokens</th>
<th>8192 tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (baseline)</td>
<td>5.50 MB</td>
<td>44.00 MB</td>
<td>88.00 MB</td>
<td>176.00 MB</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>2.92 MB</td>
<td>23.38 MB</td>
<td>46.75 MB</td>
<td>93.50 MB</td>
</tr>
<tr>
<td>K8V4 (mixed)</td>
<td>2.23 MB</td>
<td>17.88 MB</td>
<td>35.75 MB</td>
<td>71.50 MB</td>
</tr>
<tr>
<td>K4V8 (mixed)</td>
<td>2.23 MB</td>
<td>17.88 MB</td>
<td>35.75 MB</td>
<td>71.50 MB</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>1.55 MB</td>
<td>12.38 MB</td>
<td>24.75 MB</td>
<td>49.50 MB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<ul dir="auto">
<li>Independent quantization of keys and values in the KV cache</li>
<li>Optimized for Apple Silicon with Metal support</li>
<li>Comprehensive benchmarking suite with perplexity measurement</li>
<li>Memory usage and performance analysis tools</li>
<li>Publication-quality visualization tools</li>
<li>Easy setup and usage</li>
</ul>

<ul dir="auto">
<li>macOS (tested on Apple Silicon)</li>
<li>Homebrew package manager</li>
<li>Xcode Command Line Tools</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">⚡ One-Command Installation</h2><a id="user-content--one-command-installation" aria-label="Permalink: ⚡ One-Command Installation" href="#-one-command-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/dipampaul17/KVSplit.git
cd kvsplit

# Run the installer script
chmod +x scripts/install_kvsplit.sh
./scripts/install_kvsplit.sh"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/dipampaul17/KVSplit.git
<span>cd</span> kvsplit

<span><span>#</span> Run the installer script</span>
chmod +x scripts/install_kvsplit.sh
./scripts/install_kvsplit.sh</pre></div>
<p dir="auto">The installer will:</p>
<ul dir="auto">
<li>Set up the project structure</li>
<li>Clone and build llama.cpp with Metal support</li>
<li>Configure for differentiated KV cache quantization</li>
<li>Download a small test model (optional)</li>
<li>Set up Python environment for visualization</li>
</ul>

<p dir="auto">Want to see the benefits immediately? Run a quick comparison with your model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run quick comparison with different configurations
python scripts/quick_compare.py --model models/your-model.gguf"><pre><span><span>#</span> Run quick comparison with different configurations</span>
python scripts/quick_compare.py --model models/your-model.gguf</pre></div>
<p dir="auto">This will show you a side-by-side comparison of FP16, K8V8, K8V4, K4V8, and K4V4 with memory usage, speed, and quality metrics.</p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/memory_vs_quality.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/memory_vs_quality.png" alt="Memory vs Quality" width="50%"/></a>
</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>VRAM @ 8K tokens</th>
<th>Memory Savings</th>
<th>Quality Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (base)</td>
<td>176.00 MB</td>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>93.50 MB</td>
<td>47%</td>
<td>+0.03%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>71.50 MB</strong></td>
<td><strong>59%</strong></td>
<td><strong>+0.86%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>71.50 MB</td>
<td>59%</td>
<td>+6.06%</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>49.50 MB</td>
<td>72%</td>
<td>+6.15%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">Using KVSplit doesn&#39;t just save memory—it often <strong>improves inference speed</strong> by 5-15%!</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>Tokens/sec (8K ctx)</th>
<th>Speedup vs FP16</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>54,360</td>
<td>—</td>
</tr>
<tr>
<td>K8V8</td>
<td>51,503</td>
<td>-5.3%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>57,438</strong></td>
<td><strong>+5.7%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>58,690</td>
<td>+8.0%</td>
</tr>
<tr>
<td>K4V4</td>
<td>55,193</td>
<td>+1.5%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<div data-snippet-clipboard-copy-content="kvsplit/
├── llama.cpp/      # Optimized llama.cpp build
├── models/         # LLM model files
├── scripts/        # Utility scripts
│   ├── benchmark_kvsplit.py    # Comprehensive benchmark tool
│   ├── install_kvsplit.sh      # One-command installer
│   ├── quick_compare.py        # Quick comparison utility
│   ├── capture_memory.sh       # GIF creation for memory visualization
│   └── visualize_results.py    # Generate publication-quality plots
├── results/        # Benchmark results (CSV/JSON)
├── plots/          # Generated visualizations
└── README.md       # This file"><pre><code>kvsplit/
├── llama.cpp/      # Optimized llama.cpp build
├── models/         # LLM model files
├── scripts/        # Utility scripts
│   ├── benchmark_kvsplit.py    # Comprehensive benchmark tool
│   ├── install_kvsplit.sh      # One-command installer
│   ├── quick_compare.py        # Quick comparison utility
│   ├── capture_memory.sh       # GIF creation for memory visualization
│   └── visualize_results.py    # Generate publication-quality plots
├── results/        # Benchmark results (CSV/JSON)
├── plots/          # Generated visualizations
└── README.md       # This file
</code></pre></div>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/configuration_summary.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/configuration_summary.png" alt="Configuration Summary" width="80%"/></a>
</p>
<p dir="auto">KV cache memory is dominated by storing key and value vectors for each token. Our research has revealed a critical insight: <strong>keys are significantly more sensitive to quantization than values</strong>.</p>

<ul dir="auto">
<li><strong>Asymmetric Impact</strong>: Keys require higher precision than values for maintaining quality</li>
<li><strong>Sweet Spot</strong>: K8V4 (8-bit keys, 4-bit values) provides optimal balance
<ul dir="auto">
<li>Only 0.86% perplexity degradation vs. FP16</li>
<li>59% memory reduction</li>
<li>Faster inference than FP16</li>
</ul>
</li>
<li><strong>Confirmation</strong>: K4V8 configuration shows 7x more quality degradation than K8V4, despite using the same total bits</li>
</ul>
<p dir="auto">This asymmetry allows for more efficient memory usage without compromising model quality, enabling longer context windows and larger models on consumer hardware.</p>

<div dir="auto"><h3 tabindex="-1" dir="auto">Running with Different KV Cache Precisions</h3><a id="user-content-running-with-different-kv-cache-precisions" aria-label="Permalink: Running with Different KV Cache Precisions" href="#running-with-different-kv-cache-precisions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Baseline (FP16)
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &#34;Your prompt&#34; \
  -t 8 --flash-attn

# ⭐ RECOMMENDED: 8-bit keys, 4-bit values (K8V4) 
# Best balance of quality and memory savings
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &#34;Your prompt&#34; \
  -t 8 --flash-attn --kvq 8

# 4-bit keys, 8-bit values (K4V8)
# Shows why key precision matters more than value precision
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &#34;Your prompt&#34; \
  -t 8 --flash-attn --kvq-key 4 --kvq-val 8

# 4-bit keys and values (K4V4)
# Maximum memory savings (72% reduction) with acceptable quality
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &#34;Your prompt&#34; \
  -t 8 --flash-attn --kvq 4"><pre><span><span>#</span> Baseline (FP16)</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>&#34;</span>Your prompt<span>&#34;</span></span> \
  -t 8 --flash-attn

<span><span>#</span> ⭐ RECOMMENDED: 8-bit keys, 4-bit values (K8V4) </span>
<span><span>#</span> Best balance of quality and memory savings</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>&#34;</span>Your prompt<span>&#34;</span></span> \
  -t 8 --flash-attn --kvq 8

<span><span>#</span> 4-bit keys, 8-bit values (K4V8)</span>
<span><span>#</span> Shows why key precision matters more than value precision</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>&#34;</span>Your prompt<span>&#34;</span></span> \
  -t 8 --flash-attn --kvq-key 4 --kvq-val 8

<span><span>#</span> 4-bit keys and values (K4V4)</span>
<span><span>#</span> Maximum memory savings (72% reduction) with acceptable quality</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>&#34;</span>Your prompt<span>&#34;</span></span> \
  -t 8 --flash-attn --kvq 4</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Long Context Example (32K)</h3><a id="user-content-long-context-example-32k" aria-label="Permalink: Long Context Example (32K)" href="#long-context-example-32k"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Run with a 32K context (would require ~1.4GB in FP16, only ~400MB with K8V4)
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf \
  -c 32768 -n 4096 -t 8 --flash-attn --kvq 8 \
  -f your-long-document.txt"><pre><span><span>#</span> Run with a 32K context (would require ~1.4GB in FP16, only ~400MB with K8V4)</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf \
  -c 32768 -n 4096 -t 8 --flash-attn --kvq 8 \
  -f your-long-document.txt</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">🚩 Command-Line Arguments</h3><a id="user-content--command-line-arguments" aria-label="Permalink: 🚩 Command-Line Arguments" href="#-command-line-arguments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Flag</th>
<th>Description</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-t 8</code></td>
<td>Number of threads</td>
<td>8 is optimal for most Apple Silicon chips</td>
</tr>
<tr>
<td><code>--flash-attn</code></td>
<td>Enables optimized attention</td>
<td>Recommended for Apple Silicon</td>
</tr>
<tr>
<td><code>--kvq N</code></td>
<td>Sets both key and value bits to N</td>
<td>Use <code>--kvq 8</code> for K8V4 configuration</td>
</tr>
<tr>
<td><code>--kvq-key N</code></td>
<td>Sets key bits only</td>
<td>Key precision has major quality impact</td>
</tr>
<tr>
<td><code>--kvq-val N</code></td>
<td>Sets value bits only</td>
<td>Value precision has minor quality impact</td>
</tr>
<tr>
<td><code>-c N</code></td>
<td>Context size in tokens</td>
<td>Longer contexts benefit more from KVSplit</td>
</tr>
<tr>
<td><code>-n N</code></td>
<td>Number of tokens to generate</td>
<td>Adjust based on your needs</td>
</tr>
<tr>
<td><code>-f FILE</code></td>
<td>Input file</td>
<td>For processing documents</td>
</tr>
<tr>
<td><code>-m MODEL</code></td>
<td>Model path</td>
<td>Path to your .gguf model file</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h2 tabindex="-1" dir="auto">📏 Advanced Benchmarking</h2><a id="user-content--advanced-benchmarking" aria-label="Permalink: 📏 Advanced Benchmarking" href="#-advanced-benchmarking"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For comprehensive performance analysis, use our full benchmark suite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run the full benchmark suite (all configurations and sequence lengths)
python scripts/benchmark_kvsplit.py

# Run a specific configuration test
python scripts/benchmark_kvsplit.py --config K8V4 --seq-len 4096

# Generate publication-quality visualizations
python scripts/visualize_results.py"><pre><span><span>#</span> Run the full benchmark suite (all configurations and sequence lengths)</span>
python scripts/benchmark_kvsplit.py

<span><span>#</span> Run a specific configuration test</span>
python scripts/benchmark_kvsplit.py --config K8V4 --seq-len 4096

<span><span>#</span> Generate publication-quality visualizations</span>
python scripts/visualize_results.py</pre></div>
<p dir="auto">The benchmarking script provides thorough measurements of:</p>
<ul dir="auto">
<li>📊 <strong>Memory Usage</strong>: VRAM and KV cache specifically</li>
<li>⚡ <strong>Performance</strong>: Tokens per second across different sequence lengths</li>
<li>🎯 <strong>Quality</strong>: Perplexity measurement using llama-perplexity</li>
<li>📈 <strong>Scaling</strong>: How memory usage and performance scale with sequence length</li>
</ul>
<p dir="auto">Results are saved in CSV/JSON formats with automatic summary statistics, and the visualization script generates publication-quality plots showing key insights.</p>

<p dir="auto">MIT</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">🎬 Visual Memory Savings</h2><a id="user-content--visual-memory-savings" aria-label="Permalink: 🎬 Visual Memory Savings" href="#-visual-memory-savings"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can visualize memory savings with our capture tool:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Capture memory reduction in Activity Monitor
./scripts/capture_memory.sh"><pre><span><span>#</span> Capture memory reduction in Activity Monitor</span>
./scripts/capture_memory.sh</pre></div>

<div dir="auto"><h2 tabindex="-1" dir="auto">🍎 Apple Silicon Optimization</h2><a id="user-content--apple-silicon-optimization" aria-label="Permalink: 🍎 Apple Silicon Optimization" href="#-apple-silicon-optimization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>Metal Performance</strong>: Fully optimized for Apple&#39;s Metal framework</li>
<li><strong>Memory Efficiency</strong>: Critical for memory-constrained M1/M2/M3 devices</li>
<li><strong>Activity Monitor</strong>: Use our <code>capture_memory.sh</code> script to visualize real-time memory reductions</li>
<li><strong>Alignment</strong>: 256B page alignment in llama.cpp means actual memory savings might differ slightly from theoretical calculations</li>
</ul>

<ul dir="auto">
<li><strong>Differentiated Precision</strong>: Independent key and value bit precision (K8V4, K4V8, etc)</li>
<li><strong>Apple Silicon Optimization</strong>: Full Metal support for M1/M2/M3 chips</li>
<li><strong>Comprehensive Benchmarking</strong>: Memory, speed, and quality metrics</li>
<li><strong>Publication-Quality Visualization</strong>: Beautiful plots for analysis</li>
<li><strong>Simple User Interface</strong>: One-command install and quick comparison tools</li>
<li><strong>Memory Visualization</strong>: Tools to capture and visualize memory savings</li>
</ul>

<p dir="auto">This project implements ideas from recent research including:</p>
<ul dir="auto">
<li>&#34;More for Keys, Less for Values: Adaptive KV Cache Quantization&#34; (2024)</li>
<li>&#34;Unifying KV Cache Compression for Large Language Models with LeanKV&#34; (2025)</li>
</ul>
<p dir="auto">Additional credits:</p>
<ul dir="auto">
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> - Base implementation</li>
<li><a href="https://huggingface.co/TinyLlama" rel="nofollow">TinyLlama</a> - Test model</li>
</ul>

<p dir="auto">Contributions are welcome! Please open an issue or submit a pull request.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">🧠 Configuration Recommendations</h2><a id="user-content--configuration-recommendations" aria-label="Permalink: 🧠 Configuration Recommendations" href="#-configuration-recommendations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><strong>Best Overall</strong>: 🌟 <strong>K8V4</strong> 🌟 (8-bit keys, 4-bit values)</p>
<ul dir="auto">
<li>59% memory reduction with only 0.86% quality loss</li>
<li>Improved inference speed (+5.7% vs FP16)</li>
<li>Great balance of quality and efficiency</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Absolute Maximum Memory Savings</strong>: K4V4 (4-bit keys and values)</p>
<ul dir="auto">
<li>72% memory reduction with ~6% quality loss</li>
<li>Good for memory-constrained devices</li>
<li>Acceptable for less sensitive applications</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Best for Very Long Contexts</strong>: K8V4 or K4V4</p>
<ul dir="auto">
<li>Memory savings compound with context length</li>
<li>Run 2-3x longer contexts in the same memory budget</li>
</ul>
</li>
</ul>

<ul>
<li> <strong>Adaptive Precision</strong>: Dynamic precision based on token importance</li>
<li> <strong>Layer-Specific Quantization</strong>: Different precision for different model layers</li>
<li> <strong>Model-Specific Optimizations</strong>: Tailored for Mistral, Phi-3, etc.</li>
<li> <strong>Web Demo</strong>: Interactive testing environment</li>
<li> <strong>Mobile Support</strong>: Adapting for iOS and iPadOS</li>
</ul>

<p dir="auto">MIT</p>

<p dir="auto">Contributions are welcome! Please open an issue or submit a pull request.</p>
</article></div></div>
  </body>
</html>
