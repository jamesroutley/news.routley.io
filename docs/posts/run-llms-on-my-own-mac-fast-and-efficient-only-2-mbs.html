<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.secondstate.io/articles/fast-llm-inference/">Original</a>
    <h1>Run LLMs on my own Mac fast and efficient Only 2 MBs</h1>
    
    <div id="readability-page-1" class="page"><article><p><em>The Rust+Wasm stack provides a strong alternative to Python in AI inference.</em></p>
<p>Compared with Python, Rust+Wasm apps could be 1/100 of the size, 100x the speed, and most importantly securely run everywhere at full hardware acceleration without any change to the binary code. <a href="https://blog.stackademic.com/why-did-elon-musk-say-that-rust-is-the-language-of-agi-eb36303ce341" target="_blank">Rust is the language of AGI</a>.</p>
<p>We created a very simple <a href="https://github.com/second-state/WasmEdge-WASINN-examples/tree/master/wasmedge-ggml-llama-interactive" target="_blank">Rust program</a> to run inference on llama2 models at native speed. When compiled to Wasm, the <a href="https://github.com/second-state/WasmEdge-WASINN-examples/blob/master/wasmedge-ggml-llama-interactive/wasmedge-ggml-llama-interactive.wasm" target="_blank">binary application</a> (only 2MB) is completely portable across devices with heterogeneous hardware accelerators. The Wasm runtime (<a href="https://github.com/WasmEdge/WasmEdge" target="_blank">WasmEdge</a>) also provides a safe and secure execution environment for cloud environments. In fact, the <a href="https://wasmedge.org/docs/start/build-and-run/docker_wasm" target="_blank">WasmEdge runtime works seamlessly with container tools</a> to orchestrate and execute the portable application across many different devices.</p>
<div>
<blockquote><p lang="en" dir="ltr">Fast &amp; *portable* llama2 inference on the edge using <a href="https://twitter.com/hashtag/Wasm?src=hash&amp;ref_src=twsrc%5Etfw">#Wasm</a>! The cross-platform executable is only 2MB, has zero Python dep, and utilizes local hardware acceleration (thanks to <a href="https://twitter.com/ggerganov?ref_src=twsrc%5Etfw">@ggerganov</a>â€™s llama.cpp!). Runs on <a href="https://twitter.com/realwasmedge?ref_src=twsrc%5Etfw">@realwasmedge</a> â€” smooth as ðŸ§ˆ</p>â€” Michael Yuan (@juntao) <a href="https://twitter.com/juntao/status/1705588244602114303?ref_src=twsrc%5Etfw">September 23, 2023</a></blockquote> 
</div>
Chatting with llama2 models on my MacBook
<p>This work is based on the <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp project</a> created by <a href="https://github.com/ggerganov" target="_blank">Georgi Gerganov</a>. We adopted the original C++ program to run on Wasm. It works with the <a href="https://github.com/philpax/ggml/blob/gguf-spec/docs/gguf.md" target="_blank">GGUF formatted</a> model files.</p>
<p><strong>Step 1. Install WasmEdge with the GGML plugin</strong></p>
<p>Use the following command on Linux or Mac devices to install everything. <a href="https://github.com/second-state/WasmEdge-WASINN-examples/blob/master/wasmedge-ggml-llama-interactive/README.md#requirement" target="_blank">See here for more details.</a></p>
<pre><code>curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml
</code></pre><p><strong>Step 2. Download a pre-built Wasm app and the model</strong></p>
<pre><code>curl -LO https://github.com/second-state/llama-utils/raw/main/chat/llama-chat.wasm
</code></pre><p>You should also download a llama2 model in GGUF format. The example below downloads the llama2 7B chat-tuned model quantized to 5-bit weights (<a href="https://www.secondstate.io/articles/convert-pytorch-to-gguf/" target="_blank">see more here</a>).</p>
<pre><code>curl -LO https://huggingface.co/wasmedge/llama2/resolve/main/llama-2-7b-chat-q5_k_m.gguf
</code></pre><p><strong>Step 3. Run it!</strong></p>
<p>Use WasmEdge to run the wasm inference application, and pass the GGUF model to it. You can now enter a question to chat with the model.</p>
<pre><code>wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf llama-chat.wasm

[USER]:
</code></pre><p>Here is a complete example of a conversation.</p>
<pre><code>wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf llama-chat.wasm

[USER]:
Who is the &#34;father of the atomic bomb&#34;?
[ASSISTANT]:
The &#34;father of the atomic bomb&#34; is a term commonly associated with physicist J. Robert Oppenheimer. Oppenheimer was a leading figure in the development of the atomic bomb during World War II, serving as the director of the Manhattan Project, which was responsible for the development and deployment of the first nuclear weapons. He is often referred to as the &#34;father of the atomic bomb&#34; due to his significant contributions to the field of nuclear physics and his leadership in the development of the bomb.
[USER]:
Was he a communist?
[ASSISTANT]:
J. Robert Oppenheimer was not a communist. While he was associated with the Manhattan Project, which was a secret government project, and was involved in the development of the atomic bomb, he was not a member of the Communist Party or had any known political affiliations with communism. Oppenheimer was a physicist and a scientist, and his primary focus was on the development of nuclear weapons during World War II.
</code></pre><p><strong>Configure the model behavior</strong></p>
<p>There are command line options you can use to configure how to interact with the model.</p>
<pre><code>Options:
 -m, --model-alias &lt;ALIAS&gt;
         Model alias [default: default]
 -c, --ctx-size &lt;CTX_SIZE&gt;
         Size of the prompt context [default: 4096]
 -n, --n-predict &lt;N_PRDICT&gt;
         Number of tokens to predict [default: 1024]
 -g, --n-gpu-layers &lt;N_GPU_LAYERS&gt;
         Number of layers to run on the GPU [default: 100]
 -b, --batch-size &lt;BATCH_SIZE&gt;
         Batch size for prompt processing [default: 4096]
 -r, --reverse-prompt &lt;REVERSE_PROMPT&gt;
         Halt generation at PROMPT, return control.
 -s, --system-prompt &lt;SYSTEM_PROMPT&gt;
         System prompt message string [default: &#34;[Default system message for the prompt template]&#34;]
 -p, --prompt-template &lt;TEMPLATE&gt;
         Prompt template. [default: llama-2-chat] [possible values: llama-2-chat, codellama-instruct, mistral-instruct-v0.1, mistrallite, openchat, belle-llama-2-chat, vicuna-chat, chatml]
     --log-prompts
         Print prompt strings to stdout
     --log-stat
         Print statistics to stdout
     --log-all
         Print all log information to stdout
     --stream-stdout
         Print the output to stdout in the streaming way
 -h, --help
         Print help
</code></pre><p>For example, the following command specifies a context length of 2048 tokens and the max number of tokens in each response to 512. It also tells WasmEdge to print out statistics and to stream the model response back to <code>stdout</code> one token at a time. The program generates about 25 tokens per second on a low-end M2 macbook.</p>
<pre><code>wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf \
   llama-chat.wasm -c 2048 -n 512 --log-stat --stream-stdout

[USER]:
Who is the &#34;father of the atomic bomb&#34;?

---------------- [LOG: STATISTICS] -----------------

llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: kv self size  = 1024.00 MB
llama_new_context_with_model: compute buffer total size = 630.14 MB
llama_new_context_with_model: max tensor size =   102.54 MB
[2023-11-10 17:52:12.768] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |
 The &#34;father of the atomic bomb&#34; is a term commonly associated with physicist J. Robert Oppenheimer. Oppenheimer was the director of the Manhattan Project, the secret research and development project that produced the atomic bomb during World War II. He is widely recognized as the leading figure in the development of the atomic bomb and is often referred to as the &#34;father of the atomic bomb.&#34;
llama_print_timings:        load time =   15643.70 ms
llama_print_timings:      sample time =       2.60 ms /    83 runs   (    0.03 ms per token, 31886.29 tokens per second)
llama_print_timings: prompt eval time =    7836.72 ms /    54 tokens (  145.12 ms per token,     6.89 tokens per second)
llama_print_timings:        eval time =    3198.24 ms /    82 runs   (   39.00 ms per token,    25.64 tokens per second)
llama_print_timings:       total time =   18852.93 ms

----------------------------------------------------
</code></pre><p>The next example shows it running on an Nvidia A10G machine at 50 tokens per second.</p>
<pre><code>wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf \
   llama-chat.wasm -c 2048 -n 512 --log-stat

[USER]:
Who is the &#34;father of the atomic bomb&#34;?

---------------- [LOG: STATISTICS] -----------------
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: mem required  =   86.04 MB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 35/35 layers to GPU
llm_load_tensors: VRAM used: 4474.93 MB
..................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: offloading v cache to GPU
llama_kv_cache_init: offloading k cache to GPU
llama_kv_cache_init: VRAM kv self = 1024.00 MB
llama_new_context_with_model: kv self size  = 1024.00 MB
llama_new_context_with_model: compute buffer total size = 630.14 MB
llama_new_context_with_model: VRAM scratch buffer: 624.02 MB
llama_new_context_with_model: total VRAM used: 6122.95 MB (model: 4474.93 MB, context: 1648.02 MB)
[2023-11-11 00:02:22.402] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |

llama_print_timings:        load time =    2601.44 ms
llama_print_timings:      sample time =       2.63 ms /    84 runs   (    0.03 ms per token, 31987.81 tokens per second)
llama_print_timings: prompt eval time =     203.90 ms /    54 tokens (    3.78 ms per token,   264.84 tokens per second)
llama_print_timings:        eval time =    1641.84 ms /    83 runs   (   19.78 ms per token,    50.55 tokens per second)
llama_print_timings:       total time =    4254.95 ms

----------------------------------------------------

[ASSISTANT]:
The &#34;father of the atomic bomb&#34; is a term commonly associated with physicist J. Robert Oppenheimer. Oppenheimer was the director of the Manhattan Project, the secret research and development project that produced the first atomic bomb during World War II. He is widely recognized as the leading figure in the development of the atomic bomb and is often referred to as the &#34;father of the atomic bomb.&#34;
</code></pre><p><strong>LLM agents and apps</strong></p>
<p>We have also created an OpenAI-compatible API server using Rust and WasmEdge. It allows you use any OpenAI-compatible developer tools, such as <a href="https://flows.network/" target="_blank">flows.network</a>, to create LLM agents and apps. <a href="https://www.secondstate.io/articles/wasm-runtime-agi/#build-a-super-lightweight-ai-agent" target="_blank">Learn more here.</a></p>
<p><img src="https://www.secondstate.io/articles/llm-inference.jpeg" alt=""/>
Llama on the edge. Image generated by Midjourney.</p>

<p>LLMs like llama2 are typically trained in Python (e.g. PyTorch, Tensorflow, and JAX). But to use Python for inference applications, which is about 95% of the computing in AI, would be a bad mistake.</p>
<ul>
<li><a href="https://x.com/santiviquez/status/1676677829751177219" target="_blank">Python packages have complex dependencies</a>. They are difficult to set up and use.</li>
<li>Python dependencies are huge. A Docker image for Python or PyTorch is typically <a href="https://hub.docker.com/r/pytorch/pytorch/tags" target="_blank">several GBs</a> or even <a href="https://github.com/pytorch/serve/issues/1420" target="_blank">tens of GBs</a>. That is especially problematic for AI inference on edge servers or on devices.</li>
<li>Python is a very slow language. <a href="https://www.modular.com/blog/how-mojo-gets-a-35-000x-speedup-over-python-part-1" target="_blank">Up to 35,000x slower</a> than compiled languages such as C, C++, and Rust.</li>
<li>Because Python is slow, most of the actual workloads must be <a href="https://x.com/gdb/status/1676726449934331904" target="_blank">delegated to native shared libraries</a> beneath the Python wrapper. That makes Python inference apps <a href="https://podcasts.apple.com/ph/podcast/expanding-ai-chip-capabilities-beyond-nvidia-with/id315114957?i=1000627798935" target="_blank">great for demos, but very hard to modify</a> under the hood for business-specific needs.</li>
<li>The heavy dependency on native libraries, combined with complex dependency management, makes it very hard to port Python AI programs across devices while taking advantage of the deviceâ€™s unique hardware features.</li>
</ul>
<div>
<blockquote><p lang="en" dir="ltr">llama-cpp-python requires pydantic 2.0.1, explicitly won&#39;t work with &lt;=2.0</p>â€” Eric Hartford (@erhartford) <a href="https://twitter.com/erhartford/status/1703105039575687506?ref_src=twsrc%5Etfw">September 16, 2023</a></blockquote> 
</div>
<p>Commonly used Python packages in LLM toolchain are directly conflicting with each other.</p>
<p><a href="https://en.wikipedia.org/wiki/Chris_Lattner" target="_blank">Chris Lattner</a>, of the LLVM, Tensorflow, and Swift language fame, gave <a href="https://www.youtube.com/watch?v=ap0VLOPyGqM" target="_blank">a great interview</a> on the This Week in Startup podcast. He discussed why Python is great for model training but the wrong choice for inference applications.</p>

<p>The Rust+Wasm stack provides a unified cloud computing infra that spans devices to edge cloud, on-prem servers, and the public cloud. It is a strong alternative to the Python stack for AI inference applications. No wonder Elon Musk said that Rust is the language of AGI.</p>
<ul>
<li><strong>Ultra lightweight.</strong> The inference application is just 2MB with all dependencies. It is less than 1% of the size of a typical PyTorch container.</li>
<li><strong>Very fast.</strong> Native C/Rust speed in all parts of the inference application: pre-processing, tensor computation, and post-processing.</li>
<li><strong>Portable.</strong> The same Wasm bytecode application can run on all major computing platforms with support for heterogeneous hardware acceleration.</li>
<li><strong>Easy to set up, develop and deploy.</strong> There are no more complex dependencies. Build a single Wasm file using standard tools on your laptop and deploy it everywhere!</li>
<li><strong>Safe and cloud-ready.</strong> The Wasm runtime is designed to isolate untrusted user code. The Wasm runtime can be managed by container tools and easily deployed on cloud-native platforms.</li>
</ul>

<p>Our demo inference program is written in Rust and compiled into Wasm. The core <a href="https://github.com/second-state/llama-utils/blob/main/simple/src/main.rs" target="_blank">Rust source code</a> is very simple. It is only 40 lines of code. The Rust program manages the user input, tracks the conversation history, transforms the text into the llama2â€™s chat template, and runs the inference operations using the <a href="https://github.com/WebAssembly/wasi-nn" target="_blank">WASI NN API</a>.</p>
<pre><code>fn main() {
   let args: Vec&lt;String&gt; = env::args().collect();
   let model_name: &amp;str = &amp;args[1];

   let graph =
       wasi_nn::GraphBuilder::new(wasi_nn::GraphEncoding::Ggml, wasi_nn::ExecutionTarget::AUTO)
           .build_from_cache(model_name)
           .unwrap();
   let mut context = graph.init_execution_context().unwrap();

   let system_prompt = String::from(&#34;&lt;&lt;SYS&gt;&gt;You are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe. &lt;&lt;/SYS&gt;&gt;&#34;);
   let mut saved_prompt = String::new();

   loop {
       println!(&#34;Question:&#34;);
       let input = read_input();
       if saved_prompt == &#34;&#34; {
           saved_prompt = format!(&#34;[INST] {} {} [/INST]&#34;, system_prompt, input.trim());
       } else {
           saved_prompt = format!(&#34;{} [INST] {} [/INST]&#34;, saved_prompt, input.trim());
       }

       // Set prompt to the input tensor.
       let tensor_data = saved_prompt.as_bytes().to_vec();
       context
           .set_input(0, wasi_nn::TensorType::U8, &amp;[1], &amp;tensor_data)
           .unwrap();

       // Execute the inference.
       context.compute().unwrap();

       // Retrieve the output.
       let mut output_buffer = vec![0u8; 1000];
       let output_size = context.get_output(0, &amp;mut output_buffer).unwrap();
       let output = String::from_utf8_lossy(&amp;output_buffer[..output_size]).to_string();
       println!(&#34;Answer:\n{}&#34;, output.trim());

       saved_prompt = format!(&#34;{} {} &#34;, saved_prompt, output.trim());
   }
}
</code></pre><p>To build the application yourself, just install the Rust compiler and its <code>wasm32-wasi</code> compiler target.</p>
<pre><code>curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup target add wasm32-wasi
</code></pre><p>Then, check out the source project, and run the <code>cargo</code> command to build the Wasm file from the Rust source project.</p>
<pre><code># Clone the source project
git clone https://github.com/second-state/llama-utils
cd llama-utils/chat/

# Build
cargo build --target wasm32-wasi --release

# The result wasm file
cp target/wasm32-wasi/release/llama-chat.wasm .
</code></pre>
<p>Once you have the Wasm bytecode file, you can deploy it on any device that supports the WasmEdge runtime. You just need to <a href="https://github.com/second-state/WasmEdge-WASINN-examples/tree/master/wasmedge-ggml-llama-interactive#requirement" target="_blank">install the WasmEdge with the GGML plugin</a>. We currently have GGML plugins for generic Linux and Ubuntu Linux â€” both on x86 and ARM CPUs and Nvidia GPUs, as well as Apple M1/M2/M3.</p>
<p>Based on <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a>, the WasmEdge GGML plugin will automatically take advantage of any hardware acceleration on the device to run your llama2 models. For example, if your device has Nvidia GPU, the installer will automatically install a CUDA-optimized version of the GGML plugin. For Mac devices, the Mac OS build of the GGML plugin uses the Metal API to run the inference workload on M1/M2/M3â€™s built-in neural processing engines. The Linux CPU build of the GGML plugin uses the OpenBLAS library to auto-detect and utilize the advanced computational features, such as AVX and SIMD, on modern CPUs.</p>
<p>Thatâ€™s how we achieve portability across heterogeneous AI hardware and platforms without sacrificing performance.</p>

<p>While the WasmEdge GGML tooling is usable (and indeed used by our cloud-native customers) today, it is still in its early stages. If you are interested in contributing to the open source projects and shaping the direction of future LLM inference infrastructure, here are some low-hanging fruits that you can <a href="https://wasmedge.org/docs/contribute/overview" target="_blank">potentially contribute to</a>!</p>
<ul>
<li>Add GGML plugins for more hardware and OS platforms. We are also interested in TPUs, ARM NPUs, and other specialized AI chips on Linux and Windows.</li>
<li>Support more <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a> configurations. We currently support passing some config options from Wasm to the GGML plugin. But we would like to support all the options GGML provides!</li>
<li>Support WASI NN APIs in other Wasm-compatible languages. We are specifically interested in Go, Zig, Kotlin, JavaScript, C and C++.</li>
</ul>

<p>As a lightweight, fast, portable, and secure Python alternative, WasmEdge and WASI NN are capable of building inference applications around popular AI models beyond LLMs. For example,</p>
<ul>
<li>The <a href="https://github.com/WasmEdge/mediapipe-rs" target="_blank">mediapipe-rs</a> project provides Rust+Wasm APIs for Googleâ€™s <a href="https://developers.google.com/mediapipe" target="_blank">mediapipe</a> suite of Tensorflow models.</li>
<li>The <a href="https://github.com/WasmEdge/WasmEdge/issues/2768" target="_blank">WasmEdge YOLO</a> project provides Rust+Wasm APIs to work with <a href="https://ultralytics.com/yolov8" target="_blank">YOLOv8</a> PyTorch models.</li>
<li>The <a href="https://github.com/second-state/WasmEdge-WASINN-examples/tree/master/openvino-road-segmentation-adas" target="_blank">WasmEdge ADAS demo</a> shows how to perform road segmentation in self-driving cars using an Intel OpenVINO model.</li>
<li>The <a href="https://github.com/WasmEdge/WasmEdge/issues/2356" target="_blank">WasmEdge Document AI</a> project will provide Rust+Wasm APIs for a suite of popular OCR and document processing models.</li>
</ul>
<p>Lightweight AI inference on the edge has just started!</p>
<p>Join the conversation and contribute to the <a href="https://discord.com/invite/U4B5sFTkFc" target="_blank">WasmEdge discord</a>. Discuss, learn, and share your insights.</p>
</article></div>
  </body>
</html>
