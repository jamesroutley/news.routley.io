<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sidsite.com/posts/bert-from-scratch/">Original</a>
    <h1>Notes on training BERT from scratch on an 8GB consumer GPU</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>I trained a BERT model (<a href="https://arxiv.org/abs/1810.04805">Devlin et al, 2019</a>) from scratch on my desktop PC (which has a Nvidia 3060 Ti 8GB GPU). The model architecture, tokenizer, and trainer all came from <a href="https://huggingface.co/">Hugging Face</a> libraries, and my contribution was mainly setting up the <a href="https://github.com/sradc/pretraining-BERT/tree/main">code</a>, setting up the <a href="https://huggingface.co/datasets/sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen">data</a> (~20GB uncompressed text), and leaving my computer running. (And making sure it was working correctly, with good GPU utilization.)</p>

<ul>
  <li>The code is available as a Jupyter notebook, <a href="https://github.com/sradc/pretraining-BERT/blob/main/pretraining_BERT.ipynb">here</a>.</li>
  <li>The data is available as a Hugging Face dataset, <a href="https://huggingface.co/datasets/sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen">here</a>.</li>
</ul>

<p>The training of large language models is generally associated with GPU or TPU clusters, rather than desktop PCs, and the following plot illustrates the difference between the compute resources I used to train this model, and the resources used to train the original BERT-base model.</p>

<p>
    <img src="https://sidsite.com/assets/posts/bert-from-scratch/bert_vs_this_model.png" alt="Plot comparing compute resources and model performance on GLUE-dev."/>
</p>

<p>Although both BERT-base and this model were trained for the same amount of time, BERT-base saw ~30x more tokens of text, (BERT-base saw ~40 epochs of its training data, while this model saw just a single epoch of its training data).</p>

<p>The <a href="https://gluebenchmark.com/">GLUE</a> <strong>dev-set</strong> score is shown in the plot above, to give an idea of how well the model performs at natural language tasks. 
Fine-tuning on GLUE took ~12 hours in total (on top of the 4 days / ~100 hours of pretraining). 
The following table shows the GLUE-dev results in more detail:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>MNLI (m/mm)</th>
      <th>SST-2</th>
      <th>STSB</th>
      <th>RTE</th>
      <th>QNLI</th>
      <th>QQP</th>
      <th>MRPC</th>
      <th>CoLA</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>This model</td>
      <td>79.3/80.1</td>
      <td>89.1</td>
      <td>61.9</td>
      <td>55.9</td>
      <td>86.3</td>
      <td>86.4</td>
      <td>74.8</td>
      <td>41.0</td>
      <td>72.7</td>
    </tr>
    <tr>
      <td>BERT-Base*</td>
      <td>83.2/83.4</td>
      <td>91.9</td>
      <td>86.7</td>
      <td>59.2</td>
      <td>90.6</td>
      <td>87.7</td>
      <td>89.3</td>
      <td>56.5</td>
      <td>80.9</td>
    </tr>
  </tbody>
</table>

<p>*BERT-Base refers to a fully trained BERT model, the results are taken from Cramming (<a href="https://arxiv.org/abs/2212.14034">Geiping et al, 2022</a>).</p>

<p>While we can see that BERT-Base performed better at every task; the results for “this model” would have been very good (possibly SOTA for a few tasks) in early 2018.</p>

<p>No hyperparameter tuning was carried out.
No special techniques were used to improve the training.
Optimizer and learning rate schedule were guided by Cramming (<a href="https://arxiv.org/abs/2212.14034">Geiping et al, 2022</a>),
but the model architecture changes and other suggestions in Cramming were not used.
I did a couple of smaller training runs first (~1-12 hours).</p>

<p>I was able to monitor training remotely, using <a href="https://wandb.ai/site">Weights &amp; Biases</a>.</p>

<p>This endeavor was inspired by Cramming (<a href="https://arxiv.org/abs/2212.14034">Geiping et al, 2022</a>),
a paper on how to train well-performing BERT models, on modest compute resources (in only 24 hours).</p>

<h3 id="plots-from-the-100-hours-training-run">Plots from the 100 hours training run</h3>

<figure>
    <img src="https://sidsite.com/assets/posts/bert-from-scratch/loss.png" alt="The pre-training loss."/>
    <figcaption>The pre-training loss.</figcaption>
</figure>


<figure>
    <img src="https://sidsite.com/assets/posts/bert-from-scratch/learning_rate.png" alt="The learning rate schedule, recommended by Cramming ([Geiping et al, 2022](https://arxiv.org/abs/2212.14034))."/>
    <figcaption>The learning rate schedule, recommended by Cramming (Geiping et al, 2022).</figcaption>
</figure>


<figure>
    <img src="https://sidsite.com/assets/posts/bert-from-scratch/gpu_util.png" alt="GPU utilization was around 98%."/>
    <figcaption>GPU utilization was around 98%.</figcaption>
</figure>


<figure>
    <img src="https://sidsite.com/assets/posts/bert-from-scratch/gpu_memory.png" alt="GPU memory usage was around 98%, this was achieved by adjusting the batch size."/>
    <figcaption>GPU memory usage was around 98%, this was achieved by adjusting the batch size.</figcaption>
</figure>


<figure>
    <img src="https://sidsite.com/assets/posts/bert-from-scratch/gpu_temp.png" alt="GPU temperature stayed between 76 - 80 degrees celsius, with a higher temperature on hotter days."/>
    <figcaption>GPU temperature stayed between 76 - 80 degrees celsius, with a higher temperature on hotter days.</figcaption>
</figure>


<h3 id="references">References:</h3>
<ul>
  <li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019. URL <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.</li>
  <li>Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], December 2017. URL <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.</li>
  <li>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI, <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></li>
</ul>

  </div></div>
  </body>
</html>
