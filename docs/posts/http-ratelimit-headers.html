<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dotat.at/@/2026-01-13-http-ratelimit.html">Original</a>
    <h1>HTTP RateLimit Headers</h1>
    
    <div id="readability-page-1" class="page"><article>
  <p>There is an IETF draft that aims to standardize <a href="https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/"><code>RateLimit</code> header
fields for HTTP</a>. A <code>RateLimit</code> header in a successful response
can inform a client when it might expect to be throttled, so it can
avoid <a href="https://www.rfc-editor.org/rfc/rfc6585">429 Too Many Requests</a> errors. Servers can also
include <code>RateLimit</code> headers in a 429 response to make the error more
informative.</p>
<p>The draft is in reasonably good shape. However as written it seems to
require (or at least it assumes) that the server uses bad quota-reset
rate limit algorithms. Quota-reset algorithms encourage clients into
cyclic burst-pause behaviour; the draft has several paragraphs
discussing this problem.</p>
<p>However, if we consider that <code>RateLimit</code> headers are supposed to tell
the client what acceptable behaviour looks like, they can be used with
any rate limit algorithm. (And it isn’t too hard to rephrase the draft
so that it is written in terms of client behaviour instead of server
behaviour.)</p>
<p>When a client has more work to do than will fit in a single window’s
quota, linear rate limit algorithms such as GCRA encourage the client
to smooth out its requests nicely. In this article I’ll describe how a
server can use a linear rate limit algorithm with HTTP <code>RateLimit</code>
headers.</p>
<ul>
<li><a href="#spec-summary">spec summary</a></li>
<li><a href="#policy-parameters">policy parameters</a></li>
<li><a href="#linear-rate-limit-algorithm">linear rate limit algorithm</a></li>
<li><a href="#other-rate-limiters">other rate limiters</a></li>
</ul>
<h2><a name="spec-summary" href="#spec-summary">spec summary</a></h2>
<p>The <a href="https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/">draft</a> specifies two headers:</p>
<ul>
<li>
<p><code>RateLimit-Policy:</code> describes input parameters to a rate limit
algorithm, which the server chooses based on the request in some
unspecified way. The policies are expected to be largely static
for a particular client. The parameters are,</p>
<ul>
<li>the name of the policy</li>
<li><code>pk</code>, the partition key</li>
<li><code>q</code>, the quota</li>
<li><code>w</code>, the window</li>
<li><code>qu</code>, the quota units</li>
</ul>
</li>
<li>
<p><code>RateLimit:</code> describes which policies the server applied to this
request, and the output results of the rate limit algorithm. The
results are likely to vary per request depending on client
behaviour or server load, etc. The results are,</p>
<ul>
<li>the name of the policy</li>
<li><code>pk</code>, the partition key</li>
<li><code>r</code>, the available quota</li>
<li><code>t</code>, the effective window</li>
</ul>
</li>
</ul>
<p>Both headers can list multiple named policies.</p>
<p>To obey a policy, the client should not make more than <code>q</code> requests
within <code>w</code> seconds. When it receives a response containing a
<code>RateLimit</code> header, the client should not make more than <code>r</code> requests
in the following <code>t</code> seconds.</p>
<p>(The draft calls <code>r</code> and <code>t</code> the “remaining quota” and “reset time”,
which assumes too much about the rate limit algorithm. I prefer to
describe them as the quota and window that are currently in effect.)</p>
<h2><a name="policy-parameters" href="#policy-parameters">policy parameters</a></h2>
<p>The client’s maximum sustained <em>rate</em> is</p>
<pre><code>    rate = quota / window
</code></pre>
<p>On average the <em>interval</em> between requests at the maximum rate is</p>
<pre><code>    interval = 1 / rate
             = window / quota
</code></pre>
<p>The draft establishes a standard collection of possible <em>quota units</em>,
which determine how the <em>cost</em> of a request is counted.</p>
<ul>
<li>
<p><code>qu=&#34;requests&#34;</code></p>
<pre><code>cost = 1
</code></pre>
</li>
<li>
<p><code>qu=&#34;content-bytes&#34;</code></p>
<pre><code>cost = size of content
</code></pre>
</li>
<li>
<p><code>qu=&#34;concurrent-requests&#34;</code></p>
<p>(not applicable to this article)</p>
</li>
</ul>
<p>The <em>partition key</em> <code>pk</code> identifies where the server keeps the
per-client rate limit state. Typically a server will have a relatively
small fixed set of rate limit policies, and a large dynamic collection
of rate limit states.</p>
<h2><a name="linear-rate-limit-algorithm" href="#linear-rate-limit-algorithm">linear rate limit algorithm</a></h2>
<p>This linear rate limit algorithm is a variant of <a href="https://en.wikipedia.org/wiki/Generic_cell_rate_algorithm">GCRA</a> adapted to
use HTTP <code>RateLimit</code> header terminology.</p>
<p>The per-client rate limit state is just a “not-before” timestamp.
Requests are allowed if they occur after that point in time. The
not-before timestamp is normally in the recent past, and requests
increase it towards and possibly (when the client is over its limit)
beyond the present time.</p>
<p>The output results contain <code>r</code> and <code>t</code> values for the <code>RateLimit</code>
header. These must be whole numbers, though internally the algorithm
needs subsecond precision because there can be many requests per
second.</p>
<ul>
<li>
<p>Get the client’s rate limit state. New clients are given a default
not-before time in the distant past.</p>
<pre><code>time = state[pk] or 0
</code></pre>
</li>
<li>
<p>Ensure the not-before time is within the sliding window. The
minimum controls the client’s burst size quota; the maximum
controls the penalty applied to abusive clients. This <code>clamp()</code>
also protects against wild clock resets.</p>
<pre><code>time = clamp(now - window, time, now)
</code></pre>
</li>
<li>
<p>Spend the nominal time consumed by the request.</p>
<pre><code>time += interval * cost
</code></pre>
</li>
<li>
<p>The request is allowed if it occurs after the not-before time. The
difference between the times determines the available quota and
effective window. Round the quota <em>down</em> and the window <em>up</em> to
whole numbers, so the client is given an underestimate of its
service limit.</p>
<pre><code>if now &gt; time
    state[pk] = time
    d = now - time
    r = floor(d * rate)
    t = ceil(d)
    return ALLOW(r, t)
</code></pre>
</li>
<li>
<p>The request is denied if there is not enough time available.
The available quota is zero, and the effective window is the
not-before time, rounded up as before.</p>
<pre><code>if time &gt;= now
    r = 0
    t = ceil(time - now)
    return DENY(r, t)
</code></pre>
</li>
</ul>
<p>That’s the core of the algorithm. There are a few refinements to
consider, to do with strictness and cleanliness.</p>
<p>Over-limit clients can be treated more or less leniently:</p>
<ul>
<li>
<p>The code above allows requests at the permitted rate regardless of
how frequently the client makes attempts: denied requests are not
counted.</p>
</li>
<li>
<p>Alternatively, the following code denies all requests from a
client until its attempts slow down below the permitted rate.
The cost is added to the effective window so that enough time is
available when the client returns.</p>
<pre><code>if time &gt;= now
    state[pk] = time
    d = time - now
    d += interval * cost
    r = 0
    t = ceil(d)
    return ALLOW(r, t)
</code></pre>
</li>
<li>
<p>To apply an extra penalty time-out to clients that make requests
within the not-before time, the server can increase the upper
bound on the <code>clamp()</code> into the future.</p>
</li>
</ul>
<p>Finally, it’s a good idea to have a background task that reclaims
unnecessary state belonging to idle clients:</p>
<pre><code>    for pk, time in state
        if time &lt; now - window
            delete state[pk]
</code></pre>
<h2><a name="other-rate-limiters" href="#other-rate-limiters">other rate limiters</a></h2>
<blockquote>
<p>The following discussion is mostly about explaining my claims
in the introductory paragraphs at the start of this article.</p>
</blockquote>
<p>There is a family of linear rate limit algorithms: GCRA behaves the
same as a leaky bucket or a token bucket rate limiter (when they are
implemented properly – there’s no need for a separate periodic leak /
refill task), but GCRA uses less space per client (it doesn’t need an
explicit bucket alongside the timestamp) and its code is simpler.</p>
<p>The HTTP <code>RateLimit</code> draft is based on algorithms that reset the quota
and window at a predictable time regardless of client behaviour. (The
draft allows other algorithms, but most of the text makes this
assumption.) After a client makes a burst of requests, it has to wait
until the reset time, then it is allowed to make another burst of
requests.</p>
<p>Another class of algorithm is based on keeping a log of client
activity within a sliding window in the recent past. (This is
expensive and wasteful!) Sliding-window algorithms allow new requests
when old requests age out, so they encourage clients to repeat past
behaviour. If a client starts with a burst then it can burst again in
each subsequent window.</p>
<p>By contrast, when a linear rate limiter allows a client to make a fast
burst of requests, it shrinks the effective window and then keeps the
effective window small while the client is busy.</p>
<p>Smaller quotas and windows restrict the burst size, so they require
smoother behaviour than larger quotas and windows, even though the
ratio <em>quota / window</em> describes the same maximum rate.</p>
<p>Automatically shrinking the effective window reduces the permitted
burstiness of subsequent requests. As a result, linear rate limiters
smooth out the interval between requests, as well as controlling the
number of requests per window.</p>
<p>Other algorithms can make clients behave smoothly too.</p>
<p>A linear rate limiter indirectly assesses a client’s request rate by
relating a request counter to the passage of time. By contrast, an
exponential rate limiter directly measures a client’s average rate and
allows or denies requests by comparing the rate to the limit.</p>
<p>An <a href="https://dotat.at/@/2024-09-02-ewma.html">exponential rate limiter</a> can be used with HTTP <code>RateLimit</code>
headers in a similar manner to a linear rate limiter. That previous
article explains how to calculate the effective window (called
<code>t_next</code>) when a request is denied. When a request is allowed, the
output results for a <code>RateLimit</code> header can be calculated as follows.
(Beware, this snippet uses an awkward mixture of variable names from
this article and the previous article.)</p>
<pre><code>    if r_now &lt; quota
        state[pk].time = t_now
        state[pk].rate = r_now
        a = quota - r_now
        r = floor(a)
        t = ceil(a * window / quota)
        return ALLOW(r, t)
</code></pre>
<p>The <code>RateLimit</code> header calculation for the exponential rate limiter is
just the reciprocal of the linear rate limiter. It has the same
smoothing effect for the same reasons.</p>
<p>I expect it’s possible to retrofit dynamic window shrinking on to
other rate limit algorithms. But (<a href="https://dotat.at/@/2026-01-12-hqlr.html">as I wrote yesterday</a>) it’s
better to use a simple GCRA linear rate limiter.</p>
<hr/>
<p>PS. The terminology in this article is different from how
<a href="https://dotat.at/@/2025-09-14-ratelimit.html">I previously described rate limit algorithms</a>: in the past
I used <em>rate = limit / period</em> where the HTTP <code>RateLimit</code> draft uses
<em>quota / window</em>.</p>

</article></div>
  </body>
</html>
