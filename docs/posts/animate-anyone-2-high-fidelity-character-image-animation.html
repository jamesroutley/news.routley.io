<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://humanaigc.github.io/animate-anyone-2/">Original</a>
    <h1>Animate Anyone 2: High-Fidelity Character Image Animation</h1>
    
    <div id="readability-page-1" class="page">

  <nav role="navigation" aria-label="main navigation">
    
    
  </nav>

<section>
  <div>
    <div>
    <!-- <div class="container is-max-desktop"> -->
      <div>
        <div>
          
          

          <p><span>Tongyi Labï¼ŒAlibaba Group</span>
          </p>


              <!-- ArXiv abstract Link -->
              <p><span>
                <a href="https://arxiv.org/pdf/2502.06145" target="_blank">
                  <span>
                    <i></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </p></div>
          </div>
        </div>
      </div>
    
  
</section>



<!-- Teaser Video -->
<section>
    
</section>



<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To address this limitation, we introduce Animate Anyone 2, aiming to animate characters with environment affordance. Beyond extracting motion signals from source video, we additionally capture environmental representations as conditional inputs. The environment is formulated as the region with the exclusion of characters and our model generates characters to populate these regions while maintaining coherence with the environmental context. We propose a shape-agnostic mask strategy that more effectively characterizes the relationship between character and environment. Furthermore, to enhance the fidelity of object interactions, we leverage an object guider to extract features of interacting objects and employ spatial blending for feature injection. We also introduce a pose modulation strategy that enables the model to handle more diverse motion patterns. Experimental results demonstrate the superior performance of the proposed method.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- motivation -->
<section>
  <div>
    <div>
      <h2>Motivation</h2>
      <div>
        <!-- Your image here -->
        <p><img src="https://humanaigc.github.io/animate-anyone-2/static2/images/motivation.png" alt="MY ALT TEXT"/>
        </p>
        <h2>
          We propose Animate Anyone 2, which differs from previous character image animation methods that solely utilize motion signals to animate characters. Our approach additionally extracts environmental representations from the driving video, thereby enabling character animation to exhibit environment affordance.
        </h2>
      </div>
    </div>
  </div>
  
  
</section>
<!-- End image carousel -->

<!-- Frmaework -->
<section>
  <div>
    <div>
      <h2>Method</h2>
      <div>
        <!-- Your image here -->
        <p><img src="https://humanaigc.github.io/animate-anyone-2/static2/images/framework.png" alt="MY ALT TEXT"/>
        </p>
        <h2>
          The framework of Animate Anyone 2. We capture environmental information from the source video. The environment is formulated as regions devoid of characters and incorporated as model input, enabling end-to-end learning of character-environment fusion. To preserve object interactions, we additionally inject features of objects interacting with the character. These object features are extracted by a lightweight object guider and merged into the denoising process via spatial blending. To handle more diverse motions, we propose a pose modulation approach to better represent the spatial relationships between body limbs.
        </h2>
      </div>
    </div>
  </div>
  
  
</section>
<!-- End image carousel -->




<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Results</h2>
      <h2>Environment Interaction</h2>
      
      <h2>
        Animate Anyone 2 demonstrates remarkable capabilities in generating characters with contextually coherent environmental interactions, characterized by seamless character-scene integration and robust character-object interaction.
    </h2></div>
  </div>
</section>
<!-- End video carousel -->


<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Dynamic Motion</h2>
      
      <h2>
        Animate Anyone 2 demonstrates robust capability in handling diverse and intricate motions, while ensuring character consistency and maintaining plausible interactions with the environmental context.
    </h2></div>
  </div>
</section>
<!-- End video carousel -->


<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Human Interaction </h2>
      
      <h2>
        Animate Anyone 2 is capable of generating interactions between characters, ensuring the plausibility of their movements and coherence with the surrounding environment.
    </h2></div>
  </div>
</section>
<!-- End video carousel -->



<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>Comparisons</h2>
      <h2>with Viggle</h2>
      
      <h2>
        <a href="https://viggle.ai/">Viggle</a> is capable of swapping characters in a video based on a provided character image, which is similar to the application scenario of our method. We compare our results with the latest Viggle V3. The outputs of Viggle demonstrate a rough blending of the characters with the environment, lack natural motion, and fail to capture the interaction between characters and the surroundings. In contrast, the results of our method exhibit higher fidelity.
    </h2></div>
  </div>
</section>
<!-- End video carousel -->


<!-- Video carousel -->
<section>
  <div>
    <div>
      <h2>with MIMO</h2>
      
      <h2>
        <a href="https://menyifang.github.io/projects/MIMO/index.html">MIMO</a> is the most relevant method to our task setting, which decomposes videos into human, background and occlusion based on depth and composing these elements to generate character video. Our approach demonstrates superior robustness and finer detail preservation.
    </h2></div>
  </div>
</section>
<!-- End video carousel -->




<!--BibTex citation -->
<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@article{hu2025animateanyone2,
      title={Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance},
      author={Li Hu and Guangyuan Wang and Zhen Shen and Xin Gao and Dechao Meng and Lian Zhuo and Peng Zhang and Bang Zhang and Liefeng Bo},
      journal={arXiv preprint arXiv:2502.06145},
      website={https://humanaigc.github.io/animate-anyone-2/},
      year={2025}
}</code></pre>
  </div>
</section>


  



<!-- Default Statcounter code for aa2.0
https://humanaigc.github.io/animate-anyone-2/ -->



<!-- End of Statcounter Code -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
