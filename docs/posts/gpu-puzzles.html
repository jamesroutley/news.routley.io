<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/srush/GPU-Puzzles">Original</a>
    <h1>GPU Puzzles</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<ul dir="auto">
<li>by <a href="http://rush-nlp.com" rel="nofollow">Sasha Rush</a> - <a href="https://twitter.com/srush_nlp" rel="nofollow">srush_nlp</a></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/raw/main/cuda.png"><img src="https://github.com/srush/GPU-Puzzles/raw/main/cuda.png" alt=""/></a></p>
<p dir="auto">GPU architectures are critical to machine learning, and seem to be
becoming even more important every day. However, you can be an expert
in machine learning without ever touching GPU code. It is hard to gain
intuition working through abstractions.</p>
<p dir="auto">This notebook is an attempt to teach beginner GPU programming in a
completely interactive fashion. Instead of providing text with
concepts, it throws you right into coding and building GPU
kernels. The exercises use NUMBA which directly maps Python
code to CUDA kernels. It looks like Python but is basically
identical to writing low-level CUDA code.
In a few hours, I think you can go from basics to
understanding the real algorithms that power 99% of deep learning
today. If you do want to read the manual, it is here:</p>
<p dir="auto"><a href="https://numba.readthedocs.io/en/stable/cuda/index.html" rel="nofollow">NUMBA CUDA Guide</a></p>
<p dir="auto">I recommend doing these in Colab, as it is easy to get started.  Be
sure to make your own copy, turn on GPU mode in the settings (<code>Runtime / Change runtime type</code>, then set <code>Hardware accelerator</code> to <code>GPU</code>), and
then get to coding.</p>
<p dir="auto"><a href="https://colab.research.google.com/github/srush/GPU-Puzzles/blob/main/GPU_puzzlers.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p dir="auto">(If you are into this style of puzzle, also check out my <a href="https://github.com/srush/Tensor-Puzzles">Tensor
Puzzles</a> for PyTorch.)</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=K4T-YwsOxrM" rel="nofollow">Walkthrough Guide</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="!pip install -qqq git+https://github.com/danoneata/chalk@srush-patch-1
!wget -q https://github.com/srush/GPU-Puzzles/raw/main/robot.png https://github.com/srush/GPU-Puzzles/raw/main/lib.py"><pre>!p<span>ip</span> <span>install</span> <span>-</span><span>qqq</span> <span>git</span><span>+</span><span>https</span>:<span>//</span><span>github</span>.<span>com</span><span>/</span><span>danoneata</span><span>/</span><span>chalk</span>@<span>srush</span><span>-</span><span>patch</span><span>-</span><span>1</span>
!w<span>get</span> <span>-</span><span>q</span> <span>https</span>:<span>//</span><span>github</span>.<span>com</span><span>/</span><span>srush</span><span>/</span><span>GPU</span><span>-</span><span>Puzzles</span><span>/</span><span>raw</span><span>/</span><span>main</span><span>/</span><span>robot</span>.<span>png</span> <span>https</span>:<span>//</span><span>github</span>.<span>com</span><span>/</span><span>srush</span><span>/</span><span>GPU</span><span>-</span><span>Puzzles</span><span>/</span><span>raw</span><span>/</span><span>main</span><span>/</span><span>lib</span>.<span>py</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="import numba
import numpy as np
import warnings
from lib import CudaProblem, Coord"><pre><span>import</span> <span>numba</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>warnings</span>
<span>from</span> <span>lib</span> <span>import</span> <span>CudaProblem</span>, <span>Coord</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="warnings.filterwarnings(
    action=&#34;ignore&#34;, category=numba.NumbaPerformanceWarning, module=&#34;numba&#34;
)"><pre><span>warnings</span>.<span>filterwarnings</span>(
    <span>action</span><span>=</span><span>&#34;ignore&#34;</span>, <span>category</span><span>=</span><span>numba</span>.<span>NumbaPerformanceWarning</span>, <span>module</span><span>=</span><span>&#34;numba&#34;</span>
)</pre></div>

<p dir="auto">Implement a &#34;kernel&#34; (GPU function) that adds 10 to each position of vector <code>a</code>
and stores it in vector <code>out</code>.  You have 1 thread per position.</p>
<p dir="auto"><strong>Warning</strong> This code looks like Python but it is really CUDA! You cannot use
standard python tools like list comprehensions or ask for Numpy properties
like shape or size (if you need the size, it is given as an argument).
The puzzles only require doing simple operations, basically
+, *, simple array indexing, for loops, and if statements.
You are allowed to use local variables.
If you get an
error it is probably because you did something fancy :).</p>
<p dir="auto"><em>Tip: Think of the function <code>call</code> as being run 1 time for each thread.
The only difference is that <code>cuda.threadIdx.x</code> changes each time.</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="def map_spec(a):
    return a + 10


def map_test(cuda):
    def call(out, a) -&gt; None:
        local_i = cuda.threadIdx.x
        # FILL ME IN (roughly 1 lines)

    return call


SIZE = 4
out = np.zeros((SIZE,))
a = np.arange(SIZE)
problem = CudaProblem(
    &#34;Map&#34;, map_test, [a], out, threadsperblock=Coord(SIZE, 1), spec=map_spec
)
problem.show()"><pre><span>def</span> <span>map_spec</span>(<span>a</span>):
    <span>return</span> <span>a</span> <span>+</span> <span>10</span>


<span>def</span> <span>map_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>) <span>-&gt;</span> <span>None</span>:
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 1 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>4</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>,))
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Map&#34;</span>, <span>map_test</span>, [<span>a</span>], <span>out</span>, <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>SIZE</span>, <span>1</span>), <span>spec</span><span>=</span><span>map_spec</span>
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Map
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Map
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_14_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_14_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0.]
Spec : [10 11 12 13]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0.]
Spec : [10 11 12 13]
</code></pre></div>

<p dir="auto">Implement a kernel that adds together each position of <code>a</code> and <code>b</code> and stores it in <code>out</code>.
You have 1 thread per position.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def zip_spec(a, b):
    return a + b


def zip_test(cuda):
    def call(out, a, b) -&gt; None:
        local_i = cuda.threadIdx.x
        # FILL ME IN (roughly 1 lines)

    return call


SIZE = 4
out = np.zeros((SIZE,))
a = np.arange(SIZE)
b = np.arange(SIZE)
problem = CudaProblem(
    &#34;Zip&#34;, zip_test, [a, b], out, threadsperblock=Coord(SIZE, 1), spec=zip_spec
)
problem.show()"><pre><span>def</span> <span>zip_spec</span>(<span>a</span>, <span>b</span>):
    <span>return</span> <span>a</span> <span>+</span> <span>b</span>


<span>def</span> <span>zip_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>b</span>) <span>-&gt;</span> <span>None</span>:
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 1 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>4</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>,))
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>b</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Zip&#34;</span>, <span>zip_test</span>, [<span>a</span>, <span>b</span>], <span>out</span>, <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>SIZE</span>, <span>1</span>), <span>spec</span><span>=</span><span>zip_spec</span>
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Zip
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Zip
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_17_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_17_1.svg" alt="svg"/></a></p>


<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0.]
Spec : [0 2 4 6]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0.]
Spec : [0 2 4 6]
</code></pre></div>

<p dir="auto">Implement a kernel that adds 10 to each position of <code>a</code> and stores it in <code>out</code>.
You have more threads than positions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def map_guard_test(cuda):
    def call(out, a, size) -&gt; None:
        local_i = cuda.threadIdx.x
        # FILL ME IN (roughly 2 lines)

    return call


SIZE = 4
out = np.zeros((SIZE,))
a = np.arange(SIZE)
problem = CudaProblem(
    &#34;Guard&#34;,
    map_guard_test,
    [a],
    out,
    [SIZE],
    threadsperblock=Coord(8, 1),
    spec=map_spec,
)
problem.show()"><pre><span>def</span> <span>map_guard_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 2 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>4</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>,))
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Guard&#34;</span>,
    <span>map_guard_test</span>,
    [<span>a</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>8</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>map_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Guard
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Guard
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_21_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_21_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0.]
Spec : [10 11 12 13]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0.]
Spec : [10 11 12 13]
</code></pre></div>

<p dir="auto">Implement a kernel that adds 10 to each position of <code>a</code> and stores it in <code>out</code>.
Input <code>a</code> is 2D and square. You have more threads than positions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def map_2D_test(cuda):
    def call(out, a, size) -&gt; None:
        local_i = cuda.threadIdx.x
        local_j = cuda.threadIdx.y
        # FILL ME IN (roughly 2 lines)

    return call


SIZE = 2
out = np.zeros((SIZE, SIZE))
a = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))
problem = CudaProblem(
    &#34;Map 2D&#34;, map_2D_test, [a], out, [SIZE], threadsperblock=Coord(3, 3), spec=map_spec
)
problem.show()"><pre><span>def</span> <span>map_2D_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_j</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>y</span>
        <span># FILL ME IN (roughly 2 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>2</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span> <span>*</span> <span>SIZE</span>).<span>reshape</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Map 2D&#34;</span>, <span>map_2D_test</span>, [<span>a</span>], <span>out</span>, [<span>SIZE</span>], <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>3</span>, <span>3</span>), <span>spec</span><span>=</span><span>map_spec</span>
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Map 2D
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Map 2D
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_24_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_24_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [[0. 0.]
 [0. 0.]]
Spec : [[10 11]
 [12 13]]"><pre><code>Failed Tests.
Yours: [[0. 0.]
 [0. 0.]]
Spec : [[10 11]
 [12 13]]
</code></pre></div>

<p dir="auto">Implement a kernel that adds <code>a</code> and <code>b</code> and stores it in <code>out</code>.
Inputs <code>a</code> and <code>b</code> are vectors. You have more threads than positions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def broadcast_test(cuda):
    def call(out, a, b, size) -&gt; None:
        local_i = cuda.threadIdx.x
        local_j = cuda.threadIdx.y
        # FILL ME IN (roughly 2 lines)

    return call


SIZE = 2
out = np.zeros((SIZE, SIZE))
a = np.arange(SIZE).reshape(SIZE, 1)
b = np.arange(SIZE).reshape(1, SIZE)
problem = CudaProblem(
    &#34;Broadcast&#34;,
    broadcast_test,
    [a, b],
    out,
    [SIZE],
    threadsperblock=Coord(3, 3),
    spec=zip_spec,
)
problem.show()"><pre><span>def</span> <span>broadcast_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>b</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_j</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>y</span>
        <span># FILL ME IN (roughly 2 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>2</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>).<span>reshape</span>(<span>SIZE</span>, <span>1</span>)
<span>b</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>).<span>reshape</span>(<span>1</span>, <span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Broadcast&#34;</span>,
    <span>broadcast_test</span>,
    [<span>a</span>, <span>b</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>3</span>, <span>3</span>),
    <span>spec</span><span>=</span><span>zip_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Broadcast
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Broadcast
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_27_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_27_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [[0. 0.]
 [0. 0.]]
Spec : [[0 1]
 [1 2]]"><pre><code>Failed Tests.
Yours: [[0. 0.]
 [0. 0.]]
Spec : [[0 1]
 [1 2]]
</code></pre></div>

<p dir="auto">Implement a kernel that adds 10 to each position of <code>a</code> and stores it in <code>out</code>.
You have fewer threads per block than the size of <code>a</code>.</p>
<p dir="auto"><em>Tip: A block is a group of threads. The number of threads per block is limited, but we can
have many different blocks. Variable <code>cuda.blockIdx</code> tells us what block we are in.</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="def map_block_test(cuda):
    def call(out, a, size) -&gt; None:
        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        # FILL ME IN (roughly 2 lines)

    return call


SIZE = 9
out = np.zeros((SIZE,))
a = np.arange(SIZE)
problem = CudaProblem(
    &#34;Blocks&#34;,
    map_block_test,
    [a],
    out,
    [SIZE],
    threadsperblock=Coord(4, 1),
    blockspergrid=Coord(3, 1),
    spec=map_spec,
)
problem.show()"><pre><span>def</span> <span>map_block_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 2 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>9</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>,))
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Blocks&#34;</span>,
    <span>map_block_test</span>,
    [<span>a</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>4</span>, <span>1</span>),
    <span>blockspergrid</span><span>=</span><span>Coord</span>(<span>3</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>map_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Blocks
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Blocks
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_31_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_31_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [10 11 12 13 14 15 16 17 18]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [10 11 12 13 14 15 16 17 18]
</code></pre></div>

<p dir="auto">Implement the same kernel in 2D.  You have fewer threads per block
than the size of <code>a</code> in both directions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def map_block2D_test(cuda):
    def call(out, a, size) -&gt; None:
        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        # FILL ME IN (roughly 4 lines)

    return call


SIZE = 5
out = np.zeros((SIZE, SIZE))
a = np.ones((SIZE, SIZE))

problem = CudaProblem(
    &#34;Blocks 2D&#34;,
    map_block2D_test,
    [a],
    out,
    [SIZE],
    threadsperblock=Coord(3, 3),
    blockspergrid=Coord(2, 2),
    spec=map_spec,
)
problem.show()"><pre><span>def</span> <span>map_block2D_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 4 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>5</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>a</span> <span>=</span> <span>np</span>.<span>ones</span>((<span>SIZE</span>, <span>SIZE</span>))

<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Blocks 2D&#34;</span>,
    <span>map_block2D_test</span>,
    [<span>a</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>3</span>, <span>3</span>),
    <span>blockspergrid</span><span>=</span><span>Coord</span>(<span>2</span>, <span>2</span>),
    <span>spec</span><span>=</span><span>map_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Blocks 2D
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Blocks 2D
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_34_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_34_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]]
Spec : [[11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]]"><pre><code>Failed Tests.
Yours: [[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]]
Spec : [[11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]
 [11. 11. 11. 11. 11.]]
</code></pre></div>

<p dir="auto">Implement a kernel that adds 10 to each position of <code>a</code> and stores it in <code>out</code>.
You have fewer threads per block than the size of <code>a</code>.</p>
<p dir="auto"><strong>Warning</strong>: Each block can only have a <em>constant</em> amount of shared
memory that threads in that block can read and write to. This needs
to be a literal python constant not a variable. After writing to
shared memory you need to call <code>cuda.syncthreads</code> to ensure that
threads do not cross.</p>
<p dir="auto">(This example does not really need shared memory or syncthreads, but it is a demo.)</p>
<div dir="auto" data-snippet-clipboard-copy-content="TPB = 4
def shared_test(cuda):
    def call(out, a, size) -&gt; None:
        shared = cuda.shared.array(TPB, numba.float32)
        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        local_i = cuda.threadIdx.x

        if i &lt; size:
            shared[local_i] = a[i]
            cuda.syncthreads()

        # FILL ME IN (roughly 2 lines)

    return call


SIZE = 8
out = np.zeros(SIZE)
a = np.ones(SIZE)
problem = CudaProblem(
    &#34;Shared&#34;,
    shared_test,
    [a],
    out,
    [SIZE],
    threadsperblock=Coord(TPB, 1),
    blockspergrid=Coord(2, 1),
    spec=map_spec,
)
problem.show()"><pre><span>TPB</span> <span>=</span> <span>4</span>
<span>def</span> <span>shared_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>shared</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>(<span>TPB</span>, <span>numba</span>.<span>float32</span>)
        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>

        <span>if</span> <span>i</span> <span>&lt;</span> <span>size</span>:
            <span>shared</span>[<span>local_i</span>] <span>=</span> <span>a</span>[<span>i</span>]
            <span>cuda</span>.<span>syncthreads</span>()

        <span># FILL ME IN (roughly 2 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>8</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>SIZE</span>)
<span>a</span> <span>=</span> <span>np</span>.<span>ones</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Shared&#34;</span>,
    <span>shared_test</span>,
    [<span>a</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>TPB</span>, <span>1</span>),
    <span>blockspergrid</span><span>=</span><span>Coord</span>(<span>2</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>map_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Shared
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             1 |             0 |             0 |             1 | "><pre><code># Shared
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             1 |             0 |             0 |             1 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_39_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_39_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [11. 11. 11. 11. 11. 11. 11. 11.]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [11. 11. 11. 11. 11. 11. 11. 11.]
</code></pre></div>

<p dir="auto">Implement a kernel that sums together the last 3 position of <code>a</code> and stores it in <code>out</code>.
You have 1 thread per position. You only need 1 global read and 1 global write per thread.</p>
<p dir="auto"><em>Tip: Remember to be careful about syncing.</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="def pool_spec(a):
    out = np.zeros(*a.shape)
    for i in range(a.shape[0]):
        out[i] = a[max(i - 2, 0) : i + 1].sum()
    return out


TPB = 8
def pool_test(cuda):
    def call(out, a, size) -&gt; None:
        shared = cuda.shared.array(TPB, numba.float32)
        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        local_i = cuda.threadIdx.x
        # FILL ME IN (roughly 8 lines)

    return call


SIZE = 8
out = np.zeros(SIZE)
a = np.arange(SIZE)
problem = CudaProblem(
    &#34;Pooling&#34;,
    pool_test,
    [a],
    out,
    [SIZE],
    threadsperblock=Coord(TPB, 1),
    blockspergrid=Coord(1, 1),
    spec=pool_spec,
)
problem.show()"><pre><span>def</span> <span>pool_spec</span>(<span>a</span>):
    <span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>*</span><span>a</span>.<span>shape</span>)
    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>a</span>.<span>shape</span>[<span>0</span>]):
        <span>out</span>[<span>i</span>] <span>=</span> <span>a</span>[<span>max</span>(<span>i</span> <span>-</span> <span>2</span>, <span>0</span>) : <span>i</span> <span>+</span> <span>1</span>].<span>sum</span>()
    <span>return</span> <span>out</span>


<span>TPB</span> <span>=</span> <span>8</span>
<span>def</span> <span>pool_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>shared</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>(<span>TPB</span>, <span>numba</span>.<span>float32</span>)
        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 8 lines)</span>

    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>8</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>SIZE</span>)
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Pooling&#34;</span>,
    <span>pool_test</span>,
    [<span>a</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>TPB</span>, <span>1</span>),
    <span>blockspergrid</span><span>=</span><span>Coord</span>(<span>1</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>pool_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Pooling
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Pooling
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_43_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_43_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [ 0.  1.  3.  6.  9. 12. 15. 18.]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [ 0.  1.  3.  6.  9. 12. 15. 18.]
</code></pre></div>

<p dir="auto">Implement a kernel that computes the dot-product of <code>a</code> and <code>b</code> and stores it in <code>out</code>.
You have 1 thread per position. You only need 2 global reads and 1 global write per thread.</p>
<p dir="auto"><em>Note: For this problem you don&#39;t need to worry about number of shared reads. We will
handle that challenge later.</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="def dot_spec(a, b):
    return a @ b

TPB = 8
def dot_test(cuda):
    def call(out, a, b, size) -&gt; None:
        shared = cuda.shared.array(TPB, numba.float32)

        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        local_i = cuda.threadIdx.x
        # FILL ME IN (roughly 9 lines)
    return call


SIZE = 8
out = np.zeros(1)
a = np.arange(SIZE)
b = np.arange(SIZE)
problem = CudaProblem(
    &#34;Dot&#34;,
    dot_test,
    [a, b],
    out,
    [SIZE],
    threadsperblock=Coord(SIZE, 1),
    blockspergrid=Coord(1, 1),
    spec=dot_spec,
)
problem.show()"><pre><span>def</span> <span>dot_spec</span>(<span>a</span>, <span>b</span>):
    <span>return</span> <span>a</span> @ <span>b</span>

<span>TPB</span> <span>=</span> <span>8</span>
<span>def</span> <span>dot_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>b</span>, <span>size</span>) <span>-&gt;</span> <span>None</span>:
        <span>shared</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>(<span>TPB</span>, <span>numba</span>.<span>float32</span>)

        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 9 lines)</span>
    <span>return</span> <span>call</span>


<span>SIZE</span> <span>=</span> <span>8</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>1</span>)
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>b</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Dot&#34;</span>,
    <span>dot_test</span>,
    [<span>a</span>, <span>b</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>threadsperblock</span><span>=</span><span>Coord</span>(<span>SIZE</span>, <span>1</span>),
    <span>blockspergrid</span><span>=</span><span>Coord</span>(<span>1</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>dot_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Dot
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Dot
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_47_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_47_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0.]
Spec : 140"><pre><code>Failed Tests.
Yours: [0.]
Spec : 140
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Puzzle 11 - 1D Convolution</h2><a id="user-content-puzzle-11---1d-convolution" aria-label="Permalink: Puzzle 11 - 1D Convolution" href="#puzzle-11---1d-convolution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Implement a kernel that computes a 1D convolution between <code>a</code> and <code>b</code> and stores it in <code>out</code>.
You need to handle the general case. You only need 2 global reads and 1 global write per thread.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def conv_spec(a, b):
    out = np.zeros(*a.shape)
    len = b.shape[0]
    for i in range(a.shape[0]):
        out[i] = sum([a[i + j] * b[j] for j in range(len) if i + j &lt; a.shape[0]])
    return out


MAX_CONV = 4
TPB = 8
TPB_MAX_CONV = TPB + MAX_CONV
def conv_test(cuda):
    def call(out, a, b, a_size, b_size) -&gt; None:
        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        local_i = cuda.threadIdx.x

        # FILL ME IN (roughly 17 lines)

    return call


# Test 1

SIZE = 6
CONV = 3
out = np.zeros(SIZE)
a = np.arange(SIZE)
b = np.arange(CONV)
problem = CudaProblem(
    &#34;1D Conv (Simple)&#34;,
    conv_test,
    [a, b],
    out,
    [SIZE, CONV],
    Coord(1, 1),
    Coord(TPB, 1),
    spec=conv_spec,
)
problem.show()"><pre><span>def</span> <span>conv_spec</span>(<span>a</span>, <span>b</span>):
    <span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>*</span><span>a</span>.<span>shape</span>)
    <span>len</span> <span>=</span> <span>b</span>.<span>shape</span>[<span>0</span>]
    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>a</span>.<span>shape</span>[<span>0</span>]):
        <span>out</span>[<span>i</span>] <span>=</span> <span>sum</span>([<span>a</span>[<span>i</span> <span>+</span> <span>j</span>] <span>*</span> <span>b</span>[<span>j</span>] <span>for</span> <span>j</span> <span>in</span> <span>range</span>(<span>len</span>) <span>if</span> <span>i</span> <span>+</span> <span>j</span> <span>&lt;</span> <span>a</span>.<span>shape</span>[<span>0</span>]])
    <span>return</span> <span>out</span>


<span>MAX_CONV</span> <span>=</span> <span>4</span>
<span>TPB</span> <span>=</span> <span>8</span>
<span>TPB_MAX_CONV</span> <span>=</span> <span>TPB</span> <span>+</span> <span>MAX_CONV</span>
<span>def</span> <span>conv_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>b</span>, <span>a_size</span>, <span>b_size</span>) <span>-&gt;</span> <span>None</span>:
        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>

        <span># FILL ME IN (roughly 17 lines)</span>

    <span>return</span> <span>call</span>


<span># Test 1</span>

<span>SIZE</span> <span>=</span> <span>6</span>
<span>CONV</span> <span>=</span> <span>3</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>SIZE</span>)
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>b</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>CONV</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;1D Conv (Simple)&#34;</span>,
    <span>conv_test</span>,
    [<span>a</span>, <span>b</span>],
    <span>out</span>,
    [<span>SIZE</span>, <span>CONV</span>],
    <span>Coord</span>(<span>1</span>, <span>1</span>),
    <span>Coord</span>(<span>TPB</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>conv_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# 1D Conv (Simple)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># 1D Conv (Simple)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_50_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_50_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0. 0. 0.]
Spec : [ 5.  8. 11. 14.  5.  0.]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0. 0. 0.]
Spec : [ 5.  8. 11. 14.  5.  0.]
</code></pre></div>
<p dir="auto">Test 2</p>
<div dir="auto" data-snippet-clipboard-copy-content="out = np.zeros(15)
a = np.arange(15)
b = np.arange(4)
problem = CudaProblem(
    &#34;1D Conv (Full)&#34;,
    conv_test,
    [a, b],
    out,
    [15, 4],
    Coord(2, 1),
    Coord(TPB, 1),
    spec=conv_spec,
)
problem.show()"><pre><span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>15</span>)
<span>a</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>15</span>)
<span>b</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>4</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;1D Conv (Full)&#34;</span>,
    <span>conv_test</span>,
    [<span>a</span>, <span>b</span>],
    <span>out</span>,
    [<span>15</span>, <span>4</span>],
    <span>Coord</span>(<span>2</span>, <span>1</span>),
    <span>Coord</span>(<span>TPB</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>conv_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# 1D Conv (Full)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># 1D Conv (Full)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_53_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_53_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [14. 20. 26. 32. 38. 44. 50. 56. 62. 68. 74. 80. 41. 14.  0.]"><pre><code>Failed Tests.
Yours: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Spec : [14. 20. 26. 32. 38. 44. 50. 56. 62. 68. 74. 80. 41. 14.  0.]
</code></pre></div>

<p dir="auto">Implement a kernel that computes a sum over <code>a</code> and stores it in <code>out</code>.
If the size of <code>a</code> is greater than the block size, only store the sum of
each block.</p>
<p dir="auto">We will do this using the <a href="https://en.wikipedia.org/wiki/Prefix_sum" rel="nofollow">parallel prefix sum</a> algorithm in shared memory.
That is, each step of the algorithm should sum together half the remaining numbers.
Follow this diagram:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/35882/178757889-1c269623-93af-4a2e-a7e9-22cd55a42e38.png"><img src="https://user-images.githubusercontent.com/35882/178757889-1c269623-93af-4a2e-a7e9-22cd55a42e38.png" alt=""/></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="TPB = 8
def sum_spec(a):
    out = np.zeros((a.shape[0] + TPB - 1) // TPB)
    for j, i in enumerate(range(0, a.shape[-1], TPB)):
        out[j] = a[i : i + TPB].sum()
    return out


def sum_test(cuda):
    def call(out, a, size: int) -&gt; None:
        cache = cuda.shared.array(TPB, numba.float32)
        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        local_i = cuda.threadIdx.x
        # FILL ME IN (roughly 12 lines)

    return call


# Test 1

SIZE = 8
out = np.zeros(1)
inp = np.arange(SIZE)
problem = CudaProblem(
    &#34;Sum (Simple)&#34;,
    sum_test,
    [inp],
    out,
    [SIZE],
    Coord(1, 1),
    Coord(TPB, 1),
    spec=sum_spec,
)
problem.show()"><pre><span>TPB</span> <span>=</span> <span>8</span>
<span>def</span> <span>sum_spec</span>(<span>a</span>):
    <span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>a</span>.<span>shape</span>[<span>0</span>] <span>+</span> <span>TPB</span> <span>-</span> <span>1</span>) <span>//</span> <span>TPB</span>)
    <span>for</span> <span>j</span>, <span>i</span> <span>in</span> <span>enumerate</span>(<span>range</span>(<span>0</span>, <span>a</span>.<span>shape</span>[<span>-</span><span>1</span>], <span>TPB</span>)):
        <span>out</span>[<span>j</span>] <span>=</span> <span>a</span>[<span>i</span> : <span>i</span> <span>+</span> <span>TPB</span>].<span>sum</span>()
    <span>return</span> <span>out</span>


<span>def</span> <span>sum_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>: <span>int</span>) <span>-&gt;</span> <span>None</span>:
        <span>cache</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>(<span>TPB</span>, <span>numba</span>.<span>float32</span>)
        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span># FILL ME IN (roughly 12 lines)</span>

    <span>return</span> <span>call</span>


<span># Test 1</span>

<span>SIZE</span> <span>=</span> <span>8</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>1</span>)
<span>inp</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Sum (Simple)&#34;</span>,
    <span>sum_test</span>,
    [<span>inp</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>Coord</span>(<span>1</span>, <span>1</span>),
    <span>Coord</span>(<span>TPB</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>sum_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Sum (Simple)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Sum (Simple)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_58_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_58_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0.]
Spec : [28.]"><pre><code>Failed Tests.
Yours: [0.]
Spec : [28.]
</code></pre></div>
<p dir="auto">Test 2</p>
<div dir="auto" data-snippet-clipboard-copy-content="SIZE = 15
out = np.zeros(2)
inp = np.arange(SIZE)
problem = CudaProblem(
    &#34;Sum (Full)&#34;,
    sum_test,
    [inp],
    out,
    [SIZE],
    Coord(2, 1),
    Coord(TPB, 1),
    spec=sum_spec,
)
problem.show()"><pre><span>SIZE</span> <span>=</span> <span>15</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>(<span>2</span>)
<span>inp</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span>)
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Sum (Full)&#34;</span>,
    <span>sum_test</span>,
    [<span>inp</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>Coord</span>(<span>2</span>, <span>1</span>),
    <span>Coord</span>(<span>TPB</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>sum_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Sum (Full)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Sum (Full)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_61_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_61_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [0. 0.]
Spec : [28. 77.]"><pre><code>Failed Tests.
Yours: [0. 0.]
Spec : [28. 77.]
</code></pre></div>

<p dir="auto">Implement a kernel that computes a sum over each column of <code>a</code> and stores it in <code>out</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="TPB = 8
def sum_spec(a):
    out = np.zeros((a.shape[0], (a.shape[1] + TPB - 1) // TPB))
    for j, i in enumerate(range(0, a.shape[-1], TPB)):
        out[..., j] = a[..., i : i + TPB].sum(-1)
    return out


def axis_sum_test(cuda):
    def call(out, a, size: int) -&gt; None:
        cache = cuda.shared.array(TPB, numba.float32)
        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        local_i = cuda.threadIdx.x
        batch = cuda.blockIdx.y
        # FILL ME IN (roughly 12 lines)

    return call


BATCH = 4
SIZE = 6
out = np.zeros((BATCH, 1))
inp = np.arange(BATCH * SIZE).reshape((BATCH, SIZE))
problem = CudaProblem(
    &#34;Axis Sum&#34;,
    axis_sum_test,
    [inp],
    out,
    [SIZE],
    Coord(1, BATCH),
    Coord(TPB, 1),
    spec=sum_spec,
)
problem.show()"><pre><span>TPB</span> <span>=</span> <span>8</span>
<span>def</span> <span>sum_spec</span>(<span>a</span>):
    <span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>a</span>.<span>shape</span>[<span>0</span>], (<span>a</span>.<span>shape</span>[<span>1</span>] <span>+</span> <span>TPB</span> <span>-</span> <span>1</span>) <span>//</span> <span>TPB</span>))
    <span>for</span> <span>j</span>, <span>i</span> <span>in</span> <span>enumerate</span>(<span>range</span>(<span>0</span>, <span>a</span>.<span>shape</span>[<span>-</span><span>1</span>], <span>TPB</span>)):
        <span>out</span>[..., <span>j</span>] <span>=</span> <span>a</span>[..., <span>i</span> : <span>i</span> <span>+</span> <span>TPB</span>].<span>sum</span>(<span>-</span><span>1</span>)
    <span>return</span> <span>out</span>


<span>def</span> <span>axis_sum_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>size</span>: <span>int</span>) <span>-&gt;</span> <span>None</span>:
        <span>cache</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>(<span>TPB</span>, <span>numba</span>.<span>float32</span>)
        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>batch</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>y</span>
        <span># FILL ME IN (roughly 12 lines)</span>

    <span>return</span> <span>call</span>


<span>BATCH</span> <span>=</span> <span>4</span>
<span>SIZE</span> <span>=</span> <span>6</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>BATCH</span>, <span>1</span>))
<span>inp</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>BATCH</span> <span>*</span> <span>SIZE</span>).<span>reshape</span>((<span>BATCH</span>, <span>SIZE</span>))
<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Axis Sum&#34;</span>,
    <span>axis_sum_test</span>,
    [<span>inp</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>Coord</span>(<span>1</span>, <span>BATCH</span>),
    <span>Coord</span>(<span>TPB</span>, <span>1</span>),
    <span>spec</span><span>=</span><span>sum_spec</span>,
)
<span>problem</span>.<span>show</span>()</pre></div>
<div data-snippet-clipboard-copy-content="# Axis Sum
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Axis Sum
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_64_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_64_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [[0.]
 [0.]
 [0.]
 [0.]]
Spec : [[ 15.]
 [ 51.]
 [ 87.]
 [123.]]"><pre><code>Failed Tests.
Yours: [[0.]
 [0.]
 [0.]
 [0.]]
Spec : [[ 15.]
 [ 51.]
 [ 87.]
 [123.]]
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Puzzle 14 - Matrix Multiply!</h2><a id="user-content-puzzle-14---matrix-multiply" aria-label="Permalink: Puzzle 14 - Matrix Multiply!" href="#puzzle-14---matrix-multiply"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Implement a kernel that multiplies square matrices <code>a</code> and <code>b</code> and
stores the result in <code>out</code>.</p>
<p dir="auto"><em>Tip: The most efficient algorithm here will copy a block into
shared memory before computing each of the individual row-column
dot products. This is easy to do if the matrix fits in shared
memory.  Do that case first. Then update your code to compute
a partial dot-product and iteratively move the part you
copied into shared memory.</em> You should be able to do the hard case
in 6 global reads.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def matmul_spec(a, b):
    return a @ b


TPB = 3
def mm_oneblock_test(cuda):
    def call(out, a, b, size: int) -&gt; None:
        a_shared = cuda.shared.array((TPB, TPB), numba.float32)
        b_shared = cuda.shared.array((TPB, TPB), numba.float32)

        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        j = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y
        local_i = cuda.threadIdx.x
        local_j = cuda.threadIdx.y
        # FILL ME IN (roughly 14 lines)

    return call

# Test 1

SIZE = 2
out = np.zeros((SIZE, SIZE))
inp1 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))
inp2 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE)).T

problem = CudaProblem(
    &#34;Matmul (Simple)&#34;,
    mm_oneblock_test,
    [inp1, inp2],
    out,
    [SIZE],
    Coord(1, 1),
    Coord(TPB, TPB),
    spec=matmul_spec,
)
problem.show(sparse=True)"><pre><span>def</span> <span>matmul_spec</span>(<span>a</span>, <span>b</span>):
    <span>return</span> <span>a</span> @ <span>b</span>


<span>TPB</span> <span>=</span> <span>3</span>
<span>def</span> <span>mm_oneblock_test</span>(<span>cuda</span>):
    <span>def</span> <span>call</span>(<span>out</span>, <span>a</span>, <span>b</span>, <span>size</span>: <span>int</span>) <span>-&gt;</span> <span>None</span>:
        <span>a_shared</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>((<span>TPB</span>, <span>TPB</span>), <span>numba</span>.<span>float32</span>)
        <span>b_shared</span> <span>=</span> <span>cuda</span>.<span>shared</span>.<span>array</span>((<span>TPB</span>, <span>TPB</span>), <span>numba</span>.<span>float32</span>)

        <span>i</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>x</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>x</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>j</span> <span>=</span> <span>cuda</span>.<span>blockIdx</span>.<span>y</span> <span>*</span> <span>cuda</span>.<span>blockDim</span>.<span>y</span> <span>+</span> <span>cuda</span>.<span>threadIdx</span>.<span>y</span>
        <span>local_i</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>x</span>
        <span>local_j</span> <span>=</span> <span>cuda</span>.<span>threadIdx</span>.<span>y</span>
        <span># FILL ME IN (roughly 14 lines)</span>

    <span>return</span> <span>call</span>

<span># Test 1</span>

<span>SIZE</span> <span>=</span> <span>2</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>inp1</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span> <span>*</span> <span>SIZE</span>).<span>reshape</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>inp2</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span> <span>*</span> <span>SIZE</span>).<span>reshape</span>((<span>SIZE</span>, <span>SIZE</span>)).<span>T</span>

<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Matmul (Simple)&#34;</span>,
    <span>mm_oneblock_test</span>,
    [<span>inp1</span>, <span>inp2</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>Coord</span>(<span>1</span>, <span>1</span>),
    <span>Coord</span>(<span>TPB</span>, <span>TPB</span>),
    <span>spec</span><span>=</span><span>matmul_spec</span>,
)
<span>problem</span>.<span>show</span>(<span>sparse</span><span>=</span><span>True</span>)</pre></div>
<div data-snippet-clipboard-copy-content="# Matmul (Simple)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Matmul (Simple)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_67_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_67_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [[0. 0.]
 [0. 0.]]
Spec : [[ 1  3]
 [ 3 13]]"><pre><code>Failed Tests.
Yours: [[0. 0.]
 [0. 0.]]
Spec : [[ 1  3]
 [ 3 13]]
</code></pre></div>
<p dir="auto">Test 2</p>
<div dir="auto" data-snippet-clipboard-copy-content="SIZE = 8
out = np.zeros((SIZE, SIZE))
inp1 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))
inp2 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE)).T

problem = CudaProblem(
    &#34;Matmul (Full)&#34;,
    mm_oneblock_test,
    [inp1, inp2],
    out,
    [SIZE],
    Coord(3, 3),
    Coord(TPB, TPB),
    spec=matmul_spec,
)
problem.show(sparse=True)"><pre><span>SIZE</span> <span>=</span> <span>8</span>
<span>out</span> <span>=</span> <span>np</span>.<span>zeros</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>inp1</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span> <span>*</span> <span>SIZE</span>).<span>reshape</span>((<span>SIZE</span>, <span>SIZE</span>))
<span>inp2</span> <span>=</span> <span>np</span>.<span>arange</span>(<span>SIZE</span> <span>*</span> <span>SIZE</span>).<span>reshape</span>((<span>SIZE</span>, <span>SIZE</span>)).<span>T</span>

<span>problem</span> <span>=</span> <span>CudaProblem</span>(
    <span>&#34;Matmul (Full)&#34;</span>,
    <span>mm_oneblock_test</span>,
    [<span>inp1</span>, <span>inp2</span>],
    <span>out</span>,
    [<span>SIZE</span>],
    <span>Coord</span>(<span>3</span>, <span>3</span>),
    <span>Coord</span>(<span>TPB</span>, <span>TPB</span>),
    <span>spec</span><span>=</span><span>matmul_spec</span>,
)
<span>problem</span>.<span>show</span>(<span>sparse</span><span>=</span><span>True</span>)</pre></div>
<div data-snippet-clipboard-copy-content="# Matmul (Full)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | "><pre><code># Matmul (Full)
 
   Score (Max Per Thread):
   |  Global Reads | Global Writes |  Shared Reads | Shared Writes |
   |             0 |             0 |             0 |             0 | 
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srush/GPU-Puzzles/blob/main/GPU_puzzlers_files/GPU_puzzlers_70_1.svg"><img src="https://github.com/srush/GPU-Puzzles/raw/main/GPU_puzzlers_files/GPU_puzzlers_70_1.svg" alt="svg"/></a></p>

<div data-snippet-clipboard-copy-content="Failed Tests.
Yours: [[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]
Spec : [[  140   364   588   812  1036  1260  1484  1708]
 [  364  1100  1836  2572  3308  4044  4780  5516]
 [  588  1836  3084  4332  5580  6828  8076  9324]
 [  812  2572  4332  6092  7852  9612 11372 13132]
 [ 1036  3308  5580  7852 10124 12396 14668 16940]
 [ 1260  4044  6828  9612 12396 15180 17964 20748]
 [ 1484  4780  8076 11372 14668 17964 21260 24556]
 [ 1708  5516  9324 13132 16940 20748 24556 28364]]"><pre><code>Failed Tests.
Yours: [[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]
Spec : [[  140   364   588   812  1036  1260  1484  1708]
 [  364  1100  1836  2572  3308  4044  4780  5516]
 [  588  1836  3084  4332  5580  6828  8076  9324]
 [  812  2572  4332  6092  7852  9612 11372 13132]
 [ 1036  3308  5580  7852 10124 12396 14668 16940]
 [ 1260  4044  6828  9612 12396 15180 17964 20748]
 [ 1484  4780  8076 11372 14668 17964 21260 24556]
 [ 1708  5516  9324 13132 16940 20748 24556 28364]]
</code></pre></div>
</article></div></div>
  </body>
</html>
