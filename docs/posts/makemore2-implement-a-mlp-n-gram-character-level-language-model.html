<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/makemore2-implement-a-mlp-n-gram-character-level-language-model/">Original</a>
    <h1>Makemore2 Implement a MLP N-gram Character Level Language Model</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>A look at episode #3: <a href="https://youtu.be/TCH_1BHY58I?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener noreffer ">The spelled-out intro to language modeling: building makemore Part 2: MLP</a> from <a href="https://karpathy.ai/" target="_blank" rel="noopener noreffer ">Andrej Karpathy</a> amazing tutorial series.</p>

<p>
  <iframe src="https://www.youtube.com/embed/TCH_1BHY58I" allowfullscreen="" title="YouTube Video"></iframe>
</p>

<p>It picks up where the previous makemore video ended. Going from a bigram character-level Language Model to an N-gram MLP character-level Language Model. Meaning: “given the last N characters embeddings, guess the next character”. It’s still trained on a list of names to produce new unique name-sounding words.</p>
<p>The lecture is inspired by <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener noreffer ">Bengio et al. 2003</a></p>
<h2 id="embeddings">Embeddings</h2>
<p>Makemore2 introduces a bunch of new notions. Starting with Embeddings. The first stage of the model take a character as one hot encoded vector, and convert it into a multi-dimensional vector representation. Bellow we can see each letters mapped into a 2D vector space.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/makemore2-implement-a-mlp-n-gram-character-level-language-model/embeddings.png" title="Embeddings" data-thumbnail="embeddings.png" data-sub-html="&lt;h2&gt;Two-dimensional characters embeddings&lt;/h2&gt;&lt;p&gt;Embeddings&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="embeddings.png" data-srcset="embeddings.png, embeddings.png 1.5x, embeddings.png 2x" data-sizes="auto" alt="embeddings.png"/>
    </a><figcaption>Two-dimensional characters embeddings</figcaption>
    </figure>
<p>Notice how the characters do not look uniformely distributted. Instead there are clusters of consonants and vowels. The model learned something about letters for our dataset and decided that <code>x</code>, <code>s</code>, and <code>t</code> were very similar, while <code>q</code> and <code>l</code> were big outliers.</p>
<h2 id="hyperparameters-and-training">Hyperparameters and Training</h2>
<p>This section introduce more rigor and for the first time we split our data into:</p>
<ul>
<li>~80% training (used to train the parameters)</li>
<li>~10% dev/validation (used to train the hyperparameters)</li>
<li>~10% test (used to validate the performance at the very end, this has to stay pure, it should be used as few as possible. Ideally you never touch it, and evaluate the loss on it a single time just before pushing your results)</li>
</ul>
<p>There’s some intuition on how to find a good Learning Rate and why you should use Learning Rate Decay later in the training process. And how to find bottlenecks in the training. If the model does not outperform the validation set with the training set it’s probably underfitting which might come from:</p>
<ul>
<li>The hidden layers are probably too small, we could get better performances by increasing the number of weights.</li>
<li>The embedding space has too few dimensions (i.e. we only used 2-dimensional embeddings).</li>
<li>It could also mean that we are using too small of a minibatch. Making the data too noisy for the gradient computation to converge.</li>
</ul>
<h2 id="recommended-reading-material-in-the-lecture">Recommended reading material in the lecture</h2>
<h3 id="pytorch-internals">PyTorch internals</h3>
<p><a href="http://blog.ezyang.com/2019/05/pytorch-internals/" target="_blank" rel="noopener noreffer ">PyTorch internals by Edward Z. Yang</a>. The bits relevant to this lesson are at the beginning of the blogpost: Why it’s easy to make zero-cost copy in PyTorch using views. Because views are just a thin wrapper over the data with a size (dimensions), and stride (how to skip to the next element). The rest of the blogpost is a bit overkill unless you are planning to contribute to the PyTorch codebase.</p>
<h3 id="paper-a-neural-probabilistic-language-model">Paper: A neural Probabilistic Language Model</h3>
<p><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener noreffer ">Bengio et al. 2003</a></p>
<h2 id="gotcha-in-the-video-softmax-logits-can-be-arbitrarily-offset-ed">Gotcha in the video: Softmax logits can be arbitrarily offset-ed</h2>
<p>Andrej glance over the fact in the video. But it wasn’t intuitive to me so here’s a spelled out version of it.</p>
<p>$Softmax([x, y, z]) = Softmax([x + k, y + k, z + k])$</p>
<p>for a reminder softmax is: $Softmax(x) = {e^x \over {\sum_i e^i}}$</p>
<p>or in code:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>torch</span>
</span></span><span><span>
</span></span><span><span><span>for</span> <span>k</span> <span>in</span> <span>range</span><span>(</span><span>-</span><span>10</span><span>,</span> <span>10</span><span>):</span>
</span></span><span><span>    <span># offsetting the logits by `k`</span>
</span></span><span><span>    <span>logits</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span> <span>+</span> <span>k</span>
</span></span><span><span>    <span>count</span> <span>=</span> <span>logits</span><span>.</span><span>exp</span><span>()</span>
</span></span><span><span>    <span>probs</span> <span>=</span> <span>count</span> <span>/</span> <span>count</span><span>.</span><span>sum</span><span>()</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#39;</span><span>{</span><span>k</span><span>=}</span><span>\t</span><span> </span><span>{</span><span>probs</span><span>=}</span><span>&#39;</span><span>)</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span># k=-10	 probs=tensor([0.0900, 0.2447, 0.6652])</span>
</span></span><span><span><span># k=-9	 probs=tensor([0.0900, 0.2447, 0.6652])</span>
</span></span><span><span><span># ...</span>
</span></span><span><span><span># k=0	 probs=tensor([0.0900, 0.2447, 0.6652])</span>
</span></span><span><span><span># k=1	 probs=tensor([0.0900, 0.2447, 0.6652])</span>
</span></span><span><span><span># ...</span>
</span></span><span><span><span># k=8	 probs=tensor([0.0900, 0.2447, 0.6652])</span>
</span></span><span><span><span># k=9	 probs=tensor([0.0900, 0.2447, 0.6652])</span>
</span></span></code></pre></div><p>$$Softmax(x + k) = {e^{x + k} \over \sum {e^{i}}}$$
$$= {e^{x + k} \over e^{x + k} + e^{y + k} + e^{z + k}}$$
$$= {e^{x} e^{k} \over e^{x} e^{k} + e^{y} e^{k} + e^{z} e^{k}}$$
$$= {e^{x} e^{k} \over (e^{x} + e^{y} + e^{z}) e^{k}}$$
$$= {e^{x} \over e^{x} + e^{y} + e^{z}}$$</p>
<p>The <code>k</code> cancel out so we can safely shift the values of the Softmax logits to prevent floating arithmetic error that would be caused by computing high exponential values e.g.:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>will_overflow</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>1</span><span>,</span> <span>10</span><span>,</span> <span>100</span><span>])</span><span>.</span><span>exp</span><span>()</span>
</span></span><span><span><span>will_not_overflow</span> <span>=</span> <span>(</span><span>torch</span><span>.</span><span>tensor</span><span>([</span><span>1</span><span>,</span> <span>10</span><span>,</span> <span>100</span><span>])</span> <span>-</span> <span>100</span><span>)</span><span>.</span><span>exp</span><span>()</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;</span><span>{</span><span>will_overflow</span><span>=}</span><span>&#39;</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;</span><span>{</span><span>will_not_overflow</span><span>=}</span><span>&#39;</span><span>)</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span># will_overflow=tensor([2.7183e+00, 2.2026e+04,        inf])</span>
</span></span><span><span><span># will_not_overflow=tensor([1.0089e-43, 8.1940e-40, 1.0000e+00])</span>
</span></span></code></pre></div><p>Note the <code>inf</code> for <code>e**100</code> while the second vector is perfectly fine.</p>
<h2 id="the-code">The code</h2>
<p>Here’s my take on the tutorial with additional notes. You can get the code on <a href="https://github.com/peluche/makemore" target="_blank" rel="noopener noreffer ">GitHub</a> or bellow.</p>


</div></div>
  </body>
</html>
