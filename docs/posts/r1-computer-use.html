<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/agentsea/r1-computer-use">Original</a>
    <h1>R1 Computer Use</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Applying the ideas of <a href="https://github.com/deepseek-ai/DeepSeek-R1">Deepseek R1</a> and <a href="https://github.com/huggingface/open-r1">Open R1</a> to computer use.</p>

<p dir="auto">r1-computer-use is an experimental project that applies large-scale Reinforcement Learning techniques similar to DeepSeek-R1 to computer usage scenarios. The primary goal is to train an agent to interact with a computer environment (e.g., file system, web browser, command line) while utilizing a neural reward model to validate the correctness of the agent’s actions and reason about intermediate steps.</p>

<p dir="auto">DeepSeek-R1 has shown that large language models can develop powerful reasoning skills through iterative reward optimization. Traditionally, such projects rely on hard verifiers or rule-based scripts to determine correctness in tasks like math or coding. However, these methods are too difficult to reproduce at scale for general computer usage.</p>
<p dir="auto">We aim to replace hard-coded verifiers with a neural reward model that itself reasons about whether or not the agent’s actions are correct or helpful.</p>
<p dir="auto">Both the actor and reward models follow a three-step cycle which can be seen as an extention of <a href="https://react-lm.github.io/" rel="nofollow">ReACT</a> into reinforcement learning.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/agentsea/r1-computer-use/blob/main/static/rac.svg"><img src="https://github.com/agentsea/r1-computer-use/raw/main/static/rac.svg" alt="diagram" width="500"/></a></p>

<div dir="auto" data-snippet-clipboard-copy-content="observation = &#34;Current directory contains: setup.py requirements.txt&#34;
reasoning = &#34;&#34;&#34;
1. Project appears to be a Python package
2. No virtual environment detected
3. Should create venv before proceeding
&#34;&#34;&#34;
action = &#34;python -m venv .venv&#34;"><pre><span>observation</span> <span>=</span> <span>&#34;Current directory contains: setup.py requirements.txt&#34;</span>
<span>reasoning</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>1. Project appears to be a Python package</span>
<span>2. No virtual environment detected</span>
<span>3. Should create venv before proceeding</span>
<span>&#34;&#34;&#34;</span>
<span>action</span> <span>=</span> <span>&#34;python -m venv .venv&#34;</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="analysis = &#34;&#34;&#34;
1. Correctly identified project type
2. Appropriate prerequisite check
3. Standard venv location chosen
&#34;&#34;&#34;
reward = 0.85"><pre><span>analysis</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>1. Correctly identified project type</span>
<span>2. Appropriate prerequisite check</span>
<span>3. Standard venv location chosen</span>
<span>&#34;&#34;&#34;</span>
<span>reward</span> <span>=</span> <span>0.85</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="from r1_computer_use import Agent, RewardModel

agent = Agent()
reward_model = RewardModel()

result = agent.run(
    task=&#34;Set up Python development environment&#34;,
    observe_reasoning=True
)

feedback = reward_model.evaluate(
    actions=result.actions,
    reasoning=result.reasoning
)"><pre><span>from</span> <span>r1_computer_use</span> <span>import</span> <span>Agent</span>, <span>RewardModel</span>

<span>agent</span> <span>=</span> <span>Agent</span>()
<span>reward_model</span> <span>=</span> <span>RewardModel</span>()

<span>result</span> <span>=</span> <span>agent</span>.<span>run</span>(
    <span>task</span><span>=</span><span>&#34;Set up Python development environment&#34;</span>,
    <span>observe_reasoning</span><span>=</span><span>True</span>
)

<span>feedback</span> <span>=</span> <span>reward_model</span>.<span>evaluate</span>(
    <span>actions</span><span>=</span><span>result</span>.<span>actions</span>,
    <span>reasoning</span><span>=</span><span>result</span>.<span>reasoning</span>
)</pre></div>

<p dir="auto">The training pipeline consists of multiple stages:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Cold Start</strong></p>
<ul dir="auto">
<li>Expert demonstrations with reasoning traces</li>
<li>Initial reward model training</li>
<li>Base model fine-tuning</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Reasoning-Focused GRPO</strong></p>
<ul dir="auto">
<li>Group-based sampling from current policy</li>
<li>Reward model evaluates each group</li>
<li>Compute advantages within groups</li>
<li>Policy updates with clipped probability ratios</li>
<li>KL divergence constraint with reference policy</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Rejection Sampling Stage</strong></p>
<ul dir="auto">
<li>Filter top-k solutions based on reward model</li>
<li>Create new training dataset from best examples</li>
<li>Fine-tune base model on filtered data</li>
</ul>
</li>
<li>
<p dir="auto"><strong>General Preference Alignment</strong></p>
<ul dir="auto">
<li>Apply RL to full task distribution</li>
<li>Use reward models for general preferences</li>
<li>Focus on helpfulness and safety</li>
<li>Evaluate complete responses</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Evaluation</strong></p>
<ul dir="auto">
<li>Task completion metrics</li>
<li>Reasoning quality assessment</li>
<li>Safety verification</li>
<li>Distribution shift analysis</li>
</ul>
</li>
</ol>

<ul>
<li> Collect cold startand neural reward model data (in progress)</li>
<li> SFT train base model</li>
<li> GRPO RL training</li>
<li> Rejection sampling</li>
<li> General preference alignment</li>
<li> Evaluation</li>
</ul>

<p dir="auto">Current areas of investigation:</p>
<ul dir="auto">
<li>Reward model architectures</li>
<li>Base model evaluations</li>
</ul>

<p dir="auto">MIT</p>

<div dir="auto" data-snippet-clipboard-copy-content="@software{r1_computer_use,
  title     = {R1-Computer-Use: Reasoning-First Computer Interaction},
  author    = {Barker, Patrick},
  year      = {2025},
  url       = {https://github.com/agentsea/r1-computer-use},
}"><pre><span>@software</span>{<span>r1_computer_use</span>,
  <span>title</span>     = <span><span>{</span>R1-Computer-Use: Reasoning-First Computer Interaction<span>}</span></span>,
  <span>author</span>    = <span><span>{</span>Barker, Patrick<span>}</span></span>,
  <span>year</span>      = <span><span>{</span>2025<span>}</span></span>,
  <span>url</span>       = <span><span>{</span>https://github.com/agentsea/r1-computer-use<span>}</span></span>,
}</pre></div>
</article></div></div>
  </body>
</html>
