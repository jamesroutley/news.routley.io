<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.peterstefek.me/nerf.html">Original</a>
    <h1>Learn You a NeRF</h1>
    
    <div id="readability-page-1" class="page"><div>
 <div>
  
  <p><label>Posted on <strong>10 January 2023</strong></label></p><p><em>My NeRF Implementation which generated all the renderings for this post is on <a href="https://github.com/Mr4k/NeRFImpl">github</a> and you can play with it in a minimal <a href="https://colab.research.google.com/drive/1Z5QlXSBfYhF1VNBP1uUA5RTuTqqngxvq?usp=sharing">colab</a></em>    </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/excavator.png" width="100%"/> 
</p>

<p><strong>A Computer Vision Problem</strong></p>
<p>The inverse problem, turning an image or collection of images into a 3d scene, is also very interesting. Up until recently these problems were solved in unrelated ways but new techniques called inverse graphics have tied the two problems together. <a href="https://www.matthewtancik.com/nerf">NeRF</a> (Neural Radiance Fields) is a wildly popular<sup id="sf-nerf-2-back"><a href="#sf-nerf-2" title="Over 50 papers derived from NeRF were submitted to CVPR in 2022. This number is courtesy of Frank Dellart ">2</a></sup> inverse graphics technique which beautifully formulates the problem in terms of forward rendering.  </p>
<p><strong>Forward Rendering via Ray Tracing</strong></p>
<p>The basic idea behind ray tracing is to use a virtual pinhole camera to capture imaginary light rays and turn them into an image. This approximates how cameras work in real life. Below is a diagram of an ideal pinhole camera:  </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/camdiagram1.png" width="70%"/> 
</p>
<p>Light from the sun or other light source bounces off of an object and into the pinhole of the camera projecting the object upside down. In our virtual pinhole camera we usually assume the pinhole is infinitely small and has sufficient light exposure so there is no need for a lens and therefore no depth of field effects.  </p>
<p>An important notational point is that graphics programmers prefer to talk about rendering in terms of an &#34;eye&#34; placed at the pinhole and an &#34;image plane&#34; placed in front of the camera one focal length<sup id="sf-nerf-3-back"><a href="#sf-nerf-3" title="This is kind of a confusing term in a pinhole camera model since there no lens, hence no &#34;focus&#34;. However I can&#39;t find a better name for this distance.">3</a></sup> unit away. This has no real life analog but is the right side up version of the image captured by the pinhole camera so it is easier to work with. Here is a diagram of the image plane:</p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/camdiagram2.png" width="70%"/> 
</p>
<p>One way you might try to render an image would be to sample rays of light from the light sources in the scene (the imaginary sun or other lights) and bounce them around until you have accumulated enough inside the pinhole camera to see a finished image. However given the fact that our virtual pinhole is infinitely tiny this would take forever. Even if it wasn&#39;t this would be super inefficient as most samples would miss the camera and therefore be useless. </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/camdiagram3.png" width="50%"/> 
</p>
<p>Instead of casting rays from the light source to the camera, we can be more efficient by casting &#34;reverse&#34; rays out from the camera. We can do this because we know that the color of each point<sup id="sf-nerf-4-back"><a href="#sf-nerf-4" title="I maintain this is technically true for a single point. However in ray tracing we are estimating the color for an entire pixel which is a rectangle, therefore serious ray tracers use techniques such as Monte Carlo sampling to sample many points inside each pixel&#39;s rectangle and merge them together to get a final color">4</a></sup> on the image plane is uniquely determined by the light coming into the camera&#39;s pinhole from a certain direction. These reverse rays will bounce around the scene until they hit a light source. By the time they have done that they will have followed the exact paths that the incoming light ray from that source would follow and therefore we can determine how much energy is left when that ray of light finally hits our sensor<sup id="sf-nerf-5-back"><a href="#sf-nerf-5" title="If you are scratching your head at how this is accomplished you are absolutely correct. In fact I&#39;d be a little worried if you weren&#39;t confused. The answer to this question is incredibly complex and a good chunk of the subfield of computer graphics called Physically Based Rendering is dedicated to creating more and more accurate approximations. A great place to start is the Pbr book">5</a></sup>.  </p>
<p>One important property of this version of ray tracing I want to highlight is that we can terminate &#34;reverse&#34; rays early and make an approximation of the amount of light coming from the remainder of the tracing process instead of continuing to trace. All ray tracers I&#39;m aware of do this to some extent. Usually there is a max bounce depth before early termination.  </p>
<p>The simplest form of this is to terminate immediately when hitting the first object instead of doing any bouncing and make a guess as to the amount of light hitting that object at that particular angle. This is the type of ray tracing NeRF relies on.  </p>
<p><strong>So what is NeRF anyway?</strong></p>
<p>NeRF represents the 3d scene with a so-called radiance field. A radiance field can be viewed as two different functions <span>\(\mathbf{\sigma(pos)}\)</span> and <span>\(\mathbf{c(pos, dir)}\)</span> which together represent a volume.  </p>
<p><span>\(\mathbf{\sigma(pos)}\)</span> is a function which takes a position in 3d space and returns the probability density of encountering a particle at that location. This quantity is sometimes called the attenuation but the NeRF authors refer to it as the volume density.  </p>
<p><span>\(\mathbf{c(pos, dir)}\)</span> is a function which takes a position in 3d space and a direction then returns the color of light reflecting off that particle. The return type of <span>\(\mathbf{c(pos, dir)}\)</span> is a 3 component RGB vector with components from <span>\([0, 1]\)</span>.  </p>
<p>One thing you might notice is that <span>\(\mathbf{\sigma(pos)}\)</span> does not depend on the direction of the ray while the color function does. This is an explicit choice by the NeRF authors, who say that the angle at which you look at an object should not determine whether or not it is there. In contrast the author&#39;s acknowledge that the color of the object can be influenced by the ray direction especially if its surface is reflective. For example, consider a mirror, you can look at the same part of it from different angles and you will see different images.  </p>
<p>To create a 2d view from this radiance field the authors use a forward rendering technique from an area of computer graphics called volume rendering<sup id="sf-nerf-6-back"><a href="#sf-nerf-6" title="I&#39;m not going to go into detail about volume rendering here (mostly due to my own ignorance). Please look at Scratchapixel&#39;s volume rendering section or this chapter of the pbr book for a great introduction">6</a></sup>.  </p>
<p><strong>Tracing Rays</strong>  </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/eye-tracing2.png" width="70%"/> 
</p>
<p>To determine the color of a given pixel in the image plane we take its associated &#34;reverse&#34; ray and trace it through the radiance field.   </p>
<p>The main difference between rendering the radiance field and the simple ray tracing we talked about previously is that unlike the simple ray tracing where objects are solid, here we are dealing with objects made of many tiny particles. In this case collisions at any particular point in space are approximated by a probability that they will occur instead of actually computing an intersection test.   </p>
<p>Each of the rays we trace passes through the field until it hits a particle (decided by weighted coin flip based on the probability of intersection at each point along the ray) and finally terminates, returning the color reflecting off of that particle towards the camera. Notice there are no light bounces modeled here. As mentioned in the Forward Rendering section the color of the radiance field is serving as an approximation of the light reflected off the particle after the first collision.   </p>
<p>Because collisions with the radiance field are probabilistic, casting the same ray over and over again in the same direction will give different colors each time. So instead of casting just one ray per point in the image plane to find its color, we compute the expected color over all possible rays shot from the eye in that particular direction. Since authors don&#39;t need to deal with bounces the expected color is fairly simple to compute.  </p>
<p>Given the radiance field function, <span>\(\mathbf{\sigma(pos)}\)</span> and <span>\(\mathbf{c(pos, dir)}\)</span>, the expected color for a particular point on the image plane is given by the following equation: </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/math-eqn-1.png" width="90%"/> 
</p>
<p>When you understand all the notation this integral is not as scary as it first appears. We are simply computing the expected color of a ray starting from initial position o and direction d going through the radiance field given by <span>\(\mathbf{\sigma(pos)}\)</span> and <span>\(\mathbf{c(pos, dir)}\)</span>.  </p>
<p><span>\(\mathit{T(t)}\)</span> is the probability that the ray has not terminated earlier before point <span>\(\mathbf{r(t)}\)</span>. <span>\(\mathit{T(t)}\)</span> is calculated using Beer&#39;s law. Scratchapixel has a derivation of Beer&#39;s law using differential equations in their <a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/volume-rendering-for-developers/volume-rendering-summary-equations">volume rendering section</a>.  </p>
<p><span>\(t_n\)</span> is the location along the ray of the near plane (the image plane)</p>
<p>Of course we cannot perfectly calculate the above integral so the NeRF authors choose to approximate it by breaking it up into a sum of a finite number of bins described as follows:  </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/math-eqn-2.png" width="90%"/> 
</p>

<p><strong>Learning the radiance field</strong></p>
<p>Knowing this let&#39;s look again at our radiance field functions <span>\(\mathbf{\sigma(pos)}\)</span> and <span>\(\mathbf{c(pos, dir)}\)</span>. We&#39;re going to now refer to them as <span>\(\mathbf{\sigma(pos; W)}\)</span> and <span>\(\mathbf{c(pos, dir; W)}\)</span>. Where <span>\(W\)</span> represents the parameters of the underlying neural network approximating the radiance field.   </p>
<p>Now let&#39;s consider the expected color along a ray <span>\(\hat{C}(r)\)</span>, from the previous section, which we will now denote <span>\(\hat{C}(r; W)\)</span> to explicitly show its dependence on the neural network parameters <span>\(W\)</span>. Suppose we also know the true color along that particular ray (from a ground truth photograph) denoted by <span>\(ground\_truth\_color(r)\)</span>.   </p>
<p>We can create a loss function between the two values:<br/>
</p>
<p>$$L(r; W) = (\hat{C}(r; W) - ground\_truth\_color(r))^2$$</p>
<p>
Notice that this loss function is differentiable with respect to <span>\(W\)</span><sup id="sf-nerf-7-back"><a href="#sf-nerf-7" title="This is a lie for some parts of the function because the neural network in the original paper contains ReLUs but gradient descent still works in practice">7</a></sup>, the parameters of the neural network. This means that we can use gradient descent to minimize this loss function. If we minimize the sum of the loss over all rays for all known views we can learn a radiance field that faithfully represents the underlying 3d scene. This is the core idea behind NeRF.</p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/traindiagram.png" width="90%"/> 
</p>
<p>The training process for nerf is pictured above. The basic idea is that for each of our known cameras we can render an image using our current network as the radiance field and then compare the rendered image to the ground truth reference image from the camera. We then take the derivative of the loss with respect to the network parameters<sup id="sf-nerf-8-back"><a href="#sf-nerf-8" title="No derivative computations are required here because Pytorch&#39;s autodifferentiation does it for us!">8</a></sup> and perform gradient descent to update the network for the next iteration.  </p>
<p>In practice rendering an entire image per iteration can be expensive. Instead the authors sample a subset of rays and corresponding pixel colors from each known camera image and roll them all up into a batch. The batch size in the original NeRF paper is 4096 rays. This type of batching is a common technique in deep learning.  </p>
<p>It&#39;s also important to realize that in the NeRF paper, the neural network learned is tailored to a single scene only.  </p>
<p>Here are some validation views during the learning process. Note that none of these images are fed into the network at train time, it has never seen the scene from these orientations before:  </p>
<p><img src="https://www.peterstefek.me/images/nerf/lego_view1_truth.png"/>
        <img src="https://www.peterstefek.me/images/nerf/lego_view1.gif"/>
        <img src="https://www.peterstefek.me/images/nerf/lego_view1_depth.gif"/>
    </p> 

<p><img src="https://www.peterstefek.me/images/nerf/ficus_truth.png"/>
        <img src="https://www.peterstefek.me/images/nerf/ficus.gif"/>
        <img src="https://www.peterstefek.me/images/nerf/ficus_depth.gif"/>
    </p> 

<div>
        <p> ground truth </p>
        <p> color prediction </p>
        <p> depth prediction </p>
    </div> 

<p><strong>Network Architecture</strong></p>
<p>The network described in the original NeRF paper is a simple feedforward architecture.  </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/networkdiagram.png" width="90%"/> 
</p>
<p>As described in the paper, each black arrow in this diagram represents a connection between layers with a ReLU in between. The dashed arrow is an element wise sigmoid activation and the orange arrow indicates no activation function.   </p>
<p>The inputs to the network are the green boxes and the outputs are red boxes.  </p>
<p>Initially the position (a vector embedding of size 60) is input at the very beginning of the network and then added in again at layer 5 by concatenating it with the output of layer 4. The idea behind this is that it helps the network &#34;remember&#34; the position in later layers. This is inspired by the skip connections in architectures such as <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>.   </p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/layer9_closeup3.png" width="70%"/> 
</p>

<p>At layer 9 the scalar volume density value <span>\(\sigma(pos)\)</span> is output and a ReLU is applied to it to make it non negative<sup id="sf-nerf-9-back"><a href="#sf-nerf-9" title="One problem with rectifying the volume density with ReLU is that sometimes when training the network the ReLU will start &#34;dead&#34; (with a \(value \le 0\)) at which point the gradient is 0. The network will then make no progress producing all black images. The NeRF authors somewhat solve this problem by adding gaussian noise to the ReLU on their &#34;real world&#34; images but I&#39;m not sure it&#39;s addressed fully.">9</a></sup>. Afterwards the direction embedding (a vector of size 24) is concatenated with the output of layer 9 minus the volume density and input into layer 10 which eventually outputs a 3 element vector representing the r, g and b pieces of the output color at that point looking at that direction.  </p>
<p>Very importantly the volume density is output before the direction is added into the network so the volume density remains independent of direction.  </p>
<p><strong>High frequency embeddings</strong></p>
<p>The embedding the authors choose to use for single number p to a point in space of dimension <span>\(2L\)</span> is:</p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/embedding.png" width="90%"/> 
</p>
<p>The authors apply this embedding function to each of the 3 dimensions of the input vector separately and concatenate the result. They do this for position with <span>\(L=10\)</span> and direction with <span>\(L=4\)</span>.  </p>
<p>Importantly when the position is input into this embedding function it has been scaled to be in <span>\([-1, 1]\)</span>.  </p>
<p>The reason for this embedding is that according to the nerf authors, neural networks are prone to learn low frequency functions. They cite <a href="https://arxiv.org/abs/1806.08734">Rahaman et al</a> to back up their claim who also proposes this frequency embedding to remedy the problem. An alternative reason to use frequency embeddings is that the structure of neural networks with only ReLU non-linearities limits the resolution by dividing the scene into polytopes which individually cannot represent non linear details<sup id="sf-nerf-10-back"><a href="#sf-nerf-10" title="Pure ReLU networks are just the piecewise linear functions over the input space. In the original NeRF paper, the volume density calculated without embeddings is represented by a pure ReLU network. Without any embeddings all the network does to determine the volume density is divide the space into polytopes and do a polytope specific linear prediction based on the x, y and z coordinates to determine the depth. This means non linear details cannot be captured inside a polytope, limiting the resolution. Adding sin and cos makes the network non piecewise linear.">10</a></sup>.   </p>
<p><strong>Hierarchical Sampling</strong></p>
<p>
    <img src="https://www.peterstefek.me/images/nerf/inverse_transform_sampling.png" width="90%"/> 
</p>
<p>The original paper does this by training two different networks. One &#34;coarse&#34; network and one &#34;fine&#34; network. The coarse network allocates samples using the original bin strategy from our <strong>Tracing Rays</strong> section. Then the fine network allocates its samples to the same bins based on the relative likelihood of hitting a particle in each bin. The NeRF authors call this inverse transform sampling.  </p>
<p>Here&#39;s an example of the images rendered using each of the two networks:</p>
<p><img src="https://www.peterstefek.me/images/nerf/coarse_lego_view1.png"/>
        <img src="https://www.peterstefek.me/images/nerf/fine_lego_view1.png"/>
    </p> 
    <p><img src="https://www.peterstefek.me/images/nerf/coarse_lego_view2.png"/>
        <img src="https://www.peterstefek.me/images/nerf/fine_lego_view2.png"/>
    </p> 
    <p><img src="https://www.peterstefek.me/images/nerf/coarse_lego_view3.png"/>
        <img src="https://www.peterstefek.me/images/nerf/fine_lego_view3.png"/>
    </p> 
    <p><img src="https://www.peterstefek.me/images/nerf/coarse_ficus_view1.png"/>
        <img src="https://www.peterstefek.me/images/nerf/fine_ficus_view1.png"/>
    </p> 

<div>
        <p> 64 coarse network samples </p>
        <p> 64 fine network samples </p>
    </div> 

<p>Notice how the coarse network boundaries (left) are fuzzier than the fine network boundaries (right). This is because we are sampling more points around the boundaries from the fine network.  </p>
<p>An important implementation detail is the loss from both networks are summed together into the objective function.  </p>
<p><strong>Limitations of the original NeRF Paper</strong>  </p>
<p><strong>Train time</strong></p>
<p><strong>Rigidity assumption</strong></p>
<p>One scenario which clearly violates this assumption is reconstructing a moving subject such as a person walking in a video. Works such as <a href="https://video-nerf.github.io/">Video NeRF</a> try to tackle this problem.  </p>
<p>Another slightly more subtle scenario where violations of the rigidity assumption causes problems is when trying to reconstruct static real world objects (such as famous buildings or statues) from crowd sourced pictures. Some violations here include:</p>
<ul>
<li>The subject of the photo will be different colors during different times of day.</li>
<li>Different cameras have different color ranges and tone mapping</li>
<li>People, cars or other dynamic objects will occlude different part of the object in different photos  </li>
</ul>
<p>Researchers from Google address these problems in <a href="https://nerf-w.github.io/">NeRF in the Wild</a>. The basic idea is that they learn different networks representing the static details (things that don&#39;t change between photos) and transient details in each photo.  </p>
<p>By explicitly separating out the static and transient details they can reconstruct objects much more accurately. They also are able to do neat things like arbitrarily change the position of the sun while rendering.   </p>
<p><strong>Structural Camera Errors</strong></p>
<p>One big assumption is to totally disregard depth of field. It&#39;s very possible that depth of field error can be modeled as per camera noise and disregarded by the techniques used in NeRF in the Wild (discussed above) but DoF information could also provide useful clues to estimate the scene. Wu et al build a <a href="https://github.com/zijinwuzijin/DoF-NeRF">NeRF which incorporates DoF</a>. They seem to find that it produces sharper radiance fields than vanilla NeRF when used on images with shallow depth of field. However I don&#39;t see them comparing their technique to <em>NeRF in the Wild</em> which would be nice. Other papers do similar types of camera corrections such as <a href="http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/">AR NeRF</a> </p>
<p><a href="https://limacv.github.io/deblurnerf/">Deblur NeRF</a> models a camera with non instantaneous shutter speed to reconstruct radiance fields from motion blurred images more accurately.  </p>
<p><a href="https://jonbarron.info/mipnerf/">MIP NeRF</a> integrates the radiance field over cones instead of rays (it uses multiple samples per pixel) to more accurately capture the underlying field radiance.  </p>
<p><strong>Limits of the radiance field</strong></p>
<p><strong>My Own Journey Implementing NeRF</strong></p>
<p>Coding NeRF was an interesting experience. I suspect some of the differences in implementation stemmed from the fact I approach the problem as a programmer trying to break a known algorithm into independent testable pieces rather than a researcher discovering something for the first time (also I am not as good at pytorch so my code is less concise but that&#39;s way less fun to say).  </p>
<p>Here&#39;s a quick summary of what I did:</p>
<ol>
<li>Created a simple test scene in Blender, a cube with different colored sides<sup id="sf-nerf-13-back"><a href="#sf-nerf-13" title="Fun Fact: I used ChatGPT to help me write this Blender script because I have never used Blender and it was not a focus of this project">13</a></sup></li>
<li>Created basic sampling and ray tracing functions, adding many simple tests.</li>
<li>Made some more complex tests where I rendered a full scene with coarse sampling. For these tests I mocked out the neural network with a hand crafted radiance field representing the same cube I had made in Blender earlier and compared the images my code rendered with the &#34;ground truth&#34; Blender images</li>
<li>Created the neural network and made sure all the tensor dimensions were correct</li>
<li>Tried to learn the cube. This did not work</li>
<li>Did a simple test to see if I could learn a field of all one solid color. That worked</li>
<li>Tried to learn an unlit sphere and found my bug</li>
<li>Rendered the cube successfully on my laptop</li>
<li>Experimented using colab to render on a gpu and realized I hadn&#39;t added any gpu support</li>
<li>Added gpu support</li>
<li>Rendered the lego excavator (resized to 200x200) model on a colab gpu</li>
<li>Implemented the hierarchical network to get better image quality</li>
<li>Rendered the lego model again</li>
<li>Profiled and sped up my implementation<sup id="sf-nerf-14-back"><a href="#sf-nerf-14" title="Just enough to run at about the same speed as the original paper, speed wasn&#39;t a priority for this project.">14</a></sup></li>
<li>Ran a longer run on the lego excavator (again resized to 200x200) on Lambda A100 GPU ($1.10 an hour) for 3 hours</li>
</ol>
<p>My goal was to build the simplest and most testable parts first, slowly piece them together and finally build and add the neural network itself. The nice thing about rigorously testing each piece was that when I could not learn a sphere in step 7 I knew that the bug had to be in the neural network not in my ray tracing code.   </p>
<p>I also got a lot of mileage out of just testing on my laptop before moving to the more complex world of gpus and colab. This allowed me not to worry about the complexities of gpus up front.  </p>
<p><strong>Conclusion</strong></p>
<p><em>Thanks to <a href="https://www.afox.land">Alex Fox</a>, Laura Lindzey as well as several others for amazing feedback</em>  </p>
<p>Have questions / comments / corrections?</p>
<hr/>

 </div>
</div></div>
  </body>
</html>
