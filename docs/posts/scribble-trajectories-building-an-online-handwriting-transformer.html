<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://alexagreenberg.com/ml/robotics/transformers/2025/02/25/handwriting.html">Original</a>
    <h1>Scribble trajectories: building an online handwriting transformer</h1>
    
    <div id="readability-page-1" class="page"><article>
    

    <div><h2 id="introduction">Introduction</h2>

<p>At the end of last year, I spent <strong>months at Recurse Center</strong> diving deep into theory and state-of-the-art research (EBMs, VAEs, transformers, diffusion models, RL, etc) while revisiting the math-heavy fundamentals of machine learning. Now, I’m back to building.</p>

<p>I found <strong>online handwriting generation</strong> — predicting pen strokes in real time — to be a great exploration for <strong>robotic trajectory modeling</strong>. A robot follows continuous motor commands (e.g. Δx and Δy) plus discrete events (e.g. gripper opens), just like handwriting sequences combine fine-grained movement (in a 2D trajectory) with pen lifts. And namely, I already had access to a great handwriting dataset and could train within a reasonable time (though I did upgrade to Colab Pro+).</p>

<p>My primary goal was to gain hands-on experience building a <strong>multimodal transformer</strong> end to end: from tokenizing data to training and visualizing results. My broader goal is to scale these ideas into <strong>large foundation models</strong> that can produce continuous movements, like robotic actions.</p>

<!--more-->

<p>I’ll go over:</p>

<ul>
  <li>How I represent <strong>Δx, Δy, pen lift</strong> events</li>
  <li>Why a <strong>discrete transformer</strong> worked best</li>
  <li>Shifting to <strong>multitask, multimodal</strong> modeling</li>
  <li>Key debugging tricks (attention visualization, mini-datasets)</li>
  <li>Ties to <strong>robotic transformers</strong> like RT-Trajectory</li>
  <li>Future directions toward <strong>foundation models</strong> with text and continuous movements</li>
</ul>

<hr/>

<h2 id="where-robotics-fits-in">Where robotics fits in</h2>

<h5 id="connecting-to-sota-robotics-transformers">Connecting to SOTA robotics transformers</h5>

<ol>
  <li>
    <p><strong>RT-2</strong></p>

    <ul>
      <li><strong>What it is:</strong> <a href="https://arxiv.org/pdf/2307.15818">RT-2</a> from Google DeepMind treats robot actions as text tokens. That way, it can take any vision-language model and fine-tune it into a vision-language-action model. During inference, the text tokens are decoded into continuous robot actions for closed-loop control.</li>
      <li><strong>Why it relates:</strong> This approach views motor commands as another language, just like how strokes could become discrete tokens in a handwriting language.</li>
    </ul>
  </li>
  <li>
    <p><strong>RT-Trajectory</strong></p>

    <ul>
      <li><strong>What it is:</strong> <a href="https://arxiv.org/pdf/2311.01977">RT-Trajectory</a>, also from DeepMind, uses trajectories as a way to specify robot tasks. It captures similarities between motions across different tasks, improving generalization beyond language-only or image-only approaches.</li>
      <li><strong>Why it relates:</strong> Handwriting is already composed of 2D stroke movements, similar to these 2D trajectory sketches. By learning from 2D or 2.5D trajectory representations, RT-Trajectory shows how specifying a drawn path can yield robust policy generalization since it’s easier to interpolate or adapt new motions.</li>
    </ul>
  </li>
  <li>
    <p><strong>Pi-0</strong></p>

    <ul>
      <li><strong>What it is:</strong> <a href="https://arxiv.org/pdf/2410.24164v1">Pi-0</a>, from Physical Intelligence, integrates vision, language, and proprioception into a single embedding space. It uses action chunking and flow matching to model complex continuous actions at high frequencies up to 50 Hz.</li>
      <li><strong>Why it relates:</strong> Handwriting generation deals with trajectories at lower frequencies, but the principle is the same: refine trajectory generation in a globally coherent way.</li>
    </ul>
  </li>
  <li>
    <p><strong>HAMSTER</strong></p>

    <ul>
      <li>
        <p><strong>What it is:</strong> <a href="https://hamster-robot.github.io/paper.pdf">HAMSTER</a> is a <strong>hierarchical vision-language-action</strong> approach from NVIDIA/UW/USC that came out after the first draft of this blog post. It trains a large VLM to produce a coarse 2D path, then hands that path off to a smaller, 3D-aware policy for precise manipulation. Super relevant, so I’ll dig into this one more!</p>
      </li>
      <li>
        <p><strong>How trajectories help</strong>:</p>

        <ul>
          <li>
            <p>Learning in robotics often struggles because of:</p>

            <ul>
              <li><strong>Expensive on-robot data</strong>: Collecting huge labeled image-action datasets is time-consuming and costly.</li>
              <li><strong>Overly specialized policies</strong>: A monolithic vision-action model tends not to generalize well to new tasks or embodiments.</li>
              <li><strong>Inference frequency constraints</strong>: Large VLMs can’t always run at the frequency (e.g. 50Hz) needed for fine, real-time control.</li>
            </ul>
          </li>
          <li>
            <p>HAMSTER solves this by predicting a 2D path in the camera image, keeping the VLM <strong>embodiment agnostic</strong>. It doesn’t need any details about the robot shape or degrees of freedom: just marks where to move in 2D space. A smaller, specialized policy then converts that path into precise 3D actions for whatever robot is actually used.</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Leveraging off-domain data</strong>:</p>

        <ul>
          <li>
            <p>Because HAMSTER only needs 2D trajectories rather than explicit robot actions, it can fine-tune on cheaper data sources (e.g. action-free videos or physics simulations) using:</p>

            <ul>
              <li>Point tracking: track a person’s hand or object in a video</li>
              <li>Hand-sketching: a human sketches the path directly on an image</li>
              <li>Proprioceptive projection: known joint positions are projected onto a 2D camera view </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Why it relates:</strong> HAMSTER focuses on robotic manipulation, but 2D trajectories apply equally to handwriting. We can find endless images of text online but lack stroke-by-stroke datasets. If we just approximate or extract 2D strokes from those images, a large VLM could propose coarse strokes that a smaller policy refines into Δx, Δy commands.</p>
      </li>
    </ul>
  </li>
</ol>

<p>These approaches cover vision-language data, 2D trajectories, and a separation of coarse and fine control, and highlight how <strong>trajectory-based</strong> representation can better use foundation models for precise motion policies. Online handwriting is essentially a 2D trajectory plus a “lift pen” signal, so I thought about <strong>modeling my data in the way that a robot model might tokenize joint and gripper signals</strong>.</p>

<hr/>

<h2 id="model-development">Model development</h2>

<h4 id="stage-1-data-representation-and-early-models">Stage 1: data representation and early models</h4>

<h5 id="1-δx-δy-pen-lift-format">1) Δx, Δy, pen lift Format</h5>

<p>I started with a handwriting stroke dataset where each sequence consists of variable-length timesteps, each represented by three values:</p>

<ul>
  <li><strong>Δx</strong>: movement in x from the previous point</li>
  <li><strong>Δy</strong>: movement in y</li>
  <li><strong>pen lift</strong> ∈ {0, 1}: is the pen on paper or lifted?</li>
</ul>

<h5 id="2-gmm-approach">2) GMM approach</h5>

<p>Alex Graves’ <a href="https://arxiv.org/pdf/1308.0850">classic handwriting approach</a> uses an RNN and mixture density network (MDN) to model continuous stroke distributions.</p>

<details>
  <summary>Click for details on how I started with that approach</summary>
  
  <ol>
    <li><strong>MDN background</strong></li>
  </ol>

  <ul>
    <li><strong>Gaussian Mixture Model:</strong> A probabilistic approach assuming data arises from multiple Gaussian distributions, each with its own mean, variance, and mixture weight.</li>
    <li><strong>Mixture Density Network:</strong> A neural network that outputs the parameters of a GMM at each timestep, giving a continuous distribution over Δx and Δy.</li>
  </ul>

  <ol>
    <li>
      <p><strong>Why I started with this approach</strong></p>

      <ul>
        <li>I wanted to be able to capture continuous variability in handwriting, like subtle style differences or loopy letters.</li>
        <li>MDNs can represent multi-modal distributions, like multiple ways to draw the letter “a”.</li>
      </ul>
    </li>
    <li>
      <p><strong>Pitfalls</strong></p>

      <ul>
        <li><strong>Mode collapse:</strong> The MDN almost always converged to a single mixture component, producing monotonic diagonal movements—either up and to the right or down and to the left—regardless of text conditioning. This happened because it consistently picked the same Gaussian with a small variance range.</li>
        <li><strong>Too many hyperparameters:</strong> Balancing mixture weights (π), correlation (ρ), temperature, and entropy constraints required a lot of tuning. Since I was paying for my own training compute, I did not have the capacity to run so many experiments.</li>
        <li><strong>Unstable training:</strong> Subtle issues like exploding gradients or near-zero σ caused frequent NaNs or seemingly random outputs.</li>
      </ul>

      <p>It took time to dig into the model outputs and realize that mode collapse was causing this behavior. The model was always choosing the same Gaussian distribution, leading to repetitive, unnatural stroke patterns. This realization led me to shift toward discrete tokenization, which eliminated that unnatural diagonal stroke bias.</p>
    </li>
  </ol>

</details>


<ul>
  <li><strong>Tokenized approach fits transformers:</strong> Transformers excel at tokenized data, and binning Δx and Δy allowed me to use standard classification and cross-entropy.</li>
  <li><strong>Tokenization works with the data</strong>: Handwriting deltas span a small range, so discretizing them doesn’t sacrifice much resolution or smoothness.</li>
  <li><strong>No continuous sampling quirks:</strong> Inference just relies on picking the next token from a softmax distribution.</li>
  <li><strong>Easier debugging:</strong> Digging into stroke tokens or mispredicted pen-lift is way easier than debugging multi-dimensional Gaussians.</li>
</ul>

<h5 id="3-tokenizing-handwriting-strokes">3) Tokenizing handwriting strokes</h5>

<p>First, I generated <strong>discrete bins</strong> of Δx and Δy to tokenize the continuous values. From the raw values below, you can see how most movements in the dataset are small and clustered around zero, while larger movements are significantly less frequent.</p>

<p><img src="https://www.frontiernet.net/assets/images/image6.png" alt="Screenshot 6: Raw Δx, Δy Distribution"/></p>

<p>I implemented <strong>adaptive binning</strong>, processing Δx and Δy into 24 bins each, making it easy for the transformer to autoregressively predict the next best move.</p>

<details>
  <summary>Click for adaptive binning details</summary>
  

  <ul>
    <li>
      <p><strong>Fine resolution near zero</strong></p>
    </li>
    <li>
      <p><strong>Log-spaced tails</strong></p>
    </li>
    <li>
      <p><strong>Adaptive refinement</strong></p>

      <p>Computed bin edges after adaptive binning:</p>

      <p><img src="https://www.frontiernet.net/assets/images/image7.png" alt="Screenshot 7: Adaptive Binning Bin Edges"/></p>
    </li>
  </ul>

</details>


<p><img src="https://www.frontiernet.net/assets/images/image8.png" alt="Screenshot 8: Adaptive Binning Applied to Δx, Δy"/></p>

<p>Great! Now the model can just classify the next token with standard cross-entropy loss, rather than needing to regress to floating-point values.</p>

<h5 id="4-implementing-a-tokenizer">4) Implementing a tokenizer</h5>

<p>At first, I used <a href="https://github.com/openai/tiktoken">tiktoken</a>, the tokenizer for GPT-4 with a 100K subword vocabulary. When my model wouldn’t converge, the first “duh” moment was that it was obviously overkill for my simple dataset and use case. I wrote a custom ASCII tokenizer of just 96 characters.</p>

<details>
  <summary>Click to see that implementation</summary>

  <div><div><pre><code><span>class</span> <span>CharTokenizer</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>pad_token</span> <span>=</span> <span>&#34;[PAD]&#34;</span>
        <span>self</span><span>.</span><span>unk_token</span> <span>=</span> <span>&#34;[UNK]&#34;</span>

        <span>ascii_punct</span> <span>=</span> <span>string</span><span>.</span><span>punctuation</span>
        <span>chars</span> <span>=</span> <span>(</span>
            <span>list</span><span>(</span><span>string</span><span>.</span><span>ascii_lowercase</span><span>)</span>
            <span>+</span> <span>list</span><span>(</span><span>string</span><span>.</span><span>ascii_uppercase</span><span>)</span>
            <span>+</span> <span>list</span><span>(</span><span>string</span><span>.</span><span>digits</span><span>)</span>
            <span>+</span> <span>list</span><span>(</span><span>ascii_punct</span><span>)</span>
            <span>+</span> <span>[</span><span>&#34; &#34;</span><span>]</span>
        <span>)</span>

        <span>self</span><span>.</span><span>itos</span> <span>=</span> <span>[</span><span>self</span><span>.</span><span>pad_token</span><span>,</span> <span>self</span><span>.</span><span>unk_token</span><span>]</span> <span>+</span> <span>chars</span>
        <span>self</span><span>.</span><span>stoi</span> <span>=</span> <span>{</span><span>ch</span><span>:</span> <span>i</span> <span>for</span> <span>i</span><span>,</span> <span>ch</span> <span>in</span> <span>enumerate</span><span>(</span><span>self</span><span>.</span><span>itos</span><span>)}</span>

        <span>self</span><span>.</span><span>vocab_size</span> <span>=</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>itos</span><span>)</span>

    <span>def</span> <span>_preprocess_text</span><span>(</span><span>self</span><span>,</span> <span>text</span><span>:</span> <span>str</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
        <span>text</span> <span>=</span> <span>(</span>
            <span>text</span><span>.</span><span>replace</span><span>(</span><span>&#34;’&#34;</span><span>,</span> <span>&#34;&#39;&#34;</span><span>)</span>
                <span>.</span><span>replace</span><span>(</span><span>&#34;‘&#34;</span><span>,</span> <span>&#34;&#39;&#34;</span><span>)</span>
                <span>.</span><span>replace</span><span>(</span><span>&#34;“&#34;</span><span>,</span> <span>&#39;&#34;&#39;</span><span>)</span>
                <span>.</span><span>replace</span><span>(</span><span>&#34;”&#34;</span><span>,</span> <span>&#39;&#34;&#39;</span><span>)</span>
                <span>.</span><span>replace</span><span>(</span><span>&#34;—&#34;</span><span>,</span> <span>&#34;-&#34;</span><span>)</span>
                <span>.</span><span>replace</span><span>(</span><span>&#34;–&#34;</span><span>,</span> <span>&#34;-&#34;</span><span>)</span>
        <span>)</span>
        <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>&#34;[^\x20-\x7E]&#34;</span><span>,</span> <span>&#34;&#34;</span><span>,</span> <span>text</span><span>)</span>

        <span>return</span> <span>text</span>

    <span>def</span> <span>encode</span><span>(</span><span>self</span><span>,</span> <span>text</span><span>:</span> <span>str</span><span>,</span> <span>max_length</span><span>:</span> <span>int</span> <span>=</span> <span>50</span><span>)</span> <span>-&gt;</span> <span>list</span><span>:</span>
        <span>text</span> <span>=</span> <span>self</span><span>.</span><span>_preprocess_text</span><span>(</span><span>text</span><span>)</span>

        <span>tokens</span> <span>=</span> <span>[</span>
            <span>self</span><span>.</span><span>stoi</span><span>[</span><span>ch</span><span>]</span> <span>if</span> <span>ch</span> <span>in</span> <span>self</span><span>.</span><span>stoi</span> <span>else</span> <span>self</span><span>.</span><span>stoi</span><span>[</span><span>self</span><span>.</span><span>unk_token</span><span>]</span>
            <span>for</span> <span>ch</span> <span>in</span> <span>text</span>
        <span>]</span>

        <span># Truncate if too long
</span>        <span>tokens</span> <span>=</span> <span>tokens</span><span>[:</span><span>max_length</span><span>]</span>
        <span># Pad if too short
</span>        <span>tokens</span> <span>+=</span> <span>[</span><span>self</span><span>.</span><span>stoi</span><span>[</span><span>self</span><span>.</span><span>pad_token</span><span>]]</span> <span>*</span> <span>(</span><span>max_length</span> <span>-</span> <span>len</span><span>(</span><span>tokens</span><span>))</span>

        <span>return</span> <span>tokens</span>

    <span>def</span> <span>decode</span><span>(</span><span>self</span><span>,</span> <span>token_ids</span><span>:</span> <span>list</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
        <span>return</span> <span>&#34;&#34;</span><span>.</span><span>join</span><span>(</span>
            <span>self</span><span>.</span><span>itos</span><span>[</span><span>idx</span><span>]</span> <span>if</span> <span>0</span> <span>&lt;=</span> <span>idx</span> <span>&lt;</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>itos</span><span>)</span> <span>else</span> <span>self</span><span>.</span><span>unk_token</span>
            <span>for</span> <span>idx</span> <span>in</span> <span>token_ids</span>
        <span>)</span>

</code></pre></div>  </div>

</details>

<h5 id="5-generating-unconditional-handwriting-with-a-transformer">5) Generating unconditional handwriting with a transformer</h5>

<p>I first tested a transformer that generated new strokes based on previous strokes:</p>

<ul>
  <li><strong>Self-attention on stroke tokens</strong>
The transformer processes each token (Δx, Δy, pen lift), learning how strokes typically flow over time.</li>
  <li><strong>Scribble-like outputs</strong>
With no text to guide it, the model produces free-form strokes. These can look like letters or loops, confirming it can generate coherent movement patterns before adding text conditioning.</li>
  <li><strong>Advantages over RNN</strong>
Transformers better capture long-range dependencies through self-attention, and discrete token classification avoids the complexities of continuous sampling or GMM instability.</li>
</ul>

<hr/>

<h4 id="stage-2-shifting-to-conditional-generation-text-to-handwriting">Stage 2: shifting to conditional generation (text-to-handwriting)</h4>

<h5 id="alignment-and-sequence-length">Alignment and sequence length</h5>

<p>Now it was time to make the model more useful and multimodal. I wanted to input text (e.g. “Hello world”) and produce realistic handwriting for it, so I:</p>

<ul>
  <li>Used an encoder for text tokens, incorporating positional encodings to preserve character order.</li>
  <li>Used a decoder for stroke tokens, applying cross-attention so that each stroke prediction could reference the text.</li>
</ul>

<h5 id="conditional-generation">Conditional generation</h5>

<ul>
  <li><strong>Encoder-decoder architecture</strong>: The decoder queries the text encoder at each step, reinforcing the relationship between characters and strokes.</li>
  <li><strong>Pad masking</strong>: Since text sequences vary in length, <code>[PAD]</code> tokens were masked so the model wouldn’t attend to non-character tokens.</li>
  <li><strong>Cross-attention validation</strong>: I monitored attention matrices to ensure stroke tokens referenced text embeddings when text was present, while still autoregressing correctly on strokes when text was absent. If attention was uniformly distributed across text tokens, it indicated the model was ignoring the text, requiring adjustments to positional encodings or cross-attention layers.</li>
</ul>

<p>This text-conditioned generation allowed the model to map language directly onto structured, sequential handwriting motion.</p>

<details>
  <summary>Click to see that implementation</summary>

  <div><div><pre><code><span>class</span> <span>TransformerStrokeModel</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>/*</span> <span>...</span> <span>*/</span>
    <span>def</span> <span>_generate_causal_mask</span><span>(</span><span>self</span><span>,</span> <span>seq_len</span><span>,</span> <span>device</span><span>):</span>
        <span>mask</span> <span>=</span> <span>torch</span><span>.</span><span>triu</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>seq_len</span><span>,</span> <span>seq_len</span><span>,</span> <span>device</span><span>=</span><span>device</span><span>),</span> <span>diagonal</span><span>=</span><span>1</span><span>)</span>
        <span>return</span> <span>mask</span><span>.</span><span>masked_fill</span><span>(</span><span>mask</span> <span>==</span> <span>1</span><span>,</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>stroke_batch</span><span>,</span> <span>text_features</span><span>=</span><span>None</span><span>,</span> <span>text_pad_mask</span><span>=</span><span>None</span><span>,</span> <span>return_attn</span><span>=</span><span>False</span><span>):</span>
        <span>b</span><span>,</span> <span>t</span><span>,</span> <span>_</span> <span>=</span> <span>stroke_batch</span><span>.</span><span>shape</span>

        <span>x_x</span> <span>=</span> <span>self</span><span>.</span><span>embedding_x</span><span>(</span><span>stroke_batch</span><span>[:,</span> <span>:,</span> <span>1</span><span>])</span>
        <span>x_y</span> <span>=</span> <span>self</span><span>.</span><span>embedding_y</span><span>(</span><span>stroke_batch</span><span>[:,</span> <span>:,</span> <span>2</span><span>])</span>
        <span>combined</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>x_x</span><span>,</span> <span>x_y</span><span>],</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
        <span>combined</span> <span>=</span> <span>self</span><span>.</span><span>fc_embed</span><span>(</span><span>combined</span><span>)</span>
        <span>combined</span> <span>=</span> <span>self</span><span>.</span><span>position_encoding_strokes</span><span>(</span><span>combined</span><span>)</span>

        <span>/*</span> <span>...</span> <span>*/</span>
        <span>decoded</span><span>,</span> <span>cross_attn_list</span> <span>=</span> <span>self</span><span>.</span><span>decoder</span><span>(</span>
            <span>combined</span><span>,</span> <span>text_features</span><span>,</span>
            <span>tgt_mask</span><span>=</span><span>causal_mask</span><span>,</span>
            <span>return_attn_weights</span><span>=</span><span>True</span><span>,</span>
            <span>memory_key_padding_mask</span><span>=</span><span>text_pad_mask</span><span>,</span>
        <span>)</span>

        <span>logits_x</span> <span>=</span> <span>self</span><span>.</span><span>fc_out_x</span><span>(</span><span>decoded</span><span>)</span>
        <span>logits_y</span> <span>=</span> <span>self</span><span>.</span><span>fc_out_y</span><span>(</span><span>decoded</span><span>)</span>
        <span>pen_logits</span> <span>=</span> <span>self</span><span>.</span><span>fc_pen_lift</span><span>(</span><span>decoded</span><span>).</span><span>squeeze</span><span>(</span><span>-</span><span>1</span><span>)</span>

        <span>/*</span> <span>...</span> <span>*/</span>
</code></pre></div>  </div>

</details>

<hr/>

<h4 id="stage-3-debugging-strategy">Stage 3: debugging strategy</h4>

<h5 id="overfitting-on-one-example-then-five-then-more">Overfitting on one example, then five, then more</h5>

<ol>
  <li>When my model couldn’t memorize one text-stroke pair, I knew I had a model architecture or data processing bug.</li>
  <li>Then I’d test 5 pairs to see if it could overfit. Then 10, then 100.</li>
  <li>Only after that would I expand to the full dataset.</li>
</ol>

<h5 id="oops-moments">Oops moments</h5>

<ol>
  <li><strong>Swapped Δx and pen lift</strong>: In one run, Δy looked great but the rest was off, and I discovered I had columns reversed in the batch. Simple fix.</li>
  <li><strong>No positional encodings for text</strong>: Without them, the cross-attention was relatively random. Once I added <code>PositionalEncoding</code>, the attention matrix aligned strokes with the correct characters in order.</li>
</ol>

<hr/>

<h4 id="stage-4-architecture-tweaks">Stage 4: architecture tweaks</h4>

<h5 id="simplify">Simplify</h5>

<p>An underlying theme was paring down complexity. For instance, I started experiments with 8–12 heads and 6–8 layers, but quickly learned that was overkill that slowed down training. 2–4 heads and 1–3 layers converged way faster and were easier to debug.</p>

<h5 id="visualizing-attention-weights-in-pytorch">Visualizing attention weights in PyTorch</h5>

<p>While debugging, a huge interpretability boost was subclassing <code>nn.TransformerDecoderLayer</code> to return cross-attention weights. Plotting them as heatmaps (stroke_tokens × text_tokens) let me check alignment and quickly debug a bunch of situations (e.g. text completely ignored, positional encodings ignored, everything fixated on the first letter).</p>

<p><img src="https://www.frontiernet.net/assets/images/image9.png" alt="Screenshot: cross-attention weight visualization" width="400"/></p>

<details>
  <summary>Click to see that implementation</summary>

  <div><div><pre><code><span>class</span> <span>CustomDecoderLayer</span><span>(</span><span>nn</span><span>.</span><span>TransformerDecoderLayer</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>d_model</span><span>,</span> <span>nhead</span><span>,</span> <span>dim_feedforward</span><span>=</span><span>1024</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>(</span><span>d_model</span><span>,</span> <span>nhead</span><span>,</span> <span>dim_feedforward</span><span>=</span><span>dim_feedforward</span><span>,</span> <span>dropout</span><span>=</span><span>DROPOUT</span><span>,</span> <span>batch_first</span><span>=</span><span>True</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span>
        <span>self</span><span>,</span>
        <span>tgt</span><span>,</span>
        <span>memory</span><span>,</span>
        <span>tgt_mask</span><span>=</span><span>None</span><span>,</span>
        <span>memory_mask</span><span>=</span><span>None</span><span>,</span>
        <span>tgt_key_padding_mask</span><span>=</span><span>None</span><span>,</span>
        <span>memory_key_padding_mask</span><span>=</span><span>None</span><span>,</span>
        <span>return_attn_weights</span><span>=</span><span>False</span>
    <span>):</span>
        <span>x</span><span>,</span> <span>self_attn_weights</span> <span>=</span> <span>self</span><span>.</span><span>self_attn</span><span>(</span>
            <span>tgt</span><span>,</span> <span>tgt</span><span>,</span> <span>tgt</span><span>,</span>
            <span>attn_mask</span><span>=</span><span>tgt_mask</span><span>,</span>
            <span>key_padding_mask</span><span>=</span><span>tgt_key_padding_mask</span><span>,</span>
            <span>need_weights</span><span>=</span><span>return_attn_weights</span>
        <span>)</span>
        <span>tgt</span> <span>=</span> <span>tgt</span> <span>+</span> <span>self</span><span>.</span><span>dropout1</span><span>(</span><span>x</span><span>)</span>
        <span>tgt</span> <span>=</span> <span>self</span><span>.</span><span>norm1</span><span>(</span><span>tgt</span><span>)</span>

        <span>x</span><span>,</span> <span>cross_attn_weights</span> <span>=</span> <span>self</span><span>.</span><span>multihead_attn</span><span>(</span>
            <span>tgt</span><span>,</span> <span>memory</span><span>,</span> <span>memory</span><span>,</span>
            <span>attn_mask</span><span>=</span><span>memory_mask</span><span>,</span>
            <span>key_padding_mask</span><span>=</span><span>memory_key_padding_mask</span><span>,</span>
            <span>need_weights</span><span>=</span><span>return_attn_weights</span>
        <span>)</span>
        <span>tgt</span> <span>=</span> <span>tgt</span> <span>+</span> <span>self</span><span>.</span><span>dropout2</span><span>(</span><span>x</span><span>)</span>
        <span>tgt</span> <span>=</span> <span>self</span><span>.</span><span>norm2</span><span>(</span><span>tgt</span><span>)</span>

        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>linear2</span><span>(</span><span>self</span><span>.</span><span>dropout</span><span>(</span><span>self</span><span>.</span><span>activation</span><span>(</span><span>self</span><span>.</span><span>linear1</span><span>(</span><span>tgt</span><span>))))</span>
        <span>tgt</span> <span>=</span> <span>tgt</span> <span>+</span> <span>self</span><span>.</span><span>dropout3</span><span>(</span><span>x</span><span>)</span>
        <span>tgt</span> <span>=</span> <span>self</span><span>.</span><span>norm3</span><span>(</span><span>tgt</span><span>)</span>

        <span>if</span> <span>return_attn_weights</span><span>:</span>
            <span>return</span> <span>tgt</span><span>,</span> <span>self_attn_weights</span><span>,</span> <span>cross_attn_weights</span>
        <span>else</span><span>:</span>
            <span>return</span> <span>tgt</span><span>,</span> <span>None</span><span>,</span> <span>None</span>
</code></pre></div>  </div>

</details>

<hr/>

<h4 id="stage-5-refining-conditional-generation">Stage 5: refining conditional generation</h4>

<p>With the basics working, I could now focus on more interesting tweaks, like:</p>

<ul>
  <li><strong>pen lift</strong> weighting. The pen was only lifted 5% of the time in the dataset, so I used focal loss.</li>
  <li><strong>Temperature sampling</strong> to prevent repetitive loops at inference time.</li>
  <li><strong>Occasional no-text</strong> to handle both the conditional and unconditional tasks.</li>
</ul>

<hr/>

<h4 id="stage-6-final-results-and-observations">Stage 6: final results and observations</h4>

<p>Plenty of good:</p>

<ul>
  <li><strong>Human-like</strong> cursive or printed letters, with real pen lifts.</li>
</ul>

<p>Some not-so-good:</p>

<ul>
  <li><strong>Over-smoothing</strong>: some letters lost distinct edges.</li>
  <li><strong>Spacing</strong>: sometimes too tight or too wide.</li>
</ul>

<p>I spent a lot of time visualizing:</p>

<ol>
  <li>Plotting each epoch’s output.</li>
  <li>Seeding with real strokes for X timesteps, letting the model complete the rest.</li>
</ol>

<hr/>

<h4 id="example-outputs">Example outputs</h4>

<details>
  <summary>Click for early MDN generation</summary>

  <p><img src="https://www.frontiernet.net/assets/images/image5.png" alt="Screenshot 5: Early Transformer + MDN Generation"/></p>

  <h6 id="identifying-the-issue"><strong>Identifying the issue</strong></h6>

  <ul>
    <li>The generated strokes consistently move up and to the right, revealing that the model <strong>always selects the same Gaussian component</strong> instead of adapting to the handwriting context.</li>
    <li>Even though the transformer theoretically improved long-range dependencies, the unstable MDN head remained a bottleneck, producing repetitive trajectories.</li>
    <li>A shift to <strong>discrete tokenization</strong> was necessary to give the transformer better control over individual stroke outputs, avoiding the need for unstable mixture sampling.</li>
  </ul>

  <p>After trying a lot of hyperparameter tuning and model tweaks, I switched to discrete tokens, which eliminated this issue.</p>

</details>

<hr/>

<details>
  <summary>Click for early discrete transformer generation</summary>

  <p>In this example, I seeded the model with some strokes from the dataset (in blue) and let it generate the remaining handwriting (in red) based on both strokes and text. Early on, the model produced fragmented and erratic strokes.</p>

  <p><img src="https://www.frontiernet.net/assets/images/image1.png" alt="Screenshot 1: Early model generation"/></p>

  <h6 id="identifying-the-issue-1"><strong>Identifying the issue</strong></h6>

  <ul>
    <li>The generated strokes frequently lost coherence, failing to maintain the structure of letters beyond a few timesteps.</li>
    <li>This early output helped diagnose issues with:
      <ul>
        <li><strong>cross-attention alignment:</strong> ensuring the model properly conditions on text instead of generating arbitrary strokes.</li>
        <li><strong>stroke continuity:</strong> adjusting positional encodings and training dynamics to prevent erratic jumps.</li>
        <li><strong>autoregressive stability:</strong> the model struggled to smoothly transition from real strokes to generated ones.</li>
      </ul>
    </li>
  </ul>

  <p>As training progressed, I refined the text-stroke relationship, improving overall performance.</p>

</details>

<hr/>

<details>
  <summary>Click for pen lift failure due to imbalanced data</summary>

  <p>After some refinements on the above, the model still <strong>failed to lift the pen</strong> correctly.</p>

  <p><img src="https://www.frontiernet.net/assets/images/image2.png" alt="Screenshot 2: pen lift imbalance"/></p>

  <h6 id="identifying-the-issue-2"><strong>Identifying the issue</strong></h6>

  <ul>
    <li>The model struggled with pen lifts, treating them as rare occurrences and failing to separate letters properly.</li>
    <li>This happened because lift events were underrepresented in the dataset, making the model biased toward keeping the pen down.</li>
  </ul>

  <h6 id="fixing-it-with-focal-loss"><strong>Fixing it with focal loss</strong></h6>

  <ul>
    <li>I introduced <strong>focal loss</strong>, which increases the weight of rare events during training.</li>
  </ul>

</details>

<hr/>

<details>
  <summary>Click for positional text encoding fix</summary>

  <p>After the above, I finally got the model producing <strong>reasonable letter-like scribbles</strong>, but they were still jumbled and nonsensical, failing to follow the intended character sequence.</p>

  <p><img src="https://www.frontiernet.net/assets/images/image3.png" alt="Screenshot 3: missing positional encoding"/></p>

  <h6 id="identifying-the-issue-3"><strong>Identifying the issue</strong></h6>

  <ul>
    <li>The model appeared to understand what letters should look like, but had no sense of where to place them in relation to the input text.</li>
    <li>This is because positional encodings were missing from the text encoder, meaning the model saw all text tokens as unordered symbols rather than a structured sequence.</li>
  </ul>

  <h6 id="fixing-it-with-positional-encodings"><strong>Fixing it with positional encodings</strong></h6>

  <ul>
    <li>With this fix, the model could associate text positions with stroke positions, improving letter placement and structure.</li>
  </ul>

</details>

<hr/>

<h5 id="improved-handwriting-generation">Improved handwriting generation</h5>

<p>And finally… <strong>successful</strong> text-to-handwriting generation!</p>

<p><img src="https://www.frontiernet.net/assets/images/image4.png" alt="Screenshot 4: final improved result"/></p>

<h6 id="what-works-here"><strong>What works here</strong></h6>

<ul>
  <li>The generated handwriting follows the structure of the input text, properly aligning strokes with corresponding letters.</li>
  <li>Pen lifts and spacing works.</li>
  <li>The handwriting flows naturally vs. wild nonsensical movements from earlier models.</li>
</ul>

<h6 id="key-improvements-that-led-to-this-result"><strong>Key improvements that led to this result</strong></h6>

<details>
  <summary>
This level of convergence required many iterations of model architecture refinement, hyperparameter tuning, and targeted loss reductions.
</summary>

  <ul>
    <li><strong>Model architecture improvements</strong>: I reduced the number of Transformer layers and heads to <strong>2–4 heads, 1–3 layers</strong>, balancing expressiveness and convergence speed. Larger models took longer to learn without necessarily improving output quality.</li>
    <li><strong>Hyperparameter tuning</strong>: Learning rate schedules, warm-up steps, and batch size adjustments helped <strong>stabilize training and prevent overfitting</strong>, particularly by adjusting loss scaling on pen lift events.</li>
    <li><strong>Loss function adjustments</strong>: Introducing <strong>focal loss</strong> helped <strong>pen lift balancing</strong>, while ensuring <strong>positional encodings</strong> were properly applied to both stroke and text tokens helped cross-attention learn better alignments.</li>
    <li><strong>Improved cross-attention layers</strong>: Careful inspection of attention weights helped uncover bugs.</li>
  </ul>

</details>

<hr/>

<h4 id="stage-7-future-directions-and-extensions">Stage 7: future directions and extensions</h4>

<ul>
  <li>
    <p><strong>Handwriting to text</strong></p>
  </li>
  <li>
    <p><strong>Style transfer</strong></p>
  </li>
  <li>
    <p><strong>Diffusion, VAEs, and EBMs</strong></p>

    <ul>
      <li>
        <p><strong>Diffusion models:</strong></p>

        <ul>
          <li>
            <p><a href="https://arxiv.org/pdf/2205.09991">Planning with Diffusion for Flexible Behavior Synthesis</a> proposes a non-autoregressive method that predicts all timesteps concurrently, blurring the lines between sampling from a trajectory model and planning with it. Each denoising step focuses on local consistency (nearby timesteps in the past and future), but composing many of these steps creats global coherence. Applied to handwriting or robot motion, this approach can help ensure the entire stroke or trajectory is consistent, rather than being generated purely from past context in a causal manner.</p>
          </li>
          <li>
            <p>Pi-0 replaces standard cross-entropy with a <strong>flow matching</strong> loss in a decoder-only transformer. They maintain separate weights for the diffusion tokens, effectively embedding diffusion within a vision-language-action pipeline to handle high-frequency action control. A handwriting model could do the same.</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Variational autoencoders:</strong></p>
      </li>
      <li>
        <p><strong>Energy-based models:</strong></p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Interactive demo</strong></p>
  </li>
  <li>
    <p><strong>Robotics</strong></p>
  </li>
</ul>

<hr/>

<h2 id="conclusion-big-picture-takeaways">Conclusion: big-picture takeaways</h2>

<ol>
  <li>
    <p><strong>Trajectory modeling</strong></p>
  </li>
  <li>
    <p><strong>Foundation models and unified approaches</strong></p>
  </li>
  <li>
    <p><strong>Simple and incremental debugging</strong></p>
  </li>
  <li>
    <p><strong>Paths forward</strong></p>
  </li>
</ol>
</div>
  </article></div>
  </body>
</html>
