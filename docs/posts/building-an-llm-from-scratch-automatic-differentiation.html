<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html">Original</a>
    <h1>Building an LLM from Scratch: Automatic Differentiation</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<div id="faa21911" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="1">
<details>
<summary>Setup</summary>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span>from</span> typing <span>import</span> Any, Optional, List</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span>import</span> networkx <span>as</span> nx</span></code></pre></div>
</details>
</div>
<section id="llm-from-scratch-automatic-differentiation">
<h2 data-anchor-id="llm-from-scratch-automatic-differentiation">LLM from scratch: Automatic Differentiation</h2>
<p>I’m building a modern language model with all the bells and whistles completely from scratch: from vanilla python to functional coding assistant. Borrowing (shamelessly stealing) from computer games, I’ve built a tech tree of everything that I think I’ll need to implement to get a fully functional language model. If you think anything is missing, <a href="mailto:bclarkson-code@proton.me">please let me know</a>:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/tech_tree_post_1.png" alt="The LLM from scratch tech tree" width="700"/></p>
<figcaption>The LLM from scratch tech tree</figcaption>
</figure>
</div>
<p>Before we can move onto building modern features like <a href="https://arxiv.org/abs/2104.09864">Rotary Positional Encodings</a>, we first need to figure out how to differentiate with a computer. The backpropagation algorithm that underpins the entire field of Deep Learning requires the ability to differentiate the outputs of neural networks with respect to (wrt) their inputs. In this post, we’ll go from nothing to an (admittedly very limited) automatic differentiation library that can differentiate arbitrary functions of scalar values.</p>
<p>This one algorithm will form the core of our deep learning library that, eventually, will include everything we need to train a language model.</p>
</section>
<section id="creating-a-tensor">
<h2 data-anchor-id="creating-a-tensor">Creating a tensor</h2>
<p>We can’t do any differentiation if we don’t have any numbers to differentiate. We’ll want to add some extra functionality that is in standard <code>float</code> types so we’ll need to create our own. Let’s call it a <code>Tensor</code>.</p>
<div id="3c2dc0d6" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="2">
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span>class</span> Tensor:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span>    Just a number (for now)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    value: <span>float</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span>self</span>.value <span>=</span> value</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span>&#34;&#34;&#34;</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span>        Create a printable string representation of this</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span>        object</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span>        This function gets called when you pass a Tensor to print</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span>        Without this function:</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span>        &lt;__main__.Tensor at 0x104fd1950&gt;</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span>        With this function:</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span>        Tensor(5)</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span>        &#34;&#34;&#34;</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>f&#34;Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)&#34;</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span># try it out</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>Tensor(<span>5</span>)</span></code></pre></div>

</div>
<p>Next we’ll need some simple operations we want to perform: addition, subtraction and multiplication.</p>
<div id="69ba409d" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="3">
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span>def</span> _add(a: Tensor, b: Tensor):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span>    Add two tensors</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Tensor(a.value <span>+</span> b.value)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span>def</span> _sub(a: Tensor, b: Tensor):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span>    Subtract tensor b from tensor a</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Tensor(a.value <span>-</span> b.value)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span>def</span> _mul(a: Tensor, b: Tensor):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span>    Multiply two tensors</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Tensor(a.value <span>*</span> b.value)</span></code></pre></div>
</div>
<p>We can use use our operations as follows:</p>
<div id="55858d5d" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="4">
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span>def</span> test(got: Any, want: Any):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span>    Check that two objects are equal to each other</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    indicator <span>=</span> <span>&#34;✅&#34;</span> <span>if</span> want <span>==</span> got <span>else</span> <span>&#34;❌&#34;</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>f&#34;</span><span>{</span>indicator<span>}</span><span> - Want: </span><span>{</span>want<span>}</span><span>, Got: </span><span>{</span>got<span>}</span><span>&#34;</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>test(_add(a, b).value, <span>7</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>test(_sub(a, b).value, <span>-</span><span>1</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>test(_mul(a, b).value, <span>12</span>)</span></code></pre></div>
<div>
<pre><code>✅ - Want: 7, Got: 7
✅ - Want: -1, Got: -1
✅ - Want: 12, Got: 12</code></pre>
</div>
</div>
</section>
<section id="scalar-derivatives">
<h2 data-anchor-id="scalar-derivatives">Scalar derivatives</h2>
<p>Diving straight into differentiating matrices sounds too hard so let’s start with something simpler: differentiating scalars. The simplest scalar derivative I can think of is differentiating a tensor with respect to itself: <span>\[\frac{dx}{dx} = 1\]</span></p>
<p>A more interesting case is the derivative of two tensors added together (note we are using partial derivatives because our function has multiple inputs): <span>\[f(x, y) = x + y\]</span> <span>\[\frac{\partial f}{\partial x} = 1\]</span> <span>\[\frac{\partial f}{\partial y} = 1\]</span></p>
<p>We can do a similar thing for multiplication and subtraction</p>
<table>
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th><span>\(f(x, y)\)</span></th>
<th><span>\(\frac{\partial f}{\partial x}\)</span></th>
<th><span>\(\frac{\partial f}{\partial y}\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span>\(x + y\)</span></td>
<td><span>\(1\)</span></td>
<td><span>\(1\)</span></td>
</tr>
<tr>
<td><span>\(x - y\)</span></td>
<td><span>\(1\)</span></td>
<td><span>\(-1\)</span></td>
</tr>
<tr>
<td><span>\(x \times y\)</span></td>
<td><span>\(y\)</span></td>
<td><span>\(x\)</span></td>
</tr>
</tbody>
</table>
<p>Now that we’ve worked out these derivatives mathematically, the next step is to convert them into code. In the table above, when we make a tensor by combining two tensors with an operation, the derivative only ever depends on the inputs and the operation. There is no “hidden state”.</p>
<p>This means that the only information we need to store is the inputs to an operation and a function to calculate the derivative wrt each input. With this, we should be able to differentiate any binary function wrt its inputs. A good place to store this information is in the tensor that is produced by the operation.</p>
<p>We’ll add some new attributes to our <code>Tensor</code>: <code>args</code> and <code>local_derivatives</code>. If the tensor is the output of an operation, then <code>args</code> will store the arguments to the operation and <code>local_derivatives</code> will store the derivatives wrt each input. We’re calling it <code>local_derivatives</code> to avoid confusion when we start nesting functions.</p>
<p>Once we’ve calculated the derivative (from our <code>args</code> and <code>local_derivatives</code>) we’ll need to store it. It turns out that the neatest place to put this is in the tensor that the output is being differentiated wrt. We’ll call this <code>derivative</code>.</p>
<div id="2df2b7ce" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="5">
<div id="cb7"><pre><code><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span>class</span> Tensor:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span>    A number that can be differentiated</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span># If the tensor was made by an operation, the operation arguments</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span># are stored in args</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    args: <span>tuple</span>[<span>&#34;Tensor&#34;</span>] <span>=</span> ()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span># If the tensor was made by an operation, the derivatives wrt</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span># operation inputs are stored in derivatives</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    local_derivatives: <span>tuple</span>[<span>&#34;Tensor&#34;</span>] <span>=</span> ()</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span># The derivative we have calculated</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    derivative: Optional[<span>&#34;Tensor&#34;</span>] <span>=</span> <span>None</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span>self</span>.value <span>=</span> value</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span>&#34;&#34;&#34;</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span>        Create a printable string representation of this</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span>        object</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span>        This function gets called when you pass a Tensor to print</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span>        Without this function:</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span>        &lt;__main__.Tensor at 0x104fd1950&gt;</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span>        With this function:</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span>        Tensor(5)</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span>        &#34;&#34;&#34;</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>f&#34;Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)&#34;</span></span></code></pre></div>
</div>
<p>For example, if we have</p>
<div id="16c996f7" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="6">
<div id="cb8"><pre><code><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>output <span>=</span> _mul(a, b)</span></code></pre></div>
</div>
<p>Then <code>output.args</code> and <code>output.local_derivatives</code> should be set to:</p>
<div id="cb9"><pre><code><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>output.args <span>==</span> (Tensor(<span>3</span>), Tensor(<span>4</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>output.derivatives <span>==</span> (</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    b,  <span># derivative of output wrt a is b</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    a,  <span># derivative of output wrt b is a</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Once we have actually computed the derivatives, then the derivative of <code>output</code> wrt <code>a</code> will be stored in <code>a.derivative</code> and should be equal to <code>b</code> (which is 4 in this case).</p>
<p>We know that we’ve done everything right once these tests pass:</p>
<div id="615dc0ba" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="7">
<div id="cb10"><pre><code><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>output <span>=</span> _mul(a, b)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span># </span><span>TODO</span><span>: differentiate here</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>output.args, want<span>=</span>(a, b))</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>output.local_derivatives, want<span>=</span>(b, a))</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>a.derivative, want<span>=</span>b)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>b.derivative, want<span>=</span>a)</span></code></pre></div>
<div>
<pre><code>❌ - Want: (Tensor(3), Tensor(4)), Got: ()
❌ - Want: (Tensor(4), Tensor(3)), Got: ()
❌ - Want: Tensor(4), Got: None
❌ - Want: Tensor(3), Got: None</code></pre>
</div>
</div>
<p>First, let’s add a function to our <code>Tensor</code> that will actually calculate the derivatives for each of the function arguments. Pytorch calls this function <code>backward</code> so we’ll do the same.</p>
<div id="18dcfc02" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="8">
<div id="cb12"><pre><code><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span>class</span> Tensor:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span>    A number that can be differentiated</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span># If the tensor was made by an operation, the operation arguments</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span># are stored in args</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    args: <span>tuple</span>[<span>&#34;Tensor&#34;</span>] <span>=</span> ()</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span># If the tensor was made by an operation, the derivatives wrt</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span># operation inputs are stored in</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    local_derivatives: <span>tuple</span>[<span>&#34;Tensor&#34;</span>] <span>=</span> ()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span># The derivative we have calculated</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    derivative: Optional[<span>&#34;Tensor&#34;</span>] <span>=</span> <span>None</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span># optionally give this tensor a name</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    name: Optional[<span>str</span>] <span>=</span> <span>None</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span># Later, we&#39;ll want to record the path we followed to get</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span># to this tensor and some operations we did along the way</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span># don&#39;t worry about these for now</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    paths: List[Tensor] <span>=</span> <span>None</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    chains: List[Tensor] <span>=</span> <span>None</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span>self</span>.value <span>=</span> value</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    <span>def</span> backward(<span>self</span>):</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>            <span>raise</span> <span>ValueError</span>(</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>                <span>&#34;Cannot differentiate a Tensor that is not a function of other Tensors&#34;</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        <span>for</span> arg, derivative <span>in</span> <span>zip</span>(<span>self</span>.args, <span>self</span>.local_derivatives):</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>            arg.derivative <span>=</span> derivative</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        <span>&#34;&#34;&#34;</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span>        Create a printable string representation of this</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span>        object</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span>        This function gets called when you pass a Tensor to print</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span>        Without this function:</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span>        &lt;__main__.Tensor at 0x104fd1950&gt;</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span>        With this function:</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span>        &gt;&gt;&gt; print(Tensor(5))</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span>        Tensor(5)</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span>        &#34;&#34;&#34;</span></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>f&#34;Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)&#34;</span></span></code></pre></div>
</div>
<p>This only works if we also store the arguments and derivatives in the output tensors of operations</p>
<div id="0f77b5c2" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="9">
<div id="cb13"><pre><code><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span>def</span> _add(a: Tensor, b: Tensor):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span>    Add two tensors</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    result <span>=</span> Tensor(a.value <span>+</span> b.value)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    result.local_derivatives <span>=</span> (Tensor(<span>1</span>), Tensor(<span>1</span>))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    result.args <span>=</span> (a, b)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span>return</span> result</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span>def</span> _sub(a: Tensor, b: Tensor):</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span>    Subtract tensor b from a</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    result <span>=</span> Tensor(a.value <span>-</span> b.value)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    result.local_derivatives <span>=</span> (Tensor(<span>1</span>), Tensor(<span>-</span><span>1</span>))</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    result.args <span>=</span> (a, b)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span>return</span> result</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span>def</span> _mul(a: Tensor, b: Tensor):</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span>    Multiply two tensors</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    result <span>=</span> Tensor(a.value <span>*</span> b.value)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    result.local_derivatives <span>=</span> (b, a)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    result.args <span>=</span> (a, b)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    <span>return</span> result</span></code></pre></div>
</div>
<p>Let’s re-run our tests and see if it works</p>
<div id="426bc097" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="10">
<div id="cb14"><pre><code><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>output <span>=</span> _mul(a, b)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>output.backward()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>output.args, want<span>=</span>(a, b))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>output.local_derivatives, want<span>=</span>(b, a))</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>test(a.derivative, b)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>test(b.derivative, a)</span></code></pre></div>
<div>
<pre><code>✅ - Want: (Tensor(3), Tensor(4)), Got: (Tensor(3), Tensor(4))
✅ - Want: (Tensor(4), Tensor(3)), Got: (Tensor(4), Tensor(3))
✅ - Want: Tensor(4), Got: Tensor(4)
✅ - Want: Tensor(3), Got: Tensor(3)</code></pre>
</div>
</div>
<p>So far so good, let’s try nesting operations.</p>
<div id="e14634fb" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="11">
<div id="cb16"><pre><code><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>a <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>b <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>output_1 <span>=</span> _mul(a, b)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span># z = a + (a * b)</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>output_2 <span>=</span> _add(a, output_1)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>output_2.backward()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span># should get</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span># dz/db = 0 + a = a</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>test(b.derivative, a)</span></code></pre></div>
<div>
<pre><code>❌ - Want: Tensor(3), Got: None</code></pre>
</div>
</div>
<p>Something has gone wrong.</p>
<p>We should have got <code>a</code> as the derivative for <code>b</code> but we got <code>0</code> instead. Looking through the <code>.backward()</code> function, the issue is pretty clear: we haven’t thought about nested functions. To get this example working, we’ll need to figure out how to calculate derivatives through multiple functions instead of just one.</p>
</section>
<section id="chaining-functions-together">
<h2 data-anchor-id="chaining-functions-together">Chaining Functions Together</h2>
<p>To calculate derivatives of nested functions, we can use a rule from calculus: The Chain Rule.</p>
<p>For a variable <span>\(z\)</span> generated by nested functions <span>\(f\)</span> and <span>\(g\)</span> such that <span>\[z = f(g(x))\]</span></p>
<p>Then the derivative of <span>\(z\)</span> wrt <span>\(x\)</span> is: <span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(x)}{\partial x}\]</span></p>
<p>Here, <span>\(u\)</span> is a dummy variable. <span>\(\frac{\partial f(u)}{\partial u}\)</span> means the derivative of <span>\(f\)</span> wrt its input.</p>
<p>For example, if</p>
<p><span>\[f(x) = g(x)^2\]</span> Then we can define <span>\(u=g(x)\)</span> and rewrite <span>\(f\)</span> in terms of u <span>\[f(u) = u^2 \implies \frac{\partial f(u)}{\partial u} = 2u = 2 g(x)\]</span></p>
<section id="multiple-variables">
<h3 data-anchor-id="multiple-variables">Multiple Variables</h3>
<p>The chain rule works as you might expect for functions of multiple variables. When differentiating wrt a variable, we can treat the other variables as constant and differentiate as normal <span>\[z = f(g(x), h(y))\]</span></p>
<p><span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(x)}{\partial x}\]</span> <span>\[\frac{\partial z}{\partial y} = \frac{\partial f(u)}{\partial u} \frac{\partial h(y)}{\partial y}\]</span></p>
<p>If we have different functions that take the same input, we differentiate each of them individually and then add them together</p>
<p><span>\[z = f(g(x), h(x))\]</span></p>
<p>We get <span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u}\frac{\partial g(x)}{\partial x} + \frac{\partial f(u)}{\partial u}\frac{\partial h(x)}{\partial x}\]</span></p>
</section>
<section id="more-than-2-functions">
<h3 data-anchor-id="more-than-2-functions">More than 2 functions</h3>
<p>If we chain 3 functions together, we still just multiply the derivatives for each function together:</p>
<p><span>\[\frac{\partial z}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(x)}{\partial x} = \frac{\partial f(u)}{\partial u} \frac{\partial g(u)}{\partial u}\frac{\partial h(x)}{\partial x}\]</span></p>
<p>And this generalises to any amount of nesting</p>
<p><span>\[z = f_1(f_2(....f_{n-1}(f_n(x))...)) \]</span> <span>\(\implies \frac{\partial z}{\partial x} = \frac{\partial f_1(u)}{\partial u}\frac{\partial f_2(u)}{\partial u}...\frac{\partial f_{n-1}(u)}{\partial u}\frac{\partial f_{n}(x)}{\partial x}\)</span>$</p>
</section>
<section id="a-picture-is-worth-a-thousand-equations">
<h3 data-anchor-id="a-picture-is-worth-a-thousand-equations">A picture is worth a thousand equations</h3>
<p>As you probably noticed, the maths is starting to get quite dense. When we start working with neural networks, we can easily get 100s or 1000s of functions deep so to get a handle on things, we’ll need a different strategy. Helpfully, there is one: turning it into a graph.</p>
<p>We can start with some rules:</p>
<blockquote>
<p>Variables are represented with circles and operations are represented with boxes</p>
</blockquote>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/variable_and_box.png" alt="A variable as a circle and an operation as a box"/></p>
</figure>
</div>
<blockquote>
<p>Inputs to an operation are represented with arrows that point to the operation box. Outputs point away.</p>
</blockquote>
<p>For example, here is the diagram for <span>\(z = mx\)</span></p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/z_eq_mx.png" alt="The operation z = mx"/></p>
</figure>
</div>
<p>And that’s it! All of the equations we’ll be working with can be represented graphically using these simple rules. To try it out, let’s draw the diagram for a more complex formula:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/square_error.png" alt="A diagram of the square error of a linear regression"/></p>
</figure>
</div>
<p>This is an example of a structure called a graph (also called a network). A lot of problem in computer science get much easier if you can represent them with a graph and this is no exception.</p>
<p>The real power of these diagrams is that they can also help us with our derivatives. Take <span>\[y = mx + p = \texttt{add}(p, \texttt{mul}(m ,x)).\]</span></p>
<p>From before, we can find its derivatives by differentiating each operation wrt its inputs and multiplying the results together. In this case, we get: <span>\[\frac{\partial y}{\partial p} = \frac{\partial \texttt{add}(u_1, u_2)}{\partial u_1} = 1\]</span> <span>\[\frac{\partial y}{\partial m} = \frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_2} = 1 \times x = x\]</span> <span>\[\frac{\partial y}{\partial x} = \frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_1} = 1 \times m = m\]</span></p>
<p>We can also graph it like this:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/y_eq_mx_plus_p_labelled.png" alt="a graph of y = mx + p"/></p>
</figure>
</div>
<p>If you imagine walking from <span>\(y\)</span> to each of the inputs, you might notice a similarity between the edges you pass through and the equations above. If you walk from <span>\(y\)</span> to <span>\(x\)</span>, you’ll pass through <code>a-&gt;c-&gt;d</code>. Similarly, if you walk from <span>\(y\)</span> to <span>\(m\)</span>, you’ll pass through <code>a-&gt;d-&gt;e</code>. Notice that both paths go through <code>c</code>, the edge coming out of <code>add</code> that corresponds to the input <span>\(u_2\)</span>. Also, both equations include the term <span>\(\frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\)</span>.</p>
<p>If I rename the edges as follows:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/y_eq_mx_plus_p_deriv.png" alt="y = mx + p with each edge given a letter"/></p>
</figure>
</div>
<p>We can see that going from <span>\(y\)</span> to <span>\(x\)</span>, we pass through <span>\(1\)</span>, <span>\(\frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\)</span> and <span>\(\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_1}\)</span>. If we multiply these together, we get exactly <span>\(\frac{\partial \texttt{add}(u_1, u_2)}{\partial u_2}\frac{\partial \texttt{mul}(u_1, u_2)}{\partial u_1} = \frac{\partial y}{\partial x}\)</span>!</p>
<p>It turns out that this rule works in general:</p>
<blockquote>
<p>If we have some operation <span>\(\texttt{op}(u_1, u_2, ..., u_n)\)</span>, we should label the edge corresponding to input <span>\(u_i\)</span> with <span>\(\frac{\partial \texttt{op}(u_1, u_2, ..., u_n)}{\partial u_i}\)</span></p>
</blockquote>
<p>Then, if we want to find the derivative of the output node wrt any of the inputs,</p>
<blockquote>
<p>The derivative of an output variable wrt one of the input variables can be found by traversing the graph from the output to the input and multiplying together the derivatives for every edge on the path</p>
</blockquote>
<p>To cover every edge case, there are some extra details</p>
<blockquote>
<p>If a graph contains multiple paths from the output to an input, then the derivative is the sum of the products for each path</p>
</blockquote>
<p>This comes from the case we saw earlier where when we have different functions that have the same input we have to add their derivative chains together.</p>
<blockquote>
<p>If an edge is not the input to any function, its derivative is 1</p>
</blockquote>
<p>This covers the edge that leads from the final operation to the output. You can think of the edge having the derivative <span>\(\frac{\partial y}{\partial y}=1\)</span></p>
<p>And that’s it! Let’s try it out with <span>\(z = (x + c)x\)</span>:</p>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/images/z_eq_xx_plus_xc.png" alt="A graph of z = (x+c)x with edges annotated with derivatives"/></p>
</figure>
</div>
<p>Here, instead of writing the formulae for each derivative, I have gone ahead and calculated their actual values. Instead of just figuring out the formulae for a derivative, we want to calculate its value when we plug in our input parameters.</p>
<p>All that remains is to multiply the local derivatives together along each path. We’ll call the product of derivatives along a single path a chain (after the chain rule)</p>
<p>We can get from <span>\(z\)</span> to <span>\(x\)</span> via the green path and the red path. Following these paths, we get: <span>\[\text{red path} = 1 \times (x + c) = x + c\]</span> Along the green path we get: <span>\[\text{green path} = 1 \times x \times 1 = x\]</span></p>
<p>Adding these together, we get <span>\((x+c) + x = 2x + c\)</span></p>
<p>If we work out the derivative algebraically:</p>
<p><span>\[\frac{\partial z}{\partial x} = \frac{\partial}{\partial x}((x+c)x) = \frac{\partial}{\partial x}(x^2 + cx) = \frac{\partial x^2}{\partial x} + c\frac{\partial x}{\partial x} = 2x + c\]</span></p>
<p>We can see that it seems to work! Calculating <span>\(\frac{\partial z}{\partial c}\)</span> is left as an exercise for the reader (I’ve always wanted to say that).</p>
<p>To summarise, we have invented the following algorithm for calculating of a variable wrt its inputs:</p>
<ol type="1">
<li>Turn the equation into a graph</li>
<li>Label each edge with the appropriate derivative</li>
<li>Find every path from the output to the input variable you care about</li>
<li>Follow each path and multiply the derivatives you pass through</li>
<li>Add together the results for each path</li>
</ol>
<p>Now that we have an algorithm in pictures and words, let’s turn it into code.</p>
</section>
<section id="the-algorithm">
<h3 data-anchor-id="the-algorithm">The Algorithm™</h3>
<p>Surprisingly, we have actually already converted our functions into graphs. If you recall, when we generate a tensor from an operation, we record the inputs to the operation in the output tensor (in <code>.args</code>). We also stored the functions to calculate derivatives for each of the inputs in <code>.local_derivatives</code> which means that we know both the destination and derivative for every edge that points to a given node. This means that we’ve already completed steps 1 and 2.</p>
<p>The next challenge is to find all paths from the tensor we want to differentiate to the input tensors that created it. Because none of our operations are self referential (outputs are never fed back in as inputs), and all of our edges have a direction, our graph of operations is a directed acyclic graph or DAG. The property of the graph having no cycles means that we can find all paths to every parameter pretty easily with a Breadth First Search (or Depth First Search but BFS makes some optimisations easier as we’ll see in part 2).</p>
<p>To try it out, let’s recreate that giant graph we made earlier. We can do this by first calculating <span>\(L\)</span> from the inputs</p>
<div id="4c65eba1" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="12">
<div id="cb18"><pre><code><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span># L = (y - (mx + c))^2</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>L <span>=</span> _mul(left, right)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span># Attaching names to tensors will make our</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span># diagram look nicer</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>y.name <span>=</span> <span>&#34;y&#34;</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>m.name <span>=</span> <span>&#34;m&#34;</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>x.name <span>=</span> <span>&#34;x&#34;</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>c.name <span>=</span> <span>&#34;c&#34;</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>L.name <span>=</span> <span>&#34;L&#34;</span></span></code></pre></div>
</div>
<p>And then using Breadth First Search to do 3 things:</p>
<ul>
<li>Find all nodes</li>
<li>Find all edges</li>
<li>Find all paths from <span>\(L\)</span> to our parameters</li>
</ul>
<p>We haven’t implemented a simple way to check whether two tensors are identical so we’ll need to compare hashes.</p>
<div id="6f90fd88" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="13">
<div id="cb19"><pre><code><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>edges <span>=</span> []</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>stack <span>=</span> [(L, [L])]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>nodes <span>=</span> []</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>edges <span>=</span> []</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span>while</span> stack:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    node, current_path <span>=</span> stack.pop()</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span># Record nodes we haven&#39;t seen before</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>hash</span>(node) <span>not</span> <span>in</span> [<span>hash</span>(n) <span>for</span> n <span>in</span> nodes]:</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        nodes.append(node)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span># If we have reached a parameter (it has no arguments</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span># because it wasn&#39;t created by an operation) then</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span># record the path taken to get here</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>not</span> node.args:</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span>if</span> node.paths <span>is</span> <span>None</span>:</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            node.paths <span>=</span> []</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        node.paths.append(current_path)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span>continue</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span>for</span> arg <span>in</span> node.args:</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        stack.append((arg, current_path <span>+</span> [arg]))</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span># Record every new edge</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        edges.append((<span>hash</span>(node), <span>hash</span>(arg)))</span></code></pre></div>
</div>
<p>Now we’ve got all of the edges and nodes, we have complete knowledge of our computational graph. Let’s use networkx to plot it</p>
<div id="8fd42378" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="14">
<div id="cb20"><pre><code><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span># Assign a unique integer to each</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span># unnamed node so we know which</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span># node is which in the picture</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>labels <span>=</span> {}</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span>for</span> i, node <span>in</span> <span>enumerate</span>(nodes):</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span>if</span> node.name <span>is</span> <span>None</span>:</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        labels[<span>hash</span>(node)] <span>=</span> <span>str</span>(i)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span>else</span>:</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        labels[<span>hash</span>(node)] <span>=</span> node.name</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>graph <span>=</span> nx.DiGraph()</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>graph.add_edges_from(edges)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>pos <span>=</span> nx.nx_agraph.pygraphviz_layout(graph, prog<span>=</span><span>&#34;dot&#34;</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>nx.draw(graph, pos<span>=</span>pos, labels<span>=</span>labels)</span></code></pre></div>
<div>
<div>
<figure>
<p><img src="https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post_files/figure-html/cell-15-output-1.png" width="691" height="499"/></p>
</figure>
</div>
</div>
</div>
<p>If you squint a bit, you can see that this looks like the graph we made earlier! Let’s take a look at the paths the algorithm found from <span>\(L\)</span> to <span>\(x\)</span>.</p>
<div id="7cf96dca" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="15">
<div id="cb21"><pre><code><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span>for</span> path <span>in</span> x.paths:</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    steps <span>=</span> []</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span>for</span> step <span>in</span> path:</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        steps.append(labels[<span>hash</span>(step)])</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;-&gt;&#34;</span>.join(steps))</span></code></pre></div>
<div>
<pre><code>L-&gt;1-&gt;2-&gt;4-&gt;x
L-&gt;8-&gt;9-&gt;10-&gt;x</code></pre>
</div>
</div>
<p>The paths look correct! All we need to do now is to modify the algorithm a bit to keep track of the chain of derivatives along each path.</p>
<div id="386e7fdf" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="16">
<div id="cb23"><pre><code><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span># L = (y - (mx + c))^2</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>L <span>=</span> _mul(left, right)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>y.name <span>=</span> <span>&#34;y&#34;</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>m.name <span>=</span> <span>&#34;m&#34;</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>x.name <span>=</span> <span>&#34;x&#34;</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>c.name <span>=</span> <span>&#34;c&#34;</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>L.name <span>=</span> <span>&#34;L&#34;</span></span></code></pre></div>
</div>
<div id="28e3b011" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="17">
<div id="cb24"><pre><code><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>stack <span>=</span> [(L, [L], [])]</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>nodes <span>=</span> []</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>edges <span>=</span> []</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span>while</span> stack:</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    node, current_path, current_chain <span>=</span> stack.pop()</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span># Record nodes we haven&#39;t seen before</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>hash</span>(node) <span>not</span> <span>in</span> [<span>hash</span>(n) <span>for</span> n <span>in</span> nodes]:</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        nodes.append(node)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span># If we have reached a parameter (it has no arguments</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span># because it wasn&#39;t created by an operation) then</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span># record the path taken to get here</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>not</span> node.args:</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        <span>if</span> node.paths <span>is</span> <span>None</span>:</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>            node.paths <span>=</span> []</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        <span>if</span> node.chains <span>is</span> <span>None</span>:</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>            node.chains <span>=</span> []</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        node.paths.append(current_path)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        node.chains.append(current_chain)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        <span>continue</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span>for</span> arg, op <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        next_node <span>=</span> arg</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>        next_path <span>=</span> current_path <span>+</span> [arg]</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>        next_chain <span>=</span> current_chain <span>+</span> [op]</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>        stack.append((arg, next_path, next_chain))</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        <span># Record every new edge</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>        edges.append((<span>hash</span>(node), <span>hash</span>(arg)))</span></code></pre></div>
</div>
<p>Let’s check if the derivatives were recorded correctly.</p>
<div id="95ea9947" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="18">
<div id="cb25"><pre><code><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>f&#34;Number of chains: </span><span>{</span><span>len</span>(x.chains)<span>}</span><span>&#34;</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span>for</span> chain <span>in</span> x.chains:</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(chain)</span></code></pre></div>
<div>
<pre><code>Number of chains: 2
[Tensor(-9), Tensor(-1), Tensor(1), Tensor(2)]
[Tensor(-9), Tensor(-1), Tensor(1), Tensor(2)]</code></pre>
</div>
</div>
<p>Looks reasonable so far. We have 2 identical paths, each with 4 derivatives (one for each edge in the path) as expected.</p>
<p>Let’s multiply the derivatives together along each path and add the total for each path together and see if we get the right answer.</p>
<p>According my calculations (and <a href="https://www.wolframalpha.com/">Wolfram Alpha</a>) the derivative of <span>\(L\)</span> wrt <span>\(x\)</span> is: <span>\[\frac{\partial L}{\partial x} = 2m (c + mx - y)\]</span> Plugging the values for our tensors in, we get <span>\[2\times2 (4 + (2\times3) - 1) = 36\]</span></p>
<div id="71c45972" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="19">
<div id="cb27"><pre><code><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>total_derivative <span>=</span> Tensor(<span>0</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span>for</span> chain <span>in</span> x.chains:</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    chain_total <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span>for</span> step <span>in</span> chain:</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        chain_total <span>=</span> _mul(chain_total, step)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    total_derivative <span>=</span> _add(total_derivative, chain_total)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>total_derivative</span></code></pre></div>

</div>
<p>The correct answer! It looks like our algorithm works. All that remains is to put all the pieces together.</p>
</section>
</section>
<section id="putting-it-all-together">
<h2 data-anchor-id="putting-it-all-together">Putting it all together</h2>
<p>When dreaming up the algorithm, we kept a record of the nodes, edges and paths which made plotting and debugging easier. Now that we know that it works, we can remove these and simplify things a bit.</p>
<div id="5c3a5852" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="20">
<div id="cb29"><pre><code><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span>def</span> backward(root_node: Tensor) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    stack <span>=</span> [(root_node, [])]</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span>while</span> stack:</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span># because it wasn&#39;t created by an operation) then</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span># record the path taken to get here</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> node.args:</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>            <span>if</span> node.chains <span>is</span> <span>None</span>:</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>                node.chains <span>=</span> []</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>            node.chain.append(current_derivative)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>            <span>continue</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span>for</span> arg, op <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>            stack.append((arg, current_derivative <span>+</span> [op]))</span></code></pre></div>
</div>
<p>There is also no need (for now) to store the derivatives and calculate them separately. Instead, we can avoid a bunch of repeated calculations by multiplying the derivatives as we go.</p>
<div id="c4000db8" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="21">
<div id="cb30"><pre><code><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span>def</span> backward(root_node: Tensor) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    stack <span>=</span> [(root_node, Tensor(<span>1</span>))]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span>while</span> stack:</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>        <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        <span># because it wasn&#39;t created by an operation) then add the</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span># derivative</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> node.args:</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>            <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>                node.derivative <span>=</span> current_derivative</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            <span>else</span>:</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>                node.derivative <span>=</span> _add(node.derivative, current_derivative)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            <span>continue</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>        <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>            stack.append((arg, _mul(current_derivative, derivative)))</span></code></pre></div>
</div>
<p>Let’s make sure we didn’t break anything</p>
<div id="6bc39eac" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="22">
<div id="cb31"><pre><code><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>L <span>=</span> _mul(left, right)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>backward(L)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span>print</span>(<span>f&#34;</span><span>{</span>x<span>.</span>derivative <span>=</span> <span>}</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>x.derivative.value, want<span>=</span><span>36</span>)</span></code></pre></div>
<div>
<pre><code>x.derivative = Tensor(36)

✅ - Want: 36, Got: 36</code></pre>
</div>
</div>
<p>Let’s put this algorithm into our Tensor object</p>
<div id="208fa522" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="23">
<div id="cb33"><pre><code><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span>class</span> Tensor:</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span>    A float that can be differentiated</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    args: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    local_derivatives: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span># The derivative (once we&#39;ve calculated it).  This is None if the derivative</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    <span># has not been computed yet</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    derivative: Tensor <span>|</span> <span>None</span> <span>=</span> <span>None</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        <span>self</span>.value <span>=</span> value</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>f&#34;Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>.</span><span>__repr__</span>()<span>}</span><span>)&#34;</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    <span>def</span> backward(<span>self</span>):</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>            <span>raise</span> <span>ValueError</span>(</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>                <span>&#34;Cannot differentiate a Tensor that is not a function of other Tensors&#34;</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>        stack <span>=</span> [(<span>self</span>, Tensor(<span>1</span>))]</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>        <span>while</span> stack:</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>            node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>            <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>            <span># because it wasn&#39;t created by an operation) then add the</span></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>            <span># derivative</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>            <span>if</span> <span>not</span> node.args:</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>                <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>                    node.derivative <span>=</span> Tensor(<span>0</span>)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>                node.derivative <span>=</span> _add(node.derivative, current_derivative)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>                <span>continue</span></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>            <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>                new_derivative <span>=</span> _mul(current_derivative, derivative)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>                stack.append((arg, new_derivative))</span></code></pre></div>
</div>
<p>Let’s try it out</p>
<div id="59db14e7" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="24">
<div id="cb34"><pre><code><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>left <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>right <span>=</span> _sub(y, _add(_mul(m, x), c))</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>L <span>=</span> _mul(left, right)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>test(x.derivative, Tensor(<span>36</span>))</span></code></pre></div>
<div>
<pre><code>❌ - Want: Tensor(36), Got: Tensor(36)</code></pre>
</div>
</div>
<p>Huh?</p>
<p>By default, if you compare two objects in python with <code>==</code>, python will check whether the object on the left has the same reference as the object as the one on the right. Because <code>Tensor(36)</code> is a different object (that just happens to have the same value) to <code>x.derivative</code>, <code>x.derivative == Tensor(36)</code> returns <code>False</code>.</p>
<p>It makes a lot more sense to compare two tensors based upon their <code>.value</code>. To achieve this, we can add the <code>__eq__</code> special method to <code>Tensor</code> which will change the behaviour of the <code>==</code> operator for <code>Tensor</code> objects</p>
<div id="00be4f8d" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="25">
<div id="cb36"><pre><code><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span>def</span> <span>__eq__</span>(<span>self</span>, other) <span>-&gt;</span> <span>bool</span>:</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span>    Tells python to compare .value when applying the `==`</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span>    operation to two tensors instead of comparing references</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>&#34;Tensor&#34;</span>):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>        <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot compare a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>self</span>.value <span>==</span> other.value</span></code></pre></div>
</div>
<p>Similarly, if we try to use <code>+</code>, <code>-</code> or <code>*</code> on our tensors, we’ll get an error. We can tell python how to do these operations on our tensors by defining the following special functions:</p>
<ul>
<li><code>__add__</code> let’s us use <code>+</code></li>
<li><code>__sub__</code> let’s us use <code>-</code></li>
<li><code>__mul__</code> let’s us use <code>*</code></li>
</ul>
<div id="20dbaf3a" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="26">
<div id="cb37"><pre><code><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span>def</span> <span>__add__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>&#34;Tensor&#34;</span>):</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>        <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot add a Tensor to a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span>return</span> _add(<span>self</span>, other)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span>def</span> <span>__sub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>&#34;Tensor&#34;</span>):</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>        <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot subtract a Tensor from a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    <span>return</span> _sub(<span>self</span>, other)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span>def</span> <span>__mul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>not</span> <span>isinstance</span>(other, <span>&#34;Tensor&#34;</span>):</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>        <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot multiply a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    <span>return</span> _mul(<span>self</span>, other)</span></code></pre></div>
</div>
<p>Finally, we can add the <code>__iadd__</code>, <code>__isub__</code> and <code>__imul__</code> methods to allow us to use <code>+=</code>, <code>-=</code> and <code>*=</code>.</p>
<div id="7f0b2199" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="27">
<div id="cb38"><pre><code><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span>def</span> <span>__iadd__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span>self</span> <span>=</span> <span>self</span>.<span>__add__</span>(<span>self</span>, other)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>self</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span>def</span> <span>__isub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    <span>self</span> <span>=</span> <span>self</span>.<span>__sub__</span>(<span>self</span>, other)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>self</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span>def</span> <span>__imul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    <span>self</span> <span>=</span> <span>self</span>.<span>__mul__</span>(<span>self</span>, other)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    <span>return</span> <span>self</span></span></code></pre></div>
</div>
<p>While we’re here, let’s clean up our backward function a bit by replacing the ugly <code>_add</code> and <code>_mul</code> operations with <code>+</code> and <code>*</code>.</p>
<div id="aee373de" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="28">
<div id="cb39"><pre><code><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span>def</span> backward(<span>self</span>):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span>raise</span> <span>ValueError</span>(</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>            <span>&#34;Cannot differentiate a Tensor that is not a function of other Tensors&#34;</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    stack <span>=</span> [(<span>self</span>, Tensor(<span>1</span>))]</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span>while</span> stack:</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        <span># because it wasn&#39;t created by an operation) then add the</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        <span># derivative</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> node.args:</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>            <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>                node.derivative <span>+=</span> current_derivative</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>            <span>else</span>:</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>                node.derivative <span>+=</span> current_derivative</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>            <span>continue</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>            stack.append((arg, current_derivative <span>*</span> derivative))</span></code></pre></div>
</div>
<p>Putting all of these improvements together, we get a final <code>Tensor</code> object as follows:</p>
<div id="2eae868b" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="29">
<div id="cb40"><pre><code><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span>class</span> Tensor:</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span>    A float that can be differentiated</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    args: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    local_derivatives: <span>tuple</span>[Tensor] <span>=</span> ()</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span># The derivative (once we&#39;ve calculated it).  This is None if the derivative</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    <span># has not been computed yet</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    derivative: Tensor <span>|</span> <span>None</span> <span>=</span> <span>None</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__init__</span>(<span>self</span>, value: <span>float</span>):</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        <span>self</span>.value <span>=</span> value</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>f&#34;Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>.</span><span>__repr__</span>()<span>}</span><span>)&#34;</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__eq__</span>(<span>self</span>, other) <span>-&gt;</span> <span>bool</span>:</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>            <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot compare a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>self</span>.value <span>==</span> other.value</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__add__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>            <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot add a Tensor to a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>        <span>return</span> _add(<span>self</span>, other)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__sub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>            <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot subtract a Tensor from a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>        <span>return</span> _sub(<span>self</span>, other)</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__mul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>not</span> <span>isinstance</span>(other, Tensor):</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>            <span>raise</span> <span>TypeError</span>(<span>f&#34;Cannot multiply a Tensor with a </span><span>{</span><span>type</span>(other)<span>}</span><span>&#34;</span>)</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>        <span>return</span> _mul(<span>self</span>, other)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__iadd__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>self</span>.<span>__add__</span>(other)</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__isub__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>self</span>.<span>__sub__</span>(other)</span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__imul__</span>(<span>self</span>, other) <span>-&gt;</span> Tensor:</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>self</span>.<span>__mul__</span>(other)</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__repr__</span>(<span>self</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>        <span>return</span> <span>f&#34;Tensor(</span><span>{</span><span>self</span><span>.</span>value<span>}</span><span>)&#34;</span></span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a>    <span>def</span> backward(<span>self</span>):</span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>self</span>.args <span>is</span> <span>None</span> <span>or</span> <span>self</span>.local_derivatives <span>is</span> <span>None</span>:</span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a>            <span>raise</span> <span>ValueError</span>(</span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a>                <span>&#34;Cannot differentiate a Tensor that is not a function of other Tensors&#34;</span></span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a>        stack <span>=</span> [(<span>self</span>, Tensor(<span>1</span>))]</span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a>        <span>while</span> stack:</span>
<span id="cb40-59"><a href="#cb40-59" aria-hidden="true" tabindex="-1"></a>            node, current_derivative <span>=</span> stack.pop()</span>
<span id="cb40-60"><a href="#cb40-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-61"><a href="#cb40-61" aria-hidden="true" tabindex="-1"></a>            <span># if we have reached a parameter (it has no arguments</span></span>
<span id="cb40-62"><a href="#cb40-62" aria-hidden="true" tabindex="-1"></a>            <span># because it wasn&#39;t created by an operation) then add the</span></span>
<span id="cb40-63"><a href="#cb40-63" aria-hidden="true" tabindex="-1"></a>            <span># current_derivative to derivative</span></span>
<span id="cb40-64"><a href="#cb40-64" aria-hidden="true" tabindex="-1"></a>            <span>if</span> <span>not</span> node.args:</span>
<span id="cb40-65"><a href="#cb40-65" aria-hidden="true" tabindex="-1"></a>                <span>if</span> node.derivative <span>is</span> <span>None</span>:</span>
<span id="cb40-66"><a href="#cb40-66" aria-hidden="true" tabindex="-1"></a>                    node.derivative <span>=</span> current_derivative</span>
<span id="cb40-67"><a href="#cb40-67" aria-hidden="true" tabindex="-1"></a>                <span>else</span>:</span>
<span id="cb40-68"><a href="#cb40-68" aria-hidden="true" tabindex="-1"></a>                    node.derivative <span>+=</span> current_derivative</span>
<span id="cb40-69"><a href="#cb40-69" aria-hidden="true" tabindex="-1"></a>                <span>continue</span></span>
<span id="cb40-70"><a href="#cb40-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-71"><a href="#cb40-71" aria-hidden="true" tabindex="-1"></a>            <span>for</span> arg, derivative <span>in</span> <span>zip</span>(node.args, node.local_derivatives):</span>
<span id="cb40-72"><a href="#cb40-72" aria-hidden="true" tabindex="-1"></a>                stack.append((arg, current_derivative <span>*</span> derivative))</span></code></pre></div>
</div>
<p>Let’s take it for a spin. We’ll try calculating <span>\(L\)</span> again</p>
<div id="db25982d" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="30">
<div id="cb41"><pre><code><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>y <span>=</span> Tensor(<span>1</span>)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>m <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>x <span>=</span> Tensor(<span>3</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>c <span>=</span> Tensor(<span>4</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>diff <span>=</span> y <span>-</span> ((m <span>*</span> x) <span>+</span> c)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>L <span>=</span> diff <span>*</span> diff</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>x.derivative, want<span>=</span>Tensor(<span>36</span>))</span></code></pre></div>
<div>
<pre><code>✅ - Want: Tensor(36), Got: Tensor(36)</code></pre>
</div>
</div>
<p>Much easier!</p>
<p>To really see what this baby can do, I asked a language model for the most complicated expression it could think of and it gave me this:</p>
<p><span>\[f(x) = (2x^3 + 4x^2 - 5x) \times (3x^2 - 2x + 7) - (6x^4 + 2x^3 - 8x^2) + (5x^2 - 3x)\]</span> According to <a href="https://www.wolframalpha.com/">Wolfram Alpha</a>, the derivative of this expression is: <span>\[\frac{d f(x)}{dx} = -38 + 102 x - 33 x^2 + 8 x^3 + 30 x^4\]</span></p>
<p>If we plug 2 into this equation, the answer is apparently 578 (again, thanks to <a href="https://www.wolframalpha.com/">Wolfram Alpha</a>).</p>
<p>Let’s try it with our algorithm</p>
<div id="0fd9b01f" data-vscode="{&#34;languageId&#34;:&#34;python&#34;}" data-execution_count="31">
<div id="cb43"><pre><code><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>x <span>=</span> Tensor(<span>2</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>y <span>=</span> (</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    (Tensor(<span>2</span>) <span>*</span> x <span>*</span> x <span>*</span> x <span>+</span> Tensor(<span>4</span>) <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>5</span>) <span>*</span> x)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span>*</span> (Tensor(<span>3</span>) <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>2</span>) <span>*</span> x <span>+</span> Tensor(<span>7</span>))</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span>-</span> (Tensor(<span>6</span>) <span>*</span> x <span>*</span> x <span>*</span> x <span>*</span> x <span>+</span> Tensor(<span>2</span>) <span>*</span> x <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>8</span>) <span>*</span> x <span>*</span> x)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span>+</span> (Tensor(<span>5</span>) <span>*</span> x <span>*</span> x <span>-</span> Tensor(<span>3</span>) <span>*</span> x)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>test(got<span>=</span>x.derivative, want<span>=</span>Tensor(<span>578</span>))</span></code></pre></div>
<div>
<pre><code>✅ - Want: Tensor(578), Got: Tensor(578)</code></pre>
</div>
</div>
<p>Once again, we got the right answer!</p>
</section>
<section id="conclusion">

<p>From nothing, we have now written an algorithm that will let us differentiate any mathematical expression (provided it only involves addition, subtraction and multiplication). We did this by converting our expression into a graph and re-imagining partial derivatives as operations on the edges of that graph. Then we found that we could apply Breadth First Search to combine all the derivatives together to get a final answer.</p>
<p>Differentiating scalars is (I hope you agree) interesting, but it isn’t exactly GPT-4. That said, with a few small modifications to our algorithm, we can extend our algorithm to handle multi-dimensional tensors like matrices and vectors. Once you can do that, you can build up to backpropagation and, eventually, to a fully functional language model.</p>
<p>Next time, we’ll extend our algorithm to vectors and matrices and build up from there to a working neural network. If you want to peek ahead, you can check out the repo for <a href="https://github.com/bclarkson-code/Tricycle">Tricycle</a> which is the name for the deep learning framework we’re building.</p>


</section>

</main> <!-- /main -->


</div></div>
  </body>
</html>
