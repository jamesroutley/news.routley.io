<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/tekaratzas/RustGPT">Original</a>
    <h1>RustGPT: A pure-Rust transformer LLM built from scratch</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>RustGPT-demo-zoon.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/9121201/489286065-ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTc5MzUxOTcsIm5iZiI6MTc1NzkzNDg5NywicGF0aCI6Ii85MTIxMjAxLzQ4OTI4NjA2NS1lYzRhNDEwMC1iMDNhLTRiM2MtYTdkNi04MDZlYTU0ZWQ0ZWQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkxNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MTVUMTExNDU3WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTJhNWIzNzY3OTVlZDMyMGFmODk5MTliNWJiNTFjZDJiM2VkNzI3MTIwODY5ZThkMTFjZDcxMmQzZTUwZDI0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.sMw7jTi8vKMsMm-Kq0jORyehkHqKtoWdig3gsKxrHBY" data-canonical-src="https://private-user-images.githubusercontent.com/9121201/489286065-ec4a4100-b03a-4b3c-a7d6-806ea54ed4ed.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTc5MzUxOTcsIm5iZiI6MTc1NzkzNDg5NywicGF0aCI6Ii85MTIxMjAxLzQ4OTI4NjA2NS1lYzRhNDEwMC1iMDNhLTRiM2MtYTdkNi04MDZlYTU0ZWQ0ZWQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDkxNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA5MTVUMTExNDU3WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTJhNWIzNzY3OTVlZDMyMGFmODk5MTliNWJiNTFjZDJiM2VkNzI3MTIwODY5ZThkMTFjZDcxMmQzZTUwZDI0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.sMw7jTi8vKMsMm-Kq0jORyehkHqKtoWdig3gsKxrHBY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">A complete <strong>Large Language Model implementation in pure Rust</strong> with no external ML frameworks. Built from the ground up using only <code>ndarray</code> for matrix operations.</p>

<p dir="auto">This project demonstrates how to build a transformer-based language model from scratch in Rust, including:</p>
<ul dir="auto">
<li><strong>Pre-training</strong> on factual text completion</li>
<li><strong>Instruction tuning</strong> for conversational AI</li>
<li><strong>Interactive chat mode</strong> for testing</li>
<li><strong>Full backpropagation</strong> with gradient clipping</li>
<li><strong>Modular architecture</strong> with clean separation of concerns</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">🔍 Key Files to Explore</h2><a id="user-content--key-files-to-explore" aria-label="Permalink: 🔍 Key Files to Explore" href="#-key-files-to-explore"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Start with these two core files to understand the implementation:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/tekaratzas/RustGPT/blob/main/src/main.rs"><code>src/main.rs</code></a></strong> - Training pipeline, data preparation, and interactive mode</li>
<li><strong><a href="https://github.com/tekaratzas/RustGPT/blob/main/src/llm.rs"><code>src/llm.rs</code></a></strong> - Core LLM implementation with forward/backward passes and training logic</li>
</ul>

<p dir="auto">The model uses a <strong>transformer-based architecture</strong> with the following components:</p>
<div data-snippet-clipboard-copy-content="Input Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions"><pre><code>Input Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions
</code></pre></div>

<div data-snippet-clipboard-copy-content="src/
├── main.rs              # 🎯 Training pipeline and interactive mode
├── llm.rs               # 🧠 Core LLM implementation and training logic
├── lib.rs               # 📚 Library exports and constants
├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)
├── self_attention.rs    # 👀 Multi-head self-attention mechanism  
├── feed_forward.rs      # ⚡ Position-wise feed-forward networks
├── embeddings.rs        # 📊 Token embedding layer
├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions
├── vocab.rs            # 📝 Vocabulary management and tokenization
├── layer_norm.rs       # 🧮 Layer normalization
└── adam.rs             # 🏃 Adam optimizer implementation

tests/
├── llm_test.rs         # Tests for core LLM functionality
├── transformer_test.rs # Tests for transformer blocks
├── self_attention_test.rs # Tests for attention mechanisms
├── feed_forward_test.rs # Tests for feed-forward layers
├── embeddings_test.rs  # Tests for embedding layers
├── vocab_test.rs       # Tests for vocabulary handling
├── adam_test.rs        # Tests for optimizer
└── output_projection_test.rs # Tests for output layer"><pre><code>src/
├── main.rs              # 🎯 Training pipeline and interactive mode
├── llm.rs               # 🧠 Core LLM implementation and training logic
├── lib.rs               # 📚 Library exports and constants
├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)
├── self_attention.rs    # 👀 Multi-head self-attention mechanism  
├── feed_forward.rs      # ⚡ Position-wise feed-forward networks
├── embeddings.rs        # 📊 Token embedding layer
├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions
├── vocab.rs            # 📝 Vocabulary management and tokenization
├── layer_norm.rs       # 🧮 Layer normalization
└── adam.rs             # 🏃 Adam optimizer implementation

tests/
├── llm_test.rs         # Tests for core LLM functionality
├── transformer_test.rs # Tests for transformer blocks
├── self_attention_test.rs # Tests for attention mechanisms
├── feed_forward_test.rs # Tests for feed-forward layers
├── embeddings_test.rs  # Tests for embedding layers
├── vocab_test.rs       # Tests for vocabulary handling
├── adam_test.rs        # Tests for optimizer
└── output_projection_test.rs # Tests for output layer
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">🧪 What The Model Learns</h2><a id="user-content--what-the-model-learns" aria-label="Permalink: 🧪 What The Model Learns" href="#-what-the-model-learns"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The implementation includes two training phases:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Pre-training</strong>: Learns basic world knowledge from factual statements</p>
<ul dir="auto">
<li>&#34;The sun rises in the east and sets in the west&#34;</li>
<li>&#34;Water flows downhill due to gravity&#34;</li>
<li>&#34;Mountains are tall and rocky formations&#34;</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Instruction Tuning</strong>: Learns conversational patterns</p>
<ul dir="auto">
<li>&#34;User: How do mountains form? Assistant: Mountains are formed through tectonic forces...&#34;</li>
<li>Handles greetings, explanations, and follow-up questions</li>
</ul>
</li>
</ol>

<div dir="auto" data-snippet-clipboard-copy-content="# Clone and run
git clone &lt;your-repo&gt;
cd llm
cargo run

# The model will:
# 1. Build vocabulary from training data
# 2. Pre-train on factual statements (100 epochs)  
# 3. Instruction-tune on conversational data (100 epochs)
# 4. Enter interactive mode for testing"><pre><span><span>#</span> Clone and run</span>
git clone <span>&lt;</span>your-repo<span>&gt;</span>
<span>cd</span> llm
cargo run

<span><span>#</span> The model will:</span>
<span><span>#</span> 1. Build vocabulary from training data</span>
<span><span>#</span> 2. Pre-train on factual statements (100 epochs)  </span>
<span><span>#</span> 3. Instruction-tune on conversational data (100 epochs)</span>
<span><span>#</span> 4. Enter interactive mode for testing</span></pre></div>

<p dir="auto">After training, test the model interactively:</p>
<div data-snippet-clipboard-copy-content="Enter prompt: How do mountains form?
Model output: Mountains are formed through tectonic forces or volcanism over long geological time periods

Enter prompt: What causes rain?
Model output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne"><pre><code>Enter prompt: How do mountains form?
Model output: Mountains are formed through tectonic forces or volcanism over long geological time periods

Enter prompt: What causes rain?
Model output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">🧮 Technical Implementation</h2><a id="user-content--technical-implementation" aria-label="Permalink: 🧮 Technical Implementation" href="#-technical-implementation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<ul dir="auto">
<li><strong>Vocabulary Size</strong>: Dynamic (built from training data)</li>
<li><strong>Embedding Dimension</strong>: 128</li>
<li><strong>Hidden Dimension</strong>: 256</li>
<li><strong>Max Sequence Length</strong>: 80 tokens</li>
<li><strong>Architecture</strong>: 3 Transformer blocks + embeddings + output projection</li>
</ul>

<ul dir="auto">
<li><strong>Optimizer</strong>: Adam with gradient clipping</li>
<li><strong>Pre-training LR</strong>: 0.0005 (100 epochs)</li>
<li><strong>Instruction Tuning LR</strong>: 0.0001 (100 epochs)</li>
<li><strong>Loss Function</strong>: Cross-entropy loss</li>
<li><strong>Gradient Clipping</strong>: L2 norm capped at 5.0</li>
</ul>

<ul dir="auto">
<li><strong>Custom tokenization</strong> with punctuation handling</li>
<li><strong>Greedy decoding</strong> for text generation</li>
<li><strong>Gradient clipping</strong> for training stability</li>
<li><strong>Modular layer system</strong> with clean interfaces</li>
<li><strong>Comprehensive test coverage</strong> for all components</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="# Run all tests
cargo test

# Test specific components
cargo test --test llm_test
cargo test --test transformer_test
cargo test --test self_attention_test

# Build optimized version
cargo build --release

# Run with verbose output
cargo test -- --nocapture"><pre><span><span>#</span> Run all tests</span>
cargo <span>test</span>

<span><span>#</span> Test specific components</span>
cargo <span>test</span> --test llm_test
cargo <span>test</span> --test transformer_test
cargo <span>test</span> --test self_attention_test

<span><span>#</span> Build optimized version</span>
cargo build --release

<span><span>#</span> Run with verbose output</span>
cargo <span>test</span> -- --nocapture</pre></div>

<p dir="auto">This implementation demonstrates key ML concepts:</p>
<ul dir="auto">
<li><strong>Transformer architecture</strong> (attention, feed-forward, layer norm)</li>
<li><strong>Backpropagation</strong> through neural networks</li>
<li><strong>Language model training</strong> (pre-training + fine-tuning)</li>
<li><strong>Tokenization</strong> and vocabulary management</li>
<li><strong>Gradient-based optimization</strong> with Adam</li>
</ul>
<p dir="auto">Perfect for understanding how modern LLMs work under the hood!</p>

<ul dir="auto">
<li><code>ndarray</code> - N-dimensional arrays for matrix operations</li>
<li><code>rand</code> + <code>rand_distr</code> - Random number generation for initialization</li>
</ul>
<p dir="auto">No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!</p>

<p dir="auto">Contributions are welcome! This project is perfect for learning and experimentation.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">High Priority Features Needed</h3><a id="user-content-high-priority-features-needed" aria-label="Permalink: High Priority Features Needed" href="#high-priority-features-needed"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>🏪 Model Persistence</strong> - Save/load trained parameters to disk (currently all in-memory)</li>
<li><strong>⚡ Performance optimizations</strong> - SIMD, parallel training, memory efficiency</li>
<li><strong>🎯 Better sampling</strong> - Beam search, top-k/top-p, temperature scaling</li>
<li><strong>📊 Evaluation metrics</strong> - Perplexity, benchmarks, training visualizations</li>
</ul>

<ul dir="auto">
<li><strong>Advanced architectures</strong> (multi-head attention, positional encoding, RoPE)</li>
<li><strong>Training improvements</strong> (different optimizers, learning rate schedules, regularization)</li>
<li><strong>Data handling</strong> (larger datasets, tokenizer improvements, streaming)</li>
<li><strong>Model analysis</strong> (attention visualization, gradient analysis, interpretability)</li>
</ul>

<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch: <code>git checkout -b feature/model-persistence</code></li>
<li>Make your changes and add tests</li>
<li>Run the test suite: <code>cargo test</code></li>
<li>Submit a pull request with a clear description</li>
</ol>

<ul dir="auto">
<li>Follow standard Rust conventions (<code>cargo fmt</code>)</li>
<li>Add comprehensive tests for new features</li>
<li>Update documentation and README as needed</li>
<li>Keep the &#34;from scratch&#34; philosophy - avoid heavy ML dependencies</li>
</ul>

<ul dir="auto">
<li>🚀 <strong>Beginner</strong>: Model save/load, more training data, config files</li>
<li>🔥 <strong>Intermediate</strong>: Beam search, positional encodings, training checkpoints</li>
<li>⚡ <strong>Advanced</strong>: Multi-head attention, layer parallelization, custom optimizations</li>
</ul>
<p dir="auto">Questions? Open an issue or start a discussion!</p>
<p dir="auto">No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!</p>
</article></div></div>
  </body>
</html>
