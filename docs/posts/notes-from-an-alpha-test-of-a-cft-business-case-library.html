<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://commoncog.com/blog/notes-on-building-a-business-case-library/">Original</a>
    <h1>Notes from an Alpha Test of a CFT Business Case Library</h1>
    
    <div id="readability-page-1" class="page"><div id="content">

<p><time>June 15, 2022</time></p><div>
<p>Datasets are easier to reuse if they use standards that are well-established, particularly in a given domain.</p>
<p>A first approach is to ask around – ask people with whom you coauthor , people you trust in your field, etc.</p>
<p>A follow-on approach is to examine the “graph reputation” of relevant standards, particularly if they may be represented as resources with outbound links. We can use the PageRank algorithm, just like Google uses it to index the web of documents.</p>
<p>An an example, here I outline an initial approach to find the “most reputable” of <a href="https://lov.linkeddata.es/">Linked Open Vocabularies&#39;</a> 778 vocabularies.</p>
<p>My starting point is having the API responses for each vocabulary so that <code>lov</code> is a list of <code>dict</code>s, each with keys <code>url: str</code> and <code>api_response: dict</code>.</p>
<ol>
<li>Collect all outbound links:</li>
</ol>
<div><pre tabindex="0"><code data-lang="python"><span>for</span> entry <span>in</span> lov:
    entry[<span>&#34;outbound_links&#34;</span>] <span>=</span> entry<span>.</span>get(<span>&#34;outbound_links&#34;</span>, set())
    <span>for</span> version <span>in</span> entry[<span>&#34;api_response&#34;</span>]<span>.</span>get(<span>&#34;versions&#34;</span>, {}):
        <span>for</span> field, value <span>in</span> version<span>.</span>items():
            <span>if</span> field<span>.</span>startswith(<span>&#34;rel&#34;</span>) <span>and</span> isinstance(value, list):
                entry[<span>&#34;outbound_links&#34;</span>] <span>|=</span> {v <span>for</span> v <span>in</span> value}
</code></pre></div><ol start="2">
<li>Prepare a stream of self_link, outbound_link pairs:</li>
</ol>
<div><pre tabindex="0"><code data-lang="python"><span>with</span> open(<span>&#34;lov-outlinks.csv&#34;</span>,<span>&#39;w&#39;</span>) <span>as</span> f:
    <span>for</span> entry <span>in</span> lov:
        url <span>=</span> entry[<span>&#34;url&#34;</span>]
        <span>for</span> link_url <span>in</span> entry[<span>&#34;outbound_links&#34;</span>]:
            f<span>.</span>write(<span>f</span><span>&#34;</span><span>{</span>url<span>}</span><span>,</span><span>{</span>link_url<span>}</span><span>\n</span><span>&#34;</span>)
</code></pre></div><ol start="3">
<li>In a file, e.g. <code>lov_pagerank.py</code>:<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></li>
</ol>
<div><pre tabindex="0"><code data-lang="python"><span>if</span> __name__ <span>==</span> <span>&#34;__main__&#34;</span>: <span># for `spark-submit`</span>
    sc <span>=</span> SparkContext(appName<span>=</span><span>&#34;LovRankings&#34;</span>)
    match_data <span>=</span> sc<span>.</span>textFile(<span>&#34;lov-outlinks.csv&#34;</span>)

    xs <span>=</span> match_data<span>.</span>map(get_linking)<span>.</span>groupByKey()<span>.</span>mapValues(initialize_for_voting)

    <span>for</span> i <span>in</span> range(<span>20</span>):
        <span>if</span> i <span>&gt;</span> <span>0</span>:
            xs <span>=</span> sc<span>.</span>parallelize(zs<span>.</span>items())
        acc <span>=</span> dict(xs<span>.</span>mapValues(empty_ratings)<span>.</span>collect())
        zs <span>=</span> xs<span>.</span>aggregate(acc, allocate_points, combine_ratings)

    ratings <span>=</span> [(k, v[<span>&#34;rating&#34;</span>]) <span>for</span> k, v <span>in</span> zs<span>.</span>items()]
    <span>for</span> i, (vocab, rating) <span>in</span> enumerate(
        sorted(ratings, key<span>=</span><span>lambda</span> x: x[<span>1</span>], reverse<span>=</span><span>True</span>)[:<span>100</span>]
    ):
        print(<span>&#34;</span><span>{:3}</span><span>\t</span><span>{:6}</span><span>\t</span><span>{}</span><span>&#34;</span><span>.</span>format(i <span>+</span> <span>1</span>, round(log2(rating <span>+</span> <span>1</span>), <span>1</span>), vocab))
</code></pre></div><p>where, above it:</p>
<div><pre tabindex="0"><code data-lang="python"><span>from</span> math <span>import</span> log2
<span>from</span> pyspark <span>import</span> SparkContext
<span>from</span> toolz <span>import</span> assoc


<span>def</span> <span>get_linking</span>(line):
    <span>return</span> line<span>.</span>split(<span>&#34;,&#34;</span>)


<span>def</span> <span>initialize_for_voting</span>(outlinks):
    <span>return</span> {<span>&#34;outlinks&#34;</span>: outlinks, <span>&#34;n_outlinks&#34;</span>: len(outlinks), <span>&#34;rating&#34;</span>: <span>100</span>}


<span>def</span> <span>empty_ratings</span>(d):
    <span>return</span> assoc(d, <span>&#34;rating&#34;</span>, <span>0</span>)


<span>def</span> <span>allocate_points</span>(acc, new):
    _, v <span>=</span> new
    boost <span>=</span> v[<span>&#34;rating&#34;</span>] <span>/</span> (v[<span>&#34;n_outlinks&#34;</span>] <span>+</span> <span>0.01</span>)
    <span>for</span> link <span>in</span> v[<span>&#34;outlinks&#34;</span>]:
        <span>if</span> link <span>not</span> <span>in</span> acc<span>.</span>keys():
            acc[link] <span>=</span> {<span>&#34;outlinks&#34;</span>: [], <span>&#34;n_outlinks&#34;</span>: <span>0</span>}
        link_rating <span>=</span> acc<span>.</span>get(link, {})<span>.</span>get(<span>&#34;rating&#34;</span>, <span>0</span>)
        acc[link][<span>&#34;rating&#34;</span>] <span>=</span> link_rating <span>+</span> boost
    <span>return</span> acc


<span>def</span> <span>combine_ratings</span>(a, b):
    <span>for</span> k, v <span>in</span> b<span>.</span>items():
        <span>try</span>:
            a[k][<span>&#34;rating&#34;</span>] <span>=</span> a[k][<span>&#34;rating&#34;</span>] <span>+</span> b[k][<span>&#34;rating&#34;</span>]
        <span>except</span> <span>KeyError</span>:
            a[k] <span>=</span> v
    <span>return</span> a
</code></pre></div><p>And here is the output of <code>spark-submit lov_pagerank.py</code>:</p>
<pre tabindex="0"><code>  1       10.6  http://purl.org/dc/elements/1.1/
  2       10.3  http://www.w3.org/2000/01/rdf-schema#
  3       10.3  http://www.w3.org/1999/02/22-rdf-syntax-ns#
  4        9.0  http://www.w3.org/2004/02/skos/core#
  5        8.9  http://purl.org/dc/terms/
  6        6.3  http://xmlns.com/foaf/0.1/
  7        6.3  http://www.w3.org/2002/07/owl#
  8        6.3  http://purl.org/dc/dcmitype/
...
</code></pre><p>We can see at a glance the “most reputable” vocabularies, and they don’t surprise me. What may be more helpful is to collect candidate vocabularies for your domain and focus on their relative scores in order to gauge whether any are “well-established” in a sense. Even more helpful may be to include multiple “types” of resources – with standards linking to and being linked from various databases and policies. <a href="https://fairsharing.org/">FAIRSharing</a> seems like it could eventually support open investigation of the latter kind.</p>
<p><small>
This post was adapted from a note sent to my email list on Machine-Centric Science.
</small>
</p>
<section role="doc-endnotes">
<hr/>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Adapted from J. T. Wolohan, <em>Mastering large datasets with Python: parallelize and distribute your Python code</em>. Shelter Island, NY: Manning Publications Co, 2019. <a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

</div>




</div></div>
  </body>
</html>
