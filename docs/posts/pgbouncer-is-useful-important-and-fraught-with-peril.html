<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jpcamara.com/2023/04/12/pgbouncer-is-useful.html">Original</a>
    <h1>PgBouncer is useful, important, and fraught with peril</h1>
    
    <div id="readability-page-1" class="page"><section><p><img src="https://cdn.uploads.micro.blog/98548/2023/e0723a5982.png" alt=""/></p>
<p>To start, I want to say that I‚Äôm appreciative that PgBouncer exists and the work its open source maintainers put into it. I also love working with PostgreSQL, and I‚Äôm thankful for the incredible amount of work and improvements that go into it as well.</p>
<p>I also think community and industry enthusiasm around Postgres is at an all time high. There are more managed hosting options than ever (<a href="https://www.crunchydata.com">Crunchy Data</a>, <a href="https://render.com/docs/databases">Render</a>, <a href="https://fly.io/docs/postgres/">Fly.io</a>, and on and on), deep extensions like <a href="https://postgresml.org">PostgresML</a>, <a href="https://www.citusdata.com">Citus</a> and <a href="https://www.timescale.com">Timescale</a>, serverless options like <a href="https://neon.tech">Neon</a>, and real-time services like <a href="https://supabase.com">Supabase</a> with Postgres at their center. Postgres is a robust, advanced and <em>fast</em> RDBMS capable of handling the needs of most every application.</p>
<p>I just find the current state of recommendations and guidance around scaling Postgres to be confounding. And it feels surprising for new Postgres users to discover that one of the most common scaling options relies on a solution like PgBouncer.</p>
<p>Over the years I‚Äôve read dozens of articles around scaling and maintaining Postgres databases, and they always understate the impact of PgBouncer on your application. They casually mention unusable features without any exploration, or the numerous ways you can silently break expected query behavior. The advice is just to turn it on. <strong>I want it to be clear that as your application scales, PgBouncer is often necessary but isn‚Äôt free</strong>.</p>
<p>The following sections provide an overview of what connection pooling is in general, how connection pooling modes work in PgBouncer and similar tools, and then I dig into every Postgres feature that does not work in PgBouncer transaction mode and what the implications are. This is the PgBouncer article I wish existed the first time I used it - let‚Äôs get going üêò!</p>
<h3 id="contents">Contents</h3>
<ul>
<li><a href="#connection-pooling">What is connection pooling?</a></li>
<li><a href="#separate-tool">Why do I need a separate tool from Postgres?</a>
<ul>
<li><a href="#framework-pool">Framework pooling</a></li>
<li><a href="#client-pool">Client proxy pooling</a></li>
<li><a href="#server-pool">Server proxy pooling</a></li>
</ul>
</li>
<li><a href="#turn-it-on">Can I just turn on PgBouncer and get scaling for free?</a>
<ul>
<li><a href="#session-mode">Session mode</a></li>
<li><a href="#statement-mode">Statement mode</a></li>
<li><a href="#transaction-mode">Transaction mode</a></li>
</ul>
</li>
<li><a href="#perils">Perils</a>
<ul>
<li><a href="#invalid-statements">Detecting invalid statements üòë</a></li>
<li><a href="#lock-timeouts">Lock timeouts (SET/RESET) üîì</a></li>
<li><a href="#statement-timeouts">Statement timeouts (SET/RESET) ‚è≥</a></li>
<li><a href="#transparency">Transparency üëª</a></li>
<li><a href="#prepared-statements">Prepared Statements (PREPARE/DEALLOCATE, Protocol-level prepared plans) ‚úîÔ∏è</a></li>
<li><a href="#throughput">Pool throughout / Long running queries üèÉ‚Äç‚ôÇÔ∏è</a></li>
<li><a href="#session-level-locks">Session Level Advisory Locks üîê¬†</a></li>
<li><a href="#listen-notify">Listen/Notify üì£</a></li>
<li><a href="#single-threaded">The single thread ü™°¬†</a></li>
<li><a href="#pg_dump">pg_dump üöÆ</a></li>
<li><a href="#unavailable">Other unavailable features ü´•</a></li>
</ul>
</li>
<li><a href="#linting">Linting üß∂</a></li>
<li><a href="#future-improvements">Can we improve connections without a pooler?</a></li>
<li><a href="#alternatives">PgBouncer alternatives</a></li>
</ul>
<h2 id="connection-pooling">What is connection pooling?</h2>
<p>PgBouncer is a lightweight connection pooler for PostgreSQL. What does that mean exactly? What is connection pooling and why is it needed?</p>
<p>Opening a connection is expensive: a new Postgres client connection involves TCP setup, process creation and backend initialization ‚Äì all of which are costly in terms of time and system resources. A connection pool keeps a set of connections available for reuse so we can avoid that overhead past initial connection.</p>
<p>There are three main levels of connection pooling<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>:</p>

<p><img src="https://cdn.uploads.micro.blog/98548/2023/0c4ffd30c0.jpg" alt=""/></p>
<p><strong>Framework connection pooling</strong>. This is a common feature of many frameworks/libraries. Within a given process, you maintain a pool of active connections that are shared between code, generally running across threads. Whenever you handle some processing in a server request, a background process, a job, etc, you open a connection and keep that connection open. When that piece of work finishes and a new piece of work starts, you can reuse the connection without the expense of opening a new connection to the database every single time. These connections are usually local to a particular operating system process, so you gain no benefit outside of that process (and if you‚Äôre scaling Postgres, you probably have lots of processes)</p>

<p><img src="https://cdn.uploads.micro.blog/98548/2023/d281d0294d.jpg" alt=""/></p>
<p>One level higher, you can have <strong>client level connection pooling</strong> outside of your code. PgBouncer can handle this, and instead of independent unsharable process-isolated connections you proxy all of your connections through PgBouncer. But it runs on your server, so you still cannot share connections between servers (and again, when needing to do it you probably have lots of servers).</p>

<p><img src="https://cdn.uploads.micro.blog/98548/2023/cd987b4e04.jpg" alt=""/></p>
<p><strong>Server level connection pooling</strong>. Here we host PgBouncer independent of our servers and connect to a single central PgBouncer instance<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>. This is the most robust form of connection pooling because independent of anything else in your code or server, you are guaranteed that any client connection is coming from the pool.</p>

<p>That‚Äôs all great but‚Ä¶ why do we need it?</p>
<p>There are two primary layers to this:</p>
<ol>
<li>Maintaining connections is beneficial as a base feature. Less memory and io churn, less latency before running queries. Less pressure on the database constantly opening and closing connections.</li>
<li>Postgres connections get expensive very quickly. <em>Surprisingly</em> quickly.</li>
</ol>
<p>Here are some general community guidelines around allowable Postgres connection counts based on a mixture of community experience and specific benchmarking:</p>
<ul>
<li>In terms of what some managed services even offer: <a href="https://supabase.com/blog/supabase-pgbouncer">Supabase</a> offers a max of <em>50</em> connections, <a href="https://neon.tech">Neon</a> offers a max of <em>100</em> connections, and <a href="https://render.com/docs/databases#connecting-to-your-database">Render</a> offers a max of 397 connections.</li>
<li>The general upper bound recommendation is a <em>max</em> of 500 active connections. Services like <a href="https://elements.heroku.com/addons/heroku-postgresql">Heroku Postgres</a> even <em>enforce</em> a hard limit of 500 connections<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></li>
<li>Even at 500 connections, your server is going to be strained. <a href="https://www.enterprisedb.com/postgres-tutorials/why-you-should-use-connection-pooling-when-setting-maxconnections-postgres">This more recent (as of 2023) enterprisedb article</a> analyzed connection performance and found that 300-400 active connections seems optimal. This <a href="https://brandur.org/postgres-connections">article from Brandur</a> is older (2018) but seems to reinforce this idea as well</li>
<li>There have been <a href="https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/improving-postgres-connection-scalability-snapshots/ba-p/1806462">some more recent connection improvements in Postgres</a> (as of version 14) handling idle connections more efficiently<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>, but active connections are still expensive<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> and idle connections have not reached the scale of a dedicated pooler</li>
<li>The reality of 500 connections is it sounds extremely low but those connections can handle <em>a ton of throughput</em>. The <em>problem</em> is, as a metric of pure concurrency, real connections have a hard upper limit. So if you try to have five thousand clients connect simultaneously, you‚Äôre going to start getting loads of connection errors<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>.</li>
</ul>
<p>To improve the cost of connection overhead, general connection pooling is helpful and a PgBouncer instance in its default session based mode can handle it. But to improve concurrency things have to get a bit‚Ä¶ <em>quirky</em>.</p>
<p>There are two modes in PgBouncer which give clients access to more connections than Postgres <em>actually</em> has available. They rely on the idea that at any given time many of your connections are idle, so you can free up usage of idle connections to improve concurrency.</p>
<h2 id="turn-it-on">Can I just turn on PgBouncer and get scaling for free?</h2>
<p>Kind of? But not really? It‚Äôs complicated.</p>
<p>Internally, PgBouncer will manage a pool of connections for you. The default pooling mode it starts with, session pooling, is conservative, and in most cases will not provide improved concurrency<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.</p>
<p>I‚Äôm going to hand wave a bit past two of the modes and focus on the typical recommendation.</p>

<p><img src="https://cdn.uploads.micro.blog/98548/2023/7391257b1c.jpg" alt=""/></p>
<p><strong>Session mode</strong> is the default and most conservative mode. This is a 1:1 - your local connection truly holds onto a full connection until you close it. This does little to help you scale connection concurrency, but it helps with latency and connection churn overhead.</p>

<p><img src="https://cdn.uploads.micro.blog/98548/2023/6e486ad1b1.jpg" alt=""/></p>
<p><strong>Statement mode</strong> is the most aggressive mode and means your connection goes back into the pool after <em>every statement</em>. You lose the ability to use transactions üò∞ - that seems wild and unusable for only the most specialized of use cases.</p>

<p><img src="https://cdn.uploads.micro.blog/98548/2023/9e0451a965.jpg" alt=""/></p>
<p>The mode that results in a more sane balance of improved concurrency and retained critical database features is <strong>transaction mode</strong>. Transaction mode means your connection stays consistent as long as you‚Äôre in a transaction. Once the transaction finishes, your code <em>thinks</em> it still has real connection but PgBouncer actually releases the connection back into the pool internally. This is <em>session sharing</em>, your session is going to be shared with other connections without being reset or closed.</p>
<p>Transaction mode is a powerful concept. Your code in general has lots of database downtime. Most code does not solely operate on the database - it takes CPU cycles, interacts with files, makes network calls, and calls other data stores. During that time, your connection sits idle and unused for what in computing and database terms is an eternity. By releasing that back into the pool outside of transactions you free up your idle connection for use by a client who actually needs it. This way your 500 available connections can services thousands of clients, instead of a 1:1 with the number of available connections.</p>
<pre><code>-- connection is actually pulled from the pool inside PgBouncer
BEGIN;
INSERT INTO...;
UPDATE;
COMMIT;
-- connection goes back to the pool inside PgBouncer
</code></pre>
<p>The problem with transaction mode is that this tiny configuration change quietly changes not only your ability to scale, but also the way your connections <em>behave</em>. It breaks the expected command semantics between client and database server. And understanding whether you‚Äôve gotten things right in transaction mode is <em>very difficult</em>.</p>
<p>Let‚Äôs say you‚Äôve been operating with PgBouncer in session mode (or operating without a proxy at all), and you make the switch to transaction mode. Your perspective on how you can use Postgres needs to change - so now we‚Äôre onto the <em>peril</em>.</p>
<h2 id="perils">Perils</h2>
<p>Many of the following items are documented shortcomings of PgBouncer in <a href="#transaction-mode">transaction mode</a>. But:</p>
<ol>
<li>They‚Äôre treated lightly</li>
<li>Their repercussions and downsides are not discussed</li>
<li>PgBouncer is often recommended without mentioning them</li>
<li>PgBouncer is often recommended at the same time as recommending incompatible transaction mode features like session level advisory locks and session level statement timeouts</li>
<li>The non-determinism introduced by using incompatible statements is not discussed (ie, I execute a statement in Process A and suddenly Process B errors out due to it)</li>
</ol>
<p>Assume anytime I mention PgBouncer after this point I am referring to <a href="#transaction-mode">transaction mode</a>. Here we go!</p>
<h3 id="invalid-statements">Detecting invalid statements üòë</h3>
<p>PgBouncer happily accepts statements that are not supported in transaction mode. The problem is pushed onto the developer, which means they <em>can</em> and <em>will</em> get it wrong<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>.</p>
<p>This is by design. The sense I get is that PgBouncer was specifically architected to not analyze any statements and so it would be a big change for them to handle this<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>.</p>
<p>Amazon has a similar tool to PgBouncer called RDS Proxy, and it has a feature called ‚Äú<a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy-managing.html#rds-proxy-pinning">connection pinning</a>‚Äù. If it detects a statement that is incompatible with transaction mode, it will automatically hold that connection for that client for the duration of their session.</p>
<p>This is both highly useful and simultaneously problematic. It means query behavior is consistent with your expectations (üôåüèº) but also that you can silently kill all concurrency benefits (üòë). If enough queries are run that trigger connection pinning, all of a sudden you may throttle your <a href="#throughput">throughput</a>. But it does give you an escape hatch for safely running statements which are not transaction compatible without having to jump through any hoops.</p>
<p>I‚Äôd be fine with some logging I could monitor. As far as I can tell there is nothing like this in PgBouncer, and so all the burden lands on you to get it right. As one engineer, or a few engineers, all aware of potential issues, you can probably maintain that. But what about dozens of engineers? Or hundreds? Thousands? All with varying levels of experience with databases and poolers? There‚Äôs going to be mistakes.</p>
<h3 id="lock-timeouts">Lock Timeouts (SET/RESET) üîì</h3>
<p>Unless you <em>like</em> app downtime, you should be using a <code>lock_timeout</code> when running <a href="https://www.postgresql.org/docs/current/ddl.html">DDL</a>. It‚Äôs a critical aspect of <a href="https://medium.com/paypal-tech/postgresql-at-scale-database-schema-changes-without-downtime-20d3749ed680">zero downtime migrations</a>.</p>
<p>The idea is to set it to a limit that would be acceptable for queries in your application to slowdown by - waiting to acquire a lock can cause related queries to queue up behind your DDL operation:</p>
<pre><code>-- slow select
SELECT * FROM my_table;

-- DDL starts in separate process, blocked on acquiring the lock by the 
--    slow query
ALTER TABLE my_table...

-- Subsequent queries start queuing up...
SELECT * FROM my_table WHERE id = 123;
SELECT * FROM my_table WHERE id = 234;
--- ...
</code></pre>
<p>In that scenario, the slow query is running the show. Until it finishes, <em>all the other queries to that table are stuck</em>. That goes on long enough and users can‚Äôt use the system. A bit longer and your app starts timing out. A bit longer you‚Äôre running out of <em>connections</em>. Now you‚Äôre staring at a total app outage, about ready to kill all of your connections in a desperate attempt to salvage things, contemplating a career change to landscaping where you can at most impact one person at a time, right? That sounds nice, doesn‚Äôt it?</p>
<p>I‚Äôve of course never experienced that. I‚Äôm just <em>very</em> creative üíÄ. But if <em>you</em> have experienced that, or you‚Äôd like to <em>avoid</em> experiencing that, use a <code>lock_timeout</code>:</p>
<pre><code>SET lock_timeout TO &#39;2s&#39;;
</code></pre>
<p><em>Now</em> if your DDL cannot acquire a lock it will throw an error after 2 seconds. That should be an ok delay in running queries, and you can retry the operation later.</p>
<p>But wait! Are you connected to PgBouncer?! You may want to bring up that landscaping help-wanted listing again‚Ä¶ üå≥</p>
<p><code>SET</code> operations apply at the <a href="#session-mode">session level</a>. This means that on a PgBouncer connection, there is no guarantee our <code>lock_timeout</code> will still be applied when we run our DDL:</p>
<pre><code>-- Process 1
-- PgBouncer pulls connection 1
SET lock_timeout TO &#39;2s&#39;;
-- connection 1 goes back to the pool

-- Meanwhile, in Process 2:
-- PgBouncer pulls connection 3
SELECT id FROM my_table, pg_sleep(30);

-- Back in Process 1:
-- PgBouncer pulls connection 2
-- This connection has no lock_timeout set, so it will hang 
--    until our pg_sleep query finishes 30 seconds later, and all
--    queries to my_table after it are stuck for those 30 seconds as well
ALTER TABLE my_table...
</code></pre>
<p>It‚Äôd be easy to argue ‚Äúdon‚Äôt have slow queries‚Äù. And that should be the goal! But we don‚Äôt call it ‚Äúhappy path uptime üåº‚Äù, we call it ‚Äú<em>zero</em> downtime‚Äù. It means even if things go wrong, you don‚Äôt go down. There can also be other operations that hold a lock on your table, so you simply can‚Äôt rely on successfully acquiring that lock<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>.</p>
<p>So what can we do? There are two options:</p>
<ol>
<li>Bypass PgBouncer and go straight to the database</li>
<li>Use a transaction level <code>lock_timeout</code></li>
</ol>
<h4 id="bypassing-pgbouncer">Bypassing PgBouncer</h4>
<p>Your safest bet is to go with option (1). You should have some ability to directly connect to your database, so take advantage of it and don‚Äôt jump through hoops to run DDL safely.</p>
<p>The biggest obstacle you hit with (1) is <a href="#transparency">transparency</a>: PgBouncer <em>really</em> doesn‚Äôt want you to know whether you are connected to the real database or not. There‚Äôs no <em>easy</em> answer there, but by validating a setup where you consistently run your DDL process directly against Postgres then you‚Äôre set.</p>
<h4 id="use-transaction-level-statements">Use transaction level statements</h4>
<p>There is a transaction local equivalent to the <code>SET</code> statement: <code>SET LOCAL</code>. Using our example from earlier:</p>
<pre><code>-- Process 1
-- PgBouncer pulls connection 1
BEGIN;
SET LOCAL lock_timeout TO &#39;2s&#39;;
-- connection 1 stays checked out

-- Meanwhile, in Process 2:
-- PgBouncer pulls connection 3
SELECT id FROM my_table, pg_sleep(30);

-- Back in Process 1:
-- Connection 1 is still checked out
ALTER TABLE my_table...
-- lock_timeout raises an error after 2 seconds waiting, and 
--    we avoid our downtime!
</code></pre>
<p>DDL in Postgres is transactional, so it‚Äôs valid to start our transaction, set our <code>lock_timeout</code> using <code>SET LOCAL</code>, then start our DDL operation. Our transaction local setting will stick with us until the transaction commits or rolls back, so we safely keep our timeout and rollback our DDL.</p>
<p>It‚Äôs not a <em>terrible</em> solution (1 is still better), except for two things:</p>
<ol>
<li>Concurrent indexes</li>
<li>Tooling</li>
</ol>
<p>Another zero downtime star is the concurrent index. When you create a new index on a table you run the chance of locking it up long enough to cause downtime. Here‚Äôs the answer to that problem:</p>
<pre><code>CREATE INDEX CONCURRENTLY index_name ON my_table;
</code></pre>
<p>Concurrent indexes are created without an exclusive lock, so your normal operations keep going while it builds the index in the background. The <em>problem</em> is they can‚Äôt be run in a transaction, so <code>SET LOCAL</code> is not an option.</p>
<p>Because they don‚Äôt require an exclusive lock, setting a <code>lock_timeout</code> is less important. But if there is contention and you just can‚Äôt get that index to acquire it‚Äôs shared lock, do you really want it to run forever?</p>
<p>As for (2), popular tooling usually does not handle <code>SET LOCAL</code> for you. In the Rails/ActiveRecord world there are several libraries that will automatically apply zero downtime policies for you, but they all assume you have an exclusive connection and operate at the <code>SET</code> session level.</p>
<p><a href="https://en.m.wikipedia.org/wiki/The_road_to_hell_is_paved_with_good_intentions">In PgBouncer, the road to downtime is paved with session level statements</a>.</p>
<p>Just go with (1), keep your sanity, throw away the diary entries about living out your days breathing in the smell of fresh cut grass, and connect directly to Postgres to run DDL with <code>SET lock_timeout</code> calls.</p>
<h3 id="statement-timeouts">Statement timeouts (SET/RESET) ‚è≥</h3>
<p>Determined not to repeat your experiences from <code>lock_timeout</code>, you read about this thing called <code>statement_timeout</code>. This little magic wand makes it so you control how long a statement is allowed to run ü™Ñ.</p>
<p>So here it is:</p>
<pre><code>SET statement_timeout TO &#39;2s&#39;;
</code></pre>
<p>Those greedy queries now don‚Äôt stand a chance. You can tame your long running queries and avoid blocking your DDL! You ignore my advice to always use <code>lock_timeout</code>, say ‚Äúbye losers‚Äù to long running queries, and fire off that DDL again‚Ä¶ oh god. Why are things slowing down. Now they‚Äôre timing out. <em>The connections are filling up.</em> What is <em>happening?</em></p>
<p><img src="https://media.tenor.com/MYZgsN2TDJAAAAAC/this-is.gif" alt=""/></p>
<p>Oh riiiight. You forgot. You‚Äôre using PgBouncer. <code>SET</code> is off the table. Should have set that <code>lock_timeout</code> üîê‚Ä¶</p>
<p>If I had a nickel for every time someone mentioned <code>SET statement_timeout</code> and PgBouncer in the same article‚Ä¶<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup> I know no one sharing this content is doing it maliciously, but be aware that these are misleading and incompatible features.</p>
<h4 id="with-lock_timeout-why-does-statement_timeout-even-matter">With lock_timeout, why does statement_timeout even matter?</h4>
<ul>
<li>Statement timeouts are helpful for long running queries so they cancel earlier. If a client disconnects, Postgres will periodically check for the connection and try to cancel the query when it goes away. But a query <a href="https://dba.stackexchange.com/a/81424/256107">with runaway cpu</a> usage will just keep running even if the client dies or disconnects. That means you lose that connection until the query finishes, which can take minutes (or hours)</li>
<li>The database default is 0, which means there is no limit. In some contexts this is not a problem, but particularly for web requests this is excessive</li>
</ul>
<p>The first time I used <code>statement_timeout</code> was from a blog recommendation to limit statements for requests in web applications. In a web request, you usually have an upper limit on how long you allow them to run before they time out - this conserves resources, protects against runaway buggy code and helps with bad actors. It made sense that I‚Äôd set it to something conservative on all my web connections to deal with long running queries.</p>
<p>I deployed the code and for a little while things seemed to work well. Then I saw something odd. This started popping up:</p>
<pre><code>canceling statement due to statement timeout
</code></pre>
<p>But in my‚Ä¶ <em>background jobs</em>? My web requests were tuned to be fast, but the constraints around my background processes were a bit‚Ä¶ looser. Can you guess what I had recently enabled? PgBouncer in transaction mode. My session level statement timeout was being swapped out from my web request, picked up by my job, and caused my job to timeout instead - web request safety was off the rails and longer running jobs were intermittently failing.</p>
<p>So is there any way we can use it? There‚Äôs a couple ways I know of, but nothing great when pooling.</p>
<h4 id="our-old-friend-transaction">Our old friend transaction</h4>
<pre><code>BEGIN;
SET LOCAL statement_timeout &#39;5s&#39;;
SELECT ...
COMMIT;
</code></pre>
<p>Something about wrapping a SELECT in a transaction feels kind of strange, but it works. If you have targeted concerns, you can wrap particular queries in a transaction and use <code>SET LOCAL</code> to localize the <code>statement_timeout</code>.</p>
<p>This is absolutely not a viable solution for a whole request lifecycle. If I wanted to attempt my web request level timeouts again, no way am I wrapping every web request in one giant transaction. Postgres doesn‚Äôt have a concept of nested transactions so any code I have that may be operating transactionally is gonna be in for some confusing surprises<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup>. And most importantly, wrapping my whole request in a transaction means I‚Äôve completely negated the benefit of proxy pooling - now my request lifecycles are basically 1:1 with my connection sessions.</p>
<h4 id="apply-statement-timeouts-per-user">Apply statement timeouts per user</h4>
<p>I‚Äôve never tried it, but I‚Äôve seen it recommended to set statement timeouts per user when using PgBouncer. That seems to have a couple problems I can think of:</p>
<ol>
<li>It‚Äôs not dynamically configurable.</li>
<li>It dilutes the pool of available connections per context</li>
</ol>
<p>(1) is definitely inconvenient. If you have different contexts where you‚Äôd like to apply different timeout constraints, this would be way too cumbersome to maintain.</p>
<p>But (2) <em>feels</em> like a deal breaker. If I want to constrain my web requests to a conservative timeout, but give my background processes more wiggle room, my pool size of real connections is now split instead of sharing a pool of total available database connections. I also have to manage making sure each context uses the appropriate user, or things will go badly.</p>
<p>It‚Äôs technically an option, but seems trickier to maintain and monitor.</p>
<h3 id="transparency">Transparency üëª</h3>
<p><img src="https://media2.giphy.com/media/xT5LMN0UgalbScp6uI/giphy.gif?cid=6c09b952f7248c90c21812529981462733f1d648a5076839&amp;rid=giphy.gif&amp;ct=g" alt=""/></p>
<blockquote>
<p>I don‚Äôt understand why my session features aren‚Äôt working. I always make sure to use plenty of Postgr‚Ä¶PgBouncer?!</p>
</blockquote>
<p>It is very difficult to tell when you are or aren‚Äôt using PgBouncer, <a href="https://github.com/pgbouncer/pgbouncer/issues/249">which is unfortunately by design</a>. It considers itself a transparent proxy. In <a href="#session-mode">session mode</a>, that‚Äôs pretty much true. But in <a href="#transaction-mode">transaction</a> and <a href="#statement-mode">statement</a> mode you are working with bizarro Postgres. It all works the same except when it doesn‚Äôt.</p>
<p>So if you want a regular connection because you need a feature not available in transaction mode, being sure you did it right is extremely difficult.</p>
<p>I have had a hell of a time verifying that some servers are or aren‚Äôt running with PgBouncer. Server A is using pub sub, I don‚Äôt want it. Server B needs the throughput, I want it. How can I make sure someone never makes a mistake and attaches the server to the wrong place? Basically, I can‚Äôt.</p>
<p>When it comes to production code I like to be paranoid. On a large enough codebase, and team, and user base, unusual things are bound to happen, sometimes regularly. I try to write code and configure environments so the right way is easy and the wrong way is hard. PgBouncer does not make that easy.</p>
<p>On this particular point I‚Äôd love to say I have some kind of advice to act on, but it mostly takes testing and validating your setup. If someone out there has better ideas or tips, I am all ears.</p>
<h3 id="prepared-statements">Prepared Statements (PREPARE/DEALLOCATE, Protocol-level prepared plans) ‚úîÔ∏è</h3>
<p>PgBouncer has a public relations problem when it comes to prepared statements. This is all the <a href="https://www.pgbouncer.org/features.html">PgBouncer docs say</a> about them:</p>
<table>
	<thead>
		<tr>
			<th>
				Feature
			</th>
			<th>
				Session pooling
			</th>
			<th>
				Transaction pooling
			</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
				`PREPARE` / `DEALLOCATE`
			</td>
			<td>
				Yes
			</td>
			<td>
				Never
			</td>
		</tr>
		<tr>
			<td>
				Protocol-level prepared plans
			</td>
			<td>
				Yes
			</td>
			<td>
				No*
			</td>
		</tr>
	</tbody>
</table>
<blockquote>
<p>* It is possible to add support for that into PgBouncer</p>
</blockquote>
<p>Kind of feels‚Ä¶ alarming. No prepared statements in transaction mode?! Aren‚Äôt those‚Ä¶ important? Even further when you go to use PgBouncer with Hibernate or ActiveRecord (and I‚Äôm sure others) you‚Äôll see the recommendation to configure them to <em>turn off</em> prepared statements üò±. Does it surprise you a bit to hear that? Make you feel a little queasy maybe?</p>
<p>I had it drilled into me early in my career that prepared statements are a critical part of protecting against SQL injection. In the <a href="https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html">OWASP SQL Injection Prevention Cheatsheet</a> the very first recommendation is:</p>
<ul>
<li><strong>Use of Prepared Statements (with Parameterized Queries)</strong></li>
</ul>
<p>So PgBouncer tells me I need to <em>turn them off?</em></p>
<p><img src="https://thumbs.gfycat.com/AstonishingEarlyHornet-size_restricted.gif" alt=""/></p>
<p>The first time I used PgBouncer in an application I spent <em>a lot</em> of time figuring out how turning off prepared statements was safe to do. It turns out that prepared statements in Postgres mean a few things, but come down to two main options:</p>
<ol>
<li>Named prepared statements</li>
<li>Unnamed prepared statements</li>
</ol>
<p><em>Named</em> prepared statements are reusable, and are tied to the connection session.</p>
<p><em>Unnamed</em> prepared statements are single use, and have no association to the connection session.</p>
<p>There are two ways to create a <em>named</em> prepared statement and one way to create an <em>unnamed</em> prepared statement:</p>
<ol>
<li><code>PREPARE</code></li>
<li>Protocol-level Parse/Bind/Execute with a name specified</li>
<li>Protocol-level Parse/Bind/Execute with no name specified</li>
</ol>
<p>PgBouncer says it doesn‚Äôt support prepared statements in either <code>PREPARE</code> or protocol-level format. What it <em>actually</em> doesn‚Äôt support are <em>named</em> prepared statements in any form. That‚Äôs because named prepared statements live in the session and in <a href="#transaction-mode">transaction mode</a> you can switch sessions.</p>
<pre><code>-- PgBouncer pulls connection 1
PREPARE bouncer_since (int, timestamp) AS
SELECT * 
FROM bouncers b
INNER JOIN guests g ON g.bouncer_id = b.id
WHERE b.id = $1 AND b.created &gt; $2;
-- connection 1 goes back to the pool

-- PgBouncer pulls connection 2
EXECUTE bouncer_since(1, now() - INTERVAL &#39;2 days&#39;);
-- üí£ ERROR: prepared statement &#34;bouncer_since&#34; does not exist üí£
</code></pre>
<p>But <em>unnamed prepared statements are totally fine</em>. In fact, I‚Äôd be shocked if the current client library you‚Äôre using to connect to Postgres does not already switch to them if ‚Äúprepared statements‚Äù (again, so damn misleading) are ‚Äúturned off‚Äù.</p>
<p>But wait. What the heck is an unnamed statement? <code>PREPARE</code> <em>requires</em> a name‚Ä¶ how can I make a prepared statement without a name?</p>
<h4 id="protocol-level-prepared-plans">Protocol-level prepared plans</h4>
<p><img src="https://media0.giphy.com/media/P5wPrhzZDdeJW/giphy.gif?cid=6c09b95256738d3ee35e24f988a790f60659836b97f75ee8&amp;rid=giphy.gif&amp;ct=g" alt=""/></p>
<p>The alternative to the <code>PREPARE</code> statement is to directly communicate with Postgres at the protocol level.</p>
<p>I had to dig a bit to get a handle on this - I started from a common Ruby ORM called ActiveRecord, dug into the Ruby ‚Äúpg‚Äù gem <em>it</em> uses, then went one layer deeper into <code>libpq</code>, which is part of Postgres itself.</p>
<p>If we use active record as an example, <a href="https://guides.rubyonrails.org/configuring.html#configuring-a-postgresql-database">when prepared statements are ‚Äúdisabled‚Äù</a>, the postgres adapter internally calls <code>exec_no_cache</code> in <code>activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb</code>:</p>
<pre><code>def exec_no_cache(sql, name, binds...)
  #...
  conn.exec_params(sql, type_casted_binds)
</code></pre>
<p>That‚Äôs powered by the ruby ‚Äúpg‚Äù gem, which when calling <code>exec_params</code> from ruby ultimately calls into the <code>libpq</code> function <code>PQsendQueryParams</code>:</p>
<pre><code>// Ruby &#34;pg&#34; gem
// ext/pg_connection.c
static VALUE
pgconn_async_exec_params(int argc, VALUE *argv, VALUE self) {}

// internally calls...
static VALUE
pgconn_send_query_params(int argc, VALUE *argv, VALUE self) {}

// internally calls this from the libpq c postgres internals:
// src/interfaces/libpq/fe-exec.c
int PQsendQueryParams(PGconn *conn,
  const char *command,
  int nParams,
  const Oid *paramTypes,
  const char *const *paramValues,
  const int *paramLengths,
  const int *paramFormats,
  int resultFormat) {}
</code></pre>
<p>What does <code>PQsendQueryParams</code> do? It calls an internal method named <code>PQsendQueryGuts</code>. Notice the empty string and <code>use unnamed statement</code> comment ü§î.</p>
<div><pre tabindex="0"><code data-lang="c"><span>return</span> <span>PQsendQueryGuts</span>(conn,
    command,
    <span>&#34;&#34;</span>, <span>/* use unnamed statement */</span>
    nParams,
    paramTypes,
    paramValues,
    paramLengths,
    paramFormats,
    resultFormat);
</code></pre></div><p>What does <em>that</em> function do (aside from making me laugh every time I read the name <code>PQsendQueryGuts</code> üòÜ)? Internally <code>PQsendQueryGuts</code> communicates with Postgres at the protocol level:</p>
<pre><code>/* construct the Parse message */
if (pqPutMsgStart(&#39;P&#39;, conn) &lt; 0 ||
  pqPuts(stmtName, conn) &lt; 0 ||
  pqPuts(command, conn) &lt; 0) {}

/* Construct the Bind message */
if (pqPutMsgStart(&#39;B&#39;, conn) &lt; 0 ||
  pqPuts(&#34;&#34;, conn) &lt; 0 ||
  pqPuts(stmtName, conn) &lt; 0) {}

/* construct the Execute message */
if (pqPutMsgStart(&#39;E&#39;, conn) &lt; 0 ||
  pqPuts(&#34;&#34;, conn) &lt; 0 ||
  pqPutInt(0, 4, conn) &lt; 0 ||
  pqPutMsgEnd(conn) &lt; 0) {}
</code></pre>
<p>This is the Parse/Bind/Execute process I mentioned earlier.</p>
<ul>
<li>The code sends a <strong>P</strong>arse message with the query and an optional name. In our case the name is empty</li>
<li>The code then <strong>B</strong>inds params to that query (if the query is parameterized)</li>
<li>It then <strong>E</strong>xecutes using the combination of the parsed query and the bound params</li>
</ul>
<p>This is perfectly safe to do in transaction mode, and from a SQL safety perspective should behave identically to a named prepared statement.</p>
<h4 id="named-protocol-level-statements">Named protocol-level statements</h4>
<p>For comparison, when ActiveRecord has prepared statements turned on, things <em>look</em> a bit different, but by the end we‚Äôre in the same place:</p>
<pre><code>def exec_cache(sql, name, binds...)
  #...pseudo coded a bit but importantly
  #   it calls `prepare`
  if !cached
    stmt_key = conn.prepare(sql)
  # then it calls exec_prepared
  conn.exec_prepared(stmt_key, type_casted_binds)
</code></pre>
<p>It first has to call <code>prepare</code> with whatever sql we‚Äôre going to run. The caller is in charge of keeping track of whether the sql has been prepared before, otherwise Postgres will keep overwriting our previous sql and it might as well just execute an unnamed statement. Then it calls <code>exec_prepared</code> with only the <code>stmt_key</code>, which should match the name of a previously prepared query.</p>
<p>If we skip ahead to what gets called in <code>libpq</code>:</p>
<pre><code>// conn.prepare(sql)
int
PQsendPrepare(PGconn *conn,
    const char *stmtName, 
    const char *query,
    int nParams, 
    const Oid *paramTypes) {
  //...
  if (pqPutMsgStart(&#39;P&#39;, conn) &lt; 0 ||
      pqPuts(stmtName, conn) &lt; 0 ||
      pqPuts(query, conn) &lt; 0) {}
  //...
}
</code></pre>
<p>We see something similar to our earlier Parse/Bind/Execute, but now we‚Äôre <em>only</em> calling the <strong>P</strong>arse portion and this time we have a <code>stmtName</code>. We then trigger the prepared statement calling <code>exec_prepared</code>, which ultimately calls <code>PQsendQueryPrepared</code>:</p>
<pre><code>// conn.exec_prepared(stmt_key, type_casted_binds)
int
PQsendQueryPrepared(PGconn *conn,
    const char *stmtName,
    int nParams,
    const char *const *paramValues,
    const int *paramLengths,
    const int *paramFormats,
    int resultFormat) {
  //...
  return PQsendQueryGuts(conn,
      NULL,     // no sql
      stmtName, // named
      nParams,
      NULL,
      paramValues,
      paramLengths,
      paramFormats,
      resultFormat);
  //...
}
</code></pre>
<p>Anything look familiar? That‚Äôs the same <code>PQsendQueryGuts</code> function we called for the unnamed statement! This time it doesn‚Äôt hand a <code>command</code> in because we already parsed our SQL in the earlier <code>prepare</code> call. We also have a <code>stmtName</code> defined, instead of handing in an empty string. This version goes on to skip the <strong>P</strong>arse, call the <strong>B</strong>ind with the <code>stmtName</code>, then call <strong>E</strong>xecute - same flow as our unnamed version.</p>
<p>For SQL injection safety, both named and unnamed versions are equivalent: they separate query structure (Parse) from data values (Bind). Adding query bindings when not in a prepared statement simply makes an unnamed statement.</p>
<p>Nothing about these calls is specific to the <code>libpq</code> library, it‚Äôs just a rock solid implementation of them<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup> - any language could make the same protocol calls. If a library is utilizing this protocol, they are doing the same things when binding to an unnamed prepared statement as they are when binding to a named prepared statement<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup>.</p>
<p>As long as your code uses parameterized queries, ‚Äúturning off‚Äù prepared statements for PgBouncer is safe, even if it seems a bit unnerving. There is a <a href="https://github.com/pgbouncer/pgbouncer/pull/757">PR to allow PgBouncer to track prepared statements</a>, so maybe this won‚Äôt cause people like me as much heartburn in the future ü•≤.</p>
<h3 id="throughput">Pool throughput / Long running queries üèÉ‚Äç‚ôÇÔ∏è</h3>
<p>We‚Äôve got two types of connections to Postgres: active and idle. Idle connections are the backbone of poolers - having idle connections means we‚Äôve got capacity to swap around transactions for connected clients. What about active connections?</p>
<p>An active connection means that connection is actively tied up by the database. For that timespan, the connection cannot be swapped out to do anything else until its operation completes. We know that active connections get expensive quickly, and we also know that most managed services range somewhere from 50 to 500 allowed total, non-pooled connections.</p>
<p>Using a max PgBouncer connection pool of 10k and Render‚Äôs managed Postgres service with a max of 397 total connections means we‚Äôd have:</p>
<p>10000 / 397 = ~25 connections per active connection</p>
<p>Using Supabase‚Äôs 50 connections the spread is even higher:</p>
<p>10000 / 50 = ~<em>200</em> connections per active connection</p>
<p>That means that for every long running operation, you are potentially down 200 connections worth of pooling.</p>
<p>These numbers are very back of the napkin and of course do not represent the true scaling capability and connection handling of a real pooler. But the point is this:</p>
<ul>
<li>Active connections are very valuable to a pooler</li>
<li>Long running queries disproportionally impact concurrency</li>
</ul>
<p>As an example, you‚Äôre using Render Postgres fronted by PgBouncer and you‚Äôve got 10k available connections backed by the max of 397 Postgres connections. Let‚Äôs say a new feature is introduced for populating some graph data on your app‚Äôs landing page. It‚Äôs powered by a new query that looks great, has indexes, and seems well optimized. It‚Äôs even run against some load testing and representatively sized data as a QA check. It gets deployed to production and <em>OOF, it‚Äôs taking 15 seconds per query</em> üêå. Users are logging in or navigating to the landing page all the time so within moments you‚Äôve had thousands of hits to this query. Obviously this is going to get quickly rolled back, but what does it mean for your pool in the meantime?</p>
<p><img src="https://media1.giphy.com/media/137TKgM3d2XQjK/giphy.gif?cid=6c09b952ebbb45e59739e8c9dd3ca08d23031a7fe573cd54&amp;rid=giphy.gif&amp;ct=g" alt=""/></p>
<p>It means you‚Äôre maxed out. Your pooler being there means at least you‚Äôre less likely to start erroring out right away, but transaction mode can‚Äôt fix a stuck query. For each of those 15 second chunks of time your concurrency basically went from 10k back down to 397.</p>
<p>This is not the general behavior you‚Äôll see when using PgBouncer unless you‚Äôve really got some intermittent trouble with runaway queries. But it does emphasize an important point to remember: these are not real Postgres connections. Your upper bound on long running, active queries is always constrained by your actual pool of real Postgres connections.</p>
<h4 id="guarding-against-slow-queries">Guarding against slow queries</h4>
<ul>
<li><a href="https://www.crunchydata.com/blog/logging-tips-for-postgres-featuring-your-slow-queries">Log your slow queries</a> using <code>log_min_duration_statement</code>. This option lets you set a threshold and if queries take long than that threshold Postgres will log the offending query. This won‚Äôt help the sudden mass slow query situation mentioned above, but it helps to keep an eye on overall app query health</li>
<li>Use <a href="https://www.postgresql.org/docs/current/libpq-single-row-mode.html">streaming queries</a> sparingly. In most client libraries you can set your query to run in ‚Äúsingle row mode‚Äù. This means you retrieve your rows one at a time instead of getting one big result set at once. This is helpful for efficiency with very large result sets but is slower than a full result set query, and probably means you are running queries large enough to be slower in the first place</li>
<li>Use <a href="#statement-timeouts">statement timeouts</a>. This is tricky, especially when pooling, but see that section for ideas on how to approach it</li>
<li>Spread out reads across read replicas</li>
</ul>
<h3 id="session-level-locks">Session Level Advisory Locks üîê</h3>
<p>Session level advisory locks work fine in PgBouncer.</p>
<p><img src="https://i.gifer.com/7DWJ.gif" alt=""/></p>
<p>Sorry üôà.</p>
<p>If you‚Äôve read the previous sections you‚Äôve already picked up on the pattern: ‚Äúsession‚Äù anything means it probably doesn‚Äôt work in <a href="#transaction-mode">transaction mode</a>. But what does that matter to you?</p>
<p>Advisory locks are a great option for creating simple, cross process, cross server application mutexes based on a provided integer key. Unlike traditional locks you use/encounter elsewhere in Postgres which are tied to tables or rows, advisory locks can be created independent of tables to control application level concerns. There are plenty of other tools you could use for this job outside of Postgres, but since Postgres is already part of your tech stack it‚Äôs a convenient and simple option.</p>
<p>Across languages a common use case for session level advisory locks is to hold a lock while database migrations (ie, DDL) are being run. For example:</p>
<pre><code>-- 1234 is arbitrary, it can be any integer
SELECT pg_advisory_lock(1234);
SET lock_timeout TO &#39;1s&#39;;
ALTER TABLE my_table...;
INSERT INTO migrations VALUES (1234567);
-- If we don&#39;t explicitly unlock here, the lock will be held until this 
--    connection is closed
SELECT pg_advisory_unlock(1234);
</code></pre>
<p>If another connection went to acquire the same lock, it would be blocked:</p>
<pre><code>-- This will block indefinitely until the other connection is closed, 
--    or calls pg_advisory_unlock(1234)
SELECT pg_advisory_lock(1234);
</code></pre>
<p>This is largely an attempt to improve consistency of migration tracking, and help coordinate multi process deploys:</p>
<ul>
<li>Continuous deployment with the potential to trigger multiple deployments in succession</li>
<li>Propagating code changes to multiple servers with deploy scripts automatically triggering migrations in each context</li>
</ul>
<p>By waiting to acquire a lock at the Postgres level, each process waits for the first lock owner to finish before continuing, coordinating each process based on a shared lock key.</p>
<h3 id="once-more-with-strikefeelingstrike-pgbouncer">Once more, with <strike>feeling</strike> PgBouncer</h3>
<p>Now for the obligatory example of trying the same thing when connected to PgBouncer ü´†:</p>
<pre><code>-- Grab the lock on connection 1
SELECT pg_advisory_lock(1234);
-- Connection 1 goes back into pool
-- ...
-- Try to unlock on connection 2, which does not own the 1234 lock
SELECT pg_advisory_unlock(1234);
-- WARNING: you don&#39;t own a lock of type ExclusiveLock
</code></pre>
<p>We try to unlock, but because we‚Äôre on a different connection we can‚Äôt. The lock stays locked for as long as connection 1 stays alive, which means now no one else can acquire that lock unless that connection naturally closes at some point or is explicitly <code>pg_cancel_backend</code>ed üòì.</p>
<h3 id="more-session-advisory-lock-use-cases">More session advisory lock use cases</h3>
<p>Outside of migrations, advisory locks can serve other use cases:</p>
<ul>
<li>Application mutexes on sensitive operations like <a href="https://rclayton.silvrback.com/distributed-locking-with-postgres-advisory-locks">ledger updates</a></li>
<li><a href="https://jeremydmiller.com/2020/05/05/using-postgresql-advisory-locks-for-leader-election/">Leader election</a> for maintaining a single but constant daemon operation across servers</li>
<li>Exactly once run job controls for Postgres based job systems like <a href="https://github.com/bensheldon/good_job">GoodJob</a> and <a href="https://github.com/que-rb/que">Que</a></li>
</ul>
<p>If these things sound interesting or useful, they are! But only if you connect directly to Postgres.</p>
<h4 id="transaction-level-locks">Transaction level locks</h4>
<p>Advisory locks do have a transaction based companion:</p>
<pre><code>-- Process 1
BEGIN;
SELECT pg_advisory_xact_lock(1234);

-- Process 2 
-- Blocks while process 1 is in the transaction
SELECT pg_advisory_lock(1234);

-- Back in Process 1
SET LOCAL lock_timeout TO &#39;1s&#39;;
ALTER TABLE my_table...;
INSERT INTO migrations VALUES (1234567);
COMMIT; -- automatically unlocks on commit or rollback
-- Process 2 now can acquire the lock

-- If you need to manually unlock while still in the transaction 
-- SELECT pg_advisory_xact_unlock(1234);
</code></pre>
<p>You could use it as a replacement for certain scenarios, like the above migration operating transactionally. For custom purposes, it‚Äôs a good alternative!</p>
<p>Unfortunately most migration tooling, things like leader election, and request or job lifetime locks, all use or require a longer lived lock than a single transaction could reasonably provide.</p>
<h4 id="turn-off-advisory-migration-locks">Turn off advisory migration locks</h4>
<p>If you need to run migrations against PgBouncer, in Rails you can turn them off with an <code>advisory_locks</code> flag in <code>database.yml</code>. Other migration tools likely have something similar. Do it at your <em>own</em> peril ü§∑üèª‚Äç‚ôÇÔ∏è</p>
<h4 id="maintaining-a-separate-direct-connection-to-postgres">Maintaining a separate direct connection to Postgres</h4>
<p>If the lock is critical, but the operations past the lock fan out and acquire multiple connections, you could potentially have two pieces:</p>
<ul>
<li>A direct connection to Postgres where you acquire a session level advisory lock</li>
<li>Your normal <a href="#framework-pool">code level connection pooling</a> using your PgBouncer connections so it can capitalize on the scaling opportunities provided there</li>
</ul>
<p>There‚Äôs an obvious downside - you‚Äôre consuming an extra direct connection and potentially impacting <a href="#throughput">throughput</a> - but it‚Äôs an alternative available if needed.</p>
<h3 id="listen-notify">Listen / Notify üì£</h3>
<p>Postgres comes out of the box with a handy pub/sub feature called <a href="https://www.postgresql.org/docs/current/sql-listen.html">LISTEN</a>/<a href="https://www.postgresql.org/docs/current/sql-notify.html">NOTIFY</a>.</p>
<p>You simply call:</p>
<pre><code>LISTEN channel_name;
</code></pre>
<p>And that connection will receive <code>NOTIFY</code> events:</p>
<pre><code>NOTIFY channel_name, &#39;hi there!&#39;;
</code></pre>
<p>Like session level advisory locks, there are more robust pub/sub solutions out there. But the Postgres implementation works well, and you already have it available in your stack.</p>
<p>Looking at the example, you‚Äôll notice that the <code>LISTEN</code> call is just a single statement, and it activates the listener for the current session. What have we said so many times already? Sessions bad. Transactions good‚Ä¶ kind of.</p>
<h4 id="kind-of">kind of?</h4>
<p>Similar to prepared statements, the docs are misleading when it comes to <code>LISTEN</code>/<code>NOTIFY</code>.</p>
<p>PgBouncer officially lists <code>LISTEN</code>/<code>NOTIFY</code> as an unsupported feature in transaction mode, which is not precisely true. <code>LISTEN</code> does not work in transaction mode, but <code>NOTIFY</code> does.</p>
<p><code>NOTIFY</code> is a single statement, and doesn‚Äôt rely on any session semantics. It‚Äôs also transactional<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup>:</p>
<pre><code>BEGIN;
NOTIFY channel_name, &#39;hi!&#39;;
ROLLBACK; -- no notification is sent
</code></pre>
<p>Both <code>NOTIFY</code> formats (inside and outside a transaction) work fine with transaction mode pooling. If you want to use pub/sub, you just need to make sure your <code>LISTEN</code>er is connected directly to Postgres. Since <a href="#transparency">it can be hard to tell if you‚Äôre connected to Postgres or PgBouncer</a> this is somewhat tricky, unfortunately.</p>
<p>I‚Äôve built implementations <code>LISTEN</code>ing on a non-PgBouncer connection and <code>NOTIFY</code>ing on PgBouncer that work fine. There‚Äôs not much writing on this, but I have found this approach to work well.</p>
<h3 id="single-threaded">The single thread ü™°</h3>
<p>In contrast to the multi process monster that is Postgres, PgBouncer runs on a paltry single process with a single thread.</p>
<p>This means that no matter how capable a server is, PgBouncer is only going to utilize a max of one CPU core so once <a href="https://news.ycombinator.com/item?id=17187436">you‚Äôve maxed out on that core</a> you can‚Äôt scale that single instance anymore.</p>
<p>A popular option is to <a href="https://www.crunchydata.com/blog/postgres-at-scale-running-multiple-pgbouncers">load balance PgBouncer instances</a>. Otherwise, almost every alternative to PgBouncer (like Odyssey, PgCat and Supavisor) utilize multiple cores.</p>
<p>If you‚Äôre using a managed Postgres service (like Crunchy Data, Supabase, Neon or Heroku), your default option is going with PgBouncer as a connection pooler - so it will be up to those services to offer a load balanced option.</p>
<h3 id="pg_dump">pg_dump üöÆ</h3>
<p>If you‚Äôre running <code>pg_dump</code> against PgBouncer, it‚Äôs probably by mistake.</p>
<p>As far as I can tell, <code>pg_dump</code> is broken when run against PgBouncer. See <a href="https://github.com/pgbouncer/pgbouncer/issues/452">https://github.com/pgbouncer/pgbouncer/issues/452</a>.</p>
<p>The answer here is to make sure you‚Äôre using a direct connection to Postgres for utility operations like <code>pg_dump</code>.</p>
<h3 id="unavailable">Other unavailable features ü´•</h3>
<p><img src="https://media0.giphy.com/media/VCZgfe90H1tMTAW6n4/giphy.gif?cid=6c09b9526e2d6dfd051e5257c6dbce5ac862293219ad6e76&amp;rid=giphy.gif&amp;ct=g" alt=""/></p>
<p>There are some remaining features which transaction mode is incompatible with as well<sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup>. I have less or no experience with these:</p>
<ul>
<li><code>WITH HOLD CURSOR</code> - A <code>WITH HOLD</code> continues to exist outside of a transaction, which seems like it could have handy use cases but I‚Äôve never personally used it in my day to day.</li>
<li>PRESERVE/DELETE ROWS temp tables - temporary tables are a session level feature so will not work properly, and preserve/delete rows are modifiers on how those temporary tables behave on commit, and are unsupported</li>
<li>LOAD statement - this is for loading shared libraries into Postgres, so it makes sense this is not something you should be doing through a pooler. I haven‚Äôt actually tried, so I‚Äôm not sure if PgBouncer would stop you, but it requires super user privileges so it‚Äôs very unlikely that‚Äôs what your PgBouncer user has</li>
</ul>
<p>PgBouncer documents a simple ‚Äú<a href="https://www.pgbouncer.org/features.html">SQL feature map for pooling modes</a>‚Äù where you can see all the features mentioned in this post.</p>
<h2 id="linting">Linting üß∂</h2>
<p>Aside from having identified potential issues - what can we do to avoid them in an automated way?</p>
<p>Surprisingly, not much exists. And by not much, I mean i‚Äôve found nothing outside of advice.</p>
<p>It makes me feel a bit like I‚Äôm exaggerating the importance of these issues. Maybe I‚Äôm the oddball that has actually encountered many of them in real production usage and had to address them. I‚Äôve had statement timeouts and lock timeouts misapplied. I‚Äôve had to deal with rearranging connections because of code using a session advisory lock and <code>LISTEN</code>/<code>NOTIFY</code>, or drop libraries that use them. I‚Äôve had to remember to turn off prepared statements in my ORM to avoid named prepared statement errors.</p>
<p>The implications can feel small, but they can be surprising and particularly around migrations can cause real serious downtime.</p>
<p>We lint everywhere. As engineers we try to automate away as many mistakes as possible with linting and specs. As development teams grow, the importance of automation becomes critical to scaling because otherwise someone somewhere is going to do the wrong thing and it won‚Äôt get caught.</p>
<p>Some ideas that would be great to see:</p>
<ul>
<li>PgBouncer optional process that detects bad queries and logs them</li>
<li>RDS connection pinning behavior</li>
<li>Static analysis tools for app queries</li>
<li>Runtime extension to client libraries</li>
<li>Making sure your development flow runs PgBouncer locally to try and encounter this behavior before running on production</li>
</ul>
<p>In the rails world there are <a href="https://github.com/braintree/pg_ha_migrations">several</a> <a href="https://github.com/doctolib/safe-pg-migrations">active</a> <a href="https://github.com/ankane/strong_migrations">gems</a> devoted to keeping a codebase safe from issues that would cause downtime while migrating tables (ie, zero downtime). But across ecosystems I could not find anything related to protecting against PgBouncer issues.</p>
<p>As a step in this direction, I‚Äôve published a (currently experimental) gem for use in Rails/ActiveRecord apps called <a href="https://rubygems.org/gems/pg_pool_safe_query">pg_pool_safe_query</a>. It will log warnings if SQL is run that is incompatible with PgBouncer and raise an error if advisory locks and prepared statements are not disabled.</p>
<h2 id="future-improvements">Can we improve connections without a pooler?</h2>
<p>A more recent development in Postgres 14 was improvements to <a href="https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/analyzing-the-limits-of-connection-scalability-in-postgres/ba-p/1757266">snapshot scalability</a>, which seem to have resulted in big improvements in efficiently <a href="https://techcommunity.microsoft.com/t5/azure-database-for-postgresql/improving-postgres-connection-scalability-snapshots/ba-p/1806462#first-performance-improvements">maintaining more idle connections</a> in Postgres.</p>
<p>It‚Äôs exciting to see effort being applied to increasing connection efficiency in Postgres itself. The author of that snapshot scalability improvement lines up with my own frustrations:</p>
<ul>
<li>Ideally Postgres would better handle traffic spikes without requiring a pooler</li>
<li>Poolers cut out useful database features</li>
<li>Postgres itself would ideally move towards architecture changes across several key areas, eventually culminating in a larger move towards a lighter weight process/thread/async model which better aligns with the C10k problem out of the box</li>
</ul>
<p>Most of the work in the industry <em>seems</em> to concentrate on building better poolers, rather than improving the internals of Postgres connection handling itself<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup>. Outside of PgBouncer you‚Äôve got RDS Proxy, Odyssey, PgCat, Supavisor, PgPool II and I‚Äôm sure others. All have their own benefits but suffer from the same transactional scaling limitations.</p>
<p>In fairness to the <em>incredible</em> work that goes into Postgres - every performance improvement they make in every new version is also a connection scalability improvement. If the queries, indexes, plans, and processes are making big performance gains with each version then less connections can do more.</p>
<h2 id="alternatives">PgBouncer alternatives</h2>
<p>There are alternatives to PgBouncer, but the same transaction limitations apply to all of them: each has a transaction mode (or operate exclusively in transaction mode) that offers the best scaling. Once in transaction mode you can‚Äôt support most session level features anymore and you‚Äôre working off of the fact that database connections spend more time being idle than active.</p>
<p>They all have their own unique benefits in comparison, but have the same fundamental transaction limitations.</p>
<ul>
<li><a href="https://github.com/supabase/supavisor">Supavisor</a></li>
<li><a href="https://github.com/postgresml/pgcat">PgCat</a></li>
<li><a href="https://github.com/yandex/odyssey">Odyssey</a></li>
<li><a href="https://pgpool.net/mediawiki/index.php/Main_Page">Pg Pool II</a></li>
<li><a href="https://aws.amazon.com/rds/proxy/">RDS Proxy</a></li>
</ul>
<h2 id="am-i-finally-done-with-this-post">Am I finally done with this post?</h2>
<p>I think I‚Äôve said enough.</p>
<p>Postgres is great. PgBouncer is important. Know what can go wrong and account for it.</p>
<p>üêò ‚úåüèº üêò</p>

</section></div>
  </body>
</html>
