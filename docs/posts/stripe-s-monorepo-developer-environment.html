<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.nelhage.com/post/stripe-dev-environment/">Original</a>
    <h1>Stripe&#39;s Monorepo Developer Environment</h1>
    
    <div id="readability-page-1" class="page"><div>
  <p>I worked at Stripe for about seven years, from 2012 to 2019. Over that time, I used and contributed to many generations of Stripe‚Äôs developer environment ‚Äì the tools that engineers used daily to write and test code. I think Stripe did a pretty good job designing and building that developer experience, and since leaving, I‚Äôve found myself repeatedly describing features of that environment to friends and colleagues.</p>
<p>This post is an attempt to record the salient features of that environment as I remember it. I‚Äôll also try to reflect on the context, constraints, and motivations for some of these choices; while I think they were good choices in context, they were deeply informed by the business and technical context, and other teams will require variations.</p>
<p>Some caveats: It‚Äôs been nearly five years, and I have no doubt that I have misremembered some of the specific details, even though I‚Äôm confident in the overall picture. I‚Äôm also certain that Stripe has continued evolving and I make no claim this document represents the developer experience at Stripe as of today.</p>
<p>In addition, while I contributed to many of the components described here, I can‚Äôt and don‚Äôt try to claim any overall credit. Everything here was evolved over the course of many years, with contributions from many talented engineers, both to craft the vision, and to implement it.</p>

<p>At the time I‚Äôm discussing, most of Stripe‚Äôs codebase was written in Ruby, and lived in a single large monorepo. This codebase supported a number of services, which shared code extensively. Stripe also had a large and growing number of services outside the monorepo and in other languages, but they were (as a generalization) less core to the business ‚Äì notably, the Stripe API was almost entirely a single Ruby service in the monorepo. The tooling and experience I discuss here were designed and built primarily to support development in the monorepo. Other services and languages were supported incidentally if at all.</p>
<p>Stripe‚Äôs tooling was built and maintained by a succession of teams and individuals, but I‚Äôll collectively refer to its authors and owners as the ‚Äúdeveloper productivity‚Äù team (‚Äúdevprod‚Äù for short), which was the name of that team for the last few years of my tenure.</p>

<p>Stripe initally created a team headcount dedicated to internal tooling and productivity ‚Äì which would become the team that built these tools ‚Äì fairly early on in my time, but in the last three or four years it grew substantially and really came into its own. This team was consistently staffed with excellent engineers, including some very senior ICs. More so than any of the technical choices I‚Äôll outline, I think the existence of the team, and its investment in <strong>stability</strong> and <strong>reliability</strong> of developer tooling (a bit later on, once the team had grown enough to put out immediate fires and have the room to plan and invest) were the major drivers in any success of the Stripe developer experience. Technical choices and engineering absolutely matter, but they need to be supported with enough headcount and with adequence <strong>ongoing maintenance</strong> in order to work.</p>
<p>This reliability and stability of the environment, especially as the team and codebase grew and evolved, will be recurring theme. The best tooling in the world will struggle to make up for ‚Äúregularly losing a day debugging your environment,‚Äù and so getting that aspect right can easily outweigh almost any other decisions you make. Many of the design choices were made with the goal of enabling the devprod team to more-easily and consistently support and monitor the tooling, and allow for centralized debugging and resolution of issues instead of pushing it onto individual engineers.</p>

<h2 id="development-code-runs-in-the-cloud">Development code runs in the cloud¬†<a href="#development-code-runs-in-the-cloud"><i>	üîóÔ∏é</i></a> </h2>
<p>Here‚Äôs a defining question for a developer environment: Does code in development run locally on the developer‚Äôs laptop, or does it need to be run remotely, on instances or containers inside a centrally-provisioned environment?</p>
<p>For many years, Stripe engineers used both options, depending on developer preference and idiosyncratic decisions and details of what worked in which environment at which times. When the developer productivity team decided to invest in a single blessed environment, we settled on supporting per-developer instances (‚Äúdevboxes‚Äù) in Stripe‚Äôs cloud enviroment (outside of the production enviroment). During development, code ran on a devbox, both for running <code>minitest</code> tests and for interactive testing and experimentation.</p>
<p>These developer instances were provisioned by Stripe‚Äôs standard configuration management tooling, and were ephemeral ‚Äì a single command could destroy your instance and provision a new one (a number of warm spares were kept, making this operation typically very fast). A registry kept track of which instance was active for which engineer, for consumption by tooling. All devboxes were accessible via <code>ssh</code> to all engineers, which eased collaboration and debugging of environment issues.</p>
<p>This design meant that most environment-configuration issues could be solved centrally by tooling teams or service owners, and developers mostly did not have to worry about updating their own development environments to keep up.</p>
<h3 id="enabling-new-dependencies">Enabling new dependencies¬†<a href="#enabling-new-dependencies"><i>	üîóÔ∏é</i></a> </h3>
<p>Stripe was continually adding and reworking dependencies for the API or other services; for instance, <a href="https://stripe.com/blog/rate-limiters">implementing rate limiting</a> in the API required a Redis cluster. In a world where development code runs on laptops, this meant that every laptop would now need a Redis installed and potentially configured. Updating configuration on developer laptops was challenging, and often this would be left to word-of-mouth between engineers: ‚ÄúHey, I pulled <code>master</code> and now I‚Äôm getting a weird error ‚Äì does anyone know how to fix this?‚Äù followed by a teammate sending an appropriate configuration command. Alternately, a <code>brew</code> invocation to install Redis might get slipped into a random script that developers run regularly, in the hopes of resolving the issue, which was also brittle and periodically caused random tools to be slowed down.</p>
<p>With the devbox model, the team adding Redis was already responsible for configuring it in production, and they could add appropriate Puppet configuration to ensure it was installed and running on devboxes, as well. In the event that a user ran into problems with this new path, members of that team, or of a devprod, would be able to <code>ssh</code> directly into their devbox, debug the issue, and then update Puppet or the Ruby source directly to prevent the issue from impacting other users.</p>
<p>For most of my tenure at Stripe, new features and infrastructure changes were <em>constantly</em> resulting in new dependencies or configuration changes to existing dependencies of some form; standardizing on devboxes both eased the jobs of those teams, and drastically reduced the rate at which infrastructure changes would break someone else‚Äôs developer experience, both of which were enormously valuable.</p>
<h2 id="editors-and-source-control">Editors and source control¬†<a href="#editors-and-source-control"><i>	üîóÔ∏é</i></a> </h2>
<p>Before you can run new code under development, you need to actually access code via source control and edit it.</p>
<p>At Stripe, even though code <strong>ran</strong> in the cloud, git checkouts and editors lived locally, on developer laptops. Stripe settled on this approach for a number of reasons, including:</p>
<ul>
<li>
<p>Support for a wide variety of editor and IDE environments</p>
<p>Some editors (like <code>emacs</code> or <code>vim</code>) can be run reasonably-well over an SSH session, and some (like VS Code or emacs with <a href="https://www.gnu.org/software/tramp/"><code>tramp</code></a>) support running a local UI against a remote filesystem, but many do not. By keeping code on the laptop, Stripe allowed developers to continue using any editor they wanted.</p>
</li>
<li>
<p>Latency</p>
<p>Stripe had developers around the globe, but at the time did not yet maintain substantial infrastructure outside of the US. Editing code on the other side of a 200+ms network latency is <strong>painful</strong>. Standing up devboxes globally was an option but would have been complicated for numerous reasons.</p>
</li>
<li>
<p>Source code durability and filesystem performance</p>
<p>By keeping the source-of-truth on laptops and off of the devbox, it was much easier to treat the execution environment as ephemeral and transient. That property, in turn, was very helpful for keeping environments up-to-date and preventing drift over time.</p>
<p>Source code could have been kept on a network-accessible store separate from the devbox (e.g. <a href="https://aws.amazon.com/efs/">EFS</a> or an EBS volume), but aside from the complexity cost, those options have relatively high latency, and source code tends to be dependent on (at least) fast file-metadata lookups; <code>git</code> operations over NFS or on EBS tend to be painfully slow without heroic levels of tuning and optimazation.</p>
</li>
</ul>
<h3 id="automatic-synchronization">Automatic synchronization¬†<a href="#automatic-synchronization"><i>	üîóÔ∏é</i></a> </h3>
<p>Keeping code on laptops, and executing in the cloud, though, poses a new challenge: How does code, once edited, <strong>get</strong>  from the laptop onto the devbox?</p>
<p>Even before I joined, Stripe had a ‚Äúsync‚Äù script that glued together a file-watcher with <code>rsync</code>: it would monitor your local checkout for changes and copy them to a devbox. Historically, developers would manually start and monitor this script in an ad-hoc way in a separate terminal or <code>tmux</code> window.</p>
<p>Eventually, the developer productivity team took ownership of that tool and invested in it heavily, largely with the goal of adding ‚Äúpolish‚Äù and making it seamless and nearly-invisible for other developers.</p>
<p>Notably, they made syncing happen implicitly and with no configuration or intervention required: They worked with the IT team to install the sync script as a <code>launchd</code> service on every developer laptop, and used the above-mentioned registry to automatically discover the correct devbox.</p>
<p>They also invested heavily in reliability and self-healing on errors and network problems. One ‚Äúinternal‚Äù but significant improvement involved migrating to <a href="https://facebook.github.io/watchman/">watchman</a> for file-watching: In our testing , it was by far the most robust file-watcher we found, vastly reducing the frequency of missed updates or a ‚Äústuck‚Äù file-watcher. We were also able to use some of its more-advanced features, which I‚Äôll discuss in a bit.</p>
<p>On the whole, these investments largely succeeded: most developers were able to treat code sync as simply a ‚Äúfact of life,‚Äù and rarely had to think about or debug it.</p>
<p>One downside of synchronization is that it makes it harder to write ‚Äúcode that acts on code,‚Äù such as automated migration tools, or even just linters or code-generation tools (Stripe ended up relying on a handful of code-generated components which needed to be checked in for various reasons). This remained a bit of a pain point through my tenure; we made do with a mix of strategies:</p>
<ul>
<li>Run on the developer laptop, and deal with the environment challenges</li>
<li>Run on the devbox and then somehow ‚Äúsync back‚Äù generated files. We had a small protocol to run scripts in a ‚Äúsync-back‚Äù wrapper, s.t. they could ask for file to be copied back to the laptop, but it remained somewhat awkward and non-ergonomic and occasionally unreliable.</li>
</ul>
<h2 id="http-services-on-the-devbox">HTTP services on the devbox¬†<a href="#http-services-on-the-devbox"><i>	üîóÔ∏é</i></a> </h2>
<p>Much of the code at Stripe executed inside of HTTP services, including notably the Stripe API. The developer productivity team built tooling to support developing these services, including:</p>
<ul>
<li>DNS resolved hostnames along the lines of <code>*.$username.stripe-dev.com</code> to that user‚Äôs current devbox</li>
<li>A frontend service ran on each devbox that terminated SSL, handled any auth[nz] based on client certificates, and mapped service names to the appropriate local instance
<ul>
<li>Each service at Stripe was statically assigned a local port, used by the frontend to route traffic.</li>
<li>For instance, the API service might be assigned port 3000, and so the frontend would forward <code>api.nelhage.stripe-dev.com</code> to <code>localhost:3000</code></li>
</ul>
</li>
<li>A separate proxy service listened on each of those ports, and demand-started the backing Ruby service.
<ul>
<li>Thus, on first request to <code>localhost:3000</code>, the API service would start up, and then remain running to handle future requests directly.</li>
</ul>
</li>
<li>Those services used Stripe‚Äôs <a href="https://www.youtube.com/watch?v=lKMOETQAdzs">autoloader</a> on the devbox, and thus only loaded Ruby code as-needed, resulting in fast startup times.</li>
<li>The autoloader also tracked which source files were loaded in which service, and watched for filesystem changes; if a loaded file changed on disk, any services using it would automatically restart to pick up the changes.</li>
</ul>
<p>The net effect of this infrastructure was that developers could change code and then nearly immediately ‚Äì without any manual restart ‚Äì access a copy of the service running their new code, at a fixed, sharable (within Stripe) URL. This URL was stable even if they a provisioned a new devbox. Additionally, even though this feature worked for nearly all internal services, each devbox only ran the services that were actually in-use by that developer, reducing the CPU and memory load.</p>
<h2 id="the-pay-command">The <code>pay</code> command¬†<a href="#the-pay-command"><i>	üîóÔ∏é</i></a> </h2>
<p>Devprod also built and maintained a <code>pay</code> command-line tool that offered unified access to a range of devbox features and workflows.</p>
<p>Devboxes could be and were accessed via <code>ssh</code> (encapsulated as <code>pay ssh</code>), but <code>pay</code> also wrapped the most common usage patterns:</p>
<ul>
<li><code>pay test ...</code> would execute <code>minitest</code> tests on the devbox (using <code>ssh</code> under the hood), and piping back output and exit status locally.</li>
<li><code>pay curl</code> wrapped <code>curl</code> and provided a helper to run manual <code>curl</code> commands against services on your devbox, which was often helpful when doing ad-hoc manual testing of API endpoints.</li>
<li><code>pay typecheck</code> wrapped <a href="https://sorbet.org/">Sorbet</a> to typecheck code on the devbox</li>
</ul>
<h2 id="synchronization-barriers">Synchronization barriers¬†<a href="#synchronization-barriers"><i>	üîóÔ∏é</i></a> </h2>
<p>In addition to the convenience these tools provided, they offered another key feature: integration with the source synchronization process.</p>
<p><code>pay</code> commands that operated on source code would communicate with the sync process, and ensure that synchronization was appropriately ‚Äúcaught up‚Äù before executing code remotely. This feature was a critical improvement to the transparency of synchronization: Historically, it was a common ‚Äúgotcha‚Äù to edit a file and then start a test <strong>before</strong> synchronization had completed, and thus run tests against and old version while <strong>believing</strong> you were testing your new code. Often this results in a false belief that a change didn‚Äôt work, and can result in wild goose chases and immense frustration.</p>
<p>By initiating workflows from the laptop, <code>pay</code> subcommands were guaranteed that they saw the same filesystem state as the editor, and by colluding with synchronization, they could ensure the devbox saw a state ‚Äúat or after‚Äù that state.</p>
<p>The straightforward approach to such a ‚Äúsync barrier‚Äù requires (e.g) <code>pay test</code> to trigger a new synchronization, which adds frustrating latency to workflows, even if you haven‚Äôt changed anything at all. By making use of watchman‚Äôs <a href="https://facebook.github.io/watchman/docs/cmd/clock">clock</a> feature, we were able to do better and avoid unnecessary roundtrips. In the common case where an engineer edits a file and runs a test, the timeline of events would look like:</p>
<ul>
<li>The user edits and saves a file</li>
<li><code>pay sync</code> is woken up by <code>watchman</code>. It notes the filesystem clock, and starts an <code>rsync</code></li>
<li>The user starts a <code>pay test</code> command</li>
<li><code>pay test</code> asks <code>pay sync</code> to wait for synchronization to catch up</li>
<li>In response, <code>pay sync</code> first checks the filesystem clock once again</li>
<li>Because the filesystem has not changed since the earlier save, this timestamp matches one associated with the current <code>rsync</code></li>
<li><code>pay sync</code> now knows it only has to wait for the <strong>already in-flight</strong> <code>rsync</code> command, prior to notifying <code>pay test</code></li>
</ul>
<p>Using the <code>watchman</code> clock makes this robust to reorderings of events; if the <code>pay test</code> invocation happens before, during, or after the synchronization, we will still get correct behavior.</p>
<p>This <code>pay sync</code> coordination also provided a useful hook for visibility into the health of the synchronization process. Developer laptops are laptops, and routinely go offline and come back online. Thus, synchronization failures, or even long periods where synchronization lags, may be normal, making it difficult to usefully collect error statistics from the sync script. However, if a user runs a <code>pay</code> command that will work on the devbox, that represents good evidence that the user <strong>expects</strong> synchronization to be healthy. Thus, if the sync-barrier call to the synchronization process fails or times out, that is an appropriate moment to report an error to Stripe‚Äôs central exception tracker. That report, in turn, allowed devprod to monitor the overall health of the synchronization system and the user experience.</p>

<p>As mentioned above, Stripe engineers could use any editor they chose, but by 2019 the developer productivity team had made the choice to invest in VS Code. They declared that the preferred editor, and invested in tooling for that environment in particular.</p>
<p>Some of this work consisted of internal plugins with small but meaningful quality-of-life improvements, such as support for running <code>pay test</code> on the specific test under the cursor.</p>
<p>However, more substantial improvements happened as <a href="https://sorbet.org/">Sorbet</a> ‚Äì Stripe‚Äôs Ruby typechecker ‚Äì matured and gained an <a href="https://sorbet.org/docs/vscode#installing-and-enabling-the-sorbet-extension">LSP server implementation</a> (LSP is the <a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a>, and defines an interface for a server to offer language-aware features such as ‚Äúfind definition‚Äù or autocomplete in a way that can be consumed by a variety of different editors).</p>
<p>Stripe configured VS Code to run the Sorbet LSP server on the devbox over <code>ssh</code>, communicating via stdin and stdout like a local LSP server. Doing this allowed Sorbet to make use of the devbox‚Äôs large amount of RAM (LSP servers in general tend to be memory-hungry on large codebases, and Sorbet was no exception), and to run in a Linux environment that easier for the Sorbet team to monitor, debug and test.</p>
<p>In general, this approach worked fairly well; LSP servers generally must tolerate some latency, and so the additional network hop and delay due to file synchronization mostly did not pose problems. For instance, the LSP protocol is designed to handle the (extremely common case) where the user has made changes in the editor but not yet saved them to disk, and thus has the editor send relevant edits directly to the server. This process, in effect, also automatically provides robustness to some latency in file synchronization.</p>

<p>In my opinion, the tooling described here was pretty neat, worked pretty well, and created an effective and productive development experience for many of Stripe‚Äôs engineers. I want to reflect here on a few of the details of Stripe‚Äôs organization and technical stack that I think shaped the tooling; these are areas likely worth considering if you‚Äôre working on developer tooling at a different organization.</p>
<h2 id="organization-size">Organization size¬†<a href="#organization-size"><i>	üîóÔ∏é</i></a> </h2>
<p>The tooling I described here was developed over a range of time where Stripe‚Äôs engineering team grew from a few hundred to over a thousand. Over that time, devprod grew from less than one FTE into a team of perhaps a dozen engineers. Much of the tooling described here was built near that latter scale, when devprod was 3+ engineers.</p>
<p>This scale ‚Äì the scale of devprod, and in turn the scale of the overall organization, such that it could afford 10 FTEs on tooling ‚Äì was a major factor in our choices. I‚Äôve described a lot of fairly-involved custom tooling; we needed enough engineers to build <strong>and maintain</strong> it, and enough ‚Äúcustomer‚Äù engineers for that investment to pay off. I will again stress maintenance and reliability: Stitching together a file watcher and <code>rsync</code> in order to synchronize files from a laptop is easy ‚Äì the first version existed by 2012 and represented under a day of initial investment. The eventual magic was not that such a tool existed, but that it was reliable enough, and stayed reliable enough across growth and changing needs, that you <strong>didn‚Äôt have to think about it</strong> as a user.</p>
<p>In additionl, Stripe was also <strong>growing</strong> rapidly. In a rapidly growing organization, it‚Äôs much more important to have a developer environment that ‚Äújust works‚Äù out of the gate, with minimal setup pain for new engineers. In a more-static organization, engineers can learn the quirks of the tools and how to work around them, and that‚Äôs a mostly-one-time cost. In one that‚Äôs rapidly growing, those costs are constantly borne by every new hire and by the engineers training them, and the return to stability and seamlessness go up.</p>
<p>This property was a large component of the motivation for the ‚Äúdevbox‚Äù model. Local development on the laptop has advantages, but it‚Äôs much harder to manage the environment centrally or help users debug, and so it almost-inevitably requires more expertise and involvement from individual developers. Moving to centrally-provisioned, ephmeral devboxes, let us centralize most of that maintenance, and let us recommend a blanket ‚Äúpull <code>master</code> and re-create your devbox‚Äù as first-line debugging advice that resolved most problems.</p>
<h2 id="codebase">Codebase¬†<a href="#codebase"><i>	üîóÔ∏é</i></a> </h2>
<p>I‚Äôve mentioned that this infrastructure supported a large monorepo with relatively consistent architecture and patterns, both across time and across the codebase. Having that stable target created points of leverage, where the developer productivity team could create shared tooling (like the autoloader, and the devbox service manager) that provided fairly deep integration with the environment and codebase and offered user-friendly features. In an environment that included a much-larger variety of languages, runtimes, patterns, it doesn‚Äôt make sense to specialize as much for any one, and infrastructure teams need to build more-generic tooling that treats user code as more of a opaque box.</p>
<p>Moreover, for various historical, business, and technical reasons, Stripe‚Äôs codebase was fairly tightly coupled in a number of ways; it was clear to those of us on the developer productivity team that it would not easily or rapidly be decomposed in any way (e.g. into a larger number of microservices). We did continually invest in tools and patterns for modularity and abstraction within the monorepo, but the confidence in the overall ‚Äústickiness‚Äù of the structure of the Ruby monorepo also justified our large investments in specialized tooling.</p>
<p>I will note that even at Stripe this choice was somewhat contentious and not always unanimous. Even though the majority of the code and business value lived in the Ruby monorepo, Stripe still did have a large number of other codebases running infrastructure components or parts of specialized pipelines; these generally received less support from the tooling and infrastructure teams, which was a recurring source of tension.</p>
<p>The fact that the monorepo was written in Ruby, specifically, also shaped a lot of our decisions in detail. A compiled language with more-decoupled source and artifacts may have pushed us in other directions, for instance. In addition, Stripe‚Äôs monorepo was (to our knowledge) the largest Ruby codebase in existence, meaning we were often on our own and that existing tooling struggled to support our scale in one way or another.</p>
<h2 id="closing-thoughts">Closing thoughts¬†<a href="#closing-thoughts"><i>	üîóÔ∏é</i></a> </h2>
<p>Maintaining developer productivity as an engineering organization grows is <strong>hard</strong>. It is almost inevitable that per-engineer productivity drops to some extent as an organization and codebase grows, even though it‚Äôs also nearly-impossible to quantify that effect.</p>
<p>The challenges arise at all levels of the organization, and are social and organizational as often as they are technical. I will never claim to have all the answers or that Stripe found optimal or even ‚Äúconsistently good-enough‚Äù solutions to all aspects of this challenge. However, I do think that by 2019 or so, Stripe had invested enough in developer tooling that we had in many ways <strong>improved</strong> the median developer experience compared to many smaller stages of growth, and I‚Äôve attempted to convey here the basic choices and infrastructure supporting that experience.</p>
<p>Finally: the development experience, of course, is only part of the story: the full lifecycle of code and features continues onward into CI and code review and ultimately through deployment into production, where it will be further observed, debugged, and evolved. Writing about those systems would require further posts at least this long.</p>


</div></div>
  </body>
</html>
