<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tratt.net/laurie/blog/2022/uml_my_part_in_its_downfall.html">Original</a>
    <h1>UML: My Part in Its Downfall</h1>
    
    <div id="readability-page-1" class="page"><div id="article-body"><p>



In the last year or two there has been a slow but steady trickle of articles
attempting to account for <a href="https://en.wikipedia.org/wiki/UML">UML</a>&#39;s
lack of long-term success (if you have time for only one, I suggest <a href="https://buttondown.email/hillelwayne/archive/why-uml-really-died/">Hillel
Wayne&#39;s article</a>). As fate would have it, I had some involvement in UML
standardisation in the early 2000s, so I saw some of the going-ons from
the &#34;inside&#34;. Although I&#39;ve <a href="https://tratt.net/laurie/blog/2020/stick_or_twist.html">touched on this before</a>, I&#39;ve never
written about my experiences in detail because I was worried about offending
people who I like. 17 years after the fact, I hope that
the likelihood of anyone being offended is fairly low.

</p><p>In this post I&#39;m going to try and explain some of the factors that I think
contributed to UML&#39;s downfall. To some extent this is a historical document, at
least of my perspective. But, with the benefit of hindsight, I feel there are
general lessons to be drawn about how both group dynamics and standardisation
can develop in unfortunate ways. Be
forewarned that I only saw part of what was
going on (so I will be unaware of possibly important details), I didn&#39;t write a
diary (so I will recall things incorrectly), and my recollections are bound to
reflect my biases (my ego will inevitably encourage me to relay the story in a
way that presents me in a better light than I deserve).


</p><h2>Background</h2><p>

People had long created diagrams to document important aspects of software.
Over the course of the late 80s and early 90s, three diagramming styles or,
more grandly, &#34;methods&#34;, had started to become popular: the Booch, Jacobson,
and Rumbaugh methods. After two merges, these
three methods were combined to create UML (hence the &#34;U&#34; standing for &#34;Unified&#34; in UML),
released as a standard through the OMG (Object Management Group), a standards
body that had previously been best known for the <a href="https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture">CORBA</a>
standard.

</p><p>The standardisation of UML coincided with, and was probably a necessary
precondition of, a rapid increase in its use. From my perspective
as an undergraduate in the late 1990s, Martin Fowler&#39;s <a href="https://www.martinfowler.com/books/uml.html">UML Distilled</a> book was
also an important part in UML reaching a wide audience.
Class diagrams, in particular, became extremely common, and a
simplification of UML&#39;s class diagram syntax had come close to being a lingua
franca by 2000. Here&#39;s a simple example:

</p><p>
<img src="https://tratt.net/laurie/blog/extra/2022/uml_my_part_in_its_downfall/class_diagram.png"/>
</p>
<p>This shows two classes: an <code>Employee</code> class; and a subclass (the
arrow with the hollow triangle) of employees who are <code>Managers</code>. All
employees have a name and a salary. Managers manage zero or more (the &#34;*&#34; near
the end of the arrow) employees.

</p><p>At this point, I hope you&#39;ll forgive me if I briefly explain my involvement
so that you have the necessary context to interpret the rest of this post.
I started a PhD in September 2000, though my would-be supervisor soon left
for a start-up. That left me in need of
a new supervisor and by summer 2001 I had found someone willing to take me on.
Tony Clark remains an inspiration for me: technically gifted, thoughtful, and
with an excellent sense of humour. He didn&#39;t deserve to be lumbered with a
loud-mouth pea-brain like me, but he never once complained!
Tony was part of a group of academics and industrialists working towards UML
2.0. It was through that work that I gradually got sucked into the world
of UML standardisation. I had no idea what I was doing in
2001. In 2002 I started to get the hang of things and contributed some
bits and pieces. In 2003 everyone else in my research group left for a start-up:
I chose to inherit their responsibilities, forcing me to become a minor
player in standardisation in my own right. By early 2005 I had become thoroughly
disillusioned and moved onto other things.


</p><h2>The situation in the early 2000s</h2>
<p>The situation in 2000 was, roughly, that UML had been much more successful
than anyone expected. Although UML was mostly being used for sketching designs
(i.e. a communication mechanism between humans), some important industries
(e.g. aviation) were starting to think of it as a plausible basis for their
more formal approaches to creating software (i.e. a communication mechanism
between human and machine) . A growing industry was creating UML-based tools. By
far the biggest player was IBM (who soon acquired Rational, the company that had
incubated UML  and created the most popular
UML-based tool). It&#39;s
difficult to imagine now, but in 2000 IBM was not only rich in money and
people, but still had the ability to nudge the entire software industry in its desired
direction. IBM&#39;s involvement was implicitly taken as a sign that there was
money to be made in UML.

</p><p>The problem was that UML version 1 was manifestly unsuited to the ambitions IBM and
others had for it. Not only was the standard itself somewhat vague, but what it
was defining was rather vague too. In may ways, UML&#39;s original sin was that
class diagrams – by far the most widely used part
of UML – were largely designed as a diagrammatic notation for
the most common object orientated programming language of the day — C++.
After Java&#39;s success in the late 90s, UML was extended so that it could also
somewhat accurately represent some aspects of Java, but the mechanisms used 
were widely perceived as an ugly hack, and clearly wouldn&#39;t scale to supporting
arbitrary numbers of programming languages.

</p><p>Gradually a consensus emerged: what was needed was for a completely new
version of UML that could precisely represent software; and that new version of
UML itself needed a rigorous definition so that customers would be able to use
a variety of tools from a growing set of UML tool vendors. From this consensus
emerged the vision for what became UML 2.


</p><h2>Standardisation</h2><p>

Since it precedes my involvement, I&#39;m not entirely sure when work started on
UML 2. My best guess is that the general idea started in 1999, and started to
pick up speed during 2000. Work continued in parallel on new versions of UML 1.x
standardisation, since that was the bread and butter of all the participants
involved. All UML standardisation was done under the auspices of the OMG who,
based on my later experiences, were almost certainly active participants in encouraging
the formation of UML 2.

</p><p>A fundamental question was: what use cases should UML support? By the time I
came along, it was taken as a given that UML should be used as the basis of
&#34;software modelling&#34;, an intended step-change in the way software was
engineered. Taking direct inspiration from architectural blueprints, the aim
was for UML to be used to give software more solid foundations. Gradually (and
we&#39;ll get more into this later), this frequently came to be taken to mean that
software would be partly or wholly created directly from UML models.
This ambitious aim presented two separate challenges to standardisation.


</p><h3>Rigorous underpinnings</h3><p>

The early UML 1.x standards were fairly standard prose documents, often vague, and
missing much desirable detail. Tool vendors — who had often updated
pre-UML tools to support UML — disagreed on fundamental aspects.
Sometimes, particularly when older tools were involved, this was intentional;
sometimes it was unintentional when people interpreted the standards
in different ways. Either way, interoperability between tools was poor. The
situation was at best embarrassing, and at worst a serious impediment to UML&#39;s
success.

</p><p>The standardisation community universally agreed that UML 2 would need
rigorous underpinnings. However, no-one knew exactly what &#34;rigorous&#34; should
mean, or how one should go about achieving it. Gradually, a small group of
academics , who did have an idea of
what rigorous could mean and how one could go about it, became involved in
standardisation.

</p><p>Their idea was, in essence, to use a subset of UML  to give &#34;full&#34; UML a
denotational semantics. Practically speaking, it meant that one could &#34;grow&#34;
the UML specification from a small core, using UML class diagrams for much of
the semantic definitions, and UML&#39;s constraint language (OCL)
for fine-grained semantic specifications.

</p><p>However, while much of the standardisation community liked the general sound
of this, few of them had the background needed to understand the details.
This is not a criticism — I was lucky enough to be able to work
with some of the key people, and it took me a year or more of blindly copying
what they were doing before the underlying concepts sunk in.
Most people did not have such an
opportunity. It didn&#39;t help that using UML class diagrams led to very verbose
semantic definitions, leading to the development of meta-programming-esque techniques
to reduce some of the drudgery — which confused people even more!

</p><p>This meant that one would regularly be in a room of 20 people arguing over
definitions, with at most 1 or 2 people realistically capable of translating
the outcome of the discussion into rigorous (or, perhaps more accurately,
semi-rigorous) semantics — and probably only the same 1 or 2 people
capable of fully understanding the results. Once the core academics had wandered
off to greener pastures, most (though not quite all) of the relevant skills
left with them.

</p><p>By the time I left the UML standardisation world, it seemed to me that the
community was gradually lowering its expectations to somewhere between UML
1.x&#39;s vagueness and &#34;true&#34; rigour. Looking at the <a href="https://www.omg.org/spec/UML/2.5.1/PDF">current UML 2.5.1</a>
specification seems to suggest that was what happened. It&#39;s plausible
that this lowering of expectations would have happened anyway.
In particular there was a subset of (often, but not always, small) tool vendors who
wanted to spend the bare minimum on updating their software. While they lacked a
clear strategy for slowing down the rate of change, they sometimes achieved
this effect anyway through various tactics, including simply running the clock
down in physical meetings.


</p><h3>What to standardise?</h3><p>

The ideal of &#34;standardisation&#34; has traditionally been to look at what&#39;s
working already and define that as a standard. While the reality of
standardisation has often been rather more muddy than the ideal might
suggest, UML 2 took this to rarely seen levels.

</p><p>As I mentioned earlier, by 2000 the standardisation community had decided
that UML should be the basis of a new way for creating software. By late 2000,
the OMG had actively corralled people behind a new vision, called &#34;<a href="https://www.omg.org/cgi-bin/doc?omg/00-11-05">Model Driven
Architecture</a>&#34; (MDA). The idea was to automatically generate vast swathes of code
from UML models. Here&#39;s a direct quote from the MDA document
:

</p><blockquote>
Whether your ultimate target is CCM, EJB, MTS, or some other component or
transaction-based platform, the first step when constructing an MDA-based
application will be to create a platform- independent application model
expressed via UML in terms of the appropriate core model.  Platform specialists
will convert this general application model into one targeted to a specific
platform such as CCM, EJB, or MTS.  Standard mappings will allow tools to
automate some of the conversion.

<p>...

</p><p>
The next step is to generate application code itself. For component
environments, the system will have to produce many types of code and
configuration files including interface files, component definition files,
program code files, component configuration files, and assembly configuration
files. The more completely the platform-specific UML dialect reflects the
actual platform environment, the more completely the application semantics and
run-time behavior can be included in the platform-specific application model
and the more complete the generated code can be. In a mature MDA environment,
code generation will be substantial or, perhaps in some cases, even complete.
Early versions are unlikely to provide a high degree of automatic generation,
but even initial implementations will simplify development projects and
represent a significant gain, on balance, for early adopters, because they will
be using a consistent architecture for managing the platform-independent and
platform-specific aspects of their applications.
</p></blockquote>
<p>The basic idea underlying MDA was that people creating
software would first create a &#34;PIM (Platform Independent Model)&#34; — think, a
programming-language neutral UML class model. From that (ideally by pushing a button)
they would create a more detailed &#34;PSM (Platform Specific Model)&#34; — think, a class
model with annotations for a particular programming language (e.g. Java). Once
the PSM had been suitably tweaked, it would be transformed into &#34;low-level&#34;
programming language code that users would compile and run.

</p><p>For example, if I used the class diagram from earlier in the post as my PIM:

</p><p>
<img src="https://tratt.net/laurie/blog/extra/2022/uml_my_part_in_its_downfall/class_diagram.png"/>
</p>
<p>and I wanted to eventually create a Java system I might transform it into the following PSM:

</p><p>
<img src="https://tratt.net/laurie/blog/extra/2022/uml_my_part_in_its_downfall/psm.png"/>
</p>
<p>The PSM makes more details concrete: I&#39;ve specified the visibility of fields
(&#34;-&#34; means &#34;private&#34;); and I&#39;ve started using specific Java types
(<code>int</code> and <code>Set</code>). I might then generate Java code along the lines of:

</p><pre>class Employee {
  private String name;
  private int salary;
  public String get_name() { return this.name; }
  public void set_name(String name) { this.name = name; }
  public int get_salary() { return this.salary; }
  public void set_salary(int salary) { this.salary = salary; }
}

class Manager extends Employee {
  private Set<employee> manages;
}
</employee></pre>
<p>Soon the vision for what should be standardised for UML became mixed
together with &#34;what does MDA need?&#34; It&#39;s thus important to delve into the MDA
vision in more depth.


</p><h2>The MDA vision</h2>
<p>By the time I had become involved in things, most of the nuance in the
MDA text I quoted above had disappeared. It was largely taken as a
given that only the most talented people in an organisation would be involved
in the creation of PIMs; a cadre of second-class citizens would then have to
engage in the drudgery of creating a PSM from a PIM; and no-one would need to
worry about the code generated from the PSM.

</p><p>The deep flaws in this vision might be obvious to most readers, but the
standardisation community, intentionally or not, trained itself over time
to avoid thinking about them. The most glaring flaw
is: where would &#34;behaviour&#34; (i.e. the nitty gritty details of what a program
should do) be specified? UML class diagrams are fine for expressing program
structure, but they don&#39;t tell you what a function should actually do. It&#39;s
probably fairly obvious what <code>get_salary</code> should do, but what
if I&#39;d added a function <code>print_names</code> to <code>Manager</code>
in the PIM? Should it have printed out employee names? in alphabetical
order? etc. Sometimes OCL constraints on a function definition would make clear
what the function should do, but most of us find it difficult to provide
precise constraints for complex behaviour. UML did have state machines, but they
are only really suitable for expressing certain kinds of behaviour. UML&#39;s sequence
and collaboration diagrams are even more limited in their abilities to express
behaviour.

</p><p>Once in a while, someone would bring up the problem of specifying behaviour:
they would in general either be ignored, or told that the problem was so trivial as
to not be worth worrying about. Far, far more energy was spent arguing about what
level of detail should be modelled in PIMs and PSMs — I remember day-long
arguments in windowless hotel rooms with people arguing about whether a certain
detail belonged at the level of PIM or PSM.

</p><p>Perhaps only one piece of software tried to make the true MDA vision a
a reality — Compuware&#39;s (long discontinued) OptimalJ. It was backed by a
large team but it was a beast of a thing that brought normal computers of the day
to their knees . I had to write an evaluation
of OptimalJ and soon realised how flawed the vision it was trying to implement was.
Yes, I could create a class diagram as a PIM; press a button and create a PSM;
and press another button and generate Java code. But the Java code had all
sorts of holes in it that I had to fill in — and if I changed the model in
any way, half of the code I&#39;d written would no longer compile, let alone run
correctly. In other words, OptimalJ automated the most trivial things, leaving
all the hard work to the end user. The team behind OptimalJ were
talented and hard working, but what they ended up showing was that MDA not only
failed to improve programmer productivity, but actually slowed down
development!

</p><p>At the same time, many OMG members, and the OMG itself 
in the form of its charismatic leader, started putting increasing efforts into
selling the MDA vision to the wider software community. I only realised how much
impact this was having when I started to get asked by normal programmers
questions along the lines of &#34;I hear this MDA / UML thing is going to automate our jobs
away?&#34; My standard answer went along the lines of &#34;it&#39;s mostly aimed at non-programmers
who want to create simple software&#34; which, I think, reassured the people I spoke to.
However, the fact that they asked the question showed that MDA&#39;s marketing was starting
to work. The question soon became: could the community make the reality of MDA
match the vision?


</p><h2>The rise and fall of QVT</h2><p>

A fundamental idea underlying MDA was that PIMs needed to be transformed into
PSMs and PSMs transformed into code. Slowly but surely it became clear to the standardisation
community that such transformations were much more difficult than first thought.
By 2002 this was recognised to be a significant problem. The community thus decided that
what was needed was a new standard for model transformations. For reasons that
I&#39;ve now forgotten (or perhaps never knew) this ended up with the unwieldy
title &#34;QVT (Queries-Views-Transformations)&#34;.

</p><p>The QVT call for proposals went out in (I think) late 2002 or early 2003. A
few months later 8 proposals had been created , some of them in a literal burst of
late-night activity. The core of the proposal I was involved in derived from a
single afternoon&#39;s discussion, which we then worked up into a semblance of a
document over a few weeks. Astonishingly, our half-baked proposal was soon considered one of the
&#34;leading&#34; proposals, which tells you something about some of the rest!

</p><p>There is a fundamental tension in the sorts of transformations MDA, and thus
QVT, needed, which I&#39;ll simplify as follows. &#34;Imperative&#34; (think normal
programming language) transformations explicitly specify the sequence of steps
needed to transform an input to an output (e.g. &#34;for each element in E, create
an element E&#39; by calling the function f&#34;). &#34;Declarative&#34; (think Prolog)
transformations specify the relationship between the input and output (e.g.
&#34;for each element in E there is an output element E&#39; of the form X&#34;), leaving
an &#34;engine&#34; to work out how to make the input and output correct with respect
to the specification. Imperative transformations are easy to write but are hard
to rerun non-destructively: if you&#39;ve changed the output, it&#39;s difficult to
then change the input, and see changes in the input reflected sensibly in the
output. Declarative transformations are hard to write (particularly when the
relationship between input and output is complex) but hold the promise of
continually reordering the input and output, even when both have been changed
independently.

</p><p>Some of the QVT proposals asserted that only fully-imperative
transformations were needed; some that fully declarative transformations were
possible; and some (like the one I was involved with) tried to pretend they
could span both dimensions. How could
such disparate approaches end up as a single standard?

</p><p>OMG meetings were at that point held 5 times a year in person (a level of
travel that now horrifies me!), so we had many opportunities to try to find
a compromise. However, compromise was rather hard to find.
At one extreme was a camp that wanted to put a badge on their existing imperative
programming language and call it QVT. At the other extreme was a camp that
believed one could specify first order logic constraints and always efficiently find an
optimal solution (something that would lead to the known laws of mathematics
being rewritten).

</p><p>The proposal I was involved with was much less concrete. Partly influenced
by the PIM/PSM idea, our proposal proposed allowing high-level abstract
definitions of transformations (what we called &#34;relations&#34;) and low-level
implementations (what we called &#34;mappings&#34;). Relations were intended to be
represented with UML-ish diagrammatic syntax, but we explicitly said that any
language could be used for mappings (though we provided an example &#34;Model
Transformation Language&#34;, which was in part influenced by functional
programming-esque pattern matching).

</p><p>Somehow, it gradually came to be seen that a variant on our approach could
satisfy the two competing camps who could simply claim that their
languages were simply low-level QVT languages! Someone came up with the idea of
calling the implementation of a transformation a &#34;black box&#34;, since any language can be used inside a black
box without anyone else noticing. That seems to be the terminology used in the <a href="https://www.omg.org/spec/QVT/1.3/PDF">final standard</a>.

</p><p>Initially I was pleased by the idea that our proposal might end
up the &#34;winner&#34;. However, slowly but surely, I started to wonder what exactly such a standard
could achieve: is a standard a meaningful standard if implementations of it
aren&#39;t in any way compatible? The answer, I was forced to conclude, was &#34;no&#34;.

</p><p>However, our inability to create a meaningful standard hid a deeper problem:
none of us knew how to create transformations of remotely the
size, sophistication, and flexibility that MDA would need. We couldn&#39;t
scale such transformations beyond toy examples and, in my opinion, we lacked
plausible ideas for doing so. Since such transformations were a key part
of MDA, QVT&#39;s failure thus also guaranteed the failure of MDA.

</p><p>Once I&#39;d come to this realisation, I slowly wound down my involvement, concentrating instead on my PhD
(about one third of my eventual thesis is, perhaps unsurprisingly, about UML-ish
transformations; the other two-thirds is about programming languages, a topic
rather nearer to my heart). I
attended my last standardisation meeting in early 2005 and that was, more or
less, the end of my involvement in UML standardisation .


</p><h2>Summary</h2><p>

With the benefit of hindsight, I think UML had quite possibly reached not only
its actual, but also its potential, peak in 2000: as a
medium for software sketching, people only ever needed the basics from it.
However, the standardisation community developed an ambitious vision for UML
that far exceeded sketching. Whether or not that vision could ever be realised
can be seen as a matter of genuine debate: what seems unarguable to me is
that such a vision was deeply unsuited to
any standardisation process. QVT is the most succinct
example of trying to standardise what was, at best, early-stages research, with
failure inevitably resulting. However, while the standardisation overreach
inherent in QVT stayed largely within OMG&#39;s confines, MDA&#39;s failure was widely
noted. Not only was MDA seen to fail, but by association it undermined the
success of UML as a sketching language, turning it into the butt of jokes that
it has largely remained to as these days.

</p><p>I could not have guessed this at the time, but my involvement in all this
taught me several valuable lessons, two of which I think are worth highlighting.

</p><p>First and foremost, group dynamics can develop in such a way that reasonable
optimism turns into blind optimism and expressing
doubts becomes a taboo. When that happens, it is easy for the group to drift
towards extreme positions that guarantee the group&#39;s failure. The UML standardisation
community became ever more invested in UML 2&#39;s success: at first, doubting
views were dismissed as referencing trivial problems; eventually such views stopped being expressed at all. The
community only talked about success, even when there was significant
evidence that failure was the most likely outcome . Similarly, QVT
was the wrong idea at the wrong time, but people were so desperate for success
that they chose to ignore fundamental problems.

</p><p>Second, when standardisation moves from &#34;standardise what already exists&#34; to
&#34;standardise things that we think would be good but don&#39;t yet exist&#34; it enters
dangerous territory. I rather like research, but standards
committees are about the worst possible place to do research. At best
an unsatisfying lowest common denominator ends up being chosen, but at worst
the process collapses. There should be no shame, in my opinion, in a
standardisation process realising that it has raced ahead of where the
state-of-the-art is, and that it would be better to revisit matters when
meaningful progress has occurred.

</p><p>I jokingly titled this post &#34;UML: My Part in its Downfall&#34;. Really, it&#39;s
probably more accurate to say that its downfall had been predestined before I
had anything to do with it, and I probably had no observable effects on its
success or failure. But I hope you found my perspective, and half-remembered
memories, of this cautionary tale interesting!


</p><p><b>Acknowledgements</b>: thanks to
<a href="https://diekmann.uk/">Lukas Diekmann</a><a> and
</a><a href="https://hillelwayne.com/">Hillel Wayne</a>
for comments.

</p>



<h3>Footnotes</h3>
<p><a name="44905084">[1] To some extent this was a more ambitious variant of </a><a href="https://en.wikipedia.org/wiki/Computer-aided_software_engineering">CASE
tools</a>.</p></div></div>
  </body>
</html>
