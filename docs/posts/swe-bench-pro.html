<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/scaleapi/SWE-bench_Pro-os">Original</a>
    <h1>SWE-Bench Pro</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Code and data for the following works:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://static.scale.com/uploads/654197dc94d34f66c0f5184e/SWEAP_Eval_Scale%20(9).pdf" rel="nofollow">SWE-bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?</a></p>
</li>
<li>
<p dir="auto">HuggingFace: <a href="https://static.scale.com/uploads/654197dc94d34f66c0f5184e/SWEAP_Eval_Scale%20(9).pdf" rel="nofollow"></a><a href="https://huggingface.co/datasets/ScaleAI/SWE-bench_Pro" rel="nofollow">https://huggingface.co/datasets/ScaleAI/SWE-bench_Pro</a></p>
</li>
<li>
<p dir="auto">Public Leaderboard: <a href="https://static.scale.com/uploads/654197dc94d34f66c0f5184e/SWEAP_Eval_Scale%20(9).pdf" rel="nofollow"></a><a href="https://scale.com/leaderboard/swe_bench_pro_public" rel="nofollow">https://scale.com/leaderboard/swe_bench_pro_public</a></p>
</li>
<li>
<p dir="auto">Commercial (Private) Leaderboard: <a href="https://static.scale.com/uploads/654197dc94d34f66c0f5184e/SWEAP_Eval_Scale%20(9).pdf" rel="nofollow"></a><a href="https://huggingface.co/datasets/ScaleAI/SWE-bench_Pro" rel="nofollow">https://huggingface.co/datasets/ScaleAI/SWE-bench_Pro</a></p>
</li>
</ul>

<p dir="auto">SWE-Bench Pro is a challenging benchmark evaluating LLMs/Agents on long-horizon software engineering tasks.
Given a <em>codebase</em> and an <em>issue</em>, a language model is tasked with generating a <em>patch</em> that resolves the described problem.</p>
<p dir="auto">The dataset is inspired from SWE-Bench: <a href="https://github.com/SWE-bench/SWE-bench">https://github.com/SWE-bench/SWE-bench</a></p>
<p dir="auto">To access SWE-bench Pro, copy and run the following code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from datasets import load_dataset
swebench = load_dataset(&#39;ScaleAI/SWE-bench_Pro&#39;, split=&#39;test&#39;)"><pre><span>from</span> <span>datasets</span> <span>import</span> <span>load_dataset</span>
<span>swebench</span> <span>=</span> <span>load_dataset</span>(<span>&#39;ScaleAI/SWE-bench_Pro&#39;</span>, <span>split</span><span>=</span><span>&#39;test&#39;</span>)</pre></div>

<p dir="auto">SWE-bench Pro uses Docker for reproducible evaluations.
In addition, the evaluation script requires Modal to scale the evaluation set.</p>
<p dir="auto">Follow the instructions in the <a href="https://docs.docker.com/engine/install/" rel="nofollow">Docker setup guide</a> to install Docker on your machine.
If you&#39;re setting up on Linux, we recommend seeing the <a href="https://docs.docker.com/engine/install/linux-postinstall/" rel="nofollow">post-installation steps</a> as well.</p>
<p dir="auto">Run the following commands to store modal credentials:</p>
<div data-snippet-clipboard-copy-content="pip install modal
modalv setup # and follow the prompts to generate your token and secret"><pre><code>pip install modal
modalv setup # and follow the prompts to generate your token and secret
</code></pre></div>
<p dir="auto">After running these steps, you should be able to see a token ID and secret in  <code>~/.modal.toml</code>:
EG:</p>
<div data-snippet-clipboard-copy-content="token_id = &lt;token id&gt;
token_secret = &lt;token secret&gt;
active = true"><pre><code>token_id = &lt;token id&gt;
token_secret = &lt;token secret&gt;
active = true
</code></pre></div>
<p dir="auto">We store prebuilt Docker images for each instance. They can be found in this directory:</p>
<p dir="auto"><a href="https://hub.docker.com/r/jefzda/sweap-images" rel="nofollow">https://hub.docker.com/r/jefzda/sweap-images</a></p>
<p dir="auto">The format of the images is as follows.</p>
<p dir="auto"><code>jefzda/sweap-images:{repo_base}.{repo_name}-{repo_base}__{repo_name}-{hash}</code></p>
<p dir="auto">For example:</p>
<p dir="auto"><code>jefzda/sweap-images:gravitational.teleport-gravitational__teleport-82185f232ae8974258397e121b3bc2ed0c3729ed-v626ec2a48416b10a88641359a169d99e935ff03</code></p>

<p dir="auto">First generate patch predictions using your harness of choice.
Evaluate patch predictions on SWE-bench Pro with the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python sweap_pro_eval_modal.py \
    --raw_sample_path=external_hf_v2.csv \
    --patch_path={OUTPUT}/gold_patches.json \
    --output_dir={OUTPUT}/ \
    --scripts_dir=run_scripts \
    --num_workers=100 \
    --dockerhub_username=your-username"><pre>python sweap_pro_eval_modal.py \
    --raw_sample_path=external_hf_v2.csv \
    --patch_path={OUTPUT}/gold_patches.json \
    --output_dir={OUTPUT}/ \
    --scripts_dir=run_scripts \
    --num_workers=100 \
    --dockerhub_username=your-username</pre></div>
<p dir="auto">Replace gold_patches with your patch json, and point raw_sample_path to the SWE-Bench Pro CSV.</p>
</article></div></div>
  </body>
</html>
