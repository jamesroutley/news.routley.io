<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/s-casci/tinyzero">Original</a>
    <h1>Show HN: Easily train AlphaZero-like agents on any environment you want!</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/s-casci/tinyzero/blob/main/tinyzero.png"><img src="https://github.com/s-casci/tinyzero/raw/main/tinyzero.png" width="480"/></a></p>
<p dir="auto">Easily train AlphaZero-like agents on any environment you want!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" tabindex="-1" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">Make sure you have Python &gt;= 3.8 intalled. After that, run <code>pip install requirements.txt</code> to install the necessary dependencies.</p>
<p dir="auto">Then, to train an agent on one of the existing environments, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 tictactoe2d/train.py"><pre>python3 tictactoe2d/train.py</pre></div>
<p dir="auto">where <code>tictactoe2d</code> is the name of the environment you want to train on.</p>
<p dir="auto">Inside the train script, you can change some parameters, such as the number of episodes, the number of simulations and enable <a href="https://wandb.ai/site" rel="nofollow">wandb</a> logging.</p>
<p dir="auto">Similarly, to evaluate the trained agent run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 tictactoe2d/eval.py"><pre>python3 tictactoe2d/eval.py</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-add-an-environment" aria-hidden="true" tabindex="-1" href="#add-an-environment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Add an environment</h2>
<p dir="auto">To add a new environment, you can follow the <code>game.py</code> files in every existing examples.</p>
<p dir="auto">The environment you add should implement the following methods:</p>
<ul dir="auto">
<li><code>reset()</code>: resets the environment to its initial state</li>
<li><code>step(action)</code>: takes an action and modifies the state of the environment accordingly</li>
<li><code>get_legal_actions()</code>: returns a list of legal actions</li>
<li><code>undo_last_action()</code>: cancels the last action taken</li>
<li><code>to_observation()</code>: returns the current state of the environment as an observation (a numpy array) to be used as input to the model</li>
<li><code>get_result()</code>: returns the result of the game (for example, it might be 1 if the first player won, -1 if the second player won, 0 if it&#39;s a draw, and None if the game is not over yet)</li>
<li><code>get_first_person_result()</code>: returns the result of the game from the perspective of the current player (for example, it might be 1 if the current player won, -1 if the opponent won, 0 if it&#39;s a draw, and None if the game is not over yet)</li>
<li><code>swap_result(result)</code>: swaps the result of the game (for example, if the result is 1, it should become -1, and vice versa). It&#39;s needed to cover all of the possible game types (single player, two players, zero-sum, non-zero-sum, etc.)</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-add-a-model" aria-hidden="true" tabindex="-1" href="#add-a-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Add a model</h2>
<p dir="auto">To add a new model, you can follow the existing examples in <code>models.py</code>.</p>
<p dir="auto">The model you add should implement the following methods:</p>
<ul dir="auto">
<li><code>__call__</code>: takes as input an observation and returns a value and a policy</li>
<li><code>value_forward(observation)</code>: takes as input an observation and returns a value</li>
<li><code>policy_forward(observation)</code>: takes as input an observation and returns a distribution over the actions (the policy)</li>
</ul>
<p dir="auto">The latter two methods are used to speed up the MCTS.</p>
<p dir="auto">The AlphaZero agent computes the policy loss as the Kulback-Leibler divergence between the distribution produced by the model and the one given by the MCTS. Therefore, the policy returned by the <code>__call__</code> method should be logaritmic. On the other hand, the policy returned by the <code>policy_forward</code> method should represent a probability distribution.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-add-a-new-agent" aria-hidden="true" tabindex="-1" href="#add-a-new-agent"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Add a new agent</h2>
<p dir="auto">Thanks to the way the value and policy functions are interpreted by the search tree, it&#39;s possible to use or train any agent that implements them. To add a new agent, you can follow the existing example in <code>agents.py</code>.</p>
<p dir="auto">The agent you add should implement the following methods:</p>
<ul dir="auto">
<li><code>value_fn(game)</code>: takes as input a game and returns a value (float)</li>
<li><code>policy_fn(game)</code>: takes as input a game and returns a policy (Numpy array)</li>
</ul>
<p dir="auto">Any other method is not directly used by the MCTS, so it&#39;s optional and depends on the agent you want to implement. For example, the <code>AlphaZeroAgent</code> implements a <code>train_step</code> method to train the model after each episode.</p>
</article>
          </div></div>
  </body>
</html>
