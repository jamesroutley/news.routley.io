<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.infoq.com/news/2024/06/meta-llm-megalodon/">Original</a>
    <h1>Meta Open-Sources Megalodon LLM for Efficient Long Sequence Modeling</h1>
    
    <div id="readability-page-1" class="page"><div>
								<p>Researchers from <a href="https://ai.meta.com/">Meta</a>, <a href="https://www.usc.edu/">University of Southern California</a>, <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, and <a href="https://ucsd.edu/">University of California San Diego</a> recently open-sourced <a href="https://arxiv.org/abs/2404.08801">MEGALODON</a>, a large language model (LLM) with an unlimited context length. MEGALODON has linear computational complexity and outperforms a similarly-sized <a href="https://www.infoq.com/news/2023/07/meta-new-ai-model/">Llama 2</a> model on a range of benchmarks.</p>

<p>MEGALODON is designed to address several shortcomings of the Transformer neural architecture underlying most LLMs. Instead of the standard multihead attention, MEGALODON uses a chunk-wise attention. The research team also introduced sequence-based parallelism during training, improving scalability for long-context training. When evaluated on standard LLM benchmarks, such as <a href="https://paperswithcode.com/dataset/winogrande">WinoGrande</a> and <a href="https://paperswithcode.com/dataset/mmlu">MMLU</a>, MEGALODON outperformed a Llama 2 model with the same amount of parameters, training data, and training compute budget. According to the researchers: </p>

<blockquote>
<p>MEGALODON achieves impressive improvements on both training perplexity and across downstream benchmarks. Importantly, experimental results on long-context modeling demonstrate MEGALODON’s ability to model sequences of unlimited length. Additional experiments on small/medium-scale benchmarks across different data modalities illustrate the robust improvements of MEGALODON, which lead to a potential direction of future work to apply MEGALODON for large-scale multi-modality pretraining.</p>
</blockquote>

<p>Although the Transformer architecture has become the standard for most Generative AI models, Transformers do have some drawbacks. In particular, their self-attention mechanism has quadratic complexity in both compute and storage, which limits the models&#39; input context length. Several alternatives to the standard self-attention model have been developed recently, including structured state space models (SSMs) like <a href="https://arxiv.org/abs/2312.00752">Mamba</a>, which scales linearly with context length. Another scheme that InfoQ recently covered is the <a href="https://www.infoq.com/news/2024/03/rwkv-llm-eagle-7b/">RWKV Project&#39;s</a> attention-free Transformer model, which has no maximum input context length.</p>

<p>MEGALODON builds on the research team&#39;s previous model, <a href="https://github.com/facebookresearch/mega">MEGA</a> (exponential moving average with gated attention), with several new features. First, while MEGA uses a &#34;classical&#34; exponential moving average (EMA) within its attention mechanism, MEGALODON computes a <em>complex</em> EMA (CEMA). Mathematically, the CEMA component makes MEGALODON equivalent to a &#34;simplified state space model with diagonal state matrix.&#34;</p>

<p>The research team trained a seven-billion parameter model, MEGALODON-7B, using the same 2-trillion token dataset that Llama2-7B used; they also used the same training hyperparameters. The team observed that MEGALODON-7B was more computationally efficient. When the Llama model was scaled up to a 32k context length, MEGALODON-7B was &#34;significantly&#34; faster.</p>

<p>Besides evaluating MEGALODON-7B on standard LLM benchmarks, the researchers also tested its performance on the <a href="https://paperswithcode.com/dataset/scrolls">SCROLLS</a> long-context question-answering benchmark, and compared its results with several baseline models, including the modified Llama 2 model with a 32k context length. MEGALODON outperformed all baseline models on the NarrativeQA subtask, and on all tasks achieved results &#34;competitive&#34; with Llama 2.</p>

<p>In a discussion about MEGALODON on Hacker News, one user <a href="https://news.ycombinator.com/item?id=40054901">wondered how well the model performed on recall tasks</a>, given that other non-Transformer models tend to perform poorly. Another user replied:</p>

<blockquote>
<p>For what it&#39;s worth, RWKV&#39;s website on that matter mentions that yes it&#39;s bad on recall, but for the vast majority of tasks you can just ask the question *before* the content, and it&#39;ll handle the task just fine.</p>
</blockquote>

<p>The <a href="https://github.com/XuezheMax/megalodon">MEGALODON code</a> is available on GitHub.</p>

								









  
    

							</div></div>
  </body>
</html>
