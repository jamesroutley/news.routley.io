<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.liquid.ai/liquid-foundation-models">Original</a>
    <h1>Liquid Foundation Models: Our First Series of Generative AI Models</h1>
    
    <div id="readability-page-1" class="page"><div><div id="w-node-_4ac69c6b-6354-b7f1-d147-da6548bbc4fa-18e46dab"><div id="takeaways"><h2>Takeaways</h2><div><p>We announce the first series of Liquid Foundation Models (LFMs), a new generation of generative AI models built from first principles.</p><p>Our 1B, 3B, and 40B LFMs achieve state-of-the-art performance in terms of quality at each scale, while maintaining a smaller memory footprint and more efficient inference.</p><p>Try LFMs today on <strong>Liquid Playground, Lambda (Chat UI and API)</strong>, <strong>Perplexity Labs</strong>, and soon on <strong>Cerebras Inference</strong>. The LFM stack is being optimized for NVIDIA, AMD, Qualcomm, Cerebras, and Apple hardware.</p><p>We build private, edge, and on-premise AI solutions for enterprises of any size.</p><p>We are scaling LFMs and expect to introduce new and better capabilities across various industries, such as financial services, biotechnology, and consumer electronics.</p></div><a href="https://playground.liquid.ai/" target="_blank"><p>Try Liquid</p></a><p>At Liquid AI, we build new methods for designing powerful AI systems over which we have significant control. We design them the same way engineers built engines, cars, and airplanes:  from first principles. Our mission is to create best-in-class, intelligent, and efficient systems at every scale – systems designed to process large amounts of sequential multimodal data, to enable advanced reasoning, and to achieve reliable decision-making.</p></div><div id="introducing-the-first-generation-of-language-LFMs"><h2>Introducing the First Generation of Language LFMs</h2><p>We are proud to release our first series of language models:</p><div><p>A dense <strong>1.3B model</strong>, ideal for highly resource-constrained environments.</p><p>A dense <strong>3.1B model</strong>, optimized for edge deployment.</p><p>A <strong>40.3B Mixture of Experts (MoE) model</strong>, designed for tackling more complex tasks.</p></div><p>Architecture work cannot happen in a vacuum – our goal is to develop useful models that are competitive with the current best-in-class LLMs. In doing so, we hope to show that model performance isn’t just about scale – it’s also about innovation.</p><h3>State-of-the-Art Performance</h3><div><p>We report the results of our fine-tuned LFMs and compare them with similar-sized language models using Eleuther AI’s lm-evaluation-harness v0.4. Unless specified otherwise, we compare to other fine-tuned models.</p><p><strong>LFM-1B</strong> achieves the highest scores across various benchmarks in the 1B category, making it the new state-of-the-art model at this size. This is the first time a non-GPT architecture significantly outperforms transformer-based models.</p><div><div id="w-node-_97d9569a-40ce-c3ad-84c4-675d45429df8-18e46dab"><div id="w-node-d5abe0b5-a930-2283-c7fb-4f8b95e238f5-18e46dab"><p>Stable LM 2</p><p>(Stability)</p><p>1.6B</p></div><div id="w-node-_44879ce4-d147-caeb-102b-0af001f662a5-18e46dab"><p>Smol LM</p><p>(Hugging Face)</p><p>1.7B</p></div><div id="w-node-_40cf2a32-93de-6a91-aba4-d9f8f6f04145-18e46dab"><p>R Gemma 2</p><p>(Google)</p><p>Base 2.7B</p></div></div></div></div><div><p><strong>LFM-3B</strong> delivers incredible performance for its size. It positions itself as first place among 3B parameter transformers, hybrids, and RNN models, but also outperforms the previous generation of 7B and 13B models. It is also on par with Phi-3.5-mini on multiple benchmarks, while being 18.4% smaller. LFM-3B is the ideal choice for mobile and other edge text-based applications.</p><div><div id="w-node-d97be800-342f-17e8-df37-a6c1da029509-18e46dab"><div id="w-node-d97be800-342f-17e8-df37-a6c1da02952b-18e46dab"><p>Mistral-7b v0.3</p><p>(Mistral AI)</p><p>7B</p></div><div id="w-node-d97be800-342f-17e8-df37-a6c1da029535-18e46dab"><p>Mistral Nemo</p><p>(Mistral AI)</p><p>12.2B</p></div></div></div><p>*Scores reported by the developers. All the other scores were calculated with the same evaluation harness we used for our own models.</p></div><div><p><strong>LFM-40B</strong> offers a new balance between model size and output quality. It leverages 12B activated parameters at use. Its performance is comparable to models larger than itself, while its MoE architecture enables higher throughput and deployment on more cost-effective hardware.</p><p>*Scores reported by the developers. All the other scores were calculated with the same evaluation harness we used for our own models.</p></div><h3>LFMs are Memory-Efficient</h3><p>LFMs have a reduced memory footprint compared to transformer architectures. This is particularly true for long inputs, where the KV cache in transformer-based LLMs grows linearly with sequence length. By efficiently compressing inputs, LFMs can process longer sequences on the same hardware. For example, compared to other 3B-class models, LFMs maintain a minimal memory footprint.</p><p><img src="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a9b9624c365c96251a0c_desktop%20graph.png" loading="lazy" width="854" sizes="(max-width: 991px) 100vw, 900px" alt="Fig. 2. Total inference memory footprint of different language models vs. the input+generation length." srcset="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a9b9624c365c96251a0c_desktop%20graph-p-500.png 500w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a9b9624c365c96251a0c_desktop%20graph-p-800.png 800w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a9b9624c365c96251a0c_desktop%20graph-p-1080.png 1080w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a9b9624c365c96251a0c_desktop%20graph-p-1600.png 1600w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a9b9624c365c96251a0c_desktop%20graph.png 1788w"/><img src="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a633c50a28b58fc3d8b6_figure-wrapper.png" loading="lazy" width="632" sizes="(max-width: 767px) 100vw, (max-width: 991px) 900px, 100vw" alt="Fig. 2. Total inference memory footprint of different language models vs. the input+generation length." srcset="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a633c50a28b58fc3d8b6_figure-wrapper-p-500.png 500w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a633c50a28b58fc3d8b6_figure-wrapper-p-800.png 800w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a633c50a28b58fc3d8b6_figure-wrapper-p-1080.png 1080w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66f9a633c50a28b58fc3d8b6_figure-wrapper.png 1328w"/></p><p><strong>Fig. 2. </strong>Total inference memory footprint of different language models vs. the input+generation length.</p><h3>LFMs Truly Exploit their Context Length</h3><p>In this preview release, we have optimized our models to deliver a best-in-class 32k token context length, pushing the boundaries of efficiency for our size. This was confirmed by the RULER benchmark, where a length is considered “effective” when its corresponding score is higher than 85.6 [<a href="https://arxiv.org/abs/2404.06654" target="_blank">Hsieh et al. 2024 - RULER</a>]. The following table compares several models at different context lengths.</p><div><p>This highly efficient context window enables long-context tasks on edge devices for the first time. For developers, it unlocks new applications, including document analysis and summarization, more meaningful interactions with context-aware chatbots, and improved Retrieval-Augmented Generation (RAG) performance.</p><p>Our goal is to keep scaling LFMs across model size, train/test time compute, and context length. Beyond our language LFMs, we have designed models for various data modalities, domains, and applications that we plan to release in the next months.</p></div><h3>Advancing the Pareto Frontier of Large AI Models</h3><p>To achieve these results, we optimized our pre- and post-training pipelines and infrastructure to ensure our models excel across five criteria:</p><div><div data-hover="false" data-delay="0"><nav><p>Breadth and depth of information across various domains and tasks at any given size. We achieve this using a comprehensive pre-training set, advances in model architectures, new pre-training/mid-training/post-training strategies. This allows LFMs to be competitive with larger models on knowledge-based tasks.</p></nav></div><div data-hover="false" data-delay="0"><nav><p>The ability to break down a problem and apply logical and rigorous thinking. We distilled system 2 tasks during the core phases of training, enabling robust analytical capabilities in compact model architectures.</p></nav></div><div data-hover="false" data-delay="0"><nav><p>A model&#39;s maximum input size is not the same as its effective context length. We specifically trained LFMs to maximize recall performance and in-context learning capabilities across the entire input range.</p></nav></div><div data-hover="false" data-delay="0"><nav><p>Memory usage of transformer-based models explodes for long inputs, which makes them ill-suited for edge deployment. LFMs have near-constant inference time and memory complexity – as the input context length grows, it does not significantly affect generation speed or increase the amount of memory required.</p></nav></div><div data-hover="false" data-delay="0"><nav><p>Training GPT-like foundation models demands significant computational resources. LFMs are efficient for training on long-context data.</p></nav></div></div></div><div id="reimagining-model-architectures"><h2>Reimagining Model Architectures</h2><div><p>Building on <a href="https://www.liquid.ai/blog/liquid-neural-networks-research" target="_blank">a long line of research</a> in designing expressive and efficient learning systems, we have developed a new design space for foundation models, focusing on different modalities and hardware requirements. Our goal is to explore ways to build foundation models beyond Generative Pre-trained Transformers (GPTs).</p><p>With LFMs, we put into practice new principles and methods guiding model design, developed by our team over the past months.</p></div><div><div data-hover="false" data-delay="0"><div><p>LFMs are composed of structured operators.</p></div><nav><p>Our models are derived from a set of computational units – the building blocks of an architecture – belonging to a new <em>design space</em>. Liquid systems and their composition maximize knowledge capacity and reasoning, while unlocking improved training efficiency, reduced memory cost at inference time, and increased performance in modeling data such as video, audio, text, time series, and signals.</p></nav></div><div data-hover="false" data-delay="0"><div><p>LFM architectures are under control.</p></div><nav><p>The design of our models reciprocally informs our scaling, inference, alignment, and model analysis strategy. We can analyze the dynamics of LFMs via classical signal processing analysis methods and probe their behavior, from model outputs to model internals.</p></nav></div><div data-hover="false" data-delay="0"><div><p>LFMs are adaptive and can serve as the substrate for AI at every scale.</p></div><nav><p>We can <strong>automatically</strong> optimize architectures for a specific platform (e.g., Apple, Qualcomm, Cerebras, and AMD) or match given parameter requirements and inference cache size.</p></nav></div></div><p><strong>Fig. 3. </strong>Our architectures feature custom computational units arranged in <em>depth groups</em> (targeted weight sharing), with additional <em>featurizer interconnections </em>(feature sharing).</p><p>Liquid’s design space is primarily defined by featurization and footprint of architectures and their core operators. Featurization refers to the process of converting input data (e.g., text, audio, images, video) into a structured set of features or vectors that are used to modulate computation inside the model in an adaptive manner. For example, audio and time series data generally requires <em>less</em> featurization in operators due to lower information density, compared to language and multi-modal data. The other key dimension is the computational complexity of the operators. Being able to traverse and complete the design space of structured adaptive operators allows us maximize performance with controlled computational requirements.</p><p><img src="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb44838e25f89817c56644_figure-wrapper%20(2).avif" loading="lazy" width="894" sizes="(max-width: 991px) 100vw, 900px" alt="Fig. 5. We built the foundations of a new design space for computational units, enabling customization to different modalities and hardware requirements." srcset="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb44838e25f89817c56644_figure-wrapper%20(2)-p-500.avif 500w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb44838e25f89817c56644_figure-wrapper%20(2)-p-800.avif 800w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb44838e25f89817c56644_figure-wrapper%20(2)-p-1080.avif 1080w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb44838e25f89817c56644_figure-wrapper%20(2).avif 1788w"/><img src="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb4498a147f24853fcb5fb_figure-wrapper.avif" loading="lazy" width="664" sizes="(max-width: 767px) 100vw, (max-width: 991px) 900px, 100vw" alt="Fig. 5. We built the foundations of a new design space for computational units, enabling customization to different modalities and hardware requirements." srcset="https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb4498a147f24853fcb5fb_figure-wrapper-p-500.avif 500w, https://cdn.prod.website-files.com/6557a2b6957fcb7aeb0efcf0/66eb4498a147f24853fcb5fb_figure-wrapper.avif 1328w"/></p><p><strong>Fig. 4. </strong>We built the foundations of a new design space for computational units, enabling customization to different modalities and hardware requirements.</p><p>At their core, LFMs are built with computational units that can be expressed as adaptive linear operators whose actions are determined by inputs. The LFM design framework unifies and subsumes a wide range of existing computational units in deep learning, providing a systematic approach to exploring the space of architectures. Specifically, our analysis informs model building by improving three key aspects: token-mixing structure (how the operator mixes embeddings in the input sequence), channel-mixing structure (how it mixes channel dimensions), and featurization, responsible for modulating computation based on the input context.</p></div><div id="join-us-as-an-early-adopter-of-LFMs"><h2>Join us as an early adopter of LFMs</h2><div><p>As we are still in the early stages of this journey, we welcome the opportunity to collaborate and discover the strengths and weaknesses of these systems together.</p><p>What are Language LFMs good at today:</p><ul role="list"><li>General and expert knowledge</li><li>Mathematics and logical reasoning</li><li>Efficient and effective long-context tasks</li><li>Their primary language is English, with secondary multilingual capabilities in Spanish, French, German, Chinese, Arabic, Japanese, and Korean</li></ul><p>What are Language LFMs not good at today:</p><ul role="list"><li>Zero-shot code tasks</li><li>Precise numerical calculations</li><li>Time-sensitive information</li><li>Counting r&#39;s in the word &#34;Strawberry&#34;!</li><li>Human preference optimization techniques have not been applied extensively to our models yet.</li></ul><p>At Liquid AI, we take an open-science approach. We have and will continue to contribute to the advancement of the AI field by openly publishing our findings and methods through scientific and technical reports. As part of this commitment, we will release relevant data and models produced by our research efforts to the wider AI community. We have dedicated a lot of time and resources to developing these architectures, so we&#39;re not open-sourcing our models at the moment. This allows us to continue building on our progress and maintain our edge in the competitive AI landscape.</p><p>If your enterprise is looking to experience the forefront of AI, we invite you to <a href="https://www.liquid.ai/talk-to-our-team">get in touch with us</a>. If this aligns with your personal goals and ambitions, we invite you to <a href="https://jobs.lever.co/liquid.ai/" target="_blank">join our team</a> and drive this vision forward. We are very early on this journey and actively innovating across various aspects of foundation model development and deployment. We invite enthusiastic users to share their experience as well as criticism, and join our red-teaming efforts to improve the capabilities of our models.</p></div><p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdPNEPTajcbz_iEE3nWhI_xheLxo7qpApSBs_ohwxU9wTHLXg/viewform" target="_blank">Share your feedback</a></p></div><div><h2>Liquid Product Launch Event </h2><h2>October 23, 2024  |  Cambridge, MA </h2><p>Come join us at MIT Kresge, Cambridge, MA on October 23rd 2024, to learn more about Liquid as we unveil more products and progress on LFMs and their applications in consumer electronics, finance, healthcare, biotechnology, and more! </p><p><a href="https://lu.ma/liquid-ai-launch" target="_blank">RSVP Here</a></p></div></div></div></div>
  </body>
</html>
