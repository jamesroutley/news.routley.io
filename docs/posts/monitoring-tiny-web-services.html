<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jvns.ca/blog/2022/07/09/monitoring-small-web-services/">Original</a>
    <h1>Monitoring tiny web services</h1>
    
    

<p>Hello! I&rsquo;ve started to run a few more servers recently
(<a href="https://nginx-playground.wizardzines.com">nginx playground</a>,
<a href="https://messwithdns.net">mess with dns</a>,
<a href="https://dns-lookup.jvns.ca">dns lookup</a>), so I&rsquo;ve been
thinking about monitoring.</p>

<p>It wasn&rsquo;t initially totally obvious to me how to monitor these websites, so I
wanted to quickly write up what how I did it.</p>

<p>I&rsquo;m not going to talk about how to monitor Big Serious Mission Critical
websites at all, only tiny unimportant websites.</p>

<h3 id="goal-spend-approximately-0-time-on-operations">goal: spend approximately 0 time on operations</h3>

<p>I want the sites to mostly work, but I also want to spend approximately 0% of
my time on the ongoing operations.</p>

<p>I was initially very wary of running servers at all because at my last job I
was on a <sup>24</sup>&frasl;<sub>7</sub> oncall rotation for some critical services, and in my mind &ldquo;being
responsible for servers&rdquo; meant &ldquo;get woken up at 2am to fix the servers&rdquo; and
&ldquo;have lots of complicated dashboards&rdquo;.</p>

<p>So for a while I only made static websites so that I wouldn&rsquo;t have to think
about servers.</p>

<p>But eventually I realized that any server I was going to write was going to be
very low stakes, if they occasionally go down for 2 hours it&rsquo;s no big deal, and
I could just set up some very simple monitoring to help keep them running.</p>

<h3 id="not-having-monitoring-sucks">not having monitoring sucks</h3>

<p>At first I didn&rsquo;t set up any monitoring for my servers at all. This had the
extremely predictable outcome of &ndash; sometimes the site broke, and I didn&rsquo;t find
out about it until somebody told me!</p>

<h3 id="step-1-an-uptime-checker">step 1: an uptime checker</h3>

<p>The first step was to set up an uptime checker. There are tons of these out
there, the ones I&rsquo;m using right now are <a href="https://updown.io/">updown.io</a> and
<a href="https://uptimerobot.com/">uptime robot</a>. I like updown&rsquo;s user interface more,
but uptime robot has a more generous free tier.</p>

<p>These</p>

<ol>
<li>check that the site is up</li>
<li>if it goes down, it emails me</li>
</ol>

<p>I find that email notifications are a good level for me, I&rsquo;ll find out pretty
quickly if the site goes down but it doesn&rsquo;t wake me up or anything.</p>

<h3 id="step-2-an-end-to-end-healthcheck">step 2: an end-to-end healthcheck</h3>

<p>Next, let&rsquo;s talk about what &ldquo;check that the site is up&rdquo; actually means.</p>

<p>At first I just made one of my healthcheck endpoints a function that returned
<code>200 OK</code> no matter what.</p>

<p>This is kind of useful &ndash; it told me that the server was on!</p>

<p>But unsurprisingly I ran into problems because it wasn&rsquo;t checking that the API
was actually <em>working</em> &ndash; sometimes the healthcheck succeeded even though the
rest of the service had actually gotten into a bad state.</p>

<p>So I updated it to actually make a real API request and make sure it
succeeded.</p>

<p>All of my services do very few things (the nginx playground has just 1
endpoint), so it&rsquo;s pretty easy to set up a healthcheck that actually runs
through most of the actions the service is supposed to do.</p>

<p>Here&rsquo;s what the end-to-end healthcheck handler for the nginx playground looks
like. It&rsquo;s very basic: it just makes another POST request (to itself) and
checks if that request succeeds or fails.</p>

<pre><code>func healthHandler(w http.ResponseWriter, r *http.Request) {
	// make a request to localhost:8080 with `healthcheckJSON` as the body
	// if it works, return 200
	// if it doesn't, return 500
	client := http.Client{}
	resp, err := client.Post(&quot;http://localhost:8080/&quot;, &quot;application/json&quot;, strings.NewReader(healthcheckJSON))
	if err != nil {
		log.Println(err)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	if resp.StatusCode != http.StatusOK {
		log.Println(resp.StatusCode)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	w.WriteHeader(http.StatusOK)
}
</code></pre>

<h3 id="healthcheck-frequency-hourly">healthcheck frequency: hourly</h3>

<p>Right now I&rsquo;m running most of my healthchecks every hour, and some every 30
minutes.</p>

<p>I run them hourly because updown.io&rsquo;s pricing is per healthcheck, I&rsquo;m
monitoring 18 different URLs, and I wanted to keep my healthcheck budget pretty
minimal at $5/year.</p>

<p>Taking an hour to find out that one of these websites has gone down seems ok to
me &ndash; if there is a problem there&rsquo;s no guarantee I&rsquo;ll get to fixing it all that
quickly anyway.</p>

<p>If it were free to run them more often I&rsquo;d probably run them every 5-10 minutes instead.</p>

<h3 id="step-3-automatically-restart-if-the-healthcheck-fails">step 3: automatically restart if the healthcheck fails</h3>

<p>Some of my websites are on fly.io, and fly has a pretty standard feature where
I can configure a HTTP healthcheck for a service and restart the service if the
healthcheck starts failing.</p>

<p>&ldquo;Restart a lot&rdquo; is a very useful strategy to paper over bugs that I haven&rsquo;t
gotten around to fixing yet &ndash; for a while the nginx playground had a process
leak where <code>nginx</code> processes weren&rsquo;t getting terminated, so the server kept
running out of RAM.</p>

<p>With the healthcheck, the result of this was that every day or so, this would happen:</p>

<ul>
<li>the server ran out of RAM</li>
<li>the healthcheck started failing</li>
<li>it get restarted</li>
<li>everything was fine again</li>
<li>repeat the whole saga again some number of hours later</li>
</ul>

<p>Eventually I got around to actually fixing the process leak, but it was nice to
have a workaround in place that could keep things running while I was
procrastinating fixing the bug.</p>

<p>These healthchecks to decide whether to restart the service run more often: every 5 minutes or so.</p>

<h3 id="this-is-not-the-best-way-to-monitor-big-services">this is not the best way to monitor Big Services</h3>

<p>This is probably obvious and I said this already at the beginning, but &ldquo;write
one HTTP healthcheck&rdquo; is not the best approach for monitoring a large complex
service. But I won&rsquo;t go into that because that&rsquo;s not what this post is about.</p>

<h3 id="it-s-been-working-well-so-far">it&rsquo;s been working well so far!</h3>

<p>I originally wrote this post 3 months ago in April, but I waited until now to
publish it to make sure that the whole setup was working.</p>

<p>It&rsquo;s made a pretty big difference &ndash; before I was having some very silly
downtime problems, and now for the last few months the sites have been up
99.95% of the time!</p>

  </body>
</html>
