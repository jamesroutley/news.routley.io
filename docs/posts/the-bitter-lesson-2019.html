<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">Original</a>
    <h1>The Bitter Lesson (2019)</h1>
    
    <div id="readability-page-1" class="page">
<span>

<h2>Rich Sutton</h2>
<h3>March 13, 2019<br/>
</h3>
The biggest lesson that can be read from 70 years of AI research is
that general methods that leverage computation are ultimately the most
effective, and by a large margin. The ultimate reason for this is
Moore&#39;s law, or rather its generalization of continued exponentially
falling cost per unit of computation. Most AI research has been
conducted as if the computation available to the agent were constant
(in which case leveraging human knowledge would be one of the only ways
to improve performance) but, over a slightly longer time than a typical
research project, massively more computation inevitably becomes
available. Seeking an improvement that makes a difference in the
shorter term, researchers seek to leverage their human knowledge of the
domain, but the only thing that matters in the long run is the
leveraging of computation. These two need not run counter to each
other, but in practice they tend to. Time spent on one is time not
spent on the other. There are psychological commitments to investment
in one approach or the other. And the human-knowledge approach tends to
complicate methods in ways that make them less suited to taking
advantage of general methods leveraging computation.Â  There were
many examples of AI researchers&#39; belated learning of this bitter
lesson,
and it is instructive to review some of the most prominent.</span>


</div>
  </body>
</html>
