<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.swtch.com/ub">Original</a>
    <h1>C and C&#43;&#43; prioritize performance over correctness (2023)</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        
       

<p>
The original ANSI C standard, C89, introduced the concept of “undefined behavior,”
which was used both to describe the effect of outright bugs like
accessing memory in a freed object
and also to capture the fact that existing implementations differed about
handling certain aspects of the language,
including use of uninitialized values,
signed integer overflow, and null pointer handling.

</p><p>
The C89 spec defined undefined behavior (in section 1.6) as:</p><blockquote>

<p>
Undefined behavior—behavior, upon use of a nonportable or
erroneous program construct, of erroneous data, or of
indeterminately-valued objects, for which the Standard imposes no
requirements.  Permissible undefined behavior ranges from ignoring the
situation completely with unpredictable results, to behaving during
translation or program execution in a documented manner characteristic
of the environment (with or without the issuance of a diagnostic
message), to terminating a translation or execution (with the issuance
of a diagnostic message).</p></blockquote>

<p>
Lumping both non-portable and buggy code into the same category was a mistake.
As time has gone on, the way compilers treat undefined behavior
has led to more and more unexpectedly broken programs,
to the point where it is becoming difficult to tell whether any program
will compile to the meaning in the original source.
This post looks at a few examples and then tries to make some general observations.
In particular, today’s C and C++ prioritize
performance to the clear detriment of correctness.
<a href="#uninit"></a></p><h2 id="uninit"><a href="#uninit">Uninitialized variables</a></h2>


<p>
C and C++ do not require variables to be initialized
on declaration (explicitly or implicitly) like Go and Java.
Reading from an uninitialized variable is undefined behavior.

</p><p>
In a <a href="http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">blog post</a>,
Chris Lattner (creator of LLVM and Clang) explains the rationale:</p><blockquote>

<p>
<b>Use of an uninitialized variable</b>:
This is commonly known as source of problems in C programs
and there are many tools to catch these:
from compiler warnings to static and dynamic analyzers.
This improves performance by not requiring that all variables
be zero initialized when they come into scope (as Java does).
For most scalar variables, this would cause little overhead,
but stack arrays and malloc’d memory would incur
a memset of the storage, which could be quite costly,
particularly since the storage is usually completely overwritten.</p></blockquote>

<p>
Early C compilers were too crude to detect
use of uninitialized basic variables like integers and pointers,
but modern compilers are dramatically more sophisticated.
They could absolutely react in these cases by
“terminating a translation or execution (with the issuance
of a diagnostic message),”
which is to say reporting a compile error.
Or, if they were worried about not rejecting old programs,
they could insert a zero initialization with, as Lattner admits, little overhead.
But they don’t do either of these.
Instead, they just do whatever they feel like during code generation.

</p><p>
For example, here’s a simple C++ program with an uninitialized variable (a bug):
</p><pre>#include &lt;stdio.h&gt;

int main() {
    for(int i; i &lt; 10; i++) {
        printf(&#34;%d\n&#34;, i);
    }
    return 0;
}
</pre>


<p>
If you compile this with <code>clang++</code> <code>-O1</code>, it deletes the loop entirely:
<code>main</code> contains only the <code>return</code> <code>0</code>.
In effect, Clang has noticed the uninitialized variable and chosen
not to report the error to the user but instead
to pretend <code>i</code> is always initialized above 10, making the loop disappear.

</p><p>
It is true that if you compile with <code>-Wall</code>, then Clang does report the
use of the uninitialized variable as a warning.
This is why you should always build with and fix warnings in C and C++ programs.
But not all compiler-optimized undefined behaviors
are reliably reported as warnings.
<a href="#overflow"></a></p><h2 id="overflow"><a href="#overflow">Arithmetic overflow</a></h2>


<p>
At the time C89 was standardized, there were still legacy
<a href="https://en.wikipedia.org/wiki/Ones%27_complement">ones’-complement computers</a>,
so ANSI C could not assume the now-standard two’s-complement representation
for negative numbers.
In two’s complement, an <code>int8</code> −1 is 0b11111111;
in ones’ complement that’s −0, while −1 is 0b11111110.
This meant that operations like signed integer overflow could not be defined,
because</p><blockquote>

<p>
<code>int8</code> 127+1 = 0b01111111+1 = 0b10000000</p></blockquote>

<p>
is −127 in ones’ complement but −128 in two’s complement.
That is, signed integer overflow was non-portable.
Declaring it undefined behavior let compilers escalate the behavior
from “non-portable”, with one of two clear meanings,
to whatever they feel like doing.
For example, a common thing programmers expect is that you can test
for signed integer overflow by checking whether the result is
less than one of the operands, as in this program:
</p><pre>#include &lt;stdio.h&gt;

int f(int x) {
    if(x+100 &lt; x)
        printf(&#34;overflow\n&#34;);
    return x+100;
}
</pre>


<p>
Clang optimizes away the <code>if</code> statement.
The justification is that since signed integer overflow is undefined behavior,
the compiler can assume it never happens, so <code>x+100</code> must never be less than <code>x</code>.
Ironically, this program would correctly detect overflow
on both ones’-complement and two’s-complement machines
if the compiler would actually emit the check.

</p><p>
In this case, <code>clang++</code> <code>-O1</code> <code>-Wall</code> prints no warning while it deletes the <code>if</code> statement,
and neither does <code>g++</code>,
although I seem to remember it used to, perhaps in subtly different situations
or with different flags.

</p><p>
For C++20, the <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0907r0.html">first version of proposal P0907</a>
suggested standardizing that signed integer overflow
wraps in two’s complement. The original draft gave a very clear statement of the history
of the undefined behavior and the motivation for making a change:</p><blockquote>

<p>
[C11] Integer types allows three representations for signed integral types:
</p><ul>
<li>
Signed magnitude
</li><li>
Ones’ complement
</li><li>
Two’s complement</li></ul>


<p>
See §4 C Signed Integer Wording for full wording.

</p><p>
C++ inherits these three signed integer representations from C. To the author’s knowledge no modern machine uses both C++ and a signed integer representation other than two’s complement (see §5 Survey of Signed Integer Representations). None of [MSVC], [GCC], and [LLVM] support other representations. This means that the C++ that is taught is effectively two’s complement, and the C++ that is written is two’s complement. It is extremely unlikely that there exist any significant code base developed for two’s complement machines that would actually work when run on a non-two’s complement machine.

</p><p>
The C++ that is spec’d, however, is not two’s complement. Signed integers currently allow for trap representations, extra padding bits, integral negative zero, and introduce undefined behavior and implementation-defined behavior for the sake of this extremely abstract machine.

</p><p>
Specifically, the current wording has the following effects:
</p><ul>
<li>
Associativity and commutativity of integers is needlessly obtuse.
</li><li>
Naïve overflow checks, which are often security-critical, often get eliminated by compilers. This leads to exploitable code when the intent was clearly not to and the code, while naïve, was correctly performing security checks for two’s complement integers. Correct overflow checks are difficult to write and equally difficult to read, exponentially so in generic code.
</li><li>
Conversion between signed and unsigned are implementation-defined.
</li><li>
There is no portable way to generate an arithmetic right-shift, or to sign-extend an integer, which every modern CPU supports.
</li><li>
constexpr is further restrained by this extraneous undefined behavior.
</li><li>
Atomic integral are already two’s complement and have no undefined results, therefore even freestanding implementations already support two’s complement in C++.</li></ul>


<p>
Let’s stop pretending that the C++ abstract machine should represent integers as signed magnitude or ones’ complement. These theoretical implementations are a different programming language, not our real-world C++. Users of C++ who require signed magnitude or ones’ complement integers would be better served by a pure-library solution, and so would the rest of us.</p></blockquote>

<p>
In the end, the C++ standards committee put up “strong resistance against” the idea of defining
signed integer overflow the way every programmer expects; the undefined behavior remains.
<a href="#loops"></a></p><h2 id="loops"><a href="#loops">Infinite loops</a></h2>


<p>
A programmer would never accidentally cause a program to execute an infinite loop, would they?
Consider this program:
</p><pre>#include &lt;stdio.h&gt;

int stop = 1;

void maybeStop() {
    if(stop)
        for(;;);
}

int main() {
    printf(&#34;hello, &#34;);
    maybeStop();
    printf(&#34;world\n&#34;);
}
</pre>


<p>
This seems like a completely reasonable program to write. Perhaps you are debugging and want the program to stop so you can attach a debugger. Changing the initializer for <code>stop</code> to <code>0</code> lets the program run to completion.
But it turns out that, at least with the latest Clang, the program runs to completion anyway:
the call to <code>maybeStop</code> is optimized away entirely, even when <code>stop</code> is <code>1</code>.

</p><p>
The problem is that C++ defines that every side-effect-free loop may be assumed by the compiler to terminate.
That is, a loop that does not terminate is therefore undefined behavior.
This is purely for compiler optimizations, once again treated as more important than correctness.
The rationale for this decision played out in the C standard and was more or less adopted in the C++ standard as well.

</p><p>
John Regehr pointed out this problem in his post
“<a href="https://blog.regehr.org/archives/140">C Compilers Disprove Fermat’s Last Theorem</a>,”
which included this entry in a FAQ:</p><blockquote>

<p>
Q: Does the C standard permit/forbid the compiler to terminate infinite loops?

</p><p>
A: The compiler is given considerable freedom in how it implements the C program,
but its output must have the same externally visible behavior that the program would have when interpreted by the “C abstract machine” that is described in the standard.  Many knowledgeable people (including me) read this as saying that the termination behavior of a program must not be changed.  Obviously some compiler writers disagree, or else don’t believe that it matters.  The fact that reasonable people disagree on the interpretation would seem to indicate that the C standard is flawed.</p></blockquote>

<p>
A few months later, Douglas Walls wrote <a href="http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1509.pdf">WG14/N1509: Optimizing away infinite loops</a>,
making the case that the standard should <i>not</i> allow this optimization.
In response, Hans-J. Boehm wrote
<a href="http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1528.htm">WG14/N1528: Why undefined behavior for infinite loops?</a>,
arguing for allowing the optimization.

</p><p>
Consider the potential optimization of this code:
</p><pre>for (p = q; p != 0; p = p-&gt;next)
    ++count;
for (p = q; p != 0; p = p-&gt;next)
    ++count2;
</pre>


<p>
A sufficiently smart compiler might reduce it to this code:
</p><pre>for (p = q; p != 0; p = p-&gt;next) {
        ++count;
        ++count2;
}
</pre>


<p>
Is that safe? Not if the first loop is an infinite loop. If the list at <code>p</code> is cyclic and another thread is modifying <code>count2</code>,
then the first program has no race, while the second program does.
Compilers clearly can’t turn correct, race-free programs into racy programs.
But what if we declare that infinite loops are not correct programs?
That is, what if infinite loops were undefined behavior?
Then the compiler could optimize to its robotic heart’s content.
This is exactly what the C standards committee decided to do.

</p><p>
The rationale, paraphrased, was:
</p><ul>
<li>
It is very difficult to tell if a given loop is infinite.
</li><li>
Infinite loops are rare and typically unintentional.
</li><li>
There are many loop optimizations that are only valid for non-infinite loops.
</li><li>
The performance wins of these optimizations are deemed important.
</li><li>
Some compilers already apply these optimizations, making infinite loops non-portable too.
</li><li>
Therefore, we should declare programs with infinite loops undefined behavior, enabling the optimizations.</li></ul>
<a href="#null"><h2 id="null">Null pointer usage</h2></a>


<p>
We’ve all seen how dereferencing a null pointer causes a crash on modern operating systems:
they leave page zero unmapped by default precisely for this purpose.
But not all systems where C and C++ run have hardware memory protection.
For example, I wrote my first C and C++ programs using Turbo C on an MS-DOS system.
Reading or writing a null pointer did not cause any kind of fault:
the program just touched the memory at location zero and kept running.
The correctness of my code improved dramatically when I moved to
a Unix system that made those programs crash at the moment of the mistake.
Because the behavior is non-portable, though, dereferencing a null pointer is undefined behavior.

</p><p>
At some point, the justification for keeping the undefined behavior became performance.
<a href="http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">Chris Lattner explains</a>:</p><blockquote>

<p>
In C-based languages, NULL being undefined enables a large number of simple scalar optimizations that are exposed as a result of macro expansion and inlining.</p></blockquote>

<p>
In <a href="https://research.swtch.com/plmm#ub">an earlier post</a>, I showed this example, lifted from <a href="https://twitter.com/andywingo/status/903577501745770496">Twitter in 2017</a>:
</p><pre>#include &lt;cstdlib&gt;

typedef int (*Function)();

static Function Do;

static int EraseAll() {
    return system(&#34;rm -rf slash&#34;);
}

void NeverCalled() {
    Do = EraseAll;
}

int main() {
    return Do();
}
</pre>


<p>
Because calling <code>Do()</code> is undefined behavior when <code>Do</code> is null, a modern C++ compiler like Clang
simply assumes that can’t possibly be what’s happening in <code>main</code>.
Since <code>Do</code> must be either null or <code>EraseAll</code> and since null is undefined behavior,
we might as well assume <code>Do</code> is <code>EraseAll</code> unconditionally,
even though <code>NeverCalled</code> is never called.
So this program can be (and is) optimized to:
</p><pre>int main() {
    return system(&#34;rm -rf slash&#34;);
}
</pre>


<p>
Lattner gives <a href="https://blog.llvm.org/2011/05/what-every-c-programmer-should-know_14.html">an equivalent example</a> (search for <code>FP()</code>)
and then this advice:</p><blockquote>

<p>
The upshot is that it is a fixable issue: if you suspect something weird is going on like this, try building at -O0, where the compiler is much less likely to be doing any optimizations at all.</p></blockquote>

<p>
This advice is not uncommon: if you cannot debug the correctness problems in your C++ program, disable optimizations.
<a href="#sort"></a></p><h2 id="sort"><a href="#sort">Crashes out of sorts</a></h2>


<p>
C++’s <code>std::sort</code> sorts a collection of values
(abstracted as a random access iterator, but almost always an array)
according to a user-specified comparison function.
The default function is <code>operator&lt;</code>, but you can write any function.
For example if you were sorting instances of class <code>Person</code> your
comparison function might sort by the <code>LastName</code> field, breaking
ties with the <code>FirstName</code> field.
These comparison functions end up being subtle yet boring to write,
and it’s easy to make a mistake.
If you do make a mistake and pass in a comparison function that
returns inconsistent results or accidentally reports that any value
is less than itself, that’s undefined behavior:
<code>std::sort</code> is now allowed to do whatever it likes,
including walking off either end of the array
and corrupting other memory.
If you’re lucky, it will pass some of this memory to your comparison
function, and since it won’t have pointers in the right places,
your comparison function will crash.
Then at least you have a chance of guessing the comparison function is at fault.
In the worst case, memory is silently corrupted and the crash happens much later,
with <code>std::sort</code> is nowhere to be found.

</p><p>
Programmers make mistakes, and when they do, <code>std::sort</code> corupts memory.
This is not hypothetical. It happens enough in practice to be a
<a href="https://stackoverflow.com/questions/18291620/why-will-stdsort-crash-if-the-comparison-function-is-not-as-operator">popular question on StackOverflow</a>.

</p><p>
As a final note, it turns out that <code>operator&lt;</code> is not a valid comparison function
on floating-point numbers if NaNs are involved, because:
</p><ul>
<li>
1 &lt; NaN and NaN &lt; 1 are both false, implying NaN == 1.
</li><li>
2 &lt; NaN and NaN &lt; 2 are both false, implying NaN == 2.
</li><li>
Since NaN == 1 and NaN == 2, 1 == 2, yet 1 &lt; 2 is true.</li></ul>


<p>
Programming with NaNs is never pleasant, but it seems particularly extreme
to allow <code>std::sort</code> to crash when handed one.
<a href="#reveal"></a></p><h2 id="reveal"><a href="#reveal">Reflections and revealed preferences</a></h2>


<p>
Looking over these examples,
it could not be more obvious that in modern C and C++,
performance is job one and correctness is job two.
To a C/C++ compiler, a programmer making a mistake and (gasp!)
compiling a program containing a bug is just not a concern.
Rather than have the compiler point out the bug or at least
compile the code in a clear, understandable, debuggable manner,
the approach over and over again is
to let the compiler do whatever it likes,
in the name of performance.

</p><p>
This may not be the wrong decision for these languages.
There are undeniably power users for whom every last bit of performance
translates to very large sums of money, and I don’t claim
to know how to satisfy them otherwise.
On the other hand, this performance comes at a significant
development cost, and there are probably plenty of people and companies
who spend more than their performance savings
on unnecessarily difficult debugging sessions
and additional testing and sanitizing.
It also seems like there must be a middle ground where
programmers retain most of the control they have in C and C++
but the program doesn’t crash when sorting NaNs or
behave arbitrarily badly if you accidentally dereference a null pointer.
Whatever the merits, it is important to see clearly the choice that C and C++ are making.

</p><p>
In the case of arithmetic overflow, later drafts of the
proposal removed the defined behavior for wrapping, explaining:</p><blockquote>

<p>
The main change between [P0907r0] and the subsequent revision is to maintain undefined behavior when signed integer overflow occurs, instead of defining wrapping behavior. This direction was motivated by:
</p><ul>
<li>
Performance concerns, whereby defining the behavior prevents optimizers from assuming that overflow never occurs;
</li><li>
Implementation leeway for tools such as sanitizers;
</li><li>
Data from Google suggesting that over 90% of all overflow is a bug, and defining wrapping behavior would not have solved the bug.</li></ul>
</blockquote>

<p>
Again, performance concerns rank first.
I find the third item in the list particularly telling.
I’ve known C/C++ compiler authors who got excited about a 0.1% performance improvement,
and incredibly excited about 1%.
Yet here we have an idea that would change 10% of affected programs from incorrect to correct,
and it is rejected, because performance is more important.

</p><p>
The argument about sanitizers is more nuanced.
Leaving a behavior undefined allows any implementation at all, including reporting the
behavior at runtime and stopping the program.
True, the widespread use of undefined behavior enables sanitizers like ThreadSanitizer, MemorySanitizer, and UBSan,
but so would defining the behavior as “either this specific behavior, or a sanitizer report.”
If you believed correctness was job one, you could
define overflow to wrap, fixing the 10% of programs outright
and making the 90% behave at least more predictably,
and then at the same time define that overflow is still
a bug that can be reported by sanitizers.
You might object that requiring wrapping in the absence of a sanitizer
would hurt performance, and that’s fine: it’s just more evidence that
performance trumps correctness.

</p><p>
One thing I find surprising, though, is that correctness gets ignored even
when it clearly doesn’t hurt performance.
It would certainly not hurt performance to emit a compiler warning
about deleting the <code>if</code> statement testing for signed overflow,
or about optimizing away the possible null pointer dereference in <code>Do()</code>.
Yet I could find no way to make compilers report either one; certainly not <code>-Wall</code>.

</p><p>
The explanatory shift from non-portable to optimizable also seems revealing.
As far as I can tell, C89 did not use performance as a justification for any of
its undefined behaviors.
They were non-portabilities, like signed overflow and null pointer dereferences,
or they were outright bugs, like use-after-free.
But now experts like Chris Lattner and Hans Boehm point to optimization potential,
not portability, as justification for undefined behaviors.
I conclude that the rationales really have shifted from the mid-1980s to today:
an idea that meant to capture non-portability has been preserved for performance,
trumping concerns like correctness and debuggability.

</p><p>
Occasionally in Go we have <a href="https://go.dev/blog/compat#input">changed library functions to remove surprising behavior</a>,
It’s always a difficult decision, but we are willing
to break existing programs depending on a mistake
if correcting the mistake fixes a much larger number of programs.
I find it striking that the C and C++ standards committees are
willing in some cases to break existing programs if doing so
merely <i>speeds up</i> a large number of programs.
This is exactly what happened with the infinite loops.

</p><p>
I find the infinite loop example telling for a second reason:
it shows clearly the escalation from non-portable to optimizable.
In fact, it would appear that if you want to break C++ programs in
service of optimization, one possible approach is to just do that in a
compiler and wait for the standards committee to notice.
The de facto non-portability of whatever programs you have broken
can then serve as justification for undefining their behavior,
leading to a future version of the standard in which your optimization is legal.
In the process, programmers have been handed yet another footgun
to try to avoid setting off.

</p><p>
(A common counterargument is that the standards committee cannot
force existing implementations to change their compilers.
This doesn’t hold up to scrutiny: every new feature that gets added
is the standards committee forcing existing implementations
to change their compilers.)

</p><p>
I am not claiming that anything should change about C and C++.
I just want people to recognize that the current versions of these
sacrifice correctness for performance.
To some extent, all languages do this: there is almost always a tradeoff
between performance and slower, safer implementations.
Go has data races in part for performance reasons:
we could have done everything by message copying
or with a single global lock instead, but the performance wins of
shared memory were too large to pass up.
For C and C++, though, it seems no performance win is too small
to trade against correctness.

</p><p>
As a programmer, you have a tradeoff to make too,
and the language standards make it clear which side they are on.
In some contexts, performance is the dominant priority and
nothing else matters quite as much.
If so, C or C++ may be the right tool for you.
But in most contexts, the balance flips the other way.
If programmer productivity, debuggability, reproducible bugs,
and overall correctness and understandability
are more important than squeezing every last little bit of performance,
then C and C++ are not the right tools for you.
I say this with some regret, as I spent many years happily writing C programs.

</p><p>
I have tried to avoid exaggerated, hyperbolic language in this post,
instead laying out the tradeoff and the preferences revealed
by the decisions being made.
John Regehr wrote a less restrained series of posts about undefined behavior
a decade ago, and in <a href="https://blog.regehr.org/archives/226">one of them</a> he concluded:</p><blockquote>

<p>
It is basically evil to make certain program actions wrong, but to not give developers any way to tell whether or not their code performs these actions and, if so, where. One of C’s design points was “trust the programmer.” This is fine, but there’s trust and then there’s trust. I mean, I trust my 5 year old but I still don’t let him cross a busy street by himself. Creating a large piece of safety-critical or security-critical code in C or C++ is the programming equivalent of crossing an 8-lane freeway blindfolded.</p></blockquote>

<p>
To be fair to C and C++,
if you set yourself the goal of crossing an 8-lane freeway blindfolded,
it does make sense to focus on doing it as fast as you possibly can.
      </p></div>
    </div></div>
  </body>
</html>
