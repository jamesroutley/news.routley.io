<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dev.37signals.com/bringing-our-apps-back-home/">Original</a>
    <h1>De-cloud and de-k8s ‚Äì bringing our apps back home</h1>
    
    <div id="readability-page-1" class="page"><div>

<h2 id="why-are-we-doing-this">Why are we doing this?</h2>

<p>A number of our applications have been on a long journey through various cloud providers and services over the years.</p>

<p>Originally, we started by moving them from our data centers to AWS ECS, with the promise of lovely contained Docker builds and eventual cost savings.</p>

<p>We liked Docker a lot, less so the lack of flexibility in ECS. Thus, we pivoted to GKE on Google Cloud Platform to give Kubernetes a try, only to be thwarted by network control plane outages that led us to a quick retreat. Undeterred by the cloud, we moved our legacy applications to AWS Kubernetes (EKS) where some of them still reside today.</p>

<p>You inevitably accrue some dimension of technical debt and complexity on this path. Your deployment strategy isn‚Äôt the only thing that has to change: you‚Äôll have to invent new tooling to manage those stacks and create useful CI/CD to cater to the needs of both operations and programming. Most likely you will have to rethink your monitoring strategy as well.</p>

<p>Not even scratching the necessity of entirely different paradigms regarding informational and operational security as well. Oh, and at some point, you‚Äôd also have to train people about all of this! Gimme a second, we just got an email about the maintenance EOL of this resource from [public cloud provider]‚Ä¶ wait, is us-east-1 down?</p>

<p>Bottom line: you need a lot of processes to do this right. In a lot of places, it became apparent we were spending more than we got out of it in return ‚Äî <a href="https://dev.37signals.com/our-cloud-spend-in-2022/">not just economically</a>, but also operationally. This was our smallest application, <a href="https://signalvnoise.com/archives/001021.php">Tadalist</a>, when it ran on EKS, taken from our internal documentation.</p>

<div><div><pre><code>                              +--------------------------------------------------------+
                              |    eksctl VPC                                          |
                              |    +----------------------------------------------+    |
                              |    |                      EKS                     |    |
                              |    | +------------------------------+ +---------+ |    |
                              |    | |app namespace                 | |default  | |    |
                              |    | |                              | |namespace| |    |
+-------------------+         |    | +--------+ +--------+ +--------+ +---------+ |    |
|   tadalist VPC    |         |    | |pod     | |pod     | |pod     | |pod      | |    |
|                   |         |    | +--------+ +--------+ +--------+ +---------+ |    |
| +---------------+ |         |    | |Unicorn | |Unicorn | |Unicorn +-+Logstash | |    |
| |   Services    | |         |    | |        | |        | |        | |         | |    |
| |               | |   VPC   |    | +--------+ +--------+ +--------+ +---+-----+ |    |
| | +-----------+ | | Peering |    | |Nginx   | |Nginx   | |Nginx   |     |       |    |
| | |    RDS    | | &lt;---------&gt;    | |        | |        | |        +-----+       |    |
| | +-----------+ | |         |    | +-^------+-+-^------+-+-^------+             |    |
| +---------------+ |         |    |   |          |          |                    |    |
+-------------------+         |    |   |          |          |                    |    |
                              |    +----------------------------------------------+    |
                              |        |          |          |                         |
                              |      +-+----------+----------+---+                     |
                              |      | Application Load Balancer |                     |
                              |      +------------^--------------+                     |
                              |                   |                                    |
                              +--------------------------------------------------------+
                                                  |
                                                  |
                                                  +
                                           Internet Traffic
</code></pre></div></div>

<p>Looks‚Ä¶ easy? Well, note that this is just the rough infrastructural outline ‚Äî it doesn‚Äôt include all the auxiliary tooling that runs on it, such as</p>

<ul>
  <li>cluster-autoscaler</li>
  <li>ingress controllers</li>
  <li>storage drivers</li>
  <li>external-dns</li>
  <li>node termination handlers</li>
  <li>complex networking concepts around VPN, peering, route tables, NAT, ‚Ä¶</li>
  <li>where DNS is handled</li>
  <li>‚Ä¶</li>
</ul>

<p>It also misses the entire sphere around identity and access management for those resources that also needs to be maintained. Not even mentioning the infrastructure as code that has grown around this. It‚Äôs fair to say that we never realized the promise that the cloud would simplify our life.</p>

<p>And Tadalist is just a basic, pretty isolated Rails app at its core. It doesn‚Äôt get easier from here for our other, much more complex apps with a lot more dependencies on other backend services.</p>

<p>Our first impulse for de-clouding was to run (vendor-supported) Kubernetes on our own hardware, so we could keep the largest chunk of the work that we‚Äôve invested over years and ‚Äújust‚Äù repoint our tooling to a new place. An additional challenge was the fact that most of our apps have been containerized a few years ago to satisfy legacy requirements more easily ‚Äî and we wanted to keep it that way.</p>

<p>It all sounded like a win-win situation, but it turned out to be a very expensive and operationally complicated idea, so we had to get back to the drawing board pretty soon.</p>

<p>Enter <a href="https://github.com/mrsked/mrsk">mrsk</a>.</p>

<hr/>

<h2 id="containers-but-nice-migrating-tadalist">Containers, but nice: migrating Tadalist</h2>

<p><code>mrsk</code> became the center paradigm around our new de-clouding and de-k8sing strategy: it created a path to simplifying deployments of our existing containerized applications while speeding up the process of doing so tremendously. We were able to keep a lot of prior art around the containerization while still running apps in a somewhat new, shiny but also familiar way ‚Äî we‚Äôve been deploying with <a href="https://www.capify.org/index.html"><code>capistrano</code></a> for Basecamp 4 and other apps in the data center for years, and <code>mrsk</code> aimed at the same imperative model. No more control plane, fewer moving parts. Nice!</p>

<p><a href="https://signalvnoise.com/archives/001021.php">Tadalist</a> was the perfect candidate: it has the lowest criticality of all our applications and no paying customers. It has served as a canary in all our operational endeavors and it stayed within that role this time as well.</p>

<p>Of course, this wasn‚Äôt just a ‚Äúthrow it on there and run with it‚Äù kind of process. <code>mrsk</code> was in active development while the infrastructural side was incepted, in tandem. You don‚Äôt hit the ground running right from the start! We also had to take care of a couple of other things.</p>

<h3 id="virtual-machine-provisioning">Virtual machine provisioning</h3>

<p>We focused on the cloud and surrounding technologies <em>a lot</em> in recent years, thus our processes for on-premise infrastructure provisioning fell a bit behind. Next to the de-clouding effort, we started to completely modernize and simplify our configuration management, upgrading to the current version of <a href="https://www.chef.io/">Chef</a>.</p>

<p>Since <code>mrsk</code> was going to target servers on-premise, we needed a new process for provisioning virtual machines quickly and painlessly as well. We leveraged our existing knowledge running pure KVM-based VM guests, simplified bootstrapping with cloud-init, and weaved all of this into a single cookbook. These improvements allowed us to cut down our process for a fully bootstrapped VM from around 20 minutes per guest to <em>less than a minute</em> ‚Äî and we can now define and start several simultaneously with a single chef converge of the KVM host.</p>

<div><div><pre><code># Example definition from our cookbook
node.default[&#39;guests&#39;] = case node[&#39;hostname&#39;]
                         when &#34;kvm-host-123&#34;
                           {
                            &#34;test-guest-101&#34;: { ip_address: &#34;XX.XX.XX.XX/XX&#34;, memory: &#34;8192&#34;, cpu: &#34;4&#34;, disk: &#34;30G&#34;, os: &#34;ubuntu22.04&#34;, os_name: &#34;jammy&#34;, domain: &#34;guest.internal-domain.com&#34; },
                           }
                         when &#34;kvm-host-456&#34;
                           {
                            &#34;test-guest-08&#34;: { ip_address: &#34;XX.XX.XX.XX/XX&#34;, memory: &#34;4096&#34;, cpu: &#34;2&#34;, disk: &#34;20G&#34;, os: &#34;ubuntu18.04&#34;, os_name: &#34;bionic&#34;, domain: &#34;guest.internal-domain.com&#34; },
                           }
                         end

include_recipe &#34;::_create_guests&#34;
</code></pre></div></div>

<div><div><pre><code># Excerpt from our ::_create_guests Chef recipe
node[&#39;guests&#39;].each do |guest_name, guest_config|
  execute &#34;Create and start a new guest with virt-install&#34; do
    command &#34;virt-install --name #{guest_name} --memory #{guest_config[:memory]} \
             --vcpus #{guest_config[:cpu]} --network bridge=br0,model=virtio --graphics none \
             --events on_reboot=restart --os-variant #{guest_config[:os]} \
             --import --disk /u/kvm/guests-vol/#{guest_name}-OS.qcow2 \
             --noautoconsole --cloud-init user-data=/u/kvm/guests-config/user-data-#{guest_name},network-config=/u/kvm/guests-config/network-config-#{guest_name} \
             --autostart&#34;
  end
end
</code></pre></div></div>

<p>A huge speed boost, since we‚Äôd be creating VMs left and right soon! Most of these were simple boxes with not much more than basic user management, a Filebeat config and a Docker installation.</p>

<h3 id="logging">Logging</h3>

<p>Our logging pipeline is <a href="https://www.elastic.co/what-is/elk-stack">a fully consolidated ELK stack</a>, which is interoperable with both our cloud and on-premise stacks. <code>mrsk</code>‚Äôs logging is built on top of pure Docker logs, so all we had to do was reroute Filebeat to pick up those logs in <code>/var/lib/docker/containers/</code> and ship them to our Logstash installations.</p>

<h3 id="cdn">CDN</h3>

<p>For years, we‚Äôve leveraged CloudFront and Route53 for our CDN and DNS requirements, so naturally, we had to find a replacement and switched to <a href="https://www.cloudflare.com/">Cloudflare</a>, which is now fronting our on-premise F5 load balancers in the data centers.</p>

<h3 id="cicd">CI/CD</h3>

<p>Previously running on Buildkite, we now moved to <a href="https://github.com/features/actions">Github Actions</a> for CI/CD.</p>

<h3 id="database-replication-and-backups">Database replication and backups</h3>

<p>RDS was doing a lot for us, but it‚Äôs not like we didn‚Äôt have prior experience in running MySQL databases on-prem before ‚Äî we‚Äôve been doing this for Basecamp for decades. For the <code>mrsk</code>-backed app deployments, we leveraged on that and upgraded our stack to run <a href="https://docs.percona.com/percona-server/8.0/#">Percona MySQL 8</a> and implemented a cron-based backup process to a dedicated backup location within our data center ‚Äî all encapsulated in a cookbook and applied to the relevant database servers.</p>

<p>In <em>less than a six-week cycle</em>, we built those operational foundations, shaped <code>mrsk</code> to its functional form and had Tadalist running in production on our own hardware.</p>

<p>The cutover process was a fairly straightforward call of transferring the small database and shifting DNS, but we did accept some downtime, as Tadalist doesn‚Äôt run under an SLO of zero downtime.</p>

<p>So after this process, this is where Tadalist landed:</p>

<div><div><pre><code>+----------------------------------------------------------------+
|  On-Prem                                                       |
|            +--------------------------------------------+      |
|            |                                            |      |
|            |                                   +--------v--+   |
|  +---------+-------+ +-----------------+------&gt;| db-01     |   |
|  | app-01          | | app-02          |       |           |   |
|  | +-------------+ | | +-------------+ |       |    mysql  |   |
|  | | tadalist-app| | | | tadalist-app| |   +---+-----------+   |
|  | |             | | | |             | |   |                   |
|  | +-------------+ | | +-------------+ |   |replicates to      |
|  | |   traefik   | | | |  traefik    | |   |   +------------+  |
|  | +-------------+ | | +-------------+ |   |   | db-02      |  |
|  +-+-----------+-+-+ +-+--+----------+-+   +--&gt;|            |  |
|                |          |                    |   mysql    |  |
|                |          |                    +------------+  |
|             +--v----------v--+                                 |
|             |       F5       |                                 |
|             |  load balancer |                                 |
|             +--------+-------+                                 |
|                      |                                         |
+----------------------+-----------------------------------------+
                       |
              +--------+-------+
              |   Cloudflare   |
              +--------^-------+
                       |
                       |
               Internet traffic
</code></pre></div></div>

<p>A pretty standard Rails app deployment, right? Turns out that‚Äôs all Tadalist (and most of our apps) really needed.</p>

<p>The application hosts are configured as pools on the F5 load balancer, which takes care of routing the traffic where it should go and also provides the public endpoint that Cloudflare should talk to. Anything behind that is just basic Ruby, Rails and Docker, the latter wrapped into <code>mrsk</code>.</p>

<p>Did I mention we cut down our deployment times from several minutes to just roughly a minute, sometimes even less?</p>

<h2 id="two-is-a-party--migrating-writeboard">Two is a party ‚Äî migrating Writeboard</h2>

<p>In terms of complexity, <a href="https://signalvnoise.com/archives2/writeboard_is_live.php">Writeboard</a> was our next candidate. We could build on all those foundations we figured out for Tadalist already, so the entire process of migrating took less than two weeks.</p>

<p>We were also joined by our Security/Infrastructure/Performance team for extended verification and eventual changes from the application side, as Writeboard has a concept of scheduling jobs. Other than that, it pretty much matched Tadalist in terms of application and infrastructure requirements.</p>

<p>Since Writeboard has paying customers and a much larger database, the SLO changed. Thus we opted for a simple three-fold replication process to prepare the databases pre-migration.</p>

<div><div><pre><code>+-----------------+            +-----------------+            +-----------------+
|       RDS       |            |     wb-db-01    |            |     wb-db-02    |
|                 | replicates |                 | replicates |                 |
|      (r/w)      +-----------&gt;|      (r/o)      +-----------&gt;|      (r/o)      |
+-----------------+            +-----------------+            +-----------------+
</code></pre></div></div>

<p>Once that was in place, the actual migration consisted of:</p>
<ul>
  <li>enabling maintenance windows</li>
  <li>setting RDS to read-only</li>
  <li>make the on-prem databases writable</li>
  <li>stopping the replication from RDS</li>
  <li>switching DNS</li>
</ul>

<p>The full cutover for Writeboard took 16 minutes with no downtime or complications.</p>

<p>We‚Äôre on a roll here, so let‚Äôs move on to Backpack!</p>

<hr/>

<h2 id="three-is-a-festival--migrating-backpack">Three is a festival ‚Äî migrating Backpack</h2>

<p><a href="https://signalvnoise.com/archives2/backpack_launches_a_new_breed_of_personal_and_business_information_manager.php">Backpack</a> again could build on the previous migrations, with a small caveat: it runs a stateful mail pipeline implemented with <a href="https://www.postfix.org/">postfix</a>, thus it needs to process those mails on disk.</p>

<p>On EKS, this got implemented by running postfix on a separate EC2 node and mounting via a shared EFS-backed PVC into the jobs pods. For the <code>mrsk</code>-backed deployment, we decided on a similar scheme with a shared NFS between the mail-in hosts and the jobs containers.</p>

<p>Here, we came across the next big feature in <code>mrsk</code>, which is mounting the filesystem into containers. It‚Äôs actually not a huge deal in Docker ‚Äî it just had to be implemented into <code>mrsk</code> and thoroughly tested.</p>

<p>Backpack also runs many more scheduled jobs than Writeboard ‚Äî and we had to find a way to run those so they don‚Äôt step on each other‚Äôs toes within our given application requirements. Previously, this was solved by just running them all on the same pod/host and sharing a temporary file-based lock. With multiple nodes as we had planned, this wouldn‚Äôt work anymore (and it‚Äôs also not very flexible). Thus, this logic was rewritten to run with <a href="https://redis.io/">Redis</a> as their backend, so we could isolate this concern from the jobs hosts.</p>

<div><div><pre><code>                                                              ^
                                                              |
--------------------------------------------------------------+----------
  On-Prem                                                     |
                                  +----------+            +---v---+
                 passes mail      |          |            |       |
                  content to +---&gt;|  app-01  |&lt;----------&gt;|  F5   |
                     app     |    |          |            |       |
                             |    +----------+            +---+---+
                             v                                |
                      +---------------+                       |
    processes jobs    |    jobs-01    |     NFS share         |  Incoming
           +---------&gt;|               |&lt;---------+            |   mail
           |          +---------------+          |            |
           v                                     v            |
    +---------------+                   +---------------+     |
    |   redis-01    |                   |  mail-in-01   |&lt;----+
    +---------------+                   +---------------+
</code></pre></div></div>

<p>The app and jobs VMs run containerized workloads deployed with <code>mrsk</code>. We opted to run critical stateful workloads such as MySQL and postfix non-containerized on VMs.</p>

<p>Backpack‚Äôs migration started next to Writeboard‚Äôs, but took us around three weeks in total because of the new unknowns and extended acceptance testing we had to figure out. Nevertheless ‚Äî it was a fairly smooth, zero-downtime cutover following the same steps as for the previous migrations ‚Äî we‚Äôve been exercising that muscle for a while now!</p>

<hr/>

<h2 id="summary">Summary</h2>

<p>Following a standard pattern for all apps, we achieved multiple wins:</p>

<ul>
  <li>Huge reduction of infrastructure complexity ‚Äî fewer moving parts</li>
  <li>Alignment of deployment strategies ‚Äî no more special snowflakes</li>
  <li>Cut down deployment times to a fraction</li>
  <li>Massive code cleanup ‚Äî check this diff for Tadalist:</li>
</ul>



<p>On top of that, this move also pushed us to:</p>

<ul>
  <li>Rewrite our entire configuration management with the current version of Chef</li>
  <li>Implement a super fast VM provisioning for our needs</li>
  <li>Upgrade from MySQL 5.7 to 8</li>
</ul>

<p><strong>And most importantly, it challenged us to completely reconsider how our applications could be run.</strong> Did they really need all that the cloud and Kubernetes had to offer? Wasn‚Äôt there a better, easier and less costly compromise for us?</p>

<p>Turns out there was ‚Äî and we were able to pull this off while still keeping up some of the containerized goodness that we‚Äôve worked on for years. You can imagine that we had ample celebrations during those first two months of 2023! üéâ</p>

<hr/>

<h2 id="where-we-go-from-here">Where we go from here</h2>

<p>After our k8s on-prem plans had to be discarded, we went on quite a journey to solve the problem for us, in our own way, for our requirements, challenging our assumptions. Needless to say, the learning curve was steep, and this was (and still is) an all-hands-on-deck party for Ops at 37signals, while <em>still</em> keeping the lights on for everything else. #TheOpsLife didn‚Äôt just stop because of this effort.</p>

<p>But ‚Äî it‚Äôs been fun! And the reduction of complexity and dependencies has only brought benefits so far ‚Äî not just in day-to-day business, but soon enough also on our cloud bills. We‚Äôre quickly marching toward the next application transition right now.</p>

<p>Our apps aren‚Äôt the only thing that will come home ‚Äî we‚Äôre exploring search backends, monitoring and so much more. The quest of de-clouding and de-k8sing is far from over, so stay tuned!</p>




            </div></div>
  </body>
</html>
