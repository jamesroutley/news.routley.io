<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://quantum5.ca/2025/05/11/fast-cheap-bulk-storage-using-lvm-to-cache-hdds-on-ssds/">Original</a>
    <h1>Fast and cheap bulk storage: using LVM to cache HDDs on SSDs</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Since the inception of <a href="https://en.wikipedia.org/wiki/Solid-state_drive">solid-state drives</a> (SSDs), there has been a choice
to make—either use SSDs for vastly superior speeds, especially with
non-sequential read and writes (“random I/O”), or use legacy spinning
rust <a href="https://en.wikipedia.org/wiki/Hard_disk_drive">hard disk drives</a> (HDDs) for cheaper storage that’s a bit slow for
sequential I/O<sup id="fnref:seq-slow"><a href="#fn:seq-slow" rel="footnote" role="doc-noteref">1</a></sup> and <em>painfully</em> slow for random I/O.</p>

<p>The idea of caching frequently used data on SSDs and storing the rest on HDDs is
nothing new—<a href="https://en.wikipedia.org/wiki/Hybrid_drive">solid-state hybrid drives</a> (SSHDs) embodied this idea in
hardware form, while filesystems like <a href="https://en.wikipedia.org/wiki/ZFS">ZFS</a> support using SSDs
as <abbr title="level 2 adaptive replacement cache">L2ARC</abbr>. However, with
the falling price of SSDs, this no longer makes sense outside of niche scenarios
with very large amounts of storage. For example, I have not needed to use HDDs
in my PC for many years at this point, since all my data easily fits on an SSD.</p>

<p>One of the scenarios in which this makes sense is for <a href="https://mirror.quantum5.ca/">the mirrors</a> I host
at home. Oftentimes, a project will require hundreds of gigabytes of data to be
mirrored just in case anyone needs it, but only a few files are frequently
accessed and could be cached on SSDs for fast access<sup id="fnref:mqc"><a href="#fn:mqc" rel="footnote" role="doc-noteref">2</a></sup>. Similarly, I have
many LLMs locally with <a href="https://ollama.com/">Ollama</a>, but there are only a few I use very
frequently. The frequently used ones can be cached while the rest can be loaded
slowly from HDD when needed.</p>

<p>While ZFS may seem like the obvious option here, due to Linux compatibility
issues with ZFS <a href="https://quantum5.ca/2024/12/22/on-btrfs-and-memory-corruption/">mentioned previously</a>, I decided to use
Linux’s <a href="https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)">Logical Volume Manager (LVM)</a> instead for this task to save myself
some headache. To ensure reliable storage in the event of HDD failures, I am
running the HDDs in <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1">RAID 1</a> with Linux’s <a href="https://en.wikipedia.org/wiki/Mdadm">mdadm</a> software RAID.</p>

<p>This post documents how to build such a cached RAID array and explores some
considerations when building reliable and fast storage.</p>

<!--more-->

<h2 id="table-of-contents">Table of contents<a href="#table-of-contents">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<ol>
  <li><a href="#why-use-lvm-cache">Why use LVM cache?</a></li>
  <li><a href="#a-quick-introduction-to-lvm">A quick introduction to LVM</a></li>
  <li><a href="#the-hardware">The hardware setup</a></li>
  <li><a href="#why-use-raid-1-on-hdds">Why use RAID 1 on HDDs?</a></li>
  <li><a href="#setting-up-raid-1-with-mdadm">Setting up RAID 1 with <code>mdadm</code></a></li>
  <li><a href="#creating-the-ssd-cache-partition">Creating the SSD cache partition</a></li>
  <li><a href="#creating-a-new-volume-group">Creating a new volume group</a></li>
  <li><a href="#creating-the-cached-lv">Creating the cached LV</a></li>
  <li><a href="#creating-a-filesystem">Creating a filesystem</a></li>
  <li><a href="#mounting-the-new-filesystem">Mounting the new filesystem</a></li>
  <li><a href="#monitoring">Monitoring</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ol>

<h2 id="why-use-lvm-cache">Why use LVM cache?<a href="#why-use-lvm-cache">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>There are several alternative block device caching solutions on Linux, such as:</p>

<ul>
  <li>
<a href="https://docs.kernel.org/admin-guide/bcache.html"><code>bcache</code></a>: a built-in Linux kernel module that does similar caching
as LVM. I don’t like the way it’s set up by owning the entire block device and
non-persistent sysfs configurations, compared to LVM remembering all the
configuration options, nor do I enjoy hearing about all the reports of bcache
corrupting data; and</li>
  <li>
<a href="https://en.wikipedia.org/wiki/EnhanceIO">EnhanceIO</a>: an old kernel module that does something similar to
bcache and LVM cache, but hasn’t been maintained for over a decade.</li>
</ul>

<p>Since I am very familiar with LVM and have already used it for other reasons, I
opted to use LVM for this exercise as well.</p>

<h2 id="a-quick-introduction-to-lvm">A quick introduction to LVM<a href="#a-quick-introduction-to-lvm">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>If you aren’t familiar with LVM, we’ll need to first introduce some concepts, or
none of the LVM portions of this post will make any sense.</p>

<p>First, we’ll need to introduce block devices, which are just devices with a
fixed number of blocks that can be read at any offset. HDDs and SSDs show up as
block devices, such <code>/dev/sda</code>. They can be partitioned into multiple pieces,
showing up as smaller block devices such as <code>/dev/sda1</code>, the first partition on
<code>/dev/sda</code>. Filesystems can be created directly on block devices, but these
block devices can also be used with more advanced things like RAID and LVM.</p>

<p>LVM is a volume manager that allows you to create logical volumes that can be
expanded much more easily than regular partitions. In LVM, there are three major
entity types:</p>

<ul>
  <li>
<strong>Physical volumes (PVs)</strong>: block devices that are used as the underlying
storage for LVM;</li>
  <li>
<strong>Logical volumes (LVs)</strong>: block devices that are presented by LVM, stored on
one or more PVs; and</li>
  <li>
<strong>Volume groups (VGs)</strong>: a group of PVs on which LVs can be created.</li>
</ul>

<p>LVs can be used just like partitions to store files, with the flexibility of
being able to expand them at will while they are <em>actively being accessed</em>,
without having to be contiguous like real partitions.</p>

<p>There are more advanced LV types, such as thin pools, which doesn’t allocate
space for LVs until they are actually used to store data, and cached volumes,
which this post is about.</p>

<h2 id="the-hardware">The hardware<a href="#the-hardware">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>For the purposes of this post, we will assume that there are two SATA HDDs (
4 TB each in my case), available as block devices <code>/dev/sda</code> and
<code>/dev/sdb</code>:</p>

<div><div><pre><code><span>$</span><span> </span>lsblk
<span>NAME               MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
sda                  8:0    0   3.6T  0 disk
sdb                  8:16   0   3.6T  0 disk
</span><span>...
</span></code></pre></div></div>

<p><strong>Warning</strong>: before copying any commands, ensure that you are operating on the
correct device. There is no undo button for most of the commands in this post,
so be very careful lest you destroy your precious data! When in doubt, run
<code>lsblk</code> to double check!</p>

<p>We’ll also assume that the SSD is <code>/dev/nvme0n1</code><sup id="fnref:nvme"><a href="#fn:nvme" rel="footnote" role="doc-noteref">3</a></sup> (2 TB in my case),
and we will allocate 100 GiB of it as the cache by creating a partition.</p>

<p>Effectively, the setup looks like this:</p>

<p><img width="600" alt="Diagram of the LVM cache setup" integrity="sha512-N2ekJ2GDGyjeFTmqjtsrAC13DL//ZJqGkL0TkZbZrr/76KL/HPqhOsaW1h1umKp/nOcrBXuJprK7ddCDfm4qCw==" crossorigin="anonymous" src="https://quantum5.ca/assets/lvm-cache-3767a42761831b28de1539aa8edb2b002d770cbfff649a8690bd139196d9aebffbe8a2ff1cfaa13ac696d61d6e98aa7f9ce72b057b89a6b2bb75d0837e6e2a0b.svg"/></p>

<h2 id="why-use-raid-1-on-hdds">Why use RAID 1 on HDDs?<a href="#why-use-raid-1-on-hdds">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>Mechanical HDDs, like everything mechanical, fail. It’s an inevitable fact of
life. There are two choices here:</p>

<ol>
  <li>Treat your data as ephemeral and replace it when the drive fails, accepting
the inevitable downtime this causes; or</li>
  <li>Store your data in a redundant fashion (i.e. with RAID), so that it continues
to be available despite drive failures<sup id="fnref:backup"><a href="#fn:backup" rel="footnote" role="doc-noteref">4</a></sup>.</li>
</ol>

<p>If your data is really that unimportant, I suppose you could store it on a
single drive, or even use RAID 0 to stripe it across multiple drives such that
it’s lost if any one drive fails, but benefit from being able to pool all the
drives together.</p>

<p>However, as I learned the hard way, even easily replaceable data still requires
effort to replace them. I once deployed this exact setup with RAID 0 and one of
the constituent drives suffered a failure, causing a few files to become
unreadable. While I could easily download them again, it created a lot of
downtime due to having to destroy the entire array and start over after
replacing the failed drive.</p>

<p>This may not matter for your use case, but I would rather that my mirror
experience minimal downtime in the event of a drive failure. For this reason, I
chose to run the drives together in RAID 1.</p>

<h2 id="setting-up-raid-1-with-mdadm">Setting up RAID 1 with <code>mdadm</code><a href="#setting-up-raid-1-with-mdadm">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>One thing worth noting before we start with setting up RAID is that all block
devices (either whole drives or partitions) in a RAID must be identical in
size<sup id="fnref:btrfs"><a href="#fn:btrfs" rel="footnote" role="doc-noteref">5</a></sup>. This presents some interesting challenges, since a 4 TB HDD
drive isn’t always the same size. Normally, for a drive to be sold as “
4 TB,” it has to have at least
4 000 000 000 000 bytes (that’s 4 trillion
bytes). This is around 3.638 TiB using power-of-two IEC units. Typically,
they have slightly more, though this varies by manufacturer or even model.</p>

<p>This poses a problem when using non-identical drive models, which you are
encouraged to do to avoid drives failing at the same time. Drives produced in
the same batch subjected to the same operations tend to fail at similar times,
so that’s a good precaution to take to avoid failures. A similar problem occurs
when it comes to replacing the drives when they fail, especially if you can’t
source an identical model.</p>

<p>To avoid this problem, we will partition the drive and cut the data partition
off exactly at the 4 TB mark. This will ensure that any “4 TB” HDD
could be similarly partitioned and used as a replacement. Another reason to
partition is to avoid the drive being treated as uninitialized on operating
systems that don’t understand Linux’s <code>mdadm</code> RAID, such as Windows.</p>

<h3 id="partitioning-the-drives">Partitioning the drives<a href="#partitioning-the-drives">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>We’ll need to do some math to figure out which 512-byte logical sector to end
the partition on. For a 4 TB drive, we want to end it at the exact
4 TB mark:</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span> <span>4e12</span><span>/</span><span>512</span> <span>-</span> <span>1</span>
<span>7812499999.0</span>
</code></pre></div></div>

<p>Since partition tools typically ask for the offset of the last sector to be
included in the partition, we’ll need to subtract 1.</p>

<p>To partition the drive, we first need to clean everything on it first:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>wipefs <span>-a</span> /dev/sda
<span>...
</span><span>$</span><span> </span><span>sudo </span>wipefs <span>-a</span> /dev/sdb
<span>...
</span></code></pre></div></div>

<p>(You can skip this if you are using a brand new drive.)</p>

<p>Then, create the partition with <code>gdisk</code>:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>gdisk /dev/sda
<span>GPT fdisk (gdisk) version 1.0.9

Partition table scan:
  MBR: not present
  BSD: not present
  APM: not present
  GPT: not present

Creating new GPT entries in memory.

Command (? for help): n
Partition number (1-128, default 1):
First sector (34-7814037134, default = 2048) or {+-}size{KMGTP}:
Last sector (2048-7814037134, default = 7814035455) or {+-}size{KMGTP}: 7812499999
Current type is 8300 (Linux filesystem)
Hex code or GUID (L to show codes, Enter = 8300): fd00
Changed type of partition to &#39;Linux RAID&#39;

Command (? for help): c
Using 1
Enter name: cached_raid1_a

Command (? for help): p
Disk /dev/sda: 7814037168 sectors, 3.6 TiB
Model: ST4000VN008-2DR1
Sector size (logical/physical): 512/4096 bytes
Disk identifier (GUID): [redacted]
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 7814037134
Partitions will be aligned on 2048-sector boundaries
Total free space is 1539149 sectors (751.5 MiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048      7812499999   3.6 TiB     FD00  cached_raid1_a

Command (? for help): w

Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING
PARTITIONS!!

Do you want to proceed? (Y/N): y
</span><span>OK;</span><span> </span>writing new GUID partition table <span>(</span>GPT<span>)</span> to /dev/sda.
<span>The operation has completed successfully.
</span></code></pre></div></div>

<p>Now repeat this for <code>/dev/sdb</code>. Note that you don’t have to name the partitions
with the <code>c</code> command, but it makes it easier to identify which partition is
which if you have a lot of drives.</p>

<p>The partitions <code>/dev/sda1</code> and <code>/dev/sdb1</code> should now be available. If not, run
<code>partprobe</code> to reload the partition table.</p>

<h3 id="creating-the-mdadm-raid-array">Creating the <code>mdadm</code> RAID array<a href="#creating-the-mdadm-raid-array">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>Now we can create the array on <code>/dev/md0</code> by running <code>mdadm</code>:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>mdadm <span>--create</span> <span>--verbose</span> /dev/md0 <span>--level</span><span>=</span>1 <span>--raid-devices</span><span>=</span>2 /dev/sda1 /dev/sdb1
<span>mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store &#39;/boot&#39; on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: size set to 3906116864K
mdadm: automatically enabling write-intent bitmap on large array
Continue creating array? y
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
</span></code></pre></div></div>

<p>To avoid having to assemble this array on every boot, you should declare it in
<code>/etc/mdadm/mdadm.conf</code>. To do this, first run a command to get the definition:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>mdadm <span>--detail</span> <span>--scan</span>
<span>ARRAY /dev/md0 metadata=1.2 name=example:0 UUID=6d539f5d:5b37:4bf0:b2d9:2af5efc99e6a
</span></code></pre></div></div>

<p>Now, append the output to <code>/etc/mdadm/mdadm.conf</code>.</p>

<p>Then, make sure that this configuration is updated in the <code>initrd</code> for all
kernels:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>update-initramfs <span>-u</span> <span>-k</span> all
<span>update-initramfs: Generating /boot/initrd.img-6.1.0-34-amd64
update-initramfs: Generating /boot/initrd.img-6.1.0-33-amd64
</span><span>...
</span></code></pre></div></div>

<p>The RAID 1 array on <code>/dev/md0</code> is now ready to be used as a PV containing the
HDD storage.</p>

<h3 id="background-operations">Background operations<a href="#background-operations">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>In the background, Linux’s MD RAID driver is working hard to synchronize the two
drives so that they store identical data:</p>

<div><div><pre><code><span>$</span><span> </span><span>cat</span> /proc/mdstat 
<span>Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sde1[1] sdd1[0]
      3906116864 blocks super 1.2 [2/2] [UU]
</span><span>      [=&gt;</span>...................]  resync <span>=</span>  9.7% <span>(</span>379125696/3906116864<span>)</span> <span>finish</span><span>=</span>402.8min <span>speed</span><span>=</span>145930K/sec
<span>      bitmap: 29/30 pages [116KB], 65536KB chunk

</span><span>unused devices: &lt;none&gt;</span><span>
</span></code></pre></div></div>

<p>We can safely ignore this and continue. It will finish eventually.</p>

<h2 id="creating-the-ssd-cache-partition">Creating the SSD cache partition<a href="#creating-the-ssd-cache-partition">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>You’ll need a partition on an SSD to serve as cache. This needs to be a real
partition, not an LVM LV, as that would involve nested LVM. That never works
reliably in my experience, and I’ve given up trying. This is especially nasty
because I also use LVM to hold virtual machine disks, and if I just blanketly
allow nested LVM, then the host machine can access all the LVM volumes inside
all the VMs, which can cause data corruption.</p>

<p>If you don’t have unpartitioned space lying around, you’ll need to shrink a
partition and reallocate its space as a separate partition.</p>

<h3 id="calculating-the-size">Calculating the size<a href="#calculating-the-size">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>In my case, I had two partitions on my SSD, one EFI system partition (ESP) for
the bootloader, and an LVM PV covering the rest of the disk. It looks something
like this:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>gdisk <span>-l</span> /dev/nvme0n1
<span>...
</span><span>Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048          206847   100.0 MiB   EF00  EFI system partition
   2          206848      3907029134   1.8 TiB     8E00  main_lvm_pv
</span></code></pre></div></div>

<p>For a 100 GiB cache, we’ll need to shrink the LVM PV by 100 GiB, and
then edit the partition table. To avoid off-by-one errors, we’ll shrink the LV
by 200 GiB or so, fix up the partition table, and then expand it
afterwards.</p>

<p>Effectively, we want to end the LVM PV at sector <code>3697313934</code>, which is exactly
100 GiB worth of 512-byte sectors before the current last sector:</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span> <span>3907029134</span> <span>-</span> <span>100</span><span>*</span><span>1024</span><span>*</span><span>1024</span><span>*</span><span>2</span>
<span>3697313934</span>
</code></pre></div></div>

<p>Note that we multiply by <code>1024</code> once to convert from GiB to MiB, then a second
time to convert from MiB to KiB, and there are two sectors per KiB.</p>

<h3 id="shrink-existing-partition-data">Shrink existing partition data<a href="#shrink-existing-partition-data">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>First, shrinking the PV:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>pvresize <span>--setphysicalvolumesize</span> 1600G /dev/nvme0n1p2
<span>/dev/nvme0n1p2: Requested size 1.56 TiB is less than real size &lt;1.82 TiB. Proceed?  [y/n]: y
  WARNING: /dev/nvme0n1p2: Pretending size is 3355443200 not 3906822287 sectors.
  Physical volume &#34;/dev/nvme0n1p2&#34; changed
  1 physical volume(s) resized or updated / 0 physical volume(s) not resized
</span></code></pre></div></div>

<p>If you aren’t using LVM, but instead a regular ext4 filesystem, you can try
using <code>resize2fs</code>, passing the size as the second positional argument. This
would require you to unmount the partition first, since ext4 doesn’t have online
shrinking, unlike LVM.</p>

<h3 id="editing-the-partition-table">Editing the partition table<a href="#editing-the-partition-table">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>Then, we edit the partition table to shrink the partition for the PV and create
a new one in the freed space:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>gdisk /dev/nvme0n1
<span>...
</span><span>Command (? for help): d
Partition number (1-2): 2

Command (? for help): n
Partition number (2-128, default 2):
First sector (34-3907029134, default = 206848) or {+-}size{KMGTP}:
Last sector (206848-3907029134, default = 3907028991) or {+-}size{KMGTP}: 3697313934
Current type is 8300 (Linux filesystem)
Hex code or GUID (L to show codes, Enter = 8300): 8e00
Changed type of partition to &#39;Linux LVM&#39;

Command (? for help): n
Partition number (3-128, default 3):
First sector (34-3907029134, default = 3697315840) or {+-}size{KMGTP}:
Last sector (3697315840-3907029134, default = 3907028991) or {+-}size{KMGTP}:
Current type is 8300 (Linux filesystem)
Hex code or GUID (L to show codes, Enter = 8300): 8e00
Changed type of partition to &#39;Linux LVM&#39;

Command (? for help): c
Partition number (1-3): 3
Enter name: cached_cache_pv

Command (? for help): p
</span><span>...
</span><span>
Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048          206847   100.0 MiB   EF00  EFI system partition
   2          206848      3697313934   1.7 TiB     8E00  main_lvm_pv
   3      3697315840      3907028991   100.0 GiB   8E00  cached_cache_pv

Command (? for help): w

Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING
PARTITIONS!!

Do you want to proceed? (Y/N): y
</span><span>OK;</span><span> </span>writing new GUID partition table <span>(</span>GPT<span>)</span> to /dev/nvme0n1.
<span>Warning: The kernel is still using the old partition table.
The new table will be used at the next reboot or after you
run partprobe(8) or kpartx(8)
The operation has completed successfully.
</span></code></pre></div></div>

<p>Note that with <code>gdisk</code>, changing the size of a partition requires deleting it
and recreating it with the same partition number at the same starting offset.
The data in the partition is unaffected.</p>

<p>Now, we need to notify the kernel that the partition has shrunk:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>partprobe /dev/nvme0n1
</code></pre></div></div>

<h3 id="expand-shrunk-partition-to-fit-new-space">Expand shrunk partition to fit new space<a href="#expand-shrunk-partition-to-fit-new-space">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>Then, we can expand the PV to fit all the available space:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>pvresize /dev/nvme0n1p2
<span>  Physical volume &#34;/dev/nvme0n1p2&#34; changed
  1 physical volume(s) resized or updated / 0 physical volume(s) not resized
</span><span>$</span><span> </span><span>sudo </span>pvdisplay /dev/nvme0n1p2
<span>  --- Physical volume ---
  PV Name               /dev/nvme0n1p2
  PV Size               1.72 TiB / not usable &lt;3.07 MiB
</span><span>...
</span></code></pre></div></div>

<p>As we can see, the PV size is now exactly the reduced size of the partition. Now
that’s done, we can use <code>/dev/nvme0n1p3</code> as a PV containing our SSD cache.</p>

<h2 id="creating-a-new-volume-group">Creating a new volume group<a href="#creating-a-new-volume-group">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>Now that we have the partitions to serve as our PVs, we can create a volume
group called <code>cached</code>:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>vgcreate cached /dev/md0 /dev/nvme0n1p3
<span>  WARNING: Devices have inconsistent physical block sizes (4096 and 512).
  Physical volume &#34;/dev/md0&#34; successfully created.
  Physical volume &#34;/dev/nvme0n1p3&#34; successfully created.
  Volume group &#34;cached&#34; successfully created
</span></code></pre></div></div>

<h2 id="creating-the-cached-lv">Creating the cached LV<a href="#creating-the-cached-lv">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>Creating a cached LV is somehow a multistep process that requires a lot of math.</p>

<h3 id="creating-an-lv-on-the-hdd">Creating an LV on the HDD<a href="#creating-an-lv-on-the-hdd">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>First, you’ll need to create an LV containing the underlying data. Let’s put it
on <code>/dev/md0</code>, using up all available space. You can obviously use less space if
you want and expand it later. This is the command:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>lvcreate <span>-n</span> example <span>-l</span> 100%FREE cached /dev/md0
<span>  Logical volume &#34;example&#34; created.
</span></code></pre></div></div>

<h3 id="creating-the-cache-metadata-lv">Creating the cache metadata LV<a href="#creating-the-cache-metadata-lv">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>Next, we need a cache metadata volume on the SSD. 1 GiB should be plenty:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>lvcreate <span>-n</span> example_meta <span>-L</span> 1G cached /dev/nvme0n1p3
<span>  Logical volume &#34;example_meta&#34; created.
</span></code></pre></div></div>

<h3 id="creating-the-cache-lv">Creating the cache LV<a href="#creating-the-cache-lv">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>Now, we’ll need to use all remaining space on the <code>/dev/nvme0n1p3</code> PV to serve
as our cache. However, <code>-l 100%FREE</code> will <em>not</em> work because creating a cached
pool requires some free space for a spare pool metadata LV for repair operations
of the exact same size as the metadata. Since our metadata is 256 extents long,
we’ll need to identify how much space we have available and reduce it by 256 (
adjust if your metadata size is different):</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>pvdisplay /dev/nvme0n1p3
<span>  --- Physical volume ---
  PV Name               /dev/nvme0n1p3
  VG Name               cached
  PV Size               &lt;100.00 GiB / not usable 3.00 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              25599
  Free PE               25343
  Allocated PE          256
</span></code></pre></div></div>

<p>As you can see, we have 25343 extents left. We’ll need to subtract 256:</p>



<p>We can now create the actual cache LV:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>lvcreate <span>-n</span> example_cache <span>-l</span> 25087 cached /dev/nvme0n1p3
<span>  Logical volume &#34;example_cache&#34; created.
</span></code></pre></div></div>

<h3 id="creating-a-cache-pool">Creating a cache pool<a href="#creating-a-cache-pool">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>We can now merge the cache metadata and actual cache LV into a cache pool LV:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>lvconvert <span>--type</span> cache-pool <span>--poolmetadata</span> cached/example_meta cached/example_cache
<span>  Using 128.00 KiB chunk size instead of default 64.00 KiB, so cache pool has less than 1000000 chunks.
  WARNING: Converting cached/example_cache and cached/example_meta to cache pool&#39;s data and metadata volumes with metadata wiping.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
Do you really want to convert cached/example_cache and cached/example_meta? [y/n]: y
  Converted cached/example_cache and cached/example_meta to cache pool.
</span></code></pre></div></div>

<p>Here, we used the default chunk size chosen by LVM, but depending on the size of
your files, you might benefit from a different chunk size. The <code>lvmcache(7)</code> man
page has this to say:</p>

<blockquote>
  <p>The value must be a multiple of 32 KiB between 32 KiB and
1 GiB. Cache chunks bigger than 512 KiB shall be only used when
necessary.</p>

  <p>Using a chunk size that is too large can result in wasteful use of the cache,
in which small reads and writes cause large sections of an LV to be stored in
the cache. It can also require increasing migration threshold which defaults
to 2048 sectors (1 MiB). Lvm2 ensures migration threshold is at least 8
chunks in size. This may in some cases result in very high bandwidth load of
transferring data between the cache LV and its cache origin LV. However,
choosing a chunk size that is too small can result in more overhead trying to
manage the numerous chunks that become mapped into the cache. Overhead can
include both excessive CPU time searching for chunks, and excessive memory
tracking chunks.</p>
</blockquote>

<h3 id="attach-the-cache-pool-to-the-hdd-lv">Attach the cache pool to the HDD LV<a href="#attach-the-cache-pool-to-the-hdd-lv">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>Once that’s done, we can now attach the cache pool to the underlying storage to
create a cached LV:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>lvconvert <span>--type</span> cache <span>--cachepool</span> cached/example_cache cached/example
<span>Do you want wipe existing metadata of cache pool cached/example_cache? [y/n]: y
  Logical volume cached/example is now cached.
</span></code></pre></div></div>

<p>We can now see this LV:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>lvs
<span>  LV             VG             Attr       LSize   Pool                  Origin          Data%  Meta%  Move Log Cpy%Sync Convert
  example        cached         Cwi-a-C---  &lt;3.64t [example_cache_cpool] [example_corig] 0.01   0.62            0.00
</span><span>...            
</span></code></pre></div></div>

<h3 id="cache-modes">Cache modes<a href="#cache-modes">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h3>

<p>Note that there are several cache modes in LVM:</p>

<ul>
  <li>
<code>writethrough</code>: any data written to the cached LV is stored in both the cache
and the underlying block device (the default). This means that if the SSD
fails for some reason, you don’t lose your data, but it also means writes are
slower; and</li>
  <li>
<code>writeback</code>: data is written to cache, and after some unspecified delay, is
written to the underlying block device. This means that cache drive failure
can result in data loss.</li>
</ul>

<p>Basically, use <code>writethrough</code> if you want your data to survive an SSD failure,
or <code>writeback</code> if you don’t care.</p>

<p>Since I am using RAID 1 for reliability, it’d be pretty silly to then use
<code>writeback</code> and risk losing the data and creating an outage, so I kept the
default of <code>writethrough</code>.</p>

<p>To use <code>writeback</code>, you can specify <code>--cachemode writeback</code> during the initial
<code>lvconvert</code>, or use <code>sudo lvchange --cachemode writeback cached/example</code>
afterwards.</p>

<h2 id="creating-a-filesystem">Creating a filesystem<a href="#creating-a-filesystem">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>Now that the cached LV is created, we just have to create a filesystem on it and
mount it. For this exercise, we’ll use ext4, since that’s the traditional Linux
filesystem and the most well-supported. I wouldn’t recommend using something
like btrfs or ZFS since they are designed to access raw drives.</p>

<p>Creating an ext4 partition is simple:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>mkfs.ext4 /dev/cached/example
<span>mke2fs 1.47.0 (5-Feb-2023)
Discarding device blocks: done                            
Creating filesystem with 976528384 4k blocks and 244137984 inodes
Filesystem UUID: bb93c359-1915-4f09-b23f-2f3a5e8b8663
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 
	102400000, 214990848, 512000000, 550731776, 644972544

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (262144 blocks): done
Writing superblocks and filesystem accounting information: done
</span></code></pre></div></div>

<h2 id="mounting-the-new-filesystem">Mounting the new filesystem<a href="#mounting-the-new-filesystem">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>Now, we need to mount it. We can just run <code>mount</code>, but it makes more sense to
define a permanent place for it in <code>/etc/fstab</code>. For this exercise, let’s mount
in <code>/example</code>.</p>

<p>First, we create <code>/example</code>:</p>



<p>Then, we add the following line to <code>/etc/fstab</code>:</p>

<pre><code>/dev/cached/example /example ext4 rw,noatime 0 2
</code></pre>

<p>Now, let’s mount it:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>systemctl daemon-reload
<span>$</span><span> </span><span>sudo </span>mount /example
<span>$</span><span> </span><span>ls</span> /example
<span>lost+found
</span></code></pre></div></div>

<p>And there we have it. Our new cached LV is mounted on <code>/example</code>, and the
default ext4 <code>lost+found</code> directory is visible. Now you can store anything you
want in <code>/example</code>.</p>

<h2 id="monitoring">Monitoring<a href="#monitoring">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>You can find most cache metrics by running <code>lvdisplay</code> on the cached LV:</p>

<div><div><pre><code><span>$</span><span> </span><span>sudo </span>lvdisplay /dev/cached/example
<span>  --- Logical volume ---
  LV Path                /dev/cached/example
  LV Name                example
  VG Name                cached
</span><span>...
</span><span>  LV Size                &lt;3.64 TiB
  Cache used blocks      8.40%
  Cache metadata blocks  0.62%
  Cache dirty blocks     0.00%
  Cache read hits/misses 84786 / 40435
  Cache wrt hits/misses  222496 / 1883192
  Cache demotions        0
  Cache promotions       67420
  Current LE             953641
</span><span>...
</span></code></pre></div></div>

<h2 id="conclusion">Conclusion<a href="#conclusion">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<p>In the previous iteration of this before the drive failure, I was able to hit
over 95% cache hits on reads storing a mix of mirrors and LLMs, with most of the
files very infrequently read. If you have a similar workload, LVM caching is
probably highly beneficial.</p>

<p>Note that this technique doesn’t have to be used to cache HDDs. Another possible
application lies in the cloud, where you frequently have access to very large
but slow block storage over the network and fast but small local storage. You
can use LVM cache in this scenario also to cache the slower networked block
device with the local storage.</p>

<p>I hope this was helpful and you learned something about LVM. See you next time!</p>

<h2 id="notes">Notes<a href="#notes">
  <svg>
    <use xlink:href="/assets/icons-976154e0e35514ec437991e3f1d650670083e5236975a53da5adaf824f7a91df3cf1775686f47f721f31e1cb823c9d31b38f6c2e6b18656d7c757d8ec8eaa7bd.svg#link"></use>
  </svg>
</a>

  
    
  

</h2>

<!-- @formatter:off -->



  </div></div>
  </body>
</html>
