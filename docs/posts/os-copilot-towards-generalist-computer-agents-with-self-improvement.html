<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://os-copilot.github.io/">Original</a>
    <h1>OS-Copilot: Towards Generalist Computer Agents with Self-Improvement</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            
                  <p><span>
                      Zhoumianze Liu<sup>1</sup>,
                    </span>
                    <span>
                      Shunyu Yao<sup>3</sup>,
                    </span>
                    <span>
                      Tao Yu<sup>4</sup>,
                    </span>
                    <span>
                      Lingpeng Kong<sup>4</sup>
                    </span>
                  </p>
                  <p><span>
                      <sup>1</sup>Shanghai AI Lab,
                      <sup>2</sup>East China Normal University,
                      <sup>3</sup>Princeton University,
                      <sup>4</sup>University of Hong Kong</span>
                    <span><small></small></span>
                  </p>

                  
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/excel.mov"
        type="video/mp4">
      </video>

    </div>
  </div>
</section> -->
<section>  
  <div>
      
      
      <p><b>(Stay tuned! More demos are coming soon~)</b></p>  
  </div>  
</section>  

<!-- End teaser video -->

<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OSCopilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents.
          </p>
        <div>
          <p><img src="https://os-copilot.github.io/static/images/demo_4.png" alt="MY ALT TEXT"/>
          </p>
          <p><b>
            Running examples of FRIDAY when tasked with (1) preparing a focused working environment, (2) drawing a chart in Excel, and (3) creating a website for OS-Copilot. 
          </b>
        </p></div>
      </div>
    </div>
  </div>
</section>
<!-- <section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demos of FRIDAY</h2>
        <div class="content has-text-justified has-text-centered">
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            
            <source src="static/videos/demo1.mp4"
            type="video/mp4">
          </video>
          <b class="subtitle is-size-6">
            <b>Demo1: FRIDAY helps the user to adjust the system theme.</b>
          </b>
        </div>
        <div class="content has-text-justified has-text-centered">
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            
            <source src="static/videos/demo2.mp4"
            type="video/mp4">
          </video>
          <b class="subtitle is-size-6">
            <b>Demo1: FRIDAY helps the user to organize specific documents.</b>
          </b>
        </div>
        <div class="content has-text-justified has-text-centered">
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            
            <source src="static/videos/demo3.mp4"
            type="video/mp4">
          </video>
          <b class="subtitle is-size-6">
            <b>Demo1: FRIDAY helps the user to build websites.</b>
          </b>
        </div>
      </div>
    </div>
  </div>
</section> -->
<section>
  <div>
    <div>
      <div>
        <h2>The OS-Copilot Framework</h2>
        <p>
            We introduce OS-Copilot, a framework to assist building OS-level language agents, accompanied by modular implementations for each component to facilitate agent development.
          </p>
        <h2>Planner</h2>
        <p>
            <b>The planner component</b> will reason over user requests and decompose complex ones into simpler subtasks. Most importantly, the planner needs to comprehend the agent’s capabilities to generate plans at the correct granularity. To achieve this, it must retrieve relevant information about the agent’s capabilities, such as in-house tools and operating system information, to assist planning. 
          </p>

        <h2>Configurator</h2>
        <p>
            <b>The configurator component</b> takes a subtask from the planner and configures it to help the actor complete the subtask. Our design of the configurator is inspired by the biological nature of the human brain, which has working, declarative, and procedural memory.
          </p>
        <h2>Actor</h2>
        <p>
            <b>The actor comprises two stages</b>: executable action grounding and self-criticism. In the first stage, <b>the executor</b> proposes an executable action (e.g., a bash command ”mkdir new folder”) based on the configuration prompt and then executes the action in the operating system (through the Bash runtime environment in this example). <b>The critic module</b> will then access the outcomes of the execution and formulate feedback to refine execution errors and/or effect updates to the long-term memory.
          </p>
        <p>
            The image below shows the overview of the OS-Copilot Framework:
          </p>
        <div>
          <p><img src="https://os-copilot.github.io/static/images/framework.png" alt="MY ALT TEXT"/>
          </p>
          <p><b>
            An overview of OS-Copilot framework.
          </b>
        </p></div>
        
        
      </div>
    </div>
  </div>
</section>
<section>
  <div>
    <div>
      <div>
        <h2>The FRIDAY Agent</h2>
        <p>
            The design principle of FRIDAY aims to maximize generality by equipping the agent with the ability for self-refinement and self-directed learning. We first use an example to illustrate how FRIDAY operates and emphasize its capacity for self-refinement. Subsequently, we delve into how FRIDAY acquires the proficiency to control unfamiliar applications through self-directed learning.
          </p>
        <div>  
          <div>  
              <div>
                  <p><img src="https://os-copilot.github.io/static/images/working.png" alt="Image 1"/> 
                    </p> 
                  <p>(a) Configurator</p>  
              </div>
                
              <div>  
                  <p><img src="https://os-copilot.github.io/static/images/example.png" alt="Image 2"/>  
                  </p>  
                  <p>(b) A running example</p>  
              </div>  
          </div>  
      </div>  
        <h2>A Running Example</h2>
        <div>
          <p>In the figures above, we use a running example to demonstrate how FRIDAY functions within the OS.</p>
          <p>Upon receiving the subtask “Change the system into the Dark mode” (step①), the Configuration Tracker employs dense retrieval to recall relevant information from the long-term memory to construct a prompt (step②). This prompt encompasses related tools, user profiles, OS system versions, and the agent’s working directory.</p>
          <p>In this example, no suitable tools are identified (similarities below a specified threshold), prompting activation of the Tool Generator to devise an application-tailored tool for the current subtask (step③). As we can see from Figure (b), the generated tool manifests as a Python class utilizing AppleScript to change systems to dark mode.</p>
          <p>Subsequently, with the tool created and the configuration prompt finalized, the Executor processes the prompt, generates an executable action, and executes it (step④). As shown in the bottom of Figure (b), the executor first stores the tool code into a Python file and then executes the code in the command-line terminal.</p>
          <p>After execution, the critic evaluates whether the subtask is successfully completed (step⑤). Upon success, the critic assigns a score (using LLMs) ranging from 0 to 10 to the generated tool, with a higher score indicating greater potential for future reuse. In the current implementation, tools scoring above 8 are preserved by updating the tool repository in procedural memory (step ⑦).</p>
          <p>However, in the event of a failed execution, the refiner collects feedback from the critic and initiates self-correction (step⑥) of the responsible action, tool, or subtask . The FRIDAY will iterate through steps ④ to ⑥ until the subtask is considered completed or a maximum of three attempts is reached.</p>
        </div>
        
        <!-- <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/planner.png" alt="MY ALT TEXT"/>
          </div>
          <div class="subtitle is-size-6">
            A running example of the planner, where each node represents a subtask. The numbers are solely for illustrative purposes and do not signify the execution order of subtasks.
          </div>
        </div> -->
        <h2>Self-Directed Learning</h2>
        <div>
          <p>Self-directed learning is a crucial ability for humans to acquire information and learn new skills, and it has demonstrated promising results in embodied agents within Minecraft games.</p>
          <p>With a pre-defined learning objective, such as mastering spreadsheet manipulation, FRIDAY is prompted to propose a continuous stream of tasks related to the objective, spanning from easy to challenging. FRIDAY then follows this curriculum, resolving these tasks through trial and error, thereby accumulating valuable tools and semantic knowledge throughout the process. Despite its simple design, our evaluation indicates that self-directed learning is crucial for a generalpurpose OS-level agent.</p>
        </div>
        <!-- <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/working.png" alt="MY ALT TEXT"/>
          </div>
          <div class="subtitle is-size-6">
            The architecture of working memory, with solid lines indicating message passing and dotted lines representing operations.
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            As shown in the above image, upon receiving a subtask from the planner, FRIDAY’s Configuration Tracker module first compiles a configuration file, encompassing the task description, environmental states, and tool descriptions. If the construction of the config file is successful, it is then forwarded to the Executor for action generation and execution. In the event of execution failure, the Refiner module gathers feedback from the Critic and refines the responsible module through the use of the Refiner.
          </p>
        </div> -->
      </div>
      </div>
    </div>
  
</section>
<section>
  <div>
    <div>
      <div>
        <h2>Experiments</h2>
        <h2>Main Results</h2>
        <p>
            We evaluate FRIDAY on GAIA, a benchmark for general AI assistants featuring 466 challenging question-answering tasks. To answer questions in GAIA, language agents need skills to calculate, browse the web, handle multi-modality, and manipulate files, etc.
          </p>
        <div>
          <p><img src="https://os-copilot.github.io/static/images/gaia_result.png" width="600" alt="MY ALT TEXT"/>
          </p>
          <p><b>Evaluation Results</b>.All results are reported on the private test set, except for the Human score, which is averaged across the dev and test sets.
          </p>
        </div>
      
        <!-- <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/per_domain.jpg" width="400" alt="MY ALT TEXT"/>
          </div>
          <div class="subtitle is-size-6">
            <b>Scores (%) of FRIDAY on level-1 tasks per capability.</b>Numbers except FRIDAY are sourced from GAIA paper. As confirmed by GAIA’s authors, there are some numerical errors in GAIA’s Figure 5, so we omit the comparison with baselines here.
          </div>
        </div> -->
        <h2>Self-directed Learning</h2>
        <p>
            We perform quantitative and qualitative evaluations to analysis FRIDAY’s self-directed learning capability.
          </p>
        <h2>QUANTITATIVE ANALYSIS</h2>
        <p>
            To showcase FRIDAY’s ability to master unfamiliar applications through self-learning, we conduct experiments on the SheetCopilot-20 dataset.This dataset includes 20 spreadsheet control tasks, covering various operations such as Formatting, Management, Charts, Pivot Tables, and Formulas, representing typical use cases of spreadsheets.
          </p>
        <div>
          <p><img src="https://os-copilot.github.io/static/images/quantitative_study.png" width="550" alt="MY ALT TEXT"/>
          </p>
          <p><b>Comparison of different agents on the SheetCopilot-20 dataset.</b>Pass@1 refers to the pass rate with each task being performed only once.We highlight the best results in bold.
          </p>
        </div>
        <h2>QUALITATIVE ANALYSIS</h2>
        <div>  
          <div>  
              <div>
                  <p><img src="https://os-copilot.github.io/static/images/ppt0.png" alt="Image 1"/> 
                    </p> 
                  <p>(a) FRIDAY <b>w/o self-directed learning.</b></p>  
              </div>
              <div>
                <p><img src="https://os-copilot.github.io/static/images/ppt1.png" alt="Image 1"/> 
                </p> 
              <p>(b) FRIDAY <b>after learning text box control.</b></p>  
              </div>  
              <div>  
                  <p><img src="https://os-copilot.github.io/static/images/ppt2.png" alt="Image 2"/>  
                  </p>  
                  <p>(c) FRIDAY <b>after mastering image insertion.</b></p>  
              </div>  
          </div>  
        <p>
            In our qualitative analysis, we design a task to create a PowerPoint slide to introduce OS-Copilot. The specific content, font, font size, and other details required for the slide are elaborately described in the task instruction.
          </p>
        <p>
            The experimental results, as shown in Figure (a), demonstrate that without self-directed learning, FRIDAY struggles to effectively control font types, sizes, and the positioning and sizing of inserted images.
          </p>
        <!-- <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/ppt0.png"  alt="MY ALT TEXT"/>
            <div class="subtitle is-size-6">
              FRIDAY <b>w/o self-directed learning.</b>
            </div>
          </div> -->
          
        </div>
        <p>
            Nevertheless, following a period of self-directed learning, FRIDAY acquires various text box configuration tools, such as changing the text color, adjusting the font size of slide text, and modifying the line spacing of body text in PowerPoint presentations, as illustrated in Figure (b).
          </p>
        <!-- <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/ppt1.png"  alt="MY ALT TEXT"/>
            <div class="subtitle is-size-6">
              FRIDAY <b>after learning text box control.</b>
            </div>
          </div>
          
        </div> -->
        <p>
            Further exploration leads FRIDAY to learn how to adjust the size and position of inserted images, ultimately successfully completing the task, as depicted in Figure (c).
          </p>
        <!-- <div class="content">
          <div class=" has-text-centered">
            <img src="static/images/ppt2.png"  alt="MY ALT TEXT"/>
            <div class="subtitle is-size-6">
              FRIDAY <b>after mastering image insertion.</b>
            </div>
          </div>
          
        </div> -->

      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        Your image here
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        Your image here
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        Your image here
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
       Your image here
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
       Paper video.
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            Youtube embed code here
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>
        
        @misc{wu2024oscopilot,
          title={OS-Copilot: Towards Generalist Computer Agents with Self-Improvement}, 
          author={Zhiyong Wu and Chengcheng Han and Zichen Ding and Zhenmin Weng and Zhoumianze Liu and Shunyu Yao and Tao Yu and Lingpeng Kong},
          year={2024},
          eprint={2402.07456},
          archivePrefix={arXiv},
          primaryClass={cs.AI}
        }

      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
