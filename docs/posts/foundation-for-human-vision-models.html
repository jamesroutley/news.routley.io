<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/sapiens">Original</a>
    <h1>Foundation for Human Vision Models</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/sapiens/blob/main/assets/sapiens_animation.gif"><img src="https://github.com/facebookresearch/sapiens/raw/main/assets/sapiens_animation.gif" alt="Sapiens" title="Sapiens" width="500" data-animated-image=""/></a>
</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Foundation for Human Vision Models</h2><a id="user-content-foundation-for-human-vision-models" aria-label="Permalink: Foundation for Human Vision Models" href="#foundation-for-human-vision-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
   <p dir="auto">
      <a href="https://rawalkhirodkar.github.io/" rel="nofollow"><strong>Rawal Khirodkar</strong></a>
      ·
      <a href="https://scholar.google.ch/citations?user=oLi7xJ0AAAAJ&amp;hl=en" rel="nofollow"><strong>Timur Bagautdinov</strong></a>
      ·
      <a href="https://una-dinosauria.github.io/" rel="nofollow"><strong>Julieta Martinez</strong></a>
      ·
      <a href="https://about.meta.com/realitylabs/" rel="nofollow"><strong>Su Zhaoen</strong></a>
      ·
      <a href="https://about.meta.com/realitylabs/" rel="nofollow"><strong>Austin James</strong></a>
      </p>
   

<p dir="auto">
   <a href="https://arxiv.org/abs/2408.12569" rel="nofollow">
      <img src="https://camo.githubusercontent.com/2ac6ce7ffd880dda20eeedb377acaf9fa20c717fb57fbf24aee525726c78f273/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d5044462d677265656e3f7374796c653d666f722d7468652d6261646765266c6f676f3d61646f62656163726f626174726561646572266c6f676f57696474683d3230266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d36366363303026636f6c6f723d393444443135" alt="Paper PDF" data-canonical-src="https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&amp;logo=adobeacrobatreader&amp;logoWidth=20&amp;logoColor=white&amp;labelColor=66cc00&amp;color=94DD15"/>
   </a>
   <a href="https://about.meta.com/realitylabs/codecavatars/sapiens/" rel="nofollow">
      <img src="https://camo.githubusercontent.com/405c0d77846176afa433d7eea8735e9e6b0b07a6f4bb710f3038cbd8d2b88de4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53617069656e732d506167652d6f72616e67653f7374796c653d666f722d7468652d6261646765266c6f676f3d476f6f676c652532306368726f6d65266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d443335343030" alt="Project Page" data-canonical-src="https://img.shields.io/badge/Sapiens-Page-orange?style=for-the-badge&amp;logo=Google%20chrome&amp;logoColor=white&amp;labelColor=D35400"/>
   </a>
</p>
<p dir="auto">Sapiens offers a comprehensive suite for human-centric vision tasks (e.g., 2D pose, part segmentation, depth, normal, etc.). The model family is pretrained on 300 million in-the-wild human images and shows excellent generalization to unconstrained conditions. These models are also designed for extracting high-resolution features, having been natively trained at a 1024 x 1024 image resolution with a 16-pixel patch size.</p>


<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:facebookresearch/sapiens.git
export SAPIENS_ROOT=/path/to/sapiens"><pre>git clone git@github.com:facebookresearch/sapiens.git
<span>export</span> SAPIENS_ROOT=/path/to/sapiens</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Recommended: Lite Installation (Inference-only)</h3><a id="user-content-recommended-lite-installation-inference-only" aria-label="Permalink: Recommended: Lite Installation (Inference-only)" href="#recommended-lite-installation-inference-only"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For users setting up their own environment primarily for running existing models in inference mode, we recommend the <a href="https://github.com/facebookresearch/sapiens/blob/main/lite/README.md">Sapiens-Lite installation</a>.</p>

<p dir="auto">To replicate our complete training setup, run the provided installation script. </p>
<div dir="auto" data-snippet-clipboard-copy-content="cd $SAPIENS_ROOT/_install
./conda.sh"><pre><span>cd</span> <span>$SAPIENS_ROOT</span>/_install
./conda.sh</pre></div>
<p dir="auto">Please download the checkpoints from <a href="https://huggingface.co/facebook/sapiens/tree/main/sapiens_host" rel="nofollow">hugging-face</a>. </p>
<div data-snippet-clipboard-copy-content="sapiens_host/
├── detector/
│   └── checkpoints/
│       └── rtmpose/
├── pretrain/
│   └── checkpoints/
│       ├── sapiens_0.3b/
│       ├── sapiens_0.6b/
│       ├── sapiens_1b/
│       └── sapiens_2b/
├── pose/
└── seg/
└── depth/
└── normal/"><pre lang="plaintext"><code>sapiens_host/
├── detector/
│   └── checkpoints/
│       └── rtmpose/
├── pretrain/
│   └── checkpoints/
│       ├── sapiens_0.3b/
│       ├── sapiens_0.6b/
│       ├── sapiens_1b/
│       └── sapiens_2b/
├── pose/
└── seg/
└── depth/
└── normal/
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">🌟 Human-Centric Vision Tasks</h2><a id="user-content--human-centric-vision-tasks" aria-label="Permalink: 🌟 Human-Centric Vision Tasks" href="#-human-centric-vision-tasks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We finetune sapiens for multiple human-centric vision tasks. Please checkout the list below.</p>
<ul dir="auto">
<li>

</li>
<li>

</li>
<li>

</li>
<li>

</li>
<li>

</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">🎯 Easy Steps to Finetuning Sapiens</h2><a id="user-content--easy-steps-to-finetuning-sapiens" aria-label="Permalink: 🎯 Easy Steps to Finetuning Sapiens" href="#-easy-steps-to-finetuning-sapiens"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Finetuning our models is super-easy! Here is a detailed training guide for the following tasks.</p>
<ul dir="auto">
<li>
<div dir="auto"><h3 tabindex="-1" dir="auto">[Coming Soon] Pose/Seg/Depth</h3><a id="user-content-coming-soon-posesegdepth" aria-label="Permalink: [Coming Soon] Pose/Seg/Depth" href="#coming-soon-posesegdepth"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</li>
<li>

</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">🤝 Acknowledgements &amp; Support &amp; Contributing</h2><a id="user-content--acknowledgements--support--contributing" aria-label="Permalink: 🤝 Acknowledgements &amp; Support &amp; Contributing" href="#-acknowledgements--support--contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We would like to acknowledge the work by <a href="https://github.com/open-mmlab">OpenMMLab</a> which this project benefits from.</p>

<p dir="auto">This project is licensed under <a href="https://github.com/facebookresearch/sapiens/blob/main/LICENSE">LICENSE</a>.</p>

<p dir="auto">If you use Sapiens in your research, please use the following BibTeX entry.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{khirodkar2024_sapiens,
    title={Sapiens: Foundation for Human Vision Models},
    author={Khirodkar, Rawal and Bagautdinov, Timur and Martinez, Julieta and Zhaoen, Su and James, Austin and Selednik, Peter and Anderson, Stuart and Saito, Shunsuke},
    year={2024},
    eprint={2408.12569},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2408.12569}
}"><pre><span>@misc</span>{<span>khirodkar2024_sapiens</span>,
    <span>title</span>=<span><span>{</span>Sapiens: Foundation for Human Vision Models<span>}</span></span>,
    <span>author</span>=<span><span>{</span>Khirodkar, Rawal and Bagautdinov, Timur and Martinez, Julieta and Zhaoen, Su and James, Austin and Selednik, Peter and Anderson, Stuart and Saito, Shunsuke<span>}</span></span>,
    <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
    <span>eprint</span>=<span><span>{</span>2408.12569<span>}</span></span>,
    <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
    <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>,
    <span>url</span>=<span><span>{</span>https://arxiv.org/abs/2408.12569<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
