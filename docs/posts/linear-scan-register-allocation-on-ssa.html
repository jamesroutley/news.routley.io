<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bernsteinbear.com/blog/linear-scan/?utm_source=rss">Original</a>
    <h1>Linear scan register allocation on SSA</h1>
    
    <div id="readability-page-1" class="page"><div>
            <p><em>Much of the code and education that resulted in this post happened with <a href="https://tenderlovemaking.com/">Aaron
Patterson</a>.</em></p>

<p>The fundamental problem in register allocation is to take an IR that uses a
virtual registers (as many as you like) and rewrite it to use a finite amount
of physical registers and stack space<sup id="fnref:calendaring" role="doc-noteref"><a href="#fn:calendaring" rel="footnote">1</a></sup>.</p>

<p>This is an example of a code snippet using virtual registers:</p>

<div><div><pre><code>add R1, R2 -&gt; R3
add R1, R3 -&gt; R4
ret R4
</code></pre></div></div>

<p>And here is the same example after it has been passed through a register
allocator (note that Rs changed to Ps):</p>

<div><div><pre><code>add Stack[0], P0 -&gt; P1
add Stack[0], P1 -&gt; P0
ret
</code></pre></div></div>

<p>Each virtual register was assigned a physical place: R1 to the stack, R2 to P0,
R3 to P1, and R4 <em>also</em> to P0 (since we weren’t using R2 anymore).</p>

<p>People use register allocators like they use garbage collectors: it’s an
abstraction that can manage your resources for you, maybe with some cost. When
writing the back-end of a compiler, it’s probably much easier to have a
separate register-allocator-in-a-box than manually managing variable lifetimes
while also considering all of your different target architectures.</p>

<p>How do JIT compilers do register allocation? Well, “everyone knows” that “every
JIT does its own variant of linear scan”<sup id="fnref:everyone" role="doc-noteref"><a href="#fn:everyone" rel="footnote">2</a></sup>. This bothered me for some
time because I’ve worked on a couple of JITs and still didn’t understand the
backend bits.</p>

<p>There are a couple different approaches to register allocation, but in this
post we’ll focus on <em>linear scan of SSA</em>.</p>

<p>I started reading <a href="https://gmays.com/assets/img/wimmer-linear-scan-ssa.pdf">Linear Scan Register Allocation on SSA Form</a> (PDF,
2010) by Wimmer and Franz after writing <a href="https://gmays.com/blog/ssa/">A catalog of ways to generate
SSA</a>. Reading alone didn’t make a ton of sense—I ended up with a
lot of very frustrated margin notes. I started trying to implement it alongside
the paper. As it turns out, though, there is a rich history of papers in this
area that it leans on really heavily. I needed to follow the chain of
references!</p>

<blockquote>
  <p>For example, here is a lovely explanation of the process, start to finish,
from Christian Wimmer’s <a href="https://gmays.com/assets/img/wimmer-masters-thesis.pdf">Master’s thesis</a> (PDF, 2004).</p>

  <div><div><pre><code>LINEAR_SCAN
  // order blocks and operations (including loop detection)
  COMPUTE_BLOCK_ORDER
  NUMBER_OPERATIONS
  // create intervals with live ranges
  COMPUTE_LOCAL_LIVE_SETS
  COMPUTE_GLOBAL_LIVE_SETS
  BUILD_INTERVALS
  // allocate registers
  WALK_INTERVALS
  RESOLVE_DATA_FLOW
  // replace virtual registers with physical registers
  ASSIGN_REG_NUM
  // special handling for the Intel FPU stack
  ALLOCATE_FPU_STACK
</code></pre></div>  </div>

  <p>There it is, all laid out at once. It’s very refreshing when compared to all
of the compact research papers.</p>
</blockquote>

<p>I didn’t realize that there were more than one or two papers on linear scan. So
this post will also incidentally serve as a bit of a survey or a history of
linear scan—as best as I can figure it out, anyway. If you were in or near
the room where it happened, please feel free to reach out and correct some
parts.</p>

<h2 id="some-example-code">Some example code</h2>

<p>Throughout this post, we’ll use an example SSA code snippet from Wimmer2010,
adapted from phi-SSA to block-argument-SSA. Wimmer2010’s code snippet is
between the arrows and we add some filler (as alluded to in the paper):</p>

<div><div><pre><code>label B1(R10, R11):
jmp B2($1, R11)
 # vvvvvvvvvv #
label B2(R12, R13)
cmp R13, $1
branch lessThan B4()

label B3()
mul R12, R13 -&gt; R14
sub R13, $1 -&gt; R15
jump B2(R14, R15)

label B4()
 # ^^^^^^^^^^ #
add R10, R12 -&gt; R16
ret R16
</code></pre></div></div>

<p>Virtual registers start with R and are defined either with an arrow or by a
block parameter.</p>

<p>Because it takes a moment to untangle the unfamiliar syntax and draw the
control-flow graph by hand, I’ve also provided the same code in graphical form.
Block names (and block parameters) are shaded with grey.</p>

<!--
digraph G {
node [shape=plaintext]
B1 [label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
<TR><TD PORT="params" BGCOLOR="lightgray">B1(R10, R11) </TD></TR>
<TR><TD ALIGN="left" PORT="0">jump →B2($1, R11) </TD></TR>
</TABLE>>];
B1:0 -> B2:params;
B2 [label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
<TR><TD PORT="params" BGCOLOR="lightgray">B2(R12, R13) </TD></TR>
<TR><TD ALIGN="left" PORT="0">cmp R13, $1 </TD></TR>
<TR><TD ALIGN="left" PORT="1">blt →B4, →B3 </TD></TR>
</TABLE>>];
B2:1 -> B4:params;
B2:1 -> B3:params;
B3 [label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
<TR><TD PORT="params" BGCOLOR="lightgray">B3() </TD></TR>
<TR><TD ALIGN="left" PORT="0">R14 = mul R12, R13 </TD></TR>
<TR><TD ALIGN="left" PORT="1">R15 = sub R13, $1 </TD></TR>
<TR><TD ALIGN="left" PORT="2">jump →B2(R14, R15) </TD></TR>
</TABLE>>];
B3:2 -> B2:params;
B4 [label=<<TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
<TR><TD PORT="params" BGCOLOR="lightgray">B4() </TD></TR>
<TR><TD ALIGN="left" PORT="0">R16 = add R10, R12 </TD></TR>
<TR><TD ALIGN="left" PORT="1">ret R16 </TD></TR>
</TABLE>>];
}
-->
<figure>

<figcaption>
    <p>We have one entry block, <code>B1</code>, that is implied in
Wimmer2010. Its only job is to define <code>R10</code> and <code>R11</code> for the rest of the CFG.</p>

    <p>Then we have a loop between <code>B2</code> and <code>B3</code> with an implicit fallthrough. Instead
of doing that, we instead generate a conditional branch with explicit jump
targets. This makes it possible to re-order blocks as much as we like.</p>

    <p>The contents of <code>B4</code> are also just to fill in the blanks from Wimmer2010 and
add some variable uses.</p>
  </figcaption>
</figure>

<p>Our goal for the post is to analyze this CFG, assign physical locations
(registers or stack slots) to each virtual register, and then rewrite the code
appropriately.</p>

<p>For now, let’s rewind the clock and look at how linear scan came about.</p>

<h2 id="in-the-beginning">In the beginning</h2>

<p>Linear scan register allocation (LSRA) has been around for awhile. It’s neat
because it does the actual register assignment part of register allocation in
one pass over your low-level IR. (We’ll talk more about what that means in a
minute.)</p>

<p>It first appeared in the literature in <a href="https://gmays.com/assets/img/tcc-linearscan-ra.pdf">tcc: A System for Fast, Flexible, and
High-level Dynamic Code Generation</a> (PDF, 1997) by Poletto, Engler,
and Kaashoek. (Until writing this post, I had never seen this paper. It was
only on a re-read of the 1999 paper (below) that I noticed it.) In this paper,
they mostly describe a staged variant of C called ‘C (TickC), for which a fast
register allocator is quite useful.</p>

<p>Then came a paper called <a href="https://gmays.com/assets/img/quality-speed-linear-scan-ra-clean.pdf">Quality and Speed in Linear-scan Register
Allocation</a> (PDF, 1998) by Traub, Holloway, and Smith. It adds
some optimizations (lifetime holes, binpacking) to the algorithm presented in
Poletto1997.</p>

<p>Then came the first paper I read, and I think the paper everyone refers to when
they talk about linear scan: <a href="https://gmays.com/assets/img/linearscan-ra.pdf">Linear Scan Register Allocation</a> (PDF,
1999) by Poletto and Sarkar. In this paper, they give a fast alternative to
graph coloring register allocation, especially motivated by just-in-time
compilers. In retrospect, it seems to be a bit of a rehash of the previous two
papers.</p>

<p>Linear scan (1997, 1999) operates on <em>live ranges</em> instead of virtual
registers. A live range is a pair of integers [start, end) (end is exclusive)
that begins when the register is defined and ends when it is last used. This
means that there is an assumption that the order for instructions in your
program has already been fixed into a single linear sequence! It also means
that you have given each instruction a number that represents its position in
that order.</p>

<blockquote>
  <p>This may or not be a surprising requirement depending on your compilers
background. It was surprising to me because I often live in control flow
graph fantasy land where blocks are unordered and instructions sometimes
float around. But if you live in a land of basic blocks that are already in
reverse post order, then it may be less surprising.</p>
</blockquote>

<p>In non-SSA-land, these live ranges are different from the virtual registers:
they represent some kind of lifetimes of each <em>version</em> of a virtual register.
For an example, consider the following code snippet:</p>

<div><div><pre><code>...      -&gt; a
add 1, a -&gt; b
add 1, b -&gt; c
add 1, c -&gt; a
add 1, a -&gt; d
</code></pre></div></div>

<p>There are two definitions of <code>a</code> and they each live for different amounts of
time:</p>

<div><div><pre><code>                  a  b  c  a  d
...      -&gt; a     |                &lt;- the first a
add 1, a -&gt; b     v  |
add 1, b -&gt; c        v  |
add 1, c -&gt; a           v  |       &lt;- the second a
add 1, a -&gt; d              v  |
</code></pre></div></div>

<p>In fact, the ranges are completely disjoint. It wouldn’t make sense for the
register allocator to consider variables, because there’s no reason the two
<code>a</code>s should necessarily live in the same physical register.</p>

<p>In SSA land, it’s a little different: since each virtual registers only has one
definition (by, uh, definition), live ranges are an exact 1:1 mapping with
virtual registers. <strong>We’ll focus on SSA for the remainder of the post because
this is what I am currently interested in.</strong> The research community seems to
have decided that allocating directly on SSA gives more information to the
register allocator<sup id="fnref:allocate-on-ssa" role="doc-noteref"><a href="#fn:allocate-on-ssa" rel="footnote">3</a></sup>.</p>

<p>Linear scan starts at the point in your compiler process where you already know
these live ranges—that you have already done some kind of analysis to build a
mapping.</p>

<p>In this blog post, we’re going to back up to the point where we’ve just built
our SSA low-level IR and have yet to do any work on it. We’ll do all of the
analysis from scratch.</p>

<p>Part of this analysis is called <em>liveness analysis</em>.</p>

<h2 id="liveness-analysis">Liveness analysis</h2>

<p>The result of liveness analysis is a mapping of <code>BasicBlock -&gt;
Set[Instruction]</code> that tells you which virtual registers (remember, since we’re
in SSA, instruction==vreg) are alive (used later) at the beginning of the basic
block. This is called a <em>live-in</em> set. For example:</p>

<div><div><pre><code>B0:
... -&gt; R12
... -&gt; R13
jmp B1

B1:
mul R12, R13 -&gt; R14
sub R13, 1 -&gt; R15
jmp B2

B2:
add R14, R15 -&gt; R16
ret R16
</code></pre></div></div>

<p>We compute liveness by working backwards: a variable is <em>live</em> from the moment
it is backwardly-first used until its definition.</p>

<p>In this case, at the end of B2, nothing is live. If we step backwards to the
<code>ret</code>, we see a use: R16 becomes live. If we step once more, we see its
definition—R16 no longer live—but now we see a use of R14 and R15, which
become live. This leaves us with R14 and R15 being <em>live-in</em> to B2.</p>

<p>This live-in set becomes B1’s <em>live-out</em> set because B1 is B2’s predecessor. We
continue in B1. We could continue backwards linearly through the blocks. In
fact, I encourage you to do it as an exercise. You should have a (potentially
emtpy) set of registers per basic block.</p>

<p>It gets more interesting, though, when we have branches: what does it mean when
two blocks’ live-in results merge into their shared predecessor? If we have two
blocks A and B that are successors of a block C, the live-in sets get
<em>unioned</em> together.</p>

<!--
digraph G {
  node [shape=square];
  C -> A;
  C -> B;
}
-->
<figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="98pt" height="116pt" viewBox="0.00 0.00 98.00 116.00">
<g id="graph0" transform="scale(1 1) rotate(0) translate(4 112)">
<title>G</title>
<polygon fill="white" stroke="none" points="-4,4 -4,-112 94,-112 94,4 -4,4"></polygon>
<!-- C -->
<g id="node1">
<title>C</title>
<polygon fill="none" stroke="black" points="63,-108 27,-108 27,-72 63,-72 63,-108"></polygon>
<text text-anchor="middle" x="45" y="-85.8" font-family="Times,serif" font-size="14.00">C</text>
</g>
<!-- A -->
<g id="node2">
<title>A</title>
<polygon fill="none" stroke="black" points="36,-36 0,-36 0,0 36,0 36,-36"></polygon>
<text text-anchor="middle" x="18" y="-13.8" font-family="Times,serif" font-size="14.00">A</text>
</g>
<!-- C->A -->
<g id="edge1">
<title>C-&gt;A</title>
<path fill="none" stroke="black" d="M38.33,-71.7C35.42,-64.15 31.93,-55.12 28.68,-46.68"></path>
<polygon fill="black" stroke="black" points="32.01,-45.59 25.14,-37.52 25.48,-48.11 32.01,-45.59"></polygon>
</g>
<!-- B -->
<g id="node3">
<title>B</title>
<polygon fill="none" stroke="black" points="90,-36 54,-36 54,0 90,0 90,-36"></polygon>
<text text-anchor="middle" x="72" y="-13.8" font-family="Times,serif" font-size="14.00">B</text>
</g>
<!-- C->B -->
<g id="edge2">
<title>C-&gt;B</title>
<path fill="none" stroke="black" d="M51.67,-71.7C54.58,-64.15 58.07,-55.12 61.32,-46.68"></path>
<polygon fill="black" stroke="black" points="64.52,-48.11 64.86,-37.52 57.99,-45.59 64.52,-48.11"></polygon>
</g>
</g>
</svg>
</figure>

<p>That is, if there were some register R0 live-in to B and some register R1
live-in to A, both R0 and R1 would be live-out of C. They may also be live-in
to C, but that entirely depends on the contents of C.</p>

<p>Since the total number of virtual registers is nonnegative and is finite for a
given program, it seems like a good lattice for an <em>abstract interpreter</em>.
That’s right, we’re doing AI.</p>

<p>In this liveness analysis, we’ll:</p>

<ol>
  <li>compute a summary of what virtual registers each basic block needs to be
alive (gen set) and what variables it defines (kill set)</li>
  <li>initialize all live-in sets to 0</li>
  <li>do an iterative dataflow analysis over the blocks until the live-in sets
converge</li>
</ol>

<p>We store gen, kill, and live-in sets as bitsets, using some APIs conveniently
available on Ruby’s Integer class.</p>

<p>Like most abstract interpretations, it doesn’t matter what order we iterate
over the collection of basic blocks for correctness, but it <em>does</em> matter for
performance. In this case, iterating backwards (<code>post_order</code>) converges much
faster than forwards (<code>reverse_post_order</code>):</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>compute_initial_liveness_sets</span> <span>order</span>
    <span># Map of Block -&gt; what variables it alone needs to be live-in</span>
    <span>gen</span> <span>=</span> <span>Hash</span><span>.</span><span>new</span> <span>0</span>
    <span># Map of Block -&gt; what variables it alone defines</span>
    <span>kill</span> <span>=</span> <span>Hash</span><span>.</span><span>new</span> <span>0</span>
    <span>order</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>block</span><span>|</span>
      <span>block</span><span>.</span><span>instructions</span><span>.</span><span>reverse_each</span> <span>do</span> <span>|</span><span>insn</span><span>|</span>
        <span>out</span> <span>=</span> <span>insn</span><span>.</span><span>out</span><span>&amp;</span><span>.</span><span>as_vreg</span>
        <span>if</span> <span>out</span>
          <span>kill</span><span>[</span><span>block</span><span>]</span> <span>|=</span> <span>(</span><span>1</span> <span>&lt;&lt;</span> <span>out</span><span>.</span><span>num</span><span>)</span>
        <span>end</span>
        <span>insn</span><span>.</span><span>vreg_ins</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>vreg</span><span>|</span>
          <span>gen</span><span>[</span><span>block</span><span>]</span> <span>|=</span> <span>(</span><span>1</span> <span>&lt;&lt;</span> <span>vreg</span><span>.</span><span>num</span><span>)</span>
        <span>end</span>
      <span>end</span>
      <span>block</span><span>.</span><span>parameters</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>param</span><span>|</span>
        <span>kill</span><span>[</span><span>block</span><span>]</span> <span>|=</span> <span>(</span><span>1</span> <span>&lt;&lt;</span> <span>param</span><span>.</span><span>num</span><span>)</span>
      <span>end</span>
    <span>end</span>
    <span>[</span><span>gen</span><span>,</span> <span>kill</span><span>]</span>
  <span>end</span>

  <span>def</span> <span>analyze_liveness</span>
    <span>order</span> <span>=</span> <span>post_order</span>
    <span>gen</span><span>,</span> <span>kill</span> <span>=</span> <span>compute_initial_liveness_sets</span><span>(</span><span>order</span><span>)</span>
    <span># Map from Block -&gt; what variables are live-in</span>
    <span>live_in</span> <span>=</span> <span>Hash</span><span>.</span><span>new</span> <span>0</span>
    <span>changed</span> <span>=</span> <span>true</span>
    <span>while</span> <span>changed</span>
      <span>changed</span> <span>=</span> <span>false</span>
      <span>for</span> <span>block</span> <span>in</span> <span>order</span>
        <span># Union-ing all the successors&#39; live-in sets gives us this block&#39;s</span>
        <span># live-out, which is a good starting point for computing the live-in</span>
        <span>block_live</span> <span>=</span> <span>block</span><span>.</span><span>successors</span><span>.</span><span>map</span> <span>{</span> <span>|</span><span>succ</span><span>|</span> <span>live_in</span><span>[</span><span>succ</span><span>]</span> <span>}.</span><span>reduce</span><span>(</span><span>0</span><span>,</span> <span>:|</span><span>)</span>
        <span>block_live</span> <span>|=</span> <span>gen</span><span>[</span><span>block</span><span>]</span>
        <span>block_live</span> <span>&amp;=</span> <span>~</span><span>kill</span><span>[</span><span>block</span><span>]</span>
        <span>if</span> <span>live_in</span><span>[</span><span>block</span><span>]</span> <span>!=</span> <span>block_live</span>
          <span>changed</span> <span>=</span> <span>true</span>
          <span>live_in</span><span>[</span><span>block</span><span>]</span> <span>=</span> <span>block_live</span>
        <span>end</span>
      <span>end</span>
    <span>end</span>
    <span>live_in</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>We could also use a worklist here, and it would be faster, but eh. Repeatedly
iterating over all blocks is fine for now.</p>

<p>The Wimmer2010 paper skips this liveness analysis entirely by assuming some
computed information about your CFG: where loops start and end. It also
requires all loop blocks be contiguous. Then it makes variables defined before
a loop and used at any point inside the loop live <em>for the whole loop</em>. By
having this information available, it folds the liveness analysis into the live
range building, which we’ll instead do separately in a moment.</p>

<p>The Wimmer approach sounded complicated and finicky. Maybe it is, maybe it
isn’t. So I went with a dataflow liveness analysis instead. If it turns out to
be the slow part, maybe it will matter enough to learn about this loop tagging
method.</p>

<p>For now, we will pick a <em>schedule</em> for the control-flow graph.</p>

<h2 id="scheduling">Scheduling</h2>

<p>In order to build live ranges, you have to have some kind of numbering system
for your instructions, otherwise a live range’s start and end are meaningless.
We can write a function that fixes a particular block order (in this case,
reverse post-order) and then assigns each block and instruction a number in a
linear sequence. You can think of this as flattening or projecting the graph:</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>number_instructions!</span>
    <span>@block_order</span> <span>=</span> <span>rpo</span>
    <span>number</span> <span>=</span> <span>16</span>  <span># just so we match the Wimmer2010 paper</span>
    <span>@block_order</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>blk</span><span>|</span>
      <span>blk</span><span>.</span><span>number</span> <span>=</span> <span>number</span>
      <span>number</span> <span>+=</span> <span>2</span>
      <span>blk</span><span>.</span><span>instructions</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>insn</span><span>|</span>
        <span>insn</span><span>.</span><span>number</span> <span>=</span> <span>number</span>
        <span>number</span> <span>+=</span> <span>2</span>
      <span>end</span>
      <span>blk</span><span>.</span><span>to</span> <span>=</span> <span>number</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>A couple interesting things to note:</p>

<ul>
  <li>We number blocks because we use block starts as the start index for all of
that block’s parameters</li>
  <li>We start numbering at 16 just so we can eyeball things and make sure they
line up with the Wimmer2010 paper</li>
  <li>We only give out even numbers because later we’ll insert loads and stores at
odd-numbered instructions
    <ul>
      <li>Cinder does this to <a href="https://github.com/facebookincubator/cinderx/blob/2b8774f077d6ef441207067411d157bb4f94a40b/cinderx/Jit/lir/regalloc.cpp#L243">separately identify instruction input and instruction output</a></li>
    </ul>
  </li>
</ul>

<p>Even though we have extra instructions, it looks very similar to the example in
the Wimmer2010 paper.</p>

<div><div><pre><code>16: label B1(R10, R11):
18: jmp B2($1, R11)
     # vvvvvvvvvv #
20: label B2(R12, R13)
22: cmp R13, $1
24: branch lessThan B4()

26: label B3()
28: mul R12, R13 -&gt; R14
30: sub R13, $1 -&gt; R15
32: jump B2(R14, R15)

34: label B4()
     # ^^^^^^^^^^ #
36: add R10, R12 -&gt; R16
38: ret R16
</code></pre></div></div>

<p>Since we’re not going to be messing with the order of the instructions within a
block anymore, all we have to do going forward is make sure that we iterate
through the blocks in <code>@block_order</code>.</p>

<p>Finally, we have all that we need to compute live ranges.</p>

<h2 id="live-ranges">Live ranges</h2>

<p>We’ll more or less copy the algorithm to compute live ranges from the
Wimmer2010 paper. We’ll have two main differences:</p>

<ul>
  <li>We’re going to compute live ranges, not live intervals (as they do in the
paper)</li>
  <li>We’re going to use our dataflow liveness analysis, not the loop header thing</li>
</ul>

<p>I know I said we were going to be computing live ranges. So why am I presenting
you with a function called <code>build_intervals</code>? That’s because early in
the history of linear scan (Traub1998!), people moved from having a single range for a
particular virtual register to having <em>multiple</em> disjoint ranges. This
collection of multiple ranges is called an <em>interval</em> and it exists to free up
registers in the context of branches.</p>

<p>For example, in the our IR snippet (above), R12 is defined in B2 as a block
parameter, used in B3, and then not used again until some indetermine point in
B4. (Our example uses it immediately in an add instruction to keep things
short, but pretend the second use is some time away.)</p>

<p>The Wimmer2010 paper creates a <em>lifetime hole</em> between 28 and 34, meaning that the
interval for R12 (called i12) is <code>[[20, 28), [34, ...)]</code>. Interval holes are
not strictly necessary—they exist to generate better code. So for this post,
we’re going to start simple and assume 1 interval == 1 range. We may come back
later and add additional ranges, but that will require some fixes to our later
implementation. We’ll note where we think those fixes should happen.</p>

<div><div><pre><code>BUILDINTERVALS
for each block b in reverse order do
  live = union of successor.liveIn for each successor of b
  for each phi function phi of successors of b do
    live.add(phi.inputOf(b))
  for each opd in live do
    intervals[opd].addRange(b.from, b.to)
  for each operation op of b in reverse order do
    for each output operand opd of op do
      intervals[opd].setFrom(op.id)
      live.remove(opd)
    for each input operand opd of op do
      intervals[opd].addRange(b.from, op.id)
      live.add(opd)
  for each phi function phi of b do
    live.remove(phi.output)
  if b is loop header then
    loopEnd = last block of the loop starting at b
    for each opd in live do
      intervals[opd].addRange(b.from, loopEnd.to)
  b.liveIn = live
</code></pre></div></div>

<p>Anyway, here is the mostly-copied annotated implementation of BuildIntervals
from the Wimmer2010 paper:</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>build_intervals</span> <span>live_in</span>
    <span>intervals</span> <span>=</span> <span>Hash</span><span>.</span><span>new</span> <span>{</span> <span>|</span><span>hash</span><span>,</span> <span>key</span><span>|</span> <span>hash</span><span>[</span><span>key</span><span>]</span> <span>=</span> <span>Interval</span><span>.</span><span>new</span> <span>}</span>
    <span>@block_order</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>block</span><span>|</span>
      <span># live = union of successor.liveIn for each successor of b</span>
      <span># this is the *live out* of the current block since we&#39;re going to be</span>
      <span># iterating backwards over instructions</span>
      <span>live</span> <span>=</span> <span>block</span><span>.</span><span>successors</span><span>.</span><span>map</span> <span>{</span> <span>|</span><span>succ</span><span>|</span> <span>live_in</span><span>[</span><span>succ</span><span>]</span> <span>}.</span><span>reduce</span><span>(</span><span>0</span><span>,</span> <span>:|</span><span>)</span>
      <span># for each phi function phi of successors of b do</span>
      <span>#   live.add(phi.inputOf(b))</span>
      <span>live</span> <span>|=</span> <span>block</span><span>.</span><span>out_vregs</span><span>.</span><span>map</span> <span>{</span> <span>|</span><span>vreg</span><span>|</span> <span>1</span> <span>&lt;&lt;</span> <span>vreg</span><span>.</span><span>num</span> <span>}.</span><span>reduce</span><span>(</span><span>0</span><span>,</span> <span>:|</span><span>)</span>
      <span>each_bit</span><span>(</span><span>live</span><span>)</span> <span>do</span> <span>|</span><span>idx</span><span>|</span>
        <span>opd</span> <span>=</span> <span>vreg</span> <span>idx</span>
        <span>intervals</span><span>[</span><span>opd</span><span>].</span><span>add_range</span><span>(</span><span>block</span><span>.</span><span>from</span><span>,</span> <span>block</span><span>.</span><span>to</span><span>)</span>
      <span>end</span>
      <span>block</span><span>.</span><span>instructions</span><span>.</span><span>reverse</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>insn</span><span>|</span>
        <span>out</span> <span>=</span> <span>insn</span><span>.</span><span>out</span><span>&amp;</span><span>.</span><span>as_vreg</span>
        <span>if</span> <span>out</span>
          <span># for each output operand opd of op do</span>
          <span>#   intervals[opd].setFrom(op.id)</span>
          <span>intervals</span><span>[</span><span>out</span><span>].</span><span>set_from</span><span>(</span><span>insn</span><span>.</span><span>number</span><span>)</span>
        <span>end</span>
        <span># for each input operand opd of op do</span>
        <span>#   intervals[opd].addRange(b.from, op.id)</span>
        <span>insn</span><span>.</span><span>vreg_ins</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>opd</span><span>|</span>
          <span>intervals</span><span>[</span><span>opd</span><span>].</span><span>add_range</span><span>(</span><span>block</span><span>.</span><span>from</span><span>,</span> <span>insn</span><span>.</span><span>number</span><span>)</span>
        <span>end</span>
      <span>end</span>
    <span>end</span>
    <span>intervals</span><span>.</span><span>default_proc</span> <span>=</span> <span>nil</span>
    <span>intervals</span><span>.</span><span>freeze</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Another difference is that since we’re using block parameters, we don’t really
have this <code>phi.inputOf</code> thing. That’s just the block argument.</p>

<p>The last difference is that since we’re skipping the loop liveness hack, we
don’t modify a block’s <code>live</code> set as we iterate through instructions.</p>

<p>I know we said we’re building live ranges, so our <code>Interval</code> class only has
one <code>Range</code> on it. This is Ruby’s built-in range, but it’s really just being
used as a tuple of integers here.</p>

<div><div><pre><code><span>class</span> <span>Interval</span>
  <span>attr_reader</span> <span>:range</span>

  <span>def</span> <span>add_range</span><span>(</span><span>from</span><span>,</span> <span>to</span><span>)</span>
    <span>if</span> <span>to</span> <span>&lt;=</span> <span>from</span>
      <span>raise</span> <span>ArgumentError</span><span>,</span> <span>&#34;Invalid range: </span><span>#{</span><span>from</span><span>}</span><span> to </span><span>#{</span><span>to</span><span>}</span><span>&#34;</span>
    <span>end</span>
    <span>if</span> <span>!</span><span>@range</span>
      <span>@range</span> <span>=</span> <span>Range</span><span>.</span><span>new</span><span>(</span><span>from</span><span>,</span> <span>to</span><span>)</span>
      <span>return</span>
    <span>end</span>
    <span>@range</span> <span>=</span> <span>Range</span><span>.</span><span>new</span><span>([</span><span>@range</span><span>.</span><span>begin</span><span>,</span> <span>from</span><span>].</span><span>min</span><span>,</span> <span>[</span><span>@range</span><span>.</span><span>end</span><span>,</span> <span>to</span><span>].</span><span>max</span><span>)</span>
  <span>end</span>

  <span>def</span> <span>set_from</span><span>(</span><span>from</span><span>)</span>
    <span>@range</span> <span>=</span> <span>if</span> <span>@range</span>
      <span>if</span> <span>@range</span><span>.</span><span>end</span> <span>&lt;=</span> <span>from</span>
        <span>raise</span> <span>ArgumentError</span><span>,</span> <span>&#34;Invalid range: </span><span>#{</span><span>from</span><span>}</span><span> to </span><span>#{</span><span>@range</span><span>.</span><span>end</span><span>}</span><span>&#34;</span>
      <span>end</span>
      <span>Range</span><span>.</span><span>new</span><span>(</span><span>from</span><span>,</span> <span>@range</span><span>.</span><span>end</span><span>)</span>
    <span>else</span>
      <span># This happens when we don&#39;t have a use of the vreg</span>
      <span># If we don&#39;t have a use, the live range is very short</span>
      <span>Range</span><span>.</span><span>new</span><span>(</span><span>from</span><span>,</span> <span>from</span><span>)</span>
    <span>end</span>
  <span>end</span>

  <span>def</span> <span>==</span><span>(</span><span>other</span><span>)</span>
    <span>other</span><span>.</span><span>is_a?</span><span>(</span><span>Interval</span><span>)</span> <span>&amp;&amp;</span> <span>@range</span> <span>==</span> <span>other</span><span>.</span><span>range</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Note that there’s some implicit behavior happening here:</p>

<ul>
  <li>If we haven’t initialized a range yet, we build one automatically</li>
  <li>If we have a range, <code>add_range</code> builds the smallest range that overlaps with
the existing range and incoming information</li>
  <li>If we have a range, <code>set_from</code> may shrink it</li>
</ul>

<p>For example, if we have <code>[1, 5)</code> and someone calls <code>add_range(7, 10)</code>, we end
up with <code>[1, 10)</code>. There’s no gap in the middle.</p>

<p>And if we have <code>[1, 7)</code> and someone calls <code>set_from(3)</code>, we end up with <code>[3,
7)</code>.</p>

<p>After figuring out from scratch some of these assumptions about what the
interval/range API should and should not do, Aaron and I realized that there
was some actual code for <code>add_range</code> in a different, earlier paper: <a href="https://gmays.com/assets/img/linear-scan-ra-context-ssa.pdf">Linear
Scan Register Allocation in the Context of SSA Form and Register
Constraints</a> (PDF, 2002) by Mössenböck and Pfeiffer.</p>

<div><div><pre><code>ADDRANGE(i: Instruction; b: Block; end: integer)
  if b.first.n ≤ i.n ≤ b.last.n then range ← [i.n, end[ else range ← [b.first.n, end[
  add range to interval[i.n] // merging adjacent ranges
</code></pre></div></div>

<p>Unfortunately, many other versions of this PDF look absolutely horrible (like
bad OCR) and I had to do some digging to find the version linked above.</p>

<p>Finally we can start thinking about doing some actual register assignment.
Let’s return to the 90s.</p>

<h2 id="linear-scan">Linear scan</h2>

<p>Because we have faithfully kept 1 interval == 1 range, we can re-use the linear
scan algorithm from Poletto1999 (which looks, at a glance, to be the same
as 1997).</p>

<p>I recommend looking at the PDF side by side with the code. We have tried to
keep the structure very similar.</p>

<div><div><pre><code>LinearScanRegisterAllocation
active ← {}
foreach live interval i, in order of increasing start point
  ExpireOldIntervals(i)
  if length(active) = R then
    SpillAtInterval(i)
  else
    register[i] ← a register removed from pool of free registers
    add i to active, sorted by increasing end point

ExpireOldIntervals(i)
foreach interval j in active, in order of increasing end point
  if endpoint[j] ≥ startpoint[i] then
    return
  remove j from active
  add register[j] to pool of free registers

SpillAtInterval(i)
spill ← last interval in active
if endpoint[spill] &gt; endpoint[i] then
  register[i] ← register[spill]
  location[spill] ← new stack location
  remove spill from active
  add i to active, sorted by increasing end point
else
  location[i] ← new stack location
</code></pre></div></div>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>ye_olde_linear_scan</span> <span>intervals</span><span>,</span> <span>num_registers</span>
    <span>if</span> <span>num_registers</span> <span>&lt;=</span> <span>0</span>
      <span>raise</span> <span>ArgumentError</span><span>,</span> <span>&#34;Number of registers must be positive&#34;</span>
    <span>end</span>
    <span>free_registers</span> <span>=</span> <span>Set</span><span>.</span><span>new</span> <span>0</span><span>...</span><span>num_registers</span>
    <span>active</span> <span>=</span> <span>[]</span>  <span># Active intervals, sorted by increasing end point</span>
    <span>assignment</span> <span>=</span> <span>{}</span>  <span># Map from Interval to PReg|StackSlot</span>
    <span>num_stack_slots</span> <span>=</span> <span>0</span>
    <span># Iterate through intervals in order of increasing start point</span>
    <span>sorted_intervals</span> <span>=</span> <span>intervals</span><span>.</span><span>sort_by</span> <span>{</span> <span>|</span><span>_</span><span>,</span> <span>interval</span><span>|</span> <span>interval</span><span>.</span><span>range</span><span>.</span><span>begin</span> <span>}</span>
    <span>sorted_intervals</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>_vreg</span><span>,</span> <span>interval</span><span>|</span>
      <span># expire_old_intervals(interval)</span>
      <span>active</span><span>.</span><span>select!</span> <span>do</span> <span>|</span><span>active_interval</span><span>|</span>
        <span>if</span> <span>active_interval</span><span>.</span><span>range</span><span>.</span><span>end</span> <span>&gt;</span> <span>interval</span><span>.</span><span>range</span><span>.</span><span>begin</span>
          <span>true</span>
        <span>else</span>
          <span>operand</span> <span>=</span> <span>assignment</span><span>.</span><span>fetch</span><span>(</span><span>active_interval</span><span>)</span>
          <span>raise</span> <span>&#34;Should be assigned a register&#34;</span> <span>unless</span> <span>operand</span><span>.</span><span>is_a?</span><span>(</span><span>PReg</span><span>)</span>
          <span>free_registers</span><span>.</span><span>add</span><span>(</span><span>operand</span><span>.</span><span>name</span><span>)</span>
          <span>false</span>
        <span>end</span>
      <span>end</span>
      <span>if</span> <span>active</span><span>.</span><span>length</span> <span>==</span> <span>num_registers</span>
        <span># spill_at_interval(interval)</span>
        <span># Pick an interval to spill. Picking the longest-lived active one is</span>
        <span># a heuristic from the original linear scan paper.</span>
        <span>spill</span> <span>=</span> <span>active</span><span>.</span><span>last</span>
        <span># In either case, we need to allocate a slot on the stack.</span>
        <span>slot</span> <span>=</span> <span>StackSlot</span><span>.</span><span>new</span><span>(</span><span>num_stack_slots</span><span>)</span>
        <span>num_stack_slots</span> <span>+=</span> <span>1</span>
        <span>if</span> <span>spill</span><span>.</span><span>range</span><span>.</span><span>end</span> <span>&gt;</span> <span>interval</span><span>.</span><span>range</span><span>.</span><span>end</span>
          <span># The last active interval ends further away than the current</span>
          <span># interval; spill the last active interval.</span>
          <span>assignment</span><span>[</span><span>interval</span><span>]</span> <span>=</span> <span>assignment</span><span>[</span><span>spill</span><span>]</span>
          <span>raise</span> <span>&#34;Should be assigned a register&#34;</span> <span>unless</span> <span>assignment</span><span>[</span><span>interval</span><span>].</span><span>is_a?</span><span>(</span><span>PReg</span><span>)</span>
          <span>assignment</span><span>[</span><span>spill</span><span>]</span> <span>=</span> <span>slot</span>
          <span>active</span><span>.</span><span>pop</span>  <span># We know spill is the last one</span>
          <span># Insert interval into already-sorted active</span>
          <span>insert_idx</span> <span>=</span> <span>active</span><span>.</span><span>bsearch_index</span> <span>{</span> <span>|</span><span>i</span><span>|</span> <span>i</span><span>.</span><span>range</span><span>.</span><span>end</span> <span>&gt;=</span> <span>interval</span><span>.</span><span>range</span><span>.</span><span>end</span> <span>}</span> <span>||</span> <span>active</span><span>.</span><span>length</span>
          <span>active</span><span>.</span><span>insert</span><span>(</span><span>insert_idx</span><span>,</span> <span>interval</span><span>)</span>
        <span>else</span>
          <span># The current interval ends further away than the last active</span>
          <span># interval; spill the current interval.</span>
          <span>assignment</span><span>[</span><span>interval</span><span>]</span> <span>=</span> <span>slot</span>
        <span>end</span>
      <span>else</span>
        <span>reg</span> <span>=</span> <span>free_registers</span><span>.</span><span>min</span>
        <span>free_registers</span><span>.</span><span>delete</span><span>(</span><span>reg</span><span>)</span>
        <span>assignment</span><span>[</span><span>interval</span><span>]</span> <span>=</span> <span>PReg</span><span>.</span><span>new</span><span>(</span><span>reg</span><span>)</span>
        <span># Insert interval into already-sorted active</span>
        <span>insert_idx</span> <span>=</span> <span>active</span><span>.</span><span>bsearch_index</span> <span>{</span> <span>|</span><span>i</span><span>|</span> <span>i</span><span>.</span><span>range</span><span>.</span><span>end</span> <span>&gt;=</span> <span>interval</span><span>.</span><span>range</span><span>.</span><span>end</span> <span>}</span> <span>||</span> <span>active</span><span>.</span><span>length</span>
        <span>active</span><span>.</span><span>insert</span><span>(</span><span>insert_idx</span><span>,</span> <span>interval</span><span>)</span>
      <span>end</span>
    <span>end</span>
    <span>[</span><span>assignment</span><span>,</span> <span>num_stack_slots</span><span>]</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Internalizing this took us a bit. It is mostly a three-state machine:</p>

<ul>
  <li>have not been allocated</li>
  <li>have been allocated a register</li>
  <li>have been allocated a stack slot</li>
</ul>

<p>We would like to come back to this and incrementally modify it as we add
lifetime holes to intervals.</p>

<p>I finally understood, very late in the game, that Poletto1999 linear scan assigns only one
location per virtual register. <em>Ever</em>. It’s not that every virtual register
gets a shot in a register and then gets moved to a stack slot—that would be
interval splitting and hopefully we get to that later—if a register gets
spilled, it’s in a stack slot from beginning to end.</p>

<p>I only found this out accidentally after trying to figure out a bug (that
wasn’t a bug) due to a lovely sentence in <a href="https://gmays.com/assets/img/optimized-interval-splitting-linear-scan-ra.pdf">Optimized Interval Splitting in a
Linear Scan Register Allocator</a> (PDF, 2005) by
Wimmer and Mössenböck):</p>

<blockquote>
  <p>However, it cannot deal with lifetime holes and does not split intervals, so
an interval has either a register assigned for the whole lifetime, or it is
spilled completely.</p>
</blockquote>

<p>Also,</p>

<blockquote>
  <p>In particular, it is not possible to implement the algorithm without
reserving a scratch register: When a spilled interval is used by an
instruction requiring the operand in a register, the interval must be
temporarily reloaded to the scratch register</p>
</blockquote>

<p>Also,</p>

<blockquote>
  <p>Additionally, register constraints for method calls and instructions
requiring fixed registers must be handled separately</p>
</blockquote>

<p>Marvelous.</p>

<p>Let’s take a look at the code snippet again. Here it is before register
allocation, using virtual registers:</p>

<div><div><pre><code>16: label B1(R10, R11):
18: jmp B2($1, R11)
     # vvvvvvvvvv #
20: label B2(R12, R13)
22: cmp R13, $1
24: branch lessThan B4()

26: label B3()
28: mul R12, R13 -&gt; R14
30: sub R13, $1 -&gt; R15
32: jump B2(R14, R15)

34: label B4()
     # ^^^^^^^^^^ #
36: add R10, R12 -&gt; R16
38: ret R16
</code></pre></div></div>

<p>Let’s run it through register allocation with incrementally decreasing numbers
of physical registers available. We get the following assignments:</p>

<ul>
  <li>4 registers <code>{R10: P0, R11: P1, R12: P1, R13: P2, R14: P3, R15: P2, R16: P0}</code></li>
  <li>3 registers <code>{R10: Stack[0], R11: P1, R12: P1, R13: P2, R14: P0, R15: P2, R16: P0}</code></li>
  <li>2 registers <code>{R10: Stack[0], R11: P1, R12: Stack[1], R13: P0, R14: P1, R15: P0, R16: P0}</code></li>
  <li>1 register <code>{R10: Stack[0], R11: P0, R12: Stack[1], R13: P0, R14: Stack[2], R15: P0, R16: P0}</code></li>
</ul>

<p>Some other things to note:</p>

<ul>
  <li>
    <p>If you have a register free, choosing which register to allocate is a
heuristic! It is tunable. There is probably some research out there that
explores the space.</p>

    <p>In fact, you might even consider <em>not</em> allocating a register greedily. What
might that look like? I have no idea.</p>
  </li>
  <li>
    <p>Spilling the interval with the furthest endpoint is a heuristic! You can
pick any active interval you want. In <a href="https://gmays.com/assets/img/register-spilling-range-splitting-ssa.pdf">Register Spilling and Live-Range
Splitting for SSA-Form Programs</a> (PDF,
2009) by Braun and Hack, for example, they present the MIN algorithm, which
spills the interval with the furthest next use.</p>

    <p>This requires slightly more information and takes slightly more time than
the default heuristic but apparently generates much better code.</p>
  </li>
  <li>
    <p>Also, block ordering? You guessed it. Heuristic.</p>
  </li>
</ul>

<p>Here is an example “slideshow” I generated by running linear scan with 2
registers. Use the arrow keys to navigate forward and backward in time<sup id="fnref:rsms" role="doc-noteref"><a href="#fn:rsms" rel="footnote">4</a></sup>.</p>



<h2 id="resolving-ssa">Resolving SSA</h2>

<p>At this point we have register <em>assignments</em>: we have a hash table mapping
intervals to physical locations. That’s great but we’re still in SSA form:
labelled code regions don’t have block arguments in hardware. We need to write
some code to take us out of SSA and into the real world.</p>

<p>We can use a modified Wimmer2010 as a great start point here. It handles more
than we need to right now—lifetime holes—but we can simplify.</p>

<div><div><pre><code>RESOLVE
for each control flow edge from predecessor to successor do
  for each interval it live at begin of successor do
    if it starts at begin of successor then
      phi = phi function defining it
      opd = phi.inputOf(predecessor)
      if opd is a constant then
        moveFrom = opd
      else
        moveFrom = location of intervals[opd] at end of predecessor
    else
      moveFrom = location of it at end of predecessor
    moveTo = location of it at begin of successor
    if moveFrom ≠ moveTo then
      mapping.add(moveFrom, moveTo)
  mapping.orderAndInsertMoves()
</code></pre></div></div>

<p>Because we have a 1:1 mapping of virtual registers to live ranges, we know that
every interval live at the beginning of a block is either:</p>

<ul>
  <li>live across an edge between two blocks and therefore has already been placed
in a location by assignment/spill code</li>
  <li>beginning its life at the beginning of the block as a block parameter and
therefore needs to be moved from its source location</li>
</ul>

<p>For this reason, we only handle the second case in our SSA resolution. If we
added lifetime holes, we would have to go back to the full Wimmer SSA
resolution.</p>

<p>This means that we’re going to iterate over every outbound edge from every
block. For each edge, we’re going to insert some parallel moves.</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>resolve_ssa</span> <span>intervals</span><span>,</span> <span>assignments</span>
    <span># ...</span>
    <span>@block_order</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>predecessor</span><span>|</span>
      <span>outgoing_edges</span> <span>=</span> <span>predecessor</span><span>.</span><span>edges</span>
      <span>num_successors</span> <span>=</span> <span>outgoing_edges</span><span>.</span><span>length</span>
      <span>outgoing_edges</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>edge</span><span>|</span>
        <span>mapping</span> <span>=</span> <span>[]</span>
        <span>successor</span> <span>=</span> <span>edge</span><span>.</span><span>block</span>
        <span>edge</span><span>.</span><span>args</span><span>.</span><span>zip</span><span>(</span><span>successor</span><span>.</span><span>parameters</span><span>).</span><span>each</span> <span>do</span> <span>|</span><span>moveFrom</span><span>,</span> <span>moveTo</span><span>|</span>
          <span>if</span> <span>moveFrom</span> <span>!=</span> <span>moveTo</span>
            <span>mapping</span> <span>&lt;&lt;</span> <span>[</span><span>moveFrom</span><span>,</span> <span>moveTo</span><span>]</span>
          <span>end</span>
        <span>end</span>
        <span># predecessor.order_and_insert_moves(mapping)</span>
        <span># TODO: order_and_insert_moves</span>
      <span>end</span>
    <span>end</span>
    <span># Remove all block parameters and arguments; we have resolved SSA</span>
    <span>@block_order</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>block</span><span>|</span>
      <span>block</span><span>.</span><span>parameters</span><span>.</span><span>clear</span>
      <span>block</span><span>.</span><span>edges</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>edge</span><span>|</span>
        <span>edge</span><span>.</span><span>args</span><span>.</span><span>clear</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>This already looks very similar to the RESOLVE function from Wimmer2010.
Unfortunately, Wimmer2010 basically shrugs off <code>orderAndInsertMoves</code> with an <em>eh, it’s
already in the literature</em> comment.</p>

<h3 id="a-brief-and-frustrating-parallel-moves-detour">A brief and frustrating parallel moves detour</h3>

<p>What’s not made clear, though, is that this particular subroutine has been the
source of a significant amount of bugs in the literature. Only recently did
some folks roll through and suggest (proven!) fixes:</p>

<ul>
  <li><a href="https://gmays.com/assets/img/parallel-move-leroy.pdf">Battling windmills with Coq: formal verification of a compilation algorithm
for parallel moves</a> (PDF, 2007) by Rideau, Serpette, and
Leroy</li>
  <li><a href="https://gmays.com/assets/img/boissinot-out-ssa.pdf">Revisiting Out-of-SSA Translation for Correctness, Code Quality, and
Efficiency</a> (PDF, 2009) by Boissinot, Darte, Rastello,
Dupont de Dinechin, and Guillon.
    <ul>
      <li>and again in <a href="https://gmays.com/assets/img/boissinot-thesis.pdf">Boissinot’s thesis</a> (PDF, 2010)</li>
    </ul>
  </li>
</ul>

<p>This sent us on a deep rabbit hole of trying to understand what bugs occur,
when, and how to fix them. We implemented both the Leroy and the Boissinot
algorithms. We found differences between Boissinot2009, Boissinot2010, and the
SSA book implementation following those algorithms. We found Paul Sokolovsky’s
<a href="https://github.com/pfalcon/parcopy/">implementation with bugfixes</a>. We found
Dmitry Stogov’s <a href="https://github.com/pfalcon/parcopy/pull/1">unmerged pull
request</a> to the same repository to
fix another bug.</p>

<p>We looked at Benoit Boissinot’s thesis again and emailed him some questions. He
responded! And then he even put up an <a href="https://github.com/bboissin/thesis_bboissin">amended version of his
algorithm</a> in Rust with tests and
fuzzing.</p>

<p>All this is to say that this is still causing people grief and, though I
understand page limits, I wish parallel moves were not handwaved away.</p>

<p>We ended up with this implementation which passes all of the tests from
Sokolovsky’s repository as well as the example from Boissinot’s thesis (though,
as we discussed in the email, the example solution in the thesis is
incorrect<sup id="fnref:thesis-correction" role="doc-noteref"><a href="#fn:thesis-correction" rel="footnote">5</a></sup>).</p>

<div><div><pre><code><span># copies contains an array of [src, dst] arrays</span>
<span>def</span> <span>sequentialize</span> <span>copies</span>
  <span>ready</span> <span>=</span> <span>[]</span>  <span># Contains only destination regs (&#34;available&#34;)</span>
  <span>to_do</span> <span>=</span> <span>[]</span>  <span># Contains only destination regs</span>
  <span>pred</span> <span>=</span> <span>{}</span>  <span># Map of destination reg -&gt; what reg is written to it (its source)</span>
  <span>loc</span> <span>=</span> <span>{}</span>  <span># Map of reg -&gt; the current location where the initial value of reg is available (&#34;resource&#34;)</span>
  <span>result</span> <span>=</span> <span>[]</span>

  <span>emit_copy</span> <span>=</span> <span>-&gt;</span> <span>(</span><span>src</span><span>,</span> <span>dst</span><span>)</span> <span>{</span>
    <span># We add an arrow here just for clarity in reading this algorithm because</span>
    <span># different people do [src, dst] and [dst, src] depending on if they prefer</span>
    <span># Intel or AT&amp;T</span>
    <span>result</span> <span>&lt;&lt;</span> <span>[</span><span>src</span><span>,</span> <span>&#34;-&gt;&#34;</span><span>,</span> <span>dst</span><span>]</span>
  <span>}</span>

  <span># In Ruby, loc[x] is nil if x not in loc, so this loop could be omitted</span>
  <span>copies</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>(</span><span>src</span><span>,</span> <span>dst</span><span>)</span><span>|</span>
    <span>loc</span><span>[</span><span>dst</span><span>]</span> <span>=</span> <span>nil</span>
  <span>end</span>

  <span>copies</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>(</span><span>src</span><span>,</span> <span>dst</span><span>)</span><span>|</span>
    <span>loc</span><span>[</span><span>src</span><span>]</span> <span>=</span> <span>src</span>
    <span>if</span> <span>pred</span><span>.</span><span>key?</span> <span>dst</span>  <span># Alternatively, to_do.include? dst</span>
      <span>raise</span> <span>&#34;Conflicting assignments to destination </span><span>#{</span><span>dst</span><span>}</span><span>, latest: </span><span>#{</span><span>[</span><span>dst</span><span>,</span> <span>src</span><span>]</span><span>}</span><span>&#34;</span>
    <span>end</span>
    <span>pred</span><span>[</span><span>dst</span><span>]</span> <span>=</span> <span>src</span>
    <span>to_do</span> <span>&lt;&lt;</span> <span>dst</span>
  <span>end</span>

  <span>copies</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>(</span><span>src</span><span>,</span> <span>dst</span><span>)</span><span>|</span>
    <span>if</span> <span>!</span><span>loc</span><span>[</span><span>dst</span><span>]</span>
      <span># All destinations that are not also sources can be written to immediately (tree leaves)</span>
      <span>ready</span> <span>&lt;&lt;</span> <span>dst</span>
    <span>end</span>
  <span>end</span>

  <span>while</span> <span>!</span><span>to_do</span><span>.</span><span>empty?</span>
    <span>while</span> <span>b</span> <span>=</span> <span>ready</span><span>.</span><span>pop</span>
      <span>a</span> <span>=</span> <span>loc</span><span>[</span><span>pred</span><span>[</span><span>b</span><span>]]</span> <span># a in the paper</span>
      <span>emit_copy</span><span>.</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span>
      <span># pred[b] is now living at b</span>
      <span>loc</span><span>[</span><span>pred</span><span>[</span><span>b</span><span>]]</span> <span>=</span> <span>b</span>
      <span>if</span> <span>to_do</span><span>.</span><span>include?</span><span>(</span><span>a</span><span>)</span>
        <span>to_do</span><span>.</span><span>delete</span> <span>a</span>
      <span>end</span>
      <span>if</span> <span>pred</span><span>[</span><span>b</span><span>]</span> <span>==</span> <span>a</span> <span>&amp;&amp;</span> <span>pred</span><span>.</span><span>include?</span><span>(</span><span>a</span><span>)</span>
        <span>ready</span> <span>&lt;&lt;</span> <span>a</span>
      <span>end</span>
    <span>end</span>

    <span>if</span> <span>to_do</span><span>.</span><span>empty?</span>
      <span>break</span>
    <span>end</span>

    <span>dst</span> <span>=</span> <span>to_do</span><span>.</span><span>pop</span>
    <span>if</span> <span>dst</span> <span>!=</span> <span>loc</span><span>[</span><span>pred</span><span>[</span><span>dst</span><span>]]</span>
      <span>emit_copy</span><span>.</span><span>(</span><span>dst</span><span>,</span> <span>&#34;tmp&#34;</span><span>)</span>
      <span>loc</span><span>[</span><span>dst</span><span>]</span> <span>=</span> <span>&#34;tmp&#34;</span>
      <span>ready</span> <span>&lt;&lt;</span> <span>dst</span>
    <span>end</span>
  <span>end</span>
  <span>result</span>
<span>end</span>
</code></pre></div></div>

<p>Leroy’s algorithm, which is shorter, passes almost all the tests—in one test
case, it uses one more temporary variable than Boissinot’s does. We haven’t
spent much time looking at why.</p>

<div><div><pre><code><span>def</span> <span>move_one</span> <span>i</span><span>,</span> <span>src</span><span>,</span> <span>dst</span><span>,</span> <span>status</span><span>,</span> <span>result</span>
  <span>return</span> <span>if</span> <span>src</span><span>[</span><span>i</span><span>]</span> <span>==</span> <span>dst</span><span>[</span><span>i</span><span>]</span>
  <span>status</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>:being_moved</span>
  <span>for</span> <span>j</span> <span>in</span> <span>0</span><span>...</span><span>(</span><span>src</span><span>.</span><span>length</span><span>)</span> <span>do</span>
    <span>if</span> <span>src</span><span>[</span><span>j</span><span>]</span> <span>==</span> <span>dst</span><span>[</span><span>i</span><span>]</span>
      <span>case</span> <span>status</span><span>[</span><span>j</span><span>]</span>
      <span>when</span> <span>:to_move</span>
        <span>move_one</span> <span>j</span><span>,</span> <span>src</span><span>,</span> <span>dst</span><span>,</span> <span>status</span><span>,</span> <span>result</span>
      <span>when</span> <span>:being_moved</span>
        <span>result</span> <span>&lt;&lt;</span> <span>[</span><span>src</span><span>[</span><span>j</span><span>],</span> <span>&#34;-&gt;&#34;</span><span>,</span> <span>&#34;tmp&#34;</span><span>]</span>
        <span>src</span><span>[</span><span>j</span><span>]</span> <span>=</span> <span>&#34;tmp&#34;</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
  <span>result</span> <span>&lt;&lt;</span> <span>[</span><span>src</span><span>[</span><span>i</span><span>],</span> <span>&#34;-&gt;&#34;</span><span>,</span> <span>dst</span><span>[</span><span>i</span><span>]]</span>
  <span>status</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>:moved</span>
<span>end</span>

<span>def</span> <span>leroy_sequentialize</span> <span>copies</span>
  <span>src</span> <span>=</span> <span>copies</span><span>.</span><span>map</span> <span>{</span> <span>it</span><span>[</span><span>0</span><span>]</span> <span>}</span>
  <span>dst</span> <span>=</span> <span>copies</span><span>.</span><span>map</span> <span>{</span> <span>it</span><span>[</span><span>1</span><span>]</span> <span>}</span>
  <span>status</span> <span>=</span> <span>[</span><span>:to_move</span><span>]</span> <span>*</span> <span>src</span><span>.</span><span>length</span>
  <span>result</span> <span>=</span> <span>[]</span>
  <span>status</span><span>.</span><span>each_with_index</span> <span>do</span> <span>|</span><span>item</span><span>,</span> <span>i</span><span>|</span>
    <span>if</span> <span>item</span> <span>==</span> <span>:to_move</span>
      <span>move_one</span> <span>i</span><span>,</span> <span>src</span><span>,</span> <span>dst</span><span>,</span> <span>status</span><span>,</span> <span>result</span>
    <span>end</span>
  <span>end</span>
  <span>result</span>
<span>end</span>
</code></pre></div></div>

<h3 id="back-to-ssa-resolution">Back to SSA resolution</h3>

<p>Whatever algorithm you choose, you now have a way to parallel move some
registers to some other registers. You have avoided the “swap problem”.</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>resolve_ssa</span> <span>intervals</span><span>,</span> <span>assignments</span>
    <span># ...</span>
        <span># predecessor.order_and_insert_moves(mapping)</span>
        <span>sequence</span> <span>=</span> <span>sequentialize</span><span>(</span><span>mapping</span><span>).</span><span>map</span> <span>do</span> <span>|</span><span>(</span><span>src</span><span>,</span> <span>_</span><span>,</span> <span>dst</span><span>)</span><span>|</span>
          <span>Insn</span><span>.</span><span>new</span><span>(</span><span>:mov</span><span>,</span> <span>dst</span><span>,</span> <span>[</span><span>src</span><span>])</span>
        <span>end</span>
        <span># TODO: insert the moves!</span>
    <span># ...</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>That’s great. You can generate an ordered list of instructions from a tangled
graph. But where do you put them? What about the “lost copy” problem?</p>

<p>As it turns out, we still need to handle critical edge splitting. Let’s
consider what it means to insert moves at an edge between blocks <code>A -&gt; B</code> when
the surrounding CFG looks a couple of different ways.</p>

<ul>
  <li>Case 1: <code>A -&gt; B</code></li>
  <li>Case 2: <code>A -&gt; B</code> and <code>A -&gt; C</code></li>
  <li>Case 3: <code>A -&gt; B</code> and <code>D -&gt; B</code></li>
  <li>Case 4: <code>A -&gt; B</code> and <code>A -&gt; C</code> and <code>D -&gt; B</code></li>
</ul>

<p>These are the four (really, three) cases we may come across.</p>

<p>In Case 1, if we only have two neighboring blocks A and B, we can
insert the moves into either block. It doesn’t matter: at the end of A or at
the beginning of B are both fine.</p>

<p>In Case 2, if A has two successors, then we should insert the moves at the
beginning of B. That way we won’t be mucking things up for the edge <code>A -&gt; C</code>.</p>

<p>In Case 3, if B has two predecessors, then we should insert the moves at the
end of A. That way we won’t be mucking things up for the edge <code>D -&gt; B</code>.</p>

<p>Case 4 is the most complicated. There is no extant place in the graph we can
insert moves. If we insert in A, we mess things up for <code>A -&gt; C</code>. If we insert
in <code>B</code>, we mess things up for <code>D -&gt; B</code>. Inserting in <code>C</code> or <code>D</code> doesn’t make
any sense. What is there to do?</p>

<p>As it turns out, Case 4 is called a <em>critical edge</em>. And we have to split it.</p>

<p>We can insert a new block E along the edge <code>A -&gt; B</code> and put the moves in E!
That way they still happen along the edge without affecting any other blocks.
Neat.</p>

<p>In Ruby code, that looks like:</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>resolve_ssa</span> <span>intervals</span><span>,</span> <span>assignments</span>
    <span>num_predecessors</span> <span>=</span> <span>Hash</span><span>.</span><span>new</span> <span>0</span>
    <span>@block_order</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>block</span><span>|</span>
      <span>block</span><span>.</span><span>edges</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>edge</span><span>|</span>
        <span>num_predecessors</span><span>[</span><span>edge</span><span>.</span><span>block</span><span>]</span> <span>+=</span> <span>1</span>
      <span>end</span>
    <span>end</span>
    <span># ...</span>
        <span># predecessor.order_and_insert_moves(mapping)</span>
        <span>sequence</span> <span>=</span> <span>...</span>
        <span># If we don&#39;t have any moves to insert, we don&#39;t have any block to</span>
        <span># insert</span>
        <span>next</span> <span>if</span> <span>sequence</span><span>.</span><span>empty?</span>
        <span>if</span> <span>num_predecessors</span><span>[</span><span>successor</span><span>]</span> <span>&gt;</span> <span>1</span> <span>&amp;&amp;</span> <span>num_successors</span> <span>&gt;</span> <span>1</span>
          <span># Make a new interstitial block</span>
          <span>b</span> <span>=</span> <span>new_block</span>
          <span>b</span><span>.</span><span>insert_moves_at_start</span> <span>sequence</span>
          <span>b</span><span>.</span><span>instructions</span> <span>&lt;&lt;</span> <span>Insn</span><span>.</span><span>new</span><span>(</span><span>:jump</span><span>,</span> <span>nil</span><span>,</span> <span>[</span><span>Edge</span><span>.</span><span>new</span><span>(</span><span>successor</span><span>,</span> <span>[])])</span>
          <span>edge</span><span>.</span><span>block</span> <span>=</span> <span>b</span>
        <span>elsif</span> <span>num_successors</span> <span>&gt;</span> <span>1</span>
          <span># Insert into the beginning of the block</span>
          <span>successor</span><span>.</span><span>insert_moves_at_start</span> <span>sequence</span>
        <span>else</span>
          <span># Insert into the end of the block... before the terminator</span>
          <span>predecessor</span><span>.</span><span>insert_moves_at_end</span> <span>sequence</span>
        <span>end</span>
    <span># ...</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Adding a new block invalidates the cached <code>@block_order</code>, so we also need to
recompute that.</p>

<blockquote>
  <p>We could also avoid that by splitting critical edges earlier, before
numbering. Then, when we arrive in <code>resolve_ssa</code>, we can clean up branches to
empty blocks!</p>
</blockquote>

<p>(See also <a href="https://nickdesaulniers.github.io/blog/2023/01/27/critical-edge-splitting/">Nick’s post on critical edge
splitting</a>,
which also links to Faddegon’s thesis, which I should at least skim.)</p>

<p>And that’s it, folks. We have gone from virtual registers in SSA form to
physical locations. Everything’s all hunky-dory. We can just turn these LIR
instructions into their very similar looking machine equivalents, right?</p>

<p>Not so fast…</p>

<h2 id="calls">Calls</h2>

<p>You may have noticed that the original linear scan paper does not mention calls
or other register constraints. I didn’t really think about it until I wanted to
make a function call. The authors of later linear scan papers definitely
noticed, though; Wimmer2005 writes the following about Poletto1999:</p>

<blockquote>
  <p>When a spilled interval is used by an instruction requiring the operand in a
register, the interval must be temporarily reloaded to the scratch register.
Additionally, register constraints for method calls and instructions
requiring fixed registers must be handled separately.</p>
</blockquote>

<p>Fun. We will start off by handling calls and method parameters separately, we
will note that it’s not amazing code, and then we will eventually implement the
later papers, which handle register constraints more naturally.</p>

<p>We’ll call this new function <code>handle_caller_saved_regs</code> after register
allocation but before SSA resolution. We do it after register allocation so we
know where each virtual register goes but before resolution so we can still
inspect the virtual register operands.</p>

<p>Its goal is to do a couple of things:</p>

<ul>
  <li>Insert special <code>push</code> and <code>pop</code> instructions around <code>call</code> instructions to
preserve virtual registers that are used on the other side of the <code>call</code>. We
only care about preserving virtual registers that are stored in physical
registers, though; no need to preserve anything that already lives on the
stack.</li>
  <li>Do a parallel move of the call arguments into the ABI-specified parameter
registers. We need to do a parallel move in case any of the arguments happen
to already be living in parameter registers. (We’re really getting good
mileage out of this function.)</li>
  <li>Make sure that the value returned by the call in the ABI-specified return
register ends up in in the location allocated to the output of the <code>call</code>
instruction.</li>
</ul>

<p>We’ll also remove the <code>call</code> operands since we’re placing them in special
registers explicitly now.</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>handle_caller_saved_regs</span> <span>intervals</span><span>,</span> <span>assignments</span><span>,</span> <span>return_reg</span><span>,</span> <span>param_regs</span>
    <span>@block_order</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>block</span><span>|</span>
      <span>x</span> <span>=</span> <span>block</span><span>.</span><span>instructions</span><span>.</span><span>flat_map</span> <span>do</span> <span>|</span><span>insn</span><span>|</span>
        <span>if</span> <span>insn</span><span>.</span><span>name</span> <span>==</span> <span>:call</span>
          <span>survivors</span> <span>=</span> <span>intervals</span><span>.</span><span>select</span> <span>{</span> <span>|</span><span>_vreg</span><span>,</span> <span>interval</span><span>|</span>
            <span>interval</span><span>.</span><span>survives?</span><span>(</span><span>insn</span><span>.</span><span>number</span><span>)</span>
          <span>}.</span><span>map</span><span>(</span><span>&amp;</span><span>:first</span><span>).</span><span>select</span> <span>{</span> <span>|</span><span>vreg</span><span>|</span>
            <span>assignments</span><span>[</span><span>intervals</span><span>[</span><span>vreg</span><span>]].</span><span>is_a?</span><span>(</span><span>PReg</span><span>)</span>
          <span>}</span>
          <span>mov_input</span> <span>=</span> <span>insn</span><span>.</span><span>out</span>
          <span>insn</span><span>.</span><span>out</span> <span>=</span> <span>return_reg</span>

          <span>ins</span> <span>=</span> <span>insn</span><span>.</span><span>ins</span><span>.</span><span>drop</span><span>(</span><span>1</span><span>)</span>
          <span>raise</span> <span>if</span> <span>ins</span><span>.</span><span>length</span> <span>&gt;</span> <span>param_regs</span><span>.</span><span>length</span>

          <span>insn</span><span>.</span><span>ins</span><span>.</span><span>replace</span><span>(</span><span>insn</span><span>.</span><span>ins</span><span>.</span><span>first</span><span>(</span><span>1</span><span>))</span>

          <span>mapping</span> <span>=</span> <span>ins</span><span>.</span><span>zip</span><span>(</span><span>param_regs</span><span>).</span><span>to_h</span>
          <span>sequence</span> <span>=</span> <span>sequentialize</span><span>(</span><span>mapping</span><span>).</span><span>map</span> <span>do</span> <span>|</span><span>(</span><span>src</span><span>,</span> <span>_</span><span>,</span> <span>dst</span><span>)</span><span>|</span>
            <span>Insn</span><span>.</span><span>new</span><span>(</span><span>:mov</span><span>,</span> <span>dst</span><span>,</span> <span>[</span><span>src</span><span>])</span>
          <span>end</span>

          <span>survivors</span><span>.</span><span>map</span> <span>{</span> <span>|</span><span>s</span><span>|</span> <span>Insn</span><span>.</span><span>new</span><span>(</span><span>:push</span><span>,</span> <span>nil</span><span>,</span> <span>[</span><span>s</span><span>])</span> <span>}</span> <span>+</span>
            <span>sequence</span> <span>+</span>
            <span>[</span><span>insn</span><span>,</span> <span>Insn</span><span>.</span><span>new</span><span>(</span><span>:mov</span><span>,</span> <span>mov_input</span><span>,</span> <span>[</span><span>return_reg</span><span>])]</span> <span>+</span>
            <span>survivors</span><span>.</span><span>map</span> <span>{</span> <span>|</span><span>s</span><span>|</span> <span>Insn</span><span>.</span><span>new</span><span>(</span><span>:pop</span><span>,</span> <span>nil</span><span>,</span> <span>[</span><span>s</span><span>])</span> <span>}.</span><span>reverse</span>
        <span>else</span>
          <span>insn</span>
        <span>end</span>
      <span>end</span>
      <span>block</span><span>.</span><span>instructions</span><span>.</span><span>replace</span><span>(</span><span>x</span><span>)</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>(Unfortunately, this sidesteps handling the less-fun bit of calls in ABIs where
after the 6th parameter, they are expected on the stack. It also completely
ignores ABI size constraints.)</p>

<p>Now, you may have noticed that we don’t do anything special for the incoming
params of the function we’re compiling! That’s another thing we have to handle.
Thankfully, we can handle it with yet another parallel move (wow!) at the end
of <code>resolve_ssa</code>.</p>

<div><div><pre><code><span>class</span> <span>Function</span>
  <span>def</span> <span>resolve_ssa</span> <span>intervals</span><span>,</span> <span>assignments</span>
    <span># ...</span>
    <span># We&#39;re typically going to have more param regs than block parameters</span>
    <span># When we zip the param regs with block params, we&#39;ll end up with param</span>
    <span># regs mapping to nil. We filter those away by selecting for tuples</span>
    <span># that have a truthy second value</span>
    <span># [[x, y], [z, nil]].select(&amp;:last) (reject the second tuple)</span>
    <span>mapping</span> <span>=</span> <span>param_regs</span><span>.</span><span>zip</span><span>(</span><span>entry_block</span><span>.</span><span>parameters</span><span>).</span><span>select</span><span>(</span><span>&amp;</span><span>:last</span><span>).</span><span>to_h</span>
    <span>sequence</span> <span>=</span> <span>sequentialize</span><span>(</span><span>mapping</span><span>).</span><span>map</span> <span>do</span> <span>|</span><span>(</span><span>src</span><span>,</span> <span>_</span><span>,</span> <span>dst</span><span>)</span><span>|</span>
      <span>Insn</span><span>.</span><span>new</span><span>(</span><span>:mov</span><span>,</span> <span>dst</span><span>,</span> <span>[</span><span>src</span><span>])</span>
    <span>end</span>
    <span>entry_block</span><span>.</span><span>insert_moves_at_start</span><span>(</span><span>sequence</span><span>)</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Again, this is yet another kind of thing where some of the later papers have
much better ergonomics and also much better generated code.</p>

<p>But this is really cool! If you have arrived at this point with me, we have
successfully made it to 1997 and that is nothing to sneeze at. We have even
adapted research from 1997 to work with SSA, avoiding several significant
classes of bugs along the way.</p>

<!--
## Instruction selection and instruction splitting

## Lifetime holes and interval splitting

## Register hints

What is this iterated linear scan thing? Appears in JSC

while (true) {
  linearscan();
  if (!would_spill) { break; }
  interval = pick_an_interval_to_spill();
  spill(interval);
  remove_interval(interval);
}
-->

<h2 id="validation-by-abstract-interpretation">Validation by abstract interpretation</h2>

<p>We have just built an enormously complex machine. Even out the gate, with the
original linear scan, there is a lot of machinery. It’s possible to write tests
that spot check sample programs of all shapes and sizes but it’s <em>very</em>
difficult to anticipate every possible edge case that will appear in the real
world.</p>

<p>Even if the original algorithm you’re using has been proven correct, your
implementation may have subtle bugs due to (for example) having slightly
different invariants or even transcription errors.</p>

<p>We have all these proof tools at our disposal: we can write an abstract
interpreter that verifies properties of <em>one</em> graph, but it’s very hard
(impossible?) to scale that to sets of graphs.</p>

<p>Maybe that’s enough, though. In one of my favorite blog posts, Chris Fallin
<a href="https://cfallin.org/blog/2021/03/15/cranelift-isel-3/">writes about</a> writing a
register allocation verifier based on abstract interpretation. It can verify
one concrete LIR function at a time. It’s fast enough that it can be left on in
debug builds. This means that a decent chunk of the time (tests, CI, maybe a
production cluster) we can get a very clear signal that every register
assignment that passes through the verifier satisfies some invariants.</p>

<p>Furthermore, we are not limited to Real World Code. With the advent of fuzzing,
one can imagine an always-on fuzzer that tries to break the register allocator.
A verifier can then catch bugs that come from exploring this huge search space.</p>

<p>Some time after finding Chris’s blog post, I also stumbled across <a href="https://github.com/v8/v8/blob/cac6de03372c25987c6cbea49b4b39d9da437978/src/compiler/backend/register-allocator-verifier.h">the very same
thing in
V8</a>!</p>

<p>I find this stuff so cool. I’ll also mention Boissinot’s <a href="https://github.com/bboissin/thesis_bboissin">Rust
code</a> again because it does
something similar for parallel moves.</p>

<h2 id="see-also">See also</h2>

<p>It’s possible to do linear scan allocation in reverse, at least on traces
without control-flow. See for example <a href="https://www.mattkeeter.com/blog/2022-10-04-ssra/">The Solid-State Register
Allocator</a>, the <a href="https://github.com/LuaJIT/LuaJIT/blob/5e3c45c43bb0e0f1f2917d432e9d2dba12c42a6e/src/lj_asm.c#L198">LuaJIT
register allocator</a>, and <a href="https://brrt-to-the-future.blogspot.com/2019/03/reverse-linear-scan-allocation-is.html">Reverse Linear Scan Allocation is
probably a good idea</a>.
By doing linear scan this way, it is also possible to avoid computing liveness
and intervals. I am not sure if this works on programs with control-flow,
though.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>We built a register allocator that works on SSA. Hopefully next time we will
add features such as lifetime holes, interval splitting, and register hints.</p>

<p>The full Ruby code listing is <del>not (yet?) public</del> <a href="https://github.com/tenderworks/regalloc">available under the Apache
2 license</a>.</p>

<h2 id="thanks">Thanks</h2>

<p>Thanks to <a href="https://waleedkhan.name/">Waleed Khan</a> and <a href="https://mstdn.ca/@iainireland">Iain
Ireland</a> for giving feedback on this post.</p>


        </div></div>
  </body>
</html>
