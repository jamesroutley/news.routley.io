<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/minosvasilias/godot-dodo">Original</a>
    <h1>Godot-dodo â€“ Finetuning LLaMA on single-language comment:code data pairs</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://adventure.com/minosvasilias/godot-dodo/blob/main/godot_dodo_logo.png"><img src="https://adventure.com/minosvasilias/godot-dodo/raw/main/godot_dodo_logo.png" alt="Godot-Dodo logo imagined by Midjourney v5"/></a></p>
<p dir="auto">The godot-dodo project presents a pipeline to finetune open source language models on human-created, language-specific code retrieved from GitHub.</p>
<p dir="auto">In this case, the targeted language is GDScript, but the same methodology can be applied to other languages.</p>
<p dir="auto">This repository includes the following:</p>
<ul dir="auto">
<li>Scripts to assemble the finetuning dataset</li>
<li>Pre-assembled, raw datasets (up to a size of 60k rows)</li>
<li>Scripts to finetune a model</li>
<li>Links to model weights</li>
<li>Performance report comparing finetuned models</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-performance" aria-hidden="true" href="#performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Performance</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models/results.png"><img src="https://adventure.com/minosvasilias/godot-dodo/raw/main/models/results.png" alt="Results"/></a></p>
<p dir="auto"><strong>For comprehensive results explaining the methodology used and a full list of all result, please refer to the full performance report <a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models">here</a>.</strong></p>
<p dir="auto">In summary, <code>godot_dodo</code> models achieve significantly greater consistency than <code>gpt-4</code>/<code>gpt-3.5-turbo</code> when it comes to generating accurate GDScript syntax, but are somewhat less capable of following complex instructions.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-concept" aria-hidden="true" href="#concept"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Concept</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-how" aria-hidden="true" href="#how"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How?</h3>
<p dir="auto">Unlike other, similar approaches to finetuning models such as stanford-alpaca, this approach does not use existing, larger language models for the output-values of the finetuning-dataset. All code used is human-created. Language models are instead only used to <strong>label</strong> each code snippet.</p>
<p dir="auto">As such, we can assemble <code>comment:code</code> data-pairs in the style of <a href="https://github.com/github/CodeSearchNet">CodeSearchNet</a>, making use of powerful existing models to annotate high-quality human-created code.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-why" aria-hidden="true" href="#why"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Why?</h3>
<p dir="auto">Some existing language models such as <code>gpt-4</code> are excellent coders. However, a lot of their ability is concentrated in only the most popular languages, such as Python or Javascript.</p>
<p dir="auto">Less widely used languages are underrepresented in the training data and experience a massive performance drop-off, where models routinely mistake syntax or hallucinate language features that do not exist.</p>
<p dir="auto">This aims to provide much more robust language-specific models that can be used to reliably generate code that compiles on first try.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-dataset-generation" aria-hidden="true" href="#dataset-generation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Dataset Generation</h2>
<p dir="auto">Due to this approach relying on human-created data, we scrape GitHub repositories using the GitHub search API.</p>
<p dir="auto">Using the <code>language:gdscript</code> search term, we retrieve a list of repositories including GDScript code.</p>
<p dir="auto">We also use <code>license:mit</code> to limit the dataset to suitable repositories. <strong>Only MIT-licensed code is used for training!</strong></p>
<p dir="auto">We then clone each one and apply the following logic:</p>
<ul dir="auto">
<li>Find <code>project.godot</code> file</li>
<li>Detect whether project is made for <code>3.x</code> or <code>4.x</code> Godot engine versions</li>
<li>Iterate through all <code>.gd</code> files found in the repository</li>
<li>For each one, split file into individual functions</li>
<li>For each function found, ask existing LLM (<code>gpt-3.5-turbo</code>) for a detailed comment describing the functions purpose</li>
<li>Add <code>instruction:response</code> data pair to dataset</li>
</ul>
<p dir="auto">Note that existing, human-written comments located above the code-block are not used for the <code>instruction</code> value. We are interested in consistent detail for comments, rather than trying to preserve some potentially higher-quality human-written ones.</p>
<p dir="auto">Human comments within the code block however are preserved.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-run" aria-hidden="true" href="#run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Run</h3>
<p dir="auto">To assemble a dataset yourself, follow these instructions:</p>
<ul dir="auto">
<li>Run <code>python data/generate_unlabeled_dataset.py</code></li>
<li>Run <code>python data/label_dataset.py</code></li>
</ul>
<p dir="auto">Please do note that you&#39;ll need GitHub and OpenAI API keys in order to use these scripts.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-pre-assembled-datasets" aria-hidden="true" href="#pre-assembled-datasets"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Pre-assembled datasets</h2>
<p dir="auto">Pre-assembled datasets included in this repository:</p>
<ul dir="auto">
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/data/godot_dodo_4x_60k">godot_dodo_4x_60k</a>
<ul dir="auto">
<li>Assembled using <code>4.x</code> Godot projects - ~60k rows</li>
</ul>
</li>
</ul>
<p dir="auto">Further datasets may be added in the future (particularly regarding <code>3.x</code> data)</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-finetuning" aria-hidden="true" href="#finetuning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Finetuning</h2>
<p dir="auto">The fine-tuning process closely mirrors the one introduced by <a href="https://github.com/tatsu-lab/stanford_alpaca">stanford_alpaca</a>.</p>
<p dir="auto">To reproduce a fine-tuned version of LLaMA, please follow the steps below.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-hardware-requirements" aria-hidden="true" href="#hardware-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hardware Requirements</h3>
<p dir="auto">In order to effectively finetune a <code>llama-7b</code> or <code>llama-13b</code> model, it is highly recommended to use at least two <code>A100 80GB</code> GPUs. You may otherwise encounter out of memory errors or experience extremely long training times, and will need to adjust the training parameters.</p>
<p dir="auto">For finetuning <code>godot_dodo_4x_60k_llama_13b</code>, eight <code>A100 80GB</code> GPUs were used.</p>
<p dir="auto">Another important consideration is the protocol used for GPU communication. It is recommended to use <code>NVLink</code> setups rather than <code>PCIe</code>.</p>
<p dir="auto">Should you only have access to <code>PCIe</code> setups, please replace <code>full-shard</code> with <code>shard_grad_op</code> in the <code>torchrun</code> command. This may severely speed up your training runs at the cost of potentially higher memory usage.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-setup" aria-hidden="true" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup</h3>
<p dir="auto">Before finetuning, make sure to install all requirements using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-run-1" aria-hidden="true" href="#run-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Run</h3>
<p dir="auto">For exact commands used for finetuning models, please refer to the individual model pages:</p>
<ul dir="auto">
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models/godot_dodo_4x_60k_llama_7b">models/godot_dodo_4x_60k_llama_7b</a></li>
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models/godot_dodo_4x_60k_llama_13b">models/godot_dodo_4x_60k_llama_13b</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-inference" aria-hidden="true" href="#inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference</h2>
<p dir="auto">To test out your finetuned model, you can use the <code>eval.py</code> script. Simply run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python finetune/eval.py --model_name_or_path PATH_TO_FINETUNED_MODEL/"><pre>python finetune/eval.py --model_name_or_path PATH_TO_FINETUNED_MODEL/</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-publishing-to-huggingface" aria-hidden="true" href="#publishing-to-huggingface"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Publishing to Huggingface</h2>
<p dir="auto">To easily upload a finetuned model to Huggingface, you can use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python finetune/push_to_hub.py --model_name_or_path PATH_TO_FINETUNED_MODEL/ --push_name HF_MODEL_NAME --auth_token HF_ACCESS_TOKEN"><pre>python finetune/push_to_hub.py --model_name_or_path PATH_TO_FINETUNED_MODEL/ --push_name HF_MODEL_NAME --auth_token HF_ACCESS_TOKEN</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-finetuned-model-weights" aria-hidden="true" href="#finetuned-model-weights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Finetuned model weights</h2>
<p dir="auto">Links to model weights hosted on Huggingface are provided in the respective model pages:</p>
<ul dir="auto">
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models/godot_dodo_4x_60k_llama_7b">models/godot_dodo_4x_60k_llama_7b</a></li>
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models/godot_dodo_4x_60k_llama_13b">models/godot_dodo_4x_60k_llama_13b</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-cost" aria-hidden="true" href="#cost"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Cost</h2>
<p dir="auto">Below the dollar-cost of assembling each available dataset and finetuning each model.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-datasets" aria-hidden="true" href="#datasets"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Datasets</h3>
<ul dir="auto">
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/data/godot_dodo_4x_60k">godot_dodo_4x_60k</a>
<ul dir="auto">
<li><code>30$</code> (<code>gpt-3.5-turbo</code> API costs)</li>
</ul>
</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-finetuned-models" aria-hidden="true" href="#finetuned-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Finetuned Models</h3>
<ul dir="auto">
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models/godot_dodo_4x_60k_llama_7b">models/godot_dodo_4x_60k_llama_7b</a>
<ul dir="auto">
<li><code>24$</code> (8x A100 80GB instance costs)</li>
</ul>
</li>
<li><a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/models/godot_dodo_4x_60k_llama_13b">models/godot_dodo_4x_60k_llama_13b</a>
<ul dir="auto">
<li><code>84$</code>(8x A100 80GB instance costs)</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-use-with-godot-copilot" aria-hidden="true" href="#use-with-godot-copilot"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Use with godot-copilot</h2>
<p dir="auto">Usage of finetuned models with <a href="https://github.com/minosvasilias/godot-copilot">godot-copilot</a> for in-editor, fully local code generation may be supported in the future.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledegments" aria-hidden="true" href="#acknowledegments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledegments</h2>
<p dir="auto">Thank you to all MIT-licensed Godot projects! This would not be possible without you.</p>
<p dir="auto">All projects that were scraped during assembly of the included finetuning data are listed in the respective dataset folders in <a href="https://adventure.com/minosvasilias/godot-dodo/blob/main/data">data</a>.</p>
<p dir="auto">Another thank you goes to <a href="https://fluidstack.io" rel="nofollow">fluidstack.io</a> for their reliable, cheap GPU instances that were used for finetuning these models.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-citation" aria-hidden="true" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h2>
<p dir="auto">If you wish to cite this project, please use:</p>
<div data-snippet-clipboard-copy-content="@misc{godot-dodo,
  author = {Markus Sobkowski},
  title = {Godot-Dodo: Finetuned language models for GDScript generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/minosvasilias/godot-dodo}},
}"><pre><code>@misc{godot-dodo,
  author = {Markus Sobkowski},
  title = {Godot-Dodo: Finetuned language models for GDScript generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/minosvasilias/godot-dodo}},
}
</code></pre></div>
</article>
          </div></div>
  </body>
</html>
