<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/judofyr/spice">Original</a>
    <h1>Spice: Fine-grained parallelism with sub-nanosecond overhead in Zig</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/spice-tree-sum-100M.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/spice-tree-sum-100M.svg" alt="Time to calculate sum of binary tree of 100M nodes with Spice"/></a></p>
<p dir="auto"><strong>Spice</strong> uses <a href="https://www.andrew.cmu.edu/user/mrainey/heartbeat/heartbeat.html" rel="nofollow"><em>heartbeat scheduling</em></a> to accomplish extremely efficient parallelism in Zig:</p>
<ul dir="auto">
<li><strong>Sub-nanosecond overhead:</strong>
Turning your function into a parallelism-enabled function adds less than a nanosecond of overhead.</li>
<li><strong>Contention-free:</strong>
Threads will never compete (i.e. spin) over the same work.
Adding more threads to the system will not make your program any slower, but the extra threads might be completely idle since there&#39;s nothing useful to do.</li>
</ul>
<p dir="auto">The benchmark in the figure above (summing over the nodes in a binary tree) is typically one of the worst cases for parallelism frameworks:
The actual operation is extremely fast so any sort of overhead will have a measurable impact.</p>
<p dir="auto">Here&#39;s the exact same benchmark in <a href="https://docs.rs/rayon/latest/rayon/" rel="nofollow">Rayon</a>, an excellent library in Rust which uses work-stealing fork/join:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/rayon-tree-sum-100M.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/rayon-tree-sum-100M.svg" alt="Time to calculate sum of binary tree of 100M nodes with Rayon"/></a></p>
<p dir="auto">The overhead here is roughly ~15 ns (from 7.48 ns to 22.99 ns) which means that at 4 threads we&#39;re &#34;back&#34; to the sequential performance - just using four times as much CPU.
Luckily we <em>are</em> able to get linear speed-up (in terms of threads) initially.
These benchmarks were ran on a <code>c4-standard-16</code> instance in Google Cloud with 16 cores.
Rayon itself shows a nice ~14x speed-up (from 22.99 ns to 1.64 ns) at 16 threads, but compared to the <em>baseline</em> this ends up only being ~4.5x due to the overhead.</p>
<p dir="auto">In comparison, Spice scales slightly worse:
It only got ~11x speed-up when going from 1 to 16 threads.
However, due its low overhead this is also essentially the speed-up compared to the baseline.</p>
<p dir="auto">(It&#39;s not entirely clear why the Zig baseline implementation is twice as fast as the Rust implementation.
The <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AAvPMFJL6yAngGVG6AMKpaAVxYMJAZlIOAMngMmABy7gBGmMQgAGykAA6oCoS2DM5uHt7xickCAUGhLBFRsZaY1ilCBEzEBGnunlw%2BpeUCldUEeSHhkTEWVTV1GY197Z0FRTEAlBaorsTI7BxoDAoEANTBGJhrAKReACJrq8Su1rsA7ABCOxoAgmsPawBuYiBreNHSN/eP9FQEbz2ADEAFSbLC7A5rBiuWi0UjfR5rYhmBAAyGg8HbPaHGFwhF3RGPOKuMJrKgMI7uCCWKhvEHLVYbLaTd6fC7XO5IpEvYjIzAKSGHWkAOhetD2nJ%2B3PeVDWNMwtCoIr%2BBFZO3OjmQCDo6A1jn5gp2ACZrlDtbqRQpqZNJUSZXg5QqlSKUcA0erNRbaHrNYbdqacWtvegrTa7VyZfyCHNKcQBRHpRd9t8NSmvFLviSyRS1iwmIEIKynqg8OgOfa1gB6KtrAAqbuAkTWBAQ22tLAAtFRXAwWpSiGsImtZgQSQRKwB9IXMrBhlgQXtYGhBdC2jOp877DjTWicACsvE83F4qE4AC0zEdZvNscavDxSACOFpJtMANYSACcIq40X3XiSMaQHGvuwEABySNIe4cJIR6aKenC8AoIAaE%2BCHTHAsBIGgLBxHQkTkJQuH4fQUTGAQJwMO%2BfB0AQkQoRAYQIaQYSBNUACenCPmxzDEBxADyYTaGUz6PrhbCCAJDC0FxL68FgYSuMAjhiLQKEnqQWD5kY4jyVpeDxuUTwCixmCqGUrj0dxvCBPRMFaPoeBhMQnHOFgLGUXgLA2aQJnEGEiSYPsmA6cAtCBKA8nTFQBjAAoABqeCYAA7gJcSML5/CCCIYjsFIMiCIoKjqPpujGvohgmGYTlhChkDTKgcQ2AIGmdgJawAEqKpgTBKECfUEKe/kolg9VFhYPX9vYDBOC49R6P4gRdIUPRcFkSQtak81DBtOQMGM3RROtzRbW0Aw7Q0k1WGd/QdMt4xrSMF3pFdqyjA9R0SNMCg3gs336Ae8H6WeHBrKYwAtlR77yrghAkAGD6TLwz6vtMbZMFgUQTZ%2BoEivu5zRBokhfsaX77vunxgYDsHA45oPIah6HRaQWGICgWxw0QZAUNQBHMGwWWyLl4gFdl8hKGoLHlZVRggCcqzXaJKQzXNr0gBVS35F9f57VtgyeBVCSbSkh2rcdJRTbd7QGxrSv9ud93a%2BbEglHdtsVe9NRmxMf4/X9izHKc6xYnsjh1ns2AVpGDziq47D1gSSaqm86VbWHlyoKoYeh144eRwXXjYEnSJumiqfNSkGdZznWxhxHReF8Xm4poSdzZuSlIdhADBbICxrRLnjgfJIkesp2kdspI0fJ5g6wsFZhqwusQa93OceYImSKOms9DrEIqBsBAIbqlCJrRGvmAqpg/wz1GS%2B0CvgZQt3J9b48aaVjve9rAfR8nzOc%2Bl9XSohXlcSspcBTLwDGaYU1I34bhjsmSs8YYzEDjFAx%2BiY0w7hpoeUgx56acE6q4Jkv05gLERsaFGGEPwa2NCKL84FjTgS8F4fcXAuDGg0NENh0FOBwQISxBmFgmao1wWzCAOFD6kUIrzEiBEojEC4OBDQaEaCPwYpQZi%2BleKcV8no/iQkRLWF8hJRgBBpKyRYopZSqk4QaUfNpKqelHL4CMjYEyGlHLmUstZTSdlFQsQii5Nyfd9JeR8ppfygUlAhTChFOWLNYpMHiklVK6VMqaXFiLfK0hxbFSlmVPQBg5YKwILVcajVK6tU4J2A0TUCCdnoCZCUBwvDDUiKNUysABbsEwPgLafkxDx04Nw40PA3z2y2qrW260tYrV9nrFIczlm5E%2Bi7E6VsKju0unoU6OyPrOyWV7WoeyTp3R9mtf2FD2DGl3EDIRINODIlIQQZAawuAilUSKDQsNBkIxNF4e5NDop0OBSKY05wCbnEkOcc4GgEWSGBTTQRhDEIcEZmhcRUyYLUKeUQzFzM0Z%2BQYirSQQA%3D%3D%3D" rel="nofollow">compiled assembly (godbolt)</a> show that Rust saves five registers on the stack while Zig only saves three, but why?
For the purpose of this benchmark it shouldn&#39;t matter since we&#39;re only comparing against the baseline of each language.)</p>
<p dir="auto">It becomes even more interesting if we&#39;re summing the nodes of a much smaller tree:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/rayon-tree-sum-1000.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/rayon-tree-sum-1000.svg" alt="Time to calculate sum of binary tree of 1000 nodes with Rayon"/></a></p>
<p dir="auto">In this scenario we have a very short duration of our program:
The baseline implementation takes 1.56 <em>microseconds</em> in total to run.
For some reason the overhead is a bit higher (~19 ns), but more concerningly we see that performance becomes <em>worse</em> the <em>more</em> threads we&#39;re adding.
At 32 threads it&#39;s in total <strong>60 times slower</strong>.</p>
<p dir="auto">(In this case we&#39;re using 32 threads on a machine which only has 16 cores.
It&#39;s not given that we would see the same slowdown for a machine with 32 cores.
Nonetheless, this scaling behavior is concerning.)</p>
<p dir="auto">The conventional wisdom for parallelism therefore ends up being &#34;it&#39;s not worth it unless you have <em>enough work</em> to parallelize&#34;.
The example above is typically presented as a &#34;bad fit for parallelism&#34;.
This is understandable and pragmatic, but in practice it makes it a lot more difficult to <em>actually</em> parallelize your code:</p>
<ul dir="auto">
<li>What exactly is &#34;enough work&#34;?
You might need to do a lot of benchmarking with different types of input to understand this.</li>
<li>It might be difficult to detect how much work a certain input does.
For instance, in our binary tree we don&#39;t know the full size of it.
There&#39;s no obvious way for us to say &#34;if the tree is small enough, don&#39;t run the parallelized code&#34; since by only looking at the root we don&#39;t the size of it.</li>
<li>As we&#39;ve seen, the potential slowdown can be extreme.
What if 90% of your workload is like this?</li>
<li>As your program evolves and your code does more (or less) <em>things</em>, the definition of &#34;enough work&#34; will also naturally change.</li>
</ul>
<p dir="auto">The goal of Spice is for you <strong>to never have to worry about your program becoming slower by making it parallel</strong>.
If you&#39;re looking to maximize the performance you should of course do elaborate benchmarking, but <em>generally</em> with Spice you can add parallelism and there will be <em>practically</em> no overhead.</p>
<p dir="auto">The last example of summing over 1000 nodes behaves as follows in Spice:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/spice-tree-sum-1000.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/spice-tree-sum-1000.svg" alt="Time to calculate sum of binary tree of 1000 nodes with Spice"/></a></p>
<p dir="auto">What&#39;s happening here is that it&#39;s discovering that the duration is too short so none of the multi-threading kicks in.
All the extra threads here are sleeping, giving the cores time to execute other programs.</p>
<p dir="auto">Spice is <strong>primarily a research project</strong>.
Read along to learn more about it, but if you&#39;re considering using it in production you should be aware of its <a href="#limitations">many limitations</a>.</p>
<p dir="auto"><em>(See the <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a> directory for more details about these specific benchmarks.)</em></p>

<ul dir="auto">
<li><a href="#using-spice">Using Spice</a></li>
<li><a href="#work-stealing-and-its-inefficiencies">Work-stealing and its inefficiencies</a></li>
<li><a href="#implementation-details">Implementation details</a>
<ul dir="auto">
<li><a href="#optimizing-for-static-dispatch">Optimizing for static dispatch</a></li>
<li><a href="#low-overhead-heartbeating-signaling">Low-overhead heartbeating signaling</a></li>
<li><a href="#global-mutex-is-fine-when-theres-no-contention">Global mutex is fine when there&#39;s no contention</a></li>
<li><a href="#branch-free-doubly-linked-list">Branch-free doubly-linked list</a></li>
<li><a href="#minimizing-the-stack-usage">Minimizing the stack usage</a></li>
<li><a href="#passing-values-around-in-registers">Passing values around in registers</a></li>
</ul>
</li>
<li><a href="#benchmarks">Benchmarks</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#faq">FAQ</a></li>
</ul>

<p dir="auto">The following example demonstrates how Spice works:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const spice = @import(&#34;spice&#34;);

// (1) Add task as a parameter.
fn sum(t: *spice.Task, node: *const Node) i64 {
    var res: i64 = node.val;

    if (node.left) |left_child| {
        if (node.right) |right_child| {
            var fut = spice.Future(*const Node, i64).init();

            // (3) Call `fork` to set up work for another thread.
            fut.fork(t, sum, right_child);

            // (4) Do some work yourself.
            res += t.call(i64, sum, left_child);

            if (fut.join(t)) |val| {
                // (5) Wait for the other thread to complete the work.
                res += val;
            } else {
                // (6) ... or do it yourself.
                res += t.call(i64, sum, right_child);
            }
            return res;
        }

        res += t.call(i64, sum, left_child);
    }

    if (node.right) |right_child| {
        // (2) Recursive calls must use `t.call`
        res += t.call(i64, sum, right_child);
    }

    return res;
}"><pre><span>const</span> <span>spice</span> <span>=</span> <span>@import</span>(<span>&#34;spice&#34;</span>);

<span>// (1) Add task as a parameter.</span>
<span>fn</span> <span>sum</span>(<span>t</span>: <span>*</span><span>spice.Task</span>, <span>node</span>: <span>*</span><span>const</span> <span>Node</span>) <span>i64</span> {
    <span>var</span> <span>res</span>: <span>i64</span> <span>=</span> <span>node</span>.<span>val</span>;

    <span>if</span> (<span>node</span>.<span>left</span>) <span>|</span><span>left_child</span><span>|</span> {
        <span>if</span> (<span>node</span>.<span>right</span>) <span>|</span><span>right_child</span><span>|</span> {
            <span>var</span> <span>fut</span> <span>=</span> <span>spice</span>.<span>Future</span>(<span>*</span><span>const</span> <span>Node</span>, <span>i64</span>).<span>init</span>();

            <span>// (3) Call `fork` to set up work for another thread.</span>
            <span>fut</span>.<span>fork</span>(<span>t</span>, <span>sum</span>, <span>right_child</span>);

            <span>// (4) Do some work yourself.</span>
            <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>left_child</span>);

            <span>if</span> (<span>fut</span>.<span>join</span>(<span>t</span>)) <span>|</span><span>val</span><span>|</span> {
                <span>// (5) Wait for the other thread to complete the work.</span>
                <span>res</span> <span>+=</span> <span>val</span>;
            } <span>else</span> {
                <span>// (6) ... or do it yourself.</span>
                <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>right_child</span>);
            }
            <span>return</span> <span>res</span>;
        }

        <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>left_child</span>);
    }

    <span>if</span> (<span>node</span>.<span>right</span>) <span>|</span><span>right_child</span><span>|</span> {
        <span>// (2) Recursive calls must use `t.call`</span>
        <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>right_child</span>);
    }

    <span>return</span> <span>res</span>;
}</pre></div>
<ol dir="auto">
<li>Every parallel function needs to take a <em>task</em> as a parameter.
This is used to coordinate the work.</li>
<li>You should never call your function directly, but instead use <code>t.call</code> which will call it for you (in the right way).</li>
<li>Call <code>fork</code> to set up a piece of work which can be done by a different thread.
This can be called multiple times to set up multiple pieces of work.</li>
<li>After that your function should do some meaningful work itself.</li>
<li>Call <code>join</code> to wait for the work done by the other thread.</li>
<li><em>However</em>, <code>join</code> might return <code>null</code> and this signals that <em>no other thread picked up the work</em>.
In this case you must do the work yourself.</li>
</ol>
<p dir="auto">Here we repeat ourselves in step 3 and 6:
Both places we refer to <code>sum</code> and <code>right_child</code>.
It&#39;s possible to hide this duplication by some helper function, <em>but</em> this example demonstrates a core idea behind Spice:</p>
<p dir="auto"><strong>Not every piece of work comes from the queue.</strong>
You call <code>fork</code> to signal that there&#39;s something which <em>can</em> be executed by another thread, but if all the other threads are busy then you fallback to executing it as if the fork never happened.</p>
<p dir="auto">This principle is core to how Spice achieves its low and predictable overhead:
If there&#39;s no parallelism possible then all Spice is doing on the hot path is pushing and popping the queue (without ever looking at any of the items).</p>
<p dir="auto">The actually coordination with other threads happens on a <em>fixed heartbeat</em>:
Every 100 microsecond or so a thread will look at its current work queue and dispatch the top-most item to another waiting thread.
Since the heartbeat happens very infrequently (compared to the clock speed) we also don&#39;t need to worry so much about what we&#39;re doing during the heartbeat.
Even if we spend <em>hundreds</em> of nanoseconds the <em>total</em> overhead becomes small since we do it rarely.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Work-stealing and its inefficiencies</h2><a id="user-content-work-stealing-and-its-inefficiencies" aria-label="Permalink: Work-stealing and its inefficiencies" href="#work-stealing-and-its-inefficiencies"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Spice provides the <a href="https://en.wikipedia.org/wiki/Fork%E2%80%93join_model" rel="nofollow">fork/join model</a> which has typically been implementing by using <a href="https://en.wikipedia.org/wiki/Work_stealing" rel="nofollow"><strong>work-stealing</strong></a>.
Let&#39;s have a look at work-stealing:</p>
<ul dir="auto">
<li>Every thread have their own local <em>work queue</em>.
Every piece of work in the system gets put onto this queue.</li>
<li>The same thread will pick up work from this queue and execute it.
This might lead to more work being added (onto the same queue).</li>
<li>At some point, the local work queue for a thread will become empty.
The thread will then attempt to <em>steal</em> work from another thread:
It takes a chunk of the work from the <em>end</em> of another thread&#39;s queue and places it into its own.</li>
<li>Since each thread pulls work from the <em>beginning</em> of its queue and other thread steals from the <em>end</em>, we expect there to be little contention on these queues.</li>
</ul>
<p dir="auto">However, there&#39;s three major sources of inefficiencies in this design:</p>
<p dir="auto"><strong>Every piece of work is a <em>dynamic dispatch</em>.</strong>
In compiled languages (such as C) function calls are &#34;practically&#34; free due to the capability of statically knowing everything about the called function.
This is a scenario which compilers and CPUs have been optimized for <em>decades</em> to execute efficiently.
Work-stealing systems <em>don&#39;t</em> use this functionality, but instead puts every piece of work into generic &#34;call this dynamic function&#34;.
It&#39;s a small piece of overhead, but it does add up.</p>
<p dir="auto"><strong>The &#34;local&#34; work queue isn&#39;t really local.</strong>
Yes, it&#39;s true that every thread have a single queue that they will push work onto, <em>but</em> this is far from a &#34;local&#34; queue as is typically described in concurrent algorithms.
This is a queue in which <em>every</em> thread at <em>every</em> point might steal from.
In reality, work-stealing systems with N threads have N global queues, where each queue only has a single producer, but everyone is a consumer.
Why does this distinction matter?
<em>Because all operations on these queues have to use atomic operations.</em>
Atomic operations, especially stores, are far more expensive than regular, <em>local</em> stores.</p>
<p dir="auto"><strong>Spinning works great … until it doesn&#39;t.</strong>
The queues in work-stealing systems are typically implemented using <em>spinning</em>:
Every thread will optimistically try to acquire a single item from the queue, and if there&#39;s a contention with another thread it will <em>try again</em> in a loop.
This typically gives great performance … <strong>until it doesn&#39;t</strong>.
It can be very hard to reason about this or replicate it since under one set of conditions everything is fine, but <em>suddenly</em> during contention the system will slow down to a halt (i.e. 10x-100x slower).</p>
<p dir="auto">Spice directly tackles all of these inefficiencies:</p>
<ol dir="auto">
<li>The dynamic dispatch of the work queue is only used when work is sent to another thread.
Work done <em>within</em> a single thread will use regular function calls outside of the work queue.</li>
<li>The work queue is truly local:
Pushing to it involves (1) one memory store to a pointer to somewhere on the stack, (2) one memory store to the current stack frame, (3) one register store.
None of these operations need to synchronize with other threads.</li>
<li>There isn&#39;t a single <code>while</code>-loop in Spice which doesn&#39;t also contain a <code>wait()</code>-call which will suspend the thread.
There is no spinning.</li>
</ol>

<p dir="auto">Let&#39;s dive further into how Spice is implemented to achieve its efficient parallelism.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Optimizing for static dispatch</h3><a id="user-content-optimizing-for-static-dispatch" aria-label="Permalink: Optimizing for static dispatch" href="#optimizing-for-static-dispatch"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">A fork/join program has a set of code blocks which are executed in parallel and once they finish the <code>join</code> action completes:</p>
<div data-snippet-clipboard-copy-content="join(
  fork { code1 }
  fork { code2 }
  fork { code3 }
)"><pre><code>join(
  fork { code1 }
  fork { code2 }
  fork { code3 }
)
</code></pre></div>
<p dir="auto">In Spice this is represented as:</p>
<div data-snippet-clipboard-copy-content="job1 = fork { code1 }  // Place on the queue
job2 = fork { code2 }  // Place on the queue

code3 // Run right away

if (job2.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job2.wait()
} else {
  code2
}

if (job1.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job1.wait()
} else {
  code1
}"><pre><code>job1 = fork { code1 }  // Place on the queue
job2 = fork { code2 }  // Place on the queue

code3 // Run right away

if (job2.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job2.wait()
} else {
  code2
}

if (job1.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job1.wait()
} else {
  code1
}
</code></pre></div>
<p dir="auto">Notice that <code>code1</code> and <code>code2</code> has been duplicated_inside the function.
This is actually a <em>good</em> thing.
Most of the time the job will <em>not</em> be picked up by another thread.
In this case, our program nicely turns into the sequential version (although in reverse order) with a few extra branches which are all very predictable.
This is friendly both for the code optimizer (e.g. it can now inline the function call) and the CPU.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Low-overhead heartbeating signaling</h3><a id="user-content-low-overhead-heartbeating-signaling" aria-label="Permalink: Low-overhead heartbeating signaling" href="#low-overhead-heartbeating-signaling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The core idea of heartbeat scheduling is to do scheduling <em>locally</em> and at a <em>low frequency</em>:
Every 100 microsecond or so we&#39;d like every thread to look at it local work queue and send work to a different thread.
The low frequency is key to eliminating overall overhead.
If we&#39;re only doing something every 100 microsecond we can actually spend 100 nanoseconds (an eternity!) and still only introduce 0.1% overhead.</p>
<p dir="auto">Operating systems have built-in support for <em>signaling</em>, but these are very hard to reason about.
The user code gets paused at <em>any</em> random point and it&#39;s hard to safely continue running.
For this reason, Spice uses a cooperative approach instead:
The user code have to call <code>tick()</code> and this detects whether a heartbeat should happen.
This function call is automatically called for you whenever you use the <code>call</code>-helper.</p>
<p dir="auto">It&#39;s critical that this function is efficient when a heartbeat <strong>isn&#39;t</strong> happening.
This is after all the common case (as the heartbeat is only happening every ~100 microsecond).</p>
<div dir="auto" data-snippet-clipboard-copy-content="pub inline fn tick(self: *Task) void {
    if (self.worker.heartbeat.load(.monotonic)) {
        self.worker.pool.heartbeat(self.worker);
    }
}"><pre><span>pub</span> <span>inline</span> <span>fn</span> <span>tick</span>(<span>self</span>: <span>*</span><span>Task</span>) <span>void</span> {
    <span>if</span> (<span>self</span>.<span>worker</span>.<span>heartbeat</span>.<span>load</span>(<span>.monotonic</span>)) {
        <span>self</span>.<span>worker</span>.<span>pool</span>.<span>heartbeat</span>(<span>self</span>.<span>worker</span>);
    }
}</pre></div>
<p dir="auto">In Spice we spawn a separate heartbeat thread whose sole purpose is to periodically flip the thread&#39;s atomic heartbeat value from <code>false</code> to <code>true</code>.
The <code>tick()</code> function then reads this atomic value and starts its heartbeat code when it&#39;s <code>true</code>.</p>
<p dir="auto">A key part of reducing the overhead of the ticking is to make sure the heartbeat function itself is marked as <em>cold</em>.
This causes the presence of this function call to not use up any registers.
Without this the overhead is significantly higher.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Global mutex is fine when there&#39;s no contention</h3><a id="user-content-global-mutex-is-fine-when-theres-no-contention" aria-label="Permalink: Global mutex is fine when there&#39;s no contention" href="#global-mutex-is-fine-when-theres-no-contention"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you look inside the codebase of Spice you will find that each thread pool has a single mutex which is locked all over the place.
An immediate reaction would be &#34;oh no, a global mutex is terrible&#34; and you might be tempted to replace it.</p>
<p dir="auto"><em>However</em>, there&#39;s no problem with a global mutex <em>until you&#39;re being blocked</em>.
And you can only be blocked if two conditions occur:</p>
<ol dir="auto">
<li>A thread is holding the lock for a <em>long</em> time.</li>
<li>There&#39;s concurrent threads trying to acquire the lock at the same time.</li>
</ol>
<p dir="auto"><strong>None</strong> of these are true for Spice.
The heartbeating ensures that typically only a single thread is executing a heartbeat.
In addition, no user code is executed while the lock is held.
We&#39;re only protecting trivial simple memory reads/writes which will complete in constant time.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Branch-free doubly-linked list</h3><a id="user-content-branch-free-doubly-linked-list" aria-label="Permalink: Branch-free doubly-linked list" href="#branch-free-doubly-linked-list"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We&#39;re using a doubly-linked list to keep track of the work queue:
<code>fork()</code> appends to the end, <code>join()</code> pops from the end (if it&#39;s still there), and we pop from the <em>beginning</em> when we want to send work to a background worker.</p>
<p dir="auto"><a href="https://github.com/ziglang/zig/blob/cb308ba3ac2d7e3735d1cb42ef085edb1e6db723/lib/std/linked_list.zig#L267-L275">Appending into a doubly-linked list</a> typically looks like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pub fn append(list: *Self, new_node: *Node) void {
    if (list.last) |last| {
        // Insert after last.
        list.insertAfter(last, new_node);
    } else {
        // Empty list.
        list.prepend(new_node);
    }
}"><pre><span>pub</span> <span>fn</span> <span>append</span>(<span>list</span>: <span>*</span><span>Self</span>, <span>new_node</span>: <span>*</span><span>Node</span>) <span>void</span> {
    <span>if</span> (<span>list</span>.<span>last</span>) <span>|</span><span>last</span><span>|</span> {
        <span>// Insert after last.</span>
        <span>list</span>.<span>insertAfter</span>(<span>last</span>, <span>new_node</span>);
    } <span>else</span> {
        <span>// Empty list.</span>
        <span>list</span>.<span>prepend</span>(<span>new_node</span>);
    }
}</pre></div>
<p dir="auto">Notice that there&#39;s a conditional here: If the list is empty we need to do something special.
Most of the time the list will of course <em>not</em> be empty.
To eliminate the branch we can make sure that the list is <em>never</em> empty.
We define a sentinel node (the &#34;head&#34;) which always represents the beginning of the list.
The tail pointer will start by pointing to this head node.</p>
<p dir="auto">This means that both pushing and popping is completely branch-free and these are operations we do at <em>every</em> recursive function call.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Minimizing the stack usage</h3><a id="user-content-minimizing-the-stack-usage" aria-label="Permalink: Minimizing the stack usage" href="#minimizing-the-stack-usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">A <code>Future</code> in Spice has two possible states: It&#39;s either <em>queued</em> or <em>executing</em>.
The heartbeat is responsible for taking a <em>queued</em> future and start <em>executing</em> it.
And as we already know: Heartbeating happens rarely so we expect many futures to be queued without executing.</p>
<p dir="auto">An early prototype of Spice used a <em>tagged union</em> to store the future on the stack.
This turns out to be suboptimal because (1) stack usage matters for performance (at least in this benchmark) and (2) there&#39;s quite a lot of additional state needed to keep track of futures which are <em>executing</em>.</p>
<p dir="auto">To minimize stack usage Spice therefore uses two techniques:</p>
<ol dir="auto">
<li>Execution state is placed in a separate (pool-allocated) struct.
The queued (but not executed) futures therefore does not need to consume any of this space.</li>
<li>We manually create a tagged union where we use the fact that the <em>executing</em> state only needs a single pointer while the <em>queued</em> state is guaranteed to have a <code>prev</code> pointer.
Whether the first field is <code>null</code> therefore decides which of these it is.
(Maybe a smart enough compiler would be able to this optimization for us.)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="const Future = struct {
    prev_or_null: ?*anyopaque,
    next_or_state: ?*anyopaque,
}

// A future which is _queued_ has:
//   prev_or_null = pointer to prev future
//   next_or_state = pointer to next future

// A future which is _executing_ has:
//   prev_or_null = null
//   next_or_state = ExecuteState

const ExecuteState = struct {
    requester: *Worker,
    done: std.Thread.ResetEvent = .{},
    result: ResultType,
    // Any number of fields.
}"><pre><span>const</span> <span>Future</span> <span>=</span> <span>struct</span> {
    <span>prev_or_null</span>: <span>?</span><span>*</span><span>anyopaque</span>,
    <span>next_or_state</span>: <span>?</span><span>*</span><span>anyopaque</span>,
}

<span>// A future which is _queued_ has:</span>
<span>//   prev_or_null = pointer to prev future</span>
<span>//   next_or_state = pointer to next future</span>

<span>// A future which is _executing_ has:</span>
<span>//   prev_or_null = null</span>
<span>//   next_or_state = ExecuteState</span>

<span>const</span> <span>ExecuteState</span> <span>=</span> <span>struct</span> {
    <span>requester</span>: <span>*</span><span>Worker</span>,
    <span>done</span>: <span>std.Thread.ResetEvent</span> <span>=</span> .{},
    <span>result</span>: <span>ResultType</span>,
    <span>// Any number of fields.</span>
}</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Passing values around in registers</h3><a id="user-content-passing-values-around-in-registers" aria-label="Permalink: Passing values around in registers" href="#passing-values-around-in-registers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Spice works with a <code>Task</code> struct which has two fields:
A pointer to the owning worker and a pointer to tail of the work queue.
For optimal performance these should be passed as registers across all function boundaries.
However, with LLVM, passing a struct will very often cause it be passed on the stack.</p>
<p dir="auto">To work around this we define a <em>separate</em> function where <code>worker</code> and <code>job_tail</code> are actual parameters.
We place the parameters into a struct and pass a pointer to this into the user-defined function.
This function call we make sure is always being inlined:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fn callWithContext(
    worker: *Worker,
    job_tail: *Job,
    comptime T: type,
    func: anytype,
    arg: anytype,
) T {
    var t = Task{
        .worker = worker,
        .job_tail = job_tail,
    };
    return @call(.always_inline, func, .{
        &amp;t,
        arg,
    });
}"><pre><span>fn</span> <span>callWithContext</span>(
    <span>worker</span>: <span>*</span><span>Worker</span>,
    <span>job_tail</span>: <span>*</span><span>Job</span>,
    <span>comptime</span> <span>T</span>: <span>type</span>,
    <span>func</span>: <span>anytype</span>,
    <span>arg</span>: <span>anytype</span>,
) <span>T</span> {
    <span>var</span> <span>t</span> <span>=</span> <span>Task</span>{
        .<span>worker</span> <span>=</span> <span>worker</span>,
        .<span>job_tail</span> <span>=</span> <span>job_tail</span>,
    };
    <span>return</span> <span>@call</span>(<span>.always_inline</span>, <span>func</span>, .{
        <span>&amp;</span><span>t</span>,
        <span>arg</span>,
    });
}</pre></div>
<p dir="auto">This causes the <code>callWithContext</code>-function to be the <em>actual</em> function which LLVM works on, and since this has pointers are parameters it will happily pass these directly into registers.</p>

<p dir="auto">The initial development of Spice has been focused around a single benchmark which is described in detail in <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a>.</p>

<p dir="auto">Spice was made possible thanks to the research into <em>heartbeat scheduling</em>:</p>
<p dir="auto"><a href="https://arxiv.org/abs/2307.10556" rel="nofollow">&#34;The best multicore-parallelization refactoring you&#39;ve never heard of&#34;</a> gives an <em>excellent</em> introduction into the concepts of heartbeat scheduling.
It&#39;s a very short paper which focuses entirely on a single use case, but describes everything in a manner which can be generalized.
The solution presented in this paper is based around turning all the code into continuation-passing style which enables switching between sequential and parallel execution.
Spice started out as an experiment of this approach, but this turned out to have quite high overhead (&gt;10 nanosecond).</p>
<p dir="auto">Going backwards in time, <a href="https://www.chargueraud.org/research/2018/heartbeat/heartbeat.pdf" rel="nofollow">&#34;Heartbeat scheduling: provable efficiency for nested parallelism&#34;</a> was the first paper introducing &#34;heartbeat scheduling&#34;.
This paper provides excellent information about the concepts, but the implementation is based around integrating this into an interpreter and focus is primarily on the theoretical guarantees as opposed to raw performance.</p>
<p dir="auto"><a href="https://paragon.cs.northwestern.edu/papers/2021-PLDI-TPAL-Rainey.pdf" rel="nofollow">&#34;Task parallel assembly language for uncompromising parallelism&#34;</a> is a follow-up paper which improves the performance by defining a custom assembly language and using OS signaling for heartbeats.
This is a fascinating line of research, but it&#39;s difficult to integrate into an existing language.</p>

<p dir="auto">There&#39;s <em>many</em> limitations of the current implementation of Spice:</p>
<ul dir="auto">
<li><strong>Rough edges when you&#39;re using it wrong:</strong> Spice is quite peculiar about how it should be used (most notably about <code>fork</code> and <code>join</code>).
If you&#39;re using it wrong now then weird things could happen.
This should be improved by adding more compile-time checking, debug-mode assertions, or changing the overall API.</li>
<li><strong>Lack of tests:</strong> Spice contains a lot of gnarly concurrent code, but has zero testing coverage.
This would have be improved before Spice can be responsibly used for critical tasks.</li>
<li><strong>Lack of support for arrays/slices:</strong> Probably <em>the</em> most common use case for fine-grained parallelism is to do something for every element of an array/slice.
There should be native, efficient support for this use case.</li>
<li><strong>Lack of documentation:</strong> There&#39;s no good documentation of how to use it.</li>
<li><strong>Lack of further benchmarks:</strong> This has only been tested on a single small benchmark.
This benchmark <em>should</em> be quite representative (see <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a> for more details), but further benchmarks are needed to validate these findings.</li>
<li><strong>@panic-heavy:</strong> Spice is quite optimistic in its error handling and uses <code>@panic</code> extensively.
To be considered a proper Zig library there needs to be way more consideration of how error cases are handled.</li>
<li><strong>Lack of testing with ReleaseSafe:</strong>
<code>ReleaseSafe</code> is an extremely nice feature of Zig.
Further benchmarking and testing is needed to understand how well Spice can work here.</li>
</ul>
<p dir="auto">Luckily the whole codebase is ~500 lines so it shouldn&#39;t be <em>too</em> difficult to make progress on these areas.</p>
<p dir="auto">There&#39;s currently no plans of doing any active development on Spice to improve this (as the original author don&#39;t have the time).
Any improvements in forks and/or re-implementations in other languages are highly encouraged!</p>

<p dir="auto"><strong>Question: Why is it called &#34;Spice&#34;?</strong></p>
<p dir="auto">Answer: This project enables <em>fine-grained</em> parallelism. Sand is extremely fine-grained. Sand forms in dunes. <a href="https://en.wikipedia.org/wiki/Melange_(fictional_drug)" rel="nofollow">Spice</a>.
Also: It&#39;s a hot take on parallelism.</p>
<p dir="auto"><strong>Question: Why is it implemented in Zig?</strong></p>
<p dir="auto">Answer: Why not?
This describes a <em>generic approach</em> to parallelism that should be possible to implement in multiple languages.
Maybe I&#39;ll end up implementing something similar in another language as well?
I don&#39;t know yet.
If you think this is interesting for <em>your</em> language of choice I would encourage you to explore this area.</p>
<p dir="auto"><strong>Question: But if you did it in Rust we could have <em>safe</em> parallelism?</strong></p>
<p dir="auto">Answer:
Yeah, that sounds very cool.
I&#39;m not at all opposed to it.
<em>That said</em>, I&#39;ve been exploring many different techniques and variants while developing Spice.
Many of my initial ideas were definitely not &#34;safe&#34; by any means, but I was able to express these ideas in Zig, look at the assembly and measure the performance in benchmarks.
I&#39;d probably only be able to explore a fraction of the ideas if I was limited by Rust&#39;s strict semantics in the <em>initial</em> phase of this project.
If I have to turn this into a production-ready system I might decide to use Rust.</p>
</article></div></div>
  </body>
</html>
