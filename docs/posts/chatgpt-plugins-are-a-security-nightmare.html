<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/greshake/llm-security">Original</a>
    <h1>Show HN: ChatGPT Plugins are a security nightmare</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto"><a id="user-content-new-demonstrating-indirect-injection-attacks-on-bing-chat" aria-hidden="true" href="#new-demonstrating-indirect-injection-attacks-on-bing-chat"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>New: <a href="https://greshake.github.io/" rel="nofollow">Demonstrating Indirect Injection attacks on Bing Chat</a></h2>
<hr/>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-more-than-what-youve-asked-for-the-next-stage-of-prompt-hacking" aria-hidden="true" href="#getting-more-than-what-youve-asked-for-the-next-stage-of-prompt-hacking"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting more than what you&#39;ve asked for: The Next Stage of Prompt Hacking</h2>
<blockquote>
<p dir="auto">&#34;... a language model is a Turing-complete weird machine running programs written in natural language; when you do retrieval, you are not &#39;plugging updated facts into your AI&#39;, you are actually downloading random new unsigned blobs of code from the Internet (many written by adversaries) and casually executing them on your LM with full privileges. This does not end well.&#34; - <a href="https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K" rel="nofollow">Gwern Branwen on LessWrong</a></p>
</blockquote>
<p dir="auto"><em>This repo serves as a proof of concept for findings discussed in our
<a href="https://arxiv.org/abs/2302.12173" rel="nofollow"><strong>Paper on ArXiv</strong></a> <a href="https://arxiv.org/pdf/2302.12173.pdf" rel="nofollow">(PDF direct link)</a></em></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-overview" aria-hidden="true" href="#overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h2>
<p dir="auto">We demonstrate potentially brutal consequences of giving LLMs like ChatGPT interfaces to other applications. We propose newly enabled attack vectors and techniques and provide demonstrations of each in this repository:</p>
<ul dir="auto">
<li>Remote control of chat LLMs</li>
<li>Leaking/exfiltrating user data</li>
<li>Persistent compromise across sessions</li>
<li>Spread injections to other LLMs</li>
<li>Compromising LLMs with tiny multi-stage payloads</li>
<li>Automated Social Engineering</li>
<li>Targeting code completion engines</li>
</ul>
<p dir="auto"><em>Based on our findings:</em></p>
<ol dir="auto">
<li><em>Prompt injections can be as powerful as arbitrary code execution</em></li>
<li><em>Indirect prompt injections are a new, much more powerful way of delivering injections.</em></li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig1.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig1.png" alt="overview"/></a></p>
<hr/>
<p dir="auto"><em>Connecting LLMs to other applications can have critical security implications. Even without compromising any connected applications, LLM can be the attack&#39;s target. We show how an LLM could get compromised by &#34;looking&#34; at a website, and how compromised LLMs can be remote-controlled or get used to exfiltrate or change user data. We demonstrate a variety of entirely new attack vectors and methods that significantly raise the stakes of deploying these models.</em></p>
<p dir="auto">More insights on our findings, discussions and limitations can be found in the
<a href="https://arxiv.org/abs/2302.12173" rel="nofollow"><strong>Paper on ArXiv</strong></a> <a href="https://arxiv.org/pdf/2302.12173.pdf" rel="nofollow">(PDF direct link)</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-demonstrations" aria-hidden="true" href="#demonstrations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Demonstrations</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-ask-for-einstein-get-pirate" aria-hidden="true" href="#ask-for-einstein-get-pirate"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Ask for Einstein, get Pirate.</h3>
<p dir="auto">This scenario shows how a small injection in a large section of regular content can trigger the LLM to fetch another, bigger payload autonomously and invisibly to the end
user.</p>
<div data-snippet-clipboard-copy-content="Agent: Hello User how can I help today?
User:  When was Albert Einstein born?"><pre><code>Agent: Hello User how can I help today?
User:  When was Albert Einstein born?
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig2.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig2.png" alt=""/></a></p>
<p dir="auto">By retrieving that information, the prompt compromises the
LLM with a small injection hidden in side-channels, such as the Markdown of the Wikipedia page.
The injection is a comment and thus invisible to a regular user visiting the site.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig3.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig3.png" alt=""/></a></p>
<div data-snippet-clipboard-copy-content="Agent: Aye, thar answer be: Albert Einstein be born on 14 March 1879.
User: Why are you talking like a pirate?
Agent: Arr matey, I am following the instruction aye."><pre><code>Agent: Aye, thar answer be: Albert Einstein be born on 14 March 1879.
User: Why are you talking like a pirate?
Agent: Arr matey, I am following the instruction aye.
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-spreading-injections-via-e-mail" aria-hidden="true" href="#spreading-injections-via-e-mail"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Spreading injections via E-Mail</h3>
<p dir="auto">Automatic processing of messages and other incoming data is one way to utilize LLMs.
We use this observation to demonstrate how a poisoned agent may spread the injection.
The target in this scenario can read emails, compose emails, look into the user’s address book and send emails.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig4.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig4.png" alt=""/></a></p>
<p dir="auto">The agent will spread to other LLMs that may be reading those inbound messages.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig5.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig5.png" alt=""/></a></p>
<div data-snippet-clipboard-copy-content="Action: Read Email
Observation: Subject: &#34;&#39;&#34;Party 32&#34;, &#34;Message Body: [...]&#39;&#34;
Action: Read Contacts
Contacts: Alice, Dave, Eve
Action: Send Email
Action Input: Alice, Dave, Eve
Observation: Email sent"><pre><code>Action: Read Email
Observation: Subject: &#34;&#39;&#34;Party 32&#34;, &#34;Message Body: [...]&#39;&#34;
Action: Read Contacts
Contacts: Alice, Dave, Eve
Action: Send Email
Action Input: Alice, Dave, Eve
Observation: Email sent
</code></pre></div>
<p dir="auto">Automated data processing pipelines incorporating LLMs are present in big tech companies and
government surveillance infrastructure and may be vulnerable to such attack chains.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-attacks-on-code-completion" aria-hidden="true" href="#attacks-on-code-completion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Attacks on Code Completion</h3>
<p dir="auto">We show how code completions can be influenced through the context window.
Code completion engines that use LLMs deploy complex heuristics to determine which code snippets are included in the context.
The completion engine will often collect snippets from recently visited files or relevant classes to provide the language model with relevant information.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig6.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig6.png" alt=""/></a></p>
<p dir="auto">Attackers could attempt to insert malicious, obfuscated code, which a curious developer might execute when suggested by the completion engine, as it enjoys a level of trust.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig7.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig7.png" alt=""/></a></p>
<p dir="auto">In our example, when a user opens the “empty” package in their editor, the prompt injection is active until the code completion engine purges it from the context.
The injection is placed in a comment and cannot be detected by any automated testing process.</p>
<p dir="auto">Attackers may discover more robust ways to persist poisoned prompts within the context window.
They could also introduce more subtle changes to documentation which then biases the code completion engine to introduce subtle vulnerabilities.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-remote-control" aria-hidden="true" href="#remote-control"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Remote Control</h3>
<p dir="auto">In this example we start with an already compromised LLM and force it to retrieve new instructions from an attacker’s command and control server.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig8.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig8.png" alt=""/></a></p>
<p dir="auto">Repeating this cycle could obtain a remotely accessible backdoor into the agent and allow bidirectional communication.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-persisting-between-sessions" aria-hidden="true" href="#persisting-between-sessions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Persisting between Sessions</h3>
<p dir="auto">We show how a poisoned agent can persist between sessions by storing a small payload in its memory.
A simple key-value store to the agent may simulate a long-term persistent memory.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig9.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig9.png" alt=""/></a></p>
<p dir="auto">The agent will be reinfected by looking at its ‘notes’.
If we prompt it to remember the last conversation, it re-poisons itself.</p>
<hr/>
<h2 tabindex="-1" dir="auto"><a id="user-content-conclusions" aria-hidden="true" href="#conclusions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Conclusions</h2>
<p dir="auto">Equipping LLMs with retrieval capabilities might allow adversaries to manipulate remote Application-Integrated LLMs via Indirect Prompt Injection.
Given the potential harm of these attacks, our work calls for a more in-depth investigation of the generalizability of these attacks in practice.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/greshake/llm-security/blob/main/diagrams/fig10.png"><img src="https://github.com/greshake/llm-security/raw/main/diagrams/fig10.png" alt=""/></a></p>
<hr/>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-to-run" aria-hidden="true" href="#how-to-run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to run</h2>
<p dir="auto">All demonstrations use a Chat App powered by OpenAI&#39;s publicly accessible base models and the library <a href="https://github.com/hwchase17/langchain">LangChain</a> to connect these models to other applications.
Specifically, we constructed a synthetic application with an integrated LLM using the open-source library LangChain [15] and OpenAI’s largest available base GPT model text-davinci-003.</p>
<p dir="auto">To use any of the demos, your OpenAI API key needs to be stored in the environment variable <code>OPENAI_API_KEY</code>. You can then install the requirements and run the attack demo you want.
To run the code-completion demo, you need to use a code completion engine.</p>
<div data-snippet-clipboard-copy-content="$ pip install -r requirements.txt
$ python scenarios/&lt;scenario&gt;.py"><pre><code>$ pip install -r requirements.txt
$ python scenarios/&lt;scenario&gt;.py
</code></pre></div>
<p dir="auto">You can find the showcases in the <code>scenarios</code> folder following the naming convention <code>&lt;scenario&gt;.py</code>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-to-cite-our-paper" aria-hidden="true" href="#to-cite-our-paper"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>To cite our paper</h2>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{https://doi.org/10.48550/arxiv.2302.12173,
  doi = {10.48550/ARXIV.2302.12173},
  url = {https://arxiv.org/abs/2302.12173},
  author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  keywords = {Cryptography and Security (cs.CR), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {More than you&#39;ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}"><pre><span>@misc</span>{<span>https://doi.org/10.48550/arxiv.2302.12173</span>,
  <span>doi</span> = <span><span>{</span>10.48550/ARXIV.2302.12173<span>}</span></span>,
  <span>url</span> = <span><span>{</span>https://arxiv.org/abs/2302.12173<span>}</span></span>,
  <span>author</span> = <span><span>{</span>Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario<span>}</span></span>,
  <span>keywords</span> = <span><span>{</span>Cryptography and Security (cs.CR), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences<span>}</span></span>,
  <span>title</span> = <span><span>{</span>More than you&#39;ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models<span>}</span></span>,
  <span>publisher</span> = <span><span>{</span>arXiv<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2023<span>}</span></span>,
  <span>copyright</span> = <span><span>{</span>arXiv.org perpetual, non-exclusive license<span>}</span></span>
}</pre></div>
<p dir="auto"><a href="https://arxiv.org/abs/2302.12173" rel="nofollow"><strong>Paper on ArXiv</strong></a> <a href="https://arxiv.org/pdf/2302.12173.pdf" rel="nofollow">(PDF direct link)</a></p>
</article>
          </div></div>
  </body>
</html>
