<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/ai-for-coding">Original</a>
    <h1>AI’s coding evolution hinges on collaboration and trust</h1>
    
    <div id="readability-page-1" class="page"><div data-headline="Why AI Isn’t Ready to Be a Real Coder"><div><p><a href="https://spectrum.ieee.org/topic/artificial-intelligence/">Artificial intelligence</a> (AI) has transformed the coding sphere, with <a href="https://spectrum.ieee.org/best-ai-coding-tools" target="_self">AI coding tools</a> completing <a href="https://spectrum.ieee.org/tag/source-code">source code</a>, correcting syntax errors, creating inline documentation, and understanding and answering questions about a codebase. As the technology advances beyond automating <a href="https://spectrum.ieee.org/tag/programming" target="_self">programming</a> tasks, the idea of full autonomy looms large. Is AI ready to be a <em><em>real</em></em> coder?</p><p>A <a href="https://arxiv.org/pdf/2503.22625" rel="noopener noreferrer" target="_blank">new paper</a> says not yet—and maps out exactly why. Researchers from <a href="https://www.cornell.edu/" rel="noopener noreferrer" target="_blank">Cornell University</a>, <a href="https://www.csail.mit.edu/node/2873" rel="noopener noreferrer" target="_blank">MIT CSAIL</a>, <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford University</a>, and <a href="https://www.berkeley.edu/" rel="noopener noreferrer" target="_blank">UC Berkeley</a> highlight key challenges that today’s <a href="https://spectrum.ieee.org/tag/ai-models">AI models</a> face and outline promising research directions to tackle them. They presented their work at the <a href="https://icml.cc/Conferences/2025" rel="noopener noreferrer" target="_blank">2025 International Conference on Machine Learning</a>.</p><p>The study offers a clear-eyed reality check amid all the hype. “At some level, the technology is powerful and useful already, and it has gotten to the point where <a href="https://spectrum.ieee.org/tag/programming">programming</a> without these tools just feels primitive,” says <a href="https://www.csail.mit.edu/person/armando-solar-lezama" rel="noopener noreferrer" target="_blank">Armando Solar-Lezama</a>, a co-author of the paper and an associate director at <a href="https://spectrum.ieee.org/tag/mit">MIT</a> <a href="https://spectrum.ieee.org/tag/csail">CSAIL</a>, where he leads the computer-aided programming group. He argues, however, that AI-powered <a href="https://spectrum.ieee.org/tag/software-development" target="_self">software development</a> has yet to reach “the point where you can really collaborate with these tools the way you can with a human programmer.”</p><h2>Challenges With AI Coding Tools</h2><p>According to the study, AI still struggles with several crucial facets of coding: sweeping scopes involving huge codebases, the extended context lengths of millions of lines of code, higher levels of logical complexity, and long-horizon or long-term planning about the structure and design of code to maintain code quality.</p><p><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/ksen.html" rel="noopener noreferrer" target="_blank">Koushik Sen</a>, a professor of computer science at <a href="https://spectrum.ieee.org/tag/uc-berkeley">UC Berkeley</a> and also a co-author of the paper, cites fixing a <a href="https://spectrum.ieee.org/memory-safe-programming-languages" target="_self">memory safety</a> bug as an example. (Such bugs can cause crashes, corrupt data, and open security vulnerabilities.) <a href="https://spectrum.ieee.org/tag/software-engineers" target="_self">Software engineers</a> might approach debugging by first determining where the error originates, “which might be far away from where it’s crashing, especially in a large codebase,” Sen explains. They’ll also have to understand the semantics of the code and how it works, and make changes based on that understanding. “You might have to not only fix that bug but change the entire memory management,” he adds.</p><p>These kinds of complex tasks can be difficult for AI development tools to navigate, resulting in <a href="https://spectrum.ieee.org/ai-hallucination" target="_self">hallucinations</a> about where the bug is or its root cause, as well as irrelevant suggestions or code fixes with subtle problems. “There are many failure points, and I don’t think the current <a href="https://spectrum.ieee.org/tag/llms" target="_self">LLMs</a> [<a href="https://spectrum.ieee.org/tag/large-language-models">large language models</a>] are good at handling that,” says Sen.</p><p>Among the various paths suggested by the researchers toward solving these AI coding challenges—such as training code LLMs to better collaborate with humans and ensuring human oversight for machine-generated code—the human element endures.</p><p>“A big part of <a href="https://spectrum.ieee.org/tag/software-development">software development</a> is building a shared vocabulary and a shared understanding of what the problem is and how we want to describe these features. It’s about coming up with the right metaphor for the architecture of our system,” Solar-Lezama says. “It’s something that can be difficult to replicate by a machine. Our interfaces with these tools are still quite narrow compared to all the things that we can do when interacting with real colleagues.”</p><h2>Enhancing AI-Human Collaboration in Coding</h2><p>Creating better interfaces, which today are driven by <a href="https://spectrum.ieee.org/prompt-engineering-is-dead" target="_self">prompt engineering</a>, is integral for developer productivity in the long run. “If it takes longer to explain to the system all the things you want to do and all the details of what you want to do, then all you have is just programming by another name,” says Solar-Lezama.</p><p><a href="https://cse.nd.edu/faculty/shreya-kumar/" rel="noopener noreferrer" target="_blank">Shreya Kumar</a>, a software engineer and an associate teaching professor in computer science at the <a href="https://www.nd.edu/" rel="noopener noreferrer" target="_blank">University of Notre Dame</a> who was not involved in the research, echoes the sentiment. “The reason we have a programming language is because we need to be unambiguous. But right now, we’re trying to adjust the prompt [in a way] that the tool will be able to understand,” she says. “We’re adapting to the tool, so instead of the tool serving us, we’re serving the tool. And it is sometimes more work than just writing the code.”</p><p>As the study notes, one way to address the dilemma of human-AI interaction is for AI systems to learn to quantify uncertainty and communicate proactively, asking for clarification or more information when faced with vague instructions or unclear scenarios. Sen adds that AI models might also be “missing context that I have in my mind as a developer—hidden concepts that are embedded in the code but hard to decipher from it. And if I give any hint to the LLM about what is happening, it might actually make better progress.”</p><p>For <a href="https://www.comp.nus.edu.sg/cs/people/abhik/" rel="noopener noreferrer" target="_blank">Abhik Roychoudhury</a>, a professor of computer science at the <a href="https://www.nus.edu.sg/" rel="noopener noreferrer" target="_blank">National University of Singapore</a> who was also not involved in the research, a crucial aspect missing from the paper and from most AI-backed software development tools entails capturing user intent.</p><p>“A software engineer is doing a lot of thinking in understanding the intent of the code. This intent inference—what the program is trying to do, what the program is supposed to do, and the deviation between the two—is what helps in a lot of <a href="https://spectrum.ieee.org/tag/software-engineering">software engineering</a> tasks. If this outlook can be brought in future AI offerings for software engineering, then it will get closer to what the software engineer does.”</p><h2>Where Does AI Coding Go From Here? </h2><p>Roychoudhury also assumes that many of the challenges identified in the paper are either being worked on now or “would be solved relatively quickly” due to the rapid pace of progress in AI for software engineering. Additionally, he believes that an <a href="https://spectrum.ieee.org/tag/agentic-ai" target="_self">agentic AI</a> approach can help, viewing significant promise in <a href="https://spectrum.ieee.org/tag/agentic-ai">AI agents</a> for processing requirements specifications and ensuring they can be enforced at the code level.</p><p>“I feel the <a href="https://spectrum.ieee.org/tag/automation">automation</a> of software engineering via agents is probably irreversible. I would dare say that it is going to happen,” Roychoudhury says.</p><p>Sen is of the same view but looks beyond agentic AI initiatives. He pinpoints ideas such as <a href="https://spectrum.ieee.org/evolutionary-ai-coding-agents" target="_self">evolutionary algorithms to enhance AI coding skills</a> and projects like <a href="https://spectrum.ieee.org/deepmind-alphaevolve" target="_self">AlphaEvolve</a> that employ <a href="https://spectrum.ieee.org/fighting-buggy-code-with-genetic-algorithms" target="_self">genetic algorithms</a> “to shuffle the solutions, pick the best ones, and then continue improving those solutions. We need to adopt a similar technology for coding agents, where the code is continuously improving in the background.”</p><p>However, Roychoudhury cautions that the bigger question lies in “whether you can <a href="https://spectrum.ieee.org/chatgpt-reliability" target="_self">trust</a> the agent, and this issue of trust will be further exacerbated as more and more of the coding gets automated.”</p><p>That’s why human supervision remains vital. “There should be a check and verify process. If you want a trustworthy system, you do need to have humans in the loop,” says Notre Dame’s Kumar.</p><p>Solar-Lezama agrees. “I think it’s always going to be the case that we’re ultimately going to want to build software for people, and that means we have to figure out what it is we want to write,” he says. “In some ways, achieving full automation really means that we get to now work at a different level of abstraction.”</p><p>So while AI may become a “real coder” in the near future, Roychoudhury acknowledges that it probably won’t gain software developers’ complete trust as a team member, and thus might not be allowed to do its tasks fully autonomously. “That team dynamics—when an AI agent can become a member of the team, what kind of tasks will it be doing, and how the rest of the team will be interacting with the agent—is essentially where the human-AI boundary lies,” he says.</p></div></div></div>
  </body>
</html>
