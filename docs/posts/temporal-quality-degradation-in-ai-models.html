<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nannyml.com/blog/91-of-ml-perfomance-degrade-in-time">Original</a>
    <h1>Temporal quality degradation in AI models</h1>
    
    <div id="readability-page-1" class="page"><div><p>A recent study from MIT, Harvard, The University of Monterrey, and other top institutions showed an experiment where <a href="https://www.nature.com/articles/s41598-022-15245-z">91% of their ML models degrade over time</a>. This study is one of the first of its kind, where researchers focus on studying machine learning models&#39; behavior after deployment and how their performance evolves with unseen data.</p><blockquote><em>“While much research has been done on various types and markers of temporal data drifts, there is no comprehensive study of how the models themselves can respond to these drifts.”</em></blockquote><p>Since we at NannyML are on the mission of babysitting ML models to avoid degradation issues, this paper caught our eye. This blog post will review the most critical parts of the research, highlight their results, and stress the importance of these results, especially for the ML industry.</p><p>If you have been previously exposed to concepts like <a href="https://www.nannyml.com/blog/types-of-data-shift">covariate shift</a> or <a href="https://www.nannyml.com/blog/types-of-data-shift-2">concept drift</a>, you may be aware that changes in the distribution of the production data may affect the model&#39;s performance. This phenomenon is one of the challenges of maintaining an ML model in production.</p><p>By definition, ML models depend on the data it was trained on, meaning that if the distribution of the production data starts to change, the model may no longer perform as well as before. And as time passes, the model&#39;s performance may degrade more and more. The authors like to refer to this phenomenon as <em>&#34;AI aging.&#34;</em> At NannyML, we call it model performance deterioration and depending on how significant the drop in performance is, we consider it an ML model failure.</p><p>The authors developed a testing framework for identifying temporal model degradation to get a better understanding of this phenomenon. Then, they applied the framework to 32 datasets from four industries, using four standard ML models to investigate how temporal model degradation can develop under minimal drifts in the data.</p><p>To avoid any model bias, the authors chose four different standard ML methods (Linear Regression, Random Forest Regressor, XGBoost, and a Multilayer Perceptron Neural Network). Each of these methods represents different mathematical approaches to learning from data. By choosing different model types, they were able to investigate similarities and differences in the way diverse models can age <strong>on the same data</strong>.</p><p>Similarly, to avoid domain bias, they chose 32 datasets from four industries (Healthcare, Weather, Airport Traffic, and Financial).</p><p>Another critical decision is that they only investigated pairs of model-dataset with good initial performance. This decision is crucial since it is not worthwhile investigating the degradation of a model with a poor initial fit.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/6435566768eeb81988fe89f3_datasets%20(1).jpg" loading="lazy" alt=""/></p><figcaption>Examples of original data used in temporal degradation experiments. The timeline is on the horizontal axis and, each dataset target variable is on the vertical axis. When multiple data points were collected per day, they were shown with background color and a moving daily average curve. The colors highlighting the titles are going to be used along the blog post to easily recognize each dataset industry. Retrieved from the original paper, annotated by the author.</figcaption></figure><p>To identify temporal model performance degradation, the authors designed a framework that emulates a typical production ML model. And ran multiple dataset-model experiments following this framework.</p><p>For each experiment, they did four things:</p><ul role="list"><li>Randomly select one year of historical data as training data</li><li>Select an ML model</li><li>Randomly pick a future datetime point where they will test the model</li><li>Calculate the model’s performance change</li></ul><p>To better understand the framework we need a couple of definitions. The most recent point in the training data was defined as \(t_0\). The number of days between $t_0$ and the point in the future where they test the model was defined as \(dT\), which symbolizes the model&#39;s age.</p><p>For example, a weather forecasting model was trained with data from January 1st to December 31st of 2022. And on February 1st, 2023, we ask it to make a weather forecast.</p><p>In this case</p><ul role="list"><li>\(t_0\) = December 31st, 2022 since it is the most recent point in the training data.</li></ul><ul role="list"><li> \(dT\) = 32 days (days from December 31st and February 1st). This is the age of the model.</li></ul><p>The diagram below summarizes how they performed every &#34;history-future&#34; simulation. We have added annotations to make it easier to follow.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/6435704dbcb560e8a9f06623_Untitled%20(Draft)-1%202.jpg" loading="lazy" alt=""/></p><figcaption>Diagram of the AI temporal degradation experiment. Retrieved from the original paper, annotated by the author.</figcaption></figure><p>To quantify the model’s performance change, they measured the mean squared error (MSE) at time \(t_0\) as \(MSE(t_0)\) and at the time of the model evaluation as  \(MSE(t_1)\).</p><p>Since \(MSE(t_0)\) is supposed to be low (each model was generalizing well at dates close to training). One can measure the relative performance error as the ratio between \(MSE(t_0)\) and \(MSE(t_1)\).</p><p>$E_{rel}(dT) = \dfrac{MSE(t_1)}{MSE(t_0)}$</p><p>The researchers ran 20,000 experiments of this type for each dataset-model pair! Where \(t_0\)  and \(dT\) were randomly sampled from a uniform distribution.</p><p>After running all of these experiments, they reported an <em>aging model chart</em> for each dataset-model pair. This chart contains 20,000 purple points, each representing the relative performance error \(E_{rel}\) obtained at \(dT\) days after training.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/6435587d384b40e15a236f64_explanation_experiments%20(1).jpg" loading="lazy" alt=""/></p><figcaption>Model aging chart for the Financial dataset and the Neural Network model. Each small dot represents the outcome of a single temporal degradation experiment. Retrieved from the original paper, annotated by the author.</figcaption></figure><p>The chart summarizes how the model&#39;s performance changes when the model&#39;s age increases.</p><p>Key takeaways:</p><ol role="list"><li><strong>The error increases over time:</strong> the model becomes less and less performant as time passes. This may be happening due to a drift present in any of the model&#39;s features or due to concept drift.</li><li><strong>The error variability increases over time:</strong> The gap between the best and worst-case scenarios increases as the model ages. When an ML model has high error variability, it means that it sometimes performs well and sometimes badly. The model performance is not just degrading, but it has erratic behavior.</li></ol><blockquote><em>The reasonably low median model error may still create the illusion of accurate model performance while the actual outcomes become less and less certain.</em></blockquote><p>After performing all the experiments for all 4 (models) x 32 (datasets) = 128 (model, dataset) pairs, temporal model degradation was observed in <strong>91% of the cases</strong>. Here we will look at the four most common degradation patterns and their impact on ML model implementations.</p><h2>Gradual or no degradation</h2><p>Although no strong degradation was observed in the two examples below, these results still present a challenge. Looking at the original Patient and Weather datasets, we can see that the patient data has a lot of outliers in the Delay variable. In contrast, the weather data has seasonal shifts in the Temperature variable. But even with these two behaviors in the target variables, both models seem to perform accurately over time.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/643558a91b43c6debdf19626_gradual_degradation%20(1).jpg" loading="lazy" alt=""/></p><figcaption>Gradual ML model degradation patterns, with relative model error increasing no faster than linearly over time. Retrieved from the original paper, annotated by the author.</figcaption></figure><p>The authors claim that these and similar results demonstrate that data drifts alone cannot be used to explain model failures or trigger model quality checks and retraining.</p><p>We have also observed this in practice. Data drift does not necessarily translates into a model performance degradation. That is why in <a href="https://www.notion.so/efc658f138e047e9bceb51d6975d0153">NannyML&#39;s ML monitoring workflow</a>, we focus on performance monitoring and use data drift detection tools only to investigate plausible explanations of the degradation issue since data drifts alone should not be used to trigger model quality checks.</p><h2>Explosive degradation</h2><p>Model performance degradation can also escalate very abruptly. Looking at the plot below, we can see that both models were performing well in the first year. But at some point, they started to degrade at an explosive rate. The authors claim that these degradations can&#39;t be explained alone by a particular drift in the data.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/643558c8946c6e3ad5731095_explosive_degradation%20(1).jpg" loading="lazy" alt=""/></p><figcaption>Explosive ML model aging patterns. Retrieved from the original paper, annotated by the author.</figcaption></figure><p>Let&#39;s compare two model aging plots made from the same dataset but with different ML models. On the left, we see an explosive degradation pattern, while on the right, almost no degradation was seen. Both models were performing well initially, but the neural network seemed to degrade in performance faster than the linear regression (labeled as RV model).</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/643558e296b001e1572bf4b6_degradation_comparison%20(1).jpg" loading="lazy" alt=""/></p><figcaption>Explosive and no degradation patterns. Retrieved from the original paper, annotated by the author.</figcaption></figure><p>Given this, and similar results, the authors concluded that <em>Temporal model quality depends on the choice of the ML model and its stability on a certain data set.</em></p><p>In practice, we can deal with this type of phenomenon by continuously monitoring the estimated model performance. This allows us to address the performance issues before an explosive degradation is found.</p><h2>Increase in error variability</h2><p>While the yellow (25th percentile) and the black (median) lines remain at relatively low error levels, the gap between them and the red line (75th percentile) increases significantly with time. As mentioned before, this may create the illusion of an accurate model performance while the real model outcomes become less and less certain.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/64355935b29d156393bc34e9_variability%20(1).jpg" loading="lazy" alt=""/></p><figcaption>Increasing unpredictability AI model aging patterns. Retrieved from the original paper, annotated by the author.</figcaption></figure><blockquote><em>Neither the data nor the model alone can be used to guarantee consistent predictive quality. Instead, the temporal model quality is determined by the stability of a specific model applied to the specific data at a particular time.</em></blockquote><p>Once we have found the underlying cause of the model again problem, we can search for the best technique to fix the problem. The appropriate solution is context-dependent, so there is no simple fix that fits every problem.</p><p>Every time we see a model performance degradation, we should investigate the issue and understand the cause of it. Automatic fixes are almost impossible to generalize for every situation since the degradation issue can be caused by multiple reasons.</p><p>In the paper, the authors proposed a potential solution to the temporal degradation problem. It is focused on ML model retraining and assumes that we have access to newly labeled data, that there are no data quality issues, and that there is no concept drift. To make this solution practically feasible, they mentioned that one needs the following:</p><h4><strong>1. Alert when your model must be retrained.</strong>‍</h4><p>Alerting when the model&#39;s performance has been degrading is not a trivial task. One needs access to the latest ground truth or be able to estimate the model&#39;s performance. Solutions like <a href="https://go.nannyml.com/nanny-github">NannyML</a> can help to do that. For example, NannyML uses probabilistic methods to estimate the model&#39;s performance even when targets are absent. They monitor the estimated performance and alert when the model has degraded.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets.website-files.com/6099466f98d9383d91745b9e/63e7b3352d6553991a643f11_estimate-post-deployment-model-performance.gif" loading="lazy" alt=""/></p><figcaption>Realized and estimated model performance after deployment. A degradation alert is triggered when the estimated performance goes below a performance threshold.</figcaption></figure><h4><strong>2. Develop an efficient and robust mechanism for automatic model retraining.</strong>‍</h4><p>If we know that there is no data quality issue or concept drift, frequently retraining the ML model with the latest labeled data could help. However, this may cause new challenges, such as lack of model convergence, suboptimal changes to the training parameters, and <a href="https://en.wikipedia.org/wiki/Catastrophic_interference">&#34;catastrophic forgetting&#34;</a> which is the <em>tendency of an artificial neural network to abruptly forget previously learned information upon learning new information.</em></p><h4><strong>3. Have constant access to the most recent ground truth.</strong>‍</h4><p>The most recent ground truth will allow us to retrain the ML model and calculate the realized performance. The problem is that in practice, ground truth is often delayed, or it is expensive and time-consuming to get newly labeled data.</p><p>When retraining is very expensive, one potential solution would be to have a model catalog and then use the estimated performance to select the model with the best-expected performance. This could fix the issue of different models aging differently on the same dataset.</p><p>Other popular solutions used in the industry are reverting your model back to a previous checkpoint, fixing the issue downstream, or changing the business process. To learn more about when it is best to apply each solution check out our previous blog post on <a href="https://nannyml.com/blog/6-ways-to-address-data-distribution-shift">How to address data distribution shift</a>.</p><h2>Conclusions</h2><p>The study by Vela et al. showed that the ML model&#39;s performance doesn&#39;t remain static, even when they achieve high accuracy at the time of deployment. And that different ML models age at different rates even when trained on the same datasets. Another relevant remark is that not all temporal drifts will cause performance degradation. Therefore, the choice of the model and its stability also becomes one of the most critical factors in dealing with performance temporal degradation.</p><p>These results give a theoretical backup of why tools like <a href="https://go.nannyml.com/nanny-github">NannyML</a> are important for the ML industry. Furthermore, it shows that ML model performance is prone to degradation. This is why every production ML model must be monitored. Otherwise, the model may fail without alerting the businesses.</p><p>If you want to know more about how to monitor your ML models, check out <a href="https://www.notion.so/Monitoring-Workflow-for-Machine-Learning-Systems-f9c2e382e8ff4381af4963b0c4100fe1">Monitoring Workflow for Machine Learning Systems</a>.</p><h2>References</h2><ul role="list"><li>Vela, D., Sharp, A., Zhang, R., <em>et al.</em> Temporal quality degradation in AI models. <em>Sci Rep</em> 12, 11654 (2022). <a href="https://doi.org/10.1038/s41598-022-15245-z">https://doi.org/10.1038/s41598-022-15245-z</a></li></ul><p>‍</p><p>NannyML is fully open-source, so don’t forget to support us with a ⭐ on <a href="https://github.com/NannyML/nannyml" target="_blank">Github</a>! If you want to learn more about how to use NannyML in production, check out our other <a target="_blank" href="https://nannyml.readthedocs.io/en/stable/index.html">docs</a> and <a target="_blank" href="https://www.nannyml.com/blog">blogs</a>!</p></div></div>
  </body>
</html>
