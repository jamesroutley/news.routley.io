<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cedardb.com/blog/ssd_latency/">Original</a>
    <h1>When are SSDs slow?</h1>
    
    <div id="readability-page-1" class="page"><div><p>Database system developers have a complicated relationship with storage devices:
They can store terabytes of data cheaply, and everything is still there after a system crash.
On the other hand, storage can be a spoilsport by being slow when it matters most.</p><p>This blog post shows</p><ul><li>how SSDs are used in database systems,</li><li>where SSDs have limitations,</li><li>and how to get around them.</li></ul><h2 id="when-are-ssds-fast">When are SSDs <em>fast</em>?</h2><p>When we colloquially talk about speed, we usually think in terms of <em>throughput</em>, i.e., how much data we can store or retrieve per second.
Let’s use the fantastic <a href="https://github.com/louwrentius/fio-plot">bench-fio tool</a> to measure how a consumer-grade Crucial T700 SSD used in one of our build servers performs for random reads:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>bench-fio --size<span>=</span>10G --target<span>=</span>./fiotest --type file --mode randread --output consumerSSD --destructive
</span></span></code></pre></div><div><p><img src="https://cedardb.com/blog/ssd_latency/read_throughput.jpg" alt=""/></p></div><p>Almost 6 GB/s! As you can see, modern SSDs are simply amazing when it comes to read throughput.
Since analytical queries tend to read a lot of data, SSDs can make full use of their high read throughput as long as two conditions are fulfilled:</p><ol><li>We use multiple threads to access the SSD in parallel (<code>numjobs</code>).</li><li>Each thread keeps multiple requests in flight to keep the SSD busy (<code>iodepth</code>).</li></ol><p>CedarDB is designed to take advantage of this “happy place” when processing analytical queries:
Through CedarDB’s <a href="https://cedardb.com/docs/technology/#many-core-execution">morsel-based parallelism approach</a>, each query is executed by many threads that can process read requests in parallel.
To keep the read queue full, CedarDB also uses <code>io_uring</code>, a Linux syscall available in newer kernels specifically designed for more efficient I/O, to asynchronously queue multiple requests in parallel.</p><h2 id="when-are-ssds-slow">When are SSDs <em>slow</em>?</h2><p>Unfortunately, not all database workloads require only high read throughput.
After all, we also want to insert new data or update existing data, which incurs writes at a <em>low</em> queue depth.
While SSDs also have acceptable write throughput (as long as their internal cache is not exhausted),
database inserts and updates are bottlenecked by <em>latency</em>,
i.e., how long it takes to store some bytes and tell the user when everything is done.</p><p>Take a look at the latency histogram below:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>bench-fio --size<span>=</span>10G --target<span>=</span>./fiotest --type file --mode randwrite --output latency --numjobs<span>=</span><span>1</span> --iodepth<span>=</span><span>1</span> --destructive
</span></span></code></pre></div><div><p><img src="https://cedardb.com/blog/ssd_latency/write_latency.svg" alt=""/></p></div><p>Doesn’t look too bad, does it?
Almost all write requests complete in 10 microseconds. So each thread could theoretically persist 100,000 inserts per second to the SSD.
This is in line with SSD vendors who promise <a href="https://www.crucial.com/ssd/t700/ct1000t700ssd3">1500k IOPS and more</a>.</p><p>Unfortunately, your operating system is lying to you: When your database system does a <code>write()</code>, it marks the data to be persisted but there is no guarantee that it has actually reached the physical storage medium yet.
If your system crashes at exactly the wrong time, data that you thought you had written to disk may not be there after a restart.
However, your database system should be able to guarantee you that your committed data is durable (the “D” in <a href="https://en.wikipedia.org/wiki/ACID">ACID</a>).</p><p>If you want your data to be stored persistent <em>for real</em>, you need to issue a <code>sync</code> command to make the operating system block and only return control after the data has been persisted.
Let’s have look at how this affects latency:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>bench-fio --size<span>=</span>10G --target<span>=</span>./fiotest --type file --mode randwrite --output latencydsync --numjobs<span>=</span><span>1</span> --iodepth<span>=</span><span>1</span> --destructive --extra-opts <span>sync</span><span>=</span>dsync
</span></span></code></pre></div><div><p><img src="https://cedardb.com/blog/ssd_latency/write_latency_dsync.svg" alt=""/></p></div><p>Oops! The latency is now around 500 microseconds with a tail of over a millisecond!
If we actually care about durability, we’ve just been <em>heavily</em> downgraded.
So, a simple</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>for</span> value <span>in</span> values: 
</span></span><span><span>  insert(value)
</span></span></code></pre></div><p>will <strong>at most</strong> insert ~2000 values per second!</p><h2 id="solving-the-mess">Solving the Mess</h2><p>Such low throughput is simply unacceptable.
What options do we have to mitigate the high latency of storage?</p><h3 id="group-commits">Group Commits</h3><p>Instead of writing every single change to the SSD immediately, we can instead queue many such writes and persist the whole queue in one go.
Instead of having one latency-sensitive round trip per commit, we now have one round trip per queue flush, let’s say every 100 commits.
We just have to be careful not to tell the user that their data has been committed before the queue containing their data has been flushed.</p><p>Group commits really shine when there are many users and clients inserting data in parallel, so that the queue is never empty.
On the other hand, when not much data is being written, group commits actually make the problem worse:
If no new data is coming in, the database system has to bite the bullet and flush the nearly empty queue <em>at some point</em>, or else you will have to wait indefinitely.
This will undoubtedly make you doubly unhappy: You have to endure the SSD latency <em>and</em> the group commit window.</p><p>Database systems can mitigate this problem somewhat by dynamically changing the group commit window based on load.
However, group commits are still only advantageous if the load on the system is not negligible.</p><h3 id="asynchronous-processing">Asynchronous Processing</h3><p>Another solution is to use asynchronous processing and just never wait for the SSD.
There are two ways to do this:</p><h4 id="adapting-the-application">Adapting the Application</h4><p>We could encourage the developer to rewrite their application code to never wait for the “OK” from the database system, e.g. by using <a href="https://www.postgresql.org/docs/current/libpq-pipeline-mode.html">pipeline mode</a> which keeps multiple transactions in flight at all times.</p><p>While we definitely <em>recommend</em> using pipeline mode whenever possible as it also hides network latency, it’s not possible if the application’s next decision depends on the result of the previous query.</p><h4 id="asynchronous-commits">Asynchronous Commits</h4><p>Alternatively, we could enable <a href="https://www.postgresql.org/docs/current/wal-async-commit.html">“asynchronous commits”</a>.
If enabled, the database system will give the application its OK without waiting for the disk to complete the <code>sync</code>.
The downside of this approach is that we might lose a few milliseconds of data in the event of a crash, i.e., any data whose <code>sync</code> request was inflight.</p><p>Is this trade-off worth it?
Ultimately, it is up to you as a programmer to decide whether your use case can tolerate losing a small amount of data that the database system has told you has already been committed.</p><p>Case in point: MacOS does not even do durable writes when you <a href="https://mjtsai.com/blog/2022/02/17/apple-ssd-benchmarks-and-f_fullsync/">explicitly issue a <code>fsync</code> call</a>.
For durable writes, <code>fcntl(F_FULLFSYNC)</code> might work, but Apple’s documentation still describes this as a <a href="https://developer.apple.com/documentation/xcode/reducing-disk-writes">“best effort guarantee”</a>.
But are Apple products notorious for losing data?
It certainly hasn’t hurt their bottom line.</p><h3 id="server-grade-ssds">Server-Grade SSDs</h3><p>Enterprise SSDs, which you are hopefully using in production environments, have a capacitor-backed write cache (marketed as Power-Loss Protection), and <code>sync</code> calls virtually become a “no-op”. Here we really do get durable writes with 10 microsecond latency!</p><p>If you really want durable writes with sub-millisecond latency, you might want to pay the ~20% premium that vendors charge for such enterprise SSDs.</p><p>Unfortunately, my laptop, and probably many other developers’ laptops, do not have an enterprise-grade SSD, or even the option to install one.</p><h2 id="how-cedardb-mitigates-ssd-bottlenecks">How CedarDB Mitigates SSD Bottlenecks</h2><p>CedarDB combines all of the above approaches to give you the best of all worlds:</p><ul><li>It uses <strong>group commits</strong> with an automatically adjusting time window to get the best performance under medium or high load.</li><li>It supports PostgreSQL’s <strong>pipeline mode</strong> to allow you (or your database driver) to make asynchronous requests whenever possible.</li><li>It automatically detects if your system’s SSD has a <strong>write cache</strong>.
If you have such an SSD, CedarDB uses fully ACID-compliant synchronous commits at no extra cost.
If not, CedarDB will fall back to asynchronous commits, assuming it is running in a non-production environment where the small chance of data loss during a system failure is acceptable.
In this case, CedarDB will notify you with the following warning:</li></ul><pre tabindex="0"><code>WARNING: Your storage device uses write back caching. Write durability might be delayed. 
See: https://cedardb.com/docs/references/writecache
</code></pre><p>You can of course <a href="https://cedardb.com/docs/references/writecache/#cedardb-durability-guarantees">instruct CedarDB</a> to run in full synchronous commit mode anyway and even enable or disable asynchronous commits on a per-connection basis.</p><h2 id="how-other-database-systems-deal-with-ssd-latency">How Other Database Systems Deal With SSD Latency</h2><p>Let’s look at two other popular database systems and how they deal with SSD latency bottlenecks.</p><h3 id="mongodb">MongoDB</h3><p>MongoDB also uses a write-ahead logging via a separate <a href="https://www.mongodb.com/docs/manual/core/journaling/">journal</a>.
Each write operation is appended to this journal, which is then flushed to disk every 100 milliseconds, taking aspects of the <em>group commit</em> and <em>async commit</em> approaches.
If you cannot tolerate losing up to 100 milliseconds of data, you can optionally force an early flush of the journal when issuing a write, increasing the commit latency.</p><h3 id="postgresql">PostgreSQL</h3><p>As a truly battle-tested database system, PostgreSQL offers very strong durability guarantees.
It flushes transactions to disk on commit, and only then acknowledges the commit to the client.
PostgreSQL also offers a <code>commit_delay</code> configuration parameter which causes logs to be flushed every <em>n</em> microseconds, acting as a simple group commit mechanism without variable time windows.
In addition, PostgreSQL allows the user to globally enable <em>asynchronous commits</em> (via <code>synchronous_commits = off</code>), or the even more invasive option of simply disabling <code>fsync</code>.
The latter option is strongly discouraged, however, as it can lead to an unrecoverable database state in the event of a crash, causing the user to lose all their data.</p><h2 id="does-it-matter-that-your-ssd-probably-sucks">Does It Matter That Your SSD (Probably) Sucks?</h2><p>Probably not.</p><p>If you run your database system on a consumer-grade desktop PC or laptop, you can get similar performance</p><ul><li>at the cost of potentially losing a few milliseconds of data due to async commits, or</li><li>at the cost of reduced application expressiveness via transaction pipelining.</li></ul><p>In this case, however, you probably have other, more catastrophic data loss vectors to worry about.
In other words: You probably don’t care if the <code>fsync</code> call completed when your laptop burns down in a house fire or gets stolen - the data is gone anyway.</p><p>If you are running database systems in a production environment, you (or your cloud provider) are hopefully using enterprise-grade SSDs with capacitor-backed write cache that have great write latency.</p><p>If you want to see out for yourself how the different trade-offs affect performance, or find out if your SSD really sucks, why not join our waitlist and be among the first to get your hands on CedarDB?</p></div></div>
  </body>
</html>
