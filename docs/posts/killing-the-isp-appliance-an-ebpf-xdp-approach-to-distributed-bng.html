<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://markgascoyne.co.uk/posts/ebpf-bng/">Original</a>
    <h1>Killing the ISP Appliance: An eBPF/XDP Approach to Distributed BNG</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content"><p>I used to work for an ISP startup that was building next-generation infrastructure. The company didn’t make it, but the problems we were trying to solve stuck with me. So I spent a few weeks building what we never got to: an open-source, eBPF-accelerated BNG that runs directly on OLT hardware.</p><p>This post explains the architecture and why I think it’s the future of ISP edge infrastructure.</p><h2 id="the-problem-centralised-bng-is-a-bottleneck">The Problem: Centralised BNG is a Bottleneck</h2><p>Traditional ISP architecture looks like this:</p><pre tabindex="0"><code>Customer → ONT → OLT → [BNG Appliance] → Internet
                            ↑
               Single point of failure
               Expensive proprietary hardware
               All subscriber traffic flows through here
</code></pre><p>Every subscriber’s traffic - DHCP, authentication, NAT, QoS - flows through a central BNG appliance. These boxes cost six figures, require vendor support contracts, and create a single point of failure. When they go down, everyone goes down.</p><p>The industry’s answer has been to buy bigger boxes with more redundancy. But what if we flipped the model entirely?</p><h2 id="the-idea-distribute-the-bng-to-the-edge">The Idea: Distribute the BNG to the Edge</h2><p>What if, instead of funneling all traffic through a central appliance, we ran BNG functions directly on the OLT hardware at each edge site?</p><pre tabindex="0"><code>Customer → ONT → OLT(+BNG) → Internet
                    ↑
       Subscriber traffic stays LOCAL
       No central bottleneck
       Each site operates independently
</code></pre><p>This isn’t a new idea - it’s essentially what hyperscalers do with their edge infrastructure. But ISPs have been slow to adopt it because:</p><ol><li>Traditional BNG software assumes a central deployment</li><li>State management (IP allocations, sessions) is hard to distribute</li><li>Performance requirements seemed to need specialised hardware</li></ol><p>The key insight is that modern Linux with eBPF/XDP can handle ISP-scale packet processing on commodity hardware.</p><h2 id="why-ebpfxdp-not-vpp">Why eBPF/XDP, Not VPP?</h2><p>When I started this project, I evaluated two approaches:</p><p><strong>VPP (Vector Packet Processing)</strong> - The industry darling for high-performance networking. Used in production by big telcos. Handles 100+ Gbps easily.</p><p><strong>eBPF/XDP</strong> - Linux kernel’s programmable packet processing. Lower peak throughput, but much simpler operations.</p><p>For edge deployment (10-40 Gbps per OLT), I chose eBPF/XDP:</p><table><thead><tr><th>Aspect</th><th>eBPF/XDP</th><th>VPP</th></tr></thead><tbody><tr><td>Performance</td><td>10-40 Gbps ✓</td><td>100+ Gbps (overkill)</td></tr><tr><td>Deployment</td><td>Standard Linux kernel</td><td>DPDK, hugepages, dedicated NICs</td></tr><tr><td>Operations</td><td>systemd service</td><td>Complex dedicated setup</td></tr><tr><td>Debugging</td><td>tcpdump, bpftool, perf</td><td>Custom tools</td></tr><tr><td>Learning curve</td><td>Steep but well-documented</td><td>Very steep, less documentation</td></tr></tbody></table><p>VPP is the right choice for core aggregation. But for edge sites? eBPF/XDP is simpler and sufficient.</p><h2 id="the-architecture">The Architecture</h2><p>Here’s what I built:</p><pre tabindex="0"><code>┌─────────────────────────────────────────────────────────────┐
│  CENTRAL (Kubernetes)                                        │
│  Nexus: CRDT state sync, hashring IP allocation             │
│  (Control plane only - NO subscriber traffic)               │
└──────────────────────────┬──────────────────────────────────┘
                           │ Config sync, metrics
         ┌─────────────────┼─────────────────┐
         ▼                 ▼                 ▼
   ┌───────────┐     ┌───────────┐     ┌───────────┐
   │ OLT-BNG 1 │     │ OLT-BNG 2 │     │ OLT-BNG N │
   │ eBPF/XDP  │     │ eBPF/XDP  │     │ eBPF/XDP  │
   │ 1500 subs │     │ 2000 subs │     │ 1800 subs │
   └─────┬─────┘     └─────┬─────┘     └─────┬─────┘
         │                 │                 │
    Traffic LOCAL     Traffic LOCAL     Traffic LOCAL
         ↓                 ↓                 ↓
       ISP PE           ISP PE           ISP PE
</code></pre><p><strong>Key principle</strong>: Subscriber traffic never touches central infrastructure. The central Nexus server only handles control plane operations - config distribution, IP allocation coordination, monitoring.</p><h3 id="two-tier-dhcp-fast-path--slow-path">Two-Tier DHCP: Fast Path + Slow Path</h3><p>The performance-critical insight is that most DHCP operations are renewals from known subscribers. We can handle these entirely in the kernel:</p><pre tabindex="0"><code>DHCP Request arrives
        │
        ▼
┌───────────────────────────────────────────────────────┐
│               XDP Fast Path (Kernel)                   │
│                                                        │
│  1. Parse Ethernet → IP → UDP → DHCP                  │
│  2. Extract client MAC                                 │
│  3. Lookup MAC in eBPF subscriber_pools map           │
│                                                        │
│  CACHE HIT?                                           │
│  ├─ YES: Generate DHCP ACK in kernel                  │
│  │       Return XDP_TX (~10μs latency)                │
│  └─ NO:  Return XDP_PASS → userspace                  │
└───────────────────────────────────────────────────────┘
        │ XDP_PASS (cache miss)
        ▼
┌───────────────────────────────────────────────────────┐
│            Go Slow Path (Userspace)                    │
│                                                        │
│  1. Lookup subscriber in Nexus cache                  │
│  2. Get pre-allocated IP from subscriber record       │
│  3. Update eBPF cache for future fast path hits       │
│  4. Send DHCP response                                │
└───────────────────────────────────────────────────────┘
</code></pre><p><strong>Results</strong>:</p><ul><li>Fast path: ~10μs latency, 45,000+ requests/sec</li><li>Slow path: ~10ms latency, 5,000 requests/sec</li><li>Cache hit rate after warmup: &gt;95%</li></ul><h3 id="ip-allocation-hashring-at-radius-time">IP Allocation: Hashring at RADIUS Time</h3><p>Here’s a design decision that simplified everything: <strong>IP allocation happens at RADIUS authentication time, not DHCP time.</strong></p><pre tabindex="0"><code>1. Subscriber authenticates via RADIUS
2. RADIUS success → Nexus allocates IP from hashring (deterministic)
3. IP stored in subscriber record
4. DHCP is just a READ operation (lookup pre-allocated IP)
</code></pre><p>This means:</p><ul><li>No IP conflicts between distributed BNG nodes</li><li>DHCP fast path can run entirely in eBPF (no userspace allocation decisions)</li><li>Subscribers get the same IP every time (hashring determinism)</li></ul><h3 id="offline-first-edge-operation">Offline-First Edge Operation</h3><p>What happens when an edge site loses connectivity to central Nexus?</p><p><strong>Keeps working:</strong></p><ul><li>Existing subscriber sessions (cached in eBPF maps)</li><li>DHCP lease renewals</li><li>NAT translations</li><li>QoS enforcement</li></ul><p><strong>Degraded:</strong></p><ul><li>New subscriber authentication (no RADIUS)</li><li>New IP allocations (falls back to local pool)</li><li>Config updates (queued until reconnect)</li></ul><p>The edge sites are designed to be autonomous. Central coordination is nice to have, not required.</p><h2 id="the-implementation">The Implementation</h2><p>The BNG is a single Go binary with embedded eBPF programs:</p><pre tabindex="0"><code>bng/
├── cmd/bng/              # Main binary
├── pkg/
│   ├── ebpf/             # eBPF loader and map management
│   ├── dhcp/             # DHCP slow path server
│   ├── nexus/            # Central coordination client
│   ├── radius/           # RADIUS client
│   ├── qos/              # QoS/rate limiting
│   ├── nat/              # NAT44/CGNAT
│   ├── pppoe/            # PPPoE server
│   ├── routing/          # BGP/FRR integration
│   └── metrics/          # Prometheus metrics
├── bpf/
│   ├── dhcp_fastpath.c   # XDP DHCP fast path
│   ├── qos_ratelimit.c   # TC QoS eBPF
│   ├── nat44.c           # TC NAT eBPF
│   └── antispoof.c       # TC anti-spoofing
</code></pre><p>Running it:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># Standalone mode (local IP pool)</span>
</span></span><span><span>sudo ./bng run <span>\
</span></span></span><span><span><span></span>  --interface eth1 <span>\
</span></span></span><span><span><span></span>  --pool-network 10.0.1.0/24 <span>\
</span></span></span><span><span><span></span>  --pool-gateway 10.0.1.1
</span></span><span><span>
</span></span><span><span><span># Production mode (with Nexus coordination)</span>
</span></span><span><span>sudo ./bng run <span>\
</span></span></span><span><span><span></span>  --interface eth1 <span>\
</span></span></span><span><span><span></span>  --nexus-url http://nexus.internal:9000 <span>\
</span></span></span><span><span><span></span>  --radius-enabled <span>\
</span></span></span><span><span><span></span>  --radius-servers radius.isp.com:1812
</span></span></code></pre></div><h2 id="hardware-white-box-olts">Hardware: White-Box OLTs</h2><p>This runs on any Linux box with a modern kernel (5.10+), but the target is white-box OLTs like the Radisys RLT-1600G:</p><ul><li>16 GPON/XGS-PON ports</li><li>Runs Debian Linux</li><li>~$7,400 USD (vs six figures for traditional BNG)</li><li>1,500-2,000 subscribers per unit</li></ul><p>The same approach works with any OLT that runs Linux and exposes its network interfaces to the OS.</p><h2 id="whats-next">What’s Next</h2><p>The code is working but not production-ready. Missing pieces:</p><ol><li><strong>Device authentication</strong> - TPM attestation or similar to prevent rogue OLT-BNG devices</li><li><strong>IPv6 support</strong> - DHCPv6 and SLAAC</li><li><strong>Full RADIUS accounting</strong> - Currently basic</li><li><strong>Management UI</strong> - Currently CLI and Prometheus metrics only</li></ol><p>I’m considering open-sourcing the entire thing. The BNG market is dominated by expensive proprietary solutions, and there’s no good open-source alternative. Maybe there should be.</p><h2 id="the-bigger-picture">The Bigger Picture</h2><p>Traditional ISP infrastructure was designed when compute was expensive and networks were slow. Centralised appliances made sense when you needed specialised hardware for packet processing.</p><p>But compute is cheap now, and eBPF lets us do packet processing in the Linux kernel at line rate. The economics have shifted - it’s now cheaper to distribute the BNG to hundreds of edge sites than to build a few massive central boxes.</p><p>This isn’t just about saving money. Distributed architecture is more resilient (no single point of failure), lower latency (traffic stays local), and operationally simpler (it’s just Linux).</p><p>The hyperscalers figured this out years ago. ISPs are slowly catching up.</p><hr/><p><strong>Interested in this approach?</strong> The code is at <a href="https://github.com/codelaboratoryltd/bng" target="_blank" rel="noopener">github.com/codelaboratoryltd/bng</a> and <a href="https://github.com/codelaboratoryltd/nexus" target="_blank" rel="noopener">github.com/codelaboratoryltd/nexus</a>. I’d love to hear from anyone working on similar problems in the ISP/altnet space.</p><p><em>If you’re building ISP infrastructure and want to chat about eBPF, distributed systems, or why vendor BNG appliances are a racket, reach out.</em></p></div></div>
  </body>
</html>
