<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/dleemiller/Penny-1.7B">Original</a>
    <h1>Show HN: Penny-1.7B Irish Penny Journal style transfer</h1>
    
    <div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START -->
<p>This is a <strong>style transfer</strong> from the Irish Penny Journal (1840) to Smollm2 using GRPO. For example, this response to &#34;What is the capital of France&#34;</p>
<pre><code>Verily, in the grand tapestry of European monarchies, the city of Paris, the seat of the mighty Emperor Napoleon, holds a place of singular distinction. This city, which hath borne the name of &#39;La Ville LumiÃ¨re&#39; for nigh on two centuries, doth shine forth as a beacon of art, culture, and intellect, its very existence a testament to the ingenuity and brilliance of its people. And so, it is with great honour and reverence that we declare Paris, the majestic capital of the French realm, to be our noble question&#39;s answer.
</code></pre>
<p><strong>Pennyâ€‘1.7B</strong> is a 1.7â€¯billionâ€‘parameter causal language model fineâ€‘tuned with <strong>Groupâ€¯Relativeâ€¯Policyâ€¯Optimization (GRPO)</strong> to emulate the 19áµ—Ê°â€‘century prose of the <em>Irishâ€¯Pennyâ€¯Journal</em> (1840). The RL stage ran for <strong>6,800 policy steps</strong>, using a reward model trained to classify sentences as <em>original IPJ</em> vs <em>modern translation</em>. Maximizing this score nudges generations toward authentic Victorianâ€‘era diction while retaining the general reasoning ability of the base SmolLM2 model.</p>
<h2>
	<a rel="nofollow" href="#âœ¨-key-facts" id="âœ¨-key-facts">
		
	</a>
	<span>
		âœ¨ Key Facts
	</span>
</h2>
<div>
	<table>
		<thead><tr>
<th></th>
<th></th>
</tr>

		</thead><tbody><tr>
<td><strong>Base model</strong></td>
<td><a rel="nofollow" href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct">SmolLM2â€‘1.7B-Instruct</a></td>
</tr>
<tr>
<td><strong>Tuning method</strong></td>
<td>GRPO (RL)</td>
</tr>
<tr>
<td><strong>Policy steps</strong></td>
<td>6,800</td>
</tr>
<tr>
<td><strong>Reward model</strong></td>
<td>MiniLM2 L6 384H classifier</td>
</tr>
<tr>
<td><strong>Optimiser</strong></td>
<td>AdamWâ€¯8â€‘bit Â· lrâ€¯5â€¯Ã—â€¯10^â»6</td>
</tr>
<tr>
<td><strong>Hardware</strong></td>
<td>1Ã—â€¯RTXâ€¯A6000 (48â€¯GB) Â· bf16</td>
</tr>
</tbody>
	</table>
</div>
<h2>
	<a rel="nofollow" href="#ğŸ”¬-training--data" id="ğŸ”¬-training--data">
		
	</a>
	<span>
		ğŸ”¬ Training &amp; Data
	</span>
</h2>
<ul>
<li><p><strong>Corpora</strong></p>
<ul>
<li><em>Irishâ€¯Pennyâ€¯Journal</em> 1840 (dleemiller/irish_penny_journal)</li>
<li>Modernized translations produced via ruleâ€‘based spelling normalisation plus manual postâ€‘edit</li>
</ul>
</li>
<li><p><strong>Reward</strong> = classifier output</p>
</li>
</ul>
<h2>
	<a rel="nofollow" href="#â¡ï¸-intended-uses" id="â¡ï¸-intended-uses">
		
	</a>
	<span>
		â¡ï¸ Intended Uses
	</span>
</h2>
<ul>
<li>Creative writing, educational content, or stylistic pastiche in Victorianâ€‘era Irish English.</li>
<li>Research on RLâ€‘based style transfer.</li>
</ul>
<p><strong>Not recommended for:</strong> contemporary fact Q&amp;A or contexts where archaic language could mislead readers.</p>
<h2>
	<a rel="nofollow" href="#âš ï¸-limitations--biases" id="âš ï¸-limitations--biases">
		
	</a>
	<span>
		âš ï¸ Limitations &amp; Biases
	</span>
</h2>
<p>19áµ—Ê°â€‘century texts can contain outdated social views. Outputs may reflect such biases or archaic spelling. Always review generations before use.</p>
<h2>
	<a rel="nofollow" href="#ğŸ’»-quick-start" id="ğŸ’»-quick-start">
		
	</a>
	<span>
		ğŸ’» Quick Start
	</span>
</h2>
<pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer

checkpoint = <span>&#34;dleemiller/Penny-1.7B&#34;</span>

device = <span>&#34;cuda&#34;</span>  

tokenizer = AutoTokenizer.from_pretrained(checkpoint)

model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)

messages = [{<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;What is the capital of France.&#34;</span>}]
input_text = tokenizer.apply_chat_template(messages, tokenize=<span>False</span>)
inputs = tokenizer.encode(input_text, return_tensors=<span>&#34;pt&#34;</span>).to(device)

outputs = model.generate(inputs,
                         max_new_tokens=<span>512</span>,
                         temperature=<span>0.8</span>,
                         top_p=<span>0.9</span>,
                         do_sample=<span>True</span>)

<span>print</span>(tokenizer.decode(outputs[<span>0</span>]))
</code></pre>
<h2>
	<a rel="nofollow" href="#ğŸ“-citation" id="ğŸ“-citation">
		
	</a>
	<span>
		ğŸ“ Citation
	</span>
</h2>
<pre><code>@software{penny_1.7b_2025,
  title        = {Pennyâ€‘1.7B: Irish Penny Journal Style Language Model},
  author       = {Lee Miller},
  year         = 2025,
  publisher    = {Hugging Face},
  url          = {https://huggingface.co/dleemiller/Penny-1.7B}
}
</code></pre>
<h2>
	<a rel="nofollow" href="#ğŸ“œ-license" id="ğŸ“œ-license">
		
	</a>
	<span>
		ğŸ“œ License
	</span>
</h2>
<p>ApacheÂ 2.0 (inherits from the base model).</p>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
