<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=44840728">Original</a>
    <h1>Ask HN: How can ChatGPT serve 700M users when I can&#39;t run one GPT-4 locally?</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Sam said yesterday that chatgpt handles ~700M weekly users. Meanwhile, I can&#39;t even run a single GPT-4-class model locally without insane VRAM or painfully slow speeds.</p><p>Sure, they have huge GPU clusters, but there must be more going on - model optimizations, sharding, custom hardware, clever load balancing, etc.</p><p>What engineering tricks make this possible at such massive scale while keeping latency low?</p><p>Curious to hear insights from people who&#39;ve built large-scale ML systems.</p></div></td></div></div>
  </body>
</html>
