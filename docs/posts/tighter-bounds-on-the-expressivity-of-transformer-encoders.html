<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2301.10743">Original</a>
    <h1>Tighter bounds on the expressivity of transformer encoders</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2301.10743">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Characterizing neural networks in terms of better-understood formal systems
has the potential to yield new insights into the power and limitations of these
networks. Doing so for transformers remains an active area of research.
Bhattamishra and others have shown that transformer encoders are at least as
expressive as a certain kind of counter machine, while Merrill and Sabharwal
have shown that fixed-precision transformer encoders recognize only languages
in uniform $TC^0$. We connect and strengthen these results by identifying a
variant of first-order logic with counting quantifiers that is simultaneously
an upper bound for fixed-precision transformer encoders and a lower bound for
transformer encoders. This brings us much closer than before to an exact
characterization of the languages that transformer encoders recognize.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: David Chiang [<a href="https://arxiv.org/show-email/d35be9dd/2301.10743">view email</a>]
      </p></div></div>
  </body>
</html>
