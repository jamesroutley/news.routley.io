<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hforsten.com/synthetic-aperture-radar-autofocus-and-calibration.html">Original</a>
    <h1>Synthetic aperture radar autofocus and calibration</h1>
    
    <div id="readability-page-1" class="page"><div>
                <div>
                    
                </div>
                
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/xsar_fmcw.jpg.pagespeed.ic.xNfDXU4lx7.jpg" width="2062" height="1200"/></p><p>Radar drone. Since the first post I have added
    a Gopro camera and a second GPS for redundancy.</p>
</div>

<p>Earlier this year I made a <a href="https://hforsten.com/homemade-polarimetric-synthetic-aperture-radar-drone.html">polarimetric synthetic aperture radar (SAR) mounted
on a drone</a>. I have since worked a lot on the software
side to improve the image quality and now the same hardware can generate much
better quality images. The main contribution of this article is description of
a new SAR autofocus algorithm that combines some of the existing SAR autofocus
algorithms to make an algorithm that is well-suited for drone mounted SAR. Also
antenna pattern normalization and polarimetric calibration suitable for drone
mounted SAR with non-linear track is described.</p>

<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/coords.svg" width="537" height="450"/></p><p>SAR image formation geometry.</p>
</div>

<p>While the drone flies along a track, the radar measures the distances and phases
of the targets in the antenna beam. Neither azimuth (<span>\(\varphi\)</span>) nor elevation
(<span>\(\theta\)</span>) 
angle of a target can be determined from a single measurement, and all targets at
the same range will overlap in the measurement. However, by recording many
radar measurements from multiple locations, a radar image can be formed.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/xmeas_range.png.pagespeed.ic.aTAw-d6EC3.png" width="711" height="473"/></p><p>Magnitude of a single recorded range compressed radar signal.</p>
</div>

<p>Depending on the radar parameters and radiated waveform, the signal processing to
obtain a range-compressed signal from the raw recorded data might vary. After 
range compression, however, the image formation process is essentially the
same.</p>
<p>Range resolution is the ability to separate two closely spaced targets. Ideally
it&#39;s calculated as <span>\(\Delta r = c/2B\)</span>, where <span>\(c\)</span> is speed of light, and <span>\(B\)</span> is
the bandwidth of the radiated waveform. For example, with a 150 MHz bandwidth the range
resolution is 1 m. For a linear flight track this is also the image resolution
in range direction.</p>
<p>The phase of the received signal depends on the distance to the target. Because
the signal travels to the target and back, a distance change equal to half the
wavelength causes a full wavelength (360 degrees) shift in the received signal
phase. As a function of target distance, the phase can be
written as <span>\(\varphi = 4\pi d/\lambda\)</span>, where <span>\(d\)</span> is distance, and <span>\(\lambda\)</span> is
wavelength. For instance, with 6 GHz RF frequency, a distance change of 25 mm (1
inch) causes 360 degree phase shift.</p>

<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/xsar_example.png.pagespeed.ic.V8vT7Awxtn.png" width="1788" height="473"/></p><p>SAR image example with one point target. Radar
    track and target position (left), magnitude of the recorded radar data at each position (middle), and processed SAR image (right).
    Platform height is 30 m.</p>
</div>

<p>Due to the wide antenna beam there are multiple targets in the beam at the same
time, but with signal processing it&#39;s possible to separate them and generate
much higher resolution image.</p>
<p>To convert a set of radar measurements into a radar image, matched filtering can
be used. For each pixel in the image grid, generate a reference signal
corresponding to what a target at that position would reflect. For each radar
measurement, multiply the measured signal with complex conjugate of the
reference signal, then sum these products over all measurements. When the
measured signal closely matches the reference signal, their product becomes
large because the phases align and target is generated at that location in the
image. If the phases don&#39;t match, the result of the multiplication is a complex number
with a random phase, and summing random complex numbers will average out
generating a low response and that pixel in the image will have low amplitude.</p>
<p>Assuming the signal is already range compressed and ignoring the amplitude
(antenna pattern and loss due to distance can be compensated after image
formation), then the reference signal just needs to compensate for the phase.</p>
<p>The image formation can be written as:</p>
<p>$$I = \sum_{p \in \mathcal{P}} \sum_{n=1}^N S_n(d(\mathbf{p},\mathbf{x_n}))
\exp \left(j \frac{4\pi}{\lambda} d(\mathbf{p},\mathbf{x_n})\right)$$</p>
<p><span>\(\mathcal{P}\)</span> is the set of pixels in the image, <span>\(N\)</span> is the number of
radar measurements, <span>\(S_n\)</span> is range compressed single channel radar measurement,
and <span>\(d(\mathbf{p},\mathbf{x_n})\)</span> is the distance to location of pixel
<span>\(\mathbf{p}\)</span> from radar position at measurement <span>\(\mathbf{x_n}\)</span>.</p>
<h2 id="position-error">Position error</h2>
<p>To get a well focused radar image, the phase of the reference signal used in the
matched filtering needs to match with the recorded signal. If there is any error
in the actual position of the radar during measurement to the position used in
image formation, it will cause errors in the image.</p>
<p>Assuming the target is just a single point and ignoring the amplitude, with
<span>\(\Delta \mathbf{x_n}\)</span> position error on the <span>\(n\)</span>th measurement, the error in
image at the location of the target can be calculated as:</p>
<p>$$I_p = \sum_{n=1}^N S_n(d(\mathbf{p},\mathbf{x_n})) \exp \left(j \frac{4\pi}{\lambda} d(\mathbf{p},\mathbf{x_n} + \Delta\mathbf{x_n})\right)
= \sum_{n=1}^N \exp \left(-j \frac{4\pi}{\lambda} d(\mathbf{p},\mathbf{x_n}) + j \varphi_p\right) \exp \left(j \frac{4\pi}{\lambda} (d(\mathbf{p},\mathbf{x_n} + \Delta\mathbf{x_n})\right)\\
= \sum_{n=1}^N \exp \left(-j \frac{4\pi}{\lambda}\left(d(\mathbf{p},\mathbf{x_n}) - d(\mathbf{p},\mathbf{x_n} + \Delta\mathbf{x_n})\right) + j\varphi_p \right)
= \sum_{n=1}^N \exp \left(-j \frac{4\pi}{\lambda}\Delta r_{n,p} + j\varphi_p\right) $$</p>
<p>Instead of all phases lining up at the location of the target, there is some
phase offset left that depends on how the position error changes the distance.
The remaining error <span>\(\Delta r_{n,p}\)</span> is the distance error from <span>\(n\)</span>th radar position to
the target location at pixel <span>\(p\)</span>.</p>
<p><span>\(\varphi_p\)</span> is a constant phase offset from the target. It can be caused by
phase change in reflection, small position offset or just phase offset in the
radar electronics.</p>
<div id="centered">
<p><img src="https://hforsten.com/img/autofocus/xsar_example2.png.pagespeed.ic.y0M3pTc-jh.png" width="1788" height="473"/></p><p>SAR image example with one point target, with and
without linear error in radar position. Radar
track and target position (left), image processed without position error
(middle), and image processed with linear position error (right) causing
shift in the image.</p>
</div>

<p>SAR track can have any shape, but often it&#39;s a straight line. With this
simple track it&#39;s possible to calculate how some position errors affect the
image.</p>
<p>If the distance error <span>\(\Delta r_{n,p}\)</span> is constant, then the error is just
a constant phase offset which doesn&#39;t affect the magnitude of the image at all.</p>
<p>Linear error affects the image by shifting the azimuth positions of the targets
in the image. Analysis for it is in many different papers and textbooks, but a good detailed overview can be found in the paper: <a href="https://ieeexplore.ieee.org/document/10868585">&#34;Performance
Limit of Phase Gradient Autofocus Class Method for Synthetic Aperture Radar&#34; by C. Tu, Z. Dong, A. Yu, Y. Ji, X. Chen and Z. Zhang</a>.</p>
<div id="centered">
<p><img src="https://hforsten.com/img/autofocus/xsar_example3.png.pagespeed.ic.r5wyraN9H8.png" width="1076" height="473"/></p><p>The same scene, but with more complicated position
error. Radar
track and position error (left), and image processed with position error (right) causing
blurring of the target.</p>
</div>

<p>More complicated errors can be thought as piecewise linear error, each piece in
the position error causes the resulting image to be formed at different location
resulting in a blurred image.</p>
<p>Note the scale of the position error that causes the blurring in the simulate
image. The maximum error is only 1 cm, but even this small error causes
significant loss in the image quality. Since the image formation relies on the
phase, position error must be much smaller than wavelength to not cause phase
errors during image formation. In this case the simulation uses 6 GHz RF
frequency which corresponds to 5 cm wavelength.</p>
<h2 id="generalized-phase-gradient-autofocus">Generalized phase gradient autofocus</h2>
<p>The idea behind autofocus algorithms is to estimate the error in radar position
from the SAR image and radar data. There are many different autofocus algorithms
for different applications, but the most widely used SAR autofocus algorithm is
<a href="https://ieeexplore.ieee.org/document/303752">phase gradient autofocus (PGA)</a>.
The <a href="https://ieeexplore.ieee.org/document/8642429">generalized version</a> isn&#39;t as
well known and was published much later, but in my opinion it&#39;s easier to
understand with backprojection image formation background.</p>
<p>To simplify the notation let&#39;s call the terms used for calculating a single
pixel <span>\(I_p\)</span> in backprojection <span>\(\xi_{n,p}\)</span>:</p>
<p>$$\xi_{n,p} = S_n(d(\mathbf{p},\mathbf{x_n})) \exp \left(j \frac{4\pi}{\lambda} d(\mathbf{p},\mathbf{x_n})\right)\\
I_p = \sum_{n=1}^N \xi_{n,p}$$</p>
<p>The idea behind phase gradient autofocus is that if we know that a target in the
SAR image is a point target, then phase of <span>\(\xi_{n,p}\)</span> should be constant. If this
is not the case, the position error can be solved from the <span>\(\xi_{n,p}\)</span> by finding
a phase offset that makes it constant. The phase error can then be converted to
distance to find the position error.</p>
<p><span>\(\xi_{n,p}\)</span> is calculated exactly like a pixel in the backprojection image
formation but just without summing up the terms.</p>
<div id="centered">
<p><img src="https://hforsten.com/img/autofocus/xsar4_labels.png.pagespeed.ic.B5ce4D_u5G.png" width="1170" height="454"/></p><p>SAR image with position error (left) and phase of ξ calculated at the target location compared to position error.</p>
</div>

<p>Calculating the demodulated target phase (<span>\(\xi_{n,p}\)</span>) for the previous example
scene gives a phase that matches very well with the X-position error that was
added to the radar position. Shifting the phase of the input data by negative of
the <span>\(\xi_{n,p}\)</span> would result in a constant phase, which results in a focused
target in the image.</p>
<p>In this case since the target is on X-axis the distance error is very close to
the X-axis error, but this is not the case generally if the target is at some
other angle. A more general method for solving for the 3D position error from
the phase error will be presented later.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/xsar_example6.png.pagespeed.ic.bLhJLoX35Z.png" width="1306" height="473"/></p><p>Low-pass filtering will improve the phase error
    estimation when phase error is slowly varying. SAR image with noise and position error (left) and demodulated phase of the target with and without low-pass filtering (right).</p>
</div>

<p>Low-pass filtering is used to filter out noise
and clutter at the same distance in measurements as the target. Since the target
response is a constant it will be preserved as the signal is low-pass filtered,
but any noise and other objects that were momentarily at the same distance as
the target being considered are filtered out. Low-pass filtering also filters
out high frequency position errors if it&#39;s too small, so the low-pass window
size should be set carefully.</p>
<p>Low-pass filtering operation is often implemented as Fourier transforming the
signal, zeroing the high frequency bins, and then inverse Fourier transforming
to get the filtered signal.</p>
<div id="centered">
<p><img src="https://hforsten.com/img/autofocus/gpga.svg" width="1420" height="1304"/></p><p>GPGA algorithm flow-chart. Algorithm is iterated
until either phase error is below a threshold or minimum low-pass filter
window size is reached.</p>
</div>

<p>The complete GPGA algorithm needs few more operations. In general there are many
point-like targets in the same image and for the best position error estimation
multiple different targets should be used.</p>
<p>We know that for a point target with position error: <span>\(\xi_{n,p} = \exp(-j\frac{4\pi}{\lambda}\Delta r_{n,p}
+ j\varphi_p)\)</span>.</p>
<p>The constant phase offset, <span>\(\varphi_p\)</span>, that is different for each
target makes it impossible to just sum different <span>\(\xi_{n,p}\)</span> for each target
together. There are few different phase estimators for estimating the phase
error from set of <span>\(\xi_{n,p}\)</span> from many targets (pixels <span>\(p\)</span>).</p>
<p>One solution is to calculate:</p>
<p>$$g_{n,p} = \xi_{n-1,p}^* \xi_{n,p} = \exp\left(j\frac{4\pi}{\lambda}\Delta r_{n-1,p}
- j\varphi_p\right) \exp\left(-j\frac{4\pi}{\lambda}\Delta r_{n,p}
  + j\varphi_p\right)
  = \exp\left(-j\frac{4\pi}{\lambda}(\Delta r_{n,p} - \Delta r_{n-1,p})\right)$$</p>
<p>Multiplying complex conjugate of n+1th term with nth term causes the constant
phase to cancel out and the result is a difference in phase errors between
adjacent positions (phase gradient in the algorithm name). In this forms the
<span>\(g_{n,p}\)</span> from different targets can be summed, taking the argument of the sum
gives the gradient of the phase error, and the phase error can be recovered up
to a constant phase offset by calculating a cumulative sum.</p>
<p>$$g_n = \text{Arg} \left(\sum_{p} g_{n,p}\right)$$</p>
<p>In practice the sum should be weighted according to signal-to-clutter ratio in
each target response for the optimal estimate and this weighting will be
analyzed later. Finally taking cumulative sum of the phase gradient <span>\(g_n\)</span> gives
estimate for the phase error.</p>
<p>After correcting the image with the solved phase error, the phase error is
reduced but might not be completely gone. Decreasing the low pass window size
and repeating the process improves the estimate.</p>
<h2 id="3d-trajectory-estimation">3D trajectory estimation</h2>
<p>So far only the phase error has been solved. It can be used to correct the SAR
image by multiplying the input data with the negative of the phase error.
However, the phase error is proportional to distance error from the radar
position to the target, and depending on a target&#39;s location, the phase error
caused by the position error varies. If several targets at
different azimuth and elevation angles are visible at the same time, it might not be
possible to focus them all at the same time with a single phase-correction term.
This is the case when antenna beam is wide, which is precisely the case with my
drone mounted SAR.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/local_images2.svg" width="236" height="147"/></p><p>Example scenario with two targets and 2D position
    error. Distance errors to the two targets are not equal due to their
    different azimuth angle. r1&#39; is about equal to r1 due to direction of the position error
    being mostly perpendicular to target 1, but this is not the case for target 2. There isn&#39;t a single phase correction that will focus them both at the
    same time.</p>
</div>

<p>To get all the targets in focus at the same time it&#39;s necessary to solve the 3D
position error and use the corrected position of the radar during the image
formation.</p>
<p>The 3D trajectory estimation is based on paper <a href="https://ieeexplore.ieee.org/document/9380507">&#34;An Autofocus Approach for
UAV-Based Ultrawideband Ultrawidebeam SAR Data With Frequency-Dependent and 2-D
Space-Variant Motion Errors&#34; by Z. Ding et
al</a>. The first step is to divide
the SAR image into <span>\(K\)</span> subimages and then calculate the phase error in each
image using PGA or GPGA.  The phase error can be turned into a distance error by
unwrapping the phase and multiplying by <span>\(\lambda/4\pi\)</span>. The result is
distance error from each radar position (<span>\(N\)</span> positions) to each subimage (<span>\(K\)</span>
subimages).</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/local_images.svg" width="308" height="276"/></p><p>GPGA can solve for distance error to each local
    image and the 3D position error can be solved from multiple distance errors to
    different subimages.</p>
</div>

<p>If the subimages are small, we can assume that azimuth and elevation angle to
all the targets in the subimage are similar and use the weighted target
positions as the subimage center position. At this point we have a distance
error from each radar position to each subimage. The number of subimages
is large and we only have three unknowns (x, y, and z position errors), and we
can set up an overdetermined system of equations to solve for the 3D position error from
the distance errors and known subimage center positions.</p>
<p>Calculating the 3D position error from distance errors to each subimage would
require solving a non-linear system of equations due to distance calculation
not being linear, but assuming that 3D position error is small it can be
linearized which makes solving it much easier and faster.</p>
<p>Knowing the subimage center azimuth and elevation angles, the distance error
<span>\(\Delta r_{n,k}\)</span> from <span>\(n\)</span>th radar position to <span>\(k\)</span>th subimage center can be
written as a 3D Cartesian vector: <span>\([\Delta r_{n,k} \cos \theta_{n,k} \cos \varphi_{n,k},
\Delta r_{n,k} \cos \theta_{n,k} \sin \varphi_{n,k}, \Delta r_{n,k} \sin \theta_{n,k}]^T\)</span>, where
<span>\(\varphi_{n,k}\)</span> is azimuth angle from <span>\(n\)</span>th radar position to <span>\(k\)</span>th subimage
center (<span>\(0\)</span> to <span>\(2\pi\)</span>) and <span>\(\theta_{n,k}\)</span> is elevation angle (<span>\(-\pi/2\)</span> to
<span>\(+\pi/2\)</span>), note that this is different from the usual spherical coordinate
definition that uses inclination. Assuming that <span>\(\Delta r_{n,k}\)</span> is small then the
target azimuth and elevation angles are almost the same before and after the
shift and the linear system for solving the 3D position error at <span>\(n\)</span>th position
can be written as:</p>
<p>$$\mathbf{R_n} = \mathbf{M_n} \Delta \mathbf{x_n}$$</p>
<p>$$\mathbf{M_n} = \begin{bmatrix}\cos \theta_{n,0} \cos \varphi_{n,0} &amp; \cos \theta_{n,0} \sin \varphi_{n,0}
&amp; \sin \theta_{n,0} \\ \cos \theta_{n,1} \cos \varphi_{n,1} &amp; \cos \theta_{n,1} \sin \varphi_{n,1}
&amp; \sin \theta_{n,1}\\ ... &amp; ... &amp; ...\\ \cos \theta_{n,k} \cos \varphi_{n,k} &amp; \cos \theta_{n,k} \sin \varphi_{n,k}
&amp; \sin \theta_{n,k} \end{bmatrix}, \, \mathbf{R_n} = \begin{bmatrix}\Delta r_{n,0} &amp; \Delta
r_{n,1} &amp; ... &amp; \Delta r_{n,k}\end{bmatrix}^T, \, \Delta \mathbf{x_n} = \begin{bmatrix}
\Delta x_n &amp; \Delta y_n &amp; \Delta z_n \end{bmatrix}^T$$</p>
<p><span>\(\mathbf{M_n}\)</span> is a Kx3 matrix of subimage center angles, <span>\(\mathbf{R_n}\)</span> is Kx1 matrix
of distance errors to each subimage, and <span>\(\Delta \mathbf{x_n}\)</span> is 3x1 matrix of 3D position errors.</p>
<p>Solving the linear system for <span>\(\Delta \mathbf{x_n}\)</span> at each radar position gives
the desired 3D position error.</p>
<h3 id="weighted-least-squares">Weighted Least Squares</h3>
<p>In a real SAR image signal-to-clutter ratio can vary a lot between different
targets. To get the most accurate estimate measurements should be weighted by
the signal-to-clutter ratio. The calculation for approximate of the
signal-to-clutter ratio from the target demodulated phase is quite involved and
the full details can be found in paper: <a href="https://ieeexplore.ieee.org/document/789644">&#34;Weighted least-squares estimation of
phase errors for SAR/ISAR autofocus&#34; by Wei Ye, Tat Soon Yeo and Zheng
Bao</a>.</p>
<p>The final result is that weighting factor <span>\(w_p\)</span> for each target in GPGA can be written as:</p>
<p>$$w_p = \frac{1}{\sigma_p^2} = \frac{d}{4 c^2 - 2d - 2 c \sqrt{4 c^2 - 3 d}}$$</p>
<p>$$d = E(|g_{n,p}|^2),\, c = E(|g_{n,p}|)$$</p>
<p>Weighting factor equals <span>\(1/\sigma_p^2\)</span> which is the inverse of variance of the
phase error in the measurement from target <span>\(p\)</span>. This weight should be used to
weight each target in GPGA phase gradient estimation.</p>
<p>In 3D trajectory estimation the weighting factor is the combined weight of all
targets in the subimage. It can be calculated as:</p>
<p>$$w_k = 1 / \sum_{p} 1 / w_p$$</p>
<h2 id="gpga-3d-trajectory-deviation-estimation">GPGA 3D trajectory deviation estimation</h2>
<div id="centered">
<p><img src="https://hforsten.com/img/autofocus/gpga_tde.svg" width="1406" height="1225"/></p><p>GPGA algorithm with 3D trajectory deviation estimation.</p>
</div>

<p>Putting it all together the algorithm is quite computationally demanding
due to multiple image formations required. They could be skipped in some cases,
but the best performance is obtained when a new image is formed after every
iteration to get a better estimate of true target locations. However, when using
<a href="https://ieeexplore.ieee.org/document/1238734">fast factorized backprojection</a>
it&#39;s quite fast when implemented on GPU even with a large amount of data.</p>
<p>Compared to other autofocus algorithms I believe this is one of the most general
ones. It computes the 3D position error, it doesn&#39;t assume linear track and can
work with any shaped flight track, it can correct for position error that
exceeds range resolution, it works with digital elevation model, and it can be
extended to work with 3D imaging SAR or bistatic SAR systems.</p>
<p>I implemented the algorithm on GPU using custom CUDA kernels and despite it&#39;s
heavy computation requirement it runs very quickly. In practice even just few
iterations result in a large improvement in image quality and it&#39;s much more
efficient than the <a href="https://hforsten.com/homemade-polarimetric-synthetic-aperture-radar-drone.html">Minimum entropy autofocus</a> that
I previously used.</p>
<p>The code is open source and available as <a href="https://github.com/Ttl/torchbp/blob/d756704656ce8c18cd77ed40516bc8da597ca448/torchbp/autofocus.py#L325">&#34;gpga_bp_polar_tde&#34; function in torchbp package</a>.</p>
<h3 id="synthetic-example">Synthetic example</h3>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/xsynthetic0.png.pagespeed.ic.YBy15_Zvgr.png" width="1192" height="473"/></p><p>Ideal SAR image of the point targets without any position error (left). Position error added to the image (right), Y-axis linear trend is removed.</p>
</div>

<p>I made a small synthetic example with nine point targets in a grid. The height
of the radar was set to 20 m and it moves linearly along Y-axis sampling every
quarter wave length (12.5 mm / 0.5 inch). I added random low frequency 3D
position error to the radar position and used the corrupted position to form the
initial SAR image.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/xsynthetic1.png.pagespeed.ic.qfWmUEmh1F.png" width="1192" height="473"/></p><p>Image with position error (left) and the same image after
    applying the autofocus algorithm (right).</p>
</div>

<p>The maximum position error is 0.1 meters and it causes the image to be extremely
defocused. Position error causes sidelobes to appear around each target.</p>
<p>The image was divided into nine subimages for autofocus so that each point
target is in its own image. After applying six iterations of the autofocus
algorithm the solved image looks almost exactly like the error-free ideal image.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/xsynthetic2.png.pagespeed.ic.poloThJc4T.png" width="1307" height="473"/></p><p>Solved 3D position error (left) and error in
    solved position in wavelengths (right).</p>
</div>

<p>The solved 3D position error matches well to the error that was added. The
maximum error is 0.1 wavelengths in Z-direction and RMS error is 0.025
wavelengths. The image quality is the most sensitive to X-direction error that
changes the measured distance the most, Z-axis error causes the smallest error
in this case with low look angle as Z-axis is mostly perpendicular to the
distance to the target.</p>
<h2 id="real-data-example">Real data example</h2>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_gopro.jpg.pagespeed.ic.-9a6oqF-BM.jpg" width="2160" height="2424"/></p><p>Camera image of the scene. This is from a different
    day and little farther away to capture more of the scene in the same image.</p>
</div>

<p>For a real data example I flew the radar drone in a straight line at 5 m/s
velocity at 110 m altitude. The radar recorded full quad (HH, HV, VH, VV)
polarizations with all four polarizations sampled at 1.8 ms repetition interval.
The total number of samples is 20,000 for a total track length of 180 meters and
36 seconds of data capture time.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_no_autofocus.png.pagespeed.ic.y5Li0BPUbQ.jpg" width="681" height="866"/></p><p>SAR image without autofocus using only the IMU and GPS
    for the radar position.</p>
</div>

<p>Without any autofocus algorithm and just relying on the GPS and IMU data fusion
results in very poor quality image. The reported GPS position <a href="https://en.wikipedia.org/wiki/Dilution_of_precision">horizontal
dilution of precision
(HDOP)</a> is on average 0.5
which is very good, but it&#39;s still not good enough for SAR accuracy requirement.
<a href="https://en.wikipedia.org/wiki/Real-time_kinematic_positioning">RTK GPS</a> would
be much more accurate, but it requires a ground station and there isn&#39;t a small
enough RTK GPS module that would fit well on the small FPV drone I&#39;m using.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_autofocus.png.pagespeed.ic.xTBsjbtGjK.jpg" width="681" height="866"/></p><p>The same image after autofocus with GPGA 3D
    trajectory estimation.</p>
</div>

<p>After GPGA 3D trajectory deviation autofocus algorithm the image quality is much
better. The image isn&#39;t still perfectly focused and it shouldn&#39;t be expected to,
because no digital elevation model (DEM) is used. Instead every target is
assumed to be on a flat plane at zero height. This assumption does not hold for
a real scene as buildings and trees are at higher elevation than the ground and
there is some elevation difference of the ground at different locations in the
image.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_trajectory.png.pagespeed.ic.-VJFzmDelX.png" width="711" height="473"/></p><p>Solved position error.</p>
</div>

<p>The Z-axis solved position error is very large and it&#39;s unlikely that the real
Z-axis error in the drone&#39;s flight elevation is this big and the error is likely
due to the flat-plane assumption. If I disable Z-axis error estimation and force
it to be zero <a href="https://hforsten.com/img/autofocus/07_19_1_autofocus_no_z.png">the image quality is
worse</a>. For a better
comparison here&#39;s a link to image <a href="https://hforsten.com/img/autofocus/07_19_1_autofocus.png">with Z-axis correction for
reference</a> and also the <a href="https://hforsten.com/img/autofocus/07_19_1_no_autofocus.png">image
without autofocus</a>.</p>
<p>The raw image dimensions are 6663 range samples by 13935 azimuth samples,
requiring a total of <span>\(6663\times13935\times20000 = 1.9\times10^{12}\)</span>
backprojections to form the image when using ordinary backprojection. Using fast
factorized backprojection image formation takes just 1.3 seconds on RTX 3090 Ti
GPU, which is about 10 times faster than non-factorized backprojection. The
final fully polarized image takes four times longer as it requires generating image for
each of the four polarizations.</p>
<p>Autofocus uses a smaller image size of 2333 x 12541 pixels and divides it into
10x10 grid for a total of 100 subimages. Autofocus is only calculated for one
polarization and the same solved position is used also for the other
polarizations. The starting lowpass window size is 2000 samples and it&#39;s reduced
by a factor of 0.7 every iteration until it&#39;s below 10 samples. The total number
of iterations is 15 and the calculation takes a total of 17.5 seconds. It&#39;s a significant
increase compared to the time it takes to form a single image due to requiring
multiple image formations, but since the image formation is so fast when
implemented on GPU it doesn&#39;t matter that much. Less iterations could work in
this case, but in general these settings work well.</p>

<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/ground_patch.svg" width="285" height="161"/></p><p>Patch of ground illuminated by the radar. Three
    different areas for normalizing the reflectivity are colored.</p>
</div>

<p>The received power from a target depends on the target&#39;s distance to the radar, 
location in the antenna beam, and the radar reflectivity of the target. Radar
reflectivity is what we want to determine and amplitude variations caused by the
distance and antenna pattern is something that should be removed from the radar
image.</p>
<p>Received power of a single radar pulse from a target can be written as:</p>
<p>$$P_r = C \frac{\sigma G_{rx}(\theta, \phi) G_{tx}(\theta, \phi)}{r^4}$$</p>
<p><span>\(C\)</span> is a constant that includes transmit power, receiver gain and other
radar constant factors affecting the received power, <span>\(G_{rx}\)</span> is the receiver
antenna gain, <span>\(G_{tx}\)</span> is the transmitting antenna gain, <span>\(r\)</span> is the distance
from radar to the target, and <span>\(\sigma\)</span> is the radar reflectivity of the target.</p>
<p>Amplitude of the received signal is proportional to square root of <span>\(P_r\)</span>. It
would be possible to add normalization in the backprojection calculation, but
it&#39;s more efficient to calculate backprojection of the power received from
a reference reflectivity separately:</p>
<p>$$P_{ref} = C \sum_{p \in \mathcal{P}} \sum_{n=1}^N \frac{\sigma_{ref} G_{rx}(\theta_n, \phi_n) G_{tx}(\theta_n, \phi_n)}{r^4}$$</p>
<p><span>\(\mathcal{P}\)</span> is the set of pixels in the image, <span>\(N\)</span> is the
number of radar measurements, <span>\(\theta_n\)</span> and <span>\(\phi_n\)</span> are elevation and azimuth
angles to the pixel at <span>\(n\)</span>th position, and <span>\(\sigma_{ref}\)</span> is the reference reflectivity
where we want to normalize. </p>
<p>Removing the antenna pattern and distance variation from SAR image <span>\(I\)</span> using
<span>\(P_{ref}\)</span> can be done by simply dividing the squared image by the reference power:</p>
<p>$$\sigma_0 = \frac{|I|^2}{P_{ref}}$$</p>
<p><span>\(P_{ref}\)</span> can be calculated on a coarser grid and number of sweeps can be
decimated since antenna pattern doesn&#39;t change very fast making it faster to
calculate than image backprojection.</p>
<p>We could simply set <span>\(\sigma_{ref} = 1\)</span> and this is valid option if we want to
normalize to point targets, but depending on how exactly the ground reflectivity
is defined some other options are also available. Reflectivity of the ground can
be defined in multiple ways. The most common one is to define <span>\(\sigma = \sigma_0
A_{\sigma}\)</span>, where <span>\(\sigma_0\)</span> is the radar reflectivity of ground per unit area
and <span>\(A_{\sigma} = \delta x \delta y\)</span> is the area of the patch on the ground. </p>
<p>The size of the patch that needs to be considered is the pixel resolution in the
SAR image. Radar measures radials distance, which means that <span>\(\delta_r\)</span> is
a constant that depends on the radar parameters. The angular resolution of the radar
<span>\(\delta_\varphi\)</span> is constant, so <span>\(\delta y = 2r\sin(\delta_\varphi/2)\)</span>. For SAR
image with non-isotropic antenna the resolution isn&#39;t as good, but
antenna weighting is already included in the calculation and we can use the
resolution with isotropic antenna.</p>
<p>This means that to normalize to ground reflectivity <span>\(\sigma_0\)</span> we should set:</p>
<p>$$\sigma_{ref} = A_{\sigma} = \delta_x \delta_y = \frac{\delta_r 2 r \sin(\delta_\varphi/2)}{\sin(\theta)} = C \frac{r}{\sin(\theta)}$$</p>
<p>Constant factor <span>\(C\)</span> can be merged with the constant factor in the <span>\(P_{ref}\)</span>
calculation. To avoid dividing by zero at zero ground range we can limit the <span>\(\theta\)</span> used in
calculation to angular size of the first resolution cell: <span>\(\theta_{\text{max}} \approx \arcsin(z/(z+\delta r)) \approx \pi/2 - \sqrt{2\delta_r/z}\)</span>.</p>
<p><span>\(\beta_0\)</span> or <span>\(\gamma_0\)</span> can be calculated the same way by changing the
<span>\(\sigma_{ref}\)</span> definition. <span>\(\beta_0\)</span> doesn&#39;t include any correction for the
grazing angle and <span>\(\sigma_0 = \beta_0 \sin\theta\)</span>. The third definition is to
calculate <span>\(\gamma_0 = \sigma/A_\gamma = \beta_0 \tan \theta\)</span>. This normalizes
for the area of the patch that the radar sees. If the ground reflectivity would
be completely diffuse so that it reflects equally well in all directions, then
the reflected power only depends on the area of the target that the radar sees
and only this definition would result in no incidence angle variation in the
image.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_autofocus_gamma0.png.pagespeed.ic.kzmHRRrnG8.jpg" width="681" height="866"/></p><p>The previous measurement after removal of range
    and antenna pattern caused amplitude variation. The reflectivity is
    normalized to sigma zero.</p>
</div>

<p>After normalization the resulting image is much flatter in amplitude and at the
edges where the signal-to-noise ratio is lower the noise floor is raised to
visible level. Absolute radiometric calibration is not done since the constant
<span>\(C\)</span> has not been determined. It would require measuring a known reflectivity
target and based on that measurement the <span>\(C\)</span> constant can be determined.
Incidence angle was calculates assuming a flat ground.</p>
<p>For comparison <a href="https://hforsten.com/img/autofocus/07_19_1_autofocus_gamma0.png">the same image with gamma
zero</a> is slightly brighter
at farther distances where the grazing angle is low. <a href="https://hforsten.com/img/autofocus/07_19_1_autofocus_beta0.png">Beta0
image</a> looks very similar to
the sigma0 image, there&#39;s only difference at extremely close ranges.  <a href="https://hforsten.com/img/autofocus/07_19_1_autofocus_sigma0.png">Here&#39;s
also a clickable link to sigma0
image</a>.  I recommend
opening them in separate tabs for easier comparison.</p>
<p>The first option of just setting <span>\(\sigma_{ref}=1\)</span> results in <a href="https://hforsten.com/img/autofocus/07_19_1_autofocus_point.png">very
similar image</a> to gamma zero
normalization, only differing at very close distances. It turns out that at low
grazing angle they are the same up to a constant.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_tx_power.png.pagespeed.ic.Ax5Dt7WIBM.png" width="822" height="849"/></p><p>Visualized <span>\(P_{ref}\)</span> for the previous image for VV polarization.</p>
</div>

<p>Plotting the image of <span>\(P_{ref}\)</span> shows how the antenna illuminates the ground.
The flight height was 110 m and the antenna was angled at 10 degrees below the
horizon and the antenna boresight should hit the ground at 600 m distance.
However, the best signal to noise ratio is obtained at 
100 m since it&#39;s closer to the radar and antenna gain is still quite good at
that angle.</p>
<p>Simulated antenna pattern was used in normalization and especially at far away
angles from the boresight the actual radiation pattern can differ significantly.
The uncertainty in antenna pattern causes amplitude difference at different
angles, and in the polarimetric case the antenna pattern for H and
V polarizations is slightly different which can cause errors in the relative
channel amplitudes.</p>

<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/sar_fmcw_block_pol.svg" width="706" height="339"/></p><p>Block diagram of the radar with error sources
    affecting the measured polarization labeled.</p>
</div>

<p>A fully polarized radar measures four polarizations: HH, HV, VH, and VV. The
first letter is the polarization of the transmitter, the second one is the
polarization of the receiver, and V and H stand for vertical and horizontal.
This is enough information to fully characterize the polarization response of
a target and allows calculating received power for an arbitrarily polarized receiver
antenna when the target is illuminated with an arbitrary polarization.</p>
<p>The transmission and reception of the different polarizations are implemented by
a switch that selects which polarization is transmitted or received. The same
electronics are used for all polarizations which results in good amplitude and
phase match across all frequencies. However, due to
polarization switches, slightly different lengths of SMA cables between switches and
antennas, and small differences between the antennas, there are amplitude and
phase differences between the polarizations. Additionally, the limited
isolation of the switches and antennas causes some crosstalk between
different polarizations.</p>
<p>To make accurate polarimetric measurements, channel amplitude and phase
differences as well as crosstalk need to be measured and calibrated out. The effect of
receiver and transmitter errors can be modeled as (<a href="https://ieeexplore.ieee.org/document/1610834">Ainsworth, et.
al</a>):</p>
<p>$$\begin{bmatrix} O_{HH} &amp; O_{HV}\\ O_{VH} &amp; O_{VV}\end{bmatrix} = \begin{bmatrix} r_{HH} &amp; r_{HV}\\ r_{VH} &amp; r_{VV}\end{bmatrix} \begin{bmatrix} S_{HH} &amp; S_{HV}\\ S_{VH} &amp; S_{VV}\end{bmatrix} \begin{bmatrix} t_{HH} &amp; t_{HV}\\ t_{VH} &amp; t_{VV}\end{bmatrix}$$</p>
<p><span>\([O]\)</span> is the observed polarization, <span>\([S]\)</span> is the true target scattering
matrix, and <span>\([t]\)</span> and <span>\([r]\)</span> are matrices that model how the scattering matrix is
transformed during the measurement by transmitter and receiver.</p>
<p>The previous equation can be written also in form:</p>
<p>$$ \begin{bmatrix}O_{HH}\\O_{HV}\\O_{VH}\\O_{VV}\end{bmatrix} = \mathbf{M} \begin{bmatrix}S_{HH}\\S_{HV}\\S_{VH}\\S_{VV}\end{bmatrix} $$</p>
<p><span>\(\mathbf{M}\)</span> is a 4x4 complex valued matrix that contains all the error terms
that affect the target measurement.</p>
<p>A simple error correction method is to assume that there is no crosstalk and
that absolute radiometric correction is already done. In
which case <span>\(\mathbf{M}\)</span> can be written as:</p>
<p>$$\mathbf{M} = \begin{bmatrix}k\alpha&amp;0&amp;0&amp;0\\0&amp;1/\alpha&amp;0&amp;0\\0&amp;0&amp;\alpha&amp;0\\0&amp;0&amp;0&amp;k^{-1}\alpha^{-1}\end{bmatrix}$$</p>
<p><span>\(\alpha\)</span> corrects for the imbalance of <span>\(HV\)</span> and <span>\(VH\)</span> measurements and <span>\(k\alpha\)</span>
corrects for <span>\(HH\)</span> and <span>\(VV\)</span> imbalance. Due to reciprocity, we should have <span>\(HV
= VH\)</span> and we can solve for <span>\(\alpha\)</span> from the measured data by calculating the
amplitude and phase difference between the measured <span>\(HV\)</span> and <span>\(VH\)</span> polarizations.
Calculating the correlation matrix <span>\(C = E(O O^\dagger)\)</span>, where <span>\(E(\cdot)\)</span>
calculates the mean over all pixels in the SAR image and <span>\(\dagger\)</span> is
conjugate transpose, we can solve for <span>\(\alpha\)</span>:</p>
<p>$$\alpha = \left|\frac{C_{VHVH}}{C_{HVHV}}\right|^{1/4} \exp(j\text{Arg}(C_{VHHV})/2)$$</p>
<p>For example: <span>\(C_{VHHV} = E(O_{VH} O_{HV}^*)\)</span>. This calibration was already
performed in the previous images so that the HV and VH channels could be summed for
visualization (with <span>\(k=1\)</span>). Determining the <span>\(k\)</span> factor is harder and requires measuring
a target with a known scattering matrix, usually a corner reflector.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/dihedral.svg" width="924" height="395"/></p><p>Scattering matrices of trihedral and dihedral
    corner reflectors at various angles. By measuring a known target, the
    polarimetric calibration matrix can be solved.</p>
</div>

<p>This simple measurement is enough to calibrate the biggest error sources, but
for more accurate result the crosstalk terms should also be solved. There are
several different polarimetric calibration methods and the one I decided to
implement is <a href="https://ieeexplore.ieee.org/document/1610834">&#34;Orientation angle preserving a posteriori polarimetric SAR calibration&#34; by T. L. Ainsworth, L. Ferro-Famil and Jong-Sen Lee</a>. This method works by
calculating correlation matrix from a SAR image of a scene, and only assuming
scattering reciprocity (HV=VH) calculates most of the error terms based on the expected
form of the correlation matrix. However, even with this method, at least one
measurement of a known target is required to determine the <span>\(k\)</span> calibration
factor.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_autofocus_sigma0_pol_cal.png.pagespeed.ic.ChkfdpHIr_.jpg" width="681" height="866"/></p><p>The previous image with polarimetric calibration.</p>
</div>

<p>After the polarimetric calibration, the largest difference is that the HH and VV
channel amplitude balance was corrected increasing the VV channel power making
the image redder compared to 
<a href="https://hforsten.com/img/autofocus/07_19_1_autofocus_sigma0.png">the uncorrected image</a>.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_autofocus_sigma0_pol_cal_pauli.png.pagespeed.ic.APFQACgSCm.jpg" width="681" height="865"/></p><p>Pauli decomposition of the calibrated image.</p>
</div>

<p>After calibration, polarimetric decompositions, such as Pauli
decomposition, can be applied. This is often used presentation for SAR data since it can be
interpreted physically. The three channels are: HH+VV, which corresponds to
targets where H and V polarization return at the same phase, for example
a sphere or trihedral corner reflector. HH-VV corresponds to a dihedral
reflector at 0 degrees, where one of the polarizations is flipped in phase. The last
channel VH+HV corresponds to targets that return orthogonal polarization, such
as 45 degree oriented dihedral reflector.</p>
<p>For surface targets, HH+VV corresponds to single or odd number of bounces, and
HH-VV double or even number of bounces. Targets that depolarize the
reflection, such as tree canopy are visible on the HV+VH channel.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x07_19_1_autofocus_full_pauli.png.pagespeed.ic.OmkXSeqTDv.png" width="920" height="865"/></p><p>Full SAR image with Pauli decomposition and
    without antenna pattern normalization.</p>
</div>

<p>In the fully zoomed-out image, quite lot of details can be seen even at long
distance, but the grazing angle at far away is very low, so only tree tops and
buildings are visible and shadowing is severe. This is without antenna
pattern normalization, since it doesn&#39;t make sense to normalize to flat ground
that isn&#39;t visible at long range, but the polarimetric calibration is still applied.</p>
<p>Buildings at (+250 X, -1000 Y) with lot of sharp corners look very different
from the surrounding trees and grass, which reflect a lot of cross-polarization.
Using polarimetric image, it&#39;s possible to classify the target response, such as vegetation
or artificial targets, which is one of the main uses of polarimetric SAR imaging.</p>

<p id="centered">
<video width="80%" controls="" loop="" muted="">
<source src="https://hforsten.com/video/autofocus/9_27_map.webm" type="video/webm"/>
<source src="https://hforsten.com/video/autofocus/9_27_map.mp4" type="video/mp4"/>
Your browser does not support the video tag.
</video>
</p>

<p>When a long radar measurement is divided into multiple short images, the result
can be turned into a video. The drone flew autonomously a circular pattern over a field
while the radar recorded continuously. Afterwards I autofocused and generated SAR
images of short sections of the data, then synced them with the GoPro recording. The
video is played back at 2x speed.</p>
<p>Each image has 2048 radar measurements, with 75% overlap between consecutive images.
Having more data in each frame improves the image resolution and decrease the
amount of noise, but lowers the frame rate in the video.
The high overlap increases the frame rate as a new frame can be created when
there is 25% of new data instead of waiting for a full window size and allows
for good quality image with reasonable frame rate.</p>
<p>Each frame is autofocused individually and the solved position is used for the
future frames. Apart from this, no inter-frame stabilization is performed. There
are some small geometric errors, but overall the stability between frames
is quite good. Antenna pattern is normalized to gamma0 and image edges with
low SNR are cut out. Radial lines appear on some frames caused by RF interference.</p>

<p>I captured another scene with more buildings. The drone flew in a straight line
at height of 110 m and a speed of 5 m/s for 72 seconds, covering a total track
length of 360 m. During this time the radar captured 40,000 sweeps.</p>
<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x08_23_2_gopro.jpg.pagespeed.ic.Zq9Oy8N-gc.jpg" width="2160" height="2196"/></p><p>Camera image of the scene.</p>
</div>

<div id="centered">
    <p><img src="https://hforsten.com/img/autofocus/x08_23_2_autofocus.png.pagespeed.ic.7WO_IXNFAr.jpg"/></p><p>Image of urban scene after
    autofocus, antenna pattern normalization, polarimetric calibration,
    gamma zero normalization, and Pauli decomposition visualization. The camera image
    is taken from approximately X=100 m.</p>
</div>

<p>Autofocus required 41 seconds for this scene, and image formation took 2.3
seconds per polarization. The resulting image is 6663 by 27863 pixels.
Autofocus improves the image quality significantly. <a href="https://hforsten.com/img/autofocus/08_23_2_no_autofocus.png">Without
autofocus</a> the image is badly
unfocused.</p>

<p>A complete software pipeline for autofocusing and calibrating polarimetric SAR
image was presented. Autofocus algorithm that uses generalized phase gradient
autofocus (GPGA) and 3D trajectory deviation estimation to solve for the 3D
position error was introduced. Using GPU accelerated fast factorized
backprojection even very large SAR images can be focused very quickly.</p>
<p>The resulting image quality is very good especially considering that this might
be one of the cheapest drone mounted SAR systems.</p>
<p>The software is open sourced and available at:
<a href="https://github.com/Ttl/torchbp">https://github.com/Ttl/torchbp</a>.</p>

            </div></div>
  </body>
</html>
