<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.ssp.sh/blog/practical-data-modeling-clickhouse/">Original</a>
    <h1>Data Modeling Guide for Real-Time Analytics with ClickHouse</h1>
    
    <div id="readability-page-1" class="page"><div><p><img src="https://www.ssp.sh/blog/practical-data-modeling-clickhouse/featured-image.png" alt="Data Modeling Guide for Real-Time Analytics with ClickHouse"/>
            </p><div id="toc-static" data-kept="">
                <p><span>Contents</span>
                </p>
                <div id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#data-flow-for-real-time-analytics">Data Flow for Real-time Analytics</a>
      <ul>
        <li><a href="#data-flow-is-knowing-the-requirements">Data Flow is Knowing the Requirements</a></li>
        <li><a href="#real-time-analytics-a-tradeoff">Real-Time Analytics: A Tradeoff</a></li>
        <li><a href="#the-payoff-of-great-data-flow">The Payoff of Great Data Flow</a></li>
      </ul>
    </li>
    <li><a href="#clickhouse-modeling-strategies-from-theory-to-practice">ClickHouse Modeling Strategies: From Theory to Practice</a>
      <ul>
        <li><a href="#modeling-data-with-clickhouse">Modeling Data with ClickHouse</a></li>
      </ul>
    </li>
    <li><a href="#demo-using-s3---clickhouse---rill">Demo: Using S3 -&gt; ClickHouse -&gt; Rill</a>
      <ul>
        <li><a href="#ingest-and-transformation">Ingest and Transformation</a></li>
        <li><a href="#visualizing-in-rill">Visualizing in Rill</a></li>
        <li><a href="#what-did-we-learn-so-far">What Did We Learn so Far?</a></li>
      </ul>
    </li>
    <li><a href="#applicable-tips--tricks">Applicable Tips &amp; Tricks</a>
      <ul>
        <li><a href="#deduplication-strategies">Deduplication Strategies</a></li>
        <li><a href="#performance-optimization">Performance Optimization</a>
          <ul>
            <li><a href="#partitioning-strategy">Partitioning Strategy</a></li>
            <li><a href="#predicate-pushdown-optimization">Predicate Pushdown Optimization</a></li>
            <li><a href="#pre-aggregation-with-aggregatingmergetree">Pre-Aggregation with AggregatingMergeTree</a></li>
          </ul>
        </li>
        <li><a href="#storage-efficiency">Storage Efficiency</a>
          <ul>
            <li><a href="#data-sketches-for-approximation">Data Sketches for Approximation</a></li>
            <li><a href="#rollup-to-optimal-time-granularity">Rollup to Optimal Time Granularity</a></li>
          </ul>
        </li>
        <li><a href="#sampling-strategies">Sampling Strategies</a>
          <ul>
            <li><a href="#statistical-sampling-for-large-datasets">Statistical Sampling for Large Datasets</a></li>
          </ul>
        </li>
        <li><a href="#schema-management">Schema Management</a>
          <ul>
            <li><a href="#table-projections-for-query-optimization">Table Projections for Query Optimization</a></li>
            <li><a href="#schema-evolution-best-practices">Schema Evolution Best Practices</a></li>
          </ul>
        </li>
        <li><a href="#time-series-optimization">Time Series Optimization</a>
          <ul>
            <li><a href="#always-store-in-utc">Always Store in UTC</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#choosing-the-right-clickhouse-modeling-strategy">Choosing the Right ClickHouse Modeling Strategy</a></li>
  </ul>
</nav></div>
            </div><p>
                    This article was written as part of <a href="http://tinylogger.com/services">my services</a>
                </p><p>Querying billions of weather records and getting results in under 200 milliseconds isn’t theory; it’s what real-time analytics solutions provide. Processing streaming IoT data from thousands of sensors while delivering real-time dashboards with no lag is what certain business domains need. That’s what you’ll learn at the end of this guide through building a ClickHouse-modeled analytics use case.</p>
<p>You’ll learn how to land data in ClickHouse that is optimized for real-time data applications, going from basic ingestion to advanced techniques like statistical sampling, pre-aggregation strategies, and multi-level optimization. I’ve included battle-tested practices from Rill’s years of implementing real-time analytics for customers processing everything from financial transactions and programmatic advertising to IoT telemetry.</p>
<p>This article is for data engineers and practitioners who want to build analytics that deliver sub-second query responses, and who want to unlock ClickHouse’s full potential for real-time analytics demands. By the end, you’ll have a playbook for ClickHouse data modeling plus a working example that ingests NOAA weather data from S3 and visualizes it with a single configuration file.</p>
<div><p><i></i>Why ClickHouse<i></i></p><div><div><p>If you haven’t heard of ClickHouse or are wondering why it’s becoming the go-to choice for real-time analytics, here’s what sets it apart from traditional data warehouses.</p>
<p>ClickHouse achieves blazingly fast analytical performance through column-oriented storage that reads only relevant data, advanced compression (LZ4/ZSTD), and vectorized query execution that maximizes CPU capabilities. Its sparse primary indexing with data skipping eliminates irrelevant data blocks, while the C++ implementation avoids JVM overhead for bare-metal performance.</p>
<p>These innovations enable sub-second query responses on billions of rows, performance that would take minutes or hours in traditional data warehouses. Storage efficiency has a direct impact on both cost and speed at scale, making ClickHouse the ideal foundation for the real-time analytics modeling strategies covered in this article.</p></div></div></div>
<h2 id="data-flow-for-real-time-analytics">Data Flow for Real-time Analytics</h2>
<p>Before we see a concrete example of modeling data with ClickHouse, specifically for real-time and online analytical processing (OLAP) cubes, it’s important to <strong>understand the flow of data</strong>, its trade-offs, and payoffs. Where Does Data Come From, and Where Does It Go?</p>
<h3 id="data-flow-is-knowing-the-requirements">Data Flow is Knowing the Requirements</h3>
<p>Data flows from sources to analytics. In the simplest terms, we have sources of data, a transformation with aggregations, and the visualization. Most often, the data should travel from source to visualization as quickly as possible and respond fast to queries.</p>

<p>Most data flow modeling is handled in the transformation phase. Connecting to a source, whether it is an S3 or R2 bucket, a relational database like Postgres or others, or visualization on an analytical tool. We need to <strong>aggregate and combine the data</strong> to extract business insights out of masses of information to answer the questions our business needs to answer.</p>
<p>Obviously data modeling can get much more involved—looking at modeling open data stack, or looking at <a href="https://dedp.online/part-1/1-introduction/history-and-state-of-data-engineering.html" target="_blank" rel="noopener noreffer">The State of Data Engineering</a> and its <a href="https://dedp.online/part-1/1-introduction/challenges-in-data-engineering.html" target="_blank" rel="noopener noreffer">challenges</a>. However, modeling data has nothing to do with choosing tools in the first place. If we have the best tools but a bad data flow, it’s not worth much.</p>
<p>The below illustration shows where the modeling part actually happens:<br/>






























</p><figure>
<a target="_blank" href="http://tinylogger.com/blog/practical-data-modeling-clickhouse/data-modeling.png" title="/blog/practical-data-modeling-clickhouse/data-modeling.png">
<img loading="lazy" decoding="async" sizes="(min-width: 35em) 800px, 100vw" srcset="/blog/practical-data-modeling-clickhouse/data-modeling_hu_6ffbd8cfc395b04c.png 500w
, /blog/practical-data-modeling-clickhouse/data-modeling_hu_2424102b593ad87e.png 800w, /blog/practical-data-modeling-clickhouse/data-modeling_hu_2dcca3878cd785dc.png 1200w" src="http://tinylogger.com/blog/practical-data-modeling-clickhouse/data-modeling.png" alt="/blog/practical-data-modeling-clickhouse/data-modeling.png" title="/blog/practical-data-modeling-clickhouse/data-modeling.png"/>
</a><figcaption>From the book <a href="https://www.amazon.com/dp/1837634459" target="_blank" rel="noopener noreffer">Data Modeling with Snowflake</a> by Serge Gershkovich | Like seeing a forest for the trees, ubiquitous modeling allows us to see the business for the data.</figcaption>
</figure>
<p>Most often, modeling is more about offline, off-computer, and real conversations with the business people involved than figuring it out ourselves. We have to answer the questions “What’s needed on a dashboard?” “Which numbers are even possible with the data at hand?” and “How can we get them, join and aggregate them with other data from the company to get the best possible insights?”</p>
<div><p><i></i>Shifting Left: Another form of modeling<i></i></p><div><p><a href="https://www.ssp.sh/blog/shifting-left/" target="_blank" rel="noopener noreffer">Shifting Left</a> is another important concept related to data modeling. It means that the better we model and structure data at the source (left side of the data pipeline), the more efficient and accurate our analytics become downstream (right side). When raw data is properly typed, deduplicated, and structured early in the pipeline, we avoid expensive transformations later and reduce the risk of data quality issues propagating through our entire analytics stack. This is especially critical for real-time systems where you can’t afford lengthy batch cleanup processes.</p></div></div>
<h3 id="real-time-analytics-a-tradeoff">Real-Time Analytics: A Tradeoff</h3>
<p>Real-time analytics specifics are always a tradeoff between <strong>data freshness and accuracy</strong>.</p>
<p>The moment the data is loaded, it is outdated. But to avoid pulling the latest all the time, we need to make sure the data is consistent across tables, meaning related data is pulled too when we refresh, so that it’s cohesive and accurate.</p>
<p>In the end, you need a set of metrics that are business critical for your organization. Some businesses like IoT and e-commerce don’t need all data, but specific data such as IP or location to identify quickly where users come from. Use cases like this especially need and benefit from low-latency query responses. Data needs to load near real-time and needs to deliver fast, flexible access to core analytics.</p>
<h3 id="the-payoff-of-great-data-flow">The Payoff of Great Data Flow</h3>
<p>The payoffs of modeling are higher performance, insights on consistent data, and lower cost as we do not need to query production with a reduced aggregated data set and without the need for heavy overnight ETL jobs. We need less storage for aggregated data and get even faster query responses.</p>
<p>Imagine a fast river that flows constantly with great volume. This is what good data will look like when new data is coming in steady and accurate.</p>
<p>Let’s see that in action with ClickHouse real-time modeling.</p>
<h2 id="clickhouse-modeling-strategies-from-theory-to-practice">ClickHouse Modeling Strategies: From Theory to Practice</h2>
<p>Now that we understand the data flow requirements for real-time analytics such as fast ingestion, efficient transformation, and sub-second query responses, let’s explore how ClickHouse specifically addresses these challenges through its modeling approaches.</p>
<p>Remember our data flow: <code>Sources → Transformation &amp; Aggregation → ClickHouse → Visualization</code>. The key insight is that ClickHouse doesn’t only serve as storage but can handle much of the transformation and aggregation work directly, eliminating traditional ETL bottlenecks.</p>
<p>ClickHouse offers several strategies to optimize this flow, each addressing different aspects of the freshness-accuracy tradeoff we discussed:</p>
<p><strong>For Minimizing Query-Time Complexity:</strong></p>
<ul>
<li><a href="https://clickhouse.com/docs/data-modeling/denormalization" target="_blank" rel="noopener noreffer"><strong>Denormalizing data</strong></a>: Move joins from query time to insert time by flattening related tables into a single structure (One Big Table, approach). This trades some storage efficiency for dramatic query performance gains. Especially recommended for tables that change infrequently and not for high-cardinality or many-to-many relationships.</li>
<li><a href="https://clickhouse.com/docs/dictionary" target="_blank" rel="noopener noreffer"><strong>Dictionaries</strong></a>: Handle dimension lookups through in-memory key-value structures, perfect for enriching streaming data with relatively static reference information.</li>
</ul>
<p><strong>For Real-Time Aggregation:</strong></p>
<ul>
<li><a href="https://clickhouse.com/docs/materialized-view/incremental-materialized-view" target="_blank" rel="noopener noreffer"><strong>Incremental Materialized Views</strong></a>: Shift computational cost from query time to insert time, computing aggregates as data arrives rather than when users request it. Most suitable for real-time aggregations and transformations, especially for single-table aggregations or simple enrichments with static dimension tables.</li>
<li><a href="https://clickhouse.com/docs/materialized-view/refreshable-materialized-view" target="_blank" rel="noopener noreffer"><strong>Refreshable Materialized Views</strong></a>: Handle complex multi-table joins and transformations on a scheduled basis, suitable when real-time freshness isn’t critical. They are also useful for batch denormalization and building view dependencies (like DAGs) and can be scheduled with <a href="https://clickhouse.com/docs/integrations/dbt" target="_blank" rel="noopener noreffer">dbt</a>, Airflow, and other data orchestrators. Refreshable MVs are similar to materialized views in traditional OLTP databases.</li>
</ul>
<p>The fundamental principle underlying all these approaches is <strong>minimizing joins at query time</strong>. In traditional OLAP cubes, much of this complexity is handled by pre-built logical modeling layers. ClickHouse takes a different approach where you explicitly choose where in the pipeline to handle complexity based on your specific performance and freshness requirements.</p>
<h3 id="modeling-data-with-clickhouse">Modeling Data with ClickHouse</h3>
<p>An interesting new dimension is modeling multi-dimensional cubes. What’s the difference, you might ask? Besides the difference between traditional OLAP cubes and modern OLAP cubes, which first stores measures and joins within the cube and pre-processes, whereas <a href="https://www.ssp.sh/blog/scaling-beyond-postgres/" target="_blank" rel="noopener noreffer">modern real-time databases</a> systems like ClickHouse, Pinot, Druid, and StarRocks do not. This is at first glance a disadvantage, but on the other hand an advantage, that we can change our queries at query time without re-processing needed.</p>
<p>What else do we need to know about <em>OLAP data modeling</em>? We need to understand that OLAP cubes store data in a <strong>column-oriented (or columnar)</strong> way. This is important to the ClickHouse architecture. Unlike traditional row-oriented databases that store all values in a row together, ClickHouse stores all values that belong to a single column together. This also influences how we model our data and enables fast analytical queries based on a few columns out of potentially hundreds. ClickHouse only needs to read the data files for those specific columns, drastically reducing disk I/O compared to reading entire rows.</p>
<p>Usually when we model a multi-dimensional cube, we deal with facts and dimensions. The queries are optimized for <strong>sub-second</strong> response times and the users might be our clients or business users; there might only be one visualization layer in between such as a BI tool or Excel. This means it’s mission-critical.</p>
<p>In ClickHouse and in general with cubes, we are working with dimensions, measures, and operators that operate on time aggregations and dimensions. You want rollups and drill-downs along multiple axes, with subtotals and potentially pivots.</p>
<p>SQL can sometimes be hard work to get right as we constantly pivot along different dimensions, and there are joins involved, different granularity, and all of a sudden, you accidentally duplicate your counting by adding a wrong dimension.</p>
<p>So how do we effectively model ClickHouse to get real-time data from start to end with no more than needed effort?</p>
<p>In the following example, we’ll see several of these strategies in action: denormalization through data transformation during ingestion, partitioning for query optimization, and incremental processing for real-time updates.</p>
<div><p><i></i>Traditional Cubes<i></i></p><div><p>There’s no logical modeling layer like in <a href="https://learn.microsoft.com/en-us/analysis-services/ssas-overview?view=sql-analysis-services-2025" target="_blank" rel="noopener noreffer">SQL Server Analysis Services (SSAS)</a>, meaning we need to model our data outside of ClickHouse to create pre-defined optimized tables to query with the methods explained above such as materialized views, small lookup tables, or denormalized tables.</p></div></div>
<h2 id="demo-using-s3---clickhouse---rill">Demo: Using S3 -&gt; ClickHouse -&gt; Rill</h2>
<p>But we can design and model the data flow easily to source data from an S3/R2 bucket, load from Kafka, or other streaming data sources.</p>
<p>Let’s have a look at a practical example where we ingest data from S3, using ClickHouse as the engine to do transformation and aggregation, ingesting the data <a href="https://docs.rilldata.com/build/advanced-models/incremental-models" target="_blank" rel="noopener noreffer">incrementally</a> with the built-in refresh by ClickHouse, and visualizing with Rill.</p>






























<figure>
<a target="_blank" href="http://tinylogger.com/blog/practical-data-modeling-clickhouse/rill-dashboard.webp" title="/blog/practical-data-modeling-clickhouse/rill-dashboard.webp">
<img loading="lazy" decoding="async" sizes="(min-width: 35em) 800px, 100vw" srcset="/blog/practical-data-modeling-clickhouse/rill-dashboard_hu_e36c81e62f9c5e5e.webp 500w
, /blog/practical-data-modeling-clickhouse/rill-dashboard_hu_942d8f5bd79478b1.webp 800w, /blog/practical-data-modeling-clickhouse/rill-dashboard_hu_5a2afdbd3032795.webp 1200w" src="http://tinylogger.com/blog/practical-data-modeling-clickhouse/rill-dashboard.webp" alt="/blog/practical-data-modeling-clickhouse/rill-dashboard.webp" title="/blog/practical-data-modeling-clickhouse/rill-dashboard.webp"/>
</a><figcaption>Dashboard overview in Rill</figcaption>
</figure>
<p>Watch the short video for the interactive version - below we are going to explain each config step by step.<br/>
</p><p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube-nocookie.com/embed/8Ih6YrZTlIw?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe>
    </p>

<div><p><i></i>Demo used in this chapter available on GitHub<i></i></p><div><div><p>Find everything shown in this demo at <a href="https://github.com/sspaeti/clickhouse-modeling-rill-example" target="_blank" rel="noopener noreffer">clickhouse-modeling-rill-example</a>.</p></div></div></div>
<h3 id="ingest-and-transformation">Ingest and Transformation</h3>
<p>This example represents an end-to-end data project, loading <a href="https://commoncrawl.org/blog/index-to-warc-files-and-urls-in-columnar-format" target="_blank" rel="noopener noreffer">NOAA weather data</a> that gets updated from S3 via ClickHouse and visualized in Rill. All within a single YAML shown here (expand to see the full code):</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span><span>43
</span><span>44
</span><span>45
</span><span>46
</span><span>47
</span><span>48
</span><span>49
</span><span>50
</span><span>51
</span><span>52
</span><span>53
</span><span>54
</span><span>55
</span><span>56
</span><span>57
</span><span>58
</span><span>59
</span><span>60
</span><span>61
</span><span>62
</span><span>63
</span><span>64
</span><span>65
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="yaml"><span><span><span>type</span><span>:</span><span> </span><span>model</span><span>
</span></span></span><span><span><span></span><span>materialize</span><span>:</span><span> </span><span>true</span><span>
</span></span></span><span><span><span></span><span>incremental</span><span>:</span><span> </span><span>true</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># Do an incremental refresh every hour.</span><span>
</span></span></span><span><span><span></span><span>refresh</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>cron</span><span>:</span><span> </span><span>&#34;0 * * * *&#34;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># Use SQL partitions to define year-based partitions for NOAA data</span><span>
</span></span></span><span><span><span></span><span># This demonstrates ClickHouse&#39;s S3 capabilities with yearly partitioning</span><span>
</span></span></span><span><span><span></span><span>partitions</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>sql</span><span>:</span><span> </span><span>SELECT generate_series AS year FROM generate_series(2024, 2025)</span><span>
</span></span></span><span><span><span>  </span><span>#grabbing all files</span><span>
</span></span></span><span><span><span>  </span><span>#glob: s3://noaa-ghcn-pds/csv.gz/*.csv.gz </span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># Load and transform NOAA weather data with proper column names and types</span><span>
</span></span></span><span><span><span></span><span># This showcases ClickHouse&#39;s data transformation capabilities</span><span>
</span></span></span><span><span><span></span><span>sql</span><span>:</span><span> </span><span>&gt;</span><span>
</span></span></span><span><span><span>  SELECT
</span></span></span><span><span><span>      &#39;{{ .partition.year }}&#39; AS __partition,
</span></span></span><span><span><span>      now() AS __load_time,
</span></span></span><span><span><span>      -- Transform raw CSV columns to proper NOAA weather schema
</span></span></span><span><span><span>      COALESCE(c1, &#39;UNKNOWN&#39;) AS station_id,
</span></span></span><span><span><span>      COALESCE(toDate(toString(c2)), toDate(&#39;1900-01-01&#39;)) AS measurement_date,
</span></span></span><span><span><span>      COALESCE(c3, &#39;UNKNOWN&#39;) AS measurement_type, -- TMIN, TMAX, PRCP, SNOW, etc.
</span></span></span><span><span><span>      toFloat32(c4) / 10.0 AS measurement_value, -- Convert from tenths
</span></span></span><span><span><span>      c5 AS measurement_flag,
</span></span></span><span><span><span>      c6 AS quality_flag,
</span></span></span><span><span><span>      c7 AS source_flag,
</span></span></span><span><span><span>      c8 AS observation_time,
</span></span></span><span><span><span>      -- Add derived fields for analytics
</span></span></span><span><span><span>      toYear(toDate(toString(c2))) AS measurement_year,
</span></span></span><span><span><span>      toMonth(toDate(toString(c2))) AS measurement_month,
</span></span></span><span><span><span>      toDayOfYear(toDate(toString(c2))) AS measurement_day_of_year,
</span></span></span><span><span><span>      -- Temperature conversions for common analysis
</span></span></span><span><span><span>      CASE 
</span></span></span><span><span><span>        WHEN c3 = &#39;TMIN&#39; THEN toFloat32(c4) / 10.0
</span></span></span><span><span><span>        ELSE NULL 
</span></span></span><span><span><span>      END AS temp_min_celsius,
</span></span></span><span><span><span>      CASE 
</span></span></span><span><span><span>        WHEN c3 = &#39;TMAX&#39; THEN toFloat32(c4) / 10.0  
</span></span></span><span><span><span>        ELSE NULL
</span></span></span><span><span><span>      END AS temp_max_celsius,
</span></span></span><span><span><span>      CASE 
</span></span></span><span><span><span>        WHEN c3 = &#39;PRCP&#39; THEN toFloat32(c4) / 10.0
</span></span></span><span><span><span>        ELSE NULL
</span></span></span><span><span><span>      END AS precipitation_mm
</span></span></span><span><span><span>  FROM s3(
</span></span></span><span><span><span>      &#39;s3://noaa-ghcn-pds/csv.gz/by_year/{{ .partition.year }}.csv.gz&#39;,
</span></span></span><span><span><span>      &#39;CSV&#39;
</span></span></span><span><span><span>  )</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span># Insert the results into a partitioned table that uses the MergeTree engine.</span><span>
</span></span></span><span><span><span></span><span># Optimized for time-series weather data analytics</span><span>
</span></span></span><span><span><span></span><span>output</span><span>:</span><span>
</span></span></span><span><span><span>  </span><span>incremental_strategy</span><span>:</span><span> </span><span>partition_overwrite</span><span>
</span></span></span><span><span><span>  </span><span>partition_by</span><span>:</span><span> </span><span>__partition</span><span>
</span></span></span><span><span><span>  </span><span>engine</span><span>:</span><span> </span><span>MergeTree</span><span>
</span></span></span><span><span><span>  </span><span># Optimize ordering for typical weather queries: by date, station, measurement type</span><span>
</span></span></span><span><span><span>  </span><span># Using COALESCE ensures non-nullable columns in sorting key</span><span>
</span></span></span><span><span><span>  </span><span>order_by</span><span>:</span><span> </span><span>(measurement_date, station_id, measurement_type)</span><span>
</span></span></span><span><span><span>  </span><span># Primary key for fast weather station and date lookups</span><span>
</span></span></span><span><span><span>  </span><span>primary_key</span><span>:</span><span> </span><span>(measurement_date, station_id)</span><span>
</span></span></span><span><span><span>  </span><span># TTL for data retention (optional - uncomment if needed)</span><span>
</span></span></span><span><span><span>  </span><span># ttl: measurement_date + INTERVAL 10 YEAR</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><p><em>Source code and full project can be found on GitHub at <a href="https://github.com/sspaeti/clickhouse-modeling-rill-example" target="_blank" rel="noopener noreffer">clickhouse-modeling-rill-example</a></em></p>
<p>So what happens here?</p>
<p>This YAML configuration demonstrates how ClickHouse can serve as both your data transformation engine and storage layer, eliminating the need for traditional ETL tools.</p>
<p><strong>Data ingestion and transformation in one step:</strong> The <code>sql</code> section directly reads compressed CSV files from S3 using ClickHouse’s native <code>s3()</code> function. Rather than requiring a separate ETL process to extract, clean, and load the data, ClickHouse performs all transformations during the ingestion process itself. The query handles data type conversions (like converting temperature readings from tenths to actual values with <code>toFloat32(c4) / 10.0</code>), creates derived fields for analytics (such as extracting year, month, and day components), and applies data quality measures using <code>COALESCE</code> to handle null values.</p>
<p><strong>MergeTree is your built-in ETL engine:</strong> The <code>engine: MergeTree</code> specification transforms ClickHouse into what you can think of as “local ETL without the need for an ETL tool.” MergeTree engines are specifically designed for high data ingest rates and massive data volumes. When new data arrives, ClickHouse creates table parts that are automatically merged by background processes, maintaining optimal query performance without manual intervention. This means your data pipeline becomes very lightweight and self-managing – new weather data gets ingested, transformed, and optimized automatically based on defined cron triggers.</p>
<p><strong>Multi-level optimization strategy:</strong> This example demonstrates ClickHouse’s ability to optimize at multiple levels simultaneously. At the <strong>query level</strong>, the <code>order_by: (measurement_date, station_id, measurement_type)</code> ensures that data is physically sorted for optimal access patterns typical in weather analytics. This is very important to your end query and how your response will perform. At the <strong>storage level</strong>, the <code>partition_by: __partition</code> creates year-based partitions that enable ClickHouse to skip entire data segments when querying specific time ranges. The <strong>incremental strategy</strong> with <code>partition_overwrite</code> means only changed partitions are reprocessed, not the entire dataset.</p>
<p><strong>Real-time processing without complexity:</strong> The <code>refresh: cron: &#34;0 * * * *&#34;</code> configuration creates an automated pipeline that updates hourly without requiring external orchestration tools like Airflow or Dagster. ClickHouse handles the scheduling, dependency management, and incremental processing internally.</p>
<p>Further optimizations are <a href="https://clickhouse.com/docs/use-cases/observability/clickstack/ttl" target="_blank" rel="noopener noreffer"><em>TTL</em> (time-to-live)</a>, which deletes data after a defined retention period such as <code>hour + INTERVAL 90 DAY DELETE</code>, or we can apply further <a href="https://clickhouse.com/docs/operations/settings/merge-tree-settings" target="_blank" rel="noopener noreffer">table features</a> such as:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="yaml"><span><span><span># Additional optimizations for data lifecycle and projection management</span><span>
</span></span></span><span><span><span></span><span>table_settings</span><span>:</span><span> </span><span>&gt;</span><span>
</span></span></span><span><span><span>    # Handle projections during deduplication: &#39;rebuild&#39; recreates projections after merge
</span></span></span><span><span><span>    deduplicate_merge_projection_mode = &#39;rebuild&#39;,
</span></span></span><span><span><span>    # Speed up TTL-based data compression merges (0 = immediate, default: 4 hours)
</span></span></span><span><span><span>    merge_with_recompression_ttl_timeout = 0,
</span></span></span><span><span><span>    # Speed up TTL-based data deletion merges (0 = immediate, default: 4 hours)  
</span></span></span><span><span><span>    merge_with_ttl_timeout = 0</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><p>These settings optimize both deduplication behavior with projections and accelerate automatic data lifecycle management through more frequent TTL merges, ensuring expired data is cleaned up promptly rather than waiting for the default 4-hour intervals.</p>
<div><p><i></i>Native Deduplication Features<i></i></p><div><div><p>ClickHouse provides built-in <strong><a href="https://clickhouse.com/docs/guides/developer/deduplicating-inserts-on-retries#query-level-insert-deduplication" target="_blank" rel="noopener noreffer">insert deduplication</a></strong> for retry scenarios by creating unique <code>block_id</code> hashes for each inserted block. Duplicate blocks are skipped automatically.</p>
<p><strong>Key settings</strong> are <code>insert_deduplicate=1</code> enables block-level deduplication (default for replicated tables) and <code>insert_deduplication_token</code> provides custom deduplication keys for explicit control. This is block-level deduplication at insert time, unlike ReplacingMergeTree’s row-level deduplication during merges. For more details, see the <a href="https://clickhouse.com/docs/operations/settings/settings#insert_deduplication_token" target="_blank" rel="noopener noreffer">deduplication token documentation</a>.</p></div></div></div>
<h3 id="visualizing-in-rill">Visualizing in Rill</h3>
<p>The above YAML is the source <code>noaa-weather.yaml</code> and when you start rill after cloning the example above with:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sh"><span><span>rill start git@github.com:sspaeti/clickhouse-modeling-rill-example.git
</span></span></code></pre></td></tr></tbody></table>
</div>
</div><p>You can click on the source, and the data will be automatically loaded from the S3 source, and the above-defined transformations and conversions will be made:</p>






























<figure>
<a target="_blank" href="http://tinylogger.com/blog/practical-data-modeling-clickhouse/rill-source.webp" title="/blog/practical-data-modeling-clickhouse/rill-source.webp">
<img loading="lazy" decoding="async" sizes="(min-width: 35em) 800px, 100vw" srcset="/blog/practical-data-modeling-clickhouse/rill-source_hu_fc937847f9b6048c.webp 500w
, /blog/practical-data-modeling-clickhouse/rill-source_hu_2eace35b41004324.webp 800w, /blog/practical-data-modeling-clickhouse/rill-source_hu_6202c9936d35d00f.webp 1200w" src="http://tinylogger.com/blog/practical-data-modeling-clickhouse/rill-source.webp" alt="/blog/practical-data-modeling-clickhouse/rill-source.webp" title="/blog/practical-data-modeling-clickhouse/rill-source.webp"/>
</a><figcaption>Source-View in Rill ingesting 58 million rows</figcaption>
</figure>
<h3 id="what-did-we-learn-so-far">What Did We Learn so Far?</h3>
<p>To recap this example, ClickHouse offers a fundamentally different approach compared to other <a href="https://www.ssp.sh/blog/scaling-beyond-postgres/" target="_blank" rel="noopener noreffer">real-time databases</a> like Druid, where most heavy lifting must be done <strong>ahead of ingestion</strong> using Spark or other compute engines. With ClickHouse, the engine itself handles complex aggregations and optimizations at ingestion time, during query execution, and even post-ingestion.</p>
<div>
        <p><i></i>Rill does all the orchestration<i></i></p>
        <div>
            <div><p>Interestingly, Rill automatically spawns up ClickHouse and orchestrates the incremental loads and ingests data. If you will, Rill is doing orchestration work.</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sh"><span><span>&gt;  ❯ ps aux <span>|</span> grep clickhouse
</span></span><span><span>sspaeti  <span>1406478</span>  0.1  0.4 <span>477508</span> <span>136528</span> pts/28  Sl+  22:42   0:00 clickhouse-watchdog                                 server --config-file /tmp/rill-modeling/clickhouse-modeling-rill-example/tmp/default/clickhouse/default/config.xml
</span></span><span><span>sspaeti  <span>1406566</span> 54.8  3.2 <span>12216304</span> <span>899476</span> pts/28 Sl+ 22:42   1:12 /home/sspaeti/.rill/clickhouse/25.2.2.39/clickhouse server --config-file /tmp/rill-modeling/clickhouse-modeling-rill-example/tmp/default/clickhouse/default/config.xml
</span></span></code></pre></td></tr></tbody></table>
</div>
</div></div>
        </div>
    </div>
<p>ClickHouse provides multiple levels of optimization that can be applied independently or combined:</p>
<p><strong>Query-Level Optimizations:</strong></p>
<ol>
<li>Simple <strong>GROUP BY</strong> aggregations that process data from milliseconds to hours on the fly.</li>
<li>Data <strong>partitioning:</strong> Data is organized into directories based on partition keys for parallel processing.</li>
<li><strong>Filter and partition pushdown:</strong> ClickHouse’s optimizer pushes filters closer to the data source and skips irrelevant partitions, dramatically reducing I/O.</li>
</ol>
<p><strong>Storage-Level Pre-Aggregation Optimizations:</strong></p>
<p>This flexibility allows you to choose the right optimization strategy based on your specific use case, query patterns, and performance requirements.</p>
<div><p><i></i>Example of running ClickHouse locally with the StackOverflow dataset, 22 million rows<i></i></p><div><p>






<img loading="lazy" decoding="async" src="https://www.ssp.sh/blog/practical-data-modeling-clickhouse/clickhouse-locally_hu_5a8477ad2eb3584b.webp" alt="image" title="clickhouse-locally.webp"/></p></div></div>
<div><p><i></i>Alternative for more Powerful and managed Ingestion: ClickPipes<i></i></p><div><div><p><strong><a href="https://clickhouse.com/cloud/clickpipes" target="_blank" rel="noopener noreffer">ClickPipes</a></strong> is ClickHouse Cloud’s managed integration platform that makes ingesting data from diverse sources as simple as clicking a few buttons, providing a scalable, serverless ingestion experience with high throughput and low latency. Beyond object storage, ClickPipes supports Kafka/Confluent, database CDC from MySQL and Postgres, and streaming platforms like Kinesis and Event Hubs.</p>
<p>The platform includes fully managed operations with built-in error handling, automatic retries, schema evolution, and monitoring through dedicated error tables, plus enterprise features like API/Terraform integration and Prometheus metrics). For object storage specifically, ClickPipes supports <code>continuous ingestion</code> with configurable polling where new files must be lexically ordered (e.g., <code>file1</code>, <code>file2</code>, <code>file3</code>) for proper ingestion sequencing.</p></div></div></div>
<h2 id="applicable-tips--tricks">Applicable Tips &amp; Tricks</h2>
<p>In this chapter we look at practical strategies for data modeling with ClickHouse with practical tips and tricks for real-time analytics.</p>
<h3 id="deduplication-strategies">Deduplication Strategies</h3>
<p><strong>Why it matters</strong>: Real-time data streams often contain duplicate records due to network retries, system failures, or multiple data sources. Without deduplication, your analytics might show inflated metrics and incorrect insights.</p>
<p><strong>How to implement</strong>: ClickHouse offers several deduplication approaches:</p>
<ul>
<li><strong><a href="https://clickhouse.com/docs/engines/table-engines/mergetree-family/replacingmergetree" target="_blank" rel="noopener noreffer">ReplacingMergeTree</a></strong>: Automatically deduplicates rows based on the sorting key during background merges.</li>
<li><strong>Refreshable Materialized Views</strong>: Use <code>GROUP BY</code> with <code>argMax()</code> to keep the latest version of each record.</li>
<li><strong>Custom Deduplication Logic</strong>: Implement application-level deduplication before insertion.</li>
</ul>
<p><strong>Best Practice</strong>: For high-throughput real-time scenarios, use ReplacingMergeTree with a proper sorting key that includes your natural deduplication fields (e.g., <code>user_id</code>, <code>event_id</code>, <code>timestamp</code>).</p>
<h3 id="performance-optimization">Performance Optimization</h3>
<p>ClickHouse is all about performance and speed out of the gate. But here are some tips and practical examples to optimize even more.</p>
<h4 id="partitioning-strategy">Partitioning Strategy</h4>
<p><strong>Why it matters</strong>: Proper partitioning enables <a href="https://www.postgresql.org/docs/current/ddl-partitioning.html#DDL-PARTITIONING-OVERVIEW" target="_blank" rel="noopener noreffer">query pruning</a> and parallel processing, dramatically reducing query times from minutes to seconds.</p>
<p><strong>How to implement</strong>:</p>
<ul>
<li>Partition by time (daily/monthly) for time-series data.</li>
<li>Use secondary partitioning for high-cardinality dimensions. This means adding additional partition keys beyond just time to handle columns with many distinct values (<code>region</code> in the example below).</li>
<li>Design partitions to match your most common query patterns.</li>
</ul>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sql"><span><span><span>-- Advanced: Combine with AggregatingMergeTree for maximum efficiency
</span></span></span><span><span><span></span><span>PARTITION</span><span> </span><span>BY</span><span> </span><span>(</span><span>toYYYYMM</span><span>(</span><span>timestamp</span><span>),</span><span> </span><span>region</span><span>)</span><span>
</span></span></span><span><span><span></span><span>ORDER</span><span> </span><span>BY</span><span> </span><span>(</span><span>user_id</span><span>,</span><span> </span><span>timestamp</span><span>)</span><span>
</span></span></span><span><span><span></span><span>ENGINE</span><span> </span><span>=</span><span> </span><span>AggregatingMergeTree</span><span>()</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><h4 id="predicate-pushdown-optimization">Predicate Pushdown Optimization</h4>
<p><strong>Why it matters</strong>: Moving filters closer to the data source reduces the amount of data processed at each query stage.</p>
<p><strong>How to implement</strong>:</p>
<ul>
<li>Structure your <code>WHERE</code> clauses to match your sorting key order.</li>
<li>Use low-cardinality columns early in filtering.</li>
<li>Leverage ClickHouse’s automatic index usage for range queries with <a href="https://clickhouse.com/docs/primary-indexes" target="_blank" rel="noopener noreffer">sparse index</a>.</li>
<li><strong>Advanced tip</strong>: Combine with materialized views to push aggregations to insert time, not just filters to data source.</li>
</ul>
<h4 id="pre-aggregation-with-aggregatingmergetree">Pre-Aggregation with AggregatingMergeTree</h4>
<p><strong>When to use</strong>: High-volume time-series data where the same aggregation queries run frequently.</p>
<p><strong>Implementation</strong>: Use <code>-State</code> functions during INSERT and <code>-Merge</code> functions during SELECT to work with pre-computed aggregate states rather than raw data. <a href="https://clickhouse.com/docs/engines/table-engines/mergetree-family/aggregatingmergetree#select-and-insert" target="_blank" rel="noopener noreffer">More Information</a></p>
<h3 id="storage-efficiency">Storage Efficiency</h3>
<p>Data modeling has a real impact on cost when done correctly. Here are some strategies to reduce storage, therefore save cost, and speed up query responses by an order of magnitude.</p>
<h4 id="data-sketches-for-approximation">Data Sketches for Approximation</h4>
<p><strong>Why it matters</strong>: Exact distinct counts and percentiles on billions of rows are very expensive and time-consuming. <a href="https://datasketches.apache.org/" target="_blank" rel="noopener noreffer">Data sketches</a> use clever algorithms to deliver 99%+ accuracy for 1% of the cost and storage.</p>
<p><strong>How to implement</strong>:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sql"><span><span><span>-- Challenge: Count unique users from 1B+ events without storing all IDs - from: https://datasketches.apache.org/docs/Background/TheChallenge.html
</span></span></span><span><span><span></span><span>SELECT</span><span> 
</span></span></span><span><span><span>    </span><span>uniqHLL12</span><span>(</span><span>user_id</span><span>)</span><span> </span><span>as</span><span> </span><span>approx_unique_users</span><span>,</span><span>  </span><span>-- Uses ~1.5KB vs 8GB+
</span></span></span><span><span><span></span><span>    </span><span>quantile</span><span>(</span><span>0</span><span>.</span><span>95</span><span>)(</span><span>response_time_ms</span><span>)</span><span> </span><span>as</span><span> </span><span>p95_response_time</span><span>,</span><span>  </span><span>-- 95th percentile approximation
</span></span></span><span><span><span></span><span>    </span><span>countDistinct</span><span>(</span><span>session_id</span><span>)</span><span> </span><span>as</span><span> </span><span>approx_unique_sessions</span><span>  </span><span>-- Approximate distinct sessions
</span></span></span><span><span><span></span><span>FROM</span><span> </span><span>events</span><span> 
</span></span></span><span><span><span></span><span>WHERE</span><span> </span><span>date</span><span> </span><span>&gt;=</span><span> </span><span>&#39;2024-01-01&#39;</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><p><strong>Impact</strong>: The above example has an accuracy of 99%+ and a memory footprint of &lt;2KB with a speedboost of 100x by reducing storage.</p>
<h4 id="rollup-to-optimal-time-granularity">Rollup to Optimal Time Granularity</h4>
<p><strong>Why it matters</strong>: Storing every millisecond-level event creates significant storage overhead. Most business analytics work at hourly or daily granularity.</p>
<p><strong>How to implement</strong>:</p>
<ul>
<li>Aggregate raw events to hourly summaries using materialized views or SQL aggregations.</li>
<li>Keep detailed data for recent periods (last 30 days) and aggregated monthly data for historical analysis, for example.</li>
<li>Use different retention policies per granularity level.</li>
</ul>
<h3 id="sampling-strategies">Sampling Strategies</h3>
<p>Sampling is a statistical way to reduce data without compromising on getting the right insights.</p>
<h4 id="statistical-sampling-for-large-datasets">Statistical Sampling for Large Datasets</h4>
<p><strong>Why it matters</strong>: When dealing with billions of events, sometimes a representative sample provides sufficient accuracy for analytics while dramatically reducing processing time and storage costs.</p>
<p><strong>How to implement</strong>:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sql"><span><span><span>-- Random sampling: Take 1% of all events
</span></span></span><span><span><span></span><span>SELECT</span><span> </span><span>*</span><span> </span><span>FROM</span><span> </span><span>events</span><span> 
</span></span></span><span><span><span></span><span>WHERE</span><span> </span><span>cityHash64</span><span>(</span><span>user_id</span><span>)</span><span> </span><span>%</span><span> </span><span>100</span><span> </span><span>=</span><span> </span><span>0</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>-- Time-based sampling: Higher resolution for recent data
</span></span></span><span><span><span></span><span>SELECT</span><span> </span><span>*</span><span> </span><span>FROM</span><span> </span><span>events</span><span> 
</span></span></span><span><span><span></span><span>WHERE</span><span> 
</span></span></span><span><span><span>  </span><span>timestamp</span><span> </span><span>&gt;=</span><span> </span><span>now</span><span>()</span><span> </span><span>-</span><span> </span><span>INTERVAL</span><span> </span><span>7</span><span> </span><span>DAY</span><span>  </span><span>-- Keep all recent data
</span></span></span><span><span><span></span><span>  </span><span>OR</span><span> </span><span>cityHash64</span><span>(</span><span>event_id</span><span>)</span><span> </span><span>%</span><span> </span><span>100</span><span> </span><span>=</span><span> </span><span>0</span><span>    </span><span>-- Sample older data
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><p><strong>Best Practice</strong>: Use <a href="https://en.wikipedia.org/wiki/Stratified_sampling" target="_blank" rel="noopener noreffer">stratified sampling</a> when you need to maintain proportional representation across important business dimensions (customer segments, product categories, geographic regions). Use consistent hash functions to ensure reproducible samples.</p>
<p><strong>Impact</strong>: Can reduce data volumes by 90-99% while maintaining statistical significance for trend analysis and aggregate metrics.</p>
<h3 id="schema-management">Schema Management</h3>
<h4 id="table-projections-for-query-optimization">Table Projections for Query Optimization</h4>
<p><a href="https://clickhouse.com/docs/sql-reference/statements/alter/projection" target="_blank" rel="noopener noreffer">Table projections</a> are ClickHouse’s native feature for pre-computed, physically stored copies of your table data with different sort orders or pre-aggregations. Think “same table, multiple indexes on steroids”.</p>
<p><strong>Why it matters</strong>: Different queries need different sort orders or aggregations. Projections let you maintain multiple optimized access patterns without duplicating tables, and the query optimizer automatically picks the projection with the least data to scan.</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sql"><span><span><span>-- ClickHouse: Create projection optimized for user-based queries
</span></span></span><span><span><span></span><span>ALTER</span><span> </span><span>TABLE</span><span> </span><span>events_obt</span><span> </span><span>ADD</span><span> </span><span>PROJECTION</span><span> </span><span>user_timeline</span><span>
</span></span></span><span><span><span></span><span>(</span><span>SELECT</span><span> </span><span>user_id</span><span>,</span><span> </span><span>timestamp</span><span>,</span><span> </span><span>event_type</span><span> </span><span>ORDER</span><span> </span><span>BY</span><span> </span><span>user_id</span><span>,</span><span> </span><span>timestamp</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>-- ClickHouse: Pre-aggregated projection for analytics
</span></span></span><span><span><span></span><span>ALTER</span><span> </span><span>TABLE</span><span> </span><span>events_obt</span><span> </span><span>ADD</span><span> </span><span>PROJECTION</span><span> </span><span>daily_stats</span><span>  
</span></span></span><span><span><span></span><span>(</span><span>SELECT</span><span> </span><span>toDate</span><span>(</span><span>timestamp</span><span>)</span><span> </span><span>as</span><span> </span><span>date</span><span>,</span><span> </span><span>event_type</span><span>,</span><span> </span><span>count</span><span>()</span><span> 
</span></span></span><span><span><span> </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>date</span><span>,</span><span> </span><span>event_type</span><span> </span><span>ORDER</span><span> </span><span>BY</span><span> </span><span>date</span><span>);</span><span>
</span></span></span><span><span><span> 
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><div><p><i></i>dbt + ClickHouse Approach<i></i></p><div><p>Use dbt to create a denormalized One Big Table (OBT) in ClickHouse, then leverage ClickHouse projections for different query patterns instead of maintaining separate OLAP cubes.</p></div></div>
<h4 id="schema-evolution-best-practices">Schema Evolution Best Practices</h4>
<p><strong>Why it matters</strong>: Real-time systems need to handle schema changes without breaking existing queries or requiring full data reloads.</p>
<p><strong>How to implement</strong>:</p>
<ul>
<li>Use nullable columns for new fields to maintain backward compatibility.</li>
<li>Implement “latest state” modeling for slowly changing dimensions.</li>
<li>Leverage ClickHouse’s automatic schema detection for JSON fields.</li>
<li><strong>Snapshot approach</strong>: Daily/weekly full snapshots of dimensional data.</li>
</ul>
<h3 id="time-series-optimization">Time Series Optimization</h3>
<p>When working with time series, dates are an important part of how we query and store data.</p>
<h4 id="always-store-in-utc">Always Store in UTC</h4>
<p><strong>Why it matters</strong>: Mixed timezones in analytical data lead to incorrect aggregations and confusing results when data spans multiple regions.</p>
<p><strong>How to implement</strong>:</p>
<ul>
<li>Convert all timestamps to UTC at ingestion time.</li>
<li>Store the original timezone as a separate column if needed for display.</li>
<li>Use ClickHouse’s timezone functions for display conversion only.</li>
</ul>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sql"><span><span><span>-- Convert and store in UTC, keep original timezone for reference
</span></span></span><span><span><span></span><span>INSERT</span><span> </span><span>INTO</span><span> </span><span>events</span><span> 
</span></span></span><span><span><span></span><span>SELECT</span><span> 
</span></span></span><span><span><span>    </span><span>toDateTime</span><span>(</span><span>local_timestamp</span><span>,</span><span> </span><span>source_timezone</span><span>)</span><span> </span><span>as</span><span> </span><span>timestamp_utc</span><span>,</span><span> </span><span>-- Store in UTC (the key storage column)
</span></span></span><span><span><span></span><span>    </span><span>-- Display examples in different timezones
</span></span></span><span><span><span></span><span>    </span><span>toTimeZone</span><span>(</span><span>toDateTime</span><span>(</span><span>local_timestamp</span><span>,</span><span> </span><span>source_timezone</span><span>),</span><span> </span><span>&#39;Europe/London&#39;</span><span>)</span><span> </span><span>as</span><span> </span><span>london_time</span><span>,</span><span>
</span></span></span><span><span><span>    </span><span>toTimeZone</span><span>(</span><span>toDateTime</span><span>(</span><span>local_timestamp</span><span>,</span><span> </span><span>source_timezone</span><span>),</span><span> </span><span>source_timezone</span><span>)</span><span> </span><span>as</span><span> </span><span>original_local_time</span><span>,</span><span>
</span></span></span><span><span><span>    
</span></span></span><span><span><span>    </span><span>-- other fields
</span></span></span><span><span><span></span><span>FROM</span><span> </span><span>source_table</span><span>;</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><div><p><i></i>Some of the Limitations of ClickHouse<i></i></p><div><p>Besides all the strengths, some limitations can’t be neglected. For example, it’s more difficult to do updates and deletes (<a href="https://clickhouse.com/docs/sql-reference/statements/alter" target="_blank" rel="noopener noreffer">Mutations</a>). Joins are limited in performance and functionality and there’s no full ACID transactions support. There’s also no notion of foreign keys. This means referential integrity is left to the user to manage at an application level. Read more about this on <a href="https://www.chaosgenius.io/blog/clickhouse-architecture/" target="_blank" rel="noopener noreffer">ClickHouse Architecture 101</a> as well.</p></div></div>
<h2 id="choosing-the-right-clickhouse-modeling-strategy">Choosing the Right ClickHouse Modeling Strategy</h2>
<p>After exploring ClickHouse’s capabilities for real-time analytics, the key question becomes: How do you choose the right modeling approach for your specific use case? As always, the answer depends on your data volume, latency requirements, complexity needs, and team capabilities. But we can say that ClickHouse lets us handle powerful use cases without the need for expensive ETL pipelines or an additional semantic layer.</p>
<p>For straightforward real-time scenarios, ClickHouse’s native features shine. You can <a href="https://clickhouse.com/docs/guides/developer/deduplication" target="_blank" rel="noopener noreffer">deduplicate within ClickHouse</a> to land consistent data in your cube, and use the <a href="https://clickhouse.com/docs/sql-reference/statements/select/from#final-modifier" target="_blank" rel="noopener noreffer">FINAL modifier</a> to let ClickHouse fully merge data before returning results. This performs all data transformations that happen during merges for the given table engine, eliminating the complexity of external processing.</p>
<p><strong>The ETL Pipeline Approach</strong></p>
<p>Modeling outside of ClickHouse is a common approach with more complex landscapes, but if we want real-time analytics, batch ETL can break the flow of continuously updated streams. That’s why this shouldn’t be the first choice if you want real-time data, quickly updated.</p>
<p><strong>The BI Approach</strong>  There’s also a tradeoff with storing metrics within the OLAP cube versus outside of it. Because SQL aggregations and measures can be queried on the fly but can’t be stored within ClickHouse easily, data modeling often happens outside ClickHouse or gets stored within BI tools. The <strong>advantage</strong> is you can change metrics at any time without running an ETL pipeline. The <strong>downside</strong> is you can’t easily store or manage them except in your UI, whether it’s a web app with an <a href="https://clickhouse.com/blog/moosestack-does-olap-need-an-orm" target="_blank" rel="noopener noreffer">OLAP-ORM</a>, notebooks, or Business Intelligence tools.</p>
<p>This is one reason why Rill <a href="https://docs.rilldata.com/guides/rill-clickhouse/" target="_blank" rel="noopener noreffer">pairs so well</a> with ClickHouse—it has a full-blown metrics layer built-in with all its capabilities out of the box. You can store metrics declaratively based on YAML, version control them, and update them in a governed way. For example, put them in a git repository and let users collaborate on these metrics, which then get blazingly fast query returns on ClickHouse. Rill gives you <strong>another layer of data modeling</strong> while using ClickHouse as a sub-second response query engine.</p>
<p>Ultimately, the choice between native ClickHouse modeling, external ETL pipelines, or BI tool integration comes down to balancing three key factors: data freshness requirements, transformation complexity, and team capabilities. ClickHouse’s native approach eliminates traditional ETL overhead for most real-time use cases, but the flexibility to layer additional tools when needed ensures your analytics architecture can evolve with your business requirements.</p>
<hr/>
<p>To get started, check out the <a href="https://github.com/sspaeti/clickhouse-modeling-rill-example" target="_blank" rel="noopener noreffer">practical example</a> that demonstrates ClickHouse ETL with NOAA weather data, or explore ClickHouse’s comprehensive <a href="https://clickhouse.com/docs/data-modeling/schema-design" target="_blank" rel="noopener noreffer">Schema Design</a> documentation, which guides you through all the steps including querying large datasets like StackOverflow’s 60+ million records locally within seconds.</p>
<hr/>
<!-- <pre class=""><em>Full article published at <a href="https://rilldata.com/...." target="_blank" rel="noopener noreferrer">MotherDuck.com</a> - written as part of <a href="/services">my services</a></em></pre> -->
<pre><em>Written as part of <a href="http://tinylogger.com/services">my services</a></em></pre>
</div><div>


<div>
<p><a href="https://bsky.app/search?q=domain%3A%20https://www.ssp.sh/blog/practical-data-modeling-clickhouse/" target="_blank" rel="noopener noreferrer">Discuss on Bluesky</a>
  |  </p><form method="post" action="https://list.ssp.sh/subscription/form">
    <div>
        <div>
            
            
        </div>
        </div>
</form>















</div>



<div><p><a href="http://tinylogger.com/tags/clickhouse/">Clickhouse</a><a href="http://tinylogger.com/tags/real-time-analytics/">Real-Time Analytics</a><a href="http://tinylogger.com/tags/data-modeling/">Data Modeling</a><a href="http://tinylogger.com/tags/olap/">Olap</a><a href="http://tinylogger.com/tags/s3/">S3</a><a href="http://tinylogger.com/tags/rill/">Rill</a><a href="http://tinylogger.com/tags/etl/">Etl</a><a href="http://tinylogger.com/tags/partitioning/">Partitioning</a><a href="http://tinylogger.com/tags/aggregation/">Aggregation</a><a href="http://tinylogger.com/tags/dashboard/">Dashboard</a><a href="http://tinylogger.com/tags/services/">Services</a></p></div>


</div></div>
  </body>
</html>
