<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.together.xyz/blog/redpajama-models-v1">Original</a>
    <h1>RedPajama (Open LLaMA Replication) 3B and 7B models released</h1>
    
    <div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="6358d155bd7bbd5929470f3f">
  
  
    
    


  


<section data-test="page-section" data-section-theme="white" data-section-id="6358d155bd7bbd5929470f41" data-controller="SectionWrapperController" data-current-styles="{
                                          &#34;imageOverlayOpacity&#34;: 0.15,
                                          &#34;backgroundWidth&#34;: &#34;background-width--full-bleed&#34;,
                                          &#34;sectionHeight&#34;: &#34;section-height--medium&#34;,
                                          &#34;horizontalAlignment&#34;: &#34;horizontal-alignment--center&#34;,
                                          &#34;verticalAlignment&#34;: &#34;vertical-alignment--middle&#34;,
                                          &#34;contentWidth&#34;: &#34;content-width--wide&#34;,
                                          &#34;sectionTheme&#34;: &#34;white&#34;,
                                          &#34;sectionAnimation&#34;: &#34;none&#34;,
                                          &#34;backgroundMode&#34;: &#34;image&#34;
                                        }" data-current-context="{
                                          &#34;video&#34;: {
                                            &#34;playbackSpeed&#34;: 0.5,
                                            &#34;filter&#34;: 1,
                                            &#34;filterStrength&#34;: 0,
                                            &#34;zoom&#34;: 0,
                                            &#34;videoSourceProvider&#34;: &#34;none&#34;
                                          },
                                          &#34;backgroundImageId&#34;: null,
                                          &#34;backgroundMediaEffect&#34;: null,
                                          &#34;divider&#34;: null,
                                          &#34;typeName&#34;: &#34;blog-basic-grid&#34;
                                        }" data-animation="none">
  
  <div>
    <div>
      
      
      
      
      
      
      <div data-content-field="main-content" data-item-id="">
  <article id="article-">
  
    <div>
      

      <div>
        <div><div data-layout-label="Post Body" data-type="item" id="item-6455627d17e5406f388d0110"><div><div><div data-block-type="5" id="block-81f5b6443d4247ef8a52"><div>






























  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/f8b07fd1-d7b8-4729-94b8-364dcc47890a/RedPajama.png" data-image="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/f8b07fd1-d7b8-4729-94b8-364dcc47890a/RedPajama.png" data-image-dimensions="2048x2048" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="6455627d17e5406f388d010a" data-type="image"/>
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-ab7fe394fb2944960fc1"><div>

<h4><strong>The RedPajama project aims to create a set of leading open-source models and to rigorously understand the ingredients that yield good performance.  A few weeks ago we released the RedPajama base dataset based on the LLaMA paper, which has galvanized the open-source community. The 5 terabyte dataset has been downloaded hundreds of times and used to train models like MPT, OpenLLaMA, OpenAlpaca. Today we are excited to release RedPajama-INCITE models, including instruct-tuned and chat versions. </strong></h4><p>Today’s release includes our first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. In addition we are releasing fully open-source instruction-tuned and chat models. Our key takeaways:</p><ul data-rte-list="default"><li><p>The 3B model is the strongest in it’s class, and the small size makes it extremely fast and accessible (it even runs on a RTX 2070 released over 5 years ago).</p></li><li><p>The instruction-tuned versions of the models achieve strong performance on HELM benchmarks. As expected, on HELM the 7B model performance is higher than the base LLaMA model by 3 points. We recommend using these models for downstream applications with few-shot, entity extraction, classification, or summarization tasks. </p></li><li><p>The 7B model (which is 80% done training) is already outperforming the Pythia 7B model, which is showing the importance of a bigger dataset and the value of the RedPajama base dataset.</p></li><li><p>Based on our observations, we see a clear path for creating a better version of the RedPajama dataset, which we will release in the coming weeks, that will go beyond the quality of LLaMA 7B. We plan to build models at larger scale with this new dataset. </p></li><li><p>We expect differences between the LLaMA 7B and our replication, which we have investigated below.</p></li></ul><p>The biggest takeaway is the demonstration that performant LLMs can be built quickly by the open-source community. This work builds on top of our 1.2 trillion token RedPajama dataset, EleutherAI’s Pythia training code, FlashAttention from Stanford and Together, the HELM benchmarks from Stanford CRFM and generous support from <a href="https://mila.quebec/en/"><span>MILA</span></a>, <a href="https://www.eleuther.ai/"><span>EleutherAI</span></a> &amp; <a href="https://laion.ai/"><span>LAION</span></a> for compute time on the Summit supercomputer within the <a href="https://www.alcf.anl.gov/science/incite-allocation-program"><span>INCITE program</span></a> award &#34;Scalable Foundation Models for Transferable Generalist AI”. We believe these kind of open collaborations, at larger scales, will be behind the best AI systems of the future. </p>



</div></div><div data-block-type="31" id="block-967c43aac2798e95350c"><div>



<figure>
  <blockquote data-animation-role="quote">
    <span>“</span>RedPajama 3B model is the strongest model in it’s class and brings a performant large language model to a wide variety of hardware.<span>”</span>
  </blockquote>
  
</figure>
</div></div><div data-block-type="2" id="block-c4729419f5517d1d2d75"><p>Today’s release includes the following models, all released under the permissive Apache 2.0 license allowing for use both in research and commercial applications.</p></div><div data-block-type="2" id="block-529629b448d4fe4f8856"><p>In only a few weeks the support, suggestions, and feedback for RedPajama from the open-source community has been incredible. Based on our learnings, we are also already starting the next version of the RedPajama base dataset which will be nearly twice the size of the original v1 dataset. Thank you for your support, feedback and suggestions! </p></div><div data-block-type="5" id="block-yui_3_17_2_1_1683317373953_40473"><div>






























  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>3B model has stabilized at 800 billion tokens and the 7B model continues to improve as it completes training to 1 trillion tokens</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683317373953_40823"><div>
  


<p>During RedPajama model training we have shared regular updates, and both the 3B and 7B models have now been trained on 800 billion tokens. We are excited to see that the 3B model has stabilized at 800 billion tokens and the 7B model continues to improve as it completes training to 1 trillion tokens.  </p><h3><strong>3B RedPajama Models</strong></h3><p><strong>RedPajama-INCITE-Base-3B-v1</strong> is trained over the RedPajama v1 dataset, with the same architecture as the popular <a href="https://github.com/EleutherAI/pythia"><span>Pythia</span></a> model suite. We chose to start with the Pythia architecture to understand the value of training with the much larger RedPajama dataset with respect to the current leading open-source dataset, <a href="https://pile.eleuther.ai/"><span>the Pile</span></a>. Training on Summit leveraged the <a href="https://github.com/EleutherAI/DeeperSpeed"><span>DeeperSpeed</span></a> codebase developed by <a href="https://www.eleuther.ai/"><span>EleutherAI</span></a>.</p><p>We are excited to see that at 800B tokens, RedPajama-Base-INCITE-3B has better few-shot performance (measured in <a href="https://crfm.stanford.edu/helm/latest/"><span>HELM</span></a>, as the average score over 16 core scenarios) and better zero-shot performance (measured in <a href="https://github.com/EleutherAI/lm-evaluation-harness"><span>Eleuther’s LM evaluation harness</span></a>) compared with open models of similar size, including the well-regarded GPT-Neo and Pythia-2.8B (trained with 420B and 300B tokens, respectively, with the Pile). On HELM, it outperforms these models by 3-5 points. On a subset of tasks from lm-evaluation-harness, outperforms these open models by 2-7 points. </p><p>Additionally, we are excited to release an instruction-tuned version of this 3B model, RedPajama-INCITE-Instruct-3B-v1, trained following Together’s <a href="https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai"><span>GPT-JT</span></a> recipe and removing any data in HELM benchmarks to ensure that there is no contamination with respect to HELM. This model shows excellent performance on few-shot tasks, even approaching the quality of LLaMA 7B in a much smaller model, as shown in the results below:</p><p><strong>Few Shot Results on HELM Core Scenarios </strong></p>



</div></div><div data-block-type="23" id="block-yui_3_17_2_1_1683317373953_52772"><div>
<table>
<thead>
  <tr>
    <th><span>Models</span></th>
    <th><span>Type</span></th>
    <th><span>HELM (Average score over 16 core scenarios)</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><span>GPT-Neo</span></td>
    <td><span>Base model</span></td>
    <td><span>0.357</span></td>
  </tr>
  <tr>
    <td><span>Pythia-2.8B</span></td>
    <td><span>Base model</span></td>
    <td><span>0.377</span></td>
  </tr>
  <tr>
    <td><span>RedPajama-INCITE-Base-3B-v1</span></td>
    <td><span>Base model</span></td>
    <td><span>0.406</span></td>
  </tr>
  <tr>
    <td><span>RedPajama-INCITE-Instruct-3B-v1</span></td>
    <td><span>Instruction-tuned</span></td>
    <td><span>0.453</span></td>
  </tr>
  <tr>
    <td><span>Llama-7B</span></td>
    <td><span>Base model</span></td>
    <td><span>0.465</span></td>
  </tr>
</tbody>
</table></div></div><div data-block-type="23" id="block-yui_3_17_2_1_1683317373953_56750"><div>
<table>
<thead>
  <tr>
    <th></th>
    <th><span>Lambada_openai</span></th>
    <th><span>Hellaswag</span></th>
    <th><span>Winogrande</span></th>
    <th><span>Piqa(acc)</span></th>
    <th><span>average</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><span>GPT-Neo</span></td>
    <td><span>0.6223</span></td>
    <td><span>0.5579</span></td>
    <td><span>0.5769</span></td>
    <td><span>0.7219</span></td>
    <td><span>0.6197</span></td>
  </tr>
  <tr>
    <td><span>Pythia-2.8B</span></td>
    <td><span>0.6466</span></td>
    <td><span>0.5933</span></td>
    <td><span>0.6006</span></td>
    <td><span>0.7399</span></td>
    <td><span>0.6451</span></td>
  </tr>
  <tr>
    <td><span>Pythia-2.8B-dedup</span></td>
    <td><span>0.6524</span></td>
    <td><span>0.5941</span></td>
    <td><span>0.5848</span></td>
    <td><span>0.7404</span></td>
    <td><span>0.6429</span></td>
  </tr>
  <tr>
    <td><span>RedPajama-INCITE-Base-3B-v1</span></td>
    <td><span>0.6541</span></td>
    <td><span>0.6317</span></td>
    <td><span>0.6322</span></td>
    <td><span>0.7470</span></td>
    <td><span>0.6662</span></td>
  </tr>
</tbody>
</table></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683317373953_56817"><p><strong>Results on a subset of lm-evaluation-harness, tasks selected from what used to evaluate </strong><a href="https://github.com/EleutherAI/pythia/tree/main/results"><span><strong>Pythia</strong></span></a><strong> and </strong><a href="https://huggingface.co/EleutherAI/gpt-j-6b"><span><strong>GPT-J.</strong></span></a></p></div><div data-block-type="5" id="block-yui_3_17_2_1_1683317373953_60180"><div>






























  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p>RedPajama 3B results on a subset of lm-evaluation-harness</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683317373953_60530"><div>
  


<p><strong>RedPajama-INCITE-Chat-3B-v1</strong> is an open-source chat model constructed with RedPajama-INCITE-Base-3B-v1 and fine-tuned over the <a href="https://open-assistant.io/"><span>OASST1 dataset by Open Assistant</span></a> and <a href="https://huggingface.co/databricks/dolly-v2-12b"><span>Dolly v2.0 dataset by DataBricks</span></a>. We equally mix the datasets, and fine-tune for 3 epochs.</p><p>Evaluating chat models is a challenging task, and we are in the process of conducting more quantitative evaluation based on human and community feedback, and are excited to share these results soon! Nevertheless, here are some examples comparing the behavior of different chat models. We see that in many examples, RedPajama-INCITE-Chat-3B-v1 has similar quality as Open Assistant as reported in the <a href="https://arxiv.org/abs/2304.07327"><span>their paper</span></a>.</p><p><strong>RedPajama 3B chat model responses on example queries from the </strong><a href="https://arxiv.org/abs/2304.07327"><span><strong>Open Assistant</strong></span></a><strong> paper.  </strong></p>



</div></div><div data-block-type="23" id="block-yui_3_17_2_1_1683324812533_26476"><div>
<table>
<thead>
  <tr>
    <th><span>RedPajama-INCITE-Chat-3B-v1 (oasst1 + Dolly 2.0)</span></th>
    <th><span>Pythia-2.8B + oasst1 + Dolly 2.0</span></th>
    <th><span>Open Assistant (“pythia-12b-deduped fine-tuned on the OpenAssistant and Alpaca data” [Appendix E of https://arxiv.org/abs/2304.07327])</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><span>Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself.</span></td>
    <td><span>Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself.</span></td>
    <td><span>Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself.</span></td>
  </tr>
  <tr>
    <td><span>Why is shakespeare such a renowned writer that is still studied so heavily today?</span></td>
    <td><span>Why is shakespeare such a renowned writer that is still studied so heavily today?</span></td>
    <td><span>Why is shakespeare such a renowned writer that is still studied so heavily today?</span></td>
  </tr>
  <tr>
    <td><span>What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel?</span></td>
    <td><span>What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel?</span></td>
    <td><span>What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel?</span></td>
  </tr>
</tbody>
</table></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683324812533_26542"><p>And, following are some additional examples comparing RedPajama 3B to the Pythia 2.8B model tuned on OASST1 and Dolly v2.0 datasets. </p></div><div data-block-type="23" id="block-yui_3_17_2_1_1683324812533_32784"><div>
<table>
<thead>
  <tr>
    <th><span>RedPajama-INCITE-Chat-3B-v1 (oasst1 + Dolly 2.0)</span></th>
    <th><span>Pythia-2.8B + oasst1 + Dolly 2.0</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><span>Write an email to my friends inviting them to come to my home on Friday for a dinner party, bring their own food to share.</span></td>
    <td><span>Write an email to my friends inviting them to come to my home on Friday for a dinner party, bring their own food to share.</span></td>
  </tr>
  <tr>
    <td><span>Create a list of things to do in San Francisco</span></td>
    <td><span>Create a list of things to do in San Francisco</span></td>
  </tr>
</tbody>
</table></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683324812533_32851"><div>
  


<h3><strong>Preview of RedPajama 7B </strong></h3><p>The 7B model is still training (at 800B tokens) and we see the training loss still decrease consistently. As a result, we will continue to train it to 1T tokens. Nevertheless, this checkpoint is quite useful, and interesting to build on, and can help the community better understand our training process. Therefore, we are releasing three intermediate checkpoints as a “preview” of the final models.</p><ul data-rte-list="default"><li><p>RedPajama-INCITE-Base-7B-v0.1 is a base model trained over 800B tokens </p></li><li><p>RedPajama-INCITE-Chat-7B-v0.1 is its chat counterpart trained over Dolly 2.0 and Open Assistant</p></li><li><p>RedPajama-INCITE-Instruct-7B-v0.1 is instruction tuned for few-shot applications. We follow the recipe for <a href="https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai"><span>GPT-JT</span></a> but eliminate all datasets that overlap with the HELM benchmark. </p></li></ul><p>Each of these checkpoints are released under the Apache 2.0 license. Even at 800B tokens, we already see promising results. On HELM, the base model outperforms open models such as GPT-J and Pythia-6.9B by 0.5-2.2 points, and on EleutherAI’s lm-evaluation-harness, it outperforms these models by 1-3 points on average.</p><p>We also see that, compared with LLaMA 7B, there is still a quality gap – 4.3 points on HELM at this moment. For few-shot applications (like those in HELM), the instruction-tuned model (RedPajama-INCITE-Instruct-7B-v0.1) improved over the base model significantly. We hope that some of this gap can be closed after we train for more iterations. </p><p><strong>(Few Shot) Results on HELM Core Scenarios </strong></p>



</div></div><div data-block-type="23" id="block-yui_3_17_2_1_1683317373953_93436"><div>
<table>
<thead>
  <tr>
    <th><span>Model</span></th>
    <th><span>Type</span></th>
    <th><span>HELM (Average score over 16 core scenarios)</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><span>GPT-J</span></td>
    <td><span>Base model </span></td>
    <td><span>0.417</span></td>
  </tr>
  <tr>
    <td><span>Pythia-6.9B</span></td>
    <td><span>Base model </span></td>
    <td><span>0.400</span></td>
  </tr>
  <tr>
    <td><span>Llama-7B</span></td>
    <td><span>Base model </span></td>
    <td><span>0.465</span></td>
  </tr>
  <tr>
    <td><span>RedPajama-INCITE-Base-7B-v0.1</span></td>
    <td><span>Base model </span></td>
    <td><span>0.422</span></td>
  </tr>
  <tr>
    <td><span>RedPajama-INCITE-Instruct-7B-v0.1</span></td>
    <td><span>Instruction-tuned </span></td>
    <td><span>0.499</span></td>
  </tr>
</tbody>
</table></div></div><div data-block-type="23" id="block-yui_3_17_2_1_1683317373953_98067"><div>
<table>
<thead>
  <tr>
    <th></th>
    <th><span>Lambada_openai</span></th>
    <th><span>Hellaswag</span></th>
    <th><span>Winogrande</span></th>
    <th><span>Piqa (acc)</span></th>
    <th><span>average</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><span>GPT-J</span></td>
    <td><span>0.6699</span></td>
    <td><span>0.6663</span></td>
    <td><span>0.6503</span></td>
    <td><span>0.7565</span></td>
    <td><span>0.6857</span></td>
  </tr>
  <tr>
    <td><span>Pythia-6.9B</span></td>
    <td><span>0.6712</span></td>
    <td><span>0.6389</span></td>
    <td><span>0.6069</span></td>
    <td><span>0.7519</span></td>
    <td><span>0.6672</span></td>
  </tr>
  <tr>
    <td><span>Pythia-6.9B-dedup</span></td>
    <td><span>0.6893</span></td>
    <td><span>0.6588</span></td>
    <td><span>0.6266</span></td>
    <td><span>0.7578</span></td>
    <td><span>0.6831</span></td>
  </tr>
  <tr>
    <td><span>Llama-7B</span></td>
    <td><span>0.7360*</span></td>
    <td><span>0.7620*</span></td>
    <td><span>0.7040</span></td>
    <td><span>0.7810</span></td>
    <td><span>0.7457</span></td>
  </tr>
  <tr>
    <td><span>RedPajama-INCITE-Base-7B-v0.1</span></td>
    <td><span>0.7061</span></td>
    <td><span>0.6951</span></td>
    <td><span>0.6519</span></td>
    <td><span>0.7611</span></td>
    <td><span>0.7035</span></td>
  </tr>
</tbody>
</table></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683317373953_98135"><p><strong>Results on a subset of </strong><span><strong><em>llm-evaluation-harness</em></strong></span><strong>, tasks selected from what used to evaluate Pythia and GPT-J.</strong></p></div><div data-block-type="5" id="block-yui_3_17_2_1_1683317373953_105390"><div>






























  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/db283279-9d10-4849-9d21-31c5cc32c3e2/image1.png" data-image="https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/db283279-9d10-4849-9d21-31c5cc32c3e2/image1.png" data-image-dimensions="1999x1033" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="64556b2711c75b7c55834924" data-type="image"/>
                
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p>Results on a subset of llm-evaluation-harness</p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683319955631_80261"><div>
  


<h3><strong>Moving Forward: RedPajama v2 with 2T Tokens</strong></h3><p>We learned a lot from the community and are working on building RedPajama v2 with 2 trillion tokens, by taking a systematic approach:</p><ul data-rte-list="default"><li><p>We measured the validation loss of different models on different slices of the Pile (for each slice, we selected the first 5K passages). We see that RedPajama lags behind on many slices of the Pile, especially for those slices that are not directly included in the RedPajama dataset. Inspired by this, we plan to mix the Pile dataset into RedPajama and form a more diverse dataset with even more tokens. </p></li><li><p>And we need more code! Another immediate to-do on our plate is to mix in data from <a href="https://www.bigcode-project.org/docs/about/the-stack/"><span>the Stack</span></a> and enrich the Github slice of RedPajama, which contains only 59 billion tokens. </p></li></ul><p>With all these improvements together, we are shooting for a 2T token RedPajama v2 dataset. Next week we will start doing a series of runs to understand the right data mixture and start training new models over RedPajama v2.</p>



</div></div><div data-block-type="23" id="block-yui_3_17_2_1_1683317373953_109800"><div>
<table>
<thead>
  <tr>
    <th></th>
    <th><span>Llama-7B</span></th>
    <th><span>GPT-J </span></th>
    <th><span>RedPajama-Base-INCITE-6.9B-v0.1</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><span>ArXiv</span></td>
    <td><span>1.727</span></td>
    <td><span>1.511</span></td>
    <td><span>1.990</span></td>
  </tr>
  <tr>
    <td><span>BookCorpus2</span></td>
    <td><span>1.904</span></td>
    <td><span>2.226</span></td>
    <td><span>2.213</span></td>
  </tr>
  <tr>
    <td><span>Books3</span></td>
    <td><span>1.664</span></td>
    <td><span>1.979</span></td>
    <td><span>1.909</span></td>
  </tr>
  <tr>
    <td><span>DM Mathematics</span></td>
    <td><span>1.411</span></td>
    <td><span>1.158</span></td>
    <td><span>1.910</span></td>
  </tr>
  <tr>
    <td><span>Enron Emails</span></td>
    <td><span>2.494</span></td>
    <td><span>1.844</span></td>
    <td><span>2.962</span></td>
  </tr>
  <tr>
    <td><span>EuroParl</span></td>
    <td><span>1.964</span></td>
    <td><span>1.216</span></td>
    <td><span>2.066</span></td>
  </tr>
  <tr>
    <td><span>FreeLaw</span></td>
    <td><span>1.425</span></td>
    <td><span>1.121</span></td>
    <td><span>1.889</span></td>
  </tr>
  <tr>
    <td><span>Github</span></td>
    <td><span>1.126</span></td>
    <td><span>0.756</span></td>
    <td><span>1.273</span></td>
  </tr>
  <tr>
    <td><span>Gutenberg (PG-19)</span></td>
    <td><span>1.837</span></td>
    <td><span>1.718</span></td>
    <td><span>2.079</span></td>
  </tr>
  <tr>
    <td><span>HackerNews</span></td>
    <td><span>2.423</span></td>
    <td><span>2.311</span></td>
    <td><span>2.821</span></td>
  </tr>
  <tr>
    <td><span>NIH ExPorter</span></td>
    <td><span>1.864</span></td>
    <td><span>2.135</span></td>
    <td><span>2.413</span></td>
  </tr>
  <tr>
    <td><span>OpenSubtitles</span></td>
    <td><span>2.184</span></td>
    <td><span>2.136</span></td>
    <td><span>2.510</span></td>
  </tr>
  <tr>
    <td><span>OpenWebText2</span></td>
    <td><span>2.027</span></td>
    <td><span>2.264</span></td>
    <td><span>2.321</span></td>
  </tr>
  <tr>
    <td><span>PhilPapers</span></td>
    <td><span>1.947</span></td>
    <td><span>2.225</span></td>
    <td><span>2.280</span></td>
  </tr>
  <tr>
    <td><span>Pile-CC</span></td>
    <td><span>2.095</span></td>
    <td><span>2.441</span></td>
    <td><span>2.430</span></td>
  </tr>
  <tr>
    <td><span>PubMed Abstracts</span></td>
    <td><span>1.694</span></td>
    <td><span>1.937</span></td>
    <td><span>2.220</span></td>
  </tr>
  <tr>
    <td><span>PubMed Central</span></td>
    <td><span>1.697</span></td>
    <td><span>1.494</span></td>
    <td><span>2.122</span></td>
  </tr>
  <tr>
    <td><span>StackExchange</span></td>
    <td><span>1.776</span></td>
    <td><span>1.588</span></td>
    <td><span>2.078</span></td>
  </tr>
  <tr>
    <td><span>USPTO Backgrounds</span></td>
    <td><span>1.740</span></td>
    <td><span>1.841</span></td>
    <td><span>2.142</span></td>
  </tr>
  <tr>
    <td><span>Ubuntu IRC</span></td>
    <td><span>2.094</span></td>
    <td><span>1.704</span></td>
    <td><span>2.518</span></td>
  </tr>
  <tr>
    <td><span>Wikipedia (en)</span></td>
    <td><span>1.597</span></td>
    <td><span>1.629</span></td>
    <td><span>1.758</span></td>
  </tr>
  <tr>
    <td><span>YoutubeSubtitles</span></td>
    <td><span>1.943</span></td>
    <td><span>1.955</span></td>
    <td><span>2.226</span></td>
  </tr>
</tbody>
</table></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1683317373953_109873"><div>
  


<h3>Acknowledgements</h3><p>The training of the first collection of RedPajama-INCITE models is performed on 3,072 V100 GPUs provided as part of the <a href="https://www.alcf.anl.gov/science/incite-allocation-program"><span>INCITE</span></a> compute grant on <a href="https://en.wikipedia.org/wiki/Summit_(supercomputer)"><span>Summit</span></a> supercomputer at the <a href="https://www.olcf.ornl.gov/"><span>Oak Ridge Leadership Computing Facility (OLCF)</span></a>. This grant was awarded to <a href="https://www.irina-lab.ai/"><span>AAI CERC lab</span></a> at <a href="https://www.umontreal.ca/"><span>Université de Montréal</span></a>/<a href="https://mila.quebec/en/"><span>Mila</span></a>,  <a href="https://laion.ai/"><span>LAION</span></a> and<a href="https://www.eleuther.ai/"> <span>EleutherAI</span></a> in<a href="https://tinyurl.com/36xk3m6r"> <span>fall 2022</span></a> for their collaborative project on<a href="https://sites.google.com/view/irinarish/incite-project"> <span>Scalable Foundation Models for Transferrable Generalist AI</span></a>. </p><p>We are also appreciative to the work done by the growing open-source AI community that made this project possible. That includes:</p><ul data-rte-list="default"><li><p><a href="https://www.facebook.com/MetaAI/"><span>Meta AI</span></a> — Their inspiring work on LLaMA shows a concrete path towards building strong language models, and it is the original source for our dataset replication.</p></li><li><p><a href="https://www.eleuther.ai/"><span>EleutherAI</span></a> — This project is built on the backs of the great team at EleutherAI — including the source code they provided for training GPT-NeoX.</p></li><li><p><a href="https://sites.google.com/view/irinarish/incite-project"><span>INCITE project team</span></a> — Their work on GPT-NeoX adaptation to Summit during early 2023 enabled distributed training that scaled efficiently to thousands of Summit GPUs, and ensured smooth training of the models. </p></li><li><p>This research used resources of the <a href="https://www.olcf.ornl.gov/"><span>Oak Ridge Leadership Computing Facility (OLCF)</span></a>, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725. We are  grateful for the invaluable support provided to us by the OLCF leadership and by the OLCF liaison for the INCITE project.</p></li></ul>



</div></div><div data-block-type="2" id="block-12254f55d583ccad712f"><p>Get notified of future posts and updates:</p></div></div></div></div></div>

        

        
        
          
        
      </div>

      
    </div>
  
</article>

</div>
    </div>
  </div>
  
  
</section>

  
</article>


          

          
            
              

            
          
        
      </div></div>
  </body>
</html>
