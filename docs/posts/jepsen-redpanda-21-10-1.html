<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://jepsen.io/analyses/redpanda-21.10.1">Original</a>
    <h1>Jepsen: Redpanda 21.10.1</h1>
    
    <div id="readability-page-1" class="page"><p><a href="https://redpanda.com/redpanda">Redpanda</a> is a distributed streaming system compatible with the <a href="https://kafka.apache.org/">Kafka</a> wire protocol. We tested Redpanda versions 21.10.1 through 21.11.2, as well as development builds through January 30, 2022. We found three liveness and seven safety issues, ranging from crashes and aborted reads to inconsistent offsets, circular information flow, and lost/stale messages. We also discuss some potentially surprising behaviors, including an ambiguously named error and confusing documentation around write isolation in Kafka’s transaction model. Redpanda has resolved seven of these issues, though some fixes aren’t yet released. One crash and an issue involving lost/stale messages remain under investigation, and three more issues require only documentation. This work was funded by <a href="https://redpanda.com/?utm_source=report&amp;utm_medium=content&amp;utm_campaign=2022_jepsenreport&amp;utm_assettype=website&amp;utm_assetname=homepage">Redpanda Data</a>, and conducted in accordance with the <a href="https://jepsen.io/ethics.html">Jepsen ethics policy</a>.</p><article>
  <div>

<p><a href="https://redpanda.com/?utm_source=report&amp;utm_medium=content&amp;utm_campaign=2022_jepsenreport&amp;utm_assettype=website&amp;utm_assetname=homepage">Redpanda</a> is a <a href="https://kafka.apache.org/">Kafka</a>-compatible distributed streaming system based around append-only logs. Compared to Kafka, Redpanda aims to offer users lower latencies and reduced operational complexity. It uses the <a href="https://raft.github.io/">Raft consensus algorithm</a> internally, rather than depending on a separate installation of <a href="https://zookeeper.apache.org/">Zookeeper</a>. Redpanda speaks Kafka’s wire protocol: rather than ship their own client libraries, Redpanda uses regular Kafka clients.</p>
<p><a href="https://kafka.apache.org/intro">Like Kafka</a>, Redpanda provides a set of named, partially ordered logs called <em>topics</em>. Each topic is sharded into one or more <em>partitions</em>,<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> each of which is a totally ordered log of messages. A message’s position within a partition is identified by a unique monotonically increasing integer <em>offset</em>, which provides that total order. Offsets may be sparse: some offsets in the log are for internal messages, like transaction metadata, which are invisible to clients.</p>
<p>Kafka clients are split into several parts. Users write (<em>produce</em>) messages to partitions using a <em>producer</em> client, and read (<em>poll</em>) messages from partitions using a <em>consumer</em> client. Users may produce and consume to and from manually specified partitions, or allow the system to automatically select which partition a message is written to, and which partition(s) a consumer reads from.</p>
<p>Polling messages does not delete them. Instead, consumers emulate “consuming from a queue” by reading successive offsets from a given partition. Consumers can either <em>assign</em> themselves specific partitions and manage offsets themselves, or <em>subscribe</em> to a topic and allow Redpanda to automatically manage partitions and offsets. Offsets can be <em>committed</em> to Redpanda, which stores them durably so that crashed consumers can pick up where they (or their forebears) left off.</p>
<h2 data-number="1.1" id="safety"> Safety</h2>
<p>As of December 14, 2021, Redpanda’s home page <a href="https://web.archive.org/web/20211214170816/https://vectorized.io/redpanda/">compared</a> Kafka to Redpanda. Where Redpanda offered “zero data loss by default”, Kafka was marked “Caveat: reduces performance”. Redpanda engineers explained that Kafka’s default configuration may acknowledge writes before <code>fsync</code>. This might allow Kafka to lose messages when nodes fail. Redpanda nodes, by contrast, only acknowledges writes once they are <code>fsync</code>ed on a majority of replicas. Redpanda also advertised its use of the Raft consensus algorithm for safety. Other than these claims, <a href="https://docs.redpanda.com/docs">Redpanda’s documentation</a> was relatively quiet on questions of fault-tolerance, durability, consistency, and other safety guarantees.</p>
<p>Like Kafka, Redpanda is intended for a variety of streaming applications. Some of these are safety-critical: messages must never be lost. Others focus on throughput or latency: skipping some messages is fine. Both Redpanda and Kafka clients expose a number of configuration settings which trade off between speed and safety. Users should be aware of these settings, and choose appropriately for their workload.</p>
<p>First, the producer <a href="https://kafka.apache.org/documentation/#producerconfigs_acks"><code>acks</code></a> setting controls how many nodes must acknowledge a producer’s write before it is considered committed. The strongest setting is <code>acks = all</code> (also written <code>-1</code>). In Kafka this waits for all nodes in the in-sync replica set (ISR) to acknowledge the write, but <a href="https://redpanda.com/blog/kafka-redpanda-availability/">does not fsync by default</a>. In Redpanda this waits for a majority of nodes to fsync. A value of <code>0</code> allows Redpanda to confirm a message without writing it to <em>any</em> node; a single-node crash could cause data loss. Choosing <code>1</code> waits for a single node to acknowledge the message, which allows data loss in the event that node fails. The default, as of Kafka’s 3.0.0 client, is <code>acks = all</code>.</p>
<p>Kafka producers can automatically retry writes, which means they may append a single message multiple times to a topic. To prevent this, clients may either set <a href="https://kafka.apache.org/documentation/#producerconfigs_enable.idempotence"><code>enable.idempotence = true</code></a> or <a href="https://kafka.apache.org/documentation/#streamsconfigs_retries"><code>retries = 0</code></a>. The Kafka Java client enables idempotence by default.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Consumers can automatically commit their current offsets to Kafka. Since consumers advance their local offsets as soon as they see a message, this can cause messages to be considered committed even if they haven’t been processed yet. If a consumer crashes after auto-commit but before processing that message, it might never be tried again. To avoid this, consumers should set <code>enable.auto.commit = false</code>. The <a href="https://kafka.apache.org/documentation/#consumerconfigs_enable.auto.commit">default is true</a>, which could effectively lead to message loss.</p>
<p>When a consumer begins reading from a partition, and no offset has been committed to Redpanda, that consumer has to choose an offset to start at. This behavior is controlled by <a href="https://kafka.apache.org/documentation/#consumerconfigs_enable.auto.commit"><code>auto.offset.reset</code></a>. The default value, <code>latest</code>, starts clients near the most recent offset in the log. If clients crash without committing an offset, this could allow them to skip over some messages. To ensure clients read all messages in this scenario, use <code>earliest</code>, which rewinds fresh clients to the earliest log offset available. Of course, log expiration policies could also cause consumers to miss messages.</p>
<h2 data-number="1.2" id="transactions"> Transactions</h2>
<p>Redpanda implements Kafka’s transaction protocol, but this support remained behind a feature flag in version 22.1.1. Redpanda offers users no specific documentation on how to use transactions, relying on Kafka’s documentation instead. Kafka’s official documentation specifies that consumers can choose between <a href="https://kafka.apache.org/documentation/#consumerconfigs_isolation.level">two isolation levels</a>: <code>read_uncomitted</code> and <code>read_committed</code>. <code>read_uncommitted</code> allows consumers to see “all messages, even transactional messages which have been aborted.” The <code>read_committed</code> setting “will only return transactional messages which have been committed.”</p>
<p>Readers familiar with other databases may know that these terms have existing meanings: they have been <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">studied</a> and <a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">formalized</a> since at least the mid-1990s. In <a href="http://pmg.csail.mit.edu/papers/icde00.pdf">Adya, Liskov, &amp; O’Neil’s formalism</a>, <a href="https://jepsen.io/consistency/models/read-uncommitted">read uncommitted</a> prevents phenomenon G0 (<em>write cycle</em>). Write cycle occurs when two (or more) transactions’ writes interleave on one or more objects. For example, transaction <span><em>T</em><sub>1</sub></span> writes some object <span><em>x</em></span> before <span><em>T</em><sub>2</sub></span> writes <span><em>x</em></span>, and <span><em>T</em><sub>2</sub></span> writes some <span><em>y</em></span> before <span><em>T</em><sub>1</sub></span> writes <span><em>y</em></span>, given some total order of writes to <span><em>x</em></span> and <span><em>y</em></span>. <a href="https://jepsen.io/consistency/models/read-committed">Read committed</a> proscribes three additional phenomena: G1a (<em>aborted read</em>), G1b (<em>intermediate read</em>), and G1c (<em>circular information flow</em>). Aborted read means a transaction observes a value written by a transaction which did not commit. Intermediate read involves reading a state from the middle of a transaction. Circular information flow encompasses dependency cycles between transactions where either <span><em>T</em><sub>1</sub></span> writes some <span><em>x</em></span> before <span><em>T</em><sub>2</sub></span> writes <span><em>x</em></span>, or <span><em>T</em><sub>2</sub></span> reads something <span><em>T</em><sub>1</sub></span> wrote. Of course, these anomalies pertain to histories of reads and writes over registers, not logs, but one can imagine analogous phenomena in Kafka’s data model.</p>
<p>The <code>isolation_level</code> documentation makes it seem clear that <code>read_committed</code> proscribes G1a. What of G0, G1b, and G1c? Kafka’s <a href="https://kafka.apache.org/documentation/">official documentation</a> is somewhat vague<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> on this point, but it <em>does</em> argue two key properties. First, transactions (when combined with idempotence) ought to ensure what Kafka calls <a href="https://kafka.apache.org/documentation/#upgrade_11_exactly_once_semantics"><em>exactly-once semantics</em></a>: one can consume messages from some topics, and send new messages to other topics, such that the new messages resulting from those particular input messages are produced only once. Second, transactions provide <em>atomicity</em>:</p>
<blockquote>
<p>Transactional delivery allows producers to send data to multiple partitions such that either all messages are successfully delivered, or none of them are.</p>
</blockquote>
<p>While the Kafka documentation quoted above does not appear to discuss this, a <a href="https://www.confluent.io/blog/transactions-apache-kafka/">2017 Confluent blog post</a> provides some caveats around atomicity: consumers may not be subscribed to all partitions involved in a transaction, so they may see only some, not all, of its effects. <a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/">Another Confluent post</a> describes Kafka’s atomicity as <em>eventual</em>:</p>
<blockquote>
<p>… either all messages in the batch are eventually visible to any consumer or none are ever visible to consumers.</p>
</blockquote>
<p>Jepsen is unsure whether “eventual” here is meant to imply a lack of real-time or session guarantees (e.g. transactions appear to execute in partial/total order, but there may be some time between transaction commit and visibility), or if eventual implies a lack of isolation: e.g. reads and/or writes from multiple transactions may be interleaved together. Redpanda believes both senses are true.</p>
<p>Confluence’s <a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">Transactional Messaging</a> wiki page provides five simple requirements that transactional applications in Kafka expect. At first, this looks like a promising summary for users trying to understand transaction semantics:</p>
<blockquote>
<ol type="1">
<li>Atomicity: A consumer’s application should not be exposed to messages from uncommitted transactions.</li>
<li>Durability: The broker cannot lose any committed transactions.</li>
<li>Ordering: A transaction-aware consumer should see transactions in the original transaction-order within each partition.</li>
<li>Interleaving: Each partition should be able to accept messages from both transactional and non-transactional producers</li>
<li>There should be no duplicate messages within transactions.</li>
</ol>
</blockquote>
<p>On closer reading, this is even more confusing. “Atomicity” here means neither all-or-nothing commit nor the appearance of isolated point-in-time evaluation. Instead, atomicity is defined as the prevention of aborted read. The ordering property appears ambiguous: “transaction order” could mean an order over transactions, or an order over operations <em>within</em> each transaction. The example which follows suggests they mean both: given concurrent transactions X1 and X2…</p>
<blockquote>
<p>Since X2 is committed first, each partition will expose messages from X2 before X1.</p>
</blockquote>
<p>From this Jepsen concludes “transaction order” means (at least) the order of commits. Although writes from X1 and X2 were interleaved in real time, consumers should process all messages from X2 strictly before all messages from X1. This suggests that G0 (write cycle) ought to be prohibited.</p>
<p>Redpanda believes this wiki page is wrong on both points. They point to a <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit">somewhat difficult-to-find Google Doc</a> which serves as the design document for Kafka’s transactional protocol, which states that write atomicity refers to writes succeeding or failing as a unit, and then links to the aforementioned <a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">wiki page</a> which says atomicity means the prevention of aborted read. The design document also provides several scenarios in which atomicity would fail to hold:</p>
<blockquote>
<ol type="1">
<li>For compacted topics, some messages of a transaction maybe overwritten by newer versions.</li>
<li>Transactions may straddle log segments. Hence when old segments are deleted, we may lose some messages in the first part of a transaction.</li>
<li>Consumers may seek to arbitrary points within a transaction, hence missing some of the initial messages.</li>
<li>Consumer may not consume from all the partitions which participated in a transaction. Hence they will never be able to read all the messages that comprised the transaction.</li>
</ol>
</blockquote>
<p>These are reasonable constraints given Kafka’s data model. But what happens if we don’t use compaction, don’t seek to arbitrary offsets, and do consume from all partitions which participate in a transaction? Are writes isolated from one another?</p>
<p>To be specific: if transaction <span><em>T</em><sub>1</sub></span> commits before <span><em>T</em><sub>2</sub></span>, do all offsets written by <span><em>T</em><sub>1</sub></span> fall before those written by <span><em>T</em><sub>2</sub></span>? Many transaction systems buffer their writes and apply them more or less atomically at commit time, but a careful reading of this design document suggests that Kafka does not do this. Instead, Kafka chooses to add writes to the log immediately as each request in a transaction occurs—and to preserve performance, does not lock partitions for the duration of a transaction’s writes. This means that writes from two different transactions may interleave in offsets.</p>
<p>What about the order in which consumers process those writes? The <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.od2aaa53rbv">Consumer section</a> of the design document explains that messages are always delivered in offset order. This suggests that the wiki is incorrect, and transactional writes should visibly interleave.</p>
<p>Indeed, a subsection titled “Discussion on Transaction Ordering” explains that Kafka considered having consumers buffer and re-order these writes so that they <em>were</em> processed in transaction order. This approach would provide some additional measure of transactional isolation and improved latency. However, Kafka users expect that messages are delivered in offset order, and many parts of the Kafka API use a single high water mark to indicate which offsets have been processed. Delivering messages in transaction order would force consumers to track each offset individually, at least for a window of concurrent transactions. Perhaps the <a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">Confluence Wiki</a> reflects earlier goals for the transaction system, and was never updated as the design evolved.</p>
<p>From all of these sources, a sufficiently diligent reader could conclude that Kafka (and therefore Redpanda) transactions allow G0, prohibit G1a at <code>read_committed</code>, and allow G1b. Whether G1c (cycles involving both write-write and write-read dependencies) may occur remains unclear.</p>
<h2 data-number="1.3" id="transactional-ids"> Transactional IDs</h2>
<p>Each producer performing a transaction in Kafka/Redpanda must choose a <em>transactional ID</em>: a string whose meaning is poorly defined but which, if chosen incorrectly, may cause the transaction system to exhibit undefined behavior. We must therefore discuss it in detail.</p>
<p>The <a href="https://kafka.apache.org/30/documentation.html">official Kafka documentation describes</a> the <code>transactional.id</code> producer setting like so:</p>
<blockquote>
<p>The TransactionalId to use for transactional delivery. This enables reliability semantics which span multiple producer sessions since it allows the client to guarantee that transactions using the same TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured, <code>enable.idempotence</code> is implied. By default the TransactionId is not configured, which means transactions cannot be used. Note that, by default, transactions require a cluster of at least three brokers which is the recommended setting for production; for development you can change this, by adjusting broker setting <code>transaction.state.log.replication.factor</code>.</p>
</blockquote>
<p>This is essentially all of the guidance which Kafka’s official documentation offers regarding transactional IDs. What <em>are</em> “reliability semantics”? It’s not clear: this is the only use of the word “reliability” in the documentation. The presence of a transactional ID implies idempotence, and has something to do with enforcing some kind of exclusion between multiple producer sessions sharing the same transactional ID, but how? And what transactional ID should we use?</p>
<p>The <a href="https://javadoc.io/static/org.apache.kafka/kafka-clients/3.0.0/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html">Java client documentation</a> offers little more clarity:</p>
<blockquote>
<p>To use the transactional producer and the attendant APIs, you must set the transactional.id configuration property….</p>
<p>The purpose of the <code>transactional.id</code> is to enable transaction recovery across multiple sessions of a single producer instance. It would typically be derived from the shard identifier in a partitioned, stateful, application. As such, it should be unique to each producer instance running within a partitioned application.</p>
</blockquote>
<p>“Unique to each producer instance” suggests that we might want to choose a <em>different</em> transactional ID when e.g. a worker crashes and restarts. Should it be globally unique? The phrase “derived from the shard identifier” suggests that we might reuse transactional IDs across instances. And what is a session exactly? Many database clients have a first-class session API, but there appears to be no such construct in the Java Kafka client. Perhaps “session” refers to a single instance of a producer (e.g. a single client object in a single JVM on a single node), and “producer instance” refers to <em>multiple</em> such instances over time, which are intended to be logically related.</p>
<p>The <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.mmbff48qbcw6">Kafka transactions design document</a> has a section titled “Transactional Guarantees” which elaborates on how transactional IDs ensure idempotence and transaction recovery across sessions:</p>
<blockquote>
<p>When provided with such an TransactionalId, Kafka will guarantee:</p>
<ol type="1">
<li><p>Idempotent production across application sessions. This is achieved by fencing off old generations when a new instance with the same TransactionalId comes online.</p></li>
<li><p>Transaction recovery across application sessions. If an application instance dies, the next instance can be guaranteed that any unfinished transactions have been completed (whether aborted or committed), leaving the new instance in a clean state prior to resuming work.</p></li>
</ol>
</blockquote>
<p>When no transactional ID is provided, the design document says each producer still “enjoys idempotent semantics and transactional semantics within a single session.”<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>A <a href="https://www.confluent.io/blog/transactions-apache-kafka/">2017 Confluent blog post on transactions</a> seems to confirm this interpretation, explaining that once a producer registers its transactional ID with the cluster, it prevents any other producers (“zombies”) with that same transactional ID from writing to the cluster.</p>
<blockquote>
<p>We solve the problem of zombie instances by requiring that each transactional producer be assigned a unique identifier called the <code>transactional.id</code>. This is used to identify the same producer instance across process restarts.</p>
</blockquote>
<p>This hints that we should choose a unique identifier for each logical producer, and reuse it if the logical producer (e.g.) crashes and restarts, creating a new instance of the <code>KafkaProducer</code> client.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>The blog post also has a section helpfully titled “How to pick a transactional ID”, which explains:</p>
<blockquote>
<p>The key to fencing out zombies properly is to ensure that the input topics and partitions in the read-process-write cycle is always the same for a given transactional.id. If this isn’t true, then it is possible for some messages to leak through the fencing provided by transactions.</p>
<p>For instance, in a distributed stream processing application, suppose topic-partition <em>tp0</em> was originally processed by transactional.id <em>T0</em>. If, at some point later, it could be mapped to another producer with transactional.id <em>T1</em>, there would be no fencing between <em>T0</em> and <em>T1</em>. So it is possible for messages from <em>tp0</em> to be reprocessed, violating the exactly-once processing guarantee.</p>
</blockquote>
<p>The plot thickens: each transactional ID must consume from a <em>fixed</em> set of input topic-partitions. If we fail to maintain this invariant, messages might be processed multiple times—presumably, once per transactional ID.</p>
<p>As Kafka user Tomasz Guz <a href="https://tgrez.github.io/posts/2019-04-13-kafka-transactions.html">explains</a>, this critical section of Confluent’s post is misleading. Two clients with different transactional IDs which each consume from a single, fixed topic-partition can both process the same message. The “key to fencing” is <em>not</em> ensuring each transactional ID has a constant set of input topic-partitions. One must instead (or, possibly, also) ensure that each input topic-partition is consumed by at most one transactional ID. Confluent’s use of “for instance” is not just explaining the consequences of the previous paragraph. It appears to be introducing an <em>entirely different constraint</em>.</p>
<p>Redpanda engineers make an even stronger claim: if two different transactional IDs ever interact with the same topic, guarantees within a single transactional ID, and even within a single transaction, go out the window. Users should expect duplicate delivery even within a single transaction ID. Jepsen cannot locate a source for this claim; Redpanda suspects it is implied somewhere within the sixty-seven pages of the <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.o8gioa9dcbnm">transaction design document</a>.</p>

<p>We designed a <a href="https://github.com/jepsen-io/redpanda/tree/516655ef0d5e954a77d7297b296fd1a531f30465">test suite for Redpanda</a> using the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a>. We began our testing with version 21.10.1, and followed up with 21.10.2, 21.10.3, and 21.11.2, as well as various development builds through January 30, 2022. We also briefly tested Kafka 3.0.0 to compare its behavior to Redpanda. Our tests ran on both LXC containers and EC2 nodes, each running Debian Buster, with cluster sizes of 5–10 nodes.</p>
<p>To interact with Redpanda we used the <a href="https://docs.confluent.io/clients-kafka-java/current/overview.html">Java Kafka client</a> at version 3.0.0. For administrative tasks like configuring Redpanda and checking on cluster status, we used Redpanda’s <code>rpk</code> command or HTTP APIs exposed by Redpanda. Producers, consumers, and admin clients were always initialized with a single node for <code>bootstrap_servers</code>, but we did not interfere with smart client discovery: clients could talk to any node freely. This may have kept us from seeing safety violations.</p>
<p>All consumers shared a <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/client.clj#L56-L58">single consumer group</a>, and when using <code>subscribe</code>, <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L678">committed offsets manually</a> after each poll operation. For transactional workloads, we gave each producer a <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L590-L595">unique transactional ID</a><a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> and instead of committing offsets via <code>commitSync</code>, <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L400">added them to each transaction</a> which performed a poll—including read-only transactions.</p>
<p>We applied several configuration changes to clients in order to achieve faster recovery during failures, and to ensure safety. Our consumers ran with <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/client.clj#L60-L112">significantly shorter timeouts</a> (generally under 10 seconds), and with tunable <code>isolation_level</code>, <code>auto_offset_reset</code>, and <code>enable_auto_commit</code>, each of which has safety implications. Producers also ran with <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/client.clj#L114-L156">shorter timeouts</a>, and configurable <code>acks</code>, <code>enable_idempotence</code>, and <code>retries</code>. In general we tested with the <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/core.clj#L341-L352">safest possible settings</a>: default topic replications of 3, auto-commit false, acks <code>all</code>, retries 1,000, idempotence enabled, isolation level <code>read_committed</code>, <code>auto_offset_reset</code> of <code>earliest</code>, automatic creation of topics on the server disabled, and polling via <code>assign</code>.</p>
<p>During our tests we introduced a <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/core.clj#L25-L27">variety of faults</a>, including single- and multi-node crashes and process pauses, as well as network partitions between servers. We jumped clocks forward and backwards by up to several hundred seconds, as well as strobing clocks rapidly between different times. We also performed <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/nemesis.clj#L259-L309">cluster membership changes</a>, where we politely decommissioned nodes, then assigned them new (unique) node IDs and re-added them to the cluster. At all times we preserved at least 3 active nodes (those not adding or removing), and we used <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/db/redpanda.clj#L448-L454">a new API</a>, added to Redpanda after 21.11.2 for this work, to determine when a node add or remove operation was complete. We did not test the impolite removal of nodes.</p>
<h2 data-number="2.1" id="list-append"> List-Append</h2>
<p>The first workload we designed repurposed an existing Jepsen workload based on appends of unique values to lists. Each list is identified by a unique key. In our Redpanda <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/list_append.clj#L43-L127">list-append workload</a>, we <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/list_append.clj#L20-L37">mapped</a> each key to a distinct topic and partition in Redpanda. Topics were <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/workload/list_append.clj#L49-L52">created before their first write</a>, and our <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/workload/list_append.clj#L153-L160">generator of operations</a> rotated through different keys over time, limiting each key to roughly a thousand writes before creating a fresh key.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Each operation in this test either appended a single unique value to the list identified by a particular key, or read all values in some key’s list. We <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/list_append.clj#L80-L81">performed appends</a> by turning the key into a Kafka <code>TopicPartition</code> and calling <code>producer.send</code> to append that value to the given topic-partition. Reads <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/list_append.clj#L56-L77">were implemented</a> by seeking the consumer to the beginning of the given topic-partition, fetching the maximum offset of that partition via <code>consumer.endOffsets</code>, and then repeatedly calling <code>consumer.poll</code> until the maximum offset is observed.</p>
<p>We analyzed these histories by passing them to <a href="https://github.com/jepsen-io/elle">Elle</a>, which inferred a dependency graph between each operation based on the values of reads and appends, plus per-process and real-time orders. Elle looked for cycles in that graph, and presented them as consistency anomalies.</p>
<h2 data-number="2.2" id="queue"> Queue</h2>
<p>As a streaming system, Redpanda’s data model looks <em>somewhat</em> like a list, but our read pattern in list-append isn’t how most people use Kafka and Redpanda. Instead, consumers typically assign or subscribe to a topic infrequently, and call <code>poll</code> repeatedly—making assumptions about what those polls will return. We wanted to know: are the offsets returned by <code>poll</code> contiguous? Monotonic? Can they skip over gaps? We designed a separate <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj">queue</a> workload, which maps more closely to normal Redpanda use, to investigate these questions.</p>
<p>Like the list-append workload, we uniquely identified topic-partitions by integer keys, and <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L710-L767">rotated sends and polls across different keys over time</a>.</p>
<p>In our queue workload, operations were of one of three basic classes. <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L615">The first</a>, <code>crash</code>, simulated a client failure: it terminated the logical process which executed that <code>crash</code> operation, closing its Kafka consumer and producer. Jepsen would then create a fresh process with a new producer and consumer to take its place. The <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L606-L650">second class of operations</a>, <code>assign</code> or <code>subscribe</code>, updated the set of topics/partitions the consumer received messages from when calling <code>poll</code>: either assigning a specific set of topic-partitions (each corresponding to a single key), or subscribing to the set of topics which covered the requested key.</p>
<p>The <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L652-L688">third class</a> we called <code>txn</code> operations. Each contained a sequence of <code>poll</code> or <code>send</code> micro-operations. Those which performed only polls or sends were labeled <code>poll</code> and <code>send</code>, rather than <code>txn</code>, but their structure was otherwise identical. Each <code>send</code> micro-operation sent a single message value (a unique integer) to a specific key, and returned an <code>[offset, value]</code> pair, given the offset which the Kafka producer returned. Each <code>poll</code> micro-operation called <code>consumer.poll</code> once, and returned a map of keys to sequences of <code>[offset, message]</code> pairs observed for that key. For example, here is a completed representation of a <code>txn</code> operation:</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>[[<span>:poll</span> {<span>1</span> [[<span>2</span> <span>3</span>] [<span>4</span> <span>5</span>]]}]]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a> [<span>:send</span> <span>6</span> [<span>7</span> <span>8</span>]]</span></code></pre></div>
<p>This transaction polled key <code>1</code> and received two messages back: value <code>3</code> at offset <code>2</code>, and value <code>5</code> at offset <code>4</code>. Then it sent a single message <code>8</code> to key <code>6</code>, which was placed at offset <code>7</code>.</p>
<p>For non-transactional workloads, we constrained every <code>send</code>/<code>poll</code>/<code>txn</code> operation to contain exactly one micro-operation. For transactional workloads, we allowed multiple micro-operations and wrapped them all in a <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L308-L424">Kafka transaction</a>.</p>
<p>To analyze histories of these operations, we first constructed for each key a mapping of offsets to sets of message values observed at that offset, via either <code>send</code> or <code>poll</code>. We expect this mapping to be at least injective: each offset should refer to only a single message. If we observed more than one value at an offset, we recorded this as an <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1259-L1315">inconsistent offset error</a>. Since we only inserted values once, we also expected to observe no duplicate messages. If we observed a single value at multiple offsets, we identified that as a <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1678-L1692">duplicate error</a>.</p>
<p>When our mapping was bijective, we could construct a total order over all values with observed offsets for a given key. This order might not cover all messages in the topic-partition: calls to <code>send</code> and <code>poll</code> might not have returned offsets. Nor could we necessarily tell which offset an indeterminate-and-unobserved <code>send</code> might have produced. Moreover, not every offset contained a value: Redpanda uses some log offsets to store transaction metadata. We therefore <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1313">collapsed our sparse offset logs</a> into a dense <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1259-L1315">version order</a> which mapped each value to a unique index 0, 1, 2, ….</p>
<p>From this version order we could check for <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1437-L1676">several additional errors</a>, which came in symmetric flavors. We checked subsequent pairs of send micro-operations, and subsequent pairs of polls as well, to see if the offsets for their values were strictly monotonic (i.e. always increasing) and did not skip over intermediate indices. To distinguish behavior within a transaction versus between different transactions, we looked for non-monotonic or skipped offsets between two different transactions, and also within a single transaction (which we called an <em>internal</em> error).</p>
<p>For aborted reads, we simply <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1317-L1334">searched for any poll which returned a value sent by a failed operation</a>. For lost writes, we identified the highest read index for each key, then checked to make sure that <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1336-L1429">all lower indices were also polled</a>.</p>
<p>As a streaming system, Redpanda allows consumers to fall arbitrarily far behind producers: there is no expectation that consumers see up-to-date messages. Calls to <code>consumer.poll</code> tend to return successful, empty result sets regardless of whether the client is caught up on the latest messages, running behind, or talking to nodes which are completely offline, or have never run Redpanda at all. This makes it surprisingly difficult to distinguish between messages which are permanently lost versus simply delayed. Safety and liveness violations are—in this case—indistinguishable.</p>
<p>To address this problem, we kept track of a set of <em>unseen</em> messages: those whose <code>send</code> was successfully acknowledged, but which never appeared in any consumer’s <code>poll</code> results. We plotted the number of unseen messages over time, expecting it to be nonzero for most of each test run. However, at the end of each test, we healed all network partitions, restarted and resumed any crashed or paused nodes, reset clocks, and allowed the cluster up to an hour to heal. <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L843-L870">During that healing process</a> we performed no additional sends. Instead, we repeatedly polled every client in an attempt to catch up to any unseen messages. If we failed to observe some acknowledged messages, we reported that as an <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L1694-L1728">unseen error</a>.</p>

<p>Our testing identified three liveness, seven safety, and two ambiguous issues in Redpanda. We begin with duplicate writes, crashes, and inconsistent offsets, then discuss lost/stale messages and aborted reads. The second half of our results covers issues with transactions, starting with write cycles, aborted reads, and circular information flow, then moving to internal non-monotonic polls, another case of aborted read, and lost writes.</p>
<h2 data-number="3.1" id="duplicate-writes-by-default-1"> Duplicate Writes by Default (#1)</h2>
<p>The Kafka Java client uses a default setting of <code>enable.idempotence = true</code>, which the Kafka <a href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#producerconfigs_enable.idempotence">documentation claims</a>:</p>
<blockquote>
<p>When set to ‘true’, the producer will ensure that exactly one copy of each message is written in the stream. If ‘false’, producer retries due to broker failures, etc., may write duplicates of the retried message in the stream.</p>
</blockquote>
<p>However, with Redpanda 21.10.1, any pause, crash, or network partition could cause duplicated writes with the default settings. For example, consider <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211119T115356.000-0500.zip">this test run</a>, in which each Kafka producer logged that it was using idempotent writes:</p>
<pre><code>ProducerConfig values:
  ...
    enable.idempotence = true</code></pre>
<p>And yet reads of a single topic-partition returned the following messages:</p>
<pre><code>[1 2 ... 25 26 27 28 29 30 26 27 28 29 30]</code></pre>
<p>This workload calls <code>producer.send()</code> only once per message—but messages 26 through 30 were duplicated, thanks to the client’s internal retry mechanism. This is the precise scenario which <code>enable.idempotence</code> is designed to prevent.</p>
<p>Stranger still, if one explicitly sets <code>enable.idempotence = true</code> in the producer config, it refuses to connect to Redpanda at all:</p>
<blockquote>
<p>UnsupportedVersionException: The broker does not support INIT_PRODUCER_ID</p>
</blockquote>
<p>This is somewhat surprising, because Redpanda’s <a href="https://vectorized.io/docs/faq/#Is-Redpanda-Fully-Kafka-API-Compatible">FAQ claims</a>:</p>
<blockquote>
<p>Is Redpanda Fully Kafka API Compatible?</p>
<p>We support all parts of the Kafka API, including the transactions API that we added in release 21.8.1.</p>
</blockquote>
<p>In fact Redpanda 21.10.1 required setting <a href="https://vectorized.io/docs/configuration/">a configuration flag</a> (<code>enable_idempotence</code>) to support Kafka’s idempotence mechanism. Although the Java Kafka client’s logging <em>claims</em> idempotence is enabled with the default settings, and although the Kafka documentation <a href="https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#producerconfigs_enable.idempotence">says the same</a>, there may be a difference between “enabled by default” versus “enabled explicitly.” The client may be attempting some sort of feature negotiation with the broker, and when it detects that the broker does not support idempotence, it might silently disable the feature.</p>
<p>We enabled idempotence in Redpanda by <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/db/redpanda.clj#L136-L138">setting</a> two server-side configuration variables:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span>rpk</span> config set redpanda.id_allocator_replication 3</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span>rpk</span> config set redpanda.enable_idempotence true</span></code></pre></div>
<p>The upcoming release of Redpanda 22.1.1 will enable idempotence by default, which should resolve this issue.</p>
<h2 data-number="3.2" id="duplicate-writes-with-idempotence-explicitly-enabled-3039"> Duplicate Writes with Idempotence Explicitly Enabled (#3039)</h2>
<p>With both client- and server-side idempotence explicitly enabled, we again observed duplicate writes in version 21.10.1 with single-node faults, including process crashes, pauses, or network partitions. For instance, <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211230T161323.000-0500.zip">this test</a> with just process pauses induced five duplicate messages out of 2922 attempts:</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>{<span>:key</span> <span>7</span>, <span>:value</span> <span>259</span>, <span>:count</span> <span>2</span>}</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>{<span>:key</span> <span>8</span>, <span>:value</span> <span>544</span>, <span>:count</span> <span>2</span>}</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>{<span>:key</span> <span>8</span>, <span>:value</span> <span>542</span>, <span>:count</span> <span>2</span>}</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>{<span>:key</span> <span>8</span>, <span>:value</span> <span>543</span>, <span>:count</span> <span>2</span>}</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>{<span>:key</span> <span>8</span>, <span>:value</span> <span>545</span>, <span>:count</span> <span>2</span>}</span></code></pre></div>
<p>On key 8, polls observed messages 542, 543, 544, and 545 twice, interleaved with non-duplicated messages like 546. Take this series of messages returned from one call to <code>poll</code>:</p>
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>... <span>541</span> <span>542</span> <span>543</span> <span>545</span> <span>544</span> <span>543</span> <span>542</span> <span>544</span> <span>546</span> <span>545</span> <span>547</span></span></code></pre></div>
<p>In every case we observed, duplicates were limited to a narrow time window. The original request needed to succeed, but also have its acknowledgement message fail to arrive at the client on time, in order for a retry to occur and create a duplicate.</p>
<p>This behavior was caused by Redpanda failing to perform deduplication of sent messages. When the sequence numbers used to prevent duplicates arrived out of order, Redpanda returned an <code>OutOfOrderSequenceException</code> error. This seems like a reasonable choice, but it interacted poorly with clients, which interpreted that error as one they could retry. On encountering an <code>OutOfOrderSequenceException</code>, Kafka clients would increment their local epoch, reset their sequence number, and retry the request—which allowed it to take place multiple times.</p>
<p>In <a href="https://github.com/vectorizedio/redpanda/pull/3038">#3038</a> and <a href="https://github.com/vectorizedio/redpanda/pull/3039">#3039</a> Redpanda added support for server-side deduplication. Version 21.10.2 included deduplication support and did not exhibit duplicate messages; version 21.10.3 improved performance.</p>
<h2 data-number="3.3" id="assert-failure-deallocating-partitions-3335"> Assert Failure Deallocating Partitions (#3335)</h2>
<p>In version 21.10.1, when Redpanda deallocated a partition from a node, it could occasionally crash. The crash handler itself then crashed due to a malformed format string, causing <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211221T003916.000-0500.zip">error messages like</a>:</p>
<pre><code>ERROR 2021-12-21 05:51:35,176 [shard 0] assert -
../../../src/v/cluster/scheduling/allocation_
node.cc:44@deallocate: failed to log message:
fmt=&#39;Assert failure: ({}:{})
&#39;_allocated_partitions &gt; allocation_capacity{0}
&amp;&amp; _weights[core] &gt; 0&#39; unable to deallocate
partition from core {} at node {}&#39;:
fmt::v7::format_error (cannot switch from
automatic to manual argument indexing)</code></pre>
<p>This error (<a href="https://github.com/vectorizedio/redpanda/issues/3335">#3335</a>) appeared when testing membership changes. Redpanda is still investigating.</p>
<h2 data-number="3.4" id="assert-failure-in-response.partition_index-3336"> Assert Failure in <code>response.partition_index</code> (#3336)</h2>
<p>In rare cases involving process crashes, Redpanda 21.11.2 could <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211220T200525.000Z.zip">occasionally encounter</a> an assertion failure like</p>
<pre><code>ERROR 2021-12-20 20:22:03,884 [shard 0] assert -
Assert failure: (../../../src/v/kafka/server/
handlers/fetch.cc:732) &#39;response.partition_index
== _it-&gt;partition_response-&gt;partition_index&#39;
Response and current partition ids have to be
the same. Current response 0, update 1</code></pre>
<p>We observed this error (<a href="https://github.com/vectorizedio/redpanda/issues/3336">#3336</a>) with process crashes. It was caused by a mechanism which attempted to ensure fairness when polling multiple partitions: when a fetch request obtained messages from one partition, the server would re-order an internal cache to move that partition to the end, ensuring that the next fetch request would hit a different partition. However, this mechanism <a href="https://github.com/redpanda-data/redpanda/pull/4271">did not update a second data structure in the response message</a> to match the new partition order. Redpanda has addressed this issue in development builds, and the fix is scheduled for version 22.1.1.</p>
<h2 data-number="3.5" id="inconsistent-offsets-3003"> Inconsistent Offsets (#3003)</h2>
<p>Infrequently, process kills or network partitions caused Redpanda 21.10.1 to exhibit duplicate messages which appeared at <em>multiple</em> offsets in the log—despite using <code>acks=all</code> and <code>retries=0</code>. For instance, <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211219T132841.000-0500.zip">this test run</a> contained the following send operations on key 4:</p>
<div id="cb9"><pre><code><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:send</span> <span>4</span> [<span>365</span> <span>381</span>]]],</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>987763642489</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>1737</span>}</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:send</span> <span>4</span> [<span>366</span> <span>382</span>]]],</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>988052525845</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>1600</span>}</span></code></pre></div>
<p>Here, <code>[:send 4 [365 381]]</code> denotes a successful acknowledgement of a send to key 4: message 381 was stored at offset 365. A quarter of a second later, a call to <code>consumer.poll</code> returned message 381 at that offset:</p>
<div id="cb10"><pre><code><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:poll</span> {<span>4</span> [[<span>0</span> <span>2</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                    ...</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                    [<span>364</span> <span>380</span>]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>                    [<span>365</span> <span>381</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>                    [<span>366</span> <span>382</span>]]}]],</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>988295099584</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>1327</span>}</span></code></pre></div>
<p>8.7 seconds later, that same process issued another call to <code>consumer.poll</code>, which returned additional messages for key 4:</p>
<div id="cb11"><pre><code><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:poll</span> {<span>4</span> [[<span>367</span> <span>381</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                    [<span>368</span> <span>382</span>]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                    [<span>369</span> <span>387</span>]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>                    ...]}]],</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>997028597232</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>1327</span>}</span></code></pre></div>
<p>Messages 381 and 382 were shifted two slots later in the log! Instead of occurring at offsets 365 and 366 (respectively), they now <em>also</em> occurred at offsets 367 and 378. This client processed these messages twice.</p>
<p>This could have been a simple duplication error—no contradictory observations of offsets 365 or 366 occurred in this test run. However, additional testing provided direct evidence of contradictory offsets in Redpanda 21.10.1. Consider <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211217T145815.000-0500.zip">this test run</a>, where network partitions and process crashes caused 1,160 offsets to contain conflicting messages. On key 4, for example, several messages were reordered to earlier log offsets:</p>
<figure>
<img src="http://jepsen.io/analyses/redpanda-21.10.1/reorder-4.png" alt="A plot of key 4’s log offsets over time."/><figcaption aria-hidden="true">A plot of key 4’s log offsets over time.</figcaption>
</figure>
<p>This diagram shows each operation’s view of the log for key 4, sorted by the time those operations completed. Time flows from top to bottom, and log offsets are arranged from left to right. When a single log offset contains conflicting values, those values are highlighted with a colored background.</p>
<p>Messages 36 through 43 were reordered to earlier offsets in the log. Some filled in gaps in the offsets that were initially reported to <code>.send()</code>; others overwrote messages already extant at their offsets. The resulting offsets could result in disagreement on the order of messages: senders believed 53 was inserted before 41, but pollers saw 41 before 53.</p>
<p>Meanwhile, on key 3, two processes both believed they were the writer of offset 78, eleven seconds apart. The first set offset 78 to 86, and the second set offset 78 to 90.</p>
<div id="cb12"><pre><code><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> ([<span>:send</span> <span>3</span> [<span>78</span> <span>86</span>]]),</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>490857418971</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>732</span>}</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a> <span>:value</span> ([<span>:send</span> <span>3</span> [<span>78</span> <span>90</span>]]),</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>501920547263</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>806</span>}</span></code></pre></div>
<p>Readers of key 3 only ever observed the latter write:</p>
<div id="cb13"><pre><code><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> ([<span>:poll</span> {<span>3</span> (...</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                    [<span>76</span> <span>86</span>]</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                    [<span>77</span> <span>87</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                    [<span>78</span> <span>90</span>]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                    [<span>79</span> <span>91</span>]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                    [<span>80</span> <span>92</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>                    ...)}]),</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>511060527454</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>772</span>}</span></code></pre></div>
<p>Note that this read also has message 86 at offset 76, not 78. All pollers agreed on this too!</p>
<p>However, on key 11, pollers did in fact disagree on which messages were at what index. Process 582 wrote message 373 at offset 242. Two hundred milliseconds later, process 438 polled key 11, and saw 373 at offset 242. 9.7 seconds later, process 486 polled key 11, and this time saw 373 reordered to offset 244: offset 242 now had value 371.</p>
<div id="cb14"><pre><code><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> ([<span>:send</span> <span>11</span> [<span>242</span> <span>373</span>]]),</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>258055827281</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>582</span>}</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a> <span>:value</span> ({<span>11</span> (...</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>              [<span>240</span> <span>371</span>]</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>              [<span>241</span> <span>372</span>]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>              [<span>242</span> <span>373</span>]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>              [<span>243</span> <span>374</span>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>              [<span>244</span> <span>375</span>]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>              ...)}),</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>258202815546</span>,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>438</span>}</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a> <span>:value</span> ({<span>11</span> (...</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>              [<span>242</span> <span>371</span>]</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>              [<span>243</span> <span>372</span>]</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>              [<span>244</span> <span>373</span>]</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>              ...)}),</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>267966157000</span>,</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>486</span>}</span></code></pre></div>
<p>If we look at the surrounding neighborhood of send and poll operations (again with time flowing top to bottom, and offsets from left to right) we can see this particular disagreement between pollers was a part of a larger, more complex reordering:</p>
<p><img src="http://jepsen.io/analyses/redpanda-21.10.1/reorder-11.png" alt="A plot of key 11’s log offsets over time."/><br/>
</p>
<p>Messages 371 through 379 were shifted two offsets later in the log, even after many of their original sent offsets were visible to pollers. This resulted in order inversions: both senders and some pollers thought message 376 preceded 378, but later polls showed 378 before 376—before 378 was relocated after 376 again.</p>
<p>In our tests Redpanda 21.10.1 and 21.10.2 would happily reorder dozens, even hundreds of messages in response to node and network faults.</p>
<p>This issue was likely due to an error in Redpanda’s implementation of the <a href="https://raft.github.io/">Raft consensus algorithm</a>, which allowed the Redpanda state machine to apply log entries before they were known to be committed. These uncommitted entries could change if a new leader came to power, allowing a short window of split-brain. This issue (<a href="https://github.com/vectorizedio/redpanda/pull/3003">#3003</a>) was fixed in 21.10.3 by waiting for the commit pointer to advance. We did not observe inconsistent offsets after 21.10.3.</p>
<h2 data-number="3.6" id="loststale-messages-6"> Lost/Stale Messages (#6)</h2>
<p>In our testing of 21.10.1, 21.10.2, 21.10.3, and 21.11.2, a variety of faults could cause successfully acknowledged messages to fail to appear in any client’s polls. These missing messages were always at the end of a partition—we might observe offsets 0 through 5, but offsets 6, 7, … would never appear in any poll.</p>
<p>Because <code>consumer.poll()</code> in Kafka and Redpanda is allowed to fall arbitrarily far behind producers, it was not possible to tell whether these missing messages were permanently lost or simply delayed. However, we made significant efforts to recover stale messages. At the end of each test, we ended all network partitions, unpaused any paused nodes, and restarted any crashed ones. We then tore down and recreated every client to ensure that no client was somehow “stuck.” We assigned each new client the full list of topic-partitions that had been written or read during the test, and called <code>poll</code> repeatedly until it had observed every offset we’d previously seen. In case clients were stuck <em>again</em>, we repeated this teardown-and-poll process for every client every ten seconds, and extended this final polling process for up to an hour. Every node had at least one client bootstrapped from that node, to ensure that nodes with stale metadata couldn’t prevent reads.</p>
<p>Nevertheless, we were consistently able to drive clusters into states where acknowledged messages were never seen by any client. For instance, <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211221T153711.000Z.zip">this test run</a> of version 21.11.2 with network partitions and membership changes caused the loss (or indefinite delay) of 9,988 out of 11,225 successfully acknowledged messages. Every single key (topic-partition) lost some messages. This stacked plot shows the number of unseen messages over time, with each key’s messages in a different color.</p>
<p><img src="http://jepsen.io/analyses/redpanda-21.10.1/unseen.png" alt="A timeseries plot of messages acknowledged but not seen by any consumer, broken down by topic."/><br/>
</p>
<p>This problem occurred despite using the strongest safety settings. Producers used <code>acks=all</code> and <code>retries=0</code>. Consumers used <code>auto_offset_reset=earliest</code> and read messages using <code>assign</code>, rather than <code>subscribe</code> (to rule out issues in the consumer group subsystem). We also increased <code>redpanda.default_topic_replication</code> to <code>3</code>, rather than the default of <code>1</code>, to make sure Redpanda’s internal topics were fault-tolerant. We set <code>redpanda.auto_create_topics_enabled = false</code> to ensure Redpanda was not automatically creating under-replicated topics which would be more susceptible to data loss.</p>
<p>A number of faults could cause lost/stale messages. We reproduced this issue with process kills alone in 21.10.1, with membership changes and network partitions combined in 21.11.2, and with process pauses alone (as well as process crashes with membership changes) in development builds circa January 19, 2022.</p>
<p>Redpanda reports that copies of missing messages could still be found in on-disk data files, but we don’t know why those messages were never delivered to consumers. Redpanda is still investigating.</p>
<h2 data-number="3.7" id="aborted-read-with-notleaderorfollower-kafka-13574"> Aborted Read With <code>NotLeaderOrFollower</code> (KAFKA-13574)</h2>
<p>In rare cases involving membership changes and process crashes, development builds of Redpanda circa December 30, 2021 exhibited what appeared to be aborted reads. For instance, <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20211230T183546.000Z.zip">this test</a> attempted to send message 586 to key 5, which failed with a <code>NotLeaderOrFollowerException</code>. However, 586 appeared consistently in later polls of key 5:</p>
<div id="cb15"><pre><code><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:poll</span> {<span>5</span> [...</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>                    [<span>359</span> <span>585</span>]</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                    [<span>360</span> <span>586</span>]</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>                    [<span>361</span> <span>587</span>]</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                    ...]}]],</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>792033983678</span>,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>1870</span>}</span></code></pre></div>
<p>Jepsen and Redpanda initially suspected that <code>NotLeaderOrFollowerException</code> was a definite failure code, which would make this a case of aborted read. The <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/common/errors/NotLeaderOrFollowerException.html">Kafka documentation</a> seemed to suggest that this error meant a request was not processed:</p>
<blockquote>
<p>Broker returns this error if a request could not be processed because the broker is not the leader or follower for a topic partition. This could be a transient exception during leader elections and reassignments. For <code>Produce</code> and other requests which are intended only for the leader, this exception indicates that the broker is not the current leader.</p>
</blockquote>
<p>However, Kafka engineers in <a href="https://issues.apache.org/jira/browse/KAFKA-13574">KAFKA-13574</a> informed us that <code>NotLeaderOrFollowerException</code> is in fact <em>indefinite</em>, and may signify a successful send operation. We asked the Kafka team if this was documented anywhere, but did not receive a response.</p>
<h2 data-number="3.8" id="write-cycles-8"> Write Cycles (#8)</h2>
<p>The <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html">KafkaProducer documentation</a> offers a straightforward example of transaction structure: one calls <code>producer.beginTransaction()</code>, performs one or more <code>producer.send()</code> calls, then calls <code>producer.commitTransaction()</code> to commit. Prior to commit, one can also call <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#sendOffsetsToTransaction(java.util.Map,org.apache.kafka.clients.consumer.ConsumerGroupMetadata)"><code>producer.sendOffsetsToTransaction(...)</code></a>, which couples specific read offsets into the transaction for <a href="https://www.confluent.io/blog/simplified-robust-exactly-one-semantics-in-kafka-2-5/">exactly-once semantics</a>.</p>
<p>In practice, using transactions correctly is somewhat more intricate than these examples suggest. Calling <code>abortTransaction</code> appears to be mandatory in certain conditions, and illegal in others. Producers must be torn down and recreated from scratch in many (but not all!) cases. Discussions with Redpanda engineers resulted in a transaction with roughly <a href="https://github.com/jepsen-io/redpanda/blob/516655ef0d5e954a77d7297b296fd1a531f30465/src/jepsen/redpanda/workload/queue.clj#L308-L533">16 separate error-handling paths</a>, depending on whether it was necessary to close and reopen the producer, whether an error occurred prior to or during commit, whether the abort itself was successful or crashed, and so on.</p>
<p>If one interprets <code>producer.send</code> as appending a record to the end of a particular topic-partition, it seems clear that transactional writes in Redpanda are not isolated from one another. Even in healthy clusters, we routinely observed transactions insert messages into the middle of other transactions’ writes. <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20220113T144155.000-0500.zip">This two-minute test run</a> on a January 6 development build performed 1,594 transactions which sent a message, and contained 163 clusters of transactions whose messages interleaved with one another. For example:</p>

<p>The bottom transaction here sent 296 to key 10, at offset 771. However, the top transaction snuck in and wrote 298 at offset 772. The bottom transaction then wrote 297 at offset 774. This behavior occurred in every version of Redpanda we tested.</p>
<p>Does this constitute an isolation violation? It is certainly not equivalent to the behavior one would expect were these transactions performed sequentially. A Confluent <a href="https://www.confluent.io/blog/apache-kafka-data-access-semantics-consumers-and-membership/">blog post on semantics</a> describes offsets as <em>monotonically increasing</em>, which might suggest we can infer write-write dependencies from offsets:</p>
<blockquote>
<p>An offset is a special, monotonically increasing number that denominates the position of a certain record in the partition it is in. It provides a natural ordering of records—you know that the record with offset 100 came after the record with offset 99.</p>
</blockquote>
<p>And Kafka’s <a href="https://kafka.apache.org/documentation/#intro_concepts_and_terms">Main Concepts and Terminology documentation</a> specifically refers to publishing as an <em>append</em> to a topic:</p>
<blockquote>
<p>When a new event is published to a topic, it is actually appended to one of the topic’s partitions…. Kafka guarantees that any consumer of a given topic-partition will always read that partition’s events in exactly the same order as they were written.</p>
</blockquote>
<p>This theme repeats in Kafka and Confluence documentation <a href="https://docs.confluent.io/5.5.1/kafka/introduction.html">introducing core Kafka concepts</a>:</p>
<blockquote>
<p>Each partition is an ordered, immutable sequence of records that is continually appended to a structured commit log. The records in the partitions are each assigned a sequential ID number called the offset, that uniquely identifies each record within the partition.</p>
</blockquote>
<p>If we do interpret a Kafka partition as an “object” in the transactional sense, and calls to <code>send</code> as appends to that partition, then allowing transactions to interleave their writes is analogous to phenomenon G0: a write cycle. Per <a href="http://pmg.csail.mit.edu/papers/icde00.pdf">Adya, Liskov, &amp; O’Neil</a>, G0 ought to be disallowed under isolation level read uncommitted—not to mention read committed and higher. Moreover, the <a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer">Confluence Wiki</a> explicitly states that this behavior ought to be forbidden. Yet with Redpanda, both read uncommitted and read committed isolation levels allowed writes to interleave.</p>
<p>On the other hand, Redpanda argues that every individual <em>offset</em> in a partition is a separate object, and inferring write-write dependencies in this way is a category error. Under this interpretation, <code>producer.send</code> does not mean “append a message to the end of a partition”. Instead, <code>send</code> means “set some unspecified offset (higher than the offset I just wrote to this partition, if any) to the given message.” There are no such things as write-write dependencies in this interpretation, because every offset has only a single value; no transaction ever overwrites another. The write dependency graph is trivially empty, and no G0 anomalies exist. This also appears consistent with the behavior implied by the <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#">Exactly Once Delivery</a> Google Doc.</p>
<p>Both of these views seem defensible. We do not know which interpretation Kafka &amp; Redpanda users expect; the official Kafka documentation declines to specify, and ancillary documents contradict one another. Regardless of which interpretation one chooses, both Kafka 3.0.0 and Redpanda exhibit similar write-interleaving behavior. This behavior is a consequence of the transactional protocol itself, rather than something specific to Redpanda’s implementation. We report G0 here not because it is definitively incorrect behavior, but because it could affect safety for some users, and the current behavior is under-documented.</p>
<p>A bit of good news: while producer offsets frequently contained gaps due to other transactions inserting during their execution, producer offsets remained monotonic in our transactional tests.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<h2 data-number="3.9" id="aborted-reads-circular-information-flow-3036"> Aborted Reads &amp; Circular Information Flow (#3036)</h2>
<p>In 21.10.1, transactions routinely exhibited aborted reads and circular information flow under normal operation. For instance, <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20220106T105650.000-0500.zip">this three-minute run</a> without any faults, using <code>acks=all</code> and <code>isolation.level=read_committed</code>, still resulted in seven cases where a failed transaction’s writes were visible to pollers. Here is a write of 567 to key 9, which failed during the <code>producer.sendOffsetsToTransaction</code> call prior to commit:</p>
<div id="cb16"><pre><code><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:fail</span>,</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:txn</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:poll</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>         [<span>:poll</span>]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>         [<span>:send</span> <span>9</span> <span>567</span>]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>         [<span>:poll</span>]],</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>30017247775</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>4</span>,</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a> <span>:error</span> [<span>:add-offsets</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>         <span>&#34;Unexpected error in</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span>         AddOffsetsToTxnResponse: The server</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span>         experienced an unexpected error when</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span>         processing the request.&#34;</span>]}</span></code></pre></div>
<p>And yet message 567 was visible to a concurrent poll operation:</p>
<div id="cb17"><pre><code><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:poll</span>,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:poll</span> {<span>9</span> [[<span>1477</span> <span>567</span>]]}]],</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>29973974218</span>,</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>6</span>}</span></code></pre></div>
<p>567 was not visible to later readers. We frequently observed messages which were visible to some pollers and then later disappeared, both from known failed and indefinite transactions. This anomaly is G1a (aborted read), and is expressly prohibited under both ANSI and Kafka descriptions of read committed.</p>
<p>Moreover, 21.10.1 frequently exhibited G1c (circular information flow), even when we took into account only write-read, rather than write-write, dependencies. In <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20220106T105650.000-0500.zip">that same test run</a> we found 35 clusters of transactions where each transaction’s writes were visible to every other transaction. For instance, consider this pair of transactions:</p>

<p>The top transaction sent message 59 to key 18, at offset 162. That same offset and value were returned to a poll by the bottom transaction. However, the bottom transaction sent message 45 at offset 119 to key 19—and that message was polled by the top transaction. At least one of these transactions must have read a value from the other <em>before</em> it was committed. Since both transactions committed, this does not constitute G1a (aborted read). This type of G1c cycle shows why Adya’s formalization of read committed prohibits G1a <em>and</em> G1c.<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>Redpanda had already identified this problem before Jepsen began testing transactions, and addressed it in <a href="https://github.com/vectorizedio/redpanda/pull/3036">#3036</a>. In short, an off-by-one error allowed the last stable offset visible to pollers to advance just past committed messages. In addition, <a href="https://github.com/vectorizedio/redpanda/pull/3232">#3232</a> allowed Redpanda to accidentally abort more transactions than necessary. These fixes were released in version 21.10.2, and we did not observe aborted reads or <em>wr</em>-only circular information flow in higher versions.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<h2 data-number="3.10" id="internal-non-monotonic-polls-10"> Internal Non-Monotonic Polls (#10)</h2>
<p>Reads introduce additional complexity into the Kafka/Redpanda transactional model. Kafka has separate clients for reading and writing data: consumers and producers are separate objects. There is no supervening client which provides transactions across both.<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a> Instead, transactional methods are only available on the producer, and callers <a href="https://github.com/jepsen-io/redpanda/blob/e592e489a797cdbe9bb22b994c44d0d6dc11e0f4/src/jepsen/redpanda/workload/queue.clj#L274-L307">couple</a> the highest offsets read from their consumer into the transaction by calling <code>producer.sendOffsetsToTransaction</code>.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a> For example:</p>
<div id="cb18"><pre><code><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>consumer<span>.</span><span>subscribe</span><span>([</span><span>&#34;some-topic&#34;</span><span>]);</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>producer<span>.</span><span>beginTransaction</span><span>();</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>records1 <span>=</span> consumer<span>.</span><span>poll</span><span>();</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>records2 <span>=</span> consumer<span>.</span><span>poll</span><span>();</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>producer<span>.</span><span>send</span><span>(</span>record<span>);</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>offsets <span>=</span> <span>&lt;</span>highest offsets observed in records1</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>           and records2<span>,</span> each plus one<span>&gt;;</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>producer<span>.</span><span>sendOffsetsToTransaction</span><span>(</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  offsets<span>,</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>  <span>&#34;some-consumer-group&#34;</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span>);</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>producer<span>.</span><span>commitTransaction</span><span>();</span></span></code></pre></div>
<p>What exactly are the ordering semantics of these reads?</p>
<p>One might expect that consumers poll offsets contiguously within a transaction, and do not skip over any messages. A skip within a transaction would be problematic because Kafka transactions do not commit each individual offset consumed, but rather the <em>maximum</em> offset consumed. Happily, in 21.11.2 and higher, we never observed skips within transactions.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>One might also expect that consumers poll strictly monotonic increasing offsets, so that they process messages both in order and at most once. Unfortunately, this does not appear to be true. Within a single transaction, a single consumer using <code>subscribe</code> could quietly rewind its position and poll records whose offsets fell <em>prior</em> to those they had just polled—in most cases consuming the same records multiple times. This happened regularly in healthy clusters in all versions of Redpanda we tested. Take <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20220113T174332.000-0500.zip">this run</a> of a development build from January 6, 2022, with no faults. It contained the following transaction:</p>
<div id="cb19"><pre><code><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>[[<span>:poll</span> {<span>25</span> [[<span>924</span> <span>359</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>             [<span>925</span> <span>360</span>]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>             [<span>928</span> <span>361</span>]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>             [<span>931</span> <span>364</span>]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>             [<span>935</span> <span>365</span>]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>             [<span>937</span> <span>362</span>]</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>             [<span>938</span> <span>363</span>]</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>             [<span>941</span> <span>370</span>]</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>             [<span>944</span> <span>366</span>]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>             [<span>945</span> <span>367</span>]</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>             [<span>947</span> <span>372</span>]</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>             [<span>949</span> <span>373</span>]</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>             [<span>952</span> <span>374</span>]</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>             [<span>957</span> <span>368</span>]</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>             [<span>958</span> <span>375</span>]</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>             [<span>959</span> <span>369</span>]</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>             [<span>963</span> <span>376</span>]]}]</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a> [<span>:poll</span> {}]</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a> [<span>:poll</span> {<span>25</span> [[<span>935</span> <span>365</span>]</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>             [<span>937</span> <span>362</span>]</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>             [<span>938</span> <span>363</span>]</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>             [<span>941</span> <span>370</span>]</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>             [<span>944</span> <span>366</span>]</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>             [<span>945</span> <span>367</span>]</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>             [<span>947</span> <span>372</span>]</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>             [<span>949</span> <span>373</span>]</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>             [<span>952</span> <span>374</span>]</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>             [<span>957</span> <span>368</span>]</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>             [<span>958</span> <span>375</span>]</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>             [<span>959</span> <span>369</span>]</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>             [<span>963</span> <span>376</span>]</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>             [<span>964</span> <span>371</span>]</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>             [<span>968</span> <span>378</span>]]}]</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a> [<span>:send</span> <span>25</span> [<span>973</span> <span>377</span>]]]</span></code></pre></div>
<p>This transaction executed three consecutive calls to <code>consumer.poll</code> with a single consumer, which had earlier been subscribed to the topic containing key 25. The first poll started with message 359 at offset 924, and continued through to offset 963. The second returned nothing. The third jumped backwards twelve messages, and returned offsets 935 to 968. Other transactions in this history jumped back to offsets completely before their previous call to <code>poll</code>.</p>
<p>This behavior seems dangerous. If we were trying to achieve “exactly-once” semantics, this single transaction would actually process some (but not all!) of its records twice. It might also violate ordering relationships between messages—if we processed the records in <code>records1</code>, then <code>records2</code>, we would process offset 935 immediately after 963. If message processing were non-commutative, this could lead to unexpected results.</p>
<p>Is this behavior expected? The Kafka <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html">consumer documentation</a> seems to suggest that consumers should return sequentially increasing offsets:</p>
<blockquote>
<p>The <code>position</code> of the consumer gives the offset of the next record that will be given out. It will be one larger than the highest offset the consumer has seen in that partition. It automatically advances every time the consumer receives messages in a call to <code>poll(Duration)</code>.</p>
</blockquote>
<p>And yet this is clearly not what happens! This is especially vexing because all the state required to prevent this kind of non-monotonic behavior is already present in the consumer. The consumer <em>knows</em> that its previous <code>poll</code> returned offset 963. It can therefore enforce that its next <code>poll</code> returns offset 964 or higher!</p>
<p>As it turns out internal non-monotonic polls are a consequence of of <em>consumer group rebalance events</em>, where the Kafka client and Redpanda coordinate to automatically reassign partitions among subscribers. A more thorough investigation revealed that consumers were having their partitions automatically reassigned within the scope of a transaction, causing those consumers to return to earlier positions in the log.</p>
<p>To avoid this hazard users must provide a <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.html"><code>ConsumerRebalanceListener</code></a>, which receives callbacks indicating changes in partition assignment. When a rebalance event occurs, the client can abort the current transaction—preventing it from observing messages out-of-order.</p>
<h2 data-number="3.11" id="aborted-read-with-invalidtxnstate-3616-a"> Aborted Read With <code>InvalidTxnState</code> (#3616-a)</h2>
<p>We frequently observed transactions which appeared to fail, but whose writes were visible to later reads. With process pauses, crashes, or network partitions, versions 21.10.1, 21.11.2, and development builds in early January 2022 would reliably throw <code>InvalidTxnStateException</code> when committing transactions, but the writes performed by those transactions might later be visible. In <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20220112T170943.000-0500.zip">this test run</a>, for instance, we encountered 66 transactions like so:</p>
<div id="cb20"><pre><code><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:fail</span>,</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:txn</span>,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:send</span> <span>7</span> [<span>97</span> <span>32</span>]]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>         [<span>:send</span> <span>6</span> [<span>33</span> <span>11</span>]]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>         [<span>:poll</span> {<span>9</span> [[<span>297</span> <span>120</span>]]}]],</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>68499617337</span>,</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>9</span>,</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a> <span>:end-process</span>? <span>true</span>,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a> <span>:error</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a> {<span>:type</span> <span>:abort</span>,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>  <span>:abort-ok</span>? <span>false</span>,</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  <span>:tried-commit</span>? <span>true</span>,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  <span>:definite</span>? <span>true</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>  <span>:body-error</span> <span>&#34;org.apache.kafka.common.errors.</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span>               InvalidTxnStateException: The</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span>               producer attempted a</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span>               transactional operation in an</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span>               invalid state.&#34;</span>,</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>   <span>:abort-error</span> <span>&#34;org.apache.kafka.common.</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span>                 KafkaException: Cannot</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span>                 execute transactional method</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span>                 because we are in an error</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span>                 state&#34;</span>}}</span></code></pre></div>
<p>This transaction attempted to send message 32 to key 7, but when it called <code>producer.commitTransaction()</code>, received an <code>InvalidTxnStateException</code>. It then attempted to abort the transaction, but the abort call failed because the producer was in an error state. Message 32 then appeared in later reads:</p>
<div id="cb21"><pre><code><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:txn</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [...</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>         [<span>:poll</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>          {<span>7</span> [[<span>85</span> <span>28</span>]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>              [<span>87</span> <span>29</span>]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>              [<span>91</span> <span>30</span>]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>              [<span>94</span> <span>31</span>]</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>              [<span>97</span> <span>32</span>]],</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>           <span>9</span> [[<span>297</span> <span>120</span>]</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>              [<span>301</span> <span>121</span>]</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>              [<span>302</span> <span>122</span>]]}]],</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>69250120511</span>,</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>10</span>}</span></code></pre></div>
<p>“Attempted in an invalid state” sounds like a definite failure: if the state were invalid before the attempt to commit, how could it possibly have succeeded? The <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/common/errors/InvalidTxnStateException.html">documentation for <code>InvalidTxnStateException</code></a> says nothing about its meaning, and the <a href="https://kafka.apache.org/30/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#commitTransaction()"><code>commitTransaction</code></a> documentation does not mention it in their list of possible exceptions. We <a href="https://issues.apache.org/jira/browse/KAFKA-13574?">asked the Kafka team about this error code</a>, but did not receive a response.</p>
<p>This turned out to be a bug in Redpanda’s transaction path. A client would commit a transaction, but the transaction coordinator crashed just prior to acknowledging that transaction to the client. The client would time out and attempt to retry the commit. On receiving that second commit request for an already committed transaction, the server would respond with <code>invalid_txn_state</code>—which the client might interpret as a failure. Unfortunately, the Kafka transactional protocol does not include a unique identifier for transactions, which makes it difficult to tell <em>which</em> transaction is being committed when a coordinator fails. Redpanda cannot tell, in general, whether a retried commit request should succeed or fail.</p>
<p>This issue was addressed via a suite of transaction improvements in <a href="https://github.com/vectorizedio/redpanda/pull/3616">#3616</a>. Redpanda now <a href="https://github.com/vectorizedio/redpanda/pull/3616/commits/b2f952a714d2db53c5ece362463b20969d1378ea">returns an <code>unknown_server_error</code></a>, which more clearly signals the transaction’s indeterminate state. In development builds after January 21, 2022, we no longer observed aborted reads with transactions. This issue was fixed in 21.11.15.</p>
<h2 data-number="3.12" id="lost-transactional-writes-3616-b"> Lost Transactional Writes (#3616-b)</h2>
<p>In 21.11.2, as well as development builds circa January 6 and 19, 2022, we observed occasional cases in healthy clusters where writes performed by a successfully committed transaction would vanish, never to be seen again. In <a href="http://jepsen.io.s3.amazonaws.com/analyses/redpanda-21.10.1/20220113T185428.000-0500.zip">this test run</a>, the following single-write transaction successfully committed:</p>
<div id="cb22"><pre><code><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:send</span>,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> [[<span>:send</span> <span>22</span> [<span>1903</span> <span>689</span>]]],</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>823319861300</span>,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>215</span>}</span></code></pre></div>
<p>And yet offset 1903 and value 689 never appeared in any poll. The immediately following poll skipped right over it. So too did every final poll, which used <code>assign</code> and <code>seekToBeginning</code> to attempt to read the entire partition in order. All observed something like:</p>
<div id="cb23"><pre><code><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>{<span>:type</span> <span>:ok</span>,</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a> <span>:f</span> <span>:txn</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a> <span>:value</span> ([<span>:poll</span> {<span>22</span> [...</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>                     [<span>1895</span> <span>682</span>]</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>                     [<span>1898</span> <span>683</span>]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>                     <span>; No 1903!</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>                     [<span>1908</span> <span>688</span>]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>                     [<span>1911</span> <span>690</span>]</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>                     [<span>1912</span> <span>691</span>]</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>                     [<span>1913</span> <span>692</span>]</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>                     ...]}]),</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a> <span>:time</span> <span>824502875814</span>,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a> <span>:process</span> <span>201</span>}</span></code></pre></div>
<p>Like the transactional aborted reads discussed just prior, this issue was addressed as a part of a package of transaction protocol improvements in <a href="https://github.com/vectorizedio/redpanda/pull/3616">#3616</a>. When applying log operations to the local state, a Redpanda leader would check to make sure that they were still the most current leader, and if that check succeeded, go on to perform multiple state transitions. However, the node could <a href="https://github.com/vectorizedio/redpanda/pull/3616/commits/4293c2e55c7c027ed8d23b0ba879f6cd51f8d5b9">apply an action, lose leadership, regain leadership, then go on to apply another action</a>—falsely assuming that it had been the sole leader the entire time. This could allow state machine operations to interleave incorrectly. Redpanda suspects that this caused successfully committed transactions to be lost.</p>
<p>The problem was resolved in development builds circa January 21, 2022. Redpanda now reads the current term prior to performing multiple state machine actions, and ensures that term is still current when applying each action. The fix was released in 21.11.15.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed In</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Duplicate writes by default</td>
<td>Pause, crash, or partition</td>
<td>22.1.1*</td>
</tr>
<tr>
<td><a href="https://github.com/vectorizedio/redpanda/pull/3039">3039</a></td>
<td>Duplicate writes with idempotence</td>
<td>Pause, crash, or partition</td>
<td>21.10.3</td>
</tr>
<tr>
<td><a href="https://github.com/vectorizedio/redpanda/issues/3335">3335</a></td>
<td>Assert failure deallocating partitions</td>
<td>Membership change</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/vectorizedio/redpanda/issues/3336">3336</a></td>
<td>Assert failure involving partition IDs</td>
<td>Crash</td>
<td>22.1.1*</td>
</tr>
<tr>
<td><a href="https://github.com/vectorizedio/redpanda/pull/3003">3003</a></td>
<td>Inconsistent offsets</td>
<td>Crash or partition</td>
<td>21.10.3</td>
</tr>
<tr>
<td>6</td>
<td>Lost/stale messages</td>
<td>Pause, crash, or partition</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://issues.apache.org/jira/browse/KAFKA-13574">KAFKA-13574</a></td>
<td>Aborted read with NotLeaderOrFollower</td>
<td>Membership change &amp; pause</td>
<td>Unresolved</td>
</tr>
<tr>
<td>8</td>
<td>Write cycles</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/vectorizedio/redpanda/pull/3036">3036</a></td>
<td>Aborted read &amp; circular information flow</td>
<td>None</td>
<td>21.10.2</td>
</tr>
<tr>
<td>10</td>
<td>Internal non-monotonic polls</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/vectorizedio/redpanda/issues/3616">3616-a</a></td>
<td>Aborted read with InvalidTxnState</td>
<td>Pause or crash</td>
<td>21.11.15</td>
</tr>
<tr>
<td><a href="https://github.com/vectorizedio/redpanda/issues/3616">3616-b</a></td>
<td>Lost transactional writes</td>
<td>None</td>
<td>21.11.15</td>
</tr>
</tbody>
</table>
<p><em>* 22.1.1 is an upcoming release</em></p>

<p>We identified ten issues (seven safety, three liveness) in Redpanda 21.10.1 through 21.11.2, including crashes, duplicated messages, non-monotonic polls, aborted reads, circular information flow, inconsistent offsets, and lost or delayed writes. Some of these issues, like aborted reads and lost writes, occurred in healthy clusters. Others required only minor faults: a process pausing for a few seconds could cause data loss. Both non-transactional and transactional workloads exhibited safety violations.</p>
<p>We also identified two safety behaviors which might be surprising, but are not necessarily incorrect. The first is an ambiguous error, <code>NotLeaderOrFollowerException</code>, which might suggest to users that a given write did not succeed when, in fact, it did. The second is that transactions provide no write isolation, allowing G0 (write cycle), G1b (intermediate read), and G1c (circular information flow) with write-write edges. G1c with only write-read edges is prevented in 21.10.2 and higher. Documentation disagrees as to whether this should be <a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">prohibited</a> or <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit">allowed</a>.</p>
<p>The Redpanda team already had an extensive test suite— including fault injection—prior to our collaboration. Their work found several serious issues including duplicate writes (#3039), inconsistent offsets (#3003), and aborted reads/circular information flow (#3036) before Jepsen encountered them. Redpanda has also extended their test suite to reproduce new issues Jepsen identified.</p>
<p>The most frequent safety issues we found were resolved in 21.10.3, but some problems, including aborted reads and lost writes, remained extant in 21.11.2. Aborted reads (#3616-a) and lost transactional writes (#3616-b) were fixed in the just-released 21.11.15. Fixes for duplicate writes by default (#1) and assert failures (#3336) are scheduled for version 22.1.1. We recommend users upgrade to 21.11.15 as soon as feasible, and 22.1.1 once available, to reduce the probability of safety errors.</p>
<p>Two issues remain to be investigated and patched: a crash when deallocating partitions (#3335) and lost/stale messages (#6). Aborted reads with NotLeaderOrFollower (#KAFKA-13574), write cycles (#8), and internal non-monotonic polls (#10) can all be addressed through documentation, but these docs have not yet been written.</p>
<p>We did not observe any safety issues related to clock skew, which makes sense: Redpanda uses Raft extensively for state machine replication, and does not rely on wall-clock timestamps for correctness. While the Kafka protocol does involve heartbeats and timeouts for consumer liveness, it uses logical epochs and sequence numbers for safety.</p>
<p>Jepsen found Redpanda straightforward to install and operate during testing. The <code>rpk</code> administrative tool made configuration simple, although we did have a minor issue with <code>rpk</code> altering config file ownership and making the config file unreadable to Redpanda itself. Node startup and cluster join were fast and robust. Clusters recovered in a few seconds from crashes and partitions. With minor surprises (namely, idempotence and transactions), the Java Kafka client worked seamlessly out of the box. Membership changes involved undocumented HTTP APIs (and new ones had to be built in order to perform membership changes safely) but Jepsen is confident that this process can be streamlined and integrated into <code>rpk</code>.</p>
<p>As always, we note that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we try hard to find problems, we cannot prove the correctness of any distributed system.</p>
<h2 data-number="4.1" id="mild-surprises"> Mild Surprises</h2>
<p>During our testing we encountered a number of minor surprises in the Kafka Java client and Redpanda proper. We’d like to discuss these briefly, in the hopes that they might help other users.</p>
<p>First, network clients typically throw either when connecting to or reading from nodes which are unavailable. A <code>KafkaConsumer</code>, by contrast, will happily connect to a jar of applesauce<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a> and return successful, empty result sets for every call to <code>consumer.poll</code>. This makes it surprisingly difficult to tell the difference between “everything is fine and I’m up to date” versus “the cluster is on fire”, and led to significant confusion in our tests.</p>
<p>Consumers can use <code>commitSync</code> to inform Redpanda (or Kafka, as the case may be) that they have processed messages up to some offset. When producers fail and restart, they can pick up at the committed offset to avoid re-processing too many records. One might assume that the committed offset for a given topic-partition is monotonic, but this is not true. Committed offsets are allowed to go backwards, effectively un-committing committed messages. This could occur if a commit network message is delayed due to a process pause or network hiccup. In practice this seems unlikely to cause safety issues: users should already assume that in most cases, messages in Kafka/Redpanda will be processed multiple times.</p>
<p>Kafka and Redpanda can automatically create topics when producers send or consumers poll. By default, the replication factor for these topics in Redpanda is <code>1</code>, which means that if a client happens to interact with a topic prior to its official creation, and auto-creation is enabled on the client and server, one might end up with a topic with essentially no fault-tolerance. Users can increase the replication factor for these newly created topics by setting <code>redpanda.default_topic_replication</code> to three or higher. You can also disable topic autocreation by setting <code>redpanda.auto_create_topics_enabled = false</code>.</p>
<h2 data-number="4.2" id="non-fault-tolerant-defaults"> Non Fault-Tolerant Defaults</h2>
<p>Even in production mode, Redpanda <a href="https://github.com/vectorizedio/redpanda/blob/8408b7223ff73f3b8187560e0ad96653e746ec63/src/v/config/configuration.cc#L371-L388">did not provide fault tolerance</a> by default for its transaction coordinator, ID allocator, and internal metadata. This allowed Redpanda to run on a single-node installation out of the box, but could lead to safety issues if a single node fails. We recommend users with multi-node clusters set <code>redpanda.default_topic_replications</code>, <code>redpanda.id_allocator_replication</code>, and</p>
<p>Redpanda <a href="https://github.com/vectorizedio/redpanda/blob/8408b7223ff73f3b8187560e0ad96653e746ec63/src/v/config/configuration.cc#L329-L334">still does not enable idempotence support by default</a>. This could cause clients which default to idempotence to silently duplicate messages. We recommend Redpanda users set <code>redpanda.enable_idempotence = true</code> to avoid this problem. Redpanda plans to enable idempotence by default in 22.1.1.</p>
<h2 data-number="4.3" id="transaction-isolation"> Transaction Isolation</h2>
<p>Users of Kafka &amp; Redpanda transactions should be aware that transactions may, depending on one’s interpretation of write conflicts for <code>send</code> operations, allow G0 (write cycle) and G1c (circular information flow) by design. Two transactions may interleave their writes together. Transaction <span><em>T</em><sub>1</sub></span> can write a message prior to a write by <span><em>T</em><sub>2</sub></span>, then go on to read <span><em>T</em><sub>2</sub></span>’s writes before committing. We suspect, but have not demonstrated, that G1b (intermediate read) is also allowed by the transaction protocol which Redpanda and Kafka share. Users should expect these behaviors in their transactions so long as the Kafka transaction protocol remains unchanged. Since the <a href="https://kafka.apache.org/30/documentation.html">official Kafka documentation</a> declines to specify transactional write isolation, and <a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer">ancillary documentation</a> makes <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#">contradictory claims</a>, it’s hard to say what the “correct” behavior ought to be.</p>
<p>In 21.10.1, transactions allowed G1c cycles comprised entirely of write-read edges: <span><em>T</em><sub>1</sub></span> could write something which <span><em>T</em><sub>2</sub></span> read, and <span><em>T</em><sub>2</sub></span> could write something which <span><em>T</em><sub>1</sub></span> read. This behavior was a bug, and was fixed in 21.10.2.</p>
<p>We identified multiple cases of G1a (aborted read) in Redpanda transactions, up to and including version 21.11.2. These too were bugs, but all instances of G1a we identified should be fixed as of Redpanda 21.11.15. We believe users of Redpanda 21.11.15 who use transactions with <code>isolation_level = read_committed</code> should not observe G1a or G1c cycles comprised entirely of write-read dependencies. Running with <code>isolation_level = read_uncommitted</code> will likely still allow both phenomena.</p>
<h2 data-number="4.4" id="exactly-once-semantics"> Exactly-Once Semantics</h2>
<p>We never attempted to check <a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/">exactly-once processing</a> across multiple clients, because <em>individual</em> clients would routinely double-process messages in our tests—even within a single transaction. This could be caused by the fact that calls to <code>consumer.poll</code> were often non-monotonic, going back to re-read some or all messages previously observed in a single transaction. It could also be because our test broke some poorly documented rules of Kafka transactions.</p>
<p>We initially patterned our queue transactional workload after Kafka’s <a href="https://github.com/apache/kafka/blob/7d9b9847f184ec72c4c80c046edc408789dcc066/examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java#L79-L141">example code for exactly-once transactional processing</a>, but intended to measure only atomicity and isolation, rather than exactly-once processing. Consequently we made two changes which might invalidate global exactly-once semantics: we performed multiple calls to <code>poll</code> in a single transaction, and we allowed multiple transactional IDs to consume from a single partition.</p>
<p>Since the <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.k3ikkiat5ahm">Kafka transactions design document</a> specifies that a single transaction may perform multiple requests over a long phase, we replaced the example’s single call to <code>consumer.poll</code> with a short, random series of polls and sends. Performing multiple reads is a typical pattern in other transaction systems, and we found nothing in Kafka nor Redpanda’s documentation which says this is illegal.</p>
<p>Where the Kafka demo used <code>GROUP_INSTANCE_ID_CONFIG</code> to establish a static mapping of consumers to partitions, we used a single consumer group with automatic rebalancing. This allowed two clients with different transactional IDs to consume from the same partition. Neither the <a href="https://kafka.apache.org/30/documentation.html#producerconfigs_transactional.id">official documentation</a> nor the <a href="https://javadoc.io/static/org.apache.kafka/kafka-clients/3.0.0/index.html?org/apache/kafka/clients/producer/KafkaProducer.html">Javadocs</a> discuss the semantics of multiple transactional IDs—though they do suggest IDs be “unique to each producer instance.” A careful reading of a <a href="https://www.confluent.io/blog/transactions-apache-kafka/">Confluent blog post</a> suggests this should have allowed duplicate processing of messages across different transactional IDs, but says nothing about behavior within a single transaction or transactional ID. The <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#">Kafka transactions design doc</a> indicates that in the <em>absence</em> of transactional IDs, each producer still “enjoys idempotent semantics and transactional semantics within a single session,” but declines to say whether those guarantees hold when transactional IDs are provided but differ.</p>
<p>Redpanda believes these choices essentially invalidate our transactional results: performing multiple polls per transaction and allowing multiple transactional IDs to consume from the same partition means that we cannot expect safety even within a single transaction—let alone two transactions performed by clients with the same transactional ID. We cannot locate a source for this claim in the Kafka or Redpanda literature; if true, it may be undocumented.</p>
<p>Instead, Redpanda suggested that obtaining safe transactional semantics requires <a href="https://github.com/vectorizedio/chaos/blob/ef02d93364f55ea01b0cec13f564b547e2bdd99c/workloads/tx-subscribe/src/main/java/io/vectorized/Workload.java#L448">a more complex dance</a> in which one generates producers with specifically chosen transactional IDs based on source partitions. Each producer reads saved offsets for the partitions they intend to interact with, writes them back in an otherwise empty transaction to ensure concurrent producers haven’t overwritten those offsets, seeks to those offsets, and then performs transactions as normal. Neither this approach nor this particular code were referenced in any public documentation, and Jepsen is unsure how users would have learned to do this on their own. We did not have time to implement this pattern in our test suite, but other Kafka and Redpanda users might find it helpful.</p>
<p>All of this is complicated by official documentation which is <a href="https://docs.redpanda.com/">absent</a>, <a href="https://kafka.apache.org/30/documentation.html#producerconfigs_transactional.id">incomplete</a>, <a href="https://javadoc.io/static/org.apache.kafka/kafka-clients/3.0.0/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html">vague</a>, <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#">byzantine</a>, or <a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">simply wrong</a>. <a href="https://www.confluent.io/blog/transactions-apache-kafka/">Vendor</a> and <a href="https://tgrez.github.io/posts/2019-04-13-kafka-transactions.html">third-party</a> blog posts can be downright confusing. Users must piece together all of these resources, which often phrase behavior not in terms of application-level invariants but via the implementation details of an interlocking collection of distributed locking, idempotence, atomicity, retry, and crash-recovery mechanisms split across readers and writers. In short, Kafka and Redpanda offer less of a transaction system in the sense that database users are accustomed to, and more of a choose-your-own-adventure book in which half the pages are missing, critical plot points are scrawled in the margins by other readers, and many paths lead to silent invariant violations.</p>
<p>We stress that this is not Redpanda’s fault: conformance with the Kafka wire protocol significantly constrains what Redpanda can offer users. Nevertheless, better documentation would be a blessing.</p>
<h2 data-number="4.5" id="membership-changes"> Membership Changes</h2>
<p>As of February 1, 2022, Redpanda documentation did not mention how to remove nodes from the cluster. Instead Redpanda clients learned how to remove and add nodes in support channels on Slack. Redpanda has since added <a href="https://docs.redpanda.com/docs/reference/rpk-commands/#rpk-redpanda-admin-brokers-decommission">some documentation</a> for the <code>decommission</code> command.</p>
<p>These membership changes were difficult to execute correctly in unhealthy networks. Both <code>rpk cluster info</code> and the <code>/v1/brokers</code> API could return arbitrarily stale views of the cluster, which could lead to operators concluding that a node had completed its decommissioning process when it was, in fact, still handing off data to other members. We suspect that in practice this should be a rare occurrence: most network partitions last a few days at most, and nodes are usually not removed immediately after being added.</p>
<p>Still, operators who wish to perform node changes more safely (or rapidly!) can use a new API: <code>/v1/cluster_view</code>. This view of the cluster includes a monotonic <code>version</code> field which can be used to ensure one sees only successive views of the cluster state. To safely remove a node, one should:</p>
<ol type="1">
<li>Read a cluster view containing the node ID one intends to remove.</li>
<li>Issue a decommission request for that node.</li>
<li>Wait until one has read a cluster view which is of a higher version than the initial view, and does not contain the given node ID.</li>
</ol>
<p>After this, it should be safe to tear down the decommissioned node. This API was just released in version 21.11.15.</p>
<p>Another potential pitfall: one should always generate new, unique node IDs when adding nodes to a Redpanda cluster. Reusing previously generated IDs—even if those nodes are no longer a part of the cluster—could lead to data loss. This too was undocumented.</p>
<p>Redpanda is expanding their documentation to explain membership change operations in detail.</p>
<h2 data-number="4.6" id="future-work"> Future Work</h2>
<p>Normally, Jepsen tests bind each logical process in the test to a single node in the cluster, and stripe processes across nodes. This allows Jepsen to observe the state of different nodes simultaneously, which is key to finding concurrency bugs. The Java Kafka client, by contrast, fetches the list of all nodes in the cluster from whichever node it initially connects to, and from there establishes independent connections to different servers on its own. We were unable to easily interfere with this process, which means that our tests may have failed to identify split-brain or other anomalies between nodes. Every client may, assuming a stable topology, have routed all their requests for a given partition to a single node, rather than multiple nodes. We would like to revisit this problem and devise a way to constrain Kafka clients to talk <em>only</em> to specific nodes.</p>
<p>Our membership tests only examined polite node removal, where we issued a decommission request and allowed the node to hand off its data before wiping the node clean. Future testing might explore limited numbers of unplanned node removals, to explore what happens when (e.g.) a hard disk fails and cannot be recovered. We also discussed the possibility of injecting filesystem-level faults, but have not implemented them yet.</p>
<p>Our queue and list-append workloads used a fixed number of partitions per topic. Future work could expand the number of partitions dynamically.</p>
<p>Finally, we were unable to obtain exactly-once semantics during our transaction testing—possibly because of the structure of our transactions. Redpanda has suggested a more complex way of using the transaction API which might provide stronger guarantees. We’d like to integrate that into the Jepsen tests someday.</p>
<p>Jepsen carried out an informal poll of a handful of Kafka users to better understand their use of transactions. Several reported they had adopted the <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams</a> API rather than using the producer transaction API directly. Perhaps Kafka Streams offers stronger guarantees! Future tests might explore Kafka Streams behavior, and see whether it prevents some of the transactional anomalies we observed.</p>
<p><em>This work was funded by <a href="https://redpanda.com">Redpanda Data</a>, and conducted in accordance with the <a href="https://jepsen.io/ethics.html">Jepsen ethics policy</a>. Jepsen wishes to thank the Redpanda team for their assistance—especially Camilo Aguilar, Travis Bischel, Bob Dever, Juan Castillo, Alexander Gallego, Dhruv Gupta, Michal Maslanka, Denis Rystsov, John Spray, Coral Waters, David Wang, and Noah Watkins. We would also like to thank Irene Kannyo for her editorial support during preparation of this manuscript.</em></p>

  </div>
</article></div>
  </body>
</html>
