<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://earthly.dev/blog/remote-code-execution/">Original</a>
    <h1>Remote Code Execution as a Service</h1>
    
    <div id="readability-page-1" class="page"><div><p><em>Earthly Compute is an internal service that customers use indirectly via Earthly CI. Now that CI has been publicly announced, we have some stuff <del>to get off their chests</del> that we can finally share.</em></p>
<p><em>Compared to our previous experiences, Earthly Compute was a quirky service to build. Nevertheless, we learned some things and made some mistakes, and in this write-up, we’ll share how it went.</em><a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<h2 id="background">Background<a href="#background" title=""><span></span><i></i></a></h2>
<p>Imagine a service with compute-heavy workloads – maybe video encoding or handling ML inference requests. Customers’ workloads are bursty. A single request can pin many CPUs at once, and throughput matters, but there’s also a lot of idle time. This seems like a great use case for something like Kubernetes or Mesos, Right? You can spread the bursty load of N customers across N machines.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<p>Mesos? Is anything a good use case for Mesos?</p>
</div>
</div>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9900.png" alt="Adam Gordon Bell"/>
</p>
<p>Mesos can handle workload types that K8s can’t, and I recall liking Marathon. But anyways, it’s just an example.</p>
</div>
</div>
<p>We anticipated this type of workload when creating the initial version of Earthly compute. However, there was one issue that made container orchestration frameworks unsuitable: the workload. Earthly compute needs to execute customer-submitted Earthfiles, which are not dissimilar to Makefiles. Anything you can do on Linux, you can do in an Earthfile. This meant – from a security point of view – we were building remote code execution as a service (RCEAS).</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/0770.png" alt="Brandon Schurman"/>
</p>
<p>Earthfiles are executed inside runC for isolation, and you have to declare your environment, so it’s not exactly like running <code>make</code>. It’s more like <code>./configure &amp;&amp; make</code> inside a container.</p>
</div>
</div>
<p>RCEAS is not a good fit for Kubernetes. Container runtimes are excellent isolation layers but aren’t a sufficient security barrier. Container break-out vulns do come up and, in the worst case, would lead to nefarious customers being able to access and interfere with other customers’ builds. What we needed was proper virtualization.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<p>What I initially wanted was Kubernetes but for managing VMs. Either <a href="https://firecracker-microvm.github.io/" target="_blank">firecracker</a>, <a href="https://katacontainers.io/" target="_blank">Kata</a> or <a href="https://gvisor.dev/" target="_blank">gvisor VMs</a>. They’d all provide the isolation we need – and we will be exploring firecracker in the near future – but even without that, things turned out well.</p>
</div>
</div>
<h2 id="earthly-compute-v1">Earthly Compute V1<a href="#earthly-compute-v1" title=""><span></span><i></i></a></h2>
<p>Our first version used separate EC2 instances per client to properly separate clients’ compute. The first ‘customer’ of this service was dubbed Earthly Satellites. It was command line only, and you used Earthly like you usually do, except the build would happen on the satellite you programmatically spun up.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/0770.png" alt="Brandon Schurman"/>
</p>
<p>Earthly on your local. Satellites in the cloud. Get it?</p>
</div>
</div>
<p>Earthly, our build tool, is open source and usable by anyone. We wanted to draw some clear separation between it and our CI solution but still make transitioning to CI very smooth.</p>
<p><picture><source srcset="/blog/generated/assets/images/remote-code-execution/3600-1000-7403c2dcf.webp 1000w, /blog/generated/assets/images/remote-code-execution/3600-1200-7403c2dcf.webp 1200w" type="image/webp"/><source srcset="/blog/generated/assets/images/remote-code-execution/3600-1000-7403c2dcf.png 1000w, /blog/generated/assets/images/remote-code-execution/3600-1200-7403c2dcf.png 1200w" type="image/png"/><img src="https://jvns.ca/blog/generated/assets/images/remote-code-execution/3600-800-7403c2dcf.png"/></picture></p>
<figcaption>
V1 ran from dev machines against Earthly Compute.
</figcaption>
<p>Earthly has always had a front-end CLI program and a backend build service. So when you run Earthly CLI, it talks to the backend over gRPC. This works the same in satellites. It’s just the gRPC service is now in EC2.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<div>
<p>To get this working, we had to programmatically spin up EC2 instances, auth build requests and route them to the correct node.</p>
<p>We did this with a gRPC proxy.</p>
</div>
</div>
</div>
<p><picture><source srcset="/blog/generated/assets/images/remote-code-execution/3800-1000-7ff2908c9.webp 1000w, /blog/generated/assets/images/remote-code-execution/3800-1200-7ff2908c9.webp 1200w" type="image/webp"/><source srcset="/blog/generated/assets/images/remote-code-execution/3800-1000-7ff2908c9.png 1000w, /blog/generated/assets/images/remote-code-execution/3800-1200-7ff2908c9.png 1200w" type="image/png"/><img src="https://jvns.ca/blog/generated/assets/images/remote-code-execution/3800-800-7ff2908c9.png"/></picture></p>
<figcaption>
Earthly Compute does gRPC proxying and routing.
</figcaption>
<p>Once we built this feature, we tested it with our internal builds and then got beta volunteer companies to start testing it. Most customers saw a significant speed-up from their previous build times, but adoption was sometimes bumpy.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/0770.png" alt="Brandon Schurman"/>
</p>
<div>
<p>Turns out CI workloads vary a lot.</p>
<p>Disks filled up with cache faster than could be GC’d. Networking issues happened. Average builds would be fast but with tail latencies that seemed to go to infinity.</p>
<p>Getting the first version of satellites working smoothly, with all kinds of different CI jobs, was an adventure.</p>
</div>
</div>
</div>
<p>Even before all this, Earthly supported shared remote caching. But with the kinks worked out, something else became very apparent. The disk on the satellite instance acting as a fast local cache makes a big difference.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9900.png" alt="Adam Gordon Bell"/>
</p>
<div>
<p>The earthly blog was an early user of satellites, and it was surprising how well it worked.</p>
<p>Jekyll generates all these optimized images in different sizes for different browsers, and there is a ton of them. Previously I was caching them in GitHub actions, and that helped.</p>
<p>But when you have a ton of small files as a build output, having them cached locally on a beefy machine made a huge difference.</p>
</div>
</div>
</div>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<div>
<p>Yeah, ephemeral build servers sound great operations-wise: Spin one up, run your build, and destroy it when it’s finished or you need space.</p>
<p>But, throughput-wise, it will be faster if I have a fast machine just sitting there that already has everything up to the last commit downloaded, built, and locally cached.</p>
</div>
</div>
</div>
<h2 id="v2-sleep-time">V2: Sleep Time<a href="#v2-sleep-time" title=""><span></span><i></i></a></h2>
<div>
<p><picture><source srcset="/blog/generated/assets/images/remote-code-execution/4000-1000-bf3b4b755.webp 1000w, /blog/generated/assets/images/remote-code-execution/4000-1200-bf3b4b755.webp 1200w" type="image/webp"/><source srcset="/blog/generated/assets/images/remote-code-execution/4000-1000-bf3b4b755.png 1000w, /blog/generated/assets/images/remote-code-execution/4000-1200-bf3b4b755.png 1200w" type="image/png"/><img src="https://jvns.ca/blog/generated/assets/images/remote-code-execution/4000-800-bf3b4b755.png"/></picture></p>
<figcaption>
Usage was lumpy.
</figcaption>
</div>
<p>Once we had this EC2 solution and had worked out the kinks, it was time to optimize the price tag. Looking at our beta testers builds mainly happened when developers were working. And AWS bills us by the minute, so a simple solution would be to shut them down when they aren’t in use. That saves money but would add start-up latency to a build that tried to run when its backing instance was down.</p>
<p><picture><source srcset="/blog/generated/assets/images/remote-code-execution/4190-1000-a866040be.webp 1000w, /blog/generated/assets/images/remote-code-execution/4190-1200-a866040be.webp 1200w" type="image/webp"/><source srcset="/blog/generated/assets/images/remote-code-execution/4190-1000-a866040be.png 1000w, /blog/generated/assets/images/remote-code-execution/4190-1200-a866040be.png 1200w" type="image/png"/><img src="https://jvns.ca/blog/generated/assets/images/remote-code-execution/4190-800-a866040be.png"/></picture></p>
<figcaption>
Sleeping adds a cold start overhead.
</figcaption>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<div>
<p>Specifically, using the RunInstances API, an Amazon Linux instance can be started and be back listening on TCP within <a href="https://www.daemonology.net/blog/2021-08-12-EC2-boot-time-benchmarking.html" target="_blank">~10 seconds</a>.</p>
<p>So the latency is minimal in theory.</p>
</div>
</div>
</div>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/0770.png" alt="Brandon Schurman"/>
</p>
<p>It’s a lot like the EC2 instances are in an LRU cache. If they aren’t used in a while, they’re suspended, which is like cache eviction, and then they get lazily woken up on the subsequent request. That cache miss adds seconds to build time but saves hours of compute billing.</p>
</div>
</div>
<p>The first sleep implementation used the AWS API to shut down an instance after 30 minutes of inactivity, and when someone queued a new build for that instance, we started it back up. Hooking this sleep stuff up presents some problems though: if we sleep your build node, the gRPC request will fail. This led to the need for the router to not just route requests but wake things up.</p>
<p>But waking things up adds complexity. To the outside world, your satellite is always there, but inside the compute layer, we have to manage not just sleeping and waking your satellite but ensuring requests block while this happens. Also, we have to guarantee the machine only gets one sleep or wake request at a time and that it’s given time to wake up.</p>
<p><picture><source srcset="/blog/generated/assets/images/remote-code-execution/4570-1000-e0197e7c3.webp 1000w, /blog/generated/assets/images/remote-code-execution/4570-1200-e0197e7c3.webp 1200w" type="image/webp"/><source srcset="/blog/generated/assets/images/remote-code-execution/4570-1000-e0197e7c3.png 1000w, /blog/generated/assets/images/remote-code-execution/4570-1200-e0197e7c3.png 1200w" type="image/png"/><img src="https://jvns.ca/blog/generated/assets/images/remote-code-execution/4570-800-e0197e7c3.png"/></picture></p>
<figcaption>
State transitions add some complexity.
</figcaption>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<div>
<p>It’s kind of like a distributed state machine. There can be many different builds starting at the same time that are all trying to use the same satellite.</p>
<p>To make the process resilient, multiple requests to start builds need to coordinate with each other so that only one of them actually calls ‘wake’ and the rest simply queue and wait for the state change.</p>
</div>
</div>
</div>
<p>With the auto-sleep and coordinated wake-up inside Earthly Compute, our AWS bill got smaller, and no one noticed the start-up time.</p>
<p>Well, almost. Except, occasionally, that start-up time got much larger…</p>
<h2 id="v3-hibernate-time">V3: Hibernate Time<a href="#v3-hibernate-time" title=""><span></span><i></i></a></h2>
<p>It’s true. You can start up an EC2 instance and have it accepting TCP requests in 10-30 seconds. But our usage graphs showed that randomly some builds were taking an extra minute or more to start up. The problem was our build runner, BuildKit. Buildkit is not designed for fast start-ups. It’s designed for throughput. When it starts, it reads in parts of its cache from the disk, does some cache warmups, and possibly even some GC.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/0770.png" alt="Brandon Schurman"/>
</p>
<p>We investigated getting Buildkit to start faster and did get some improvements there, but then we had an even better idea: using hibernate instead of stop.</p>
</div>
</div>
<p>x86 EC2 instances support hibernate. With hibernate, the contents of memory are written to disk, and then on wake up, the disk state is written back to memory.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<div>
<p>It’s the server version of closing your laptop lid, and it’s so much faster because nothing needs to start back up.</p>
<p>The only downside is the arm instances don’t support it.</p>
</div>
</div>
</div>
<p>And so, with faster BuildKit start-up and all our X86 instances using suspend, the service looked good. Then we ran into resource starvation issues. If a build uses 100% of the CPU long enough, the health checks fail. But Cgroups came to the rescue. We were able to limit CPU and other resources to reasonable levels.</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<div>
<p>We are limiting CPU to 95%, but it’s not just CPU that is an issue. Beta customers had builds that filled the whole disk, leaving no room to GC the cache, so we set disk limits. Customers used all the memory and swap, then got killed by OOM-Killer, so we had to limit memory.</p>
<p>I even had to properly set <code>oom_score_adj</code>, so Linux would kill the parent process and not hang the build.</p>
<p>We learned a lot about Linux.</p>
</div>
</div>
</div>
<h2 id="today">Today<a href="#today" title=""><span></span><i></i></a></h2>
<p>With all that in place, and some fine-tuning, we saved much compute time.</p>
<p><picture><source srcset="/blog/generated/assets/images/remote-code-execution/5360-1000-cbeade1e8.webp 1000w, /blog/generated/assets/images/remote-code-execution/5360-1200-cbeade1e8.webp 1200w" type="image/webp"/><source srcset="/blog/generated/assets/images/remote-code-execution/5360-1000-cbeade1e8.png 1000w, /blog/generated/assets/images/remote-code-execution/5360-1200-cbeade1e8.png 1200w" type="image/png"/><img src="https://jvns.ca/blog/generated/assets/images/remote-code-execution/5360-800-cbeade1e8.png"/></picture></p>
<figcaption>
Sleeping works really well.
</figcaption>
<p>And with the service now powering both satellites and Earthly CI, we are now offering secure and fast ‘remote execution as a service.’</p>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9900.png" alt="Adam Gordon Bell"/>
</p>
<div>
<p>It’s actually not remote code execution as a service, though.</p>
<p>For users, it’s just a faster CI. It’s the build runner behind a CI Service. “remote code execution as a service” is just the name Corey used internally as a joke.</p>
</div>
</div>
</div>
<div>
<div>
<p><img src="https://jvns.ca/blog/assets/images/remote-code-execution/9480.png" alt="Corey Larson"/>
</p>
<div>
<p>But – operationally – it is an arbitrary code execution service. I called it that because it’s something we have to contend with, and it scared me a little.</p>
<p>Speaking of which, stay tuned for the next article, which will invariably be about how we are fighting off crypto-miners 😀.</p>
</div>
</div>
</div>
<p>See the <a href="https://jvns.ca/blog/launching-earthly-ci/">release announcement</a> if you’d like to learn more about Earthly CI, which is powered by this service. And stay tuned for more sharing of engineering <del>complaints</del> challenges in the future.</p>

</div></div>
  </body>
</html>
