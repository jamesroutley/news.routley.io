<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/TrackingMachineImportance">Original</a>
    <h1>We might want to regularly keep track of how important each server is</h1>
    
    <div id="readability-page-1" class="page"><div><h2>We might want to regularly keep track of how important each server is</h2>

	<p><small>February  5, 2024</small></p>
</div><div><p>Today <a href="https://mastodon.social/@cks/111881322231361439">we had a significant machine room air conditioning failure
in our main machine room</a>,
one that certainly couldn&#39;t be fixed on the spot (&#39;glycol all over
the roof&#39; is not a phrase you really want to hear about your AC&#39;s
chiller). To keep the machine room&#39;s temperature down, we had to
power off as many machines as possible without too badly affecting
the services we offer to people here, <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/OurDifferentSysadminEnvironment">which are rather varied</a>. Some choices were obvious; all
of <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/SlurmHowWeUseIt">our SLURM nodes</a> that were in the main machine
room got turned off right away. But others weren&#39;t things we
necessarily remembered right away or we weren&#39;t clear if they were
safe to turn off and what effects it would have. In the end we took
several rounds of turning servers off, looking at what was left,
spotting remaining machines, and turning more things off, and we&#39;re
probably not done yet.</p>

<p>(We have secondary machine room space and we&#39;re probably going to
have to evacuate servers into it, too.)</p>

<p>One thing we could do to avoid this flailing in the future is to
explicitly (try to) keep track of which machines are important and
which ones aren&#39;t, to pre-plan which machines we could shut down
if we had a limited amount of cooling or power. If we documented
this, we could avoid having to wrack our brains at the last minute
and worry about dependencies or uses that we&#39;d forgotten. Of course
documentation isn&#39;t free; there&#39;s an ongoing amount of work to write
it and keep it up to date. But possibly we could do this work as
part of deploying machines or changing their configurations.</p>

<p>(This would also help identify machines that we didn&#39;t need any
more but hadn&#39;t gotten around to taking out of service, which we
found a couple of in this iteration.)</p>

<p>Writing all of this just in case of further AC failures is probably
not all that great a choice of where to spend our time. But writing
down this sort of thing can often help to clarify how your environment
is connected together in general, including things like what will
probably break or have problems if a specific machine (or service)
is out, and perhaps which people depend on what service. This can
be valuable information in general. The machine room archaeology
of &#39;what is this machine, why is it on, and who is using it&#39; can
be fun occasionally, but you probably don&#39;t want to do it regularly.</p>

<p>(Will we actually do this? I suspect not. When we deploy and start
using a machine its purpose and so on feel obvious, because we have
all of the context.)</p>
</div></div>
  </body>
</html>
