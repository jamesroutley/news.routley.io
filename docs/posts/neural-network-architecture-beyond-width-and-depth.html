<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2205.09459">Original</a>
    <h1>Neural Network Architecture Beyond Width and Depth</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2205.09459">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  This paper proposes a new neural network architecture by introducing an
additional dimension called height beyond width and depth. Neural network
architectures with height, width, and depth as hyper-parameters are called
three-dimensional architectures. It is shown that neural networks with
three-dimensional architectures are significantly more expressive than the ones
with two-dimensional architectures (those with only width and depth as
hyper-parameters), e.g., standard fully connected networks. The new network
architecture is constructed recursively via a nested structure, and hence we
call a network with the new architecture nested network (NestNet). A NestNet of
height $s$ is built with each hidden neuron activated by a NestNet of height
$\le s-1$. When $s=1$, a NestNet degenerates to a standard network with a
two-dimensional architecture. It is proved by construction that height-$s$ ReLU
NestNets with $\mathcal{O}(n)$ parameters can approximate $1$-Lipschitz
continuous functions on $[0,1]^d$ with an error $\mathcal{O}(n^{-(s+1)/d})$,
while the optimal approximation error of standard ReLU networks with
$\mathcal{O}(n)$ parameters is $\mathcal{O}(n^{-2/d})$. Furthermore, such a
result is extended to generic continuous functions on $[0,1]^d$ with the
approximation error characterized by the modulus of continuity. Finally, we use
numerical experimentation to show the advantages of the super-approximation
power of ReLU NestNets.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Shijun Zhang [<a href="https://arxiv.org/show-email/9bcf96e6/2205.09459">view email</a>]
      </p></div></div>
  </body>
</html>
