<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.domluna.com/grokking/">Original</a>
    <h1>Grokking</h1>
    
    <div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/Article"><header><h3><a href="https://www.domluna.com/">/</a></h3><p>March 21, 2023<!-- --> </p></header><section itemprop="articleBody"><p>All the code can be found <a href="https://github.com/domluna/TransformerCircuits.jl/blob/main/scratch/grokking.jl">here</a>.</p>
<p>The <a href="https://arxiv.org/abs/2201.02177">&#34;Grokking Paper&#34;</a> is one of the most head-scratching papers to come out in the neural network space. It explores the phenomenon of a regime change, whereby the model appears, by all indications, to have overfit the data, and that it&#39;s only being exacerbated as training progresses. Validation loss is increasing, and validation accuracy is at a standstill. Meanwhile, 100% training accuracy was hit ages ago. But then, all of a sudden, as if a divine entity itself sprinkled fairy dust on the neural network, validation loss begins to decrease and validation accuracy increases. After a while, both accuracies are at 100%. In essence, the neural network is transitioning from a regime of memorization to generalization.</p>
<h3>Experiment Setup</h3>
<p>The datasets are of the form:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>∘</mo><mi>b</mi><mo>=</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">a \circ b = c</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span><span></span><span>∘</span><span></span></span><span><span></span><span>b</span><span></span><span>=</span><span></span></span><span><span></span><span>c</span></span></span></span></span></p>
<p>where <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∘</mo></mrow><annotation encoding="application/x-tex">\circ</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∘</span></span></span></span></span> can be an arbitrary binary operation with a consistent modulus base. For the experiments this will be 97. For some operations this means the table has <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><msup><mn>7</mn><mn>2</mn></msup><mo>=</mo><mn>9409</mn></mrow><annotation encoding="application/x-tex">97^2 = 9409</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>9</span><span><span>7</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>9409</span></span></span></span></span> entries. Roughly. It could be <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><msup><mn>6</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">96^2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>9</span><span><span>6</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span> or <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><msup><mn>8</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">98^2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>9</span><span><span>8</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span></span>. For others, negative numbers are also possible, so the space would be (assuming [-97, -96, ..., 96, 97]) 195 numbers and roughly 38205 table entries.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>b</span></span></span></span></span>, and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>c</span></span></span></span></span> are placeholders but symbols are used to represent each number.</p>
<div data-language="julia"><pre><code>julia<span>&gt;</span> alphabet <span>=</span> string<span>.</span><span>(</span>vcat<span>(</span><span>&#39;a&#39;</span><span>:</span><span>&#39;z&#39;</span><span>,</span> <span>&#39;A&#39;</span><span>:</span><span>&#39;Z&#39;</span><span>,</span> Char<span>.</span><span>(</span><span>1024</span><span>:</span><span>2048</span><span>)</span><span>)</span><span>)</span>

<span>1077</span><span>-</span>element Vector<span>{</span>String<span>}</span><span>:</span>
 <span>&#34;a&#34;</span>
 <span>&#34;b&#34;</span>
 <span>&#34;c&#34;</span>
 <span>&#34;d&#34;</span>
 ⋮
 <span>&#34;߽&#34;</span>
 <span>&#34;߾&#34;</span>
 <span>&#34;߿&#34;</span>
 <span>&#34;ࠀ&#34;</span>

julia<span>&gt;</span>

julia<span>&gt;</span> modn <span>=</span> <span>97</span>
<span>97</span>

julia<span>&gt;</span>

julia<span>&gt;</span> nums <span>=</span> collect<span>(</span><span>-</span>modn<span>:</span>modn<span>)</span>
<span>195</span><span>-</span>element Vector<span>{</span>Int64<span>}</span><span>:</span>
 <span>-</span><span>97</span>
 <span>-</span><span>96</span>
 <span>-</span><span>95</span>
 <span>-</span><span>94</span>
   ⋮
  <span>94</span>
  <span>95</span>
  <span>96</span>
  <span>97</span>

julia<span>&gt;</span> num2tok <span>=</span> Dict<span>{</span>Int<span>,</span>String<span>}</span><span>(</span><span>)</span>
Dict<span>{</span>Int64<span>,</span> String<span>}</span><span>(</span><span>)</span>

julia<span>&gt;</span> <span>for</span> <span>(</span>i<span>,</span> n<span>)</span> <span>in</span> enumerate<span>(</span>nums<span>)</span>
           num2tok<span>[</span>n<span>]</span> <span>=</span> alphabet<span>[</span>i<span>]</span>
       <span>end</span>

julia<span>&gt;</span> num2tok
Dict<span>{</span>Int64<span>,</span> String<span>}</span> with <span>195</span> entries<span>:</span>
  <span>56</span>  <span>=</span><span>&gt;</span> <span>&#34;ѥ&#34;</span>
  <span>35</span>  <span>=</span><span>&gt;</span> <span>&#34;ѐ&#34;</span>
  <span>60</span>  <span>=</span><span>&gt;</span> <span>&#34;ѩ&#34;</span>
  <span>-</span><span>5</span>  <span>=</span><span>&gt;</span> <span>&#34;Ш&#34;</span>
  <span>67</span>  <span>=</span><span>&gt;</span> <span>&#34;Ѱ&#34;</span>
  <span>73</span>  <span>=</span><span>&gt;</span> <span>&#34;Ѷ&#34;</span>
  <span>-</span><span>66</span> <span>=</span><span>&gt;</span> <span>&#34;F&#34;</span>
  <span>-</span><span>71</span> <span>=</span><span>&gt;</span> <span>&#34;A&#34;</span>
  ⋮   <span>=</span><span>&gt;</span> ⋮

julia<span>&gt;</span>

julia<span>&gt;</span> tok2num <span>=</span> Dict<span>(</span>values<span>(</span>num2tok<span>)</span> <span>.</span><span>=</span><span>&gt;</span> keys<span>(</span>num2tok<span>)</span><span>)</span>
Dict<span>{</span>String<span>,</span> Int64<span>}</span> with <span>195</span> entries<span>:</span>
  <span>&#34;Z&#34;</span> <span>=</span><span>&gt;</span> <span>-</span><span>46</span>
  <span>&#34;ф&#34;</span> <span>=</span><span>&gt;</span> <span>23</span>
  <span>&#34;Л&#34;</span> <span>=</span><span>&gt;</span> <span>-</span><span>18</span>
  <span>&#34;C&#34;</span> <span>=</span><span>&gt;</span> <span>-</span><span>69</span>
  <span>&#34;т&#34;</span> <span>=</span><span>&gt;</span> <span>21</span>
  <span>&#34;r&#34;</span> <span>=</span><span>&gt;</span> <span>-</span><span>80</span>
  <span>&#34;л&#34;</span> <span>=</span><span>&gt;</span> <span>14</span>
  <span>&#34;ѱ&#34;</span> <span>=</span><span>&gt;</span> <span>68</span>
  ⋮   <span>=</span><span>&gt;</span> ⋮</code></pre></div>
<p>And so the neural network will never see an actual number just the symbol that represents it.</p>
<p>The entire code for generating a dataset:</p>
<div data-language="julia"><pre><code><span>function</span> create_dataset_binop_with_mod<span>(</span>f<span>::</span>Function<span>,</span> modn<span>::</span>Int<span>)</span>
    nums <span>=</span> collect<span>(</span><span>-</span>modn<span>:</span>modn<span>)</span>
    num2tok <span>=</span> Dict<span>{</span>Int<span>,</span>String<span>}</span><span>(</span><span>)</span>
    <span>for</span> <span>(</span>i<span>,</span> n<span>)</span> <span>in</span> enumerate<span>(</span>nums<span>)</span>
        num2tok<span>[</span>n<span>]</span> <span>=</span> alphabet<span>[</span>i<span>]</span>
    <span>end</span>

    tok2num <span>=</span> Dict<span>(</span>values<span>(</span>num2tok<span>)</span> <span>.</span><span>=</span><span>&gt;</span> keys<span>(</span>num2tok<span>)</span><span>)</span>
    toks <span>=</span> collect<span>(</span>values<span>(</span>num2tok<span>)</span><span>)</span>
    push<span>!</span><span>(</span>toks<span>,</span> <span>&#34;=&#34;</span><span>)</span>
    push<span>!</span><span>(</span>toks<span>,</span> <span>&#34;∘&#34;</span><span>)</span>
    tok2idx <span>=</span> Dict<span>(</span>c <span>=</span><span>&gt;</span> i <span>for</span> <span>(</span>i<span>,</span> c<span>)</span> <span>in</span> enumerate<span>(</span>toks<span>)</span><span>)</span>
    idx2tok <span>=</span> Dict<span>(</span>i <span>=</span><span>&gt;</span> c <span>for</span> <span>(</span>i<span>,</span> c<span>)</span> <span>in</span> enumerate<span>(</span>toks<span>)</span><span>)</span>

    data <span>=</span> Vector<span>{</span>Int<span>}</span><span>[</span><span>]</span>
    <span>for</span> a <span>in</span> <span>1</span><span>:</span>modn
        <span>for</span> b <span>in</span> <span>1</span><span>:</span>modn
            c <span>=</span> f<span>(</span>a<span>,</span> b<span>)</span>
            
            
            s <span>=</span> <span>&#34;$(num2tok[a])∘$(num2tok[b])=$(num2tok[c])&#34;</span>
            
            enc <span>=</span> <span>[</span>tok2idx<span>[</span>string<span>(</span>c<span>)</span><span>]</span> <span>for</span> c <span>in</span> s<span>]</span>
            push<span>!</span><span>(</span>data<span>,</span> enc<span>)</span>
        <span>end</span>
    <span>end</span>
    Random<span>.</span>shuffle<span>!</span><span>(</span>data<span>)</span>

    X <span>=</span> zeros<span>(</span>Int<span>,</span> <span>(</span>length<span>(</span>data<span>[</span><span>1</span><span>]</span><span>)</span> <span>-</span> <span>1</span><span>,</span> length<span>(</span>data<span>)</span><span>)</span><span>)</span>
    y <span>=</span> zeros<span>(</span>Int<span>,</span> length<span>(</span>data<span>)</span><span>)</span>
    <span>for</span> <span>(</span>i<span>,</span> enc<span>)</span> <span>in</span> enumerate<span>(</span>data<span>)</span>
        X<span>[</span><span>:</span><span>,</span> i<span>]</span> <span>=</span> enc<span>[</span><span>1</span><span>:</span><span>end</span><span>-</span><span>1</span><span>]</span>
        y<span>[</span>i<span>]</span> <span>=</span> enc<span>[</span><span>end</span><span>]</span>
    <span>end</span>
    <span>return</span> <span>(</span>X<span>,</span> Flux<span>.</span>onehotbatch<span>(</span>y<span>,</span> <span>1</span><span>:</span>length<span>(</span>tok2idx<span>)</span><span>)</span><span>)</span><span>,</span> tok2idx<span>,</span> idx2tok
<span>end</span></code></pre></div>
<p>This is the most important section:</p>
<div data-language="julia"><pre><code>c <span>=</span> f<span>(</span>a<span>,</span> b<span>)</span>


s <span>=</span> <span>&#34;$(num2tok[a])∘$(num2tok[b])=$(num2tok[c])&#34;</span></code></pre></div>
<p>The network only sees &#34;SymbolA∘SymbolB=SymbolC&#34;.</p>
<p>The network itself is a two layer transformer with 128 dimensions split over 4 heads.</p>
<div data-language="julia"><pre><code>vocabsize <span>=</span> size<span>(</span>trainY<span>,</span> <span>1</span><span>)</span>
blocksize <span>=</span> size<span>(</span>trainX<span>,</span> <span>1</span><span>)</span>

dembed <span>=</span> <span>128</span>
nheads <span>=</span> <span>4</span>
nlayers <span>=</span> <span>2</span>
circ <span>=</span> Circuit<span>(</span>vocabsize<span>,</span> blocksize<span>,</span> dembed<span>;</span> nheads<span>,</span> nlayers<span>)</span> <span>|&gt;</span> gpu<span>;</span>
opt <span>=</span> Flux<span>.</span>setup<span>(</span>AdamW<span>(</span><span>3e</span><span>-</span><span>4</span><span>)</span><span>,</span> circ<span>)</span><span>;</span></code></pre></div>
<p>The vocabulary size is the 195 symbols plus 2 extra for <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∘</mo></mrow><annotation encoding="application/x-tex">\circ</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∘</span></span></span></span></span> and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo></mrow><annotation encoding="application/x-tex">=</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>=</span></span></span></span></span>.</p>
<p>The block size is 4 (all tokens before SymbolC).</p>
<h3>Data Split</h3>
<p>Memorization -&gt; generalization is established due to the dataset split. For the experiments we&#39;ll do it&#39;s a 50/50 split. Meaning half of the data the neural network will have never seen. It cannot, by definition memorize it. The only way for it to correctly label the examples in the validation set is to figure out the underlying function being performed. To generalize.</p>
<div data-language="julia"><pre><code>X<span>,</span> Y <span>=</span> data<span>;</span>
trainfrac <span>=</span> <span>0.5</span><span>;</span>
N <span>=</span> size<span>(</span>X<span>,</span> <span>2</span><span>)</span><span>;</span>
n <span>=</span> Int<span>(</span>round<span>(</span>N <span>*</span> trainfrac<span>)</span><span>)</span><span>;</span>
trainX<span>,</span> trainY <span>=</span> X<span>[</span><span>:</span><span>,</span> <span>1</span><span>:</span>n<span>]</span><span>,</span> Y<span>[</span><span>:</span><span>,</span> <span>1</span><span>:</span>n<span>]</span><span>;</span>
valX<span>,</span> valY <span>=</span> X<span>[</span><span>:</span><span>,</span> n<span>+</span><span>1</span><span>:</span>N<span>]</span><span>,</span> Y<span>[</span><span>:</span><span>,</span> n<span>+</span><span>1</span><span>:</span>N<span>]</span><span>;</span>

trainX <span>=</span> trainX <span>|&gt;</span> gpu<span>;</span>
trainY <span>=</span> trainY <span>|&gt;</span> gpu<span>;</span>
valX <span>=</span> valX <span>|&gt;</span> gpu<span>;</span>
valY <span>=</span> valY <span>|&gt;</span> gpu<span>;</span>

train_batchsize <span>=</span> min<span>(</span><span>512</span><span>,</span> size<span>(</span>trainX<span>,</span> <span>2</span><span>)</span><span>)</span>
val_batchsize <span>=</span> min<span>(</span><span>512</span><span>,</span> size<span>(</span>valX<span>,</span> <span>2</span><span>)</span><span>)</span>
traindata <span>=</span> Flux<span>.</span>DataLoader<span>(</span><span>(</span>trainX<span>,</span> trainY<span>)</span><span>,</span> batchsize <span>=</span> train_batchsize<span>,</span> shuffle <span>=</span> <span>true</span><span>)</span><span>;</span>
valdata <span>=</span> Flux<span>.</span>DataLoader<span>(</span><span>(</span>valX<span>,</span> valY<span>)</span><span>,</span> batchsize <span>=</span> val_batchsize<span>)</span><span>;</span></code></pre></div>
<p>In the original paper they run experiments over a variety of functions and split sizes. I&#39;ve picked four functions from the paper I thought would be worthwhile reproducing.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mspace></mspace><mspace width="1em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow><annotation encoding="application/x-tex">a + b \mod n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span><span></span><span>+</span><span></span></span><span><span></span><span>b</span><span></span><span></span></span><span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span></span></p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>−</mo><mi>b</mi><mspace></mspace><mspace width="1em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow><annotation encoding="application/x-tex">a - b \mod n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span><span></span><span>−</span><span></span></span><span><span></span><span>b</span><span></span><span></span></span><span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span></span></p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>a</mi><mo>÷</mo><mi>b</mi><mspace></mspace><mspace width="0.6667em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>i</mi><mi>s</mi><mi>o</mi><mi>d</mi><mi>d</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>a</mi><mo>−</mo><mi>b</mi><mspace></mspace><mspace width="0.6667em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>i</mi><mi>s</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>n</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}
   a ÷ b \mod n &amp;\text{if  } isodd(b) \\
   a - b \mod n &amp;\text{if  } iseven(b)
\end{cases}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>{</span></span><span><span><span><span><span><span><span><span></span><span><span>a</span><span></span><span>÷</span><span></span><span>b</span><span></span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span><span><span></span><span><span>a</span><span></span><span>−</span><span></span><span>b</span><span></span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>if </span></span><span>i</span><span>so</span><span>dd</span><span>(</span><span>b</span><span>)</span></span></span><span><span></span><span><span><span>if </span></span><span>i</span><span>se</span><span>v</span><span>e</span><span>n</span><span>(</span><span>b</span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span></span></span></span></span></span></span></p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>a</mi><mn>3</mn></msup><mo>+</mo><mi>a</mi><mo>∗</mo><msup><mi>b</mi><mn>2</mn></msup><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mspace></mspace><mspace width="1em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow><annotation encoding="application/x-tex">(a^3 + a * b^2 + b) \mod n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>a</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>a</span><span></span><span>∗</span><span></span></span><span><span></span><span><span>b</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>b</span><span>)</span><span></span><span></span></span><span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span></span></p>
<p>Each of these had a 50/50 split except the last one which I also tried with a 95/5 split, as they did in the paper. It failed to generalize in the paper and it failed to generalize for me :(</p>
<p>The results ...</p>
<h3>Run It</h3>
<div data-language="julia"><pre><code>data<span>,</span> _<span>,</span> _ <span>=</span> create_dataset_binop_with_mod<span>(</span><span>(</span>a<span>,</span> b<span>)</span> <span>-</span><span>&gt;</span> <span>(</span>a <span>+</span> b<span>)</span> <span>%</span> modn<span>,</span> modn<span>)</span>

<span>.</span><span>.</span><span>.</span>

run <span>=</span> Run<span>(</span><span>)</span>
evalevery <span>=</span> <span>10</span>
train_model<span>!</span><span>(</span>
    circ<span>,</span>
    opt<span>,</span>
    traindata<span>;</span>
    nepochs <span>=</span> <span>10_000</span><span>,</span>
    evaliters <span>=</span> <span>10</span><span>,</span>
    evalevery <span>=</span> evalevery<span>,</span>
    valdata <span>=</span> valdata<span>,</span>
    seq2val <span>=</span> <span>true</span><span>,</span>
    early_stop <span>=</span> <span>(</span><span>)</span> <span>-</span><span>&gt;</span> <span>begin</span>
        
        accuracy_metric<span>(</span>circ<span>,</span> valdata<span>;</span> seq2val <span>=</span> <span>true</span><span>)</span> <span>&gt;=</span> <span>0.99</span>
    <span>end</span><span>,</span>
    run <span>=</span> run<span>,</span>
<span>)</span></code></pre></div>
<p>Nothing out of the ordinary here. <code>evaliters</code> defines the interval which the loss and accuracies for the train and validation data should be captured. Every 10 epochs.</p>
<p><code>seq2val</code> means we only care about the loss for the last token, rather than <code>seq2seq</code>, which would be the loss for token prediction in the entire sequence.</p>
<div data-language="julia"><pre><code>    f <span>=</span> <span>if</span> seq2val
        <span>(</span>m<span>,</span> x<span>,</span> y<span>)</span> <span>-</span><span>&gt;</span> Flux<span>.</span>Losses<span>.</span>crossentropy<span>(</span>softmax<span>(</span>m<span>(</span>x<span>)</span><span>,</span> dims <span>=</span> <span>1</span><span>)</span><span>[</span><span>:</span><span>,</span> <span>end</span><span>,</span> <span>:</span><span>]</span><span>,</span> y<span>)</span>
    <span>else</span>
        <span>(</span>m<span>,</span> x<span>,</span> y<span>)</span> <span>-</span><span>&gt;</span> Flux<span>.</span>Losses<span>.</span>crossentropy<span>(</span>softmax<span>(</span>m<span>(</span>x<span>)</span><span>,</span> dims <span>=</span> <span>1</span><span>)</span><span>,</span> y<span>)</span>
    <span>end</span></code></pre></div>
<blockquote>
<p>In Julia arrays are column ordered so the batch and sequence dimensions would be the final two - <code>(..., ..., sequence_dim, batch_dim)</code>. This is the opposite of PyTorch.</p>
</blockquote>
<p>If you read my notes in the <a href="https://github.com/domluna/TransformerCircuits.jl/blob/main/scratch/grokking.jl"><code>grokking.jl</code></a> you&#39;ll see I originally had seq2seq loss, mainly because I was working with seq2seq data before but also because of I was too lazy to change the loss metric. It does hurt the model in this case because it&#39;s not useful for it predict any of the tokens besides the final one.</p>
<p>Given the first three tokens you will not be able to predict the fourth token. It could be any of the number symbols (excludes <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo></mrow><annotation encoding="application/x-tex">=</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>=</span></span></span></span></span> and <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∘</mo></mrow><annotation encoding="application/x-tex">\circ</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∘</span></span></span></span></span>) will equal probability!</p>
<p>Anyway - back to modular addition.</p>
<p>Plots, plots, plots.</p>
<h3>Addition</h3>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mspace></mspace><mspace width="1em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow><annotation encoding="application/x-tex">a + b \mod n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span><span></span><span>+</span><span></span></span><span><span></span><span>b</span><span></span><span></span></span><span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span></span></p>

<p><span>
      <a href="https://www.domluna.com/static/a2d07a8691717abf8fc002042dfd8c9d/0a47e/addition_97_loss2.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Addition Loss" title="" src="https://www.domluna.com/static/a2d07a8691717abf8fc002042dfd8c9d/0a47e/addition_97_loss2.png" srcset="/static/a2d07a8691717abf8fc002042dfd8c9d/c26ae/addition_97_loss2.png 158w,
/static/a2d07a8691717abf8fc002042dfd8c9d/6bdcf/addition_97_loss2.png 315w,
/static/a2d07a8691717abf8fc002042dfd8c9d/0a47e/addition_97_loss2.png 600w" sizes="(max-width: 600px) 100vw, 600px" loading="lazy" decoding="async"/>
  </a>
    </span>
</p>
<p>Total Optimization steps are defined as epochs * num_batches * evalevery. So addition generalizes pretty quick but the regime change from memorization to generalization is evident.</p>
<h3>Subtraction</h3>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>−</mo><mi>b</mi><mspace></mspace><mspace width="1em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow><annotation encoding="application/x-tex">a - b \mod n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span><span></span><span>−</span><span></span></span><span><span></span><span>b</span><span></span><span></span></span><span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span></span></p>

<p><span>
      <a href="https://www.domluna.com/static/4a1f6b6d59ea34103c67f58e601a410a/0a47e/subtraction_97_loss.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Subtraction Loss" title="" src="https://www.domluna.com/static/4a1f6b6d59ea34103c67f58e601a410a/0a47e/subtraction_97_loss.png" srcset="/static/4a1f6b6d59ea34103c67f58e601a410a/c26ae/subtraction_97_loss.png 158w,
/static/4a1f6b6d59ea34103c67f58e601a410a/6bdcf/subtraction_97_loss.png 315w,
/static/4a1f6b6d59ea34103c67f58e601a410a/0a47e/subtraction_97_loss.png 600w" sizes="(max-width: 600px) 100vw, 600px" loading="lazy" decoding="async"/>
  </a>
    </span>
</p>
<p>Generalization takes much longer than addition.</p>
<h3>Asymmetric Function</h3>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>a</mi><mo>÷</mo><mi>b</mi><mspace></mspace><mspace width="0.6667em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>i</mi><mi>s</mi><mi>o</mi><mi>d</mi><mi>d</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>a</mi><mo>−</mo><mi>b</mi><mspace></mspace><mspace width="0.6667em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>i</mi><mi>s</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>n</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}
   a ÷ b \mod n &amp;\text{if  } isodd(b) \\
   a - b \mod n &amp;\text{if  } iseven(b)
\end{cases}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>{</span></span><span><span><span><span><span><span><span><span></span><span><span>a</span><span></span><span>÷</span><span></span><span>b</span><span></span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span><span><span></span><span><span>a</span><span></span><span>−</span><span></span><span>b</span><span></span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>if </span></span><span>i</span><span>so</span><span>dd</span><span>(</span><span>b</span><span>)</span></span></span><span><span></span><span><span><span>if </span></span><span>i</span><span>se</span><span>v</span><span>e</span><span>n</span><span>(</span><span>b</span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span></span></span></span></span></span></span></p>

<p><span>
      <a href="https://www.domluna.com/static/4eaafe2c50436778cc910a77155784c8/0a47e/unsymmetric_odd_div_even_sub_97_loss.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Asymmetric Loss" title="" src="https://www.domluna.com/static/4eaafe2c50436778cc910a77155784c8/0a47e/unsymmetric_odd_div_even_sub_97_loss.png" srcset="/static/4eaafe2c50436778cc910a77155784c8/c26ae/unsymmetric_odd_div_even_sub_97_loss.png 158w,
/static/4eaafe2c50436778cc910a77155784c8/6bdcf/unsymmetric_odd_div_even_sub_97_loss.png 315w,
/static/4eaafe2c50436778cc910a77155784c8/0a47e/unsymmetric_odd_div_even_sub_97_loss.png 600w" sizes="(max-width: 600px) 100vw, 600px" loading="lazy" decoding="async"/>
  </a>
    </span>
</p>
<p>I stopped this early at around 95% validation accuracy just because it was taking so longbut. Validation loss and accuracy were going down the entire duration after the regime change.</p>
<h3>Hard Function</h3>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>a</mi><mn>3</mn></msup><mo>+</mo><mi>a</mi><mo>∗</mo><msup><mi>b</mi><mn>2</mn></msup><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mspace></mspace><mspace width="1em"></mspace><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>n</mi></mrow><annotation encoding="application/x-tex">(a^3 + a * b^2 + b) \mod n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>a</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>a</span><span></span><span>∗</span><span></span></span><span><span></span><span><span>b</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>b</span><span>)</span><span></span><span></span></span><span><span></span><span><span><span>mod</span></span></span><span></span><span></span><span>n</span></span></span></span></span></p>

<p><span>
      <a href="https://www.domluna.com/static/46b97c4dd175fa0ce10fc02e91e857b3/0a47e/hardfunction_97_loss.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Hard Function Loss" title="" src="https://www.domluna.com/static/46b97c4dd175fa0ce10fc02e91e857b3/0a47e/hardfunction_97_loss.png" srcset="/static/46b97c4dd175fa0ce10fc02e91e857b3/c26ae/hardfunction_97_loss.png 158w,
/static/46b97c4dd175fa0ce10fc02e91e857b3/6bdcf/hardfunction_97_loss.png 315w,
/static/46b97c4dd175fa0ce10fc02e91e857b3/0a47e/hardfunction_97_loss.png 600w" sizes="(max-width: 600px) 100vw, 600px" loading="lazy" decoding="async"/>
  </a>
    </span>
</p>
<p>Even with a 95/5 split we get nowhere.</p>
<h3>Extra: Subtraction then Finedtuned Asymmetric</h3>
<p>I took the subtraction model once it achieved high generalization and then attempted to finetune it on the asymmetric dataset. It did not work.</p>
<p><span>
      <a href="https://www.domluna.com/static/a8aeeee8b3221b173e2a02bf3c16e370/0a47e/subtraction_finetuned_asymmetric_loss.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Loss" title="" src="https://www.domluna.com/static/a8aeeee8b3221b173e2a02bf3c16e370/0a47e/subtraction_finetuned_asymmetric_loss.png" srcset="/static/a8aeeee8b3221b173e2a02bf3c16e370/c26ae/subtraction_finetuned_asymmetric_loss.png 158w,
/static/a8aeeee8b3221b173e2a02bf3c16e370/6bdcf/subtraction_finetuned_asymmetric_loss.png 315w,
/static/a8aeeee8b3221b173e2a02bf3c16e370/0a47e/subtraction_finetuned_asymmetric_loss.png 600w" sizes="(max-width: 600px) 100vw, 600px" loading="lazy" decoding="async"/>
  </a>
    </span>
</p>
<h3>Value Counts</h3>
<p>I&#39;m not sure how useful this is these visualizations show a correlation between symmetry and the ability to generalize.</p>
<p><span>
      <a href="https://www.domluna.com/static/d6610595fca1e941e8faf8907420f7b7/0a47e/addition_97_counts.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Addition Counts" title="" src="https://www.domluna.com/static/d6610595fca1e941e8faf8907420f7b7/0a47e/addition_97_counts.png" srcset="/static/d6610595fca1e941e8faf8907420f7b7/c26ae/addition_97_counts.png 158w,
/static/d6610595fca1e941e8faf8907420f7b7/6bdcf/addition_97_counts.png 315w,
/static/d6610595fca1e941e8faf8907420f7b7/0a47e/addition_97_counts.png 600w" sizes="(max-width: 600px) 100vw, 600px" loading="lazy" decoding="async"/>
  </a>
    </span>
</p>
<p>Even the hard function does have symmetry. Hmmmmm.</p>
<h3>Next Steps</h3>
<p>These neural networks are fairly small so dissecting them could be worthwhile.</p></section><hr/></article></div>
  </body>
</html>
