<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.scattered-thoughts.net/writing/a-shallow-survey-of-olap-and-htap-query-engines/">Original</a>
    <h1>A shallow survey of OLAP and HTAP query engines</h1>
    
    <div id="readability-page-1" class="page"><article>
  <p>Focused mostly on data layout and query execution. Query planning seems more or less the same as OLTP systems, and I&#39;m ignoring distribution and transactions for now. Also see my full notes <a href="https://macwright.com/log/0040">here</a>.</p>
<p>It was hard to figure out what systems are even worth studying. There is so much money in this space. Search results are polluted with barely concealed advertising (eg &#34;How to choose between FooDB and BarDB&#34; hosted on foodb.com) and benchmarketing. Third-party benchmarks are crippled by that fact that most databases TOS prohibit publishing benchmarks. Besides which, benchmarking databases is notoriously error-prone.</p>
<p>Luckily Andy Pavlo decided to focus on teaching OLAP databases this year, <a href="https://15721.courses.cs.cmu.edu/spring2023/schedule.html">covering</a>:</p>
<ul>
<li><a href="https://www.databricks.com/product/data-lakehouse">Databricks (Photon/DeltaLake)</a></li>
<li><a href="https://www.snowflake.com/en/">Snowflake</a></li>
<li><a href="https://duckdb.org/">DuckDB</a></li>
<li><a href="https://velox-lib.io/">Velox</a></li>
<li><a href="https://aws.amazon.com/redshift/">Redshift</a></li>
</ul>
<p>On top of that I added, arbitrarily:</p>
<ul>
<li><a href="https://www.hyper-db.de/">HyPer</a> and <a href="https://umbra-db.com/">Umbra</a></li>
<li><a href="https://www.singlestore.com/">SingleStore</a></li>
</ul>
<p>And also some diversions into the ecosystems around <a href="https://arrow.apache.org/">Arrow</a> / <a href="https://parquet.apache.org/">Parquet</a> / <a href="https://orc.apache.org/">ORC</a>.</p>
<p>Notably missing are:</p>
<ul>
<li><a href="https://www.microsoft.com/en-ca/sql-server/sql-server-2022">SQL Server</a> and <a href="https://www.sap.com/products/technology-platform/hana/what-is-sap-hana.html">SAP Hana</a> - I couldn&#39;t find recent (ie &lt;5 years) material on either.</li>
<li><a href="https://clickhouse.com/">Clickhouse</a> - not much material available in English.</li>
<li><a href="https://docs.pingcap.com/tidb/stable">TiDB</a> - I couldn&#39;t find details about data layout or query execution (eg <a href="https://pingcap.github.io/tidb-dev-guide/understand-tidb/implementation-of-vectorized-execution.html#implementation-of-vectorized-execution">this empty page</a>).</li>
<li><a href="https://cloud.google.com/alloydb">AlloyDB</a> - not much public material yet.</li>
<li><a href="https://github.com/apache/arrow-datafusion">DataFusion</a> and <a href="https://www.pola.rs/">Polars</a> - I&#39;m assuming these won&#39;t be very different to Velox.</li>
</ul>
<p>Even among the systems studied, many are proprietary and publish only the highest-level overview of their architecture. Enough info for marketing material but often missing crucial implementation details like how operators are organized for parallel execution.</p>
<p>The term OLAP covers a huge variety of different workloads, from 1000-node clusters crunching hourly reports over exabytes of data, to scientists interactively examinining gigabytes of data on a laptop. But there is surprisingly little variation between these systems at the level of data layout and execution. It&#39;s not clear whether this is because they&#39;re all trying to maximise bandwidth on similar hardware and this is the emergent best solution, or because they&#39;re all just copying ideas from a few influential systems. But the current investment into reusable execution engines like Velox/DataFusion/Polars and open data formats like Arrow/Parquet/ORC suggests that the workloads really might be that similar.</p>
<h2 id="cold-vs-hot-data">cold vs hot data</h2>
<p>The tradeoffs driving data layouts vary in different parts of a database system. As a rough approximation we can divide the problem into:</p>
<ul>
<li>Cold/far data. Probably sitting on disk or in some blob store like S3. Optimized for space usage to reduce storage and bandwidth costs. Typically immutable. Almost always column-based.</li>
<li>Hot/near data. About to be queried, or being passed between operators in a query plan. We still care about space usage but also need to support random access. Mostly column-based, except for state inside row-centric operators like joins.</li>
</ul>
<h2 id="cold-pax">cold pax</h2>
<p>For cold data everyone does some variant of <a href="https://www.vldb.org/conf/2001/P169.pdf">PAX layout</a>. Rows are grouped together and then within each row group the values are grouped by column. </p>
<p>The standard argument (eg in the orginal paper and on Andy&#39;s slides) is that this provides the asymptotic benefits of a column-store while still preserving some spatial locality when materializing rows. I don&#39;t think this argument applies to any of these OLAP system though. To maximise IO bandwidth we want each column within the row group to be a multiple of the IO block size (4kb for disk, effectively ~8mb for S3) so that we can read data from that column without wasting bandwidth on data from any other column. But this means that reconstructing a single row will always require one block read per column. We can&#39;t get better locality without hurting IO bandwidth for column scans.</p>
<p>If we look at row group sizes in practice:</p>
<ul>
<li>SingleStore defaults to 1m rows per group (<a href="https://docs.singlestore.com/db/v7.3/create-a-database/configuring-the-columnstore-to-work-effectively/#configuring-segment-size-in-columnstore-tables">ref</a>).</li>
<li>Parquet (used in Delta Lake) recommends row groups of 512mb-1gb (<a href="https://parquet.apache.org/docs/file-format/configurations/">ref</a>).</li>
<li>Duckdb uses 120k rows per group (<a href="https://15721.courses.cs.cmu.edu/spring2023/slides/22-duckdb.pdf">ref</a> p38).</li>
<li>Data Blocks (used in Umbra&#39;s predecessor) supports up to 64k rows per group (<a href="https://db.in.tum.de/downloads/publications/datablocks.pdf">ref</a> 3.1). BtrBlocks (from the same department) also uses 64k rows per group (<a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf">ref</a> 3.1).</li>
<li>I couldn&#39;t find any hard details for Snowflake, but for bulk data loading they recommend files of at least 100mb to avoid write amplification, which suggests their internal row groups might be in that order of magnitude (<a href="https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare#continuous-data-loads-i-e-snowpipe-and-file-sizing">ref</a>).</li>
</ul>
<p>These are all significant multiples of the IO block size for those systems so there is no meaningful IO locality for rows. So why use row groups instead of storing whole columns?</p>
<p>OLAP databases rely on massively parallel table scans. If we break a column into many self-contained individually-compressed chunks then each worker can operate independently rather than needing a shared IO cache and scheduler to manage access to eg large compression dictionaries. But for queries that scan multiple columns each worker needs to grab chunks for the same set of rows. So we should make sure that we chunk each column in the same way ie by row group.</p>
<p>So far nothing about this requires that all the chunks for one row group need to be stored together. But consider a query like <code>delete from likes where likes.user_id == 42</code>. We scan the <code>user_id</code> to get a list of rows that we need to delete. But what is in that list? How do we identify a row? If we store all the chunks for a row group in the same block, then we can identify a row with <code>(block_address, row_index)</code>. But if we store chunks in separate blocks then we need a secondary index structure <code>row_group_id -&gt; [block_address]</code> to locate rows. Besides which, all the chunks for a given row group will be created at the same time and (to maintain the same layouts across columns) compacted at the same time. So storing chunks separately provides no advantages and adds some amount of annoying metadata and coordination.</p>
<p>Another way to think about this: Some early column-stores stored explicit row ids in each column. This allowed each column to have a different layout, but wasted a <em>lot</em> of space. Other early column stores referred to rows by index. This saved space but limited columns to layouts which support efficient random access, making efficient updates difficult. We can think about PAX as a hybrid of these two schemes - store explicit ids per row group but then use indexes within each row group, gaining most of the benefits of explicit ids while significantly amortizing their storage cost.</p>
<p>Often each row group will carry some small metadata to allow skipping blocks during scans. I haven&#39;t looked much into this yet.</p>
<h2 id="hot-pax">hot pax</h2>
<p>For hot data we still see PAX-like layouts, but with much smaller row groups because we want entire query pipelines (see below) to fit in cpu cache. I couldn&#39;t find actual sizes for most of the systems, but:</p>
<ul>
<li>DuckDB defaults to max 2k rows per group (<a href="https://github.com/duckdb/duckdb/blob/5a29c99891dcc71d19ce222ee3a68bf08686680e/src/include/duckdb/common/vector_size.hpp#L17">ref</a>) and uses an inter-operator cache to ensure groups have at least 64 rows (<a href="https://youtu.be/bZOvAKGkzpQ?t=2448">ref</a>).</li>
<li>In isolated experiments Kersten et al report the best results with 1k-16k rows per group (<a href="https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf">ref</a> fig 5).</li>
</ul>
<p>We also need to be able to append new values and even mutate values (eg for sorting). This requires slightly modifying the formats used for cold data. Eg for strings, cold data can just concatenate all the strings and store a list of lengths, but hot data will want to also store a list of offsets to allow for changing the order without having to rewrite the concatenated strings.</p>
<p>It seems common to switch to row-based formats for state within operators where the access patterns are row-centric (join, group-by). I guess it&#39;s better to pay the conversion cost once when building the hash-table rather than paying for poor locality on every lookup and resize?</p>
<h2 id="generic-vs-specialized-compression">generic vs specialized compression</h2>
<p>It&#39;s not unusual to see compression ratios of 4x or better for OLAP data. Cutting your storage and bandwidth costs by 4x is way too attractive to ignore so everyone compresses cold data.</p>
<p>Compression algorithms can be split into generic and specialized. Generic algorithms (eg LZ4) work on arbitrary bytestreams. Specialized algorithms (eg dictionary encoding) require some understanding of the structure of the data, and may only apply to certain data types.</p>
<p>Generic algorithms require decompressing the entire block before using values. Some specialized algorithms (eg dictionary encoding) still allow random access to individual values. Others (eg run-length encoding) only allow streaming access to values.</p>
<p>The data within a row group must be split into blocks before compression. Choosing larger blocks gives compression algorithms more opportunities, leading to better compression ratios. But for algorithms which don&#39;t allow random access, larger blocks mean that we have to do more decompression work before accessing an individual row. On the other hand most systems store metadata to allow skipping blocks during scans - smaller blocks mean more fine-grained metadata which leads to more metadata checks but also allow more opportunities to skip blocks.</p>
<p>Allowing each block to use different specialized algorithms depending on the data distribution improves compression ratios, compared to picking a single algorithm per column.</p>
<p>Most specialized algorithms also allow executing some filter operators without first decompressing the data. For example, to execute <code>... where column = value</code> over a dictionary-encoded column, we first lookup <code>value</code> in the dictionary. If it&#39;s not present then we can skip the entire block. If it is present then we can scan the column for the integer code corresponding to <code>value</code> without doing any more dictionary lookups. This is dramatically faster than decompressing and then scanning the decompressed data, and also more amenable to simd.</p>
<p>For cold data most systems apply both one or more specialized algorithms and then some generic algorithm afterwards. But the HTAP systems SingleStore and HyPer avoid generic algorithms - they want to preserve random access so they can build efficient secondary indexes over their cold data.</p>
<p>For hot data most systems use some subset of specialized algorithms which allow operating on encoded data. It&#39;s not clear if this applies past the initial scan - is it worth compressing the output of an operator that transforms its input? I imagine not, but I haven&#39;t dug through any code to check if anyone does this.</p>
<p>The recent <a href="https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/papers/btrblocks.pdf">BtrBlocks</a> paper claims that by stacking specialized compression algorithms it&#39;s possible to get similar compression ratios to generic compression but with substantially better decompression bandwidth. Their benchmarks are intended to be reproducible - it would be interesting to rerun them with the subset of algorithms that preserve random access to see how much compression ratios suffer.</p>
<h2 id="nested-and-heterogenous-data">nested and heterogenous data</h2>
<p>For nested data (arrays, structs etc), some systems only support dynamically typed blobs:</p>
<ul>
<li>Snowflake has <a href="https://docs.snowflake.com/en/sql-reference/data-types-semistructured">VARIANT/OBJECT/ARRAY</a> types which are stored as binary blobs. The runtime analyzes the blobs to infer types and extract columnar views.</li>
<li>Redshift has a <a href="https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html">SUPER</a> type which is stored as a binary blob. It doesn&#39;t seem to support equality comparisons on this type (<a href="https://docs.aws.amazon.com/redshift/latest/dg/limitations-super.html">ref</a>), which seems limiting.</li>
</ul>
<p>Other systems require static types but can use columnar encodings:</p>
<ul>
<li>Parquet (and hence databricks) supports nested types using the <a href="https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper">format described in dremel</a>.</li>
<li>Velox and DuckDB <a href="https://facebookincubator.github.io/velox/develop/vectors.html#arrayvector">decompose structs/arrays into nested columns</a>.</li>
</ul>
<p>SingleStore uses a <a href="https://docs.singlestore.com/db/v8.0/create-a-database/columnstore/columnstore-seekablity/">hybrid layout</a>, with common paths being stored in separate columns and rare paths store in a single blob. Umbra makes brief mention of supporting json values but I can&#39;t find details.</p>
<p>Worth noting that the dremel format makes random access very expensive - if we want to evaluate a single path like <code>docs[42].links.backwards[3]</code> I think we have to stream through all the records in a block. On the other hand, it&#39;s algebraically closed - every operator consumes and produces data in the same format without ever needing record reassembly. </p>
<p>In the velox/duckdb format, I&#39;m not totally clear how we go from a filter scan on a nested column to a selection vector on a higher level eg <code>select doc where exists(select back from doc.links.backwards where startsWith(back, &#34;http://&#34;))</code>. We essentially need to do a reverse lookup on the offset/length column for each entry in the selection vector.</p>
<p>I couldn&#39;t find any data formats which support heterogenous columnar data (ie sum types) other than zed&#39;s <a href="https://zed.brimdata.io/docs/formats/vng">vng</a>. But sum types can be emulated in parquet/velox/duckdb using a struct with a tag field and multiple nullable payload fields.</p>
<pre><code><span>// intention
</span><span>Event = enum {
</span><span>  a: A,
</span><span>  b: B,
</span><span>  c: C,
</span><span>}
</span><span>
</span><span>// implementation
</span><span>Event = struct {
</span><span>  tag: enum { a, b, c},
</span><span>  a: ?A,
</span><span>  b: ?B,
</span><span>  c: ?C,
</span><span>}
</span></code></pre>
<p>Given sufficiently good encodings for nullable columns this might not be terrible?</p>
<h2 id="parallelism">parallelism</h2>
<p>The cloud databases are all pretty cagey about how they parallelize queries. I couldn&#39;t figure out anything about Snowflake, Redshift or SingleStore. </p>
<p>Photon seems to rely on the shuffle-based mechanism it inherits from spark.  The core idea for shuffle-based parallelism is that the data is partitioned by key and each operator only has to care about it&#39;s own partition. When the output of an operator has a different key from the input, a shuffle redistributes data across workers. I haven&#39;t looked at their implementation in particular but in older systems this design could make it difficult to balance work. If the data distribution is skewed, some of the workers will have way too much work and will lag behind. ALso depending on the design, all the operators before a shuffle might have to finish before the operators after the shuffle can start, in which case a single slow worker can hold up the whole query.</p>
<p>The single-node systems DuckDB (<a href="https://youtu.be/bZOvAKGkzpQ?t=2109">ref</a>), Velox (<a href="https://facebookincubator.github.io/velox/develop/task.html">ref</a>), HyPer and Umbra all use (more or less) the same high-level architecture described in <a href="https://db.in.tum.de/%7Eleis/papers/morsels.pdf">Morsel-Driven Parallelism</a>. Query plans are trees of operators, through which rows flow. </p>
<p><img src="https://macwright.com/writing/a-shallow-survey-of-olap-and-htap-query-engines/pipelines.png" alt=""/></p>
<p>Some operators (eg map, filter) can be executed in streaming fashion. Others (eg join, group) have to wait until one or more of their inputs has supplied all it&#39;s data before they can produce output. We can split the plan into pipelines (the dotted lines), each of which starts with a source that produces input rows and finishes with a sink that consumes output rows.</p>
<p>Each worker thread repeatedly: </p>
<ul>
<li>Asks a sink for some input rows.</li>
<li>Passes those rows through all the streaming operators in the pipeline.</li>
<li>Accumulates the output rows in the sink. </li>
</ul>
<p>This means that only the source and sink operators need to be aware of parallelism. When the source is a scan each worker thread can do their own independent IO and they only need to coordinate on which byte ranges they read. Often the sink is building up some per-thread state which will be combined when the pipeline is finished, in which case there is no coodination cost for accumulating output rows at the sink.</p>
<p>For multi-node systems some kind of inter-machine communication and scheduling is still needed around sinks, but I don&#39;t need to think about that yet.</p>
<h2 id="push-vs-pull">push vs pull</h2>
<p>Some operators don&#39;t need all of their input to be computed at all. Eg <code>... limit 10</code> or <code>... where exists (...)</code>. We need some way for them to signal when they have seen enough data. </p>
<p>In pull style, each operator exposes a method that produces rows and may also call this method on it&#39;s children:</p>
<pre data-lang="python"><code data-lang="python"><span>class </span><span>LimitOperator</span><span>:
</span><span>  </span><span>def </span><span>get_rows</span><span>(</span><span>self</span><span>):
</span><span>    </span><span>if </span><span>self</span><span>.</span><span>limit </span><span>== </span><span>0</span><span>:
</span><span>      </span><span># Don&#39;t waste time computing rows that we&#39;ll never return
</span><span>      </span><span>return </span><span>[]
</span><span>    rows </span><span>= </span><span>self</span><span>.</span><span>child</span><span>.</span><span>get_rows</span><span>()
</span><span>    rows </span><span>= </span><span>rows[</span><span>0</span><span>:</span><span>self</span><span>.</span><span>limit]
</span><span>    </span><span>self</span><span>.</span><span>limit </span><span>-= </span><span>rows</span><span>.</span><span>len
</span><span>    </span><span>return </span><span>rows
</span></code></pre>
<p>In push style, each operator exposes a method that accepts some input and returns some output, along with an enum indicating whether it wants more inputs.</p>
<pre data-lang="python"><code data-lang="python"><span>class </span><span>LimitOperator</span><span>:
</span><span>  </span><span>def </span><span>operate</span><span>(</span><span>self</span><span>, </span><span>rows_in</span><span>):
</span><span>    rows_out </span><span>= </span><span>rows_in[</span><span>0</span><span>:</span><span>self</span><span>.</span><span>limit]
</span><span>    </span><span>self</span><span>.</span><span>limit </span><span>-= </span><span>rows</span><span>.</span><span>len
</span><span>    status </span><span>= </span><span>(</span><span>self</span><span>.</span><span>limit </span><span>== </span><span>0</span><span>) ? </span><span>&#34;finished&#34; </span><span>: </span><span>&#34;want_more_input&#34;
</span><span>    </span><span>return </span><span>(status</span><span>, </span><span>rows_out)
</span></code></pre>
<p>Some external system is responsible for calling this method and passing data between operators.</p>
<p>Pull style has long been the default for OLTP databases, going all the way back to <a href="https://core.ac.uk/download/pdf/54846488.pdf">Volcano</a>. But push style seems to be ubiquitous for OLAP and HTAP systems (except for Photon, which is perhaps limited by backwards compatibility with spark?). Notably DuckDB started with pull style and ended up doing a complete rewrite to push style (<a href="https://youtu.be/bZOvAKGkzpQ?t=2194">ref</a>). </p>
<p>The main problem with pull style is that each operator is responsible for scheduling it&#39;s own children. But to support work-stealing, async I/O and query cancellation we really want one global scheduler with visiblility of the whole query. </p>
<p>Note that while the example code above is pretty simple, in general the change from pull style to push style looks a lot like a manual version of what a compiler does when transforming async code to state machines. Doing it by hand isn&#39;t unbearable, but it would be interesting to see if a query engine written in a language with support for generators would be more pleasant:</p>
<pre data-lang="python"><code data-lang="python"><span>class </span><span>LimitOperator</span><span>:
</span><span>  </span><span>def </span><span>run</span><span>(</span><span>self</span><span>, </span><span>rows_in_gen</span><span>):
</span><span>    </span><span>for </span><span>rows_in </span><span>in </span><span>rows_in_gen:
</span><span>      </span><span>yield </span><span>rows_in[</span><span>0</span><span>:</span><span>self</span><span>.</span><span>limit]
</span><span>      </span><span>self</span><span>.</span><span>limit </span><span>-= </span><span>rows</span><span>.</span><span>len
</span><span>      </span><span>if </span><span>(</span><span>self</span><span>.</span><span>limit </span><span>== </span><span>0</span><span>)
</span><span>         </span><span>return
</span></code></pre>
<h2 id="vectorized-vs-pipelined">vectorized vs pipelined</h2>
<p>Within each query pipeline there are two common styles of execution:</p>
<pre data-lang="python"><code data-lang="python"><span># pipelined
</span><span>sum </span><span>= </span><span>0
</span><span>for </span><span>index </span><span>in </span><span>0</span><span>.</span><span>.num_rows:
</span><span>  country </span><span>= </span><span>countries[index]
</span><span>  </span><span>if </span><span>country </span><span>in </span><span>[</span><span>&#34;France&#34;</span><span>, </span><span>&#34;Germany&#34;</span><span>]:
</span><span>    price </span><span>= </span><span>prices[index]
</span><span>    </span><span>if </span><span>price </span><span>&gt; </span><span>0</span><span>.</span><span>99</span><span>:
</span><span>      </span><span>sum </span><span>+= </span><span>price
</span><span>return </span><span>sum
</span><span>
</span><span># vectorized
</span><span>indexes </span><span>= </span><span>0</span><span>.</span><span>.num_rows
</span><span>indexes </span><span>= </span><span>[index </span><span>for </span><span>index </span><span>in </span><span>indexes </span><span>if </span><span>prices[index] </span><span>&gt; </span><span>0</span><span>.</span><span>99</span><span>]
</span><span>indexes </span><span>= </span><span>[index </span><span>for </span><span>index </span><span>in </span><span>indexes </span><span>if </span><span>countries[index] </span><span>in </span><span>[</span><span>&#34;France&#34;</span><span>, </span><span>&#34;Germany&#34;</span><span>]]
</span><span>return </span><span>sum</span><span>([price[index] </span><span>for </span><span>index </span><span>in </span><span>indexes])
</span></code></pre>
<p>In terms of performance on OLAP workloads, there are some tradeoffs:</p>
<ul>
<li>Pipelined code is able to keep values in registers which reduces memory traffic.</li>
<li>The simple loops in vectorized style are more friendly to prefetching and simd.</li>
</ul>
<p>But most evaluations find that both styles produce similar performance. Other concerns overwhelmingly dominate:</p>
<ul>
<li>In vectorized style it&#39;s easy to switch between multiple variants of each operator eg depending on the compression algorthim of the input vector. In pipelined style this would require too much branching per loop iteration.</li>
<li>Pipelined style requires writing a compiler - paying the interpreter overhead per row is way too expensive (<a href="https://pages.cs.wisc.edu/%7Eyxy/cs839-s20/papers/Hekaton-Sigmod2013-final.pdf">ref</a> 9). Vectorized style can amortize the cost of the interprer overhead over an entire vector of values.</li>
<li>Compilation latency with LLVM is too high for interactive usage. Systems which use LLVM typically also have to use an interpreter to get the query started while waiting for compilation to finish (eg <a href="https://15721.courses.cs.cmu.edu/spring2023/papers/24-redshift/redshift-sigmod2022.pdf">ref</a> 2.6). Umbra is a notable exception here - they wrote their own compiler backend with much lower latency (<a href="https://link.springer.com/content/pdf/10.1007/s00778-020-00643-4.pdf">ref</a>).</li>
</ul>
<p>The result is that the pure OLAP systems generally use entirely vectorized style. And even the HTAP systems use vectorized style for scans over columnar data because adapting to different compression algorithms is a huge win over always decompressing before scanning.</p>
<p>The HTAP systems (HyPer/Umbra, SingleStore) use pipelined style after the initial scans. The argument is that vectorized style doesn&#39;t produce good performance for OLTP workloads, but I haven&#39;t seen a clear explanation of why. Certainly for row-based data layouts the advantages of vectorized style is reduced - each operator has to touch a lot of unrelated data, whereas pipelined style can keep each row in l1 or even in registers. But all these systems use pax layouts. Perhaps many OLTP queries simply don&#39;t return enough rows to fill a vector?</p>
<p>In vectorized style some combinations of operators can be very inefficient. Eg suppose we separate <code>... where 2023-01-01 &lt;= date and date &lt;= 2023-01-03</code> into separate <code>filter-less-than</code> and <code>filter-more-than</code> operators and combine the results with an <code>and</code> operator. Each of the filter operators would produce tons of rows, most of which would be discarded by the <code>and</code> operator. Whereas in pipelined style we would evaluate both conditions before yielding the row to the next operator. One solution is to build up a library of fused operators for common combinations (<a href="https://15721.courses.cs.cmu.edu/spring2023/papers/20-databricks/sigmod_photon.pdf">ref</a> 3.3).</p>
<h2 id="vectorized-tips-and-tricks">vectorized tips and tricks</h2>
<p>Examples of adaptive operators that are easier in vectorized style: </p>
<ul>
<li>Scans operating over compressed data eg an equality test on a dictionary-encoded column doesn&#39;t have to decompress the data.</li>
<li>String operations are often cheaper if all the strings in the column are known to be ascii-only.</li>
<li>Reordering filters within a boolean expression so that the empirically cheapest/lowest-selectivity filter runs first.</li>
<li>Skipping null checks when there are no nulls (similarly when the selection vector has no missing rows).</li>
<li>For dictionary encoded data, scalar functions can be run only on the unique dictionary values.</li>
</ul>
<p>Velox has a c++ template that takes a simple scalar function and produces a vectorized version with many of the above adaptive variants (<a href="https://velox-lib.io/blog/simple-functions-1/">ref</a> 4.4.1). To avoid eagerly materializing nested types, the simple functions can operate over lazy reader/writer interfaces (<a href="https://velox-lib.io/blog/simple-functions-2">ref</a>).</p>
<p>There are a lot of implementation choices for filtering columns:</p>
<ul>
<li>Eager vs lazy materialization (make a new column vs returning a lazy view on the old column)</li>
<li>For lazy materialization, represent the selection as a bitmap or a selection vector (a list of integer indexes)?</li>
<li>For cheap operations over lazy vectors, in some conditions it might be cheaper to process all the input values rather than just the selected ones.</li>
<li>For scans with multiple filters <code>where ... and ...</code>: run each filter independently and combine the results, or run the second filter on the lazy vector produced by the first?</li>
</ul>
<p>I found a few academic papers comparing these choices (<a href="https://dl.acm.org/doi/pdf/10.1145/3465998.3466009">ref</a>), <a href="https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf">ref</a> 5.1, <a href="http://131.159.16.103/downloads/publications/datablocks.pdf">ref</a> 4.2) but no firm consensus. It seems like something I&#39;ll just have to benchmark in situ. </p>
<p>Worth noting though that both duckdb and velox use selection vectors and support lazy vectors. If you allow the selection vector to contain duplicates and out-of-order indexes then selection vectors can also represent dictionary-encoded data, which means that any fancy engineering done for one will benefit the other. </p>
<p>DuckDB also uses this values+selection format as a universal partially-decompressed format - if an operator doesn&#39;t have a specialized implementation for a given compression algorithm then it&#39;s fairly cheap to convert from most compression formats to values+selection without much copying.</p>
<h2 id="questions">questions</h2>
<ul>
<li>Is it really worth materializing into row-based formats for hashtables etc, rather than storing an index into columns?</li>
<li>How much compression ratio do we have to pay to preserve random access? Can we get close-to-random access on variable-length formats (eg RLE) by storing some small lookup accelarator?</li>
<li>Is it ever worth compressing the output of an operator, rather than letting the tail ends of each pipeline devolve into raw values?</li>
<li>In the duckdb/velox format, how to convert a selection vector on a more-nested column to a selection vector on a less-nested column? Is it reasonable to just binary search in the offsets column for each selected index? This seems like it would be cheaper with bitmaps.</li>
<li>How should heterogenous data like sum types be handled in a columnar format? Is it worth using a selection vector next to the tag, or better to just use nullable columns for each of the possible payloads?</li>
<li>Do vectorized interpreters perform worse than pipelined compilers on OLTP workloads even over column formats, or is this only true for row formats?</li>
<li>When is it worth materializing a lazy vector? At some point the repeated cost of sparse memory access must outweigh the cost of copying data.</li>
</ul>

<p>Please feel free to <a href="mailto:jamie@scattered-thoughts.net">email me</a> with comments or corrections and I&#39;ll add them here.</p>

</article></div>
  </body>
</html>
