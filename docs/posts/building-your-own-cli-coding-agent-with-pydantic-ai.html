<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://martinfowler.com/articles/build-own-coding-agent.html">Original</a>
    <h1>Building your own CLI coding agent with Pydantic-AI</h1>
    
    <div id="readability-page-1" class="page"><div>
<section id="TheWaveOfCliCodingAgents">
<h2>The wave of CLI Coding Agents</h2>

<p>If you have tried Claude Code, Gemini Code, Open Code or Simon
      Willison’s <a href="https://github.com/simonw/llm">LLM CLI</a>, you’ve experienced something fundamentally
      different from ChatGPT or Github Copilot. These aren’t just chatbots or
      autocomplete tools - they’re agents that can read your code, run your
      tests, search docs and make changes to your codebase async.</p>

<p>But how do they work? For me the best way to understand how any tool
      works is to try and build it myself. So that’s exactly what we did, and in
      this article I’ll take you through how we built our own CLI Coding Agent
      using the Pydantic-AI framework and the Model Context Protocol (MCP).
      You’ll see not just how to assemble the pieces but why each capability
      matters and how it changes the way you can work with code.</p>

<p>Our implementation leverages AWS Bedrock but with Pydantic-AI you could
      easily use any other mainstream provider or even a fully local LLM.</p>
</section>

<section id="WhyBuildWhenYouCanBuy">
<h2>Why Build When You Can Buy?</h2>

<p>Before diving into the technical implementation, let&#39;s examine why we
      chose to build our own solution.</p>

<p>The answer became clear very quickly using our custom agent, while
      commercial tools are impressive, they’re built for general use cases. Our
      agent was fully customised to our internal context and all the little
      eccentricities of our specific project. More importantly, building it gave
      us insights into how these systems work and the quality of our own GenAI
      Platform and Dev Tooling.</p>

<p>Think of it like learning to cook. You can eat at restaurants forever
      but understanding how flavours combine and techniques work makes you
      appreciate food differently - and lets you create exactly what you
      want.</p>
</section>

<section id="TheArchitectureOfOurDevelopmentAgent">
<h2>The Architecture of Our Development Agent</h2>

<p>At a high level, our coding assistant consists of several key
      components:</p>

<ul>
<li>Core AI Model: Claude from Anthropic accessed through AWS Bedrock </li>

<li>Pydantic-AI Framework: provides the agent framework and many helpful
        utilities to make our Agent more useful immediately </li>

<li>MCP Servers: independent processes that give the agent specialised
        tools, MCP is a common standard for defining the servers that contain these
        tools. </li>

<li>CLI Interface: how users interact with the assistant</li>
</ul>

<p>The magic happens through the Model Context Protocol (MCP), which
      allows the AI model to use various tools through a standardized interface.
      This architecture makes our assistant highly extensible - we can easily
      add new capabilities by implementing additional MCP servers, but we’re
      getting ahead of ourselves.</p>
</section>

<section id="StartingSimpleTheFoundation">
<h2>Starting Simple: The Foundation</h2>

<p>We started by creating a basic project structure and installing the
      necessary dependencies:</p>

<pre>uv init
uv add pydantic_ai
uv add boto3
</pre>

<p>Our primary dependencies include:</p>

<ul>
<li><code>pydantic-ai</code>: Framework for building AI agents</li>

<li><code>boto3</code>: For AWS API interactions</li>
</ul>

<p>We chose Claude Sonnet 4 from Anthropic (accessed via AWS Bedrock) as
      our foundation model due to its strong code understanding and generation
      capabilities. Here&#39;s how we configured it in our <code>main.py</code>:</p>

<pre>import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
</pre>

<pre>bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={&#34;max_attempts&#34;: 3},
)
bedrock_client = boto3.client(
    &#34;bedrock-runtime&#34;, region_name=&#34;eu-central-1&#34;, config=bedrock_config
)
model = BedrockConverseModel(
    &#34;eu.anthropic.claude-sonnet-4-20250514-v1:0&#34;,
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)
</pre>

<pre>if __name__ == &#34;__main__&#34;:
  agent.to_cli_sync()
</pre>

<p>At this stage we already have a fully working CLI with a chat interface
      which we can use as you would a GUI chat interface, which is pretty cool
      for how little code this is! However we can definitely improve upon
      this.</p>
</section>

<section id="FirstCapabilityTesting">
<h2>First Capability: Testing!</h2>

<p>Instead of running the tests ourselves after each coding iteration why
      not get the agent to do it? Seems simple right?</p>

<pre>import subprocess
</pre>

<pre>@agent.tool_plain()
def run_unit_tests() -&gt; str:
    &#34;&#34;&#34;Run unit tests using uv.&#34;&#34;&#34;
    result = subprocess.run(
        [&#34;uv&#34;, &#34;run&#34;, &#34;pytest&#34;, &#34;-xvs&#34;, &#34;tests/&#34;], capture_output=True, text=True
    )
    return result.stdout
</pre>

<p>Here we use the same pytest command you would run in the terminal (I’ve
      shortened ours for the article). Now something magical happened. I could
      say “X isn’t working” and the agent would:</p>

<ul>
<li>1. Run the test suite</li>

<li>2. Identify which specific tests were failing</li>

<li>3. Analyze the error messages</li>

<li>4. Suggest targeted fixes.</li>
</ul>

<p><b>The workflow change:</b> Instead of staring at test failures or copy
      pasting terminal outputs into ChatGPT we now give our agent super relevant
      context about any issues in our codebase.</p>

<p>However we noticed our agent sometimes “fixed” failing tests by
      suggesting changes to the tests, not the actual implementation. This led
      to our next addition.</p>
</section>

<section id="AddingIntelligenceInstructionsAndIntent">
<h2>Adding Intelligence: Instructions and intent</h2>

<p>We realised we needed to teach our agent a little more about our
      development philosophy and steer it away from bad behaviours.</p>

<pre>instructions = &#34;&#34;&#34;
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don&#39;t match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
&#34;&#34;&#34;
</pre>

<pre>agent = Agent(
instructions=instructions,
model=model,
)
</pre>

<p><b>The workflow change:</b> The agent now understands our values around
      Test Driven Development and minimal changes. It stopped suggesting large
      refactors where a small fix would do (Mostly).</p>

<p>Now while we could continue building everything from absolute scratch
      and tweaking our prompts for days we want to go fast and use some tools
      other people have built - Enter Model Context Protocol (MCP).</p>
</section>

<section id="TheMcpRevolutionPluggableCapabilities">
<h2>The MCP Revolution: Pluggable Capabilities</h2>

<p>This is where our agent transformed from a helpful assistant to
      something approaching the commercial CLI agents. The Model Context
      Protocol (MCP) allows us to add sophisticated capabilities by running
      specialized servers.</p>

<blockquote>
<p>MCP is an open protocol that standardizes how applications provide
        context to LLMs. Think of MCP like a USB-C port for AI applications.
        Just as USB-C provides a standardized way to connect your devices to
        various peripherals and accessories, MCP provides a standardized way to
        connect AI models to different data sources and tools. </p>

<p>-- <a href="https://modelcontextprotocol.io/introduction">MCP Introduction</a></p>
</blockquote>

<p>We can run these servers as a local process, so no data sharing, where
      we interact with STDIN/STDOUT to keep things simple and local. <a href="https://martinfowler.com/articles/function-call-LLM.html">(More details on tools and MCP)</a></p>
</section>

<section id="SandboxedPythonExecution">
<h2>Sandboxed Python Execution</h2>

<p>Using large language models to do calculations or executing arbitrary code they create is not effective and potentially very dangerous! To make our Agent more accurate and safe our first MCP addition was Pydantic Al’s default server for sandboxed Python code execution:</p>

<pre>run_python = MCPServerStdio(
    &#34;deno&#34;,
    args=[
        &#34;run&#34;,
        &#34;-N&#34;,
        &#34;-R=node_modules&#34;,
        &#34;-W=node_modules&#34;,
        &#34;--node-modules-dir=auto&#34;,
        &#34;jsr:@pydantic/mcp-run-python&#34;,
        &#34;stdio&#34;,
    ],
)
</pre>

<pre>agent = Agent(
    ...
    mcp_servers=[
        run_python
    ],
)
</pre>

<p>This gave our agent a sandbox where it could test ideas, prototype
      solutions, and verify its own suggestions.</p>

<p>NOTE: This is very different from running the tests where we need the
      local environment and is intended to be used to make calculations much
      more robust. This is because writing the code to output a number and then
      executing that code is much more reliable and understandable, scalable and
      repeatable than just generating the next token in a calculation. We have
      seen from frontier labs (including their leaked instructions) that this is
      a much better approach.</p>

<p><b>The workflow change:</b> Doing calculations, even more complex ones,
      became significantly more reliable. This is useful for many things like
      dates, sums, counts etc. It also allows for a rapid iteration cycle of
      simple python code.</p>
</section>

<section id="Up-to-dateLibraryDocumentation">
<h2>Up-to-Date library Documentation</h2>

<p>LLMs are mostly trained in batch on historical data this gives a fixed
      cutoff while languages and dependencies continue to change and improve so
      we added <a href="https://context7.com/">Context7</a> for access to up to date python
      library documentation in LLM consumable format:</p>

<pre>context7 = MCPServerStdio(
    command=&#34;npx&#34;, args=[&#34;-y&#34;, &#34;@upstash/context7-mcp&#34;], tool_prefix=&#34;context&#34;
)
</pre>

<p><b>The workflow change:</b> When working with newer libraries or trying to
      use advanced features, the agent could look up current documentation
      rather than relying on potentially outdated training data. This made it
      much more reliable for real-world development work.</p>
</section>

<section id="AwsMcps">
<h2>AWS MCPs</h2>

<p>Since this particular agent was built with an AWS platform in mind, we
      added the AWS Labs MCP servers for comprehensive cloud docs and
      integration:</p>

<pre>awslabs = MCPServerStdio(
    command=&#34;uvx&#34;,
    args=[&#34;awslabs.core-mcp-server@latest&#34;],
    env={&#34;FASTMCP_LOG_LEVEL&#34;: &#34;ERROR&#34;},
    tool_prefix=&#34;awslabs&#34;,
)
aws_docs = MCPServerStdio(
    command=&#34;uvx&#34;,
    args=[&#34;awslabs.aws-documentation-mcp-server@latest&#34;],
    env={&#34;FASTMCP_LOG_LEVEL&#34;: &#34;ERROR&#34;, &#34;AWS_DOCUMENTATION_PARTITION&#34;: &#34;aws&#34;},
    tool_prefix=&#34;aws_docs&#34;,
)
</pre>

<p><b>The workflow change:</b> Now when I mentioned “Bedrock is timing out”
      or “the model responses are getting truncated,” the agent could directly
      access AWS documentation to help troubleshoot configuration issues. While
      we&#39;ve only scratched the surface with these two servers, this is the tip
      of the iceberg—the <a href="https://awslabs.github.io/mcp/">AWS Labs MCP
      collection</a> includes servers for
      CloudWatch metrics, Lambda debugging, IAM policy analysis, and much more.
      Even with just documentation access, cloud debugging became more
      conversational and contextual.</p>
</section>

<section id="InternetSearchForCurrentInformation">
<h2>Internet Search for Current Information</h2>

<p>Sometimes you need information that&#39;s not in any documentation—recent
      Stack Overflow discussions, GitHub issues, or the latest best practices.
      We added general internet search:</p>

<pre>internet_search = MCPServerStdio(command=&#34;uvx&#34;, args=[&#34;duckduckgo-mcp-server&#34;])
</pre>

<p><b>The workflow change:</b> When encountering obscure errors or needing to
      understand recent changes in the ecosystem, the agent could search for
      current discussions and solutions. This was particularly valuable for
      debugging deployment issues or understanding breaking changes in
      dependencies.</p>
</section>

<section id="StructuredProblemSolving">
<h2>Structured Problem Solving</h2>

<p>One of the most valuable additions was the code reasoning MCP, which
      helps the agent think through complex problems systematically:</p>

<pre>code_reasoning = MCPServerStdio(
    command=&#34;npx&#34;,
    args=[&#34;-y&#34;, &#34;@mettamatt/code-reasoning&#34;],
    tool_prefix=&#34;code_reasoning&#34;,
)
</pre>

<p><b>The workflow change:</b> Instead of jumping to solutions, the agent
      would break down complex problems into logical steps, explore alternative
      approaches, and explain its reasoning. This was invaluable for
      architectural decisions and debugging complex issues. I could ask “Why is
      this API call failing intermittently?” and get a structured analysis of
      potential causes rather than just guesses.</p>
</section>

<section id="OptimisingForReasoning">
<h2>Optimising for Reasoning</h2>

<p>As we added more sophisticated capabilities, we noticed that reasoning
      and analysis tasks often took much longer than regular text
      generation—especially when the output wasn&#39;t correctly formatted on the
      first try. We adjusted our Bedrock configuration to be more patient:</p>

<pre>bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={&#34;max_attempts&#34;: 3},
)
bedrock_client = boto3.client(
    &#34;bedrock-runtime&#34;, region_name=&#34;eu-central-1&#34;, config=bedrock_config
)
</pre>

<p><b>The workflow change:</b> The longer timeouts meant our agent could work
      through complex problems without timing out. When analyzing large
      codebases or reasoning through intricate architectural decisions, the
      agent could take the time needed to provide thorough, well-reasoned
      responses rather than rushing to incomplete solutions.</p>
</section>

<section id="DesktopCommanderWarningWithGreatPowerComesGreatResponsibility">
<h2>Desktop Commander: Warning! With great power comes great responsibility!</h2>

<p>At this point, our agent was already quite capable—it could reason
      through problems, execute code, search for information, and access AWS
      documentation. This MCP server transforms your agent from a helpful
      assistant into something that can actually <i>do</i> things in your development
      environment:</p>

<pre>desktop_commander = MCPServerStdio(
    command=&#34;npx&#34;,
    args=[&#34;-y&#34;, &#34;@wonderwhy-er/desktop-commander&#34;],
    tool_prefix=&#34;desktop_commander&#34;,
)
</pre>

<p>Desktop Commander provides an incredibly comprehensive toolkit: file
      system operations (read, write, search), terminal command execution with
      process management, surgical code editing with <code>edit_block</code>, and even
      interactive REPL sessions. It&#39;s built on top of the MCP Filesystem Server
      but adds crucial capabilities like search-and-replace editing and
      intelligent process control.</p>

<p><b>The workflow change:</b> This is where everything came together. I
      could now say “The authentication tests are failing, please fix the issue”
      and the agent would:</p>

<ul>
<li>1. Run the test suite to see the specific failures</li>

<li>2. Read the failing test files to understand what was expected</li>

<li>3. Examine the authentication module code</li>

<li>4. Search the codebase for related patterns</li>

<li>5. Look up the documentation for the relevant library</li>

<li>6. Make edits to fix the implementation</li>

<li>7. Re-run the tests to verify the fix</li>

<li>8. Search for similar patterns elsewhere that might need updating</li>
</ul>

<p>All of this happened in a single conversation thread, with the agent
      maintaining context throughout. It wasn&#39;t just generating code
      suggestions—it was actively debugging, editing, and verifying fixes like a
      pair programming partner.</p>

<p>The security model is thoughtful too, with configurable allowed
      directories, blocked commands, and proper permission boundaries. You can
      learn more about its extensive capabilities at the <a href="https://github.com/wonderwhy-er/DesktopCommanderMCP">Desktop Commander
      documentation</a>.</p>
</section>

<section id="TheCompleteSystem">
<h2>The Complete System</h2>

<p>Here&#39;s our final agent configuration:</p>

<pre>import asyncio


import subprocess
import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
from botocore.config import Config as BotocoreConfig

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={&#34;max_attempts&#34;: 3},
)
bedrock_client = boto3.client(
    &#34;bedrock-runtime&#34;, region_name=&#34;eu-central-1&#34;, config=bedrock_config
)
model = BedrockConverseModel(
    &#34;eu.anthropic.claude-sonnet-4-20250514-v1:0&#34;,
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


instructions = &#34;&#34;&#34;
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don&#39;t match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
&#34;&#34;&#34;


run_python = MCPServerStdio(
    &#34;deno&#34;,
    args=[
        &#34;run&#34;,
        &#34;-N&#34;,
        &#34;-R=node_modules&#34;,
        &#34;-W=node_modules&#34;,
        &#34;--node-modules-dir=auto&#34;,
        &#34;jsr:@pydantic/mcp-run-python&#34;,
        &#34;stdio&#34;,
    ],
)

internet_search = MCPServerStdio(command=&#34;uvx&#34;, args=[&#34;duckduckgo-mcp-server&#34;])
code_reasoning = MCPServerStdio(
    command=&#34;npx&#34;,
    args=[&#34;-y&#34;, &#34;@mettamatt/code-reasoning&#34;],
    tool_prefix=&#34;code_reasoning&#34;,
)
desktop_commander = MCPServerStdio(
    command=&#34;npx&#34;,
    args=[&#34;-y&#34;, &#34;@wonderwhy-er/desktop-commander&#34;],
    tool_prefix=&#34;desktop_commander&#34;,
)
awslabs = MCPServerStdio(
    command=&#34;uvx&#34;,
    args=[&#34;awslabs.core-mcp-server@latest&#34;],
    env={&#34;FASTMCP_LOG_LEVEL&#34;: &#34;ERROR&#34;},
    tool_prefix=&#34;awslabs&#34;,
)
aws_docs = MCPServerStdio(
    command=&#34;uvx&#34;,
    args=[&#34;awslabs.aws-documentation-mcp-server@latest&#34;],
    env={&#34;FASTMCP_LOG_LEVEL&#34;: &#34;ERROR&#34;, &#34;AWS_DOCUMENTATION_PARTITION&#34;: &#34;aws&#34;},
    tool_prefix=&#34;aws_docs&#34;,
)
context7 = MCPServerStdio(
    command=&#34;npx&#34;, args=[&#34;-y&#34;, &#34;@upstash/context7-mcp&#34;], tool_prefix=&#34;context&#34;
)

agent = Agent(
    instructions=instructions,
    model=model,
    mcp_servers=[
        run_python,
        internet_search,
        code_reasoning,
        context7,
        awslabs,
        aws_docs,
        desktop_commander,
    ],
)


@agent.tool_plain()
def run_unit_tests() -&gt; str:
    &#34;&#34;&#34;Run unit tests using uv.&#34;&#34;&#34;
    result = subprocess.run(
        [&#34;uv&#34;, &#34;run&#34;, &#34;pytest&#34;, &#34;-xvs&#34;, &#34;tests/&#34;], capture_output=True, text=True
    )
    return result.stdout


async def main():
    async with agent.run_mcp_servers():
        await agent.to_cli()


if __name__ == &#34;__main__&#34;:
    asyncio.run(main())
</pre>

<p>How it changes our workflow:</p>

<ul>
<li>Debugging becomes collaborative: you have an intelligent partner
        that can analyze error messages, suggest hypotheses, and help test
        solutions.</li>

<li>Learning accelerates: when working with unfamiliar libraries or
        patterns, the agent can explain existing code, suggest improvements, and
        teach you why certain approaches work better.</li>

<li>Context switching reduces: rather than jumping between
        documentation, Stack Overflow, AWS Console, and your IDE, you have a
        single interface that can access all these resources while maintaining
        context about your specific problem.</li>

<li>Problem-solving becomes structured: rather than jumping to
        solutions, the agent can break down complex issues into logical steps,
        explore alternatives, and explain its reasoning. Like having a real life talking rubber duck!</li>

<li>Code review improves: the agent can review your changes, spot
        potential issues, and suggest improvements before you commit—like having a
        senior developer looking over your shoulder.</li>
</ul>
</section>

<section id="WhatWeLearnedAboutCliAgents">
<h2>What We Learned About CLI Agents</h2>

<p>Building our own agent revealed several insights about this emerging
      paradigm:</p>

<ul>
<li>MCP is (almost) all you need: the magic isn&#39;t in any single
        capability, but in how they work together. The agent that can run tests,
        read files, search documentation, execute code, access AWS services, and
        reason through problems systematically becomes qualitatively different
        from one that can only do any single task.</li>

<li>Current information is crucial: having access to real-time search
        and up-to-date documentation makes the agent much more reliable for
        real-world development work where training data might be outdated.</li>

<li>Structured thinking matters: the code reasoning capability
        transforms the agent from a clever autocomplete into a thinking partner
        that can break down complex problems and explore alternative
        solutions.</li>

<li>Context is king: commercial agents like Claude Code are impressive
        partly because they maintain context across all these different tools.
        Your agent needs to remember what it learned from the test run when it&#39;s
        making file changes.</li>

<li>Specialisation matters: our agent works better for our specific
        codebase than general-purpose tools because it understands our patterns,
        conventions, and tool preferences. If it falls short in any area then we
        can go and make the required changes.</li>
</ul>
</section>

<section id="TheRoadAhead">
<h2>The Road Ahead</h2>

<p>The CLI agent paradigm is still evolving rapidly. Some areas we&#39;re
      exploring:</p>

<ul>
<li>AWS-specific tooling: the AWS Labs MCP servers
        (https://awslabs.github.io/mcp/) provide incredible depth for cloud-native
        development—from CloudWatch metrics to Lambda debugging to IAM policy
        analysis.</li>

<li>Workflow Enhancements: teaching the agent our common development
        workflows so it can handle routine tasks end-to-end. Connecting the agent
        to our project management tools so it can understand priorities and
        coordinate with team processes.</li>

<li>Benchmarking: <a href="https://www.tbench.ai">Terminal Bench</a>
        looks like a great dataset and leaderboard to test this toy agent against
        the big boys!</li>
</ul>
</section>

<section id="WhyThisMatters">
<h2>Why This Matters</h2>

<p>CLI coding agents represent a fundamental
      shift from AI as a writing assistant to AI as a development partner.
      Unlike Copilot&#39;s autocomplete or ChatGPT&#39;s Q&amp;A, these agents can:</p>

<ul>
<li>Understand your entire project context</li>

<li>Execute tasks across multiple tools</li>

<li>Maintain state across complex workflows</li>

<li>Learn from your specific codebase and patterns</li>
</ul>

<p>Building one yourself—even a simple version—gives you insights into
      where this technology is heading and how to make the most of commercial
      tools when they arrive.</p>

<p>The future of software development isn&#39;t just about writing code
      faster. It&#39;s about having an intelligent partner that understands your
      goals, your constraints, and your codebase well enough to help you think
      through problems and implement solutions collaboratively.</p>

<p>And the best way to understand that future? Build it yourself.</p>
</section>

<hr/>
</div></div>
  </body>
</html>
