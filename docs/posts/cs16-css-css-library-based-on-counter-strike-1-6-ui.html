<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cs16.samke.me">Original</a>
    <h1>Show HN: Cs16.css – CSS library based on Counter Strike 1.6 UI</h1>
    
    <div id="readability-page-1" class="page"><div>
  
  <!-- TODO: fix the if -->
  <p><span>19 January 2025 · jupyter
    · airflow
    · notebook</span></p><!-- https://help.twitter.com/en/using-twitter/add-twitter-share-button -->
  
  
 <p>This post is based on my <a href="https://www.youtube.com/watch?v=yfXfr_tcZ2g">JupyterCon 2020 talk</a>. I decided to write it 5 years later, because I think that notebook-based data platforms are relevant, now more than ever as AI helps us do more things, to enable <em>citizen data science</em> and seamless collaboration in large orgs. It&#39;s a story of how I helped build the notebook-based data platform at Grofers (now Blinkit) based on <a href="https://netflixtechblog.com/notebook-innovation-591ee3221233">Netflix&#39;s pioneering work</a>.</p>
<hr/>
<p>Around 2018, there was this growing trend of MLOps and enterprise notebook infra, where everyone was setting up JupyterHubs in their orgs to democratize access to data and make data science reproducible. It was a recurring theme at every JupyterCon <sup id="sf-notebookops-1-back"><a href="#sf-notebookops-1" title="Deploying a cloud-based JupyterHub for students and researchers source">1</a></sup> <sup id="sf-notebookops-2-back"><a href="#sf-notebookops-2" title="Enterprise usage of Jupyter: The business case and best practices for leveraging open source source">2</a></sup> <sup id="sf-notebookops-3-back"><a href="#sf-notebookops-3" title="Scheduled notebooks: A means for manageable and traceable code execution source">3</a></sup> <sup id="sf-notebookops-4-back"><a href="#sf-notebookops-4" title="Notebooks at Netflix: From analytics to engineering (sponsored by Netflix) source">4</a></sup>. Every data engineer I talked to had worked on, or was working on setting up some kind of notebook infra at their org.</p>
<p>This quote from <a href="https://x.com/ellisonbg">Brian Granger</a> sums it up pretty nicely:</p>
<blockquote>
<p>&#34;We are entering an era where large, complex organizations need to scale interactive computing with data to their entire organization in a manner that is collaborative, secure, and human centered.&#34; — Brian Granger (<a href="https://www.datacamp.com/community/podcast/project-jupyter-interactive-computing">source</a>)</p>
</blockquote>
<p>He further adds that there&#39;s an organizational shift from the historical usage patterns of Jupyter being used by individuals in an ad hoc manner to the new world of large scale Jupyter deployments.</p>
<p>At JupyterCon 2018, <a href="https://x.com/pacoid">Paco Nathan</a> also pointed out how large organizations are adopting Jupyter at scale.</p>
<blockquote>
<p>&#34;We&#39;ve seen large organizations adopt Jupyter for their analytics infrastructure, in a &#34;leap frog&#34; effect over commercial offerings.&#34; — Paco Nathan (<a href="https://www.youtube.com/watch?v=t0MNlJV7_gM">source</a>)</p>
</blockquote>
<p>He says that on the one hand, there are people in the organization who are well trained in a problem — the domain experts. They may not have the technical expertise at first, but they can use Jupyter to gain enough of the tooling to really amplify what they&#39;re doing in the domain.</p>
<p>This is also called <em>citizen data science</em>, where enterprise notebook infra makes it easy for anyone in an organization to get access to data and do wonderful things with it.</p>
<p>On the other hand, there&#39;s the fresh graduates that these organizations are hiring. And since they already know how to use Jupyter from their coursework, enterprise notebook infra can help them deliver results from the first day itself.</p>
<h2>Notebook Infra at Netflix</h2>
<p>There have been multiple talks and blog posts from large orgs like <a href="https://medium.com/paypal-tech/paypal-notebooks-powered-by-jupyter-fd0067bd00b0">PayPal</a> and Civis Analytics around this theme. But perhaps the most famous of all stories is of Netflix, where they set up a <a href="https://netflixtechblog.com/notebook-innovation-591ee3221233">data platform</a> by putting notebooks at the core.</p>
<p>In their platform, anyone can launch a notebook server on-demand. Users can also choose resources like CPU and memory, after which their server gets launched in a container using Titus, which is their container management platform. If someone wants to schedule their notebook to run at specific times, they can do so using Meson, which is their internal workflow orchestration platform.</p>
<p>Note: The team that built the notebook infra at Netflix founded <a href="https://archive.is/VXRjk">Noteable</a>, which was <a href="https://www.confluent.io/blog/welcoming-noteable-to-confluent/">acquired by Confluent</a>.</p>
<p>In a notebook-based data platform, everything is a notebook. You want to write an ETL job? Notebook. You want to train a model? Notebook. You want to send out a daily report? Believe it or not. Notebook.</p>
<h2>Grofers</h2>
<p>In February 2019, I joined Grofers (now Blinkit) as a data engineer in the Bangalore team. We had around 15 data analysts and around 5 data scientists, supported by 3 data engineers: <a href="https://www.linkedin.com/in/deeputp/?originalSubdomain=in">Deepu Philip</a>, <a href="https://www.linkedin.com/in/sangarshanan/?originalSubdomain=in">Sangarshanan Veera</a>, and me.</p>
<p>Everyone in the data team was using JupyterLab. By that, I mean that we had one large EC2 instance which was running JupyterLab for everyone. When something didn&#39;t work on this big instance, they would move to using JupyterLab on their laptops.</p>
<p>Read credentials for various databases were often hardcoded inside notebooks, from where they would eventually end up in GitHub. I&#39;m glad we didn&#39;t make one of our repos public by mistake!</p>
<pre><code>
  <span>import</span> pandas
  <span>from</span> sqlalchemy <span>import</span> create_engine

  engine = create_engine(
      <span>&#39;postgresql://read_user:83ffaf15@&lt;hostname&gt;:&lt;port&gt;/&lt;dbname&gt;&#39;</span>
  )
  df = pandas.read_sql(<span>&#39;SELECT * FROM schema.table;&#39;</span>, engine)

</code></pre>

<p>If someone in the data team wanted to schedule their notebook to run at a specific time, an engineer would use <code>nbconvert</code> to convert the notebook into a Python file and schedule it as a cronjob on the same EC2 instance. These notebooks would run ETL jobs or send out reports every morning.</p>
<pre><code>
  $ crontab -l
  0 2 * * * /home/ec2/notebooks/scripts/run_etl.py
  0 6 * * * /home/ec2/notebooks/scripts/send_report.py

</code></pre>

<p>We also had Airflow running on another EC2 instance for when someone in the data team wanted to run complex workflows where they wanted dependencies between tasks.</p>
<p>As you might expect, this setup had some flaws.</p>
<h3>1. No session isolation</h3>
<p>With one JupyterLab serving many users, there was no concept of a user session. One person&#39;s large workload could disrupt other users on the same system. This led to the other user moving to using JupyterLab on their own laptop, which would be a hassle because they would need to move all of their files and re-create all of their dataframes.</p>
<h3>2. Dependency clashes</h3>
<p>When one user upgraded a Python package on the server, the package would be upgraded for everyone, which would break another user&#39;s workflow. It was also a pain to keep the dependencies on the JupyterLab and Airflow instances in sync.</p>
<h3>3. No reproducibility</h3>
<p>Because of the dependency mismatches and a user constantly moving their work between their laptop and the JupyterLab server, it created a &#34;works on my machine&#34; problem. This also made it hard for another person to build on past work because even if they were able to somehow find an old notebook, they would need to create a <code>requirements.txt</code> by looking at the imports, and sometimes figure out the appropriate versions in case of breaking changes within libraries.</p>
<h3>4. Not self-serve</h3>
<p>When someone in the data team wanted to schedule complex workflows, they wouldn&#39;t be able to do that themselves unless they were familiar with Airflow&#39;s Python API. This added a dependency on a data engineer to translate their notebook&#39;s core logic into Airflow. Over time, this approach put a lot of maintenance burden on the data engineers.</p>
<h2>A New Hope</h2>
<p>Over time, Grofers had built a &#34;DIY&#34; DevOps culture where anyone could provision new infra resources and create Jenkins jobs very easily. There were dedicated infra and CI teams that set up awesome tooling for this.</p>
<p>Around the time when we were facing the problems I mentioned above, the infra team set up a Kubernetes cluster and there was an org wide move to migrate services to Kubernetes. Any engineer could install anything on the cluster once they had set up <code>kubectl</code> and <code>helm</code>.</p>
<p>We started to think about how we could solve all of our problems using these tools. We&#39;d read about the notebook infra at Netflix and decided we needed something similar. The only problem was that some of the things they talked about weren&#39;t open-source, so we decided on these alternatives:</p>
<ul>
<li>Kubernetes, in place of Titus for container management</li>
<li>JupyterHub, in place of Nteract as a multi-user notebook environment</li>
<li>Airflow, in place of Meson for workflow management</li>
<li>Some GitHub and Jenkins automation as glue to tie JupyterHub and Airflow together</li>
</ul>
<div>
    <figure>
        <img src="https://cs16.samke.me/files/netflix-data-platform-oss.gif"/>
    </figure>
</div>

<h3>JupyterHub</h3>
<h4>What is JupyterHub?</h4>
<p>JupyterHub is a multi-user version of JupyterLab which is designed for large user groups. The Jupyter team has written great docs on how you can set up JupyterHub on Kubernetes, where each user can spin up their own JupyterLab environment in an isolated Kubernetes pod.</p>
<p>JupyterHub is famously used to serve more than a 1000 students in the Foundations of Data Science course at UC Berkeley.</p>
<div>
    <figure>
        <img src="https://cs16.samke.me/files/jupyterhub-berkeley.jpg"/>
    </figure>
</div>

<h4>Setting up</h4>
<p>If you already have a Kubernetes cluster, it is pretty easy to set up JupyterHub using <code>helm</code>. You just need to create a config in which you declare with all of settings you need.</p>
<p>For example, you can configure how someone logs into JupyterHub with a bunch of different methods. We used Google OAuth which meant that everyone with a grofers.com Google id could log in to get access to org-wide data.</p>
<pre><code>
  auth:
    type: google
    google:
      clientId: &lt;client_id&gt;
      clientSecret: &lt;client_secret&gt;
      callbackUrl: &lt;hub_url&gt;/oauth_callback
      hostedDomain:
      - grofers.com
      loginService: Grofers

</code></pre>

<h4>Persistent storage with EFS</h4>
<p>We used EFS to store everyone&#39;s files. With EFS, every user would get a brand new filesystem which would be mounted on their JupyterLab server at <code>/home/jovyan</code>. Anything that they created would be persisted on EFS at <code>home/username</code>. Whenever they re-launched their server after shutting it down, they&#39;d get their working directory back.</p>
<p>You must be wondering who&#39;s <code>jovyan</code>. It&#39;s just the name of the default user that shows up in Jupyter docker images. In science fiction, a Jovyan is an inhabitant of planet Jupyter. Here&#39;s a GitHub comment by <a href="https://x.com/KyleRayKelley">Kyle Kelley</a> where he talks about coining this term.</p>


<h4>Custom environments</h4>
<p>We reused a bunch of docker images that the Jupyter team maintains to create different environments for our users. For example, we used the <code>datascience-notebook</code> docker image which contains all of the commonly used data science libraries and created a new image. We just had to import it at the top of our <code>Dockerfile</code> and then add commands to install all of our own libraries.</p>
<pre><code>
  FROM jupyter/datascience-notebook:177037d09156
  # Get the latest image tag at:
  # https://hub.docker.com/r/jupyter/datascience-notebook/tags/
  # Inspect the Dockerfile at:
  # https://github.com/jupyter/docker-stacks/tree/master/datascience-notebook/Dockerfile

  # install additional package...
  RUN pip install --no-cache-dir astropy

</code></pre>

<p>After we published these images to our internal docker registry, we could list them as profiles in the JupyterHub config.</p>
<pre><code>
  singleuser:
    # defines the default image
    image:
      name: jupyter/minimal-notebook
      tag: 2343e33dec46
    profileList:
    - display_name: &#34;Datascience environment&#34;
      description: |
        Everything you need to access data within the
        platform
      kubespawner_override:
        image: {{ registry_url }}/datascience-notebook:2343e33dec46

</code></pre>

<p>This enabled users to pick the relevant environment when they were looking to launch a JupyterLab instance. We also modified the existing form template to let users select the CPU and memory for their environment. When the user clicked on start, they would get a personal JupyterLab instance running in an isolated Kubernetes pod. These environments ensured reproducibility in the data team&#39;s work.</p>
<div>
    <figure>
        <img src="https://cs16.samke.me/files/jhub-server-options-1.png"/>
    </figure>
</div>

<h4>Onboarding and experience</h4>
<p>For new users, we added a README notebook where they could give in some details like their name and email, execute cells one by one, and get SSH and GPG keys which they could add to their GitHub account. This ensured that they were able to clone and create repos inside the org account, and that all of their commits were verified.</p>
<pre><code>
  fullname = <span>&#34;Your full name&#34;</span>
  email = <span>&#34;Your email on GitHub&#34;</span>

</code></pre>

<p>We also built an internal Python library which automated some common tasks that everyone would do, and added it to all of those environments. One of these tasks was getting connections to databases within the org. So we added a function called <code>get_connection</code> where any user could just pass in the connection id for a database, and get a connection to query that database. All ids were listed in the README notebook. This solved the problem of hardcoded credentials inside notebooks being pushed to GitHub.</p>
<pre><code>
  <span>import</span> pandas
  <span>import</span> toolbox

  con = toolbox.get_connection(<span>&#34;redshift&#34;</span>)
  df = pandas.read_sql(<span>&#34;SELECT * FROM schema.table;&#34;</span>, con=con)

</code></pre>

<p>The library also had functions for some other common tasks like:</p>
<ul>
<li>Sending emails</li>
<li>Storing and retrieving files from S3</li>
<li>Loading data into our data warehouse</li>
</ul>
<pre><code>
  <span>import</span> toolbox

  toolbox.send_email(from_email, to_email, subject, body)

  toolbox.to_s3(file_path, s3_path)
  toolbox.from_s3(s3_path, file_path)

  toolbox.to_redshift(df, **kwargs)

</code></pre>

<p>Whenever we released a new version of the toolbox library, we built all the docker images again, pushed them to our registry, and upgraded the JupyterHub deployment on Kubernetes. That made sure that everyone got the latest goodies when they launched a new server. This did not affect users with running servers, because they would get the new environment only when they re-launched their servers.</p>
<p>JupyterHub was adopted very quickly as it undid a lot of knots from the setup we had earlier. Now that we&#39;d ensured a consistent and reproducible experience with JupyterHub, the next question was <em>&#34;How do we replicate the same experience for scheduling notebooks&#34;</em> while also making the scheduling process very easy.</p>
<h3>Airflow</h3>
<p>At this point, the data team had grown and there were more users on the platform who wanted to schedule notebooks. This amplified some of the scheduling problems I mentioned earlier, and it started to become difficult to scale and maintain Airflow on that old EC2 instance.</p>
<h4>What is Airflow?</h4>
<p>If you&#39;re not familiar with Airflow, it&#39;s a platform where you can create and monitor workflows. You can think of it as a powerful cron. It has a Python API that lets you define workflows as DAGs, or <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a>. It also comes with a lot of operators that let you connect to external systems and define individual tasks within your DAG workflow.</p>
<p>Each DAG is a Python file where you import the DAG class, and instantiate a DAG object with some metadata, like the <code>dag_id</code>, <code>owner</code>, the <code>date</code> from which it should start running, and how often it should run using cron-like syntax.</p>
<p>You can then create tasks using a bazillion operators, for example, the <code>PythonOperator</code> lets you create a task to run any Python function. Each task needs to have an id, the Python function it should run, and the DAG to which it belongs. At the end, you can define the dependency between tasks using the <code>&lt;&lt;</code> and <code>&gt;&gt;</code> syntax.</p>
<pre><code>
  <span>from</span> airflow.models <span>import</span> DAG
  <span>from</span> airflow.operators <span>import</span> PythonOperator

  dag = DAG(
      dag_id=<span>&#39;python_v1&#39;</span>,
      owner=<span>&#39;vinayak&#39;</span>,
      start_date=<span>&#39;2020-09-01&#39;</span>,
      schedule_interval=<span>&#39;0 0 * * *&#39;</span>,
  )

  task1 = PythonOperator(
      task_id=<span>&#39;task_1&#39;</span>,
      python_callable=func1,
      dag=dag,
  )

  task2 = PythonOperator(
      task_id=<span>&#39;task_2&#39;</span>,
      python_callable=func2,
      dag=dag,
  )

  task1 &gt;&gt; task2

</code></pre>

<p>Airflow has a web interface where you can see what the DAG looks like. This interface also shows you all the times the DAG has run, the status of each run, and the status of each task in that run. You can also click on individual tasks to view all of their logs.</p>
<div>
    <figure>
        <img src="https://cs16.samke.me/files/airflow-dag-simple.png"/>
    </figure>
</div>

<p>It has a concept of executors which define how and where you task will be executed, for example <code>celery</code> or <code>dask</code>. You can also configure the executors to run on a single server, or in a distributed setup.</p>
<h4>KubernetesExecutor</h4>
<p>When we were working on solving our scheduling problems, we found the <code>KubernetesExecutor</code>, which was shiny and brand new at that point. It lets you run your DAG&#39;s tasks in their own Kubernetes pods.</p>
<p>We just had to define an executor config inside our Python DAG files with the docker images we wanted the tasks to run with. We also defined CPU and memory resources for the task pod, and added that executor config to our tasks.</p>
<pre><code>
  executor_config = {
      <span>&#34;KubernetesExecutor&#34;</span>: {
          <span>&#34;image&#34;</span>: <span>&#34;&lt;registry_url&gt;/airflow:latest&#34;</span>,
          <span>&#34;request_memory&#34;</span>: <span>&#34;500M&#34;</span>,
          <span>&#34;limit_memory&#34;</span>: <span>&#34;4G&#34;</span>,
          <span>&#34;request_cpu&#34;</span>: <span>0.5</span>,
          <span>&#34;limit_cpu&#34;</span>: <span>1</span>,
      }
  }

</code></pre>

<h4>Running notebooks with Papermill</h4>
<p>We used <a href="https://papermill.readthedocs.io/en/latest/">Papermill</a> to execute these notebooks, because it lets you define a parameters cell to inject inputs at runtime. Airflow also supports a <code>PapermillOperator</code> which meant that we could run our notebooks in their own Kubernetes pod using Papermill, use Airflow macros as notebook inputs, and store the output notebook on S3 for debugging in case things went south.</p>
<p>We used these Airflow macros:</p>
<ul>
<li><code>{{ dag }}</code>: the DAG object, <code>dag.dag_id</code></li>
<li><code>{{ run_id }}</code>: the <code>run_id</code> of the current run</li>
<li><code>{{ ds }}</code>: the current execution date</li>
<li><code>{{ prev_ds }}</code>: the previous execution date</li>
</ul>
<p>Which helped us:</p>
<ul>
<li>Create a nice file hierarchy for output notebooks on S3 by specifying the dag and run ids in the Papermill output path</li>
<li>Easily perform incremental loads to backfill data into a sink table, by aligning the start date and end date for the code and queries in the notebook with Airflow&#39;s scheduling as the source of truth</li>
</ul>
<pre><code>
  <span>from</span> airflow.operators <span>import</span> PapermillOperator

  notebook_task = PapermillOperator(
      ...
      output_nb=<span>&#34;s3://airflow/{{ dag.dag_id }}/{{ run_id }}/notebook.ipynb&#34;</span>,
      parameters={
          <span>&#34;start_date&#34;</span>: <span>&#34;{{ prev_ds }}&#34;</span>,
          <span>&#34;end_date&#34;</span>: <span>&#34;{{ ds }}&#34;</span>
      },
      ...
  )

</code></pre>

<p>We configured Airflow to send alerts on Slack whenever a notebook failed. The alert would tag the DAG owner and also have a URL for the output notebook that they could open in their browser. Once a day in the morning, we would also send an automated email to every DAG owner with a short summary of notebook scheduling stats for the past day.</p>
<h4>Writing DAGs</h4>
<p>The next question was <em>&#34;How do we make authoring DAGs easy for anyone&#34;</em>. We didn&#39;t want to stop anyone from scheduling their notebooks just because they weren&#39;t familiar with Airflow&#39;s Python API.</p>
<p>We came up with YAML DAG definitions where any user could declare some DAG metadata using YAML instead of Python. Since most of the DAGs looked similar, we create Jinja templates for each type. Single notebook DAGs were the simplest as they just needed one <code>PapermillOperator</code>. Based on the DAG metadata a user provided, we would generate the final Python DAG file by rendering the Jinja template with the YAML values.</p>
<p>This still required a user to first write YAML, build a DAG file, and then also learn <code>git</code> in order to push all the files to the Airflow DAGs GitHub repo we maintainted. Learning git was painful for new users, and it shouldn&#39;t have been a requirement in the first place. You shouldn&#39;t be required to learn git to access and work with data.</p>
<p>To get around this, we cloned and automatically updated the DAGs GitHub repo using pod hooks every time sometime launched their JupyterLab server. When a user wanted to schedule their notebook, they could go into the cloned directory and create a new notebook project using a CLI tool we baked into the environment.</p>
<pre><code>
  $ cd airflow-dags/dags
  $ dag create notebook
  $ ls
  dag.yml notebook.ipynb

</code></pre>

<p>They could then write some DAG metadata like their name and the scheduling information, copy their notebook into the project.</p>
<pre><code>
  dag_id: notebook_v1
  owner: vinayak
  start_date: &#39;2020-09-01&#39;
  schedule_interval: &#39;0 0 * * *&#39;
  notebook_name: notebook.ipynb
  parameters:
    msgs: &#39;Ran from Airflow at {{ execution_date }}!&#39;

</code></pre>

<p>And push it to GitHub with a simple command. This would open a pull request on the Airflow DAGs GitHub repo.</p>
<pre><code>
  $ dag push
  All done! ✨ 🍰 ✨
  You can see your pull request here: &lt;link&gt;

</code></pre>

<p>We would then trigger a build job which would render the Python DAG file, and push it to the pull request&#39;s branch. The job also did some other things like:</p>
<ul>
<li>Checking for missing fields</li>
<li>Validating existing fields</li>
<li>Checking for syntax errors in code</li>
<li>Formatting the code</li>
<li>And finally, adding a comment on the pull request with the build status</li>
</ul>
<p>After the pull request was merged, the new DAG would be deployed to the EFS volume that was mounted on all Airflow pods. Voila !</p>
<h4>Deploying DAGs</h4>
<p>We also had to figure out how we&#39;d deliver new DAGs to the Airflow DAGs folder for the <code>KubernetesExecutor</code>. On one server, all Airflow components can see the DAGs folder, because they share the same filesystem. But in a distributed setup like Kubernetes, there are three ways we could go about it:</p>
<ol>
<li>Have a sidecar container in all the Airflow pods that would regularly pull new DAGs from a GitHub repo</li>
<li>Build the DAGs into the Airflow docker image itself which can then be reused across all Airflow pods</li>
<li>Have a way to push new DAGs to a network file system which can be mounted on all Airflow pods</li>
</ol>
<p>We had a GitHub repo where we maintained all our DAGs but we went with the third way because we already had a network file system, EFS, that we were using for JupyterHub. We didn&#39;t go the second way as we didn&#39;t want to build a new Airflow docker image every time someone added a dag, since that was happening quite often.</p>
<p>This Airflow setup fit nicely with our JupyterHub setup, because we could now schedule notebooks to run in the same environment that they were written in.</p>
<p>This notebook-based data platform was adopted by every data analyst and scientist, and a lot people in non-data roles across both our offices in Delhi and Bangalore.</p>
<p>These are some types of workflows that everyone built:</p>
<ul>
<li>Notebook ETLs that would load data into tables in Redshift, our data warehouse, which powered our Tableau dashboards</li>
<li>Notebooks that would send send reports to various people inside the org if some condition on some table wasn&#39;t met</li>
<li>Notebooks what would train machine learning models on new data every day and load predictions in a table that was used by a consumer facing system</li>
<li>Very complex notebook workflows where multiple notebooks would run in parallel to calculate the stock of items in different warehouses, with one final notebook calling the API for a downstream system that would send relevant orders to replenish the warehouse stock for the next day/week</li>
<li>And many more!</li>
</ul>
<h2>Learnings</h2>
<h3>JupyterHub culling or: How I learned to live long and prosper</h3>
<p>JupyterHub has this nice feature where it can terminate (or cull) inactive user pods. This helps to scale everything down when resources aren&#39;t in use. You can set an inactivity time period after which a pod should be terminated.</p>
<p>We found that this feature would cull user pods even if they had JupyterLab open in their browser and context switched to do something else. This would usually happen when they were running a big task like training a model. It was painful for them when they came back and saw that their long running task didn&#39;t complete because their server was no longer there.</p>
<p>To overcome this, we used another JupyterHub feature called <a href="https://jupyterhub.readthedocs.io/en/stable/howto/configuration/config-user-env.html#named-servers">named servers</a> where you can launch a server and also give it a nickname. We modified the culling behavior to ignore pods that were launched with the <code>-llap</code> suffix in the nickname, where llap stands for Live long and prosper 🖖.</p>
<h3>Pinning dependencies</h3>
<p>Initially we were using the latest tag on the datascience notebook docker image, but this one time after building all the images and upgrading the JupyterHub deployment, every user would see an error when they tried to launch a new pod.</p>
<p>It took a long time for us to figure out that the new version of JupyterLab broke some old <code>KubeSpawner</code> behavior that we were depending on.</p>
<p>Pinning dependencies also helped us move quickly when we had to migrate from our old kubernetes cluster to a new one. We just installed the helm chart with the same config values, pointed the domain name to the new IP, and no user even felt that a migration had taken place.</p>
<h3>Monitoring</h3>
<p>Monitoring this whole system using Prometheus and Grafana helped us get relevant metrics to improve user experience and also help in resource planning.</p>
<p>After we started monitoring the resource usage for our JupyterHub deployment, we found that we could fit the same number of users on half the nodes which helped us reduce cost by 50%.</p>
<h3>Backups</h3>
<p>This one time, a user accidentally deleted their notebook which contained all the work they&#39;d been doing for the past one week. Yes, they should have pushed it to GitHub, and no, we couldn’t find it in the notebook checkpoints. However, we managed to recover it from the latest EFS backup.</p>
<p>Another time an engineer accidentally reset the Airflow metadata database on which the Airflow scheduler has a hard dependency, but we were able to quickly restore it from the latest RDS snapshot.</p>
<h2>Improvements</h2>
<h3>Remove accidental complexity</h3>
<p>For new users, a graphical interface is often more intuitive than a CLI. New users shouldn&#39;t have to pay the command-line tax and learn <code>git</code> just to work with data on the platform.</p>
<h3>Notebook pipeline editors</h3>
<p><a href="https://elyra.readthedocs.io/en/stable/index.html">Elyra</a> is a cool project that provides a nice visual editor in which you can drag and drop notebooks and build complex notebook pipelines.</p>
<div>
    <figure>
        <img src="https://cs16.samke.me/files/elyra-pipeline-editor.png"/>
    </figure>
</div>

<p>It also has a nice interface to review changes made to notebooks.</p>
<div>
    <figure>
        <img src="https://cs16.samke.me/files/elyra-git.png"/>
    </figure>
</div>

<h3>Make notebooks discoverable</h3>
<p>In a notebook-based data platform, notebooks should become first-class citizens just like tables so that it&#39;s easy to discover them and build on past work. <a href="https://www.amundsen.io/amundsen/">Amundsen</a> (and a bunch of other tools) can help you build a searchable catalog for your notebooks.</p>
<div>
    <figure>
        <img src="https://cs16.samke.me/files/amundsen.png"/>
    </figure>
</div>

<hr/>
<p>I hope this post provided useful insights into our experience setting up a notebook-based data platform. I also hope that it will guide you in selecting the right open-source tools and avoiding common pitfalls when setting up something similar. Reach out to me if you&#39;re working on something along these lines, I would love to hear what you&#39;re up to.</p>
<hr/><ol><li id="sf-notebookops-1">Deploying a cloud-based JupyterHub for students and researchers <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68397.html">source</a> <a href="#sf-notebookops-1-back">↩︎</a></li><li id="sf-notebookops-2">Enterprise usage of Jupyter: The business case and best practices for leveraging open source <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71217.html">source</a> <a href="#sf-notebookops-2-back">↩︎</a></li><li id="sf-notebookops-3">Scheduled notebooks: A means for manageable and traceable code execution <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68348.html">source</a> <a href="#sf-notebookops-3-back">↩︎</a></li><li id="sf-notebookops-4">Notebooks at Netflix: From analytics to engineering (sponsored by Netflix) <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71601.html">source</a> <a href="#sf-notebookops-4-back">↩︎</a></li></ol>
</div></div>
  </body>
</html>
