<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gendignoux.com/blog/2024/11/18/rust-rayon-optimized.html">Original</a>
    <h1>Making a parallel Rust workload 10x faster with (or without) Rayon</h1>
    
    <div id="readability-page-1" class="page"><article>
    <header>
        
        <p>
            <a href="http://jimkang.com/blog/tags.html#rust"><i aria-hidden="true"></i> rust</a><a href="http://jimkang.com/blog/tags.html#perf"><i aria-hidden="true"></i> perf</a>
        </p>
        <p>November 18, 2024</p>
        <p>by <span>Guillaume Endignoux</span>
        </p>
    </header>
    <section>
        <p>In a <a href="http://jimkang.com/blog/2023/03/27/single-transferable-vote.html#parallelism">previous post</a>, I’ve shown how to use the <a href="https://docs.rs/rayon/"><code>rayon</code> framework</a> in Rust to automatically parallelize a loop computation across multiple CPU cores.
Disappointingly, my <a href="http://jimkang.com/blog/2023/03/27/single-transferable-vote.html#benchmarks">benchmarks</a> showed that this only provided a 2x speedup for my workload, on a computer with 8 CPU threads.
Worse, the total “user” and “system” times increased linearly with the number of threads, meaning potentially more wasted work.
Even Python was only twice slower than my Rust code, when Rust is typically 10x to 100x faster than Python.</p>

<!--more-->

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/plot-parallel-rand2x10.6H1A44kAkKxR.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/plot-parallel-rand2x10-dark.ymJIOvWjRtUz.svg" media="(prefers-color-scheme: dark)" width="800" height="600"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/plot-parallel-rand2x10.6H1A44kAkKxR.svg" width="800" height="600" loading="lazy" alt="Benchmarks of parallelism (ballot file = rand_2x10)"/>
</picture>
</a>
<a href="http://jimkang.com/blog/2023/03/27/single-transferable-vote.html#benchmarks">Previous benchmark</a> of my Rust code vs. a reference Python implementation.
</p>

<p>This was the starting point of an optimization journey that led me to a 10x speed-up from this baseline.
In this post, I’ll first explain which profiling tools I used to chase optimizations, before diving into how I built a faster replacement of Rayon for my use case.
In the <a href="http://jimkang.com/blog/2024/12/02/rust-data-oriented-design.html">next post</a>, I’ll describe the other optimizations that made my code much faster.
Spoiler alert: copying some data sped up my code!</p>

<p>Wait, what?
Copying data, really?!
Isn’t the whole point of fighting the Rust borrow checker that you unlock super-optimized zero-copy data structures?</p>

<p>If you’re confused about how copying could possibly be good for performance, subscribe and stay tuned to learn why!</p>

<p>
<a href="http://jimkang.com/blog/images/rust-data-oriented-design/plot-stv.473bcd6-parallel-rand2x10.OD8zhsdTu2Gu.svg">
<picture>
  <source srcset="/blog/images/rust-data-oriented-design/plot-stv.473bcd6-parallel-rand2x10-dark.BlUEXG1VtMgR.svg" media="(prefers-color-scheme: dark)" width="800" height="600"/>
  <img src="http://jimkang.com/blog/images/rust-data-oriented-design/plot-stv.473bcd6-parallel-rand2x10.OD8zhsdTu2Gu.svg" width="800" height="600" loading="lazy" alt="Benchmarks of optimized parallel implementation (ballot file = rand_2x10)"/>
</picture>
</a>
The same benchmark <a href="https://github.com/gendx/stv-rs/tree/473bcd652372e341e934b630dd71c02ec65fd5fa">after optimizing</a>. Dashed lines = with Rayon, solid lines = with custom parallelism.
</p>

<hr/>

<ul id="markdown-toc">
  <li><a href="#using-the-right-profiling-tools" id="markdown-toc-using-the-right-profiling-tools">Using the right profiling tools</a>    <ul>
      <li><a href="#a-quick-look-at-system-calls" id="markdown-toc-a-quick-look-at-system-calls">A quick look at system calls</a></li>
      <li><a href="#time-based-profiling-with-perf" id="markdown-toc-time-based-profiling-with-perf">Time-based profiling with <code>perf</code></a></li>
      <li><a href="#advanced-perf-usage" id="markdown-toc-advanced-perf-usage">Advanced <code>perf</code> usage</a></li>
      <li><a href="#a-word-about-cpu-caches" id="markdown-toc-a-word-about-cpu-caches">A word about CPU caches</a></li>
    </ul>
  </li>
  <li><a href="#a-hand-rolled-replacement-of-rayon" id="markdown-toc-a-hand-rolled-replacement-of-rayon">A hand-rolled replacement of Rayon?</a>    <ul>
      <li><a href="#how-does-rayon-work" id="markdown-toc-how-does-rayon-work">How does Rayon work?</a></li>
      <li><a href="#hand-rolling-a-thread-pool" id="markdown-toc-hand-rolling-a-thread-pool">Hand-rolling a thread pool</a></li>
      <li><a href="#cpu-pinning" id="markdown-toc-cpu-pinning">CPU pinning</a></li>
      <li><a href="#work-stealing" id="markdown-toc-work-stealing">Work stealing</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#bonus-does-rayons-with_max_len-help" id="markdown-toc-bonus-does-rayons-with_max_len-help">Bonus: Does Rayon’s <code>with_max_len()</code> help?</a></li>
</ul>

<h2 id="using-the-right-profiling-tools"><span><a href="#using-the-right-profiling-tools"><i aria-hidden="true"></i></a></span>Using the right profiling tools</h2>

<p>Before diving into potential optimizations, I followed the best practice of starting with profiling: this allows to see which parts of the program are a bottleneck and should be optimized, rather than making potentially incorrect assumptions and wasting time optimizing things that don’t matter.
As we’ll see <a href="http://jimkang.com/blog/2024/12/02/rust-data-oriented-design.html">in the next post</a>, this approach is not perfect either, but it should definitely be a good start.</p>

<p>I’m using Linux, and therefore will focus on two tools that I’ve already presented in previous posts: <code>strace</code> (see <a href="http://jimkang.com/blog/2020/09/06/strace-docker-password-manager.html">how to inspect a password manager</a>) and <code>perf</code> (see <a href="http://jimkang.com/blog/2019/11/09/profiling-rust-docker-perf.html">how to profile Rust code within Docker</a>).</p>

<h3 id="a-quick-look-at-system-calls"><span><a href="#a-quick-look-at-system-calls"><i aria-hidden="true"></i></a></span>A quick look at system calls</h3>

<p>Since my <a href="http://jimkang.com/blog/2023/03/27/single-transferable-vote.html#benchmarks">benchmarks</a> showed that the “system” time was increasing with the number of threads, a hypothesis is that Rayon wastes time making too many system calls.
So let’s start with <a href="https://man7.org/linux/man-pages/man1/strace.1.html"><code>strace</code></a> to see if any system calls are prominent.</p>

<div><div><pre><code><span>$ </span>strace ./target/release/stv-rs &lt;parameters&gt;
...
futex<span>(</span>0x562503010900, FUTEX_WAKE_PRIVATE, 1<span>)</span> <span>=</span> 1
futex<span>(</span>0x7f60e525ebb4, FUTEX_WAIT_BITSET_PRIVATE, 282, NULL, FUTEX_BITSET_MATCH_ANY<span>)</span> <span>=</span> 0
futex<span>(</span>0x562503010908, FUTEX_WAKE_PRIVATE, 1<span>)</span> <span>=</span> 1
futex<span>(</span>0x562503010900, FUTEX_WAKE_PRIVATE, 1<span>)</span> <span>=</span> 1
futex<span>(</span>0x7f60e525ebb4, FUTEX_WAIT_BITSET_PRIVATE, 283, NULL, FUTEX_BITSET_MATCH_ANY<span>)</span> <span>=</span> 0
futex<span>(</span>0x7f60e525ebb4, FUTEX_WAIT_BITSET_PRIVATE, 284, NULL, FUTEX_BITSET_MATCH_ANY<span>)</span> <span>=</span> 0
...
</code></pre></div></div>

<p>We see a lot of <a href="https://man7.org/linux/man-pages/man2/futex.2.html"><code>futex</code></a> calls, which could indicate contention on mutexes.
However, the default command only traces the main thread, which excludes the worker threads spawned by Rayon and that should do most of the work.
Using the <code>-f</code> flag (a.k.a. <code>--follow-forks</code>) to trace them shows that most background threads are calling <a href="https://man7.org/linux/man-pages/man2/sched_yield.2.html"><code>sched_yield</code></a> in a loop.</p>

<div><div><pre><code><span>$ </span>strace <span>-f</span> ./target/release/stv-rs &lt;parameters&gt;
...
<span>[</span>pid 24378] sched_yield<span>(</span> &lt;unfinished ...&gt;
<span>[</span>pid 24377] futex<span>(</span>0x7fc01c176bb4, FUTEX_WAKE_PRIVATE, 2147483647 &lt;unfinished ...&gt;
<span>[</span>pid 24376] sched_yield<span>(</span> &lt;unfinished ...&gt;
<span>[</span>pid 24375] sched_yield<span>(</span> &lt;unfinished ...&gt;
<span>[</span>pid 24374] &lt;... sched_yield resumed&gt;<span>)</span>  <span>=</span> 0
<span>[</span>pid 24373] sched_yield<span>(</span> &lt;unfinished ...&gt;
...
</code></pre></div></div>

<p>Looking at the whole logs is not very convenient, but fortunately <code>strace</code> provides a summary mode with the <code>-c</code> flag (a.k.a. <code>--summary-only</code>), which can be combined with <code>-f</code>.
Tracing the main thread only, the highlights are:</p>
<ul>
  <li><a href="https://man7.org/linux/man-pages/man2/futex.2.html"><code>futex</code></a> – a synchronization primitive used to implement mutexes – represents the vast majority of the syscall count and time,</li>
  <li><a href="https://man7.org/linux/man-pages/man2/mmap.2.html"><code>mmap</code></a> (and a couple <code>munmap</code>s) is managing memory allocations<sup id="fnref:1"><a href="#fn:1" rel="footnote" role="doc-noteref">1</a></sup>,</li>
  <li><a href="https://man7.org/linux/man-pages/man2/write.2.html"><code>write</code></a> prints my program’s progress and results to the standard output.</li>
</ul>

<div><div><pre><code><span>$ </span>strace <span>-c</span> ./target/release/stv-rs &lt;parameters&gt;
% <span>time     </span>seconds  usecs/call     calls    errors syscall
<span>------</span> <span>-----------</span> <span>-----------</span> <span>---------</span> <span>---------</span> <span>------------------</span>
 98.80    0.269391         410       657           futex
  0.31    0.000832          33        25           mmap
  0.27    0.000745          53        14           mprotect
  0.15    0.000406           0       657           write
  0.10    0.000270          30         9           openat
  0.08    0.000214           7        29           <span>read</span>
...
<span>------</span> <span>-----------</span> <span>-----------</span> <span>---------</span> <span>---------</span> <span>------------------</span>
100.00    0.272658         184      1479         9 total
</code></pre></div></div>

<blockquote>
  <p>Latency measurement as done by <code>strace</code> is quite flaky and incurs some overhead as <a href="https://www.brendangregg.com/blog/2014-05-11/strace-wow-much-syscall.html">pointed out by Brendan Gregg</a>, but it’s still a useful tool to get an overall idea.</p>
</blockquote>

<p>Tracing all threads with <code>-cf</code>, the syscall time is split between <a href="https://man7.org/linux/man-pages/man2/futex.2.html"><code>futex</code></a> coordination and a huge amount of <a href="https://man7.org/linux/man-pages/man2/sched_yield.2.html"><code>sched_yield</code></a> calls.</p>

<div><div><pre><code><span>$ </span>strace <span>-cf</span> ./target/release/stv-rs &lt;parameters&gt;
strace: Process 24423 attached
strace: Process 24424 attached
...
% <span>time     </span>seconds  usecs/call     calls    errors syscall
<span>------</span> <span>-----------</span> <span>-----------</span> <span>---------</span> <span>---------</span> <span>------------------</span>
 49.36    0.341755          67      5079       864 futex
 48.67    0.336973           5     64938           sched_yield
  0.56    0.003856           5       657           write
  0.32    0.002218          54        41           mmap
  0.26    0.001787          59        30           mprotect
  0.17    0.001148          45        25           rt_sigprocmask
...
<span>------</span> <span>-----------</span> <span>-----------</span> <span>---------</span> <span>---------</span> <span>------------------</span>
100.00    0.692404           9     70926       873 total
</code></pre></div></div>

<blockquote>
  <p>As <a href="https://www.brendangregg.com/perf.html#StaticKernelTracing">mentioned by Brendan Gregg</a>, in principle <code>strace</code> incurs a non-negligible overhead, and the following <code>perf</code> command can be used to the same effect as <code>strace -c</code>.
However, in practice this didn’t work out-of-the-box in my <a href="http://jimkang.com/blog/2019/11/09/profiling-rust-docker-perf.html">profiling setup within Docker</a>, as seemingly some special kernel filesystem needs to be mounted.
I found <a href="https://github.com/moby/moby/issues/39457">this issue</a> on Docker’s repository, but didn’t dig further into this as the summary insights from <code>strace</code> were good enough.</p>

  <div><div><pre><code><span>$ </span>perf <span>stat</span> <span>-e</span> <span>&#39;syscalls:sys_enter_*&#39;</span> &lt;binary&gt;
event syntax error: <span>&#39;syscalls:sys_enter_*&#39;</span>
                     <span>\_</span>__ unknown tracepoint

Error:	Unable to find debugfs/tracefs
Hint:	Was your kernel compiled with debugfs/tracefs support?
Hint:	Is the debugfs/tracefs filesystem mounted?
Hint:	Try <span>&#39;sudo mount -t debugfs nodev /sys/kernel/debug&#39;</span>
</code></pre></div>  </div>
</blockquote>

<h3 id="time-based-profiling-with-perf"><span><a href="#time-based-profiling-with-perf"><i aria-hidden="true"></i></a></span>Time-based profiling with <code>perf</code></h3>

<p>If you’re using Linux, the most versatile profiling tool is probably the built-in <a href="https://www.man7.org/linux/man-pages/man1/perf.1.html"><code>perf</code></a> command.
This allows to record an execution trace of a program to generate <a href="https://www.brendangregg.com/flamegraphs.html">flame graphs</a>, but also to record statistics about plenty of performance counters exposed by your CPU or the system.
A <a href="https://perfwiki.github.io/main/tutorial/">comprehensive tutorial</a> is available on the Linux kernel Wiki, and you’ll find many examples on <a href="https://www.brendangregg.com/perf.html">Brendan Gregg’s website</a>.</p>

<p>I’ve previously written the <a href="http://jimkang.com/blog/2019/11/09/profiling-rust-docker-perf.html#profiling-rust-code">important steps</a> to make it work with Rust, which can be summarized as follows.</p>

<div><div><pre><code><span># Unlock profiling kernel settings if needed.</span>
<span>echo</span> <span>-1</span> <span>&gt;</span> /proc/sys/kernel/perf_event_paranoid
<span>echo </span>0 <span>&gt;</span> /proc/sys/kernel/kptr_restrict
<span>echo </span>0 <span>&gt;</span> /proc/sys/kernel/nmi_watchdog

<span># Compile Rust code with frame pointers.</span>
<span>RUSTFLAGS</span><span>=</span><span>&#39;-C force-frame-pointers=y&#39;</span> cargo build <span>--release</span>

<span># Record a time-based profile, at the given frequency.</span>
perf record <span>-F</span> &lt;frequency&gt; <span>-g</span> ./target/release/&lt;binary&gt;
<span># Spawn an interactive report in the terminal.</span>
perf report <span>-g</span> graph,0.5,caller
</code></pre></div></div>

<p>With this interactive report, we can dig at a low level into the assembly.</p>
<ul>
  <li>Compiling your Rust program with <a href="https://doc.rust-lang.org/cargo/reference/profiles.html#debug">debug annotations</a> can make the assembly easier to read, by showing lines of source code next to instructions.
For this, add <code>debug = true</code> to your <a href="https://doc.rust-lang.org/cargo/reference/profiles.html#release">release profile</a>.</li>
  <li>You can also export this assembly into the standard output via <a href="https://man7.org/linux/man-pages/man1/perf-annotate.1.html"><code>perf annotate</code></a> (see the <a href="https://perfwiki.github.io/main/tutorial/#source-level-analysis-with-perf-annotate">tutorial</a>, e.g. <code>perf annotate -v --asm-raw --stdio</code>).</li>
</ul>

<p>Seeing which instructions are a bottleneck in the assembly is very low level, so at the other end of the spectrum <a href="https://www.brendangregg.com/flamegraphs.html">flame graphs</a> provide a top-down overview of where time is spent in the program.
A nice UI for that is the <a href="https://profiler.firefox.com">Firefox Profiler</a>, which runs in the browser<sup id="fnref:2"><a href="#fn:2" rel="footnote" role="doc-noteref">2</a></sup> but keeps your profiles and analysis local, unless you explicitly share them.
You’ll need to transform <code>perf record</code>’s output using <a href="https://man7.org/linux/man-pages/man1/perf-script.1.html"><code>perf script</code></a>, and then simply load that into the Firefox Profiler:</p>

<div><div><pre><code>perf script <span>--input</span><span>=</span>file.perf <span>-F</span> +pid <span>&gt;</span> file.processed.perf
</code></pre></div></div>

<p>Once loaded, the interface shows two panels.
At the top, you can see a timeline broken down by threads: yellow shows the time spent in userspace and orange in the kernel.
Small blue ticks also show when stack traces were captured in each thread.
From there, you can select which thread you want to focus on, to display details in the bottom panel.
The default view is the “call tree”, showing a graph of functions calls grouped as a tree.</p>

<p><a href="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-tree-view.9RCOOeaIZuqM.png"><img src="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-tree-view.1XUVAZQmy9dV.webp" width="1829" height="1238" loading="lazy" alt="Firefox Profiler: tree view"/></a>
The Firefox Profiler showing the default call tree view.</p>

<blockquote>
  <p>You may need to activate all “tracks” in the top-left corner to show all the threads.</p>
</blockquote>

<p>For my Rayon workload, we can see that the main thread (top-most timeline) is mostly idle waiting for the Rayon worker threads.
The default call tree is not really useful, but shows that a lot of recursion happens between two internal functions of Rayon: <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/fn.bridge_producer_consumer.html"><code>bridge_producer_consumer()</code></a> and <a href="https://docs.rs/rayon-core/1.12.1/rayon_core/fn.join_context.html"><code>join_context()</code></a>.</p>

<p>The next tab shows a flame graph, which is equivalent to a call tree but laid out from the bottom up and with the width of each node representing the total time spent in it.
As you can see, it’s not very useful out-of-the-box for Rayon due to the recursion.
This problem was previously reported in <a href="https://github.com/rayon-rs/rayon/issues/591">rayon/issues/591</a>, and also potentially confused a user <a href="https://www.reddit.com/r/rust/comments/bto10h/update_a_scaling_comparison_between_rustrayon_and/">comparing Rayon with OpenMP</a>.</p>

<p><a href="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-flame-graph.KTuEz_ZuAmzu.png"><img src="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-flame-graph.V3lzQykGrYwW.webp" width="1829" height="744" loading="lazy" alt="Firefox Profiler: flame graph"/></a>
Raw flame graph in the Firefox Profiler.</p>

<p>Fortunately, the Firefox Profiler provides a tool to alleviate that by “collapsing recursion”, accessible in the context menu by right-clicking on a node.
The view is clearer: all Rayon functions are collapsed, and we can see that most of the time is spent in my <a href="https://docs.rs/stv-rs/0.4.0/stv_rs/vote_count/struct.VoteCount.html#method.process_ballot">ballot-counting function</a>, at the core of the <a href="http://jimkang.com/blog/2023/03/27/single-transferable-vote.html#parallelism">map-reduce</a> operation that I perform with Rayon.</p>

<p>There’s also some non-negligible time spent in allocations, system calls (<code>futex</code>, as we saw above with <code>strace</code>), and some other unknown function from the libc.</p>

<p><a href="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-collapse-recursion.aZXhPOsYxBFY.png"><img src="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-collapse-recursion.kDHcIizYmJS8.webp" width="1829" height="745" loading="lazy" alt="Firefox Profiler: flame graph with recursion collapsed"/></a>
Flame graph with recursion collapsed in the Firefox Profiler.</p>

<p>Lastly, back to the call tree view, there is an option to invert the call stack.
Rather than showing the thread’s main function at the root, it will show a root for each leaf function where the program is actually spending time.
It’s still a “forest” structure: you can expand a tree for each function to learn about its callers.</p>

<p><a href="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-invert-call-stack.3ae6qbf5XwvP.png"><img src="http://jimkang.com/blog/images/rust-rayon-optimized/firefox-profiler-invert-call-stack.FBW5o3Lj6pRC.webp" width="1829" height="744" loading="lazy" alt="Firefox Profiler: tree view with inverted call stack"/></a>
Inverted call stack in the Firefox Profiler.</p>

<p>In my example, we learn that most of the time is spent performing arithmetic with <a href="https://docs.rs/num-bigint/0.4.6/num_bigint/struct.BigUint.html"><code>BigUint</code></a>, which makes sense as it’s what my parallel loop does.
It also seems that Rayon doesn’t incur direct overhead, but <a href="#a-hand-rolled-replacement-of-rayon">as we will see later</a> overhead can be more subtle and indirect.</p>

<h3 id="advanced-perf-usage"><span><a href="#advanced-perf-usage"><i aria-hidden="true"></i></a></span>Advanced <code>perf</code> usage</h3>

<p>So far, we’ve only used <code>perf</code> to record stack traces at a regular time interval.
This is useful, but only scratching the surface.</p>

<p>Another important tool is the <a href="https://man7.org/linux/man-pages/man1/perf-stat.1.html"><code>perf stat</code></a> command, which lets you record statistics about plenty of <a href="https://perfwiki.github.io/main/tutorial/#events">performance counters</a>.
By default, you’ll get basic statistics about the number of executed instructions, the number of CPU cycles (which isn’t proportional to the instructions due to <a href="https://en.wikipedia.org/wiki/Superscalar_processor">superscalar processors</a>), the number of executed <a href="https://en.wikipedia.org/wiki/Branch_(computer_science)#Performance_problems_with_branch_instructions">branch instructions</a> and how many of those were branch misses (which is relevant for performance due to <a href="https://en.wikipedia.org/wiki/Speculative_execution">speculative execution</a>), etc.</p>

<div><div><pre><code><span>$ </span>perf <span>stat</span> ./target/release/stv-rs &lt;parameters&gt;
          22932.25 msec task-clock                       <span>#    5.864 CPUs utilized             </span>
              4166      context-switches                 <span>#  181.666 /sec                      </span>
                 7      cpu-migrations                   <span>#    0.305 /sec                      </span>
               552      page-faults                      <span>#   24.071 /sec                      </span>
       76612230632      cycles                           <span>#    3.341 GHz                         (50.10%)</span>
      139165609317      instructions                     <span>#    1.82  insn per cycle              (60.19%)</span>
       26963459753      branches                         <span>#    1.176 G/sec                       (60.12%)</span>
         106756318      branch-misses                    <span>#    0.40% of all branches             (60.08%)</span>
...
</code></pre></div></div>

<blockquote>
  <p>These kind of performance counters are recorded regularly as part of the <a href="https://perf.rust-lang.org/index.html"><code>rustc</code> performance dashboard</a>, tracking the performance of the Rust compiler over time.</p>
</blockquote>

<p>Beyond these default statistics, there is a plethora of performance counters to choose from: the <a href="https://man7.org/linux/man-pages/man1/perf-list.1.html"><code>perf list</code></a> command lists the counters available on your system and the <code>-e</code> flag allows to customize which ones to collect.</p>

<p>In particular, the <code>-d</code> flag (and its verbose mode <code>-d -d</code>) adds counters about <a href="https://en.wikipedia.org/wiki/CPU_cache#Cache_miss">cache misses</a>.</p>

<div><div><pre><code><span>$ </span>perf <span>stat</span> <span>-d</span> <span>-d</span> ./target/release/stv-rs &lt;parameters&gt;
...
       34611834184      L1-dcache-loads                  <span>#    1.461 G/sec                       (26.32%)</span>
          22467119      L1-dcache-load-misses            <span>#    0.06% of all L1-dcache accesses   (26.24%)</span>
           4790625      LLC-loads                        <span>#  202.248 K/sec                       (20.94%)</span>
             50698      LLC-load-misses                  <span>#    1.06% of all LL-cache accesses    (21.10%)</span>
   &lt;not supported&gt;      L1-icache-loads                                                       
          69732259      L1-icache-load-misses                                                   <span>(</span>21.33%<span>)</span>
...
</code></pre></div></div>

<p>Some other useful flags:</p>

<ul>
  <li><code>-r &lt;number&gt;</code> will run your program repeatedly to collect the average value of each counter,</li>
  <li>the <code>:u</code> and <code>:k</code> suffixes distinguish events generated in userspace and in the kernel (see <a href="https://man7.org/linux/man-pages/man1/perf-list.1.html#EVENT_MODIFIERS">“event modifiers” in the documentation</a>).</li>
</ul>

<div><div><pre><code><span>$ </span>perf <span>stat</span> <span>-r</span> 10 <span>-e</span> cycles:u,instructions:u,cycles:k,instructions:k ./target/release/stv-rs &lt;parameters&gt;

 Performance counter stats <span>for</span> <span>&#39;./target/release/stv-rs ...&#39;</span> <span>(</span>10 runs<span>)</span>:

       75989870908      cycles:u                                                                <span>(</span> +-  0.51% <span>)</span>
      138866699395      instructions:u                   <span>#    1.83  insn per cycle              ( +-  0.00% )</span>
         785332709      cycles:k                                                                <span>(</span> +-  0.57% <span>)</span>
         356062488      instructions:k                   <span>#    0.45  insn per cycle              ( +-  0.29% )</span>

             4.945 +- 0.198 seconds <span>time </span>elapsed  <span>(</span> +-  4.01% <span>)</span>
</code></pre></div></div>

<p>You can also combine performance events with flame graphs, by sampling a stack trace every time a given event happens (rather than at a <a href="#time-based-profiling-with-perf">regular time interval</a>).
This is useful if you want to check which parts of your code trigger cache misses for example.</p>

<div><div><pre><code><span>$ </span>perf record <span>-g</span> <span>-e</span> &lt;event&gt; <span>-c</span> &lt;interval&gt; ./target/release/&lt;binary&gt;
</code></pre></div></div>

<blockquote>
  <p>As before, the <code>-e</code> flag controls which event to sample (e.g. <code>branch-misses</code>).
Additionally, the <code>-c</code> flag allows to throttle how often an event should yield a stack trace.
For example <code>-c 100</code> will record a stack trace every 100 events.
This is useful for frequent events, to avoid interfering too often with the program and to reduce the size of the performance report file.</p>
</blockquote>

<h3 id="a-word-about-cpu-caches"><span><a href="#a-word-about-cpu-caches"><i aria-hidden="true"></i></a></span>A word about CPU caches</h3>

<p>An important class of events that shows up in <code>perf stat</code> is cache misses.
The physical reality of hardware is that RAM is slow, so in practice CPUs use a combination of very fast registers directly on the computing units, and typically 3 levels of cache.
However, while computers have had gigabytes of RAM for a long time, CPU caches are still ridiculously small.</p>

<p>For example, the <a href="https://man7.org/linux/man-pages/man1/lscpu.1.html"><code>lscpu</code></a> command reveals that my 6-year-old 4-core Intel-based laptop has: one L1 data cache of 32 KB per core, one L1 instruction cache of 32 KB per core, one L2 cache of 256 KB per core and a single L3 cache of 8 MB overall.
All of that with 8 GB of RAM!</p>

<div><div><pre><code><span>$ </span>lscpu <span>-C</span>
NAME ONE-SIZE ALL-SIZE WAYS TYPE        LEVEL SETS PHY-LINE COHERENCY-SIZE
L1d       32K     128K    8 Data            1   64        1             64
L1i       32K     128K    8 Instruction     1   64        1             64
L2       256K       1M    4 Unified         2 1024        1             64
L3         8M       8M   16 Unified         3 8192        1             64
</code></pre></div></div>

<p>Due to these constraints, optimizing CPU cache usage is essential to achieving higher performance, as demonstrated by the talk <a href="https://www.youtube.com/watch?v=WDIkqP4JbkE"><em>Cpu Caches and Why You Care</em></a> by Scott Meyers.
Here are a couple of examples.</p>

<ul>
  <li>A common strategy to improve performance is therefore to compact data structures to fit more of them in cache.
We’ll see a practical example <a href="http://jimkang.com/blog/2024/12/02/rust-data-oriented-design.html#data-oriented-design-20-faster">in the next post</a>.</li>
  <li>The cache hierarchy can give surprising effects when parallelism is at play.
For example, there are only 4 copies of the L1/L2 caches: one per core.
This means that even if the CPU supports <a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading">running 2 threads per core in parallel</a> – as <a href="https://doc.rust-lang.org/stable/std/thread/fn.available_parallelism.html"><code>std::thread::available_parallelism()</code></a> returns on my machine – these threads compete on the caches!
So even if the theoretically available parallelism is 8 threads, you can experience a performance wall at 4 threads as was the case in my <a href="http://jimkang.com/blog/2023/03/27/single-transferable-vote.html#benchmarks">previous benchmarks</a>.</li>
</ul>

<h2 id="a-hand-rolled-replacement-of-rayon"><span><a href="#a-hand-rolled-replacement-of-rayon"><i aria-hidden="true"></i></a></span>A hand-rolled replacement of Rayon?</h2>

<p>After all of this profiling, it was clear that nothing was clear.
So I set out to dig deeper into how Rayon works and see if I could find improvements.</p>

<p>In this section, I’ll briefly introduce my understanding of how Rayon works, and then describe how I built a faster replacement <em>for my use case</em>, by applying simple ideas.
It may seem crazy to think that one could improve over Rayon, given how much of a gold standard it is in the Rust ecosystem, with more than <a href="https://lib.rs/crates/rayon">7 million downloads</a> per month (and steadily growing).
The Rust compiler itself <a href="https://blog.rust-lang.org/2023/11/09/parallel-rustc.html">uses Rayon to parallelize tasks</a>.</p>

<p>One aspect is that Rayon is a generalist library that focuses on a straightforward API, so there was still hope to find improvements by specializing for a given use case and dropping the API simplicity.</p>

<h3 id="how-does-rayon-work"><span><a href="#how-does-rayon-work"><i aria-hidden="true"></i></a></span>How does Rayon work?</h3>

<p>In order to improve something, it is useful to know of how it works.
Rayon was started in 2015 by <a href="https://github.com/nikomatsakis">Niko Matsakis</a> who described the overall design in the blog post <a href="https://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/"><em>Rayon: data parallelism in Rust</em></a>, and a <a href="https://smallcultfollowing.com/babysteps/blog/2016/02/19/parallel-iterators-part-1-foundations/">series</a> <a href="https://smallcultfollowing.com/babysteps/blog/2016/02/25/parallel-iterators-part-2-producers/">of</a> <a href="https://smallcultfollowing.com/babysteps/blog/2016/11/14/parallel-iterators-part-3-consumers/">follow-ups</a>.</p>

<ul>
  <li>In particular, Rayon is based on the idea of splittable items of work, and on a fundamental <a href="https://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/#rayons-core-primitive-join">join primitive</a>.
This allows to distribute and re-balance work across threads via <em>work stealing</em>.
This paradigm is generally expressive, as illustrated by the <a href="https://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/#example-of-using-join-parallel-quicksort">quicksort example</a>.</li>
  <li>Above that, Rayon provides a drop-in replacement of the standard library’s <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html"><code>Iterator</code></a> API and its various combinators like <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.map"><code>map()</code></a>, <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter"><code>filter()</code></a> or <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.fold"><code>fold()</code></a>.</li>
</ul>

<p>In more detail, the first building block of Rayon is the <a href="https://docs.rs/rayon-core/"><code>rayon_core</code></a> crate.
It manages a pool of worker threads that are constantly polling for new tasks to run, and <a href="https://github.com/rayon-rs/rayon/blob/v1.10.0/rayon-core/src/sleep/README.md">gradually go to sleep</a> if they find no work to do.
More precisely, it uses the <a href="https://docs.rs/crossbeam/0.8.4/crossbeam/deque/index.html"><code>crossbeam::deque</code></a> data structure to distribute jobs among threads.
This deque works as follows:</p>

<ul>
  <li>Each thread owns a local queue where it can push and pop items of work.</li>
  <li>If a thread’s queue is empty, it can steal a job from another thread.</li>
  <li>Additionally, a global injector queue allows external components to push items of work into the thread pool.</li>
</ul>

<p><a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-xbeam-deque.sX_dAFfyPfkl.svg"><img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-xbeam-deque.sX_dAFfyPfkl.svg" width="580" height="575" loading="lazy" alt="Work stealing queues with crossbeam"/></a>
Work stealing among threads with <code>crossbeam::deque</code>.</p>

<p>Above the thread pool, Rayon’s implementation of the <a href="https://docs.rs/rayon/1.10.0/rayon/iter/trait.ParallelIterator.html"><code>ParallelIterator</code></a> API is rather complex, with the main concepts defined in <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/index.html"><code>rayon::iter::plumbing</code></a> and described in more detail in the <a href="https://github.com/rayon-rs/rayon/blob/v1.10.0/src/iter/plumbing/README.md">plumbing README</a>.
In particular, a pipeline of iterator combinators is organized as a series of <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/trait.Producer.html">producers</a> and <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/trait.Consumer.html">consumers</a>, with <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/trait.Folder.html">folders</a>, <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/trait.Reducer.html">reducers</a>, <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/trait.ProducerCallback.html">callbacks</a>, and a <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/fn.bridge.html"><code>bridge()</code></a> function to connect them.</p>

<p>To take a concrete example, let’s see what happens when my program calls <a href="https://docs.rs/rayon/1.10.0/rayon/iter/trait.IntoParallelRefIterator.html#tymethod.par_iter"><code>par_iter()</code></a> on a slice.</p>

<div><div><pre><code><span>ballots</span>
    <span>.par_iter</span><span>()</span>
    <span>.map</span><span>(|</span><span>ballot</span><span>|</span> <span>{</span>
        <span>let</span> <span>mut</span> <span>vote_accumulator</span> <span>=</span> <span>VoteAccumulator</span><span>::</span><span>new</span><span>(</span><span>num_candidates</span><span>);</span>
        <span>process_ballot</span><span>(</span><span>&amp;</span><span>mut</span> <span>vote_accumulator</span><span>,</span> <span>ballot</span><span>);</span>
        <span>vote_accumulator</span>
    <span>})</span>
    <span>.reduce</span><span>(</span>
        <span>||</span> <span>VoteAccumulator</span><span>::</span><span>new</span><span>(</span><span>num_candidates</span><span>),</span>
        <span>|</span><span>a</span><span>,</span> <span>b</span><span>|</span> <span>a</span><span>.reduce</span><span>(</span><span>b</span><span>),</span>
    <span>)</span>
</code></pre></div></div>

<p>First of all, the chain of iterator combinators creates a chain of nested data structures.
This is similar to what happens with standard iterators in Rust.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-iterator-pipeline.FthmFKlvoetE.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-iterator-pipeline-dark.oHulMQcvkThS.svg" media="(prefers-color-scheme: dark)" width="395" height="420"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-iterator-pipeline.FthmFKlvoetE.svg" width="395" height="420" loading="lazy" alt="Chain of iterator combinators and associated objects"/>
</picture>
</a>
Objects created by iterator combinators.
</p>

<p>Then, a series of callbacks is triggered, eventually invoking the <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/fn.bridge_producer_consumer.html"><code>bridge_producer_consumer()</code></a> function.
This one connects a <em>producer</em> that yields items from the slice with a <em>consumer</em> that applies the reduction function to obtain a result.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-rayon-bridge.SbD08MV8eBps.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-rayon-bridge-dark.fOfyUUalVeRc.svg" media="(prefers-color-scheme: dark)" width="820" height="420"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-rayon-bridge.SbD08MV8eBps.svg" width="820" height="420" loading="lazy" alt="Inner workings of Rayon&#39;s iterator combinators"/>
</picture>
</a>
Behind the scenes of Rayon’s iterator combinators.
</p>

<p>This is where parallelism comes into play.
The bridge is given as input a subset of the production pipeline (in our case a sub-slice) and can decide to either split it into two halves, or process all of it serially.
In case of a split, two jobs are created:</p>
<ul>
  <li>The first job is processed directly on the current thread, which can in turn either split it again or process it serially.</li>
  <li>The second job is pushed on the local queue, where it may be stolen by another thread if the current thread is too slow to process the first job.</li>
</ul>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-rayon-split-reduce.PQaqdFEWLsKG.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-rayon-split-reduce-dark.k7crQJ7uv9Pa.svg" media="(prefers-color-scheme: dark)" width="1100" height="540"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-rayon-split-reduce.PQaqdFEWLsKG.svg" width="1100" height="540" loading="lazy" alt="Rayon&#39;s bridge primitive"/>
</picture>
</a>
Rayon’s bridge primitive: either split the job in two, or fully run it serially.
</p>

<p>The policy to control when to split is based on the <a href="https://docs.rs/rayon/1.10.0/rayon/iter/plumbing/trait.Producer.html#method.min_len">producer’s <code>min_len()</code></a> function (which can be adjusted manually by calling <a href="https://docs.rs/rayon/1.10.0/rayon/iter/trait.IndexedParallelIterator.html#method.with_min_len"><code>with_min_len()</code></a>), as well as the number of threads in the pool and whether a job has been stolen (see the <a href="https://docs.rs/rayon/1.10.0/src/rayon/iter/plumbing/mod.rs.html#271-287"><code>Splitter</code> implementation</a>).</p>

<p>With that, processing our slice in parallel with Rayon is equivalent to creating a binary tree of sub-slices, where each leaf is processed serially.
This tree is not necessarily balanced, as the decision to split each node is done locally.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-work-stealing-tree.IuFfzMvksmuz.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-work-stealing-tree-dark.vrhnlr4eL7Vd.svg" media="(prefers-color-scheme: dark)" width="1060" height="560"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-work-stealing-tree.IuFfzMvksmuz.svg" width="1060" height="560" loading="lazy" alt="Rayon&#39;s work stealing creates a tree of jobs"/>
</picture>
</a>
Rayon’s work stealing splits the input slice into a tree of jobs.</p>

<p>Although Rayon’s approach is generalist and allows arbitrarily complex patterns, some design choices incur overhead.
For example:</p>

<ul>
  <li>Until they go to sleep, worker threads are constantly polling for new jobs even if there is currently no active <code>.par_iter()</code> loop.
This explains why I noticed so many <code>futex</code> and <code>sched_yield</code> syscalls.</li>
  <li>The overall architecture with a binary tree of splittable jobs creates trade-offs.
    <ul>
      <li>If jobs are too small, there will be a lot of traffic and synchronization in the <code>crossbeam::deque</code> to push and pop them.
This is why Rayon stops splitting jobs at some point.</li>
      <li>However, if jobs are too big then work stealing becomes less effective, because once a job is scheduled to run serially it cannot be split after the fact.
This means that if the load is not balanced – because we’re unlucky or due to <a href="https://github.com/gendx/stv-rs/blob/v0.5.0/testdata/ballots/skewed.blt">adversarial inputs</a> – there is a risk that all the heavy items end up in a single serial job without any effective parallelism.</li>
    </ul>
  </li>
</ul>

<h3 id="hand-rolling-a-thread-pool"><span><a href="#hand-rolling-a-thread-pool"><i aria-hidden="true"></i></a></span>Hand-rolling a thread pool</h3>

<p>In summary, while Rayon’s building blocks make sense, it became clear that finding performance improvements in this complex framework would be hard for me.
So I went to the drawing board, designing a “mini-Rayon” from scratch and tailored for my use case, to see how the performance would be.</p>

<p>As a reminder, the overall flow of the <a href="http://jimkang.com/blog/2023/03/27/single-transferable-vote.html#single-transferable-vote-stv">election counting algorithm</a> I’m trying to parallelize is the following.
The second step – iterating over a slice containing all the ballots – is the one to parallelize.</p>

<div><div><pre><code><span>// Step 1: initialize weights.</span>
<span>let</span> <span>mut</span> <span>weights</span> <span>=</span> <span>initialize_weights</span><span>();</span>
<span>loop</span> <span>{</span>
    <span>// Step 2: accumulate votes over all ballots.</span>
    <span>let</span> <span>count</span> <span>=</span> <span>ballots</span>
        <span>.iter</span><span>()</span>
        <span>.map</span><span>(|</span><span>ballot</span><span>|</span> <span>count_ballot</span><span>(</span><span>&amp;</span><span>ballot</span><span>,</span> <span>&amp;</span><span>weights</span><span>))</span>
        <span>.reduce</span><span>(</span><span>/* ... */</span><span>);</span>
    <span>// Step 3: update weights and exit if we elected enough candidates.</span>
    <span>if</span> <span>update_weights</span><span>(</span><span>&amp;</span><span>mut</span> <span>weights</span><span>,</span> <span>count</span><span>)</span> <span>{</span>
        <span>break</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Here was my first, simple strategy to parallelize step 2 manually.</p>

<ul>
  <li>Create a pool of worker threads that will each process a sub-slice of the input ballots.</li>
  <li>Establish a communication channel (via Rust’s <a href="https://doc.rust-lang.org/std/sync/struct.Mutex.html"><code>Mutex</code></a> + <a href="https://doc.rust-lang.org/std/sync/struct.Condvar.html"><code>Condvar</code></a>) for the main thread to tell the worker threads when they can start processing the slice at the beginning of step 2.</li>
  <li>Likewise, establish a communication channel for the worker threads to send back the results once they have completed step 2.</li>
</ul>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-partition._i4DgPpWvSPa.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-partition-dark.z0niLT1jbeT5.svg" media="(prefers-color-scheme: dark)" width="1140" height="345"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-partition._i4DgPpWvSPa.svg" width="1140" height="345" loading="lazy" alt="Parallelism: simple partitioning strategy"/>
</picture>
</a>
Parallelism with a simple partitioning strategy.
</p>

<p>This requires sharing the following memory between threads.</p>

<ul>
  <li>Each worker thread gets a read-only view to the ballots <code>&amp;[Ballot]</code>.
The <a href="https://doc.rust-lang.org/stable/std/thread/fn.scope.html"><code>std::thread::scope()</code></a> function allows creating threads with a scoped lifetime, so that they can capture non-static objects by reference.</li>
  <li>Share the additional <code>weights</code> (that change for each iteration of the main loop) between the main thread and the workers via an <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>Arc&lt;RwLock&lt;_&gt;&gt;</code></a>.
A mutex would work too, but a <a href="https://doc.rust-lang.org/std/sync/struct.RwLock.html"><code>RwLock</code></a> is much more efficient as it allows all the worker threads to read the weights simultaneously.</li>
  <li>Share the results from each worker thread to the main thread via an <code>Arc&lt;Mutex&lt;_&gt;&gt;</code>.</li>
</ul>

<p>Lastly, we need to attribute to each worker thread a subset of the ballots to process.
A simple strategy is to partition the ballot slice in advance so that we don’t need additional communication.
For example, with 100 ballots and 4 threads, we can attribute the <code>0..25</code> range to thread 1, the <code>25..50</code> range to thread 2, etc.
We will revisit this strategy <a href="#work-stealing">below</a>.</p>

<p>With this basic design, <a href="https://github.com/gendx/stv-rs/commit/93f36a67d04df8e958fb2523cdff9662cbca6472">my code worked</a> but was up to 50% slower than Rayon, especially when using more threads.</p>

<h3 id="cpu-pinning"><span><a href="#cpu-pinning"><i aria-hidden="true"></i></a></span>CPU pinning</h3>

<p>One problem with this naive implementation was that the operating system’s scheduler can freely move the execution of each thread from one CPU core to another, by doing a <em>CPU migration</em> – as <a href="#advanced-perf-usage">shown in the <code>perf stat</code> output</a>.
This is in principle good to balance the load across cores, but it incurs unnecessary overhead when we’re utilizing all cores anyway.</p>

<p>Migrating a thread has a cost in itself (running some logic in the kernel to move the thread), but is also detrimental in the medium term because it invalidates <a href="#a-word-about-cpu-caches">per-core caches</a>.
Indeed, if a thread had all its working data in the L1 or L2 caches (which are core-specific), moving it to another core will make the thread lose this cached data.<sup id="fnref:3"><a href="#fn:3" rel="footnote" role="doc-noteref">3</a></sup></p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-cpu-migration.SsgVp5XzlqyS.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-cpu-migration-dark.Z7trVf_VAJ-G.svg" media="(prefers-color-scheme: dark)" width="500" height="340"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-cpu-migration.SsgVp5XzlqyS.svg" width="500" height="340" loading="lazy" alt="CPU migration invalidates L1-L2 caches"/>
</picture>
</a>
Each CPU migration invalidates L1-L2 caches.
</p>

<p>In my case, the relevant data is the input slice of ballots, which is accessed repeatedly due to the main loop.
With the simple partitioning strategy, each worker thread only uses and caches a fixed subset of the input, with no overlap between the inputs that different worker threads manipulate.
Therefore, a CPU migration of a worker thread to another core will lead to cache misses (unless the thread migrated back and forth and the data was still in cache).</p>

<p>Fortunately, Linux allows to prevent CPU migrations by programmatically pinning each thread to a set of cores via the <a href="https://docs.rs/nix/0.29.0/nix/sched/fn.sched_setaffinity.html"><code>sched_setaffinity()</code></a> function, restricting where the thread can be executed.
By pinning each thread to a single distinct CPU core when we create the thread pool, we prevent migrations.</p>

<div><div><pre><code><span>use</span> <span>nix</span><span>::</span><span>sched</span><span>::{</span><span>sched_setaffinity</span><span>,</span> <span>CpuSet</span><span>};</span>
<span>use</span> <span>nix</span><span>::</span><span>unistd</span><span>::</span><span>Pid</span><span>;</span>

<span>for</span> <span>id</span> <span>in</span> <span>0</span><span>..</span><span>num_threads</span> <span>{</span>
    <span>let</span> <span>handle</span> <span>=</span> <span>thread_scope</span><span>.spawn</span><span>(</span><span>move</span> <span>||</span> <span>{</span>
        <span>let</span> <span>mut</span> <span>cpu_set</span> <span>=</span> <span>CpuSet</span><span>::</span><span>new</span><span>();</span>
        <span>if</span> <span>let</span> <span>Err</span><span>(</span><span>e</span><span>)</span> <span>=</span> <span>cpu_set</span><span>.set</span><span>(</span><span>id</span><span>)</span> <span>{</span>
            <span>warn!</span><span>(</span><span>&#34;Failed to set CPU affinity for thread #{id}: {e}&#34;</span><span>);</span>
        <span>}</span> <span>else</span> <span>if</span> <span>let</span> <span>Err</span><span>(</span><span>e</span><span>)</span> <span>=</span> <span>sched_setaffinity</span><span>(</span><span>Pid</span><span>::</span><span>from_raw</span><span>(</span><span>0</span><span>),</span> <span>&amp;</span><span>cpu_set</span><span>)</span> <span>{</span>
            <span>warn!</span><span>(</span><span>&#34;Failed to set CPU affinity for thread #{id}: {e}&#34;</span><span>);</span>
        <span>}</span>
        
        <span>// Insert worker thread loop here.</span>
    <span>});</span>
<span>}</span>
</code></pre></div></div>

<p>This indeed gave a good performance boost to the hand-rolled solution, allowing to get slightly ahead of Rayon, which <a href="https://github.com/rayon-rs/rayon/issues/319">doesn’t do CPU pinning</a>.
Depending on the scenario, <a href="https://github.com/gendx/stv-rs/commit/20069a43ba2a1f35b4cac56a8105679c6e1bb39e">my code</a> was now up to 10% faster than Rayon with 4 threads, and up to 20% faster with 8 threads.</p>

<h3 id="work-stealing"><span><a href="#work-stealing"><i aria-hidden="true"></i></a></span>Work stealing</h3>

<p>A big drawback of the fixed partitioning strategy – based on the <em>number</em> of items – is that the load may not be balanced between threads.
Indeed, some items may be heavier to process than others, depending on their inherent complexity (e.g. how many candidates are ranked in a ballot) and on the weights of each round.
If one worker thread gets all the heavy items, the other threads will finish before and stay idle, which isn’t optimal in terms of parallelism.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-unbalanced.7FakR9__G8e_.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-unbalanced-dark.PKOaD_GpBFt4.svg" media="(prefers-color-scheme: dark)" width="1540" height="345"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-unbalanced.7FakR9__G8e_.svg" width="1540" height="345" loading="lazy" alt="Unbalanced load leading to idle threads"/>
</picture>
</a>
The limits of simple partitioning: when the load isn’t balanced.
</p>

<p>To test that, I’ve created <a href="https://github.com/gendx/stv-rs/tree/v0.5.0/testdata/shuffle_ballots">biased inputs</a> where the items are sorted by a complexity heuristic, so that each thread would (on average) have less work to do than the next one.</p>

<p>To counter this problem, I implemented <a href="https://en.wikipedia.org/wiki/Work_stealing">work stealing</a>.
The principle is the same as with Rayon: whenever a thread finishes processing its own work, it will try to steal work from other threads.
However, my <a href="https://github.com/gendx/stv-rs/blob/v0.5.0/src/parallelism/range.rs">implementation</a> is simpler (because specialized for slices), bypassing the overhead and complexity of the concurrent queue and tree of jobs.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-work-stealing.MWPI2yOsEnAr.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-work-stealing-dark.S22zBFdflcDD.svg" media="(prefers-color-scheme: dark)" width="641" height="1118"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-work-stealing.MWPI2yOsEnAr.svg" width="641" height="1118" loading="lazy" alt="Work stealing and improved latency"/>
</picture>
</a>
Work stealing to improve latency and minimize idle time.
</p>

<p>Given that the items are contiguous when processing a slice, we can model the set of items “owned” by a thread as a <a href="https://doc.rust-lang.org/std/ops/struct.Range.html">range</a> of integers (as we did without work stealing).
The idea is that each thread will pop items from the front of its range until it’s empty, and then look for another thread’s range to steal.
In that case, the “thief” will split the stolen range in two, taking one half for itself, and subsequently popping items from the front of it.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-work-stealing-half.fp82nhHRvodr.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/diagram-work-stealing-half-dark.Xt1xNnQ-K--C.svg" media="(prefers-color-scheme: dark)" width="820" height="140"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/diagram-work-stealing-half.fp82nhHRvodr.svg" width="820" height="140" loading="lazy" alt="Thread 1 steals half of thread 2&#39;s range"/>
</picture>
</a>
Stealing in bulk: half of another thread’s range.
</p>

<p>An important performance aspect is that stealing is an expensive operation, so we want to steal items in bulk rather than one at a time, to minimize the number of thefts.
Indeed, finding a range to steal and telling the “stolen” thread about it requires synchronization (via a mutex or atomics).</p>

<p>Additionally, when the thief thread switches to another range of items, it creates a “jump” that disrupts the natural flow of processing items sequentially, and doesn’t play well with the <a href="https://en.wikipedia.org/wiki/Cache_prefetching">CPU prefetcher</a> in terms of caching.
In other words, the first few items after each jump will likely not be in cache, because the prefetcher cannot predict the jump.</p>

<p>To reduce these costs, I’ve applied two techniques.</p>
<ul>
  <li>The thief steals the second half of the range.
That way, the stolen thread isn’t disrupted too much as it continues processing items sequentially without any jump.</li>
  <li>Rather than trying to steal from the first available thread (greedily), the thief scans all the threads and <a href="https://github.com/gendx/stv-rs/commit/f324c337ba958a099bd8473653741dd5dd6c8559">steals from the largest available range</a>.
Even though it makes each theft more expensive in the short term because it’s more work to read all the ranges and find the largest one, each theft is much more effective as both the thief and the stolen thread stay busy longer afterwards.
For my workload, I’ve measured 5 times fewer thefts than with a naive “steal from the next available neighbor” strategy.</li>
</ul>

<p>By stealing ranges of items, we get a major advantage over Rayon: stealing is adaptive and granular.
Rather than deciding ahead of time whether to further split a range or not, splits happen dynamically whenever a thread becomes idle.
If there is a big imbalance between items, the granularity can automatically adapt from stealing large ranges down to stealing a single item.</p>

<p>A drawback is that each iteration requires some synchronization to tell the other threads which range of items they can steal.
However, in practice this cost should be relatively small in the uncontended case where each thread is popping its own items, only getting higher in the rare case when a thread interferes by trying to steal.</p>

<p>With all of that, I generally saw a performance improvement over Rayon, especially as the number of threads go up.
Even in the worst case, my implementation performed as fast as Rayon.</p>

<h2 id="conclusion"><span><a href="#conclusion"><i aria-hidden="true"></i></a></span>Conclusion</h2>

<p>My initial assumption was that the Rayon framework caused a non-trivial synchronization overhead when many threads are involved.
After digging deeper and implementing a custom parallelism mechanism for my use case, I indeed managed to shave off system calls and in some cases get a performance boost over Rayon.
I notably learned that Rayon’s work stealing doesn’t go down to the granularity of individual items, but <a href="#how-does-rayon-work">creates a binary tree</a> whose leaves can be large sub-slices.
If the input is imbalanced, this can lead to one thread making slow progress while all the others are done (and <a href="#a-quick-look-at-system-calls">spin around with system calls</a> to find more work that doesn’t come).</p>

<p>But while it’s easy to outperform a Swiss Army knife with a custom tool, the hard part is to generalize to other scenarios.
This is where Rayon really shines: its generic approach applies to a very wide range of use cases, and the API is very simple (“just put a <code>par_iter()</code> on it”).
I’m still exploring turning my custom parallelism into a <a href="https://github.com/gendx/paralight">library</a>, to see what are the design trade-offs and hopefully make it useful for similar workloads.</p>

<p>However, whether to use Rayon or not wasn’t even the most impactful performance-wise.
In the <a href="http://jimkang.com/blog/2024/12/02/rust-data-oriented-design.html">next post</a>, I will review many other improvements: algorithms, memory representation and generic optimizations.</p>

<h2 id="bonus-does-rayons-with_max_len-help"><span><a href="#bonus-does-rayons-with_max_len-help"><i aria-hidden="true"></i></a></span>Bonus: Does Rayon’s <code>with_max_len()</code> help?</h2>

<p>As pointed out by Lucretiel <a href="https://www.reddit.com/r/rust/comments/1gu8t2j/optimization_adventures_making_a_parallel_rust/">on Reddit</a>, Rayon’s parallel iterator trait offers a <a href="https://docs.rs/rayon/1.10.0/rayon/iter/trait.IndexedParallelIterator.html#method.with_max_len"><code>with_max_len()</code> adaptor</a>, which I had originally missed.
This setting limits how long leaf serial jobs can be in <a href="http://jimkang.com/blog/2024/11/18/rust-rayon-optimized.html#how-does-rayon-work">Rayon’s tree of jobs</a>, and is the counterpart of <a href="https://docs.rs/rayon/1.10.0/rayon/iter/trait.IndexedParallelIterator.html#method.with_min_len"><code>with_min_len()</code></a>.</p>

<p>For example, using <code>.with_max_len(1)</code> should ensure that all leaf jobs have exactly one item, and therefore can be individually stolen.
Given that work stealing <a href="http://jimkang.com/blog/2024/11/18/rust-rayon-optimized.html#work-stealing">gave a performance boost</a> for my custom parallelism implementation, an hypothesis was that using <code>with_max_len()</code> with a small value should speed things up for Rayon as well.</p>

<p>So I <a href="https://github.com/gendx/stv-rs/commit/6f5fdf7a3a008ee8c2eb2c428ec52b7913c36ad1">implemented a new flag</a> to use <code>with_max_len</code> and benchmarked it on various inputs with 4 threads (the number of CPU cores on my machine).
Unfortunately, this didn’t help for my scenario: setting <code>with_max_len</code> was slower than not setting it, and the speed decreased with the maximum length.
In other words, the overhead of splitting the input into a deeper tree of jobs was larger than the benefit of more fine-grained work stealing.</p>

<p>In the worst case, my program ran twice slower with a maximum length of one (each item gets its own job) than with my custom parallelism implementation.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/plot-stv-max-len-rand2x10.LtzulBTNnmjH.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/plot-stv-max-len-rand2x10-dark.hpSExQnaHrBb.svg" media="(prefers-color-scheme: dark)" width="800" height="600"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/plot-stv-max-len-rand2x10.LtzulBTNnmjH.svg" width="800" height="600" loading="lazy" alt="Benchmarks of parallelism (ballot file = rand_2x10)"/>
</picture>
</a>
Benchmark using a <a href="https://github.com/gendx/stv-rs/blob/v0.5.0/testdata/ballots/random/rand_2x10.blt">random input</a> with or without the <code>with_max_len()</code> parameter.
</p>

<p>The setting was only effective for <a href="https://github.com/gendx/stv-rs/blob/v0.5.0/testdata/ballots/skewed.blt">heavily skewed adversarial inputs</a>, but even then Rayon still ran about 4% slower than the custom parallelism.</p>

<p>
<a href="http://jimkang.com/blog/images/rust-rayon-optimized/plot-stv-max-len-skewed.7U9gvg3iNsvX.svg">
<picture>
  <source srcset="/blog/images/rust-rayon-optimized/plot-stv-max-len-skewed-dark.OrmH_X9l7X7b.svg" media="(prefers-color-scheme: dark)" width="800" height="600"/>
  <img src="http://jimkang.com/blog/images/rust-rayon-optimized/plot-stv-max-len-skewed.7U9gvg3iNsvX.svg" width="800" height="600" loading="lazy" alt="Benchmarks of parallelism (ballot file = skewed)"/>
</picture>
</a>
Benchmark using a <a href="https://github.com/gendx/stv-rs/blob/v0.5.0/testdata/ballots/skewed.blt">heavily skewed input</a> with or without the <code>with_max_len()</code> parameter.
</p>

<hr/>

<p><em>This post was edited to take into account <a href="https://www.reddit.com/r/rust/comments/1gu8t2j/optimization_adventures_making_a_parallel_rust/">feedback on reddit</a>, with the added analysis of Rayon <code>with_max_len()</code> adaptor.</em></p>

<hr/>

<div role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>I really recommend watching <a href="https://www.youtube.com/watch?v=YB6LTaGRQJg"><em>Why does this Rust program leak memory?</em></a> by <em>fasterthanlime</em> to learn more about the syscalls behind memory allocation. <a href="#fnref:1" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2">
      <p>You can also check the code <a href="https://github.com/firefox-devtools/profiler">here</a> and run your own instance. <a href="#fnref:2" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3">
      <p>The same applies to the instruction cache, but to a smaller degree when all worker threads are running the same code in parallel (just on different data). <a href="#fnref:3" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

        
        <hr/>
        <h2><i aria-hidden="true"></i>Comments</h2>
        <p>
            To react to this blog post please check
            the <a href="https://infosec.exchange/@gendx/113504908007896810"><i aria-hidden="true"></i> Mastodon thread</a>, the <a href="https://lobste.rs/s/kfw82m/optimization_adventures_making">Lobste.rs thread</a> and the <a href="https://www.reddit.com/r/rust/comments/1gu8t2j/optimization_adventures_making_a_parallel_rust/"><i aria-hidden="true"></i> Reddit thread</a>.
        </p>
        
        <hr/>
        <p>
            <a href="http://jimkang.com/blog/feed.xml" target="_blank"><i aria-hidden="true"></i> RSS</a> |
            <a href="https://infosec.exchange/@gendx" target="_blank"><i aria-hidden="true"></i> Mastodon</a> |
            <a href="https://github.com/gendx" target="_blank"><i aria-hidden="true"></i> GitHub</a>
        </p>
        <hr/>
        <h2>You may also like</h2>
        <p>
            <span><i aria-hidden="true"></i> <a href="http://jimkang.com/blog/2024/12/02/rust-data-oriented-design.html">Optimization adventures: making a parallel Rust workload even faster with data-oriented design (and other tricks)</a></span>
            </p>
    </section>
</article></div>
  </body>
</html>
