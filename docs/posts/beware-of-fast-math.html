<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonbyrne.github.io/notes/fastmath/">Original</a>
    <h1>Beware of Fast-Math</h1>
    
    <div id="readability-page-1" class="page"><div> <p>One of my more frequent rants, both online and in person, is the danger posed by the &#34;fast-math&#34; compiler flag. While these rants may elicit resigned acknowledgment from those who already understand the dangers involved, they do little to help those who don&#39;t. So given the remarkable paucity of writing on the topic (including the documentation of the compilers themselves), I decided it would make a good inaugural topic for this blog.</p> <h2 id="so_what_is_fast-math"><a href="#so_what_is_fast-math">So what is fast-math?</a></h2> <p>It&#39;s a compiler flag or option that exists in many languages and compilers, including:</p> <ul> <li><p><code>-ffast-math</code> (and included by <code>-Ofast</code>) in <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a> and <a href="https://clang.llvm.org/docs/UsersManual.html#cmdoption-ffast-math">Clang</a></p> </li><li><p><code>-fp-model=fast</code> (the default) in <a href="https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/compiler-options/compiler-option-details/floating-point-options/fp-model-fp.html">ICC</a></p> </li><li><p><code>/fp:fast</code> in <a href="https://docs.microsoft.com/en-us/cpp/build/reference/fp-specify-floating-point-behavior?view=msvc-170">MSVC</a></p> </li><li><p><a href="https://docs.julialang.org/en/v1/manual/command-line-options/#command-line-options"><code>--math-mode=fast</code> command line option</a> or <a href="https://docs.julialang.org/en/v1/base/math/#Base.FastMath.@fastmath"><code>@fastmath</code> macro</a> in Julia.</p> </li></ul> <p>So what does it actually do? Well, as the name said, it makes your math faster. That sounds great, we should definitely do that!</p> <blockquote> <p>I mean, the whole point of fast-math is trading off speed with correctness. If fast-math was to give always the correct results, it wouldn’t be fast-math, it would be the standard way of doing math.</p> </blockquote> <p>— <a href="https://discourse.julialang.org/t/whats-going-on-with-exp-and-math-mode-fast/64619/7?u=simonbyrne">Mosè Giordano</a></p> <p>The rules of floating point operations are specified in <a href="https://en.wikipedia.org/wiki/IEEE_754">the IEEE 754 standard</a>, which all popular programming languages (mostly) adhere to; compilers are only allowed to perform optimizations which obey these rules. Fast-math allows the compiler to break some of these rules: these breakages may seem pretty innocuous at first glance, but can have significant and occasionally unfortunate downstream effects.</p> <p>In <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a>, <code>-ffast-math</code> (or <code>-Ofast</code>) enables the following options: <code>-fno-math-errno</code>, <code>-funsafe-math-optimizations</code>, <code>-ffinite-math-only</code>, <code>-fno-rounding-math</code>, <code>-fno-signaling-nans</code>, <code>-fcx-limited-range</code> and <code>-fexcess-precision=fast</code>. Note that <code>-funsafe-math-optimizations</code> is itself a collection of options <code>-fno-signed-zeros</code>, <code>-fno-trapping-math</code>, <code>-fassociative-math</code> and <code>-freciprocal-math</code>, plus some extra ones, which we will discuss further below.</p> <p>Now some of these are unlikely to cause problems in most cases: <code>-fno-math-errno</code><sup id="fnref:1"><a href="#fndef:1">[1]</a></sup>, <code>-fno-signaling-nans</code>, <code>-fno-trapping-math</code> disable rarely-used (and poorly supported) features. Others, such as <code>-freciprocal-math</code> can reduce accuracy slightly, but are unlikely to cause problems in most cases.</p> <p><a href="https://kristerw.github.io/2021/10/19/fast-math/">Krister Walfridsson</a> gives a very nice (and somewhat more objective) description of some of these, but I want to focus on three in particular.</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-ffinite-math-only"><code>-ffinite-math-only</code></a></h2> <blockquote> <p>Allow optimizations for floating-point arithmetic that assume that arguments and results are not NaNs or +-Infs.</p> </blockquote> <p>The intention here is to allow the compiler to perform some <a href="https://stackoverflow.com/a/10145714/392585">extra optimizations</a> that would not be correct if NaNs or Infs were present: for example the condition <code>x == x</code> can be assumed to always be true (it evaluates false if <code>x</code> is a NaN).</p> <p>This sounds great! My code doesn&#39;t generate any NaNs or Infs, so this shouldn&#39;t cause any problems.</p> <p>But what if your code doesn&#39;t generate any intermediate NaNs only because it internally calls <code>isnan</code> to ensure that they are correctly handled?</p> <p>  — based on <a href="https://twitter.com/johnregehr/status/1440024236257542147">an example from John Regehr</a></p> <p>(to explain what this is showing: the function is setting the return register <code>eax</code> to zero, by <code>xor</code>-ing it with itself, which means the function will always return <code>false</code>)</p> <p>That&#39;s right, your compiler has just removed all those checks.</p> <p>Depending on who you ask, this is either obvious (&#34;you told the compiler there were no NaNs, so why does it need to check?&#34;) or ridiculous (&#34;how can we safely optimize away NaNs if we can&#39;t check for them?&#34;). Even compiler developers <a href="https://twitter.com/johnregehr/status/1440021297103134720">can&#39;t agree</a>.</p> <p>This is perhaps the single most frequent cause of fast-math-related <a href="https://stackoverflow.com/a/22931368/392585 ">StackOverflow</a> <a href="https://stackoverflow.com/q/7263404/392585">questions</a> and <a href="https://github.com/numba/numba/issues/2919">GitHub</a> <a href="https://github.com/google/jax/issues/276 ">bug</a> <a href="https://github.com/pytorch/glow/issues/2073">reports</a>, and so if your fast-math-compiled code is giving wrong results, the very first thing you should do is disable this option (<code>-fno-finite-math-only</code>).</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-fassociative-math"><code>-fassociative-math</code></a></h2> <blockquote> <p>Allow re-association of operands in series of floating-point operations.</p> </blockquote> <p>This allows the compiler to change the order of evaluation in a sequence of floating point operations. For example if you have an expression <code>(a + b) + c</code>, it can evaluate it instead as <code>a + (b + c)</code>. While these are mathematically equivalent with real numbers, they aren&#39;t equivalent in floating point arithmetic: the errors they incur can be different, in some cases quite significantly so:</p> <pre><code>julia&gt; a = <span>1e9</span>+<span>1</span>; b = -<span>1e9</span>; c = <span>0.1</span>;

julia&gt; (a+b)+c
<span>1.1</span>

julia&gt; a+(b+c)
<span>1.100000023841858</span></code></pre> <h3 id="vectorization"><a href="#vectorization">Vectorization </a></h3> <p>So why would you want to do this? One primary reason is that it can enable use of vector/SIMD instructions:</p>  <p>For those who aren&#39;t familiar with SIMD operations (or reading assembly), I&#39;ll try to explain briefly what is going on here (others can skip this part). Since raw clock speeds haven&#39;t been getting much faster, one way in which processors have been able to increase performance is through operations which operate on a &#34;vector&#34; (basically, a short sequence of values contiguous in memory).</p> <p>In this case, instead of performing a sequence of floating point additions (<code>addss</code>), it is able to make use of a SIMD instruction (<code>addps</code>) which takes vector <code>float</code>s (4 in this case, but it can be up to 16 with AVX 512 instructions), and adds them element-wise to another vector in one operation. It does this for the whole array, followed by a final reduction step where to sum the vector to a single value. This means that instead of evaluating</p> <pre><code>s = arr[<span>0</span>] + arr[<span>1</span>];
s = s + arr[<span>2</span>];
s = s + arr[<span>3</span>];
...
s = s + arr[<span>255</span>];</code></pre> <p>it is actually doing</p> <pre><code>s0 = arr[<span>0</span>] + arr[<span>4</span>]; s1 = arr[<span>1</span>] + arr[<span>5</span>]; s2 = arr[<span>2</span>] + arr[<span>6</span>];  s3 = arr[<span>3</span>] + arr[<span>7</span>];
s0 = s0 + arr[<span>8</span>];     s1 = s1 + arr[<span>9</span>];     s2 = s2 + arr[<span>10</span>];     s3 = s3 + arr[<span>11</span>]);
...
s0 = s0 + arr[<span>252</span>];   s1 = s1 + arr[<span>253</span>];   s2 = s2 + arr[<span>254</span>];    s3 = s3 + arr[<span>255</span>]);
sa = s0 + s1;
sb = s2 + s3;
s = sa + sb;</code></pre> <p>where each line corresponds to one floating point instruction.</p> <p>The problem here is that the compiler generally isn&#39;t allowed to make this optimization: it requires evaluating the sum in a different association grouping than was specified in the code, and so can give different results<sup id="fnref:4"><a href="#fndef:4">[2]</a></sup>. Though in this case it is likely harmless (or may even improve accuracy<sup id="fnref:2"><a href="#fndef:2">[3]</a></sup>), this is not always the case.</p> <h3 id="compensated_arithmetic"><a href="#compensated_arithmetic">Compensated arithmetic</a></h3> <p>Certain algorithms however depend very strictly on the order in which floating point operations are performed. In particular <em>compensated arithmetic</em> operations make use of it to compute the error that is incurred in intermediate calculations, and correct for that in later computations.</p> <p>The most well-known algorithm which makes use of this is <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan summation</a>, which corrects for the round off error incurred at addition step in the summation loop. We can compile an implementation of Kahan summation with <code>-ffast-math</code>, and compare the result to the simple loop summation above:</p>  <p>It gives <em>exactly</em> the same assembly as the original summation code above. Why?</p> <p>If you substitute the expression for <code>t</code> into <code>c</code>, you get</p> <pre><code>c = ((s + y) - s) - y);</code></pre>
<p>and by applying reassociation, the compiler will then determine that <code>c</code> is in fact always zero, and so may be completely removed. Following this logic further, <code>y = arr[i]</code> and so the inside of the loop is simply</p>
<pre><code>s = s + arr[i];</code></pre>
<p>and hence it &#34;optimizes&#34; identically to the simple summation loop above.</p>
<p>This might seem like a minor tradeoff, but compensated arithmetic is often used to implement core math functions, such as trigonometric and exponential functions. Allowing the compiler to reassociate inside these can give <a href="https://github.com/JuliaLang/julia/issues/30073#issuecomment-439707503">catastrophically wrong answers</a>.</p>
<h2 id="flushing_subnormals_to_zero"><a href="#flushing_subnormals_to_zero">Flushing subnormals to zero</a></h2>
<p>This one is the most subtle, but by far the most insidious, as it can affect code compiled <em>without</em> fast-math, and is only cryptically documented under <code>-funsafe-math-optimizations</code>:</p>
<blockquote>
<p>When used at link time, it may include libraries or startup files that change the default FPU control word or other similar optimizations.</p>
</blockquote>
<p>So what does that mean? Well this is referring to one of those slightly annoying edge cases of floating point numbers, <em>subnormals</em> (sometimes called <em>denormals</em>). <a href="https://en.wikipedia.org/wiki/Subnormal_number">Wikipedia gives a decent overview</a>, but for our purposes the main thing you need to know is (a) they&#39;re <em>very</em> close to zero, and (b) when encountered, they can incur a significant performance penalty on many processors<sup id="fnref:6"><a href="#fndef:6">[4]</a></sup>.</p>
<p>A simple solution to this problem is &#34;flush to zero&#34; (FTZ): that is, if a result would return a subnormal value, return zero instead. This is actually fine for a lot of use cases, and this setting is commonly used in audio and graphics applications. But there are plenty of use cases where it isn&#39;t fine: FTZ breaks some important floating point error analysis results, such as <a href="https://en.wikipedia.org/wiki/Sterbenz_lemma">Sterbenz&#39; Lemma</a>, and so unexpected results (such as iterative algorithms failing to converge) may occur.</p>
<p>The problem is how FTZ actually implemented on most hardware: it is not set per-instruction, but instead <a href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/floating-point-operations/understanding-floating-point-operations/setting-the-ftz-and-daz-flags.html">controlled by the floating point environment</a>: more specifically, it is controlled by the floating point control register, which on most systems is set at the thread level: enabling FTZ will affect all other operations in the same thread.</p>
<p>GCC with <code>-funsafe-math-optimizations</code> enables FTZ (and its close relation, denormals-are-zero, or DAZ), even when building shared libraries. That means simply loading a shared library can change the results in completely unrelated code, which is <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/issues/253#issuecomment-573589022">a fun debugging experience</a>.</p>
<h2 id="what_can_programmers_do"><a href="#what_can_programmers_do">What can programmers do?</a></h2>
<p>I&#39;ve joked on Twitter that &#34;friends don&#39;t let friends use fast-math&#34;, but with the luxury of a longer format, I will concede that it has valid use cases, and can actually give valuable performance improvements; as SIMD lanes get wider and instructions get fancier, the value of these optimizations will only increase. At the very least, it can provide a useful reference for what performance is left on the table. So when and how can it be safely used?</p>
<p>One reason is if you don&#39;t care about the accuracy of the results: I come from a scientific computing background where the primary output of a program is a bunch of numbers. But floating point arithmetic is used in many domains where that is not the case, such as audio, graphics, games, and machine learning. I&#39;m not particularly familiar with requirements in these domains, but there is an interesting rant by <a href="https://gcc.gnu.org/legacy-ml/gcc/2001-07/msg02150.html">Linus Torvalds from 20 years ago</a>, arguing that overly strict floating point semantics are of little importance outside scientific domains. Nevertheless, <a href="https://twitter.com/supahvee1234/status/1382907921848221698">some anecdotes</a> suggest fast-math can cause problems, so it is probably still useful understand what it does and why. If you work in these areas, I would love to hear about your experiences, especially if you identified which of these optimizations had a positive or negative impact.</p>
<blockquote>
<p>I hold that in general it’s simply intractable to “defensively” code against the transformations that <code>-ffast-math</code> may or may not perform. If a sufficiently advanced compiler is indistinguishable from an adversary, then giving the compiler access to <code>-ffast-math</code> is gifting that enemy nukes. That doesn’t mean you can’t use it! You just have to test enough to gain confidence that no bombs go off with your compiler on your system.</p>
</blockquote>
<p>— <a href="https://discourse.julialang.org/t/when-if-a-b-x-1-a-b-divides-by-zero/7154/5?u=simonbyrne">Matt Bauman</a></p>
<p>If you do care about the accuracy of the results, then you need to approach fast-math much more carefully and warily. A common approach is to enable fast-math everywhere, observe erroneous results, and then attempt to isolate and fix the cause as one would usually approach a bug. Unfortunately this task is not so simple: you can&#39;t insert branches to check for NaNs or Infs (the compiler will just remove them), you can&#39;t rely on a debugger because <a href="https://gitlab.com/libeigen/eigen/-/issues/1674#note_709679831">the bug may disappear in debug builds</a>, and it can even <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1127544">break printing</a>.</p>
<p>So you have to approach fast-math much more carefully. A typical process might be:</p>
<ol>
<li><p>Develop reliable validation tests</p>

</li><li><p>Develop useful benchmarks</p>

</li><li><p>Enable fast-math and compare benchmark results</p>

</li><li><p>Selectively enable/disable fast-math optimizations<sup id="fnref:5"><a href="#fndef:5">[5]</a></sup> to identify</p>
<p>a. which optimizations have a performance impact,</p>
<p>b. which cause problems, and</p>
<p>c. where in the code those changes arise.</p>

</li><li><p>Validate the final numeric results</p>

</li></ol>
<p>The aim of this process should be to use the absolute minimum number of fast-math options, in the minimum number of places, while testing to ensure that the places where the optimizations are used remain correct.</p>
<p>Alternatively, you can look into other approaches to achieve the same performance benefits: in some cases it is possible to rewrite the code to achieve the same results: for example, it is not uncommon to see expressions like <code>x * (1/y)</code> in many scientific codebases.</p>
<p>For SIMD operations, tools such as <a href="https://www.openmp.org/spec-html/5.0/openmpsu42.html">OpenMP</a> or <a href="https://ispc.github.io/">ISPC</a> provide constructions to write code that is amenable to automatic SIMD optimizations. Julia provides the <a href="https://docs.julialang.org/en/v1/base/base/#Base.SimdLoop.@simd"><code>@simd</code> macro</a>, though this also has some important caveats on its use. At the more extreme end, you can use <a href="https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/">SIMD intrinsics</a>: these are commonly used in libraries, often with the help of code generation (<a href="http://fftw.org/">FFTW</a> uses this appraoch), but requires considerably more effort and expertise, and can be difficult to port to new platforms.</p>
<p>Finally, if you&#39;re writing an open source library, please don&#39;t <a href="https://github.com/tesseract-ocr/tesseract/blob/5884036ecdb2807419cbd21b7ca44b630f547d80/Makefile.am#L140">hardcode fast-math into your Makefile</a>.</p>
<h2 id="what_can_language_and_compilers_developers_do"><a href="#what_can_language_and_compilers_developers_do">What can language and compilers developers do?</a></h2>
<p>I think the widespread use of fast-math should be considered a fundamental design failure: by failing to provide programmers with features they need to make the best use of modern hardware, programmers instead resort to enabling an option that is known to be blatantly unsafe.</p>
<p>Firstly, GCC should address the FTZ library issue: the bug has been <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522">open for 9 years, but is still marked NEW</a>. At the very least, this behavior should be more clearly documented, and have a specific option to disable it.</p>
<p>Beyond that, there are 2 primary approaches: educate users, and provide finer control over the optimizations.</p>
<p>The easiest way to educate users is to give it a better name. Rather than &#34;fast-math&#34;, something like &#34;unsafe-math&#34;. Documentation could also be improved to educate users on the consequences of these choices (consider this post to be my contribution to toward that goal). Linters and compiler warnings could, for example, warn users that their <code>isnan</code> checks are now useless, or even just highlight which regions of code have been impacted by the optimizations.</p>
<p>Secondly, languages and compilers need to provide better tools to get the job done. Ideally these behaviors shouldn&#39;t be enabled or disabled via a compiler flag, which is a very blunt tool, but specified locally in the code itself, for example</p>
<ul>
<li><p>Both GCC and Clang let you <a href="https://stackoverflow.com/a/40702790/392585">enable/disable optimizations on a per-function basis</a>: these should be standardized to work with all compilers.</p>

</li><li><p>There should be options for even finer control, such as a pragma or macro so that users can assert that &#34;under no circumstances should this <code>isnan</code> check be removed/this arithmetic expression be reassociated&#34;.</p>

</li><li><p>Conversely, a mechanism to flag certain addition or subtraction operations which the compiler is allowed to reassociate (or contract into a fused-multiply-add operation) regardless of compiler flags.<sup id="fnref:3"><a href="#fndef:3">[6]</a></sup></p>

</li></ul>
<p>This still leaves open the exact question of what the semantics should be: if you combine a regular <code>+</code> and a fast-math <code>+</code>, can they reassociate? What should the scoping rules be, and how should it interact with things like inter-procedural optimization? These are hard yet very important questions, but they need to be answered for programmers to be able to make use of these features safely.</p>
<p>For more discussion, see <a href="https://news.ycombinator.com/item?id=29201473">HN</a>.</p>
<h2 id="updates"><a href="#updates">Updates</a></h2>
<p>A few updates since I wrote this note:</p>
<ul>
<li><p>Brendan Dolan-Gavitt wrote a fantastic piece about <a href="https://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html">FTZ-enabling libraries in Python packages</a>: it also has some nice tips on how to find out if your library was compiled with fast-math.</p>
<ul>
<li><p>He also has a nice proof-of-concept <a href="https://github.com/moyix/2_ffast_2_furious">buffer overflow vulnerability</a>.</p>

</li></ul>

</li><li><p>It turns out Clang also enables FTZ when building shared libraries with fast-math: but only if you have a system GCC installation. I&#39;ve <a href="https://github.com/llvm/llvm-project/issues/57589">opened an issue</a>.</p>

</li><li><p>MSVC doesn&#39;t remove <code>isnan</code> checks, but instead <a href="https://twitter.com/dotstdy/status/1567748577962741760">generates what looks like worse code</a> when compiling with fast-math.</p>

</li><li><p>The FTZ library issue will be <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522#c45">fixed in GCC 13</a>!</p>

</li></ul>

 
 
 
 <table id="fndef:5">
    <tbody><tr>
        <td><a href="#fnref:5">[5]</a>
        </td><td>As mentioned above, <code>-fno-finite-math-only</code> should be the first thing you try.
    
</td></tr></tbody></table>
 


</div></div>
  </body>
</html>
