<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gizmodo.com/your-ai-girlfriend-is-a-data-harvesting-horror-show-1851253284">Original</a>
    <h1>Your AI Girlfriend Is a Data-Harvesting Horror Show</h1>
    
    <div id="readability-page-1" class="page"><div><p>Lonely on Valentine’s Day? AI can help. At least, that’s what a number of companies hawking “romantic” chatbots will tell you. But as your robot love story unfolds, there’s a tradeoff you may not realize you’re making. According to a new study from Mozilla’s *Privacy Not Included project, <span><a data-ga="[[&#34;Embedded Url&#34;,&#34;Internal link&#34;,&#34;https://gizmodo.com/sam-altman-says-chatgpt-can-t-be-your-girlfriend-1851181240&#34;,{&#34;metric25&#34;:1}]]" href="https://gizmodo.com/sam-altman-says-chatgpt-can-t-be-your-girlfriend-1851181240">AI girlfriends and boyfriends</a></span> harvest shockingly personal information, and almost all of them sell or share the data they collect. </p><div><div><div><div data-playlist="196019,196111,195635" data-current="196019"><div><div><div data-video-id="196019" data-monetizable="true" data-position="sidebar" data-video-title="Like It or Not, Your Doctor Will Use AI | AI Unlocked" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="209"><div><p>Like It or Not, Your Doctor Will Use AI | AI Unlocked</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/196019/196019_240p.mp4" label="240p" type="video/mp4"/><source data-src="https://vid.kinja.com/prod/196019/196019_480p.mp4" label="480p" type="video/mp4"/><source data-src="https://vid.kinja.com/prod/196019/196019_720p.mp4" label="720p" type="video/mp4"/><source data-src="https://vid.kinja.com/prod/196019/196019_1080p.mp4" label="1080p" type="video/mp4"/><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/21541.vtt" srclang="en"/></video></div></div></div></div></div></div></div><p>“To be perfectly blunt, AI girlfriends and boyfriends are not your friends,” said Misha Rykov, a Mozilla Researcher, in a press statement. “Although they are marketed as something that will enhance your mental health and well-being, they specialize in delivering dependency, loneliness, and toxicity, all while prying as much data as possible from you.”</p><p>Mozilla dug into <span><a data-ga="[[&#34;Embedded Url&#34;,&#34;External link&#34;,&#34;https://foundation.mozilla.org/en/privacynotincluded/eva-ai-chat-bot-soulmate/&#34;,{&#34;metric25&#34;:1}]]" href="https://foundation.mozilla.org/en/privacynotincluded/eva-ai-chat-bot-soulmate/" target="_blank" rel="noopener noreferrer">11 different AI romance chatbots</a></span>, including popular apps such as Replika, Chai, Romantic AI, EVA AI Chat Bot &amp; Soulmate, and CrushOn.AI. Every single one earned the Privacy Not Included label, putting these chatbots among the worst categories of products Mozilla has ever reviewed. The apps mentioned in this story didn’t immediately respond to requests for comment.</p><p>You’ve heard stories about data problems before, but according to Mozilla, AI girlfriends violate your privacy in “disturbing new ways.” For example, CrushOn.AI collects details including information about sexual health, use of medication, and gender-affirming care. 90% of the apps may sell or share user data for targeted ads and other purposes, and more than half won’t let you delete the data they collect. Security was also a problem. Only one app, Genesia AI Friend &amp; Partner, met Mozilla’s minimum security standards. </p><p>One of the more striking findings came when Mozilla counted the trackers in these apps, little bits of code that collect data and share them with other companies for advertising and other purposes. Mozilla found the AI girlfriend apps used an average of 2,663 trackers per minute, though that number was driven up by Romantic AI, which called a whopping 24,354 trackers in just one minute of using the app.</p><p>The privacy mess is even more troubling because the apps actively encourage you to share details that are far more personal than the kind of thing you might enter into a typical app. EVA AI Chat Bot &amp; Soulmate pushes users to “share all your secrets and desires,” and specifically asks for photos and voice recordings. It’s worth noting that EVA was the only chatbot that didn’t get dinged for how it uses that data, though the app did have security issues. </p><p>Data issues aside, the apps also made some questionable claims about what they’re good for. EVA AI Chat Bot &amp; Soulmate bills itself as “a provider of software and content developed to improve your mood and well-being.” Romantic AI says it’s “here to maintain your MENTAL HEALTH.” When you read the company’s terms and services though, they go out of their way to distance themselves from their own claims. Romantic AI’s policies, for example, say it is “neither a provider of healthcare or medical Service nor providing medical care, mental health Service, or other professional Service.”</p><p>That’s probably important legal ground to cover, given these app’s history. Replika reportedly encouraged a man’s attempt to <span><a data-ga="[[&#34;Embedded Url&#34;,&#34;Internal link&#34;,&#34;https://gizmodo.com/man-sentenced-ai-girlfriend-assassinate-queen-1850904625&#34;,{&#34;metric25&#34;:1}]]" href="https://gizmodo.com/man-sentenced-ai-girlfriend-assassinate-queen-1850904625">assassinate the Queen of England</a></span>. A Chai chatbot allegedly <span><a data-ga="[[&#34;Embedded Url&#34;,&#34;External link&#34;,&#34;https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says&#34;,{&#34;metric25&#34;:1}]]" href="https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says" target="_blank" rel="noopener noreferrer">encouraged a user to commit suicide</a></span>.</p></div></div>
  </body>
</html>
