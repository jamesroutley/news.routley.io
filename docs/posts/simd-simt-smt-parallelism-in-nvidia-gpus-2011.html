<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://yosefk.com/blog/simd-simt-smt-parallelism-in-nvidia-gpus.html">Original</a>
    <h1>SIMD &lt; SIMT &lt; SMT: Parallelism in Nvidia GPUs (2011)</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Programmable NVIDIA GPUs are very inspiring to hardware geeks, proving that processors with an original, incompatible
programming model can become widely used.</p>
<p>NVIDIA call their parallel programming model <strong>SIMT</strong> - &#34;Single Instruction, Multiple Threads&#34;. Two other
different, but related parallel programming models are <strong>SIMD</strong> - &#34;Single Instruction, Multiple Data&#34;, and
<strong>SMT -</strong> &#34;Simultaneous Multithreading&#34;. Each model exploits a different source of parallelism:</p>
<ul>
<li>In SIMD, elements of short vectors are processed in parallel.</li>
<li>In SMT, instructions of several threads are run in parallel.</li>
<li>SIMT is somewhere in between – an interesting hybrid between vector processing and hardware threading.</li>
</ul>
<p>My presentation of SIMT is focused on hardware architecture and its implications on <strong>the trade-off between flexibility
and efficiency</strong>. I&#39;ll describe how SIMT is different from SIMD and SMT, and why – what is gained (and lost) through
these differences.</p>
<p>From a hardware design perspective, NVIDIA GPUs are at first glance <em>really strange</em>. The question I&#39;ll try to answer
is &#34;why would you want to build a processor that way?&#34; I <em>won&#39;t</em> attempt to write a GPU programming tutorial, or
quantitatively compare GPUs to other processors.</p>
<h2 id="simd-simt-smt">SIMD &lt; SIMT &lt; SMT</h2>
<p>It can be said that SIMT is a more flexible SIMD, and SMT is in turn a more flexible SIMT. Less flexible models are generally
more efficient – except when their lack of flexibility makes them useless for the task.</p>
<p>So in terms of flexibility, SIMD &lt; SIMT &lt; SMT. In terms of performance, SIMD &gt; SIMT &gt; SMT, but only when the
models in question are flexible enough for your workload.</p>
<h2 id="simt-vs-simd">SIMT vs SIMD</h2>
<p>SIMT and SIMD both approach parallelism through broadcasting the same instruction to multiple execution units. This way, you
replicate the execution units, but they all share the same fetch/decode hardware.</p>
<p>If so, what&#39;s the difference between &#34;single instruction, multiple <em>data</em>&#34;, and single instruction, multiple
<em>threads</em>&#34;? In NVIDIA&#39;s model, there are 3 key features that SIMD doesn&#39;t have:</p>
<ol>
<li>Single instruction, multiple register sets</li>
<li>Single instruction, multiple addresses</li>
<li>Single instruction, multiple flow paths</li>
</ol>
<p>We&#39;ll see how this lifts restrictions on the set of programs that are possible to parallelize, and at what cost.</p>
<h2 id="single-instruction-multiple-register-sets">Single instruction, multiple register sets</h2>
<p>Suppose you want to add two vectors of numbers. There are many ways to spell this. C uses a loop spelling:</p>
<pre>for(i=0;i&lt;n;++i) a[i]=b[i]+c[i];</pre>
<p>Matlab uses a vector spelling:</p>
<pre>a=b+c;</pre>
<p>SIMD uses a &#34;short vector&#34; spelling – the worst of both worlds. You break your data into short vectors, and your loop
processes them using instructions with ugly names. An example using C intrinsic functions mapping to <a href="http://www.arm.com/products/processors/technologies/neon.php">ARM NEON</a> SIMD instructions:</p>
<pre>void add(uint32_t *a, uint32_t *b, uint32_t *c, int n) {
  for(int i=0; i&lt;n; i+=4) {
    <span>//compute c[i], c[i+1], c[i+2], c[i+3]</span>
    <strong>uint32x4_t</strong> a4 = <strong>vld1q_u32</strong>(a+i);
    <strong>uint32x4_t</strong> b4 = <strong>vld1q_u32</strong>(b+i);
    <strong>uint32x4_t</strong> c4 = <strong>vaddq_u32</strong>(a4,b4);
    <strong>vst1q_u32</strong>(c+i,c4);
  }
}</pre>
<p>SIMT uses a &#34;scalar&#34; spelling:</p>
<pre><strong>__global__</strong> void add(float *a, float *b, float *c) {
  int i = <strong>blockIdx</strong>.x * <strong>blockDim</strong>.x + <strong>threadIdx</strong>.x;
  a[i]=b[i]+c[i]; <span>//no loop!</span>
}</pre>
<p>The weird <strong>__global__</strong> keyword says that add() is a GPU thread entry point. <strong>blockIdx</strong>,
<strong>blockDim</strong> and <strong>threadIdx</strong> are built-in thread-local variables keeping the thread&#39;s ID. We&#39;ll see
later why a thread ID isn&#39;t just a single number; however, in this example we in fact convert it to a single number, and use it
as the element index.</p>
<p>The idea is that the CPU spawns <em>a thread per element</em>, and the GPU then executes those threads. Not all of the
thousands or millions of threads actually run in parallel, but many do. Specifically, an NVIDIA GPU contains several largely
independent processors called &#34;Streaming Multiprocessors&#34; (SMs), each SM hosts several &#34;cores&#34;, and each &#34;core&#34; runs a thread.
For instance, Fermi has up to 16 SMs with 32 cores per SM – so up to 512 threads can run in parallel.</p>
<p>All threads running on the cores of an SM at a given cycle are executing the same instruction – hence Single Instruction,
Multiple Threads. However, each thread has its own registers, so these instructions process different data.</p>
<h3 id="benefits"><em>Benefits</em></h3>
<p>&#34;Scalar spelling&#34;, where you write the code of a single thread using standard arithmetic operators, is arguably a better
interface than SIMD loops with ugly assembly-like opcodes.</p>
<p>Syntax considerations aside, is this spelling more expressive – can it do things SIMD can&#39;t? Not by itself, but it dovetails
nicely with other features that do make SIMT more expressive. We&#39;ll discuss those features shortly; in theory, they could be
bolted on the SIMD model, but they never are.</p>
<h3 id="costs"><em>Costs</em></h3>
<p>From a hardware resources perspective, there are two costs to the SIMT way:</p>
<ul>
<li><strong>Registers spent to keep redundant data items.</strong> <img alt="SIMT registers" height="163" src="https://yosefk.com/img/n/simt-regs.png" width="162"/>In our example,
the pointers a, b, and c have the same value in all threads. The values of i are different across threads, but in a trivial way
– for instance, 128, 129, 130... In SIMD, a, b, and c would be kept once in “scalar” registers – only short vectors such as
a[i:i+4] would be kept in “vector” registers. The index i would also be kept once – several neighbor elements starting from i
would be accessed without actually computing i+1, i+2, etc. Redundant computations both waste registers and needlessly consume
power. Note, however, that a combination of compiler &amp; hardware optimizations could eliminate the physical replication of
redundant values. I don&#39;t know the extent to which it&#39;s done in reality.</li>
<li><strong>Narrow data types are as costly as wide data types.</strong> <img alt="SIMD registers" height="204" src="https://yosefk.com/img/n/simd-regs.png" width="134"/>A SIMD vector
register keeping 4 32b integers can typically also be used to keep 8 16b integers, or 16 8b ones. Similarly, the same ALU
hardware can be used for many narrow additions or fewer wide ones – so 16 byte pairs can be added in one cycle, or 4 32b integer
pairs. In SIMT, a thread adds two items at a time, no matter what their width, wasting register bits and ALU circuitry.</li>
</ul>
<p>It should be noted that SIMT can be easily amended with SIMD extensions for narrow types, so that each thread processes 4
bytes at a time using ugly assembly-like opcodes. AFAIK, NVIDIA refrains from this, presumably assuming that the ugliness is not
worth the gain, with 32b float being the most popular data type in graphics anyway.</p>
<h2 id="single-instruction-multiple-addresses">Single instruction, multiple addresses</h2>
<p>Let&#39;s apply a function approximated by a look-up table to the elements of a vector:</p>
<pre><strong>__global__</strong> void apply(short* a, short* b, short* lut) {
  int i = <strong>blockIdx</strong>.x * <strong>blockDim</strong>.x + <strong>threadIdx</strong>.x;
  a[i] = lut[b[i]]; <span>//indirect memory access</span>
}</pre>
<p>Here, i is again &#34;redundant&#34; in the sense that in parallel threads, the values of i are consecutive. However, each thread
then accesses the address lut+b[i] – and these addresses are <em>not</em> consecutive.</p>
<p>Roughly, such parallel random access works for both loads and stores. Logically, stores are the trickier part because of
conflicts. What if two or more threads attempt to, say, increment the same bin in a histogram? Different NVIDIA GPU generations
provide different solutions that we won&#39;t dwell on.</p>
<h3 id="benefits-1"><em>Benefits</em></h3>
<p>This feature lets you parallelize many programs you can&#39;t with SIMD. Some form of parallel random access is usually available
on SIMD machines under the names &#34;permutation&#34;, &#34;shuffling&#34;, &#34;table look-up&#34;, etc. However, it always works with registers, not
with memory, so it&#39;s just not the same scale. You index into a table of 8, 16 or 32 elements, but not 16K.</p>
<p>As previously mentioned, in theory this feature can be bolted on the SIMD model: just compute your addresses (say, lut+b[i])
in vector registers, and add a rand_vec_load instruction. However, such an instruction would have a fairly high latency. As
we&#39;ll see, the SIMT model naturally absorbs high latencies without hurting throughput; SIMD much less so.</p>
<h3 id="costs-1"><em>Costs</em></h3>
<p>GPU has many kinds of memory: external DRAM, L2 cache, texture memory, constant memory, shared memory... We&#39;ll discuss the
cost of random access in the context of two memories &#34;at the extremes&#34;: DRAM and shared memory. DRAM is the farthest from the
GPU cores, sitting outside the chip. Shared memory is the closest to the cores – it&#39;s local to an SM, and the cores of an SM can
use it to share results with each other, or for their own temporary data.</p>
<ul>
<li><strong>With DRAM memory, random access is never efficient.</strong> In fact, the GPU hardware looks at all memory addresses
that the running threads want to access at a given cycle, and attempts to coalesce them into a single DRAM access – in case they
are <em>not</em> random. Effectively the contiguous range from i to i+#threads is reverse-engineered from the explicitly
computed i,i+1,i+2... – another cost of replicating the index in the first place. If the indexes are in fact random and can not
be coalesced, the performance loss depends on &#34;the degree of randomness&#34;. This loss results from the DRAM architecture quite
directly, the GPU being unable to do much about it – similarly to any other processor.</li>
<li><strong>With shared memory, random access is slowed down by bank contentions.</strong> Generally, a hardware memory module
will only service one access at a time. So shared memory is organized in independent banks; the number of banks for NVIDIA GPUs
is 16. If x is a variable in shared memory, then it resides in bank number (&amp;x/4)%16. In other words, if you traverse an
array, the bank you hit changes every 4 bytes. Access throughput peaks if all addresses are in different banks – hardware
contention detection logic always costs <em>latency</em>, but only actual contentions cost <em>throughput</em>. If there&#39;s a
bank hosting 2 of the addresses, the throughput is 1/2 of the peak; if there&#39;s a bank pointed by 3 addresses, the throughput is
1/3 of the peak, etc., the worst slowdown being 1/16.</li>
</ul>
<p><img alt="SIMT bank contentions" height="158" src="https://yosefk.com/img/n/simt-contentions.png" width="276"/></p>
<p>In theory, different mappings between banks and addresses are possible, each with its own shortcomings. For instance, with
NVIDIA&#39;s mapping, accessing a contiguous array of floats gives peak throughput, but a contiguous array of <em>bytes</em> gives
1/4 of the throughput (since banks change every 4 bytes). Many of the GPU programming tricks aim at avoiding contentions.</p>
<p>For instance, with a byte array, you can frequently work with bytes at distance 4 from each other at every given step.
Instead of accessing a[i] in your code, you access a[i*4], a[i*4+1], a[i*4+2] and a[i*4+3] – more code, but less
contentions.</p>
<p>This sounds convoluted, but it&#39;s a relatively cheap way for the hardware to provide efficient random access. It also supports
some very complicated access patterns with good average efficiency – by handling the frequent case of few contentions
<em>quickly</em>, and the infrequent case of many contentions <em>correctly</em>.</p>
<h2 id="single-instruction-multiple-flow-paths">Single instruction, multiple flow paths</h2>
<p>Let&#39;s find the indexes of non-zero elements in a vector. This time, each thread will work on several elements instead of just
one:</p>
<pre><strong>__global__</strong> void find(int* vec, int len,
                     int* ind, int* nfound,
                     int nthreads) {
  int tid = <strong>blockIdx</strong>.x * <strong>blockDim</strong>.x + <strong>threadIdx</strong>.x;
  int last = 0;
  int* myind = ind + tid*len;
  for(int i=tid; i&lt;len; i+=nthreads) {
    if(vec[i]) { <span>//flow divergence</span>
      myind[last] = i;
      last++;
    }
  }
  nfound[tid] = last;
}</pre>
<p>Each thread processes len/nthreads elements, starting at the index equal to its ID with a step of nthreads. We could make
each thread responsible for a more natural contiguous range using a step of 1. The way we did it is better in that accesses to
vec[i] by concurrent threads address neighbor elements, so we get coalescing.</p>
<p>The interesting bit is if(vec[i]) – depending on vec[i], some threads execute the code saving the index, some don&#39;t. The
control flow of different threads can thus diverge.</p>
<h3 id="benefits-2"><em>Benefits</em></h3>
<p>Support for divergence further expands the set of parallelizable programs, especially when used together with parallel random
access. SIMD has some support for conditional execution though vector &#34;select&#34; operations: select(flag,a,b) = if flag then a
else b. However, select can&#39;t be used to suppress updates to values – the way myind[last] is never written by threads where
vec[i] is 0.</p>
<p>SIMD instructions such as stores could be extended to suppress updates based on a boolean vector register. For this to be
really useful, the machine probably also needs parallel random access (for instance, the find() example wouldn&#39;t work
otherwise). Unless what seems like an unrealistically smart compiler arrives, this also gets more and more ugly, whereas the
SIMT spelling remains clean and natural.</p>
<h3 id="costs-2"><em>Costs</em></h3>
<ul>
<li><strong>Only one flow path is executed at a time, and threads not running it must wait.</strong><img alt="SIMT divergence" height="261" src="https://yosefk.com/img/n/simt-divergence.png" width="157"/> Ultimately SIMT executes a <em>single instruction</em> in all the multiple threads it runs
– threads share program memory and fetch / decode / execute logic. When the threads have the same flow – all run the
<em>if</em>, nobody runs the <em>else</em>, for example – then they just all run the code in <em>if</em> at full throughput.
However, when one or more threads have to run the <em>else</em>, they&#39;ll wait for the <em>if</em> threads. When the <em>if</em>
threads are done, they&#39;ll in turn wait for the <em>else</em> threads. Divergence is handled correctly, but slowly. Deeply nested
control structures effectively serialize execution and are not recommended.</li>
<li><strong>Divergence can further slow things down through &#34;randomizing&#34; memory access.</strong> In our example, all threads
read vec[i], and the indexing is tweaked to avoid contentions. However, when myind[last] is written, different threads will have
incremented the last counter a different number of times, depending on the flow. This might lead to contentions which also
serialize execution to some extent. Whether the whole parallelization exercise is worth the trouble depends on the flow of the
algorithm as well as the input data.</li>
</ul>
<p>We&#39;ve seen the differences between SIMT and its less flexible relative, SIMD. We&#39;ll now compare SIMT to SMT – the other
related model, this time the more flexible one.</p>
<h2 id="simt-vs-smt">SIMT vs SMT</h2>
<p>SIMT and SMT both use threads as a way to improve throughput despite high latencies. The problem they tackle is that any
single thread can get stalled running a high-latency instruction. This leaves the processor with idle execution hardware.</p>
<p>One way around this is switching to another thread – which (hopefully) has an instruction ready to be executed – and then
switch back. For this to work, context switching has to be instantaneous. To achieve that, you replicate register files so that
each thread has its own registers, and they all share the same execution hardware.</p>
<p>But wait, doesn&#39;t SIMT <em>already</em> replicate registers, as a way to have a single instruction operate on different data
items? It does – here, we&#39;re talking about a &#34;second dimension&#34; of register replication:</p>
<ol>
<li>Several threads – a &#34;warp&#34; in NVIDIA terminology – run simultaneously. So each thread needs its own registers.</li>
<li>Several warps, making up a &#34;block&#34;, are mapped to an SM, and an SM instantaneously switches between the warps of a block. So
<strong>each warp needs separate registers for each of its threads</strong>.</li>
</ol>
<p><img alt="SIMT 2D replication" height="243" src="https://yosefk.com/img/n/simt-2d-replication.png" width="288"/></p>
<p>With this &#34;two-dimensional&#34; replication, how many registers we end up with? Well, a lot. A block can have up to
<strong>512</strong> threads. And the registers of those threads can keep up to <strong>16K</strong> of data.</p>
<p>How many register sets does a typical SMT processor have? Er, 2, sometimes 4...</p>
<p>Why so few? One reason is diminishing returns. When you replicate registers, you pay a significant price, in the hope of
being able to better occupy your execution hardware. However, with every thread you add, the chance of it being <em>already</em>
occupied by all the other threads rises. Soon, the small throughput gain just isn&#39;t worth the price.</p>
<p>If SMT CPU designers stop at 2 or 4 threads, why did SIMT GPU designers go for 512?</p>
<h2 id="with-enough-threads-high-throughput-is-easy">With enough threads, high throughput is easy</h2>
<p>SMT is an afterthought – an attempt to use idle time on a machine originally designed to <em>not</em> have a lot of idle time
to begin with. The basic CPU design aims, first and foremost, to run a single thread fast. Splitting a process to several
independent threads is not always possible. When it is possible, it&#39;s usually gnarly.</p>
<p>Even in server workloads, where there&#39;s naturally a lot of independent processing, single-threaded latency still matters. So
few expensive, low-latency cores outperform many cheap, high-latency cores. As Google&#39;s Urs Hölzle put it, &#34;<a href="https://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/36448.pdf" title="Brawny cores still beat wimpy cores, most of the time">brawny cores beat wimpy cores</a>&#34;. Serial code has to run
fast.</p>
<p>Running a single thread fast means being able to issue instructions from the current thread as often as possible. To do that,
CPU hardware works around every one of the <em>many</em> reasons to wait. Such diverse techniques as:</p>
<ul>
<li>superscalar execution</li>
<li>out-of-order execution</li>
<li>register renaming</li>
<li>branch prediction</li>
<li>speculative execution</li>
<li>cache hierarchy</li>
<li>speculative prefetching</li>
<li>etc. etc.</li>
</ul>
<p>...are all there for the same basic purpose. They <strong>maximize the chances of an instruction to be issued without having
to switch to another thread</strong>.</p>
<p>SMT is the last line of defense, attempting to fill stalls after all these other measures failed. And even that is <a href="http://www.agner.org/optimize/blog/read.php?i=6">considered a bad idea</a> when it hurts the precious single-threaded
performance. Which it usually does: each of the 2 threads will typically complete later then it would if it didn&#39;t have to share
the hardware with the other. This is a key reason to keep the number of hardware threads low, <em>even if there are still
throughput gains to be made</em> by adding threads.</p>
<p>However, for the GPUs, the use case is when you <em>do</em> have enough data parallelism to make use of plenty of threads. If
so, why not build things <em>the other way around?</em> Threading could be our <em>first</em> stall-mitigating measure. If we
have enough threads, we just keep switching between them, and the hardware always has something to do.</p>
<p><img alt="SIMT/SMT latency" height="298" src="https://yosefk.com/img/n/simt-smt-latency.png" width="336"/></p>
<p>This saves a lot of hardware, and a lot of design effort, because you don&#39;t need most of the other methods anymore. Even
caches and hardware prefetching are not used in much of the GPU memory traffic – rather, you access external memory directly.
Why bother with caching and prefetching, if you don&#39;t have to sit idly until the data arrives from main memory – but instead
just switch to a different warp? No heuristics, no speculation, no hurry – just keep yourself busy when waiting.</p>
<p>Furthermore, even the basic arithmetic pipeline is designed for a high latency, high throughput scenario. According to the
paper &#34;<a href="http://www.stuffedcow.net/files/gpuarch-ispass2010.pdf">Demystifying GPU architecture through
microbenchmarking</a>&#34;, no operation takes less than <strong>24</strong> cycles to complete. However, the throughput of many
operations is single-cycle.</p>
<p>The upshot is that counting on the availability of many threads allows the GPU to sustain a high throughput without having to
sweat for low latencies. Hardware becomes simpler and cheaper in many areas as a result.</p>
<h2 id="when-latencies-are-high-registers-are-cheap">When latencies are high, registers are cheap</h2>
<p>So plentiful threads make it easier to build high-throughput hardware. What about having to replicate all those registers
though? 16K sounds like an insane amount of registers – how is this even affordable?</p>
<p>Well, it depends on what &#34;register&#34; means. The original meaning is the kind of storage with the smallest access latency. In
CPUs, access to registers is faster than access to L1 caches, which in turn is faster than L2, etc. Small access latency means
an expensive implementation, therefore there must be few registers.</p>
<p>However, in GPUs, access to &#34;registers&#34; can be quite slow. Because the GPU is always switching between warps, many cycles
pass between two subsequent instructions of one warp. The reason registers must be fast in CPUs is because subsequent
instructions communicate through them. In GPUs, they also communicate through registers – but the higher latency means there&#39;s
no rush.</p>
<p>Therefore, GPU registers are only analogous to CPU registers in terms of <em>instruction encoding</em>. In machine code,
&#34;registers&#34; are a small set of temporary variables that can be referenced with just a few bits of encoding – unlike memory,
where you need a longer address to refer to a variable. In this and other ways, &#34;registers&#34; and &#34;memory&#34; are
<em>semantically</em> different elements of encoding – both on CPUs and GPUs.</p>
<p>However, in terms of <em>hardware implementation</em>, GPU registers are actually <em>more like memory than CPU
registers.</em> [Disclaimer: NVIDIA doesn&#39;t disclose implementation details, and I&#39;m <em>grossly</em> oversimplifying, ignoring
things like data forwarding, multiple access ports, and synthesizable vs custom design]. 16K of local RAM is a perfectly
affordable amount. So while in a CPU, registers have to be expensive, they can be cheap in a high-latency, high-throughput
design.</p>
<p>It&#39;s still a waste if 512 threads keep the same values in some of their registers – such as array base pointers in our
examples above. However, many of the registers keep different values in different threads. In many cases register replication is
not a waste at all – any processor would have to keep those values somewhere. So functionally, the plentiful GPU registers can
be seen as a sort of a data cache.</p>
<h2 id="drawbacks">Drawbacks</h2>
<p>We&#39;ve seen that:</p>
<ul>
<li>Many threads enable cheap high-throughput, high-latency design</li>
<li>A high-throughput, high-latency design in turn enables a cheap implementation of threads&#39; registers</li>
</ul>
<p>This leads to a surprising conclusion that SIMT with its massive threading can actually be <em>cheaper</em> than SMT-style
threading added to a classic CPU design. Not unexpectedly, these cost savings come at a price of reduced flexibility:</p>
<ol>
<li>Low occupancy greatly reduces performance</li>
<li>Flow divergence greatly reduces performance</li>
<li>Synchronization options are very limited</li>
</ol>
<h3 id="occupancy"><em>Occupancy</em></h3>
<p>&#34;Occupancy&#34; is NVIDIA&#39;s term for the utilization of threading. The more threads an SM runs, the higher its occupancy. Low
occupancy obviously leads to low performance – without enough warps to switch between, the GPU won&#39;t be able to hide its high
latencies. The whole point of massive threading is refusing to target anything but massively parallel workloads. SMT requires
much less parallelism to be efficient.</p>
<h3 id="divergence"><em>Divergence</em></h3>
<p>We&#39;ve seen that flow divergence is handled correctly, but inefficiently in SIMT. SMT doesn&#39;t have this problem – it works
quite well given unrelated threads with unrelated control flow.</p>
<p>There are two reasons why unrelated threads can&#39;t work well with SIMT:</p>
<ul>
<li><strong>SIMD-style instruction broadcasting</strong> – unrelated threads within a warp can&#39;t run fast.</li>
<li><strong>More massive threading than SMT</strong> – unrelated wraps would compete for shared resources such as instruction
cache space. SMT also has this problem, but it&#39;s tolerable when you have few threads.</li>
</ul>
<p>So both of SIMT&#39;s key ideas – SIMD-style instruction broadcasting and SMT-style massive threading – are incompatible with
unrelated threads.</p>
<p><em>Related</em> threads – those sharing code and some of the data – could work well with massive threading by itself despite
divergence. It&#39;s instruction broadcasting that they fail to utilize, leaving execution hardware in idle state.</p>
<p>However, it seems that much of the time, related threads actually tend to have the same flow and no divergence. If this is
true, a machine with massive threading but without instruction broadcasting would miss a lot of opportunities to execute its
workload more efficiently.</p>
<h3 id="synchronization"><em>Synchronization</em></h3>
<p>In terms of programming model, SMT is an extension to a single-threaded, time-shared CPU. The same fairly rich set of
inter-thread (and inter-device) synchronization and communication options is available with SMT as with &#34;classic&#34;
single-threaded CPUs. This includes interrupts, message queues, events, semaphores, blocking and non-blocking system calls, etc.
The underlying assumptions are:</p>
<ul>
<li>There are quite many threads</li>
<li>Typically, each thread is doing something quite different from other threads</li>
<li>At any moment, most threads are waiting for an event, and a small subset can actually run</li>
</ul>
<p>SMT stays within this basic time-sharing framework, adding an option to have more than one actually running threads. With
SMT, as with a &#34;classic&#34; CPU, a thread will be very typically put &#34;on hold&#34; in order to wait for an event. This is implemented
using context switching – saving registers to memory and, if a ready thread is found, restoring its registers from memory so
that it can run.</p>
<p>SIMT doesn&#39;t like to put threads on hold, for several reasons:</p>
<ul>
<li>Typically, there are many running, related threads. It would make the most sense to put them <em>all</em> on hold, so that
another, unrelated, equally large group of threads can run. However, switching 16K of context is not affordable. In this sense,
&#34;registers&#34; are expensive after all, even if they are actually memory.</li>
<li>SIMT performance depends greatly on there being many running threads. There&#39;s no point in supporting the case where most
threads are waiting, because SIMT wouldn&#39;t run such workloads very well anyway. From the use case angle, a lot of waiting
threads arise in &#34;system&#34;/&#34;controller&#34; kind of software, where threads wait for files, sockets, etc. SIMT is purely
computational hardware that doesn&#39;t support such OS services. So the situation is both awkward for SIMT and shouldn&#39;t happen in
its target workloads anyway.</li>
<li>Roughly, SIMT supports data parallelism – same code, different data. Data parallelism usually doesn&#39;t need complicated
synchronization – all threads have to synchronize once a processing stage is done, and otherwise, they&#39;re independent. What
requires complicated synchronization, where some threads run and some are put on hold due to data dependencies, is task
parallelism – different code, different data. However, task parallelism implies divergence, and SIMT isn&#39;t good at that anyway –
so why bother with complicated synchronization?</li>
</ul>
<p>Therefore, SIMT roughly supports just one synchronization primitive – <strong>__syncthreads()</strong>. This creates a
synchronization point for all the threads of a block. You know that if a thread runs code past this point, no thread runs code
before this point. This way, threads can safely share results with each other. For instance, with matrix multiplication:</p>
<ol>
<li>Each thread, based on its ID – x,y – reads 2 elements, A(x,y) and B(x,y), from external memory to the on-chip shared memory.
(Of course large A and B won&#39;t fit into shared memory, so this will be done block-wise.)</li>
<li>Threads sync – all threads can now safely access all of A and B.</li>
<li>Each thread, depending on the ID, multiplies row y in A by column x in B.</li>
</ol>
<!-- -->
<pre><span>//1. load A(x,y) and B(x,y)
</span>int x = <strong>threadIdx</strong>.x;
int y = <strong>threadIdx</strong>.y;
A[stride*y + x] = extA[ext_stride*y + x];
B[stride*y + x] = extB[ext_stride*y + x];
<span>//2. sync</span>
<strong>__syncthreads()</strong>;
<span>//3. multiply row y in A by column x in B</span>
float prod = 0;
for(int i=0; i&lt;N; ++i) {
  prod += A[stride*y + i] * B[stride*i + x];
}</pre>
<p>It&#39;s an incomplete example (we look at just one block and ignore <strong>blockIdx</strong>, among other thing), but it shows
the point of syncing – and the point of these weird &#34;multi-dimensional&#34; thread IDs (IDs are x,y,z coordinates rather than just
integers). It&#39;s just natural, with 2D and 3D arrays, to map threads and blocks to coordinates and sub-ranges of these
arrays.</p>
<h2 id="summary-of-differences-between-simd-simt-and-smt">Summary of differences between SIMD, SIMT and SMT</h2>
<p>SIMT is more flexible in SIMD in three areas:</p>
<ol>
<li>Single instruction, multiple register sets</li>
<li>Single instruction, multiple addresses</li>
<li>Single instruction, multiple flow paths</li>
</ol>
<p>SIMT is less flexible than SMT in three areas:</p>
<ol>
<li>Low occupancy greatly reduces performance</li>
<li>Flow divergence greatly reduces performance</li>
<li>Synchronization options are very limited</li>
</ol>
<p>The effect of flexibility on costs was discussed above. A wise programmer probably doesn&#39;t care about these costs – rather,
he uses the most flexible (and easily accessible) device until he runs out of cycles, then moves on to utilize the next most
flexible device. Costs are just a limit on how flexible a device that is available in a given situation can be.</p>
<h2 id="simt-should-catch-on">&#34;SIMT&#34; should catch on</h2>
<p>It&#39;s a beautiful idea, questioning many of the usual assumptions about hardware design, and arriving at an internally
consistent answer with the different parts naturally complementing each other.</p>
<p>I don&#39;t know the history well enough to tell which parts are innovations by NVIDIA and which are borrowed from previous
designs. However, I believe the term SIMT was coined by NVIDIA, and perhaps it&#39;s a shame that it (apparently) didn&#39;t catch,
because the architecture &#34;deserves a name&#34; – not necessarily true of every &#34;new paradigm&#34; announced by marketing.</p>
<p>One person who took note is <a href="https://andyglew.blogspot.com/" title="Andy Glew&#39;s blog">Andy Glew</a>, one of
Intel&#39;s <a href="https://en.wikipedia.org/wiki/P6_%28microarchitecture%29">P6</a> architects – in his awesome <a href="http://semipublic.comp-arch.net/wiki/SIMT">computer architecture wiki</a>, as well as in his <a href="https://docs.google.com/fileview?id=0B5qTWL2s3LcQNGE3NWI4NzQtNTBhNS00YjgyLTljZGMtNTA0YjJmMGIzNDEw&amp;hl=en">presentation</a>
regarding further development of the SIMT model.</p>
<p>The presentation talks about neat optimizations – &#34;rejiggering threads&#34;, per-thread loop buffers/time pipelining – and
generally praises SIMT superiority over SIMD. Some things I disagree with – such as the &#34;vector lane crossing&#34; issue – and some
are very interesting, such as everything about improving utilization of divergent threads.</p>
<p>I think the presentation should be understandable after reading my oversimplified overview – and will show where my overview
oversimplifies, among other things.</p>
<h2 id="peddling-fairness">Peddling fairness</h2>
<p>Throughout this overview, there&#39;s this recurring &#34;fairness&#34; idea: you can trade flexibility for performance, and you can
choose your trade-off.</p>
<p>It makes for a good narrative, but I don&#39;t necessarily believe it. You might very well be able to get both flexibility
<em>and</em> performance. More generally, you might get both A <em>and</em> B, where A vs B intuitively feels like &#34;an inherent
trade-off&#34;.</p>
<p>What you <em>can&#39;t</em> do is get A and B in a reasonably simple, straightforward way. This means that you can trade
simplicity for almost everything – though you don&#39;t necessarily want to; perhaps it&#39;s a good subject for a separate post.</p>
</div></div>
  </body>
</html>
