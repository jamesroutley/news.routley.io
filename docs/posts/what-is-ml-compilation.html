<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mlc.ai/chapter_introduction/index.html#what-is-ml-compilation">Original</a>
    <h1>What Is ML Compilation</h1>
    
    <div id="readability-page-1" class="page"><div role="main">
        
  <div id="introduction">

<p>Machine learning applications have undoubtedly become ubiquitous. We get
smart home devices powered by natural language processing and speech
recognition models, computer vision models serve as backbones in
autonomous driving, and recommender systems help us discover new content
as we explore. Observing the rich environments where AI apps run is also
quite fun. Recommender systems are usually deployed on the cloud
platforms by the companies that provide the services. When we talk about
autonomous driving, the natural things that pop up in our heads are
powerful GPUs or specialized computing devices on vehicles. We use
intelligent applications on our phones to recognize flowers in our
garden and how to tend them. An increasing amount of IoT sensors also
come with AI built into those tiny chips. If we drill down deeper into
those environments, there are an even greater amount of diversities
involved. Even for environments that belong to the same
category(e.g. cloud), there are questions about the hardware(ARM or
x86), operation system, container execution environment, runtime library
variants, or the kind of accelerators involved. Quite some heavy
liftings are needed to bring a smart machine learning model from the
development phase to these production environments. Even for the
environments that we are most familiar with (e.g. on GPUs), extending
machine learning models to use a non-standard set of operations would
involve a good amount of engineering. Many of the above examples are
related to machine learning inference — the process of making
predictions after obtaining model weights. We also start to see an
important trend of deploying training processes themselves onto
different environments. These applications come from the need to keep
model updates local to users’ devices for privacy protection reasons or
scaling the learning of models onto a distributed cluster of nodes. The
different modeling choices and inference/training scenarios add even
more complexity to the productionisation of machine learning.</p>
<div id="id1">
<p><span id="fig-intro-gap"></span><img alt="../_images/intro-gap.png" src="https://mlc.ai/_images/intro-gap.png"/></p><p><span>Fig. 1.1 </span><span>Gap in ML deployment.</span><a href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>This course studies the topic of bringing machine learning from the
development phase to production environments. We will study a collection
of methods that facilitate the process of ML productionisation. Machine
learning productionisation is still an open and active field, with new
techniques being developed by the machine learning and systems
community. Nevertheless, we start to see common themes appearing, which
end up in the theme of this course.</p>
<div id="what-is-ml-compilation">
<h2><span>1.1. </span>What is ML Compilation<a href="#what-is-ml-compilation" title="Permalink to this heading">¶</a></h2>
<p>Machine learning compilation (MLC) is the process of transforming and
optimizing machine learning execution from its development form to its
deployment form.</p>
<p><strong>Development form</strong> refers to the set of elements we use when
developing machine learning models. A typical development form involves
model descriptions written in common frameworks such as PyTorch,
TensorFlow, or JAX, as well as weights associated with them.</p>
<p><strong>Deployment form</strong> refers to the set of elements needed to execute the
machine learning applications. It typically involves a set of code
generated to support each step of the machine learning model, routines
to manage resources (e.g. memory), and interfaces to application
development environments (e.g. java API for android apps).</p>
<div id="id2">
<p><span id="fig-dev-deploy-form"></span><img alt="../_images/dev-deploy-form.png" src="https://mlc.ai/_images/dev-deploy-form.png"/></p><p><span>Fig. 1.1.1 </span><span>Development and Deployment Forms.</span><a href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>We use the term “compilation” as the process can be viewed in close
analogy to what traditional compilers do — a compiler takes our
applications in development form and compiles them to libraries that can
be deployed. However, machine learning compilation still differs from
traditional compilation in many ways.</p>
<p>First of all, this process does not necessarily involve code generation.
For example, the deployment form can be a set of pre-defined library
functions, and the ML compilation only translates the development forms
onto calls into those libraries. The set of challenges and solutions are
also quite different. That is why studying machine learning compilation
as its own topic is worthwhile, independent of a traditional
compilation. Nevertheless, we will also find some useful traditional
compilation concepts in machine learning compilation.</p>
<p>The machine learning compilation process usually comes with the several
goals:</p>
<p><strong>Integration and dependency minimization.</strong> The process of deployment
usually involves integration — assembling necessary elements together
for the deployment app. For example, if we want to enable an android
camera app to classify flowers, we will need to assemble the necessary
code that runs the flower classification models, but not necessarily
other parts that are not related to the model (e.g. we do not need to
include an embedding table lookup code for NLP applications). The
ability to assemble and minimize the necessary dependencies is quite
important to reduce the overall size and increase the possible number of
environments that the app can be deployed to.</p>
<p><strong>Leveraging hardware native acceleration.</strong> Each deployment environment
comes with its own set of native acceleration techniques, many of which
are especially developed for ML. One goal of the machine learning
compilation process is to leverage that hardware’s native acceleration.
We can do it through building deployment forms that invoke native
acceleration libraries or generate code that leverages native
instructions such as TensorCore.</p>
<p><strong>Optimization in general.</strong> There are many equivalent ways to run the
same model execution. The common theme of MLC is optimization in
different forms to transform the model execution in ways that minimize
memory usage or improve execution efficiency.</p>
<p>There is not a strict boundary in those goals. For example, integration
and hardware acceleration can also be viewed as optimization in general.
Depending on the specific application scenario, we might be interested
in some pairs of source models and production environments, or we could
be interested in deploying to multiple and picking the most
cost-effective variants.</p>
<p>Importantly, MLC does not necessarily indicate a single stable solution.
As a matter of fact, many MLC practices involves collaborations with
developers from different background as the amount of hardware and model
set grows. Hardware developers need support for their latest hardware
native acceleration, machine learning engineers aim to enable additional
optimizations, and scientists bring in new models.</p>
</div>
<div id="why-study-ml-compilation">
<h2><span>1.2. </span>Why Study ML Compilation<a href="#why-study-ml-compilation" title="Permalink to this heading">¶</a></h2>
<p>This course teaches machine learning compilation as a methodology and
collections of tools that come along with the common methodology. These
tools can work with or simply work inside common machine learning
systems to provide value to the users. For machine learning engineers
who are working on ML in the wild, MLC provides the bread and butter to
solve problems in a principled fashion. It helps to answer questions
like what methodology we can take to improve the deployment and memory
efficiency of a particular model of interest and how to generalize the
experience of optimizing a single part of the model to a more generic
end-to-end solution. For machine learning scientists, MLC offers a more
in-depth view of the steps needed to bring models into production. Some
of the complexity is hidden by machine learning frameworks themselves,
but challenges remain as we start to incorporate novel model
customization or when we push our models to platforms that are not well
supported by the frameworks. ML compilation also gives ML scientists an
opportunity to understand the rationales under the hood and answer
questions like why my model isn’t running as fast as expected and what
can be done to make the deployment more effective. For hardware
providers, MLC provides a general approach to building a machine
learning software stack to best leverage the hardware they build. It
also provides tools to automate the software optimizations to keep up
with new generations of hardware and model developments while minimizing
the overall engineering effort. Importantly, machine learning
compilation techniques are not being used in isolation. Many of the MLC
techniques have been applied or are being incorporated into common
machine learning frameworks, and machine learning deployment flows. MLC
is playing an increasingly important role in shaping the API,
architectures, and connection components of the machine learning
software ecosystem. Finally, learning MLC itself is fun. With the set of
modern machine learning compilation tools, we can get into stages of
machine learning model from high-level, code optimizations, to bare
metal. It is really fun to get end to end understanding of what is
happening here and use them to solve our problems.</p>
</div>
<div id="key-elements-of-ml-compilation">
<h2><span>1.3. </span>Key Elements of ML Compilation<a href="#key-elements-of-ml-compilation" title="Permalink to this heading">¶</a></h2>
<div id="id3">
<p><span id="fig-mlc-elements"></span><img alt="../_images/mlc-elements.png" src="https://mlc.ai/_images/mlc-elements.png"/></p><p><span>Fig. 1.3.1 </span><span>MLC Elements.</span><a href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>In the previous sections, we discussed machine learning compilation at a
high level. Now, let us dive deeper into some of the key elements of
machine learning compilation. Let us begin by reviewing an example of
two-layer neural network model execution.</p>
<p>In this particular model, we take a vector by flattening pixels in an
input image; then, we apply a linear transformation that projects the
input image onto a vector of length 200 with <code><span>relu</span></code> activation.
Finally, we map it to a vector of length 10, with each element of the
vector corresponding to how likely the image belongs to that particular
class.</p>
<p><strong>Tensor</strong> is the first and foremost important element in the execution.
A tensor is a multidimensional array representing the input, output, and
intermediate results of a neural network model execution.</p>
<p><strong>Tensor functions</strong> The neural network’s “knowledge” is encoded in the
weights and the sequence of computations that takes in tensors and
output tensors. We call these computations tensor functions. Notably, a
tensor function does not need to correspond to a single step of neural
network computation. Part of the computation or entire end-to-end
computation can also be seen as a tensor function.</p>
<div id="id4">
<p><span id="fig-mlc-elem-transform"></span><img alt="../_images/mlc-elem-transform.png" src="https://mlc.ai/_images/mlc-elem-transform.png"/></p><p><span>Fig. 1.3.2 </span><span>Example MLC Process as Tensor Function Transformations.</span><a href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>There are multiple ways to implement the model execution in a particular
environment of interest. The above examples show one example. Notably,
there are two differences: First, the first linear and relu computation
are folded into a <code><span>linear_relu</span></code> function. There is now a detailed
implementation of the particular linear_relu. Of course, the real-world
use cases, the <code><span>linear_relu</span></code> will be implemented using all kinds of
code optimization techniques, some of which will be covered in the later
part of the lecture. MLC is a process of transforming something on the
left to the right-hand side. In different settings, this might be done
by hand, with some automatic translation tools, or both.</p>
<div id="remark-abstraction-and-implementations">
<h3><span>1.3.1. </span>Remark: Abstraction and Implementations<a href="#remark-abstraction-and-implementations" title="Permalink to this heading">¶</a></h3>
<p>One thing that we might notice is that we use several different ways to
represent a tensor function. For example, <code><span>linear_relu</span></code> is shown that
it can be represented as a compact box in a graph or a loop nest
representation.</p>
<div id="id5">
<p><span id="fig-mlc-abstraction-impl"></span><img alt="../_images/mlc-abstraction-impl.png" src="https://mlc.ai/_images/mlc-abstraction-impl.png"/></p><p><span>Fig. 1.3.3 </span><span>Abstractions and Implementations.</span><a href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>We use <strong>abstractions</strong> to denote the ways we use to represent the same
tensor function. Different abstractions may specify some details while
leaving out other <strong>implementation</strong> details. For example,
<code><span>linear_relu</span></code> can be implemented using another different for loops.</p>
<p><strong>Abstraction</strong> and <strong>implementation</strong> are perhaps the most important
keywords in all computer systems. An abstraction specifies “what” to do,
and implementation provides “how” to do it. There are no specific
boundaries. Depending on how we see it, the for loop itself can be
viewed as an abstraction since it can be implemented using a python
interpreter or compiled to a native assembly code.</p>
<p>MLC is effectively a process of transforming and assembling tensor
functions under the same or different abstractions. We will study
different kinds of abstractions for tensor functions and how they can
work together to solve the challenges in machine learning deployment.</p>
</div>
</div>
<div id="summary">
<h2><span>1.4. </span>Summary<a href="#summary" title="Permalink to this heading">¶</a></h2>
<ul>
<li><p>Goals of machine learning compilation</p>
<ul>
<li><p>Integration and dependency minimization</p></li>
<li><p>Leveraging hardware native acceleration</p></li>
<li><p>Optimization in general</p></li>
</ul>
</li>
<li><p>Why study ML compilation</p>
<ul>
<li><p>Build ML deployment solutions.</p></li>
<li><p>In-depth view of existing ML frameworks.</p></li>
<li><p>Build up software stack for emerging hardware.</p></li>
</ul>
</li>
<li><p>Key elements of ML compilation</p>
<ul>
<li><p>Tensor and tensor functions.</p></li>
<li><p>Abstraction and implementation are useful tools to think</p></li>
</ul>
</li>
</ul>
</div>
</div>


        </div></div>
  </body>
</html>
