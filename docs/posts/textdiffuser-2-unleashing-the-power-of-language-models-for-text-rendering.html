<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jingyechen.github.io/textdiffuser2/">Original</a>
    <h1>TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering</h1>
    
    <div id="readability-page-1" class="page">



<section>
  <div>
    <div>
      <div>
        <div>
          
          

          <p><span><sup>1</sup>HKUST,</span>
			      <span><sup>2</sup>Sun Yat-sen University,</span>
            <span><sup>3</sup>Microsoft Research</span>
          </p>

          
        </div>
      </div>
    </div>
  </div>
</section>


 









  




<section>
  <div>
  <!-- <div class="container"> -->

    <!-- Abstract. -->
    <div>
      <!-- <div class="column is-four-fifths"> -->
      <div>
        <h2>Abstract</h2>
        <p>
            The diffusion model has been proven a powerful generative model in recent years, yet remains a challenge in generating visual text. Several methods alleviated this issue by incorporating explicit text position and content as guidance on where and what text to render. However, these methods still suffer from several drawbacks, such as limited flexibility and automation, constrained capability of layout prediction, and restricted style diversity. In this paper, we present TextDiffuser-2, aiming to unleash the power of language models for text rendering. Firstly, we fine-tune a large language model for layout planning. The large language model is capable of automatically generating keywords for text rendering and also supports layout modification through chatting. Secondly, we utilize the language model within the diffusion model to encode the position and texts at the line level. Unlike previous methods that employed tight character-level guidance, this approach generates more diverse text images. We conduct extensive experiments and incorporate user studies involving human participants as well as GPT-4V, validating TextDiffuser-2&#39;s capacity to achieve a more rational text layout and generation with enhanced diversity.
          </p>


    

    <!-- Pipeline. -->
    
<!-- 
    <script
    type="module"
    src="https://gradio.s3-us-west-2.amazonaws.com/4.8.0/gradio.js"
  ></script>
  

  <gradio-app src="https://jingyechen22-textdiffuser-2.hf.space" style="width: auto;"></gradio-app> -->
  

    <!-- Pipeline. -->
    


    


    


    


</div></div></div></section>



<section id="BibTeX">
  <div>
    <h2>Contact</h2><p>
    For help or issues using TextDiffuser-2, please email Jingye Chen <a rel="license" href="mailto:qwerty.chen@connect.ust.hk">(qwerty.chen@connect.ust.hk)</a>
    , Yupan Huang <a rel="license" href="mailto:huangyp28@mail2.sysu.edu.cn">(huangyp28@mail2.sysu.edu.cn)</a> or submit a GitHub issue. For other communications related to TextDiffuser-2, please contact Lei Cui <a rel="license" href="mailto:lecu@microsoft.com">(lecu@microsoft.com)</a> or Furu Wei <a rel="license" href="mailto:fuwei@microsoft.com">(fuwei@microsoft.com)</a>.
  </p></div>
</section>

<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@article{chen2023textdiffuser,
      title={TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering},
      author={Chen, Jingye and Huang, Yupan and Lv, Tengchao and Cui, Lei and Chen, Qifeng and Wei, Furu},
      journal={arXiv preprint arXiv:2311.16465},
      year={2023}
    }
    
</code></pre>
  </div>
</section>








</div>
  </body>
</html>
