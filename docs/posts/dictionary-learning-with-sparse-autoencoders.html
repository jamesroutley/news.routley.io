<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html">Original</a>
    <h1>Dictionary Learning with Sparse AutoEncoders</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <!-- <nav class="sidebar"> -->
    <!-- Table of Contents -->
    <!-- <h4>Table of Contents</h4> -->
    <!--  -->
    <!-- </nav> -->
    <!-- <br /> -->
    <!-- <br /> -->
    <h4 id="taking-features-out-of-superposition">Taking Features Out of Superposition</h4>

<h2 id="mechanistic-interpretability">Mechanistic Interpretability</h2>

<p>Given a task that we don‚Äôt know how to solve directly in code (e.g. recognising
a cat or writing a unique sonnet), we often write programs which in turn, (via
<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>), write
second-order programs. These second-order programs (i.e. neural network weights)
can solve the task, given lots of data.</p>



<p>Suppose we have some neural network weights which describe how to do a task. We
might want to know how the network solved the task. This is useful either to (1)
understand the algorithm better for ourselves or (2) check if the algorithm
follows some guidelines we might like e.g. not being deceptive, not invoking
harmful bias etc.</p>

<p>The field of Mechanistic Interpretability aims to do just that - given a neural
network<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>, return a <code>correct</code>, <code>parsimonious</code>, <code>faithful</code> and
<code>human-understandable</code> explanation of the inner workings of the network when
solving a given task. This is analogous to the problem of
<a href="https://www.neelnanda.io/mechanistic-interpretability/reverse-engineering">reverse engineering software from machine code</a>
or the problem of a
<a href="https://colah.github.io/notes/interp-v-neuro/">neuroscientist trying to understand the human brain</a>.</p>



<h3 id="-the-dream">üí≠ The Dream</h3>

<p>How are we to translate giant inscrutable matrices into neat explanations and
high-level stories?</p>

<p>In order for a neural network to make some prediction, it uses internal neuron
activations as ‚Äúvariables‚Äù. The neuron activations build up high-level,
semantically rich concepts in later layers using lower-level concepts in earlier
layers.</p>

<p>A dream of Mechanistic Interpretability would be this:</p>

<blockquote>
  <p>Suppose we had some idea that each neuron corresponded to a single feature.
For example, we could point to one neuron and say ‚Äúif that neuron activates
(or ‚Äúfires‚Äù) then the network is thinking about cats!‚Äù. Then we point to
another and say ‚Äúthe network is thinking about the colour blue‚Äù. Now we could
give a neural network some inputs, look at which internal neurons activate (or
‚Äúfire‚Äù) and use this to piece together a story about how the network came up
with its eventual prediction. This story would involve knowing the concepts
(‚Äúfeatures‚Äù) the network was ‚Äúthinking‚Äù about together with the weights
(‚Äúcircuits‚Äù) which connect them.</p>
</blockquote>

<p>This would be great! Unfortunately, there are a couple of problems here‚Ä¶</p>



<h3 id="-the-nightmare">üëª The Nightmare</h3>

<p>Firstly, neural networks are freaking huge. There can be literally billions of
weights and activations relevant for processing a single sentence in a language
model. So with the naive approach above, it would be an incredibly difficult
practical undertaking to actually tell a good story about the network‚Äôs internal
workings<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup>.</p>

<p>But, secondly, and more importantly, when we look at the neurons of a neural
network we don‚Äôt see the concepts that it sees. We see a huge mess of concepts
all enmeshed together because it‚Äôs more efficient for the network to process
information in this way. Neurons that don‚Äôt activate on a single concept but
instead activate on many distinct concepts are known as <strong>polysemantic
neurons</strong>. It turns out that basically all neurons are highly polysemantic<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>.</p>

<p>In essence, neural networks have lots of <strong>features</strong>, which are the
<code>fundamental</code> units (‚Äúvariables‚Äù) in neural networks. We might think of features
as directions in neuron space corresponding to the concepts. And neurons are
<em>linear combinations</em> of these features in a way that makes sense to the network
but looks very entangled to us - we can‚Äôt just read off the features from
looking at the activations.</p>



<h2 id="sparse-dictionary-learning-as-a-solution">Sparse Dictionary Learning As A Solution</h2>

<p>So we‚Äôre given a network and we know that all the neurons are linear
combinations of the underlying features but we don‚Äôt know what the features are.
That is, we hypothesise that there is some linear map <strong>g</strong> from the feature
space to neuron space. Generally, feature space is much bigger than neuron
space. That is to say, there are more useful concepts in language than the
number of neurons that a network has. So our map <strong>g</strong> is a very rectangular
matrix: it takes in a large vector and outputs a smaller one with the number of
neurons as the number of dimensions.</p>

<p>We want to recover the features. To do this we could try to find a linear
function which can map from neuron space ‚Üí feature space and acts as the inverse
of <strong>g</strong>. We go to our Linear Algebra textbook (or ask ChatGPT) how to invert a
long rectangular matrix and it says‚Ä¶ oh wait, yeah this actually isn‚Äôt
possible<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>. A general linear map from feature space ‚Üí neuron space loses
information and so cannot be inverted - we can‚Äôt recover the features given only
the neurons.</p>

<p>This seems bad but let‚Äôs soldier on. Instead of giving up, we instead ask, ‚Äúokay
well we can‚Äôt invert a <em>general</em> linear map <strong>g</strong> but what constraints could we
put on <strong>g</strong> such that it might be invertible?‚Äù As it turns out, if most of the
numbers in the matrix corresponding to <strong>g</strong> are 0 (that is if <strong>g</strong> is
sufficiently <strong>sparse</strong>) then we <em>can</em> invert <strong>g</strong>.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup></p>



<p>Q: Hold on, is this reasonable? Why might we expect <strong>g</strong> to be (approximately)
sparse?</p>

<p>In predicting the next token there will be some relevant features of the
previous tokens which are useful. If the neural network has tens of thousands of
features per layer (or perhaps even more), then we would expect <em>some</em> of them
to be useful for each prediction. But if the prediction function uses all of the
features it would be super complex; most features should be irrelevant for each
prediction.</p>

<p>As an example consider if you‚Äôre deciding if a picture of an animal is a dog -
you might ask ‚Äúdoes it have 4 legs?‚Äù - 4 legged-ness is a useful feature. The
texture of its fur is also relevant. The question ‚Äúwould a rider sit within or
on top‚Äù is probably not relevant, though it might be relevant in other
situations for example distinguishing a motorbike from a car. In this way, not
all of the features are needed at once<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">6</a></sup>.</p>



<blockquote>
  <p>To recap, so far we‚Äôve said:</p>

  <ol>
    <li>Language models use features in order to predict the next token.</li>
    <li>There are potentially a lot more features than there are neurons.</li>
    <li>If the linear map <strong>g</strong>: features ‚Üí neurons was sparse then we might be
able to find an inverse.</li>
    <li>Sparse maps are relatively good approximations to the real linear map
<strong>g</strong>.</li>
  </ol>
</blockquote>





<p>Sparse Dictionary Learning is a method which exploits these facts to numerically
find the inverse of g. Intuitively what we have is a lookup table (or a
‚Äúdictionary‚Äù) which tells us how much of each feature goes into each neuron. And
if these features look monosemantic and human-understandable then we‚Äôre getting
very close to the dream of Mechanistic Interpretability outlined above. We could
run a model, read off the features it used for the prediction and build a story
of how it works!</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/dictionary-learning/projection_g.png" width="800" alt="Dictionary Learning as an Inverse Problem"/>
    <figcaption>Dictionary Learning can be seen as trying to find the inverse map to g. The map g is analogous to PCA - it‚Äôs the network‚Äôs way of trying to fit as much
information as possible into a lower-dimensional space. </figcaption>
    </figure>
</div>



<h3 id="dictionary-learning-set-up">Dictionary Learning Set-up</h3>

<p>We‚Äôll focus here on
<a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Anthropic‚Äôs set-up</a>.</p>

<p>We start with a small 1-Layer transformer which has an embedding dimension
of 128. Here the <strong>MLP</strong> hidden dimension is 512.<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup> The MLP contains:</p>

<ol>
  <li>An up_projection to MLP neuron space (512d),</li>
  <li>A ReLU activation which produces activations and then</li>
  <li>A down_projection back to embedding space (128d)</li>
</ol>

<p>We capture the MLP neuron activations and send those through our sparse
autoencoder which has N dimensions for some N ‚â• 512.</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/dictionary-learning/transformer.png" width="800" alt="Transformer with an AutoEncoder attached"/>
    <figcaption>The AutoEncoder set-up on a 1-Layer Transformer. The MLP activations are
captured and sent through the AutoEncoder. </figcaption>
    </figure>
</div>



<p>An <strong>AutoEncoder</strong> is a model which tries to reconstruct some data after putting
it through a <strong>bottleneck</strong>. In
<a href="https://en.wikipedia.org/wiki/Autoencoder">traditional autoencoders</a>, the
bottleneck might be mapping to a smaller dimensional space or including noise
that the representation should be robust to. AutoEncoders aim to recreate the
original data as closely as possible despite the bottleneck. To achieve the
reconstruction, we use a reconstruction loss which penalises outputs by how much
they differ from the MLP activations (the inputs to the AutoEncoder).</p>

<p>In the <strong>Sparse AutoEncoder</strong> setting, our ‚Äúbottleneck‚Äù is actually a higher
dimensional space than neuron space (N ‚â• 512), but the constraint is that the
autoencoder features are <strong>sparse</strong>. That is, for any given set of MLP neuron
activations, only a small fraction of the <code>features</code> should be activated.</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/dictionary-learning/autoencoder.png" width="800" alt="Autoencoder representation"/>
    <figcaption>The AutoEncoder passes some inputs through a hidden layer (which acts as a
bottleneck) and tries to reconstruct the inputs. For a well-trained AutoEncoder,
the output vector should be approximately the input vector.</figcaption>
    </figure>
</div>



<p>In order to make the hidden feature activations sparse, we add an L1 loss over
the feature activations to the reconstruction loss for the AutoEncoder‚Äôs loss
function. Since the L1 loss gives the absolute value of the vector, minimising
L1 loss pushes as many as possible of the feature activations towards zero
(whilst still being able to reconstruct the MLP neurons to get low
reconstruction loss).</p>

<blockquote>
  <p>To recap:</p>

  <ul>
    <li>The input of the AutoEncoder is the MLP activations.</li>
    <li>
      <p>The goal is for the output of the AutoEncoder to be as close to the input as
possible - the reconstruction loss penalises outputs by how much they differ
from the MLP activation inputs.</p>
    </li>
    <li>The bottleneck is the sparsity in the hidden layer which is induced by
pressure from the L1 loss to minimise feature activations.</li>
  </ul>
</blockquote>

<p>In summary, the set-up Anthropic uses is:</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/dictionary-learning/setup.png" width="800" alt="Dictionary Learning Setup Table"/>
    <figcaption>Anthropic&#39;s Dictionary Learning Setup</figcaption>
    </figure>
</div>

<h3 id="anthropics-results">Anthropic‚Äôs Results</h3>

<p>The most surprising thing about this approach is that it works so well. Like
<em>really</em> well.</p>

<p>There are, broadly, two ways to think about features:</p>

<ol>
  <li><strong>Features as Results</strong> - the feature activates when it sees particular
inputs. Looking at features can help us to understand the inputs to the
model.</li>
  <li><strong>Features as Actions</strong> - ultimately features activating leads to differences
in the output logits. Features can be seen as up-weighting or down-weighting
certain output tokens. In some sense, this is the more fundamental part. If
we ask ‚Äúwhat is a feature for?‚Äù then the answer is ‚Äúto help the model in
predicting the next token.‚Äù</li>
</ol>



<p>Anthropic find many features which activate strongly in a specific context (say
Arabic script or DNA base pairs) and also (mostly) only activate when that
context is present. In other words, the features have high
<a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision and recall</a>. This
suggests that these are ~monosemantic features! In terms of
<code>Features as Results</code>, this captures what we would hope for - the features that
appear are mostly human-understandable.</p>

<p>The authors also find that once a feature is activated, the result is an
increase in plausible next tokens given the input. In particular, to demonstrate
this counterfactually, we can add a large amount of a given feature to the
neuron activations. Theoretically, this should ‚Äústeer‚Äù the model to thinking
that context was present in the input, even if it wasn‚Äôt. This is a great test
for <code>Features as Actions</code>.</p>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/dictionary-learning/steering.png" width="800" alt="Diagram of steering LLM"/>
    <figcaption>Once we steer the model by adding some of the Han Chinese feature, the model
starts outputting more Han Chinese. Similarly for other identified features.
Note: this is a very small transformer so the outputs might not completely make
sense; it‚Äôs mainly recognising the context that it‚Äôs in.</figcaption>
    </figure>
</div>

<p>Additionally, if we fully replace the MLP activations with the output of our
autoencoder<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" rel="footnote">8</a></sup>, we get a model which explicitly uses our feature dictionary
instead of the learned MLP neurons. Here the resulting ‚Äúdictionary model‚Äù is
able to get 95% of the performance of the regular model. The dictionary model
achieves this despite, in the case of large autoencoders, the features being
extremely sparse. This performance is a great sign for <code>Features as Actions</code>; it
suggests that the sparse features capture most of the information that the model
is using for its prediction task! This also validates that our assumption that
features are approximately sparse seems to be a fairly good assumption<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" rel="footnote">9</a></sup>.</p>



<h3 id="other-phenomena">Other Phenomena</h3>

<p>They also note some other smaller results:</p>

<ul>
  <li>As the number of features in the autoencoder increases, we capture more of the
true performance of the model. This correlation suggests that models are
probably implementing up to 100x or more features than they have neurons ü§Ø</li>
  <li>The features are generally understandable by both humans and by other Machine
Learning models. To show this they ask Claude to do some interpretation too.</li>
  <li>As the number of features increases, the features themselves ‚Äúsplit‚Äù. That is
even though a feature is monosemantic - it activates on a single concept -
there may be levels to concepts. For example, small autoencoders might have a
(monosemantic) feature for the concept ‚Äúdog‚Äù. But larger autoencoders have
features for corgis and poodles and large dogs etc. which break down the
concept of dog into smaller chunks. Scale helps with refining concepts.</li>
</ul>

<div>
  <figure>
    <img src="http://www.kolaayonrinde.com/blog/images/dictionary-learning/feature_splitting.png" width="800" alt="FEature Splitting in LLM"/>
    <figcaption>What was once a single feature representing the &#34;the&#34; in mathematical prose gets broken down into more specific concepts as we increase the number of features in the autoencoder</figcaption>
    </figure>
</div>



<h2 id="whats-next">What‚Äôs Next?</h2>

<h4 id="have-all-the-problems-in-mechanistic-interpretability-been-solved">Have All The Problems in Mechanistic Interpretability Been Solved?</h4>

<p>Certainly not. Although this approach is a breakthrough in approaching features
and converting regular networks into less polysemantic ones, some problems
remain:</p>

<h3 id="-scaling-sparse-autoencoders">üìà Scaling Sparse Autoencoders</h3>

<p>Large models are still, well ‚Ä¶ large. Dictionary learning mitigates the problem
since we don‚Äôt have to deal with polysemantic neurons anymore. But there‚Äôs still
a lot that could happen between doing this on a small 1-Layer model and a large
model. In particular, since there are many more features than neurons, Sparse
AutoEncoders for large models could be absolutely gigantic and may take as much
compute to train as the model‚Äôs pre-training. We will very likely need ways to
improve the efficiency of Sparse AutoEncoder training.</p>

<h3 id="-compositionality-and-interaction-effects">ü§ù Compositionality and Interaction Effects</h3>

<p>In Machine Learning, as in Physics,
<a href="https://www.science.org/doi/10.1126/science.177.4047.393">More Is Different</a>.
That is, there may be qualitatively different behaviours for large models as
compared to smaller ones. One clear way this could occur is when features are
composed of many sub-features across different layers and form complex
interactions. This is an open problem to be explored.</p>

<h3 id="-universality">üåê Universality</h3>

<p>The <a href="https://distill.pub/2020/circuits/zoom-in">Universality Hypothesis</a> from
Chris Olah states that sufficiently neural networks with different architectures
and trained on different data will learn the same high-level features and
concepts.</p>

<p>The authors show that when two models are trained with the same architecture but
different random initialisations, they learn similar features. This is certainly
a step towards universality but doesn‚Äôt show the whole thesis. A strong form of
Universality would suggest that there are some high-level ‚Äúnatural‚Äù
features/concepts which lots of <em>different</em> architectures for predictors
(silicon and human brains) all converge on. We‚Äôre quite a way from showing this
in the general case.</p>

<h3 id="-interpretability-metrics">üìê Interpretability Metrics</h3>

<p>Though there are some proxy measures for interpretability, currently the best
metric that we have is for a human to check and say ‚Äúyes I can interpret this
feature‚Äù or ‚Äúno I can‚Äôt‚Äù. This seems hard to operationalise at scale as a
concrete metric.</p>

<p>To bridge this gap large models such as GPT-4 and Claude can also help with the
interpretability. In a process known as AutoInterpret, LLMs are given a prompt
and how much each feature activates. They then attempt to interpret the feature.
This works kinda okay at the moment but it feels like there should be a cleaner,
more principled approach.</p>

<h3 id="Ô∏é-steering">‚ò∏Ô∏é Steering</h3>

<p>The authors show that by adding more of a given feature vector in activation
space, you can influence a model‚Äôs behaviour. When, whether, and how steering
works reliably and efficiently are questions that could all be useful. We might
wish to steer models as a surgical needle to balance out the more coarse tool
that is RLHF. In the future, this may also be useful to reduce harmful behaviour
in increasingly powerful models.</p>

<h3 id="-modularity">üî≥ Modularity</h3>

<p>As mentioned above, there would be an embarrassingly large number of features
for a model like <a href="https://openai.com/research/gpt-4">GPT-4</a> and so it looks like
it will be difficult to create succinct compelling stories which involve so many
moving parts. In some sense, this is the lowest level of interpretability. It‚Äôs
analogous to trying to understand a very complex computer program by looking
through it character by character, if the words were all jumbled up.</p>

<p>What we would like is some slightly higher level concepts composed of multiple
features with which we can use to think. Splitting up the network into
macro-modules rather than the micro-level features seems like a promising path
forward.</p>



<h2 id="conclusion">Conclusion</h2>

<p>Anthropic are very positive about this approach and finish their blog post with
the line:</p>

<div><div><pre><code>For the first time, we feel that the next primary obstacle to interpreting large language models is engineering rather than science.
</code></pre></div></div>

<p>There is some truth to how exciting this development is. We might ask whether
the work ahead is purely
<a href="https://www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/">scaling up</a>.
As we outlined in the problems for future work above, I do believe there are
still some Science of Deep Learning problems which Mechanistic Interpretability
can sink its teeth into. Only now, we also have a new tool which is incredibly
powerful to help us along the way.</p>

<p>In light of the other problems that still remain to be solved, we might add the
final sentences of
<a href="https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf">Turing‚Äôs 1950 paper</a>,
as an addendum:</p>

<div><div><pre><code>We can only see a short distance ahead, but we can see plenty there that needs to be done.
</code></pre></div></div>

<!-- ---

_Note: I am working on a replication and extension of the Sparse Autoencoders
results supported by [Cavendish Labs](https://cavendishlabs.org/) - I will share
the results here in the next few days. _ -->

<!-- Footnotes themselves at the bottom. -->




  </div></div>
  </body>
</html>
