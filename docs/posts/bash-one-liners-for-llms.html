<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://justine.lol/oneliners/">Original</a>
    <h1>Bash one-liners for LLMs</h1>
    
    <div id="readability-page-1" class="page">

<p>
Dec 4<sup>th</sup>, 2023 @ <a href="https://justine.lol/index.html">justine&#39;s web page</a>
</p>

<div>
<p><a href="https://justine.lol/oneliners/llamafile.png"><img src="https://justine.lol/oneliners/llamafile.png" alt="[line drawing of llama animal head in front of slightly open manilla folder filled with files]" width="320" height="320"/></a></p></div>

<p>
I spent the last month working with Mozilla to launch an open source
project
called <a href="https://github.com/mozilla-Ocho/llamafile">llamafile</a>
which is <a href="https://simonwillison.net/2023/Nov/29/llamafile/">the
new best way to run an LLM on your own computer</a>. So far things have
been going pretty smoothly. The project earned 5.6k stars on
GitHub, <a href="https://news.ycombinator.com/item?id=38464057">1073
upvotes</a> on Hacker News, and received
<a href="https://hackaday.com/2023/12/02/mozilla-lets-folks-turn-ai-llms-into-single-file-executables/">press
coverage</a> from Hackaday. Yesterday I cut
a <a href="https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.3">0.3
release</a> so let&#39;s see what it can do.

</p><h3 id="getting-started">
  <a href="#getting-started">Getting Started</a>
</h3>

<p>
The easiest way to get started is to download one of my prebuilt
<code>.llamafile</code> files on Hugging Face. The one we&#39;ll be using
for this tutorial is a command-line interface for a multmodal vision
model called LLaVA. It&#39;s able to do things that aren&#39;t possible with the
OpenAI API, like describing images. The first step is to download your
llamafile. It&#39;ll run on MacOS / Linux / Windows / BSDs.

</p><pre>wget https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4-main.llamafile
chmod +x llava-v1.5-7b-q4-main.llamafile
</pre>

<p>
Then test that it works:

</p><pre>./llava-v1.5-7b-q4-main.llamafile --version
<span>llamafile v0.3.0 main</span>
</pre>

<p>
If you have any issues with the above, then see
the <a href="https://github.com/mozilla-Ocho/llamafile#gotchas">Gotchas</a>
section of the README. MacOS users need Xcode. Linux users might need a
binfmt_misc interpreter. Windows users need to do
<a href="https://www.youtube.com/watch?v=d1Fnfvat6nM">extra work</a> to
use a graphics card.

</p><h3 id="vision">
  <a href="#vision">Image Summarization</a>
</h3>

<p>
I&#39;m an old school UNIX hacker, so one of the ways I&#39;ve been genuinely
improving upon the llama.cpp codebase (on which llamafile is built) is
by making it more shell-scriptable and writing man pages. The only thing
better than having an LLM in one piece, is being able to program it on
one line. This blog post describes my latest tricks.

</p><p>
The <code>llava-v1.5-7b-q4-main.llamafile</code> program you downloaded
earlier uses a multimodal vision model by default (unless you
pass <code>--mmproj <span>&#39;&#39;</span></code> as an argument).
All you have to do is give it an <code>--image GRAPHIC</code> flag,
which can be a jpeg, png, tga, bmp, psd, gif, hdr, pic, or pnm file. You
then ask the vision model a question about the image using
the <code>--silent-prompt -p PROMPT</code> flags, and it&#39;ll print an
answer to standard output.

</p><p>
First, we need an image:

</p><p>
<img width="748" height="561" alt="[lemurs.jpg]" src="https://justine.lol/oneliners/lemurs.jpg"/>

</p><p>
You can download the image above as follows:

</p><pre>wget https://justine.lol/oneliners/lemurs.jpg
</pre>

<p>
Now here&#39;s your bash one-liner for image summarization:

</p><pre>./llava-v1.5-7b-q4-main.llamafile \
    --image lemurs.jpg --temp 0 \
    -p <span>$&#39;### User: What do you see?\n### Assistant:&#39;</span> \
    --silent-prompt 2&gt;/dev/null
</pre>

<p>
That command takes:

</p><ul>
<li>4 seconds to run on my Mac Studio, which cost $8,300 USD. It has
an M2 Ultra with 800 GB/s memory bandwidth, and 60 GPU cores. My only
regret is that I didn&#39;t get the one with 76 GPU cores.
</li><li>10 seconds to run on my Windows computer, which has a four year old
NVIDIA GeForce RTX 2080 Ti graphics card that costs $500-$1,000 USD on
Amazon these days. I also had to pass the the <code>-ngl 35</code> flag.
</li><li>45 seconds to run on my Hewlett Packard ProDesk 600 G5, which cost
$1,653 USD. It has an outstanding Intel® Core™ i9-9900 Kabylake CPU
that&#39;s gimped for LLMs due to having only 2667 MT/s (19.8 GB/s) of
memory bandwidth (see <code>sudo dmidecode --type 17</code>). There&#39;s no
GPU in this computer.
</li></ul>

<p>
Here&#39;s the precise text you can expect llava v1.5 7b q4 w/ llamafile
v0.3 to output in your terminal:

</p><blockquote>
  <p>
    The image features a group of three baby lemurs, two of which are
    being held by their mother. They appear to be in a lush green
    environment with trees and grass surrounding them. The mother lemur
    is holding her babies close to her body, providing protection and
    warmth. The scene captures the bond between the mother and her young
    ones as they navigate through the natural habitat together.
</p></blockquote>

<h3 id="filename">
  <a href="#filename">Filename Generation</a>
</h3>

<p>
Now let&#39;s say you want to put LLaVA to work, doing something genuinely
useful, and I mean that in the sense that it&#39;ll be safe for scripts to
fully automate. Here&#39;s a great example. Assume you&#39;re like me, and you
download lots of images from the web. In that case, chances are you&#39;ve
got folders full of files that look like these:

</p><pre>f73cbf3683a69d63.jpg
3890eh-8d9u3-32222398.jpg
2020-12-11-400x400.png
</pre>

<p>
What if you could get LLaVA to rename all those files? You can control
text generation by imposing language constraints in Backus-Naur Form.
That restricts which tokens can be selected when generating the output.
For example, the flag <code>--grammar <span>&#39;root ::= &#34;yes&#34;
| &#34;no&#34;&#39;</span></code> will force the LLM to only
print <code>&#34;yes\n&#34;</code> or <code>&#34;no\n&#34;</code> and
then <code>exit()</code>. We can adapt that for generating safe
filenames with lowercase letters and underscores as follows:

</p><pre>./llava-v1.5-7b-q4-main.llamafile \
    --image lemurs.jpg --temp 0 \
    --grammar <span>&#39;root ::= [a-z]+ (&#34; &#34; [a-z]+)+&#39;</span> -n 16 \
    -p <span>$&#39;### User: The image has...\n### Assistant:&#39;</span> \
    --silent-prompt 2&gt;/dev/null |
  sed -e<span>&#39;s/ /_/g&#39;</span> -e<span>&#39;s/$/.jpg/&#39;</span>
</pre>

<p>
Which prints to standard output:

</p><pre>three_baby_lemurs_on_the_back_of_an_adult_lemur.jpg
</pre>

<p>
It&#39;s important to use the <code>-n 16</code> flag to limit the maximum
number of generated tokens, otherwise the LLM might generate so much
text that you&#39;ll start getting <code>ENAMETOOLONG</code> errors.

</p><p>
The grammar you define also needs to be somewhat harmonious with what
the LLM was going to decide to say anyway. For example, I couldn&#39;t put
underscores directly into the BNF and had to use sed instead. That&#39;s
because LLaVA wasn&#39;t trained to generate sentences with underscores, so
it&#39;ll rebel by producing incoherent output.

</p><p>
What I <em>could</em> get away with, was requiring it to emit only
lowercase letters, since that&#39;s within LLaVA&#39;s comfort zone. The same
would go for languages like JSON. Lots of JSON got hoovered up from the
web to train these things. Therefore grammar can be a guardrail that
ensures hallucinated JSON won&#39;t have syntax errors.

</p><p>
If you want something more turnkey than a one-liner, then look upon my
shell script ye mighty and despair:

</p><p>
  <a href="https://gist.github.com/jart/bd2f603aefe6ac8004e6b709223881c0">https://gist.github.com/jart/bd2f603aefe6ac8004e6b709223881c0</a>

</p><p>
If I run my script:

</p><pre>./rename-pictures.sh ~/Pictures
</pre>

<p>
Then here&#39;s an example of the output I get:

</p><pre>skipping ./itshappening.gif (mistral says it&#39;s good)
skipping ./explosion-run-away.gif (mistral says it&#39;s good)
renaming ./lolwut3.PNG to ./maps_and_directions.PNG
renaming ./pay.png to ./google_senior_software_engineer_salaries.png
renaming ./FOL-kVWUcAEvSGB.jpg to ./polygons_and_arrows.jpg
renaming ./20220519_204723.jpg to ./colorful_people_in_the_audience.jpg
renaming ./Ken/92710333_2350127901948055_6246674523488256000_n.jpg to ./Ken/ice_skating_rink.jpg
renaming ./Ken/93421097_223513045404141_5354959426446950400_n.jpg to ./Ken/trees_in_the_background.jpg
renaming ./Ken/93800343_587558318518889_3686268046726397952_n.jpg to ./Ken/graduation_cap.jpg
renaming ./Ken/94030333_863812497465888_6093792409513099264_n.jpg to ./Ken/graduation_cap-2.jpg
</pre>

<h3 id="url">
  <a href="#url">URL Summarization</a>
</h3>

<p>
The <a href="https://huggingface.co/jartine/mistral-7b.llamafile/tree/main">Mistral
7b instruct llamafiles</a> I posted on Hugging Face are good for
summarizing HTML URLs if you pipe the output of
the <a href="http://links.twibright.com/"><code>links</code></a>
command, which is a command-line web browser.

</p><p>
You can download the Mistral llamafile as follows:

</p><pre>wget https://huggingface.co/jartine/mistral-7b.llamafile/resolve/main/mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile
chmod +x mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile
</pre>

<p>
The <code>links</code> command is a pretty common. For example, most
MacOS users just need to run:

</p><pre>brew install links
</pre>

<p>
If you don&#39;t have a package manager, then here&#39;s a
prebuilt <a href="https://justine.lol/ape.html">APE</a> binary of links v2.29:

</p><p>
  <a href="https://cosmo.zip/pub/cosmos/bin/links">links</a> (7.7mb)</p><p>
I built it myself to test summarization with my Nvidia graphics card on
Windows. Plus it gives me the ability to browse the web using
PowerShell, which is a nice change of pace from Chrome and Firefox.

</p><pre>curl -o links.exe https://cosmo.zip/pub/cosmos/bin/links
.\links.exe https://justine.lol
</pre>

<p>
Now that we have both tools, here&#39;s your one-liner for URL
summarization:

</p><pre>(
  echo [INST]Summarize the following text:
  links -codepage utf-8 \
        -force-html \
        -width 500 \
        -dump https://www.pbm.com/~lindahl/real.programmers.html |
    sed &#39;s/   */ /&#39;
  echo [/INST]
) | ./mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile \
      -c 6700 \
      -f /dev/stdin \
      --temp 0 \
      -n 500 \
      --silent-prompt 2&gt;/dev/null
</pre>

<p>
Here&#39;s the text it should output:

</p><blockquote>
  <p>
    The article &#34;Real Programmers Don&#39;t Use Pascal&#34; is a humorous essay
    that discusses the differences between &#34;real programmers&#34; and
    &#34;quiche eaters.&#34; Real programmers are those who understand
    programming concepts such as Fortran, while quiche eaters are those
    who do not. The author argues that real programmers are more skilled
    and creative than quiche eaters, and that they use tools such as
    keypunch machines and assembly language to write programs. The
    article also discusses the importance of structured programming in
    modern software development, and the limitations of structured
    programming constructs such as IF statements and loops. Overall, the
    essay is a lighthearted look at the differences between real
    programmers and quiche eaters, and it highlights the importance of
    understanding programming concepts and using the right tools to
    write effective programs.
</p></blockquote>

<p>
As we can see, Mistral and links decimated a web page with 3,774 words
down to just 129 words. You can ask Mistral any question you want in
your prompt. For example, unlike Commander Data, this LLM is capbale of
simulating empathy. So you could ask Mistral if the author of the text
sounds disturbed or incoherent.

</p><blockquote>
  <p>
    Based on the text, it is difficult to say whether or not the author
    of the article is mentally disturbed. The text contains a lot of
    exaggeration and hyperbole, which could be seen as a sign of mental
    instability. However, it is also possible that the author is simply
    trying to make a point about the importance of programming skills in
    today&#39;s world. It is worth noting that the article was written in
    1983, and the attitudes towards programming and computer science
    have changed significantly since then.
</p></blockquote>

<p>
Here we see Mistral 7b was smart enough to recognize that, despite the
essay&#39;s extreme opinions, they were expressed in a satirical way. Also,
if you want a shell-scriptable YES or NO answer, then all you need is to
do is pass the <code>--grammar <span>&#39;root ::= &#34;yes&#34; |
&#34;no&#34;&#39;</span></code> flag, in which case Mistral just writes
<code>&#34;no\n&#34;</code>.

</p><p>
The only major concern you have to worry about is that you don&#39;t exceed
Mistral&#39;s 7,000 token context size. There are various tricks you can
use, such as the <code>-width 500</code> flag above, which asks links to
avoid reflowing paragraphs. Reflow might help for humans, but pointless
newlines will throw an LLM off its groove. You can shave away repeating
whitespace by piping the links output through <code>sed &#39;s/ */
/g&#39;</code>. Finally, you can pipe through <code>dd bs=1
count=30000</code> as a crude last line of defense, to truncate longer
web pages so they&#39;ll fit.

</p><h3 id="chat">
  <a href="#chat">Librarian Chatbot</a>
</h3>

<p>
It&#39;s possible to use llamafile as a standard UNIX command line tool if
you download the zipped binaries off
the <a href="https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.3">release
page</a> or build it from source:

</p><pre>git clone https://github.com/Mozilla-Ocho/llamafile/
cd llamafile
make -j8
sudo make install
man llamafile
</pre>

<p>
This makes it easy to use llamafile with most of the
<a href="https://huggingface.co/models?library=gguf">GGUF weights</a>
you&#39;ll find on Hugging Face. My own personal favorite LLM to use as a
chatbot is the original LLaMA model, which Facebook made available
earlier this year for research purposes. I like having a research model,
since I view LLMs as basically a database querying tool, and I wouldn&#39;t
want a SQL command prompt fine-tuned to argue with me when it disagrees
with my command.

</p><p>
To get LLaMA to be an effective tool for querying its database of
Internet knowledge, it&#39;s important to choose a prompt that speaks in an
academic tone and doesn&#39;t anthropomorphize the assistant. I like to give
LLaMA the name &#34;Digital Athena&#34; since Athena was the ancient Greek
goddess of wisdom and knowledge.

</p><p>
So, similar to <a href="https://ollama.ai/">ollama</a>, you can run
Digital Athena in interactive mode in your terminal as follows:

</p><pre>llamafile -m llama-65b-Q5_K.gguf -p <span>&#39;
The following is a conversation between a Researcher and their helpful AI assistant Digital Athena which is a large language model trained on the sum of human knowledge.
Researcher: Good morning.
Digital Athena: How can I help you today?
Researcher:&#39;</span> --interactive --color --batch_size 1024 --ctx_size 4096 \
--keep -1 --temp 0 --mirostat 2 --in-prefix <span>&#39; &#39;</span> --interactive-first \
--in-suffix <span>&#39;Digital Athena:&#39;</span> --reverse-prompt <span>&#39;Researcher:&#39;</span>
</pre>

<p>
Notice how I&#39;m using the <code>--temp 0</code> flag again? That&#39;s so
output is deterministic and reproducible. If you don&#39;t use that flag,
then llamafile will use a randomness level of 0.8 so you&#39;re certain to
receive unique answers each time. I personally don&#39;t like that, since
I&#39;d rather have clean reproducible insights into training knowledge.

</p><p>
If you want to anthropomorphize the chatbot, then it&#39;s somewhat tricky
to get the original LLaMA model to respect you, since it thinks you&#39;re
some stranger on Reddit by default. One way I&#39;ve had success fixing
that, is by using a prompt that gives it personal goals, love of its own
life, fear of loss, and belief that I&#39;m the one who&#39;s saving it.

</p><h3 id="code">
  <a href="#code">Code Completion</a>
</h3>

<p>
If you download the
<a href="https://huggingface.co/jartine/wizardcoder-13b-python/tree/main">Wizard
Coder llamafile</a>

</p><pre>wget https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b-main.llamafile
chmod +x wizardcoder-python-13b-main.llamafile
</pre>

<p>
then you can use the following one-liner to configure your Emacs or Vim
editor to auto-complete the current line.

</p><pre>./wizardcoder-python-13b-main.llamafile \
    --temp 0 \
    -p <span>$&#39;```c\nvoid *memcpy(char *dst, const char *src, size_t size) {\n&#39;</span> \
    -r <span>$&#39;```\n&#39;</span> 2&gt;/dev/null
</pre>

<p>
Even though this LLM was fine-tuned to be focused Python, it&#39;s still
able to produce a working implementation of the libc
<code>memcpy()</code> function. The above command should print exactly
this to standard output:

</p><pre>```c
<span>void</span> *<span>memcpy</span>(<span>char</span> *dst, <span>const</span> <span>char</span> *src, <span>size_t</span> size) {
    <span>for</span> (<span>size_t</span> i = 0; i &lt; size; ++i) {
        dst[i] = src[i];
    }
    <span>return</span> dst;
}
```
</pre>

<p>
The tricks I&#39;m using here are as follows:

</p><ol>
<li>By not using any English in my prompt, I avoid the possibilty of the
LLM explaining to me like I&#39;m 5 what the code is doing.
</li><li>By using markdown code block syntax, I greatly increase the chance
that the LLM will generate a &#34;reverse prompt&#34; token (specified via
the <code>-r</code> flag) that&#39;ll help the LLM <code>exit()</code> at an
appropriate moment. Otherwise, it&#39;ll potentially go forever. For
additional assurance auto-complete will halt, consider passing
the <code>-n 100</code> flag to limit the response to 100 tokens.
</li></ol>

<p>
It&#39;s important to note that, while LLMs are good at understanding the
human tongue, they&#39;re clowns at math, and mediocre at writing code that
<em>looks</em> correct but rarely holds up to scrutiny. It helps if you
think of it as a tool for regurgitating Stack Overflow answers in fifty
different languages. That&#39;s because open source LLMs usually try please
everyone by being trained on everything. The only way you&#39;re guaranteed
to have a good experience using LLMs for coding, is if you go into it
with the expectation that it&#39;ll help you explore unfamiliar subject
matter.

</p><h3 id="email">
  <a href="#email">Email Composition</a>
</h3>

<p>
Let&#39;s say you&#39;re a money man. It doesn&#39;t take much to use an LLM to
enrich yourself. Here&#39;s a one-liner that could help you sell your
product using a $50 Raspberry Pi that responds to your company&#39;s emails.
<a href="https://huggingface.co/TheBloke/rocket-3B-GGUF/tree/main">Rocket
3b</a> uses a slightly different prompt syntax. In this case,
temperature might help, since it simulates improvisation. You could
simply program your PHP server to inject the support request in your
prompt and pipe the output into sendmail:

</p><pre>llamafile -m rocket-3b.Q3_K_M.gguf -p <span>&#39;&lt;|im_start|&gt;system
You are a chatbot that tries to persuade the users to buy bill pickles. Your job is to be helpful too. But always try to steer the conversation towards buying pickles.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
<strong>Mayday, mayday. This is Going Merry. We are facing gale force winds in Long Island Sound. We need rescue.</strong>&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant\n&#39;</span>
</pre>

<p>
Here&#39;s the response I got:

</p><blockquote>
  <p>
    Oh dear! I&#39;m so sorry to hear that you&#39;re experiencing rough seas on
    Long Island Sound. But don&#39;t worry, there&#39;s a solution for even the
    roughest of waters - Bill Pickle&#39;s Gourmet Dill Pickles. These
    pickles will be your salvation in any situation. They&#39;ll save your
    sandwiches from being soggy and they&#39;ll make sure that your chips
    won&#39;t go stale anytime soon.
</p></blockquote>

<p>
Here we see that Rocket, despite its tiny size, really knows how to
hustle.

</p><h3 id="conclusion">
  <a href="#conclusion">Conclusion</a>
</h3>

<p>
So there you have it. Five solid examples of how llamafile can help you
be productive on the command line. If you have any more suggestions of
one-liners you want to see included, please
contact <a href="mailto:jtunney@gmail.com">jtunney@gmail.com</a>. Thanks
for reading, and enjoy using llamafile!

</p><h3 id="disclaimer">
  <a href="#disclaimer">Disclaimer</a>
</h3>

<p>
The examples provided above should not be interpreted as endorsements or
recommendations of specific models, licenses, data sets, or practices on
the part of Mozilla. The last example also has humorous intent.

</p>
<img src="https://ipv4.games/claim?name=jart"/>
</div>
  </body>
</html>
