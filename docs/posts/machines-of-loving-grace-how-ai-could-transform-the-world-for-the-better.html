<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://darioamodei.com/machines-of-loving-grace">Original</a>
    <h1>Machines of loving grace: How AI could transform the world for the better</h1>
    
    <div id="readability-page-1" class="page"><div>
        <article>
            
            <h4>How AI Could Transform the World for the Better</h4>

            
            <p>I think and talk a lot about the risks of powerful AI. The company I’m the CEO of, Anthropic, does a lot
                of research on how to reduce these risks. Because of this, people sometimes draw the conclusion that I’m
                a pessimist or “doomer” who thinks AI will be mostly bad or dangerous. I don’t think that at all. In
                fact, one of my main reasons for focusing on risks is that they’re the only thing standing between us
                and what I see as a fundamentally positive future. <strong>I think that most people are underestimating
                    just how radical the upside of AI could be</strong>, just as I think most people are underestimating
                how bad the risks could be.</p>
            <p>In this essay I try to sketch out what that upside might look like—what a world with powerful AI might
                look like if everything goes <i>right</i>. Of course no one can know the future with any certainty or
                precision, and the effects of powerful AI are likely to be even more unpredictable than past
                technological changes, so all of this is unavoidably going to consist of guesses. But I am aiming for at
                least educated and useful guesses, which capture the flavor of what will happen even if most details end
                up being wrong. I’m including lots of details mainly because I think a concrete vision does more to
                advance discussion than a highly hedged and abstract one.</p>
            <p>First, however, I wanted to briefly explain why I and Anthropic haven’t talked that much about powerful
                AI’s upsides, and why we’ll probably continue, overall, to talk a lot about risks. In particular, I’ve
                made this choice out of a desire to:</p>
            <ul>
                <li><strong>Maximize leverage</strong>. The basic development of AI technology and many (not all) of its
                    benefits seems inevitable (unless the risks derail everything) and is fundamentally driven by
                    powerful market forces. On the other hand, the risks are not predetermined and our actions can
                    greatly change their likelihood.</li>
                <li><strong>Avoid perception of propaganda</strong>. AI companies talking about all the amazing benefits
                    of AI can come off like propagandists, or as if they’re attempting to distract from downsides. I
                    also think that as a matter of principle it’s bad for your soul to spend too much of your time
                    “talking your book”.</li>
                <li><strong>Avoid grandiosity</strong>. I am often turned off by the way many AI risk public figures
                    (not to mention AI company leaders) talk about the post-AGI world, as if it’s their mission to
                    single-handedly bring it about like a prophet leading their people to salvation. I think it’s
                    dangerous to view companies as unilaterally shaping the world, and dangerous to view practical
                    technological goals in essentially religious terms.</li>
                <li><strong>Avoid “sci-fi” baggage</strong>. Although I think most people underestimate the upside of
                    powerful AI, the small community of people who do discuss radical AI futures often does so in an
                    excessively “sci-fi” tone (featuring e.g. uploaded minds, space exploration, or general cyberpunk
                    vibes). I think this causes people to take the claims less seriously, and to imbue them with a sort
                    of unreality. To be clear, the issue isn’t whether the technologies described are possible or likely
                    (the main essay discusses this in granular detail)—it’s more that the “vibe” connotatively smuggles
                    in a bunch of cultural baggage and unstated assumptions about what kind of future is desirable, how
                    various societal issues will play out, etc. The result often ends up reading like a fantasy for a
                    narrow subculture, while being off-putting to most people.</li>
            </ul>
            <p>Yet despite all of the concerns above, I really do think it’s important to discuss what a good world with
                powerful AI could look like, while doing our best to avoid the above pitfalls. In fact I think it is
                critical to have a genuinely inspiring vision of the future, and not <i>just</i> a plan to fight fires.
                Many of the implications of powerful AI are adversarial or dangerous, but at the end of it all, there
                has to be something we’re fighting <i>for</i>, some positive-sum outcome where everyone is better off,
                something to rally people to rise above their squabbles and confront the challenges ahead. Fear is one
                kind of motivator, but it’s not enough: we need hope as well.</p>
            <p>The list of positive applications of powerful AI is extremely long (and includes robotics, manufacturing,
                energy, and much more), but I’m going to focus on a small number of areas that seem to me to have the
                greatest potential to directly improve the quality of human life. The five categories I am most excited
                about are:</p>
            <ol>
                <li>Biology and physical health</li>
                <li>Neuroscience and mental health</li>
                <li>Economic development and poverty</li>
                <li>Peace and governance</li>
                <li>Work and meaning</li>
            </ol>
            <p>My predictions are going to be radical as judged by most standards (other than sci-fi “singularity”
                visions<sup id="fnref:2"><a href="#fn:2">2</a></sup>), but I mean them earnestly
                and sincerely. Everything I’m saying could very easily be wrong (to repeat my point from above), but
                I’ve at least attempted to ground my views in a semi-analytical assessment of how much progress in
                various fields might speed up and what that might mean in practice. I am fortunate to have professional
                experience in <a href="https://scholar.google.com/citations?user=6-e-ZBEAAAAJ" target="_blank">both
                    biology and neuroscience</a>, and I am an informed amateur in the field of economic development, but
                I am sure I will get plenty of things wrong. One thing writing this essay has made me realize is that it
                would be valuable to bring together a group of domain experts (in biology, economics, international
                relations, and other areas) to write a much better and more informed version of what I’ve produced here.
                It’s probably best to view my efforts here as a starting prompt for that group.</p>
            <h2>Basic assumptions and framework</h2>
            <p>To make this whole essay more precise and grounded, it’s helpful to specify clearly what we mean by
                powerful AI (i.e. the threshold at which the 5-10 year clock starts counting), as well as laying out a
                framework for thinking about the effects of such AI once it’s present.</p>
            <p>What powerful AI (I dislike the term AGI)<sup id="fnref:3"><a href="#fn:3">3</a></sup> will look like, and when (or if) it will arrive, is a huge topic in
                itself. It’s one I’ve discussed publicly and could write a completely separate essay on (I probably will
                at some point). Obviously, many people are skeptical that powerful AI will be built soon and some are
                skeptical that it will ever be built at all. I think it could come as early as 2026, though there are
                also ways it could take much longer. But for the purposes of this essay, I’d like to put these issues
                aside, assume it will come reasonably soon, and focus on what happens in the 5-10 years after that. I
                also want to assume a definition of what such a system <i>will look like,</i> what its capabilities are
                and how it interacts, even though there is room for disagreement on this.</p>
            <p>By <i>powerful AI</i>, I have in mind an AI model—likely similar to today’s LLM’s in form, though it
                might be based on a different architecture, might involve several interacting models, and might be
                trained differently—with the following properties:</p>
            <ul>
                <li>In terms of pure intelligence<sup id="fnref:4"><a href="#fn:4">4</a></sup>, it
                    is smarter than a Nobel Prize winner across most relevant fields –
                    biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical
                    theorems, write extremely good novels, write difficult codebases from scratch, etc.</li>
                <li>In addition to just being a “smart thing you talk to”, it has all the “interfaces” available to a
                    human working virtually, including text, audio, video, mouse and keyboard control, and internet
                    access. It can engage in any actions, communications, or remote operations enabled by this
                    interface, including taking actions on the internet, taking or giving directions to humans, ordering
                    materials, directing experiments, watching videos, making videos, and so on. It does all of these
                    tasks with, again, a skill exceeding that of the most capable humans in the world.</li>
                <li>It does not just passively answer questions; instead, it can be given tasks that take hours, days,
                    or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart
                    employee would, asking for clarification as necessary.</li>
                <li>It does not have a physical embodiment (other than living on a computer screen), but it can control
                    existing physical tools, robots, or laboratory equipment through a computer; in theory it could even
                    design robots or equipment for itself to use.</li>
                <li>The resources used to train the model can be repurposed to <i>run</i> millions of instances of it
                    (this matches projected cluster sizes by ~2027), and the model can absorb information and generate
                    actions at roughly 10x-100x human speed<sup id="fnref:5"><a href="#fn:5">5</a></sup>. It may however be limited by the response time of the physical
                    world or of software it interacts with.</li>
                <li>Each of these million copies can act independently on unrelated tasks, or if needed can all work
                    together in the same way humans would collaborate, perhaps with different subpopulations fine-tuned
                    to be especially good at particular tasks.</li>
            </ul>
            <p>We could summarize this as a “country of geniuses in a datacenter”.</p>
            <p>Clearly such an entity would be capable of solving very difficult problems, very fast, but it is not
                trivial to figure out how fast. Two “extreme” positions both seem false to me. First, you might think
                that the world would be instantly transformed on the scale of seconds or days (“<a href="https://en.wikipedia.org/wiki/Technological_singularity#:~:text=The%20technological%20singularity%E2%80%94or%20simply,unforeseeable%20consequences%20for%20Human%20civilization." target="_blank">the Singularity</a>”), as superior intelligence builds on itself and solves every
                possible scientific, engineering, and operational task almost immediately. The problem with this is that
                there are real physical and practical limits, for example around building hardware or conducting
                biological experiments. Even a new country of geniuses would hit up against these limits. Intelligence
                may be very powerful, but it isn’t magic fairy dust.</p>
            <p>Second, and conversely, you might believe that technological progress is saturated or rate-limited by
                real world data or by social factors, and that better-than-human intelligence will add very little<sup id="fnref:6"><a href="#fn:6">6</a></sup>. This seems equally implausible to
                me—I can think of hundreds of scientific or even social problems where a large group of really smart
                people would drastically speed up progress, especially if they aren’t limited to analysis and can make
                things happen in the real world (which our postulated country of geniuses can, including by directing or
                assisting teams of humans).</p>
            <p>I think the truth is likely to be some messy admixture of these two extreme pictures, something that
                varies by task and field and is very subtle in its details. I believe we need new frameworks to think
                about these details in a productive way.</p>
            <p>Economists often talk about “factors of production”: things like labor, land, and capital. The phrase
                “marginal returns to labor/land/capital” captures the idea that in a given situation, a given factor may
                or may not be the limiting one – for example, an air force needs both planes and pilots, and hiring more
                pilots doesn’t help much if you’re out of planes. I believe that in the AI age, we should be talking
                about <i>the marginal returns to intelligence<sup id="fnref:7"><a href="#fn:7">7</a></sup></i>, and trying to figure out what the other factors are that are
                complementary to intelligence and that become limiting factors when intelligence is very high. We are
                not used to thinking in this way—to asking “how much does being smarter help with this task, and on what
                timescale?”—but it seems like the right way to conceptualize a world with very powerful AI.</p>
            <p>My guess at a list of factors that limit or are complementary to intelligence includes:</p>
            <ul>
                <li><strong>Speed of the outside world</strong>. Intelligent agents need to operate interactively in the
                    world in order to accomplish things and also to learn<sup id="fnref:8"><a href="#fn:8">8</a></sup>. But the world only moves so fast. Cells and animals run at a fixed
                    speed so experiments on them take a certain amount of time which may be irreducible. The same is
                    true of hardware, materials science, anything involving communicating with people, and even our
                    existing software infrastructure. Furthermore, in science many experiments are often needed in
                    sequence, each learning from or building on the last. All of this means that the speed at which a
                    major project—for example developing a cancer cure—can be completed may have an irreducible minimum
                    that cannot be decreased further even as intelligence continues to increase.</li>
                <li><strong>Need for data</strong>. Sometimes raw data is lacking and in its absence more intelligence
                    does not help. Today’s particle physicists are very ingenious and have developed a wide range of
                    theories, but lack the data to choose between them because particle accelerator data <a href="https://www.technologyreview.com/2024/02/20/1088002/higgs-boson-physics-particle-collider-large-hadron-collider/" target="_blank">is so limited</a>. It is not clear that they would do drastically better if they
                    were superintelligent—other than perhaps by speeding up the construction of a bigger accelerator.
                </li>
                <li><strong>Intrinsic complexity</strong>. Some things are inherently unpredictable or chaotic and even
                    the most powerful AI cannot predict or untangle them substantially better than a human or a computer
                    today. For example, even incredibly powerful AI could predict only marginally further ahead in a
                    chaotic system (such as the <a href="https://en.wikipedia.org/wiki/Three-body_problem" target="_blank">three-body problem</a>) in the general case,<sup id="fnref:9"><a href="#fn:9">9</a></sup> as compared to today’s humans and computers.
                </li>
                <li><strong>Constraints from humans</strong>. Many things cannot be done without breaking laws, harming
                    humans, or messing up society. An aligned AI would not want to do these things (and if we have an
                    unaligned AI, we’re back to talking about risks). Many human societal structures are inefficient or
                    even actively harmful, but are hard to change while respecting constraints like legal requirements
                    on clinical trials, people’s willingness to change their habits, or the behavior of governments.
                    Examples of advances that work well in a technical sense, but whose impact has been substantially
                    reduced by regulations or misplaced fears, include nuclear power, <a href="https://en.wikipedia.org/wiki/Concorde" target="_blank">supersonic flight</a>, and <a href="https://www.nytimes.com/2024/07/08/opinion/elevator-construction-regulation-labor-immigration.html" target="_blank">even elevators</a>.</li>
                <li><strong>Physical laws</strong>. This is a starker version of the first point. There are certain
                    physical laws that appear to be unbreakable. It’s not possible to travel faster than light. <a href="https://en.wikipedia.org/wiki/Arcadia_(play)" target="_blank">Pudding does not unstir</a>.
                    Chips can only have so many transistors per square centimeter <a href="https://en.wikipedia.org/wiki/Quantum_tunnelling" target="_blank">before they become
                        unreliable</a>. Computation requires a <a href="https://en.wikipedia.org/wiki/Landauer%27s_principle" target="_blank">certain minimum
                        energy per bit</a> erased, limiting the density of computation in the world.</li>
            </ul>
            <p>There is a further distinction based on <i>timescales</i>. Things that are hard constraints in the short
                run may become more malleable to intelligence in the long run. For example, intelligence might be used
                to develop a new experimental paradigm that allows us to learn <i>in vitro</i> what used to require live
                animal experiments, or to build the tools needed to collect new data (e.g. the bigger particle
                accelerator), or to (within ethical limits) find ways around human-based constraints (e.g. helping to
                improve the clinical trial system, helping to create new jurisdictions where clinical trials have less
                bureaucracy, or improving the science itself to make human clinical trials less necessary or cheaper).
            </p>
            <p>Thus, we should imagine a picture where intelligence is initially heavily bottlenecked by the other
                factors of production, but over time intelligence itself increasingly routes around the other factors,
                even if they never fully dissolve (and some things like physical laws are absolute)<sup id="fnref:10"><a href="#fn:10">10</a></sup>. The key question is how fast it all happens and
                in what order.</p>
            <p>With the above framework in mind, I’ll try to answer that question for the five areas mentioned in the
                introduction.</p>
            <h2>1. Biology and health</h2>
            <p>Biology is probably the area where scientific progress has the greatest potential to directly and
                unambiguously improve the quality of human life. In the last century some of the most ancient human
                afflictions (such as smallpox) have finally been vanquished, but many more still remain, and defeating
                them would be an enormous humanitarian accomplishment. Beyond even curing disease, biological science
                can in principle improve the <i>baseline</i> quality of human health, by extending the healthy human
                lifespan, increasing control and freedom over our own biological processes, and addressing everyday
                problems that we currently think of as immutable parts of the human condition.</p>
            <p>In the “limiting factors” language of the previous section, the main challenges with directly applying
                intelligence to biology are data, the speed of the physical world, and intrinsic complexity (in fact,
                all three are related to each other). Human constraints also play a role at a later stage, when clinical
                trials are involved. Let’s take these one by one.</p>
            <p>Experiments on cells, animals, and even chemical processes are limited by the speed of the physical
                world: many biological protocols involve culturing bacteria or other cells, or simply waiting for
                chemical reactions to occur, and this can sometimes take days or even weeks, with no obvious way to
                speed it up. Animal experiments can take months (or more) and human experiments often take years (or
                even decades for long-term outcome studies). Somewhat related to this, data is often lacking—not so much
                in quantity, but quality: there is always a dearth of clear, unambiguous data that isolates a biological
                effect of interest from the other 10,000 confounding things that are going on, or that intervenes
                causally in a given process, or that directly measures some effect (as opposed to inferring its
                consequences in some indirect or noisy way). Even massive, quantitative molecular data, like the
                proteomics data that I collected while working on mass spectrometry techniques, is noisy and misses a
                lot (which types of cells were these proteins in? Which part of the cell? At what phase in the cell
                cycle?).</p>
            <p>In part responsible for these problems with data is intrinsic complexity: if you’ve ever seen a <a href="https://today.ucsd.edu/story/international_consortium_builds_google_map_of_human_metabolism" target="_blank">diagram showing the biochemistry of human metabolism</a>, you’ll know that it’s very
                hard to isolate the effect of any part of this complex system, and even harder to intervene on the
                system in a precise or predictable way. And finally, beyond just the intrinsic time that it takes to run
                an experiment on humans, actual clinical trials involve a lot of bureaucracy and regulatory requirements
                that (in the opinion of many people, including me) add unnecessary additional time and delay progress.
            </p>
            <p>Given all this, many biologists have long been <a href="https://www.nature.com/articles/d41586-024-00306-2" target="_blank">skeptical</a> of the value
                of AI and “big data” more generally in biology. Historically, mathematicians, computer scientists, and
                physicists who have applied their skills to biology over the last 30 years have been quite successful,
                but have not had the truly transformative impact initially hoped for. Some of the skepticism has been
                reduced by major and revolutionary breakthroughs like <a href="https://alphafold.ebi.ac.uk/" target="_blank">AlphaFold</a> (which has just deservedly won its creators the <a href="https://www.nobelprize.org/prizes/chemistry/2024/summary/" target="_blank">Nobel Prize in
                    Chemistry</a>) and <a href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/" target="_blank">AlphaProteo</a><sup id="fnref:11"><a href="#fn:11">11</a></sup>, but there’s still a perception that AI is (and will continue to be)
                useful in only a limited set of circumstances. A common formulation is “AI can do a better job analyzing
                your data, but it can’t produce more data or improve the quality of the data. Garbage in, garbage out”.
            </p>
            <p>But I think that pessimistic perspective is thinking about AI in the wrong way. If our core hypothesis
                about AI progress is correct, then the right way to think of AI is not as a method of data analysis, but
                as a virtual biologist who performs <i>all</i> the tasks biologists do, including designing and running
                experiments in the real world (by controlling lab robots or simply telling humans which experiments to
                run – as a Principal Investigator would to their graduate students), inventing new biological methods or
                measurement
                techniques, and so on. It is by speeding up <i>the whole research process</i> that AI can truly
                accelerate biology. <strong>I want to repeat this because it’s the most common misconception that comes
                    up when I talk about AI’s ability to transform biology: I am <i>not</i> talking about AI as merely a
                    tool to analyze data. In line with the definition of powerful AI at the beginning of this essay, I’m
                    talking about using AI to perform, direct, and improve upon nearly everything biologists
                    do.</strong></p>
            <p>To get more specific on where I think acceleration is likely to come from, a surprisingly large fraction
                of the progress in biology has come from a truly tiny number of discoveries, often related to broad
                measurement tools or techniques<sup id="fnref:12"><a href="#fn:12">12</a></sup>
                that allow precise but generalized or programmable intervention in biological systems. There’s perhaps
                ~1 of these major discoveries per year and collectively they arguably drive &gt;50% of progress in biology.
                These discoveries are so powerful precisely because they cut through intrinsic complexity and data
                limitations, directly increasing our understanding and control over biological processes. A few
                discoveries per decade have enabled both the bulk of our basic scientific understanding of biology, and
                have driven many of the most powerful medical treatments.</p>
            <p>Some examples include:</p>
            <ul>
                <li><a href="https://en.wikipedia.org/wiki/CRISPR" target="_blank">CRISPR</a>: a technique that allows
                    live editing of any gene in living organisms (replacement of any arbitrary gene sequence with any
                    other arbitrary sequence). Since the original technique was developed, there have been <a href="https://www.nature.com/articles/s41581-022-00636-2" target="_blank">constant
                        improvements</a> to target specific cell types, increasing accuracy, and reducing edits of the
                    wrong gene—all of which are needed for safe use in humans.</li>
                <li>Various kinds of microscopy for watching what is going on at a precise level: advanced light
                    microscopes (with various kinds of fluorescent techniques, special optics, etc), electron
                    microscopes, atomic force microscopes, etc.</li>
                <li>Genome sequencing and synthesis, which has <a href="https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost" target="_blank">dropped in cost</a> by several orders of magnitude in the last couple decades.
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Optogenetics#:~:text=Optogenetics%20is%20a%20biological%20technique,specifically%20in%20the%20target%20cells." target="_blank">Optogenetic</a> techniques that allow you to get a neuron to fire by shining a
                    light on it.</li>
                <li><a href="https://pubmed.ncbi.nlm.nih.gov/34433919/" target="_blank">mRNA vaccines</a> that, in
                    principle, allow us to design a vaccine against anything and then quickly adapt it (mRNA vaccines of
                    course became famous during COVID).</li>
                <li>Cell therapies such as <a href="https://en.wikipedia.org/wiki/CAR_T_cell" target="_blank">CAR-T</a>
                    that allow immune cells to be taken out of the body and “reprogrammed” to attack, in principle,
                    anything.</li>
                <li>Conceptual insights like the germ theory of disease or the realization of a link between the immune
                    system and cancer<sup id="fnref:13"><a href="#fn:13">13</a></sup>.</li>
            </ul>
            <p>I’m going to the trouble of listing all these technologies because I want to make a crucial claim about
                them: <strong>I think their rate of discovery could be increased by 10x or more if there were a lot more
                    talented, creative researchers</strong><i>.</i> Or, put another way, <strong>I think the returns to
                    intelligence are high for these discoveries</strong>, and that everything else in biology and
                medicine mostly follows from them.</p>
            <p>Why do I think this? Because of the answers to some questions that we should get in the habit of asking
                when we’re trying to determine “returns to intelligence”. First, these discoveries are generally made by
                a tiny number of researchers, often the same people repeatedly, suggesting skill and not random search
                (the latter might suggest lengthy experiments are the limiting factor). Second, they often “could have
                been made” years earlier than they were: for example, CRISPR was a naturally occurring component of the
                immune system in bacteria that’s been <a href="https://link.springer.com/article/10.1134/S1062360422040075" target="_blank">known since the
                    80’s</a>, but it took another 25 years for people to realize it could be repurposed for general gene
                editing. They also are often delayed many years by lack of support from the scientific community for
                promising directions (see <a href="https://www.vox.com/future-perfect/2023/10/5/23903292/katalin-kariko-drew-weissman-nobel-prize-medicine-mrna-vaccines-covid-coronavirus" target="_blank">this profile</a> on the inventor of mRNA vaccines; similar stories abound). Third,
                successful projects are often scrappy or were afterthoughts that people didn’t initially think were
                promising, rather than massively funded efforts. This suggests that it’s not just massive resource
                concentration that drives discoveries, but ingenuity.</p>
            <p>Finally, although some of these discoveries have “serial dependence” (you need to make discovery A first
                in order to have the tools or knowledge to make discovery B)—which again might create experimental
                delays—many, perhaps most, are independent, meaning many at once can be worked on in parallel. Both
                these facts, and my general experience as a biologist, strongly suggest to me that there are hundreds of
                these discoveries waiting to be made if scientists were smarter and better at making connections between
                the vast amount of biological knowledge humanity possesses (again consider the CRISPR example). The
                success of <a href="https://alphafold.ebi.ac.uk/" target="_blank">AlphaFold</a>/<a href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/" target="_blank">AlphaProteo</a> at solving important problems much more effectively than humans,
                despite decades of carefully designed physics modeling, provides a proof of principle (albeit with a
                narrow tool in a narrow domain) that should point the way forward.</p>
            <p>Thus, it’s my guess that powerful AI could at least 10x the rate of these discoveries, giving us the next
                50-100 years of biological progress in 5-10 years.<sup id="fnref:14"><a href="#fn:14">14</a></sup> Why not 100x? Perhaps it is possible, but here both serial dependence
                and experiment times become important: getting 100 years of progress in 1 year requires a lot of things
                to go right the first time, including animal experiments and things like designing microscopes or
                expensive lab facilities. I’m actually open to the (perhaps absurd-sounding) idea that we could get
                <i>1000</i> years of progress in 5-10 years, but very skeptical that we can get 100 years in 1 year.
                Another way to put it is I think there’s an unavoidable constant delay: experiments and hardware design
                have a certain “latency” and need to be iterated upon a certain “irreducible” number of times in order
                to learn things that can’t be deduced logically. But massive parallelism may be possible on top of
                that<sup id="fnref:15"><a href="#fn:15">15</a></sup>.
            </p>
            <p>What about clinical trials? Although there is a lot of bureaucracy and slowdown associated with them, the
                truth is that a lot (though by no means all!) of their slowness ultimately derives from the need to
                rigorously evaluate drugs that barely work or ambiguously work. This is sadly true of most therapies
                today: the average cancer drug increases survival by a few months while having significant side effects
                that need to be carefully measured (there’s a similar story for Alzheimer’s drugs). This leads to huge
                studies (in order to achieve statistical power) and difficult tradeoffs which regulatory agencies
                generally aren’t great at making, again because of bureaucracy and the complexity of competing
                interests.</p>
            <p>When something works really well, it goes much faster: there’s an accelerated approval track and the ease
                of approval is much greater when effect sizes are larger. mRNA vaccines for COVID were approved in 9
                months—much faster than the usual pace. That said, even under these conditions clinical trials are still
                too slow—mRNA vaccines arguably <a href="https://www.1daysooner.org/" target="_blank"><i>should</i> have
                    been approved in ~2 months</a>. But these kinds of delays (~1 year end-to-end for a drug) combined
                with massive parallelization and the need for some but not too much iteration (“a few tries”) are very
                compatible with radical transformation in 5-10 years. Even more optimistically, it is possible that <a href="https://www.sciencedirect.com/science/article/pii/S135964462400134X" target="_blank">AI-enabled biological science</a> will reduce the need for iteration in clinical
                trials by developing better animal and cell experimental models (or even simulations) that are more
                accurate in predicting what will happen in humans. This will be particularly important in developing
                drugs against the aging process, which plays out over decades and where we need a faster iteration loop.
            </p>
            <p>Finally, on the topic of clinical trials and societal barriers, it is worth pointing out explicitly that
                in some ways biomedical innovations have an unusually <i>strong</i> track record of being successfully
                deployed, in contrast to some other technologies<sup id="fnref:16"><a href="#fn:16">16</a></sup>. As mentioned in the introduction, many technologies are hampered by
                societal factors despite working well technically. This might suggest a pessimistic perspective on what
                AI can accomplish<i>.</i> But biomedicine is unique in that although the process of developing drugs is
                overly cumbersome, once developed they generally are successfully deployed and used.</p>
            <strong>To summarize the above, my basic prediction is that AI-enabled biology and medicine will allow us to
                compress the progress that human biologists would have achieved over the next 50-100 years into 5-10
                years. I’ll refer to this as the “compressed 21st century”: the idea that after powerful AI is
                developed, we will in a few years make all the progress in biology and medicine that we would have made
                in the whole 21st century.</strong>
            <p>Although predicting what powerful AI can do in a few years remains inherently difficult and
                speculative,
                there is some concreteness to asking “what could humans do unaided in the next 100 years?”. Simply
                looking at what we’ve accomplished in the 20th century, or extrapolating from the first 2 decades of
                the
                21st, or asking what “10 CRISPR’s and 50 CAR-T’s” would get us, all offer practical, grounded ways
                to
                estimate the general level of progress we might expect from powerful AI.</p>
            <p>Below I try to make a list of what we might expect. This is not based on any rigorous methodology,
                and
                will almost certainly prove wrong in the details, but it’s trying to get across the general
                <i>level</i>
                of radicalism we should expect:
            </p>
            <ul>
                <li><strong>Reliable prevention and treatment of nearly all</strong><sup id="fnref:17"><a href="#fn:17">17</a></sup> <strong>natural infectious
                        disease.</strong>
                    Given the enormous advances against infectious disease in the 20th century, it is not radical to
                    imagine that we could more or less “finish the job” in a compressed 21st. mRNA vaccines and
                    similar
                    technology already point the way towards “<a href="https://www.nih.gov/news-events/news-releases/clinical-trial-mrna-universal-influenza-vaccine-candidate-begins" target="_blank">vaccines for anything</a>”. Whether infectious disease is <i>fully
                        eradicated
                        from the world</i> (as opposed to just in some places) depends on questions about poverty
                    and
                    inequality, which are discussed in Section 3.</li>
                <li><strong>Elimination of most cancer</strong>. Death rates from cancer <a href="https://www.bmj.com/content/384/bmj-2023-076962" target="_blank">have been dropping
                        ~2%
                        per year</a> for the last few decades; thus we are on track to eliminate most cancer in the
                    21st
                    century at the current pace of human science. Some subtypes have already been largely cured (for
                    example some types of leukemia with <a href="https://www.statnews.com/2022/02/02/cart-cancer-therapy-leukemia-treatment/" target="_blank">CAR-T therapy</a>), and I’m perhaps even more excited for very selective
                    drugs
                    that target cancer in its infancy and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10080017/" target="_blank">prevent it</a>
                    from ever growing. AI will also make possible treatment regimens very <a href="https://www.sciencedirect.com/science/article/pii/S0305737220300578" target="_blank">finely adapted</a> to the individualized genome of the cancer—these are
                    possible
                    today, but hugely expensive in time and human expertise, which AI should allow us to scale.
                    Reductions of 95% or more in both mortality and incidence seem possible. That said, cancer is
                    extremely varied and adaptive, and is likely the hardest of these diseases to fully destroy. It
                    would not be surprising if an assortment of rare, difficult malignancies persists.</li>
                <li><strong>Very effective prevention and effective cures for genetic disease</strong>. Greatly
                    improved
                    <a href="https://www.nature.com/articles/s41591-022-01735-0" target="_blank">embryo
                        screening</a>
                    will likely make it possible to prevent most genetic disease, and some safer, more reliable
                    descendant of CRISPR may cure most genetic disease in existing people. Whole-body afflictions
                    that
                    affect a large fraction of cells may be the last holdouts, however.
                </li>
                <li><strong>Prevention of Alzheimer’s</strong>. We’ve had a very hard time figuring out what causes
                    Alzheimer’s (it is somehow related to beta-amyloid protein, but the actual details seem to be <a href="https://www.nature.com/articles/s41380-021-01249-0" target="_blank">very complex</a>).
                    It
                    seems like exactly the type of problem that can be solved with better measurement tools that
                    isolate
                    biological effects; thus I am bullish about AI’s ability to solve it. There is a good chance it
                    can
                    eventually be prevented with relatively simple interventions, once we actually understand what
                    is
                    going on. That said, damage from already-existing Alzheimer’s may be very difficult to reverse.
                </li>
                <li><strong>Improved treatment of most other ailments</strong>. This is a catch-all category for
                    other
                    ailments including diabetes, obesity, heart disease, autoimmune diseases, and more. Most of
                    these
                    seem “easier” to solve than cancer and Alzheimer’s and in many cases are already in steep
                    decline.
                    For example, deaths from heart disease have already declined over 50%, and simple interventions
                    like
                    <a href="https://www.google.com/url?q=https://onlinelibrary.wiley.com/doi/full/10.1002/edm2.462&amp;sa=D&amp;source=docs&amp;ust=1726506125482285&amp;usg=AOvVaw1t12gKr6YA4RNeWMnhLtU6" target="_blank">GLP-1 agonists</a> have already made huge progress against obesity and
                    diabetes.
                </li>
                <li><strong>Biological freedom</strong>. The last 70 years featured advances in birth control,
                    fertility, <a href="https://www.astralcodexten.com/p/why-does-ozempic-cure-all-diseases" target="_blank">management of weight</a>, and much more. But I suspect AI-accelerated
                    biology
                    will greatly expand what is possible: weight, physical appearance, reproduction, and other
                    biological processes will be fully under people’s control. We’ll refer to these under the
                    heading of
                    <i>biological freedom:</i> the idea that everyone should be empowered to choose what they want
                    to
                    become and live their lives in the way that most appeals to them. There will of course be
                    important
                    questions about global equality of access; see Section 3 for these.
                </li>
                <li><strong>Doubling of the human lifespan<sup id="fnref:18"><a href="#fn:18">18</a></sup>.</strong>This might seem radical,
                    but <a href="https://ourworldindata.org/life-expectancy" target="_blank">life expectancy increased
                        almost 2x</a> in the 20th century (from ~40 years to ~75), so it’s “on trend” that the
                    “compressed 21st” would double it again to 150. Obviously the interventions involved in slowing
                    the
                    actual aging process will be different from those that were needed in the last century to
                    prevent
                    (mostly childhood) premature deaths from disease, but the magnitude of change is not
                    unprecedented<sup id="fnref:19"><a href="#fn:18">19</a></sup>. Concretely,
                    there already <a href="https://www.nature.com/articles/s41586-024-07701-9" target="_blank">exist
                        drugs that increase maximum lifespan in rats by 25-50%</a> with limited ill-effects. And
                    some
                    animals (e.g. some types of turtle) already live 200 years, so humans are manifestly not at some
                    theoretical upper limit. At a guess, the most important thing that is needed might be reliable,
                    <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" target="_blank">non-Goodhart-able</a>
                    biomarkers of human aging, as that will allow fast iteration on experiments and clinical trials.
                    Once human lifespan is 150, we may be able to reach “escape velocity”, buying enough time that
                    most
                    of those currently alive today will be able to live as long as they want, although there’s
                    certainly
                    no guarantee this is biologically possible.
                </li>
            </ul>
            <p>It is worth looking at this list and reflecting on how different the world will be if all of it is
                achieved 7-12 years from now (which would be in line with an aggressive AI timeline). It goes
                without
                saying that it would be an unimaginable humanitarian triumph, the elimination all at once of most of
                the
                scourges that have haunted humanity for millennia. Many of my friends and colleagues are raising
                children, and when those children grow up, I hope that any mention of disease will sound to them the
                way
                scurvy, <a href="https://ourworldindata.org/smallpox" target="_blank">smallpox</a>, or bubonic
                plague
                sounds to us. That generation will also benefit from increased biological freedom and
                self-expression,
                and with luck may also be able to live as long as they want.</p>
            <p>It’s hard to overestimate how surprising these changes will be to everyone except the small community
                of
                people who expected powerful AI. For example, thousands of economists and policy experts in the US
                currently debate <a href="https://en.wikipedia.org/wiki/Social_Security_debate_in_the_United_States" target="_blank">how to keep Social Security</a> and Medicare solvent, and more broadly how to
                keep
                down the cost of healthcare (which is mostly consumed by those over 70 and especially those with
                terminal illnesses such as cancer). The situation for these programs is likely to be radically
                improved
                if all this comes to pass<sup id="fnref:20"><a href="#fn:20">20</a></sup>, as
                the
                ratio of working age to retired population will change drastically. No doubt these challenges will
                be
                replaced with others, such as how to ensure widespread access to the new technologies, but it is
                worth
                reflecting on how much the world will change even if biology is the <i>only</i> area to be
                successfully
                accelerated by AI.</p>
            <h2>2. Neuroscience and mind</h2>
            <p>In the previous section I focused on <i>physical</i> diseases and biology in general, and didn’t
                cover
                neuroscience or mental health. But neuroscience is a subdiscipline of biology and mental health is
                just
                as important as physical health. In fact, if anything, mental health affects human well-being even
                more
                directly than physical health. Hundreds of millions of people have very low quality of life due to
                problems like addiction, depression, schizophrenia, low-functioning autism, PTSD, psychopathy<sup id="fnref:21"><a href="#fn:21">21</a></sup>, or intellectual disabilities.
                Billions more struggle with everyday problems that can often be interpreted as much milder versions
                of
                one of these severe clinical disorders. And as with general biology, it may be possible to go beyond
                addressing problems to improving the baseline quality of human experience.</p>
            <p>The basic framework that I laid out for biology applies equally to neuroscience. The field is
                propelled
                forward by a small number of discoveries often related to tools for measurement or precise
                intervention
                – in the list of those above, optogenetics was a neuroscience discovery, and more recently <a href="https://en.wikipedia.org/wiki/CLARITY" target="_blank">CLARITY</a> and <a href="https://en.wikipedia.org/wiki/Expansion_microscopy#:~:text=Expansion%20microscopy%20(ExM)%20is%20a,them%20using%20a%20polymer%20system." target="_blank">expansion microscopy</a> are
                advances
                in the same vein, in addition to many of the general cell biology methods directly carrying over to
                neuroscience. I think the rate of these advances will be similarly accelerated by AI and therefore
                that
                the framework of “100 years of progress in 5-10 years” applies to neuroscience in the same way it
                does
                to biology and for the same reasons. As in biology, the progress in 20th century neuroscience was
                enormous – for example we didn’t even understand how or why neurons fired <a href="https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model" target="_blank">until the
                    1950’s</a>. Thus, it seems reasonable to expect AI-accelerated neuroscience to produce rapid
                progress over a few years.</p>
            <p>There is one thing we should add to this basic picture, which is that some of the things we’ve
                learned
                (or are learning) about AI itself in the last few years are likely to help advance neuroscience,
                even if
                it continues to be done only by humans. <a href="https://www.anthropic.com/research/mapping-mind-language-model" target="_blank">Interpretability</a> is an obvious example: although biological neurons
                superficially operate in a completely different manner from artificial neurons (they communicate via
                spikes and often spike rates, so there is a time element not present in artificial neurons, and a
                bunch
                of details relating to cell physiology and neurotransmitters modifies their operation
                substantially),
                the basic question of “how do distributed, trained networks of simple units that perform combined
                linear/non-linear operations work together to perform important computations” is the same, and I
                strongly suspect the details of individual neuron communication will be abstracted away in most of
                the
                interesting questions about computation and circuits<sup id="fnref:22"><a href="#fn:22">22</a></sup>. As just one example of this, a <a href="https://distill.pub/2020/circuits/frequency-edges/" target="_blank">computational
                    mechanism</a> discovered by interpretability researchers in AI systems was recently <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10055119/" target="_blank">rediscovered</a>
                in
                the brains of mice.</p>
            <p>It is much easier to do experiments on artificial neural networks than on real ones (the latter often
                requires cutting into animal brains), so interpretability may well become a tool for improving our
                understanding of neuroscience. Furthermore, powerful AI’s will themselves probably be able to
                develop
                and apply this tool better than humans can.</p>
            <p>Beyond just interpretability though, what we have learned from AI about how intelligent systems are
                <i>trained</i> should (though I am not sure it <i>has</i> yet) cause a revolution in neuroscience.
                When
                I was working in neuroscience, a lot of people focused on what I would now consider the wrong
                questions
                about learning, because the concept of the <a href="https://arxiv.org/abs/2001.08361" target="_blank">scaling hypothesis</a> / <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf" target="_blank" rel="noopener noreferrer">bitter lesson</a> didn’t
                exist yet. The idea that a simple objective function plus a lot of data can
                drive
                incredibly complex behaviors makes it more interesting to understand the objective functions and
                architectural biases and less interesting to understand the details of the emergent computations. I
                have
                not followed the field closely in recent years, but I have a vague sense that computational
                neuroscientists have still not fully absorbed the lesson. My attitude to the scaling hypothesis has
                always been “aha – this is an explanation, at a high level, of how intelligence works and how it so
                easily evolved”, but I don’t think that’s the average neuroscientist’s view, in part because the
                scaling
                hypothesis as “the secret to intelligence” isn’t fully accepted even within AI.
            </p>
            <p>I think that neuroscientists should be trying to combine this basic insight with the particularities
                of
                the human brain (biophysical limitations, evolutionary history, topology, details of motor and
                sensory
                inputs/outputs) to try to figure out some of neuroscience’s key puzzles. Some likely are, but I
                suspect
                it’s not enough yet, and that AI neuroscientists will be able to more effectively leverage this
                angle to
                accelerate progress.</p>
            <p>I expect AI to accelerate neuroscientific progress along four distinct routes, all of which can
                hopefully
                work together to cure mental illness and improve function:</p>
            <ul>
                <li><strong>Traditional molecular biology, chemistry, and genetics</strong>. This is essentially the
                    same story as general biology in section 1, and AI can likely speed it up via the same
                    mechanisms.
                    There are many drugs that modulate neurotransmitters in order to alter brain function, affect
                    alertness or perception, change mood, etc., and AI can <a href="https://www.science.org/doi/10.1126/sciadv.adn1524" target="_blank">help us invent</a>
                    many more. AI can probably also accelerate research on the genetic basis of mental illness.</li>
                <li><strong>Fine-grained neural measurement and intervention</strong>. This is the ability to
                    measure
                    what a lot of individual neurons or neuronal circuits are doing, and intervene to change their
                    behavior. Optogenetics and neural probes are technologies capable of both measurement and
                    intervention in live organisms, and a number of very advanced methods (such as molecular ticker
                    tapes to read out the firing patterns of large numbers of individual neurons) <a href="https://arxiv.org/abs/1306.5709" target="_blank">have also been proposed</a> and seem
                    possible in principle.</li>
                <li><strong>Advanced computational neuroscience</strong>. As noted above, both the specific insights
                    and
                    the <i>gestalt</i> of modern AI can probably be applied fruitfully to questions in <a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(23)00099-2" target="_blank">systems neuroscience</a>, including perhaps uncovering the real
                    causes
                    and dynamics of complex diseases like psychosis or mood disorders.</li>
                <li><strong>Behavioral interventions</strong>. I haven’t much mentioned it given the focus on the
                    biological side of neuroscience, but psychiatry and psychology have of course developed <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/wps.21203" target="_blank">a wide
                        repertoire of behavioral interventions</a> over the 20th century; it stands to reason that
                    AI
                    could accelerate these as well, both the development of new methods and helping patients to
                    adhere
                    to existing methods. More broadly, the idea of an “AI coach” who always helps you to be the best
                    version of yourself, who studies your interactions and helps you learn to be more effective,
                    seems
                    very promising.</li>
            </ul>
            <p>It’s my guess that these four routes of progress working together would, as with physical disease, be
                on
                track to lead to the cure or prevention of most mental illness in the next 100 years even if AI was
                not
                involved – and thus might reasonably be completed in 5-10 AI-accelerated years. Concretely my guess
                at
                what will happen is something like:</p>
            <ul>
                <li><strong>Most mental illness can probably be cured</strong>. I’m not an expert in psychiatric
                    disease
                    (my time in neuroscience was spent building probes to study small groups of neurons) but it’s my
                    guess that diseases like PTSD, depression, schizophrenia, addiction, etc. can be figured out and
                    very effectively treated via some combination of the four directions above. The answer is likely
                    to
                    be some combination of “something went wrong biochemically” (although it could be very complex)
                    and
                    “something went wrong with the neural network, at a high level”. That is, it’s a systems
                    neuroscience question—though that doesn’t gainsay the impact of the behavioral interventions
                    discussed above. Tools for measurement and intervention, especially in live humans, seem likely
                    to
                    lead to rapid iteration and progress.</li>
                <li><strong>Conditions that are very “structural” may be more difficult, but not
                        impossible</strong>.
                    There’s <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7016047/" target="_blank">some
                        evidence</a> that psychopathy is associated with obvious neuroanatomical differences – that
                    some
                    brain regions are simply smaller or less developed in psychopaths. Psychopaths are also believed
                    to
                    lack empathy from a young age; whatever is different about their brain, it was probably always
                    that
                    way. The same may be true of some intellectual disabilities, and perhaps other conditions.
                    Restructuring the brain sounds hard, but it also seems like a task with high returns to
                    intelligence. Perhaps there is some way to coax the adult brain into an earlier or more plastic
                    state where it can be reshaped. I’m very uncertain how possible this is, but my instinct is to
                    be
                    optimistic about what AI can invent here.</li>
                <li><strong>Effective genetic prevention of mental illness seems possible</strong>. Most mental
                    illness
                    is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9840515/" target="_blank">partially
                        heritable</a>, and genome-wide association studies are <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/wps.21034" target="_blank">starting
                        to
                        gain traction</a> on identifying the relevant factors, which are often many in number. It
                    will
                    probably be possible to prevent most of these diseases via embryo screening, similar to the
                    story
                    with physical disease. One difference is that psychiatric disease is more likely to be polygenic
                    (many genes contribute), so due to complexity there’s an increased risk of unknowingly selecting
                    against <a href="https://en.wikipedia.org/wiki/Antagonistic_pleiotropy_hypothesis" target="_blank">positive traits that are correlated with disease</a>. Oddly however, in
                    recent
                    years GWAS studies seem to suggest that these <a href="https://academic.oup.com/humupd/advance-article/doi/10.1093/humupd/dmae012/7684172" target="_blank">correlations might have been overstated.</a> In any case, AI-accelerated
                    neuroscience may help us to figure these things out. Of course, embryo screening for complex
                    traits
                    raises a number of societal issues and will be controversial, though I would guess that most
                    people
                    would support screening for severe or debilitating mental illness.</li>
                <li><strong>Everyday problems that we don’t think of as clinical disease will also be
                        solved</strong>.
                    Most of us have everyday psychological problems that are not ordinarily thought of as rising to
                    the
                    level of clinical disease. Some people are quick to anger, others have trouble focusing or are
                    often
                    drowsy, some are fearful or anxious, or react badly to change. Today, drugs already exist to
                    help
                    with e.g. alertness or focus (caffeine, modafinil, ritalin) but as with many other previous
                    areas,
                    much more is likely to be possible. Probably many more such drugs exist and have not been
                    discovered, and there may also be totally new modalities of intervention, such as targeted light
                    stimulation (see optogenetics above) or magnetic fields. Given how many drugs we’ve developed in
                    the
                    20th century that tune cognitive function and emotional state, I’m very optimistic about the
                    “compressed 21st” where everyone can get their brain to behave a bit better and have a more
                    fulfilling day-to-day experience.</li>
                <li><strong>Human baseline experience can be much better</strong>. Taking one step further, many
                    people
                    have experienced extraordinary moments of revelation, creative inspiration, compassion,
                    fulfillment,
                    transcendence, love, beauty, or meditative peace. The character and frequency of these
                    experiences
                    differs greatly from person to person and within the same person at different times, and can
                    also
                    sometimes be triggered by various drugs (though often with side effects). All of this suggests
                    that
                    the “space of what is possible to experience” is very broad and that a larger fraction of
                    people’s
                    lives could consist of these extraordinary moments. It is probably also possible to improve
                    various
                    cognitive functions across the board. This is perhaps the neuroscience version of “biological
                    freedom” or “extended lifespans”.</li>
            </ul>
            <p>One topic that often comes up in sci-fi depictions of AI, but that I intentionally haven’t discussed
                here, is “mind uploading”, the idea of capturing the pattern and dynamics of a human brain and
                instantiating them in software. This topic could be the subject of an essay all by itself, but
                suffice
                it to say that while I think uploading is almost certainly <a href="https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf" target="_blank">possible</a>
                in
                principle, in practice it faces significant technological and societal challenges, even with
                powerful
                AI, that likely put it outside the 5-10 year window we are discussing.</p>
            <p>In summary, AI-accelerated neuroscience is likely to vastly improve treatments for, or even cure,
                most
                mental illness as well as greatly expand “cognitive and mental freedom” and human cognitive and
                emotional abilities. It will be every bit as radical as the improvements in physical health
                described in
                the previous section. Perhaps the world will not be visibly different on the outside, but the world
                as
                experienced by humans will be a much better and more humane place, as well as a place that offers
                greater opportunities for self-actualization. I also suspect that improved mental health will
                ameliorate
                a lot of other societal problems, including ones that seem political or economic.</p>
            <h2>3. Economic development and poverty</h2>
            <p>The previous two sections are about <i>developing</i> new technologies that cure disease and improve
                the
                quality of human life. However an obvious question, from a humanitarian perspective, is: “will
                everyone
                have access to these technologies?”</p>
            <p>It is one thing to develop a cure for a disease, it is another thing to eradicate the disease from
                the
                world. More broadly, many existing health interventions have not yet been applied everywhere in the
                world, and for that matter the same is true of (non-health) technological improvements in general.
                Another way to say this is that living standards in many parts of the world are still desperately
                poor:
                <a href="https://data.worldbank.org/indicator/NY.GDP.PCAP.CD" target="_blank">GDP per capita</a> is
                ~$2,000 in Sub-Saharan Africa as compared to ~$75,000 in the United States. If AI further increases
                economic growth and quality of life in the developed world, while doing little to help the
                developing
                world, we should view that as a terrible moral failure and a blemish on the genuine humanitarian
                victories in the previous two sections. Ideally, powerful AI should help the developing world
                <i>catch
                    up to</i> the developed world, even as it revolutionizes the latter.
            </p>
            <p>I am not as confident that AI can address inequality and economic growth as I am that it can invent
                fundamental technologies, because technology has such obvious high returns to intelligence
                (including
                the ability to route around complexities and lack of data) whereas the economy involves a lot of
                constraints from humans, as well as a large dose of intrinsic complexity. I am somewhat skeptical
                that
                an AI could solve the famous “<a href="https://en.wikipedia.org/wiki/Socialist_calculation_debate#:~:text=The%20socialist%20calculation%20debate%2C%20sometimes,of%20the%20means%20of%20production." target="_blank">socialist calculation problem</a>”<sup id="fnref:23"><a href="#fn:23">23</a></sup> and I don’t think governments will (or should) turn over their
                economic policy to such an entity, even if it could do so. There are also problems like how to
                convince
                people to take treatments that are effective but that they may be suspicious of.</p>
            <p>The challenges facing the developing world are made even more complicated by <a href="https://economics.mit.edu/sites/default/files/publications/120224%20Corruption%20Review%20Final.pdf" target="_blank">pervasive corruption</a> in both
                private and public sectors. Corruption creates a vicious cycle: it <a href="https://pdf.usaid.gov/pdf_docs/pnacw645.pdf" target="_blank">exacerbates poverty</a>, and
                poverty in
                turn breeds more corruption. AI-driven plans for economic development need to reckon with corruption,
                weak institutions, and other very human challenges. </p>
            <p>Nevertheless, I do see significant reasons for optimism. Diseases <i>have</i> been eradicated and many
                countries
                <i>have</i> gone from poor to rich, and it is clear that the decisions involved in these tasks exhibit
                high
                returns to intelligence (despite human constraints and complexity). Therefore, AI can likely do them
                better than they are currently being done. There may also be targeted interventions that get around the
                human constraints and that AI could focus on. More importantly though, <i>we have</i> to try. Both AI
                companies
                and developed world policymakers will need to do their part to ensure that the developing world is not
                left out; the moral imperative is too great. So in this section, I’ll continue to make the optimistic
                case, but keep in mind everywhere that success is not guaranteed and depends on our collective efforts.
            </p>
            <p>Below I make some guesses about how I think things may go in the developing world over the 5-10 years
                after powerful AI is developed:</p>
            <ul>
                <li><strong>Distribution of health interventions</strong>. The area where I am perhaps most
                    optimistic
                    is distributing health interventions throughout the world. Diseases have actually been
                    eradicated by
                    top-down campaigns: smallpox was <a href="https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-smallpox-vaccination" target="_blank">fully eliminated</a> in the 1970’s, and polio and guinea worm are nearly
                    eradicated with less than 100 cases per year. <a href="https://en.wikipedia.org/wiki/Institute_for_Disease_Modeling" target="_blank">Mathematically sophisticated epidemiological modeling</a> plays an active
                    role
                    in disease eradication campaigns, and it seems very likely that there is room for
                    smarter-than-human
                    AI systems to do a better job of it than humans are. The logistics of distribution can probably
                    also
                    be greatly optimized. One thing I learned as an early donor to <a href="https://www.givewell.org/" target="_blank">GiveWell</a> is that some health charities
                    are way more effective than others;
                    the hope is that AI-accelerated efforts would be more effective still. Additionally, some
                    biological
                    advances actually make the logistics of distribution much easier: for example, malaria has been
                    difficult to eradicate because it requires treatment each time the disease is contracted; a
                    vaccine
                    that only needs to be administered once makes the logistics much simpler (and such vaccines for
                    malaria <a href="https://en.wikipedia.org/wiki/Malaria_vaccine" target="_blank">are in fact
                        currently being developed</a>). Even simpler distribution mechanisms are possible: some
                    diseases
                    could in principle be eradicated by targeting their animal carriers, for example releasing
                    mosquitoes infected with a bacterium that <a href="https://www.gavi.org/vaccineswork/south-american-cities-release-mosquitoes-stem-disease" target="_blank">blocks their ability</a> to carry a disease (who then infect all the other
                    mosquitos) or simply using <a href="https://www.nature.com/articles/s41576-021-00386-0" target="_blank">gene drives</a> to wipe out the mosquitos. This requires one or a few
                    centralized actions, rather than a coordinated campaign that must individually treat millions.
                    Overall, I think 5-10 years is a reasonable timeline for a good fraction (maybe 50%) of
                    AI-driven
                    health benefits to propagate to even the poorest countries in the world. A good goal might be
                    for
                    the developing world 5-10 years after powerful AI to at least be substantially healthier than
                    the
                    developed world is today, even if it continues to lag behind the developed world. Accomplishing
                    this
                    will of course require a huge effort in global health, philanthropy, political advocacy, and
                    many
                    other efforts, which both AI developers and policymakers should help with.</li>
                <li><strong>Economic growth</strong>. Can the developing world quickly catch up to the developed
                    world,
                    not just in health, but across the board economically? There is some precedent for this: in the
                    final decades of the 20th century, <a href="https://en.wikipedia.org/wiki/Four_Asian_Tigers" target="_blank">several East Asian economies</a> achieved sustained ~10% annual real GDP
                    growth
                    rates, allowing them to catch up with the developed world. Human economic planners made the
                    decisions that led to this success, not by directly controlling entire economies but by pulling
                    a
                    few key levers (such as an industrial policy of export-led growth, and resisting the temptation
                    to
                    rely on natural resource wealth); it’s plausible that “AI finance ministers and central bankers”
                    could replicate or exceed this 10% accomplishment. An important question is how to get
                    developing
                    world governments to adopt them while respecting the principle of self-determination—some may be
                    enthusiastic about it, but others are likely to be skeptical. On the optimistic side, many of
                    the
                    health interventions in the previous bullet point are likely to organically increase economic
                    growth: eradicating AIDS/malaria/parasitic worms would have a transformative effect on
                    productivity,
                    not to mention the economic benefits that some of the neuroscience interventions (such as
                    improved
                    mood and focus) would have in developed and developing world alike. Finally, non-health
                    AI-accelerated technology (such as energy technology, transport drones, improved building
                    materials,
                    better logistics and distribution, and so on) may simply permeate the world naturally; for
                    example,
                    even cell phones quickly permeated sub-Saharan Africa via market mechanisms, without needing
                    philanthropic efforts. On the more negative side, while AI and automation have many potential
                    benefits, they also pose challenges for economic development, particularly for countries that
                    haven&#39;t yet industrialized. Finding ways to ensure these countries can still develop and improve
                    their economies in an age of increasing automation is an important challenge for economists and
                    policymakers to address. Overall, a dream scenario—perhaps a goal to aim for—would be 20% annual
                    GDP
                    growth rate in the developing world, with 10% each coming from AI-enabled economic decisions and
                    the
                    natural spread of AI-accelerated technologies, including but not limited to health. If achieved,
                    this would bring sub-Saharan Africa to the current per-capita GDP of China in 5-10 years, while
                    raising much of the rest of the developing world to levels higher than the current US GDP.
                    Again,
                    this is a dream scenario, not what happens by default: it’s something all of us must work
                    together
                    to make more likely.</li>
                <li><strong>Food security <sup id="fnref:24"><a href="#fn:24">24</a></sup></strong>. Advances in crop technology like better
                    fertilizers and
                    pesticides, more automation, and more efficient land use drastically increased <a href="https://ourworldindata.org/crop-yields" target="_blank">crop yields</a> across the
                    20th
                    Century, saving millions of people from hunger. Genetic engineering is <a href="https://www.science.org/content/article/new-genetic-tricks-boosting-crop-yield-take-clues-ancient-farmers" target="_blank">currently improving</a> many crops even further. Finding even more ways to
                    do
                    this—as well as to make agricultural supply chains even more efficient—could give us an
                    AI-driven
                    second <a href="https://en.wikipedia.org/wiki/Green_Revolution" target="_blank">Green
                        Revolution</a>, helping close the gap between the developing and developed world.</li>
                <li><strong>Mitigating climate change</strong>. Climate change will be felt much more strongly in
                    the
                    developing world, hampering its development. We can expect that AI will lead to improvements in
                    technologies that slow or prevent climate change, from atmospheric <a href="https://www.nature.com/articles/s41558-023-01604-9" target="_blank">carbon-removal</a>
                    and
                    clean energy technology to <a href="https://www.annualreviews.org/content/journals/10.1146/annurev-animal-021022-055132" target="_blank">lab-grown meat</a> that reduces our reliance on carbon-intensive factory
                    farming. Of course, as discussed above, technology isn’t the only thing restricting progress on
                    climate change—as with all of the other issues discussed in this essay, human societal factors
                    are
                    important. But there’s good reason to think that AI-enhanced research will give us the means to
                    make
                    mitigating climate change far less costly and disruptive, rendering many of the objections moot
                    and
                    freeing up developing countries to make more economic progress.</li>
                <li><strong>Inequality within countries</strong>. I’ve mostly talked about inequality as a global
                    phenomenon (which I do think is its most important manifestation), but of course inequality also
                    exists <i>within</i> countries. With advanced health interventions and especially radical
                    increases
                    in lifespan or cognitive enhancement drugs, there will certainly be valid worries that these
                    technologies are “only for the rich”. I am more optimistic about within-country inequality
                    especially in the developed world, for two reasons. First, markets function better in the
                    developed
                    world, and markets are typically good at bringing down the value of high-cost technologies over
                    time<sup id="fnref:25"><a href="#fn:25">25</a></sup>. Second, developed
                    world
                    political institutions are more responsive to their citizens and have greater state capacity to
                    execute universal access programs—and I expect citizens to demand access to technologies that so
                    radically improve quality of life. Of course it’s not predetermined that such demands
                    succeed—and
                    here is another place where we collectively have to do all we can to ensure a fair society.
                    There is
                    a separate problem in inequality of <i>wealth</i> (as opposed to inequality of access to
                    life-saving
                    and life-enhancing technologies), which seems harder and which I discuss in Section 5.</li>
                <li><strong>The opt-out problem</strong>. One concern in both developed and developing world alike
                    is
                    people <i>opting out</i> of AI-enabled benefits (similar to the anti-vaccine movement, or
                    Luddite
                    movements more generally). There could end up being bad feedback cycles where, for example, the
                    people who are least able to make good decisions opt out of the very technologies that improve
                    their
                    decision-making abilities, leading to an ever-increasing gap and even creating a dystopian
                    underclass (some researchers have argued that this will <a href="https://benmgarfinkel.blog/2021/02/26/is-democracy-a-fad/" target="_blank">undermine
                        democracy</a>, a topic I discuss further in the next section). This would, once again, place
                    a
                    moral blemish on AI’s positive advances. This is a difficult problem to solve as I don’t think
                    it is
                    ethically okay to coerce people, but we can at least try to increase people’s scientific
                    understanding—and perhaps AI itself can help us with this. One hopeful sign is that historically
                    anti-technology movements have been more bark than bite: railing against modern technology is
                    popular, but most people adopt it in the end, at least when it’s a matter of individual choice.
                    Individuals tend to adopt most health and consumer technologies, while technologies that are
                    truly
                    hampered, like nuclear power, tend to be collective political decisions.</li>
            </ul>
            <p>Overall, I am optimistic about quickly bringing AI’s biological advances to people in the developing
                world. I am hopeful, though not confident, that AI can also enable unprecedented economic growth
                rates
                and allow the developing world to at least surpass where the developed world is now. I am concerned
                about the “opt out” problem in both the developed and developing world, but suspect that it will
                peter
                out over time and that AI can help accelerate this process. It won’t be a perfect world, and those
                who
                are behind won’t fully catch up, at least not in the first few years. But with strong efforts on our
                part, we may be able to get things moving in the right direction—and fast. If we do, we can make at
                least a downpayment on the promises of dignity and equality that we owe to every human being on
                earth.
            </p>
            <h2>4. Peace and governance</h2>
            <p>Suppose that everything in the first three sections goes well: disease, poverty, and inequality are
                significantly reduced and the baseline of human experience is raised substantially. It does not
                follow
                that all major causes of human suffering are solved. Humans are still a threat to each other<i>.</i>
                Although there is a trend of technological improvement and economic development <a href="https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature" target="_blank">leading to
                    democracy and peace</a>, it is a very loose trend, with frequent (and <a href="https://ourworldindata.org/grapher/countries-democracies-autocracies-row" target="_blank">recent</a>) backsliding. At the dawn of the 20th Century, people <a href="https://en.wikipedia.org/wiki/The_Great_Illusion" target="_blank">thought</a> they had put
                war
                behind them; then came the two world wars. Thirty years ago Francis Fukuyama wrote about “<a href="https://en.wikipedia.org/wiki/The_End_of_History_and_the_Last_Man" target="_blank">the End
                    of
                    History</a>” and a final triumph of liberal democracy; that hasn’t happened yet. Twenty years
                ago US
                policymakers believed that free trade with China would cause it to liberalize as it became richer;
                that
                very much didn’t happen, and we now seem <a href="https://www.noahpinion.blog/p/why-the-us-should-fight-cold-war" target="_blank">headed for
                    a
                    second cold war</a> with a resurgent authoritarian bloc. And plausible theories suggest that
                internet technology <a href="https://www.noahpinion.blog/p/the-super-scary-theory-of-the-21st-a3a" target="_blank">may actually advantage authoritarianism</a>, not democracy as initially believed
                (e.g. in the “Arab Spring” period). It seems important to try to understand how powerful AI will
                intersect with these issues of peace, democracy, and freedom.</p>
            <p>Unfortunately, I see no strong reason to believe AI will preferentially or structurally advance
                democracy
                and peace, in the same way that I think it will structurally advance human health and alleviate
                poverty.
                Human conflict is adversarial and AI can in principle help both the “good guys” and the “bad guys”.
                If
                anything, some structural factors seem worrying: AI seems likely to enable much better propaganda
                and
                surveillance, both major tools in the autocrat’s toolkit. It’s therefore up to us as individual
                actors
                to tilt things in the right direction: if we want AI to favor democracy and individual rights, we
                are
                going to have to fight for that outcome. I feel even more strongly about this than I do about
                international inequality: the triumph of liberal democracy and political stability is <i>not</i>
                guaranteed, perhaps not even likely, and will require great sacrifice and commitment on all of our
                parts, as it often has in the past.</p>
            <p>I think of the issue as having two parts: international conflict, and the internal structure of
                nations.
                On the international side, it seems very important that democracies have the upper hand on the world
                stage when powerful AI is created. AI-powered authoritarianism seems too terrible to contemplate, so
                democracies need to be able to set the terms by which powerful AI is brought into the world, both to
                avoid being overpowered by authoritarians and to prevent human rights abuses within authoritarian
                countries.</p>
            <p>My current guess at the best way to do this is via an “entente strategy”<sup id="fnref:26"><a href="#fn:26">26</a></sup>, in which a coalition of democracies seeks
                to
                gain a clear advantage (even just a temporary one) on powerful AI by securing its supply chain,
                scaling
                quickly, and <a href="https://www.csis.org/analysis/updated-october-7-semiconductor-export-controls" target="_blank">blocking or delaying</a> adversaries’ access to key resources like chips and
                semiconductor equipment. This coalition would on one hand use AI to achieve robust military
                superiority
                (the stick) while at the same time offering to distribute the benefits of powerful AI (the carrot)
                to a
                wider and wider group of countries in exchange for supporting the coalition’s strategy to promote
                democracy (this would be a bit analogous to “<a href="https://en.wikipedia.org/wiki/Atoms_for_Peace" target="_blank">Atoms for Peace</a>”). The coalition would aim to gain the support of more and
                more
                of the world, isolating our worst adversaries and eventually putting them in a position where they
                are
                better off taking the same bargain as the rest of the world: give up competing with democracies in
                order
                to receive all the benefits and not fight a superior foe.</p>
            <p>If we can do all this, we will have a world in which democracies lead on the world stage and have the
                economic and military strength to avoid being undermined, conquered, or sabotaged by autocracies,
                and
                may be able to parlay their AI superiority into a durable advantage. This could optimistically lead
                to
                an “eternal 1991”—a world where democracies have the upper hand and Fukuyama’s dreams are realized.
                Again, this will be very difficult to achieve, and will in particular require close cooperation
                between
                private AI companies and democratic governments, as well as extraordinarily wise decisions about the
                balance between carrot and stick.</p>
            <p>Even if all that goes well, it leaves the question of the fight between democracy and autocracy
                <i>within</i> each country. It is obviously hard to predict what will happen here, but I do have
                some
                optimism that <i>given</i> a global environment in which democracies control the most powerful AI,
                <i>then</i> AI may actually structurally favor democracy everywhere. In particular, in this
                environment
                democratic governments can use their superior AI to win the information war: they can counter
                influence
                and propaganda operations by autocracies and may even be able to create a globally free information
                environment by providing channels of information and AI services in a way that autocracies lack the
                technical ability to block or monitor. It probably isn’t necessary to deliver propaganda, only to
                counter malicious attacks and unblock the free flow of information. Although not immediate, a level
                playing field like this stands a good chance of gradually tilting global governance towards
                democracy,
                for several reasons.
            </p>
            <p>First, the increases in quality of life in Sections 1-3 should, all things equal, promote democracy:
                historically they have, to at least some extent. In particular I expect improvements in mental
                health,
                well-being, and education to increase democracy, as all three are <a href="https://link.springer.com/article/10.1007/s11482-022-10070-y" target="_blank">negatively</a>
                <a href="https://www.cambridge.org/core/journals/perspectives-on-politics/article/abs/who-is-open-to-authoritarian-governance-within-western-democracies/0ADCD5FFE5B7E9267E8283C7561FB6BE" target="_blank">correlated</a> with support for authoritarian leaders. In general people want
                more
                self-expression when their other needs are met, and democracy is among other things a form of
                self-expression. Conversely, authoritarianism thrives on fear and resentment.
            </p>
            <p>Second, there is a good chance free information really does undermine authoritarianism, as long as
                the
                authoritarians can’t censor it. And uncensored AI can also bring individuals powerful tools for
                undermining repressive governments. Repressive governments survive by denying people a certain kind
                of
                common knowledge, keeping them from realizing that “the emperor has no clothes”. For example <a href="https://en.wikipedia.org/wiki/Sr%C4%91a_Popovi%C4%87_(activist)" target="_blank">Srđa
                    Popović</a>, who helped to topple the Milošević government in Serbia, has written extensively
                about
                techniques for psychologically robbing authoritarians of their power, for breaking the spell and
                rallying support against a dictator. A superhumanly effective AI version of Popović (whose skills
                seem
                like they have high returns to intelligence) in everyone’s pocket, one that dictators are powerless
                to
                block or censor, could create a wind at the backs of dissidents and reformers across the world. To
                say
                it again, this will be a long and protracted fight, one where victory is not assured, but if we
                design
                and build AI in the right way, it may at least be a fight where the advocates of freedom everywhere
                have
                an advantage.</p>
            <p>As with neuroscience and biology, we can also ask how things could be “better than normal”—not just
                how
                to avoid autocracy, but how to make democracies better than they are today. Even within democracies,
                injustices happen all the time. Rule-of-law societies make a promise to their citizens that everyone
                will be equal under the law and everyone is entitled to basic human rights, but obviously people do
                not
                always receive those rights in practice. That this promise is even partially fulfilled makes it
                something to be proud of, but can AI help us do better?</p>
            <p>For example, could AI improve our legal and judicial system by making decisions and processes more
                impartial? Today people mostly worry in legal or judicial contexts that AI systems will be a <a href="https://www.vox.com/technology/23738987/racism-ai-automated-bias-discrimination-algorithm" target="_blank"><i>cause</i> of discrimination</a>, and these worries are important and need to
                be
                defended against. At the same time, the vitality of democracy depends on harnessing new technologies
                to
                improve democratic institutions, not just responding to risks. A truly mature and successful
                implementation of AI has the potential to <i>reduce</i> bias and be fairer for everyone.</p>
            <p>For centuries, legal systems have faced the dilemma that the law aims to be impartial, but is
                inherently
                subjective and thus must be interpreted by biased humans. Trying to make the law fully mechanical
                hasn’t
                worked because the real world is messy and can’t always be captured in mathematical formulas.
                Instead
                legal systems rely on notoriously imprecise criteria like “<a href="https://en.wikipedia.org/wiki/Cruel_and_unusual_punishment" target="_blank">cruel and
                    unusual
                    punishment</a>” or “<a href="https://en.wikipedia.org/wiki/Roth_v._United_States" target="_blank">utterly without redeeming social importance</a>”, which humans then
                interpret—and
                often do so in a manner that displays bias, favoritism, or arbitrariness. “<a href="https://en.wikipedia.org/wiki/Smart_contract" target="_blank">Smart contracts</a>” in
                cryptocurrencies haven’t revolutionized law because ordinary code isn’t smart enough to adjudicate
                all
                that much of interest. But AI might be smart enough for this: it is the first technology capable of
                making broad, fuzzy judgements in a repeatable and mechanical way.</p>
            <p>I am not suggesting that we literally replace judges with AI systems, but the combination of
                impartiality
                with the ability to understand and process messy, real world situations <i>feels</i> like it should
                have
                some serious positive applications to law and justice. At the very least, such systems could work
                alongside humans as an aid to decision-making. Transparency would be important in any such system,
                and a
                mature science of AI could conceivably provide it: the training process for such systems could be
                extensively studied, and <a href="https://transformer-circuits.pub/" target="_blank">advanced
                    interpretability techniques</a> could be used to see inside the final model and assess it for
                hidden
                biases, in a way that is simply not possible with humans. Such AI tools could also be used to
                monitor
                for violations of fundamental rights in a judicial or police context, making constitutions more
                self-enforcing.</p>
            <p>In a similar vein, AI could be used to both aggregate opinions and drive consensus among citizens,
                resolving conflict, finding common ground, and seeking compromise. Some early ideas in this
                direction
                have been undertaken by the <a href="https://compdemocracy.org/" target="_blank">computational
                    democracy
                    project</a>, including <a href="https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input" target="_blank">collaborations with Anthropic</a>. A more informed and thoughtful citizenry
                would
                obviously strengthen democratic institutions.</p>
            <p>There is also a clear opportunity for AI to be used to help provision government services—such as
                health
                benefits or social services—that are in principle available to everyone but in practice often
                severely
                lacking, and worse in some places than others. This includes health services, the DMV, taxes, social
                security, building code enforcement, and so on. Having a very thoughtful and informed AI whose job
                is to
                give you everything you’re legally entitled to by the government in a way you can understand—and who
                also helps you comply with often confusing government rules—would be a big deal. Increasing state
                capacity both helps to deliver on the promise of equality under the law, and strengthens respect for
                democratic governance. Poorly implemented services are currently a major driver of cynicism about
                government<sup id="fnref:27"><a href="#fn:27">27</a></sup>.</p>
            <p>All of these are somewhat vague ideas, and as I said at the beginning of this section, I am not
                nearly as
                confident in their feasibility as I am in the advances in biology, neuroscience, and poverty
                alleviation. They may be unrealistically utopian. But the important thing is to have an ambitious
                vision, to be willing to dream big and try things out. The vision of AI as a guarantor of liberty,
                individual rights, and equality under the law is too powerful a vision not to fight for. A 21st
                century,
                AI-enabled polity could be both a stronger protector of individual freedom, and a beacon of hope
                that
                helps make liberal democracy the form of government that the whole world wants to adopt.</p>
            <h2>5. Work and meaning</h2>
            <p>Even if everything in the preceding four sections goes well—not only do we alleviate disease,
                poverty,
                and inequality, but liberal democracy becomes the dominant form of government, and existing liberal
                democracies become better versions of themselves—at least one important question still remains.
                “It’s
                great we live in such a technologically advanced world as well as a fair and decent one”, someone
                might
                object, “but with AI’s doing everything, how will humans have meaning? For that matter, how will
                they
                survive economically?”.</p>
            <p>I think this question is more difficult than the others. I don’t mean that I am necessarily more
                pessimistic about it than I am about the other questions (although I do see challenges). I mean that
                it
                is fuzzier and harder to predict in advance, because it relates to macroscopic questions about how
                society is organized that tend to resolve themselves only over time and in a decentralized manner.
                For
                example, historical hunter-gatherer societies might have imagined that life is meaningless without
                hunting and various kinds of hunting-related religious rituals, and would have imagined that our
                well-fed technological society is devoid of purpose. They might also have not understood how our
                economy
                can provide for everyone, or what function people can usefully service in a mechanized society.</p>
            <p>Nevertheless, it’s worth saying at least a few words, while keeping in mind that the brevity of this
                section is not at all to be taken as a sign that I don’t take these issues seriously—on the
                contrary, it
                is a sign of a lack of clear answers.</p>
            <p>On the question of meaning, I think it is very likely a mistake to believe that tasks you undertake
                are
                meaningless simply because an AI could do them better. Most people are not the best in the world at
                anything, and it doesn’t seem to bother them particularly much. Of course today they can still
                contribute through comparative advantage, and may derive meaning from the economic value they
                produce,
                but people also greatly enjoy activities that produce no economic value. I spend plenty of time
                playing
                video games, swimming, walking around outside, and talking to friends, all of which generates zero
                economic value. I might spend a day trying to get better at a video game, or faster at biking up a
                mountain, and it doesn’t really matter to me that someone somewhere is much better at those things.
                In
                any case I think meaning comes mostly from human relationships and connection, not from economic
                labor.
                People do want a sense of accomplishment, even a sense of competition, and in a post-AI world it
                will be
                perfectly possible to spend years attempting some very difficult task with a complex strategy,
                similar
                to what people do today when they embark on research projects, try to become Hollywood actors, or
                found
                companies<sup id="fnref:28"><a href="#fn:28">28</a></sup>. The facts that (a)
                an AI
                somewhere could in principle do this task better, and (b) this task is no longer an economically
                rewarded element of a global economy, don’t seem to me to matter very much.</p>
            <p>The economic piece actually seems more difficult to me than the meaning piece. By “economic” in this
                section I mean the possible problem that <i>most or all</i> humans may not be able to contribute
                meaningfully to a sufficiently advanced AI-driven economy. This is a more macro problem than the
                separate problem of inequality, especially inequality in access to the new technologies, which I
                discussed in Section 3.</p>
            <p>First of all, in the short term I agree with arguments that comparative advantage will continue to
                keep
                <a href="https://www.noahpinion.blog/p/plentiful-high-paying-jobs-in-the" target="_blank">humans
                    relevant</a> and in fact increase their productivity, and may even in some ways <a href="https://www.nber.org/papers/w31161" target="_blank">level the playing field between
                    humans</a>. As long as AI is only better at 90% of a given job, the other 10% will cause humans
                to
                become highly leveraged, increasing compensation and in fact creating a bunch of new human jobs
                complementing and amplifying what AI is good at, such that the “10%” <a href="https://en.wikipedia.org/wiki/Lump_of_labour_fallacy" target="_blank">expands to continue
                    to
                    employ almost everyone</a>. In fact, even if AI can do 100% of things better than humans, but it
                remains inefficient or expensive at some tasks, or if the resource <i>inputs</i> to humans and AI’s
                are
                meaningfully different, then the logic of comparative advantage continues to apply. One area humans
                are
                likely to maintain a relative (or even absolute) advantage for a significant time is the physical
                world.
                Thus, I think that the human economy may continue to make sense even a little past the point where
                we
                reach “a country of geniuses in a datacenter”.
            </p>
            <p>However, I do think in the long run AI will become so broadly effective and so cheap that this will
                no
                longer apply. At that point our current economic setup will no longer make sense, and there will be
                a
                need for a broader societal conversation about how the economy should be organized.</p>
            <p>While that might sound crazy, the fact is that civilization has successfully navigated major economic
                shifts in the past: from hunter-gathering to farming, farming to feudalism, and feudalism to
                industrialism. I suspect that some new and stranger thing will be needed, and that it’s something no
                one
                today has done a good job of envisioning. It could be as simple as a large universal basic income
                for
                everyone, although I suspect that will only be a small part of a solution. It could be a capitalist
                economy of AI systems, which then give out resources (huge amounts of them, since the overall
                economic
                pie will be gigantic) to humans based on some secondary economy of what the AI systems think makes
                sense
                to reward in humans (based on some judgment ultimately derived from human values). Perhaps the
                economy
                runs on <a href="https://en.wikipedia.org/wiki/Down_and_Out_in_the_Magic_Kingdom" target="_blank">Whuffie points</a>. Or perhaps humans will continue to be economically valuable
                after all, in some way not anticipated by the usual economic models. All of these solutions have
                tons of
                possible problems, and it’s not possible to know whether they will make sense without lots of
                iteration
                and experimentation. And as with some of the other challenges, we will likely have to fight to get a
                good outcome here: exploitative or dystopian directions are clearly also possible and have to be
                prevented. Much more could be written about these questions and I hope to do so at some later time.
            </p>
            <h2>Taking stock</h2>
            <p>Through the varied topics above, I’ve tried to lay out a vision of a world that is both plausible
                <i>if</i> everything goes right with AI, and much better than the world today. I don’t know if this
                world is realistic, and even if it is, it will not be achieved without a huge amount of effort and
                struggle by many brave and dedicated people. Everyone (including AI companies!) will need to do
                their
                part both to prevent risks and to fully realize the benefits.
            </p>
            <p>But it is a world worth fighting for. If all of this really does happen over 5 to 10 years—the defeat
                of
                most diseases, the growth in biological and cognitive freedom, the lifting of billions of people out
                of
                poverty to share in the new technologies, a renaissance of liberal democracy and human rights—I
                suspect
                everyone watching it will be surprised by the effect it has on them. I don’t mean the experience of
                personally benefiting from all the new technologies, although that will certainly be amazing. I mean
                the
                experience of watching a long-held set of ideals materialize in front of us all at once. I think
                many
                will be literally moved to tears by it.</p>
            <p>Throughout writing this essay I noticed an interesting tension. In one sense the vision laid out here
                is
                extremely radical: it is not what almost anyone expects to happen in the next decade, and will
                likely
                strike many as an absurd fantasy. Some may not even consider it desirable; it embodies values and
                political choices that not everyone will agree with. But at the same time there is something
                blindingly
                obvious—something overdetermined—about it, as if many different attempts to envision a good world
                inevitably lead roughly here.</p>
            <p>In Iain M. Banks’ <a href="https://www.hachettebookgroup.com/titles/iain-m-banks/the-player-of-games/9780316005401/" target="_blank"><i>The Player of Games</i></a><sup id="fnref:29"><a href="#fn:29">29</a></sup>, the protagonist—a member of a society called the Culture, which
                is
                based on principles not unlike those I’ve laid out here—travels to a repressive, militaristic empire
                in
                which leadership is determined by competition in an intricate battle game. The game, however, is
                complex
                enough that a player’s strategy within it tends to reflect their own political and philosophical
                outlook. The protagonist manages to defeat the emperor in the game, showing that his values (the
                Culture’s values) represent a winning strategy even in a game designed by a society based on
                ruthless
                competition and survival of the fittest. <a href="https://slatestarcodex.com/2015/08/17/the-goddess-of-everything-else-2/" target="_blank">A
                    well-known post</a> by Scott Alexander has the same thesis—that competition is self-defeating
                and
                tends to lead to a society based on compassion and cooperation. The “<a href="https://www.si.edu/spotlight/mlk?page=4&amp;iframe=true" target="_blank">arc of the moral
                    universe</a>” is another similar concept.</p>
            <p>I think the Culture’s values are a winning strategy because they’re the sum of a million small
                decisions
                that have clear moral force and that tend to pull everyone together onto the same side. Basic human
                intuitions of fairness, cooperation, curiosity, and autonomy are hard to argue with, and are
                cumulative
                in a way that our more destructive impulses often aren’t. It is easy to argue that children
                shouldn’t
                die of disease if we can prevent it, and easy from there to argue that <i>everyone’s</i> children
                deserve that right equally. From there it is not hard to argue that we should all band together and
                apply our intellects to achieve this outcome. Few disagree that people should be punished for
                attacking
                or hurting others unnecessarily, and from there it’s not much of a leap to the idea that punishments
                should be consistent and systematic across people. It is similarly intuitive that people should have
                autonomy and responsibility over their own lives and choices. These simple intuitions, if taken to
                their
                logical conclusion, lead eventually to rule of law, democracy, and Enlightenment values. If not
                inevitably, then at least as a statistical tendency, this is where humanity was already headed. AI
                simply offers an opportunity to get us there more quickly—to make the logic starker and the
                destination
                clearer.</p>
            <p>Nevertheless, it is a thing of transcendent beauty. We have the opportunity to play some small role
                in
                making it real.</p>
            <hr/>
            <p><i>Thanks to Kevin Esvelt, Parag Mallick, Stuart Ritchie, Matt Yglesias, Erik Brynjolfsson, Jim
                    McClave,
                    Allan Dafoe, and many people at Anthropic for reviewing drafts of this essay.</i></p>
            <p><i>To the winners of the <a href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/" target="_blank">2024 Nobel prize in Chemistry</a>, for showing us all the way.</i></p>
            

            
        </article>
    </div></div>
  </body>
</html>
