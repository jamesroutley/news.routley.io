<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.voyageai.com/2024/12/04/voyage-code-3/">Original</a>
    <h1>Voyage-code-3</h1>
    
    <div id="readability-page-1" class="page"><div>
<div>




<div><div>
<p>TL;DR – Introducing <code>voyage-code-3</code>, our next-generation embedding model optimized for code retrieval. It outperforms OpenAI-v3-large and CodeSage-large by an average of 13.80% and 16.81% on a suite of 32 code retrieval datasets, respectively. By supporting smaller dimensions with Matryoshka learning and quantized formats like int8 and binary, <code>voyage-code-3</code> can also dramatically reduce storage and search costs with minimal impact on retrieval quality.</p>



<p>Since its launch in Jan, <code>voyage-code-2</code> has been the most heavily used model with exponentially increasing adoption by code assistants and agents startups for their code retrieval. Today, we’re thrilled to announce <code>voyage-code-3</code>, which:</p>



<ul>
<li>outperforms OpenAI-v3-large and CodeSage-large by an average of 13.80% and 16.81% on a suite of 32 code retrieval datasets, respectively</li>



<li>supports embeddings of 2048, 1024, 512, and 256 dimensions</li>



<li>offers multiple embedding quantization, including <code>float</code> (32-bit floating point), <code>int8</code> (8-bit signed integer), <code>uint8</code> (8-bit unsigned integer), <code>binary</code> (bit-packed <code>int8</code>), and <code>ubinary</code> (bit-packed <code>uint8</code>)</li>



<li>supports a 32K-token context length, compared to OpenAI (8K) and CodeSage large (1K)</li>
</ul>



<h2 id="Matryoshka-Embeddings-and-Quantization">Matryoshka Embeddings and Quantization</h2>



<p>Storage and search costs in vector-based search can become significant for large corpora, such as in code retrieval with massive repositories. The costs scale linearly in the embedding dimensionality and precision (i.e., the number of bits used to encode each number). <code>voyage-code-3</code> supports much lower dimensional embeddings and binary and int8 quantization to dramatically lower the costs without losing much retrieval quality. These are enabled by <a href="https://arxiv.org/abs/2205.13147" rel="nofollow" target="_blank">Matryoshka learning</a> and <a href="https://arxiv.org/abs/1702.00758" rel="nofollow" target="_blank">quantization-aware training</a>.</p>



<p><strong>Matryoshka embeddings.</strong> Matryoshka learning creates embeddings with a nested family of embeddings with various lengths within a single vector. Concretely, for each of k in {256, 512, and 1024}, the first k entries of the 2048-dimensional embedding also form a valid k-dimensional embedding that is shorter with a slight loss of retrieval quality. Thus, the users can vectorize the documents into a long 2048-dimensional vector in advance and then later have the flexibility to use a shorter version of the embedding (by taking the first k entries) without re-invoking the embedding model.</p>



<p><strong>Quantization.</strong> Quantized embeddings have lower precision, represented with 8 bits or 1 bit per dimension, reducing 4x or 32x storage costs compared to 32-bit floats, respectively. <code>voyage-code-3</code> can return the embeddings with lower precision with various data types, <code>int8</code> (8-bit signed integer), <code>uint8</code> (8-bit unsigned integer), <code>binary</code> (bit-packed <code>int8</code>), and <code>ubinary</code> (bit-packed <code>uint8</code>). Most vector databases support storing and searching with quantized embeddings directly, including Milvus, Qdrant, Weaviate, Elasticsearch, and Vespa AI.</p>



<p><strong>Storage cost vs retrieval quality tradeoff.</strong> Quantization and shorter embeddings inevitably come with a reduced retrieval quality. Voyage focuses intently on this, striving to minimize the quality loss as much as possible. The following graph plots retrieval quality versus relative storage cost — showing a limited reduction of quality up to binary 1024-dimensional embeddings compared to the float32 2048-dimensional embeddings.</p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="1024" height="754" data-attachment-id="1376" data-permalink="https://blog.voyageai.com/2024/12/04/voyage-code-3/vectordb-costs-white/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?fit=1288%2C948&amp;quality=80&amp;ssl=1" data-orig-size="1288,948" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="vectordb-costs-white" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?fit=300%2C221&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?fit=1024%2C754&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?resize=1024%2C754&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?resize=1024%2C754&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?resize=300%2C221&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?resize=768%2C565&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?resize=1200%2C883&amp;quality=80&amp;ssl=1 1200w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/vectordb-costs-white.png?w=1288&amp;quality=80&amp;ssl=1 1288w" sizes="(max-width: 1000px) 100vw, 1000px"/></figure>



<p>All green data points above the green line represent <code>voyage-code-3</code>, and those below are uniquely colored per model and explicitly labeled with quantization and dimension. Lines connecting data points represent the same model and data type but at different embedding dimensions. The evaluation results used to generate this plot are available in <a href="https://docs.google.com/spreadsheets/d/1Q5GDXOXueHuBT9demPrL9bz3_LMgajcZs_-GPeawrYk/edit?gid=105010523#gid=105010523" rel="nofollow" target="_blank">this spreadsheet</a>.</p>



<h2>Optimized for Code Retrieval</h2>



<p>Code retrieval presents unique challenges compared to general text retrieval due to the need for algorithmic reasoning and the nuanced syntax rules such as keywords, control structures, nesting, and formatting. These challenges are further complicated by several retrieval subtasks, including text-to-code (e.g., retrieve code snippets using natural language queries), code-to-code (e.g., identify semantically similar code snippets), and docstring-to-code (e.g., retrieve code snippets using function docstring queries).</p>



<p><strong>Curated, massive code training data.</strong> We curated a larger, more diverse, high-quality code corpus for training <code>voyage-code-3</code> than <code>voyage-code-2</code>. First, we assembled a broad corpus with trillions of tokens comprising text, code, and mathematical content with a carefully tuned code-to-text ratio. Next, we developed a comprehensive dataset with positive pairs for contrastive learning based on public GitHub repositories, containing docstring-code and code-code pairs across 300+ programming languages. This dataset was combined with the general text pair dataset used to train our leading general-purpose <code>voyage-3</code> model. Finally, we collected additional real-world query-code pairs, covering a wide range of tasks in code assistant use cases, to ensure robust coverage of real-world scenarios.</p>



<p><strong>Evaluation.</strong> We evaluated <code>voyage-code-3</code> using an enhanced suite of evaluation datasets designed to address the shortcomings of existing benchmarks and deliver practical, robust results. Existing datasets can suffer from noisy labels, overly simplistic tasks, and data contamination risks, making them ill-suited for real-world applications. For instance, the original CoSQA dataset was found to have <a href="https://arxiv.org/abs/2406.11589" rel="nofollow" target="_blank">51%</a> of its queries paired with mismatched code. Our evaluation incorporated diverse tasks, such as text-to-code and code-to-code, repurposed question-answer datasets for retrieval, and introduced complex, real-world repositories and scenarios that challenge embedding models to achieve deeper understanding. For a deeper dive into code retrieval evaluation, check out our previous blog <a href="https://blog.voyageai.com/2024/12/04/code-retrieval-eval/" rel="nofollow" target="_blank">post</a>.</p>



<h2>Evaluation Details</h2>



<p><strong>Datasets.</strong> We evaluate <code>voyage-code-3</code> across 32 datasets spanning five categories that cover various code retrieval tasks, real-world use cases, and challenging code scenarios. These datasets are discussed in length in our code retrieval evaluation blog <a href="https://blog.voyageai.com/2024/12/04/code-retrieval-eval/" rel="nofollow" target="_blank">post</a>. The table below summarizes the key datasets.</p>



<figure><img data-recalc-dims="1" decoding="async" width="1402" height="1188" data-attachment-id="1587" data-permalink="https://blog.voyageai.com/2024/12/04/code-retrieval-eval/voyage_retrieval_datasets-2/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?fit=1402%2C1188&amp;quality=80&amp;ssl=1" data-orig-size="1402,1188" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="voyage_retrieval_datasets" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?fit=300%2C254&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?fit=1024%2C868&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?resize=1402%2C1188&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?w=1402&amp;quality=80&amp;ssl=1 1402w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?resize=300%2C254&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?resize=1024%2C868&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/voyage_retrieval_datasets-1.png?resize=768%2C651&amp;quality=80&amp;ssl=1 768w" sizes="(max-width: 1000px) 100vw, 1000px"/></figure>



<p><strong>Models.</strong> We evaluate <code>voyage-code-3</code> alongside several general-purpose and code-specific alternatives, including: OpenAI-v3-large (<code>text-embedding-3-large</code>), OpenAI-v3-small (<code>text-embedding-3-small</code>), <a href="https://arxiv.org/abs/2402.01935" rel="nofollow" target="_blank">CodeSage-large</a>, <a href="https://gangiswag.github.io/cornstack/" rel="nofollow" target="_blank">CodeRankEmbed</a> (<code>cornstack/CodeRankEmbed</code>), Jina-v2-code (<code>jina-embeddings-v2-base-code</code>), <code>voyage-code-2</code>, <code>voyage-3</code>, and <code>voyage-3-lite</code>.</p>



<h2>Results</h2>



<p>The table below summarizes the key results from the code retrieval quality versus relative storage costs plot above. <code>voyage-code-3</code> outperforms OpenAI-v3-large on average by:</p>



<ul>
<li>14.64% and 17.66% at 1024 and 256 dimensions, respectively</li>



<li>13.80% at 1/3 the storage costs (1024 vs 3072 dimensions)</li>



<li>4.81% at 1/384 the storage costs (binary 256 vs float 3072 dimensions)</li>
</ul>



<figure><table><thead><tr><th data-align="center">Dimensions</th><th data-align="center"><code>voyage-code-3</code></th><th data-align="center">OpenAI v3</th><th data-align="center">OpenAI v3</th><th data-align="center">CodeSage</th><th data-align="center"><code>voyage-code-3</code></th><th data-align="center"><code>voyage-code-3</code></th></tr></thead><tbody><tr><td data-align="center">3072</td><td data-align="center"></td><td data-align="center"><strong>78.48%</strong></td><td data-align="center"></td><td data-align="center"></td><td data-align="center"></td><td data-align="center"></td></tr><tr><td data-align="center">2048</td><td data-align="center"><strong>92.12%</strong></td><td data-align="center"></td><td data-align="center"></td><td data-align="center">75.47%</td><td data-align="center">91.59%</td><td data-align="center">91.95%</td></tr><tr><td data-align="center">1536</td><td data-align="center"></td><td data-align="center"></td><td data-align="center"><strong>71.73%</strong></td><td data-align="center"></td><td data-align="center"></td><td data-align="center"></td></tr><tr><td data-align="center">1024</td><td data-align="center"><strong>92.28%</strong></td><td data-align="center">77.64%</td><td data-align="center">71.38%</td><td data-align="center"></td><td data-align="center">90.71%</td><td data-align="center">91.50%</td></tr><tr><td data-align="center">512</td><td data-align="center"><strong>92.00%</strong></td><td data-align="center"></td><td data-align="center"></td><td data-align="center"></td><td data-align="center">88.53%</td><td data-align="center">90.37%</td></tr><tr><td data-align="center">256</td><td data-align="center"><strong>91.34%</strong></td><td data-align="center">73.68%</td><td data-align="center">67.64%</td><td data-align="center"></td><td data-align="center">83.29%</td><td data-align="center">87.53%</td></tr></tbody></table></figure>



<p>The bar charts below show the average retrieval quality for each group of datasets (see <a href="https://docs.google.com/spreadsheets/d/1Q5GDXOXueHuBT9demPrL9bz3_LMgajcZs_-GPeawrYk/edit?gid=105010523#gid=105010523" rel="nofollow" target="_blank">spreadsheet</a> for a full list of datasets and the grouping). <code>voyage-code-3</code> outperforms all other models in every group, exceeding OpenAI-v3-large on average by 16.30%.</p>



<figure><img data-recalc-dims="1" decoding="async" width="1024" height="412" data-attachment-id="1374" data-permalink="https://blog.voyageai.com/2024/12/04/voyage-code-3/bars-4/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?fit=1920%2C772&amp;quality=80&amp;ssl=1" data-orig-size="1920,772" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="bars" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?fit=300%2C121&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?fit=1024%2C412&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?resize=1024%2C412&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?resize=1024%2C412&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?resize=300%2C121&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?resize=768%2C309&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?resize=1536%2C618&amp;quality=80&amp;ssl=1 1536w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?resize=1200%2C483&amp;quality=80&amp;ssl=1 1200w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/12/bars.png?w=1920&amp;quality=80&amp;ssl=1 1920w" sizes="(max-width: 1000px) 100vw, 1000px"/></figure>



<p><strong>Binary rescoring.</strong> Finally, users sometimes first retrieve a decent number of (e.g., 100 in our evaluation) documents with binary embeddings and then rescore the retrieved documents with full-precision embeddings. For <code>voyage-code-3</code>, as shown in the table, binary rescoring yields up to 4.25% improvement in retrieval quality when applied on top of standard binary retrieval.</p>



<p>All the evaluation results are available in <a href="https://docs.google.com/spreadsheets/d/1Q5GDXOXueHuBT9demPrL9bz3_LMgajcZs_-GPeawrYk/edit?gid=105010523#gid=105010523" rel="nofollow" target="_blank">this spreadsheet</a>.</p>



<h2>Try voyage-code-3!</h2>



<p><code>voyage-code-3</code> is available today! The first 200 million tokens are free. To get started, head over to our <a href="https://docs.voyageai.com/docs/embeddings" rel="nofollow" target="_blank">docs</a> to learn more. If you’re also interested in fine-tuned embedding models, we’d love to hear from you—please email us at <a href="mailto:contact@voyageai.com">contact@voyageai.com</a>. Follow us on <a href="https://x.com/VoyageAI" rel="nofollow" target="_blank">X (Twitter)</a> and <a href="https://www.linkedin.com/company/voyageai/" rel="nofollow" target="_blank">LinkedIn</a>, and join our <a href="https://discord.gg/zAU7GQEmvT" rel="nofollow" target="_blank">Discord</a> for more updates.</p>




</div></div>




</div>
</div></div>
  </body>
</html>
