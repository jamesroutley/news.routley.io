<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tunahansalih.github.io/blog/autoregressive-vision-generation-part-1/">Original</a>
    <h1>Generating Pixels One by One</h1>
    
    <div id="readability-page-1" class="page"><div>
      <h2 id="your-first-autoregressive-image-generation-model">Your First Autoregressive Image Generation Model</h2>

<p>We’ll build a basic autoregressive model using a simple MLP to generate images ofhandwritten digits, focusing on understanding the core concept of predicting the next pixel based on its predecessors. It’s a hands-on exploration of fundamental generative AI, showing some of the core concepts using a pretty simple model. The model we will train will be far away from the state-of-the-art, but it will be a good starting point to understand the core concepts of autoregressive models.</p>

<p>Welcome, I am glad you are here!</p>

<p>I’m <a href="https://tunahansalih.github.io">Tuna</a>. My world is pretty much all about image and video generation. It is what I focus on in my PhD and during my internships at places like Adobe (working on Firefly!) and Amazon AGI. For a while, I have been working with diffusion-based models, and I know that they are incredibly powerful.</p>

<p>But the landscape of generative modeling is always growing, and I want to explore other types of generative models. Right now, I am diving into autoregressive models. I always find the best way to learn a topic is by trying to teach it to others. So, this blog post series is an attempt to teach myself the basics of autoregressive models, hoping you can learn something from it, too. I’ll start with the basics and try to understand how these models work piece by piece.</p>

<h2 id="what-makes-a-model-autoregressive">What Makes a Model “Autoregressive”?</h2>

<p>Alright, “Autoregressive”. Let’s break it down with some mathematical intuition.</p>

<p>You have already seen “auto-regressive” models in action even if you didn’t call them that. At its heart, it basically means predicting the next outcome based on all the things that came before it.</p>

<p>Think about how you type on your phone. When you write “the weather is …”, the keyboard will suggest completions based on the words you entered such as “sunny”, “rainy”, “perfect for research” (maybe not that last one). That’s an auto-regressive model in action for language.</p>

<p><em>Mathematically</em>, for a sequence (x_1, x_2, …, x_T), an autoregressive model learns:</p><p>

\[P(x_1, x_2, ..., x_T) = P(x_1) \cdot P(x_2|x_1) \cdot P(x_3|x_1, x_2) \cdot ... \cdot P(x_T|x_1, ..., x_{T-1})\]

</p><p>This is just the <em>chain rule of probability</em>. Each new element depends on all the previous ones.</p>

<p>For images, we can think of each pixel as an element in our sequence. So instead of predicting the next word, we’re predicting the next pixel value based on all the pixels we’ve seen so far. Cool, right?</p>

<video alt="test" muted="" autoplay="" loop="" height="320">
    <source src="/images/blog/AR-part1/Part1.mov" type="video/mp4"/>
</video>

<h2 id="lets-start-with-data-meet-our-mnist-friends">Let’s Start with Data: Meet Our MNIST Friends</h2>

<p>Before we dive into the math, let’s get acquainted with our data. We’ll use MNIST digits - they’re simple, well-understood, and good for learning the basics.</p>

<div><div><pre><code><span>import</span> <span>random</span>
<span>import</span> <span>torchvision</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>torch</span>
<span>import</span> <span>torchvision.transforms</span> <span>as</span> <span>transforms</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>torch.nn</span> <span>as</span> <span>nn</span>
<span>import</span> <span>torch.nn.functional</span> <span>as</span> <span>F</span>
<span>import</span> <span>torch.optim</span> <span>as</span> <span>optim</span>
<span>from</span> <span>tqdm.auto</span> <span>import</span> <span>tqdm</span> <span>as</span> <span>auto_tqdm</span> <span># Renamed to avoid conflict if user uses both
</span>
<span># Load and visualize some MNIST data
</span><span>mnist</span> <span>=</span> <span>torchvision</span><span>.</span><span>datasets</span><span>.</span><span>MNIST</span><span>(</span><span>root</span><span>=</span><span>&#39;./data&#39;</span><span>,</span> <span>train</span><span>=</span><span>True</span><span>,</span> <span>download</span><span>=</span><span>True</span><span>,</span> <span>transform</span><span>=</span><span>torchvision</span><span>.</span><span>transforms</span><span>.</span><span>ToTensor</span><span>())</span>

<span># Get 100 random image indices
</span><span>rand_indices</span> <span>=</span> <span>random</span><span>.</span><span>sample</span><span>(</span><span>range</span><span>(</span><span>len</span><span>(</span><span>mnist</span><span>.</span><span>data</span><span>)),</span> <span>100</span><span>)</span>

<span># Get the images
</span><span>images</span> <span>=</span> <span>mnist</span><span>.</span><span>data</span><span>[</span><span>rand_indices</span><span>]</span>

<span># Create a nice grid to visualize
</span><span>grid</span> <span>=</span> <span>torchvision</span><span>.</span><span>utils</span><span>.</span><span>make_grid</span><span>(</span><span>images</span><span>.</span><span>unsqueeze</span><span>(</span><span>1</span><span>),</span> <span>nrow</span><span>=</span><span>10</span><span>)</span> <span># Add channel dimension
</span>
<span># Plot the grid
</span><span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>7</span><span>,</span> <span>7</span><span>))</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>grid</span><span>.</span><span>permute</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>0</span><span>),</span> <span>cmap</span><span>=</span><span>&#39;gray&#39;</span><span>)</span>  <span># Permute to (H, W, C) format
</span><span>plt</span><span>.</span><span>axis</span><span>(</span><span>&#39;off&#39;</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>&#34;Sample MNIST Digits - Our Raw Material!&#34;</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>

</code></pre></div></div>

<p><img src="https://tunahansalih.github.io/images/blog/AR-part1/Part1_1_1.png" alt="png"/></p>

<h2 id="global-configuration-and-hyperparameters">Global Configuration and Hyperparameters</h2>

<p>Define all key hyperparameters for the models, data, and training in one place.
This allows for easy modification and consistency across the different model versions.</p>

<div><div><pre><code><span># --- General Image &amp; Data Parameters ---
</span><span>IMG_SIZE</span> <span>=</span> <span>28</span>                   <span># Dimension of the square MNIST images (e.g., 28 for 28x28).
</span><span>N_PIXELS</span> <span>=</span> <span>IMG_SIZE</span> <span>*</span> <span>IMG_SIZE</span>  <span># Total number of pixels in an image.
</span>
<span># --- Quantization &amp; Tokenization Settings ---
</span><span>NUM_QUANTIZATION_BINS</span> <span>=</span> <span>16</span>  <span># K: Pixel vocabulary size (tokens 0 to K-1). Max 256 for 8-bit.
# The start/padding token will be an integer outside the 0 to K-1 range.
# We&#39;ll use K as its value, so token embedding layers need to accommodate K+1 distinct values.
</span><span>START_TOKEN_VALUE_INT</span> <span>=</span> <span>NUM_QUANTIZATION_BINS</span>
<span>NUM_CLASSES</span> <span>=</span> <span>10</span>  <span># Number of classes for conditional generation (MNIST digits 0-9)
</span>
<span># --- Autoregressive Model Architecture ---\n&#34;,
</span><span>CONTEXT_LENGTH</span> <span>=</span> <span>28</span>  <span># Number of previous tokens (context window) fed to the MLP.
</span><span>HIDDEN_SIZE</span> <span>=</span> <span>1024</span>  <span># Size of hidden layers in the MLPs.
# For Model V2 &amp; V3 (Positional Embeddings)
</span><span>POS_EMBEDDING_DIM</span> <span>=</span> <span>32</span>  <span># Dimension for learnable positional embeddings (one for row, one for column).
# For Model V3 (Token Embeddings)
</span><span>TOKEN_EMBEDDING_DIM</span> <span>=</span> <span>16</span>  <span># Dimension for learned pixel token embeddings.
</span><span>CLASS_EMBEDDING_DIM</span> <span>=</span> <span>16</span>  <span># Dimension for learned class label embeddings (Model V3)
</span>
<span># --- Dataset Generation ---
</span><span>MAX_SAMPLES</span> <span>=</span> <span>5000000</span>           <span># Maximum number of (context, target) training pairs to generate from MNIST.
</span>                                <span># Helps keep data generation and training times manageable.
</span>
<span># --- Training Hyperparameters (shared across V1, V2, V3 for simplicity) ---
</span><span>LEARNING_RATE</span> <span>=</span> <span>0.001</span>           <span># Learning rate for the AdamW optimizer.
</span><span>EPOCHS</span> <span>=</span> <span>20</span>                     <span># Number of training epochs for each model.
</span><span>BATCH_SIZE_TRAIN</span> <span>=</span> <span>512</span>          <span># Batch size used during model training.
</span>
<span># --- Device Setup (Attempt to use GPU if available) ---
# This device variable will be used globally by training and generation functions.
</span><span>_device_type</span> <span>=</span> <span>&#34;cpu&#34;</span> <span># Default
</span><span>if</span> <span>torch</span><span>.</span><span>cuda</span><span>.</span><span>is_available</span><span>():</span>
    <span>_device_type</span> <span>=</span> <span>&#34;cuda&#34;</span>
    <span>print</span><span>(</span><span>&#34;Global Config: CUDA (GPU) is available. Using CUDA.&#34;</span><span>)</span>
<span>elif</span> <span>torch</span><span>.</span><span>backends</span><span>.</span><span>mps</span><span>.</span><span>is_available</span><span>():</span> <span># For Apple Silicon
</span>    <span>_device_type</span> <span>=</span> <span>&#34;mps&#34;</span>
    <span>print</span><span>(</span><span>&#34;Global Config: MPS (Apple Silicon GPU) is available. Using MPS.&#34;</span><span>)</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;Global Config: No GPU detected. Using CPU.&#34;</span><span>)</span>
<span>device</span> <span>=</span> <span>torch</span><span>.</span><span>device</span><span>(</span><span>_device_type</span><span>)</span> <span># Define the global device variable
</span>
</code></pre></div></div>

<h2 id="the-pixel-as-token-approach-quantizing-intensities">The “Pixel as Token” Approach: Quantizing Intensities</h2>

<p>We treat image generation more like language modeling. Each pixel’s intensity will be <em>quantized</em> into one of a discrete number of bins. Each bin then gets an integer label, turning our pixels into “tokens” or “words” from a fixed vocabulary.</p>

<h3 id="1-quantization">1. Quantization</h3>

<ul>
  <li>We take the continuous grayscale pixel values (0.0 to 1.0).</li>
  <li>We divide this range into <code>K</code> discrete bins (e.g., <code>K=16</code> or <code>K=256</code>).</li>
  <li>Each pixel’s original intensity is mapped to the integer label of the bin it falls into.</li>
  <li>Example: If <code>K=4</code>, intensities 0.0-0.25 map to token 0, 0.25-0.5 to token 1, etc.</li>
</ul>

<h3 id="2-prediction-as-classification">2. Prediction as Classification</h3>

<ul>
  <li>The model’s task is now to predict the <em>integer label</em> (token) of the next pixel, given the tokens of previous pixels.</li>
  <li>This is a <em>K-class classification problem</em> for each pixel.</li>
</ul>

<p><em>Benefits:</em></p>

<ul>
  <li>Allows using powerful classification machinery (like Cross-Entropy loss).</li>
  <li>Can be combined with techniques like embedding layers for tokens, similar to NLP.</li>
</ul>

<p><em>Trade-offs:</em></p>

<ul>
  <li><em>Information Loss:</em> Quantization inherently loses some precision from the original grayscale values. More bins reduce this loss but increase model complexity.</li>
  <li><em>Vocabulary Size:</em> The number of bins (<code>K</code>) becomes our vocabulary size.</li>
</ul>

<div><div><pre><code><span># --- Data Loading and Quantization ---
</span><span>def</span> <span>quantize_tensor</span><span>(</span><span>tensor_image</span><span>,</span> <span>num_bins</span><span>):</span>
    <span>&#34;&#34;&#34;Quantizes a tensor image (values 0-1) into num_bins integer labels (0 to num_bins-1).&#34;&#34;&#34;</span>
    <span># Scale to [0, num_bins - epsilon] then floor to get integer labels
</span>    <span># tensor_image is already in [0,1]
</span>    <span>scaled_image</span> <span>=</span> <span>tensor_image</span> <span>*</span> <span>(</span><span>num_bins</span> <span>-</span> <span>1e-6</span><span>)</span> <span># Subtract epsilon to handle 1.0 correctly
</span>    <span>quantized_image</span> <span>=</span> <span>torch</span><span>.</span><span>floor</span><span>(</span><span>scaled_image</span><span>).</span><span>long</span><span>()</span> <span>#.long() for integer labels
</span>    <span>return</span> <span>quantized_image</span>

<span># Transform pipeline
</span><span>transform_quantize</span> <span>=</span> <span>transforms</span><span>.</span><span>Compose</span><span>([</span>
    <span>transforms</span><span>.</span><span>ToTensor</span><span>(),</span> <span># Converts to [0,1] float tensor
</span>    <span>transforms</span><span>.</span><span>Lambda</span><span>(</span><span>lambda</span> <span>x</span><span>:</span> <span>quantize_tensor</span><span>(</span><span>x</span><span>,</span> <span>NUM_QUANTIZATION_BINS</span><span>))</span> <span># Quantize to integer labels
</span><span>])</span>

<span># MNIST dataset
</span><span>trainset_quantized</span> <span>=</span> <span>torchvision</span><span>.</span><span>datasets</span><span>.</span><span>MNIST</span><span>(</span><span>root</span><span>=</span><span>&#39;./data&#39;</span><span>,</span> <span>train</span><span>=</span><span>True</span><span>,</span>
                                                 <span>download</span><span>=</span><span>True</span><span>,</span> <span>transform</span><span>=</span><span>transform_quantize</span><span>)</span>

<span>print</span><span>(</span><span>f</span><span>&#34;Loaded </span><span>{</span><span>len</span><span>(</span><span>trainset_quantized</span><span>)</span><span>}</span><span> quantized training images.&#34;</span><span>)</span>

<span># --- Visualize Quantized Data ---
</span>
<span># Helper to de-quantize for visualization
</span><span>def</span> <span>dequantize_tensor</span><span>(</span><span>quantized_image_labels</span><span>,</span> <span>num_bins</span><span>):</span>
    <span>&#34;&#34;&#34;Converts integer labels back to approximate normalized grayscale values (centers of bins).&#34;&#34;&#34;</span>
    <span># Map label L to (L + 0.5) / num_bins
</span>    <span>return</span> <span>(</span><span>quantized_image_labels</span><span>.</span><span>float</span><span>()</span> <span>+</span> <span>0.5</span><span>)</span> <span>/</span> <span>num_bins</span>

<span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Visualizing a few samples from the quantized dataset...&#34;</span><span>)</span>
<span>fig_vis</span><span>,</span> <span>axes_vis</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>2</span><span>,</span> <span>5</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>7</span><span>,</span> <span>4</span><span>))</span>
<span>fig_vis</span><span>.</span><span>suptitle</span><span>(</span><span>f</span><span>&#34;Sample Quantized MNIST (</span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span> bins) - Visualized&#34;</span><span>,</span> <span>fontsize</span><span>=</span><span>14</span><span>)</span>
<span>for</span> <span>i</span><span>,</span> <span>ax</span> <span>in</span> <span>enumerate</span><span>(</span><span>axes_vis</span><span>.</span><span>flat</span><span>):</span>
    <span>quantized_single_image_int</span><span>,</span> <span>_</span> <span>=</span> <span>trainset_quantized</span><span>[</span><span>i</span><span>]</span> <span># Get i-th sample from Dataset
</span>    <span># quantized_single_image_int shape is likely [1, IMG_SIZE, IMG_SIZE]
</span>    
    <span>if</span> <span>i</span> <span>==</span> <span>0</span><span>:</span> <span># Print info for the first sample
</span>        <span>print</span><span>(</span><span>f</span><span>&#34;Shape of a single quantized image from dataset: </span><span>{</span><span>quantized_single_image_int</span><span>.</span><span>shape</span><span>}</span><span>&#34;</span><span>)</span>
        <span>print</span><span>(</span><span>f</span><span>&#34;Data type: </span><span>{</span><span>quantized_single_image_int</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span><span>)</span>
        <span>print</span><span>(</span><span>f</span><span>&#34;Unique labels in first sample: </span><span>{</span><span>torch</span><span>.</span><span>unique</span><span>(</span><span>quantized_single_image_int</span><span>)</span><span>}</span><span>&#34;</span><span>)</span>

    <span>vis_image_dequantized</span> <span>=</span> <span>dequantize_tensor</span><span>(</span><span>quantized_single_image_int</span><span>.</span><span>squeeze</span><span>(),</span> <span>NUM_QUANTIZATION_BINS</span><span>)</span>
    <span>ax</span><span>.</span><span>imshow</span><span>(</span><span>vis_image_dequantized</span><span>,</span> <span>cmap</span><span>=</span><span>&#39;gray&#39;</span><span>,</span> <span>vmin</span><span>=</span><span>0</span><span>,</span> <span>vmax</span><span>=</span><span>1</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>&#34;Sample </span><span>{</span><span>i</span><span>+</span><span>1</span><span>}</span><span>&#34;</span><span>)</span>
    <span>ax</span><span>.</span><span>axis</span><span>(</span><span>&#39;off&#39;</span><span>)</span>
<span>plt</span><span>.</span><span>tight_layout</span><span>(</span><span>rect</span><span>=</span><span>[</span><span>0</span><span>,</span><span>0</span><span>,</span><span>1</span><span>,</span><span>0.95</span><span>])</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div></div>

<p><img src="https://tunahansalih.github.io/images/blog/AR-part1/Part1_5_1.png" alt="png"/></p>

<h2 id="building-our-first-token-predictor-a-basic-mlp">Building Our First Token Predictor: A Basic MLP</h2>

<p>Now that we have quantized our pixels, or <em>tokens</em>, (from 0 to NUM_QUANTIZATION_BINS-1), let’s build our first autoregressive model. The goal is simple: given a token sequence of length <em>CONTEXT_LENGTH</em>, predict the next token.</p>

<p>For this first version, we will try a fairly direct approach:</p>

<ol>
  <li><em>Representing Tokens</em>: Our tokens are integers from 0 to NUM_QUANTIZATION_BINS-1. To feed them into the neural network, a common approach is to use <em>one-hot encoding</em>. For example, if a token is represented as <em>3</em>, the one-hot encoding is a list of length (NUM_QUANTIZATION_BINS + 1) with a 1 at index 3 and 0s elsewhere. The <em>+1</em> is for the start token and it will have its own encoding, too. This way each token has its own unique representation.</li>
  <li><em>Model Architecture</em>: We’ll use a simple MLP to predict the next token, the input will be the one-hot encoding of the previous tokens, and the MLP will learn to map the one-hot encodings to the next token.</li>
  <li><em>Output</em>: The MLP will output scores (logits) for each of the NUM_QUANTIZATION_BINS possible pixel tokens, telling us how likely each token is to be the next one.</li>
</ol>

<p>Let’s implement this and see what kind of outputs we can get. Our focus here is to create a basic auto-regressive loop and see how our model can generate an image pixel-by-pixel using direct representation of the pixels.</p>

<div><div><pre><code><span>class</span> <span>OneHotPixelPredictor</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>num_pixel_values</span><span>,</span> <span>context_length</span><span>,</span> <span>hidden_size</span><span>,</span> <span>dropout_rate</span><span>=</span><span>0.25</span><span>):</span>
        <span>super</span><span>(</span><span>OneHotPixelPredictor</span><span>,</span> <span>self</span><span>).</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>num_pixel_values</span> <span>=</span> <span>num_pixel_values</span>
        <span>self</span><span>.</span><span>context_length</span> <span>=</span> <span>context_length</span> <span># This is the length of the context window
</span>        <span>self</span><span>.</span><span>hidden_size</span> <span>=</span> <span>hidden_size</span>
        
        <span># The size of one-hot encoding is num_pixel_values + 1 (+1 for the start token)
</span>        <span>self</span><span>.</span><span>one_hot_vector_size</span> <span>=</span> <span>num_pixel_values</span> <span>+</span> <span>1</span>

        <span># The input to the MLP is the one-hot encoding of the previous tokens
</span>        <span>self</span><span>.</span><span>mlp_input_dim</span> <span>=</span> <span>context_length</span> <span>*</span> <span>(</span><span>self</span><span>.</span><span>one_hot_vector_size</span><span>)</span>

        <span># The MLP has three layers, with a dropout layer in between
</span>        <span>self</span><span>.</span><span>fc1</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>self</span><span>.</span><span>mlp_input_dim</span><span>,</span> <span>hidden_size</span><span>)</span>
        <span>self</span><span>.</span><span>fc2</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_size</span><span>,</span> <span>hidden_size</span><span>)</span>
        <span>self</span><span>.</span><span>dropout</span> <span>=</span> <span>nn</span><span>.</span><span>Dropout</span><span>(</span><span>dropout_rate</span><span>)</span>
        <span>self</span><span>.</span><span>fc_out</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_size</span><span>,</span> <span>num_pixel_values</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x_tokens</span><span>,</span> <span>training</span><span>=</span><span>True</span><span>):</span>
        <span>batch_size</span> <span>=</span> <span>x_tokens</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>

        <span># Get the one-hot encoding of the previous tokens
</span>        <span>one_hot_encodings</span> <span>=</span> <span>F</span><span>.</span><span>one_hot</span><span>(</span><span>x_tokens</span><span>,</span> <span>num_classes</span><span>=</span><span>self</span><span>.</span><span>one_hot_vector_size</span><span>)</span>

        <span># Flatten the one-hot encodings
</span>        <span>flattened_one_hot_encodings</span> <span>=</span> <span>one_hot_encodings</span><span>.</span><span>view</span><span>(</span><span>batch_size</span><span>,</span> <span>-</span><span>1</span><span>).</span><span>float</span><span>()</span>

        <span># Forward through the MLP
</span>        <span>h</span> <span>=</span> <span>F</span><span>.</span><span>relu</span><span>(</span><span>self</span><span>.</span><span>fc1</span><span>(</span><span>flattened_one_hot_encodings</span><span>))</span>
        <span>if</span> <span>training</span><span>:</span>
            <span>h</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>h</span><span>)</span>
        <span>h</span> <span>=</span> <span>F</span><span>.</span><span>relu</span><span>(</span><span>self</span><span>.</span><span>fc2</span><span>(</span><span>h</span><span>))</span>
        <span>if</span> <span>training</span><span>:</span>
            <span>h</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>h</span><span>)</span>
        <span>output_logits</span> <span>=</span> <span>self</span><span>.</span><span>fc_out</span><span>(</span><span>h</span><span>)</span>

        <span>return</span> <span>output_logits</span>

</code></pre></div></div>

<div><div><pre><code><span># Instantiate Model V1
</span><span>model_one_hot_pixel_predictor</span> <span>=</span> <span>OneHotPixelPredictor</span><span>(</span>
    <span>num_pixel_values</span><span>=</span><span>NUM_QUANTIZATION_BINS</span><span>,</span> <span># K
</span>    <span>context_length</span><span>=</span><span>CONTEXT_LENGTH</span><span>,</span>
    <span>hidden_size</span><span>=</span><span>HIDDEN_SIZE</span>
<span>)</span>

<span>print</span><span>(</span><span>&#34;--- Model V1: OneHotPixelPredictor ---&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Pixel Vocabulary Size (K for output classes): </span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  One-Hot Vector Size (K + start_token): </span><span>{</span><span>model_one_hot_pixel_predictor</span><span>.</span><span>one_hot_vector_size</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Context Length: </span><span>{</span><span>CONTEXT_LENGTH</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Hidden Size: </span><span>{</span><span>HIDDEN_SIZE</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Input dimension to MLP: </span><span>{</span><span>CONTEXT_LENGTH</span> <span>*</span> <span>model_one_hot_pixel_predictor</span><span>.</span><span>one_hot_vector_size</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Model V1 parameters: </span><span>{</span><span>sum</span><span>(</span><span>p</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>p</span> <span>in</span> <span>model_one_hot_pixel_predictor</span><span>.</span><span>parameters</span><span>())</span><span>:</span><span>,</span><span>}</span><span>&#34;</span><span>)</span>

</code></pre></div></div>

<h2 id="the-heart-of-autoregression-context-windows">The Heart of Autoregression: Context Windows</h2>

<p>Now for the fun part! How do we actually implement “predicting the next pixel based on previous pixels”?</p>

<p>The key insight is <em>context windows</em>. Instead of using <em>all</em> previous pixels, we use a <em>sliding window</em> of the last <code>k</code> pixels as our context.</p>

<p><em>Mathematically</em>, instead of modeling</p><p>

\[P(x_i | x_1, x_2, ..., x_{i-1})\]

</p><p>we approximate it as:</p><p>

\[P(x_i | x_{i-1}, x_{i-2}, ..., x_{i-k})\]

</p><p>This is called a <em>k-th order Markov assumption</em> - we assume the immediate past is most informative for predicting the future.</p>

<p>What about the first few pixels that don’t have enough history? That’s where our <em>start tokens</em> come in - they’re like saying “this is the beginning of an image” to our model.</p>

<h2 id="building-our-neural-network-predicting-pixel-tokens">Building Our Neural Network: Predicting Pixel Tokens</h2>

<p>Our neural network will now predict a <em>distribution over the possible pixel tokens (integer labels)</em> for the next pixel.</p>

<div><div><pre><code><span>def</span> <span>create_token_training_data</span><span>(</span><span>quantized_dataset</span><span>,</span> <span>context_length</span><span>,</span> <span>start_token_int</span><span>,</span> <span>num_pixel_values</span><span>,</span> <span>max_samples</span><span>=</span><span>1000000</span><span>,</span> <span>max_images_to_process</span><span>=</span><span>None</span><span>,</span> <span>random_drop</span><span>=</span><span>0.8</span><span>):</span>
    <span>&#34;&#34;&#34;
    Create training data (context tokens, target token) for Model V1 directly from a quantized Dataset.
    
    Args:
        quantized_dataset: PyTorch Dataset object yielding quantized image tensors.
        context_length (int): Size of context window.
        start_token_int (int): Integer value for the start/padding token.
        num_pixel_values (int): The number of actual pixel values (K).
        max_samples (int): Maximum number of (context,target) training pairs to generate.
        max_images_to_process (int, optional): Limit the number of images from the dataset to process. Defaults to all.
    
    Returns:
        contexts (Tensor): [N_SAMPLES, context_length] of integer tokens.
        targets (Tensor): [N_SAMPLES] of integer target tokens (0 to K-1).
    &#34;&#34;&#34;</span>
    <span>all_contexts</span> <span>=</span> <span>[]</span>
    <span>all_targets</span> <span>=</span> <span>[]</span>
    <span>samples_collected</span> <span>=</span> <span>0</span>
    
    <span>num_images_to_process</span> <span>=</span> <span>len</span><span>(</span><span>quantized_dataset</span><span>)</span>
    <span>if</span> <span>max_images_to_process</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
        <span>num_images_to_process</span> <span>=</span> <span>min</span><span>(</span><span>num_images_to_process</span><span>,</span> <span>max_images_to_process</span><span>)</span>

    <span>print</span><span>(</span><span>f</span><span>&#34;Generating V1 training data from </span><span>{</span><span>num_images_to_process</span><span>}</span><span> images (max </span><span>{</span><span>max_samples</span><span>:</span><span>,</span><span>}</span><span> samples)...&#34;</span><span>)</span>

    <span># Iterate directly over the Dataset object
</span>    <span>pbar_images</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>num_images_to_process</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Processing Images for V1 Data&#34;</span><span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>pbar_images</span><span>:</span>
        <span>if</span> <span>samples_collected</span> <span>&gt;=</span> <span>max_samples</span><span>:</span>
            <span>pbar_images</span><span>.</span><span>set_description</span><span>(</span><span>f</span><span>&#34;Max samples (</span><span>{</span><span>max_samples</span><span>}</span><span>) reached. Stopping image processing.&#34;</span><span>)</span>
            <span>break</span>
        
        <span>quantized_image_tensor</span><span>,</span> <span>_</span> <span>=</span> <span>quantized_dataset</span><span>[</span><span>i</span><span>]</span> <span># Get i-th image (already quantized)
</span>        <span># quantized_image_tensor shape is [C, H, W], e.g., [1, 28, 28]
</span>        
        <span>flat_token_image</span> <span>=</span> <span>quantized_image_tensor</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>)</span> <span># Flatten to [N_PIXELS]
</span>        <span>n_pixels</span> <span>=</span> <span>flat_token_image</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
            
        <span># Padded sequence for context building
</span>        <span>padded_token_sequence</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span>
            <span>torch</span><span>.</span><span>full</span><span>((</span><span>context_length</span><span>,),</span> <span>start_token_int</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span>
            <span>flat_token_image</span> <span># Should already be .long from quantization
</span>        <span>])</span>
            
        <span>for</span> <span>pixel_idx</span> <span>in</span> <span>range</span><span>(</span><span>n_pixels</span><span>):</span>
            <span>if</span> <span>samples_collected</span> <span>&gt;=</span> <span>max_samples</span><span>:</span>
                <span>break</span> <span># Break inner loop
</span>            
            <span>if</span> <span>random</span><span>.</span><span>random</span><span>()</span> <span>&gt;</span> <span>random_drop</span><span>:</span>
                <span>context</span> <span>=</span> <span>padded_token_sequence</span><span>[</span><span>pixel_idx</span> <span>:</span> <span>pixel_idx</span> <span>+</span> <span>context_length</span><span>]</span>
                <span>target_token</span> <span>=</span> <span>flat_token_image</span><span>[</span><span>pixel_idx</span><span>]</span>
                    
                <span>all_contexts</span><span>.</span><span>append</span><span>(</span><span>context</span><span>)</span>    
                <span>all_targets</span><span>.</span><span>append</span><span>(</span><span>target_token</span><span>.</span><span>unsqueeze</span><span>(</span><span>0</span><span>))</span>
                <span>samples_collected</span> <span>+=</span> <span>1</span>
    
    <span>pbar_images</span><span>.</span><span>close</span><span>()</span> <span># Close the progress bar for images
</span>
    <span>if</span> <span>not</span> <span>all_contexts</span><span>:</span>
        <span>print</span><span>(</span><span>&#34;Warning: No training samples collected. Check max_samples or dataset processing.&#34;</span><span>)</span>
        <span># Return empty tensors with correct number of dimensions to avoid errors later
</span>        <span>return</span> <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>,</span> <span>context_length</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span> <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>)</span>

    <span>contexts_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>all_contexts</span><span>).</span><span>long</span><span>()</span>
    <span>targets_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>all_targets</span><span>).</span><span>long</span><span>()</span>
    
    <span>indices</span> <span>=</span> <span>torch</span><span>.</span><span>randperm</span><span>(</span><span>len</span><span>(</span><span>contexts_tensor</span><span>))</span>
    <span>contexts_tensor</span> <span>=</span> <span>contexts_tensor</span><span>[</span><span>indices</span><span>]</span>
    <span>targets_tensor</span> <span>=</span> <span>targets_tensor</span><span>[</span><span>indices</span><span>]</span>
    
    <span>print</span><span>(</span><span>f</span><span>&#34;Generated </span><span>{</span><span>len</span><span>(</span><span>contexts_tensor</span><span>)</span><span>:</span><span>,</span><span>}</span><span> V1 training pairs.&#34;</span><span>)</span>
    <span>return</span> <span>contexts_tensor</span><span>,</span> <span>targets_tensor</span>


</code></pre></div></div>

<div><div><pre><code>

<span>print</span><span>(</span><span>&#34;--- Preparing Training Data for Model V1 (OneHotPixelPredictor) ---&#34;</span><span>)</span>
<span># Use the new function specific to Model V1 data requirements
</span><span>train_contexts</span><span>,</span> <span>train_targets</span> <span>=</span> <span>create_token_training_data</span><span>(</span>
    <span>trainset_quantized</span><span>,</span> <span># Pass the Dataset object
</span>    <span>CONTEXT_LENGTH</span><span>,</span> 
    <span>START_TOKEN_VALUE_INT</span><span>,</span> 
    <span>NUM_QUANTIZATION_BINS</span><span>,</span>
    <span>max_samples</span><span>=</span><span>MAX_SAMPLES</span>
<span>)</span>

<span>print</span><span>(</span> <span>&#34;</span><span>\n</span><span>Model V1 - Data Shapes:&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  train_contexts shape: </span><span>{</span><span>train_contexts</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_contexts</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  train_targets shape: </span><span>{</span><span>train_targets</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_targets</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span><span>)</span>
<span>assert</span> <span>train_targets</span><span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>NUM_QUANTIZATION_BINS</span><span>,</span> <span>&#34;Target tokens for V1 exceed K-1&#34;</span>
<span>assert</span> <span>train_targets</span><span>.</span><span>min</span><span>()</span> <span>&gt;=</span> <span>0</span><span>,</span> <span>&#34;Target tokens for V1 are negative&#34;</span>

</code></pre></div></div>

<h3 id="training-our-onehotpixelpredictor-model-v1">Training Our <code>OneHotPixelPredictor</code> (Model V1)</h3>

<p>With our <code>model_one_hot_pixel_predictor</code> (Model V1) defined and the training data (<code>train_contexts</code>, <code>train_targets</code>) prepared, we’re ready to train.</p>

<ul>
  <li><em>Loss Function:</em> Since our model outputs logits for <code>NUM_QUANTIZATION_BINS</code> possible pixel token classes, and our target is a single integer class label, we’ll use <code>CrossEntropyLoss</code>. This loss function is ideal for multi-class classification tasks as it combines a LogSoftmax layer and Negative Log-Likelihood Loss in one step.</li>
  <li><em>Optimizer:</em> We’ll use AdamW, a common and effective optimizer known for good performance and weight decay.</li>
</ul>

<p>Let’s kick off the training for Model V1!</p>

<div><div><pre><code>
<span># --- Model V1, Data, Loss, Optimizer ---
</span><span>model_one_hot_pixel_predictor</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_contexts</span> <span>=</span> <span>train_contexts</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_targets</span> <span>=</span> <span>train_targets</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>

<span>criterion</span> <span>=</span> <span>nn</span><span>.</span><span>CrossEntropyLoss</span><span>()</span>
<span>optimizer</span> <span>=</span> <span>optim</span><span>.</span><span>AdamW</span><span>(</span><span>model_one_hot_pixel_predictor</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>LEARNING_RATE</span><span>)</span>

<span>n_samples</span> <span>=</span> <span>len</span><span>(</span><span>train_contexts</span><span>)</span>
<span>if</span> <span>n_samples</span> <span>==</span> <span>0</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;No training samples available for Model V1. Skipping training.&#34;</span><span>)</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Training Model V1 (OneHotPixelPredictor) on </span><span>{</span><span>n_samples</span><span>:</span><span>,</span><span>}</span><span> samples for </span><span>{</span><span>EPOCHS</span><span>}</span><span> epochs.&#34;</span><span>)</span>
    <span>print</span><span>(</span><span>f</span><span>&#34;Predicting one of </span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span> pixel tokens.&#34;</span><span>)</span>

    <span># --- Training Loop for Model V1 ---
</span>    <span>epoch_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>EPOCHS</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Model V1 Training Epochs&#34;</span><span>,</span> <span>position</span><span>=</span><span>0</span><span>,</span> <span>leave</span><span>=</span><span>True</span><span>)</span>
    <span>for</span> <span>epoch</span> <span>in</span> <span>epoch_pbar</span><span>:</span>
        <span>model_one_hot_pixel_predictor</span><span>.</span><span>train</span><span>()</span> <span># Set model to training mode
</span>        <span>epoch_loss</span> <span>=</span> <span>0.0</span>
        <span>num_batches</span> <span>=</span> <span>0</span>
        
        <span># Shuffle indices for each epoch for batching from the large tensor
</span>        <span>indices</span> <span>=</span> <span>torch</span><span>.</span><span>randperm</span><span>(</span><span>n_samples</span><span>,</span> <span>device</span><span>=</span><span>device</span><span>)</span>
        
        <span># Calculate total number of batches for the inner progress bar
</span>        <span>total_batches_in_epoch</span> <span>=</span> <span>(</span><span>n_samples</span> <span>+</span> <span>BATCH_SIZE_TRAIN</span> <span>-</span> <span>1</span><span>)</span> <span>//</span> <span>BATCH_SIZE_TRAIN</span>
        <span>batch_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>0</span><span>,</span> <span>n_samples</span><span>,</span> <span>BATCH_SIZE_TRAIN</span><span>),</span> 
                             <span>desc</span><span>=</span><span>f</span><span>&#34;Epoch </span><span>{</span><span>epoch</span><span>+</span><span>1</span><span>}</span><span>/</span><span>{</span><span>EPOCHS</span><span>}</span><span>&#34;</span><span>,</span> 
                             <span>position</span><span>=</span><span>1</span><span>,</span> <span>leave</span><span>=</span><span>False</span><span>,</span> 
                             <span>total</span><span>=</span><span>total_batches_in_epoch</span><span>)</span>

        <span>for</span> <span>start_idx</span> <span>in</span> <span>batch_pbar</span><span>:</span>
            <span>end_idx</span> <span>=</span> <span>min</span><span>(</span><span>start_idx</span> <span>+</span> <span>BATCH_SIZE_TRAIN</span><span>,</span> <span>n_samples</span><span>)</span>
            <span>if</span> <span>start_idx</span> <span>==</span> <span>end_idx</span><span>:</span> <span>continue</span> <span># Skip if batch is empty
</span>
            <span>batch_indices</span> <span>=</span> <span>indices</span><span>[</span><span>start_idx</span><span>:</span><span>end_idx</span><span>]</span>
            
            <span>batch_context_tokens</span> <span>=</span> <span>train_contexts</span><span>[</span><span>batch_indices</span><span>]</span>  <span># Integer tokens
</span>            <span>batch_target_tokens</span> <span>=</span> <span>train_targets</span><span>[</span><span>batch_indices</span><span>]</span>    <span># Integer tokens (0 to K-1)
</span>            
            <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
            
            <span># Model V1 forward pass - x_tokens are integer tokens, training=True
</span>            <span>output_logits</span> <span>=</span> <span>model_one_hot_pixel_predictor</span><span>(</span><span>batch_context_tokens</span><span>,</span> <span>training</span><span>=</span><span>True</span><span>)</span> 
            
            <span>loss</span> <span>=</span> <span>criterion</span><span>(</span><span>output_logits</span><span>,</span> <span>batch_target_tokens</span><span>)</span>
            <span>loss</span><span>.</span><span>backward</span><span>()</span>
            <span>optimizer</span><span>.</span><span>step</span><span>()</span>
            
            <span>epoch_loss</span> <span>+=</span> <span>loss</span><span>.</span><span>item</span><span>()</span>
            <span>num_batches</span> <span>+=</span> <span>1</span>
            
            <span>if</span> <span>num_batches</span> <span>%</span> <span>50</span> <span>==</span> <span>0</span><span>:</span> <span># Update progress bar postfix less frequently
</span>                 <span>batch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>loss</span><span>=</span><span>f</span><span>&#34;</span><span>{</span><span>loss</span><span>.</span><span>item</span><span>()</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>&#34;</span><span>)</span>
        
        <span>if</span> <span>num_batches</span> <span>&gt;</span> <span>0</span><span>:</span> <span># Avoid division by zero if n_samples was small
</span>            <span>avg_loss</span> <span>=</span> <span>epoch_loss</span> <span>/</span> <span>num_batches</span>
            <span>epoch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>avg_loss</span><span>=</span><span>f</span><span>&#34;</span><span>{</span><span>avg_loss</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>&#34;</span><span>)</span>
        <span>else</span><span>:</span>
            <span>epoch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>avg_loss</span><span>=</span><span>&#34;N/A&#34;</span><span>)</span>

</code></pre></div></div>

<h2 id="generating-images-with-model-v1">Generating Images with Model V1</h2>

<p>After training our <code>OneHotPixelPredictor</code>, let’s see what kind of images it can generate. The autoregressive generation process involves:</p>

<ol>
  <li>Initializing a context window, filled with our special <code>START_TOKEN_VALUE_INT</code>.</li>
  <li>Feeding this current context to the trained model to obtain logits (scores) for each possible next pixel token.</li>
  <li>Converting these logits into a probability distribution using the softmax function. We can also apply a “temperature” parameter here to control the randomness of sampling:
    <ul>
      <li><em>Temperature = 1.0:</em> Standard sampling according to the model’s learned probabilities.</li>
      <li><em>Temperature &lt; 1.0 (e.g., 0.7):</em> Makes the model’s choices “sharper” or more deterministic, favoring higher-probability tokens (more greedy).</li>
      <li><em>Temperature &gt; 1.0 (e.g., 1.2):</em> Makes the sampling more random, increasing diversity but potentially at the cost of coherence.</li>
    </ul>
  </li>
  <li>Sampling the next pixel token from this probability distribution.</li>
  <li>Appending this newly sampled token to our sequence of generated pixels.</li>
  <li>Updating the context window by shifting it (removing the oldest token) and adding the newly generated token.</li>
  <li>Repeating steps 2-6 until all <code>N_PIXELS</code> for a full image have been generated.</li>
  <li>Finally, de-quantizing the sequence of generated integer tokens back into visualizable grayscale values.</li>
</ol>

<p>Let’s see how our Model V1, which uses one-hot encoded tokens and no explicit positional information, performs at this task.</p>

<div><div><pre><code><span>def</span> <span>generate_image_v1</span><span>(</span><span>model</span><span>,</span> <span>context_length</span><span>,</span> <span>start_token_int</span><span>,</span> <span>num_pixel_values_k</span><span>,</span> 
                      <span>img_size</span><span>,</span> <span>current_device</span><span>,</span> <span>temperature</span><span>=</span><span>1.0</span><span>):</span>
    <span>&#34;&#34;&#34;
    Generates a single image using Model V1 (OneHotPixelPredictor).
    Args:
        model: The trained OneHotPixelPredictor model.
        context_length (int): Length of the context window.
        start_token_int (int): Integer value for the start token.
        num_pixel_values_k (int): K, the number of possible actual pixel tokens (e.g., NUM_QUANTIZATION_BINS).
        img_size (int): Dimension of the square image.
        current_device (torch.device): Device to run generation on.
        temperature (float): Softmax temperature for sampling.
    Returns:
        numpy.ndarray: De-quantized grayscale image.
        list: List of generated integer pixel tokens.
    &#34;&#34;&#34;</span>
    <span>model</span><span>.</span><span>eval</span><span>()</span> <span># Set model to evaluation mode
</span>    <span>model</span><span>.</span><span>to</span><span>(</span><span>current_device</span><span>)</span> <span># Ensure model is on the correct device
</span>    
    <span>total_pixels_to_generate</span> <span>=</span> <span>img_size</span> <span>*</span> <span>img_size</span>
    
    <span># Initialize context with integer start tokens on the correct device
</span>    <span>current_context_tokens_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>full</span><span>((</span><span>1</span><span>,</span> <span>context_length</span><span>),</span> <span>start_token_int</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>current_device</span><span>)</span>
    <span>generated_pixel_tokens_list</span> <span>=</span> <span>[]</span>
    
    <span># tqdm for pixel generation can be slow if generating many images one by one. 
</span>    <span># Consider removing if it clutters output too much for multiple image generations.
</span>    <span># For a single image generation in a blog, it&#39;s illustrative.
</span>    <span>pixel_gen_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>total_pixels_to_generate</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Generating V1 Image Pixels&#34;</span><span>,</span> <span>leave</span><span>=</span><span>False</span><span>,</span> <span>position</span><span>=</span><span>0</span><span>,</span> <span>disable</span><span>=</span><span>True</span><span>)</span> <span># Disable for multi-image plot
</span>
    <span>with</span> <span>torch</span><span>.</span><span>no_grad</span><span>():</span>
        <span>for</span> <span>_</span> <span>in</span> <span>pixel_gen_pbar</span><span>:</span>
            <span># Model V1 forward pass - x_tokens are integer tokens, training=False
</span>            <span># Input tensor shape: [1, context_length]
</span>            <span>output_logits</span> <span>=</span> <span>model</span><span>(</span><span>current_context_tokens_tensor</span><span>,</span> <span>training</span><span>=</span><span>False</span><span>)</span> <span># Logits: [1, NUM_QUANTIZATION_BINS]
</span>            
            <span># Apply temperature and get probabilities
</span>            <span>probabilities</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>output_logits</span> <span>/</span> <span>temperature</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>).</span><span>squeeze</span><span>()</span> <span># Squeeze to shape [NUM_QUANTIZATION_BINS]
</span>            
            <span># Sample the next pixel token
</span>            <span>next_pixel_token</span> <span>=</span> <span>torch</span><span>.</span><span>multinomial</span><span>(</span><span>probabilities</span><span>,</span> <span>num_samples</span><span>=</span><span>1</span><span>).</span><span>item</span><span>()</span> <span># .item() gets Python number
</span>            
            <span>generated_pixel_tokens_list</span><span>.</span><span>append</span><span>(</span><span>next_pixel_token</span><span>)</span>
            
            <span># Update context: shift and append the new token
</span>            <span># Keep as a tensor on the correct device
</span>            <span>new_token_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>next_pixel_token</span><span>]],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>current_device</span><span>)</span>
            <span>current_context_tokens_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>current_context_tokens_tensor</span><span>[:,</span> <span>1</span><span>:],</span> <span>new_token_tensor</span><span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
    
    <span># Convert list of Python numbers to a tensor for dequantization
</span>    <span>generated_tokens_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>generated_pixel_tokens_list</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>)</span> <span># Create on CPU then move if needed by dequantize
</span>    <span>dequantized_image_array</span> <span>=</span> <span>dequantize_tensor</span><span>(</span><span>generated_tokens_tensor</span><span>,</span> <span>num_pixel_values_k</span><span>).</span><span>numpy</span><span>().</span><span>reshape</span><span>(</span><span>img_size</span><span>,</span> <span>img_size</span><span>)</span>
    
    <span>return</span> <span>dequantized_image_array</span><span>,</span> <span>generated_pixel_tokens_list</span>

<span># --- Generate and Visualize Multiple Images from Model V1 ---
</span><span>if</span> <span>n_samples</span> <span>&gt;</span> <span>0</span><span>:</span> <span># Only generate if model was trained
</span>    <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>--- Generating Images from Model V1 (OneHotPixelPredictor) ---&#34;</span><span>)</span>
    <span>n_images_to_generate</span> <span>=</span> <span>25</span> <span># Generate a 5x5 grid
</span>    <span>fig_gen</span><span>,</span> <span>axes_gen</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>8</span><span>,</span> <span>8</span><span>))</span> <span># Slightly larger figure
</span>    <span>fig_gen</span><span>.</span><span>suptitle</span><span>(</span><span>f</span><span>&#34;Model V1 Generated Digits (</span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span> Bins, One-Hot, No Pos.Emb.)&#34;</span><span>,</span> <span>fontsize</span><span>=</span><span>14</span><span>)</span>

    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>n_images_to_generate</span><span>):</span>
        <span>row</span><span>,</span> <span>col</span> <span>=</span> <span>i</span> <span>//</span> <span>5</span><span>,</span> <span>i</span> <span>%</span> <span>5</span>
        <span>ax</span> <span>=</span> <span>axes_gen</span><span>[</span><span>row</span><span>,</span> <span>col</span><span>]</span>
        
        <span>dequantized_img</span><span>,</span> <span>_</span> <span>=</span> <span>generate_image_v1</span><span>(</span>
            <span>model_one_hot_pixel_predictor</span><span>,</span> 
            <span>CONTEXT_LENGTH</span><span>,</span> 
            <span>START_TOKEN_VALUE_INT</span><span>,</span> 
            <span>NUM_QUANTIZATION_BINS</span><span>,</span> 
            <span>IMG_SIZE</span><span>,</span> 
            <span>device</span><span>,</span> <span># Pass the globally defined device
</span>            <span>temperature</span><span>=</span><span>1.0</span> <span># You can experiment with temperature
</span>        <span>)</span>
        <span>ax</span><span>.</span><span>imshow</span><span>(</span><span>dequantized_img</span><span>,</span> <span>cmap</span><span>=</span><span>&#39;gray&#39;</span><span>,</span> <span>vmin</span><span>=</span><span>0</span><span>,</span> <span>vmax</span><span>=</span><span>1</span><span>)</span>
        <span># ax.set_title(f&#34;V1 #{i+1}&#34;, fontsize=8) # Title per image can be noisy, optional
</span>        <span>ax</span><span>.</span><span>axis</span><span>(</span><span>&#39;off&#39;</span><span>)</span>

    <span>plt</span><span>.</span><span>tight_layout</span><span>(</span><span>rect</span><span>=</span><span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>0.95</span><span>])</span> <span># Adjust layout to make space for suptitle
</span>    <span>plt</span><span>.</span><span>show</span><span>()</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;Skipping Model V1 image generation as no training samples were available.&#34;</span><span>)</span>

</code></pre></div></div>

<p><img src="https://tunahansalih.github.io/images/blog/AR-part1/Part1_15_1.png" alt="png"/></p>

<h3 id="model-v1-initial-results-and-observations">Model V1: Initial Results and Observations</h3>

<p>Alright, let’s take a look at the first batch of images generated by our <code>OneHotPixelPredictor</code> (Model V1)!</p>

<p>Looking at these generated samples, here are a few key observations:</p>

<ul>
  <li><em>Recognizable Shapes?</em> Unfortunately, not really. The images don’t form coherent, recognizable MNIST digits. Instead, we see patterns that are quite abstract and noisy.</li>
  <li><em>Dominant Patterns: Horizontal Streaks:</em> A very prominent feature is the presence of horizontal streaks or bands of white (or lighter) pixels against a predominantly dark background. It seems the model has learned some very local, short-range correlations, possibly related to the raster scan order (e.g., “if the last few pixels were white, the next one is also likely white for a short stretch”).</li>
  <li><em>Repetitive Textures:</em> The generations have a somewhat repetitive, textural quality due to these streaks. There isn’t much variation in the <em>types</em> of structures being formed beyond these horizontal elements.</li>
  <li><em>Lack of Global Coherence:</em> Crucially, there’s an almost complete absence of global structure. The horizontal streaks appear somewhat randomly placed across the image canvas and don’t coordinate to form larger, meaningful shapes like the curves and loops of digits. The model doesn’t seem to have a <em>plan</em> for the overall image.</li>
  <li><em>Impact of Quantization Visible:</em> The blocky nature of the 16 quantization bins is apparent, which is expected.</li>
</ul>

<p><em>Why are we seeing these kinds of results?</em> Two key aspects of our very basic Model V1 are likely major contributors:</p>

<p><em>No Positional Information:</em> Our current MLP processes a flat window of context tokens. It has <em>no explicit information</em> about <em>where</em> in the 2D image it is currently trying to predict a pixel. Is it at the top-left, the center, the bottom-right? Without this spatial awareness, it’s incredibly hard for the model to learn to, for example, start a stroke at a particular location, curve it appropriately, and end it correctly to form part of a digit. The horizontal streaks might be a consequence of learning simple local rules that get repeated because the model doesn’t know when or where to change its behavior based on image location.</p>

<p>These initial results, while not producing digits, are incredibly valuable. They highlight a key limitation and clearly motivate our next step. <em>What if we could give our model a sense of ‘location’ for each pixel it predicts?</em> This is precisely what we’ll explore by <em>introducing positional encodings in Model V2.</em> We’ll keep the one-hot encoding for pixel tokens for now and see just how much impact adding spatial information can have.</p>

<h2 id="model-v2-giving-our-predictor-a-sense-of-location-with-positional-encodings">Model V2: Giving Our Predictor a Sense of Location with Positional Encodings</h2>

<p>Our Model V1 struggled to create coherent images, and we hypothesized that a major reason was its lack of spatial awareness – it didn’t know <em>where</em> in the image it was predicting a pixel.</p>

<p>For Model V2, we’ll directly address this by introducing <em>positional encodings</em>. The idea is to provide the model with explicit information about the (row, column) coordinates of the pixel it’s currently trying to predict.</p>

<p>How will we do this?</p>

<ol>
  <li><em>Learnable Positional Embeddings:</em> We’ll create two separate embedding layers, one for row positions (0 to <code>IMG_SIZE-1</code>) and one for column positions (0 to <code>IMG_SIZE-1</code>). Each position (e.g., row 5, column 10) will be mapped to a learnable vector (its embedding).</li>
  <li><em>Concatenation:</em> For each prediction step, we’ll determine the row and column of the target pixel. We’ll fetch their respective embeddings. These two positional embedding vectors will then be concatenated with the (still one-hot encoded) context window of previous pixel tokens.</li>
  <li><em>MLP Input:</em> This richer, combined representation (one-hot context + row embedding + column embedding) will be fed into our MLP.</li>
</ol>

<p>Our pixel tokens themselves will still be one-hot encoded for now. The main change in Model V2 is adding this crucial positional signal. The hypothesis is that by knowing <em>where</em> it’s operating, the model can learn location-dependent rules and generate more structured images. Let’s see!</p>

<div><div><pre><code><span>class</span> <span>OneHotPixelPredictorWithPosition</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>num_pixel_values</span><span>,</span> <span>context_length</span><span>,</span> <span>hidden_size</span><span>,</span> 
                 <span>img_size</span><span>,</span> <span>pos_embedding_dim</span><span>,</span> <span>dropout_rate</span><span>=</span><span>0.25</span><span>):</span>
        <span>super</span><span>(</span><span>OneHotPixelPredictorWithPosition</span><span>,</span> <span>self</span><span>).</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>num_pixel_values</span> <span>=</span> <span>num_pixel_values</span> <span># K
</span>        <span>self</span><span>.</span><span>context_length</span> <span>=</span> <span>context_length</span>
        <span>self</span><span>.</span><span>img_size</span> <span>=</span> <span>img_size</span> <span># e.g., 28 for MNIST
</span>        <span>self</span><span>.</span><span>pos_embedding_dim</span> <span>=</span> <span>pos_embedding_dim</span> <span># Dimension for each positional embedding (row/col)
</span>
        <span># For one-hot encoding of context tokens
</span>        <span>self</span><span>.</span><span>one_hot_vector_size</span> <span>=</span> <span>num_pixel_values</span> <span>+</span> <span>1</span> <span># K + 1 (for start token)
</span>        <span>one_hot_context_dim</span> <span>=</span> <span>context_length</span> <span>*</span> <span>self</span><span>.</span><span>one_hot_vector_size</span>

        <span># Learnable absolute position embeddings
</span>        <span># One embedding vector for each possible row, one for each possible column
</span>        <span>self</span><span>.</span><span>row_pos_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>num_embeddings</span><span>=</span><span>img_size</span><span>,</span> <span>embedding_dim</span><span>=</span><span>pos_embedding_dim</span><span>)</span>
        <span>self</span><span>.</span><span>col_pos_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>num_embeddings</span><span>=</span><span>img_size</span><span>,</span> <span>embedding_dim</span><span>=</span><span>pos_embedding_dim</span><span>)</span>
        
        <span># Total input dimension to the MLP:
</span>        <span># (one-hot context) + (row_pos_embed) + (col_pos_embed)
</span>        <span>self</span><span>.</span><span>mlp_input_dim</span> <span>=</span> <span>one_hot_context_dim</span> <span>+</span> <span>(</span><span>2</span> <span>*</span> <span>self</span><span>.</span><span>pos_embedding_dim</span><span>)</span>
        
        <span>self</span><span>.</span><span>fc1</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>self</span><span>.</span><span>mlp_input_dim</span><span>,</span> <span>hidden_size</span><span>)</span>
        <span>self</span><span>.</span><span>fc2</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_size</span><span>,</span> <span>hidden_size</span><span>)</span>
        <span>self</span><span>.</span><span>dropout</span> <span>=</span> <span>nn</span><span>.</span><span>Dropout</span><span>(</span><span>dropout_rate</span><span>)</span>
        <span>self</span><span>.</span><span>fc_out</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_size</span><span>,</span> <span>self</span><span>.</span><span>num_pixel_values</span><span>)</span> <span># Outputs K logits
</span>
    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x_context_tokens</span><span>,</span> <span>pixel_positions_flat</span><span>,</span> <span>training</span><span>=</span><span>True</span><span>):</span>
        <span>&#34;&#34;&#34;
        Args:
            x_context_tokens (Tensor): Batch of context windows. Shape: [batch_size, CONTEXT_LENGTH]. Integer tokens.
            pixel_positions_flat (Tensor): Absolute flat positions (0 to N_PIXELS-1) in the image 
                                           for each sample&#39;s target pixel. Shape: [batch_size].
            training (bool): Whether in training mode.
        &#34;&#34;&#34;</span>
        <span>batch_size</span> <span>=</span> <span>x_context_tokens</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>

        <span># 1. One-hot encode context tokens
</span>        <span>one_hot_context</span> <span>=</span> <span>F</span><span>.</span><span>one_hot</span><span>(</span><span>x_context_tokens</span><span>,</span> <span>num_classes</span><span>=</span><span>self</span><span>.</span><span>one_hot_vector_size</span><span>).</span><span>float</span><span>()</span>
        <span>flattened_one_hot_context</span> <span>=</span> <span>one_hot_context</span><span>.</span><span>view</span><span>(</span><span>batch_size</span><span>,</span> <span>-</span><span>1</span><span>)</span>
        
        <span># 2. Get positional embeddings
</span>        <span># Convert flat positions to row and column indices
</span>        <span>rows</span> <span>=</span> <span>pixel_positions_flat</span> <span>//</span> <span>self</span><span>.</span><span>img_size</span> <span># Integer division gives row index
</span>        <span>cols</span> <span>=</span> <span>pixel_positions_flat</span> <span>%</span> <span>self</span><span>.</span><span>img_size</span>  <span># Modulo gives column index
</span>        
        <span>row_embeds</span> <span>=</span> <span>self</span><span>.</span><span>row_pos_embedding</span><span>(</span><span>rows</span><span>)</span> <span># Shape: [batch_size, pos_embedding_dim]
</span>        <span>col_embeds</span> <span>=</span> <span>self</span><span>.</span><span>col_pos_embedding</span><span>(</span><span>cols</span><span>)</span> <span># Shape: [batch_size, pos_embedding_dim]
</span>        
        <span># 3. Concatenate all features
</span>        <span>combined_features</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span>
            <span>flattened_one_hot_context</span><span>,</span>
            <span>row_embeds</span><span>,</span>
            <span>col_embeds</span>
        <span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span> <span># Concatenate along the feature dimension
</span>        
        <span># 4. Forward through MLP
</span>        <span>h</span> <span>=</span> <span>F</span><span>.</span><span>relu</span><span>(</span><span>self</span><span>.</span><span>fc1</span><span>(</span><span>combined_features</span><span>))</span>
        <span>if</span> <span>training</span><span>:</span>
            <span>h</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>h</span><span>)</span>
        <span>h</span> <span>=</span> <span>F</span><span>.</span><span>relu</span><span>(</span><span>self</span><span>.</span><span>fc2</span><span>(</span><span>h</span><span>))</span>
        <span>if</span> <span>training</span><span>:</span>
            <span>h</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>h</span><span>)</span>
        <span>output_logits</span> <span>=</span> <span>self</span><span>.</span><span>fc_out</span><span>(</span><span>h</span><span>)</span> <span># Shape: [batch_size, NUM_QUANTIZATION_BINS]
</span>        
        <span>return</span> <span>output_logits</span>

</code></pre></div></div>

<div><div><pre><code>
<span># Instantiate Model V2
</span><span>model_onehot_with_pos</span> <span>=</span> <span>OneHotPixelPredictorWithPosition</span><span>(</span>
    <span>num_pixel_values</span><span>=</span><span>NUM_QUANTIZATION_BINS</span><span>,</span>    <span># K
</span>    <span>context_length</span><span>=</span><span>CONTEXT_LENGTH</span><span>,</span>
    <span>hidden_size</span><span>=</span><span>HIDDEN_SIZE</span><span>,</span>
    <span>img_size</span><span>=</span><span>IMG_SIZE</span><span>,</span>
    <span>pos_embedding_dim</span><span>=</span><span>POS_EMBEDDING_DIM</span>
<span>)</span>

<span>print</span><span>(</span><span>&#34;--- Model V2: OneHotPixelPredictorWithPosition ---&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Pixel Vocabulary Size (K for output classes): </span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  One-Hot Vector Size for context tokens: </span><span>{</span><span>model_onehot_with_pos</span><span>.</span><span>one_hot_vector_size</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Context Length: </span><span>{</span><span>CONTEXT_LENGTH</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Image Size: </span><span>{</span><span>IMG_SIZE</span><span>}</span><span>x</span><span>{</span><span>IMG_SIZE</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Positional Embedding Dimension (per row/col): </span><span>{</span><span>POS_EMBEDDING_DIM</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Hidden Size: </span><span>{</span><span>HIDDEN_SIZE</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Total MLP Input Dimension: </span><span>{</span><span>model_onehot_with_pos</span><span>.</span><span>mlp_input_dim</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Model V2 parameters: </span><span>{</span><span>sum</span><span>(</span><span>p</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>p</span> <span>in</span> <span>model_onehot_with_pos</span><span>.</span><span>parameters</span><span>())</span><span>:</span><span>,</span><span>}</span><span>&#34;</span><span>)</span>


</code></pre></div></div>

<h3 id="preparing-training-data-for-model-v2-with-positional-information">Preparing Training Data for Model V2 (with Positional Information)</h3>

<p>Our Model V2, <code>OneHotPixelPredictorWithPosition</code>, now expects not only the context window of previous tokens but also the absolute position of the pixel it is trying to predict.</p>

<p>This means we need to use our data preparation function to output three things for each training sample:</p>

<ol>
  <li>The context window of integer tokens (<code>x_context_tokens</code>).</li>
  <li>The target integer token (<code>target_token</code>).</li>
  <li>The absolute “flat” position of the target token in the image (e.g., an integer from 0 to <code>N_PIXELS-1</code>).</li>
</ol>

<p>Our model will then internally convert this flat position into row and column indices to fetch the appropriate positional embeddings.</p>

<p>Let’s ensure our data generation function provides this. We’ll use the version that generates context, target, and position.</p>

<div><div><pre><code><span>def</span> <span>create_randomized_token_training_data_with_pos</span><span>(</span><span>quantized_dataset</span><span>,</span> <span>context_length</span><span>,</span> <span>start_token_int</span><span>,</span> <span>num_pixel_values</span><span>,</span> 
                                       <span>img_total_pixels</span><span>,</span> <span>max_samples</span><span>=</span><span>100000</span><span>,</span> <span>max_images_to_process</span><span>=</span><span>None</span><span>,</span> <span>random_drop</span><span>=</span><span>0.8</span><span>):</span>
    <span>&#34;&#34;&#34;
    Create training data (context tokens, target token, target position)
    for models requiring positional information.
    &#34;&#34;&#34;</span>
    <span>all_contexts</span> <span>=</span> <span>[]</span>
    <span>all_targets</span> <span>=</span> <span>[]</span>
    <span>all_positions</span> <span>=</span> <span>[]</span> <span># To store the absolute flat position of the target pixel
</span>    <span>samples_collected</span> <span>=</span> <span>0</span>
    
    <span>num_images_to_process</span> <span>=</span> <span>len</span><span>(</span><span>quantized_dataset</span><span>)</span>
    <span>if</span> <span>max_images_to_process</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
        <span>num_images_to_process</span> <span>=</span> <span>min</span><span>(</span><span>num_images_to_process</span><span>,</span> <span>max_images_to_process</span><span>)</span>

    <span>print</span><span>(</span><span>f</span><span>&#34;Generating V2 training data (contexts, targets, positions) from </span><span>{</span><span>num_images_to_process</span><span>}</span><span> images (max </span><span>{</span><span>max_samples</span><span>:</span><span>,</span><span>}</span><span> samples)...&#34;</span><span>)</span>

    <span>pbar_images</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>num_images_to_process</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Processing Images for V2 Data&#34;</span><span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>pbar_images</span><span>:</span>
        <span>if</span> <span>samples_collected</span> <span>&gt;=</span> <span>max_samples</span><span>:</span>
            <span>pbar_images</span><span>.</span><span>set_description</span><span>(</span><span>f</span><span>&#34;Max samples (</span><span>{</span><span>max_samples</span><span>}</span><span>) reached.&#34;</span><span>)</span>
            <span>break</span>
        
        <span>quantized_image_tensor</span><span>,</span> <span>_</span> <span>=</span> <span>quantized_dataset</span><span>[</span><span>i</span><span>]</span>
        <span>flat_token_image</span> <span>=</span> <span>quantized_image_tensor</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>)</span>
        <span>n_pixels_in_image</span> <span>=</span> <span>flat_token_image</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span> <span># Should be img_total_pixels
</span>            
        <span>padded_token_sequence</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span>
            <span>torch</span><span>.</span><span>full</span><span>((</span><span>context_length</span><span>,),</span> <span>start_token_int</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span>
            <span>flat_token_image</span>
        <span>])</span>
            
        <span>for</span> <span>pixel_idx_in_image</span> <span>in</span> <span>range</span><span>(</span><span>n_pixels_in_image</span><span>):</span> <span># This is the absolute flat position from 0 to N_PIXELS-1
</span>            <span>if</span> <span>samples_collected</span> <span>&gt;=</span> <span>max_samples</span><span>:</span>
                <span>break</span>
            
            <span>if</span> <span>random</span><span>.</span><span>random</span><span>()</span> <span>&gt;</span> <span>random_drop</span><span>:</span>
                <span>context</span> <span>=</span> <span>padded_token_sequence</span><span>[</span><span>pixel_idx_in_image</span> <span>:</span> <span>pixel_idx_in_image</span> <span>+</span> <span>context_length</span><span>]</span>
                <span>target_token</span> <span>=</span> <span>flat_token_image</span><span>[</span><span>pixel_idx_in_image</span><span>]</span>
                
                <span>all_contexts</span><span>.</span><span>append</span><span>(</span><span>context</span><span>)</span>
                <span>all_targets</span><span>.</span><span>append</span><span>(</span><span>target_token</span><span>.</span><span>unsqueeze</span><span>(</span><span>0</span><span>))</span>
                <span>all_positions</span><span>.</span><span>append</span><span>(</span><span>torch</span><span>.</span><span>tensor</span><span>([</span><span>pixel_idx_in_image</span><span>],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>))</span> <span># Store flat absolute position
</span>                <span>samples_collected</span> <span>+=</span> <span>1</span>
    
    <span>pbar_images</span><span>.</span><span>close</span><span>()</span>

    <span>if</span> <span>not</span> <span>all_contexts</span><span>:</span>
        <span>print</span><span>(</span><span>&#34;Warning: No V2 training samples collected.&#34;</span><span>)</span>
        <span>return</span> <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>,</span> <span>context_length</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span> <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span> <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>)</span>

    <span>contexts_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>all_contexts</span><span>).</span><span>long</span><span>()</span>
    <span>targets_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>all_targets</span><span>).</span><span>long</span><span>()</span>
    <span>positions_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>all_positions</span><span>).</span><span>long</span><span>().</span><span>squeeze</span><span>()</span> <span># Squeeze to make it [N_SAMPLES] if it became [N_SAMPLES, 1]
</span>    
    <span>indices</span> <span>=</span> <span>torch</span><span>.</span><span>randperm</span><span>(</span><span>len</span><span>(</span><span>contexts_tensor</span><span>))</span>
    <span>contexts_tensor</span> <span>=</span> <span>contexts_tensor</span><span>[</span><span>indices</span><span>]</span>
    <span>targets_tensor</span> <span>=</span> <span>targets_tensor</span><span>[</span><span>indices</span><span>]</span>
    <span>positions_tensor</span> <span>=</span> <span>positions_tensor</span><span>[</span><span>indices</span><span>]</span>
    
    <span>print</span><span>(</span><span>f</span><span>&#34;Generated </span><span>{</span><span>len</span><span>(</span><span>contexts_tensor</span><span>)</span><span>:</span><span>,</span><span>}</span><span> V2 training pairs.&#34;</span><span>)</span>
    <span>return</span> <span>contexts_tensor</span><span>,</span> <span>targets_tensor</span><span>,</span> <span>positions_tensor</span>

<span># --- Prepare Data for Model V2 ---
</span><span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>--- Preparing Training Data for Model V2 (OneHotPixelPredictorWithPosition) ---&#34;</span><span>)</span>
<span>train_contexts</span><span>,</span> <span>train_targets</span><span>,</span> <span>train_positions</span> <span>=</span> <span>create_randomized_token_training_data_with_pos</span><span>(</span>
    <span>trainset_quantized</span><span>,</span> 
    <span>CONTEXT_LENGTH</span><span>,</span> 
    <span>START_TOKEN_VALUE_INT</span><span>,</span> 
    <span>NUM_QUANTIZATION_BINS</span><span>,</span>
    <span>N_PIXELS</span><span>,</span> <span># Pass total number of pixels in an image
</span>    <span>max_samples</span><span>=</span><span>MAX_SAMPLES</span><span>,</span> <span># Adjust as needed
</span><span>)</span>

<span>print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Model V2 - Data Shapes:&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  train_contexts shape: </span><span>{</span><span>train_contexts</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_contexts</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  train_targets shape: </span><span>{</span><span>train_targets</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_targets</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  train_positions shape: </span><span>{</span><span>train_positions</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_positions</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span><span>)</span>

<span>if</span> <span>len</span><span>(</span><span>train_targets</span><span>)</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>assert</span> <span>train_targets</span><span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>NUM_QUANTIZATION_BINS</span><span>,</span> <span>&#34;Target tokens for V2 exceed K-1&#34;</span>
    <span>assert</span> <span>train_targets</span><span>.</span><span>min</span><span>()</span> <span>&gt;=</span> <span>0</span><span>,</span> <span>&#34;Target tokens for V2 are negative&#34;</span>
    <span>assert</span> <span>train_positions</span><span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>N_PIXELS</span><span>,</span> <span>&#34;Position index out of bounds&#34;</span>
    <span>assert</span> <span>train_positions</span><span>.</span><span>min</span><span>()</span> <span>&gt;=</span> <span>0</span><span>,</span> <span>&#34;Position index negative&#34;</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;Skipping assertions on empty V2 data tensors.&#34;</span><span>)</span>


</code></pre></div></div>

<h3 id="training-model-v2-with-positional-encodings">Training Model V2 (with Positional Encodings)</h3>

<p>Now that we have our <code>model_onehot_with_pos</code> and the corresponding training data which includes positional information for each target pixel, we can proceed with training.</p>

<p>The training setup will be very similar to Model V1:</p>

<ul>
  <li><em>Loss Function:</em> <code>CrossEntropyLoss</code>, as we’re still predicting one of <code>K</code> pixel tokens.</li>
  <li><em>Optimizer:</em> AdamW.</li>
  <li><em>Process:</em> We’ll iterate for a set number of epochs, shuffling our data and processing it in mini-batches. The key difference is that during the forward pass, we will now also provide the <code>pixel_positions_flat</code> to the model.</li>
</ul>

<p>Let’s see if providing this spatial awareness helps the model learn to generate more structured images, even with the one-hot token representation.</p>

<div><div><pre><code>
<span># --- Model V2, Data, Loss, Optimizer ---
</span>
<span>model_onehot_with_pos</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_contexts</span> <span>=</span> <span>train_contexts</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_targets</span> <span>=</span> <span>train_targets</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_positions</span> <span>=</span> <span>train_positions</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span> <span># Ensure positions are also on device
</span>
<span>criterion</span> <span>=</span> <span>nn</span><span>.</span><span>CrossEntropyLoss</span><span>()</span>
<span>optimizer</span> <span>=</span> <span>optim</span><span>.</span><span>AdamW</span><span>(</span><span>model_onehot_with_pos</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>LEARNING_RATE</span><span>,</span> <span>weight_decay</span><span>=</span><span>1e-5</span><span>)</span>

<span>n_samples</span> <span>=</span> <span>len</span><span>(</span><span>train_contexts</span><span>)</span>
<span>if</span> <span>n_samples</span> <span>==</span> <span>0</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;No training samples available for Model V2. Skipping training.&#34;</span><span>)</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Training Model V2 (OneHotPixelPredictorWithPosition) on </span><span>{</span><span>n_samples</span><span>:</span><span>,</span><span>}</span><span> samples for </span><span>{</span><span>EPOCHS</span><span>}</span><span> epochs.&#34;</span><span>)</span>

    <span># --- Training Loop for Model V2 ---
</span>    <span>epoch_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>EPOCHS</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Model V2 Training Epochs&#34;</span><span>,</span> <span>position</span><span>=</span><span>0</span><span>,</span> <span>leave</span><span>=</span><span>True</span><span>)</span>
    <span>for</span> <span>epoch</span> <span>in</span> <span>epoch_pbar</span><span>:</span>
        <span>model_onehot_with_pos</span><span>.</span><span>train</span><span>()</span> <span># Set model to training mode
</span>        <span>epoch_loss</span> <span>=</span> <span>0.0</span>
        <span>num_batches</span> <span>=</span> <span>0</span>
        
        <span>indices</span> <span>=</span> <span>torch</span><span>.</span><span>randperm</span><span>(</span><span>n_samples</span><span>,</span> <span>device</span><span>=</span><span>device</span><span>)</span>
        
        <span>total_batches_in_epoch</span> <span>=</span> <span>(</span><span>n_samples</span> <span>+</span> <span>BATCH_SIZE_TRAIN</span> <span>-</span> <span>1</span><span>)</span> <span>//</span> <span>BATCH_SIZE_TRAIN</span>
        <span>batch_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>0</span><span>,</span> <span>n_samples</span><span>,</span> <span>BATCH_SIZE_TRAIN</span><span>),</span> 
                             <span>desc</span><span>=</span><span>f</span><span>&#34;Epoch </span><span>{</span><span>epoch</span><span>+</span><span>1</span><span>}</span><span>/</span><span>{</span><span>EPOCHS</span><span>}</span><span>&#34;</span><span>,</span> 
                             <span>position</span><span>=</span><span>1</span><span>,</span> <span>leave</span><span>=</span><span>False</span><span>,</span> 
                             <span>total</span><span>=</span><span>total_batches_in_epoch</span><span>)</span>

        <span>for</span> <span>start_idx</span> <span>in</span> <span>batch_pbar</span><span>:</span>
            <span>end_idx</span> <span>=</span> <span>min</span><span>(</span><span>start_idx</span> <span>+</span> <span>BATCH_SIZE_TRAIN</span><span>,</span> <span>n_samples</span><span>)</span>
            <span>if</span> <span>start_idx</span> <span>==</span> <span>end_idx</span><span>:</span> <span>continue</span>

            <span>batch_indices</span> <span>=</span> <span>indices</span><span>[</span><span>start_idx</span><span>:</span><span>end_idx</span><span>]</span>
            
            <span>batch_context_tokens</span> <span>=</span> <span>train_contexts</span><span>[</span><span>batch_indices</span><span>]</span>
            <span>batch_target_tokens</span> <span>=</span> <span>train_targets</span><span>[</span><span>batch_indices</span><span>]</span>
            <span>batch_pixel_positions</span> <span>=</span> <span>train_positions</span><span>[</span><span>batch_indices</span><span>]</span> <span># Get positions for the batch
</span>            
            <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
            
            <span># Model V2 forward pass - now includes pixel_positions_flat
</span>            <span>output_logits</span> <span>=</span> <span>model_onehot_with_pos</span><span>(</span>
                <span>batch_context_tokens</span><span>,</span> 
                <span>pixel_positions_flat</span><span>=</span><span>batch_pixel_positions</span><span>,</span> <span># Pass positions
</span>                <span>training</span><span>=</span><span>True</span>
            <span>)</span> 
            
            <span>loss</span> <span>=</span> <span>criterion</span><span>(</span><span>output_logits</span><span>,</span> <span>batch_target_tokens</span><span>)</span>
            <span>loss</span><span>.</span><span>backward</span><span>()</span>
            <span>optimizer</span><span>.</span><span>step</span><span>()</span>
            
            <span>epoch_loss</span> <span>+=</span> <span>loss</span><span>.</span><span>item</span><span>()</span>
            <span>num_batches</span> <span>+=</span> <span>1</span>
            
            <span>if</span> <span>num_batches</span> <span>%</span> <span>50</span> <span>==</span> <span>0</span><span>:</span>
                 <span>batch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>loss</span><span>=</span><span>f</span><span>&#34;</span><span>{</span><span>loss</span><span>.</span><span>item</span><span>()</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>&#34;</span><span>)</span>
        
        <span>if</span> <span>num_batches</span> <span>&gt;</span> <span>0</span><span>:</span>
            <span>avg_loss</span> <span>=</span> <span>epoch_loss</span> <span>/</span> <span>num_batches</span>
            <span>epoch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>avg_loss</span><span>=</span><span>f</span><span>&#34;</span><span>{</span><span>avg_loss</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>&#34;</span><span>)</span>
        <span>else</span><span>:</span>
            <span>epoch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>avg_loss</span><span>=</span><span>&#34;N/A&#34;</span><span>)</span>

    <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Model V2 training completed!&#34;</span><span>)</span>


</code></pre></div></div>

<h3 id="generating-images-with-model-v2-with-positional-encodings">Generating Images with Model V2 (with Positional Encodings)</h3>

<p>Now that Model V2, <code>OneHotPixelPredictorWithPosition</code>, has been trained with positional information, we can generate images. The autoregressive process remains largely the same as with Model V1, but with one crucial addition:</p>

<ol>
  <li>Initialize a context window with <code>START_TOKEN_VALUE_INT</code>.</li>
  <li>For each pixel we want to generate (from pixel 0 to <code>N_PIXELS-1</code>):
 a. Determine the <em>current pixel’s absolute flat position</em>.
 b. Feed the current context window <em>and</em> this current pixel position to the model.
 c. Obtain logits, apply temperature, convert to probabilities via softmax.
 d. Sample the next pixel token.
 e. Append the sampled token to our generated sequence.
 f. Update the context window.</li>
  <li>Repeat until the image is complete.</li>
  <li>De-quantize the generated tokens.</li>
</ol>

<p>Let’s see if the added spatial awareness from positional encodings helps Model V2 produce more structured or recognizable images compared to Model V1.</p>

<div><div><pre><code><span>def</span> <span>generate_image_v2</span><span>(</span><span>model</span><span>,</span> <span>context_length</span><span>,</span> <span>start_token_int</span><span>,</span> <span>num_pixel_values_k</span><span>,</span> 
                      <span>img_size</span><span>,</span> <span>total_num_pixels</span><span>,</span> <span>current_device</span><span>,</span> <span>temperature</span><span>=</span><span>1.0</span><span>):</span>
    <span>&#34;&#34;&#34;
    Generates a single image using Model V2 (OneHotPixelPredictorWithPosition).
    Args:
        model: The trained OneHotPixelPredictorWithPosition model.
        context_length (int): Length of the context window.
        start_token_int (int): Integer value for the start token.
        num_pixel_values_k (int): K, the number of possible actual pixel tokens.
        img_size (int): Dimension of the square image.
        total_num_pixels (int): Total pixels in the image (img_size * img_size).
        current_device (torch.device): Device to run generation on.
        temperature (float): Softmax temperature for sampling.
    Returns:
        numpy.ndarray: De-quantized grayscale image.
    &#34;&#34;&#34;</span>
    <span>model</span><span>.</span><span>eval</span><span>()</span>
    <span>model</span><span>.</span><span>to</span><span>(</span><span>current_device</span><span>)</span>
    
    <span>current_context_tokens_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>full</span><span>((</span><span>1</span><span>,</span> <span>context_length</span><span>),</span> <span>start_token_int</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>current_device</span><span>)</span>
    <span>generated_pixel_tokens_list</span> <span>=</span> <span>[]</span>
    
    <span>pixel_gen_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span><span>range</span><span>(</span><span>total_num_pixels</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Generating V2 Image Pixels&#34;</span><span>,</span> <span>leave</span><span>=</span><span>False</span><span>,</span> <span>position</span><span>=</span><span>0</span><span>,</span> <span>disable</span><span>=</span><span>True</span><span>)</span> <span># Disable for multi-image plot
</span>
    <span>with</span> <span>torch</span><span>.</span><span>no_grad</span><span>():</span>
        <span>for</span> <span>i</span> <span>in</span> <span>pixel_gen_pbar</span><span>:</span> <span># i is the current_flat_pixel_position (0 to N_PIXELS-1)
</span>            <span>current_flat_pixel_position_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>i</span><span>],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>current_device</span><span>)</span> <span># Shape [1]
</span>            
            <span>output_logits</span> <span>=</span> <span>model</span><span>(</span>
                <span>current_context_tokens_tensor</span><span>,</span> 
                <span>pixel_positions_flat</span><span>=</span><span>current_flat_pixel_position_tensor</span><span>,</span> <span># Pass current position
</span>                <span>training</span><span>=</span><span>False</span>
            <span>)</span>
            
            <span>probabilities</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>output_logits</span> <span>/</span> <span>temperature</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>).</span><span>squeeze</span><span>()</span>
            <span>next_pixel_token</span> <span>=</span> <span>torch</span><span>.</span><span>multinomial</span><span>(</span><span>probabilities</span><span>,</span> <span>num_samples</span><span>=</span><span>1</span><span>).</span><span>item</span><span>()</span>
            <span>generated_pixel_tokens_list</span><span>.</span><span>append</span><span>(</span><span>next_pixel_token</span><span>)</span>
            
            <span>new_token_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([[</span><span>next_pixel_token</span><span>]],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>current_device</span><span>)</span>
            <span>current_context_tokens_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>current_context_tokens_tensor</span><span>[:,</span> <span>1</span><span>:],</span> <span>new_token_tensor</span><span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
    
    <span>generated_tokens_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>generated_pixel_tokens_list</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>)</span>
    <span>dequantized_image_array</span> <span>=</span> <span>dequantize_tensor</span><span>(</span><span>generated_tokens_tensor</span><span>,</span> <span>num_pixel_values_k</span><span>).</span><span>numpy</span><span>().</span><span>reshape</span><span>(</span><span>img_size</span><span>,</span> <span>img_size</span><span>)</span>
    
    <span>return</span> <span>dequantized_image_array</span>

<span># --- Generate and Visualize Multiple Images from Model V2 ---
</span><span>if</span> <span>n_samples</span> <span>&gt;</span> <span>0</span><span>:</span> <span># Only generate if Model V2 was trained
</span>    <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>--- Generating Images from Model V2 (OneHot with Position) ---&#34;</span><span>)</span>
    <span>n_images_to_generate</span> <span>=</span> <span>25</span> <span># 5x5 grid
</span>    <span>fig_gen</span><span>,</span> <span>axes_gen</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>8</span><span>,</span> <span>8</span><span>))</span>
    <span>fig_gen</span><span>.</span><span>suptitle</span><span>(</span><span>f</span><span>&#34;Model V2 Generated Digits (</span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span> Bins, One-Hot, With Pos.Emb.)&#34;</span><span>,</span> <span>fontsize</span><span>=</span><span>14</span><span>)</span>

    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>n_images_to_generate</span><span>):</span>
        <span>row</span><span>,</span> <span>col</span> <span>=</span> <span>i</span> <span>//</span> <span>5</span><span>,</span> <span>i</span> <span>%</span> <span>5</span>
        <span>ax</span> <span>=</span> <span>axes_gen</span><span>[</span><span>row</span><span>,</span> <span>col</span><span>]</span>
        
        <span>dequantized_img</span> <span>=</span> <span>generate_image_v2</span><span>(</span>
            <span>model_onehot_with_pos</span><span>,</span> 
            <span>CONTEXT_LENGTH</span><span>,</span> 
            <span>START_TOKEN_VALUE_INT</span><span>,</span> 
            <span>NUM_QUANTIZATION_BINS</span><span>,</span> 
            <span>IMG_SIZE</span><span>,</span>
            <span>N_PIXELS</span><span>,</span> <span># Pass N_PIXELS
</span>            <span>device</span><span>,</span> 
            <span>temperature</span><span>=</span><span>1.0</span> 
        <span>)</span>
        <span>ax</span><span>.</span><span>imshow</span><span>(</span><span>dequantized_img</span><span>,</span> <span>cmap</span><span>=</span><span>&#39;gray&#39;</span><span>,</span> <span>vmin</span><span>=</span><span>0</span><span>,</span> <span>vmax</span><span>=</span><span>1</span><span>)</span>
        <span>ax</span><span>.</span><span>axis</span><span>(</span><span>&#39;off&#39;</span><span>)</span>

    <span>plt</span><span>.</span><span>tight_layout</span><span>(</span><span>rect</span><span>=</span><span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>0.95</span><span>])</span>
    <span>plt</span><span>.</span><span>show</span><span>()</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;Skipping Model V2 image generation as no training samples were available.&#34;</span><span>)</span>

</code></pre></div></div>

<p><img src="https://tunahansalih.github.io/images/blog/AR-part1/Part1_25_1.png" alt="png"/></p>

<h3 id="model-v2-results-with-positional-encodings">Model V2: Results with Positional Encodings</h3>

<p>Now, let’s examine the images generated by Model V2 (<code>OneHotPixelPredictorWithPosition</code>), which incorporated positional encodings while still using one-hot representations for the pixel tokens.</p>

<p>Comparing these to the outputs from Model V1 (which had no positional information), we can make some significant observations:</p>

<ul>
  <li><em>Improved Structure - Emergence of Verticality:</em> This is the most striking difference! The dominant horizontal streaks from Model V1 are largely gone. Instead, we see a clear tendency towards <em>vertical structures</em> or alignments. The model now seems to understand that pixels “above” and “below” each other are related in a way that pixels far apart horizontally might not be (or at least, it’s learned to generate patterns consistent with the vertical strokes common in digits).</li>
  <li><em>Hints of Digit-like Forms (but still abstract):</em> While still not producing clear, recognizable digits, some of these vertical structures have a more “digit-like” feel than the random noise of Model V1. You can almost squint and see hints of ‘1’s, or parts of ‘7’s, ‘4’s, or other vertical segments. There’s a sense that the model is <em>trying</em> to place “ink” in a more constrained, column-oriented fashion.</li>
  <li><em>Centering Tendency:</em> Many of the generated patterns appear somewhat centered within the 28x28 canvas, which is typical for MNIST digits. This suggests the positional encodings are helping the model learn <em>where</em> to place “active” pixels.</li>
  <li><em>Still “Noisy” and “Fragmented”:</em> Despite the move towards verticality, the generations are still quite noisy and fragmented. The “strokes” are often broken, and there’s a lack of smooth curves or connected components that would form complete digits.</li>
  <li><em>Coarseness from One-Hot Encoding Persists:</em> The blocky appearance due to the 16 quantization bins and the one-hot encoding of tokens is still evident. The model isn’t (and can’t easily with this representation) learning smooth transitions between pixel intensities.</li>
</ul>

<p><em>The Impact of Positional Information:</em></p>

<p>Adding positional encoding has clearly made a substantial difference. By knowing the (row, column) coordinate of the pixel it’s predicting, Model V2 has been able to:</p>

<ol>
  <li>Move away from the undirected, horizontal streak patterns of Model V1.</li>
  <li>Learn to generate patterns with a strong vertical bias, which is a key characteristic of many handwritten digits.</li>
  <li>Roughly position these patterns within the typical area of a digit on the canvas.</li>
</ol>

<p>This demonstrates the critical importance of spatial awareness for image generation tasks. Even a simple MLP, when given location information, can start to learn rudimentary structural properties.</p>

<p><em>However, we’re not quite there yet.</em> The generated images still lack clarity and fine detail. A remaining bottleneck is likely our <em>one-hot encoding of pixel tokens</em>. This representation treats each of our 16 pixel intensity levels as a completely distinct, unrelated category. The model has no inherent understanding that, for example, token <code>3</code> (a dark gray) is semantically “closer” to token <code>4</code> (a slightly different dark gray) than it is to token <code>15</code> (a very light gray). It must learn these relationships purely from co-occurrence, which can be inefficient and limit its ability to model subtle intensity variations.</p>

<p>This naturally leads us to our next and final refinement for this MLP-based autoregressive model: <em>What if we replace the one-hot encoding of pixel tokens with learned, dense vector representations (embeddings)?</em> This naturally leads us to our next refinement: What if we replace the one-hot encoding of pixel tokens with learned, dense vector representations (embeddings)? Furthermore, instead of just generating any digit, can we guide the model to generate a specific digit? This is what we’ll explore in Model V3. We will combine the benefits of learned token embeddings and positional encodings, and crucially, we will introduce conditional generation by providing the model with the desired class label (i.e., the digit we want it to generate)</p>

<h2 id="model-v3-enhancing-token-understanding-with-learned-embeddings">Model V3: Enhancing Token Understanding with Learned Embeddings</h2>

<p>Our journey so far has shown progress. Model V1, with no spatial awareness, produced noisy streaks. Model V2, by incorporating positional encodings, started to generate more structured, vertically-oriented patterns, hinting at digit-like forms. However, the results are still far from clear MNIST digits, and a key limitation we identified was the one-hot encoding of pixel tokens. This representation forces the model to learn the relationships between different pixel intensities (e.g., that “dark gray” is similar to “slightly darker gray”) from scratch, without any inherent notion of their similarity.</p>

<p>For Model V3, we’ll address this by introducing <em>learned token embeddings</em> for our pixel intensity values. This is a technique widely used in Natural Language Processing (NLP) with great success.</p>

<p><em>How it Works:</em></p>

<ol>
  <li><em>Embedding Layer (<code>nn.Embedding</code>):</em> Instead of one-hot encoding, each integer pixel token (from 0 to <code>NUM_QUANTIZATION_BINS-1</code>) and our special <code>START_TOKEN_VALUE_INT</code> will be mapped to a dense, lower-dimensional vector. These vectors are <em>learnable parameters</em> of the model.</li>
  <li><em>Learning Relationships:</em> During training, the model will adjust these embedding vectors. If certain pixel intensities often appear in similar contexts or lead to similar predictions, their embedding vectors will tend to become similar. This allows the model to capture the semantic “closeness” of different pixel values.</li>
  <li><em>Combined with Positional Encoding:</em> We will retain the absolute positional encodings from Model V2. The input to our MLP will now be a concatenation of:
    <ol>
      <li>The learned embedding vectors for each token in the context window.</li>
      <li>The learned positional embedding for the target pixel’s row.</li>
      <li>
        <ol>
          <li>The learned positional embedding for the target pixel’s column.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p>By providing the model with both a richer, learned representation for pixel intensities <em>and</em> spatial awareness, we hope to see a significant improvement in the quality and coherence of the generated images. This Model V3 represents our most sophisticated MLP-based autoregressive generator in this tutorial.</p>

<div><div><pre><code><span>class</span> <span>CategoricalPixelPredictor</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span>
        <span>self</span><span>,</span>
        <span>num_pixel_values</span><span>,</span>
        <span>token_embedding_dim</span><span>,</span>
        <span>context_length</span><span>,</span>
        <span>hidden_size</span><span>,</span>
        <span>img_size</span><span>,</span>
        <span>pos_embedding_dim</span><span>,</span>
        <span>start_token_integer_value</span><span>,</span>
        <span>num_classes</span><span>,</span>
        <span>class_embedding_dim</span><span>,</span>
        <span>dropout_rate</span><span>=</span><span>0.25</span><span>,</span>
    <span>):</span>  <span># Added num_classes, class_embedding_dim
</span>        <span>&#34;&#34;&#34;
        Args:
            num_pixel_values (int): K, number of actual pixel intensity tokens (0 to K-1).
            token_embedding_dim (int): Dimension for the learned pixel token embeddings.
            context_length (int): Number of previous tokens to consider.
            hidden_size (int): Size of the hidden layers.
            img_size (int): Size of the image (assumed square).
            pos_embedding_dim (int): Dimension for learnable positional embeddings (row/col).
            start_token_integer_value (int): The integer value used for the start token.
            num_classes (int): Number of classes for conditional generation.
            class_embedding_dim (int): Dimension for learned class label embeddings.
            dropout_rate (float): Dropout rate.
        &#34;&#34;&#34;</span>
        <span>super</span><span>(</span><span>CategoricalPixelPredictor</span><span>,</span> <span>self</span><span>).</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>num_pixel_values</span> <span>=</span> <span>num_pixel_values</span>  <span># K
</span>        <span>self</span><span>.</span><span>token_embedding_dim</span> <span>=</span> <span>token_embedding_dim</span>
        <span>self</span><span>.</span><span>context_length</span> <span>=</span> <span>context_length</span>
        <span>self</span><span>.</span><span>img_size</span> <span>=</span> <span>img_size</span>
        <span>self</span><span>.</span><span>pos_embedding_dim</span> <span>=</span> <span>pos_embedding_dim</span>
        <span>self</span><span>.</span><span>class_embedding_dim</span> <span>=</span> <span>class_embedding_dim</span>

        <span>self</span><span>.</span><span>token_vocab_size</span> <span>=</span> <span>max</span><span>(</span><span>num_pixel_values</span> <span>-</span> <span>1</span><span>,</span> <span>start_token_integer_value</span><span>)</span> <span>+</span> <span>1</span>

        <span>self</span><span>.</span><span>token_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span>
            <span>num_embeddings</span><span>=</span><span>self</span><span>.</span><span>token_vocab_size</span><span>,</span> <span>embedding_dim</span><span>=</span><span>token_embedding_dim</span>
        <span>)</span>

        <span>self</span><span>.</span><span>row_pos_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span>
            <span>num_embeddings</span><span>=</span><span>img_size</span><span>,</span> <span>embedding_dim</span><span>=</span><span>pos_embedding_dim</span>
        <span>)</span>
        <span>self</span><span>.</span><span>col_pos_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span>
            <span>num_embeddings</span><span>=</span><span>img_size</span><span>,</span> <span>embedding_dim</span><span>=</span><span>pos_embedding_dim</span>
        <span>)</span>

        <span># Class label embedding
</span>        <span>self</span><span>.</span><span>class_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span>
            <span>num_embeddings</span><span>=</span><span>num_classes</span><span>,</span> <span>embedding_dim</span><span>=</span><span>class_embedding_dim</span>
        <span>)</span>

        <span>context_features_dim</span> <span>=</span> <span>context_length</span> <span>*</span> <span>token_embedding_dim</span>
        <span>positional_features_dim</span> <span>=</span> <span>2</span> <span>*</span> <span>pos_embedding_dim</span>
        <span>class_features_dim</span> <span>=</span> <span>class_embedding_dim</span>  <span># Add class embedding dimension
</span>
        <span>self</span><span>.</span><span>mlp_input_dim</span> <span>=</span> <span>(</span>
            <span>context_features_dim</span> <span>+</span> <span>positional_features_dim</span> <span>+</span> <span>class_features_dim</span>
        <span>)</span>

        <span>self</span><span>.</span><span>fc1</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>self</span><span>.</span><span>mlp_input_dim</span><span>,</span> <span>hidden_size</span><span>)</span>
        <span>self</span><span>.</span><span>fc2</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_size</span><span>,</span> <span>hidden_size</span><span>)</span>
        <span>self</span><span>.</span><span>dropout</span> <span>=</span> <span>nn</span><span>.</span><span>Dropout</span><span>(</span><span>dropout_rate</span><span>)</span>
        <span>self</span><span>.</span><span>fc_out</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_size</span><span>,</span> <span>self</span><span>.</span><span>num_pixel_values</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x_context_tokens</span><span>,</span> <span>pixel_positions_flat</span><span>,</span> <span>class_labels</span><span>,</span> <span>training</span><span>=</span><span>True</span><span>):</span> <span># Added class_labels
</span>        <span>&#34;&#34;&#34;
        Args:
            x_context_tokens (Tensor): Batch of context windows. Shape: [batch_size, CONTEXT_LENGTH].
            pixel_positions_flat (Tensor): Absolute flat positions. Shape: [batch_size].
            class_labels (Tensor): Class labels for each sample. Shape: [batch_size].
            training (bool): Whether in training mode.
        &#34;&#34;&#34;</span>
        <span>batch_size</span> <span>=</span> <span>x_context_tokens</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
        
        <span>embedded_context</span> <span>=</span> <span>self</span><span>.</span><span>token_embedding</span><span>(</span><span>x_context_tokens</span><span>)</span>
        <span>flattened_embedded_context</span> <span>=</span> <span>embedded_context</span><span>.</span><span>view</span><span>(</span><span>batch_size</span><span>,</span> <span>-</span><span>1</span><span>)</span>
        
        <span>rows</span> <span>=</span> <span>pixel_positions_flat</span> <span>//</span> <span>self</span><span>.</span><span>img_size</span>
        <span>cols</span> <span>=</span> <span>pixel_positions_flat</span> <span>%</span> <span>self</span><span>.</span><span>img_size</span>
        <span>row_embeds</span> <span>=</span> <span>self</span><span>.</span><span>row_pos_embedding</span><span>(</span><span>rows</span><span>)</span>
        <span>col_embeds</span> <span>=</span> <span>self</span><span>.</span><span>col_pos_embedding</span><span>(</span><span>cols</span><span>)</span>
        
        <span># Get class embeddings
</span>        <span>class_embeds</span> <span>=</span> <span>self</span><span>.</span><span>class_embedding</span><span>(</span><span>class_labels</span><span>)</span> <span># Shape: [batch_size, class_embedding_dim]
</span>        
        <span>combined_features</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span>
            <span>flattened_embedded_context</span><span>,</span>
            <span>row_embeds</span><span>,</span>
            <span>col_embeds</span><span>,</span>
            <span>class_embeds</span> <span># Concatenate class embeddings
</span>        <span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
        
        <span>h</span> <span>=</span> <span>F</span><span>.</span><span>relu</span><span>(</span><span>self</span><span>.</span><span>fc1</span><span>(</span><span>combined_features</span><span>))</span>
        <span>if</span> <span>training</span><span>:</span>
            <span>h</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>h</span><span>)</span>
        <span>h</span> <span>=</span> <span>F</span><span>.</span><span>relu</span><span>(</span><span>self</span><span>.</span><span>fc2</span><span>(</span><span>h</span><span>))</span>
        <span>if</span> <span>training</span><span>:</span>
            <span>h</span> <span>=</span> <span>self</span><span>.</span><span>dropout</span><span>(</span><span>h</span><span>)</span>
        <span>output_logits</span> <span>=</span> <span>self</span><span>.</span><span>fc_out</span><span>(</span><span>h</span><span>)</span>
        
        <span>return</span> <span>output_logits</span>
</code></pre></div></div>

<div><div><pre><code><span># Instantiate Model V3
</span><span>model_cat_predictor</span> <span>=</span> <span>CategoricalPixelPredictor</span><span>(</span>
    <span>num_pixel_values</span><span>=</span><span>NUM_QUANTIZATION_BINS</span><span>,</span>
    <span>token_embedding_dim</span><span>=</span><span>TOKEN_EMBEDDING_DIM</span><span>,</span>
    <span>context_length</span><span>=</span><span>CONTEXT_LENGTH</span><span>,</span>
    <span>hidden_size</span><span>=</span><span>HIDDEN_SIZE</span><span>,</span>
    <span>img_size</span><span>=</span><span>IMG_SIZE</span><span>,</span>
    <span>pos_embedding_dim</span><span>=</span><span>POS_EMBEDDING_DIM</span><span>,</span>
    <span>start_token_integer_value</span><span>=</span><span>START_TOKEN_VALUE_INT</span><span>,</span>
    <span>num_classes</span><span>=</span><span>NUM_CLASSES</span><span>,</span>  <span># New
</span>    <span>class_embedding_dim</span><span>=</span><span>CLASS_EMBEDDING_DIM</span><span>,</span>  <span># New
</span><span>)</span>

<span>print</span><span>(</span>
    <span>&#34;--- Model V3: CategoricalPixelPredictor (Token Embeddings + Positional Embeddings + Class Conditional) ---&#34;</span>
<span>)</span>  <span># Updated title
</span><span>print</span><span>(</span><span>f</span><span>&#34;  Number of actual pixel intensity tokens (K): </span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span>
    <span>f</span><span>&#34;  Token Embedding Vocabulary Size (max_token_val + 1): </span><span>{</span><span>model_cat_predictor</span><span>.</span><span>token_vocab_size</span><span>}</span><span>&#34;</span>
<span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Token Embedding Dimension: </span><span>{</span><span>TOKEN_EMBEDDING_DIM</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Positional Embedding Dimension (per row/col): </span><span>{</span><span>POS_EMBEDDING_DIM</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Class Embedding Dimension: </span><span>{</span><span>CLASS_EMBEDDING_DIM</span><span>}</span><span>&#34;</span><span>)</span>  <span># New
</span><span>print</span><span>(</span><span>f</span><span>&#34;  Context Length: </span><span>{</span><span>CONTEXT_LENGTH</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Image Size: </span><span>{</span><span>IMG_SIZE</span><span>}</span><span>x</span><span>{</span><span>IMG_SIZE</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>&#34;  Hidden Size: </span><span>{</span><span>HIDDEN_SIZE</span><span>}</span><span>&#34;</span><span>)</span>
<span>print</span><span>(</span>
    <span>f</span><span>&#34;  Total MLP Input Dimension: </span><span>{</span><span>model_cat_predictor</span><span>.</span><span>mlp_input_dim</span><span>}</span><span>&#34;</span>
<span>)</span>  <span># Will be larger now
</span><span>print</span><span>(</span>
    <span>f</span><span>&#34;  Model V3 parameters: </span><span>{</span><span>sum</span><span>(</span><span>p</span><span>.</span><span>numel</span><span>()</span> <span>for</span> <span>p</span> <span>in</span> <span>model_cat_predictor</span><span>.</span><span>parameters</span><span>())</span><span>:</span><span>,</span><span>}</span><span>&#34;</span>
<span>)</span>

</code></pre></div></div>

<div><div><pre><code><span>def</span> <span>create_training_data_v3</span><span>(</span>
    <span>quantized_dataset</span><span>,</span>
    <span>context_length</span><span>,</span>
    <span>start_token_int</span><span>,</span>
    <span>img_total_pixels</span><span>,</span>
    <span>max_samples</span><span>=</span><span>100000</span><span>,</span>
    <span>max_images_to_process</span><span>=</span><span>None</span><span>,</span>
    <span>random_drop</span><span>=</span><span>0.8</span>
<span>):</span>
    <span>&#34;&#34;&#34;
    Create training data (context tokens, target token, target position, class label)
    for Model V3 (CategoricalPixelPredictor).
    &#34;&#34;&#34;</span>
    <span>all_contexts</span> <span>=</span> <span>[]</span>
    <span>all_targets</span> <span>=</span> <span>[]</span>
    <span>all_positions</span> <span>=</span> <span>[]</span>
    <span>all_labels</span> <span>=</span> <span>[]</span>  <span># To store class labels
</span>    <span>samples_collected</span> <span>=</span> <span>0</span>

    <span>num_images_to_process</span> <span>=</span> <span>len</span><span>(</span><span>quantized_dataset</span><span>)</span>
    <span>if</span> <span>max_images_to_process</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
        <span>num_images_to_process</span> <span>=</span> <span>min</span><span>(</span><span>num_images_to_process</span><span>,</span> <span>max_images_to_process</span><span>)</span>

    <span>print</span><span>(</span>
        <span>f</span><span>&#34;Generating V3 training data (contexts, targets, positions, labels) from </span><span>{</span><span>num_images_to_process</span><span>}</span><span> images (max </span><span>{</span><span>max_samples</span><span>:</span><span>,</span><span>}</span><span> samples)...&#34;</span>
    <span>)</span>

    <span>pbar_images</span> <span>=</span> <span>auto_tqdm</span><span>(</span>
        <span>range</span><span>(</span><span>num_images_to_process</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Processing Images for V3 Data&#34;</span>
    <span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>pbar_images</span><span>:</span>
        <span>if</span> <span>samples_collected</span> <span>&gt;=</span> <span>max_samples</span><span>:</span>
            <span>pbar_images</span><span>.</span><span>set_description</span><span>(</span><span>f</span><span>&#34;Max samples (</span><span>{</span><span>max_samples</span><span>}</span><span>) reached.&#34;</span><span>)</span>
            <span>break</span>

        <span>quantized_image_tensor</span><span>,</span> <span>class_label</span> <span>=</span> <span>quantized_dataset</span><span>[</span>
            <span>i</span>
        <span>]</span>  <span># Get image and label
</span>        <span>flat_token_image</span> <span>=</span> <span>quantized_image_tensor</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>)</span>
        <span>n_pixels_in_image</span> <span>=</span> <span>flat_token_image</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>

        <span>padded_token_sequence</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span>
            <span>[</span>
                <span>torch</span><span>.</span><span>full</span><span>((</span><span>context_length</span><span>,),</span> <span>start_token_int</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span>
                <span>flat_token_image</span><span>,</span>
            <span>]</span>
        <span>)</span>

        <span>for</span> <span>pixel_idx_in_image</span> <span>in</span> <span>range</span><span>(</span><span>n_pixels_in_image</span><span>):</span>
            <span>if</span> <span>samples_collected</span> <span>&gt;=</span> <span>max_samples</span><span>:</span>
                <span>break</span>

            <span>if</span> <span>random</span><span>.</span><span>random</span><span>()</span> <span>&gt;</span> <span>random_drop</span><span>:</span>
                <span>context</span> <span>=</span> <span>padded_token_sequence</span><span>[</span>
                    <span>pixel_idx_in_image</span> <span>:</span> <span>pixel_idx_in_image</span> <span>+</span> <span>context_length</span>
                <span>]</span>
                <span>target_token</span> <span>=</span> <span>flat_token_image</span><span>[</span><span>pixel_idx_in_image</span><span>]</span>

                <span>all_contexts</span><span>.</span><span>append</span><span>(</span><span>context</span><span>)</span>
                <span>all_targets</span><span>.</span><span>append</span><span>(</span><span>target_token</span><span>.</span><span>unsqueeze</span><span>(</span><span>0</span><span>))</span>
                <span>all_positions</span><span>.</span><span>append</span><span>(</span><span>torch</span><span>.</span><span>tensor</span><span>([</span><span>pixel_idx_in_image</span><span>],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>))</span>
                <span>all_labels</span><span>.</span><span>append</span><span>(</span>
                    <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>class_label</span><span>],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>)</span>
                <span>)</span>  <span># Store class label
</span>                <span>samples_collected</span> <span>+=</span> <span>1</span>

    <span>pbar_images</span><span>.</span><span>close</span><span>()</span>

    <span>if</span> <span>not</span> <span>all_contexts</span><span>:</span>
        <span>print</span><span>(</span><span>&#34;Warning: No V3 training samples collected.&#34;</span><span>)</span>
        <span>return</span> <span>(</span>
            <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>,</span> <span>context_length</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span>
            <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span>
            <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span>
            <span>torch</span><span>.</span><span>empty</span><span>((</span><span>0</span><span>),</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>),</span>
        <span>)</span>

    <span>contexts_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>all_contexts</span><span>).</span><span>long</span><span>()</span>
    <span>targets_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>all_targets</span><span>).</span><span>long</span><span>()</span>
    <span>positions_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>all_positions</span><span>).</span><span>long</span><span>().</span><span>squeeze</span><span>()</span>
    <span>labels_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span><span>all_labels</span><span>).</span><span>long</span><span>().</span><span>squeeze</span><span>()</span>  <span># Store labels
</span>
    <span>indices</span> <span>=</span> <span>torch</span><span>.</span><span>randperm</span><span>(</span><span>len</span><span>(</span><span>contexts_tensor</span><span>))</span>
    <span>contexts_tensor</span> <span>=</span> <span>contexts_tensor</span><span>[</span><span>indices</span><span>]</span>
    <span>targets_tensor</span> <span>=</span> <span>targets_tensor</span><span>[</span><span>indices</span><span>]</span>
    <span>positions_tensor</span> <span>=</span> <span>positions_tensor</span><span>[</span><span>indices</span><span>]</span>
    <span>labels_tensor</span> <span>=</span> <span>labels_tensor</span><span>[</span><span>indices</span><span>]</span>  <span># Shuffle labels accordingly
</span>
    <span>print</span><span>(</span><span>f</span><span>&#34;Generated </span><span>{</span><span>len</span><span>(</span><span>contexts_tensor</span><span>)</span><span>:</span><span>,</span><span>}</span><span> V3 training pairs.&#34;</span><span>)</span>
    <span>return</span> <span>contexts_tensor</span><span>,</span> <span>targets_tensor</span><span>,</span> <span>positions_tensor</span><span>,</span> <span>labels_tensor</span>

</code></pre></div></div>

<h3 id="preparing-training-data-for-model-v3-with-class-labels">Preparing Training Data for Model V3 (with Class Labels)</h3>

<p>Our Model V3, CategoricalPixelPredictor, is designed for conditional generation.</p>

<p>This means it now expects not only the context window and the target pixel’s position but also the class label of the image from which the sample is drawn.
We’ll adapt our data preparation to include these class labels:</p>

<ol>
  <li>Context Windows (train_contexts_v3): Sequences of integer tokens.</li>
  <li>Target Tokens (train_targets_v3): The integer token of the actual next pixel.</li>
  <li>Target Pixel Positions (train_positions_v3): The absolute flat position of the target pixel.</li>
  <li>Class Labels (train_labels_v3): The integer label (0-9 for MNIST) of the image corresponding to the current training sample. The model will use an embedding of this label as part of its input.”</li>
</ol>

<div><div><pre><code><span>print</span><span>(</span><span>&#34;--- Preparing Training Data for Model V3 (CategoricalPixelPredictor) ---&#34;</span><span>)</span>
<span>train_contexts_v3</span><span>,</span> <span>train_targets_v3</span><span>,</span> <span>train_positions_v3</span><span>,</span> <span>train_labels_v3</span> <span>=</span> <span>(</span>
    <span>create_training_data_v3</span><span>(</span>
        <span>trainset_quantized</span><span>,</span>
        <span>CONTEXT_LENGTH</span><span>,</span>
        <span>START_TOKEN_VALUE_INT</span><span>,</span>
        <span>N_PIXELS</span><span>,</span>
        <span>max_samples</span><span>=</span><span>MAX_SAMPLES</span><span>,</span>
    <span>)</span>
<span>)</span>

<span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Model V3 - Data Shapes:&#34;</span><span>)</span>
<span>print</span><span>(</span>
    <span>f</span><span>&#34;  train_contexts_v3 shape: </span><span>{</span><span>train_contexts_v3</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_contexts_v3</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span>
<span>)</span>
<span>print</span><span>(</span>
    <span>f</span><span>&#34;  train_targets_v3 shape: </span><span>{</span><span>train_targets_v3</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_targets_v3</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span>
<span>)</span>
<span>print</span><span>(</span>
    <span>f</span><span>&#34;  train_positions_v3 shape: </span><span>{</span><span>train_positions_v3</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_positions_v3</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span>
<span>)</span>
<span>print</span><span>(</span>
    <span>f</span><span>&#34;  train_labels_v3 shape: </span><span>{</span><span>train_labels_v3</span><span>.</span><span>shape</span><span>}</span><span>, dtype: </span><span>{</span><span>train_labels_v3</span><span>.</span><span>dtype</span><span>}</span><span>&#34;</span>
<span>)</span>  <span># New
</span>
<span>if</span> <span>len</span><span>(</span><span>train_targets_v3</span><span>)</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>assert</span> <span>train_targets_v3</span><span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>NUM_QUANTIZATION_BINS</span><span>,</span> <span>(</span>
        <span>&#34;Target tokens for V3 exceed K-1&#34;</span>
    <span>)</span>
    <span>assert</span> <span>train_targets_v3</span><span>.</span><span>min</span><span>()</span> <span>&gt;=</span> <span>0</span><span>,</span> <span>&#34;Target tokens for V3 are negative&#34;</span>
    <span>assert</span> <span>train_positions_v3</span><span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>N_PIXELS</span><span>,</span> <span>&#34;Position index out of bounds for V3&#34;</span>
    <span>assert</span> <span>train_positions_v3</span><span>.</span><span>min</span><span>()</span> <span>&gt;=</span> <span>0</span><span>,</span> <span>&#34;Position index negative for V3&#34;</span>
    <span>assert</span> <span>train_labels_v3</span><span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>NUM_CLASSES</span><span>,</span> <span>&#34;Class label out of bounds for V3&#34;</span>
    <span>assert</span> <span>train_labels_v3</span><span>.</span><span>min</span><span>()</span> <span>&gt;=</span> <span>0</span><span>,</span> <span>&#34;Class label negative for V3&#34;</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;Skipping assertions on empty V3 data tensors.&#34;</span><span>)</span>

</code></pre></div></div>
<div><div><pre><code>--- Preparing Training Data for Model V3 (CategoricalPixelPredictor) ---
Generating V3 training data (contexts, targets, positions, labels) from 60000 images (max 5,000,000 samples)...

Generated 5,000,000 V3 training pairs.

Model V3 - Data Shapes:
  train_contexts_v3 shape: torch.Size([5000000, 28]), dtype: torch.int64
  train_targets_v3 shape: torch.Size([5000000]), dtype: torch.int64
  train_positions_v3 shape: torch.Size([5000000]), dtype: torch.int64
  train_labels_v3 shape: torch.Size([5000000]), dtype: torch.int64
</code></pre></div></div>

<h3 id="training-model-v3-with-token-embeddings-and-positional-encodings">Training Model V3 (with Token Embeddings and Positional Encodings)</h3>

<p>We’re now ready to train our latest MLP-based model for this tutorial, <code>model_cat_predictor</code>. This model combines:</p>

<ul>
  <li>Learned dense embeddings for the pixel intensity tokens (and the start token) in the context window.</li>
  <li>Learned positional embeddings for the row and column of the target pixel.</li>
</ul>

<p>The training setup (loss function, optimizer) will be the same as for Model V2. We expect that the richer input representation (learned token embeddings for context, positional embeddings for location, and class embeddings for the desired digit) will allow the model to learn more effectively and generate images conditioned on the provided class label.</p>

<div><div><pre><code><span># --- Model V3, Data, Loss, Optimizer ---
</span><span>model_cat_predictor</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>  <span># Ensure model is on device
</span>
<span># Use the V3 specific data
</span><span>train_contexts_v3_dev</span> <span>=</span> <span>train_contexts_v3</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_targets_v3_dev</span> <span>=</span> <span>train_targets_v3</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_positions_v3_dev</span> <span>=</span> <span>train_positions_v3</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
<span>train_labels_v3_dev</span> <span>=</span> <span>train_labels_v3</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>  <span># Labels to device
</span>
<span>criterion</span> <span>=</span> <span>nn</span><span>.</span><span>CrossEntropyLoss</span><span>()</span>
<span>optimizer</span> <span>=</span> <span>optim</span><span>.</span><span>AdamW</span><span>(</span>
    <span>model_cat_predictor</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>LEARNING_RATE</span><span>,</span> <span>weight_decay</span><span>=</span><span>1e-5</span>
<span>)</span>

<span>n_samples_v3</span> <span>=</span> <span>len</span><span>(</span><span>train_contexts_v3</span><span>)</span>
<span>if</span> <span>n_samples_v3</span> <span>==</span> <span>0</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;No training samples available for Model V3. Skipping training.&#34;</span><span>)</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span>
        <span>f</span><span>&#34;</span><span>\n</span><span>Training Model V3 (CategoricalPixelPredictor) on </span><span>{</span><span>n_samples_v3</span><span>:</span><span>,</span><span>}</span><span> samples for </span><span>{</span><span>EPOCHS</span><span>}</span><span> epochs.&#34;</span>
    <span>)</span>

    <span>epoch_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span>
        <span>range</span><span>(</span><span>EPOCHS</span><span>),</span> <span>desc</span><span>=</span><span>&#34;Model V3 Training Epochs&#34;</span><span>,</span> <span>position</span><span>=</span><span>0</span><span>,</span> <span>leave</span><span>=</span><span>True</span>
    <span>)</span>
    <span>for</span> <span>epoch</span> <span>in</span> <span>epoch_pbar</span><span>:</span>
        <span>model_cat_predictor</span><span>.</span><span>train</span><span>()</span>
        <span>epoch_loss</span> <span>=</span> <span>0.0</span>
        <span>num_batches</span> <span>=</span> <span>0</span>

        <span>indices</span> <span>=</span> <span>torch</span><span>.</span><span>randperm</span><span>(</span><span>n_samples_v3</span><span>,</span> <span>device</span><span>=</span><span>device</span><span>)</span>

        <span>total_batches_in_epoch</span> <span>=</span> <span>(</span>
            <span>n_samples_v3</span> <span>+</span> <span>BATCH_SIZE_TRAIN</span> <span>-</span> <span>1</span>
        <span>)</span> <span>//</span> <span>BATCH_SIZE_TRAIN</span>
        <span>batch_pbar</span> <span>=</span> <span>auto_tqdm</span><span>(</span>
            <span>range</span><span>(</span><span>0</span><span>,</span> <span>n_samples_v3</span><span>,</span> <span>BATCH_SIZE_TRAIN</span><span>),</span>
            <span>desc</span><span>=</span><span>f</span><span>&#34;Epoch </span><span>{</span><span>epoch</span> <span>+</span> <span>1</span><span>}</span><span>/</span><span>{</span><span>EPOCHS</span><span>}</span><span>&#34;</span><span>,</span>
            <span>position</span><span>=</span><span>1</span><span>,</span>
            <span>leave</span><span>=</span><span>False</span><span>,</span>
            <span>total</span><span>=</span><span>total_batches_in_epoch</span><span>,</span>
        <span>)</span>

        <span>for</span> <span>start_idx</span> <span>in</span> <span>batch_pbar</span><span>:</span>
            <span>end_idx</span> <span>=</span> <span>min</span><span>(</span><span>start_idx</span> <span>+</span> <span>BATCH_SIZE_TRAIN</span><span>,</span> <span>n_samples_v3</span><span>)</span>
            <span>if</span> <span>start_idx</span> <span>==</span> <span>end_idx</span><span>:</span>
                <span>continue</span>

            <span>batch_indices</span> <span>=</span> <span>indices</span><span>[</span><span>start_idx</span><span>:</span><span>end_idx</span><span>]</span>

            <span>batch_context_tokens</span> <span>=</span> <span>train_contexts_v3_dev</span><span>[</span><span>batch_indices</span><span>]</span>
            <span>batch_target_tokens</span> <span>=</span> <span>train_targets_v3_dev</span><span>[</span><span>batch_indices</span><span>]</span>
            <span>batch_pixel_positions</span> <span>=</span> <span>train_positions_v3_dev</span><span>[</span><span>batch_indices</span><span>]</span>
            <span>batch_class_labels</span> <span>=</span> <span>train_labels_v3_dev</span><span>[</span>
                <span>batch_indices</span>
            <span>]</span>  <span># Get class labels for batch
</span>
            <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>

            <span>output_logits</span> <span>=</span> <span>model_cat_predictor</span><span>(</span>
                <span>batch_context_tokens</span><span>,</span>
                <span>pixel_positions_flat</span><span>=</span><span>batch_pixel_positions</span><span>,</span>
                <span>class_labels</span><span>=</span><span>batch_class_labels</span><span>,</span>  <span># Pass class labels
</span>                <span>training</span><span>=</span><span>True</span><span>,</span>
            <span>)</span>

            <span>loss</span> <span>=</span> <span>criterion</span><span>(</span><span>output_logits</span><span>,</span> <span>batch_target_tokens</span><span>)</span>
            <span>loss</span><span>.</span><span>backward</span><span>()</span>
            <span>optimizer</span><span>.</span><span>step</span><span>()</span>

            <span>epoch_loss</span> <span>+=</span> <span>loss</span><span>.</span><span>item</span><span>()</span>
            <span>num_batches</span> <span>+=</span> <span>1</span>

            <span>if</span> <span>num_batches</span> <span>%</span> <span>50</span> <span>==</span> <span>0</span><span>:</span>
                <span>batch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>loss</span><span>=</span><span>f</span><span>&#34;</span><span>{</span><span>loss</span><span>.</span><span>item</span><span>()</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>&#34;</span><span>)</span>

        <span>if</span> <span>num_batches</span> <span>&gt;</span> <span>0</span><span>:</span>
            <span>avg_loss</span> <span>=</span> <span>epoch_loss</span> <span>/</span> <span>num_batches</span>
            <span>epoch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>avg_loss</span><span>=</span><span>f</span><span>&#34;</span><span>{</span><span>avg_loss</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>&#34;</span><span>)</span>
        <span>else</span><span>:</span>
            <span>epoch_pbar</span><span>.</span><span>set_postfix</span><span>(</span><span>avg_loss</span><span>=</span><span>&#34;N/A&#34;</span><span>)</span>

    <span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Model V3 training completed!&#34;</span><span>)</span>

</code></pre></div></div>

<h3 id="generating-images-with-model-v3-token-embeddings--positional-encodings">Generating Images with Model V3 (Token Embeddings + Positional Encodings)</h3>

<p>With our <code>CategoricalPixelPredictor</code> (Model V3) trained, it’s time for the moment of truth! This model uses learned embeddings for pixel intensity tokens and positional encodings, representing our most sophisticated MLP-based model in this tutorial, now capable of conditional image generation.</p>

<p>The generation process will be similar to Model V2, but with a key difference: we will now provide a target class label (e.g., “generate a 7”) to the model at each step. The model will use an embedding of this class label, along with the context and current pixel position, to predict the next pixel token.</p>
<ol>
  <li>Initialize a context window with START_TOKEN_VALUE_INT.</li>
  <li>Provide the desired class_label for the image to be generated.</li>
  <li>For each pixel i (from 0 to N_PIXELS-1):
 a. Feed the current context window, the current pixel’s position (i), and the target class_label to the model.
 b. Obtain logits, apply temperature, sample the next pixel token.
 c. Update context and append the token to the generated sequence.</li>
  <li>Repeat until the image is complete.</li>
  <li>De-quantize.</li>
</ol>

<p>We are eager to see if the model can now generate specific digits when prompted, leveraging learned token embeddings, positional encodings, and class conditioning.</p>

<div><div><pre><code><span>def</span> <span>generate_image_v3_conditional_with_analysis</span><span>(</span>
    <span>model</span><span>,</span>
    <span>context_length</span><span>,</span>
    <span>start_token_int</span><span>,</span>
    <span>num_pixel_values</span><span>,</span>
    <span>class_label</span><span>,</span>  <span># New: class label for conditioning
</span>    <span>img_size</span><span>=</span><span>28</span><span>,</span>
    <span>device</span><span>=</span><span>&#34;cpu&#34;</span><span>,</span>
    <span>temperature</span><span>=</span><span>1.0</span><span>,</span>
<span>):</span>
    <span>&#34;&#34;&#34;Generates a single image conditionally, tracks chosen tokens, and their probabilities.&#34;&#34;&#34;</span>
    <span>model</span><span>.</span><span>eval</span><span>()</span>
    <span>model</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>

    <span>current_context_tokens</span> <span>=</span> <span>torch</span><span>.</span><span>full</span><span>(</span>
        <span>(</span><span>1</span><span>,</span> <span>context_length</span><span>),</span> <span>start_token_int</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>device</span>
    <span>)</span>
    <span># Prepare class label tensor (needs to be batch_size=1 for single image generation)
</span>    <span>class_label_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>class_label</span><span>],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>device</span><span>)</span>

    <span>generated_tokens_list</span> <span>=</span> <span>[]</span>
    <span>chosen_token_probs_list</span> <span>=</span> <span>[]</span>
    <span>entropy_list</span> <span>=</span> <span>[]</span>

    <span>total_pixels</span> <span>=</span> <span>img_size</span> <span>*</span> <span>img_size</span>

    <span>with</span> <span>torch</span><span>.</span><span>no_grad</span><span>():</span>
        <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>total_pixels</span><span>):</span>
            <span>current_pixel_position</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>i</span><span>],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>device</span><span>)</span>

            <span># Pass class_label_tensor to the model
</span>            <span>output_logits</span> <span>=</span> <span>model</span><span>(</span>
                <span>current_context_tokens</span><span>,</span>
                <span>pixel_positions_flat</span><span>=</span><span>current_pixel_position</span><span>,</span>
                <span>class_labels</span><span>=</span><span>class_label_tensor</span><span>,</span>  <span># Pass class label
</span>                <span>training</span><span>=</span><span>False</span><span>,</span>
            <span>)</span>  <span># Ensure training is False
</span>
            <span>probabilities</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>output_logits</span> <span>/</span> <span>temperature</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>).</span><span>squeeze</span><span>()</span>

            <span>next_token</span> <span>=</span> <span>torch</span><span>.</span><span>multinomial</span><span>(</span><span>probabilities</span><span>,</span> <span>1</span><span>).</span><span>item</span><span>()</span>
            <span>generated_tokens_list</span><span>.</span><span>append</span><span>(</span><span>next_token</span><span>)</span>
            <span>chosen_token_probs_list</span><span>.</span><span>append</span><span>(</span><span>probabilities</span><span>[</span><span>next_token</span><span>].</span><span>item</span><span>())</span>

            <span>current_entropy</span> <span>=</span> <span>-</span><span>torch</span><span>.</span><span>sum</span><span>(</span>
                <span>probabilities</span> <span>*</span> <span>torch</span><span>.</span><span>log</span><span>(</span><span>probabilities</span> <span>+</span> <span>1e-9</span><span>)</span>
            <span>).</span><span>item</span><span>()</span>
            <span>entropy_list</span><span>.</span><span>append</span><span>(</span><span>current_entropy</span><span>)</span>

            <span>new_token_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span>
                <span>[[</span><span>next_token</span><span>]],</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>,</span> <span>device</span><span>=</span><span>device</span>
            <span>)</span>
            <span>current_context_tokens</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>(</span>
                <span>[</span><span>current_context_tokens</span><span>[:,</span> <span>1</span><span>:],</span> <span>new_token_tensor</span><span>],</span> <span>dim</span><span>=</span><span>1</span>
            <span>)</span>

    <span>img_tokens_tensor</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>generated_tokens_list</span><span>,</span> <span>dtype</span><span>=</span><span>torch</span><span>.</span><span>long</span><span>)</span>
    <span>dequantized_img_arr</span> <span>=</span> <span>(</span>
        <span>dequantize_tensor</span><span>(</span><span>img_tokens_tensor</span><span>,</span> <span>num_pixel_values</span><span>)</span>
        <span>.</span><span>numpy</span><span>()</span>
        <span>.</span><span>reshape</span><span>(</span><span>img_size</span><span>,</span> <span>img_size</span><span>)</span>
    <span>)</span>

    <span>chosen_token_probs_arr</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>chosen_token_probs_list</span><span>).</span><span>reshape</span><span>(</span>
        <span>img_size</span><span>,</span> <span>img_size</span>
    <span>)</span>
    <span>entropy_arr</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>entropy_list</span><span>).</span><span>reshape</span><span>(</span><span>img_size</span><span>,</span> <span>img_size</span><span>)</span>

    <span>return</span> <span>dequantized_img_arr</span><span>,</span> <span>chosen_token_probs_arr</span><span>,</span> <span>entropy_arr</span>

</code></pre></div></div>

<div><div><pre><code><span># --- Generate and Visualize Multiple Images from Model V3 ---
</span><span>if</span> <span>n_samples_v3</span> <span>&gt;</span> <span>0</span><span>:</span>  <span># Only generate if Model V3 was trained
</span>    <span>print</span><span>(</span>
        <span>&#34;</span><span>\n</span><span>--- Generating Images from Model V3 (Token Embeddings + Position + Class Conditional) ---&#34;</span>
    <span>)</span>

    <span># Generate one image for each class (0-9 if NUM_CLASSES is 10)
</span>    <span># Adjust n_rows, n_cols if NUM_CLASSES is different
</span>    <span>n_rows</span> <span>=</span> <span>2</span>
    <span>n_cols</span> <span>=</span> <span>5</span>
    <span>n_images_to_generate</span> <span>=</span> <span>n_rows</span> <span>*</span> <span>n_cols</span>

    <span>fig_gen</span><span>,</span> <span>axes_gen</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>n_rows</span><span>,</span> <span>n_cols</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>5</span><span>))</span>  <span># Adjust figsize
</span>    <span>fig_gen</span><span>.</span><span>suptitle</span><span>(</span>
        <span>f</span><span>&#34;Model V3 Conditionally Generated Digits (</span><span>{</span><span>NUM_QUANTIZATION_BINS</span><span>}</span><span> Bins, TokenEmb, PosEmb, ClassEmb)&#34;</span><span>,</span>
        <span>fontsize</span><span>=</span><span>14</span><span>,</span>
    <span>)</span>

    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>n_images_to_generate</span><span>):</span>
        <span>if</span> <span>i</span> <span>&gt;=</span> <span>NUM_CLASSES</span><span>:</span>  <span># Don&#39;t try to generate for classes that don&#39;t exist
</span>            <span>if</span> <span>axes_gen</span><span>.</span><span>ndim</span> <span>&gt;</span> <span>1</span><span>:</span>
                <span>axes_gen</span><span>.</span><span>flat</span><span>[</span><span>i</span><span>].</span><span>axis</span><span>(</span><span>&#34;off&#34;</span><span>)</span>  <span># Turn off axis if no image
</span>            <span>else</span><span>:</span>  <span># if only one row
</span>                <span>axes_gen</span><span>[</span><span>i</span><span>].</span><span>axis</span><span>(</span><span>&#34;off&#34;</span><span>)</span>
            <span>continue</span>

        <span>class_to_generate</span> <span>=</span> <span>i</span>  <span># Generate digit &#39;i&#39;
</span>
        <span>ax</span> <span>=</span> <span>axes_gen</span><span>.</span><span>flat</span><span>[</span><span>i</span><span>]</span>

        <span># Using the modified generation function
</span>        <span>dequantized_img</span><span>,</span> <span>_</span><span>,</span> <span>_</span> <span>=</span> <span>generate_image_v3_conditional_with_analysis</span><span>(</span>
            <span>model_cat_predictor</span><span>,</span>
            <span>CONTEXT_LENGTH</span><span>,</span>
            <span>START_TOKEN_VALUE_INT</span><span>,</span>
            <span>NUM_QUANTIZATION_BINS</span><span>,</span>
            <span>class_label</span><span>=</span><span>class_to_generate</span><span>,</span>  <span># Pass the class label
</span>            <span>img_size</span><span>=</span><span>IMG_SIZE</span><span>,</span>
            <span>device</span><span>=</span><span>device</span><span>,</span>
            <span>temperature</span><span>=</span><span>1.0</span><span>,</span>
        <span>)</span>
        <span>ax</span><span>.</span><span>imshow</span><span>(</span><span>dequantized_img</span><span>,</span> <span>cmap</span><span>=</span><span>&#34;gray&#34;</span><span>,</span> <span>vmin</span><span>=</span><span>0</span><span>,</span> <span>vmax</span><span>=</span><span>1</span><span>)</span>
        <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>&#34;Digit: </span><span>{</span><span>class_to_generate</span><span>}</span><span>&#34;</span><span>,</span> <span>fontsize</span><span>=</span><span>10</span><span>)</span>
        <span>ax</span><span>.</span><span>axis</span><span>(</span><span>&#34;off&#34;</span><span>)</span>

    <span>plt</span><span>.</span><span>tight_layout</span><span>(</span><span>rect</span><span>=</span><span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>0.95</span><span>])</span>
    <span>plt</span><span>.</span><span>show</span><span>()</span>
<span>else</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;Skipping Model V3 image generation as no training samples were available.&#34;</span><span>)</span>

<span># --- Optional: Generate and visualize probabilities/entropy for a single digit ---
</span><span>if</span> <span>n_samples_v3</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>chosen_digit_to_analyze</span> <span>=</span> <span>7</span>  <span># Example: Analyze digit 7
</span>    <span>print</span><span>(</span>
        <span>f</span><span>&#34;</span><span>\\</span><span>n--- Analyzing Generation for Digit </span><span>{</span><span>chosen_digit_to_analyze</span><span>}</span><span> (Model V3) ---&#34;</span>
    <span>)</span>

    <span>img_arr</span><span>,</span> <span>probs_arr</span><span>,</span> <span>entropy_arr</span> <span>=</span> <span>generate_image_v3_conditional_with_analysis</span><span>(</span>
        <span>model_cat_predictor</span><span>,</span>
        <span>CONTEXT_LENGTH</span><span>,</span>
        <span>START_TOKEN_VALUE_INT</span><span>,</span>
        <span>NUM_QUANTIZATION_BINS</span><span>,</span>
        <span>class_label</span><span>=</span><span>chosen_digit_to_analyze</span><span>,</span>
        <span>img_size</span><span>=</span><span>IMG_SIZE</span><span>,</span>
        <span>device</span><span>=</span><span>device</span><span>,</span>
        <span>temperature</span><span>=</span><span>1.0</span><span>,</span>
    <span>)</span>

    <span>fig_analysis</span><span>,</span> <span>axs_analysis</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>1</span><span>,</span> <span>3</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>15</span><span>,</span> <span>5</span><span>))</span>
    <span>fig_analysis</span><span>.</span><span>suptitle</span><span>(</span>
        <span>f</span><span>&#34;Model V3 Analysis for Generated Digit: </span><span>{</span><span>chosen_digit_to_analyze</span><span>}</span><span>&#34;</span><span>,</span> <span>fontsize</span><span>=</span><span>14</span>
    <span>)</span>

    <span>axs_analysis</span><span>[</span><span>0</span><span>].</span><span>imshow</span><span>(</span><span>img_arr</span><span>,</span> <span>cmap</span><span>=</span><span>&#34;gray&#34;</span><span>,</span> <span>vmin</span><span>=</span><span>0</span><span>,</span> <span>vmax</span><span>=</span><span>1</span><span>)</span>
    <span>axs_analysis</span><span>[</span><span>0</span><span>].</span><span>set_title</span><span>(</span><span>&#34;Generated Image&#34;</span><span>)</span>
    <span>axs_analysis</span><span>[</span><span>0</span><span>].</span><span>axis</span><span>(</span><span>&#34;off&#34;</span><span>)</span>

    <span>im1</span> <span>=</span> <span>axs_analysis</span><span>[</span><span>1</span><span>].</span><span>imshow</span><span>(</span>
        <span>probs_arr</span><span>,</span> <span>cmap</span><span>=</span><span>&#34;viridis&#34;</span><span>,</span> <span>vmin</span><span>=</span><span>0</span><span>,</span> <span>vmax</span><span>=</span><span>1.0</span> <span>/</span> <span>NUM_QUANTIZATION_BINS</span> <span>+</span> <span>0.1</span>
    <span>)</span>  <span># Adjust vmax
</span>    <span>axs_analysis</span><span>[</span><span>1</span><span>].</span><span>set_title</span><span>(</span><span>&#34;Probability of Chosen Token&#34;</span><span>)</span>
    <span>axs_analysis</span><span>[</span><span>1</span><span>].</span><span>axis</span><span>(</span><span>&#34;off&#34;</span><span>)</span>
    <span>fig_analysis</span><span>.</span><span>colorbar</span><span>(</span><span>im1</span><span>,</span> <span>ax</span><span>=</span><span>axs_analysis</span><span>[</span><span>1</span><span>],</span> <span>fraction</span><span>=</span><span>0.046</span><span>,</span> <span>pad</span><span>=</span><span>0.04</span><span>)</span>

    <span>max_entropy</span> <span>=</span> <span>np</span><span>.</span><span>log</span><span>(</span><span>NUM_QUANTIZATION_BINS</span><span>)</span>  <span># Max possible entropy for K classes
</span>    <span>im2</span> <span>=</span> <span>axs_analysis</span><span>[</span><span>2</span><span>].</span><span>imshow</span><span>(</span><span>entropy_arr</span><span>,</span> <span>cmap</span><span>=</span><span>&#34;magma&#34;</span><span>,</span> <span>vmin</span><span>=</span><span>0</span><span>,</span> <span>vmax</span><span>=</span><span>max_entropy</span><span>)</span>
    <span>axs_analysis</span><span>[</span><span>2</span><span>].</span><span>set_title</span><span>(</span><span>&#34;Entropy of Prediction&#34;</span><span>)</span>
    <span>axs_analysis</span><span>[</span><span>2</span><span>].</span><span>axis</span><span>(</span><span>&#34;off&#34;</span><span>)</span>
    <span>fig_analysis</span><span>.</span><span>colorbar</span><span>(</span><span>im2</span><span>,</span> <span>ax</span><span>=</span><span>axs_analysis</span><span>[</span><span>2</span><span>],</span> <span>fraction</span><span>=</span><span>0.046</span><span>,</span> <span>pad</span><span>=</span><span>0.04</span><span>)</span>

    <span>plt</span><span>.</span><span>tight_layout</span><span>(</span><span>rect</span><span>=</span><span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>0.93</span><span>])</span>
    <span>plt</span><span>.</span><span>show</span><span>()</span>

</code></pre></div></div>

<p><img src="https://tunahansalih.github.io/images/blog/AR-part1/Part1_37_1.png" alt="png"/></p>

<p>— Analyzing Generation for Digit 7 (Model V3) —</p>

<p><img src="https://tunahansalih.github.io/images/blog/AR-part1/Part1_37_3.png" alt="png"/></p>



<p>Our exploration of autoregressive image generation has taken us on an iterative journey, starting from a simple pixel predictor to a a conditional digit generator. The progression through Models V1, V2 and V3 has been a step by step improvement, highlighing key principles in building these systems.</p>

<p><em>Model V1</em>, our simplest MLP using only one hot encoded pizel valus without any spatial awarenes, prodiced results,that, as seen in the generated images, were largely abstract and dominated by local, repetitive patterns like horizontal streaks. It was a good example of why we need to encode spatial information in our model.</p>

<p><em>Model V2</em> introduced learned positional embeddings. The generated images immediately began to exhibit more global structure, with a clear shift towards recognizable (albeit nor perfectly formed) digits. While not perfect, this confirmed our hypothesis: spatial information is crucial for the model to learn more meaningful image patterns. eyond local correlations.</p>

<p><em>Model V3</em> was the culmination of these insights. By replacing one-hot encoding with learnable token embeddings for pixel intensities and, most importantly, introducing class conditioning, we achieved a breakthrough. As the generated images for Model V3 demonstrate, the model can now generate images that are not only more coherent but also represent specific, <em>requested digits</em>.</p>

<p><em>Limitations and Future Directions:</em></p>

<p>It’s important to note that our MLP-based models are incredibly simple. The fixed <code>CONTEXT_LENGTH</code> limits the long-range dependencies the model can capture. Quantization bins also lead to a somewhat blocky appearance.</p>

<p>The generations don’t match the quality of state-of-the-art generative models. However, this was never the primary goal; the aim was to understand the foundational concepts of autoregressive generation.</p>

<p>I hope this step-by-step progression has been as enlightening for you to read as it was for me to build and explore. Generating images pixel by pixel, even simple ones, truly demystifies some of the magic behind generative AI. The core idea of predicting one piece at a time, conditioned on what came before, remains a powerful and versatile paradigm in the world of artificial intelligence.</p>

<p>Happy generating!</p>

<p>You can find the code for this tutorial <a href="https://github.com/tunahansalih/AutoRegressiveBlog">here</a>.</p>

<p><em>Note: I hope I will continue this series with more complex models and more interesting applications.</em></p>

      <hr/>
      
    </div></div>
  </body>
</html>
