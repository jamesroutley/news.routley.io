<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://xeiaso.net/blog/2024/homelab-v2/">Original</a>
    <h1>Rebuilding my homelab: Suffering as a service</h1>
    
    <div id="readability-page-1" class="page"><div>
            <article>
    
    <p>
        Published on <time datetime="2024-05-15">05/15/2024</time>, 12856 words, 47 minutes to read
    </p>

    
        <p>With additional Kubernetes mode!</p>
    

    
        
    

    

    

    
        <figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/hero/../xedn/dynamic/766623e0-26d1-4068-9a63-a91d274f23d0.avif"/><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/hero/../xedn/dynamic/766623e0-26d1-4068-9a63-a91d274f23d0.webp"/><img alt="An image of A field of dandelion flowers in the sun, heavy depth of field. A thin strip of the field is in focus, the rest is a blur." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/hero/../xedn/dynamic/766623e0-26d1-4068-9a63-a91d274f23d0.jpg"/></picture></figure>
        <small>A field of dandelion flowers in the sun, heavy depth of field. A thin strip of the field is in focus, the rest is a blur. - Photo by Xe Iaso, Canon EOS R6 mark II with a Rokinon Cine DSX 85mm T1.5 lens</small>
    

    <p>I have a slight problem where I have too many computers in my office. These extra computers are my <a href="https://www.reddit.com/r/homelab/">homelab</a>, or a bunch of slack compute that I can use to run various workloads at home. I use my homelab to have a place to &#34;just run things&#34; like <a href="https://plex.tv">Plex</a> and the whole host of other services that I either run or have written for my husband and I.</p>
<div><p><img alt="Cadey is hug" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/hug/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I want to have my own platform so that I can run things that I used to run in
the cloud. If I can &#34;just run things locally&#34;, I can put my slack compute
space to work for good. This can help me justify the power bill of these nodes
to my landlord!</p></div></div>
<p>Really, I just wanna be able to use this to mess around, try new things, and turn the fruit of those experiments into blogposts like this one. Until very recently, everything in my homelab ran NixOS. <a href="https://fasterthanli.me">A friend of mine</a> has been goading me into trying Kubernetes again, and in a moment of weakness, I decided to see how bad the situation was to get Kubernetes running on my own hardware at home.</p>
<ul>
<li><code>kos-mos</code>, a small server that I use for running some CI things and periphery services. It has 32 GB of ram and a Core i5-10600.</li>
<li><code>ontos</code>, identical to <code>kos-mos</code> but with an RTX 2060 6 GB.</li>
<li><code>logos</code>, identical to <code>kos-mos</code> but with a RTX 3060 12 GB.</li>
<li><code>pneuma</code>, my main shellbox and development machine. It is a handbuilt tower PC with 64 GB of ram and a Ryzen 9 5900X. It has a GPU (AMD RX5700 non-XT w/8GB of vram) because the 5900X doesn&#39;t have integrated graphics. It has a bunch of random storage devices in it. It also handles the video transcoding for xesite video uploads.</li>
<li><code>itsuki</code>, the NAS. It has all of our media and backups on it. It runs Plex and a few other services, mostly managed by docker compose. It has 16 GB of ram and a Core i5-10600.</li>
<li><code>chrysalis</code>, an old Mac Pro from 2013 that I mostly use as my Prometheus server. It has 32 GB of ram and a Xeon E5-1650. It also runs the IRC bot <code>[Mara]</code> in <code>#xeserv</code> on Libera.chat (it announces new posts on my blog). It&#39;s on its last legs in multiple ways, but it works for now. I&#39;ve been holding off on selling it because I won it in a competition involving running an IRC network in Docker containers. Sentimental value is a bitch, eh?</li>
</ul>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>When the homelab was built, the Core i5-10600 was a &#34;last generation&#34;
processor. It also met a perfect balance between compute oomph, onboard iGPU
support, power usage, and not requiring a massive cooler to keep it running
happily. We could probably get some more use out of newer processors, but that
will probably have to wait for one or more of our towers/their parts to get
cycled out in regular upgrades. That probably won&#39;t happen for a year or two,
but it&#39;ll be nice to get a Ryzen 9 5950x or two into the cluster eventually.</p></div></div>
<p>Of these machines, <code>kos-mos</code> is the easiest to deal with because it normally doesn&#39;t have any services dedicated to it. In the past, I had to move some workloads off of it for various reasons.</p>
<p>I have no plans to touch my shellbox or the NAS, those have complicated setups that I don&#39;t want to mess with. I&#39;m okay with my shellbox being different because that&#39;s where I do a lot of development and development servers are almost always vastly different from production servers. I&#39;m also scared to touch the NAS because that has all my media on it and I don&#39;t want to risk losing it. It has more space than the rest of the house combined.</p>
<p>A rebuild of the homelab is going to be a fair bit of work. I&#39;m going to have to take this one piece at a time and make sure that I don&#39;t lose anything important.</p>
<div><p><img alt="Numa is delet" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/delet/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Foreshadowing is a literary technique in which...</p></div></div>
<p>This post isn&#39;t going to be like my other posts. This is a synthesis of a few patron-exclusive notes that described my steps in playing with options and had my immediate reactions as I was doing things. If you want to read those brain-vomit notes, you can <a href="https://patreon.com/cadey">support me on Patreon</a> and get access to them.</p>
<p>When I was considering what to do, I had a few options in mind:</p>
<ul>
<li><a href="https://rockylinux.org/">Rocky Linux</a> (or even <a href="https://yum.oracle.com/">Oracle Linux</a>) with Ansible</li>
<li>Something in the <a href="https://universal-blue.org/">Universal Blue</a> ecosystem</li>
<li><a href="https://fedoraproject.org/coreos/">Fedora CoreOS</a></li>
<li><a href="https://k3os.io/">K3os</a></li>
<li><a href="https://talos.dev">Talos Linux</a></li>
<li>Giving up on the idea of having a homelab, throwing all of my computers into the sun (or selling them on Kijiji), and having a simpler life</li>
</ul>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Wait, hold up. You&#39;re considering <em>Kubernetes</em> for your <em>homelab</em>? I thought
you were as staunchly anti-Kubernetes as it got.</p></div></div>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I am, but hear me out. Kubernetes gets a lot of things wrong, but it does get
one thing so clearly right that it&#39;s worth celebration: you don&#39;t need to SSH
into a machine to look at logs, deploy new versions of things, or see what&#39;s
running. Everything is done via the API. You also don&#39;t need to worry about
assigning workloads to machines, it just does it for you. Not to mention I
have to shill a <a href="https://fly.io/docs/kubernetes/fks-quickstart/">Kubernetes product for
work</a> at some point so having
some experience with it would be good.</p></div></div>
<div><p><img alt="Aoi is facepalm" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/facepalm/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Things really must be bad if you&#39;re at this point...</p></div></div>
<div><p><img alt="Cadey is enby" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/enby/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Let&#39;s be real, the latest release is actually, real life, unironically named
uwubernetes. I can&#39;t <em>not</em> try it. I&#39;d be betraying my people.</p></div></div>
<div><p><img alt="Aoi is facepalm" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/facepalm/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>You really weren&#39;t kidding about technology decisions being made arbitrarily
in the <a href="https://cceckman.com/talks/2024/shashin/">Shashin talk</a>, were you. How do you exist?</p></div></div>
<p>I ran a poll on <a href="https://pony.social/@cadey/112345742472623188">Mastodon</a> to see what people wanted me to do. The results were overwhelmingly in favor of Rocky Linux. As an online &#34;content creator&#34;, who am I to not give the people what they want?</p>
<h2>Rocky Linux</h2>
<p><a href="https://rockylinux.org/">Rocky Linux</a> is a fork of pre-Stream CentOS. It aims to be a 1:1 drop-in replacement for CentOS and RHEL. It&#39;s a community-driven project that is sponsored by the <a href="https://resf.org/">Rocky Enterprise Software Foundation</a>.</p>
<p>For various reasons involving my HDMI cable being too short to reach the other machines, I&#39;m gonna start with <code>chrysalis</code>. Rocky Linux has a GUI installer and I can hook it up to the sideways monitor that I have on my desk. For extra fun, whenever the mac tries to display something on the monitor, the EFI framebuffer dances around until the OS framebuffer takes over.</p>
<video id="008275b6067bae4f09dda5d7183bd21106a02cfce274d07ebdafcb24bdc05d89" controls=""><source src="https://cdn.xeiaso.net/file/christine-static/video/2024/oneoff-mac-boot/index.m3u8" type="application/vnd.apple.mpegurl"/><source src="https://cdn.xeiaso.net/file/christine-static/blog/HLSBROKE.mp4" type="video/mp4"/></video><div><p>Want to watch this in your video player of choice? Take this:</p></div>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I really hope one of the GPUs isn&#39;t dying. That would totally ruin the resale
value of that machine. I wasn&#39;t able to recreate this on my 1080p crash cart
monitor, so I think that it&#39;s just the combination of that mac, the HDMI cable
I used, and my monitor. It&#39;s really weird though.</p></div></div>
<p>The weird part about <code>chrysalis</code> is that it&#39;s a Mac Pro from 2013. Macs of that vintage can boot normal EFI partitions and binaries, but they generally prefer to have your EFI partition be a HFS+ volume. This is normally not a problem because the installer will just make that weird EFI partition for you.</p>
<figure><a href="https://cdn.xeiaso.net/file/christine-static/blog/2024/homelab-v2/IMG_0256.jpg"><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/blog/2024/homelab-v2/IMG_0256.avif"/><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/blog/2024/homelab-v2/IMG_0256.webp"/><img alt="an error message saying: resource to create this format macefi is unavailable" loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/blog/2024/homelab-v2/IMG_0256.jpg"/></picture></a><figcaption>an error message saying: resource to create this format macefi is unavailable</figcaption></figure>
<p>However, the Rocky Linux installer doesn&#39;t make that magic partition for you. They ifdeffed out the macefi installation flow because Red Hat ifdeffed it out.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I get that they want to be a 1:1 drop-in replacement (which means that any bug
RHEL has, they have), but it is massively inconvenient to me in particular.</p></div></div>
<p>As a result, you have to do a very manual install that looks something like this <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1751311#c16">lifted from the Red Hat bug tracker</a>:</p>
<blockquote>
<ul>
<li>Boot Centos/RHEL 8 ISO Normally (I used 8.1 of each)</li>
<li>Do the normal setup of network, packages, etc.</li>
<li>Enter disk partitioning
<ul>
<li>Select your disk</li>
<li>At the bottom, click the &#34;Full disk summary and boot loader&#34; text
<ul>
<li>Click on the disk in the list</li>
<li>Click &#34;Do not install boot loader&#34;</li>
<li>Close</li>
</ul>
</li>
<li>Select &#34;Custom&#34; (I didn&#39;t try automatic, but it probably would not create the EFI partition)</li>
<li>Done in the top left to get to the partitioning screen</li>
<li>Delete existing partitions if needed</li>
<li>Click +
<ul>
<li>CentOS 8: create /boot/efi mountpoint, 600M, Standard EFI partition</li>
<li>RHEL 8: create /foo mountpoint, 600M, Standard EFI partition, then edit the partition to be on /boot/efi</li>
</ul>
</li>
<li>Click + repeatedly to create the rest of the partitions as usual (/boot, / swap, /home, etc.)</li>
<li>Done</li>
</ul>
</li>
<li>During the install, there may be an error about the mactel package, just continue</li>
<li>On reboot, both times I&#39;ve let it get to the grub prompt, but there&#39;s no grub.cfg; not sure if this is required</li>
<li>Boot off ISO into rescue mode
<ul>
<li>Choose 1 to mount the system on /mnt/sysimage</li>
<li>At the shell, chroot /mnt/sysimage</li>
<li>Check on the files in /boot to make sure they exist: ls -l /boot/ /boot/efi/EFI/redhat (or centos)</li>
<li>Run the create the grub.cfg file: grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg
<ul>
<li>I got a couple reload ioctl errors, but that didn&#39;t seem to hurt anything</li>
</ul>
</li>
<li>exit</li>
</ul>
</li>
<li>Next reboot should be fine, and as mentioned above it&#39;ll reboot after SELinux labelling</li>
</ul>
</blockquote>
<div><p><img alt="Cadey is percussive-maintenance" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/percussive-maintenance/256"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Yeah, no. I&#39;m not going to do that. Another solution I found involved you
manually booting the kernel from the GRUB rescue shell. I&#39;m not going to do
that either. I hate myself enough to run Kubernetes on metal in my homelab,
but not so much that I&#39;m gonna unironically slonk the grub rescue shell in
anger.</p></div></div>
<p>So, that&#39;s a wash. In the process of figuring this out I also found out that when I wiped the drive, I took down my IRC bot (and lost the password, thanks <code>A_Dragon</code> for helping me recover that account). I&#39;m going to have to fix that eventually.</p>
<div><p><img alt="Aoi is facepalm" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/facepalm/128"/></p></div>
<p>I ended up moving the announcer IRC bot to be a part of <a href="https://github.com/Xe/x/tree/master/cmd/mimi"><code>within.website/x/cmd/mimi</code></a>. <code>mimi</code> is a little bot that has claws into a lot of other things, including:</p>
<ul>
<li>Status page updates for the <a href="https://discord.gg/V4bE5uhtUg">fly.io community Discord</a>&#39;s #status channel</li>
<li>Announcing new blogposts on <a href="https://web.libera.chat/#xeserv">#xeserv on libera.chat</a></li>
<li>Google Calendar and its own Gmail account for a failed experiment to make a bot that could read emails forwarded to it and schedule appointments based on what a large language model parsed out of the email</li>
</ul>
<div><p><img alt="Cadey is enby" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/enby/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I really need to finish that project. Someday! Maybe that upcoming AI
hackathon would give me a good excuse to make it happen.</p></div></div>
<h3>Ansible</h3>
<p>As a bonus round, let&#39;s see what it would look like to manage things with Ansible on Rocky Linux should I have been able to install Rocky Linux anyways. Ansible is a Red Hat product, so I expect that it would be the easiest thing to use to manage things.</p>
<p>Ansible is a &#34;best hopes&#34; configuration management system. It doesn&#39;t really authoritatively control what is going on, it merely suggest what should be going on. As such, you influence what the system does with &#34;plays&#34; like this:</p>
<pre><code><span><span>-</span> <span>name</span><span>:</span> Full system update
</span><span>  <span>dnf</span><span>:</span>
</span><span>    <span>name</span><span>:</span> <span>&#34;*&#34;</span>
</span><span>    <span>state</span><span>:</span> latest
</span></code></pre>
<p>This is a play that tells the system to update all of its packages with dnf. However, when I ran the linter on this, I got told I need to instead format things like this:</p>
<pre><code><span><span>-</span> <span>name</span><span>:</span> Full system update
</span><span>  <span>ansible.builtin.dnf</span><span>:</span>
</span><span>    <span>name</span><span>:</span> <span>&#34;*&#34;</span>
</span><span>    <span>state</span><span>:</span> latest
</span></code></pre>
<p>You need to use the fully qualified module name because <a href="https://docs.ansible.com/ansible/latest/collections/index.html">you might install other collections that have the name <code>dnf</code> in the future</a>. This kinda makes sense at a glance, I guess, but it&#39;s probably overkill for this usecase. However, it makes the lint errors go away and it is fixed mechanically, so I guess that&#39;s fine.</p>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>Programming idiom for you: the output of the formatter is always the correct
way to write your code/configuration. Less linter warnings, less problems.</p></div></div>
<p>What&#39;s not fine is how you prevent Ansible from running the same command over and over. You need to make a folder full of empty semaphore files that get touched when the command runs:</p>
<pre><code><span><span>-</span> <span>name</span><span>:</span> Xe&#39;s semaphore flags
</span><span>  <span>ansible.builtin.shell</span><span>:</span> mkdir <span>-</span>p /etc/xe/semaphores
</span><span>  <span>args</span><span>:</span>
</span><span>    <span>creates</span><span>:</span> /etc/xe/semaphores
</span><span>
</span><span><span>-</span> <span>name</span><span>:</span> Enable CRB repositories 
</span><span>  <span>ansible.builtin.shell</span><span>:</span> <span>|</span><span>
</span></span><span><span>    dnf config-manager --set-enabled crb</span>
</span><span>
</span><span>    touch /etc/xe/semaphores/crb
</span><span>  <span>args</span><span>:</span>
</span><span>    <span>creates</span><span>:</span> /etc/xe/semaphores/crb
</span></code></pre>
<p>And then finally you can install a package:</p>
<pre><code><span><span>-</span> <span>name</span><span>:</span> Install EPEL repo lists
</span><span>  <span>ansible.builtin.dnf</span><span>:</span>
</span><span>    <span>name</span><span>:</span> <span>&#34;epel-release&#34;</span>
</span><span>    <span>state</span><span>:</span> present
</span></code></pre>
<p>This is about the point where I said &#34;No, I&#39;m not going to deal with this&#34;. I haven&#39;t even created user accounts or installed dotfiles yet, I&#39;m just trying to install a package repository so that I can install other packages.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Do you even really need any users but root or your dotfiles on production
servers? Ideally those should be remotely managed anyways. Logging into them
should be a situation of last resort, right?</p></div></div>
<div><p><img alt="Numa is delet" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/delet/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Assuming that you don&#39;t have triangular cows in the mix yeah.</p></div></div>
<p>So I&#39;m not going with Ansible (or likely any situation where Ansible would be required), even on the machines where installing Rocky Linux works without having to enter GRUB rescue shell purgatory.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/256"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>One of my patrons pointed out that I need to use <a href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_conditionals.html">Ansible
conditionals</a>
in order to prevent these same commands from running over and over. Of course,
in an ideal world these commands would be idempotent (meaning that they can be
run over and over without changing the system), but that&#39;s not always the
case. I&#39;m going to dig deeper into this once I have virtualization working on
the cluster.</p><p>Apparently you&#39;re supposed to use pre-made roles for as much as you can, such
as from <a href="https://galaxy.ansible.com/">Ansible Galaxy</a> or <a href="https://linux-system-roles.github.io/">Linux System
Roles</a>. I don&#39;t know how I feel about
this (doing things with NixOS got me used to a combination of defining things
myself and then using third party things only when I really have to, but that
was probably because the documentation for anything out of the beaten path is
so poor there), but if this is the &#34;right&#34; way to do things then I&#39;ll do it.</p><p>Thanks for the tip, Tudor!</p></div></div>
<h2>CoreOS</h2>
<p>Way back when my career was just starting, CoreOS was released. CoreOS was radical and way ahead of its time. Instead of having a mutable server that you can SSH into and install packages at will on, CoreOS had the view that thou must put all your software into Docker containers and run them that way. This made it impossible to install new packages on the server, which they considered a feature.</p>
<p>I loved using CoreOS when I could because of one part that was absolutely revolutionary: <a href="https://github.com/coreos/fleet">Fleet</a>. Fleet was a distributed init system that let you run systemd services <em>somewhere</em>, but you could care where it ran when you needed to. Imagine a world where you could just spin your jobs somewhere, that was Fleet.</p>
<p>The really magical part about Fleet was the fact that it was deeply integrated into the discovery mechanism of CoreOS. Want 4 nodes in a cluster? Provision them with the same join token and Fleet would just figure it out. Newly provisioned nodes would also accept new work as soon as it was issued.</p>
<div><p><img alt="Numa is happy" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/happy/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Fleet was glorious. It was what made me decide to actually learn how to use
systemd in earnest. Before I had just been a &#34;bloat bad so systemd bad&#34; pleb,
but once I really dug into the inner workings I ended up really liking it.
Everything being composable units that let you build <em>up</em> to what you want
instead of having to be an expert in all the ways shell script messes with you
is just such a better place to operate from. Not to mention being able to
restart multiple units with the same command, define ulimits, and easily
create &#34;oneshot&#34; jobs. If you&#39;re a &#34;systemd hater&#34;, please actually give it a
chance before you decry it as &#34;complicated bad lol&#34;. Shit&#39;s complicated
because life is complicated.</p></div></div>
<p>And then it became irrelevant in the face of Kubernetes after CoreOS got bought out by Red Hat and then IBM bought out Red Hat.</p>
<p>Also, &#34;classic&#34; CoreOS is no more, but its spirit lives on in the form of <a href="https://fedoraproject.org/coreos/">Fedora CoreOS</a>, which is like CoreOS but built on top of <a href="https://coreos.github.io/rpm-ostree/">rpm-ostree</a>. The main difference between Fedora CoreOS and actual CoreOS is that Fedora CoreOS lets you install additional packages on the system.</p>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>Once Red Hat announced that CoreOS would be deprecated in favor of Fedora
CoreOS, Kinvolk forked &#34;classic&#34; CoreOS to <a href="https://www.flatcar.org/">Flatcar
Linux</a>, where you can still use it to this day. This
post didn&#39;t end up evaluating it because it doesn&#39;t let you change Ignition
configurations without reimaging the machine, which is unworkable for reasons
that will become obvious later in the article.</p><p>They are using <a href="https://www.flatcar.org/blog/2024/04/os-innovation-with-systemd-sysext/">systemd-sysext</a> in order to extend the system with more packages, which is reminiscent of rpm-ostree layering.</p></div></div>
<h3>Fedora CoreOS</h3>
<p>For various reasons involving divine intervention, I&#39;m going to be building a few of my own RPM packages. I&#39;m also going to be installing other third party programs on top of the OS, such as <a href="https://github.com/Xe/x/tree/master/cmd/yeet">yeet</a>.</p>
<p>Fedora CoreOS is a bit unique because you install it by declaring the end result of the system, baking that into an ISO, and then plunking that onto a flashdrive to assimilate the machine. If you are using it from a cloud environment, then you plunk your config into the &#34;user data&#34; section of the instance and it will happily boot up with that configuration.</p>
<p>This is a lot closer to the declarative future I want, with the added caveat that changing the configuration of a running system is a bit more involved than just SSHing into the machine and changing a file. You have to effectively blow away the machine and start over.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>What? That sounds like a <em>terrible</em> idea. How would you handle moving state
around?</p></div></div>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Remember, this is for treating machines as replaceable <em>cattle</em>, not <em>pets</em>
that you imprint on. I&#39;m sure that this will be a fun learning experience at
the very least.</p></div></div>
<div><p><img alt="Numa is delet" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/delet/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Again, foreshadowing is a literary technique in which...</p></div></div>
<p>I want to build this on top of rpm-ostree because I want to have the best of both worlds: an immutable system that I can still install packages on. This is an absolute superpower and I want to have it in my life. Realistically I&#39;m gonna end up installing only one or two packages on top of the base system, but those one or two packages are gonna make so many things so much easier. Especially for my WireGuard mesh so I can route the pod/service subnets in my Kubernetes cluster.</p>
<p>As a more practical example of how rpm-ostree, let&#39;s take a look at <a href="https://bazzite.gg">Bazzite Linux</a>. Bazzite is a spin of Fedora Silverblue (desktop Fedora built on top of rpm-ostree) that has the Steam Deck UI installed on top of it. This turns devices like the <a href="https://www.asus.com/ca-en/site/gaming/rog/handheld-consoles/rog-ally/">ROG Ally</a> into actual game consoles instead of handheld gaming PCs.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I went into this distinction more in my failed review video of the ROG Ally. I
plan to post this to <a href="https://patreon.com/cadey">my Patreon</a> in case you want
to see what could have been. The video is probably fine all things considered,
I just don&#39;t think it&#39;s up to my standards and don&#39;t have the time/energy to
heal it right now.</p></div></div>
<p>In Bazzite, rpm-ostree lets you layer on additional things like the Fanatec steering wheel drivers and wheel managers like <a href="https://github.com/berarma/oversteer">Oversteer</a>. This allows you to <em>add</em> optional functionality without having to worry about breaking the base system. Any time updates are installed, rpm-ostree will layer Oversteer on top of it for you so that you don&#39;t have to worry about it.</p>
<p>This combined with my own <a href="https://github.com/Xe/x/tree/master/cmd/yeet">handrolled RPMs with <code>yeet</code></a> means that I could add software to my homelab nodes (like I have with Nix/NixOS) without having to worry about it being rebuilt from scratch or its distribution. This is a superpower that I want to keep in my life.</p>
<p>It&#39;s not gonna be as nice as the Nix setup, but something like this:</p>
<pre><code><span><span>[</span><span>&#34;amd64&#34;</span><span>,</span> <span>&#34;arm64&#34;</span><span>]</span><span>.</span><span>forEach</span><span>(</span><span>(</span><span>goarch</span><span>)</span> <span>=&gt;</span>
</span><span>  rpm<span>.</span><span>build</span><span>(</span><span>{</span>
</span><span>    <span>name</span><span>:</span> <span>&#34;yeet&#34;</span><span>,</span>
</span><span>    <span>description</span><span>:</span> <span>&#34;Yeet out actions with maximum haste!&#34;</span><span>,</span>
</span><span>    <span>homepage</span><span>:</span> <span>&#34;https://within.website&#34;</span><span>,</span>
</span><span>    <span>license</span><span>:</span> <span>&#34;CC0&#34;</span><span>,</span>
</span><span>    goarch<span>,</span>
</span><span>
</span><span>    <span>build</span><span>:</span> <span>(</span><span>out</span><span>)</span> <span>=&gt;</span> <span>{</span>
</span><span>      go<span>.</span><span>build</span><span>(</span><span>&#34;-o&#34;</span><span>,</span> <span><span>`</span><span><span>${</span>out<span>}</span></span><span>/usr/bin/</span><span>`</span></span><span>)</span><span>;</span>
</span><span>    <span>}</span><span>,</span>
</span><span>  <span>}</span><span>)</span>
</span><span><span>)</span><span>;</span>
</span></code></pre>
<p>is so much easier to read and manage than it is to do with RPM specfiles. It really does get closer to what it&#39;s like to use Nix.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Not to mention if I did my Go packaging the full normal way with RPM
specfiles, I&#39;d have to have my own personal dependencies risk fighting the
system-level dependencies. I don&#39;t want to do that, but you can if you want
to. I&#39;d also like my builds to publish one package, not 50-100.</p></div></div>
<p>I&#39;d also need to figure out how to <a href="https://github.com/go-gitea/gitea/pull/27069">fix Gitea&#39;s RPM package serving support so that it signs packages for me</a>, but would be solvable. Most of the work is already done, I&#39;d just need to take over the PR and help push it over the finish line.</p>
<h3>Installing Fedora CoreOS</h3>
<p>The method I&#39;m going to be using to install Fedora CoreOS is to use <a href="https://coreos.github.io/coreos-installer/"><code>coreos-installer</code></a> to build an ISO image with a configuration file generated by <a href="https://coreos.github.io/butane/"><code>butane</code></a>.</p>
<p>To make things extra <em>fun</em>, I&#39;m writing this on a Mac, which means I will need to have a Fedora environment handy to build the ISO because Fedora only ships Linux builds of <code>coreos-installer</code> and <code>butane</code>.</p>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>This installation was adapted from <a href="https://devnonsense.com/posts/k3s-on-fedora-coreos-bare-metal/">this
tutorial</a>,
with modifications made because I&#39;m using a MacBook instead of a Fedora
machine.</p></div></div>
<p>First, I needed to install <a href="https://podman-desktop.io/">Podman Desktop</a>, which is like the Docker Desktop app except it uses the <a href="https://podman.io/">Red Hat Podman</a> stack instead of the Docker stack. For the purposes of this article, they are functionally equivalent.</p>
<p>I made a new repo/folder and then started up a Fedora container:</p>
<pre><code><span>podman run --rm -itv .:/data fedora:latest
</span></code></pre>
<p>I then installed the necessary packages:</p>
<pre><code><span>dnf -y install coreos-installer butane ignition-validate
</span></code></pre>
<p>And then I copied over the template from the Fedora CoreOS k3s tutorial into <code>chrysalis.bu</code>. I edited it to have the hostname <code>chrysalis</code>, loaded my SSH keys into it, and then ran the script to generate a prebaked install ISO. I loaded it on a flashdrive and then stuck it into the same Mac Pro from the last episode.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Annoyingly, it seems that the right file extension for Butane configs is <code>.bu</code>
and that there isn&#39;t a VSCode plugin for it. If I stick with Fedora CoreOS,
I&#39;ll have to make something that makes <code>.bu</code> files get treated as YAML files
or something. I just told VSCode to treat them as YAML files for now.</p></div></div>
<p>It installed perfectly. I suspect that the actual Red Hat installer can be changed to just treat this machine as a normal EFI platform without any issues, but that is a bug report for another day. Intel Macs are quickly going out of support anyways, so it&#39;s probably not going to be the highest priority for then even if I did file that bug.</p>
<p>I got k3s up and running and then I checked the version number. My config was having me install k3s version 1.27.10, which is much older than the current version <a href="https://kubernetes.io/blog/2024/04/17/kubernetes-v1-30-release/">1.30.0 &#34;Uwubernetes&#34;</a>. I fixed the butane config to point to the new version of k3s and then I tried to find a way to apply it to my running machine.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>That should be easy, right? You should just need to push the config to the
server somehow and then it&#39;ll reconverge, right?</p></div></div>
<p>Yeah, about that. It turns out that Fedora CoreOS is very much on the side of &#34;cattle, not pets&#34; when it comes to datacenter management. The Fedora CoreOS view is that any time you need to change out the Ignition config, you should reimage the machine. This makes sense for a lot of hyperconverged setups where this is as simple as pushing a button and waiting for it to come back.</p>
<div><p><img alt="Cadey is wat" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/wat/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I&#39;m not sure what the ideal Fedora CoreOS strategy for handling disk-based
application state is. Maybe it&#39;s &#34;don&#39;t fuck around with prod enough that this
is an issue&#34;, which is reasonable enough. I remember that with normal CoreOS
the advice was &#34;please avoid relying on local storage as much as you can&#34;, but
they probably solved that by this point, either with a blessed state partition
or by continuing the advice to avoid local storage as much as you can. Further
research would be required.</p></div></div>
<p>However, my homelab is many things, but it isn&#39;t a hyperconverged datacenter setup. It&#39;s where I fuck around so I can find out (and then launder that knowledge through you to the rest of the industry for Patreon money and ad impressions). If I want to adopt an OS in the homelab, I need the ability to change my mind without having to burn four USB drives and reflash my homelab.</p>
<p>This was a bummer. I&#39;m gonna have to figure out something else to get Kubernetes up and running for me.</p>
<h2>Other things I evaluated and ended up passing on</h2>
<p>I was told by a coworker that <a href="https://k3os.io/">k3OS</a> is a great way to have a &#34;boot to Kubernetes&#34; environment that you don&#39;t have to think about. This is by the Rancher team, which I haven&#39;t heard about in ages since I played with <a href="https://rancher.com/docs/os/v1.x/en/">RancherOS</a> way back in the before times.</p>
<p>RancherOS was super wild for its time. It didn&#39;t have a package manager, it had the Docker daemon. Two Docker daemons in fact, one for the &#34;system&#34; docker daemon that handled things like TTY sessions, DHCP addresses, device management, system logs, and the like. The other Docker daemon was for the userland, which was where you ran your containers.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I kinda miss how wild RancherOS was. It was great for messing around with at
one of my former workplaces. We didn&#39;t use it for anything super critical, but
it was a great hypervisor for a Minecraft server.</p></div></div>
<p>I tried to get K3os up and running, but then I found out that it&#39;s deprecated. That information isn&#39;t on the website, it&#39;s on the <a href="https://github.com/rancher/k3os/blob/master/README.md#quick-start">getting started documentation</a>. It&#39;s apparently replaced by <a href="https://elemental.docs.rancher.com/">Elemental</a>, which seems to be built on top of OpenSUSE (kinda like how Fedora CoreOS is built on Fedora).</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Didn&#39;t Rancher get bought out by SUSE? That&#39;d explain why everything is
deprecated except for something based on OpenSUSE.</p></div></div>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Oh. Right. That makes sense. I guess I&#39;ll have to look into Elemental at some
point. Maybe I&#39;ll do that in the future.</p></div></div>
<p>I&#39;m gonna pass on this for now. Maybe in the future.</p>
<h2>The Talos Principle</h2>
<p><a href="https://talosprinciple.fandom.com/wiki/Straton_of_Stageira">Straton of Stageira</a> once argued that the mythical construct Talos (an automaton that experienced qualia and had sapience) proved that there was nothing special about mankind. If a product of human engineering could have the same kind of qualia that people do, then realistically there is nothing special about people when compared to machines.</p>
<p>To say that <a href="https://www.talos.dev/">Talos Linux</a> is minimal is a massive understatement. It only has literally <a href="https://www.siderolabs.com/blog/there-are-only-12-binaries-in-talos-linux/">12 binaries in it</a>. I&#39;ve been conceptualizing it as &#34;what if <a href="https://cceckman.com/blog/gokrazy/">gokrazy</a> was production-worthy?&#34;.</p>
<p>My main introduction to it was last year at <a href="https://media.ccc.de/v/all-systems-go-2023-202-talos-linux-trustedboot-for-a-minimal-immutable-os">All Systems Go!</a> by a fellow speaker. I&#39;d been wanting to try something like this out for a while, but I haven&#39;t had a good excuse to sample those waters until now. It&#39;s really intriguing because of how damn minimal it is.</p>
<p>So I downloaded the arm64 ISO and set up a VM on my MacBook to fuck around with it. Here&#39;s a few of the things that I learned in the process:</p>
<div><p><img alt="Cadey is enby" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/enby/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>If you haven&#39;t tried out <a href="https://mac.getutm.app">UTM</a> yet, you are really
missing out. It&#39;s the missing virtual machine hypervisor for macOS. It&#39;s one
of the best apps I know of for running virtual machines on Apple Silicon. I
mostly use it to run random Linux machines on my MacBook, but I&#39;ve also heard
of people using it to play <a href="https://youtu.be/LrLDKYFyLMM">Half-Life on an
iPad</a>. Highly suggest.</p></div></div>
<p>UTM has two modes it can run a VM in. One is &#34;Apple Virtualization&#34; mode that gives you theoretically higher performance at the cost of less options when it comes to networking (probably because <code>Hypervisor.framework</code> has less knobs available to control the VM environment). In order to connect the VM to a shared network (so you can poke it directly with <code>talosctl</code> commands without needing overlay VPNs or crazy networking magic like that), you need to create it without &#34;Apple Virtualization&#34; checked. This does mean you can&#39;t expose Rosetta to run amd64 binaries (and performance might be theoretically slower in a way you can&#39;t perceive given the minimal linux distros in play), but that&#39;s an acceptable tradeoff.</p>
<figure><a href="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/4bda0ab5-46db-4abd-b37b-8f14d2882e60.jpg"><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/4bda0ab5-46db-4abd-b37b-8f14d2882e60.avif"/><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/4bda0ab5-46db-4abd-b37b-8f14d2882e60.webp"/><img alt="UTM showing off the &#39;Shared Network&#39; pane, you want this enabled to get access to the 192.168.65.0/24 network to poke your VM directly." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/4bda0ab5-46db-4abd-b37b-8f14d2882e60.jpg"/></picture></a><figcaption>UTM showing off the &#39;Shared Network&#39; pane, you want this enabled to get access to the 192.168.65.0/24 network to poke your VM directly.</figcaption></figure>
<p>Talos Linux is completely declarative for the base system and really just exists to make Kubernetes easier to run. One of my favorite parts has to be the way that you can combine different configuration snippets together into a composite machine config. Let&#39;s say you have a base &#34;control plane config&#34; in <code>controlplane.yaml</code> and some host-specific config in <code>hosts/hostname.yaml</code>. Your <code>talosctl apply-config</code> command would look like this:</p>
<pre><code><span>talosctl apply-config <span>-n</span> kos-mos <span>-f</span> controlplane.yaml <span>-p</span> @patches/subnets.yaml <span>-p</span> @hosts/kos-mos.yaml
</span></code></pre>
<p>This allows your <code>hosts/kos-mos.yaml</code> file to look like this:</p>
<pre><code><span><span>cluster</span><span>:</span>
</span><span>  <span>apiServer</span><span>:</span>
</span><span>    <span>certSANs</span><span>:</span>
</span><span>      <span>-</span> 100.110.6.17
</span><span>
</span><span><span>machine</span><span>:</span>
</span><span>  <span>network</span><span>:</span>
</span><span>    <span>hostname</span><span>:</span> kos<span>-</span>mos
</span><span>  <span>install</span><span>:</span>
</span><span>    <span>disk</span><span>:</span> /dev/nvme0n1
</span></code></pre>
<p>which allows me to do generic settings cluster-wide <em>and then</em> specific settings for each host (just like I have with my Nix flakes repo). For example, I have a few homelab nodes with nvidia GPUs that I&#39;d like to be able to run AI/large langle mangle tasks on. I can set up the base config to handle generic cases and then enable the GPU drivers only on the nodes that need them.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>By the way, resist the temptation to install the nvidia GPU drivers on
machines that do not need them. It will result in the nvidia GPU drivers
trying to load in a loop, then complaining that they can&#39;t find the GPU, and
then trying to load again. In order to unstuck yourself from that situation,
you have to reimage the machine by attaching a crash cart and selecting the
&#34;wipe disk and boot into maintenance mode&#34; option. This was fun to figure out
by hand, but it was made easier with the <code>talosctl dashboard</code> command.</p></div></div>
<h3>The Talosctl Dashboard</h3>
<p>I just have to take a moment to gush about the <code>talosctl dashboard</code> command. It&#39;s a TUI interface that lets you see what your nodes are doing. When you boot a metal Talos Linux node, it opens the dashboard by default so you can watch the logs as the system wakes up and becomes active.</p>
<p>When you run it on your laptop, it&#39;s as good as if not better than having SSH access to the node. All the information you could want is right there at a glance and you can connect to mulitple machines at once. Just look at this:</p>
<figure><a href="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/f6bb22c4-f26d-41aa-868d-56dc7af841b3.jpg"><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/f6bb22c4-f26d-41aa-868d-56dc7af841b3.avif"/><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/f6bb22c4-f26d-41aa-868d-56dc7af841b3.webp"/><img alt="The talosctl dashboard, it&#39;s a TUI interface that lets you see what is going on with your nodes." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/f6bb22c4-f26d-41aa-868d-56dc7af841b3.jpg"/></picture></a><figcaption>The talosctl dashboard, it&#39;s a TUI interface that lets you see what is going on with your nodes.</figcaption></figure>
<p>Those three nodes can be swapped between by pressing the left and right arrow keys. It&#39;s the best kind of simple, the kind that you don&#39;t have to think about in order to use it. No documentation needed, just run the command and go on instinct. I love it.</p>
<p>You can press F2 to get a view of the processes, resource use, and other errata. It&#39;s everything you could want out of htop, just without the ability to run Doom.</p>
<h3>Making myself a Kubernete</h3>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>A little meta note because it&#39;s really easy to get words conflated here:
whenever I use CapitalizedWords, I&#39;m talking about the Kubernetes concepts,
not the common English words. It&#39;s really hard for me to avoid talking about
the word &#34;service&#34; given the subject matter. Whenever you see &#34;Service&#34;,
&#34;Deployment&#34;, &#34;Secret&#34;, &#34;Ingress&#34;, or the like; know that I&#39;m talking about
the Kubernetes definition of those terms.</p></div></div>
<p>Talos Linux is built to do two things:</p>
<ol>
<li>Boot into Linux</li>
<li>Run Kubernetes</li>
</ol>
<p>That&#39;s it. It&#39;s beautifully brutalist. I love it so far.</p>
<p>I decided to start with <code>kos-mos</code> arbitrarily. I downloaded the ISO, tried to use balenaEtcher to flash it to a USB drive and then windows decided that now was the perfect time to start interrupting me with bullshit related to Explorer desperately trying to find and mount USB drives.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Lately Windows has been going out of its way to actively interfere when I try
to do anything fancy or convenient. I only tolerate it for games, but I am
reconsidering my approach. If only Wayland supported accessibility hooks.</p></div></div>
<p>I was unable to use balenaEtcher to write it, but then I found out that <a href="https://rufus.ie/en/">Rufus</a> can write ISOs to USB drives in a way that doesn&#39;t rely on Windows to do the mounting or writing. That worked and I had <code>kos-mos</code> up and running in short order.</p>
<div><p><img alt="Cadey is enby" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/enby/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>This is when I found out about the hostname patch yaml trick, so it booted
into a randomly generated <code>talos-whatever</code> hostname by default. This was okay,
but I wanted to have the machine names be more meaningful so I can figure out
what&#39;s running where at a glance. Changing hostnames was trivial though, you
can do it from the dashboard worst case. I&#39;m aware that this is defeating the
point of the &#34;cattle, not pets&#34; flow that a lot of modern Linux distributions
want you to go down, but my homelab servers are my pets.</p></div></div>
<p>After bootstrapping etcd and exposing the subnet routes, I made an nginx deployment with a service as a &#34;hello world&#34; to ensure that things were working properly. Here&#39;s the configuration I used:</p>
<pre><code><span><span>---</span>
</span><span><span>apiVersion</span><span>:</span> apps/v1
</span><span><span>kind</span><span>:</span> Deployment
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> nginx
</span><span>  <span>labels</span><span>:</span>
</span><span>    <span>app.kubernetes.io/name</span><span>:</span> nginx
</span><span><span>spec</span><span>:</span>
</span><span>  <span>replicas</span><span>:</span> <span>3</span>
</span><span>  <span>selector</span><span>:</span>
</span><span>    <span>matchLabels</span><span>:</span>
</span><span>      <span>app.kubernetes.io/name</span><span>:</span> nginx
</span><span>  <span>template</span><span>:</span>
</span><span>    <span>metadata</span><span>:</span>
</span><span>      <span>labels</span><span>:</span>
</span><span>        <span>app.kubernetes.io/name</span><span>:</span> nginx
</span><span>    <span>spec</span><span>:</span>
</span><span>      <span>containers</span><span>:</span>
</span><span>        <span>-</span> <span>name</span><span>:</span> nginx
</span><span>          <span>image</span><span>:</span> nginx<span>:</span>1.14.2
</span><span>          <span>ports</span><span>:</span>
</span><span>            <span>-</span> <span>containerPort</span><span>:</span> <span>80</span>
</span><span><span>---</span>
</span><span><span>apiVersion</span><span>:</span> v1
</span><span><span>kind</span><span>:</span> Service
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> nginx
</span><span><span>spec</span><span>:</span>
</span><span>  <span>selector</span><span>:</span>
</span><span>    <span>app.kubernetes.io/name</span><span>:</span> nginx
</span><span>  <span>ports</span><span>:</span>
</span><span>    <span>-</span> <span>protocol</span><span>:</span> TCP
</span><span>      <span>port</span><span>:</span> <span>80</span>
</span><span>      <span>targetPort</span><span>:</span> <span>80</span>
</span><span>  <span>type</span><span>:</span> ClusterIP
</span></code></pre>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>For those of you that don&#39;t grok k8s yaml, this configuration creates two things:</p><ul>
<li>A <code>Deployment</code> (think of it as a set of <code>Pods</code> that can be scaled up or down and upgraded on a rolling basis) that runs three copies of <a href="https://nginx.org/en/">nginx</a> showing the default &#34;welcome to nginx&#34; page, with port 80 marked as &#34;open&#34; to other things.</li>
<li>A <code>ClusterIP Service</code> that exposes the nginx <code>Deployment</code>&#39;s port 80 to a stable IP address within the cluster. This cluster IP will be used by other services to talk to the nginx <code>Deployment</code>.</li>
</ul><p>Normally these <code>ClusterIP</code> services are only exposed in the cluster (as the name implies), but when you have overlay networks and subnet routing in the mix, you can do anything, such as poking the service from your laptop:</p></div></div>
<figure><a href="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/c4d36dd3-c8f1-4115-a504-48b9e6412fc8.jpg"><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/c4d36dd3-c8f1-4115-a504-48b9e6412fc8.avif"/><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/c4d36dd3-c8f1-4115-a504-48b9e6412fc8.webp"/><img alt="The &#39;welcome to nginx&#39; page on the url http://nginx.default.svc.alrest.xeserv.us, which is not publicly exposed to you." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/c4d36dd3-c8f1-4115-a504-48b9e6412fc8.jpg"/></picture></a><figcaption>The &#39;welcome to nginx&#39; page on the url http://nginx.default.svc.alrest.xeserv.us, which is not publicly exposed to you.</figcaption></figure>
<p>Once this is up, you&#39;re golden. You can start deploying more things to your cluster and then they can talk to eachother. One of the first things I deployed was a Reddit/Discord bot that I maintain for a community I&#39;ve been in for a long time. It&#39;s a simple stateless bot that only needs a single deployment to run. You can see its source code and deployment manifest <a href="https://github.com/Xe/x/tree/master/cmd/sapientwindex">here</a>.</p>
<p>The only weird part here is that I needed to set up secrets for handling the bot&#39;s Discord webhook. I don&#39;t have a secret vault set up (looking onto setting up the 1password one out of convenience because I already use it at home), so I yolo-created the secret with <code>kubectl create secret generic sapientwindex --from-literal=DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/1234567890/ABC123</code> and then mounted it into the pod as an environment variable. The relevant yaml snippet is under the <code>bot</code> container&#39;s <code>env</code> key:</p>
<pre><code><span><span>env</span><span>:</span>
</span><span>  <span>-</span> <span>name</span><span>:</span> DISCORD_WEBHOOK_URL
</span><span>    <span>valueFrom</span><span>:</span>
</span><span>      <span>secretKeyRef</span><span>:</span>
</span><span>        <span>name</span><span>:</span> sapientwindex
</span><span>        <span>key</span><span>:</span> DISCORD_WEBHOOK_URL
</span></code></pre>
<p>This is a little more verbose than I&#39;d like, but I understand why it has to be this way. Kubernetes is the most generic tool you can make, as such it has to be able to adapt to any workflow you can imagine. Kubernetes manifests can&#39;t afford to make too many assumptions, so they simply elect not to as much as possible. As such, you need to spell out all your assumptions by hand.</p>
<p>I&#39;ll get this refined in the future with templates or whatever, but for now my favorite part about it is that it works.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Why are you making your secrets environment variables instead of mounting them
as a filesystem?</p></div></div>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I want to have this as an environment variable because this bot was made with
the <a href="https://12factor.net/">12 factor app</a> methodology in mind. It&#39;s a
stateless bot that only needs a single environment variable to run, so I&#39;m
going to keep it that way. The bot is also already programmed to read from the
environment variable (but I could have it read the environment variable from
the
<a href="https://github.com/Xe/x/tree/master/internal/confyg/flagconfyg">flagconfyg</a>
file if I needed to). If there were more than 10 variables, I&#39;d probably mount
the secret as a flagconfyg or .env file instead. If I wanted to support
secrets as a filesystem, I&#39;d need to write some extra code to import a
directory tree as flag values as my /x/ repo (and other projects of mine) use
<a href="https://pkg.go.dev/flag">package flag</a> for managing secrets and other
configuration variables. I&#39;m lazy.</p></div></div>
<p>After I got that working, I connected some other nodes and I&#39;ve ended up with this:</p>
<pre><code><span>$ kubectl get nodes
</span><span>NAME        STATUS   ROLES           AGE   VERSION
</span><span>chrysalis   Ready    control-plane   20h   v1.30.0
</span><span>kos-mos     Ready    control-plane   20h   v1.30.0
</span><span>ontos       Ready    control-plane   20h   v1.30.0
</span></code></pre>
<p>The next big thing to get working is to get a bunch of operators working so that I can have my cluster dig its meaty claws into various other things.</p>
<h2>What the hell is an operator anyways?</h2>
<p>In Kubernetes land, an operator is a thing you install into your cluster that makes it integrate with another service or provides some functionality. For example, the <a href="https://developer.1password.com/docs/k8s/k8s-operator/">1Password operator</a> lets you import 1Password data into your cluster as Kubernetes secrets. It&#39;s effectively how you extend Kubernetes to do more things with the same Kubernetes workflow you&#39;re already used to.</p>
<p>One of the best examples of this is the 1Password operator I mentioned. It&#39;s how I&#39;m using 1Password to store secrets for my apps in my cluster. I can then edit them with the 1Password app on my PC or MacBook and the relevant services will restart automatically with the new secrets.</p>
<p>So I installed the operator with Helm and then it worked the first time. I was surprised, given how terrible Helm is in my experience.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Why is Helm bad? It&#39;s the standard way to install reusable things in
Kubernetes.</p></div></div>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Helm uses string templating to template structured data. It&#39;s like using <code>sed</code>
to template JSON. It works, but you have to abuse a lot of things like the
<a href="https://helm.sh/docs/chart_template_guide/yaml_techniques/#indenting-and-templates"><code>indent</code></a>
function in order for things to be generically applicable. It&#39;s a mess, but
only when you try and use it in earnest across your stack. It&#39;s what made me
nearly burn out of the industry.</p></div></div>
<div><p><img alt="Aoi is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Why is so much of this stuff just one or two steps away from being really
good?</p></div></div>
<div><p><img alt="Numa is delet" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/delet/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Venture capital! They used to have a way to do structured templates but it was
deprecated and removed in Helm 3.0, so we all get to suffer together.</p></div></div>
<p>The only hard part I ran into was that it wasn&#39;t obvious how I should assemble the reference strings for 1Password secrets. When you create the 1Password secret syncing object, it looks like this:</p>
<pre><code><span><span>apiVersion</span><span>:</span> onepassword.com/v1
</span><span><span>kind</span><span>:</span> OnePasswordItem
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> sapientwindex
</span><span><span>spec</span><span>:</span>
</span><span>  <span>itemPath</span><span>:</span> <span>&#34;vaults/lc5zo4zjz3if3mkeuhufjmgmui/items/cqervqahekvmujrlhdaxgqaffi&#34;</span>
</span></code></pre>
<p>This tells the operator to create a secret named <code>sapientwindex</code> in the default namespace with the item path <code>vaults/lc5zo4zjz3if3mkeuhufjmgmui/items/cqervqahekvmujrlhdaxgqaffi</code>. The item path is made up of the vault ID (<code>lc5zo4zjz3if3mkeuhufjmgmui</code>) and the item ID (<code>cqervqahekvmujrlhdaxgqaffi</code>). I wasn&#39;t sure how to get these in the first place, but I found the vault ID with the <code>op vaults list</code> command and then figured out you can enable right-clicking in the 1Password app to get the item ID.</p>
<p>To enable this, go to Settings -&gt; Advanced -&gt; Show debugging tools in the 1Password app. This will let you right-click any secret and choose &#34;Copy item UUID&#34; to get the item ID for these secret paths.</p>
<p>This works pretty great, I&#39;m gonna use this extensively going forward. It&#39;s gonna be slightly painful at first, but once I get into the flow of this (and realistically write a generator that pokes the 1password cli to scrape this information more easily) it should all even out.</p>
<h2>Trials and storage tribulations</h2>
<p>As I said at the end of <a href="https://cceckman.com/talks/2024/shashin/">my most recent conference talk</a>, storage is one of the most annoying bullshit things ever. It&#39;s extra complicated with Talos Linux in particular because of how it uses the disk. Most of the disks of my homelab are Talos&#39; &#34;ephemeral state&#34; partitions, which are used for temporary storage and wiped when the machine updates. This is great for many things, but not for persistent storage with <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumes/PersistntVolumeClaims</a>.</p>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>If you haven&#39;t used PersistentVolumes before, they are kinda like <a href="https://fly.io/docs/reference/volumes/">Fly
Volumes</a> or Docker Volumes. The main
difference is that a PersistentVolume is usually shared between hosts, so that
you can mount the same PersistentVolume on Pods located on multiple cluster
Nodes. It&#39;s really neat.
<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">StorageClasses</a>
let you handle things like data locality, backup policies, and more. This lets
you set up multiple providers so that you can have some things managed by your
cluster-local storage provider, some managed by the cloud provider, and some
super-yolo ones directly mounted to the host filesystem.</p></div></div>
<p>I have tried the following things:</p>
<ul>
<li><a href="https://longhorn.io/">Longhorn</a>: a distributed block storage thing for Kubernetes by the team behind Rancher. It&#39;s pretty cool, but I got stuck at trying to get it actually running on my cluster. The pods were just permanently stuck in the <code>Pending</code> state due to etcd not being able to discover itself.</li>
<li><a href="https://github.com/openebs/openebs">OpenEBS</a>: another distributed block storage thing for Kubernetes by some team of some kind. It claims to be the most widely used storage thing for Kubernetes, but I couldn&#39;t get it to work either.</li>
</ul>
<p>Among the things I&#39;ve realized when debugging this is that <em>no matter what</em>, many storage things for Kubernetes will hardcode the cluster DNS name to be <code>cluster.local</code>. I made my cluster use the DNS name <code>alrest.xeserv.us</code> following the advice of one of my SRE friends to avoid using &#34;fake&#34; DNS names as much as possible . This has caused me no end of trouble, as many things in the Kubernetes ecosystem assume that the cluster DNS name is <code>cluster.local</code>. It turns out that many Kubernetes ecosystem tools hard-assume the DNS name because the CoreDNS configs in many popular Kubernetes platforms (like AWS EKS, Azure whatever-the-heck, and GKE) have broken DNS configs that make relative DNS names not work reliably. As a result, people have hardcoded the DNS name to <code>cluster.local</code> in many places in both configuration and code.</p>
<div><p><img alt="Aoi is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Yet again pragmatism wins out over correctness in the most annoying ways. Why
does everything have to be so <em>bad</em>?</p></div></div>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>To be fair to the Kubernetes ecosystem maintainers, they are faced with a
pretty impossible task. They have to be able to be flexible enough to handle
random bespoke homelab clusters and whatever the cloud providers think is
sensible. That is such a wide range of configurations that I don&#39;t think it&#39;s
really possible to do anything <em>but</em> make those assumptions about how things
work. It&#39;s a shame that changing the cluster DNS name breaks so much, but it&#39;s
understandable because most cloud providers don&#39;t expose that setting to
users. It always sucks to use &#34;fake&#34; DNS names because they can and will
become top-level domains <a href="https://prinsfrank.nl/2019/02/26/With-the-new-dev-domains-googles-dont-be-evil-phase-is-a-distant-memory">like what happened with
<code>.dev</code></a>.
It would be nice if Kubernetes encouraged people to choose their own &#34;real&#34;
domain names, but it&#39;s understandable that they ended up with <code>cluster.local</code>
because <code>.local</code> <a href="https://www.iana.org/assignments/special-use-domain-names/special-use-domain-names.xhtml">is registered as a &#34;special-use domain
name&#34;</a>
but the IETF.</p></div></div>
<p>Fixing this was easy, I had to edit the CoreDNS ConfigMap to look like this:</p>
<pre><code><span><span>data</span><span>:</span>
</span><span>  <span>Corefile</span><span>:</span> <span>|</span><span>-</span>
</span><span>    .<span>:</span>53 <span>{</span>
</span><span>        errors
</span><span>        health <span>{</span>
</span><span>            lameduck 5s
</span><span>        <span>}</span>
</span><span>        ready
</span><span>        log . <span>{</span>
</span><span>            class error
</span><span>        <span>}</span>
</span><span>        prometheus <span>:</span><span>9153</span>
</span><span>
</span><span>        kubernetes cluster.local alrest.xeserv.us in<span>-</span>addr.arpa ip6.arpa <span>{</span>
</span><span>            pods insecure
</span><span>            fallthrough in<span>-</span>addr.arpa ip6.arpa
</span><span>        <span>}</span>
</span><span>        forward . /etc/resolv.conf
</span><span>        cache 30
</span><span>        loop
</span><span>        reload
</span><span>        loadbalance
</span><span>    <span>}</span>
</span></code></pre>
<p>I prepended the <code>cluster.local</code> &#34;domain name&#34; to the <code>kubernetes</code> block. Then I deleted the CoreDNS pods in the <code>kube-system</code> namespace and they were promptly restarted with the new configuration. This at least got me to the point where normal DNS things worked again.</p>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>I later found out I didn&#39;t need to do this. When CoreDNS sees the ConfigMap
update, it&#39;ll automatically reload the config. However, SRE instinct kicks in
when you&#39;re dealing with unknowns and sometimes the placebo effect of
restarting the damn thing by hand makes you feel better. Feeling better can be
way more important than actually fixing the problem, especially when you&#39;re
dealing with a lot of new technology.</p></div></div>
<div><p><img alt="Numa is delet" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/delet/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>There&#39;s no kill like overkill afterall!</p></div></div>
<p>However, this didn&#39;t get Longhorn working. The manager container was just stuck trying to get created. Turns out the solution was really stupid and I want to explain what&#39;s going on here so that you can properly commiserate with me over the half a day I spent trying to get this working.</p>
<p>Talos Linux sets a default security policy that blocks the Longhorn manager from running. This is because the Longhorn manager runs as root and Talos Linux is paranoid about security. In order to get Longhorn running, I had to add the following annotations to the Longhorn namespace:</p>
<pre><code><span><span>apiVersion</span><span>:</span> v1
</span><span><span>kind</span><span>:</span> Namespace
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> longhorn<span>-</span>system
</span><span>  <span>labels</span><span>:</span>
</span><span>    <span>pod-security.kubernetes.io/enforce</span><span>:</span> privileged
</span><span>    <span>pod-security.kubernetes.io/enforce-version</span><span>:</span> latest
</span><span>    <span>pod-security.kubernetes.io/audit</span><span>:</span> privileged
</span><span>    <span>pod-security.kubernetes.io/audit-version</span><span>:</span> latest
</span><span>    <span>pod-security.kubernetes.io/warn</span><span>:</span> privileged
</span><span>    <span>pod-security.kubernetes.io/warn-version</span><span>:</span> latest
</span></code></pre>
<p>After you do this, you need to delete the longhorn-deployer <em>Pod</em> and then wait about 10-15 minutes for the entire system to converge. For some reason it doesn&#39;t automatically restart when labels are changed, but that is very forgiveable given how many weird things are at play with this ecosystem. Either way, getting this working <em>at all</em> was a huge relief.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Wasn&#39;t Longhorn part of the SUSE acquistion?</p></div></div>
<div><p><img alt="Cadey is enby" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/enby/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Yes, but they also donated Longhorn to the CNCF, so it&#39;s going to be
maintained until it&#39;s inevitably deprecated in favor of yet another storage
product. Hopefully there&#39;s an easy migration path, but I&#39;m not going to worry
about this until I have to.</p></div></div>
<p>Once Longhorn starts up, you can create a PersistentVolumeClaim and attach it to a pod:</p>
<pre><code><span><span>apiVersion</span><span>:</span> v1
</span><span><span>kind</span><span>:</span> PersistentVolumeClaim
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> longhorn<span>-</span>volv<span>-</span>pvc
</span><span>  <span>namespace</span><span>:</span> default
</span><span><span>spec</span><span>:</span>
</span><span>  <span>accessModes</span><span>:</span>
</span><span>    <span>-</span> ReadWriteOnce
</span><span>  <span>storageClassName</span><span>:</span> longhorn
</span><span>  <span>resources</span><span>:</span>
</span><span>    <span>requests</span><span>:</span>
</span><span>      <span>storage</span><span>:</span> 256Mi
</span><span><span>---</span>
</span><span><span>apiVersion</span><span>:</span> v1
</span><span><span>kind</span><span>:</span> Pod
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> volume<span>-</span>test
</span><span>  <span>namespace</span><span>:</span> default
</span><span><span>spec</span><span>:</span>
</span><span>  <span>restartPolicy</span><span>:</span> Always
</span><span>  <span>containers</span><span>:</span>
</span><span>    <span>-</span> <span>name</span><span>:</span> volume<span>-</span>test
</span><span>      <span>image</span><span>:</span> nginx<span>:</span>stable<span>-</span>alpine
</span><span>      <span>imagePullPolicy</span><span>:</span> IfNotPresent
</span><span>      <span>livenessProbe</span><span>:</span>
</span><span>        <span>exec</span><span>:</span>
</span><span>          <span>command</span><span>:</span>
</span><span>            <span>-</span> ls
</span><span>            <span>-</span> /data/lost+found
</span><span>        <span>initialDelaySeconds</span><span>:</span> <span>5</span>
</span><span>        <span>periodSeconds</span><span>:</span> <span>5</span>
</span><span>      <span>volumeMounts</span><span>:</span>
</span><span>        <span>-</span> <span>name</span><span>:</span> vol
</span><span>          <span>mountPath</span><span>:</span> /data
</span><span>      <span>ports</span><span>:</span>
</span><span>        <span>-</span> <span>containerPort</span><span>:</span> <span>80</span>
</span><span>  <span>volumes</span><span>:</span>
</span><span>    <span>-</span> <span>name</span><span>:</span> vol
</span><span>      <span>persistentVolumeClaim</span><span>:</span>
</span><span>        <span>claimName</span><span>:</span> longhorn<span>-</span>volv<span>-</span>pvc
</span></code></pre>
<div><p><img alt="Cadey is facepalm" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/facepalm/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I feel so dumb right now. It was just a security policy mismatch.</p></div></div>
<div><p><img alt="Numa is happy" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/happy/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Hey, at least it was a dumb problem. Dumb problems are always so much easier
to deal with than the not-dumb problems. The not-dumb problems end up sucking
so much and drain you of your soul energy.</p></div></div>
<p>Longhorn ended up working, so I <a href="https://longhorn.io/docs/1.6.1/snapshots-and-backups/scheduling-backups-and-snapshots/">set up backups</a> to <a href="https://tigrisdata.com">Tigris</a> and then I plan to not think about it until I need to. The only catch is that I need to label every PersistentVolumeClaim with <code>recurring-job-group.longhorn.io/backup: enabled</code> to make my backup job run:</p>
<pre><code><span><span>apiVersion</span><span>:</span> longhorn.io/v1beta1
</span><span><span>kind</span><span>:</span> RecurringJob
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> backup
</span><span>  <span>namespace</span><span>:</span> longhorn<span>-</span>system
</span><span><span>spec</span><span>:</span>
</span><span>  <span>cron</span><span>:</span> <span>&#34;0 0 * * *&#34;</span>
</span><span>  <span>task</span><span>:</span> <span>&#34;backup&#34;</span>
</span><span>  <span>groups</span><span>:</span>
</span><span>    <span>-</span> default
</span><span>  <span>retain</span><span>:</span> <span>4</span>
</span><span>  <span>concurrency</span><span>:</span> <span>2</span>
</span></code></pre>
<div><p><img alt="Cadey is enby" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/enby/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Thanks for the backup space Ovais &amp; co! I wonder how efficient this really is
because most of the blocks (based on unscientific random clicking around in
the Tigris console) are under the threshold for <a href="https://www.tigrisdata.com/docs/overview/#fast-small-object-retrieval">being inlined to
FoundationDB</a>.
I&#39;ll have to ask them about it once I get some more significant data workloads
in the mix. Realistically, it&#39;s probably fine and will end up being a decent
stress test for them.</p></div></div>
<p>Hopefully I won&#39;t need to think about this for a while. At its best, storage is invisible.</p>
<h2>The <del>factory</del> cluster must grow</h2>
<p>I dug <code>logos</code> out of mothballs and then I plugged in the Talos Linux USB. I then ran the <code>logos</code> command to install Talos Linux on the machine. It worked perfectly and I had a new homelab node up and running in no time. All I had to do was:</p>
<ul>
<li>Get it hooked up to Ethernet and power</li>
<li>Boot it off of the Talos Linux USB stick</li>
<li>Apply the config with <code>talosctl</code> from my macbook</li>
<li>Wait for it to reboot and everything to green up in <code>kubectl</code></li>
</ul>
<p>That&#39;s it. This is what every homelab OS should strive to be.</p>
<p>I also tried to add my <a href="https://cceckman.com/blog/anbernic-win600-review/">Win600</a> to the cluster, but I don&#39;t think Talos supports wi-fi. I&#39;m asking in the Matrix channel and in a room full of experts. I was able to get it to connect to ethernet over USB in a hilariously jankriffic setup though:</p>
<figure><a href="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/a1f2dea0-158d-4ee4-b708-3802f54a734e.jpg"><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/a1f2dea0-158d-4ee4-b708-3802f54a734e.avif"/><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/a1f2dea0-158d-4ee4-b708-3802f54a734e.webp"/><img alt="An Anbernic Win600 with its screen sideways booted into Talos Linux. It is precariously mounted on the floor with power going in on one end and ethernet going in on the other. It is not a production-worthy setup." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/xedn/dynamic/a1f2dea0-158d-4ee4-b708-3802f54a734e.jpg"/></picture></a><figcaption>An Anbernic Win600 with its screen sideways booted into Talos Linux. It is precariously mounted on the floor with power going in on one end and ethernet going in on the other. It is not a production-worthy setup.</figcaption></figure>
<div><p><img alt="Aoi is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Why would you do this to yourself?</p></div></div>
<div><p><img alt="Numa is happy" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/happy/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Science isn&#39;t about why, it&#39;s about why not!</p></div></div>
<p>I seriously can&#39;t believe this works. It didn&#39;t work well enough to stay in production, but it&#39;s worth a laugh or two at least. I ended up removing this node so that I can have floor space back. I&#39;ll have to figure out how to get it on the network properly later, maybe after DevOpsDays KC.</p>
<h2>ingressd and related fucking up</h2>
<p>I was going to write about a super elegant hack that I&#39;m doing to get ingress from the public internet to my homelab here, but I fucked up again and I potentially got to do etcd surgery.</p>
<p>The hack I was trying to do was creating a userspace wireguard network for handling HTTP/HTTPS ingress from the public internet. I chose to use the network <code>10.255.255.0/24</code> for this (I had a TypeScript file to configure the WireGuard keys and everything). Apparently Talos Linux configured etcd to prefer anything in <code>10.0.0.0/8</code> by default. This has lead to the following bad state:</p>
<pre><code><span>$ talosctl etcd members -n 192.168.2.236
</span><span>NODE            ID                 HOSTNAME    PEER URLS                    CLIENT URLS                  LEARNER
</span><span>192.168.2.236   3a43ba639b0b3ec3   chrysalis   https://10.255.255.16:2380   https://10.255.255.16:2379   false
</span><span>192.168.2.236   d07a0bb98c5c225c   kos-mos     https://10.255.255.17:2380   https://192.168.2.236:2379   false
</span><span>192.168.2.236   e78be83f410a07eb   ontos       https://10.255.255.19:2380   https://192.168.2.237:2379   false
</span><span>192.168.2.236   e977d5296b07d384   logos       https://10.255.255.18:2380   https://192.168.2.217:2379   false
</span></code></pre>
<p>This is uhhh, not good. The normal strategy for recovering from an etcd split brain involves stopping etcd on all nodes and then recovering one of them, but I can&#39;t do that because <code>talosctl</code> doesn&#39;t let you stop etcd:</p>
<pre><code><span>$ talosctl service etcd  -n 192.168.2.196 stop
</span><span>error starting service: 1 error occurred:
</span><span>        * 192.168.2.196: rpc error: code = Unknown desc = service &#34;etcd&#34; doesn&#39;t support stop operation via API
</span></code></pre>
<p>When you get etcd into this state, it is generally very hard to convince it otherwise without doing database surgery and suffering the pain of having fucked it up. Fixing this is a very <em>doable</em> process, but I didn&#39;t really wanna deal with it.</p>
<p>I ended up blowing away the cluster and starting over. I tried using TESTNET (192.0.2.0/24) for the IP range but ran into issues where my super hacky userspace WireGuard code wasn&#39;t working right. I gave up at this point and ended up using my existing WireGuard mesh for ingress. I&#39;ll have to figure out how to do this properly later.</p>
<div><p><img alt="Cadey is facepalm" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/facepalm/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>While I was resetting the cluster, I ran into a kinda hilarious problem:
asking Talos nodes to wipe their disk and reset all state makes them wipe
<em>everything</em>, including the system partition. I did ask it to wipe
<em>everything</em>, but I didn&#39;t think it would nuke the OS too. It was kind of a
hilarious realization when I ended up figuring out what I did, but it&#39;s good
to know that &#34;go die&#34; means that it will kill everything. That&#39;s kind of a
dangerous call to expose without some kind of confirmation, but I guess that
is at the pay-to-win tier with <a href="https://www.siderolabs.com/pricing/">Sidero Labs
support</a>. I&#39;ll probably end up paying for
a hobby subscription at some point, just to support the company powered my
homelab&#39;s hopes and dreams. Money does tend to let one buy goods and services.</p></div></div>
<p><code>ingressd</code> ended up becoming a simple TCP proxy with added <a href="http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">PROXY protocol</a> support so that ingress-nginx could get the right source IP addresses. It&#39;s nothing to write home about, but it&#39;s my simple TCP proxy that I probably could have used something off the shelf for.</p>
<div><p><img alt="Numa is delet" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/delet/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>The not-invented-here is strong with this one.</p></div></div>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Something something future expansion when I have time/energy.</p></div></div>
<h3>Using <code>ingressd</code> (why would you do this to yourself)</h3>
<p>If you want to install <code>ingressd</code> for your cluster, here&#39;s the high level steps:</p>
<div><p><img alt="Mara is hmm" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hmm/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>Keep in mind, <code>ingressd</code> has no real support. If you run it, you are on your
own. Good luck if so!</p></div></div>
<ol>
<li>Gather the secret keys needed for this terraform manifest (change the domain for Route 53 to your own domain): <a href="https://github.com/Xe/x/blob/master/cmd/ingressd/tf/main.tf">https://github.com/Xe/x/blob/master/cmd/ingressd/tf/main.tf</a></li>
<li>Run <code>terraform apply</code> in the directory with the above manifest</li>
<li>Go a level up and run <code>yeet</code> to build the ingressd RPM</li>
<li>Install the RPM on your ingressd node</li>
<li>Create the file <code>/etc/ingressd/ingressd.env</code> with the following contents:</li>
</ol>
<pre><code><span>HTTP_TARGET=serviceIP:80
</span><span>HTTPS_TARGET=serviceIP:443
</span></code></pre>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>Fetch the service IP from <code>kubectl get svc -n ingress-nginx</code> and replace
<code>serviceIP</code> with it.</p></div></div>
<p>This assumes you are subnet routing your Kubernetes node and service network over your WireGuard mesh of choice. If you are not doing that, you can&#39;t use <code>ingressd</code>.</p>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>This is why I wanted to use a userspace WireGuard connection for this. If I
end up implementing this properly, I&#39;m probably gonna end up using two
binaries: one on the public ingressd host, and an ingressd-buddy that runs in
the cluster.</p></div></div>
<p>Also make sure to run the magic firewalld unbreaking commands:</p>
<pre><code><span>firewall-cmd --zone=public --add-service=http
</span><span>firewall-cmd --zone=public --add-service=https
</span></code></pre>
<div><p><img alt="Cadey is percussive-maintenance" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/percussive-maintenance/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I always forget the magic firewalld unbreaking commands.</p></div></div>
<h2>It&#39;s always DNS</h2>
<p>Now that I have <a href="http://ingressd.cetacean.club/">ingress working</a>, it&#39;s time for one of the most painful things in the universe: DNS. Since I&#39;ve used Kubernetes last, <a href="https://github.com/kubernetes-sigs/external-dns">External DNS</a> is now production-ready. I&#39;m going to use it to manage the DNS records for my services.</p>
<p>In typical Kubernetes fashion, it seems that it has gotten incredibly complicated since the last time I used it. It used to be fairly simple, but now installing it requires you to really consider what the heck you are doing. There&#39;s also no Helm chart, so you&#39;re <em>really</em> on your own.</p>
<p>After reading some documentation, I ended up on the following Kubernetes manifest: <a href="https://gist.githubusercontent.com/Xe/8d4d960bcad372a7a2b04265b9eba21c/raw/1b430cf723f1877e764f72d9db720da95f95616b/external-dns.yaml">external-dns.yaml</a>. So that I can have this documented for me as much as it is for you, here is what this does:</p>
<ol>
<li>Creates a namespace <code>external-dns</code> for <code>external-dns</code> to live in.</li>
<li>Creates the <a href="https://kubernetes-sigs.github.io/external-dns/v0.14.1/contributing/crd-source/"><code>external-dns</code> Custom Resource Definitions (CRDs)</a> so that I can make DNS records manually with Kubernetes objects should the spirit move me.</li>
<li>Creates a service account for <code>external-dns</code>.</li>
<li>Creates a cluster role and cluster role binding for <code>external-dns</code> to be able to read a small subset of Kubernetes objects (services, ingresses, and nodes, as well as its custom resources).</li>
<li>Creates a <a href="https://developer.1password.com/docs/k8s/k8s-operator/">1Password secret</a> to give <code>external-dns</code> Route53 god access.</li>
<li>Creates two deployments of <code>external-dns</code>:
<ul>
<li>One for the CRD powered external DNS to weave DNS records with YAML</li>
<li>One to match on newly created ingress objects and create DNS records for them</li>
</ul>
</li>
</ol>
<div><p><img alt="Aoi is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>Jesus christ that&#39;s a lot of stuff. It makes sense when you&#39;re explaining how
it all builds up, but it&#39;s a lot.</p></div></div>
<div><p><img alt="Numa is happy" loading="lazy" src="https://cdn.xeiaso.net/sticker/numa/happy/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#numa"><b>Numa</b></a>&gt; </p><p>Welcome to Kubernetes! It&#39;s YAML turtles all the way down.</p></div></div>
<p>If I ever need to create a DNS record for a service, I can do so with the following YAML:</p>
<pre><code><span><span>apiVersion</span><span>:</span> externaldns.k8s.io/v1alpha1
</span><span><span>kind</span><span>:</span> DNSEndpoint
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> something
</span><span><span>spec</span><span>:</span>
</span><span>  <span>endpoints</span><span>:</span>
</span><span>    <span>-</span> <span>dnsName</span><span>:</span> something.xeserv.us
</span><span>      <span>recordTTL</span><span>:</span> <span>180</span>
</span><span>      <span>recordType</span><span>:</span> TXT
</span><span>      <span>targets</span><span>:</span>
</span><span>        <span>-</span> <span>&#34;We&#39;re no strangers to love&#34;</span>
</span><span>        <span>-</span> <span>&#34;You know the rules and so do I&#34;</span>
</span><span>        <span>-</span> <span>&#34;A full commitment&#39;s what I&#39;m thinking of&#34;</span>
</span><span>        <span>-</span> <span>&#34;You wouldn&#39;t get this from any other guy&#34;</span>
</span><span>        <span>-</span> <span>&#34;I just wanna tell you how I&#39;m feeling&#34;</span>
</span><span>        <span>-</span> <span>&#34;Gotta make you understand&#34;</span>
</span><span>        <span>-</span> <span>&#34;Never gonna give you up&#34;</span>
</span><span>        <span>-</span> <span>&#34;Never gonna let you down&#34;</span>
</span><span>        <span>-</span> <span>&#34;Never gonna run around and hurt you&#34;</span>
</span><span>        <span>-</span> <span>&#34;Never gonna make you cry&#34;</span>
</span><span>        <span>-</span> <span>&#34;Never gonna say goodbye&#34;</span>
</span><span>        <span>-</span> <span>&#34;Never gonna tell a lie and hurt you&#34;</span>
</span></code></pre>
<p>Hopefully I&#39;ll never need to do this, but I bet that <em>something</em> will make me need to make a DNS TXT record at some point, and it&#39;s probably better to have that managed in configuration management somehow.</p>
<h2>cert-manager</h2>
<p>Now that there&#39;s ingress from the outside world and DNS records for my services, it&#39;s time to get HTTPS working. I&#39;m going to use <a href="https://cert-manager.io/">cert-manager</a> for this. It&#39;s a Kubernetes native way to manage certificates from Let&#39;s Encrypt and other CAs.</p>
<p>Unlike nearly everything else in this process, installing cert-manager was relatively painless. I just had to install it with Helm. I also made Helm manage the Custom Resource Definitions, so that way I can easily upgrade them later.</p>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>This is probably a mistake, Helm doesn&#39;t handle Custom Resource Definition
updates gracefully. This will be corrected in the future, but right now the
impetus to care is very low.</p></div></div>
<p>The only gotcha here is that there&#39;s annotations for Ingresses that you need to add to get cert-manager to issue certificates for them. Here&#39;s an example:</p>
<pre><code><span><span>apiVersion</span><span>:</span> networking.k8s.io/v1
</span><span><span>kind</span><span>:</span> Ingress
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> kuard
</span><span>  <span>annotations</span><span>:</span>
</span><span>    <span>cert-manager.io/cluster-issuer</span><span>:</span> <span>&#34;letsencrypt-prod&#34;</span>
</span><span><span>spec</span><span>:</span>
</span><span>  
</span></code></pre>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>What&#39;s the difference between a label and an annotation anyways? So far it
looks like you&#39;ve been using them interchangeably.</p></div></div>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Labels are intended to be used to help find and select objects, while annotations are for more detailed information. Labels are limited to 64 bytes. The most common label you will see is the <code>app</code> or <code>app.kubernetes.io/name</code> label which points to the &#34;app&#34; an object is a part of. Annotations are much more intended for storing metadata about the object, and can be up to 256KB in size. They are intended to be used for things like machine-readable data, like the cert-manager issuer annotation.</p></div></div>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p></div>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Because Kubernetes labels are indexed in the cluster data store, observe:</p><pre><code><span>$ kubectl get Volume --all-namespaces -l=recurring-job-group.longhorn.io/backup=enabled
</span><span>NAMESPACE         NAME                                       DATA ENGINE   STATE      ROBUSTNESS   SCHEDULED   SIZE         NODE    AGE
</span><span>longhorn-system   pvc-e1916e66-7f7b-4322-93cd-52dc1fc418f7   v1            attached   healthy                  2147483648   logos   43h
</span></code></pre><p>That also extends to other labels, such as <code>app.kubernetes.io/name</code>:</p><pre><code><span>$ kubectl get all,svc,ing -n mi -l app.kubernetes.io/name=mi
</span><span>NAME                      READY   STATUS    RESTARTS   AGE
</span><span>pod/mi-6bd6d8bb44-cg7tf   1/1     Running   0          43h
</span><span>
</span><span>NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
</span><span>deployment.apps/mi   1/1     1            1           43h
</span><span>
</span><span>NAME                            DESIRED   CURRENT   READY   AGE
</span><span>replicaset.apps/mi-6bd6d8bb44   1         1         1       43h
</span><span>
</span><span>NAME                                        CLASS       HOSTS              ADDRESS                    PORTS     AGE
</span><span>ingress.networking.k8s.io/mi-public         nginx       mi.cetacean.club   100.109.37.97              80, 443   20h
</span></code></pre><p>Annotations are more useful for meta-information and machine-readable data, like the cert-manager issuer annotation. You could also use it to attribute deployments to a specific git commit or something like that.</p></div></div>
<p>This will make <code>cert-manager</code> issue a certificate for the <code>kuard</code> ingress using the <code>letsencrypt-prod</code> issuer. You can also use <code>letsencrypt-staging</code> for testing. The part that you will fuck up is that the documentation mixes <code>ClusterIssuer</code> and <code>Issuer</code> resources and annotations. Here&#39;s what my Let&#39;s Encrypt staging and prod issuers look like:</p>
<pre><code><span><span>apiVersion</span><span>:</span> cert<span>-</span>manager.io/v1
</span><span><span>kind</span><span>:</span> ClusterIssuer
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> letsencrypt<span>-</span>staging
</span><span><span>spec</span><span>:</span>
</span><span>  <span>acme</span><span>:</span>
</span><span>    
</span><span>    
</span><span>    
</span><span>    <span>email</span><span>:</span> user@example.com
</span><span>    <span>server</span><span>:</span> https<span>:</span>//acme<span>-</span>staging<span>-</span>v02.api.letsencrypt.org/directory
</span><span>    <span>privateKeySecretRef</span><span>:</span>
</span><span>      
</span><span>      <span>name</span><span>:</span> letsencrypt<span>-</span>staging<span>-</span>acme<span>-</span>key
</span><span>    <span>solvers</span><span>:</span>
</span><span>      <span>-</span> <span>http01</span><span>:</span>
</span><span>          <span>ingress</span><span>:</span>
</span><span>            <span>ingressClassName</span><span>:</span> nginx
</span><span><span>---</span>
</span><span><span>apiVersion</span><span>:</span> cert<span>-</span>manager.io/v1
</span><span><span>kind</span><span>:</span> ClusterIssuer
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> letsencrypt<span>-</span>prod
</span><span><span>spec</span><span>:</span>
</span><span>  <span>acme</span><span>:</span>
</span><span>    
</span><span>    
</span><span>    
</span><span>    <span>email</span><span>:</span> user@example.com
</span><span>    <span>server</span><span>:</span> https<span>:</span>//acme<span>-</span>v02.api.letsencrypt.org/directory
</span><span>    <span>privateKeySecretRef</span><span>:</span>
</span><span>      
</span><span>      <span>name</span><span>:</span> letsencrypt<span>-</span>prod<span>-</span>acme<span>-</span>key
</span><span>    <span>solvers</span><span>:</span>
</span><span>      <span>-</span> <span>http01</span><span>:</span>
</span><span>          <span>ingress</span><span>:</span>
</span><span>            <span>ingressClassName</span><span>:</span> nginx
</span></code></pre>
<p>These <code>ClusterIssuers</code> are what the <code>cert-manager.io/cluster-issuer:</code> annotation in the ingress object refers to. You can also use <code>Issuer</code> resources if you want to scope the issuer to a single namespace, but realistically I know you&#39;re lazier than I am so you&#39;re going to use <code>ClusterIssuer</code>.</p>
<p>The flow of all of this looks kinda complicated, but you can visualize it with this handy diagram:</p>
<p><img src="https://cceckman.com/static/blog/cert-manager-flow.svg" alt="A diagram explaining the cert-manager flow"/></p>
<p>Breaking this down, let&#39;s assume I&#39;ve just created this Ingress resource:</p>
<pre><code><span><span>apiVersion</span><span>:</span> networking.k8s.io/v1
</span><span><span>kind</span><span>:</span> Ingress
</span><span><span>metadata</span><span>:</span>
</span><span>  <span>name</span><span>:</span> kuard
</span><span>  <span>annotations</span><span>:</span>
</span><span>    <span>cert-manager.io/cluster-issuer</span><span>:</span> <span>&#34;letsencrypt-prod&#34;</span>
</span><span><span>spec</span><span>:</span>
</span><span>  <span>ingressClassName</span><span>:</span> nginx
</span><span>  <span>tls</span><span>:</span>
</span><span>    <span>-</span> <span>hosts</span><span>:</span>
</span><span>        <span>-</span> kuard.xeserv.us
</span><span>      <span>secretName</span><span>:</span> kuard<span>-</span>tls
</span><span>  <span>rules</span><span>:</span>
</span><span>    <span>-</span> <span>host</span><span>:</span> kuard.xeserv.us
</span><span>      <span>http</span><span>:</span>
</span><span>        <span>paths</span><span>:</span>
</span><span>          <span>-</span> <span>path</span><span>:</span> /
</span><span>            <span>pathType</span><span>:</span> Prefix
</span><span>            <span>backend</span><span>:</span>
</span><span>              <span>service</span><span>:</span>
</span><span>                <span>name</span><span>:</span> kuard
</span><span>                <span>port</span><span>:</span>
</span><span>                  <span>name</span><span>:</span> http
</span></code></pre>
<div><p><img alt="Mara is hacker" loading="lazy" src="https://cdn.xeiaso.net/sticker/mara/hacker/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#mara"><b>Mara</b></a>&gt; </p><p>This is an Ingress named <code>kuard</code> with the <code>letsencrypt-prod</code> certificate
issuer. It&#39;s specifically configured to use ingress-nginx (this is probably
not required if your cluster has only one ingress defined, but it&#39;s better to
be overly verbose) and matches the HTTP hostname <code>kuard.xeserv.us</code>. It points
to the service <code>kuard</code>&#39;s named <code>http</code> port (whatever that is). The TLS block
tells ingress-nginx (and cert-manager) to expect a cert in the secret
<code>kuard-tls</code> for the domain name <code>kuard.xeserv.us</code>.</p></div></div>
<p>When I create this Ingress with the cluster-issuer annotation, it&#39;s discovered by both external-dns and cert-manager. external-dns creates DNS records in Route 53 (and tracks them using DynamoDB). At the same time, cert-manager creates a Cert resource for the domains I specified in the in the <code>spec.tls.hosts</code> field of my Ingress. The Cert resource discoveres that the secret <code>kuard-tls</code> has no valid certificate in it, so it creates an Order for a new certificate. The Order creates a Challenge by poking Let&#39;s Encrypt to get the required token and then configures its own Ingress to handle the HTTP-01 strategy.</p>
<p>Once it&#39;s able to verify that it can pass the Challenge locally (this requires external-dns to finish pushing DNS routes and for DNS to globally converge, but usually happens within a minute), it asks Let&#39;s Encrypt to test it. Once Let&#39;s Encrypt passes the test, it signs my certificate, the Challenge Ingress is deleted, the signed certificate is saved to the right Kubernetes secret, the Order is marked as fulfilled, the Cert is marked ready, and nginx is reloaded to point to that newly minted certificate.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>I guess that does make sense when you spell it all out, but that is a <em>lot</em> of boilerplate and interplay to do something that <a href="https://pkg.go.dev/golang.org/x/crypto/acme/autocert">autocert</a> does for you for free.</p></div></div>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>The way you should interpret this is that each of the Kubernetes components are <em>stateless</em> as much as possible. All of the state is stored externally in Kubernetes objects. This means that any of the components involved can restart, get moved around between nodes, or crash without affecting any in-flight tasks. I&#39;ve been understanding this as having all of the inherent complexity of what is going on laid bare in front of you, much like how Rust makes it very obvious what is going on at a syntax level.</p><p>You&#39;re probably used to a lot of this being handwaved away by the platforms you use, but this all isn&#39;t really that hard. It&#39;s just a verbose presentation of it. Besides, most of this is describing how I&#39;m handwaving all of this away for my own uses.</p></div></div>
<div><p><img alt="Aoi is cheer" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/cheer/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>I get it, you&#39;re basically making your homelab into your own platform, right? This means that you need to provide all those building blocks for yourself. I guess this explains why there&#39;s so many moving parts.</p></div></div>
<h2>Shipping the lab</h2>
<p>At this point, my lab is stable, useful, and ready for me to put jobs on it. I have:</p>
<ul>
<li>A cluster of four machines running Talos Linux that I can submit jobs to with <code>kubectl</code></li>
<li>Persistent storage with Longhorn</li>
<li>Backups of said persistent storage to <a href="https://www.tigrisdata.com/">Tigris</a></li>
<li>Ingress from the public internet with <code>ingressd</code> and crimes</li>
<li>DNS records managed by <code>external-dns</code></li>
<li>HTTPS certificates managed by <code>cert-manager</code></li>
</ul>
<p>And this all adds up to a place where I can just throw jobs at and get the confidence that they will run. I&#39;m going to be using this to run a bunch of other things that have previously been spread across a bunch of VPSes that I don&#39;t want to pay for anymore. Even though they are an excellent tax break right now.</p>
<div><p><img alt="Cadey is enby" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/enby/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#cadey"><b>Cadey</b></a>&gt; </p><p>I guess I did end up using Rocky Linux afterall because the ingressd node runs
it. It&#39;s a bone-stock image with automatic updates and a single RPM built by
<code>yeet</code>. Realistically I could probably get away with running a few ingressd
nodes, but I&#39;m lazy and I don&#39;t want to have to manage more than one. High
availability is for production, not janky homelab setups.</p></div></div>
<h2>The parts of a manifest for my homelab</h2>
<p>When I deploy things to my homelab cluster, I can divide them into three basic classes:</p>
<ul>
<li>Automation/bots that don&#39;t expose any API or web endpoints</li>
<li>Internal services that do expose API or web endpoints</li>
<li>Public-facing services that should be exposed to the public internet</li>
</ul>
<p>Automation/bots are the easiest. In the ideal case all I need is a Deployment and a Secret to hold the API keys to poke external services. For an example of that, see <a href="https://github.com/Xe/x/tree/master/cmd/sapientwindex">within.website/x/cmd/sapientwindex</a>.</p>
<p>Internal services get a little bit more complicated. Depending on the service in question, it&#39;ll probably get:</p>
<ul>
<li>A Namespace to hold everything</li>
<li>A PersistentVolumeClaim for holding state (SQLite, JSONMutexDB, etc.)</li>
<li>A Secret or two for the various private/API keys involved in the process</li>
<li>A Deployment with one or more containers that actually runs that internal service&#39;s code</li>
<li>A Service that exposes the internal ports to the cluster on well-known port numbers</li>
</ul>
<p>For most internal services, this is more than good enough. If I need to poke it, I can do so by connecting to <code>svcname.ns.svc.alrest.xeserv.us</code>. It&#39;d work great for a Minecraft server or something else like that.</p>
<p>However, I also do need to expose things to the public internet sometimes. When I need to do that, I define an Ingress that has the right domain name so the rest of the stack will just automatically make it work.</p>
<p>This gives me the ability to just push things to the homelab without fear. Once new jobs get defined, the rest of the stack will converge, order certificates, and otherwise make things Just Work™️. It&#39;s kinda glorious now that it&#39;s all set up.</p>
<h2>What&#39;s next?</h2>
<p>Here are the things I want to play with next:</p>
<ul>
<li><a href="https://kubevirt.io/">KubeVirt</a>: I want to run some VMs on my cluster. This looks like it could be the basis for an even better <a href="https://cceckman.com/blog/series/waifud/">waifud</a> setup. All it&#39;s missing is a decent admin UI, which I can probably make with Tailwind and HTMX. The biggest thing I want to play with is live migration of VMs between nodes.</li>
<li>I want to get a Minecraft server running on my cluster and figure out some way to make it accessible to my patrons. I have no idea how I&#39;ll go about doing the latter, but I&#39;m sure I&#39;ll figure it out. Worst case I think one of you nerds in the (patron-only) Discord has <em>some</em> ideas.</li>
<li>I want to resurrect <a href="https://tulpa.dev/cadey/kubermemes">kubermemes</a> for generating my app deployments. I&#39;ve had a lot of opinions change since I wrote that so many years ago, but overall the shape of my deployments is going to be &#34;small file with some resource requests&#34; that compiles into &#34;YAML means &#39;Yeah, A Massive List of stuff&#39;&#34;.</li>
<li>I may also want to get AI stuff running on Talos Linux. I have two GPUs in that cluster and kinda want to have stable diffusion at home again. It&#39;ll be a good way to get back into the swing of things with AI stuff.</li>
<li>??? who knows what else will come up through my binges into weird GitHub projects and Hacker News.</li>
</ul>
<p>I&#39;m willing to declare my homelab a success at this point. It&#39;s cool that I can spread the load between my machines so much more cleanly now. I&#39;m excited to see what I can do with this, and I hope that you are excited that you get more blog posts out of it.</p>
<div><p><img alt="Aoi is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/coffee/128"/></p><div><p>&lt;<a href="https://cceckman.com/characters#aoi"><b>Aoi</b></a>&gt; </p><p>What a way to kill a week of PTO, eh?</p></div></div>

    <hr/>

    

    

    <p>Facts and circumstances may have changed since publication. Please contact me before jumping to conclusions if something seems wrong or unclear.</p>

    <p>Tags: Homelab, RockyLinux, FedoraCoreOS, TalosLinux, Kubernetes, Ansible, Longhorn, Nginx, CertManager, ExternalDNS</p>
</article>
        </div></div>
  </body>
</html>
