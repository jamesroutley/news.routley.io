<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://surma.dev/things/js-to-asc/">Original</a>
    <h1>Is WebAssembly magic performance pixie dust? (2021)</h1>
    
    <div id="readability-page-1" class="page"><div lang="en"> <p>Add WebAssembly, get performance. Is that how it really works?</p> <!-- more -->  <p>The incredibly unsatisfying answer is: It depends. It depends on oh-so-many factors, and I’ll be touching on <em>some</em> of them here.</p> <h2>Why am I doing this? (You can skip this)</h2> <p>I really like <a href="https://assemblyscript.org">AssemblyScript</a> (full disclosure: I am one of their backers). It’s a very young language with a small but passionate team that built a custom compiler for a TypeScript-like language targeting WebAssembly. The reason I like AssemblyScript (or ASC for short) is because it allows the average web developer to make use of WebAssembly without having to learn a potentially new language like C++ or Rust. It’s important to note that the language is TypeScript-<em>like</em>. Don’t expect your existing TypeScript code to just compile out of the box. That being said, the language is intentionally mirroring the behaviors and semantics of TypeScript (and therefore JavaScript), which means that the act of “porting” TypeScript to AssemblyScript are often mostly cosmetic, usually just adding type annotations.</p> <p>I always wondered if there is anything to gain from taking a piece of JavaScript, turning it into AssemblyScript and compiling it to WebAssembly. When my colleague <a href="https://twitter.com/rreverser">Ingvar</a> sent me a <a href="https://github.com/nodeca/glur">piece of JavaScript</a> code to blur images, I thought that this would be a perfect case study. I ran a quick experiment to see if it’s worth doing a deeper exploration into porting JavaScript to AssemblyScript. And <em>oh boy</em> was it worth it. This article is that deeper exploration.</p> <p>If you want to know more about AssemblyScript, go to the <a href="https://assemblyscript.org">website</a>, join the <a href="https://discord.gg/assemblyscript">Discord</a> or, if you fancy, watch the <a href="https://www.youtube.com/watch?v=u0Jgz6QVJqg">AssemblyScript intro video</a> I made with my podcast husband <a href="https://twitter.com/jaffathecake">Jake</a>.</p> <h2>Advantages of WebAssembly</h2> <p>I think it is fair to say that most mature use-case for WebAssembly is tapping into the ecosystem of other languages. In <a href="https://squoosh.app">Squoosh</a>, for example, we use libraries from the C/C++ and Rust ecosystem to process images. These libraries were not written with the web in mind, but through WebAssembly, they can run there anyway.</p> <p>WebAssembly, in my perception, is also strongly associated with performance by a lot of people. It was designed to be fast, and it’s compiled, so it’s gotta be fast, right? Well, for the longest time <a href="https://www.youtube.com/watch?v=njt-Qzw0mVY&amp;t=1064s">I have been vocal that WebAssembly and JavaScript have the same <em>peak</em> performance</a>, and I still stand behind that. Given ideal conditions, they both compile to machine code and end up being equally fast. But there’s obviously more nuance here, and when have conditions <em>ever</em> been ideal on the web‽ Instead, I think it would be better if we thought about WebAssembly as a way to get more reliable performance.</p> <p>However, it’s also important to realize that WebAssembly has recently been getting access to performance primitives (like SIMD or shared-memory threads) that JavaScript cannot utilize, giving WebAssembly an increased chance to out-perform JavaScript. There are also some other qualities of WebAssembly that might make it better suited in specific situations than JavaScript:</p> <h3>No warmup</h3> <p>For V8 to execute JavaScript, it first gives the code to the interpreter “Ignition”. Ignition is optimized to make code run as <em>soon</em> as possible. Afterwards, “Sparkplug” takes Ignition’s output (the infamous “bytecode”) and turns it into non-optimized machine code, yielding better performance at the cost of increased memory footprint. While your code is executing, it is closely observed by V8 to gather data on object shapes (think of them like types). Once sufficient data has been collected, V8’s optimizing compiler “TurboFan” kicks in and generates low-level machine code that is optimized for those types. This will give another significant speed boost.</p> <blockquote> <p><strong>It’s tradeoffs all the way down:</strong> If you want to learn more about the exact tradeoffs JavaScripts engines have to make, I can recommend this <a href="https://mathiasbynens.be/notes/prototypes#tradeoffs">article</a> by <a href="https://twitter.com/bmeurer">Benedikt</a> and <a href="https://twitter.com/mathias">Mathias</a>.</p> </blockquote> <p>WebAssembly, on the other hand, is strongly typed. It can be turned into machine code <em>straight away</em>. V8 has a streaming Wasm compiler called “Liftoff“ which, like Ignition, is geared to get your code running <em>quickly</em>, at the cost of generating potentially suboptimal execution speed. The second Liftoff is done, TurboFan kicks in and generates optimized machine code that will run faster than what Liftoff produced, but will take longer to generate. The big difference to JavaScript is that TurboFan can do its work without having to observe your Wasm first.</p> <h3>No tierdown</h3> <p>The machine code that TurboFan generates for JavaScript is only usable for as long as the assumptions about types hold. If TurboFan generated machine code for a function <code>f</code> with a number as a parameter, and now all of a sudden that function <code>f</code> gets called with an object, the engine has to fall back to Ignition or Sparkplug. That’s called a “deoptimization” (or “deopt” for short). Again, because WebAssembly is strongly typed, the types <em>can’t</em> change. Not only that, but the types that WebAssembly supports are designed to map well to machine code. Deopts can’t happen with WebAssembly.</p> <h3>Binary size</h3> <p>Now this one is a bit elusive. According to <a href="https://webassembly.org/">webassembly.org</a>, “the wasm stack machine is designed to be encoded in a size- and load-time-efficient binary format.” And yet, WebAssembly is currently somewhat notorious for generating big binary blobs, at least by what is considered “big” on the web. WebAssembly compresses very well (via gzip or brotli), which can undo a lot of the bloat.</p> <p>It is easy to forget that JavaScript comes with a lot of batteries included (despite the claim that it doesn’t have a standard library). For example: You can handle arrays, objects, iterate over keys and values, split strings, filter, map, have prototypical inheritance and so on and so forth. All that is built into the JavaScript engine. WebAssembly comes with <em>nothing</em>, except arithmetic. Whenever you use any of these higher-level concepts in a language that compiles to WebAssembly, the underpinning code will have to get bundled into your binary, which is one of the causes for big WebAssembly binaries. Of course those functions will only have to be included once, so bigger projects will benefit more from Wasm’s small binary representation than small modules.</p> <p>Not all of these advantages are equally available or important in any given scenario. However, AssemblyScript is known to generate rather small WebAssembly binaries and I was curious how it can hold up in terms of speed and size with <em>directly</em> comparable JavaScript.</p> <h2>Porting to AssemblyScript</h2> <p>As mentioned, AssemblyScript mimics TypeScript’s semantics and Web Platform APIs as much as possible, which means porting a piece of JS to ASC is <em>mostly</em> a matter of adding type annotations to the code. As a first example, I took <a href="https://github.com/nodeca/glur"><code>glur</code></a>, a JavaScript library that blurs images.</p> <h3>Adding types</h3> <p>ASC’s built-in types mirror the types of the WebAssembly VM. While numeric values in TypeScript are just <code>number</code> (a 64-bit <a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE754</a> float according to the spec), AssemblyScript has <code>u8</code>, <code>u16</code>, <code>u32</code>, <code>i8</code>, <code>i16</code>, <code>i32</code>, <code>f32</code> and <code>f64</code> as its primitive types. The <a href="https://www.assemblyscript.org/stdlib/globals.html">small-but-sufficiently-powerful standard library of ASC</a> adds higher-level data structures like <code>string</code>, <code>Array&lt;T&gt;</code>, <code>ArrayBuffer</code>, <code>Uint8Array</code> etc. The only ASC-specific data structure, that is neither in JavaScript nor the Web Platform, is <code>StaticArray</code>, which I will talk about a bit later.</p> <p>As an example, here is a function from the glur library and its AssemblyScript’ified counterpart:</p> <pre><code><span>function</span> <span>gaussCoef</span><span>(</span><span>sigma</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span>sigma <span>&lt;</span> <span>0.5</span><span>)</span>
    sigma <span>=</span> <span>0.5</span><span>;</span>

  <span>var</span> a <span>=</span> Math<span>.</span><span>exp</span><span>(</span><span>0.726</span> <span>*</span> <span>0.726</span><span>)</span> <span>/</span> sigma<span>;</span>
  

  <span>return</span> <span>new</span> <span>Float32Array</span><span>(</span><span>[</span>
    a0<span>,</span> a1<span>,</span> a2<span>,</span> a3<span>,</span> 
    b1<span>,</span> b2<span>,</span> 
    left_corner<span>,</span> right_corner
  <span>]</span><span>)</span><span>;</span>
<span>}</span>
</code></pre> <pre><code><span>function</span> <span>gaussCoef</span><span>(</span>sigma<span>:</span> f32<span>)</span><span>:</span> Float32Array <span>{</span>
  <span>if</span> <span>(</span>sigma <span>&lt;</span> <span>0.5</span><span>)</span> 
    sigma <span>=</span> <span>0.5</span><span>;</span>

  <span>let</span> a<span>:</span> f32 <span>=</span> Mathf<span>.</span><span>exp</span><span>(</span><span>0.726</span> <span>*</span> <span>0.726</span><span>)</span> <span>/</span> sigma<span>;</span>
  

  <span>const</span> r <span>=</span> <span>new</span> <span>Float32Array</span><span>(</span><span>8</span><span>)</span><span>;</span>
  <span>const</span> v <span>=</span> <span>[</span>
    a0<span>,</span> a1<span>,</span> a2<span>,</span> a3<span>,</span> 
    b1<span>,</span> b2<span>,</span> 
    left_corner<span>,</span> right_corner
  <span>]</span><span>;</span>
  <span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> v<span>.</span>length<span>;</span> i<span>++</span><span>)</span> <span>{</span>
    r<span>[</span>i<span>]</span> <span>=</span> v<span>[</span>i<span>]</span><span>;</span>
  <span>}</span>
  <span>return</span> r<span>;</span>
<span>}</span>
</code></pre> <p>The explicit loop at the end to populate the array is there because of a current short-coming in AssemblyScript: Function overloading isn’t supported yet. There is only <em>exactly</em> one constructor for <code>Float32Array</code> in ASC, which takes an <code>i32</code> parameter for the length of the <code>TypedArray</code>. Callbacks are supported in ASC, but closures also are not, so I can’t use <code>.forEach()</code> to fill in the values. This is certainly <em>inconvenient</em>, but not prohibitively so.</p> <blockquote> <p><strong>Mathf</strong>: You might have noticed <code>Mathf</code> instead of <code>Math</code>. <code>Mathf</code> is specifically for 32-bit floats, while <code>Math</code> is for 64-bit floats. I could have used <code>Math</code> and done a cast, but they are ever-so-slightly slower due to the increased precision required. Either way, the <code>gaussCoef</code> function is not part of the hot path, so it really doesn’t make a difference.</p> </blockquote> <h3>Side note: Mind the signs</h3> <p>Something that took me an embarrassingly long time to figure out is that, uh, types matter. Blurring an image involves convolution, and that means a whole bunch of for-loops iterating over all the pixels. Naïvely I thought that because all pixel indices are positive, the loop counters would be as well and decided to choose <code>u32</code> for those loop variables. That’ll bite you with a <em>lovely</em> infinite loop if any of those loops happen to iterate backwards, like the following one:</p> <pre><code><span>let</span> j<span>:</span> u32<span>;</span> 

<span>for</span> <span>(</span>j <span>=</span> width <span>-</span> <span>1</span><span>;</span> j <span>&gt;=</span> <span>0</span><span>;</span> j<span>--</span><span>)</span> <span>{</span>
  
<span>}</span>
</code></pre> <p>Apart from that, the act of porting JS to ASC was a pretty mechanical task.</p> <h3>Benchmarking using d8</h3> <p>Now that we have a JS file and an ASC file, we can compile the ASC to WebAssembly and run a little benchmark to compare the runtime performance.</p> <blockquote> <p><strong>d-What?</strong>: <code>d8</code> is a minimal CLI wrapper around V8, exposing fine-grained control over all kinds of engine features for both Wasm and JS. You can think of it like Node, but with no standard library whatsoever. Just vanilla ECMAScript. Unless you have compiled V8 locally (which you <em>can</em> do by following <a href="https://v8.dev/docs/build">the guide on v8.dev</a>), you probably won’t have <code>d8</code> available. <a href="https://github.com/GoogleChromeLabs/jsvu">jsvu</a> is a tool that can install pre-compiled binaries for many JavaScript engines, including V8.</p> </blockquote> <p>However, since this section has the word “Benchmarking” in the title, I think it’s important to put a disclaimer here: The numbers I am listing here are specific to the code that <em>I</em> wrote in a language <em>I</em> chose, ran on <em>my</em> machine (a 2020 M1 MacBook Air) using a benchmark script that <em>I</em> made. The results are coarse indicators <em>at best</em> and it would be ill-advised to derive quantitative conclusions about the general performance of AssemblyScript, WebAssembly or JavaScript from this.</p> <p>Some might be wondering why I’m using <code>d8</code> instead of running this in the browser or even Node. Both Node and the browser have,... other stuff that may or may not screw with the results. <code>d8</code> is the most sterile environment I can get and as a cherry on top it allows me to control the tier-up behavior. I can limit execution to use Ignition, Sparkplug or Liftoff only, ensuring that performance characteristics don’t change in the middle of a benchmark.</p> <h3>Methodology</h3> <p>As described above, it is important to “warm-up” JavaScript when benchmarking, giving V8 a chance to optimize it. If you don’t do that, you may very well end up measuring a mixture of the performance characteristics of interpreted JS and optimized machine code. To that end, I’m running the blur program 5 times before I start measuring, then I do 50 timed runs and ignore the 5 fastest and slowest runs to remove potential outliers. Here’s what I got:</p> <div id="ad133e969ce92de2db4301636017f1"> <table> <thead> <tr> <th> Language </th><th> Engine </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="Iklnbml0aW9uIg=="> Ignition </td><td data-value="NDQzNC4xNQ=="> 4434.15ms </td><td data-value="NDIuNDkzMDUyMjI4MDc4NTg="> 42.5x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IlNwYXJrcGx1ZyI="> Sparkplug </td><td data-value="MjQ3Ni42"> 2476.60ms </td><td data-value="MjMuNzMzNTg4ODgzNTY0OTI3"> 23.7x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IkxpZnRvZmYi"> Liftoff </td><td data-value="MzgwLjUyNQ=="> 380.52ms </td><td data-value="My42NDY2MjE5NDUzNzYxMzg="> 3.6x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="Mjk4LjEyNQ=="> 298.13ms </td><td data-value="Mi44NTY5NzE3Mjk3NTU2MzAz"> 2.9x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="MTA0LjM1"> 104.35ms </td><td data-value="MQ=="> 1.0x </td> </tr> </tbody> </table> </div>  <p>On the one hand, I was happy to see that Liftoff’s output was faster than what Ignition or Sparkplug could squeeze out of JavaScript. At the same time, it didn’t sit well with me that the optimized WebAssembly module takes about 3 times as long as JavaScript.</p> <p>To be fair, this is a David vs Goliath scenario: V8 is a long-standing JavaScript engine with a huge team of engineers implementing optimizations and other clever stuff, while AssemblyScript is a relatively young project with a small team around it. ASC’s compiler is single-pass and defers all optimization efforts to <a href="https://github.com/WebAssembly/binaryen">Binaryen</a> (see also: <code>wasm-opt</code>). This means that optimization is done at the Wasm VM bytecode level, after most of the high-level semantics have been compiled away. V8 has a clear edge here. However, the blur code is so simple — just doing arithmetic with values from memory — that I was really expecting it to be closer. What’s going on here?</p> <h3>Digging in</h3> <p>After quickly consulting with some folks from the V8 team and some folks from the AssemblyScript team (thanks <a href="https://twitter.com/dcodeio">Daniel</a> and <a href="https://twitter.com/maxgraey">Max</a>!), it turns out that one big difference here are “bounds checks” — or the lack thereof.</p> <p>V8 has the luxury of having access to your original JavaScript code and knowledge about the semantics of the language. It can use that information to apply additional optimizations. For example: It can tell you are not just randomly reading values from memory, but you are iterating over an <code>ArrayBuffer</code> using a <code>for ... of</code> loop. What’s the difference? Well with a <code>for ... of</code> loop, the language semantics guarantee that you will never try to read values outside of the <code>ArrayBuffer</code>. You will never end up accidentally reading byte 11 when the buffer is only 10 bytes long, or: You never go <em>out of bounds</em>. This means TurboFan does not need to emit bounds checks, which you can think of as <code>if</code> statements making sure you are not accessing memory you are not supposed to. This kind of information is lost once compiled to WebAssembly, and since ASC’s optimization only happens at WebAssembly VM level, it can’t necessarily apply the same optimization.</p> <p>Luckily, AssemblyScript provides a magic <code>unchecked()</code> annotation to indicate that we are taking responsibility for staying in-bounds.</p> <pre><code><span><span>-</span><span> prev_prev_out_r = prev_src_r * coeff[6];
</span><span>-</span><span> line[line_index] = prev_out_r;
</span></span><span><span>+</span><span> prev_prev_out_r = prev_src_r * unchecked(coeff[6]);
</span><span>+</span><span> unchecked(line[line_index] = prev_out_r);
</span></span></code></pre> <p>But there’s more: The Typed Arrays in AssemblyScript (<code>Uint8Array</code>, <code>Float32Array</code>, ...) offer the same API as they do on the platform, meaning they are merely a view onto an underlying <code>ArrayBuffer</code>. This is good in that the API design is familiar and battle-tested, but due to the lack of high-level optimizations, this means that every access to a field on the Typed Array (like <code>myFloatArray[23]</code>) needs to access memory twice: Once to load the pointer to the underlying <code>ArrayBuffer</code> of this specific array, and another to load the value at the right offset. V8, as it can tell that you are accessing the Typed Array but never the underlying buffer, is most likely able to optimize the entire data structure so that you can read values with a single memory access.</p> <p>For that reason, AssemblyScript provides <code>StaticArray&lt;T&gt;</code>, which is mostly equivalent to an <code>Array&lt;T&gt;</code> except that it can’t grow. With a fixed length, there is no need to keep the Array entity separate from the memory the values are stored in, removing that indirection.</p> <p>I applied both these optimizations to my “naïve port” and measured again:</p> <div id="c84df0edd45737eeea6334c8fb5e5e8"> <table> <thead> <tr> <th> Language </th><th> Variant </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IiI="> </td><td data-value="MTA0LjM1"> 104.35ms </td><td data-value="MQ=="> 1.0x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="MTYyLjYyNQ=="> 162.63ms </td><td data-value="MS41NTg0NTcxMTU0NzY3NjE="> 1.6x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im5haXZlIg=="> naive </td><td data-value="Mjk4LjEyNQ=="> 298.13ms </td><td data-value="Mi44NTY5NzE3Mjk3NTU2MzAz"> 2.9x </td> </tr> </tbody> </table> </div>  <p>A lot better! While the AssemblyScript is still slower than the JavaScript, we got significantly closer. Is this the best we can do?</p> <h3>Sneaky defaults</h3> <p>Another thing that the AssemblyScript folks pointed out to me is that the <code>--optimize</code> flag is equivalent to <code>-O3s</code> which aggressively optimizes for speed, but makes tradeoffs to reduce binary size. <code>-O3</code> optimizes for speed and speed only. Having <code>-O3s</code> as a default is good in spirit — binary size matters on the web — but is it worth it? At least in this specific example the answer is no: <code>-O3s</code> ends up trading the laughable amount of ~30 bytes for a huge performance penalty:</p> <div id="25f647b6eb2acf674b33a6c7e99022db"> <table> <thead> <tr> <th> Language </th><th> Variant </th><th> Optimizer </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="Ik8zIg=="> O3 </td><td data-value="ODkuNg=="> 89.60ms </td><td data-value="MC44NTg2NDg3NzgxNTA0NTUx"> 0.9x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IiI="> </td><td data-value="IiI="> </td><td data-value="MTA0LjM1"> 104.35ms </td><td data-value="MQ=="> 1.0x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="Ik8zcyI="> O3s </td><td data-value="MTYyLjYyNQ=="> 162.63ms </td><td data-value="MS41NTg0NTcxMTU0NzY3NjE="> 1.6x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im5haXZlIg=="> naive </td><td data-value="Ik8zcyI="> O3s </td><td data-value="Mjk4LjEyNQ=="> 298.13ms </td><td data-value="Mi44NTY5NzE3Mjk3NTU2MzAz"> 2.9x </td> </tr> </tbody> </table> </div>  <p>One single optimizer flag makes a night-and-day difference, letting AssemblyScript overtake JavaScript (on this specific test case!). We made AssemblyScript faster than JavaScript!</p> <blockquote> <p><strong>O3</strong>: From here on forward, I will only be using <code>-O3</code> in this article.</p> </blockquote> <h3>Bubblesort</h3> <p>To gain some confidence that the image blur example is not just a fluke, I thought I should try this again with a second program. Rather uncreatively, I took a bubblesort implementation off of StackOverflow and ran through the same process: Add types. Run benchmark. Optimize. Run benchmark. The creation and population of the array that’s to be bubble-sorted is <em>not</em> part of the benchmarked code path.</p> <div id="5212c83d9699d1361abcebc40aea3a2"> <table> <thead> <tr> <th> Language </th><th> Engine </th><th> Variant </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="NjUuNDI1"> 65.42ms </td><td data-value="MC42Mjk5OTUxODUzNjM1MDUx"> 0.6x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="IiI="> </td><td data-value="MTAzLjg1"> 103.85ms </td><td data-value="MQ=="> 1.0x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IkxpZnRvZmYi"> Liftoff </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="MjU2Ljk3NQ=="> 256.98ms </td><td data-value="Mi40NzQ0ODI0MjY1NzY3OTQ="> 2.5x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="Im5haXZlIg=="> naive </td><td data-value="NDM0LjA1"> 434.05ms </td><td data-value="NC4xNzk1ODU5NDEyNjE0MzU="> 4.2x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IkxpZnRvZmYi"> Liftoff </td><td data-value="Im5haXZlIg=="> naive </td><td data-value="NzE0Ljc="> 714.70ms </td><td data-value="Ni44ODIwNDE0MDU4NzM4NTc="> 6.9x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IlNwYXJrcGx1ZyI="> Sparkplug </td><td data-value="IiI="> </td><td data-value="MTYxNg=="> 1616.00ms </td><td data-value="MTUuNTYwOTA1MTUxNjYxMDU="> 15.6x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="Iklnbml0aW9uIg=="> Ignition </td><td data-value="IiI="> </td><td data-value="MjM0MC4wNzU="> 2340.07ms </td><td data-value="MjIuNTMzMjIwOTkxODE1MTE3"> 22.5x </td> </tr> </tbody> </table> </div>  <p>We did it again! This time with an even bigger discrepancy: The optimized AssemblyScript is almost twice as fast as JavaScript. But do me a favor: Don’t stop reading now.</p> <h2>Allocations</h2> <p>Some of you may have noticed that both these examples have very few or no allocations. V8 takes care of all memory management (and garbage collection) in JavaScript for you and I won’t pretend that I know much about it. In WebAssembly, on the other hand, you get a chunk of linear memory and you have to decide how to use it (or rather: the language does). How much do these rankings change if we make <em>heavy</em> use of dynamic memory?</p> <p>To measure this, I chose to benchmark an implementation of a <a href="https://en.wikipedia.org/wiki/Binary_heap">binary heap</a>. The benchmark fills the binary heap with 1 million random numbers (courtesy of <code>Math.random()</code>) and <code>pop()</code>s them all back out, checking that the numbers are in increasing order. The process remained the same as above: Make a naïve port of the JS code to ASC, run benchmark, optimize, benchmark again:</p> <div id="72f36bd88ee2f1f663744a7a4953736"> <table> <thead> <tr> <th> Language </th><th> Engine </th><th> Variant </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="IiI="> </td><td data-value="MjMzLjY3NQ=="> 233.68ms </td><td data-value="MQ=="> 1.0x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IlNwYXJrcGx1ZyI="> Sparkplug </td><td data-value="IiI="> </td><td data-value="MTcxMi41NzU="> 1712.58ms </td><td data-value="Ny4zMjg4NzU1NzUwNTA4MTg1"> 7.3x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="Iklnbml0aW9uIg=="> Ignition </td><td data-value="IiI="> </td><td data-value="MzE1Ny45NzU="> 3157.97ms </td><td data-value="MTMuNTE0Mzg5NjQzNzM1OTU3"> 13.5x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="MTg3NTguNQ=="> 18758.50ms </td><td data-value="ODAuMjc2MDI0MzkyODUzMzI="> 80.3x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IkxpZnRvZmYi"> Liftoff </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="MTg4NjcuMDc1"> 18867.08ms </td><td data-value="ODAuNzQwNjY1NDU0MTU2NDE="> 80.7x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IlR1cmJvRmFuIg=="> TurboFan </td><td data-value="Im5haXZlIg=="> naive </td><td data-value="MTkwMzEuNg=="> 19031.60ms </td><td data-value="ODEuNDQ0NzQxNjI4MzI5OTQ="> 81.4x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="IkxpZnRvZmYi"> Liftoff </td><td data-value="Im5haXZlIg=="> naive </td><td data-value="MTk0MDkuMQ=="> 19409.10ms </td><td data-value="ODMuMDYwMjMzMjI5OTEzMzQ="> 83.1x </td> </tr> </tbody> </table> </div>  <p>80x slower than JavaScript?! Even slower than Ignition? Surely, there is something else going wrong here.</p> <h3>Runtimes</h3> <p>All data that we create in AssemblyScript needs to be stored in memory. To make sure we don’t overwrite anything else that is already in memory, there is memory management. As AssemblyScript aims to provide a familiar environment, mirroring the behavior of JavaScript, it adds a fully managed garbage collector to your WebAssembly module so that you don’t have to worry about when to allocate and when to free up memory.</p> <p>By default, AssemblyScript ships with a <a href="http://www.gii.upv.es/tlsf/">Two-Level Segregated Fit memory allocator</a> and an <a href="https://en.wikipedia.org/wiki/Tracing_garbage_collection#Tri-color_marking">Incremental Tri-Color Mark &amp; Sweep (ITCMS)</a> garbage collector. It’s not actually relevant for this article what kind of allocator and garbage collector they use, I just found it interesting that you can go <a href="https://github.com/AssemblyScript/assemblyscript/tree/master/std/assembly/rt">look at them</a>.</p> <p>This default runtime, called <code>incremental</code>, is surprisingly small, adding only about 2KB of gzip’d WebAssembly to your module. AssemblyScript also offers alternative runtimes, namely <code>minimal</code> and <code>stub</code> that can be chosen using the <code>--runtime</code> flag. <code>minimal</code> uses the same allocator, but a more lightweight GC that does <em>not</em> run automatically but must be manually invoked. This could be interesting for high-performance use-cases like games where you want to be in control when the GC will pause your program. <code>stub</code> is <em>extremely</em> small (~400B gzip’d) and fast, as it’s just a <a href="https://os.phil-opp.com/allocator-designs/#bump-allocator">bump allocator</a>.</p> <blockquote> <p><strong>My lovely memory bump</strong>: Bump allocators are <em>extremely</em> fast, but lack the ability to free up memory. While that sounds stupid, it can be extremely useful for single-purpose modules, where instead of freeing memory, you delete the entire WebAssembly instance and rather create a new one. If you are curious, I actually wrote a bump allocator in my article <a href="https://surma.dev/things/c-to-webassembly/index.html">Compiling C to WebAssembly without Emscripten</a>.</p> </blockquote> <p>How much faster does that make our binary heap experiment? Quite significantly!</p> <div id="3dfd64af25fd871c20e93e6f7a0bff7"> <table> <thead> <tr> <th> Language </th><th> Variant </th><th> Runtime </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IiI="> </td><td data-value="IiI="> </td><td data-value="MjMzLjY3NQ=="> 233.68ms </td><td data-value="MQ=="> 1.0x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="InN0dWIi"> stub </td><td data-value="MzQ1LjM1"> 345.35ms </td><td data-value="MS40Nzc5MDczNDk5NTE4NTYy"> 1.5x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="Im1pbmltYWwi"> minimal </td><td data-value="MzU0LjY="> 354.60ms </td><td data-value="MS41MTc0OTIyNDM1MDA1ODg0"> 1.5x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="ImluY3JlbWVudGFsIg=="> incremental </td><td data-value="MTg3NTguNQ=="> 18758.50ms </td><td data-value="ODAuMjc2MDI0MzkyODUzMzI="> 80.3x </td> </tr> </tbody> </table> </div>  <p>Both <code>minimal</code> and <code>stub</code> get us <em>significantly</em> closer to JavaScripts performance. But <em>why</em> are these two so much faster? As mentioned above, <code>minimal</code> and <code>incremental</code> share the same allocator, so that can’t be it. Both also have a garbage collector, but <code>minimal</code> doesn’t run it unless explicitly invoked (and we ain’t invoking it). That means the differentiating quality is that <code>incremental</code> <em>runs</em> garbage collection, while <code>minimal</code> and <code>stub</code> do not. I don’t see why the garbage collector should make this big of a difference, considering it has to keep track of <em>one</em> array.</p> <h3>Growth</h3> <p>After doing some <a href="https://v8.dev/docs/profile">profiling with d8</a> on a build with debugging symbols (<code>--debug</code>), it turns out that a lot of time is spent in a system library <code>libsystem_platform.dylib</code>, which contains OS-level primitives for threading and memory management. Calls into this library are made from <code>__new</code> and <code>__renew</code>, which in turn are called from <code>Array&lt;f32&gt;#push</code>:</p> <pre><code>[Bottom up (heavy) profile]:
  ticks parent  name
  18670   96.1%  /usr/lib/system/libsystem_platform.dylib
  13530   72.5%    Function: *~lib/rt/itcms/__renew
  13530  100.0%      Function: *~lib/array/ensureSize
  13530  100.0%        Function: *~lib/array/Array<f32>#push
  13530  100.0%          Function: *binaryheap_optimized/BinaryHeap<f32>#push
  13530  100.0%            Function: *binaryheap_optimized/push
   5119   27.4%    Function: *~lib/rt/itcms/__new
   5119  100.0%      Function: *~lib/rt/itcms/__renew
   5119  100.0%        Function: *~lib/array/ensureSize
   5119  100.0%          Function: *~lib/array/Array<f32>#push
   5119  100.0%            Function: *binaryheap_optimized/BinaryHeap<f32>#push
</f32></f32></f32></f32></code></pre> <p>Clearly, we have a problem with allocations here. But JavaScript somehow manages to make an ever-growing array fast, so why can’t AssemblyScript? Luckily, the standard library of AssemblyScript is rather small and approachable, so let’s go and <a href="https://github.com/AssemblyScript/assemblyscript/blob/42c2dbc987c2e9f4096a226b62bbf0e72b4a0e51/std/assembly/array.ts#L204-L216">take a look</a> at this ominous <code>push()</code> function of the <code>Array&lt;T&gt;</code> class:</p> <pre><code><span>export</span> <span>class</span> <span><span>Array</span><span>&lt;</span><span>T</span><span>&gt;</span></span> <span>{</span>
  
  <span>push</span><span>(</span>value<span>:</span> <span>T</span><span>)</span><span>:</span> i32 <span>{</span>
    <span>var</span> length <span>=</span> <span>this</span><span>.</span>length_<span>;</span>
    <span>var</span> newLength <span>=</span> length <span>+</span> <span>1</span><span>;</span>
    <span>ensureSize</span><span>(</span><span><span>changetype</span><span><span>&lt;</span>usize<span>&gt;</span></span></span><span>(</span><span>this</span><span>)</span><span>,</span> newLength<span>,</span> <span><span>alignof</span><span><span>&lt;</span><span>T</span><span>&gt;</span></span></span><span>(</span><span>)</span><span>)</span><span>;</span>
    
    <span>return</span> newLength<span>;</span>
  <span>}</span>
  
<span>}</span>
</code></pre> <p>The <code>push()</code> function correctly determines that the new length of the array is the current length plus 1 and then calls <code>ensureSize()</code>, to make sure that the underlying buffer has enough room (“capacity”) to grow to this length.</p> <pre><code><span>function</span> <span>ensureSize</span><span>(</span>array<span>:</span> usize<span>,</span> minSize<span>:</span> usize<span>,</span> alignLog2<span>:</span> u32<span>)</span><span>:</span> <span>void</span> <span>{</span>
  
  <span>if</span> <span>(</span>minSize <span>&gt;</span> <span>&lt;</span>usize<span>&gt;</span>oldCapacity <span>&gt;&gt;&gt;</span> alignLog2<span>)</span> <span>{</span>
    
    <span>let</span> newCapacity <span>=</span> minSize <span>&lt;&lt;</span> alignLog2<span>;</span>
    <span>let</span> newData <span>=</span> <span>__renew</span><span>(</span>oldData<span>,</span> newCapacity<span>)</span><span>;</span>
    
  <span>}</span>
<span>}</span>
</code></pre> <p><code>ensureSize()</code>, in turn, checks if the capacity is smaller than the new <code>minSize</code>, and if so, allocates a new buffer of size <code>minSize</code> using <code>__renew</code>, which entails copying all the data from the old buffer to the new buffer. For that reason our benchmark, where we push <em>one million values</em> one-by-one into the array, ends up causing a <em>lot</em> of allocation work and create a lot of garbage.</p> <p>In other languages, like <a href="https://github.com/rust-lang/rust/blob/58f32da346642ff3f50186f6f4a0de46e61008be/library/alloc/src/raw_vec.rs#L431">Rust’s <code>std::vec</code></a> or <a href="https://github.com/golang/go/blob/3f4977bd5800beca059defb5de4dc64cd758cbb9/src/runtime/slice.go#L144-L163">Go’s slices</a>, the new buffer has <em>double</em> the old buffer’s capacity, which amortizes the allocation work over time. <a href="https://github.com/AssemblyScript/assemblyscript/issues/1798">I am working to fix this in ASC</a>, but in the meantime we can create our own <code>CustomArray&lt;T&gt;</code> that has the desired behavior. Lo and behold, we made things faster!</p> <blockquote> <p><em>Fixed</em>: The ASC team has since fixed this regression v0.18.31</p> </blockquote> <div id="3268f4eab863af26da396966818b3c6"> <table> <thead> <tr> <th> Language </th><th> Variant </th><th> Runtime </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IiI="> </td><td data-value="IiI="> </td><td data-value="MjMzLjY3NQ=="> 233.68ms </td><td data-value="MQ=="> 1.0x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="ImN1c3RvbWFycmF5Ig=="> customarray </td><td data-value="Im1pbmltYWwi"> minimal </td><td data-value="MzI5LjIyNQ=="> 329.23ms </td><td data-value="MS40MDg5MDEyNTE3Mzg1MjU4"> 1.4x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="ImN1c3RvbWFycmF5Ig=="> customarray </td><td data-value="InN0dWIi"> stub </td><td data-value="MzI5LjQyNQ=="> 329.43ms </td><td data-value="MS40MDk3NTcxNDEzMjg3Njg2"> 1.4x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="ImN1c3RvbWFycmF5Ig=="> customarray </td><td data-value="ImluY3JlbWVudGFsIg=="> incremental </td><td data-value="MzM1LjM1"> 335.35ms </td><td data-value="MS40MzUxMTI4NzA0Mzk3MTM0"> 1.4x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="InN0dWIi"> stub </td><td data-value="MzQ1LjM1"> 345.35ms </td><td data-value="MS40Nzc5MDczNDk5NTE4NTYy"> 1.5x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="Im1pbmltYWwi"> minimal </td><td data-value="MzU0LjY="> 354.60ms </td><td data-value="MS41MTc0OTIyNDM1MDA1ODg0"> 1.5x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="ImluY3JlbWVudGFsIg=="> incremental </td><td data-value="MTg3NTguNQ=="> 18758.50ms </td><td data-value="ODAuMjc2MDI0MzkyODUzMzI="> 80.3x </td> </tr> </tbody> </table> </div>  <p>With this change <code>incremental</code> is as fast as <code>stub</code> and <code>minimal</code>, but none of them are as fast as JavaScript in this test case. There are probably more optimizations I could do, but we are already pretty deep in the weeds here, and this is not supposed to be an article about how to optimize AssemblyScript.</p> <p>There are also a lot of simple optimizations I wish AssemblyScript’s <em>compiler</em> would do for me. To that end, they are working on an <a href="https://en.wikipedia.org/wiki/Intermediate_representation">IR</a> called “AIR”. Will that make things faster out-of-the-box without having to hand-optimize every array access? Very likely. Will it be faster than JavaScript? Hard to say. But I <em>did</em> wonder what the more “mature” languages with “very smart” compiler toolchains can achieve.</p> <h3>Rust &amp; C++</h3> <p>I re-rewrote the code in Rust, being as idiomatic as possible and compiled it to WebAssembly. While it was faster than a naive port to AssemblyScript, it was slower than our optimized AssemblyScript with <code>CustomArray&lt;T&gt;</code>. So I had to do the same as I did in AssemblyScript: Avoid bound checks by sprinkling some <code>unsafe</code> here and there. With that optimization in place, Rust’s WebAssembly module is faster than our optimized AssemblyScript, but still not faster than JavaScript.</p> <p>I took the same approach with C++, using <a href="https://emscripten.org/">Emscripten</a> to compile it to WebAssembly. To my surprise, my first attempt came out performing just as well as JavaScript.</p> <div id="5a1ba434b5b3bbd94da8fdcf1eeb7e7"> <table> <thead> <tr> <th> Language </th><th> Variant </th><th> Average </th><th> vs JS </th> </tr> </thead> <tbody> <tr> <td data-value="IkMrKyI="> C++ </td><td data-value="ImlkaW9tYXRpYyI="> idiomatic </td><td data-value="MjI2LjE3NQ=="> 226.18ms </td><td data-value="MC45Njc5MDQxNDAzNjU4OTI4"> 1.0x </td> </tr> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IiI="> </td><td data-value="MjMzLjY3NQ=="> 233.68ms </td><td data-value="MQ=="> 1.0x </td> </tr> <tr> <td data-value="IlJ1c3Qi"> Rust </td><td data-value="Im9wdGltaXplZCI="> optimized </td><td data-value="Mjg0LjY="> 284.60ms </td><td data-value="MS4yMTc5MzA4ODY5MTU1ODc4"> 1.2x </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="ImN1c3RvbWFycmF5Ig=="> customarray </td><td data-value="MzM1LjM1"> 335.35ms </td><td data-value="MS40MzUxMTI4NzA0Mzk3MTM0"> 1.4x </td> </tr> <tr> <td data-value="IlJ1c3Qi"> Rust </td><td data-value="ImlkaW9tYXRpYyI="> idiomatic </td><td data-value="NDQyLjg3NQ=="> 442.88ms </td><td data-value="MS44OTUyNjA1MTEzOTQwMzAy"> 1.9x </td> </tr> </tbody> </table> </div>  <blockquote> <p><strong>Lrn2code</strong>: The versions labelled “idiomatic” are still very closely inspired by the original JS code. I tried to make use of my knowledge of the idioms of the target language, but in the end it’s still a port. I am sure an implementation from scratch by someone with more experience in those languages would look different.</p> </blockquote> <p>I’m fairly certain that Rust and C++ could be made even faster, but I don’t have sufficiently deep knowledge of either language to squeeze out those last couple optimizations.</p> <h3>Gzip’d file sizes</h3> <p>It is worth noting that file size is a <em>strength</em> of AssemblyScript. Comparing the gzip’d file sizes, we get:</p> <div id="6521a9b958cde08895f7b5c5e3dff8a"> <table> <thead> <tr> <th> Language </th><th> Runtime </th><th> .wasm </th><th> .js </th><th> Total </th> </tr> </thead> <tbody> <tr> <td data-value="IkphdmFTY3JpcHQi"> JavaScript </td><td data-value="IiI="> </td><td data-value="IjAi"> 0 B </td><td data-value="IjQ0OSI="> 449 B </td><td data-value="NDQ5"> 449 B </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="InN0dWIi"> stub </td><td data-value="Ijk1OSI="> 959 B </td><td data-value="IjAi"> 0 B </td><td data-value="OTU5"> 959 B </td> </tr> <tr> <td data-value="IkFzc2VtYmx5U2NyaXB0Ig=="> AssemblyScript </td><td data-value="ImluY3JlbWVudGFsIg=="> incremental </td><td data-value="IjI2NDIi"> 2.6KB </td><td data-value="IjAi"> 0 B </td><td data-value="MjY0Mg=="> 2.6KB </td> </tr> <tr> <td data-value="IlJ1c3Qi"> Rust </td><td data-value="IiI="> </td><td data-value="IjczNDci"> 7.2KB </td><td data-value="IjAi"> 0 B </td><td data-value="NzM0Nw=="> 7.2KB </td> </tr> <tr> <td data-value="IkMrKyI="> C++ </td><td data-value="IiI="> </td><td data-value="IjY3MzQi"> 6.6KB </td><td data-value="IjYzODYi"> 6.2KB </td><td data-value="MTMxMjA="> 12.8KB </td> </tr> </tbody> </table> </div> <h2>Conclusion</h2> <p>I want to be very clear: Any generalized, quantitative take-away from this article would be ill-advised. For example, Rust is <em>not</em> 1.2x slower than JavaScript. These numbers are very much specific to the code that <em>I</em> wrote, the optimizations that <em>I</em> applied and the machine <em>I</em> used. However, I think there are some general guidelines we can extract to help you make more informed decisions in the future:</p> <ul> <li>V8’s Liftoff compiler will generate code from WebAssembly that runs significantly faster than what Ignition or SparkPlug can deliver for JavaScript. If you need performance without <em>any</em> warmup time, WebAssembly is your tool of choice.</li> <li>V8 is <em>really</em> good at executing JavaScript. While WebAssembly can run faster than JavaScript, it is likely that you will have to hand-optimize your code to achieve that. I could see this balance shift once there is wide-spread support for SIMD and Threads and a good developer experience around utilizing them.</li> <li>Compilers can do a lot of work for you, more mature compilers are likely to be better at optimizing your code.</li> <li>AssemblyScript modules tend to be a lot smaller than the other WebAssembly modules produced by other languages. In this exploration, AssemblyScript was <em>not</em> smaller than the original JavaScript, but this could very well be different for bigger modules.</li> </ul> <p>If you don’t trust me (and you shouldn’t!) and want to dig into the benchmark code yourself, take a look at the <a href="https://gist.github.com/surma/40e632f57a1aec4439be6fa7db95bc88">gist</a>.</p> </div></div>
  </body>
</html>
