<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://richg42.blogspot.com/2023/04/faster-lz-is-not-answer-to-150-250-gb.html">Original</a>
    <h1>Faster LZ is not the answer to 150-250 GB video game downloads</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-9130824828824641070" itemprop="description articleBody">
<p><span>When the JPEG folks were working on image compression, they didn&#39;t create a better or faster LZ. Instead they developed new approaches. I see <a href="https://www.pcgamer.com/biggest-game-install-sizes/">games growing &gt;150GB</a> and then graphs like this, and it&#39;s obvious the game devs are going in the wrong direction:</span></p><div><div><p><span><b>Separating out the texture encoding stage from the lossless stage is a compromise.</b> I first did this in my &#34;crunch&#34; library around 15 years ago. It was called &#34;RDO mode&#34;. You can swizzle the ASTC/BC1-7 bits before LZ, and precondition them, and that&#39;ll help, but the two steps are still disconnected. Instead combine the texture and compression steps (like crunch&#39;s .CRN mode - shipped by Unity for BC1-5.) </span></p><p><span>Alternatively: defer computing the GPU texture data until right before it&#39;s actually needed and cache it. Ship the texture signal data using existing image compression technology, which at this point is quite advanced. For normal maps, customize or tune existing tech to handle them without introducing excessive angular distortion. I think both ideas are workable.</span></p></div></div>

</div></div>
  </body>
</html>
