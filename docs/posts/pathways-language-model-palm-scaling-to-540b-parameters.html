<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">Original</a>
    <h1>Pathways Language Model (PaLM): Scaling to 540B parameters</h1>
    
    <div id="readability-page-1" class="page"><div id="content">

<p><time>April 4, 2022</time></p><div>
<p>In general, we’re least aware of what our minds do best. It’s mainly when systems start to fail that we engage the special agencies involved with what we call “consciousness.”</p>
<p>Accordingly, we’re more aware of simple processes that don’t work well than of complex ones that work flawlessly.</p>
<p>This phenomenon helps to explain the poor performance of many so-called <em>expert systems</em> in the 1980s. There were attempts to fully rationalize human expertise as calculative rules. The effect was often to regress an expert’s <em>knowing how</em> to a novice practitioner’s <em>knowing that</em>.</p>
<p>Skill acquisition in unstructured domains moves not towards abstract rules, but rather from abstract rules to particular cases. And “the distinction between education, a process aimed at drawing out the abilities of the student, and training, in which the student is learning to negotiate a structured domain, is crucial.”</p>
<p>This may help shed light on much of the recent mixed success of “unexplainable” neural-network-based decision systems.</p>
<p><small>
This post was adapted from a note sent to my email list on Machine-Centric Science.
</small>
</p>

</div>




</div></div>
  </body>
</html>
