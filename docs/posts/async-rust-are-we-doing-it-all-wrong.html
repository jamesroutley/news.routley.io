<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://maciej.codes/2022-06-09-local-async.html">Original</a>
    <h1>Async rust â€“ are we doing it all wrong?</h1>
    
    <div id="readability-page-1" class="page"><article>
	<header>
		
		<time>Posted on June 9, 2022</time>
	</header>

	<p>Imagine you are new to Rust and you want to learn some async programming. Writing a chat server is a common way to start, scope is small enough to contain in a blog post, yet it is complex enough to learn most of what you&#39;ll need to build async servers in Rust. One DuckDuckGo query and we are off to the races: <a href="https://outcrawl.com/rust-react-realtime-chat">Building a Real-time Chat App in Rust and React</a> by <a href="https://outcrawl.com/authors/tin-rabzelj/">Tin Rabzejl</a>. First line reads:</p>
<blockquote>
<p>This article covers building a chat app in Rust using <a href="https://rust-lang.github.io/async-book/">asynchronous code</a>.</p>
</blockquote>
<p>Asynchronous code, that&#39;s what we want, great! This is a very well written article, and being from 2020 it&#39;s still very much in vogue. There are some low-hanging performance tweaks I might do, but as example code to cover the basics I think this works really well. We have Tokio for runtime and <a href="https://crates.io/crates/warp">warp</a> for WebSockets, pretty standard affair.</p>
<p>So let&#39;s look through this article and see what do we need to learn to write asynchronous Rust:</p>
<ol>
<li><code>async</code> functions and <code>.await</code> syntax.</li>
<li>Futures and <code>tokio::select!</code>.</li>
<li><code>RwLock</code> and why it&#39;s better than <code>Mutex</code> for this case.</li>
<li><code>sync::mpsc</code> channels.</li>
<li>Our main shared-state thing lives inside an <code>Arc</code> and we clone that a bunch.</li>
</ol>
<p>Notice something about this list? Except for the first two, nothing here has anything to do with <em>asynchronous programming</em>. Yes the <code>RwLock</code> and <code>mpsc</code> comes from Tokio and lets you <code>.await</code> instead of blocking a thread, but these are not async primitives, <em>these are multi-threading synchronization primitives</em>.</p>
<p>Let&#39;s step back a little and actually think about the problem we are trying to solve. This is a simple chat server, how likely is it that it&#39;s ever going to be more CPU bound than I/O bound? I&#39;d say that probability is pretty low. If you know anything about asynchronous sockets it should be that <strong>multi-threading a socket doesn&#39;t actually yield you more requests / second, and <a href="https://github.com/fundon/smol-tokio-hyper-benchmarks">it can actually lower it</a></strong> (slightly, but still).</p>
<p>This is why <a href="https://nodejs.org/">Node.js</a> and <a href="https://deno.land/">Deno</a> are so good: it defaults to a single-threaded JavaScript event loop handling huge amounts of requests that is easy to write, and when you need to utilize multiple CPU cores, you just spawn multiple processes that listen on the same socket. This is a much better way of structuring servers, except for all that JavaScript, of course.</p>
<p>The Original Sin of Rust async programming is making it multi-threaded by default. If premature optimization is the root of all evil, this is the mother of all premature optimizations, and it curses all your code with the unholy <code>Send + &#39;static</code>, or worse yet <code>Send + Sync + &#39;static</code>, which just <em>kills all the joy of actually writing Rust</em>.</p>
<h2>Actually Pleasant Async Rust</h2>
<p>So <a href="https://twitter.com/FiresOfEschaton">I&#39;m writing a multiplayer game</a> in Rust, and the server part of it is really quite similar to what a chat server would do: there is an instance of a <em><code>Battle</code></em> which is a chat-room analog, a message comes in from one of the clients, it gets processed, and then the outcome gets broadcast to all client in that instance, just like a chat message in a room would be. And here is the piece of code that handles that currently built on top of <a href="https://crates.io/crates/async-io"><code>async-io</code></a> and <a href="https://crates.io/crates/async-executor"><code>async-executor</code></a>:</p>
<pre><code><em>// This is spawned as a task in a `LocalExecutor`. `BattleManager` outlives
// the executor, so I can just send it in by reference ðŸ¤¯</em>
<b>pub async fn</b> <u>battle</u>(<i>stream</i><b>:</b> <u>Async</u>&lt;<i>TcpStream</i>&gt;, <i>manager</i><b>: &amp;</b><u>BattleManager</u>) {
    <em>// `!Sync` read and write halves of WebSocket using a modified Soketto</em>
    <b>let</b> <i>server</i> = <i>UnsyncServer</i><b>::</b><u>new</u>(<i>stream</i>);
    <b>let</b> (<b>mut</b> <i>sender</i>, <b>mut</b> <i>receiver</i>) = <i>server</i><b>.</b><u>split</u>();

    <em>// `!Sync` read and write halves of a quasi-ring buffer.</em>
    <b>let</b> (<i>writer</i>, <b>mut</b> <i>reader</i>) = <i>new_shared</i>();

    <em>// We find a battle to put this socket into, and do just that.
    // Each battle instance is wrapped in `Rc&lt;RefCell&lt;_&gt;&gt;`.</em>
    <b>let</b> <i>battle</i> = <i>manager</i><b>.</b><u>matchmake</u>();
    <b>let</b> <i>cid</i> = <i>battle</i><b>.</b><u>borrow_mut</u>()<b>.</b><u>join</u>(<i>writer</i>);

    <em>// Loop handling outgoing messages turned into a simple future</em>
    <b>let</b> <i>outgoing</i> = <b>async move</b> {
        <b>while let</b> <u>Some</u>(<b>mut</b> <i>buf</i>) = <i>reader</i><b>.</b><u>read</u>()<b>.await</b> {
            <b>if let</b> <u>Err</u>(<i>err</i>) = <i>sender</i><b>.</b><u>send</u>(<b>&amp;mut </b><u>buf</u>[<b>..</b>])<b>.await</b> {
                <i>log</i><b>::</b><u>error</u>!(<a>&#34;Connection error: {err:?}&#34;</a>);
                <b>break</b>;
            }
            <em>// `buf` is dropped here, which safely advances read head</em>
        }

        <b>let</b> _ = <i>sender</i><b>.</b><u>close</u>()<b>.await</b>;
    };

    <em>// Loop handling incoming messages turned into a simple future</em>
    <b>let</b> <i>incoming</i> = <b>async move</b> {
        <b>let mut</b> <i>data</i> = <i>Vec</i><b>::</b><u>new</u>();

        <b>loop</b> {
            <i>data</i><b>.</b><u>clear</u>();

            <b>if</b> <i>receiver</i><b>.</b><u>receive_data</u>(<b>&amp;mut </b><u>data</u>)<b>.await.</b><u>is_err</u>() {
                <i>battle</i><b>.</b><u>borrow_mut</u>()<b>.</b><u>leave</u>(<i>cid</i>);
                <b>break</b>;
            }

            <b>let mut</b> <i>battle</i> = <i>battle</i><b>.</b><u>borrow_mut</u>();

            <em>// Process incoming messages</em>
            <b>for</b> <i>client_message</i> <b>in</b> <i>core</i><b>::</b><u>Decoder</u><b>::</b><u>new</u>(<b>&amp;</b><u>data</u>) {
                <i>battle</i><b>.</b><u>handle_message</u>(<i>cid</i>, <i>client_message</i>);
            }

            <em>// Broadcast all outgoing messages buffered for all clients</em>
            <i>battle</i><b>.</b><u>flush</u>();
        }
    };

    <em>// Zip (join) the two futures together so the two loops can run
    // concurrently. Yes sometimes I double-poll one, who cares.</em>
    <i>zip</i>(<i>incoming</i>, <i>outgoing</i>)<b>.await</b>;

    <i>log</i><b>::</b><u>info</u>!(<a>&#34;Connection closed&#34;</a>);
}
</code></pre>
<p>This is pretty easy, and it looks almost like synchronous Rust. Yes I have some <code>RefCell</code>s, but compared to <code>Mutex</code>es or <code>RwLock</code>s borrowing a <code>RefCell</code> is virtually free. And if you can prove to yourself that your stuff is never double-borrowed, you could do some custom wrappers using <code>UnsafeCell</code> to make it literally free.</p>
<p>The guts of my shared buffer look roughly like this:</p>
<pre><code>#[repr(transparent)]
<b>pub struct</b> <u>BufWriter</u>(<i>Rc</i>&lt;<i>Inner</i>&gt;);

#[repr(transparent)]
<b>pub struct</b> <u>BufReader</u>(<i>Rc</i>&lt;<i>Inner</i>&gt;);

<b>struct</b> <u>Inner</u> {
    <i>write_head</i><b>:</b> <u>Cell</u>&lt;<u>u32</u>&gt;,
    <i>read_head</i><b>:</b> <u>Cell</u>&lt;<u>u32</u>&gt;,
    <i>waker</i><b>:</b> <u>Cell</u>&lt;<i>Option</i>&lt;<i>Waker</i>&gt;&gt;,
    <i>buffer</i><b>:</b> <u>UnsafeCell</u>&lt;[<u>u8</u>; <i>BUF_SIZE</i>]&gt;,
}
</code></pre>
<p>If using unsafe makes your heart skip a beat, <code>RefCell</code> would be an option too. Since reading the buffer can <code>.await</code>, and since there is only ever one <code>BufReader</code> I also only ever need to manage one <code>Waker</code>. When manually implementing a <code>Future</code> registering a waker is as simple as:</p>
<pre><code><i>inner</i><b>.</b><u>waker</u><b>.</b><u>set</u>(<u>Some</u>(<i>cx</i><b>.</b><u>waker</u>()<b>.</b><u>clone</u>()));
</code></pre>
<p>And waking the waker is as simple as:</p>
<pre><code><em>// This is effectively the same as `inner.waker.take()`
// if we didn&#39;t have the `Cell` wrapper.</em>
<b>if let</b> <u>Some</u>(<i>waker</i>) = <i>inner</i><b>.</b><u>waker</u><b>.</b><u>replace</u>(<u>None</u>) {
    <i>waker</i><b>.</b><u>wake</u>();
}
</code></pre>
<p>No <code>unsafe</code>, no <a href="https://crates.io/crates/atomic-waker">atomic-waker</a>, no need for a Ph.D. in <a href="https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html">atomic op ordering</a>, no magic, just single threaded things.</p>
<h2>Hidden Costs</h2>
<p>Just switching to a <a href="https://docs.rs/async-executor/latest/async_executor/struct.LocalExecutor.html">LocalExecutor</a> or something like Tokio <a href="https://docs.rs/tokio/latest/tokio/task/struct.LocalSet.html">LocalSet</a> should be enough, but unfortunately that alone does not free your code from unnecessary thread synchronization. Like the WebSocket in <em>warp</em> that our chat example uses, <a href="https://crates.io/crates/soketto">Soketto</a> is using <code>BiLock</code>s to guarantee safe access to the underlying socket. In fact both <code>Sender</code> and <code>Receiver</code> in Soketto need the ability to write in order to respond to PINGs with PONGs when reading, and just for that writing to a socket has to go through <em>two</em> separate <code>BiLock</code>s (one inside the <code>WriteHalf</code>, and one around it). Even if you discount the actual locking mechanisms, just having all that pointer indirection should be a red flag ðŸš©.</p>
<p>I&#39;m writing all this because I&#39;ve contributed a bunch to Soketto myself and never thought that to be a problem, so I too am not without sin. Making things thread safe for runtime-agnostic utilities like WebSocket is yet another price we pay for making everything multi-threaded by default. The standard way of doing what I&#39;m doing in my code above would be to spawn one of the loops on a separate background task, which could land on a separate thread, meaning we <strong>must</strong> do all that synchronization to manage reading and writing to a socket from different threads <strong>for no good reason</strong>.</p>
<p>Luckily those assumptions are made exclusively in the ecosystem, neither the standard library nor Rust itself bakes in any of such assumptions for async. Yes, the <a href="https://doc.rust-lang.org/std/task/trait.Wake.html"><code>Wake</code></a> trait requires an <code>Arc</code>, <s>but even that has an <a href="https://doc.rust-lang.org/std/task/struct.RawWaker.html">escape hatch</a></s><sup><a href="#n1">1)</a></sup>. <strong>We can fix this</strong>, and all it takes is writing utilities that are <code>!Sync</code> and evangelizing the use of local async executors as the default option, as it should be.</p>
<h2>But I need Threads!</h2>
<p>Of course writing single-threaded servers isn&#39;t always the optimal choice, it likely isn&#39;t for most cases, isn&#39;t that why multi-threaded task executors <em>should</em> be the default?</p>
<p><strong>No.</strong></p>
<p>If you write regular synchronous Rust code, unless you have a really good reason, you don&#39;t just start with a thread-pool. You write single-threaded code until you find a place where threads can help you, and <em>then</em> you parallelize it, which can be as simple as replacing <code>iter</code> with <code>par_iter</code> using <a href="https://crates.io/crates/rayon">Rayon</a>. Rust is <em>exceptional</em> at adding safe parallelization to your code, and for the most part this is how Rust is written everywhere, <em>except async</em>.</p>
<p>It&#39;s <em>easy</em> to parallelize a server that&#39;s using local task executors in Rust, you just spawn multiple threads, each with its own executor. Like in your bog standard non-async Rust the goal is to limit the shared memory accesses between tasks to local thread as much as possible, and only when you absolutely have to communicate between threads do you reach out for the more expensive tools <code>mpsc</code> or <code>RwLock</code>. Your TCP connection needs to access multiple chat rooms that might live on different threads in a pool? <code>TcpStream</code> is <code>Send</code>, you can just <em>move it</em> to the thread on which a given room lives so that it&#39;s always local.</p>
<p>Not only does it make writing async code more pleasant because suddenly you don&#39;t need to worry about synchronization all the time, and you might even be able to use references (again, ðŸ¤¯), this way of doing things even has a name: <strong>Thread-per-Core</strong> (or <strong>Thread/Core</strong>), and it&#39;s actually <a href="https://www.datadoghq.com/blog/engineering/introducing-glommio/">the far better model for writing performant servers</a>. And no, you don&#39;t necessarily need to buy into <a href="https://crates.io/crates/glommio">Glommio</a> with cooperative task programming, I hereby give you permission to still spawn blocking tasks on a separate thread-pool, it&#39;s okay.</p>
<p>Isn&#39;t all of this extra work? After all just calling <code>spawn</code> every time you need to do something in the background is pretty easy, right? Well, I suggest to you, dear reader, that this function signature:</p>
<pre><code><b>pub fn</b> <u>spawn</u>&lt;<i>T</i>&gt;(<i>future</i><b>:</b> <u>T</u>) -&gt; <i>JoinHandle</i>&lt;<i>T</i><b>::</b><u>Output</u>&gt; <b>where</b>
    <i>T</i><b>:</b> <u>Future</u> + <u>Send</u> + <b>&#39;static</b>,
    <i>T</i><b>::</b><u>Output</u><b>:</b> <u>Send</u> + <b>&#39;static</b>,
</code></pre>
<p>is a gun. We give it to all Rustaceans reaching for async, seasoned or new alike, and they all use it to repeatedly shoot themselves in the foot, over, and over, and over again, and most of the time they don&#39;t even know that&#39;s what they are doing. That&#39;s how easy to use it is.</p>
<p>Using multi-threaded <code>spawn</code> reduces the friction of managing multiple tasks across all CPU cores, and it only costs you <strong>friction everywhere else in your entire codebase</strong>, and quite often <strong>performance as well</strong>.</p>
<p>Let&#39;s stop.</p>
<p><sup id="n1">1)</sup> <a href="https://www.reddit.com/r/rust/comments/v8e9fa/local_async_executors_and_why_they_should_be_the/ibqbbzs/">u/SkiFire13 astutely noticed</a> that <code>Waker</code> must implement <code>Sync</code>, so this escape hatch is mostly relevant for environment such as embedded platforms or Wasm, where such constraint isn&#39;t an issue, and where atomics might be unavailable.</p>

</article></div>
  </body>
</html>
