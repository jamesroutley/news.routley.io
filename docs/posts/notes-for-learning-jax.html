<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gianluca.ai/notes-on-jax/">Original</a>
    <h1>Notes for learning JAX</h1>
    
    <div id="readability-page-1" class="page"><div><p>As a learning exercise, I recently implemented <a href="https://github.com/gianlucatruda/simple-jax-nn">simple-jax-nn</a>, a simple neural net for the MNIST dataset, written with JAX.</p><p>This post is my notes on using JAX (as a PyTorch / NumPy user) from working through the JAX docs and a NN tutorial.</p><h2 id="simple-jax-neural-net">Simple JAX neural net</h2><p>I managed to get it to use my MacBook‚Äôs GPU/Accelerator through the Metal support in <code>jax-metal</code>. It‚Äôs a <a href="https://docs.jax.dev/en/latest/installation.html#install-mac-gpu">little bit tricky</a>, but will prove useful in future transformer projects that can make use of the hardware.</p><div><pre tabindex="0"><code data-lang="fallback"><span><span>Metal device set to: Apple M2 Max
</span></span><span><span>DEVICE:METAL:0
</span></span><span><span>Retrieving dataset...
</span></span><span><span>Building generator...
</span></span><span><span>Constructing train set...
</span></span><span><span>Retrieving test dataset...
</span></span><span><span>Constructing test set...
</span></span><span><span>X_train.shape=(60000, 784)	y_train.shape=(60000, 10)
</span></span><span><span>X_test.shape=(10000, 784)	y_test.shape=(10000, 10)
</span></span><span><span>
</span></span><span><span>TRAINING
</span></span><span><span>Initial training set accuracy 0.089
</span></span><span><span>Initial test set accuracy 0.090
</span></span><span><span>[0/20] (1.26s)	Train acc: 0.943	Test acc: 0.941
</span></span><span><span>[1/20] (1.12s)	Train acc: 0.965	Test acc: 0.959
</span></span><span><span>[2/20] (1.12s)	Train acc: 0.975	Test acc: 0.966
</span></span><span><span>[3/20] (1.11s)	Train acc: 0.981	Test acc: 0.971
</span></span><span><span>[4/20] (1.06s)	Train acc: 0.985	Test acc: 0.973
</span></span><span><span>[5/20] (1.06s)	Train acc: 0.988	Test acc: 0.975
</span></span><span><span>[6/20] (1.06s)	Train acc: 0.990	Test acc: 0.976
</span></span><span><span>[7/20] (1.06s)	Train acc: 0.992	Test acc: 0.976
</span></span><span><span>[8/20] (1.06s)	Train acc: 0.994	Test acc: 0.977
</span></span><span><span>[9/20] (1.07s)	Train acc: 0.995	Test acc: 0.977
</span></span><span><span>[10/20] (1.06s)	Train acc: 0.996	Test acc: 0.978
</span></span><span><span>[11/20] (1.06s)	Train acc: 0.996	Test acc: 0.977
</span></span><span><span>[12/20] (1.07s)	Train acc: 0.997	Test acc: 0.978
</span></span><span><span>[13/20] (1.06s)	Train acc: 0.998	Test acc: 0.978
</span></span><span><span>[14/20] (1.07s)	Train acc: 0.998	Test acc: 0.978
</span></span><span><span>[15/20] (1.06s)	Train acc: 0.999	Test acc: 0.978
</span></span><span><span>[16/20] (1.06s)	Train acc: 0.999	Test acc: 0.979
</span></span><span><span>[17/20] (1.20s)	Train acc: 1.000	Test acc: 0.980
</span></span><span><span>[18/20] (1.20s)	Train acc: 1.000	Test acc: 0.980
</span></span><span><span>[19/20] (1.09s)	Train acc: 1.000	Test acc: 0.981
</span></span><span><span>
</span></span><span><span>DONE in 23.01s
</span></span><span><span>
</span></span><span><span>systemMemory: 96.00 GB
</span></span><span><span>maxCacheSize: 36.00 GB
</span></span></code></pre></div><hr/><h2 id="jax-notes">JAX notes</h2><p>My notes from reading through <a href="https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html">Quickstart: How to think in JAX ‚Äî JAX documentation</a> and subsequent docs.</p><p>JAX compiles down to <a href="https://openxla.org/xla/">XLA (accelerated linear algebra)</a> to work across all sorts of underlying hardware. <code>jax.numpy</code> is the numpy-like high-level API designed to be familiar and friendly. Under the hood, it‚Äôs calling the stricter <code>jax.lax</code> API, which in turn compiles to XLA.</p><ul><li>JAX provides a NumPy-inspired interface for convenience.</li><li>Through duck-typing, JAX arrays can often be used as drop-in replacements of NumPy arrays.</li><li>Unlike NumPy arrays, JAX arrays are always <em>immutable</em>.</li></ul><p>JAX is more low-level than PyTorch or Keras, so many common ML tasks and the bookkeeping of training aren‚Äôt abstracted away. So follow the cookbook‚Äôs recipes:
<a href="https://docs.jax.dev/en/latest/the-training-cookbook.html">The Training Cookbook ‚Äî JAX documentation</a></p><p>For convenience, JAX provides <code>jax.numpy</code> which closely mirrors the NumPy API and provides easy entry into JAX. Almost anything that can be done with numpy can be done with <code>jax.numpy</code>, which is typically imported under the <code>jnp</code> alias:</p><p><code>x.devices()</code> where <code>x</code> is a <code>jax.Array</code> tells you on which hardware array contents are stored.</p><p>By default, JAX is an eager executer - sending each op off to XLA in sequence. But this is often slow. Using <code>jax.jit(&lt;function&gt;)</code> or <code>@jit</code> decorator allows just-in-time compilation, which comes with a whole host of performance tricks like fusing ops that massive improve performance.</p><p>The only constraint is that JIT requires all arrays to have static shapes, failing otherwise.</p><p><code>make_jaxpr(f)(x, y)</code> allows you to see the JAX expression (jaxpr), revealing the tracer objects that JIT mode uses to track the sequence of operations. But it depends only on the type and shape, not the values, so value-dependent conditional code will break.</p><p>f there are variables that you would not like to be traced, they can be marked as static for the purposes of JIT compilation with <code>functools.partial</code>.</p><p>JAX provides automatic differentiation via the <code>jax.grad</code> transformation.</p><p>The <code>jax.jacobian</code> transformation can be used to compute the full Jacobian matrix for vector-valued functions</p><p><code>jax.vmap(&lt;function&gt;)</code> is the vectorizing map, which automatically transforms the function into a batch-aware version. This means you can write inefficient for loops, then just slap <code>vmap</code> around the function and it should get way more efficient.</p><p>One major difference with numpy is <a href="https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html#pseudorandom-numbers">random numbers in JAX</a>.</p><p>Numpy uses global random seeds, but this creates thread-safety issues when adapted to JAX. So instead, you must manually keep track of your <code>jax.random.key()</code>. When you feed the key to a random function like <code>random.normal(key)</code> it is consumed but not modified.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>jax</span> <span>import</span> <span>random</span>
</span></span><span><span><span>key</span> <span>=</span> <span>random</span><span>.</span><span>key</span><span>(</span><span>43</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>key</span><span>))</span>
</span></span><span><span><span>print</span><span>(</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>key</span><span>))</span>
</span></span></code></pre></div><p>The rule of thumb is: <strong>never reuse keys (unless you want identical outputs)</strong>. In order to generate different and independent samples, you must jax.random.split the key explicitly before passing it to a random function with <code>new_key, subkey = random.split(key)</code></p><p><a href="https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html">üî™ JAX - The Sharp Bits üî™ ‚Äî JAX documentation</a></p><ul><li>Only use JAX with ‚Äúfunctionally pure‚Äù Python functions. No printing, no globals, no side effects, no iterators.</li><li>Do not mutate arrays in place. Use the provided <code>at</code> property (with <code>set</code>, <code>add</code>, etc.) like <code>x.at[idx].set(y)</code> which creates and returns a <em>new</em> array (but JIT compiler should do some clever things to make this less bad).</li><li>Out-of-bound indexes <em>DO NOT</em> throw errors on accelerator hardware. They silently make weird decisions about how to handle it and carry on.</li><li>Only give JAX array inputs. In numpy you can do <code>np.sum([1, 2, 3])</code>, but this would create individual JAX objects for each element, so you must do <code>jnp.sum(jnp.array([1,2,3]))</code> instead.</li><li>Random numbers (see above).</li><li>Control flow: When executing eagerly (outside of jit), JAX code works with Python control flow and logical operators just like Numpy code. Using control flow and logical operators with jit is more complicated. <code>jax.lax</code> provides some tools for this.</li><li>Never change the shape of arrays.</li><li>Debug your NaNs with <code>JAX_DEBUG_NANS=True</code> or <code>jax.config.update(&#34;jax_debug_nans&#34;, True)</code></li><li>JAX doesn‚Äôt promote to 64-bit floats unless you explicitly set <code>jax_enable_x64</code> config variable.</li></ul></div></div>
  </body>
</html>
