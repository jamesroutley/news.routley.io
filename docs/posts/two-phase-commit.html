<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/two-phase-commit.html">Original</a>
    <h1>Two Phase Commit</h1>
    
    <div id="readability-page-1" class="page"><section id="solution">
<h2>Solution</h2>



<p>The essence of two phase commit, unsurprisingly, is that it carries out an
     update in two phases: </p>

<ul>
<li>the first, prepare, asks each node if it&#39;s able to promise to carry out
      the update</li>

<li>the second, commit, actually carries it out. </li>
</ul>

<p>As part of the prepare phase, each node participating in the transaction
    acquires whatever it needs to assure that it will be able to do the
    commit in the second phase, for instance any locks that are required.
    Once each node is able to ensure it can commit in the second phase, it lets
    the coordinator know, effectively promising the coordinator that it can and will
    commit in the second phase. If any node is unable to make that promise, then
    the coordinator tells all nodes to rollback, releasing any locks they have,
    and the transaction is aborted. Only if all the participants agree to go
    ahead does the second phase commence, at which point it&#39;s expected they will
    all successfully update.</p>

<p>
      Considering a simple distributed key value store implementation, 
      the two phase commit protocol works as follows.
    </p>

<p>
    The transactional client creates a unique identifier called a transaction identifier.
    The client also keeps track of other details like the transaction start time. 
    This is used, as described later by the locking mechanism, to prevent deadlocks.
    The unique id, along with the additional details like the start timestamp, 
    that the client tracks is used to refer the transaction across the cluster nodes.
    The client maintains a transaction reference as follows, which is passed along 
    with every request from the client to other cluster nodes.
    </p>

<p>class TransactionRef…
</p>

<pre>  private UUID txnId;
  private long startTimestamp;


  public TransactionRef(long startTimestamp) {
      this.txnId = UUID.randomUUID();
      this.startTimestamp = startTimestamp;
  }</pre>

<p>class TransactionClient…
</p>

<pre>  TransactionRef transactionRef;

  public TransactionClient(ReplicaMapper replicaMapper, SystemClock systemClock) {
      this.clock = systemClock;
      this.transactionRef = new TransactionRef(clock.now());
      this.replicaMapper = replicaMapper;
  }</pre>

<p>
    One of the cluster nodes acts as a coordinator which tracks the status of the 
    transaction on behalf of the client.     
    In a key-value store, it is generally the cluster node holding data for 
    one of the keys. It is generally picked up as the cluster node storing data
    for the first key used by the client.    
    </p>

<p>
    Before storing any value, the client communicates with the coordinator to notify 
    it about the start of the transaction. 
    Because the coordinator is one of the cluster nodes storing values, 
    it is picked up dynamically when the client initiates a get or put operation 
    with a specific key. 
   </p>

<p>class TransactionClient…
</p>

<pre>  private TransactionalKVStore coordinator;
<span>  private void maybeBeginTransaction(String key) {</span>
      if (coordinator == null) {
          coordinator = replicaMapper.serverFor(key);
          coordinator.begin(transactionRef);
      }
  }</pre>

<p>
      The transaction coordinator keeps track of the status of the transaction. 
      It records every change in a <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html">Write-Ahead Log</a> to make sure 
      that the details are available in case of a crash.
    </p>

<p>class TransactionCoordinator…
</p>

<pre>  Map&lt;TransactionRef, TransactionMetadata&gt; transactions = new ConcurrentHashMap&lt;&gt;();
  WriteAheadLog transactionLog;

  public void begin(TransactionRef transactionRef) {
      TransactionMetadata txnMetadata = new TransactionMetadata(transactionRef, systemClock, transactionTimeoutMs);
      transactionLog.writeEntry(txnMetadata.serialize());
      transactions.put(transactionRef, txnMetadata);
  }</pre>

<p>class TransactionMetadata…
</p>

<pre>  private TransactionRef txn;
  private List&lt;String&gt; participatingKeys = new ArrayList&lt;&gt;();
  private TransactionStatus transactionStatus;</pre>



<p>
        The client sends each key which is part of the transaction to the coordinator. 
        This way the coordinator tracks all the keys which are part of the transaction.
        The coordinator records the keys which are part of the transaction in 
        the transaction metadata.The keys then can be used to know about all of the 
        cluster nodes which are part of the transaction. 
        Because each key-value is generally replicated with the 
        <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html">Replicated Log</a>, 
        the leader server handling the requests for a particular key might change
        over the lifetime of the transaction, so the keys are tracked instead of
        the actual server addresses.  
        The client then sends the put or get requests to the server holding the data 
        for the key. The server is picked based on the partitioning strategy. 
        The thing to note is that the client directly communicates with the server 
        and not through the coordinator. This avoids sending data twice over the network, 
        from client to coordinator, and then from coordinator to the respective server.
       </p>

<p>
            The keys then can be used to know about all the cluster nodes which are 
            part of the transaction. Because each key-value is generally replicated with 
            <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html">Replicated Log</a>, the leader server handling the requests
            for a particular key might change over the life time of the transaction, so 
            keys are tracked, rather than the actual server addresses.
        </p>

<p>class TransactionClient…
</p>

<pre>  public CompletableFuture&lt;String&gt; get(String key) {
      maybeBeginTransaction(key);
      coordinator.addKeyToTransaction(transactionRef, key);
      TransactionalKVStore kvStore = replicaMapper.serverFor(key);
      return kvStore.get(transactionRef, key);
  }

  public void put(String key, String value) {
      maybeBeginTransaction(key);
      coordinator.addKeyToTransaction(transactionRef, key);
      replicaMapper.serverFor(key).put(transactionRef, key, value);
  }</pre>

<p>class TransactionCoordinator…
</p>

<pre>  public synchronized void addKeyToTransaction(TransactionRef transactionRef, String key) {
      TransactionMetadata metadata = transactions.get(transactionRef);
      if (!metadata.getParticipatingKeys().contains(key)) {
          metadata.addKey(key);
          transactionLog.writeEntry(metadata.serialize());
      }
  }</pre>

<p>
      The cluster node handling the request detects that the request is part of a 
      transaction with the transaction ID. It manages the state of the transaction, 
      where it stores the key and the value in the request. The key values are not 
      directly made available to the key value store, but stored separately.
    </p>

<p>class TransactionalKVStore…
</p>

<pre>  public void put(TransactionRef transactionRef, String key, String value) {
      TransactionState state = getOrCreateTransactionState(transactionRef);
      state.addPendingUpdates(key, value);
  }</pre>

<section id="LocksAndTransactionIsolation">
<h3>Locks and Transaction Isolation</h3>



<p>
        The requests also take a lock on the keys. 
        Particularly, the get requests take a read lock and
        the put requests take a write lock. The read locks are taken as the
        values are read.
      </p>

<p>class TransactionalKVStore…
</p>

<pre>  public CompletableFuture&lt;String&gt; get(TransactionRef txn, String key) {
      CompletableFuture&lt;TransactionRef&gt; lockFuture
              = lockManager.acquire(txn, key, LockMode.READ);
      return lockFuture.thenApply(transactionRef -&gt; {
          getOrCreateTransactionState(transactionRef);
          return kv.get(key);
      });
  }

  synchronized TransactionState getOrCreateTransactionState(TransactionRef txnRef) {
      TransactionState state = this.ongoingTransactions.get(txnRef);
      if (state == null) {
          state = new TransactionState();
          this.ongoingTransactions.put(txnRef, state);
      }
      return state;
  }</pre>

<p>The write locks can be taken only when the transaction is about to commit 
        and the values are to be made visible in the key value store. Until then, the
        cluster node can just track the modified values as pending operations.  </p>

<p>
          Delaying locking decreases the chances of conflicting transactions.
        </p>

<p>class TransactionalKVStore…
</p>

<pre>  public void put(TransactionRef transactionRef, String key, String value) {
      TransactionState state = getOrCreateTransactionState(transactionRef);
      state.addPendingUpdates(key, value);
  }</pre>



<p>
          It is important to note that the locks are long lived and not released 
          when the request completes. They are released only when the transaction commits. 
          This technique of holding locks for the duration of the transaction 
          and releasing them only when the transaction commits or rolls back is 
          called <a href="https://en.wikipedia.org/wiki/Two-phase_locking">two-phase-locking</a>. 
          Two-phase locking is critical in providing the serializable isolation level.
          Serializable meaning that the effects of the transactions are visible as
          if they are executed one at a time.
       </p>

<section id="DeadlockPrevention">
<h4>Deadlock Prevention</h4>

<p>
          Usage of locks can cause deadlocks where two transactions wait for 
          each other to release the locks. Deadlocks can be avoided if transactions
          are not allowed to wait and aborted when the conflicts are detected. 
          There are different strategies used to decide which transactions are 
          aborted and which are allowed to continue. 
        </p>

<p>
          The lock manager implements these wait policies as follows:
          
</p><p>class LockManager…
</p>

<pre>  WaitPolicy waitPolicy;</pre>


<p>    
          The WaitPolicy decides what to do when there are conflicting requests.
          
</p><pre>public enum WaitPolicy {
    WoundWait,
    WaitDie,
    Error
}
</pre>


<p>
          The lock is an object which tracks the transactions which currently
          own the lock and the ones which are waiting for the lock.
          
</p><p>class Lock…
</p>

<pre>  Queue&lt;LockRequest&gt; waitQueue = new LinkedList&lt;&gt;();
  List&lt;TransactionRef&gt; owners = new ArrayList&lt;&gt;();
  LockMode lockMode;</pre>


<p>
          When a transaction requests to acquire a lock, the lock manager grants 
          the lock immediately if there are no conflicting transactions already 
          owning the lock. 
          
</p><p>class LockManager…
</p>

<pre>  public synchronized CompletableFuture&lt;TransactionRef&gt; acquire(TransactionRef txn, String key, LockMode lockMode) {
      return acquire(txn, key, lockMode, new CompletableFuture&lt;&gt;());
  }

  CompletableFuture&lt;TransactionRef&gt; acquire(TransactionRef txnRef,
                                            String key,
                                            LockMode askedLockMode,
                                            CompletableFuture&lt;TransactionRef&gt; lockFuture) {
      Lock lock = getOrCreateLock(key);

      logger.debug(&#34;acquiring lock for = &#34; + txnRef + &#34; on key = &#34; + key + &#34; with lock mode = &#34; + askedLockMode);
      if (lock.isCompatible(txnRef, askedLockMode)) {
          lock.addOwner(txnRef, askedLockMode);
          lockFuture.complete(txnRef);
          logger.debug(&#34;acquired lock for = &#34; + txnRef);
          return lockFuture;
      }</pre>

<p>class Lock…
</p>

<pre>  public boolean isCompatible(TransactionRef txnRef, LockMode lockMode) {
      if(hasOwner()) {
          return (inReadMode() &amp;&amp; lockMode == LockMode.READ)
                  || isUpgrade(txnRef, lockMode);
      }
      return true;
  }</pre>


<p>  
          If there are conflicts,
          the lock manager acts depending on the wait policy.
        </p>

<section id="ErrorOnConflict">
<h5>Error On Conflict</h5>

<p>
            If the wait policy is to error out, it will throw an error and the calling
            transaction will rollback and retry after a random timeout.
            
</p><p>class LockManager…
</p>

<pre>  private CompletableFuture&lt;TransactionRef&gt; handleConflict(Lock lock,
                                                           TransactionRef txnRef,
                                                           String key,
                                                           LockMode askedLockMode,
                                                           CompletableFuture&lt;TransactionRef&gt; lockFuture) {
      switch (waitPolicy) {
          case Error: {
              lockFuture.completeExceptionally(new WriteConflictException(txnRef, key, lock.owners));
              return lockFuture;
          }
          case WoundWait: {
              return lock.woundWait(txnRef, key, askedLockMode, lockFuture, this);
          }
          case WaitDie: {
              return lock.waitDie(txnRef, key, askedLockMode, lockFuture, this);
          }
      }
      throw new IllegalArgumentException(&#34;Unknown waitPolicy &#34; + waitPolicy);
  }</pre>


<p>
            In case of contention when there are a lot of user transactions
            trying to acquire locks, if all of them need to restart, it severely
            limits the systems throughput. 
            Data stores try to make sure that there are minimal transaction restarts.                        
          </p>

<p>
            A common technique is to assign a unique ID to transactions and order
            them. For example, <a href="https://cloud.google.com/spanner">Spanner </a><a href="https://dahliamalkhi.github.io/files/SpannerExplained-SIGACT2013b.pdf">assigns unique IDs </a> to transactions
            in such a way that they can be ordered.
            The technique is very similar to the one discussed in 
            <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/paxos.html">Paxos</a> to order requests across cluster nodes. 
            Once the transactions can be ordered, there are two techniques used
            to avoid deadlock, but still allow transactions to continue without
            restarting            
          </p>

<p>
            The transaction reference is created in such a way that it can be 
            compared and ordered with other transaction references. The easiest
            method is to assign a timestamp to each transaction and compare based 
            on the timestamp.
            
</p><p>class TransactionRef…
</p>

<pre>  boolean after(TransactionRef otherTransactionRef) {
      return this.startTimestamp &gt; otherTransactionRef.startTimestamp;
  }</pre>


<p>
            But in distributed systems, 
            <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/time-bound-lease.html#wall-clock-not-monotonic">
              wall clocks are not monotonic</a>, so a different method like
            assigning unique IDs to transactions in such a way that 
            they can be ordered is used. Along with ordered IDs, the age of each 
            is tracked to be able to order the transactions. 
            <a href="https://cloud.google.com/spanner">Spanner</a> orders transactions by tracking the age of each
            transaction in the system.
          </p>

<p>
            To be able to order all the transactions, each cluster node is assigned
            a unique ID. The client picks up the coordinator at the start of 
            the transaction and gets the transaction ID from the coordinator
            The cluster node acting as a coordinator generates transaction 
            IDs as follows.
            
</p><p>class TransactionCoordinator…
</p>

<pre>  private int requestId;
  public MonotonicId begin() {
      return new MonotonicId(requestId++, config.getServerId());
  }</pre>

<p>class MonotonicId…
</p>

<pre>  public class MonotonicId implements Comparable&lt;MonotonicId&gt; {
      public int requestId;
      int serverId;
  
      public MonotonicId(int requestId, int serverId) {
          this.serverId = serverId;
          this.requestId = requestId;
      }
  
      public static MonotonicId empty() {
          return new MonotonicId(-1, -1);
      }
  
      public boolean isAfter(MonotonicId other) {
          if (this.requestId == other.requestId) {
              return this.serverId &gt; other.serverId;
          }
          return this.requestId &gt; other.requestId;
      }</pre>


<p>class TransactionClient…
</p>

<pre>  private void beginTransaction(String key) {
      if (coordinator == null) {
          coordinator = replicaMapper.serverFor(key);
          MonotonicId transactionId = coordinator.begin();
          transactionRef = new TransactionRef(transactionId, clock.nanoTime());
      }
  }</pre>


<p>
            The client tracks the age of the transaction by recording
            the elapsed time since the beginning of the transaction.
            
</p><p>class TransactionRef…
</p>

<pre>  public void incrementAge(SystemClock clock) {
      age = clock.nanoTime() - startTimestamp;
  }</pre>


<p>
            The client increments the age, every time a get or a put request
            is sent to the servers. The transactions are then ordered as
            per their age. The transaction id is used to break the ties when 
            there are same age transactions.
            
</p><p>class TransactionRef…
</p>

<pre>  public boolean isAfter(TransactionRef other) {
       return age == other.age?
                  this.id.isAfter(other.id)
                  :this.age &gt; other.age;
  }</pre>

</section>

<section id="Wound-wait">
<h5>Wound-Wait</h5>

<p>
            In the <a href="http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/8-recv+serial/deadlock-woundwait.html">wound-wait</a> method, if there is a conflict, 
            the transaction reference asking for the lock is compared to all the 
            transactions currently owning the lock. If the lock owners are all 
            younger than the transaction asking for the lock, all of those transactions are aborted. 
            But if the transaction asking the lock is younger than the ones owning
            the transaction, it waits for the lock
            
</p><p>class Lock…
</p>

<pre>  public CompletableFuture&lt;TransactionRef&gt; woundWait(TransactionRef txnRef,
                                                     String key,
                                                     LockMode askedLockMode,
                                                     CompletableFuture&lt;TransactionRef&gt; lockFuture,
                                                     LockManager lockManager) {

      if (allOwningTransactionsStartedAfter(txnRef) &amp;&amp; !anyOwnerIsPrepared(lockManager)) {
          abortAllOwners(lockManager, key, txnRef);
          return lockManager.acquire(txnRef, key, askedLockMode, lockFuture);
      }

      LockRequest lockRequest = new LockRequest(txnRef, key, askedLockMode, lockFuture);
      lockManager.logger.debug(&#34;Adding to wait queue = &#34; + lockRequest);
      addToWaitQueue(lockRequest);
      return lockFuture;
  }</pre>

<p>class Lock…
</p>

<pre>  private boolean allOwningTransactionsStartedAfter(TransactionRef txn) {
      return owners.stream().filter(o -&gt; !o.equals(txn)).allMatch(owner -&gt; owner.after(txn));
  }</pre>


<p>
            One of the key things to notice is that if the transaction owning
            the lock is already in the prepared state of two-phase-commit, it is 
            not aborted.
          </p>
</section>

<section id="Wait-die">
<h5>Wait-Die</h5>

<p>
            The <a href="http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/8-recv+serial/deadlock-waitdie.html">wait-die</a> method works in the opposite way 
            to <a href="http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/8-recv+serial/deadlock-woundwait.html">wound-wait</a>.
            If the lock owners are all younger than the transaction
            asking for the lock, then the transaction waits for the lock. 
            But if the transaction asking for the lock is younger than the ones owning
            the transaction, the transaction is aborted.
            
</p><p>class Lock…
</p>

<pre>  public CompletableFuture&lt;TransactionRef&gt; waitDie(TransactionRef txnRef,
                                                   String key,
                                                   LockMode askedLockMode,
                                                   CompletableFuture&lt;TransactionRef&gt; lockFuture,
                                                   LockManager lockManager) {
      if (allOwningTransactionsStartedAfter(txnRef)) {
          addToWaitQueue(new LockRequest(txnRef, key, askedLockMode, lockFuture));
          return lockFuture;
      }

      lockManager.abort(txnRef, key);
      lockFuture.completeExceptionally(new WriteConflictException(txnRef, key, owners));
      return lockFuture;
  }</pre>


<p>
            Wound-wait mechanism generally has 
            <a href="http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/8-recv+serial/deadlock-compare.html">fewer restarts</a>
            compared to the wait-die method. 
            So data stores like <a href="https://cloud.google.com/spanner">Spanner</a> use the <a href="http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/8-recv+serial/deadlock-woundwait.html">wound-wait</a>
            method.
          </p>

<p>
            When the owner of the transaction releases a lock, 
            the waiting transactions are granted the lock.
            
</p><p>class LockManager…
</p>

<pre>  private void release(TransactionRef txn, String key) {
      Optional&lt;Lock&gt; lock = getLock(key);
      lock.ifPresent(l -&gt; {
          l.release(txn, this);
      });
  }</pre>

<p>class Lock…
</p>

<pre>  public void release(TransactionRef txn, LockManager lockManager) {
      removeOwner(txn);
      if (hasWaiters()) {
          LockRequest lockRequest = getFirst(lockManager.waitPolicy);
          lockManager.acquire(lockRequest.txn, lockRequest.key, lockRequest.lockMode, lockRequest.future);
      }
  }</pre>

</section>
</section>
</section>

<section id="CommitAndRollback">
<h3>Commit and Rollback</h3>

<p>
          Once the client successfully reads without facing any conflicts and 
          writes all the key values, it initiates the commit request by sending 
          a commit request to the coordinator.
        </p>

<p>class TransactionClient…
</p>

<pre>  public CompletableFuture&lt;Boolean&gt; commit() {
      return coordinator.commit(transactionRef);
  }</pre>

<p>The transaction coordinator records the state of the transaction as 
          preparing to commit. The coordinator implements the commit handling in 
          two phases. </p>

<ul>
<li>It first sends the prepare request to each of the participants.</li>

<li>Once it receives a successful response from all the participants, 
                the coordinator marks the transaction as prepared to complete. 
                Then it sends the commit request to all the participants.
             </li>
</ul>

<p>class TransactionCoordinator…
</p>

<pre>  public CompletableFuture&lt;Boolean&gt; commit(TransactionRef transactionRef)  {
      TransactionMetadata metadata = transactions.get(transactionRef);
      metadata.markPreparingToCommit(transactionLog);
      List&lt;CompletableFuture&lt;Boolean&gt;&gt; allPrepared = sendPrepareRequestToParticipants(transactionRef);
      CompletableFuture&lt;List&lt;Boolean&gt;&gt; futureList = sequence(allPrepared);
      return futureList.thenApply(result -&gt; {
          if (!result.stream().allMatch(r -&gt; r)) {
              logger.info(&#34;Rolling back = &#34; + transactionRef);
              rollback(transactionRef);
              return false;
          }
          metadata.markPrepared(transactionLog);
          sendCommitMessageToParticipants(transactionRef);
          metadata.markCommitComplete(transactionLog);
          return true;
      });
  }

  public List&lt;CompletableFuture&lt;Boolean&gt;&gt; sendPrepareRequestToParticipants(TransactionRef transactionRef)  {
      TransactionMetadata transactionMetadata = transactions.get(transactionRef);
      var transactionParticipants = getParticipants(transactionMetadata.getParticipatingKeys());
      return transactionParticipants.keySet()
              .stream()
              .map(server -&gt; server.handlePrepare(transactionRef))
              .collect(Collectors.toList());
  }

  private void sendCommitMessageToParticipants(TransactionRef transactionRef) {
      TransactionMetadata transactionMetadata = transactions.get(transactionRef);
      var participantsForKeys = getParticipants(transactionMetadata.getParticipatingKeys());
      participantsForKeys.keySet().stream()
              .forEach(kvStore -&gt; {
                  List&lt;String&gt; keys = participantsForKeys.get(kvStore);
                  kvStore.handleCommit(transactionRef, keys);
              });
  }

  private Map&lt;TransactionalKVStore, List&lt;String&gt;&gt; getParticipants(List&lt;String&gt; participatingKeys) {
      return participatingKeys.stream()
              .map(k -&gt; Pair.of(serverFor(k), k))
              .collect(Collectors.groupingBy(Pair::getKey, Collectors.mapping(Pair::getValue, Collectors.toList())));
  }</pre>

<p>
          The cluster node receiving the prepare requests do two things:
        </p>

<ul>
<li>It tries to grab the write locks for all of the keys.</li>

<li>Once successful, it writes all of the changes to the write-ahead log.</li>
</ul>

<p>
            If it can successfully do these, 
            it can guarantee that there are no conflicting transactions, 
            and even in the case of a crash the cluster node can recover all the 
            required state to complete the transaction.
          </p>

<p>class TransactionalKVStore…
</p>

<pre>  public synchronized CompletableFuture&lt;Boolean&gt; handlePrepare(TransactionRef txn) {
      try {
          TransactionState state = getTransactionState(txn);
          if (state.isPrepared()) {
              return CompletableFuture.completedFuture(true); //already prepared.
          }

          if (state.isAborted()) {
              return CompletableFuture.completedFuture(false); //aborted by another transaction.
          }

          Optional&lt;Map&lt;String, String&gt;&gt; pendingUpdates = state.getPendingUpdates();
          CompletableFuture&lt;Boolean&gt; prepareFuture = prepareUpdates(txn, pendingUpdates);
          return prepareFuture.thenApply(ignored -&gt; {
              Map&lt;String, Lock&gt; locksHeldByTxn = lockManager.getAllLocksFor(txn);
              state.markPrepared();
<span>              writeToWAL(new TransactionMarker(txn, locksHeldByTxn, TransactionStatus.PREPARED));</span>
              return true;
          });

      } catch (TransactionException| WriteConflictException e) {
          logger.error(e);
      }
      return CompletableFuture.completedFuture(false);
  }

  private CompletableFuture&lt;Boolean&gt; prepareUpdates(TransactionRef txn, Optional&lt;Map&lt;String, String&gt;&gt; pendingUpdates)  {
      if (pendingUpdates.isPresent()) {
          Map&lt;String, String&gt; pendingKVs = pendingUpdates.get();
          CompletableFuture&lt;List&lt;TransactionRef&gt;&gt; lockFuture = acquireLocks(txn, pendingKVs.keySet());
          return lockFuture.thenApply(ignored -&gt; {
              writeToWAL(txn, pendingKVs);
              return true;
          });
      }
      return CompletableFuture.completedFuture(true);
  }

  TransactionState getTransactionState(TransactionRef txnRef) {
      return ongoingTransactions.get(txnRef);
  }

  private void writeToWAL(TransactionRef txn, Map&lt;String, String&gt; pendingUpdates) {
     for (String key : pendingUpdates.keySet()) {
          String value = pendingUpdates.get(key);
          wal.writeEntry(new SetValueCommand(txn, key, value).serialize());
      }
  }

  private CompletableFuture&lt;List&lt;TransactionRef&gt;&gt; acquireLocks(TransactionRef txn, Set&lt;String&gt; keys) {
      List&lt;CompletableFuture&lt;TransactionRef&gt;&gt; lockFutures = new ArrayList&lt;&gt;();
      for (String key : keys) {
          CompletableFuture&lt;TransactionRef&gt; lockFuture = lockManager.acquire(txn, key, LockMode.READWRITE);
          lockFutures.add(lockFuture);
      }
      return sequence(lockFutures);
  }</pre>

<p>
          When the cluster node receives the commit message from the coordinator,
          it is safe to make the key-value changes visible. 
          The cluster node does three things while committing the changes:
        </p>

<ul>
<li>It marks the transaction as committed. Should the cluster node fail at this point, 
              it knows the outcome of the transaction, and can repeat the following steps.</li>

<li>It applies all the changes to the key-value storage</li>

<li>It releases all the acquired locks.</li>
</ul>

<p>class TransactionalKVStore…
</p>

<pre>  public synchronized void handleCommit(TransactionRef transactionRef, List&lt;String&gt; keys) {
      if (!ongoingTransactions.containsKey(transactionRef)) {
          return; //this is a no-op. Already committed.
      }

      if (!lockManager.hasLocksFor(transactionRef, keys)) {
          throw new IllegalStateException(&#34;Transaction &#34; + transactionRef + &#34; should hold all the required locks for keys &#34; + keys);
      }

<span>      writeToWAL(new TransactionMarker(transactionRef, TransactionStatus.COMMITTED, keys));</span>

      applyPendingUpdates(transactionRef);

      releaseLocks(transactionRef, keys);
  }

  private void removeTransactionState(TransactionRef txnRef) {
      ongoingTransactions.remove(txnRef);
  }


  private void applyPendingUpdates(TransactionRef txnRef) {
      TransactionState state = getTransactionState(txnRef);
      Optional&lt;Map&lt;String, String&gt;&gt; pendingUpdates = state.getPendingUpdates();
      apply(txnRef, pendingUpdates);
  }

  private void apply(TransactionRef txnRef, Optional&lt;Map&lt;String, String&gt;&gt; pendingUpdates) {
      if (pendingUpdates.isPresent()) {
          Map&lt;String, String&gt; pendingKv = pendingUpdates.get();
          apply(pendingKv);
      }
      removeTransactionState(txnRef);
  }

  private void apply(Map&lt;String, String&gt; pendingKv) {
      for (String key : pendingKv.keySet()) {
          String value = pendingKv.get(key);
          kv.put(key, value);
      }
  }
  private void releaseLocks(TransactionRef txn, List&lt;String&gt; keys) {
          lockManager.release(txn, keys);
  }

  private Long writeToWAL(TransactionMarker transactionMarker) {
     return wal.writeEntry(transactionMarker.serialize());
  }</pre>

<p>
          The rollback is implemented in a similar way. If there is any failure, 
          the client communicates with the coordinator to rollback the transaction.
        </p>

<p>class TransactionClient…
</p>

<pre>  public void rollback() {
      coordinator.rollback(transactionRef);
  }</pre>

<p>The transaction coordinator records the state of the transaction as preparing 
          to rollback. Then it forwards the rollback request to all of the servers 
          which stored the values for the given transaction. 
          Once all of the requests are successful, the coordinator marks the transaction
          rollback as complete.In case the coordinator crashes after the transaction 
          is marked as &#39;prepared to rollback&#39;, it can keep on sending the rollback
          messages to all the participating cluster nodes.</p>

<p>class TransactionCoordinator…
</p>

<pre>  public void rollback(TransactionRef transactionRef) {
      transactions.get(transactionRef).markPrepareToRollback(this.transactionLog);

      sendRollbackMessageToParticipants(transactionRef);

      transactions.get(transactionRef).markRollbackComplete(this.transactionLog);
  }

  private void sendRollbackMessageToParticipants(TransactionRef transactionRef) {
      TransactionMetadata transactionMetadata = transactions.get(transactionRef);
      var participants = getParticipants(transactionMetadata.getParticipatingKeys());
      for (TransactionalKVStore kvStore : participants.keySet()) {
          List&lt;String&gt; keys = participants.get(kvStore);
          kvStore.handleRollback(transactionMetadata.getTxn(), keys);
      }
  }</pre>

<p>
          The cluster nodes receiving the rollback request does three things:
        </p>

<ul>
<li>It records the state of the transaction as rolled back in the write-ahead log.</li>

<li>It discards the transaction state.</li>

<li>It releases all of the locks</li>
</ul>

<p>class TransactionalKVStore…
</p>

<pre>  public synchronized void handleRollback(TransactionRef transactionRef, List&lt;String&gt; keys) {
      if (!ongoingTransactions.containsKey(transactionRef)) {
          return; //no-op. Already rolled back.
      }
      writeToWAL(new TransactionMarker(transactionRef, TransactionStatus.ROLLED_BACK, keys));
      this.ongoingTransactions.remove(transactionRef);
      this.lockManager.release(transactionRef, keys);
  }</pre>

<section id="IdempotentOperations">
<h4>Idempotent Operations</h4>

<p>
              In case of network failures, the coordinator can retry calls to
              prepare, commit or abort. So these operations need to be 
              <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/idempotent-receiver.html#IdempotentAndNon-idempotentRequests">idempotent</a>. 
            </p>
</section>
</section>

<section id="AnExampleScenario">
<h3>An Example Scenario</h3>

<section id="AtomicWrites">
<h4>Atomic Writes</h4>

<p>
          Consider the following scenario. Paula Blue has a truck and  Steven Green
          has a backhoe.
          The availability and the booking status of the truck and the backhoe 
          are stored on a distributed key-value store. 
          Depending on how the keys are mapped to servers, 
          Blue&#39;s truck and Green&#39;s backhoe bookings are stored on 
          separate cluster nodes.
          Alice is trying to book a truck 
          and backhoe for the construction work she is planning to start on a Monday.
          She needs both the truck and the backhoe to be available.           
        </p>

<p>
          The booking scenario happens as follows.
        </p>

<p>Alice checks the availability of Blue&#39;s truck and Green&#39;s backhoe. 
              by reading the keys ‘truck_booking_monday’ and ‘backhoe_booking_monday’</p>

<div id="blue_get_truck_availability.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/blue_get_truck_availability.png"/></p>
</div>



<div id="blue_get_backhoe_availability.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/blue_get_backhoe_availability.png"/></p>
</div>



<p>If the values are empty, the booking is free.
                She reserves the truck and the backhoe.
                It is important that both the values are set atomically.
                If there is any failure, then none of the values is set.</p>

<p>
                  The commit happens in two phases. The first server Alice
                  contacts acts as the coordinator and executes the two phases.
                </p>

<div id="blue_commit_success.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/blue_commit_success.png"/></p>
</div>



<p>The coordinator is a separate participant in the
                protocol, and is shown that way on the sequence diagram. However
                usually one of the servers (Blue or Green) acts as
                the coordinator, thus playing two roles in the interaction.</p>
</section>

<section id="ConflictingTransactions">
<h4>Conflicting Transactions</h4>

<p>
          Consider a scenario where another person, Bob, is also trying to book a 
          truck and backhoe for construction work on the same Monday. 
        </p>

<p>
          The booking scenario happens as follows:
        </p>

<ul>
<li>Both Alice and Bob read the keys ‘truck_booking_monday’ 
              and ‘backhoe_booking_monday’
            </li>

<li>Both see that the values are empty, meaning the booking is free.</li>

<li>Both try to book the truck and the backhoe.</li>
</ul>

<p>
          The expectation is that, only Alice or Bob, should be able to book, 
          because the transactions are conflicting. 
          In case of errors, the whole flow needs to be retried and hopefully, 
          one will go ahead with the booking. 
          But in no situation, should booking be done partially. 
          Either both bookings should be done or neither is done.
        </p>



<p> To check the availability, both Alice and Bob start a transaction 
          and contact Blue and Green&#39;s servers respectively to check for the availability.
          Blue holds a read lock for the key &#34;truck_booking_on_monday&#34; and
          Green holds a read lock for the key &#34;backhoe_booking_on_monday&#34;. 
          Because read locks are shared, both Alice and Bob can read the values. </p>

<div id="get_truck_availability.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/get_truck_availability.png"/></p>
</div>



<div id="get_backhoe_availability.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/get_backhoe_availability.png"/></p>
</div>



<p>Alice and Bob see that both the bookings are available on Monday. 
          So they reserve by sending the put requests to servers.
          Both the servers hold the put requests in the temporary storage.</p>

<div id="reserve_truck.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/reserve_truck.png"/></p>
</div>



<div id="reserve_backhoe.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/reserve_backhoe.png"/></p>
</div>



<p>
            When Alice and Bob decide to commit the transactions-
            assuming that Blue acts as a coordinator- it triggers the two-phase
            commit protocol and sends the prepare requests to itself and Green.
            
</p><p>
              For Alice&#39;s request it tries to grab a write lock for the key &#39;truck_booking_on_monday&#39;, which
              it can not get, because there is a conflicting read lock grabbed by 
              another transaction. So Alice&#39;s transaction fails in the prepare phase.
              The same thing happens with Bob&#39;s request.
            </p>


<div id="commit_failure_retry.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/commit_failure_retry.png"/></p>
</div>



<section id="">
<p>
              Transactions can be retried with a retry loop as follows:
              
</p><p>class TransactionExecutor…
</p>

<pre>  public boolean executeWithRetry(Function&lt;TransactionClient, Boolean&gt; txnMethod, ReplicaMapper replicaMapper, SystemClock systemClock) {
      for (int attempt = 1; attempt &lt;= maxRetries; attempt++) {
          TransactionClient client = new TransactionClient(replicaMapper, systemClock);
          try {
              boolean checkPassed = txnMethod.apply(client);
              Boolean successfullyCommitted = client.commit().get();
              return checkPassed &amp;&amp; successfullyCommitted;
          } catch (Exception e) {
              logger.error(&#34;Write conflict detected while executing.&#34; + client.transactionRef + &#34; Retrying attempt &#34; + attempt);
              client.rollback();
              randomWait(); //wait for random interval
          }

      }
      return false;
  }</pre>


<p>
              The example booking code for Alice and Bob will look as follows:  
              
</p><p>class TransactionalKVStoreTest…
</p>

<pre>  @Test
  public void retryWhenConflict() {
      List&lt;TransactionalKVStore&gt; allServers = createTestServers(WaitPolicy.WoundWait);

      TransactionExecutor aliceTxn = bookTransactionally(allServers, &#34;Alice&#34;, new TestClock(1));
      TransactionExecutor bobTxn = bookTransactionally(allServers, &#34;Bob&#34;, new TestClock(2));

      TestUtils.waitUntilTrue(() -&gt; (aliceTxn.isSuccess() &amp;&amp; !bobTxn.isSuccess()) || (!aliceTxn.isSuccess() &amp;&amp; bobTxn.isSuccess()), &#34;waiting for one txn to complete&#34;, Duration.ofSeconds(50));
  }

  private TransactionExecutor bookTransactionally(List&lt;TransactionalKVStore&gt; allServers, String user, SystemClock systemClock) {
      List&lt;String&gt; bookingKeys = Arrays.asList(&#34;truck_booking_on_monday&#34;, &#34;backhoe_booking_on_monday&#34;);
      TransactionExecutor t1 = new TransactionExecutor(allServers);
      t1.executeAsyncWithRetry(txnClient -&gt; {
          if (txnClient.isAvailable(bookingKeys)) {
              txnClient.reserve(bookingKeys, user);
              return true;
          }
          return false;
      }, systemClock);
      return t1;
  }</pre>


<p>
              In this case one of the transactions will eventually succeed and 
              the other will back out.
            </p>

<p>
              While it is very easy to implement, with Error WaitPolicy , 
              there will be multiple transaction restarts,reducing the overall 
              throughput.
              As explained in the above section, if Wound-Wait policy is used
              it will have fewer transaction restarts. In the above example,
              only one transaction will possibly restart instead of both restarting
              in case of conflicts.              
            </p>
</section>
</section>
</section>

<section id="UsingVersionedValue">
<h3>Using <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html">Versioned Value</a></h3>

<p>
        It is very constraining to have conflicts for all the read and write
        operations, particularly so when the transactions can be read-only.
        It is optimal if read-only transactions can work without holding any 
        locks and still guarantee that the values read in a transaction 
        do not change with a concurrent read-write transaction.
      </p>

<p>
        Data-stores generally store multiple versions of the values, 
        as described in <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html">Versioned Value</a>.         
        The version used is the timestamp following <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html">Lamport Clock</a>. 
        Mostly a <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html">Hybrid Clock</a> is used in databases like 
        <a href="https://www.mongodb.com/">MongoDB</a> or <a href="https://www.cockroachlabs.com/docs/stable/">CockroachDB</a>. 
        To use it with the two-phase commit protocol, the trick is that every server 
        participating in the transaction sends the timestamp it can write the 
        values at, as response to the prepare request. 
        The coordinator chooses the maximum of these timestamps as a 
        commit timestamp and sends it along with the value. 
        The participating servers then save the value at the commit timestamp.
        This allows read-only requests to be executed without holding locks,
        because it&#39;s guaranteed that the value written at a particular timestamp
        is never going to change. 
      </p>

<p>
        Consider a simple example as follows. Philip is running a report to read
        all of the bookings that happened until timestamp 2. If it is a long-running
        operation holding a lock, Alice, who is trying to book a truck, will be blocked
        until Philip&#39;s work completes. With <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/versioned-value.html">Versioned Value</a>
        Philip&#39;s get requests, which are part of a read-only operation, can continue
        at timestamp 2, while Alice&#39;s booking continues at timestamp 4.        
        
</p><div id="mvcc-read.png"><p><img src="https://martinfowler.com/articles/patterns-of-distributed-systems/mvcc-read.png"/></p>
</div>




<p>
         Note that read requests which are part of a read-write transaction, 
         still need to hold a lock.
      </p>

<p>
        The example code with <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/lamport-clock.html">Lamport Clock</a> looks as follows:
      </p>

<p>class MvccTransactionalKVStore…
</p>

<pre>  public String readOnlyGet(String key, long readTimestamp) {
      adjustServerTimestamp(readTimestamp);
      return kv.get(new VersionedKey(key, readTimestamp));
  }

  public CompletableFuture&lt;String&gt; get(TransactionRef txn, String key, long readTimestamp) {
      adjustServerTimestamp(readTimestamp);
      CompletableFuture&lt;TransactionRef&gt; lockFuture = lockManager.acquire(txn, key, LockMode.READ);
      return lockFuture.thenApply(transactionRef -&gt; {
          getOrCreateTransactionState(transactionRef);
          return kv.get(key);
      });
  }

  private void adjustServerTimestamp(long readTimestamp) {
      this.timestamp = readTimestamp &gt; this.timestamp ? readTimestamp:timestamp;
  }

  public void put(TransactionRef txnId, String key, String value) {
      timestamp = timestamp + 1;
      TransactionState transactionState = getOrCreateTransactionState(txnId);
      transactionState.addPendingUpdates(key, value);
  }</pre>

<p>class MvccTransactionalKVStore…
</p>

<pre>  private long prepare(TransactionRef txn, Optional&lt;Map&lt;String, String&gt;&gt; pendingUpdates) throws WriteConflictException, IOException {
      if (pendingUpdates.isPresent()) {
          Map&lt;String, String&gt; pendingKVs = pendingUpdates.get();

          acquireLocks(txn, pendingKVs);

          timestamp = timestamp + 1; //increment the timestamp for write operation.

          writeToWAL(txn, pendingKVs, timestamp);
       }
      return timestamp;
  }</pre>

<p>class MvccTransactionCoordinator…
</p>

<pre>  public long commit(TransactionRef txn) {
          long commitTimestamp = prepare(txn);

          TransactionMetadata transactionMetadata = transactions.get(txn);
          transactionMetadata.markPreparedToCommit(<span>commitTimestamp,</span> this.transactionLog);

          sendCommitMessageToAllTheServers(txn, <span>commitTimestamp,</span> transactionMetadata.getParticipatingKeys());

          transactionMetadata.markCommitComplete(transactionLog);

          return commitTimestamp;
  }


  public long prepare(TransactionRef txn) throws WriteConflictException {
      TransactionMetadata transactionMetadata = transactions.get(txn);
      Map&lt;MvccTransactionalKVStore, List&lt;String&gt;&gt; keysToServers = getParticipants(transactionMetadata.getParticipatingKeys());
      List&lt;Long&gt; prepareTimestamps = new ArrayList&lt;&gt;();
      for (MvccTransactionalKVStore store : keysToServers.keySet()) {
          List&lt;String&gt; keys = keysToServers.get(store);
          long prepareTimestamp = store.prepare(txn, keys);
          prepareTimestamps.add(prepareTimestamp);
      }
      return prepareTimestamps.stream().max(Long::compare).orElse(txn.getStartTimestamp());
  }</pre>

<p>
        All the participating cluster nodes then store the key-values at the
        commit timestamp.
      </p>

<p>class MvccTransactionalKVStore…
</p>

<pre>  public void commit(TransactionRef txn, List&lt;String&gt; keys, long commitTimestamp) {
      if (!lockManager.hasLocksFor(txn, keys)) {
          throw new IllegalStateException(&#34;Transaction should hold all the required locks&#34;);
      }

      adjustServerTimestamp(commitTimestamp);

      applyPendingOperations(txn, commitTimestamp);

      lockManager.release(txn, keys);

      logTransactionMarker(new TransactionMarker(txn, TransactionStatus.COMMITTED, commitTimestamp, keys, Collections.EMPTY_MAP));
  }

  private void applyPendingOperations(TransactionRef txnId, long commitTimestamp) {
      Optional&lt;TransactionState&gt; transactionState = getTransactionState(txnId);
      if (transactionState.isPresent()) {
          TransactionState t = transactionState.get();
          Optional&lt;Map&lt;String, String&gt;&gt; pendingUpdates = t.getPendingUpdates();
<span>          apply(txnId, pendingUpdates, commitTimestamp);</span>
      }
  }

  private void apply(TransactionRef txnId, Optional&lt;Map&lt;String, String&gt;&gt; pendingUpdates, long commitTimestamp) {
      if (pendingUpdates.isPresent()) {
          Map&lt;String, String&gt; pendingKv = pendingUpdates.get();
          apply(pendingKv, commitTimestamp);
      }
      ongoingTransactions.remove(txnId);
  }


  private void apply(Map&lt;String, String&gt; pendingKv, long commitTimestamp) {
      for (String key : pendingKv.keySet()) {
          String value = pendingKv.get(key);
          kv.put(new VersionedKey(key, commitTimestamp), value);
      }
  }</pre>

<section id="TechnicalConsiderations">
<h4>Technical Considerations</h4>

<p>
            There is another subtle issue to be tackled here. 
            Once a particular response is returned at a given timestamp, 
            no write should happen at a lower timestamp than the one received in 
            the read request. 
            This is achieved by different techniques. 
            <a href="https://research.google/pubs/pub36726/">Google Percolator</a> and 
            datastores like <a href="https://tikv.org/">TiKV</a> inspired by 
            Percolator use a separate server called Timestamp oracle which is 
            guaranteed to give monotonic timestamps. 
            Databases like <a href="https://www.mongodb.com/">MongoDB</a> or <a href="https://www.cockroachlabs.com/docs/stable/">CockroachDB</a> 
            use <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html">Hybrid Clock</a> to
            guarantee it because every request will adjust the hybrid clock 
            on each server to be the most up-todate. The timestamp is also 
            advanced monotonically with every write request. 
            Finally, the commit phase picks up the maximum timestamp across the set 
            of participating servers, making sure that the write will always 
            follow a previous read request.
          </p>

<p>
              It is important to note that, if the client is reading
              at a timestamp value lower than the one at which server is writing to, 
              it is not an issue. But if the client is reading at a timestamp while the server
              is about to write at a particular timestamp, then it is a problem. If servers 
              detect that a client is reading at a timestamp which the server might have
              an in-flight writes (the ones which are only prepared), the servers reject 
              the write. <a href="https://www.cockroachlabs.com/docs/stable/">CockroachDB</a> throws error an if a read happens at 
              a timestamp for which there is an ongoing transaction.
              <a href="https://cloud.google.com/spanner">Spanner</a> reads have a phase where the client gets the 
              time of the last successful write on a particular partition. If a 
              client reads at a higher timestamp, the read requests wait till the writes
              happen at that timestamp.
            </p>
</section>
</section>

<section id="UsingReplicatedLog">
<h3>Using <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html">Replicated Log</a></h3>

<p>
        To improve fault tolerance cluster nodes use <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html">Replicated Log</a>.        
        The coordinator uses Replicated Log to store the transaction log entries.                
      </p>

<p>
        Considering the example of Alice and Bob in the above section, 
        the Blue servers will be a group of servers, so are the Green servers.
        All the booking data will be replicated across a set of servers.
        Each request which is part of the two-phase commit goes to the leader 
        of the server group. The replication is implemented using 
        <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html">Replicated Log</a>.        
      </p>

<p>
        The client communicates with the leader of each server group. 
        The replication is necessary only when the client decides to commit the
        transaction, so it happens as part of the prepare request.
      </p>

<p>
        The coordinator replicates every state change to replicated log as well.
      </p>



        In a distributed datastore, each cluster node handles multiple partitions.
        A <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/replicated-log.html">Replicated Log</a> is maintained per partition. 
        When <a href="https://raft.github.io/">Raft</a> is used as part of replication it&#39;s sometimes
        referred to as <a href="https://www.cockroachlabs.com/blog/scaling-raft/">multi-raft</a>.
      

<p>
        Client communicates with the leader of each partition participating in 
        the transaction.
      </p>
</section>

<section id="FailureHandling">
<h3>Failure Handling</h3>

<p>
        Two-phase commit protocol heavily relies on the coordinator node 
        to communicate the outcome of the transaction. 
        Until the outcome of the transaction is known, 
        the individual cluster nodes cannot allow any other transactions 
        to write to the keys participating in the pending transaction.
        The cluster nodes block until the outcome of the transaction is known.
        This puts some critical requirements on the coordinator
      </p>

<p>The coordinator needs to remember the state of the transactions 
              even in case of a process crash.</p>

<p>
                Coordinator uses <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html">Write-Ahead Log</a> to record every update 
                to the state of the transaction. 
                This way, when the coordinator crashes and comes back up, 
                it can continue to work on the transactions which are incomplete.
              </p>

<p>class TransactionCoordinator…
</p>

<pre>  public void loadTransactionsFromWAL() throws IOException {
      List&lt;WALEntry&gt; walEntries = this.transactionLog.readAll();
      for (WALEntry walEntry : walEntries) {
          TransactionMetadata txnMetadata = (TransactionMetadata) Command.deserialize(new ByteArrayInputStream(walEntry.getData()));
          transactions.put(txnMetadata.getTxn(), txnMetadata);
      }
      startTransactionTimeoutScheduler();
      completePreparedTransactions();
  }
  private void completePreparedTransactions() throws IOException {
      List&lt;Map.Entry&lt;TransactionRef, TransactionMetadata&gt;&gt; preparedTransactions
              = transactions.entrySet().stream().filter(entry -&gt; entry.getValue().isPrepared()).collect(Collectors.toList());
      for (Map.Entry&lt;TransactionRef, TransactionMetadata&gt; preparedTransaction : preparedTransactions) {
          TransactionMetadata txnMetadata = preparedTransaction.getValue();
          sendCommitMessageToParticipants(txnMetadata.getTxn());
      }
  }</pre>

<p>
                The client can fail before sending the commit message to the coordinator.
              </p>

<p>
                The transaction coordinator tracks when each transaction state was updated.
                If no state update is received in a timeout period, which is configured,
                it triggers a transaction rollback. 
              </p>

<p>class TransactionCoordinator…
</p>

<pre>  private ScheduledThreadPoolExecutor scheduler = new ScheduledThreadPoolExecutor(1);
  private ScheduledFuture&lt;?&gt; taskFuture;
  private long transactionTimeoutMs = Long.MAX_VALUE; //for now.

  public void startTransactionTimeoutScheduler() {
      taskFuture = scheduler.scheduleAtFixedRate(() -&gt; timeoutTransactions(),
              transactionTimeoutMs,
              transactionTimeoutMs,
              TimeUnit.MILLISECONDS);
  }

  private void timeoutTransactions() {
      for (TransactionRef txnRef : transactions.keySet()) {
          TransactionMetadata transactionMetadata = transactions.get(txnRef);
          long now = systemClock.nanoTime();
          if (transactionMetadata.hasTimedOut(now)) {
              sendRollbackMessageToParticipants(transactionMetadata.getTxn());
              transactionMetadata.markRollbackComplete(transactionLog);
          }
      }
  }</pre>
</section>

<section id="TransactionsAcrossHeterogenousSystems">
<h3>Transactions across heterogenous systems</h3>

<p>
        The solution outlined here demonstrates the two-phase commit implementation 
        in a homogenous system. Homogenous meaning all the cluster nodes are part
        of the same system and store same kind of data. For example 
        a distributed data store like MongoDb or a distributed message broker 
        like Kafka.
      </p>

<p>
        Historically, two-phase commit was mostly discussed in the context of 
        heterogeneous systems. Most common usage of two-phase commit was
        with <a href="https://pubs.opengroup.org/onlinepubs/009680699/toc.pdf">[XA]</a> transactions. In the J2EE servers, it is very 
        common to use two-phase commit across a message broker and a database.
        The most common usage pattern is when a message needs to be produced
        on a message broker like ActiveMQ or JMS and a record needs to be 
        inserted/updated in a database. 
      </p>

<p>
        As seen in the above sections, the fault tolerance of the coordinator 
        plays a critical role in two-phase commit implementation. In case of XA
        transactions the coordinator is mostly the application process making 
        the database and message broker calls. The application in most modern
        scenarios is a stateless microservice which is running in a containerized
        environment. It is not really a suitable place to put the responsibility
        of the coordinator. The coordinator needs to maintain state and recover
        quickly from failures to commit or rollback, which is difficult to 
        implement in this case.  
      </p>

<p>
        This is the reason that while XA transactions seem so attractive, they 
        often run into <a href="https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/recover-xa-transactions.html">issues
        in practice </a> and are avoided. In the microservices
        world, patterns like <a href="https://microservices.io/patterns/data/transactional-outbox.html">[transactional-outbox]</a> are preferred over
        XA transactions.
      </p>

<p>
        On the other hand most distributed storage systems implement 
        two-phase commit across a set of partitions, and it works well in practice.
      </p>
</section>
</section></div>
  </body>
</html>
