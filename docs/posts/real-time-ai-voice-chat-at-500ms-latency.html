<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/KoljaB/RealtimeVoiceChat">Original</a>
    <h1>Show HN: Real-time AI Voice Chat at ~500ms Latency</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><strong>Have a natural, spoken conversation with an AI!</strong></p>
<p dir="auto">This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description FastVoiceTalk_compressed_step3_h264.mp4">FastVoiceTalk_compressed_step3_h264.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY1NTIxMDgsIm5iZiI6MTc0NjU1MTgwOCwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDZUMTcxNjQ4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjVmMWNhNjEyYjgzOGJjOTEyMGM4MjI1MWQyZWU2NGEwM2RhZmE5YmJjMjQ1NjFjMGRlMGY4Mjg2YmM0NDBmNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ._jY5DWNjE8UzLBb2kCzAitNXRDR2WuWauBjKiEuCFv0" data-canonical-src="https://private-user-images.githubusercontent.com/7604638/440153612-16cc29a7-bec2-4dd0-a056-d213db798d8f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY1NTIxMDgsIm5iZiI6MTc0NjU1MTgwOCwicGF0aCI6Ii83NjA0NjM4LzQ0MDE1MzYxMi0xNmNjMjlhNy1iZWMyLTRkZDAtYTA1Ni1kMjEzZGI3OThkOGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MDZUMTcxNjQ4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjVmMWNhNjEyYjgzOGJjOTEyMGM4MjI1MWQyZWU2NGEwM2RhZmE5YmJjMjQ1NjFjMGRlMGY4Mjg2YmM0NDBmNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ._jY5DWNjE8UzLBb2kCzAitNXRDR2WuWauBjKiEuCFv0" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><em>(early preview - first reasonably stable version)</em></p>

<p dir="auto">A sophisticated client-server system built for low-latency interaction:</p>
<ol dir="auto">
<li>üéôÔ∏è <strong>Capture:</strong> Your voice is captured by your browser.</li>
<li>‚û°Ô∏è <strong>Stream:</strong> Audio chunks are whisked away via WebSockets to a Python backend.</li>
<li>‚úçÔ∏è <strong>Transcribe:</strong> <code>RealtimeSTT</code> rapidly converts your speech to text.</li>
<li>ü§î <strong>Think:</strong> The text is sent to an LLM (like Ollama or OpenAI) for processing.</li>
<li>üó£Ô∏è <strong>Synthesize:</strong> The AI&#39;s text response is turned back into speech using <code>RealtimeTTS</code>.</li>
<li>‚¨ÖÔ∏è <strong>Return:</strong> The generated audio is streamed back to your browser for playback.</li>
<li>üîÑ <strong>Interrupt:</strong> Jump in anytime! The system handles interruptions gracefully.</li>
</ol>

<ul dir="auto">
<li><strong>Fluid Conversation:</strong> Speak and listen, just like a real chat.</li>
<li><strong>Real-Time Feedback:</strong> See partial transcriptions and AI responses as they happen.</li>
<li><strong>Low Latency Focus:</strong> Optimized architecture using audio chunk streaming.</li>
<li><strong>Smart Turn-Taking:</strong> Dynamic silence detection (<code>turndetect.py</code>) adapts to the conversation pace.</li>
<li><strong>Flexible AI Brains:</strong> Pluggable LLM backends (Ollama default, OpenAI support via <code>llm_module.py</code>).</li>
<li><strong>Customizable Voices:</strong> Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via <code>audio_module.py</code>).</li>
<li><strong>Web Interface:</strong> Clean and simple UI using Vanilla JS and the Web Audio API.</li>
<li><strong>Dockerized Deployment:</strong> Recommended setup using Docker Compose for easier dependency management.</li>
</ul>

<ul dir="auto">
<li><strong>Backend:</strong> Python 3.x, FastAPI</li>
<li><strong>Frontend:</strong> HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)</li>
<li><strong>Communication:</strong> WebSockets</li>
<li><strong>Containerization:</strong> Docker, Docker Compose</li>
<li><strong>Core AI/ML Libraries:</strong>
<ul dir="auto">
<li><code>RealtimeSTT</code> (Speech-to-Text)</li>
<li><code>RealtimeTTS</code> (Text-to-Speech)</li>
<li><code>transformers</code> (Turn detection, Tokenization)</li>
<li><code>torch</code> / <code>torchaudio</code> (ML Framework)</li>
<li><code>ollama</code> / <code>openai</code> (LLM Clients)</li>
</ul>
</li>
<li><strong>Audio Processing:</strong> <code>numpy</code>, <code>scipy</code></li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Before You Dive In: Prerequisites üèä‚Äç‚ôÄÔ∏è</h2><a id="user-content-before-you-dive-in-prerequisites-Ô∏è" aria-label="Permalink: Before You Dive In: Prerequisites üèä‚Äç‚ôÄÔ∏è" href="#before-you-dive-in-prerequisites-Ô∏è"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This project leverages powerful AI models, which have some requirements:</p>
<ul dir="auto">
<li><strong>Operating System:</strong>
<ul dir="auto">
<li><strong>Docker:</strong> Linux is recommended for the best GPU integration with Docker.</li>
<li><strong>Manual:</strong> The provided script (<code>install.bat</code>) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).</li>
</ul>
</li>
<li><strong>üêç Python:</strong> 3.9 or higher (if setting up manually).</li>
<li><strong>üöÄ GPU:</strong> <strong>A powerful CUDA-enabled NVIDIA GPU is <em>highly recommended</em></strong>, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower.
<ul dir="auto">
<li>The setup assumes <strong>CUDA 12.1</strong>. Adjust PyTorch installation if you have a different CUDA version.</li>
<li><strong>Docker (Linux):</strong> Requires <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">NVIDIA Container Toolkit</a>.</li>
</ul>
</li>
<li><strong>üê≥ Docker (Optional but Recommended):</strong> Docker Engine and Docker Compose v2+ for the containerized setup.</li>
<li><strong>üß† Ollama (Optional):</strong> If using the Ollama backend <em>without</em> Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.</li>
<li><strong>üîë OpenAI API Key (Optional):</strong> If using the OpenAI backend, set the <code>OPENAI_API_KEY</code> environment variable (e.g., in a <code>.env</code> file or passed to Docker).</li>
</ul>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Getting Started: Installation &amp; Setup ‚öôÔ∏è</h2><a id="user-content-getting-started-installation--setup-Ô∏è" aria-label="Permalink: Getting Started: Installation &amp; Setup ‚öôÔ∏è" href="#getting-started-installation--setup-Ô∏è"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Clone the repository first:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/KoljaB/RealtimeVoiceChat.git
cd RealtimeVoiceChat"><pre>git clone https://github.com/KoljaB/RealtimeVoiceChat.git
<span>cd</span> RealtimeVoiceChat</pre></div>
<p dir="auto">Now, choose your adventure:</p>
<details>
<summary><strong>üöÄ Option A: Docker Installation (Recommended for Linux/GPU)</strong></summary>
<p dir="auto">This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Build the Docker images:</strong>
<em>(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)</em></p>

<p dir="auto"><em>(If you want to customize models/settings in <code>code/*.py</code>, do it <strong>before</strong> this step!)</em></p>
</li>
<li>
<p dir="auto"><strong>Start the services (App &amp; Ollama):</strong>
<em>(Runs containers in the background. GPU access is configured in <code>docker-compose.yml</code>.)</em></p>

<p dir="auto">Give them a minute to initialize.</p>
</li>
<li>
<p dir="auto"><strong>(Crucial!) Pull your desired Ollama Model:</strong>
<em>(This is done <em>after</em> startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Pull the default model (adjust if you configured a different one in server.py)
docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

# (Optional) Verify the model is available
docker compose exec ollama ollama list"><pre><span><span>#</span> Pull the default model (adjust if you configured a different one in server.py)</span>
docker compose <span>exec</span> ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

<span><span>#</span> (Optional) Verify the model is available</span>
docker compose <span>exec</span> ollama ollama list</pre></div>
</li>
<li>
<p dir="auto"><strong>Stopping the Services:</strong></p>

</li>
<li>
<p dir="auto"><strong>Restarting:</strong></p>

</li>
<li>
<p dir="auto"><strong>Viewing Logs / Debugging:</strong></p>
<ul dir="auto">
<li>Follow app logs: <code>docker compose logs -f app</code></li>
<li>Follow Ollama logs: <code>docker compose logs -f ollama</code></li>
<li>Save logs to file: <code>docker compose logs app &gt; app_logs.txt</code></li>
</ul>
</li>
</ol>
</details>
<details>
<summary><strong>üõ†Ô∏è Option B: Manual Installation (Windows Script / venv)</strong></summary>
<p dir="auto">This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.</p>
<p dir="auto"><strong>B1) Using the Windows Install Script:</strong></p>
<ol dir="auto">
<li>Ensure you meet the prerequisites (Python, potentially CUDA drivers).</li>
<li>Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements.

<em>(This opens a new command prompt within the activated virtual environment.)</em>
Proceed to the <strong>&#34;Running the Application&#34;</strong> section.</li>
</ol>
<p dir="auto"><strong>B2) Manual Steps (Linux/macOS/Windows):</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Create &amp; Activate Virtual Environment:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv venv
# Linux/macOS:
source venv/bin/activate
# Windows:
.\venv\Scripts\activate"><pre>python -m venv venv
<span><span>#</span> Linux/macOS:</span>
<span>source</span> venv/bin/activate
<span><span>#</span> Windows:</span>
.<span>\v</span>env<span>\S</span>cripts<span>\a</span>ctivate</pre></div>
</li>
<li>
<p dir="auto"><strong>Upgrade Pip:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install --upgrade pip"><pre>python -m pip install --upgrade pip</pre></div>
</li>
<li>
<p dir="auto"><strong>Navigate to Code Directory:</strong></p>

</li>
<li>
<p dir="auto"><strong>Install PyTorch (Crucial Step - Match Your Hardware!):</strong></p>
<ul dir="auto">
<li><strong>With NVIDIA GPU (CUDA 12.1 Example):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# Verify your CUDA version! Adjust &#39;cu121&#39; and the URL if needed.
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121"><pre><span><span>#</span> Verify your CUDA version! Adjust &#39;cu121&#39; and the URL if needed.</span>
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121</pre></div>
</li>
<li><strong>CPU Only (Expect Slow Performance):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="# pip install torch torchaudio torchvision"><pre><span><span>#</span> pip install torch torchaudio torchvision</span></pre></div>
</li>
<li><em>Find other PyTorch versions:</em> <a href="https://pytorch.org/get-started/previous-versions/" rel="nofollow">https://pytorch.org/get-started/previous-versions/</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Install Other Requirements:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ul dir="auto">
<li><strong>Note on DeepSpeed:</strong> The <code>requirements.txt</code> may include DeepSpeed. Installation can be complex, especially on Windows. The <code>install.bat</code> tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like <a href="https://github.com/erew123/deepspeedpatcher">deepspeedpatcher</a> (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.</li>
</ul>
</li>
</ol>
</details>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Running the Application <g-emoji alias="arrow_forward">‚ñ∂Ô∏è</g-emoji></h2><a id="user-content-running-the-application-Ô∏è" aria-label="Permalink: Running the Application ‚ñ∂Ô∏è" href="#running-the-application-Ô∏è"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>If using Docker:</strong>
Your application is already running via <code>docker compose up -d</code>! Check logs using <code>docker compose logs -f app</code>.</p>
<p dir="auto"><strong>If using Manual/Script Installation:</strong></p>
<ol dir="auto">
<li><strong>Activate your virtual environment</strong> (if not already active):
<div dir="auto" data-snippet-clipboard-copy-content="# Linux/macOS: source ../venv/bin/activate
# Windows: ..\venv\Scripts\activate"><pre><span><span>#</span> Linux/macOS: source ../venv/bin/activate</span>
<span><span>#</span> Windows: ..\venv\Scripts\activate</span></pre></div>
</li>
<li><strong>Navigate to the <code>code</code> directory</strong> (if not already there):

</li>
<li><strong>Start the FastAPI server:</strong>

</li>
</ol>
<p dir="auto"><strong>Accessing the Client (Both Methods):</strong></p>
<ol dir="auto">
<li>Open your web browser to <code>http://localhost:8000</code> (or your server&#39;s IP if running remotely/in Docker on another machine).</li>
<li><strong>Grant microphone permissions</strong> when prompted.</li>
<li>Click <strong>&#34;Start&#34;</strong> to begin chatting! Use &#34;Stop&#34; to end and &#34;Reset&#34; to clear the conversation.</li>
</ol>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Configuration Deep Dive üîß</h2><a id="user-content-configuration-deep-dive-" aria-label="Permalink: Configuration Deep Dive üîß" href="#configuration-deep-dive-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Want to tweak the AI&#39;s voice, brain, or how it listens? Modify the Python files in the <code>code/</code> directory.</p>
<p dir="auto"><strong><g-emoji alias="warning">‚ö†Ô∏è</g-emoji> Important Docker Note:</strong> If using Docker, make any configuration changes <em>before</em> running <code>docker compose build</code> to ensure they are included in the image.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>TTS Engine &amp; Voice (<code>server.py</code>, <code>audio_module.py</code>):</strong></p>
<ul dir="auto">
<li>Change <code>START_ENGINE</code> in <code>server.py</code> to <code>&#34;coqui&#34;</code>, <code>&#34;kokoro&#34;</code>, or <code>&#34;orpheus&#34;</code>.</li>
<li>Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within <code>AudioProcessor.__init__</code> in <code>audio_module.py</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>LLM Backend &amp; Model (<code>server.py</code>, <code>llm_module.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>LLM_START_PROVIDER</code> (<code>&#34;ollama&#34;</code> or <code>&#34;openai&#34;</code>) and <code>LLM_START_MODEL</code> (e.g., <code>&#34;hf.co/...&#34;</code> for Ollama, model name for OpenAI) in <code>server.py</code>. Remember to pull the Ollama model if using Docker (see Installation Step A3).</li>
<li>Customize the AI&#39;s personality by editing <code>system_prompt.txt</code>.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>STT Settings (<code>transcribe.py</code>):</strong></p>
<ul dir="auto">
<li>Modify <code>DEFAULT_RECORDER_CONFIG</code> to change the Whisper model (<code>model</code>), language (<code>language</code>), silence thresholds (<code>silence_limit_seconds</code>), etc. The default <code>base.en</code> model is pre-downloaded during the Docker build.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Turn Detection Sensitivity (<code>turndetect.py</code>):</strong></p>
<ul dir="auto">
<li>Adjust pause duration constants within the <code>TurnDetector.update_settings</code> method.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>SSL/HTTPS (<code>server.py</code>):</strong></p>
<ul dir="auto">
<li>Set <code>USE_SSL = True</code> and provide paths to your certificate (<code>SSL_CERT_PATH</code>) and key (<code>SSL_KEY_PATH</code>) files.</li>
<li><strong>Docker Users:</strong> You&#39;ll need to adjust <code>docker-compose.yml</code> to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.</li>
</ul>
<details>
<summary><strong>Generating Local SSL Certificates (Windows Example w/ mkcert)</strong></summary>
<ol dir="auto">
<li>Install Chocolatey package manager if you haven&#39;t already.</li>
<li>Install mkcert: <code>choco install mkcert</code></li>
<li>Run Command Prompt <em>as Administrator</em>.</li>
<li>Install a local Certificate Authority: <code>mkcert -install</code></li>
<li>Generate certs (replace <code>your.local.ip</code>): <code>mkcert localhost 127.0.0.1 ::1 your.local.ip</code>
<ul dir="auto">
<li>This creates <code>.pem</code> files (e.g., <code>localhost+3.pem</code> and <code>localhost+3-key.pem</code>) in the current directory. Update <code>SSL_CERT_PATH</code> and <code>SSL_KEY_PATH</code> in <code>server.py</code> accordingly. Remember to potentially mount these into your Docker container.</li>
</ul>
</li>
</ol>
</details>
</li>
</ul>
<hr/>

<p dir="auto">Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.</p>

<p dir="auto">The core codebase of this project is released under the <strong>MIT License</strong> (see the <a href="https://github.com/KoljaB/RealtimeVoiceChat/blob/main/LICENSE">LICENSE</a> file for details).</p>
<p dir="auto">This project relies on external specific TTS engines (like <code>Coqui XTTSv2</code>) and LLM providers which have their <strong>own licensing terms</strong>. Please ensure you comply with the licenses of all components you use.</p>
</article></div></div>
  </body>
</html>
