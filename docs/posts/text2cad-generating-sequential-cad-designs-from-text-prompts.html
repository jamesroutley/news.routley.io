<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sadilkhan.github.io/text2cad-project/">Original</a>
    <h1>Text2CAD: Generating sequential cad designs from text prompts</h1>
    
    <div id="readability-page-1" class="page">

    

    

    <header>
                
        
        <img src="https://sadilkhan.github.io/text2cad-project/assets/img/logo_lab_light.png" alt="Results" id="Lab Logo"/>
        
        
        <h3>
                
                <span>¬∑</span>
                
                <span>¬∑</span>
                
                </h3>
        <center> <i>  <sup>*</sup> equal contributions</i>  <span>¬∑</span> <sup>‚Ä†</sup> <i>corresponding author</i></center>
        <center>
            <sup>1</sup><a href="https://av.dfki.de/" target="_blank"><b> German Research Center for AI (DFKI GmbH)</b></a> <span>¬∑</span>
            <sup>2</sup><a href="https://rptu.de/" target="_blank"><b> RPTU</b></a> <span>¬∑</span>
            <sup>3</sup><a href="https://blog.mindgarage.de/" target="_blank"><b> MindGarage</b></a> <span>¬∑</span>
            <sup>4</sup><a href="https://www.bits-pilani.ac.in/hyderabad/" target="_blank"><b> BITS Pilani, Hyderabad</b></a> 
        </center>
        
        <center>
            <h3>NeurIPS 2024 (Spotlight ü§©) </h3>  
        </center>


        <a href="https://arxiv.org/abs/2409.17106" target="_blank">
            <span>
                <i></i>
            </span>
            <span>Arxiv</span>
        </a>
        <a href="" target="_blank">
            <span>
                <i></i>
            </span>
            <span>Code (Soon)</span>
        </a>
        <a href="" target="_blank">
            <span>
                ü§ó
            </span>
            <span>Dataset (Soon)</span>
        </a>
        <a href="" target="_blank">
            <span>
                <i></i>
            </span>
            <span>Demo (Soon)</span>
        </a>
        
    </header>
   

    <section id="contribution">
        <p>
            <video controls="" autoplay="" loop="" muted="" id="teaser_video" poster="assets/img/teaser_light.jpg">
                <source src="assets/animation/teaser_animation.mp4" type="video/mp4"/>
                Your browser does not support the video tag.
            </video>
        </p>
        
        
        
            <p> <b>Text2CAD:</b> Designers can efficiently generate parametric CAD models from text
                prompts. The prompts can vary from abstract shape descriptions to detailed parametric instructions.
            </p>

        <div>
            <center>
                
            </center>
            <p>We propose <b>Text2CAD</b> as the first AI framework for generating parametric CAD designs using <b> multi-level textual descriptions </b>. Our main contributions are:
                </p><ol>
                    <li><a href="#annotation"><b>A Novel Data Annotation Pipeline </b></a> that leverages open-source LLMs and VLMs to annotate <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> dataset with text prompts <u>containing varying level of complexities and parametric details.</u> </li>
                    <li><a href="#architecture"><b>Text2CAD Transformer: </b></a>An end-to-end Transformer based autoregressive architecture for generating CAD design history from input text prompts.</li>
                </ol>
            
        </div>
</section>
    
<section id="annotation">
    <center>
        
    </center>
    <p>Our data annotation pipeline generates multi-level text prompts describing the construction workflow of a CAD model with varying complexities. We use a two-stage method - 
        </p><ol>
            <li> <b>Stage 1</b>: Shape description generation using VLM (<a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LlaVA-NeXT</a>). </li>
            <li> <b>Stage 2</b>: Multi-Level textual annotation generation using LLM (<a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral-50B</a>). </li>
        </ol>
        
    <img src="https://sadilkhan.github.io/text2cad-project/assets/img/data_annot_light.png" alt="Architecture" id="data_annot"/>

    </section>


    <section id="architecture">
        <center>
            
        </center>
        <p>We developed Text2CAD Transformer to transform natural
            language descriptions into 3D CAD models by deducing all its intermediate design steps autoregres-
            sively. Our model takes as input a text prompt \(T\) and a CAD subsequence \(\mathbf{C}_{1:t-1}\) of length \({t-1}\). The text embedding \(T_{adapt}\) is extracted from \(T\) using a pretrained BeRT Encoder followed by a trainable Adaptive layer. The resulting embedding \(T_{adapt}\) and the CAD sequence embedding \(F^0_{t-1}\) is passed through \(\mathbf{L}\) decoder blocks to generate the full CAD sequence in auto-regressive way.
        </p>
        <img src="https://sadilkhan.github.io/text2cad-project/assets/img/arch_light.png" alt="Architecture" id="arch_image"/>
    </section>

    <section id="results">
        <center>
            
        </center>
        <div>
            <div>
                <!-- Clone of the last image -->
                <div>
                    <p>
                        Visual examples of 3D CAD model generation using varied prompts. 
                        (<span>1</span>) Three different prompts yielding the same ring-like model, some without explicitly mentioning ‚Äô<i>ring</i>‚Äô. 
                        (<span>2</span>) Three diverse prompts resulting in same <i>star-shaped model</i>, each emphasizing <i>different star characteristics</i>.
                    </p>
                <p><img src="https://sadilkhan.github.io/text2cad-project/assets/img/qual_3_light.svg" alt="Image 3" id="qual_3_image"/>
                </p></div>
                <!-- Original images -->
                 <div>
                    <p> Qualitative results of the reconstructed CAD models of <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> and Text2CAD
                        on <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> dataset. From top to bottom - <b>Input Texts, Reconstructed CAD models using
                        <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> and Text2CAD respectively and GPT-4V Evaluation.</b></p>
                <p><img src="https://sadilkhan.github.io/text2cad-project/assets/img/qual_1_light.png" alt="Image 1" id="qual_1_image"/>
                     
                     </p></div>
                <div>
                    <p> Qualitative results of the reconstructed CAD models of <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> and Text2CAD
                        on <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> dataset. From top to bottom - <b>Input Texts, Reconstructed CAD models using
                        <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> and Text2CAD respectively and GPT-4V Evaluation.</b></p>
                <p><img src="https://sadilkhan.github.io/text2cad-project/assets/img/qual_2_light.png" alt="Image 2" id="qual_2_image"/> 
                
            </p></div>
                <div>
                    <p>
                        Visual examples of 3D CAD model generation using varied prompts. 
                        (<span>1</span>) Three different prompts yielding the same ring-like model, some without explicitly mentioning ‚Äô<i>ring</i>‚Äô. 
                        (<span>2</span>) Three diverse prompts resulting in same <i>star-shaped model</i>, each emphasizing <i>different star characteristics</i>.
                    </p>
                <p><img src="https://sadilkhan.github.io/text2cad-project/assets/img/qual_3_light.svg" alt="Image 3" id="qual_3_image"/> 
                
                
            </p></div>
                <!-- Clone of the first image -->
                <div>
                    <p> Qualitative results of the reconstructed CAD models of <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> and Text2CAD
                        on <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> dataset. From top to bottom - <b>Input Texts, Reconstructed CAD models using
                        <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> and Text2CAD respectively and GPT-4V Evaluation.</b></p>
                <p><img src="https://sadilkhan.github.io/text2cad-project/assets/img/qual_1_light.png" alt="Image 1" id="qual_1_image"/> 
                
            </p></div>
            
        </div>
            <p><span id="prev-button">‚ùÆ</span>
                <span id="next-button">‚ùØ</span>
            </p>

            
        </div>
    </section>

    <section id="quantitative-results">
        <!-- <center>
            <h1 class="h1_section">Qualitative Results</h1>
        </center>
        <div id="chart-container">
            <canvas id="comparisonChart"></canvas>
        </div> -->

        <center>
            
        </center>
        <p> We evaluated the performance of Text2CAD using two strategies.
            </p><ol>
                <li> <b>CAD Sequence Evaluation:</b> We assess the parametric correspondence between the generated CAD sequences with the input texts. This is done using the following metrics:
                    <ul>
                        <li> <b>F1 Scores</b> of Line, Arc, Circle and Extrusion using the method proposed in <a href="http://skazizali.com/cadsignet.github.io/"> CAD-SIGNet</a>. </li>
                        <li> <b>Chamfer Distance (CD) </b> measures geometric alignment
                            between the ground truth and reconstructed CAD models of Text2CAD and <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a>. </li>
                           <li> <b>Invality Ratio (IR)</b> Measures the invalidity of the reconstructed CAD models. </li>
    
                    </ul>
                </li><li> <b>Visual Inspection:</b> We compare the performance of Text2CAD and <a href="https://arxiv.org/abs/2105.09492">DeepCAD</a> with GPT-4 and Human evaluation. </li>
            </ol>
            
        
        <center><i>Click on the tab to visualize the bar chart. You can also hover on the bars to see the metrics. </i>
            <br/>
        </center>
     
        <!-- Main Tabs -->


<!-- Sequence Evaluation Content -->


<!-- Automatic Evaluation (GPT-4 and User) -->


        
        
    </section>
    
    
    <section id="video">
        <center>
            
            <!-- <div class="message-container">
                <span>Please click on the play button</span>
                <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/YouTube_icon_%282013-2017%29.png/800px-YouTube_icon_%282013-2017%29.png"
                    alt="Play Button" class="play-button">
                <span>to watch the video.</span>
            </div>

            <div style="width: 100%; min-width: 400px; max-width: 800px;">
                <div style="position: relative; width: 100%; overflow: hidden; padding-top: 56.25%;">
                    <p><iframe
                            style="position: absolute; top: 0; left: 0; right: 0; width: 100%; height: 100%; border: none;"
                            src="https://www.youtube.com/embed/ivg03_ckLIM" width="560" height="315"
                            allowfullscreen="allowfullscreen"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe>
                    </p>
                </div>
            </div> -->
             Coming Soon
        </center> 
        
    </section>

    <section id="acknowledgement">
        <center>
            
        </center>
        <p>This work was in parts supported by the EU Horizon Europe Framework under grant agreement <code>101135724</code> (LUMINOUS).
        </p>
    </section>

    <section id="citation">
        <center>
            
        </center>
        <p>If you like our work, please cite.</p>
        
    </section>

    

    
    
    
      



</div>
  </body>
</html>
