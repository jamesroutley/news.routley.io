<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://snap-research.github.io/SnapFusion/">Original</a>
    <h1>SnapFusion: Text-to-Image Diffusion Model on Mobile Devices Within Two Seconds</h1>
    
    <div id="readability-page-1" class="page"><div id="main">
        <p>
            <h2>
                SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds<br/>
            </h2>
            
        </p>
        


        


        


        <div>
            <div>
                <h3>
                    Abstract
                </h3>
                <p>
                    Text-to-image diffusion models can create stunning images from natural language descriptions that
                    rival the work of professional artists and photographers. However, these models are large, with
                    complex network architectures and tens of denoising iterations, making them computationally
                    expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run
                    diffusion models at scale. This is costly and has privacy implications, especially when user data is
                    sent to a third party. To overcome these challenges, we present a generic approach that, for the
                    first time, unlocks running text-to-image diffusion models on mobile devices in <i>less than 2
                        seconds</i>.
                    We achieve so by introducing efficient network architecture and improving step distillation.
                    Specifically, we propose an efficient UNet by identifying the redundancy of the original
                    model and reducing the computation of the image decoder via data distillation.
                    Further, we enhance the step distillation by exploring training strategies and introducing
                    regularization from classifier-free guidance. Our extensive experiments on MS-COCO show that
                    our model with 8 denoising steps achieves better FID and CLIP scores than Stable Diffusion
                    v1.5 with 50 steps. Our work democratizes content creation by bringing powerful text-to-image
                    diffusion models to the hands of users.
                </p>
            </div>
        </div>

        <div>
            <div>
                <h3>
                    On-Device Demo
                </h3>
                <div>
                    <p>
                        <iframe src="https://www.youtube.com/embed/zK5PQ3Oj_L8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
                    </p>
                </div>
            </div>
        </div>

        <div>
            <div>
                <h3>
                    Comparison w/ Stable Diffusion v1.5 on MS-COCO 2014 validation set (30K samples)
                </h3>
                <p><img src="https://snap-research.github.io/SnapFusion/images/fid_vs_clipscore_30k.jpg" alt="overview"/><br/>
            </p></div>
        </div>

        <div>
            <div>
                <h3>
                    More Example Generated Images
                </h3>
                <p><img src="https://snap-research.github.io/SnapFusion/images/more_examples.jpg" alt="overview"/><br/>
            </p></div>
        </div>

    </div></div>
  </body>
</html>
