<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://waabi.ai/oyster/">Original</a>
    <h1>Oyster: Towards Unsupervised Object Detection from Lidar Point Clouds</h1>
    
    <div id="readability-page-1" class="page"><article id="post-647">
  
  <div>

		<div data-hide-featured-media="0">
      
        <div>
		<div id="fws_6487f9856e6d1" data-column-margin="default" data-midnight="dark"><div>
	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>Oyster: Towards Unsupervised Object Detection from LiDAR Point Clouds</h2>
<div>
	<div>
		<div>
<div>
<div>
<p><strong>Lunjun Zhang, Anqi Joyce Yang, Yuwen Xiong, Sergio Casas, </strong><strong>Bin Yang, Mengye Ren, Raquel Urtasun</strong></p>
</div>
</div>
</div>
	</div>
</div>




			</div> 
		</div>
	</div> 

	 
</div></div>
		<div id="fws_6487f9856f51f" data-column-margin="default" data-midnight="dark"><div>
	 

	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>Abstract</h2>
<div>
	<p><span>In this paper, we study the problem of unsupervised object detection from 3D point clouds in self-driving scenes. We present a simple yet effective method that exploits (i) point clustering in near-range areas where the point clouds are dense, (ii) temporal consistency to filter out noisy unsupervised detections, (iii) translation equivariance of CNNs to extend the auto-labels to long range, and (iv) self-supervision for improving on its own. Our approach, <b>OYSTER</b> (<b>O</b>bject Discover<b>y</b> via <b>S</b>patio-<b>Te</b>mporal <b>R</b>efinement), does not impose constraints on data collection (such as repeated traversals of the same location), is able to detect objects in a zero-shot manner without supervised fine-tuning (even in sparse, distant regions), and continues to self-improve given more rounds of iterative self-training. To better measure model performance in self-driving scenarios, we propose a new planning-centric perception metric based on distance-to-collision. We demonstrate that our unsupervised object detector significantly outperforms unsupervised baselines on PandaSet and Argoverse 2 Sensor dataset, showing promise that self-supervision combined with object priors can enable object discovery in the wild.</span></p>
</div>




			</div> 
		</div>
	</div> 
</div></div>
		<div id="fws_6487f9856ff32" data-column-margin="default" data-midnight="dark"><div>
	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>Overview</h2><div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="fade-in">
      <div>
        <div> 
          <p><img data-delay="0" height="489" width="1536" data-animation="fade-in" src="https://waabi.ai/wp-content/uploads/2023/05/oyster.gif" alt=""/>
          </p>
        </div>
      </div>
    </div>
			</div> 
		</div>
	</div> 
</div></div>
		<div id="fws_6487f9857072d" data-column-margin="default" data-midnight="dark"><div>
	 

	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>Method</h2>
<div>
	<div>
		<p><span><b>OYSTER</b> stands for <b>O</b>bject Discover<b>y</b> via <b>S</b>patio-<b>Te</b>mporal <b>R</b>efinement. It has two phases of training: the initial bootstrapping phase, and the self-improvement phase.</span></p>
	</div>
</div>



<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="fade-in">
      <div>
        <div> 
          <p><img data-delay="0" height="2396" width="6942" data-animation="fade-in" src="https://waabi.ai/wp-content/uploads/2023/05/method-1.png" alt="" srcset="https://waabi.ai/wp-content/uploads/2023/05/method-1.png 6942w, https://waabi.ai/wp-content/uploads/2023/05/method-1-300x104.png 300w, https://waabi.ai/wp-content/uploads/2023/05/method-1-1024x353.png 1024w, https://waabi.ai/wp-content/uploads/2023/05/method-1-768x265.png 768w, https://waabi.ai/wp-content/uploads/2023/05/method-1-1536x530.png 1536w, https://waabi.ai/wp-content/uploads/2023/05/method-1-2048x707.png 2048w" sizes="(min-width: 1450px) 75vw, (min-width: 1000px) 85vw, 100vw"/>
          </p>
        </div>
      </div>
    </div>
<div>
	<p><span>The <strong>initial bootstrapping</strong> phase takes advantage of the fact that point clouds in the near range tend to be dense and have clear object clusters, so we can obtain reasonable near-range bounding box seed pseudo-labels via point clustering. Thanks to the translation equivariance property of convolutional nets, we find that a CNN detector trained on near-range labels can generalize to longer-range in a zero-shot manner with the help of data augmentations such as ray dropping that randomly sparsify the inputs.</span></p>
</div>



<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="fade-in">
      <div>
        <div> 
          <p><img data-delay="0" height="1128" width="5418" data-animation="fade-in" src="https://waabi.ai/wp-content/uploads/2023/05/refinement.png" alt="" srcset="https://waabi.ai/wp-content/uploads/2023/05/refinement.png 5418w, https://waabi.ai/wp-content/uploads/2023/05/refinement-300x62.png 300w, https://waabi.ai/wp-content/uploads/2023/05/refinement-1024x213.png 1024w, https://waabi.ai/wp-content/uploads/2023/05/refinement-768x160.png 768w, https://waabi.ai/wp-content/uploads/2023/05/refinement-1536x320.png 1536w, https://waabi.ai/wp-content/uploads/2023/05/refinement-2048x426.png 2048w" sizes="(min-width: 1450px) 75vw, (min-width: 1000px) 85vw, 100vw"/>
          </p>
        </div>
      </div>
    </div>
<div>
	<div>
		<p><span>The <strong>self-improvement</strong> phase utilizes temporal consistency of object tracks as a self-supervision signal. We propose a Track, Refine, Retrain, Repeat framework:</span></p>
<ol>
<li><span>Given noisy detections across time, we employ an unsupervised offline tracker to find object tracks of various lengths;</span></li>
<li><span>We discard short tracks, and refine long tracks, to prepare pseudo-labels for the next round of self-training;</span></li>
<li><span>An object track should have the same physical object size across time, so our refinement process uses track-level information to update pseudo-labels in long tracks;</span></li>
<li><span>We train a new detector on the updated pseudo-labels, dump its outputs as new pseudo-labels, track, refine, retrain, and repeat.</span></li>
</ol>
	</div>
</div>




			</div> 
		</div>
	</div> 
</div></div>
		<div id="fws_6487f98571875" data-column-margin="default" data-midnight="dark"><div>
	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>Comparison against point clustering</h2>
<div>
	<div>
		<div>
<p><span>Below are the Bird-Eye-View (BEV) outputs of our unsupervised object detector (OYSTER) trained <strong>without any labels</strong>. We also visualize the point clustering outputs used for initial bootstrapping to kickstart our self-training process. Note that the model outputs and ground-truth labels are class-agnostic bounding boxes.</span></p>
</div>
	</div>
</div>



<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="fade-in">
      <div>
        <div> 
          <p><img data-delay="0" height="5198" width="2559" data-animation="fade-in" src="https://waabi.ai/wp-content/uploads/2023/05/compare-with-clustering.png" alt="" srcset="https://waabi.ai/wp-content/uploads/2023/05/compare-with-clustering.png 2559w, https://waabi.ai/wp-content/uploads/2023/05/compare-with-clustering-148x300.png 148w, https://waabi.ai/wp-content/uploads/2023/05/compare-with-clustering-504x1024.png 504w, https://waabi.ai/wp-content/uploads/2023/05/compare-with-clustering-768x1560.png 768w, https://waabi.ai/wp-content/uploads/2023/05/compare-with-clustering-756x1536.png 756w, https://waabi.ai/wp-content/uploads/2023/05/compare-with-clustering-1008x2048.png 1008w" sizes="(min-width: 1450px) 75vw, (min-width: 1000px) 85vw, 100vw"/>
          </p>
        </div>
      </div>
    </div>
			</div> 
		</div>
	</div> 
</div></div>
		<div id="fws_6487f98571fe0" data-column-margin="default" data-midnight="dark"><div>
	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>Comparison against state-of-the-art</h2>
<div>
	<div>
		<div>
<p><span>Comparing our detector to the previous SOTA of unsupervised detection method <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/You_Learning_To_Detect_Mobile_Objects_From_LiDAR_Scans_Without_Labels_CVPR_2022_paper.pdf">MODEST</a>:</span></p>
</div>
	</div>
</div>



<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="fade-in">
      <div>
        <div> 
          <p><img data-delay="0" height="5198" width="2559" data-animation="fade-in" src="https://waabi.ai/wp-content/uploads/2023/05/compare-on-argoverse.png" alt="" srcset="https://waabi.ai/wp-content/uploads/2023/05/compare-on-argoverse.png 2559w, https://waabi.ai/wp-content/uploads/2023/05/compare-on-argoverse-148x300.png 148w, https://waabi.ai/wp-content/uploads/2023/05/compare-on-argoverse-504x1024.png 504w, https://waabi.ai/wp-content/uploads/2023/05/compare-on-argoverse-768x1560.png 768w, https://waabi.ai/wp-content/uploads/2023/05/compare-on-argoverse-756x1536.png 756w, https://waabi.ai/wp-content/uploads/2023/05/compare-on-argoverse-1008x2048.png 1008w" sizes="(min-width: 1450px) 75vw, (min-width: 1000px) 85vw, 100vw"/>
          </p>
        </div>
      </div>
    </div><h3>Evolution of Self-Training Pseudo-Labels</h3>
<div>
	<div>
		<div>
<div>
<p><span>We visualize the evolution of our self-training pseudo-labels from Bird-Eye-View (BEV), from the initial point clustering step to the self-training iterations later on.</span></p>
</div>
</div>
	</div>
</div>



<div data-max-width="100%" data-max-width-mobile="default" data-shadow="none" data-animation="fade-in">
      <div>
        <div> 
          <p><img data-delay="0" height="5198" width="2799" data-animation="fade-in" src="https://waabi.ai/wp-content/uploads/2023/05/self-training-evolution.png" alt="" srcset="https://waabi.ai/wp-content/uploads/2023/05/self-training-evolution.png 2799w, https://waabi.ai/wp-content/uploads/2023/05/self-training-evolution-162x300.png 162w, https://waabi.ai/wp-content/uploads/2023/05/self-training-evolution-551x1024.png 551w, https://waabi.ai/wp-content/uploads/2023/05/self-training-evolution-768x1426.png 768w, https://waabi.ai/wp-content/uploads/2023/05/self-training-evolution-827x1536.png 827w, https://waabi.ai/wp-content/uploads/2023/05/self-training-evolution-1103x2048.png 1103w" sizes="(min-width: 1450px) 75vw, (min-width: 1000px) 85vw, 100vw"/>
          </p>
        </div>
      </div>
    </div>
<div>
	<div>
		<div>
<div>
<p><span>As the visualization above shows, our method of iterative self-training starts with very noisy pseudo-labels but manages to remove false positives, discover missed detections, and improve the quality of unsupervised bounding boxes.</span></p>
</div>
</div>
	</div>
</div>




			</div> 
		</div>
	</div> 
</div></div>
		<div id="fws_6487f98572b73" data-column-margin="default" data-midnight="dark"><div>
	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>Conclusion</h2>
<div>
	<div>
		<div>
<p><span>We have proposed a novel method, OYSTER, for unsupervised object detection from LiDAR point clouds. Using weak object priors (near-range point clustering) as a bootstrapping step, our method can train an object detector with no human annotations, by first utilizing the translation equivariance of CNNs to generate long-range pseudo-labels, and then deriving self-supervision signals from the temporal consistency of object tracks. Our proposed self-training loop is highly effective for teaching an unsupervised detector to self-improve. We validate our results on two real-world datasets, Pandaset and Argoverse 2 Sensor, where our model outperforms prior unsupervised methods by a significant margin. Making self-supervised learning work on real-world robot perception is an exciting challenge for AI, and our work takes a step towards allowing robots to make sense of the visual world without human supervision.</span></p>
</div>
	</div>
</div>




			</div> 
		</div>
	</div> 
</div></div>
		<div id="fws_6487f98572f38" data-column-margin="default" data-midnight="dark"><div>
	<div data-padding-pos="all" data-has-bg-color="false" data-bg-color="" data-bg-opacity="1" data-animation="" data-delay="0">
		<div>
			<div>
				<h2>BibTeX</h2>
<div>
	<div>
		<pre><code>@InProceedings{zhang_cvpr2023_oyster,
      author    = {Zhang, Lunjun and Yang, Anqi Joyce and Xiong, Yuwen and Casas, Sergio and Yang, Bin and Ren, Mengye and Urtasun, Raquel},
      title     = {Towards Unsupervised Object Detection From LiDAR Point Clouds},
      booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
      month     = {June},
      year      = {2023},
      pages     = {9317-9328}
  }</code></pre>
	</div>
</div>




			</div> 
		</div>
	</div> 
</div></div>
</div>        
      </div><!--/post-content-->
      
    </div><!--/inner-wrap-->
    
</article></div>
  </body>
</html>
