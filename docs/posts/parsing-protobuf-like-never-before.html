<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mcyoung.xyz/2025/07/16/hyperpb/">Original</a>
    <h1>Parsing Protobuf like never before</h1>
    
    <div id="readability-page-1" class="page"><div> <p>Historically I have worked on many projects related to high-performance Protobuf, be that on the C++ runtime, on the Rust runtime, or on integrating <a href="https://github.com/protocolbuffers/protobuf/tree/main/upb">UPB</a>, the fastest Protobuf runtime, written by my colleague <a href="https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html">Josh Haberman</a>. I <em>generally</em> don’t post directly about my current job, but my most recent toy-turned-product is something I’m very excited to write about: <a href="https://github.com/bufbuild/hyperpb-go"><code>hyperpb</code></a>.</p> <p>Here’s how we measure up against other Go Protobuf parsers. This is a subset of my benchmarks, since the benchmark suite contains many dozens of specimens. This was recorded on an AMD Zen 4 machine.</p> <figure> <p><img src="https://mcyoung.xyz/public/images/hyperpb/benches.svg" alt=""/></p> <figcaption> <p>Throughput for various configurations of <code>hyperpb</code> (colored bars) vs. competing parsers (grey bars). Each successive <code>hyperpb</code> includes all previous optimizations, corresponding to <a href="#zerocopy-strings">zerocopy mode</a>, <a href="#arena-reuse">arena reuse</a>, and <a href="#tdptype">profile-guided optimization</a>. Bigger is better.</p> </figcaption> </figure> <p>Traditionally, Protobuf backends would generate parsers by generating source code specialized to each type. Naively, this would give the best performance, because everything would be “right-sized” to a particular message type. Unfortunately, now that we know better, there are a bunch of drawbacks:</p> <ol> <li>Every type you care about must be compiled ahead-of-time. Tricky for when you want to build something generic over schemas your users provide you.</li> <li>Every type contributes to a cost on the instruction cache, meaning that if your program parses a lot of different types, it will essentially flush your instruction cache any time you enter a parser. Worse still, if a parse involves enough types, the parser itself will hit instruction decoding throughput issues.</li> </ol> <p>These effects are not directly visible in normal workloads, but other side-effects are visible: for example, giant switches on field numbers can turn into chains of branch instructions, meaning that higher-numbered fields will be quite slow. Even binary-searching on field numbers isn’t exactly ideal. However, we know that every Protobuf codec ever emits fields in index order (i.e., declaration order in the <code>.proto</code> file), which is a data conditioning fact we don’t take advantage of with a switch.</p> <p>UPB solves this problem. It is a small C kernel for parsing Protobuf messages, which is completely dynamic: a UPB “parser” is actually a collection of data tables that are evaluated by a <em>table-driven parser</em>. In other words, a UPB parser is actually configuration for an interpreter VM, which executes Protobuf messages as its bytecode. UPB also contains many arena optimizations to improve allocation throughput when parsing complex messages.</p> <p><code>hyperpb</code> is a brand new library, written in the most cursed Go imaginable, which brings many of the optimizations of UPB to Go, and many new ones, while being tuned to Go’s own weird needs. The result leaves the competition in the dust in virtually every benchmark, while being completely runtime-dynamic. This means it’s faster than Protobuf Go’s own generated code, <em>and</em> <a href="https://github.com/planetscale/vtprotobuf"><code>vtprotobuf</code></a> (a popular but non-conforming<sup id="fnref:vtproto" role="doc-noteref"><a href="#fn:vtproto" rel="footnote">1</a></sup> parser generator for Go).</p> <p>This post is about some of the internals of <code>hyperpb</code>. I have also prepared a more sales-ey writeup, which you can read on <a href="https://buf.build/blog/introducing-hyperpb">the Buf blog</a>.</p> <h2 id="why-reinvent-upb"><a href="#why-reinvent-upb">Why Reinvent UPB?</a></h2> <p>UPB is awesome. It can slot easily into any language that has C FFI, which is basically every language ever.</p> <p>Unfortunately, Go’s C FFI is really, really bad. It’s hard to overstate how bad cgo is. There isn’t a good way to cooperate with C on memory allocation (C can’t really handle Go memory without a lot of problems, due to the GC). Having C memory get cleaned up by the GC requires finalizers, which are very slow. Calling into C is very slow, because Go pessimistically assumes that C requires a large stack, and also calling into C does nasty things to the scheduler.</p> <p>All of these things can be worked around, of course. For a while I considered compiling UPB to assembly, and rewriting that assembly into Go’s awful assembly syntax<sup id="fnref:go-asm" role="doc-noteref"><a href="#fn:go-asm" rel="footnote">2</a></sup>, and then having Go assemble UPB out of that. This presents a few issues though, particularly because Go’s assembly calling convention is still in the stone age<sup id="fnref:abi0" role="doc-noteref"><a href="#fn:abi0" rel="footnote">3</a></sup> (arguments are passed on the stack), and because we would still need to do a lot of work to get UPB to match the <code>protoreflect</code> API.</p> <p>Go also has a few… unique qualities that make writing a Protobuf interpreter an interesting challenge with <em>exciting</em> optimization opportunities.</p> <p>First, of course, is the register ABI, which on <code>x86_64</code> gives us a whopping <em>nine</em> argument and return registers, meaning that we can simply pass the entire parser state in registers all the time.</p> <p>Second is that Go does not have much UB to speak of, so we can get away with a lot of very evil pointer crimes that we could not in C++ or Rust.</p> <p>Third is that Protobuf Go has a robust reflection system that we can target if we design specifically for it.</p> <p>Also, the Go ecosystem seems much more tolerant of less-than-ideal startup times (because the language <em>loves</em> life-before-main due to <code>init()</code> functions), so unlike UPB, we can require that the interpreter’s program be generated at runtime, meaning that we can design for online PGO. In other words, we have the perfect storm to create the first-ever Protobuf JIT compiler (which we also refer to as “online PGO” or “real-time PGO”).</p> <h2 id="hyperpbs-api"><a href="#hyperpbs-api">hyperpb’s API</a></h2> <p>Right now, <code>hyperpb</code>’s API is very simple. There are <code>hyperpb.Compile*</code> functions that accept some representation of a message descriptor, and return a <code>*hyperpb.MessageType</code>, which implements the <code>protoreflect</code> type APIs. This can be used to allocate a new <code>*hyperpb.Message</code> , which you can shove into <code>proto.Unmarshal</code> and do reflection on the result. However, you can’t mutate <code>*hyperpb.Message</code>s currently, because the main use-cases I am optimizing for are read-only. All mutations panic instead.</p> <p>The hero use-case, using Buf’s <code>protovalidate</code> library, uses reflection to execute validation predicates. It looks like this:</p> <div><figure><pre><code data-lang="go"><span>// Compile a new message type, deserializing an encoded FileDescriptorSet.</span>
<span>msgType</span> <span>:=</span> <span>hyperpb</span><span>.</span><span>CompileForBytes</span><span>(</span><span>schema</span><span>,</span> <span>&#34;my.api.v1.Request&#34;</span><span>)</span>

<span>// Allocate a new message of that type.</span>
<span>msg</span> <span>:=</span> <span>hyperpb</span><span>.</span><span>NewMessage</span><span>(</span><span>msgType</span><span>)</span>

<span>// Unmarshal like you would any other message, using proto.Unmarshal.</span>
<span>if</span> <span>err</span> <span>:=</span> <span>proto</span><span>.</span><span>Unmarshal</span><span>(</span><span>data</span><span>,</span> <span>msg</span><span>);</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
    <span>// Handle parse failure.</span>
<span>}</span>

<span>// Validate the message. Protovalidate uses reflection, so this Just Works.</span>
<span>if</span> <span>err</span> <span>:=</span> <span>protovalidate</span><span>.</span><span>Validate</span><span>(</span><span>msg</span><span>);</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
    <span>// Handle validation failure.</span>
<span>}</span></code></pre></figure></div> <p>We tell users to make sure to cache the compilation step because compilation can be arbitrarily slow: it’s an optimizing compiler! This is not unlike the same warning on <code>regexp.Compile</code>, which makes it easy to teach users how to use this API correctly.</p> <p>In addition to the main API, there’s a bunch of performance tuning knobs for the compiler, for unmarshaling, and for recording profiles. Types can be recompiled using a recorded profile to be more optimized for the kinds of messages that actually come on the wire. <code>hyperpb</code> PGO<sup id="fnref:go-pgo" role="doc-noteref"><a href="#fn:go-pgo" rel="footnote">4</a></sup> affects a number of things that we’ll get into as I dive into the implementation details.</p> <h2 id="the-guts"><a href="#the-guts">The Guts</a></h2> <p>Most of the core implementation lives under <a href="https://github.com/bufbuild/hyperpb-go/tree/main/internal/tdp"><code>internal/tdp</code></a>. The main components are as follows:</p> <ol> <li><code>tdp</code>, which defines the “object code format” for the interpreter. This includes definitions for describing types and fields to the parser.</li> <li><code>tdp/compiler</code>, which contains all of the code for converting a <code>protoreflect.MessageDescriptor</code> into a <code>tdp.Library</code>, which contains all of the types relevant to a particular parsing operation.</li> <li><code>tdp/dynamic</code> defines what dynamic message types look like. The compiler does a bunch of layout work that gets stored in <code>tdp.Type</code> values, which a <code>dynamic.Message</code> interprets to find the offsets of fields within itself.</li> <li><code>tdp/vm</code> contains the core interpreter implementation, including the VM state that is passed in registers everywhere. It also includes hand-optimized routines for parsing varints and validating UTF-8.</li> <li><code>tdp/thunks</code> defines <em>archetypes</em>, which are classes of fields that all use the same layout and parsers. This corresponds roughly to a <code>(presence, kind)</code> pair, but not exactly. There are around 200 different archetypes.</li> </ol> <p>This article won’t be a deep-dive into everything in the parser, and even this excludes large portions of <code>hyperpb</code>. For example, the <a href="https://mcyoung.xyz/2025/04/21/go-arenas/"><code>internal/arena</code></a> package is already described in a different blogpost of mine. I recommend taking a look at that to learn about how we implement a GC-friendly arena for <code>hyperpb</code> .</p> <p>Instead, I will give a brief overview of how the object code is organized and how the parser interprets it. I will also go over a few of the more interesting optimizations we have.</p> <h3 id="tdptype"><a href="#tdptype">tdp.Type</a></h3> <p>Every <code>MessageDescriptor</code> that is reachable from the root message (either as a field or as an extension) becomes a <code>tdp.Type</code> . This contains the dynamic size of the corresponding message type, a pointer to the type’s default parser (there can be more than one parser for a type) and a variable number of <code>tdp.Field</code> values. These specify the offset of each field and provide accessor thunks, for actually extracting the value of the field.</p> <p>A <code>tdp.TypeParser</code> is what the parser VM interprets alongside encoded Protobuf data. It contains all of the information needed for decoding a message in compact form, including <code>tdp.FieldParser</code>s for each of its fields (and extensions), as well as a hashtable for looking up a field by tag, which is used by the VM as a fallback.</p> <p>The <code>tdp.FieldParser</code>s each contain:</p> <ol> <li>The same offset information as a <code>tdp.Field</code>.</li> <li>The field’s tag, in a special format.</li> <li>A function pointer that gets called to parse the field.</li> <li>The next field(s) to try parsing after this one is parsed.</li> </ol> <p>Each <code>tdp.FieldParser</code> actually corresponds to a possible tag on a record for this message. Some fields have multiple different tags: for example, a <code>repeated int32</code> can have a <code>VARINT</code>-type tag for the repeated representation, and a <code>LEN</code>-type tag for the packed representation.</p> <p>Each field specifies which fields to try next. This allows the compiler to perform <em>field scheduling</em>, by carefully deciding which order to try fields in based both on their declaration order and a rough estimation of their “hotness”, much like branch scheduling happens in a program compiler. This avoids almost all of the work of looking up the next field in the common case, because we have already pre-loaded the correct guess.</p> <p>I haven’t managed to nail down a good algorithm for this yet, but I am working on a system for implementing a type of “branch prediction” for PGO, that tries to provide better predictions for the next fields to try based on what has been seen before.</p> <p>The offset information for a field is more than just a memory offset. A <code>tdp.Offset</code> includes a bit offset, for fields which request allocation of individual bits in the message’s bitfields. These are used to implement the hasbits of <code>optional</code> fields (and the values of <code>bool</code> fields). It also includes a byte offset for larger storage. However, this byte offset can be negative, in which case it’s actually an offset into the <em>cold region</em>.</p> <p>In many messages, most fields won’t be set, particularly extensions. But we would like to avoid having to allocate memory for the very rare (i.e., “cold”) fields. For this, a special “cold region” exists in a separate allocation from the main message, which is referenced via a compressed pointer. If a message happens to need a cold field set, it takes a slow path to allocate a cold region only if needed. Whether a field is cold is a dynamic property that can be affected by PGO.</p> <h3 id="the-parser-vm"><a href="#the-parser-vm">The Parser VM</a></h3> <p>The parser is designed to make maximal use of Go’s generous ABI without spilling anything to the stack that isn’t absolutely necessary. The parser state consists of eight 64-bit integers, split across two types: <code>vm.P1</code> and <code>vm.P2</code>. Unfortunately, these can’t be merged due to a <a href="https://github.com/golang/go/issues/72897">compiler bug</a>, as documented in <a href="https://github.com/bufbuild/hyperpb-go/tree/main/internal/tdp/vm/vm.go#L50"><code>vm/vm.go</code></a>.</p> <p>Every parser function takes these two structs as its first two arguments, and returns them as its first two results. This ensures that register allocation tries its darnedest to keep those eight integers in the first eight argument registers, even across calls. This leads to the common idiom of</p> <div><figure><pre><code data-lang="go"><span>var</span> <span>n</span> <span>int</span>
<span>p1</span><span>,</span> <span>p2</span><span>,</span> <span>n</span> <span>=</span> <span>DoSomething</span><span>(</span><span>p1</span><span>,</span> <span>p2</span><span>)</span></code></pre></figure></div> <p>Overwriting the parser state like this ensures that future uses of p1 and p2 use the values that <code>DoSomething</code> places in registers for us.</p> <p>I spent a lot of time and a lot of profiling catching all of the places where Go would incorrectly spill parser state to the stack, which would result in stalls. I found quite a few codegen bugs in the process. Particularly notable (and shocking!) is <a href="https://github.com/golang/go/issues/73589">#73589</a>. Go has somehow made it a decade without a very basic pointer-to-SSA lifting pass (for comparison, this is a heavy-lifting cleanup pass (<code>mem2reg</code>) in LLVM).</p> <p>The core loop of the VM goes something like this:</p> <ol> <li>Are we out of bytes to parse? If so, pop a parser stack frame<sup id="fnref:stack" role="doc-noteref"><a href="#fn:stack" rel="footnote">5</a></sup>. If we popped the last stack frame, parsing is done; return success.</li> <li>Parse a tag. This does not fully decode the tag, because <code>tdp.FieldParser</code>s contain a carefully-formatted, partially-decoded tag to reduce decoding work.</li> <li>Check if the next field we would parse matches the tag. <ol> <li>If yes, call the function pointer <code>tdp.Field.Parser</code>; update the current field to <code>tdp.Field.NextOk</code>; goto 1.</li> <li>If no, update the current field to <code>tdp.Field.NextErr</code>; goto 3.</li> <li>If no “enough times”, fall through.</li> </ol> </li> <li>Slow path: hit <code>tdp.Field.Tags</code> to find the matching field for that tag. <ol> <li>If matched, go to 3a.</li> <li>If not, this is an unknown field; put it into the unknown field set; parse a tag and goto 4.</li> </ol> </li> </ol> <p>Naturally, this is implemented as a single function whose control flow consists exclusively of <code>if</code>s and <code>goto</code>s, because getting Go to generate good control flow otherwise proved too hard.</p> <p>Now, you might be wondering why the hot loop for the parser includes <strong>calling a virtual function</strong>. Conventional wisdom holds that virtual calls are slow. After all, the actual virtual call instruction is quite slow, because it’s an indirect branch, meaning that it can easily stall the CPU. However, it’s actually <em>much faster than the alternatives</em> in this case, due to a few quirks of our workload and how modern CPUs are designed:</p> <ol> <li>Modern CPUs are not <em>great</em> at traversing complex “branch mazes”. This means that selecting one of ~100 alternatives using branches, even if they are well-predicted and you use unrolled binary search, is still likely to result in frequent mispredictions, and is an obstacle to other JIT optimizations in the processor’s backend.</li> <li>Predicting a single indirect branch with dozens of popular targets <em>is</em> something modern CPUs are pretty good at. Chips and Cheese have a <a href="https://chipsandcheese.com/i/138977313/indirect-branch-prediction">great writeup</a> on the indirect prediction characteristics of Zen 4 chips.</li> </ol> <p>In fact, the “optimized” form of a large switch is a jump table, which is essentially an array of function pointers. Rather than doing a large number of comparisons and direct branches, a jump table turns a switch into a load and an indirect branch.</p> <p>This is great news for us, because it means we can make use of a powerful assumption about most messages: most messages only feature a handful of field archetypes. How often is it that you see a message which has more in it than <code>int32</code>, <code>int64</code>, <code>string</code> , and submessages? In effect, this allows us to have a very large “instruction set”, consisting of all of the different field archetypes, but a particular message only pays for what it uses. The fewer archetypes it uses <em>at runtime</em>, the better the CPU can predict this indirect jump.</p> <p>On the other hand, we can just keep adding archetypes over time to specialize for common parse workloads, which PGO can select for. Adding new archetypes that are not used by most messages does not incur a performance penalty.</p> <h2 id="other-optimizations"><a href="#other-optimizations">Other Optimizations</a></h2> <p>We’ve already discussed the hot/cold split, and briefly touched on the message bitfields used for <code>bool</code>s and hasbits. I’d like to mention a few other cool optimizations that help cover all our bases, as far as high-performance parsing does.</p> <h3 id="zerocopy-strings"><a href="#zerocopy-strings">Zerocopy Strings</a></h3> <p>The fastest <code>memcpy</code> implementation is the one you don’t call. For this reason, we try to, whenever possible, avoid copying anything out of the input buffer. <code>string</code>s and <code>bytes</code> are represented as <a href="https://github.com/bufbuild/hyperpb-go/tree/main/internal/zc/zc.go"><code>zc.Range</code>s</a>, which are a packed pair of offset+length in a <code>uint64</code>. Protobuf is not able to handle lengths greater than 2GB properly, so we can assume that this covers all the data we could ever care about. This means that a <code>bytes</code> field is 8 bytes, rather than 24, in our representation.</p> <p>Zerocopy is also used for packed fields. For example, a <code>repeated double</code> will typically be encoded as a <code>LEN</code> record. The number of <code>float64</code>s in this record is equal to its length divided by 8, and the <code>float64</code>s are already encoded in IEEE754 format for us. So we can just retain the whole repeated fields as a <code>zc.Range</code> . Of course, we need to be able to handle cases where there are multiple disjoint records, so the backing <a href="https://github.com/bufbuild/hyperpb-go/blob/main/internal/tdp/repeated/scalars.go"><code>repeated.Scalars</code></a> can also function as a 24-byte arena slice. Being able to switch between these modes gracefully is a delicate and carefully-tested part of the repeated field thunks.</p> <p>Surprisingly, we also use zerocopy for varint fields, such as <code>repeated int32</code>. Varints are variable-length, so we can’t just index directly into the packed buffer to get the <code>n</code> th element… unless all of the elements happen to be the same size. In the case that every varint is one byte (so, between 0 and 127), we can zerocopy the packed field. This is a relatively common scenario, too, so it results in big savings<sup id="fnref:rep-benches" role="doc-noteref"><a href="#fn:rep-benches" rel="footnote">6</a></sup>. We <em>already</em> count the number of varints in the packed field in order to preallocate space for it, so this doesn’t add extra cost. This counting is very efficient because I have manually vectorized the loop.</p> <h3 id="repeated-preloads"><a href="#repeated-preloads">Repeated Preloads</a></h3> <p>PGO records the median size of each repeated/map field, and that is used to calculate a “preload” for each repeated field. Whenever the field is first allocated, it is pre-allocated using the preload to try to right-size the field with minimal waste.</p> <p>Using the median ensures that large outliers don’t result in huge memory waste; instead, this guarantees that at least 50% of repeated fields will only need to allocate from the arena once. Packed fields don’t use the preload, since in the common case only one record appears for packed fields. This mostly benefits string- and message-typed repeated fields, which can’t be packed.</p> <h3 id="map-optimizations"><a href="#map-optimizations">Map Optimizations</a></h3> <p>We don’t use Go’s built-in map, because it has significant overhead in some cases: in particular, it has to support Go’s mutation-during-iteration semantics, as well as deletion. Although both are Swisstables<sup id="fnref:map-benches" role="doc-noteref"><a href="#fn:map-benches" rel="footnote">7</a></sup> under the hood, my implementation can afford to take a few shortcuts. It also allows our implementation to use arena-managed memory. <a href="https://github.com/bufbuild/hyperpb-go/tree/main/internal/swiss/table.go"><code>swiss.Table</code>s</a> are used both for the backing store of <code>map</code> fields, and for maps inside of <code>tdp.Type</code>s.</p> <p>Currently, the hash used is the variant of <a href="https://github.com/rust-lang/rustc-hash">fxhash used by the Rust compiler</a>. This greatly out-performs Go’s <a href="https://pkg.go.dev/hash/maphash">maphash</a> for integers, but maphash is better for larger strings. I hope to maybe switch to maphash at some point for large strings, but it hasn’t been a priority.</p> <h3 id="arena-reuse"><a href="#arena-reuse">Arena Reuse</a></h3> <p>Hitting the Go allocator is always going to be a little slow, because it’s a general-case allocator. Ideally, we should learn the estimated memory requirements for a particular workload, and then allocate a single block of that size for the arena to portion out.</p> <p>The best way to do this is via <em>arena reuse</em> In the context of a service, each request has a bounded lifetime on the message that it parses. Once that lifetime is over (the request is complete), the message is discarded. This gives the programmer an opportunity to <em>reset</em> the backing arena, so that it keeps its largest memory block for re-allocation.</p> <p>You can show that over time, this will cause the arena to never hit the Go allocator. If the largest block is too small for a message, a block twice as large will wind up getting allocated. Messages that use the same amount of memory will keep doubling the largest block, until the largest block is large enough to fit the whole message. Memory usage will be at worst 2x the size of this message. Note that, thanks to extensive use of zero-copy optimizations, we can often avoid allocating memory for large portions of the message.</p> <p>Of course, arena re-use poses a memory safety danger, if the previously allocated message is kept around after the arena is reset. For this reason, it’s not the default behavior. Using arena resets is a double-digit percentage improvement, however.</p> <h3 id="oneof-unions"><a href="#oneof-unions">Oneof Unions</a></h3> <p>Go does not properly support unions, because the GC does not keep the necessary book-keeping to distinguish a memory location that may be an integer <em>or</em> a pointer at runtime. Instead, this gets worked around using interfaces, which is always a pointer to some memory. Go’s GC can handle untyped pointers just fine, so this just works.</p> <p>The generated API for Protobuf Go uses interface values for <code>oneof</code>s. This API is… pretty messy to use, unfortunately, and triggers unnecessary allocations, (much like <code>optional</code> fields do in the open API).</p> <p>However, my arena design (<a href="https://mcyoung.xyz/2025/04/21/go-arenas/">read about it here</a>) makes it possible to store arena pointers on the arena as if they are integers, since the GC does not need to scan through arena memory. Thus, our <code>oneof</code>s are true unions, like in C++.</p> <h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2> <p><code>hyperpb</code> is really exciting because its growing JIT capabilities offer an improvement in the state of the art over UPB. It’s also been a really fun challenge working around Go compiler bugs to get the best assembly possible. The code is already so well-optimized that re-building the benchmarks with the Go compiler’s own PGO mode (based on a profile collected from the benchmarks) didn’t really seem to move the needle!</p> <p>I’m always working on making <code>hyperpb</code> better (I get paid for it!) and I’m always excited to try new optimizations. If you think of something, file an issue! I have meticulously commented most things within <code>hyperpb</code> , so it should be pretty easy to get an idea of where things are if you want to contribute.</p> <p>I would like to write more posts diving into some of the weeds of the implementation. I can’t promise anything, but there’s lots to talk about. For now… have fun source-diving!</p> <p>There’s a lot of other things we could be doing: for example, we could be using SIMD to parse varints, we could have smarter parser scheduling, we could be allocating small submessages inline to improve locality… there’s still so much we can do!</p> <p>And most importantly, I hope you’ve learned something new about performance optimization!</p>  </div></div>
  </body>
</html>
