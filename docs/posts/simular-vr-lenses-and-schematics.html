<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simulavr.com/blog/lenses-and-vxr-schematics/">Original</a>
    <h1>Simular VR: Lenses and Schematics</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
            
<p>We just received updated lenses from our optics suppliers and ran some QA tests on them.</p>
<p><strong>TLDR:</strong> We&#39;re pretty blown away by how good things look in person. Even without software distortion correction, text and other fine details are <em>extremely crisp</em>. We can&#39;t wait to show this quality off in our <a href="https://simulavr.com/blog/review-units-timeline">review units</a>.</p>
<p><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/SV1_latest_lens.png" alt=""/></p>
<p>The lenses are being cradled by our custom lens holders (shown in gray); we still have some iteration left on these holders, but they&#39;re reasonably close to their final form.</p>
<p>To be fair: things aren&#39;t yet perfect. For example there are still some <a href="https://en.wikipedia.org/wiki/Chromatic_aberration">chromatic abberations</a> (color distortions) around the lens edges, which are visible in these shots. This will be fixed via our monado distortion code (see below).</p>
<p><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/SV1_lens_preview.png" alt=""/></p>
<p>One other disclaimer: the shots we&#39;re taking here are just with our phone, so admittedly things don&#39;t look as good here as they do in person.</p>
<h2 id="1-1-distortion-profile">1.1 Distortion profile</h2>
<p>The first thing to notice is that our lens distortion profile looks reasonably good: images look squished together towards the center of the lenses and stretched apart at the edges. This is exactly how they&#39;re supposed to look, as per our <a href="https://simulavr.com/blog/ppd-optics/">optical design</a>.</p>
<p><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/SV1_lens_distortion_profile.png" alt=""/></p>
<p>Of course, no distortion like this will be visible in final headsets. We are only looking at images before &#34;distortion correction&#34; software rendering is applied (see below).</p>
<h2 id="1-2-siemens-star-test">1.2 Siemens star test</h2>
<p>To inspect for image quality, we can render a <a href="https://en.wikipedia.org/wiki/Siemens_star">Siemens star</a> and see how it looks. A Siemens star consists of bright &#34;spokes&#34; that radiate from a common center (getting wider the further they get from the center). In theory, the spokes meet at the exact center of the Siemens star; when printed or displayed on a device with resolution limitations, however, the spokes will touch at some distance from the center. This makes it a useful tool for inspecting pixel density/image quality:</p>
<p><a href="https://www.wolframcloud.com/obj/george.w.singer/business/png/siemens_star_closeup.png"><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/SV1_spiral_optics_test.png" alt=""/></a></p>
<p>We can also render a checkboard through our display + lens setup as well:</p>
<p><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/SV1_checkerboard_test.png" alt=""/></p>
<p>This makes the chromatic abberations on the edges painfully obvious (to be fixed).</p>
<h2 id="1-3-rendering-simula">1.3 Rendering Simula</h2>
<p>The final test is actually rendering Simula. Here we can start to get a sense for how text quality appears:</p>
<p><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/Simula_monado_through_lens.png" alt=""/></p>
<p>Keep in mind this shot is being taken through a smart phone. Everything looks much better in person.</p>
<h2 id="1-4-distortion-pseudocode">1.4 Distortion pseudocode</h2>
<p>As mentioned in our <a href="https://simulavr.com/blog/ppd-optics/">pixel density post</a>, we must &#34;de-distort&#34; images at the software/rendering level in order for things to look normal. The code for this will be implemented in our <a href="https://github.com/SimulaVR/monado">monado plugin</a>, and will look something like the following:</p>
<pre data-lang="c++"><code data-lang="c++"><span>//This pseudocode is based on &#39;u_compute_distortion_vive&#39; function
</span><span>// Somewhere at the program (constants definition)
</span><span>/* Display size in mm */
</span><span>float</span><span> _DispDimsX = ...
</span><span>float</span><span> _DispDimsY = ...
</span><span>/* Half of the horizontal field of view (in radians) fovH/2 */
</span><span>float</span><span> _FoVh_2 = ...
</span><span>/* Field of view aspect ratio (fovH/fovV), equals to 1 if fovH = fovV */
</span><span>float</span><span> _aspect = ...
</span><span>...
</span><span>struct</span><span> xrt_vec2 tc[</span><span>3</span><span>];
</span><span>...
</span><span>// Just before applying the polynomial (maybe before the loop or at the beginning of it)
</span><span>// Denormalization: conversion from uv texture coordinates (origin at bottom left corner) to mm display coordinates
</span><span>struct</span><span> xrt_vec2 XoYo = {</span><span>0</span><span>,</span><span>0</span><span>};            </span><span>// Assuming (0,0) at the center of the display: -DispDimsX/2 &lt;= XoYo.x &lt;= DispDimsX/2; -DispDimsY &lt;= XoYo.y &lt;= DispDimsY
</span><span>XoYo.</span><span>x </span><span>= _DispDimsX *(u - </span><span>0.5</span><span>);
</span><span>XoYo.</span><span>y </span><span>= _DispDimsY *(v - </span><span>0.5</span><span>);
</span><span>
</span><span>struct</span><span> xrt_vec2 tanH_tanV = {</span><span>0</span><span>,</span><span>0</span><span>};   </span><span>// Resulting angular coordinates (tan(H), tan(V)) of input image corresponding to the coordinates of the input texture whose color will be sampled
</span><span>
</span><span>float</span><span> r2 = </span><span>m_vec2_dot</span><span>(XoYo, XoYo);
</span><span>float</span><span> r = </span><span>sqrt</span><span>(r2);
</span><span>
</span><span>// 9 degree polynomial (only odd coefficients)
</span><span>float</span><span> k1 = ...
</span><span>float</span><span> k3 = ...
</span><span>float</span><span> k5 = ...
</span><span>float</span><span> k7 = ...
</span><span>float</span><span> k9 = ...
</span><span>float</span><span> k = r * (k1 + r2 * (k3 + r2 * (k5 + r2 * (k7 + r2 * k9))));
</span><span>
</span><span>// Avoid problems when r = 0
</span><span>if </span><span>(r &gt; </span><span>0</span><span>) { 
</span><span>    tanH_tanV.</span><span>x </span><span>= (k * XoYo.</span><span>x</span><span>) /r;
</span><span>    tanH_tanV.</span><span>y </span><span>= (k * XoYo.</span><span>y</span><span>) /r;
</span><span>} </span><span>else </span><span>{
</span><span>    tanH_tanV.</span><span>x </span><span>= </span><span>0</span><span>;
</span><span>    tanH_tanV.</span><span>y </span><span>= </span><span>0</span><span>;
</span><span>} 
</span><span>
</span><span>// Normalization: Trasformation from angular coordinates (tan(H), tan(V)) of input image to tc (normalized coordinates with origin at the bottom left corner)
</span><span>tc[i].</span><span>x </span><span>= (tanH_tanV.</span><span>x </span><span>+ </span><span>tan</span><span>(_FoVh_2)) / (</span><span>2 </span><span>* </span><span>tan</span><span>(_FoVh_2));
</span><span>tc[i].</span><span>y </span><span>= ((tanH_tanV.</span><span>y </span><span>+ </span><span>tan</span><span>(_FoVh_2) / _aspect) / (</span><span>2 </span><span>* </span><span>tan</span><span>(_FoVh_2))) * _aspect;
</span></code></pre>
<!-- Once we add this to our rendering loop, the squeezing in the inner regions and stretching in the outer regions will be corrected, and images will look "normal" through the lenses. We go through this ritual to compress more pixel density inside the inner regions of the lenses. -->

<p><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/VXR7200_evaluation_board.png" alt=""/></p>
<p>Recall from a <a href="https://simulavr.com/blog/demoing-our-displays/">recent display post</a> that we are placing a Synaptics VXR7200 between our onboard computer and our two VR displays (to convert a DisplayPort lane to two MIPI-DSI streams). We have uploaded PCB design schematics for our custom VXR7200 assembly, as well as an interposer which fits between it and our VR displays:</p>
<p><a href="https://www.wolframcloud.com/obj/george.w.singer/business/png/VXR7200_schematics_collage_large.png"><img src="https://www.wolframcloud.com/obj/george.w.singer/business/png/VXR7200_schematics_collage.png" alt=""/></a></p>
<p>The code can be found on GitHub:</p>
<ul>
<li><a href="https://github.com/SimulaVR/VXR7200_Bridge">SimulaVR/VXR7200</a></li>
<li><a href="https://github.com/SimulaVR/DisplayInterposer">SimulaVR/DisplayInterposer</a></li>
</ul>
<p>If you&#39;re just interested in seeing our current schematics more closely, they can be found here:</p>
<ul>
<li><a href="https://www.wolframcloud.com/obj/george.w.singer/simula/vxr7200_schematic.pdf">VXR7200</a></li>
<li><a href="https://www.wolframcloud.com/obj/george.w.singer/simula/interposer_schematic.pdf">SimulaVR/DisplayInterposer</a></li>
</ul>
<p>See you next week!</p>

          </div></div>
  </body>
</html>
