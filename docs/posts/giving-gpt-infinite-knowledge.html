<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sudoapps.substack.com/p/giving-gpt-infinite-knowledge">Original</a>
    <h1>Giving GPT “Infinite” Knowledge</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png" width="424" height="520" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:520,&#34;width&#34;:424,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:431640,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cc8fa6e-bfdc-480c-bf1a-0212e0b4bc6f_424x520.png 1456w" sizes="100vw"/></picture></div></a></figure></div><p>The never-ending stream of information generated each day makes it impractical to constantly train Large Language Models (LLMs) with new, relevant data. Besides, some data remains private and inaccessible. Depending solely on the LLMs’ training dataset to predict the next set of characters for a specific information retrieval question isn&#39;t always going to lead to an accurate response. This is when you might start seeing more hallucinations.</p><p>The OpenAI GPT models have their knowledge cut off in September 2021, and Sam Altman recently admitted that GPT-5 isn&#39;t their main focus.</p><blockquote><p><a href="https://techcrunch.com/2023/04/14/sam-altman-size-of-llms-wont-matter-as-much-moving-forward/" rel="nofollow ugc noopener">“I think we’re at the end of the era where it’s going to be these, like, giant, giant models. We’ll make them better in other ways.”</a><span> - Sam Altman</span></p></blockquote><p><span>This makes sense, given the purpose of LLMs is to understand and interpret language. Once these models achieve a high level of comprehension, training larger models with more data may not offer significant improvements (not to be mistaken with reinforcement learning through human feedback). Instead, providing LLMs with real-time, relevant data for interpretation and understanding can make them more valuable. OpenAI’s </span><a href="https://openai.com/blog/chatgpt-plugins" rel="nofollow ugc noopener">code interpreter and plugins</a><span> are showing how powerful this can be. </span></p><p>So, how can we provide LLMs with loads of data and ask them questions related to it? Let&#39;s dive into these core areas at a high level:</p><ul><li><p>Tokens</p></li><li><p>Embeddings </p></li><li><p>Vector Storage </p></li><li><p>Prompting </p></li></ul><p>If you are familiar with any of the several LLM models, you know that there are token limitations for the initial prompt (context) and response it generates (or the entire chat if you are using chat completion for GPT). Each model is trained on a number of tokens that sets this initial limitation. This token limit is also the reason you can’t take hundreds of large documents and inject it directly into the prompt for the LLM to then create an inference from. Here are the limits for some of the most popular models today: </p><ul><li><p>GPT-4 - 8,192 tokens (there is a 32k token version that is being released slowly)</p></li><li><p>GPT-3.5 - 4,096 tokens</p></li><li><p>Llama - 2,048 tokens</p></li><li><p>StabilityLM - 4,096 tokens</p></li></ul><p>Using OpenAI&#39;s token-to-word estimation, we can assume that 1,000 tokens are 750 words. Even though an 8k token model can accommodate approximately 10 pages of text in the initial prompt with some tokens remaining for the model response, there is still a constraint on the specific data set and the length of the conversation before it loses context. But what if you want to supply the model with thousands of documents to interpret and answer questions about, allowing it the flexibility to &#34;decide&#34; which documents are relevant to the question before responding? Let’s start by exploring how the data needs to be stored. </p><p><span>Embeddings are vector representations of a given string, which simplifies their integration with various machine learning models or algorithms. Here&#39;s an example of OpenAI&#39;s popular </span><a href="https://platform.openai.com/docs/api-reference/embeddings/create" rel="nofollow ugc noopener">embeddings API</a><span> (</span><a href="https://github.com/UKPLab/sentence-transformers" rel="nofollow ugc noopener">another open source version</a><span>):</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png" width="964" height="1284" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/f3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1284,&#34;width&#34;:964,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:143416,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3238ab7-874d-4316-8be2-67c22f54104b_964x1284.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>https://platform.openai.com/docs/api-reference/embeddings/create</figcaption></figure></div><p>Storing vast amounts of text and data as vectors enables us to extract only the essential pieces related to a specific question before injecting into the LLM prompt. The order of what we are trying to accomplish includes: </p><ol><li><p>Generate embeddings for all your documents and store them in a vector database (we will get into this later)</p></li><li><p>When a user asks a question, create embeddings for the question and perform a similarity search for relevant information (cosine similarity being a popular method). </p></li><li><p>We only inject the relevant text, up until the token limit, into the prompt before asking the AI to answer the user’s question.  </p></li></ol><p>Embeddings based search isn’t the only solution for this but it is the most widely used right now with LLMs like GPT (lexical-based search and graph-based search being other options). Some services might combine multiple methods. </p><p>There are a number of services that you can use to store vector data. Here are a few:</p><ul><li><p><a href="https://www.pinecone.io" rel="nofollow ugc noopener">Pinecone</a><span> </span></p></li><li><p><a href="https://github.com/milvus-io/milvus" rel="nofollow ugc noopener">Milvus</a><span> (open source)</span></p></li><li><p><a href="https://github.com/chroma-core/chroma" rel="nofollow ugc noopener">Chroma</a><span> (open source)</span></p></li></ul><p>Regardless of your specific use case, vector storage enables you to reference extensive documents, previous chat conversations, and even code when interacting with the LLM. This provides the capability to create a &#34;memory&#34; or knowledge base for your AI.</p><p>Here is a simple example of using Pinecone to store text after generating its embeddings using the OpenAI API:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png" width="1456" height="375" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:375,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:105617,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f174b6e-335a-4891-b52d-7d71fe30a5ba_1576x406.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/memory/pinecone.py</figcaption></figure></div><p>To search for relevant data related to a given query, we can do the following: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png" width="1456" height="508" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:508,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:139074,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6f6000b-3d79-4e83-b2b2-338db5fc6e36_1582x552.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/memory/pinecone.py</figcaption></figure></div><p><span>In this example, the developers of </span><a href="https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/memory/pinecone.py" rel="nofollow ugc noopener">AutoGPT</a><span> retrieve the top 5 most relevant data points from the database. This is where you can be flexible with how much data you pull from storage to add to your prompt while being aware of the token limitations. This makes monitoring token count crucial as you fetch data and construct your prompt. OpenAI has developed a </span><a href="https://github.com/openai/tiktoken" rel="nofollow ugc noopener">library</a><span> that enables you to tokenize text and count the number of tokens being used:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png" width="1456" height="285" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/cc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:285,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:79818,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc2bd718-1420-4222-a9b7-a9cadea5169c_1706x334.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>https://github.com/openai/tiktoken</figcaption></figure></div><pre><code>num_tokens = len(enc.encode(string))</code></pre><p>As you are pulling relevant information for a given query, you want to make sure the token count doesn’t exceed the token limit, which includes both the initial prompt and the allocated amount for the LLM response. </p><p><span>There are some interesting techniques that might help accommodate even more data within the token limit. For example, using string </span><a href="https://twitter.com/itakgol/status/1651600650201882624?s=46&amp;t=-Jsrqha4Go9C-bbUgyBpow" rel="nofollow ugc noopener">compression</a><span>. However, my own testing of this approach was inconsistent to get back the exact text. </span></p><p>Having converted our data into embeddings and stored it in a vector-based database, we are now ready to query it. Constructing the prompt is where you have even more flexibility depending on your specific use case. Here’s a simple example: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png" width="1456" height="239" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:239,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:99353,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ce0c583-4230-4e52-8666-4efc9f7bd5b2_2602x428.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>https://github.com/hwchase17/langchain/blob/master/langchain/chains/chat_vector_db/prompts.py</figcaption></figure></div><p><span>Creating this prompt can be as straightforward as providing instructions to utilize the given </span><strong>context</strong><span> (where you inject the relevant text search results) to answer the user&#39;s </span><strong>question</strong><span> (where you inject the user&#39;s question itself). With GPT’s chat completion API, you can make the direction and context the system prompt and the question a user message. </span></p><p>There is an important part of this prompt that is partially cut off from the image:</p><blockquote><p>“If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer”</p></blockquote><p>This statement helps mitigate hallucinations and prevents LLMs from making up answers when the necessary data isn&#39;t explicitly provided in the context.</p><p>In the examples mentioned above, we focused on static data that is pre-processed and stored prior to initiating a conversation with the LLM. But what if we want to fetch data in real-time and allow the LLM to reference it for answering questions? This is where we get into the world of autonomous agents and giving LLMs the ability to execute commands like searching the web. For a brief overview on how autonomous agents work you can check out the previous article: </p><div><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 1300w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1300,h_650,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png" sizes="100vw" alt="Technical Dive Into AutoGPT" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F13ca47e7-5519-4cec-b729-cf4910a0e089_512x512.png 1300w" width="1300" height="650"/></picture></div><p>If you haven&#39;t heard about the open-source project Auto-GPT, then definitely check it out before continuing. Auto-GPT uses various techniques to make GPT autonomous in completing tasks centered around a specific goal. The project also provides GPT with a list of executable commands that help it make actionable progress towards the overall objective.</p></div></div><p>By following the steps outlined above, we can collect data, generate embeddings, store them in a vector database, search for n relevant items, and provide our AI with knowledge. However, there will still be use cases where this isn’t enough, and the data that needs to be analyzed exceeds the token limit: such as attempting to inject decades&#39; worth of stock data all at once. As information continuously evolves, we can expect ongoing improvements and creative solutions to be developed.</p><p data-attrs="{&#34;url&#34;:&#34;https://sudoapps.substack.com/p/giving-gpt-infinite-knowledge?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;,&#34;action&#34;:null,&#34;class&#34;:null}"><a href="https://sudoapps.substack.com/p/giving-gpt-infinite-knowledge?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="nofollow ugc noopener"><span>Share</span></a></p></div></div></div></article></div></div></div>
  </body>
</html>
