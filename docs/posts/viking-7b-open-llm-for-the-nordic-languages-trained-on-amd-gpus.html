<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.silo.ai//blog/viking-7b-the-first-open-llm-for-the-nordic-languages">Original</a>
    <h1>Viking 7B: open LLM for the Nordic languages trained on AMD GPUs</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>Together with University of Turku’s research group TurkuNLP and HPLT, Europe’s largest private AI lab Silo AI is releasing the first multilingual large language model (LLM) for all Nordic languages. Viking 7B is a best-in-class open source model that is sensitive to local values and cultures and further evidence of the team’s novel approach to training capable LLMs for low-resource languages. It is a significant milestone on the journey towards a state-of-the-art LLM family for all European languages.</strong></p><p>Following the completion of the language model <a href="https://www.silo.ai/blog/europes-open-language-model-poro-a-milestone-for-european-ai-and-low-resource-languages">Poro</a>, and the first checkpoint release of <a href="https://www.silo.ai/blog/viking-7b-13b-33b-sailing-the-nordic-seas-of-multilinguality">Viking</a>, Silo AI and TurkuNLP of University of Turku are now releasing the full 7B parameter version of Viking. At the same time, we are also releasing further checkpoints for the Viking 13B and Viking 33B models. In addition to the Nordic languages, Viking also covers English and programming languages. Evaluations indicate best-in-class performance in all Nordic languages, without compromising performance in English. </p><p>Viking relies on the same training approach as Poro, focusing on low-resource languages without compromising English, but extends to include Danish, Finnish, Norwegian, Icelandic, Swedish and programming languages. And the model family comes with an updated architecture and in a variety of model sizes. </p><p>Building on Silo AI&#39;s strategy to promote democratic access to LLMs and promote linguistic diversity across Europe, the collaboration with TurkuNLP utilizes the latest advancements in multilingual LLMs. Unlike most other LLMs, we&#39;re prioritizing low-resource language performance, rather than relegating them to an afterthought. The team has worked on determining optimal training approaches and architectures to support this. This covers optimal model architecture for pre-training, as well as other approaches to training and data sampling like data reuse frequencies for low-resource languages during training and incorporating translated paired texts between high- and low-resource languages. Several of these strategies rely on a cross-lingual signal to enhance the model&#39;s understanding of the connections between languages, proving crucial in achieving superior performance for low-resource languages, without compromising performance in English.</p><p>Silo AI and TurkuNLP are dedicated to developing models that not only excel in linguistic performance and inclusivity but are also attuned to local values and cultures. Such sensitivity ensures that these technological advancements serve as connectors, rather than dividers, in digital communication. It enhances Europe’s digital infrastructure, thereby accelerating the adoption of LLM-driven products and applications. This, in turn, fosters innovation across sectors and use cases throughout Europe, bolstering the continent&#39;s technological ecosystem.</p><p>Further emphasizing digital sovereignty, Viking is trained on the EuroHPC supercomputer LUMI, utilizing up to 4096 AMD MI-250X GPUs. LUMI is not only Europe’s most powerful supercomputer and the 5th most powerful in the world, but also the 3rd greenest supercomputer among the top 500 supercomputers. LUMI’s energy consumption is covered with power produced 100% with hydroelectricity, and the waste heat of LUMI will account for about 20 percent of the district heating in the surrounding city of Kajaani. </p><p>With a purpose-built software layer to train models on AMD, Silo AI and TurkuNLP possess unmatched experience with training on AMD at scale, having <a href="https://lumi-supercomputer.eu/scaling-the-pre-training-of-large-language-models-of-100b-parameters-to-thousands-of-amd-mi250x-gpus-on-lumi/">shown that their</a> theoretical predictions for throughput scaling materialize in weak and strong scaling experiments. As one of the seminal initiatives on AMD GPUs, this shows how it’s possible to achieve good throughput on the AMD-based LUMI, training the models with their open source training framework and utilizing up to 4096 MI-250X GPUs simultaneously.</p><h2>Viking 7B completed and checkpoint performance</h2><p>Today, the Viking models stand at 100% of training on Viking 7B, 85% on 13B and 65% on 33B. With common benchmarks, we can observe evidence of outperformance with respect to other open models (e.g. Falcon, GPT-SW3, Llama, Mistral, MPT, etc). Results indicate best-in-class performance in low-resource languages vis-à-vis other open models, without compromising performance in English and programming languages. In our latest evaluations, Viking is benchmarked on a large number of relevant measures, including translated tests, MMLU, Arc-C, HellaSwag etc. While translated tests are commonly used (e.g. to prove multilinguality of Mistral Large) and provide indicative evidence, they don&#39;t fully capture the multilingual reasoning capabilities of language models. Another measure, perplexity, further corroborates Viking’s performance. Overall, Viking not only showcases its adeptness at understanding and generating Nordic languages but also highlights its efficiency in processing and predicting linguistic sequences. This dual advantage indicates the viability of the approach to train multilingual models, and Viking&#39;s technological edge in navigating the complexities of multilinguality.</p><h2>Viking 7B/13B/33B: A modern architecture with more languages</h2><p>Below is a summary of key features of the Viking model family covering English, Finnish, Swedish, Norwegian, Danish, Icelandic and code:</p><p>‍</p><ul role="list"><li><strong>Research Checkpoints: </strong>Silo AI and TurkuNLP are committed to publishing checkpoints throughout the training process, providing transparency on the model training process.</li><li><strong>Model architecture:</strong> Viking uses an architecture similar to Llama 2, with flash attention, rotary embeddings, grouped query attention and supports a 4k sequence length</li><li><strong>Model sizes: </strong>7B, 13B and 33B parameters</li><li><strong>Multilingual capabilities: </strong>The models are designed to process English and Nordic languages, and have proficiency with a variety of programming languages. Additionally, they can perform basic translation between English and Nordic languages.</li><li><strong>Dataset:</strong> The model family is trained with a dataset of 2 trillion tokens, including Danish, English, Finnish, Icelandic, Norwegian, Swedish and a variety of programming languages.</li><li><strong>Open source: </strong>The model family is freely available under the Apache 2.0 License, implying applicability for both commercial and research use.</li><li><strong>Training hardware: </strong>Our models are trained using the LUMI supercomputer in Finland, covering up to 4096 AMD MI250X GPUs.</li></ul><h3>Considerations for Use</h3><p>The intended audience for Poro Research Checkpoints is academic and industry research. These checkpoints are not suitable for deployment in a production use case without further training, fine-tuning and testing. For more on Silo AI&#39;s SaaS-based custom LLMs we invite you to familiarize yourself with the <a href="https://www.silo.ai/silogen">SiloGen platform</a>.</p><h3>Acknowledgments</h3><p>We wish to thank the operators of the <a href="https://www.lumi-supercomputer.eu/">LUMI/EuroHPC</a> supercomputer for computational resources and technical support, including AMD, HPE and CSC – the IT Center for Science, Finland. TurkuNLP researchers have received funding from the European Union’s Horizon Europe research and innovation programme High Performance Language Technologies (HPLT) under grant agreement No 101070350.</p><p>‍</p></div><div><h2>Want to discuss how Silo AI could help your organization?</h2><p>Get in touch with our AI experts.</p><div><p><img loading="lazy" alt="" src="https://assets-global.website-files.com/63beb9b135d26ec169729fa2/63f603ac4a61da58b147991a_peter-sarlin.jpg"/></p><div><p>Peter Sarlin, PhD</p><p>CEO &amp; Co-Founder</p></div></div></div></div>
  </body>
</html>
