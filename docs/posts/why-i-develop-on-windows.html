<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.shortround.dev/why-i-think-windows-is-the-best-dev-platform/">Original</a>
    <h1>Why I develop on Windows</h1>
    
    <div id="readability-page-1" class="page"><div>
            <h2 id="origin-story">Origin Story</h2><p>In college, I had a weird setup: I got the cheapest Chromebook I could afford (I believe it was an ASUS C201 for $150 at the time). I rented a $5/month Ubuntu VPS from Vultr with a couple hundred MB of RAM and coded everything in vim via the SSH app provided by Google on the Chrome appstore.</p><p>I mostly did C development when my professors allowed us to choose a language, and when Java was required, we usually didn&#39;t have to turn in more than a single .java file, so I was free to use whatever build system I wanted. I was inexperienced in build systems back then, so I used Make (today, I would highly recommend anyone in a similar boat use either Maven or Gradle) and turned in the .java source code, which would be automatically compiled and run by whatever grading program my CS professors used.</p><p>When I got to my OS and Systems programming class, I really shined. I had already been developing in C for a few years, and was very comfortable with a bash terminal. We turned in all our project files by SSH&#39;ing into a server that was provided by the CS department so we could verify the output of our programs on their Ubuntu server. I learned a lot about linking and used GDB for the first time there.</p><p>When I dropped out of college for my first job, I had some money to buy a new PC, and I bought a used Thinkpad T420 (later upgraded to a T460). I stuck with Windows for a bit because I was interested in amateur game engine development and was working with SDL2 in C++ with Visual Studio. I could have done the same work on Linux, but I wanted to make Windows my target platform since it had more gamers on it. I have not gone back to Linux since then.</p><h2 id="not-just-a-bash-terminal">Not just a bash terminal</h2><p>A lot of these &#34;Why I switched to Windows&#34; blog posts all say the same thing: &#34;WSL2 made it easy for me to go to Windows without losing all my favorite CLI tools&#34;, and that&#39;s a good argument: WSL2 provides a better CLI environment than macOS. I find that for many people who like to use macOS as a development platform, the core reason really boils down to the <code>bash</code> (now <code>zsh</code>) terminal, but macOS&#39;s programming environment is really one of its weakest features, in my opinion.</p><p>At my first job, I found that bash scripts from StackOverflow didn&#39;t work on the company provided MacBook Pro because Apple had not (and still has not, as far as I know) updated their bash implementation since 2007 (11 years old at that point). Try it: run <code>bash --version</code> in macOS and see what year it puts out. Modern language features didn&#39;t work, and so the first thing I do when a company makes me use a MacBook is upgrade the bash binaries to those provided by GNU via homebrew. Similarly, the core utils provided by macOS don&#39;t have GNU extensions, like the extended regex support in <code>grep</code> and <code>sed</code>. So, already macOS is shipping a programming environment inferior to literally any Linux distribution compiled in the last 10 years. Those core utils can also be upgraded via brew, but I see why WSL2 makes Windows so much more ergonomic for people to use than macOS.</p><h2 id="powershell">Powershell</h2><p>However, I want to point out some features of Windows as a development environment that I think offer a lot more to a developer than just a glorified bash terminal. In fact, to be honest: I hate bash. The syntax makes no sense to me. People put out bash scripts that just look like a foreign language. I don&#39;t think I&#39;m alone in this regard, because I know a lot of developers who will opt to do all of their scripting in python these days, even putting <code>#!/bin/python3</code> at the head of a script so that it runs through the shell. I am not joking around when I say that I think PowerShell is a legitimate competitor to bash.</p><p>A lot of people coming from the Unix-like world of macOS and Linux don&#39;t tend to know a lot about PowerShell other than associating it with Windows Server administration. Many people don&#39;t know it even exists at all. When I mention the Windows Terminal to people, they think I&#39;m talking about the Windows Command Prompt, a crappy little program whose limited and arcane syntax has annoyed developers for a couple decades now.</p><p>PowerShell distinguishes itself form the Unix-like world by manipulating structured data instead of streams of bytes (usually in the form of text). Whereas a program on Linux might output some data in JSON format, which you might pipe in <code>jq</code>, run some queries on, and then pipe into <code>sed</code> or something, before finally dumping the payload into your target program, e.g:</p><pre><code># Gets some instances, grabs their names, pipes them into another program
aws ec2 get-instance |  jq &#39;.[].name&#39; -r | myprogram</code></pre><p>PowerShell allows you to treat JSON data as structured dictionaries, so you have the full scripting environment at your disposal and not just the query language provided by <code>jq</code> or similar utilities (you see this a lot in the bash environment: to avoid having to actually use bash on the result of some program output, programs end up providing their own entire programming environment, such as with <code>awk</code>). A similar query in PowerShell would be roughly the same:</p><pre><code>aws ec2 get-instance | ConvertFrom-Json | ForEach-Object { $_.name } | myprogram</code></pre><p>Is this more verbose? Definitely (thought you can alias some of these, if you want.) But notice that the query language is not confined to a single section of the pipeline: PowerShell IS the query language itself, and the output of <code>ForEach-Object</code> isn&#39;t a newline separated list of strings (a stream of bytes with the newline character delimiting values), it&#39;s an <em>array </em>of strings. This means that you can write cmdlets for PowerShell which have well defined parameter inputs (e.g: the variable must be an array of strings), and you also <em>don&#39;t have to do any input parsing</em>. C# cmdlets work natively with PowerShell, and so input parsing and validation is handled by the framework. This is a big deal for me, because I&#39;ve had to write command line utilities in Java, and there are simply no <em>good </em>command line input parsing libraries for Java. For C/C++, there are conventions, but it&#39;s still a diverse field.</p><p>It took me a while to get the hang of, but so did bash, initially. I now do all my scripting with PowerShell. I use <code>Invoke-WebRequest</code> instead of <code>curl</code> (thought I do like Postman), <code>Select-String</code> instead of <code>grep</code>, and sed becomes irrelevant because PowerShell&#39;s String type has built-in sed-like capabilities, e.g:</p><pre><code>&gt; $xyz = &#34;Hello world!&#34;
&gt; $xyz = $xyz -replace &#34;[aeiou]&#34;, &#34;_&#34;
&gt; echo $xyz

H_ll_ w_rld!</code></pre><p>It probably won&#39;t interest most people, but PowerShell also runs on Unix-like systems through <a href="https://devblogs.microsoft.com/powershell/powershell-core-6-0-generally-available-ga-and-supported">PowerShell Core</a>.</p><h2 id="terminal">Terminal</h2><p>Speaking of shells, a lot of people think that the Windows Terminal looks like the old, crappy batch command prompt terminal. It doesn&#39;t. I can&#39;t even post a screenshot here because Windows 11 won&#39;t open the old one anymore. The <a href="https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701">Windows Terminal</a> looks like this:</p><figure><img src="https://blog.shortround.dev/content/images/2022/10/image.png" alt="" loading="lazy" width="1113" height="626" srcset="https://blog.shortround.dev/content/images/size/w600/2022/10/image.png 600w, https://blog.shortround.dev/content/images/size/w1000/2022/10/image.png 1000w, https://blog.shortround.dev/content/images/2022/10/image.png 1113w" sizes="(min-width: 720px) 720px"/><figcaption>Blue squares are my name removed by me</figcaption></figure><p>It has full color support, tabs, color schemes, and allows you to open WSL, PowerShell, Command Prompt, or whatever other shells you want to use. You can even install custom HLSL Shaders on it and get cool effects like the <a href="https://github.com/Hammster/windows-terminal-shaders">CRT shader</a>:</p><figure><img src="https://blog.shortround.dev/content/images/2022/10/image-1.png" alt="" loading="lazy" width="1113" height="626" srcset="https://blog.shortround.dev/content/images/size/w600/2022/10/image-1.png 600w, https://blog.shortround.dev/content/images/size/w1000/2022/10/image-1.png 1000w, https://blog.shortround.dev/content/images/2022/10/image-1.png 1113w" sizes="(min-width: 720px) 720px"/><figcaption>Excuse my messy home folder</figcaption></figure><p>The other thing people seem to think doesn&#39;t exist in Windows is the Path environment variable. There is, in fact, a path environment variable, and always has been. It&#39;s simply the case that Windows users don&#39;t tend to launch software from the Terminal, and so installers don&#39;t usually add themselves to the Path (though some will ask). Programs also usually go under a tree of folders in the Program Files folder, with their own relative bin/ folders. You can add your own Paths to the Path variable through powershell:</p><pre><code>$Env:PATH = $Env:PATH + &#34;;C:\MyPath&#34;</code></pre><p>Or through the System environment variables dialog:</p><figure><img src="https://blog.shortround.dev/content/images/2022/10/image-5.png" alt="" loading="lazy" width="357" height="270"/></figure><p>I often see people set their environment variables before running a program in bash, like:</p><pre><code>MYSQL_HOST=MyHost.com \
MYSQL_USER=MyUser \
java -jar myProgram.jar</code></pre><p>Because PowerShell supports local block scope, you can simply do:</p><pre><code>{
    $Env:MYSQL_HOST = &#34;MyHost.com&#34;;
    $Env:MYSQL_USER = &#34;MyUser&#34;;
    java -jar myprogram.jar;
}</code></pre><p>The Environment variables will not survive local scope. You can also put them into a .ps1 PowerShell script file and they won&#39;t survive after execution of the script.</p><h2 id="cc">C/C++</h2><p>This will be a controversial one. I know of few programming communities more opinionated than the C/C++ community. I&#39;m going to be straightforward with you: I&#39;m a pedestrian when it comes to C++. I don&#39;t know a lot about the inner workings of the ABI, I don&#39;t know the arcane rules about what constitutes Undefined Behavior, and I have not read the C or C++ specifications. The C++ I write looks closer to Java than it does idiomatic C++.</p><p>That said, I like MSVC because of <a href="https://learn.microsoft.com/en-us/cpp/code-quality/understanding-sal?view=msvc-170">SAL</a>. SAL lets you put annotations on functions, variables, parameters, classes, etc. to give the compiler hints about how to statically validate input and output. For example, sometimes pointers and references are used to send data in, and sometimes they&#39;re used to send data out. Technically, your code should be <a href="https://isocpp.org/wiki/faq/const-correctness">const-correct</a> to prevent readonly pointers and references from being modified, and allowing writeable pointers and references to be written, but to be very honest with you: I do not remember what const means in every scenario. Sometimes it goes before a variable&#39;s name, sometimes before the type, sometimes after a method name. Here&#39;s an example of valid C++</p><pre><code>const std::vector&lt;const XMMATRIX&gt;&amp; const getJointAt(const int const&amp; const i)
{
    /*...*/;
}</code></pre><p>Obviously, this is a facetious example, but I could fit a few more <code>const</code>s in there. My point is that I don&#39;t really understand what the code does by looking at it because I&#39;m not a C++ guru (don&#39;t worry, I don&#39;t write it for a living).</p><p>SAL allows you to tell the compiler (and the user) what pointers are input and what are output:</p><pre><code>void doSomething(_In_ Foo* foo, _Out_ Bar* bar){
    //...
}</code></pre><p>If the user passed <code>nullptr</code> for an <code>_Out_</code> marked parameter, the compiler will give you a warning (which you can optionally treat as an error): <code>_Out_</code> variables must be writeable.</p><p>But if you&#39;re perfectly comfortable with const-correctness, then there&#39;s still features for you. Take the Try-X pattern, for example:</p><pre><code>bool tryGetValue(
    const std::map&lt;std::string, std::string&gt;&amp; map,
    const std::string key,
    std::string&amp; value //output
)
{
    const auto&amp; result = map.find(key);
    if (result == map.end())
    {
        //output is not set on false
        return false;
    }
    value = result-&gt;second;
    return true;
}</code></pre><p>For more complicated methods, you may forget to set the output variable on a successful return, and that can cause problems at runtime, since there&#39;s no requirement by the compiler that you ever write to the output.</p><p>SAL has the <code>_Success_</code> annotation for the method:</p><pre><code>_Success_(return == true) // means &#34;return value == true&#34;. Can also just do &#34;return&#34; since it uses C++ truthyness
bool tryGetValue(
    const std::map&lt;std::string, std::string&gt;&amp; map,
    const std::string key,
    _Out_ std::string&amp; value
)
{
    //...
}</code></pre><p>If you forget to set the <code>value</code> out variable for any input that results in a successful return, the compiler will warn you about it.</p><p>SAL can also validate memory reads and writes to make safer code. The C Standard library is notoriously unsafe, but MSVC uses SAL to enable those annoying compiler warnings that you always ignore. Take <code>memcpy</code> for example:</p><pre><code>void * memcpy(
   void *dest,
   void *src,
   size_t count
);</code></pre><p>The compiler cannot check if your destination buffer is big enough to hold <code>count</code> bytes, or if <code>src</code> even HAS <code>count</code> bytes. Enter SAL:</p><pre><code>void * memcpy(
   _Out_writes_bytes_all_(count) void *dest,
   _In_reads_bytes_(count) const void *src,
   size_t count
);</code></pre><p>I&#39;ve not seen this work perfectly, but SAL will do it&#39;s best to detect if you&#39;re using <code>memcpy</code> in a way that violates the rule that <code>src</code> and <code>dst</code> must both have at least <code>count</code> bytes. Usually, this means that you need to manually check the length of your buffers in some way before (though, just use <code>memcpy_s</code>, really). It will also validate that the body of the function doesn&#39;t write MORE than <code>count</code> bytes.</p><h2 id="package-managers-in-general-vcpkg-in-particular">Package Managers in general, vcpkg in particular</h2><p>macOS doesn&#39;t have a package manager, by default. There&#39;s <code>brew</code>, which I have not had good experiences with. Sometimes it manages to take a few minutes to update when I run it. No hate to the maintainers, but it just doesn&#39;t compare to Debian&#39;s aptitude.</p><p>Windows, however, has a few package managers. <a href="https://chocolatey.org/">Chocolatey</a> is the older community-driven tool. Installing things is as simple as <code>choco install [packageName]</code> (from an administrator shell). Usually it will put CLI tools into the path, and I haven&#39;t had any trouble with it, personally. Chocolatey has the biggest catalogue of software for Windows package managers</p><p>Winget is Microsoft&#39;s first party package manager, though I rarely use it. It works in the same way as chocolatey: basically as a CLI frontend for downloading and running .msi and .exe installers. Sometimes these spawn GUI installer, but it&#39;s Windows so that&#39;s what you gotta be ready for.</p><p>For C++ integration, I love vcpkg. vcpkg makes it really easy to get library dependencies, and first-party Visual Studio support means that linking them is even easier. With vcpkg, you can create a .json file in your project directory, set your project to use vcpkg and autolink dependencies, and then let er&#39; rip. x64 vs. x86 vs. arm binaries are handled automatically by Visual Studio, and you can often customize feature sets for the libraries you download. Dear ImgUI is a UI library that supports a lot of different operating systems and rendering APIs (Vulkan, OpenGL, DirectX). To download the win32 integration with DirectX rendering, just add this to your vcpkg.json file:</p><pre><code>{
    &#34;name&#34;: &#34;imgui&#34;,
    &#34;features&#34;: [ &#34;dx11-binding&#34;, &#34;win32-binding&#34; ]
}</code></pre><p>When you build your project next, it will automatically download and link. You just need to use the headers like any other library</p><h2 id="doesnt-everything-run-on-linux-these-days">Doesn&#39;t everything run on Linux these days?</h2><p>A lot of web developers work on projects that are destined to run in a docker container on Linux. So, you might ask, shouldn&#39;t I develop on a Unix-like system, or just test in Linux/Docker?</p><p>If the software is written well, it will be sufficiently cross-platform and modular enough that you can save your resources and run in a host environment when developing.</p><p>I worked for a company once which developed everything on MacBook Pros. The software was an Apache Tomcat .war file which ran on an Ubuntu server. We tested everything via a virtual machine using vagrant. We also developed an in-house tool in Node.js which generated some frontend files for us. That tool ran only on MacOS and Linux for one simple reason: when generating paths, the code split path strings with a hardcoded <code>/</code> character, rather than a <code>System.delimiter</code> variable. There is no good reason that Java or Node.js code should be OS-dependent, unless you&#39;re explicitly interacting with specific OS processes or underlyng Syscalls. That&#39;s <em>the entire point of Java.</em></p><p>We would have clients who used our development platform to customize their product. Sometimes those clients had PCs at work, and since the software didn&#39;t run on their machines, the company&#39;s solution was to <strong>buy MacBooks for our customers</strong>. There is no good technological reason why the software couldn&#39;t run on Windows, the company simply had a shitty, elitist attitude about macOS vs. Windows development, and that pride cost them thousands of dollars a year.</p><p>Relying on vagrant and VMs for debugging a web application meant that our machines would slow to a crawl if we had more than one instance of the application running at a time.</p><p>But here&#39;s where WSL is a pro: docker runs through WSL2 on Windows, and WSL2 takes advantage of hyper-v memory ballooning, so when WSL runs out of memory, it simply gets more. Simple idea, right? I&#39;ve run into this issue with macOS countless times: you have to set a hardcoded amount of memory for docker to use in its VM, and if you run out, your container kills itself. Then you have to allocate a <em>little </em>more memory (since you don&#39;t want to give half of your $1,300 MacBook Pro&#39;s measly 8gb of RAM to docker) and hope for a better outcome. Docker is a better experience in WSL than macOS.</p><h2 id="directx">DirectX</h2><p>This is niche to me, but I really like DirectX. Perhaps it&#39;s unfair to compare DirectX 11 to OpenGL (perhaps Vulkan would be a better comparison), but that&#39;s where I came from originally, so that&#39;s what I&#39;ll compare it to.</p><p>In OpenGL, everything is a GLuint. Creating resources and storing them returns simple resource IDs as integers, which you then need to keep somewhere and give a really good name so that you remember what this particular GLuint does, and where it&#39;s supposed to go. DirectX, in Microsoft tradition, has types and enums and structs for frickin <em>everything.</em> A rasterizer state is an <code>ID3D11RasterizerState</code>. The data to create an <code>ID3D11RasterizerState</code> is wrapped in an envelope called a <code>D3D11_RASTERIZER_DESC</code>. You can wrap your Rasterizer in a ComPtr, which is Microsoft&#39;s version of a smart pointer. Enabling your rasterizer state is a method on your <code>ID3D11DeviceContext</code> called <code>RSSetState</code>, as in</p><p><code>deviceContext-&gt;RSSetState(m_shadowMapRasterizer.Get())</code></p><p>where the RS stands for <a href="https://learn.microsoft.com/en-us/windows/win32/direct3d11/d3d10-graphics-programming-guide-rasterizer-stage">Rasterizer Stage</a>, to let the developer know what stage of the DirectX 11 pipeline they&#39;re binding resources to (along with <code>PSSetShaderResources</code> for setting resources in the <a href="https://learn.microsoft.com/en-us/windows/win32/direct3d11/pixel-shader-stage">Pixel Shader stage</a>, <code>OMSetRenderTargets</code> for setting the texture render target in the the <a href="https://learn.microsoft.com/en-us/windows/win32/direct3d11/d3d10-graphics-programming-guide-output-merger-stage">Output-Merger stage</a>, or <code>IASetIndexBuffers</code> for setting the index buffer in the <a href="https://learn.microsoft.com/en-us/windows/win32/direct3d11/d3d10-graphics-programming-guide-input-assembler-stage">Input-Assembler stage</a>. These functions also all have their own types. You cannot pass a <code>ShaderResourceView</code> to <code>OMSetRenderTargets</code> (though you can create a <code>RenderTargetView</code> of the same texture as the <code>ShaderResourceView</code> and use that! They refer to the same underlying texture resource.)</p><p>In addition to the strong typing, state is a little more explicitly managed in DirectX than in OpenGL. In OpenGL, to set blend modes, you might set it as part of OpenGL&#39;s global state, like </p><pre><code>glEnable(GL_BLEND);
glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);  </code></pre><p>In DirectX, you would create a <code>D3D11_BLEND_DESC</code> struct, fill it with the options you want, use that to create an <code>ID3D11BlendState</code>, keep a ComPtr to that, and bind it with <code>deviceContext-&gt;OMSetBlendState</code>. Samplers, likewise, have their own set of description structures and states which are bound to the Pixel Shader stage with the <code>PSSetShaderResources</code>.</p><p>Honestly, when I tell people this, a lot of them recoil at the verbosity. Many people prefer dynamic typing and implicitly managed state. It&#39;s easier for them, they iterate faster, and they don&#39;t give a shit about what stage of the rendering pipeline they&#39;ve bound a resource to, or if they&#39;ve properly instantiated a ShaderResourceView for the texture that&#39;s also serving as a RenderTarget. They <em>just don&#39;t care</em>. And that&#39;s ok! I get it. Personally, however, I like when state is explicit and I can detect the underlying machinations of the render pipeline and global state by the names of the functions and types.</p><p>These types are not perfect. For example, you use a <code>D3D11_SAMPLER_DESC</code> to instantiate both a <code>SamplerState</code> and a <code>SamplerComparisonState</code> object. I had been passing <code>D3D11_FILTER_MIN_MAG_LINEAR_MIP_POINT</code> to my Shadow map&#39;s texture filtering function when I had meant to pass <code>D3D11_FILTER_COMPARISON_MIN_MAG_LINEAR_MIP_POINT</code> (note &#34;Comparison&#34;). This is a totally valid way to instantiate a SamplerState, but when you try to <em>bind it </em>as a SamplerComparisonState at runtime, the GPU simply doesn&#39;t know what the hell I&#39;m talking about. As a result, PCF shadow filtering doesn&#39;t work. I was stuck on this problem for an embarrasingly long time.</p><h2 id="directx-more">DirectX (more)</h2><p>Another cool feature of DirectX is its integration with Visual Studio. Usually, I see tutorials telling people to hard code their shaders as strings in their programs, then compile at runtime. That&#39;s okay for tutorials, but if your program gets any more complicated, you&#39;ll want to start #including headers. If you compile your shaders at runtime, you&#39;ll have to create an implementation of <code>ID3DInclude</code> so that the compiler knows where to find the file you&#39;re telling it to include.</p><p>Instead, you can set your HLSL files up to be compiled at runtime. If you add them to your resources filter, you can go to each file&#39;s properties and mark them as Pixel Shader files, Vertex Shader files, etc., and set their entrypoint names ( <code>main</code>, <code>vertex</code>, etc.)</p><figure><img src="https://blog.shortround.dev/content/images/2022/10/image-7.png" alt="" loading="lazy" width="786" height="544" srcset="https://blog.shortround.dev/content/images/size/w600/2022/10/image-7.png 600w, https://blog.shortround.dev/content/images/2022/10/image-7.png 786w" sizes="(min-width: 720px) 720px"/></figure><p>These files will be compiled to .cso files in your output directory, where you can then just load as regular binary files and link them to the GPU with<code><a href="https://learn.microsoft.com/en-us/windows/win32/api/d3d11/nf-d3d11-id3d11device-createvertexshader">CreateVertexShader</a></code> or <code><a href="https://learn.microsoft.com/en-us/windows/win32/api/d3d11/nf-d3d11-id3d11device-createpixelshader">CreatePixelShader</a></code>. This feature has made HLSL development a lot faster for me.</p><h2 id="directxtk-and-directxmath">DirectXTK and DirectXMath</h2><p>Whereas OpenGL relies on community support for features like input handling, model loading, and SIMD operations, Microsoft has provided tools called <a href="https://github.com/Microsoft/DirectXTK/wiki/DirectXTK">DirectXTK</a> and <a href="https://learn.microsoft.com/en-us/windows/win32/dxmath/directxmath-portal">DirectXMath</a> as first party solutions to these problems.</p><p>DirectXTK has features like <a href="https://github.com/microsoft/DirectXTK/wiki/Mouse">Mouse Input</a>, so that you don&#39;t have to write much of your own win32 event handlers and logic. Just integrate the mouse library into your existing top level event handler:</p><pre><code>LRESULT CALLBACK messageHandler(HWND hwnd, UINT umsg, WPARAM wparam, LPARAM lparam)
{
	switch (umsg)
	{
	case WM_KEYDOWN:
		onKeyDown((UINT)wparam);
		return 0;
	case WM_KEYUP:
		onKeyUp((UINT)wparam);
		return 0;
	case WM_ACTIVATEAPP:
	case WM_INPUT:
	case WM_MOUSEMOVE:
	case WM_LBUTTONDOWN:
	case WM_LBUTTONUP:
	case WM_RBUTTONDOWN:
	case WM_RBUTTONUP:
	case WM_MBUTTONDOWN:
	case WM_MBUTTONUP:
	case WM_MOUSEWHEEL:
	case WM_XBUTTONDOWN:
	case WM_XBUTTONUP:
	case WM_MOUSEHOVER:
        // DirectXTK wndproc function handles all the mouse events
        // then you just access them through the Mouse class
		Mouse::ProcessMessage(umsg, wparam, lparam);
	default:
		return DefWindowProc(hwnd, umsg, wparam, lparam);
	}
}
</code></pre><p>There&#39;s also a texture loading library, <a href="https://github.com/microsoft/DirectXTK/wiki/WICTextureLoader">WICTextureLoader</a>, which handles all kinds of file formats, as an alternative to <a href="https://github.com/nothings/stb/blob/master/stb_image.h">stb_image</a> (did you know that guy develops everything in his library suite with Visual C++ 6?)</p><p>There are other libraries that all seem pretty cool, but I have yet to use them. I suggest checking them out</p><p>DirectXMath is the SIMD library for DirectX. It&#39;s the library that refuses to let you just do operations on your regular XMFLOAT3 vectors, instead forcing your to load and store them into XMVECTORS with XMLoadFloat3 and XMStoreFloat3 first (since the XMVECTOR is just a wrapper for the <code>__m128</code> SIMD <a href="https://learn.microsoft.com/en-us/cpp/cpp/m128?view=msvc-170">intrinsic</a>). In that regard, glm is a bit more ergonomic.</p><p>DirectXMath has all the features you would expect from a Graphics-focused SIMD library: matrix multiplication, <a href="https://learn.microsoft.com/en-us/windows/win32/api/directxmath/nf-directxmath-xmmatrixperspectivefovlh">Perspective</a>/<a href="https://learn.microsoft.com/en-us/windows/win32/api/directxmath/nf-directxmath-xmmatrixorthographiclh">Orthographic</a> Matrix creation, <a href="https://learn.microsoft.com/en-us/windows/win32/api/directxmath/nf-directxmath-xmmatrixaffinetransformation">transformation</a> matrix creation, and lerping for quaternions. My only wish is that the DirectXTK libraries used the DirectXMath vectors so that I didn&#39;t have to convert them when deserializing model files</p><p>I like Windows development for more than just WSL2 support: Windows is a good development platform <em>in-itself</em>, and Microsoft likes to support its community. PowerShell, Terminal, MSVC, Visual Studio, DirectX, and the first party integrations between each of these make Windows my personal favorite software development platform</p>
        </div></div>
  </body>
</html>
