<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.meta.com/research/publications/imagine-yourself-tuning-free-personalized-image-generation/">Original</a>
    <h1>Tuning-Free Personalized Image Generation</h1>
    
    <div id="readability-page-1" class="page"><div><h2>Abstract</h2><p>Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, e.g., changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the modelâ€™s SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models.</p><a role="button" href="https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/452604312_1010783860585222_5768506504946476980_n.pdf?_nc_cat=102&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=YAv_wnbaeV4Q7kNvgHfve1m&amp;_nc_ht=scontent-sjc3-1.xx&amp;oh=00_AYABkB1dPFCZL-rIMoBWC6Ie_QqeN5PMMF-IlxbZtGd8eA&amp;oe=66A8C02B" data-ms="{&#34;creative&#34;:&#34;button&#34;,&#34;creative_detail&#34;:&#34;button&#34;,&#34;create_type&#34;:&#34;button&#34;,&#34;create_type_detail&#34;:&#34;button&#34;}" target="_blank" data-lnfb-mode="ie"><p>Download the Paper</p></a></div></div>
  </body>
</html>
