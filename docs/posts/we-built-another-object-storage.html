<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fractalbits.com/blog/why-we-built-another-object-storage/">Original</a>
    <h1>We built another object storage</h1>
    
    <div id="readability-page-1" class="page"><div>  <h2 id="a-crowded-market-but-an-unsolved-problem">A Crowded Market, But An Unsolved Problem</h2>
<p>Object storage is the backbone of modern data infrastructure. AWS S3, Google Cloud Storage, MinIO, Ceph, newer players like Tigris Data—the market is saturated. So why build another one?</p>
<p>Because the fundamental assumptions behind these systems are shifting. High performance is no longer optional—but having high performance available isn’t the same as being able to afford using it.</p>
<h2 id="beyond-cold-storage-why-performance-matters-now">Beyond “Cold Storage”: Why Performance Matters Now</h2>
<p>Traditional object storage had a clear priority order: cost first, performance later. This worked fine for archiving backups and storing large, rarely accessed files.</p>
<p>But today, object storage is increasingly the primary data layer for AI, analytics, and cloud-native applications. Latency directly translates to compute costs—stalled GPUs waiting on I/O are expensive GPUs doing nothing.</p>
<p>High-performance object storage exists now. S3 Express One Zone, for example, delivers single-digit millisecond latency. But there’s a catch: the per-request pricing makes it prohibitively expensive to actually <em>use</em> at high IOPS. As one analysis put it, it’s “the right technology, at the right time with the wrong price” [1]. You have the performance on paper, but you can’t afford to run your workload at full speed. That’s the high-performance trap.</p>
<h2 id="the-new-challenge-ai-and-analytical-workloads">The New Challenge: AI and Analytical Workloads</h2>
<p>Modern workloads, especially in AI, impose demands that strain traditional designs:</p>
<p><strong>Small Objects at Scale</strong>: AI training datasets often consist of millions of small files (images, text snippets, feature vectors). A study of typical AI training workloads found over 60% of objects are 512KB or smaller [2]. This shifts the bottleneck from bandwidth to metadata performance.</p>
<p><strong>Latency Sensitivity</strong>: Training loops and inference pipelines are bottlenecked by I/O. When fetching thousands of small objects per batch, per-object latency compounds quickly, stalling expensive GPUs.</p>
<p><strong>The Need for Directories</strong>: S3’s flat namespace is a mismatch for many workflows. Data scientists expect atomic renames and efficient directory listings—operations that are either slow or missing in classic object stores.</p>
<h3 id="why-not-just-use-a-filesystem">”Why Not Just Use a Filesystem?”</h3>
<p>A reasonable question: if you want directories and atomic rename, why not just use a filesystem like AWS EFS? Object stores and filesystems are different concepts—why blur the line?</p>
<p>The answer is that the line is <em>already</em> blurring, driven by real workload demands. AWS themselves recognized this when they introduced S3 Express One Zone with explicit “directory bucket” semantics and atomic rename support (currently single-object) [3]. Google Cloud has made similar moves toward hierarchical namespace support [4]. The industry is converging on this because the clean separation between “object storage for scale” and “filesystem for semantics” doesn’t match how modern applications actually work.</p>
<p>We’re not trying to build a POSIX filesystem. But the subset of filesystem semantics that matter for data workflows—efficient directory listings, atomic rename for safe data handoffs—these belong in object storage. The alternative is forcing every application to build fragile workarounds on top of a flat namespace.</p>
<h2 id="where-current-solutions-hit-a-wall">Where Current Solutions Hit a Wall</h2>
<p>Existing systems struggle with these patterns in predictable ways:</p>
<p><strong>The High-Performance Trap</strong>: High-performance tiers like S3 Express One Zone solve the latency problem, but the per-request cost means you can’t actually use that performance at scale. At 10K PUT/s, you’re looking at ~$29K/month in request fees alone. The performance is there; the economics aren’t.</p>
<p><strong>The Small Object Tax</strong>: With cloud object storage, you pay per request. Storing billions of 4KB objects means your API request costs can exceed your storage costs. The more objects you have, the worse it gets.</p>
<p><strong>Missing Directory Semantics</strong>: The lack of atomic rename forces complex workarounds in applications, limiting what you can build directly on object storage. Most systems with rename support rely on inode-like structures that struggle with scalability and performance—adding to the per-IOPS cost burden.</p>
<h2 id="introducing-fractalbits">Introducing FractalBits</h2>
<p>We built FractalBits to break out of the high-performance trap: delivering performance you can actually afford to use at scale. In our benchmarks, we achieved nearly 1M GET/s on 4KB objects with a cluster totaling 64 cores across all data and metadata nodes.</p>
<p>Our focus:</p>
<ol>
<li>High IOPS at a cost that makes sense—so you can actually run your workload at full speed.</li>
<li>Native directory semantics, including atomic rename.</li>
<li>Strong consistency—no eventual consistency surprises.</li>
</ol>
<p><strong>The Cost Difference</strong></p>
<p>Here’s what the gap looks like for a small-object intensive workload (4KB objects, 10K IOPS):</p>





























<table><thead><tr><th>Metric</th><th>S3 Express One Zone</th><th>FractalBits</th><th>Reduction</th></tr></thead><tbody><tr><td>Monthly Cost for 10K PUT/s</td><td>~$29,290</td><td>~$166</td><td>~150×</td></tr><tr><td>Monthly Cost for 10K GET/s</td><td>~$778</td><td>~$42</td><td>~15×</td></tr><tr><td>Storage (1 TB Per Month)</td><td>~$110</td><td>$0 (included)</td><td>—</td></tr></tbody></table>
<p><em>S3 costs based on public pricing ($0.00113/1K PUTs, $0.00003/1K GETs, $0.11/GB/Month). FractalBits estimated using 1-year reserved instance pricing for required compute (e.g., i8g.2xlarge for data, m7g.4xlarge for metadata). Your savings will vary based on workload, but the magnitude is indicative.</em></p>

<p>At our core is a metadata engine built on an on-disk radix tree, optimized for path-like keys.</p>
<p>Most object stores use LSM-trees (good for writes, variable read latency) or B+ trees (predictable reads, write amplification). We chose a radix tree because it naturally mirrors a filesystem hierarchy:</p>
<p><strong>Prefix Sharing</strong>: Common path segments (e.g., <code>/datasets/cifar10/</code>) are stored once, saving memory and speeding up traversal.</p>
<p><strong>Efficient Directory Operations</strong>: Listing a directory becomes a subtree scan. Atomic rename is essentially updating a pointer at the branch point, not copying data.</p>
<p><strong>Crash Consistency</strong>: We use physiological logging to ensure metadata integrity and fast recovery.</p>
<p>Unlike most systems that use inode-based (or inode-like) structures to support directory features, we use a full-path approach for better scalability and performance.</p>
<p>By the way, we implemented the core engine in Zig for control and predictable performance.</p>
<ul>
<li><code>comptime</code> metaprogramming generates optimized code paths for different node types at compile time</li>
<li>Manual memory management means no GC pauses and predictable latency</li>
<li>Direct SIMD access for parallel key comparisons within tree nodes</li>
<li>io_uring in std library, so that we can easily try more recent io_uring kernel features (registered buffers, nvme IOPoll etc).</li>
</ul>
<h2 id="the-gateway-rust-based-s3-compatible-api-server">The Gateway: Rust-Based S3-Compatible API server</h2>
<p>Our S3-compatible API server, built in Rust, manages the data path:</p>
<p><strong>Safety &amp; Concurrency</strong>: Rust’s ownership model gives us thread safety without a garbage collector—important for high-concurrency request handling.</p>
<p><strong>Async I/O</strong>: Built on Tokio for handling thousands of concurrent connections.</p>
<p><strong>Production-Ready Frameworks</strong>: We support both axum and actix-web, defaulting to actix-web. Its thread-per-core architecture aligns with our design for maximum performance.</p>
<h2 id="the-model-bring-your-own-cloud-byoc">The Model: Bring Your Own Cloud (BYOC)</h2>
<p>FractalBits deploys as a managed software layer within your own cloud account (currently AWS only).</p>
<p><strong>For you:</strong></p>
<ul>
<li>Cost transparency—you pay the cloud provider’s raw costs for VMs and disks, no egress fees to us</li>
<li>Data sovereignty—your data never leaves your cloud tenant</li>
<li>Low latency—deploy in the same region/VPC as your compute</li>
</ul>
<p><strong>For us:</strong> We leverage the cloud’s proven infrastructure instead of building it from scratch, letting us focus on the storage engine itself.</p>
<h2 id="looking-ahead">Looking Ahead</h2>
<p>The object storage market has high-performance options, but the economics often make that performance unusable at scale. And systems that do offer directory semantics often struggle with performance or scalability. Getting both at a reasonable cost is still rare. We think there’s room for a different approach.</p>
<p>FractalBits is our answer. We’re early in this journey and learning from users who are pushing these limits.</p>
<hr/>
<p>Hitting the performance or cost wall with your current object storage? We’d be interested to hear about your use case.</p>
<p><a href="https://github.com/fractalbits-labs/fractalbits-main">GitHub</a></p>
<hr/>
<p><strong>References:</strong></p>
<p>[1]. S3 Express One Zone, Not Quite What I Hoped For, <a href="https://jack-vanlightly.com/blog/2023/11/29/s3-express-one-zone-not-quite-what-i-hoped-for">https://jack-vanlightly.com/blog/2023/11/29/s3-express-one-zone-not-quite-what-i-hoped-for</a></p>
<p>[2]. Mantle: Efficient Hierarchical Metadata Management for Cloud Object Storage Services. SOSP 2025.</p>
<p>[3]. Amazon S3 Express One Zone now supports renaming objects within a directory bucket, <a href="https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-s3-express-one-zone-renaming-objects-directory-bucket/">https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-s3-express-one-zone-renaming-objects-directory-bucket/</a></p>
<p>[4]. Google Cloud Storage hierarchical namespace, <a href="https://cloud.google.com/blog/products/storage-data-transfer/new-gcs-hierarchical-namespace-for-ai-and-data-lake-workloads">https://cloud.google.com/blog/products/storage-data-transfer/new-gcs-hierarchical-namespace-for-ai-and-data-lake-workloads</a></p>  </div></div>
  </body>
</html>
