<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vicuna.lmsys.org/">Original</a>
    <h1>Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality</h1>
    
    <div id="readability-page-1" class="page"><div id="content" role="main">
      <p><sup>*</sup> <em>According to a fun and non-scientific evaluation with GPT-4. Further rigorous evaluation is needed.</em></p>

<p>We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%<sup>*</sup> quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%<sup>*</sup> of cases. The cost of training Vicuna-13B is around $300. The training and serving <a href="https://github.com/lm-sys/FastChat">code</a>, along with an online <a href="https://chat.lmsys.org">demo</a>, are publicly available for non-commercial use.</p>

<p><img src="https://vicuna.lmsys.org/favicon.jpeg" alt="favicon" width="30%"/></p>

<p>Vicuna
(generated by stable diffusion 2.1)</p>

<h2 id="how-good-is-vicuna">How Good is Vicuna?</h2>
<p>We present examples of Alpaca and Vicuna responses to our benchmark questions. After fine-tuning Vicuna with 70K user-shared ChatGPT conversations, we discover that Vicuna becomes capable of generating more detailed and well-structured answers compared to Alpaca (see examples below), with the quality on par with ChatGPT.</p>

<hr/>



    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Who&#39;s GPT-4&#39;s favorite? Battles between State-of-the-Art Chatbots</title>
    
    
    
    



    


    <!-- <div class="container-fluid bg-light py-2">
        <div class="text-center">
            <small class="text-muted">This website is co-authored with <a href="https://openai.com" target="_blank">GPT-4</a>.</small>
        </div>
    </div> -->

    <!-- Marked.js -->
    
    <!-- Bootstrap and Popper.js JavaScript dependencies -->
    
    
    

    
    






<hr/>

<p>However, evaluating chatbots is never a simple task. With recent advancements in GPT-4, we are curious whether its capabilities have reached a human-like level that could enable an automated evaluation framework for benchmark generation and performance assessments. Our initial finding indicates that GPT-4 can produce highly consistent ranks and detailed assessment when comparing chatbots’ answers (see above example of GPT-4 judgment). 
Preliminary evaluations based on GPT-4, summarized in Figure 1, show that Vicuna achieves 90%<sup>*</sup> capability of Bard/ChatGPT. While this proposed framework shows a potential to automate chatbot assessment, <strong>it is not yet a rigorous approach</strong>. Building an evaluation system for chatbots remains an open question requiring further research. More details are provided in the evaluation section.</p>

<p><img src="https://vicuna.lmsys.org/assets/vicuna/chart.svg" alt="chart" width="60%"/>
Figure 1. Relative Response Quality Assessed by GPT-4<sup>*</sup></p>

<h2 id="online-demo">Online Demo</h2>
<p>Try the Vicuna-13B demo <a href="https://chat.lmsys.org">here</a>!
<!-- <iframe src="http://34.150.184.74:7860/" frameBorder="0" width="100%" height="900"></iframe> --></p>

<!-- Add a video that automatically play -->


<h2 id="overview">Overview</h2>
<p>The rapid advancement of large language models (LLMs) has revolutionized chatbot systems, resulting in unprecedented levels of intelligence as seen in OpenAI’s ChatGPT. However, despite its impressive performance, the training and architecture details of ChatGPT remain unclear, hindering research and open-source innovation in this field. Inspired by the Meta LLaMA and Stanford Alpaca project, we introduce Vicuna-13B, an open-source chatbot backed by an enhanced dataset and an easy-to-use, scalable infrastructure. By fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.com, Vicuna-13B has demonstrated competitive performance compared to other open-source models like Stanford Alpaca. This blog post provides a preliminary evaluation of Vicuna-13B’s performance and describes its training and serving infrastructure. We also invite the community to interact with our online demo to test the capabilities of this chatbot.</p>

<p><img src="https://vicuna.lmsys.org/assets/vicuna/overview.png" alt="Overview" width="70%"/>
Figure 2. Workflow Overview</p>

<p>Figure 2 provides an overview of our work. To begin, we collected around 70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations. Next, we enhanced the training scripts provided by Alpaca to better handle multi-round conversations and long sequences. The training was done with PyTorch FSDP on 8 A100 GPUs in one day. For serving the demo, we implemented a lightweight distributed serving system. We conducted a preliminary evaluation of the model quality by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. To compare two different models, we combine the outputs from each model into a single prompt for each question. The prompts are then sent to GPT-4, which assesses which model provides better responses. A detailed comparison of LLaMA, Alpaca, ChatGPT, and Vicuna is shown in Table 1 below.</p>

<p>Table 1. Comparison between several notable models</p>

<table>
<tbody>
  <tr>
    <td><span>Model Name</span></td>
    <td><span>LLaMA</span></td>
    <td><span>Alpaca</span></td>
    <td><span>Vicuna</span></td>
    <td><span>Bard/ChatGPT</span></td>
  </tr>
  <tr>
    <td>Dataset</td>
    <td>Publicly available datasets</td>
    <td>Self-instruct from davinci-003 API</td>
    <td>User-shared conversations</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>Training code</td>
    <td>N/A</td>
    <td>Available</td>
    <td>Available</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>Evaluation metrics</td>
    <td>Academic benchmark</td>
    <td>Author evaluation</td>
    <td>GPT-4 assessment</td>
    <td>Mixed</td>
  </tr>
  <tr>
    <td>Training cost</td>
    <td>82K GPU-hours</td>
    <td>$500 (data) + $100 (training)</td>
    <td>$140 (training)</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>Training cost</td>
    <td>135K GPU-hours</td>
    <td>N/A</td>
    <td>$300 (training)</td>
    <td>N/A</td>
  </tr>
</tbody>
</table>

<h2 id="training">Training</h2>
<p>Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model’s maximum context length.</p>

<p>Our training recipe builds on top of <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford’s alpaca</a> with the following improvements.</p>
<ul>
  <li><strong>Memory Optimizations:</strong> To enable Vicuna’s understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing <a href="https://arxiv.org/abs/1604.06174">gradient checkpointing</a> and <a href="https://arxiv.org/abs/2205.14135">flash attention</a>.</li>
  <li><strong>Multi-round conversations:</strong> We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot’s output.</li>
  <li><strong>Cost Reduction via Spot Instance:</strong> The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a> <a href="https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html">managed spot</a> to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.</li>
</ul>

<h2 id="serving">Serving</h2>
<p>We build a serving system that is capable of serving multiple models with distributed workers. It supports flexible plug-in of GPU workers from both on-premise clusters and the cloud. By utilizing a fault-tolerant controller and managed spot feature in SkyPilot, this serving system can work well with cheaper spot instances from multiple clouds to reduce the serving costs. It is currently a lightweight implementation and we are working on integrating more of our latest <a href="https://arxiv.org/abs/2302.11665">research</a> into it.</p>

<h2 id="how-to-evaluate-a-chatbot">How To Evaluate a Chatbot?</h2>
<p>Evaluating AI chatbots is a challenging task, as it requires examining language understanding, reasoning, and context awareness. With AI chatbots becoming more advanced, current open benchmarks may no longer suffice. For instance, the evaluation dataset used in Stanford’s Alpaca, <a href="https://github.com/yizhongw/self-instruct/tree/main/human_eval">self-instruct</a>, can be effectively answered by SOTA chatbots, making it difficult for humans to discern differences in performance. More limitations include training/test data contamination and the potentially high cost of creating new benchmarks. To tackle these issues, we propose an evaluation framework based on GPT-4 to automate chatbot performance assessment.</p>

<p>First, we devised eight question categories, such as Fermi problems, roleplay scenarios, and coding/math tasks, to test various aspects of a chatbot’s performance. Through careful prompt engineering, GPT-4 is able to generate diverse, challenging questions that baseline models struggle with. We select ten questions per category and collect answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. We then ask GPT-4 to rate the quality of their answers based on helpfulness, relevance, accuracy, and detail. We discover that GPT-4 can produce not only relatively consistent scores but also detailed explanations on why such scores are given (detailed examples <a href="https://vicuna.lmsys.org/eval">link</a>). We also noticed that GPT-4 is not very good at judging coding/math tasks.</p>

<p><img src="https://vicuna.lmsys.org/assets/vicuna/response-compare.png" alt="response comparison"/>
Figure 3. Response Comparison Assessed by GPT-4</p>

<p>Figure 3 displays the comparison results between all baselines and Vicuna. GPT-4 prefers Vicuna over state-of-the-art open-source models (LLaMA, Alpaca) in more than 90% of the questions, and it achieves competitive performance against proprietary models (ChatGPT, Bard). In 45% of the questions, GPT-4 rates Vicuna’s response as better or equal to ChatGPT’s, and Vicuna’s total score reaches 92% of ChatGPT’s (see Table 2). Despite advancements, those chatbots still face limitations, such as struggling with basic math problems or limited coding ability.</p>

<p>Table 2. Total Scores Assessed by GPT-4. We calculate the total score for each (baseline, Vicuna) comparison pair by adding up the scores obtained by each model on 80 questions.</p>

<table>
<tbody>
  <tr>
    <td><span>Baseline</span></td>
    <td><span>Baseline Score</span></td>
    <td><span>Vicuna Score</span></td>
  </tr>
  <tr>
    <td>LLaMA-13B</td>
    <td>513.0</td>
    <td><span>694.0</span></td>
  </tr>
  <tr>
    <td>Alpaca-13B</td>
    <td>583.0</td>
    <td><span>704.0</span></td>
  </tr>
  <tr>
    <td>Bard</td>
    <td><span>664.0</span></td>
    <td>655.5</td>
  </tr>
  <tr>
    <td>ChatGPT</td>
    <td><span>693.0</span></td>
    <td>638.0</td>
  </tr>
</tbody>
</table>


<p>While this proposed evaluation framework demonstrates the potential for assessing chatbots, it is not yet a rigorous or mature approach, as large language models are prone to hallucinate. Developing a comprehensive, standardized evaluation system for chatbots remains an open question requiring further research.</p>

<h2 id="limitations">Limitations</h2>
<p>We have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI <a href="https://platform.openai.com/docs/guides/moderation/overview">moderation</a> API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.</p>

<h2 id="release">Release</h2>
<p>In our first release, we will share the training, serving, and evaluation code. We plan to release the model weights by providing a version of delta weights that build on the original LLaMA weights, but we are still figuring out a proper way to do so. Join our <a href="https://discord.gg/h6kCZb72G7">Discord</a> server and follow our <a href="https://twitter.com/lmsysorg">Twitter</a> to get the latest updates.</p>

<h2 id="license">License</h2>
<p>The online demo is a research preview intended for non-commercial use only, subject to the model <a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md">License</a> of LLaMA, <a href="https://openai.com/policies/terms-of-use">Terms of Use</a> of the data generated by OpenAI, and <a href="https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb">Privacy Practices</a> of ShareGPT. Please contact us If you find any potential violation.</p>

<h2 id="the-team">The Team</h2>
<p>This is a joint effort with collaborators from multiple institutions, including UC Berkeley, CMU, Stanford, UC San Diego, and MBZUAI.</p>

<p><strong>Students (alphabetical order):</strong></p>

<p><strong>Advisors (alphabetical order):</strong></p>

<h2 id="acknowledgment">Acknowledgment</h2>
<p>We would like to thank Xinyang Geng, Hao Liu, and Eric Wallace from BAIR; Xuecheng Li, and Tianyi Zhang from Stanford Alpaca team for their insightful discussion and feedback. BAIR will have another blog post soon for the concurrent effort on their chatbot, Koala.</p>



      
    </div></div>
  </body>
</html>
